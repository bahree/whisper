Welcome to the Twomel AI Podcast.
I'm your host Sam Charrington.
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded
earlier this month at the 33rd annual NURRIPS conference.
If you've been waiting for the Twomel pendulum to swing from workflow and deployment back
over to AI and ML research, this is your time.
We've got some great interviews in store for you over the upcoming weeks.
Before we move on, I want to send a huge thanks to our friends at Shell for their support
of the podcast and their sponsorship of this NURRIPS series.
Shell has been an early adopter of a wide variety of AI technologies to support use cases
across retail, trading, new energies, refineries, exploration, and many more, and is doing
some really interesting things, but don't take it from me.
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.
They have a very deliberate strategy of using AI right across their operation from the
drilling operations to safety.
Last year, the company established the Shell.AI Residency Program, a two-year full-time
program, which allows data scientists and AI engineers to gain experience working on
a variety of AI projects across all Shell businesses.
If you're in a position to take advantage of an opportunity like this, I'd encourage
you to hit pause now and head over to Shell.AI to learn more.
Once again, that's Shell.AI, and now on to the show.
All right, everyone.
I am here in Vancouver, British Columbia at NURRIPS, and I've got the pleasure of being
seated with Mohamed Siddhamid.
Mohamed is a machine learning and AI R&D manager at Shell.
Mohamed, welcome to the Trumbo AI platform.
Thank you, Sam.
It's really excited to be here today.
Chile, Vancouver, we're very delighted to be in Europe this year.
It's a pleasure meeting you.
It's also an exciting time to see all the latest and greatest in the Culling Edge technology
in this field.
Yeah, absolutely.
I am also very excited to be here.
This is my third NURRIPS, and in fact, I love Vancouver.
The Vancouver Convention Center is, I can't think of one that is more beautiful and in
his scenic context, so I'm really excited to be here.
And happy to see you.
Again, you, of course, were at Tomlcon AI Platforms, and it's wonderful to get a chance
to actually get you on a microphone and get a sense for what you're up to at Shell.
Shell actually has a couple of papers here at the conference, right?
We do.
Actually, we have a couple of conference papers being presented this year.
We have one actually deal with least squares for seismic interpretation and the other papers,
which is basically, we call it the facies net.
This is a new architecture we designed for our kind of well-liked classifications and
rock typing.
So it's part of the geophysics community and how we are applying machine learning for
physical systems.
Awesome.
Awesome.
And so for this conversation, we're going to dive into the facies net paper.
But before we do that, the least squares paper, tell me a little bit more about that one.
That is actually our kind of internal R&D approach for how we use machine learning and deep
learning specifically to actually generate an equivalent kind of workflow processes to
the physics-based approach for full-wave inversion.
If you think about how the geophysicist historically been doing the all imaging using seismic
data to try to identify different types of zonal interest areas, so we actually identified
a huge breakthrough in terms of time-saving, computational simplification relative to the
conventional approach.
So you mentioned phase inversion.
What is that specifically?
That's typically like when people actually have seismic surveys to measure what is the
prospects of finding hydrocarbon on the subsurface, there is an imaging process that actually
generate seismic images from this kind of seismic data.
And this is a process of taking all this data and trying to generate a high-resolution image
where you can pinpoint where is your kind of area of interest look like.
Historically, it's been a very computationally intensive.
This is required almost a month amount of effort, and now a traditional, high-performance
computing has been applied to these types of problems.
And that's quite frankly the highest investments in the energy business, like it's not very
common to see a data center with almost 20, 30 beta-flops of computation cycles in a data
center in an oil and gas company.
And that's actually one of the things we rely on, even for training our deep reinforcement
learning models for this type of applications.
I don't think we got to the inversion, the phase inversion, where does phase inversion
fit into that scenario?
So this is typically when we take the migration of waves into the subsurface of Earth and
trying to reconstruct an image based on these kind of reflectivity of sound waves that
goes to the penetrate the subsurface.
So basically you reconstruct the whole image based on these reflectivity of sound waves.
Oh, got it.
And so this paper, then, with all that as context, is showing what?
Basically, it's showing how we can computationally efficiently reconstruct the whole problem using
a deep learning framework approach.
So relying on sequence models to be able to generate these type of image construction and
using image recognition to be able to classify what the type of interesting zone of areas
within these images.
Oh, nice.
Nice.
And you mentioned reinforcement learning is reinforcement learning a part of this particular
problem, or something that's used more broadly or in other areas.
So actually, we're using it in different areas, typically one of the issues regarding the
space that we work on is the limited of label data.
So it's a very high and certain environment, and typically to be able to confound it in
a well-defined simulation is not applicable all the time.
So we're trying to complement that with a novel approach of defining what is the physical
boundaries of the system look like in order to be able to use reinforcement learning.
Okay.
Interesting.
Interesting.
And so the other paper that is presented here, the FacesNet paper, that's one that you
worked on personally.
Indeed.
Yeah, this is actually a very exciting paper.
We had our kind of whole team, including one of our summer intern from Stanford, the
earth she worked on this paper extensively with our extended team.
And we're very excited actually to come up with a very novel way of designing a new architecture
for how we use sequence model and recurrent neural networks to be able to basically break
out the benchmark, the best in practice standard in the field for how you basically classify
these different type of rock faces.
Okay.
So this paper, like the one we discussed a second ago, is also kind of focus on this exploration
problem.
Indeed.
That's correct.
So you think about the whole life cycle of how quickly you can maturate your hydrocarbon
discovery.
This is typically several months could be 9 to 12 month timeframe.
And by introducing how we can apply machine learning to this type of a problem, we're actually
aiming to reduce that what we call cycle time by almost 50%.
That's why we try trying to tackle the problem by identifying these bottleneck areas, which
really is a computational intensive or require massive amount of data and human effort to
be able to use human judgment to classify and make some sort of an inference.
And so this paper, you're proposing a new network architecture here or?
We actually proposing a new architecture by combining two different type of approaches.
We're using actually a sequence model to be able to actually detect what is the sequence
of these different phases type.
And for people who might not be familiar with that, typically on the on the subsurface
of earth, they are kind of an alternation of sequence between, for example, clean sand,
it could be a clay, a dirty sand, and different types of phases type or lithology types that
usually we penetrate.
And you can think of this as typically a way of identifying where is the area of interest
that you would like to target in your targeting zone.
And the images that are forming your input data are, I'm envisioning them as cross sections,
I guess, that are or a stack of these different types of phases that appear in a sequence.
And you're trying to determine or be able to predict the different layers.
The novel way actually we deal with this, typically the features are representing geological
features, this could be something like a gamma ray, neutron prosity, permeability, and
so forth.
But what we actually discovered that for the conventional method that trying to use machine
landing to solve this type of a problem, they quickly ran into an issue of not taking
into consideration the sequence nature and overlapping by trying to predict the next
type of label of a phase without taking into consideration the previous one.
So what we have done is we actually converted these different well logs.
It's basically like a prosity log using neutron gamma rays to actually as spectrograms.
So basically now you're generating an image with a texture feeling that represent the sequencing
based on depth.
So we're using a short term Fourier transfer to be able to convert all these kind of log
measurements into a spectrogram with a texture representation.
So now you have an image to train based on.
The spectrogram-like image is the output of the thing that you are, you know, that this
paper is talking about or it's the input.
That's the input.
So the nice thing is usually we have a geologist and petrophysicist who actually extract
a labeled sample based on very limited amount of course.
These are sample being extracted from the earth and they were able to actually delineate
what each type of label represents.
So we have some of a small amount of labeled data by subject matter experts.
But the input for this model is the spectrogram image that we actually generate based on
the measurements.
Okay.
And the spectrogram image is based on the same type of imaging that we were speaking about
with the previous paper, the sonar type imaging.
These are different.
Okay.
This is actually one of the, to my knowledge, this is the first time we actually convert
some of the well-locks data that actually think, can think about it as a depth-based measurement
to reflect what is your gamma transfer in earth to an actual image and use that as a
training input for our model.
Oh, got it.
So you've got these walls in place, you've got something that is going kind of up and
down the walls, a sensor head and you're taking measurements of, you say gamma rays and
neutral ones.
Right.
So exactly your spot on.
So these are the typical measurements.
Every time a well has been drilled, you actually take a logging measurement.
So basically you like to measure there is a stivity around the well, which is basically
give you a different kind of representation of what type of rock around that.
If there is a high reflectivity, this is likely to be a sand, it could be a clay, it could
be some sort of a mixture, like a cemented sand or so forth.
So in this particular situation, we have about six different type of what we call physical
properties.
We use it as an input for our model after converting it to these spectrograms.
And the matter of the outcome which we're trying to predict is what type of faces we're
actually looking for.
In this particular situation, we have five different labels.
So it's a multi-class classification.
But that's the unique aspect, historically what most of the state of the art technology
in that field, people are able to actually predict at most two or three classes.
And they kind of misses the majority of other relevant classes because of either the distribution
factor is very low.
And you basically, the high distribution of a class that's dominant is typically to be
classified correctly.
And this is actually something we're very careful about to be able to use our balanced accuracy
as a measure of evaluation, not the general accuracy for the model.
Maybe for a little bit more context, what does being able to predict the facial type allow
you to do?
So this is actually allow the whole field development plan to be able to develop your field, how
you basically determine where is the chance, the likelihood of finding the hydrocarbon
deposition in certain area?
Where do you place your future well?
So this is based solely on exeploration wells.
Usually you have only about five or six wells being drilled at the early phase of development
for your field.
And the intent here is you need some sort of high accuracy, high fidelity that would give
you confidence to design the plan for future field development plans.
So for example, in certain areas you might be drilling up to two, three hundred wells
and you need to be able to identify where exactly you place that well.
So you'd be only interested in one type of facial type.
So this kind of clean sand that's where the reservoir is placed and being able to predict
that area with that type of faces with high accuracy has a huge implication down the
field for your field development.
Well, you know, where this paper fits into the process as being a little bit further
along than the previous paper, the previous paper, you are doing kind of, you know, sonar-based
geological studies to try to get a lay of the land generally before you might put these
exploratory wells in and then, you know, your next phase and I'm sure I'm dramatically
familiar with this, but the next phase might be, hey, we'll deploy a few wells to get
a sense for, you know, what it actually looks like down there and get some centers underground
and then that then gives you the information that you need to figure out where to deploy
your next set of investments which might be much larger in scale and much more expensive.
Absolutely.
You're exactly correct.
So the first paper we talked about is part of concepts, election evaluation.
So how do you basically have some sort of economic assessment evaluation and confidence that
there is a likelihood of potential value in that prospect.
The second paper we talked about is a decision has been made to go with a specific field,
but you need to be able to maximize your development plan down the road in the future.
And so the innovation in this paper again is the a kind of transformation of the features
into this image structure, Graham, this image type and then applying sequence models
to that image type to get state-of-the-art level prediction of the facial, the multi-class
facial classifier.
Exactly.
Okay, let's talk a little bit about the development of the model and kind of what drove the
eventual direction of that.
So it sounds like one of the key inspirations here was that, hey, this is kind of a sequential
a problem that might lend itself to a sequential type of approach.
Did you discover that or is that where you started?
Actually, we had an iterative kind of experiment.
This is part of an ongoing research project in our portfolio.
And we're very kind of focused in terms of trying different architectural design, but
we actually have a brief conceived notion that one of the limitations of the published
literature in the field is you actually overlooking some of the sequence aspect of how these
faces are sequence in the physical system.
Well too, for example, speech recognition where you need to be able to predict what is
for example the next word or the next sentence based on the sequence of your previous text.
So that actually inspires to think about how do you construct a sequence model that would
take into account?
What is this kind of overlapping nature of different faces that the way it actually manifested
itself in a physical system?
So we looked into the recurrent neural network architecture and we tried to determine how
we can actually leverage this sort of spectrogram into that notion.
But in that particular situation, the one of the primary limitation is it will only allow
you to train on a single kind of image texture structure.
And this is typically have a limited amount of information from a practical standpoint
for two physicists and geologists to make that sort of an inference.
So think about one physical property, I mentioned like for example Gamma Ray, has a very good
indication of what face is type is that.
So by taking that initial measurement, that first kind of input feature, converting it
into a spectrogram and using it as an input, now we're able to generate a sequence model
that's only solely based on one feature representation.
But we actually went the step ahead and we said, now we need to think about training based
on multiple images, multiple of these kind of spectrograms.
And in this case, we actually have to adjust our network structure to have an encoder, decoder
up front to be able to take these kind of seek multiple input features set and use it
as a feed forward to our recurrent neural network.
So that's actually one of our kind of a breakthrough moment of saying, huh?
Now we are able to include additional feature representation.
And it's also very kind of dynamic in certain areas where you want to generalize this model.
You might not have the same set of input features, but our architecture is very flexible to allow
people to say, for example, you have feature X, that's not part of the existing input for
the model.
It's very easy to incorporate that and use the encoder decoder layer to be able to actually
use that as an additional input.
So the input to the encoder decoder layer is multiple images, each corresponding to one
of these features, gamma rays, neutrons, etc.
And so you're transforming these N dimensions of images to some kind of single space and
then using that as your feature set for the sequence model.
Correct.
That's exactly.
We have initially a five hidden layer for doing basically the encoding for encoding decoding.
And the output is fed into a sequence model RNN with two layers.
And then the last layer is just a soft knacks, which actually output, which kind of faces
is that output, whether this is a clean sound or a dirty sound or cement and so forth.
The spectrogram images say one of the images, the gamma ray, one, is it a single channel
image or a multi channel image?
It's a multi channel image and you think about this.
This is part of the construction of the depth sequence that's on the initial gamma ray.
So you will have basically the input when you generate the spectrogram is going to create
this kind of a multi channel image that was used as a feature input for our model.
Meaning the channels are the time series of measurements or because I thought we would
have gotten away from the time series by doing the Fourier transforming and getting to
afraid the domain.
That's correct.
What would the channels represent then?
So this is a 3D channel.
So it's a 2D image, but we actually projected as a 3D input for the model itself.
And so when we think about using images as inputs, the obvious thing that comes to mind
is a CNN or some type of model like that or the sequence models that you're using, CNN
type of sequence model or is there some kind of, you know, did you explore that direction
or?
For that direction, for this particular particular application, this is a bidirectional recurrent
neural network.
And the reason being that is especially when you have a limited amount of training set
for a number of these images, the performance and the stability of the model is actually
much, much more stable and able to converge rather than expecting, for example, a CNN
with a very large amount of label data.
So one of the primary reasons that dictated the selection of the architecture we used
is how much amount of data we have and how much labeling actually accurate labeling
for these data is available.
Got it.
Makes sense.
How do you construct your performance evaluation?
You mentioned that you're looking at your bias weighted metric.
Did you have to iterate to that or is it obvious what to do there?
In fact, we looked at different types of loss evaluation functions.
So we looked into the general accuracy and the balanced accuracy, F1 score and so forth.
And one of the things that's typically reported in the literature, people actually look at
the accuracy.
But as I mentioned before, the drawback for that is you always correct with the predominant
class in your kind of outcome.
And in this particular situation, we're very kind of focused about taking that kind of
balanced accuracy in place.
So we came up with our drive metric based on multiple performance indicators to come
up with more human interpretable and also provide some rationalization for the human
expert to interpret that and take it into consideration forward.
But it's been a more an evaluation and to compare that as a benchmarking mechanism,
we actually compared our faces net performance against several of the state of our technologies.
Most people use, for example, naive based classification, random forest and other types
of model to be able to actually do the classifications.
And actually the paper have a very decent representation almost to close to a nine or ten
different algorithms that's been reported and showing the performance difference.
Okay.
And what was the performance difference?
How did you deal with it?
It's quite significant, actually.
The highest accuracy rate that's being reported is about 72%.
So we're actually above like 75% accuracy.
And this is across all the five different classes.
The most actually people get it right for like three classes out of five.
So we are able to accurately, correctly classify more number of faces type than just focusing
on the high representation of classes available in your training set.
Okay.
Okay.
Cool.
Where does this direction of research take you?
What's next and pursuing this work?
One thing actually we're very passionate about.
We always active to outsource some of our architecture.
This is not the first paper we outsource the code to the community.
In fact, we have a GUDNN outsource library.
It's a model that we developed for the GUE science community.
And it's been contributed to our public GitHub.
So we're kind of actively trying to drive this as a mainstream application for machine
learning within the physics community in general.
And so is this one already up on GitHub?
It's actually on its way in GitHub.
As we speak, as the paper is being presented, this should be available for people to actually
stop using and actually contribute to expansion of this approach.
Who are the folks that you expect to be using stuff like this?
Is it a primarily academic research labs or other energy companies or it's a joint actually.
Because quite frankly, we're very optimistic about the level of collaboration between
industry and academics.
As I mentioned, the outcome of this results is a combination between our internal R&D research
team and also some of our people from the academia who were part of that research project.
Moving forward, the intent here is to make these as part of industry standard.
For example, across there's a consortium of open subsurface data universe platform for
experimentation and trial that's been signed up by more than 83 different academic institutions
and industry partners to work on improving different types of or developing new different
type of algorithms for different sorts of application, whether this is for image-based
applications or an structured based type of modeling to be able to speed up and accelerate
some of the bottlenecks, especially for computational time, more about human intensive efforts
that need to be put in place for sorting these types of problems.
Along with the code, do you also publish a data set to support folks researching in this
era or are there well-known or already-existent data sets in the space?
With good news, there's a lot of open source data sets available for that.
The beauty about this approach is enabling people to actually take these kind of generally
publicly available feature sets of physical properties and converting them into these
kind of texture-based images to try to apply our architecture for these type of problems.
This is actually a common challenge that facing most of the previous research in this area,
how you can use a very sparse representation of non-humogeneous feature sets.
The correlation between these geological features is very crucial to be able to correctly
predict what type of faces that expect it as an outcome.
Using this kind of an image-based approach for training, take some of this kind of deficiency
or limitation out of the equation.
Even how important the image creation part of this is, the Fourier transform is that specific
code that you're providing in the repository as well, or is it something that you just
need to explain how to do it, run your data through MATLAB's FFT routine or presumably
you have to tell people how to do that part also.
This is actually in our situation, we recommend people to use the standard techniques that's
being described in the paper, because this is part of the feature, like pre-processing,
like they can be processing, and it varies from one type of a problem to another.
For example, if we're trying to apply this for example sensor data that's coming from
satellites or vibration data that's measured over time, then it might require a different
type of parameterization for that short time for your transformation to be able to generate
the same representation of important features.
But doing it is not a mystery, you just explain how it can be done.
It's a standard process.
But it sounds like you can apply this general technique beyond geological sciences to a
bunch of other areas.
Can you talk through some of the examples you just mentioned?
One of the other examples we also envision using this for is for any type of equipment
kind of abnormality detection, a common example for that if you have an equipment that have
a different type of vibration rate over time, then usually you would have a sensor measurement
over time for how that vibration signal is being captured.
If you actually convert that using the Fourier transform, you would be able to generate an
image representation for how that kind of vibration feature looked like over time and being
able to use that as a representation input for model development is also a possibility.
Yeah, I would expect for this kind of, for like this predictive maintenance thing
where we're talking about where we have a time series of, you know, sensor readings
that the FFT part of that would be fairly standard, right, to get a frequency domain feature.
But the image part of that is the part that I'm still trying to wrap my head around
a little bit.
If you think about the main issue with any type of kind of anomaly detection is usually
the difficulty of determining what is, what can institute an anomaly because over time
they might be a concept drift from what is normal, but that still haven't actually materialized
to the point of being an anomaly, it's kind of a continuation.
Being able to actually represent that kind of change in the distribution over time would
give you a different kind of representation for how things really evolve before you can
actually use a static threshold of determining by exceeding that type of an upper bound
tree you already fall into that space.
So it's, I'm not saying that's the only way of doing it, but if you want, for example,
to account for normal variation within the boundary before it reaches an extreme outlier
level, then that's a way of attenuating that signal to noise ratio to make it more
useful and more generalizable as well.
And so from that perspective, is it fair to think of the spectrogram as, you know, not
so much your processing in image, but you just have a bunch of vectors of frequency components
relative to time?
That's actually a fair representation, and really what you want it to do is you want
to representation that time frequency domain in a more kind of an image kind of structure
to be able to determine which part of that image is more relevant more salient to provide
more informative features set for training your model.
Yeah, I'm still struggling with within the, like when I think about an image, I'm thinking
about things like convolutions that are taking, you know, that are operating in kind of
multiple dimensions and kind of aggregating and things like that.
How does that play out in the RNN scenario?
Or does it play out in the encoder scenario?
I guess it plays out in the encoder, right?
Exactly, right.
And kind of, if you think about it, these are images that's captured over multiple period
of time.
Kind of depth-based is reflecting, taking an image a shot at each kind of depth-based approach.
So you can have, every have a feet, you would have a different image generated, and you
would be able to see a transition as if you're having a moving object from point A to point
B.
Every time you take a shot, that frame would represent a moving aspect or an adjustment
to the image that's being captured.
That makes sense.
Thank you for helping me get there.
Thank you.
But I'd love to hear you kind of talk a little bit about some of the other things you're
working on there.
I've had your colleague, Dan, on the podcast a couple of times, talking about some of the
work that you're doing in the platform space, and you attended the Twilcon platforms conference,
kind of in furtherance of that.
So clearly, you're building platforms to support lots of different things.
What are some of the other things that you're working on there?
I'm glad you actually raised that question.
I think people typically, when they think about how you operationalize these type of models,
they think about maybe I'm getting a very kind of high-accurate model.
It's performed very well on my training data and testing data set.
But we're actually very kind of thinking about and very focused on the idea, how do you
make sure this is deployable and scaled to a large application use?
And that's why we kind of designing and building this kind of expandable open architecture platform
based on Kubernetes, to be able to actually commissioning the same type of development
and training environment and being able to push it to different types of deployment mechanism.
Currently, we have ways of deploying models within, for example, our central data warehouse
or typically we push things to the cloud.
We have a very likely distributed kind of an environment for hosting these applications.
But the key ingredient here is how do we actually go from point A to point B because in order
to be able to provide real insights in a very timely manner, the amount of data acquisition
and data transformation is very crucial to be able to handle in a more systematic way.
And that's why all the pipelines and feeding of our crucial kind of feature sets that
are required for any type of a model is something we spend quite a bit of time of designing
building and making sure that everybody have the access to this in order to be able to
using it in a day-to-day operation.
So that's the primary essence of designing the platform.
One of the things we're also very passionate about is how we can right now have a very
large kind of a library of models that's actually reusable.
We hardly tend to start things from scratch.
We have a library of dealing with things about ingesting a large amount of different types
of data structure, whether it's time series or unstructured or large seismic volumes and
so forth.
And then being able to very quickly scale these ups and run them in a massive pedal, I think
the largest kind of number of models, we have about close to 500,000 models for different
predictive subsystem units across the whole organization, being running in real time.
So someone could go to some catalog and choose FaciusNet and just drop it into a pipeline
somewhere?
Exactly.
So they will specify, for example, what is your feature set look like and where is actually
located?
You specify, for example, the volume that you're going to be trained on and that would give
you some recommendation about what kind of GPUs you would require and give you also an estimate
about what time it actually takes to complete that job.
And we're putting that in perspective as a benchmark for what is a conventional approach
and workflow would take typically.
And I think we're very pleased to see that we're making a significant amount of progress
on reducing that cycle of time.
Also so that you announced AI Fellows program, can you talk a little bit about that?
Yes.
Actually, our shell.ai residency program is one of the exciting programs we're happy
to announce recently.
This is actually one of the programs we announced last year in NURPS.
We have happily now have our first cohort of AI residents being on board.
This is a two-year program where we invite people who are PhD or postdocs or some people
with an industry experience in machine learning and AI to come and work with us across different
type of challenging problem, basically defining what is the next wave of solving these challenging
problems, whether this happened to be in the energy space or how the whole energy transition,
new parts of the business, to be able to work on these problems for two to three different
rotations.
So currently we have people who are working on the hydrogen business, how we optimize
the generation of hydrogen across the whole of our new energy platform mix, people who
are working on the interesting projects in the exploration side, people working on the
trading and asset optimization, people working on predictive maintenance.
And the goal here is it's a two-way collaboration.
It gives people the opportunity to shape the direction and innovation within machine learning
application across different types of domains, but at the same time we provide enough training
challenges and support for people to publish, collaborate and openly challenge the status
goal of how we do things.
You said you have your first cohort in place or are you out there recruiting your second
cohort?
And actually we currently actively are looking for the brilliant minds to come and create us.
So always welcoming people in our space and we encourage people to check out shell.ai
residency program on our shell website, very kind of interesting.
We have a lot of Q and A's and a little bit kind of a videos about the existing residents,
what they're working on and what's their experience so far.
Awesome.
Awesome.
Well, Mohammed, it's so great to catch up with you as always.
Any kind of parting words about your experience as far in Europe?
We're just kind of getting started, but it's an amazing, actually.
The last couple of days I had been very fortunate to participate in different affinity groups.
Actually, this year also we sponsored a woman in ML workshop, but I actually had a chance
to stop by Black and AI and other type of affinity groups.
It's really interesting.
Lots of interesting ideas, the poster session yesterday was fantastic, and I'm looking
forward to actually spend the next few days and getting to see more about these kind of
outstanding ideas coming through.
Awesome.
Awesome.
Same here.
Well, thanks once again.
Thank you Sam.
Really pleasure talking to you.
Thank you.
My pleasure.
Cheers.
All right, everyone.
That's our show for today.
For more information on today's guest or our NURPS podcast series, head over to twimmolaii.com
slash NURPS 2019.
Thanks once again to Shell for sponsoring this week's series.
Check out the Shell.ai Residency program by typing Shell.ai into your browser's address
bar.
Thanks so much for listening.
Happy Holidays and catch you next time.
