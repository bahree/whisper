Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Hey what's up everyone, producer Amari here.
Before we get to the show, I want to remind you that our study group for the Fast AI
course, a Code First Introduction to Natural Language Processing, begins this Saturday,
December 14th.
This course will cover NOP applications like Topic Modeling, Classification, Language
Modeling, and Translation.
To join the study group and the broader Twimal Online community, head over to TwimalAI.com
slash community.
Submitting that form will trigger an invitation to our Slack, and once you're there, join
the appropriate study group channel.
Hope to see you online.
Alright everyone, I am on the line with Stephen Merritti.
Stephen is an NLP and deep learning researcher.
Working on a start-up, I'll let him mention that.
Stephen's a long time friend of the show and was a participant in our conversation back.
One exactly was that, it was TwimalTalk 234, but that was all around the OpenAI GP2 release
and we had that great panel discussion about the controversy that surrounded it.
But now Stephen's back for a standalone interview, particularly on the topic of his recent
paper, single-headed attention, RNN, stop thinking with your head.
Stephen, welcome to the TwimalAI podcast.
Yeah, I'm looking forward to diving into this, but before we do, you didn't have an opportunity
to give us a full background when we did that panel.
So why don't you share a little bit about how you got to working in this area?
Absolutely, I was lucky to actually start working in natural language processing in late
high school.
Brilliant professor by the name of Dr. James Kern helps run a summer camp for high school
kids, and there he introduced, not just programming, but also natural language processing.
And back in those days, well before deep learning, it was far more of the traditional methods.
But I got obsessed with it from then, so basically the second I was at university, I was interested
in working in the field and published some interesting papers primarily on maximum entropy
models and on combinatorial, categorial grammar pausing if any of the readers' viewers
are aware.
But since then, yeah, it's been a bit of a well-end, so when I came over to the US, I went
to Harvard University for masters, and there I was really obsessed with large data sets,
thinking that was the right way to kind of machine intelligence or artificial intelligence.
And since I left then, I worked at a nonprofit called CommonCroll, who they basically
download billions of pages from the internet every month to turn into a data set, so think
of it as your kind of own small version of Google's data store.
Then I worked at Metamind, which was eventually acquired by Salesforce and became Salesforce
Research.
And that was where I got most interested in language models.
And since then, I've left Salesforce Research and started my own startup called DDX times.
And the idea with that is that I think that language models are amazing tools, just kind
of new ways of handling a lot of the computation and linguistic costs that we think of every
day.
And I'm really interested in kind of unlocking language as a data set, you know, beyond
the kind of surface level work that we traditionally do.
So the tagline of the company is, you know, language is humanity's longest running program.
And it's kind of accumulated all of this craft and all of this information over time and
hopefully language models will be the way to start unpackaging that and helping humans
actually look at millions of web pages or, you know, hundreds of years of documents
without having to read through it all themselves.
Well, since language models will be playing such a prominent role in this discussion, maybe
we should start from the top and have you share for those that aren't familiar with
the terminology, what a language model specifically is trying to do.
Absolutely, so the kind of most likely example of language models that everyone has probably
run into is the order suggests on your mobile phone.
So as you're typing some phrases, kind of highly likely and instead of you're hitting
each of the keys on your mobile phone, it'll just pop up a word and suggest that instead.
So that's kind of the example of language model that most people see.
But it turns out if you keep scaling these language models up, if you use, you know,
a far more powerful device than your phone, just trying to predict the next token, whether
that is a character, a word, or a piece of a word, then the machine learning model that
you produce there ends up encapsulating a lot of kind of interesting pieces of knowledge
about not just language, but kind of the structure and uses of language itself.
So there have been amazing examples of just predicting the next character on language modeling
data sets.
One example is OpenAI's sentiment neuron, where they, without giving the machine learning
model any extra information, they gave it a number of Amazon review reviews and they
asked it to predict the next character.
So imagine you're just sitting at the keyboard trying to guess the next character that someone
is typing.
That's all this machine learning model is doing.
But from the knowledge that's distilled in that, it's able to accurately predict whether
it's a positive or a negative review.
And in fact, when you're generating, you can basically also in a language model, continue
generating as if it's a positive or a negative review.
So if I was reviewing my headphones, I would say, well, make it positive.
And like the headphones have a beautiful sound, crisp, and a very long battery life to
draw, but then you could flip it negative and language models, say, the headphones have
static and are constantly annoying and slight ringing in my ears.
So these language models, as you keep scaling them up, seem to be capturing more and more
at the very least surface knowledge of text, if not kind of deeper, interesting connections
that humans might not be fully aware of.
And so this example you gave, and examples like at GPT-2 and others where you're predicting
paragraphs and paragraphs, it's all coming from this fundamental ability to predict the
next character.
Yes.
And in those cases, it's usually the next token where that might be a word piece, so you
might break a word down into, let's say, running because running or so on, but exactly
that point is just predicting this next token.
And you can, of course, make the task more complex, but it turns out even just providing
a huge chunk of text without any additional information about, say, the domain of the
text or where the text comes from, even the language of the text, you're able to stop
bling out these interesting details.
And so this paper that you just published, the SA R&N paper, tell us a little bit about
the motivation for the paper.
Yeah, there are two main motivations for the paper.
One is that the language modeling field of late has primarily been dominated by just
a single type of neural architecture.
So a single type of machine learning model that's being created to kind of solve the problem.
And as I've seen from both language modeling itself, but almost every other field in machine
learning, there's usually many different ways to tackle a problem.
And just as one technique has gotten a lot of advantage in recent works, doesn't necessarily
mean we should be focusing all of our time, all of our effort on that.
So what I did was rather than using the kind of dominant neural architecture, which is
called the transformer architecture, I used an older architecture called the LSTM,
long short term memory, many of your listeners probably would have heard of, to get basically
quite similar state-of-the-art results.
It's still not quite at state-of-the-art, but I also used far less resources, which was
the second part.
I really wanted a language model that was trainable by the majority of people, because
a lot of the more recent language models have required tens or hundreds of thousands of
dollars worth of either cloud compute or equipment and so on.
So it's really asking the question of what would this field look like if we headed potentially
a different direction in the past or saw different results and everyone pushed on a different
direction.
And also are we sure that we can't achieve many of these results with far less time, far
less compute, which will mean that it's more available for more people, more researchers,
more practitioners to actually use these results and try it on their own data sets and
tasks.
You mentioned a couple of things, which I'll take a quick second to point folks to some
references from here on the podcast.
Back in July, I did a show with Emma Strubel, who did a paper on the environmental impact
of just the kind of large-scale language model training that you're referring to.
As you mentioned, these models take tons of compute to train and that is all powered
by energy and has a huge environmental impact and she traced that down to the actual carbon
load required to train some of these models and made some comparisons about them.
So that is an interesting show for folks to check out and then we'll be talking a little
bit about LSTM's and of course the authority on that is JÃ¼rgen Schmidhuber and I just
show with him about the LSTM back in August of 2017, until we'll talk 44.
But before we talk specifically about the model you created based on the LSTM, you know,
walk us through the transformer architecture that folks are spending so much time on
now, what is it doing?
How does it work?
Absolutely.
So the transformer architecture takes a quite different look at how to do language modeling.
I might start with LSTM's or RNN's recurrent neural networks and the word they're recurrent
is that it takes a word at a time and kind of imagine your reading left to right over
a page.
Well, transformers have a very different take on this.
You get up to a current word, let's say I've just thrown a page in front of you and I
ask you to kind of guess what the next word would be at the bottom of the page.
Your eyes might dart around to multiple different places on the page and in fact, you know,
you might look at first maybe the general topic of the language without looking at many
of the words individually or how they're composed.
You might skirt around and look whether or not you see specific persons name.
So the general idea behind transformers is that they actually use attention over dozens
or hundreds or thousands of the past kind of words or wordpaces that you're looking at
on the page and each of these words end up kind of doing the exact same thing.
They look at all the surrounding words as context using attention.
So a lot of the time, the most important thing for attention is it'll just look at words
that are local to itself.
It'll look at the word previous to itself for the last few.
It might also focus on other most likely related concepts.
So if you saw there were maybe a pet name on not petting a animal species on the page,
you might look for other animal species on the page or something else like that.
So this isn't potentially the best example of describing transformers.
It's been a while since I've had to try and think through it from kind of the ground
up.
But the idea is each word in the page would perform attention to get context about itself
and about the other words on the page, which also enables you to do some very interesting
differences to traditional models.
These models usually don't have any concept of what word is necessarily before or after
it.
It will usually learn that or have some amount of that knowledge injected.
That also means you can extend it quite readily to tasks such as images where you might be
talking about pixels looking around orals or audio as well.
So they're not kind of fundamentally sequential models like an RNN or LSTM.
The relationships between the words are built up solely based on this attention mechanism.
That's absolutely correct.
Yeah.
There's no sequentially necessarily forced on it.
You can add in a few kind of hints or ideas like that.
But it suddenly doesn't have the same idea of sequentiality as like a recurrent neural
network where you're kind of walking one word at a time.
And because of that, one of the main advantages is that you can parallelize a lot of this.
So rather than having to walk one word at a time when you're doing processing, transformers
can usually perform all of these computations across these words in parallel as well.
So there are certain advantages that's primarily during training, but there are certain
advantages about the way in which you can parallelize the work too because you make less
of this constraint on kind of sequentiality.
You kind of mentioned the couple goals.
One was to kind of just take on the idea of transformers and explore whether you could
achieve similar goals with other types of models.
But that is kind of a starting point of inspiration. How did you end up at a specific model this
Shah RNN?
In the past, most of the language models were kind of fixed around either recurrent neural
networks, but they're also some convolutional neural networks.
So almost as similar purchase you would have for handling a vision task.
But for me, in particular, I appreciate and like the aspect of the LSTM, I think it's
more in line with how, at least I can visualize how I would read or think about reading text.
And that's suddenly nothing to do with how the human brain thinks or whatever else, but
it's suddenly at least an abstraction that I can think about and reason through.
As you're reading text usually, at least I don't know about you, but I usually go woodward.
And I might glance back at a different part of the page, but I'm suddenly not glancing
around the page for every word that I'm looking at.
And so the LSTM or the RNN are far better fits for that.
You have an interior kind of hidden state, so some sort of memory, and you're storing
aspects of the current text that you're reading in that memory and using that to predict
the next word.
And that I feel is, at least in my mind, a kind of more intuitive fit for how I read compared
to, say, these multi-headed attention mechanisms used by transformers, where it's equivalent
of every word you look at on the page glancing for some of these models, hundreds of times
at different locations on the page to pull information back and forth.
And they don't generally have this concept of memory as well.
Some of the most recent transform of paper papers have added memory, but it's suddenly
nothing quite like the LSTM's at the moment, which I feel has kind of, as I said, a far
better, intuitive understanding of how we'd go through and reading text.
You know, talking about these two different types of models, you know, we're contrasting
the attention mechanism versus the sequentiality, but they're not necessarily mutually exclusive.
In fact, you use attention as part of the Shah RNN, and attention has been used in conjunction
with RNNs for quite some time, isn't that right?
Yeah.
And that was one of the aspects that I was particularly interested in, and actually researching as well.
The question of how much of the most recent results for transformers are mostly results
of these attention mechanisms themselves, versus, you know, maybe what aspects are missing
from previous models.
So transformers, they are entirely this multi-headed attention concept, versus LSTM's
in the past, have generally, mostly just being the LSTM and only added maybe a layer of
attention here or there, very sparsely.
But the research seemed to have stopped in probably 2017-18 on extending LSTM's direction.
So rather than adding attention heads to these existing RNN models, it was kind of literally
the paper that started, a lot of this was called attention is all you need, which gave
kind of rise to this transformer approach.
And people have seemed to stick pretty close to the paper's title, using almost purely
attention for a lot of these tasks.
So before transformers took off two or three years ago now, I was also exploring using
a very simple form of attention on top of these RNN models.
And it was referred to as a point of sentinel model.
And the idea is it was that it would allow your LSTM, which is mainly just looking at words
one at a time, to look back with like one passer of the page to try and pull some information
back.
So the kind of intuitive idea was imagine that I give you a highlighter and you're allowed
to highlight some of the words on the page that you think are relevant to predicting the
next word.
And that had incredibly good results, but it wasn't kind of properly integrated into the
model. It was mainly about pulling in kind of rare words that the model might not understand
from an kind of information point of view, but it might understand from a positional point
of view.
So my last name might be a good example of that.
Maybe the model has never seen Stephen Merritti, but with an attention mechanism, you highlight
just Merritti and say, well, just pull that word copy and paste it here.
So that was the idea initially behind my point of sentinel.
And that was based off of a lot of work on point networks, which were kind of precursors
to these transformers and multi-headed attention.
But allowing the RNN itself to actually use that to update its kind of internal repository
of knowledge to update its internal memory wasn't heavily used beforehand.
And so that was the direction that I was interested in taking it.
And also asking, well, these transform models have had great success with these dozens
or hundreds of heads, how much, how many heads do you actually need for doing this work
and how complex do they need to be?
Is it enough to literally just do one highlight of the page or do you require 20, 100 different
colors for your highlighter and different levels of granularity?
And the most surprising result for me is that a single head of attention, so allowing
the model to kind of glance back at the page once, is enough to get near state of the
art results when you kind of combine it with an LSTM.
And now is there a kind of measure of the complexity of the LSTM network, like the number of time
steps or something like that that is relevant here to get a sense for, well, complexity.
Yeah, there are a few different directions we could look at.
And that was also one of the ones that I was interested in thinking about.
A lot of the time, these larger models aren't actually kind of sensible to put into production.
They either make a great deal of references or they require a great deal of memory for
keeping, you know, those references in memory or a great deal of like flops, floating
point operations are basically the certain amount of compute for your GPU.
And yeah, part of the idea was that because I'm using only a single head of attention,
and it's a far simpler one, that's the reference to stop thinking with your head, I'm actually
able to add in far more tokens into this, the past reference kind of memory window.
So the size of the page, if you want to think of it that way, that you're able to look back
over the text, but also that LSTM's, because it's just got this one time step, when you're
generating, you're interested in just producing one time step.
And LSTM by itself, you have a very small amount of computation you need to do.
Versus a transformer, as we mentioned, transform is at least so far, don't have any aspect of
this memory.
So every word that they look at, you have to do this full computation over again, and look
back all the way back over the page, doing a great deal of compute usually.
Is that at both training and inference?
Yes, basically.
And it also gets particularly interesting when it comes to inference, because a lot of
the parallelization advantages that you had earlier on for training kind of disappear.
Even training, you already have all the text kind of, you can cheat and imagine that you're
able to predict the next word, because you've got this written down text exactly as it is.
So it's equivalent of, I guess, you or me like reading over an existing paper or article,
versus when you're generating, you obviously can't kind of glance forward to what you would
have written five or ten sentences ahead.
And so the model has to step very slowly, one step after the other.
And when you have a kind of parallel model, as you do with the transform.
And so the model, you're able to train it on just a single GPU.
Yeah.
So all of this lived on a single GPU.
Thanks to Nvidia, they donated to me some time ago.
But yes, a single GPU and almost all the experiments were under 24 hours.
So there's this being this huge push towards cloud compute and everything else like that,
which there's absolutely no problem with.
But they're certainly generally not cost effective.
And I generally want my research to be replicable by, say, a grad student or a freshman or just
someone who's interested and maybe just has the GPU and their gaming laptop.
I think there are so many interesting different potential directions for research that it's
a shame to limit the field to people who have huge amounts of compute or some way of getting
free credits on these cloud services.
I've certainly been offered enough of them.
There are many companies that offer these credits to try to encourage people to play around
and they'll usually give it to the researchers or universities or so on.
But at the end of the day, if not everyone has access to those same kind of free credits
and free resources, then we're limiting the field and limiting the breadth of people
who can contribute.
So you created this model based on attention and LSTM and then you ran it against a benchmark
in this case with the NWIC 8, Hunter Prize Wikipedia data set and you found reasonable
performance with that.
Tell us a little bit about the approach to benchmarking you took here.
So the NWIC 8 data set, it's also referred to as a Hutter Wikipedia Prize.
And the idea was that it's just grabbing a possible dump of Wikipedia.
So that's the first 100 million bytes also of English Wikipedia from it's many years
back now.
I think it was 2006.
And this is kind of an interesting data set for a number of reasons.
One is that it's traditionally being used for traditional compression.
So if you think of zip files or that type of thing, this was the data set that was frequently
used to show whether or not a new compression technique was better than past ones.
So it has been kind of heavily thought through by a number of different people in terms
of traditional data compression.
And one of the fascinating things about language models is that they are actually also data
compresses that certainly not used generally to actually compress files that we might deal
with every day.
But by being able to better predict the next token, it actually means you're able to spend
less and less bits storing the data set itself.
And this has been considered a potential demonstration of kind of the idea that compression and machine
learning and artificial intelligence kind of all wrap up into one thing.
Because compression at the end of the day is being able to produce a larger sequence
from a smaller sequence because you have some knowledge about the structure of how the
data goes together.
And in the exact same way, if you have a knowledge about the structure of a given language
that you're talking in, let's say it's English, or the structure of XML, which is also part
of this data dump, you're able to better predict the next token, which means you're able
to better compress the file.
So this is a really interesting data set in my mind.
And it also, of course, runs over Wikipedia, which has already a great kind of collection
of all of the world's information in it.
So that was the data set that I was interested in exploring.
And yes, it's been one of these standard data sets to explore for language models for
quite some time.
And it turns out with this relatively simple architecture that kind of jumped off in a
different direction, my sharp LSTM, I was able to achieve results that would have been
instead of a year or two ago, unfortunately.
But in able to achieve these results in 12 to maybe 20 hours, depending on your GPU or
depending on the exact formulation of the model.
So far, faster than a lot of these kind of larger models and almost with a minimal size
as well.
So does the result and performance that you've seen in the benchmarking generalize to other
types of tasks?
For example, many might be familiar with these transformer models and language models like
GPT-2 through just a text prediction type of a task where you give it a prompt and you
have it generate some number of words.
And to expand on that, often, we're surprised at how coherent that text reads.
Would you expect that this model performs similarly on that type of task and generate
text that feel so kind of spookily human?
Yeah, it's a great question and it's one in merely, I don't have direct evidence for
yet, but at least for this model and this data set, when you compare it against the other
models, which are all transformer based almost, the numbers are quite similar and when I'm
looking at the output, it's quite similar.
The difference would be in scaling.
So many of these existing transformer models have run over far larger data sets.
The N-Wik8 data set is only, unfortunately, about 100 megabytes of Wikipedia.
So far smaller than many of these models that are now training over dozens, hundreds of
gigabytes of quite varied text.
So the model I use, it certainly produces some fun and hilarious and interesting Wikipedia
pages.
In fact, I love using it to get the kind of reception, like the New York Times writer
said that this article was the most terrible idea in all of existence.
Usually this model ends up with beautiful, pithy quirks, but unfortunately, because it's
mainly focused on the Wikipedia domain, it isn't as expressive as a lot of these larger
models trained over far more varied texts.
Now that isn't an issue with the model itself, that's an issue with the data set that I have.
And the fact that I was mainly trying to get it to run quite quickly.
But I don't think there is necessarily a limit or going to be an extreme difference in how
this model would perform for a lot of these kind of text completion tasks.
But you would have to scale it up.
And one of the other main centerpieces of the article, the paper that I was talking
about is the fact that because there was so much attention on performance for the
past few years, no pun intended, there's models that ended up getting a lot of the focus
when it comes to both the research, but also the practical engineering behind how to make
these models scale.
There are indeed certain advantages that just make it far easier to scale transformers.
But there are also many known techniques to making other neural networks, formulations
faster such as the LSTM, that we haven't actually pursued because we're all going off in
this transformer direction.
And I feel like that's a bit of a shame, because as more people spend more time working on
a very particular brand or a particular style of solution to a task, these other solutions
which might not actually be worse kind of fall to the side.
And so that's really part of what I was interested in kind of pointing out with this research,
but up until kind of fairly recently, these results would have been steady art.
And maybe people have been focusing more on how do you optimize an LSTM or how do we improve
the way and we can form or how do we scale it up.
But that kind of tension instead went to transformers.
When I've played with models like GPT-2 in the past, it becomes very clear, very quickly
that the model, it's not creating new texts, it's kind of stitching together things that
it has memorized or seen in the past.
And these transformer models, because they're huge models, they have a huge parameter
space and they can memorize a lot of stuff.
So does a smaller model, you've talked about scaling it up, can it scale up in the same
way so that it can remember as much stuff?
Yeah, the concept of scaling up the model, I guess you can go in two different directions.
One is you can scale up a small model to handle a larger data set or you can of course scale
up the kind of model architecture from this smaller model to a larger one and also at the
same time throwing more data.
I guess let's pull that apart.
So one is scaling up a small model to a larger data set.
And it is going to be a limit on how well, at least in the kind of traditional ways that
language models operate, a small model will work.
There's just, you know, some limit, it's not going to be able to perfectly memorize all
of Harry Potter if you, you know, all of these different books, if you're feeding them
in, just because there's a finite amount of space, a finite number of parameters that
this language model is able to use.
But that's also kind of a different question compared to can a small model be able to
coherently generate text, maybe especially if you gave it, you know, the first few pages
of Harry Potter to thumb through because we wouldn't call someone unintelligent if say,
I don't know, I gave the first few pages of Harry Potter to my friend who would never
read the book and say, try and continue writing along.
If they don't make a reference to Snape, that's a very different kind of problem than
them not being able to actually coherently generate text.
So we are kind of conflating two aspects of language models.
How much can the language model memorize from the world around it, you know, how much
of the text that's been written or topics in the world can it kind of cram into its memory?
And that's usually the form of parameters, that's where the lot of these models scale.
And the secondary question of, well, can it coherently generate text, you know, how much
of it is actually some level of understanding of the structure of text or how text might
work and how much of it is actually just kind of, as you said, reproducing what it's already
read, what it's already kind of memorized.
And I'm personally, hopefully, not convinced that we have to keep scaling these models
up, you know, as a human being, if I, you know, gave my friend that task of completing
some Harry Potter fanfiction and they'd never read the books before, maybe I could point
them to the Harry Potter Wikipedia page and maybe that would be enough.
Maybe they want to thumb through a Wikipedia that's specific to Harry Potter.
Maybe they need to read through the book, but I suddenly wouldn't have to, you know,
get them an extra chunk of brain just in order to handle that type of task.
And so that's the other kind of question and hope in my work.
Maybe we're scaling up in the direction of being able to memorize more and more of this
data far too early compared to, you know, having these models potentially be able to refer
to new data when it needs to and thus be able to generalize better.
So yeah, I do think that the two quite different questions, which isn't to say that they also
don't interoperate and interact as well, because one of the fascinating things about text
is that it's really, if you think it was like memorizing results, humans have spent
a lot of time generating the text and usually text is beautifully distilled, condensed,
kind of form of knowledge.
And so there is obviously a lot of benefit to that.
And there are obviously ways in which that can help a model train, but, you know, how
much of that is necessary or maybe a completely different question is, do we think that these
models can generate something that it hasn't read about before?
Because that is potentially, you know, there are two different directions we can think
about with language models, one is kind of minimizing the complexity of these data sets.
The idea that I'll never be able to read all of the internet or all of Wikipedia, but
hopefully I don't necessarily need to get to the result or to the information that is
interesting or useful for my task.
But then the second one is, well, if you want it to generate something, as we've been
using with GPT-2 and Transform Excel and a lot of these language models, if we want
to get it to generate something that's actually quite intelligent, is it enough to be able
to kind of copy together other pieces of knowledge from the past?
And I don't know.
It's an interesting kind of very open question.
Cool.
There's a section of the paper where you go into a discussion around tokenization attacks
and that kind of thing.
What was that bit all about?
Yeah.
So I'll admit this is a section where I'm still having to rethink a lot of what I'm
pondering, but one of the questions is that I published one of the standard data sets
for this task called WikiTax103.
And generally, when you're trying to determine whether or not a language model is performing
well or not, you have a specific data set and you get it to try and predict an under like
a test section, a section that hasn't seen before.
And you asked how confused were you by say, you know, all of these words in succession.
And tokenization is the idea that you can break these up into kind of different components.
You might be able to say, just split on spaces and you would consider those words.
So running, ran, and so forth end up all being separate, but the other concept is you
could break it up into parts of words.
So the example I have is specialized because you have special, you have specialized,
specialized.
If you break it up into those three different chunks, you can say, well, okay, the word
special means this type of thing.
When I add the suffix, eyes, you know, I specialize in a field or I specialized adding
the D, the additional suffix on top of that, you can see you break the word up into these
components where even if you don't necessarily understand all of one part, you can better
understand other pieces.
So that was part of the idea behind the tokenization attack, whether it was actually fair for these
different models to compare the numbers by breaking up across these different tokens.
Turns out I think it actually is, I've been turned smarter people than me have pointed
out many of the issues in my thinking, but there's also still an issue when it comes to
generating text.
So if you've seen from GPT2 or if you've been playing around with that or these other
online models, most of the time these models have been trained with what we would consider
gold text.
So text that we would hopefully have pulled from an intelligent source so it actually makes
sense.
And if the language model say guesses the wrong word next, it can recover because you're
telling it well actually know that word was wrong, you actually meant to say this.
But if you've played around with these models and you've had it start to generate something
incorrect, it'll usually start to say getting into this weird loop or start to repeat the
wrong fact over and over and over again because it says, well, okay, I've seen that text
generated.
I assume that everything before it was correct and I'm not going to go back and fix it.
So the different sizes of these text chunks can also impact that when it comes to generation.
But yeah, the tokenization attack I've got to admit is less of a well formed argument
as it turns out.
It did kind of feel a little bit like you were geeking out and having fun after the
real work of the paper was done.
I think it was less well formed than the rest of the paper.
So I'll have to admit that much.
Yes.
You know, is your hope that folks will build on the sharp RNN or you know, put it in the
production, put in the actual use like what do you, you know, what's your ultimate hope
for this work and where do you see it going?
Yeah, I've had people already contact me about pushing something like it into a
production, because there are a number of advantages, especially for production usage
or for if you're running small models.
There are some advantages to it that you don't get out of the transformer.
But honestly, my main hurt is for us as a field to kind of think through these two different
directions that we're going.
One is that we have something that works.
We have language models and they seem to work well, turns out scaling them up.
They generally seem to work better.
But as we scale them up, we are kind of locking ourselves into two different things where
we're focusing all of our research on this one specific formulation of how to solve the
task, which we don't know is necessarily correct yet.
And we're locking in a lot about engineering effort into solving the task using that particular
formulation, which means that it's more and more difficult for different potential
approaches to actually even get a start.
The model slower, or it's just not able to be scaled because the frameworks aren't there
and so on.
And I am hoping that the community kind of really thinks about that, because if we kind
of followed this example, I'm not sure if your readers are aware, but one of the first
deep learning examples was the cat example that Google did in, I think it was 2012, where
they used, it was along the lines of 16,000 CPU cores to kind of look at small images in
order to better classify our number of small classes amongst them, whether or not a cat
was in the image.
And that kind of sounds crazy now because you can do that on your mobile phone.
But that was because we also went a different direction and we realized GPUs were better
for kind of solving this type of task and we have improved the architecture and so on over
time.
Imagine kind of a different history where we instead focus just on using CPUs like this.
Maybe the only people who'd be able to do machine learning at this point were the Googles
or the Microsofts or what have you that have the 16,000 CPUs hanging about.
And so the more we focus on a particular direction and forget about the fact that these
models are becoming larger and larger and more unwieldy to train and that there are potentially
other more efficient ways to solve the problem.
While we might find ourselves locked into a future where only a handful of like large organizations
are able to do this research and the worst part would be we'd be locked into this future
not because it's the only way forward but because it's just the way forward that we've
all pushed.
So that's really my hope for this paper.
But additionally if people use the model itself I'll be glad to.
So kind of put another way your general senses that we've kind of over indexed on exploit
in this particular case and you're kind of encouraging us to explore more yet there's
more there to to try.
That's a beautiful way of putting it yes we have exploited a great deal and that doesn't
necessarily mean it's the best solution to the task and the the worrying part is you
know the more we exploit in one direction the more it seems that all other paths are useless
but that's not necessarily because that useless but because of the amount of effort put
into just you know making this one design work.
For folks that want to play around with this you've got code available on GitHub is that
right?
Indeed there's code available on GitHub it isn't the most beautiful card but I am also
going to be rewriting it some point soon with a particular focus on using it in production
and using it for a number of different tasks because the code base as it stands is really
my research code base where you know you have a number of different options in here you're
poking and prodding and trying things out and certainly haven't optimized it for speed
even though it's you know able to train in only 12 or 20 hours on a GPU.
But yes there's code available if anyone's interested in playing around with it as a few
have already started doing please do so and I'll help as much as I can but yeah the
code is available so get to playing.
Yeah and I just pulled up the GitHub page and noticed that one of the kind of assumptions
lies assertions that I made earlier was that this model would have fewer parameters than
the transformers and at least in the case of the ones that you've mentioned in the table
that's not necessarily the case.
Yeah the number and on that remake page is actually a little higher than it actually
is in the paper 53 million parameters versus I think it's 41 and yes I mentioned that
in the paper as well there are a few different directions for that.
One is that LSTMs they generally need to have a fairly large hidden state in order to
kind of retain memory.
If your readers are aware of how it works the recurrence the kind of forget gates means
that it kind of continues left to right I won't go into too much detail about that but
yeah the exact in terms of competing purely on parameters there are models that are slightly
better and that's really quite reasonable.
I also don't think I mentioned the adaptive transformer which does a fair degree better
as well but this is more of a proof of concept paper it's possible that you could kind of
trim the shot LSTM down to try to to catch these as well but that's also then a question
of you know do these necessarily train faster to the use less compute when it comes prediction
time to so it's really a bit of a trade off on a number of different directions.
Yeah yeah I guess the biggest thing to jump out to me is that my intuition that the reason
why transformers you know these large scale transformers were better at kind of pulling
all these texts out is because they had more parameters but that's not necessarily the
case.
No the multi-headed tension like that's the only thing that this paper is not saying at
all it's not saying that my method is better it's just kind of asking like you know how
many attention heads are important how do these necessarily work because you know I could
have well and fully found that you know attention heads were the only thing that was necessary
for this but that didn't end up being the case.
But one of the interesting things about these transformables is you if you see those results
the LSTM is only full layers deep so there are four LSTMs that you kind of pass the information
through versus these transform models the two that we were talking about that have lower
parameter accounts that have 12 layers each.
So you can also either reuse parameters or perform a great deal more computation to potentially
get similar results by kind of maybe doing the same task you know a few times in a slightly
more intelligent way and I guess the comparison with that would be either certain compression
algorithms you know the more time you spend compressing the smaller file size so there
are kind of these intuitive ideas as well.
So it's yes possible that by doing a far deeper model that requires more compute we'd
be able to get better results but that's all that's all kind of open research questions
yeah.
Are you surprised by the reception to it or was it in line with what you thought?
I mean also the reception has been bit more than I was expecting the other issue which
is interesting is you know you probably noticed in the paper I was just having fun with
it.
I mean that's obvious it's a real research paper but like there's this whole secondary
discussion some people have gotten very angry talking about the scientific voice and whether
or not I was breaking it.
The paper is how do you put quite literary and fun and lighthearted it doesn't take
itself seriously and you know that makes it I think rather accessible and interesting
but it sounds like others you're getting some flak for that.
I mean it suddenly ended up being a polarizing thing I need or in retrospect it I do think
I've suddenly gone too far in some directions and the main one the main point I really
wish I had done better was it is less accessible to non-English speakers and that's something
that I in no way ever wanted to have happen and so that's the main thing I've suddenly
decided on I want to make it far more accessible which you know that might just be additional
material very simplified explanations as some papers have like blog posts that really
distill everything out or worth through examples one step at a time but yeah they're kind
of two other directions that I was thinking about this from one is that I'm now an independent
researcher I don't have these large companies backing me and papers are really intensive
things and my natural writing style the way in which I enjoy writing is you know along
the lines of in the paper some directions as I said I've gone too far but other directions
people have really enjoyed and it kind of provided me a lot of a lot of room to think
about what I was writing why I'm doing language modeling like what it means to me and and
so on but the other direction as well is that I think in this professionalized setting
a lot of I have this entire kind of section in there where I talk about how you'd have
to present what I was writing in order for it to be kind of accepted at conferences whilst
whilst I I have gone too far I think it's also worth it for the academic community to really
reflect on what they're expecting and what they're enforcing when it comes to papers one
friend said I should you know and rightly so you should be dispassionate when it comes
to scientific voice you shouldn't put your opinion into it but just because it isn't
obvious on the surface that given paper has an opinion doesn't mean that it that there
is no bias within that paper and whether that's the the papers it compares against the the
approaches it tries the results it does or doesn't show I think this is this is something
that the field should actually think about and you know a friend of mine said maybe I'm
not the right person to think about that or do it but I don't think that science has
to have a single voice and I don't know I the main thing as well is that you know as
I said I'm an independent researcher I enjoyed writing it that's probably the best way
for me to to produce more content so if the if you want more content that's kind of
the way that I'm writing myself right and you know writing pages of text and getting these
results is no easy effects are anyway that that's part of the battle as I said I'm still
I'm still really just thinking through it myself the next paper I will I'll try simultaneously
turning it down within the main section of the text but maybe I'll do something ridiculous
like put all the all my funnicides and footnotes or into the appendix or something else like
that maybe it maybe you need to train a language model on a bunch of boring archive papers
and then run your paper through it or something like that to turn it into academic speak yes
the the room of all of the benefits of these is yeah maybe all the optional you're sure
the hidden text and sections explored and fill out with all the the ridiculous expansions
yeah right right yeah yeah I hadn't seen that that aspect of the the response to it that's
another corner of scientific Twitter I guess well yeah it's it's interesting it kind of split
depending on community lines reddit was the one that was most kind of 50 50 when it came to that
hacken years had a little bit of a a discussion on it but wasn't that heavy and Twitter was you know
mercilessly support the other thing is it's all sort of private channels as well that there are
some some friends of mine who didn't want to you know say it publicly just because they they
considered this a mistake I made and that they just didn't want to they though as long as I you
know don't do it again sort of thing yeah as I said I'm I'm thinking through it all like a great
deal you know maybe maybe I just need to separate out everything and keep the papers boring but
I don't know I'm still thinking through a little bit well that would be a great loss but you
know I certainly you know there are certainly as you've mentioned aspects of the argument that
resonate in terms of accessibility you know to folks that are less familiar with the particular
idioms and things like that you're using I mean at the same time it's it's opening up to a
broader like as you said it's opening up to a broader audience I had like you know venture
capitalists or students that I helped teach in Australia who are still high school students or
so on being able being able to read the paper and get the vast majority of the content you know
and then at the same time a lot of these papers if we're talking about reproducibility or
understandability you know I've included my curd and people have already reproduced it a lot of
the more complex works I've spent like there was one paper that I spent almost a year trying to
replicate it was some of the worst worst chunks in my life because this paper was doing better on
a task that I was really trying to solve it was a language modeling paper I ended up wasting so
much time so much compute so much energy and that was because the authors even though it was a
professionally written and well received and published paper the authors didn't include all the
details to reproduce the work that's kind of what I mean by like a lot of this is kind of you know
as I said my paper is not necessarily the right balance of it but that there's this reality behind
the papers that is just papered or go yeah they're fun intended but like that there's this
full throughout it says like you know oh we only tried this thing it's actually you know you tried
10 different things and you only reported on that or we don't quite know how this works but we're
going to pretend we do for the sake of like we're worried about reviewer number two saying that
this is the obviously don't understand how it works it's not right in our approach or what
happier so at the very least you know I think my paper makes it obvious that writing styles are
thought about and enforced but maybe they're not thought about and forced as as publicly as as we
might might need to think about them yeah well you know I think at the end of the day for a paper
who's one of whose primary goals is to ask questions of the research community you know
it strikes me as totally appropriate that it you know ask those questions in a way that
itself ask questions yeah I think that's the other thing I wanted the off sheet when you when
you're talking about the community as a whole like I can't decide on the direction of the community
and I can't necessarily there there are certain things that you can't scientifically prove one way
over the other like I can't say that we took their own direction by focusing on transformers I can
just ask the question but maybe there's the right way to do that as well yeah I mean it's you know
I don't know that it's a right or wrong thing it's you know it's kind of what we've done and I think
you know you're you're touching on that it's not the only way and I would hope people would get
that whether they can get those other ways funded at this point because everything is so focused
on transformers that's a whole other question but yeah maybe you've given someone a data point
that you know that says that you know their thing should be funded yeah that's the other I mean
the one of the other first is as well like training language models quickly my postcard based
AWD LSTM because it was so quick to train similar times it was like 12 hours for many of these
different datasets you know experiment is we're able to they say do it in like dozens or hundreds
of languages or dozens or hundreds of variants versus these large language models I know friends
who they set off the training job and even with you know the 50 or $100,000 spent on some cloud
compute thing they're waiting one two three four months in order for to get just one result I
that's the other thing as well not just being an independent research but even when I was a
researcher who was funded I don't like the idea that the research in the community is just
becoming more and more predicated on you having access to all this equipment I think there are
fascinating questions that you know your listeners or even like the high school students I teach
in Australia anyone else would be able to ask and actually answer in a completely scientific
and fascinating way but only if we keep making sure that these models are actually
inner trainable the purchable applicable to the normal person and a lot of these pre-trained
language models that were being getting recently where the training that for example GPT2
the Salesforce reaches such as language model as well many of these these different
approaches they're no longer releasing the training code for it so you get these huge
model blogs you know GP2 GPT2 excel what have you but no one's necessarily able to reproduce it
so any research that's on top of that is predicated on either that company or those research
organizations continuing to release those results of those models or again on the idea that
you don't have control of the underlying architecture like you can't decide whether it's a
transformer or an LSTM or whatever else you have to live with whatever else is put out in front of
you and I don't know it just I I I'm ready to admit one day that you know we have to like
focus on one direction or you know the compute resources do indeed get this better result and
you know we can't try all these different things but I don't think that day is yet and I don't
think that day is necessarily soon well thanks so much for taking the time to share what you're up to
and talk through this paper really interesting stuff and I am really looking forward to seeing
what the community does and what you do to kind of extend it all right everyone that is our show
for today for more information on today's show including our guest heads to tomalei.com slash shows
once again the co-first introduction to NLP study group begins this Saturday December 14th
head to tomalei.com slash community to get signed up now thanks for listening and we'll see you
next week.
