1
00:00:00,000 --> 00:00:17,920
Hello and welcome to another episode of Tumel Talk, the podcast where I interview interesting

2
00:00:17,920 --> 00:00:23,120
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:23,120 --> 00:00:26,360
I'm your host Sam Charrington.

4
00:00:26,360 --> 00:00:32,200
As usual, thanks so much to everyone who sent in their favorite quote from a recent podcast.

5
00:00:32,200 --> 00:00:36,520
Another batch of stickers got mailed out this week, so keep those quotes coming.

6
00:00:36,520 --> 00:00:40,400
So far this month, we've sent stickers to five different countries.

7
00:00:40,400 --> 00:00:45,400
If you're new to the Tumel worldwide family, you too can get a sticker.

8
00:00:45,400 --> 00:00:51,400
Just let us know your favorite quote from this or any recent Tumel and AI episode.

9
00:00:51,400 --> 00:00:57,080
You can do that via posting a comment to the show notes page by tweeting us at Tumel AI

10
00:00:57,080 --> 00:01:03,120
or at Sam Charrington or via a post on our Facebook page.

11
00:01:03,120 --> 00:01:07,920
Your feedback is very important to us and we appreciate every bit of it.

12
00:01:07,920 --> 00:01:10,600
We're going to jump right into our show.

13
00:01:10,600 --> 00:01:15,520
You may recall that we spent some time in New York City a few weeks ago at the NYU Future

14
00:01:15,520 --> 00:01:18,320
Labs AI Summit.

15
00:01:18,320 --> 00:01:22,760
While there, I had the opportunity to sneak backstage and interview a few of the great

16
00:01:22,760 --> 00:01:28,120
speakers from the event, including this week's guest, Catherine Hume.

17
00:01:28,120 --> 00:01:33,040
Catherine is the president of Fast Forward Labs, which is an independent machine intelligence

18
00:01:33,040 --> 00:01:37,480
research company that helps organizations accelerate their data science and machine

19
00:01:37,480 --> 00:01:40,080
intelligence capabilities.

20
00:01:40,080 --> 00:01:44,960
If that name sounds familiar, that's because we had their founder Hillary Mason on the

21
00:01:44,960 --> 00:01:47,080
show a few months ago.

22
00:01:47,080 --> 00:01:50,240
We'll link to that show in the show notes, it was a great one.

23
00:01:50,240 --> 00:01:54,920
My discussion with Catherine focused on AI adoption within the enterprise.

24
00:01:54,920 --> 00:01:59,360
She shared several really interesting examples of the kind of things she's seeing enterprises

25
00:01:59,360 --> 00:02:04,560
do with machine learning in AI, and we discussed a few of the various challenges they face

26
00:02:04,560 --> 00:02:08,800
and some of the lessons her company has learned in helping them along.

27
00:02:08,800 --> 00:02:12,680
I really enjoyed our conversation and I know you will too.

28
00:02:12,680 --> 00:02:22,680
And now on to the show.

29
00:02:22,680 --> 00:02:26,760
Hey, everyone.

30
00:02:26,760 --> 00:02:29,040
I am here with Catherine Hume.

31
00:02:29,040 --> 00:02:32,320
Catherine is president of Fast Forward Labs.

32
00:02:32,320 --> 00:02:39,600
If you listen to the podcast, you may recall that I interviewed her colleague Hillary Mason

33
00:02:39,600 --> 00:02:47,520
on the podcast some time ago, and she's here at the Future Labs AI Summit.

34
00:02:47,520 --> 00:02:52,080
She just finished her talk and she's agreed to chat with us for a little bit.

35
00:02:52,080 --> 00:02:53,080
Catherine?

36
00:02:53,080 --> 00:02:54,920
Hey Sam, I'm happy to be here.

37
00:02:54,920 --> 00:02:55,920
Great.

38
00:02:55,920 --> 00:02:56,920
You know what?

39
00:02:56,920 --> 00:03:03,520
You have a really intriguing background, mostly or in part because I'm a linguophile and

40
00:03:03,520 --> 00:03:08,560
you speak eight languages I hear.

41
00:03:08,560 --> 00:03:16,360
But you have kind of a, you're not the PhD in neural networks that I often interview.

42
00:03:16,360 --> 00:03:21,520
So how did you kind of find your way from where you were to in this space?

43
00:03:21,520 --> 00:03:24,200
Through a long and tortured path, I suppose.

44
00:03:24,200 --> 00:03:30,200
Yeah, so my PhD is actually in comparative literature, which is where I spent a lot of time

45
00:03:30,200 --> 00:03:34,480
learning languages and reading literature in those languages in my path life.

46
00:03:34,480 --> 00:03:41,320
So I was a mathematician as an undergrad and just was fascinated by language and cultures

47
00:03:41,320 --> 00:03:44,960
and decided to shift paths and do a PhD in literature.

48
00:03:44,960 --> 00:03:49,680
And I found myself as I was working on it always really interested in what has become natural

49
00:03:49,680 --> 00:03:50,680
language processing.

50
00:03:50,680 --> 00:03:55,200
So thinking about language, not only from a, let's go out and talk with people and communicate

51
00:03:55,200 --> 00:03:58,040
perspective, but also how they work.

52
00:03:58,040 --> 00:04:06,280
And I became increasingly interested in the fact that in using AI systems, they didn't

53
00:04:06,280 --> 00:04:08,000
really treat language like language.

54
00:04:08,000 --> 00:04:13,360
So in some of the statistical developments in the early 2000s, it's my PhD from 2007

55
00:04:13,360 --> 00:04:14,360
to 2012.

56
00:04:14,360 --> 00:04:18,560
It was a lot of N gram or bi gram type work in language processing.

57
00:04:18,560 --> 00:04:22,760
And it was fascinating to me that you didn't need to think about syntax and semantics and

58
00:04:22,760 --> 00:04:26,280
all of the grammatical terms that linguists used to think about language to make sense of

59
00:04:26,280 --> 00:04:28,720
it could just be reduced to a stats problem.

60
00:04:28,720 --> 00:04:31,600
And that was totally bizarre and interesting and weird for me.

61
00:04:31,600 --> 00:04:37,440
So I developed an early interest in AI and I had a couple of jobs in software companies

62
00:04:37,440 --> 00:04:42,760
that weren't using machine learning and then found my way fortunately into this space

63
00:04:42,760 --> 00:04:43,760
eventually.

64
00:04:43,760 --> 00:04:45,760
That's amazing.

65
00:04:45,760 --> 00:04:48,840
So your talk was on selling AI into the enterprise.

66
00:04:48,840 --> 00:04:52,760
What were the main things you were trying to accomplish with the talk?

67
00:04:52,760 --> 00:04:56,920
So I think the main thing was I assume that there were a lot of young entrepreneurs in

68
00:04:56,920 --> 00:05:02,240
the audience and I wanted to help them appreciate that you can't just say, you know, I'm going

69
00:05:02,240 --> 00:05:06,200
to go from my research that I did in graduate school and I'm going to go form a company and

70
00:05:06,200 --> 00:05:08,080
everything's going to work and be magical, right?

71
00:05:08,080 --> 00:05:13,000
There's a lot of tactical hard work you have to do in order to really make money off

72
00:05:13,000 --> 00:05:17,080
of these new technologies and tools.

73
00:05:17,080 --> 00:05:18,640
So I started at three parts in my talk.

74
00:05:18,640 --> 00:05:24,520
The first one was what I called the five axioms of enterprise AI and it just went through

75
00:05:24,520 --> 00:05:27,760
a lot of the trends that we're seeing in the enterprise space across different verticals

76
00:05:27,760 --> 00:05:31,200
fast forward doesn't wear a horizontal consulting and research firm.

77
00:05:31,200 --> 00:05:34,880
So we, you know, we work with companies of all shapes and sizes, but there's some patterns

78
00:05:34,880 --> 00:05:41,560
that show up some the biggest of which I think is companies discomfort with risk and probability.

79
00:05:41,560 --> 00:05:45,560
So there's a cultural and process shift that has to take place for them to really succeed

80
00:05:45,560 --> 00:05:52,280
with AI as they move from big data data analytics where they're counting transactions that

81
00:05:52,280 --> 00:05:57,640
occurred in the past to fill out 10k forms and do reporting to experimenting with data

82
00:05:57,640 --> 00:06:02,320
to build out new revenue streams and products and there's just, it's a massive undertaking

83
00:06:02,320 --> 00:06:08,280
to reshape the mentality that folks have to actually do this well.

84
00:06:08,280 --> 00:06:12,800
And then I talked about what I consider to be some super interesting use cases of real,

85
00:06:12,800 --> 00:06:18,000
you know, real enterprise AI that we've seen in our customer base ranging from building

86
00:06:18,000 --> 00:06:23,040
out personalized, recommender systems to support sales and financial services to building

87
00:06:23,040 --> 00:06:30,680
in context to wear surgical robots that can identify critical points in surgeries to basically

88
00:06:30,680 --> 00:06:36,440
shift to the future of automated robotic surgery somewhere 10 to 15 years down the line.

89
00:06:36,440 --> 00:06:39,200
Well, maybe can we maybe walk through one of those?

90
00:06:39,200 --> 00:06:46,880
Yeah, sure, absolutely, so let's pick one, we do some work with the big four and we

91
00:06:46,880 --> 00:06:48,040
As in accounting firms?

92
00:06:48,040 --> 00:06:49,240
Big four accounting firms, yep.

93
00:06:49,240 --> 00:06:54,560
So they've got a lot of problems where they're trying to audit the financial statements

94
00:06:54,560 --> 00:06:58,000
of their clients for compliance or they'll be giving tax advice or some sort of consulting

95
00:06:58,000 --> 00:06:59,400
advice to their clients.

96
00:06:59,400 --> 00:07:03,560
So one use case is it's not the sexiest in the world, but I think it's a great machine

97
00:07:03,560 --> 00:07:04,560
or any problem.

98
00:07:04,560 --> 00:07:10,240
They came to us and they said today our tax practitioners have a hard time keeping up

99
00:07:10,240 --> 00:07:16,480
with new rulings and opinions and news for lack of a better term in the legal world related

100
00:07:16,480 --> 00:07:18,080
to their particular clients issue.

101
00:07:18,080 --> 00:07:23,520
So there's a big gap between the world of know how, let's say in law and in advising and

102
00:07:23,520 --> 00:07:27,360
the world of applied know how where you take the rules and regulations and then you apply

103
00:07:27,360 --> 00:07:30,240
it to your customer's particular situation.

104
00:07:30,240 --> 00:07:36,080
So they engaged us to try to not go so far as to automate the work of providing tax

105
00:07:36,080 --> 00:07:42,280
advice but provide more dynamic up to date and specific alerts for their tax practitioners

106
00:07:42,280 --> 00:07:48,360
on new legal documents that were coming out that were relevant for their particular

107
00:07:48,360 --> 00:07:49,560
client cases.

108
00:07:49,560 --> 00:07:55,080
So that involved are using some statistical topic modeling techniques where we went in

109
00:07:55,080 --> 00:07:59,640
and we started off actually with regular expressions and just found got the most out

110
00:07:59,640 --> 00:08:04,040
of rules that we could to sort of bootstrap up intelligence in the system and then added

111
00:08:04,040 --> 00:08:08,160
on an additional statistical layer of intelligence to get them to a point where they're now able

112
00:08:08,160 --> 00:08:12,600
to provide dynamic advice to their clients because as new rulings come in they have their

113
00:08:12,600 --> 00:08:18,080
sort of fact specific alerts that are coming out and shift around the product offering,

114
00:08:18,080 --> 00:08:22,040
repricate, repackage it and offer something different than their competitors.

115
00:08:22,040 --> 00:08:23,040
Interesting.

116
00:08:23,040 --> 00:08:31,960
If there are a couple of generalizable rules in there one being do you commonly see folks

117
00:08:31,960 --> 00:08:38,800
in traditional enterprises having their first step being using these systems to augment

118
00:08:38,800 --> 00:08:46,600
their human staff as opposed to replace and the second being you know starting out simply

119
00:08:46,600 --> 00:08:52,120
with you know things like regular expressions and then eventually building up to the statistical

120
00:08:52,120 --> 00:08:53,120
techniques.

121
00:08:53,120 --> 00:08:55,600
Yeah those are both absolutely true.

122
00:08:55,600 --> 00:09:01,800
So I think one of the common mistakes that we see is that when somebody working in the

123
00:09:01,800 --> 00:09:06,320
enterprise who's non-technical not a machine learning specialist and is interested in trying

124
00:09:06,320 --> 00:09:11,400
to do something cool with AI normally their default assumption is that they can go from

125
00:09:11,400 --> 00:09:17,120
manual to completely automated in the first pass which I think is leading to what I call

126
00:09:17,120 --> 00:09:21,960
sort of some of the big distractions and discussions around AI where we assume that in the next

127
00:09:21,960 --> 00:09:25,280
10 years the work place is going to radically shift and everybody's going to lose their

128
00:09:25,280 --> 00:09:29,200
jobs and we're going to be replaced by machines and universal basic income comes up right

129
00:09:29,200 --> 00:09:34,480
so there's all of this mania we see online and you know our sort of humbled experience

130
00:09:34,480 --> 00:09:38,440
in working with companies and practices that that's not happening anytime soon.

131
00:09:38,440 --> 00:09:43,200
How people work will change when they have machines as a you know a companion and component

132
00:09:43,200 --> 00:09:49,360
but most of the time just from a pure technical perspective it's so hard to have the amount

133
00:09:49,360 --> 00:09:53,760
of data that's required to train a system that could really perform with the levels of

134
00:09:53,760 --> 00:10:00,320
accuracy that are required to do risk-oriented tasks like healthcare, diagnostics, treating

135
00:10:00,320 --> 00:10:05,200
stocks whatever it may be that they're just you know they're mistrusting of the systems

136
00:10:05,200 --> 00:10:08,720
and it really takes a long time to actually get enough labeled training data to get a

137
00:10:08,720 --> 00:10:15,640
system to perform and then the second is yeah from a product development perspective we

138
00:10:15,640 --> 00:10:19,400
tend to at fast-forward labs tend to think that you should always start with the simplest

139
00:10:19,400 --> 00:10:24,280
algorithm that will scale so we tend to you know we don't we don't start with the neural

140
00:10:24,280 --> 00:10:28,440
network we start with the linear regression right and if it works then it's got all sorts

141
00:10:28,440 --> 00:10:32,520
of benefits it's if the accuracy is good it's interpretable we know how to debug it we

142
00:10:32,520 --> 00:10:36,600
know how to fix it we can do some feature engineering as opposed to just throwing the big

143
00:10:36,600 --> 00:10:43,600
guns at a problem that might not actually require that so yeah so you want to get touched

144
00:10:43,600 --> 00:10:50,560
on trust and you know what we can kind of wrap up as cultural issues within customers how

145
00:10:50,560 --> 00:10:57,520
do you do you and if so how do you help them kind of wrap their arms around you know these broader

146
00:10:57,520 --> 00:11:03,360
cultural issues that are you know ultimately required for them to successfully introduce

147
00:11:03,360 --> 00:11:08,480
these kinds of technologies into their organizations I think it takes a lot of education and training

148
00:11:08,480 --> 00:11:15,600
in a lot of time so there's sort of two cultural hurdles to primary culture hurdles that we see

149
00:11:16,160 --> 00:11:21,680
the first is on let's say the project planning phase where you know there's a lot of risk

150
00:11:21,680 --> 00:11:27,040
involvement experimentation involved in doing data science and AI projects so they don't lend

151
00:11:27,040 --> 00:11:30,880
themselves to they certainly don't lend themselves to a waterfall methodology and they don't even

152
00:11:30,880 --> 00:11:35,200
really lend themselves to agile because you're not really sure at each step how long it's going

153
00:11:35,200 --> 00:11:39,120
to take if it's going to work if the models are going to converge so we have to do a lot of

154
00:11:40,080 --> 00:11:46,320
just coaching to help people understand how to do the data science phase let's say in a product

155
00:11:46,320 --> 00:11:52,960
development process and and help let's say the the technologists that we're working with coach

156
00:11:52,960 --> 00:11:56,400
them to have the meetings that they're going to have to have with their management who are going

157
00:11:56,400 --> 00:12:00,320
to be skeptical and not understand why they're spending money on a project that might not work out

158
00:12:00,320 --> 00:12:05,760
right so it's a lot of the fail fast risk-based thinking that we hear about in the startup world

159
00:12:05,760 --> 00:12:11,120
that needs to be important and the enterprise the second is on let's say the consumption

160
00:12:11,120 --> 00:12:17,840
and the user like the use of outputs of the system where there's a lot of design thinking that

161
00:12:17,840 --> 00:12:24,320
has to go in to help people use predictions well so if they let's say they're using a more

162
00:12:24,320 --> 00:12:29,600
like complex statistical tool and the output is going to be a distribution that says yeah we think

163
00:12:29,600 --> 00:12:35,520
it's here's our prediction we're 82% confident and the accuracy rate is at you know 36% or something

164
00:12:35,520 --> 00:12:41,440
like that so the average user is not going to know what to do when that's what their system tells

165
00:12:41,440 --> 00:12:47,760
them so you know I think there's there's a lot of work in the bigger problem in the enterprise

166
00:12:47,760 --> 00:12:52,800
today is not is not replacing our workers but actually giving them some sort of basic statistical

167
00:12:52,800 --> 00:12:59,200
intuition so they can they can use machine learning effectively interesting yeah so I spoke

168
00:12:59,200 --> 00:13:06,320
when Hillary was on the show we spent some time talking about the this issue of agile methodologies

169
00:13:06,320 --> 00:13:14,160
not lending themselves to this type of problem which was really counterintuitive to me I thought that

170
00:13:14,160 --> 00:13:19,040
you know I guess thinking of it kind of you know in a bipolar way waterfall and agile hey you'd

171
00:13:19,040 --> 00:13:25,520
wanted you want to use agile on the topic of culture one of the most interesting stories that I've

172
00:13:25,520 --> 00:13:34,160
heard came out of lows I heard a talk by the head of innovation there who's focused on

173
00:13:34,160 --> 00:13:41,040
some of their machine learning AI types of initiatives and they were really struggling to get their

174
00:13:41,040 --> 00:13:47,680
executives kind of on board with some of these types of projects and you know I get there get

175
00:13:47,680 --> 00:13:54,320
their heads wrapped around the these you know the risk issues without them you know just kind of

176
00:13:54,320 --> 00:13:59,120
latching into these problems and taking them you know either taking them too far or you know out

177
00:13:59,120 --> 00:14:06,160
of the gate saying hey you know that'll never work right and so what they did was they they hired

178
00:14:06,160 --> 00:14:14,160
a graphic artist and they produced these comic books that basically illustrated you know a given

179
00:14:14,160 --> 00:14:23,120
future you know at lows and what they found was that the the comic books were interest in a very

180
00:14:23,120 --> 00:14:29,520
interesting way they were you know tangible but they were also constrained so you know they

181
00:14:30,800 --> 00:14:35,840
they didn't necessarily you know that an executive could say yeah I want to do that I'm

182
00:14:35,840 --> 00:14:41,520
willing to put some money behind that so so one of the examples they gave was I think they call

183
00:14:41,520 --> 00:14:47,360
it the low spot it's basically a customer service robot that roams around stores and will you

184
00:14:47,360 --> 00:14:53,120
know can answer questions and things like that and they first documented what they envisioned this

185
00:14:53,120 --> 00:14:57,280
project to be in one of these comic books and eventually got funded and I think it's in a few

186
00:14:57,280 --> 00:15:03,680
stores in northern California now are there any other kind of tricks or interesting things that you've

187
00:15:03,680 --> 00:15:09,680
seen customers do to kind of help drive these cultural shifts I think there's all sorts of

188
00:15:09,680 --> 00:15:14,560
lessons in what you just described from low so when you were when you're singing about the comic

189
00:15:14,560 --> 00:15:21,200
book and the tangibility um the imagination is really important right so um you know there's

190
00:15:21,840 --> 00:15:28,240
one thing that we see is it's and I talked about this in the talk um you know the executives that

191
00:15:28,240 --> 00:15:32,560
companies will have seen that Google is able to classify cats or they'll have seen IBM Watson and

192
00:15:32,560 --> 00:15:37,120
they'll have seen plane jeopardy and they'll have seen Google Deep Mine plane go and they're completely

193
00:15:37,120 --> 00:15:42,480
befuddled as as to how those achievements in the realm of games and in the realm of the sort of

194
00:15:42,480 --> 00:15:47,840
consumer internet and the frivolousness around it right are relevant for the tactical boring

195
00:15:47,840 --> 00:15:52,560
business problems that they have and in part that's because if you don't really understand the general

196
00:15:52,560 --> 00:15:58,400
backend technology it's it's it's hard to make the imaginative leap from you know to do the the

197
00:15:58,400 --> 00:16:03,200
the synthesis between one domain and how that might be applied in another so I think um the

198
00:16:03,200 --> 00:16:08,400
concreteness of the book right where it's not you're not leaving it up to their imagination to

199
00:16:08,400 --> 00:16:13,280
imagine what this technology might do you're playing out scenarios you're using fiction and the

200
00:16:13,280 --> 00:16:17,600
ability of the imagination to sort of to to make that happen and granted it's there's a difference

201
00:16:17,600 --> 00:16:22,720
between who knows if the reality is like the the uh the screenplay that was written but that's okay

202
00:16:22,720 --> 00:16:26,560
the the actual product can be completely different from the origin version it just just have to

203
00:16:26,560 --> 00:16:31,840
have the impetus to want to invest so I think um one of the things that we've seen it fast forward

204
00:16:31,840 --> 00:16:37,120
we have a as i'm sir hillary talked about on the podcast we have a subscription product where

205
00:16:37,120 --> 00:16:43,040
we educate our customers as to what's possible today in realm of AI very fewer those who actually

206
00:16:43,040 --> 00:16:49,520
are applying the algorithms that we teach them about but that's okay because it inspires their

207
00:16:49,520 --> 00:16:57,200
imagination it gets them excited and it's a really useful tool to then to muster a lot of the

208
00:16:57,200 --> 00:17:00,720
you know the organizational energy that's required to then do something that because practical

209
00:17:00,720 --> 00:17:07,280
might look really different um other kind of techniques that i've seen i think there is i think

210
00:17:08,000 --> 00:17:12,240
i talked about this a little bit in the talk too um we have to always remember that we're all

211
00:17:12,240 --> 00:17:18,240
just humans right and um and we respond at work kind of the same ways that respond when we're

212
00:17:18,240 --> 00:17:22,800
outside of work but we sort of we put on this work hat and suddenly think that we're supposed to

213
00:17:22,800 --> 00:17:27,760
use these processes and this stuff that we read about that is a little counterintuitive it doesn't

214
00:17:27,760 --> 00:17:35,040
necessarily feel right and my hunches and and take is that if we can use humor even i i talk

215
00:17:35,040 --> 00:17:39,440
with somebody who's just an excellent uh a bist of person the other day who would start off his

216
00:17:39,440 --> 00:17:44,720
meetings with large serious customers with some sort of gimmick and it would just shift the the

217
00:17:44,720 --> 00:17:50,640
tone in the room and lead to real honest problem solving discussion and i think if you anything

218
00:17:50,640 --> 00:17:54,480
that you can do to get that so that you're not fearful of saying something that's gonna upset

219
00:17:54,480 --> 00:17:59,920
your boss or you know having to go around corners and stuff and even from being a company a startup

220
00:17:59,920 --> 00:18:05,440
company working with a large organization to the extent to which you can create an environment where

221
00:18:05,440 --> 00:18:11,520
it's two equals thinking through something you have a much higher probability of success so

222
00:18:12,320 --> 00:18:20,720
very interesting i'm curious where at what part of the cycle does your typical client come to you

223
00:18:20,720 --> 00:18:27,760
today uh is it is it mania driven hey we want to do something around this AI stuff and we hear

224
00:18:27,760 --> 00:18:33,040
that you folks are good at it or is it we've got a specific problem and we want to solve it or

225
00:18:33,840 --> 00:18:37,920
somewhere in between can you statistically characterize the distribution

226
00:18:39,920 --> 00:18:46,720
is it what's on here i guess so it varies we definitely have a lot of um you know younger

227
00:18:46,720 --> 00:18:54,080
smart bright uh researchers and machine learning who will write in and say this is awesome i want

228
00:18:54,080 --> 00:18:58,640
i want to read your reports i want to work for you um and that's great and it helps us build our

229
00:18:58,640 --> 00:19:02,800
community but it's not necessarily what pays our bills um i think from the you know the real

230
00:19:02,800 --> 00:19:07,600
interest it'll sort of lie in in that depends on the spectrum of maturity that the different

231
00:19:07,600 --> 00:19:13,120
enterprises have sometimes folks will reach out and they're actually really at the beginning of a

232
00:19:13,120 --> 00:19:19,040
data science journey for lack of a better word and they're looking for help building out a governance

233
00:19:19,040 --> 00:19:24,640
plan doing some basic analytics etc and that's not exactly we're not we're not staffed we're not a

234
00:19:24,640 --> 00:19:28,080
heart huge consulting company that can serve those kind of requests so we have sort of a partnership

235
00:19:28,080 --> 00:19:33,280
network will push them off the ones that work um often either have a specific problem so they have

236
00:19:33,280 --> 00:19:39,760
enough uh awareness and understanding of the business potential of AI that they can pose a good

237
00:19:39,760 --> 00:19:45,840
problem but they don't necessarily have the internal resources to staff it and execute or they may

238
00:19:45,840 --> 00:19:51,680
have already they do have data scientists they're doing some work um but they're looking for a

239
00:19:51,680 --> 00:19:57,520
neutral third party who has expertise in the domain which is rare to come in and evaluate what

240
00:19:57,520 --> 00:20:02,720
they're doing these heavy the Googles and Facebook's and IBM's of the world to just make sure they're

241
00:20:02,720 --> 00:20:09,520
on the right track um so so we tend to work well uh if there is an existing data science team

242
00:20:09,520 --> 00:20:14,800
in place um and they're looking to go from you know where they are today to where they might go

243
00:20:14,800 --> 00:20:20,400
and using some more advanced techniques there's not a ton of companies that are sort of at the

244
00:20:20,400 --> 00:20:24,640
you know at that phase there's very few enterprises that are actually using deep learning like

245
00:20:24,640 --> 00:20:28,880
very few right because they don't have the data they I mentioned this in the talk too there's um

246
00:20:28,880 --> 00:20:33,360
we have this uh false impression that just because it's a big enterprise they're going to have lots

247
00:20:33,360 --> 00:20:40,080
of data they do but they haven't been considering data over the last hundred years with an i towards

248
00:20:40,080 --> 00:20:44,800
building machine learning products so they don't have labeled training sets and they don't have it

249
00:20:44,800 --> 00:20:50,400
well composed and and processed and ready to use it's there but it takes years of work to

250
00:20:50,400 --> 00:21:01,520
make it useful um do you have any perspective on um your company has uh made a name for itself

251
00:21:01,520 --> 00:21:08,160
initially I believe as a data science company and you know there's obviously um a lot of again

252
00:21:08,160 --> 00:21:13,840
kind of maniac around AI and I've been trying to kind of wrap my head wrap my head around the

253
00:21:13,840 --> 00:21:19,120
relationship between these two terms do you have a perspective on that it's a good question so

254
00:21:19,120 --> 00:21:25,280
I think uh the even the experts in the AI and machine learning community don't agree on

255
00:21:25,280 --> 00:21:30,560
what the definition of what AI is so Hillary and I will just say it's whatever computers can't do

256
00:21:30,560 --> 00:21:36,560
until they can so it really is sort of a it's our subjective impression on what the stuff is um

257
00:21:36,560 --> 00:21:42,800
I mean we consider AI to be today and if you look through the history one can go back to expert

258
00:21:42,800 --> 00:21:48,640
systems and some of the early attempts to mimic first-order logic and the days of touring and

259
00:21:48,640 --> 00:21:53,840
you know uh Claude Shannon and early information theory so um and I actually think there could be

260
00:21:53,840 --> 00:22:00,320
a resurgence of uh symbolic you know AI techniques sometime in the future as we try to move towards

261
00:22:00,320 --> 00:22:06,800
training systems with with less data but um the inheritance of the early 2000s where you know we

262
00:22:06,800 --> 00:22:12,000
really were collecting data and processing it at scales that we hadn't seen before and with

263
00:22:12,000 --> 00:22:17,360
computational power that was faster than we'd seen before I think leads it to the fact that AI

264
00:22:17,360 --> 00:22:22,880
systems today are just sort of a next extension of what we call data science uh sometimes I think

265
00:22:22,880 --> 00:22:28,880
it's meaningful to say the type of data that AI systems work on is different so we're going from

266
00:22:28,880 --> 00:22:37,120
transactional data to images text speech right so categories that historically intractable because

267
00:22:37,120 --> 00:22:42,880
they require vectors of such high dimensionality that we didn't have the backend functions that

268
00:22:42,880 --> 00:22:47,680
could approximate you know universal functions right so just get to the level of complexity where

269
00:22:47,680 --> 00:22:52,320
they actually work for that kind of data I think that's meaningful um outside of that it's

270
00:22:53,040 --> 00:22:57,040
you know what one person calls AI might be with somebody else calls data science and it's just a

271
00:22:57,040 --> 00:23:02,000
question of it's a question of which which term you want to use to get more bang for your buck yeah

272
00:23:03,040 --> 00:23:09,680
so as we wrap things up any parting thoughts for the listeners sure so we talked a little bit about

273
00:23:10,960 --> 00:23:15,680
you know other past guests that you've had that have been focused on thinking about ethics and trust

274
00:23:15,680 --> 00:23:19,760
and bias in these systems um I think it's a big issue we're actually working on it right now fast

275
00:23:19,760 --> 00:23:25,840
forward lab our upcoming topic is on interpretability so the ability that the standard sort of wisdom

276
00:23:25,840 --> 00:23:30,160
today is that if you're using a model like a neural network it's going to give you great predictions

277
00:23:30,160 --> 00:23:34,880
but you have no idea why um and I think in you know some of the large enterprises financial services

278
00:23:34,880 --> 00:23:39,280
healthcare regulated industries and even ones where consumers are just curious about why

279
00:23:40,080 --> 00:23:43,920
why the machine is telling them to do what they're what's it's telling them to do I think there's

280
00:23:43,920 --> 00:23:50,640
some cool efforts afoot to try to gain a little bit more transparency um try to you know turn a

281
00:23:50,640 --> 00:23:55,760
non-linear complex function into something that we can understand a little bit better um and

282
00:23:55,760 --> 00:24:01,600
you know it's something to the companies that a as a as a machine learning practitioner you have

283
00:24:01,600 --> 00:24:05,760
to keep your eye out for because it can be an obstacle to adoption and be that people just seem

284
00:24:05,760 --> 00:24:10,160
to be generally interested in yeah it's a huge issue and that one that I hear coming up all over

285
00:24:10,160 --> 00:24:14,800
the time yeah yeah yeah great well thanks so much Catherine it was great having you on the show

286
00:24:14,800 --> 00:24:17,600
thanks for having me

287
00:24:17,600 --> 00:24:27,600
all right everyone that's our show for today don't forget to share your favorite quotes for one

288
00:24:27,600 --> 00:24:33,360
of our 20 stickers again you can share them via the show that's paid via Twitter or via our

289
00:24:33,360 --> 00:24:41,200
Facebook page the nudge for this show will be up on truly i.com slash talk slash 20 we will

290
00:24:41,200 --> 00:24:49,920
find links to Catherine in the areas resources thank you so much for listening and catch you next time

