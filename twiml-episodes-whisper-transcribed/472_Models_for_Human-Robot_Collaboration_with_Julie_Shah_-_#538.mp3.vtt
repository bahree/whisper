WEBVTT

00:00.000 --> 00:17.440
All right, everyone. I am here with Julie Shaw. Julie is a professor at MIT. Julie, welcome to the

00:17.440 --> 00:24.880
Twomo AI podcast. Thank you so much for having me. So you are a professor of robotics in particular,

00:24.880 --> 00:30.160
and we're going to dig into your work in that field. But to get us started, I'd love to have you

00:30.160 --> 00:36.080
share a little bit about your background and how you came to work in the field. Wonderful, yeah.

00:36.080 --> 00:44.400
So I'm a professor of aeronautics and astronautics at MIT. And I also lead a robotics lab at MIT as

00:44.400 --> 00:51.600
a part of the computer science and artificial intelligence laboratory. And I have a special love of

00:51.600 --> 00:58.960
like time, critical, safety, critical applications, which is why I'm in aerospace. And so my

00:58.960 --> 01:03.680
story of how I ended up in robotics has to start with how I ended up in aerospace. And I was just

01:03.680 --> 01:10.880
like born loving airplanes and rocket chips. I always wanted to be an astronaut. And I looked at

01:10.880 --> 01:18.640
there was like a I went to space camp. I was a super nerd. And space camp that gave you a book

01:18.640 --> 01:23.200
of all the astronauts and where they all went to school. And you know, they always stress you need

01:23.200 --> 01:27.040
to be really good at math and science. And I was like, Jack, that can be good at math and science.

01:27.040 --> 01:32.320
And a very large number of them either went to the military academies or MIT. And I was like,

01:32.320 --> 01:37.920
MIT sounds like a good place. So whenever anybody would ask what I wanted to do, I would say,

01:37.920 --> 01:42.480
I want to go to MIT and study aerospace engineering. And everybody would say, oh, that's so specific.

01:42.480 --> 01:48.480
What's so specific? And I'd say, okay. And then I came to MIT and I in the aerospace department.

01:48.480 --> 01:52.880
And then, you know, pretty, pretty soon after in the department, I'd be asked, okay,

01:52.880 --> 01:57.040
see your aerospace job. But what are you going to specialize in? And I was like, oh, no, I have to

01:57.040 --> 02:04.880
specialize further. Because it turns out that, you know, aerospace, much like robotics is a systems

02:04.880 --> 02:13.840
discipline. And I really loved control theory and control systems and really loved learning about,

02:13.840 --> 02:20.880
you know, aircraft auto pilots. And how those really had to be designed to the capabilities of

02:20.880 --> 02:28.240
pilots of humans and, you know, how they could affect control over a system. Having to design for

02:28.240 --> 02:33.840
the human machine system was really exciting to me and needing to understand both sides of that

02:33.840 --> 02:40.800
equation. So I actually did my master's degree in human factors engineering to kind of go deep

02:40.800 --> 02:46.800
on the human side. And it wasn't until my PhD, I switched gears and joined the computer science

02:46.800 --> 02:51.600
and artificial intelligence laboratory and did my PhD in automated planning and scheduling.

02:52.560 --> 02:59.440
And so, you know, this was 2010. I finished up my PhD. So before the big, the big era change

02:59.440 --> 03:05.600
into, you know, to machine learning. And throughout have just been really, really interested in

03:05.600 --> 03:11.040
how you design automation and technologies and, you know, AI and machine learning to like fit

03:11.040 --> 03:16.800
like a puzzle piece against human capabilities so that you can achieve some end objective.

03:17.920 --> 03:23.520
And that's why I, you know, started a robotics lab, the interactive robotics group and my lab

03:23.520 --> 03:28.400
includes human factors engineers and aerospace engineers and computer scientists and,

03:28.400 --> 03:34.880
um, uh, others as well. So we bring kind of across disciplinary perspective to, uh, developing the

03:34.880 --> 03:40.400
technology and showing its benefit. Nice, nice. And your lab, do you still have that leaning

03:40.400 --> 03:48.480
towards flying robots as opposed to arms or walking robots or humanoid or other types of form factors?

03:48.480 --> 03:53.120
Yeah, it's interesting. It's interesting. You ask that because when I, um, when I applied for

03:53.120 --> 03:59.840
faculty positions, it was straight out of PhD and, um, and I got the offer from MIT from my home

03:59.840 --> 04:04.800
department. I did all my degrees at MIT and then I had this amazing offer from MIT from my home

04:04.800 --> 04:10.800
department, uh, but they strongly recommended slash required that I go away for a year,

04:10.800 --> 04:17.440
was before starting on the faculty. And they, um, my PhD was funded under an NSF fellowship. So it

04:17.440 --> 04:23.920
was, uh, maybe not, not super closely tied to, you know, real applications of real systems.

04:23.920 --> 04:28.080
And, um, the department head at the time suggested that I go out to Boeing and he said, you know,

04:28.080 --> 04:32.560
they have interesting applications and robotics for manufacturing. I said, oh, it sounds great.

04:32.560 --> 04:39.200
It sounds like a great way to spend a year out in Seattle. So, um, and, uh, and I just, I just,

04:39.200 --> 04:46.080
I just caught the bug of, you know, uh, manufacturing. I just loved watching, you know, 737 or

04:46.080 --> 04:50.800
737 being built in front of me and then thinking about the challenges. Again, it's like an integration

04:50.800 --> 04:56.000
challenge. Like robots can do pieces of that work, but actually the vast majority of building a large

04:56.000 --> 05:01.600
commercial airplane is still done manually, like a beehive of manual work. And so they're, um,

05:01.600 --> 05:07.520
their challenges in how you design the technology, the robotics, the intelligent robot to integrate

05:07.520 --> 05:14.160
into manual workflow. Um, and so when I came back, I had us, uh, I wanted to keep doing that. I kind of

05:14.160 --> 05:19.520
wanted to keep working in robotics for manufacturing and collaborative robots. And I, I asked, uh,

05:19.520 --> 05:23.840
a different department head on the one that hired me at the time. So, is it going to be okay if I,

05:23.840 --> 05:28.080
you know, if I work in manufacturing in aeroastro or do I need to be flying things? And he said,

05:28.640 --> 05:33.760
you know, pursue, pursue the problems that you think are important. And, um, and I, and I really

05:33.760 --> 05:40.000
appreciated that. I work, I work on aerospace and, um, you know, applications, but I also work in,

05:40.000 --> 05:47.040
um, uh, yeah, in, uh, you know, automotive, you know, um, I work in decision support for fighter pilots,

05:47.040 --> 05:52.960
but also like nurses and doctors. Um, but the key thread is, uh, I'm just, you know, I think it's

05:52.960 --> 05:57.920
really exciting to develop technology that really has to be flawless to add value and has to fit

05:57.920 --> 06:02.720
with the human in a way that, you know, doesn't, doesn't add friction, but really eases a very

06:02.720 --> 06:07.760
challenging job of a dominean expert. And so- So you can kind of envision that as a, uh,

06:07.760 --> 06:13.040
venn diagram of, you know, mission critical in one circle and requires human interaction

06:13.040 --> 06:18.560
and the other and your sweet spot is in the middle. Yeah, yeah, that's a, that's a good visualization

06:18.560 --> 06:24.880
of, uh, of whether or not I take on a project. Yes. Okay. And so, uh, let's dig a little bit deeper

06:24.880 --> 06:30.480
into that. What are some of the types of projects that you work on? Or maybe even more broadly,

06:30.480 --> 06:38.560
how do you craft a research agenda in that, uh, that intersection? So, uh, I started on the faculty

06:38.560 --> 06:47.600
at MIT just a little over 10 years ago, um, or maybe 11 years ago now. And, um, and from the, um,

06:47.600 --> 06:53.440
sort of like the vision behind our work then and still now is to be intentional about developing

06:54.000 --> 06:59.200
intelligent machines or intelligent robots or even kind of computing more broadly that enhances,

06:59.200 --> 07:06.400
um, human capability, um, and well-being. And, um, and, you know, there are many ways to do that,

07:06.400 --> 07:11.760
but, you know, starting out in the lab and going back to my PhD, I was interested in, you know,

07:11.760 --> 07:19.280
how, how far can you get in, um, developing a robot, um, that can emulate the, the capabilities

07:19.280 --> 07:25.120
of an effective human team member. Um, and that's not the only model of collaboration or teamwork

07:25.120 --> 07:30.000
for sure. Um, and it's not even, it was always a hypothesis. It's not even a given that if you

07:30.000 --> 07:34.720
develop, you know, um, a system with some of the, you know, capabilities or features of an

07:34.720 --> 07:38.160
effective human team member, that that'll even be useful. So we've always, you know,

07:38.160 --> 07:43.120
conducted experiments to try to determine, you know, whether or not that's useful. But my lab

07:43.120 --> 07:50.800
focuses on developing novel AI models and algorithms where the AI is modeling people. Um, and so

07:50.800 --> 07:55.760
there are, you know, decades of studies in what makes for effective human team work, you know,

07:55.760 --> 08:02.320
in sports psychology and studying pilots in the cockpit or doctors and nurses in, in the operating

08:02.320 --> 08:06.720
room. And it's very, it's a very practical field of study because you want to be able to train

08:06.720 --> 08:14.000
up new people to come into this profession and work effectively in a team. Um, and, uh, but I'll just

08:14.000 --> 08:19.520
brush that all aside and summarize many decades of very rigorous study in human teamwork and

08:19.520 --> 08:25.760
human team coordination, um, in terms of, you know, like, there's, uh, three capabilities that we

08:25.760 --> 08:31.760
as humans bring to teams that make us really, um, make some of us exceptional team members.

08:32.640 --> 08:37.600
And that's the ability to know what your partner is thinking, to be able to anticipate what they'll

08:37.600 --> 08:43.600
do next and then be able to use that information online, um, a circumstances unfold to, um,

08:43.600 --> 08:50.560
uh, uh, to, to sort of change your plan. And so, um, my lab has focused on doing those three things,

08:50.560 --> 08:56.000
developing models that can infer human cognitive state, predictive models of human behavior and

08:56.000 --> 09:01.840
workflow, uh, and, um, and then also, uh, set of techniques around dynamic plan execution,

09:01.840 --> 09:06.400
to be able to take those predictions and use them online to have a, what kind of play the game

09:06.400 --> 09:12.960
with you. It's primarily in, in, in development of, of novel AI models and methods, but everything

09:12.960 --> 09:18.480
we do needs to be tested and evaluated with a system actually working with a person to know whether,

09:18.480 --> 09:22.080
you know, I'm in gelating these capabilities is, is even a value.

09:23.200 --> 09:30.400
Well, one of the things that you said in there that, uh, that I don't want to be as hyperbolic

09:30.400 --> 09:39.120
as blows my mind, but yeah, it is the idea of the robots or the AI kind of predicting what's going

09:39.120 --> 09:45.760
on in their human collaborators heads. That's hard enough for a human. Uh, what does that even mean

09:45.760 --> 09:53.520
for a robot or an AI? Yeah, that's a, that's a, that's a great question. So, um, and you have to,

09:53.520 --> 10:00.160
to pursue that in an academic fashion, it needs to be a well-defined problem and the, like, the,

10:00.160 --> 10:04.880
the human cognitive state that matters in a particular setting and, you know,

10:04.880 --> 10:10.000
particular setting A is very different than the state that matters in a different setting.

10:10.000 --> 10:14.720
Sometimes when I give talks, like, sometimes the question afterwards is, is there a unifying,

10:14.720 --> 10:20.800
you know, model or approach to being able to infer human cognitive state? And I've always just been like,

10:22.400 --> 10:26.480
I don't, you know, I don't, I don't think so, but we have to maybe ask the cognitive scientist,

10:26.480 --> 10:32.880
the neuroscientist. But, uh, I think that's, that's a really, like, for me, that, the fact that

10:32.880 --> 10:39.360
there isn't, you know, want, you know, one well-defined set of states that you're really after

10:39.360 --> 10:46.000
that you're aiming to infer is actually, um, one of, like, the key research questions. And I think

10:46.000 --> 10:53.040
it is one of the key research challenges, which is, um, like, we as humans, we want to be able to

10:53.040 --> 11:00.560
shape the machine learning model or the AI's model of our world or of us, um, much like a human

11:00.560 --> 11:04.960
brings, like, different considerations or a different mental model of interacting with another human

11:04.960 --> 11:12.720
or doing one task versus another task. Um, so, um, like a, a, a key, um, the direction of our

11:12.720 --> 11:19.120
research is, is, uh, first of all, recognizing that you will never succeed at that with an unsupervised

11:19.120 --> 11:23.520
learning approach. There's, I mean, you might get super, super lucky in the latent states you

11:23.520 --> 11:29.040
infer, happen to correspond to the human's mental model of, you know, and, and, you know, what's,

11:29.040 --> 11:33.200
what's particularly important in that particular context, but practically that's not going to happen.

11:33.200 --> 11:40.160
And we, you know, um, theoretically, you know, it's, uh, it's a challenge. So, um, so the question is,

11:40.160 --> 11:46.880
what inputs can you listen from a person that's easy for a person to provide, um, that can help

11:46.880 --> 11:52.480
lock in or, uh, sort of shape the latent space or improve the efficiency of inference for the system

11:52.480 --> 11:59.840
to, uh, learn those, you know, the, um, the, the latent states are their dynamics of, uh,

11:59.840 --> 12:05.760
of the human that, that's interacting with it. Um, and, like, providing labels, like a supervised

12:05.760 --> 12:09.920
approach will work, but then where did those labels come from and how practical is it in every

12:09.920 --> 12:14.880
setting to be able to provide labels of, of relevant human cognitive state? I mean, it's just like

12:14.880 --> 12:22.960
an unreasonable request. Um, so, uh, yeah, what, on the other hand, there are easy, there are

12:22.960 --> 12:29.920
inputs that are easy for people to provide. Like, um, I can, maybe I, maybe it's helpful to ground it

12:29.920 --> 12:35.760
in a, in a concrete example. So, um, say, I have the most simple system. I'm trying to, you know,

12:35.760 --> 12:40.240
develop my own mental model of, and it's just a subway. So just go to the end of line, it turns

12:40.240 --> 12:45.520
around and it comes back. Uh, I'm originally from New Jersey. So in New York, the subway goes

12:45.520 --> 12:50.480
uptown and downtown, and that's the mental model I hold is, uh, someone from New Jersey.

12:50.480 --> 12:55.360
But in Boston, um, the subway becomes exactly the same way. It goes to the end of line, it turns

12:55.360 --> 13:00.560
around and it comes back, but we say the subway goes inbound and outbound, and the switching point

13:00.560 --> 13:06.240
is some arbitrary, like, stop in the center, which is weird. So like, you're going inbound,

13:06.240 --> 13:09.200
as long as you're going to Park Street, and as soon as you pass through Park Street, you're going

13:09.200 --> 13:12.640
outbound. And then when it turns around, it comes back, you're going inbound again until you pass

13:12.640 --> 13:18.400
in Park Street, and then you're going outbound. So it's a, it's like, my mental model of the subway

13:18.400 --> 13:23.280
in New York, in Boston, it's a two-state switching model, but it's, but my mental model is different,

13:23.280 --> 13:30.160
depending on where I'm from. Um, and so, um, you know, we, we employed like non-parametric

13:30.160 --> 13:36.640
Bayesian techniques to try to infer latent states, um, of, you know, some black box systems, say,

13:36.640 --> 13:41.680
like, a human, um, but what are the odds you're going to learn an inbound outbound switching model

13:41.680 --> 13:49.280
for Boston? Um, it's like, you know, um, so, um, but here's the thing, like, you know, maybe I don't

13:49.280 --> 13:55.440
want to label for you inbound outbound, and then New York is uptown, downtown, um, but if I can

13:55.440 --> 13:59.680
just tell you the change point, the switch point. So in New York, the switch point of my mental

13:59.680 --> 14:03.920
model that's a way is that it switches at the end of the line, and then Boston, it switches at Park

14:03.920 --> 14:08.800
Street. Um, that's enough to sort of lock in, and now we're going to hold the same mental model

14:08.800 --> 14:14.800
of how to talk about the behavior of that, that subway. Um, so, um, looking for these ways in

14:14.800 --> 14:20.160
which it's very easy for a person to provide high-level input into a machine learning model,

14:20.160 --> 14:24.720
and then enabling the machine learning model to use those inputs within its computational framework.

14:25.360 --> 14:28.320
Um, is, uh, really, really exciting to me.

14:28.320 --> 14:36.160
There are the, the inputs that you're talking about, uh, and in particular, in this case of the

14:36.160 --> 14:46.720
subway, is this an interaction time input, or is this a training input? Um, so the, the particular

14:46.720 --> 14:53.680
work that I'm, that I'm talking about there, that's done at training time. So, um, the, the idea is,

14:53.680 --> 15:00.000
um, you know, you have, uh, you have your, um, time series data, but rather than work with it in

15:00.000 --> 15:04.480
an unsupervised fashion, if you have some labels, but it's not fully labeled, and then you take

15:04.480 --> 15:10.080
this sort of high-level input from the person in the form of partial policies or partial dynamics,

15:11.280 --> 15:15.520
and then what we do is we formulate those as constraints on a, on a variational inference

15:15.520 --> 15:21.440
process. In that work, it's still done at, um, training time, and I think your question is really

15:21.440 --> 15:26.160
exciting, because, okay, so, you know, or even you have a trained model, and now you're taking,

15:26.160 --> 15:33.680
you're, you, you task to do together, new environment, or, um, uh, how it is you, first of all,

15:33.680 --> 15:38.000
identify the differences between how you want the system to behave in the new environment,

15:38.000 --> 15:44.720
or the model it holds, and be able to adapt it is, um, you know, uh, online is, uh, you know,

15:44.720 --> 15:46.640
a really exciting research direction.

15:46.640 --> 15:52.800
Yeah, I think where the question was coming from was, uh, in the context of training, it,

15:52.800 --> 15:58.880
and, you know, me trying to construct, you know, what this trained model might, uh, be, might

15:58.880 --> 16:06.480
look like, it almost sounded like a feature engineering task that, uh, a, you know, subject matter

16:06.480 --> 16:14.640
expert might apply, um, to map, you know, some raw data of whatever the data is we're looking at

16:14.640 --> 16:21.200
about these trains to, uh, you know, their mental model of the world in order to get the model to,

16:21.840 --> 16:28.320
to kind of attach to that in the training process to use, use loose terminology. Um, you know,

16:28.320 --> 16:35.600
you might, if, you know, you have your, uh, if you're kind of moving away from some center,

16:35.600 --> 16:40.080
you might, the, your feature might be the distance from that center point, or the distance from

16:40.080 --> 16:47.040
your endpoint, or something like that. And I was trying it to, I was curious whether the way that

16:47.040 --> 16:51.920
you were building that knowledge into the system was similar to feature engineering, or was it

16:51.920 --> 16:56.640
a different process altogether? That's interesting. So you're highlighting that, like, um, the,

16:56.640 --> 17:03.760
the person's role can be in, uh, you know, identifying or synthesizing the features that are

17:03.760 --> 17:10.080
meaningful to, you know, their understanding of, um, uh, you know, of, of the functioning of the

17:10.080 --> 17:14.800
system. There's, there's definitely a role there, and also, uh, support, you know, obviously

17:14.800 --> 17:22.720
support there in, um, enabling a person to, um, uh, and enabling the proper elicitation of, of

17:22.720 --> 17:28.480
those features. Um, the, the particular work that I'm referring to, it's not in the feature

17:28.480 --> 17:36.880
engineering, um, space. It's, um, so we, we, um, employ, uh, like a Bayesian graphical model,

17:36.880 --> 17:43.520
a factored model, um, with, uh, with, with particular assumptions made on, um, you know,

17:43.520 --> 17:48.560
on the structure of the model to be able to support efficient inference. So you have, um,

17:48.560 --> 17:51.920
instead of observable states about the world, like, there's things I can see about the subway,

17:51.920 --> 17:57.600
right? I can see its movement, um, uh, there's actions that are, that are taken, um,

17:57.600 --> 18:02.240
that are observable, but then there's, like, there's, there's something in the latent space

18:02.800 --> 18:06.960
that clearly impacts, you know, the behavior, the observable behavior of the system as I want

18:06.960 --> 18:12.240
to be able to predict it and describe it. Um, and I think what's interesting is either of those

18:12.240 --> 18:17.280
two-state switching models can be informative, right? Especially if you want to, if you want to

18:17.280 --> 18:22.880
predict the behavior of the system, um, uh, you know, it's, you just kind of need a two-state

18:22.880 --> 18:29.520
switching model and you'll be able to predict the behavior of the system. Um, but, uh, the, um,

18:30.320 --> 18:36.240
the, the question of, like, which, which, which one of those, um, sometimes it can help in, uh,

18:36.240 --> 18:41.040
in, in predictive power of your model, but it's critically important for being able to

18:41.760 --> 18:47.280
support a person in inspecting and understanding and doing their own projection of the behavior

18:47.280 --> 18:54.960
of this learn model in a, in a different context. Um, so, um, in, in the, in, in the approach,

18:54.960 --> 19:04.560
it's really, uh, it's really a flat, we assume a flat, um, discrete, um, latent state space.

19:05.680 --> 19:11.920
But, uh, the feature engineering question is, is, you know, is, is, is equally important.

19:11.920 --> 19:18.080
Um, like, um, I, I worked for a while on my pilot's license, which, which was really fun,

19:18.080 --> 19:22.880
and then I was a grad student and I ran out of money before I could solo, which is, I thought I

19:22.880 --> 19:26.640
had it all planned out, but you never fly for an hour. You always fly for an hour and 15 minutes,

19:26.640 --> 19:31.440
and it didn't, and I'm working out. And the irony is now that I'm in the Arrow Astro Department,

19:31.440 --> 19:35.840
like, I think there's like, actually, like a benefit, like a professional education benefit,

19:35.840 --> 19:39.120
where I could do flying lessons, and now I don't have time, and I have young kids, so,

19:39.120 --> 19:46.160
but, you know, from my, um, from my days of learning how to fly, like, it's just like, much like a,

19:46.160 --> 19:50.000
like a poor machine learning model. It's like a lot of input, and, you know, like, the spurious

19:50.000 --> 19:54.240
correlations, like, you're like, oh, maybe I should do this when I see that over there, and it's

19:54.240 --> 19:59.440
like the role of the instructor to be like, here's what you attend to. These three features mean

19:59.440 --> 20:03.920
this, like, put your fingers up, you know, do you see the horizon above or before four fingers?

20:03.920 --> 20:08.000
Now, you know, if you're flying straight and level, and, you know, like that, that role of guiding

20:08.000 --> 20:17.680
the meaningful, the meaningful use of, of, you know, the features that the system could attend to,

20:17.680 --> 20:25.680
is, is enormously valuable. Without a causal model, it can be, it can be challenging to ensure

20:25.680 --> 20:30.880
you're learning, a model that'll produce the behavior that you hope it'll produce in a, in a novel

20:30.880 --> 20:43.680
environment. Is this subway example? Is this kind of a toy example for illustration, or was there

20:43.680 --> 20:51.440
an actual problem that you were solving an actual data set? And what exactly is that problem?

20:51.440 --> 20:55.680
Yeah. Yeah. Yeah. The subway, the subway example was the very, the very most simple synthetic

20:55.680 --> 21:03.440
domain in the paper, to like illustrate the idea of, we applied it to be able to, so in, in

21:03.440 --> 21:08.800
in this larger goal of being able to observe a human and develop a predictive model of their

21:08.800 --> 21:14.240
behavior, I think a person holds certain priorities or preferences and how they want to do their work.

21:15.040 --> 21:20.400
And this is something like that is practically a challenge for us into playing robots in industrial

21:20.400 --> 21:25.120
environments. So on an assembly line, you know, someone might say, oh, they're standard work.

21:25.120 --> 21:29.760
There's like a standard way of building this car. There's a standard way of building this plane.

21:29.760 --> 21:34.720
And I believe that, too, I went in and watched how people built things. But there's actually,

21:34.720 --> 21:39.680
like a whole lot of variation in how the plane is assembled or how the car is assembled.

21:41.200 --> 21:46.720
And, you know, different people for different reasons, different biomechanical models will have

21:46.720 --> 21:54.880
different ways of performing the work. And in in something like an automotive, you know, an automotive

21:54.880 --> 22:02.800
factory, a collaborator once told me that like half a second of efficiency could like make a break

22:02.800 --> 22:07.680
the business case for introducing a new technology. So once you're looking at a robot to support a

22:07.680 --> 22:14.160
person and being like half a second faster, like small changes in the ordering of how they install,

22:14.160 --> 22:19.840
you know, something on the dashboard of a car or or their motions, being able to predict where

22:19.840 --> 22:24.640
they'll be in a fine grain way for the robot to assist, that'll make a break the success of that

22:24.640 --> 22:32.720
system in in doing that. So being able to quickly kind of lock in and model an individual person's

22:32.720 --> 22:39.040
priorities or preferences for performing a task improves the predictive power of that model.

22:39.040 --> 22:45.680
And but the key is to be able to do it with relatively little data. So the concrete use case is

22:46.960 --> 22:51.280
the person does their assembly task in a factory and you can collect data of how they

22:51.280 --> 22:57.040
how they do their work. And then you want to when the robot comes in, you want to be able to model

22:57.040 --> 23:00.880
like their intent like where they're going to reach next on the table or where they're going to

23:00.880 --> 23:07.600
walk next in in the cell environment. And different people have different again priorities and

23:07.600 --> 23:17.120
preferences. Work load or sort of fatigue level can impact, you know, your model of your predictions

23:17.120 --> 23:24.640
for the person. But rather than hand specifying a threshold based on, you know, some some measures,

23:24.640 --> 23:31.840
it would it's better to be able to learn that and tune that threshold based on the value it gives

23:31.840 --> 23:36.720
you in order in order to predict where the person will be in space and time after some number of

23:36.720 --> 23:43.760
hours. So that's the those are the those are more concrete use cases of of inferring these latent

23:43.760 --> 23:53.200
states and their and especially their dynamics. Got it got it. In the case of the the shop floor

23:54.240 --> 24:00.000
and the the preferences, it kind of going back to that earlier question I asked about run time

24:00.000 --> 24:06.240
versus training time, you know, training time you've kind of built in the capability to identify

24:06.240 --> 24:14.000
this but the identification is happening at run time, is that right? Yeah, yeah, so um okay, so

24:16.560 --> 24:22.720
this is this is this is this is fun. Okay, so like what do you need for a robot to be able to

24:22.720 --> 24:28.080
collaborate with a person online? You needed to have a model of human behavior, which had and it's

24:28.080 --> 24:34.160
maintaining a belief right over the person's, you know, latent states and a belief over, you know,

24:34.160 --> 24:39.600
what they're going to do next. And then you need a task model and maybe you're lucky enough to have,

24:39.600 --> 24:44.560
you know, a clearly specified task model, but even then you have you have a partially observable

24:44.560 --> 24:48.720
model that the robot is reasoning about in order to be able to collaborate with the human partner.

24:49.440 --> 24:56.400
So that gives you as a Palm D.P. And then you want to be able to solve that Palm D.P. online.

24:58.240 --> 25:02.560
And that and what does it mean to solve it online? It means to give the robot the ability to

25:02.560 --> 25:08.960
choose its actions, whether it's like physical actions or in some cases, even communicative actions

25:10.160 --> 25:16.160
to be able to reason on the uncertainty of what the person will do, but also to be able to

25:16.160 --> 25:21.840
reduce uncertainty based on the old action. And this is quickly interject Palm D.P. is partially

25:21.840 --> 25:26.640
observable Markov decision process. That's it, yep, yep. And the

25:26.640 --> 25:34.160
going back to the beginning of your explanation, do you have one partially observable model,

25:34.160 --> 25:41.600
like you have this task model that is, you know, which there's some noise of what the human might,

25:41.600 --> 25:47.440
how the human might perform that task, or do you have two distinct models, one about the human?

25:47.440 --> 25:52.800
And well, it sounds like one is the human's cognitive state, which you're using to influence

25:52.800 --> 25:59.440
the task model. Yes, yeah. So the human's cognitive state is not directly observable.

25:59.440 --> 26:04.560
Therefore, the model of your human is partially observable, like you can see physically what they're

26:04.560 --> 26:08.880
doing, but there's some elements about what they're doing that you can't directly observe.

26:10.160 --> 26:17.840
And because of that, you know, the robot is, you know, is aiming to reason about, you know,

26:17.840 --> 26:23.200
about how it should be behave or after what actions it should take considering this partially

26:23.200 --> 26:32.400
observable model of human behavior. The task model, in some of our works, we assume it's fully known

26:32.400 --> 26:38.000
and fully observable. But, you know, just as it's really important to lower the barrier,

26:38.000 --> 26:45.040
to quickly and easily learning a human model to work with a person, it's really important in

26:45.040 --> 26:55.680
many contexts to lower the barrier of enabling like a shop floor or a line worker or a domain expert

26:55.680 --> 27:02.720
to teach a robot a task without an applications engineer as the intermediary, or in like in our lab

27:02.720 --> 27:09.920
without, you know, your AI researcher as the intermediary. And when you, when you aim to do that,

27:09.920 --> 27:16.320
you know, you're no longer directly specifying the task and it's often advantageous for the robot

27:16.320 --> 27:22.160
to maintain a belief over the true task specification as well. So it may not definitively know, you know,

27:22.160 --> 27:27.040
the, the specification for how to form the task, but to hedge against its uncertainty, maybe in

27:27.040 --> 27:30.880
some particular context, it really needs to be very, very conservative if it's a safety critical

27:30.880 --> 27:37.600
context. In other contexts, it might have more flexibility without there being a major consequence.

27:37.600 --> 27:45.200
But that's another form of, you know, partial observability that might, that would go into a

27:45.200 --> 27:48.240
Palm DP model that includes both an agent model and a task model.

27:49.040 --> 27:56.000
Yeah, I've come into contact with, well, not physically, but there are like these co-robots or

27:56.000 --> 28:03.600
co-bots that are in use. I think there's a backster robot like there are models for humans interacting

28:03.600 --> 28:11.680
with robots. But I imagine they're fairly brittle when the human does something kind of outside

28:11.680 --> 28:17.120
of the script. Maybe the robot just waits for human to get back in the right position or something

28:17.120 --> 28:23.840
like that to continue the script. And what I'm hearing is that this is a direction for

28:23.840 --> 28:34.640
building robots or robot AI's that can more gracefully interact with humans. Is that the big picture idea?

28:34.640 --> 28:39.360
Yeah, that, that, that is the big picture idea. Like the big, the big picture idea is to figure out

28:39.360 --> 28:47.040
how you can develop and field robots that don't require a human to be a robot to work with it.

28:47.040 --> 28:52.000
So, because that's, that's, that often doesn't work very well. But, you know, there are actually

28:52.000 --> 28:57.200
many of the applications out there today. I mean, humans are very, very adaptable, but it comes

28:57.200 --> 29:04.000
at a cost. So, a cost in many different dimensions. There could be one angle where you look at how you

29:04.000 --> 29:11.120
develop and play intelligent robots to take over or supplant elements of what's being done by a human

29:11.120 --> 29:20.400
today. And there is an alternate approach where you sort of take, take what can easily be done

29:20.400 --> 29:26.800
physically by a robot today, but then recognize it's, it's, it's little pieces of existing manual

29:26.800 --> 29:32.640
work that can be done by a robot. And now you have this integration challenge and then developing AI

29:32.640 --> 29:39.120
to help ease that those integration challenges. And a lot of those integration challenges occur

29:39.120 --> 29:48.000
because humans are the ultimate uncontrollable entity. So, being able to adapt or reason explicitly

29:48.000 --> 29:55.040
over the uncertainty and what a human will do becomes really important. And a key part of doing

29:55.040 --> 30:01.920
that is being able to, for the robot, specify like, what are the true constraints that underlie,

30:02.480 --> 30:06.960
the tasks that needs to be done, and what's acceptable for working with the person.

30:08.480 --> 30:11.680
And then if you have a predictive model, what the person will do, that's all the better.

30:11.680 --> 30:17.840
Because then you can, you know, you can, you can plan over a time price and whether it be less than

30:17.840 --> 30:25.760
a second or tens of seconds. And the, and the interaction can be a little, a little more fluid.

30:27.040 --> 30:34.480
You, the, the, like the backster, the backster is an example of like one of these collaborative robots

30:34.480 --> 30:39.760
that are really game changing because they're safe enough to be right alongside a person. You

30:39.760 --> 30:43.920
don't need a cage. You don't need to remove them physically from the same space as people.

30:43.920 --> 30:48.720
And there's, we've used the universal robot and some of our deployments. We really enjoy the

30:48.720 --> 30:57.120
Franca, Amica robot. But many of the installations of these robots are still working independently of

30:57.120 --> 31:02.640
people in, in production environments. They're not working interdependently. And because of that,

31:02.640 --> 31:07.520
they're limited in, in the value they can provide or the places they can be easily deployed. And so,

31:07.520 --> 31:14.240
a key motivation of our work is, is enabling that interdependence, but rather than forcing the

31:14.240 --> 31:20.320
person to like adhere to the fixed, you know, robot motion and a robot schedule, allowing the robot to

31:20.320 --> 31:27.520
accommodate the natural flexibility or uncertainty that a person brings. And, and now the scenario that

31:27.520 --> 31:34.720
we've talked about thus far kind of creates this picture of a human with their kind of partner robot

31:34.720 --> 31:41.280
working on some tasks together. But does it, to what extent do you envision it extending to

31:41.280 --> 31:48.480
teams of, you know, one robot embedded within a team of humans or multiple robot, one human

31:48.480 --> 31:55.600
with embedded within a team of robots or, you know, a more diverse mixture of humans and robots

31:55.600 --> 32:02.800
kind of working together on, on some kind of tasks? Yeah. The work that I've been describing so far

32:02.800 --> 32:08.560
around the, the techniques for the non-perject-based and techniques for inferring latent state and

32:08.560 --> 32:15.760
their dynamics, that, that is the PhD work of Bayblab on Holcker, who is currently an assistant

32:15.760 --> 32:23.120
professor at, at Rice, at Rice University. And, actually, even though, you know, many of those

32:23.120 --> 32:31.120
studies end up being, you know, one to one, one person, one robot, the, the motivation for a

32:31.120 --> 32:37.840
lot of our work is the recognition that, you know, a lot of our work is done in teams. And we do

32:37.840 --> 32:44.480
need to know more than just the cognitive state or mental state of, you know, one, one partner

32:44.480 --> 32:50.960
in order to succeed. So then that race is really interesting questions about like, what is a team

32:50.960 --> 32:58.160
cognitive state? And how do you model that? And how can the techniques developed for, you know,

32:58.160 --> 33:07.600
modeling like one black box agent, like a person, extend to modeling the sort of, like, the team

33:07.600 --> 33:15.920
concept, the prior work on team, team performance and, and team coordination and communication.

33:15.920 --> 33:20.640
You know, those settings, maybe other than the pilot, co-pilot, you know, narrow scenario, those

33:20.640 --> 33:27.600
are all team settings involving more than two, two people. And there is this notion that team

33:27.600 --> 33:33.760
cognitive state is, is something different. And a lot of thinking and study about like what,

33:33.760 --> 33:40.480
what, what's encompassed in a team cognitive state? And they've, and I have a really exciting

33:40.480 --> 33:46.480
collaboration with Harvard Medical School looking at their operating room and like cardiac

33:47.520 --> 33:55.200
surgeries and the very complex dynamics involved in that. And the, the researchers there, they have

33:55.200 --> 34:01.200
this like super futuristic, you know, surgical simulation environment that they train the residents

34:01.200 --> 34:07.040
and, and grants from NASA to study, you know, human teamwork and failures and human teamwork

34:07.040 --> 34:14.400
from a human factors perspective. And we're in, you know, in discussion currently. And they both

34:14.400 --> 34:22.000
unhulk her in particular has a really exciting recent working paper out of his new lab in,

34:22.000 --> 34:28.480
again, these synthetic scenarios, but in extending these techniques to model some aspects of

34:28.480 --> 34:37.520
team cognitive state, flow, workflow disruptions, sort of miscommunication or signals among a team

34:37.520 --> 34:44.080
that, that have adverse outcomes for, for a team's performance. And again, you have this really

34:44.080 --> 34:52.160
interesting question of training time versus online and what, you know, like, what, what can be predicted

34:52.160 --> 34:58.000
with batch versus online and with what fidelity and how does that flow in to actually training

34:58.000 --> 35:03.840
on you, surgeon to identify the team factors that affect performance because this is done like

35:03.840 --> 35:09.040
an action review or is there a way to bring these techniques actually online in an operating room

35:09.040 --> 35:16.800
and show some aspects of, like, current coding or rating of the team state that can help, you know,

35:16.800 --> 35:28.080
potentially help potentially spur sort of a repair action online that might be useful.

35:29.600 --> 35:35.520
Yeah, it's a great question and work that is newer and early underway.

35:35.520 --> 35:42.080
Yeah, it sounds like there are elements of that that are kind of fundamental psychology

35:42.080 --> 35:50.720
research. Like, you know, what is, you know, what is team cognition separate from the individual

35:50.720 --> 35:55.920
cognition, the cognition of the individuals on the team and what would a representation of that,

35:55.920 --> 36:02.400
you know, even start to look like. Yeah, yeah, the work on that is, so the frameworks that

36:02.400 --> 36:11.440
exist for it are not computational frameworks currently, but we've had a, you know, one of the

36:11.440 --> 36:19.360
great joys and, you know, I think from the work in our lab is talking with, you know, researchers

36:19.360 --> 36:27.040
and cognitive psychology and also cognitive science and sometimes those works really have like the

36:27.040 --> 36:33.920
sort of the basis for that sort of feature, that feature identification problem that you raised.

36:35.440 --> 36:40.400
We've had success in, for example, taking dialogue acts related to,

36:42.400 --> 36:48.480
to shared understanding developed in behavioral psychology and cognitive science literature

36:48.480 --> 36:55.520
and use that to develop an inference system to be able to infer a team's state of understanding

36:55.520 --> 37:02.400
of like a common plan that they're, they're discussing. And so there, there is a lot of opportunity

37:03.680 --> 37:09.520
to leverage the decades of research and insight developed in these other fields and work together

37:09.520 --> 37:16.080
to develop computational models that can advance the automated capability of systems to, to shore up,

37:16.080 --> 37:22.160
you know, weaknesses that human teams sort of naturally have in performing these challenging tasks.

37:22.160 --> 37:29.360
You've also done some work on in the domain of cross-training between humans and robots.

37:29.360 --> 37:35.200
When I hear that, I think a little bit of like imitation learning, but I think your approach is,

37:36.000 --> 37:40.480
you know, in a fairly in a different direction than that. Can you talk a little bit about that work?

37:40.480 --> 37:48.880
Yes, yes. So my lab explored this idea of cross-training or team training applied to

37:48.880 --> 37:55.360
human robot teams. And this goes back to the very start of the lab. And the researcher that,

37:55.360 --> 38:00.000
that did this research is, was my, was a master student in my lab at the time,

38:00.000 --> 38:06.160
Stephen O's Nikolatus. And he's now an assistant professor at USC. And the motivations of that work

38:06.160 --> 38:12.880
was, if you looked, you know, then a technique from, from the literature and learning from

38:12.880 --> 38:18.800
demonstration. It would be like a person showing a task to a robot or walking a robot, you know, through,

38:18.800 --> 38:25.440
through the, the steps of the task. But if your goal was interdependent action between a person

38:25.440 --> 38:30.320
and a robot, like, how do you demonstrate that, right? Like, it's like, you'd be like, okay, robot,

38:30.320 --> 38:34.000
you do this, okay, now I'm going to do this, okay, and then what you're going to do is do that,

38:34.000 --> 38:38.960
and then I'm going to do that. It becomes kind of awkward to think through. And when you look

38:38.960 --> 38:45.360
at, again, the teaming literature, we have, we have many well-honed techniques, like tried and

38:45.360 --> 38:51.440
true techniques for training humans to work interdependently and very challenging tasks. And the,

38:51.440 --> 38:57.600
it's not always possible, but the gold standard is this thing called cross-training. So I do my part

38:57.600 --> 39:02.720
of the task, and you do your part of the task, and we try it out together, but then we switch roles,

39:02.720 --> 39:06.640
then like, I do your part of the task, and you do my part of the task, and we do that for a little

39:06.640 --> 39:11.840
while, and then we switch roles again. And it turns out from, you know, from, from a perspective

39:11.840 --> 39:16.880
of optimizing human learning, for a human, humans learning how to work in teams. This works better

39:16.880 --> 39:21.840
than just about anything else. And you, you can only really do it with small homogenous teams,

39:21.840 --> 39:27.040
like a nurse really can't take a surgeon's role. But when you have, it's kind of small homogenous

39:27.040 --> 39:32.640
teams, like, this is the, this is the way to train your team. I imagine part of the rationals that

39:32.640 --> 39:37.760
you kind of develop empathy and understanding of the other role, and it helps you, like we talked

39:37.760 --> 39:43.280
about before, kind of subtly adapt the way you do your thing so that the needs of the partner.

39:43.280 --> 39:48.000
Exactly, exactly. Like, when I do your job, I suddenly realize, like, you know, the challenges

39:48.000 --> 39:52.640
of you doing your job when I, you know, and then you come back and you adjust the way you do things

39:52.640 --> 40:00.320
for the benefit of the team. Exactly. And so, one of our, one of our goals early on was to think

40:00.320 --> 40:07.360
about, well, how can you optimize human robot team performance in following this type of learning

40:07.360 --> 40:13.520
curve? And so the first, you know, the, the first try at this was to say, well, what if,

40:13.520 --> 40:19.840
what if a human just gives the robot inputs the way, you know, the way a human would experience

40:19.840 --> 40:24.000
inputs when working with another person through cross-training? So in this, in that paradigm,

40:24.000 --> 40:29.280
first the human did their role and the robot did its role. But then the human took the robot's role

40:29.280 --> 40:33.840
in a simulation environment and, like, pretended to be the robot. And then the robot, you know,

40:33.840 --> 40:39.280
did, you know, something that the human would do. The ability to improve the quality of the

40:39.280 --> 40:44.000
human's mental model of the robot was substantial. We were able to show that in experimentation.

40:45.120 --> 40:49.920
But there was also some evidence that the inputs the person was giving in this,

40:49.920 --> 40:55.200
we important this modified reinforcement model where of higher quality than you would get through

40:55.200 --> 41:01.360
you know, standard approaches to reinforcement learning with just positive and negative feedback

41:01.360 --> 41:06.880
or reward. And that's because if a robot does something and you say, like, good robot, like,

41:06.880 --> 41:12.000
there's, there's questions as to what you mean by good robot. Was it good robot for the thing

41:12.000 --> 41:16.560
the robot just did or is it good robot for the thing that you think the robot is about to do next?

41:17.680 --> 41:22.240
Whereas when you take the equivalent of those inputs from the person actually doing the robot's

41:22.240 --> 41:28.560
job, that's more like a direct demonstration of the person for the robot. Was this project

41:28.560 --> 41:36.400
implemented in simulation or is there a physical implementation of this? Yeah, so the cross-training

41:36.400 --> 41:40.960
was implemented in simulation. So the person and robot kind of played together in this game

41:40.960 --> 41:46.480
environment and the robot, you know, in simulation showed the movements it would make. But they weren't

41:46.480 --> 41:52.560
physically working together. It was in a in the 2D simulation environment. And then we had the

41:52.560 --> 41:58.160
person walk over to the lab to the physical robot and do the task with the physical robot after

41:58.160 --> 42:04.240
training under different paradigms. And when the person robot trained via cross-training and simulation,

42:04.240 --> 42:10.400
there were both objective and subjective measures benefits to the physical interaction between

42:10.400 --> 42:17.040
the person and the robot. So that leads to an exciting direction where you can imagine, you know,

42:17.040 --> 42:22.000
a person robot or a person teaching a robot and then learning to collaborate with a robot entirely

42:22.000 --> 42:27.280
in simulation. Off the assembly line without money flying by and then you just walk out onto the

42:27.280 --> 42:30.640
assembly line and like you're good to go, you built your mental model of how to work with this

42:30.640 --> 42:37.760
robot and it's learned its model of how you'll behave and when working with it. Nice. What are some

42:37.760 --> 42:43.680
of the things you're most excited about looking forward? That early work in the lab in cross-training,

42:43.680 --> 42:51.280
we still have the goal of optimizing a human and robots ability to co-learn how to work together.

42:51.280 --> 42:58.640
That early work in cross-training was, you know, a part of the reason we pursued it in that particular

42:58.640 --> 43:04.960
way was because like what does it mean to optimize a human's learning of how to work with a

43:04.960 --> 43:12.320
learning robot? Like I didn't know, but I knew that via certain structured interactions a human

43:12.320 --> 43:18.160
should learn better. And you know, so we implemented cross-training and we're able to show this,

43:18.160 --> 43:24.400
you know, really strong benefit. Much of our work since has been looking at trying to more

43:24.400 --> 43:30.960
directly support and optimize the human's learning via the approach I mentioned earlier about

43:30.960 --> 43:37.760
enabling a person to guide a machine learning's model ability to align its learned model with

43:37.760 --> 43:45.600
the human mental model. So how can we learn a machine learning model that a latent model that

43:45.600 --> 43:53.200
is well aligned with the human mental model directly? Not indirectly via exercising certain

43:53.200 --> 44:00.800
forms of interaction. And the question you raised about how you do this at runtime is really

44:00.800 --> 44:07.680
exciting and really interesting. On the physical side, I have a group in the lab focused on this

44:08.800 --> 44:16.640
because if you say you're on an automotive line which we were and you want to gather your

44:16.640 --> 44:20.960
model of a person doing the task so that you can introduce the robot, you have a chicken and an

44:20.960 --> 44:24.400
egg problem because when you collect your data of the person doing the task on their own

44:25.040 --> 44:28.960
and then you introduce the robot, suddenly the person does their task entirely differently

44:28.960 --> 44:33.200
because the robot is there. It's like out of distribution. So then we're like, okay, so we get

44:33.200 --> 44:37.280
these real associates come through and then we're like, okay, here's what we're going to do.

44:37.840 --> 44:42.640
I'm going to be the robot here. Look, I'm moving like a robot. You do your task like you would with

44:42.640 --> 44:46.880
me as the robot supporting you and then you try to get your data that way and learn the model and

44:46.880 --> 44:52.000
then you introduce the actual robot and then the person does it differently again. And so the ability

44:52.000 --> 45:01.360
of a system to be able to follow or even guide that that learning process of a person to work with

45:01.360 --> 45:08.960
the robot becomes very important, very practically important to successful deployment of the system.

45:08.960 --> 45:17.120
And then we have a line of research also looking at deep models, neural models and

45:17.120 --> 45:23.040
following a similar line of research. Like, what are the inputs a person can give to shape that

45:23.040 --> 45:29.040
latent space so that it better aligns with a human-held mental model that's useful for some

45:29.040 --> 45:37.520
specific task. So moving from the Bayesian graphical model to deep models and, you know,

45:37.520 --> 45:45.040
and enabling that same capability there. But all towards making these systems much more easily

45:45.040 --> 45:51.200
shapeable and adaptable to the needs of someone who's not a machine learning expert.

45:51.200 --> 45:55.840
Well, Julie, thanks so much for joining us and sharing a bit about what you're up to.

45:55.840 --> 46:23.840
Thank you so much for having me.

