WEBVTT

00:00.000 --> 00:16.000
Hello and welcome to another episode of Twymilthalk, the podcast where I interview

00:16.000 --> 00:21.320
interesting people, doing interesting things in machine learning and artificial intelligence.

00:21.320 --> 00:25.080
I'm your host Sam Charrington.

00:25.080 --> 00:28.800
I'd really like to take a moment to thank all of you who listen to this show.

00:28.800 --> 00:33.040
I am constantly humbled by your interest and offers of support.

00:33.040 --> 00:37.560
Thanks to you, the show keeps reaching more and more people with last week's show being

00:37.560 --> 00:42.840
our fastest ever to 5,000 listens in just over 24 hours.

00:42.840 --> 00:47.240
So please, please, please keep reaching out via the show notes page, Twitter, Facebook,

00:47.240 --> 00:48.640
YouTube, etc.

00:48.640 --> 00:52.000
We really, really love to hear from you.

00:52.000 --> 00:58.040
Inspired by listener Beth Ann and YC on Twitter, we want to try a little contest for you.

00:58.040 --> 01:02.440
We've got some fresh new stickers on hand and we'd like to get them into your hands.

01:02.440 --> 01:05.200
I've got to say they really turned out great.

01:05.200 --> 01:07.600
So here's what you need to do to get one.

01:07.600 --> 01:12.920
Let us know your favorite quote from today's podcast via Facebook, Twitter, YouTube or

01:12.920 --> 01:18.800
SoundCloud comment or comment on the show notes page by midnight Sunday Pacific time and

01:18.800 --> 01:21.200
we'll send you one.

01:21.200 --> 01:25.440
Before we jump into our show, I wanted to remind you to check out the future of data

01:25.440 --> 01:27.960
summit that I'm organizing.

01:27.960 --> 01:32.840
It'll take place in Las Vegas and May at the Interop ITX conference and I'm excited to

01:32.840 --> 01:38.200
have already heard from a bunch of Twinmal listeners making arrangements to attend.

01:38.200 --> 01:43.000
We'll be covering some really exciting topics, including of course machine learning and AI,

01:43.000 --> 01:48.880
but also IoT and edge computing, augmented and virtual reality, blockchain, algorithmic

01:48.880 --> 01:51.320
IT operations, data security and more.

01:51.320 --> 01:53.320
It's going to be a great event.

01:53.320 --> 02:00.400
You can learn more about the summit at twinmalai.com slash future of data.

02:00.400 --> 02:02.480
And now about today's show.

02:02.480 --> 02:09.160
This week our guest is Stefano Irman, Assistant Professor of Computer Science at Stanford University

02:09.160 --> 02:14.480
and fellow at Stanford's Woods Institute for the Environment.

02:14.480 --> 02:19.440
Stefano and I met at the rework deep learning summit earlier this year where he gave a presentation

02:19.440 --> 02:22.560
on machine learning for sustainability.

02:22.560 --> 02:27.680
We spoke about a range of topics, including incorporating domain knowledge into your

02:27.680 --> 02:33.400
machine learning models, dimensionality reduction and his interest in applying machine learning

02:33.400 --> 02:40.120
and AI to addressing sustainability issues like poverty, food security and the environment.

02:40.120 --> 02:50.000
And now on to the show.

02:50.000 --> 02:54.520
Hello everyone, welcome to another episode of this week in machine learning and AI.

02:54.520 --> 03:00.640
I am here with Stefano Irman, who is an Assistant Professor at Stanford University.

03:00.640 --> 03:06.080
Stefano and I connected after his presentation at the recent rework deep learning summit

03:06.080 --> 03:09.640
where he spoke on machine learning for sustainability.

03:09.640 --> 03:11.640
Stefano wanted to say hi to the audience.

03:11.640 --> 03:13.640
Hi everybody, thanks for the invitation.

03:13.640 --> 03:15.800
It's great to be here with you today.

03:15.800 --> 03:19.040
I'm really excited to have you on the show.

03:19.040 --> 03:24.800
I enjoyed your presentation and I spent a little bit of time looking into some of the

03:24.800 --> 03:31.040
work that your group is doing at Stanford and I found a really interesting mix of fundamental

03:31.040 --> 03:37.600
research into machine learning techniques as well as a strong interest in a particular

03:37.600 --> 03:41.120
application area, notably sustainability.

03:41.120 --> 03:48.080
So why don't you get us started by talking about how those two kind of meld for you and

03:48.080 --> 03:50.520
how you arrived at your area of focus?

03:50.520 --> 03:51.520
Sure, yeah.

03:51.520 --> 03:56.920
So a lot of the research that we do in my group is really at the foundation or artificial

03:56.920 --> 03:58.600
intelligence machine learning.

03:58.600 --> 04:04.800
So we do a lot of work on probabilistic modeling of data, developing scalable and accurate

04:04.800 --> 04:10.520
inference techniques for high dimensional probabilistic models of data, knowledge representation

04:10.520 --> 04:13.720
and decision making and uncertainty techniques.

04:13.720 --> 04:18.000
So we're really interested in the whole pipeline of going from data to extracting knowledge

04:18.000 --> 04:23.440
and extracting insights from the data to using these insights to improve the way we make

04:23.440 --> 04:24.760
decisions.

04:24.760 --> 04:30.640
And a lot of the work is really foundational, so we proved theorems, developed algorithms,

04:30.640 --> 04:31.840
developed new models.

04:31.840 --> 04:38.000
But we also like to think about real world problems and I'm particularly excited about new

04:38.000 --> 04:40.960
applications areas for AI machine learning.

04:40.960 --> 04:45.000
And as you mentioned, one that I'm really excited about is this new area of computational

04:45.000 --> 04:50.760
sustainability, where we're trying to apply computer science techniques and generally

04:50.760 --> 04:56.880
use ideas from computational thinking to help solve and address some of the big sustainability

04:56.880 --> 04:58.680
issues of our times.

04:58.680 --> 05:08.040
And these include things like poverty or environmental issues or energy, sustainable energy problems,

05:08.040 --> 05:12.680
network resource management, a problem scene ecology and so forth.

05:12.680 --> 05:19.680
So I'm very, very interested in finding ways to use this new amazing technologies that we've

05:19.680 --> 05:26.480
been developing in the past 10 or 20 years in AI machine learning and use them to address

05:26.480 --> 05:32.120
problems that are extremely important, I think, but perhaps they don't, they are not studied

05:32.120 --> 05:36.440
as much as they should in the field of machine learning and AI.

05:36.440 --> 05:43.440
I think you're right that they're not studied as much as they should be, how did you arrive

05:43.440 --> 05:50.480
at that research focus as opposed to one of the more popular or buzzy buzz worthy areas

05:50.480 --> 05:55.880
like robotics and or even here in the valley, getting people to click on ads.

05:55.880 --> 05:57.640
You know, I have a red thread.

05:57.640 --> 06:04.200
So I also started with my PhD actually, so I did my PhD in computer science at Cornell

06:04.200 --> 06:06.960
University and arrived when I joined.

06:06.960 --> 06:13.040
My advisor had just received a big grant from NSF, an expedition in computing to start

06:13.040 --> 06:16.280
a whole new research area in computer science.

06:16.280 --> 06:23.400
And the idea was really to try to see whether we can take all these amazing techniques and

06:23.400 --> 06:30.000
ideas that we have and use them to address problems in the sort of in the public space.

06:30.000 --> 06:37.680
The idea being that information technology and computers and AI have really revolutionized

06:37.680 --> 06:43.640
the way people live and a lot of way people do businesses and they really changed the

06:43.640 --> 06:44.640
world.

06:44.640 --> 06:51.160
But if you think on the other hand, at the way we try to solve some of the big societal

06:51.160 --> 06:55.600
problems that we have in the public space dealing with the environment and how we manage

06:55.600 --> 07:00.800
our network resources or how we try to sort of close the gap between developing and

07:00.800 --> 07:06.880
developed countries, we're still not taking advantage of all of these ideas just because

07:06.880 --> 07:13.160
there's not so much economic incentive to sort of apply, develop the necessary models,

07:13.160 --> 07:16.880
develop the necessary algorithms, figure out how to actually use them to solve these

07:16.880 --> 07:17.880
problems.

07:17.880 --> 07:23.000
And so, you know, that was kind of how I got started and I've been working on those

07:23.000 --> 07:28.080
kind of problems basically ever since, so for almost 10 years now.

07:28.080 --> 07:29.080
Very nice.

07:29.080 --> 07:36.400
Now, in your presentation, you shared some statistics around the impact of the problems

07:36.400 --> 07:41.720
that you're going after with regard to poverty and food security and the environment.

07:41.720 --> 07:46.640
You know, in many ways, I think it's kind of obvious that these are important issues,

07:46.640 --> 07:52.400
but like you say, in a lot of ways, they're understudied because of the lack of a driving

07:52.400 --> 07:53.400
economic incentive.

07:53.400 --> 07:58.560
You know, can you share some of those stats or at least the ones that inspire you to continue

07:58.560 --> 07:59.560
pursuing this work?

07:59.560 --> 08:00.560
Right.

08:00.560 --> 08:06.760
Like I was actually looking at the 2030 agenda for sustainable development that was recently

08:06.760 --> 08:11.760
adopted by the United Nations and if you look at the kind of problems that were sort of

08:11.760 --> 08:17.360
identified as being some of the big societal challenges that sort of all the governments

08:17.360 --> 08:21.040
in the world should be working together to address.

08:21.040 --> 08:25.720
We see things like ending extreme poverty, there are still hundreds of millions of people

08:25.720 --> 08:30.120
around the world living extreme poverty or eliminating hunger.

08:30.120 --> 08:34.800
It turns out that again, there are lots of people, especially in places like Africa and

08:34.800 --> 08:37.800
Asia and so forth that don't have enough to be.

08:37.800 --> 08:44.600
Or you know, protecting biodiversity, there's this huge biodiversity loss and we need to

08:44.600 --> 08:51.000
find ways to manage our resources in a more sustainable way so that we can sort of guarantee

08:51.000 --> 08:56.360
the welfare, not just about current generation, but also our children and the generations

08:56.360 --> 08:58.000
that will come after them.

08:58.000 --> 09:04.560
So these are all big and problem problems and a lot of them, one of the challenges right

09:04.560 --> 09:05.560
there.

09:05.560 --> 09:10.000
So they figure out this partially because they involve this sort of global skill phenomena

09:10.000 --> 09:17.000
thinking about a climate change or how to manage these very large ecosystems or the fact

09:17.000 --> 09:21.360
that we need to deal with multiple agents that are sort of have different objective functions

09:21.360 --> 09:23.000
than they're interacted with each other.

09:23.000 --> 09:27.920
But there's clearly like a computational component to these problems, but so far it has

09:27.920 --> 09:29.760
not been studied so much.

09:29.760 --> 09:36.000
And so one of the focus of my research is really to try to find ways to apply these techniques

09:36.000 --> 09:40.640
from AI and computer science to help address some of these issues.

09:40.640 --> 09:46.960
And on that note, do you come at things from the perspective of, as you said, apply

09:46.960 --> 09:53.720
applying techniques that are developed in AI and machine learning research to this application

09:53.720 --> 09:59.680
area or the other way around, meaning identifying specific problems in the research area and

09:59.680 --> 10:04.480
using those to drive research around specific techniques.

10:04.480 --> 10:05.480
That's a big question.

10:05.480 --> 10:10.880
I actually like to think of it as a two-way street, so yes and yes.

10:10.880 --> 10:18.040
On the one hand, sometimes we start with a problem and we realize that maybe we don't

10:18.040 --> 10:22.440
have the tools right now, or we don't have the right models to address the problem.

10:22.440 --> 10:26.320
And that often happens because this problem haven't been studied so much with computer science.

10:26.320 --> 10:31.160
So there's always some aspect of this problem that has not been studied before or some

10:31.160 --> 10:36.280
variation that sort of gives rise to a new problem that will require new models, new

10:36.280 --> 10:41.640
model algorithms, and we can sort of publish our papers in sort of top AI machine learning

10:41.640 --> 10:42.640
conferences.

10:42.640 --> 10:49.040
And on the other hand, we also like to do a lot of foundational research, and so sometimes

10:49.040 --> 10:55.080
we know about all the capabilities that we have right now, the cutting edge of AI machine

10:55.080 --> 10:56.080
learning.

10:56.080 --> 11:00.080
So just by talking with my colleagues and Stanford, sometimes I hear about, no, I asked

11:00.080 --> 11:02.360
them, what are the problems that they are working on?

11:02.360 --> 11:09.200
And we tried to find ways to use this new ideas and apply them to help them solve this

11:09.200 --> 11:11.080
very important problems.

11:11.080 --> 11:15.440
So one reason why this, it's interesting that you take this two-way street approaches

11:15.440 --> 11:22.840
because I spend quite a bit of time following companies that are trying to democratize machine

11:22.840 --> 11:29.640
learning in AI, and often what they're doing is trying to create generalized machine learning

11:29.640 --> 11:34.480
in AI platforms, and in fact, you know, all of the large cloud companies like Google

11:34.480 --> 11:38.680
Microsoft and Amazon are trying to do that same thing.

11:38.680 --> 11:44.680
But then we have a group like yours that has a fundamental focus on a specific application

11:44.680 --> 11:50.560
area that drives deep, deep, unique research into the field.

11:50.560 --> 11:53.360
I'm wondering what's your take on that?

11:53.360 --> 11:58.960
Do you feel like what's the role of what's your take on the role of kind of generalized

11:58.960 --> 12:07.280
machine learning in AI techniques and versus very application-specific techniques?

12:07.280 --> 12:16.440
I think there's definitely a lot of value in developing tools that can be easily used

12:16.440 --> 12:23.200
by people that are not necessarily the domain experts, and I will definitely help in a lot

12:23.200 --> 12:30.520
of context, including the sustainability space, but I do believe that sometimes you really

12:30.520 --> 12:34.200
need to develop some actual research to be done.

12:34.200 --> 12:40.440
There are some problems that require actually new techniques, and so there's definitely

12:40.440 --> 12:47.000
a lot of space for developing new ideas, new models that maybe are motivated by this

12:47.000 --> 12:52.160
specific application, but then they can potentially apply it down the line to other problems

12:52.160 --> 12:54.600
that are completely different of names.

12:54.600 --> 12:59.760
That's kind of the beauty of computer science is this idea of abstraction to develop general

12:59.760 --> 13:04.960
models, these general algorithms that are maybe inspired by one specific problem, but

13:04.960 --> 13:10.200
then a few years later, somebody else comes up with a completely different new application

13:10.200 --> 13:15.200
that you would never have thought about, and they apply exactly the same algorithm to

13:15.200 --> 13:16.200
this new problem.

13:16.200 --> 13:21.080
I think that's really the power of computer science, this layers of abstraction.

13:21.080 --> 13:28.680
All right, so let's maybe try to get more concrete and talk through some of the specific

13:28.680 --> 13:36.040
research that your group is doing and how it's uniquely applied in this application area.

13:36.040 --> 13:40.960
One of the papers that I came across on your group's website, which I'll link to in the

13:40.960 --> 13:47.240
show notes, is one on deep Gaussian process for crop yield prediction based on remote

13:47.240 --> 13:49.240
sensing data.

13:49.240 --> 13:55.440
Crop yield prediction will talk about the importance of that problem and the data source

13:55.440 --> 13:59.680
and then walk us through kind of what the research is hoping to achieve.

13:59.680 --> 14:05.840
Yeah, so that particular paper, we were looking at problems in the food security space.

14:05.840 --> 14:09.560
It turns out that there are, as I mentioned earlier, there are lots of people around the

14:09.560 --> 14:17.720
world that don't have enough to eat, or suffering from various kind of food crisis, due to

14:17.720 --> 14:26.400
weather, climate change, like erosion, or rising waters, all sorts of problems are kind

14:26.400 --> 14:29.480
of causing this food security issues.

14:29.480 --> 14:34.120
And we know that the situation is going to get worse with time, but the world population

14:34.120 --> 14:38.560
is growing and we're going to have to find a way to, there's an estimated, I think, two

14:38.560 --> 14:43.480
billion more people that we're going to have in 2050 and we're going to have to find

14:43.480 --> 14:46.080
ways to feed this growing population.

14:46.080 --> 14:52.160
So the kind of problem that we looked at was that of trying to see whether we can use

14:52.160 --> 15:01.200
inexpensive, cheap, unconventional data sources, like satellite data to keep track and predict

15:01.200 --> 15:07.080
various food security measures and monitor agricultural outcomes.

15:07.080 --> 15:14.120
So in particular, we try to develop these machine learning techniques to predict just

15:14.120 --> 15:21.720
by looking at the images of the Earth from space, to predict how the kind of level of agricultural

15:21.720 --> 15:28.240
productivity of a geographical area just by looking at it from from space, essentially

15:28.240 --> 15:35.120
we developed some machine learning models that can track the growth of plants from space

15:35.120 --> 15:41.440
and use that information to predict ahead of time how much in that particular application

15:41.440 --> 15:47.720
we're looking at soybeans, but since then we've extended it to corn and other crops.

15:47.720 --> 15:52.960
We're able to actually predict very accurately from space using cheap, unconventional data

15:52.960 --> 15:57.960
sources, the level of productivity of different geographical regions.

15:57.960 --> 16:02.920
And this is important because given this kind of data, we can start collecting this kind

16:02.920 --> 16:06.400
of data, especially in developing countries, it's very expensive.

16:06.400 --> 16:12.240
We don't have much data on this kind of measures in places like Africa.

16:12.240 --> 16:20.560
And so if we had a way to measure these crop yields or other food security measures,

16:20.560 --> 16:27.200
like something or things like that, that could be extremely useful to improve the kind

16:27.200 --> 16:34.600
of policies that both governments and NGO use things like predicting whether it's going

16:34.600 --> 16:42.080
to be a farm item, a certain region, or a certain government should stock or increase

16:42.080 --> 16:50.120
the levels of food reserves in case there's an emergency and things like that.

16:50.120 --> 16:56.680
And so in this particular case, remote sensing data, the satellite imagery is where did that

16:56.680 --> 16:57.680
come from?

16:57.680 --> 17:03.760
Is this Google Maps type data, for example, or Google Earth type data, or a government

17:03.760 --> 17:05.400
or proprietary data source?

17:05.400 --> 17:07.360
It's actually publicly available data.

17:07.360 --> 17:11.840
We were using data from NASA, the MOLIS satellites.

17:11.840 --> 17:13.880
So this is publicly available data.

17:13.880 --> 17:18.200
It takes an image of the basically the entire world every eight days.

17:18.200 --> 17:24.240
And this is my spectral data, so it's not just sort of like visible RGB bands, but there's

17:24.240 --> 17:30.280
also infrared and other bands that contain additional information that we can use in our

17:30.280 --> 17:32.680
machine learning system to make our predictions.

17:32.680 --> 17:33.680
OK.

17:33.680 --> 17:41.160
So you started with the goal of taking this multimodal image-sentient image data and trying

17:41.160 --> 17:45.200
to predict crop yields based on it.

17:45.200 --> 17:52.240
And along the way, developed some new techniques around one that was called out in the paper

17:52.240 --> 17:55.000
is dimensionality reduction.

17:55.000 --> 18:00.960
Can we spend some time talking about that particular technique and its novelty and even the

18:00.960 --> 18:05.360
step before that, for folks that aren't familiar with it, what is dimensionality reduction

18:05.360 --> 18:07.680
and why is it important in this problem space?

18:07.680 --> 18:08.680
Yeah.

18:08.680 --> 18:13.040
So the challenge for this kind of application is that we didn't have a whole lot of training

18:13.040 --> 18:14.040
data available.

18:14.040 --> 18:18.200
Like these days, if you have a lot of training data, then you can take a sufficiently high-capacity

18:18.200 --> 18:25.680
model, like a big large amount of networks, and there is a high chance that with a sufficiently

18:25.680 --> 18:31.120
large amount of training data, you will be able to do a pretty good job of predicting these

18:31.120 --> 18:33.560
outcomes that we care about.

18:33.560 --> 18:38.560
But in a lot of these sustainability applications, like in this case, for predicting crop yields

18:38.560 --> 18:44.120
and also, we did some other work on predicting other kind of socio-economic measures of interest

18:44.120 --> 18:49.000
like poverty, it turns out that the amount of training data available is extremely limited,

18:49.000 --> 18:50.200
it's very, very scarce.

18:50.200 --> 18:54.680
And so you cannot just train machine learning systems, state-of-the-art machine learning

18:54.680 --> 18:57.400
system end-to-end from inputs to outputs.

18:57.400 --> 19:04.040
And so what we did is we've been developing several techniques to try to get away with

19:04.040 --> 19:06.080
less training data.

19:06.080 --> 19:10.960
One in particular that we use the one, the particular dimension, which, and other several

19:10.960 --> 19:17.680
ways to do it, sometimes, and essentially, in all the cases, we either do transfer learnings

19:17.680 --> 19:23.280
or we try to use some proxy for the measures that we care about to get some signal, and

19:23.280 --> 19:27.320
maybe pre-trained the system and learn something useful about the structure of this kind

19:27.320 --> 19:34.040
of multispectral images that we use as input, or we use some kind of prior knowledge, maybe

19:34.040 --> 19:38.720
something about something that we know about the outcomes that we're trying to predict,

19:38.720 --> 19:42.920
the relationships between input and outputs, and we can use it, we can sort of put that

19:42.920 --> 19:49.280
into the system to make sure that we can basically get very good results even when we don't

19:49.280 --> 19:50.920
have a whole lot of training data.

19:50.920 --> 19:55.640
In this particular context, the dimensionality reduction technique that we used was an idea

19:55.640 --> 20:01.120
that we had to essentially reduce the dimensionality of the inputs.

20:01.120 --> 20:05.920
An image is very high-dimensional, you have a lot of pixels, and they can take many different

20:05.920 --> 20:06.920
values.

20:06.920 --> 20:10.760
So it's a very high-ded sort of input data, lies in a very high-dimensional space, although

20:10.760 --> 20:12.200
there is a lot of structure.

20:12.200 --> 20:18.960
And so, in some sense, the information that we care about is actually can be extracted

20:18.960 --> 20:25.560
from without really looking at the exact position of all the pixels in the image.

20:25.560 --> 20:31.840
So what we came up with was an idea to reduce the dimensionality of the inputs while preserving

20:31.840 --> 20:33.760
most of the information comp.

20:33.760 --> 20:35.000
And the idea was fairly simple.

20:35.000 --> 20:41.280
The idea was that if you care about predicting crop fields, it doesn't really matter where

20:41.280 --> 20:49.880
the fields are in the image, so the soybean fields are in the image, their actual location

20:49.880 --> 20:50.880
doesn't matter.

20:50.880 --> 20:52.640
Do you say the soybean fields?

20:52.640 --> 20:54.880
Let's say the soybean fields.

20:54.880 --> 20:55.880
Soybean fields, okay?

20:55.880 --> 21:01.440
The actual position in the image, whether they appear in the top right corner or the

21:01.440 --> 21:04.440
left bottom corner doesn't matter.

21:04.440 --> 21:10.360
So you can use this prior knowledge to essentially reduce the dimensionality of the inputs while

21:10.360 --> 21:16.000
preserving the information content, and that makes learning the problem easier.

21:16.000 --> 21:22.640
So when I think about the image problem, in what way is that a high dimensionality problem

21:22.640 --> 21:27.800
and how do those dimensions correspond to the actual problem space?

21:27.800 --> 21:32.840
Well, you have basically one dimension for a pixel in the image, so if you have a thousand

21:32.840 --> 21:38.200
by a thousand pixels, there you have a million dimensions for the inputs, and that's essentially

21:38.200 --> 21:41.000
what's causing the problem.

21:41.000 --> 21:47.160
It's very high dimensional, and so it's known that there's a lot of machine learning algorithms

21:47.160 --> 21:49.120
and a lot of algorithms in computer science.

21:49.120 --> 21:54.880
They suffer from this curse of dimensionality problem that as the dimensionality grows, the

21:54.880 --> 21:57.760
volume of objects grows exponentially fast.

21:57.760 --> 22:02.480
So kind of the coverage that you have of this very high dimensional space by getting samples

22:02.480 --> 22:06.640
that you have from your training data is extremely, extremely sparse, which is so high

22:06.640 --> 22:12.040
dimensional that you have like so few points, but makes it very different to sort of infer

22:12.040 --> 22:15.840
something about what is going on in this very high dimensional space.

22:15.840 --> 22:22.440
So you have to use some kind of prior knowledge to realize that actually, well, there are certain

22:22.440 --> 22:26.240
things that don't really matter, that we're looking for things that maybe are translational

22:26.240 --> 22:33.040
invariant, or maybe we know that some bands don't matter, or we have some kind of inductive

22:33.040 --> 22:40.440
bias that we can use to input that knowledge into the system so that we can still do something

22:40.440 --> 22:45.400
useful, even though the inputs are so high dimension.

22:45.400 --> 22:49.520
So in this case, dimensionality reduction, another way of thinking about that is just

22:49.520 --> 22:55.800
getting your machine learning algorithm to focus on the important parts of the thing that

22:55.800 --> 23:03.040
you're trying to train it on as opposed to the entire space of the images.

23:03.040 --> 23:09.040
And so you mentioned another thing you mentioned in terms of techniques for accomplishing this

23:09.040 --> 23:18.080
is transfer learning, which is applying pre-existing models as kind of starter models to training

23:18.080 --> 23:19.480
application specific models.

23:19.480 --> 23:21.480
Is that the way you would describe that?

23:21.480 --> 23:26.120
So that is that you typically have a task in the case of machine learning that you want

23:26.120 --> 23:29.840
to solve, and maybe you have limited amount of training data for the task.

23:29.840 --> 23:34.480
So you can set up a different but related machine learning task for which you have plenty

23:34.480 --> 23:36.000
of training data available.

23:36.000 --> 23:40.680
And by solving, by trying to learn how to solve this new task, the hope is that you're

23:40.680 --> 23:45.000
going to learn something useful, you're going to learn some skills that then you can transfer

23:45.000 --> 23:48.800
to your regional machine learning problem that you care about.

23:48.800 --> 23:53.760
And this is often done with models trained on like ImageNet data, for example, is that

23:53.760 --> 23:55.880
something that you guys did in particular?

23:55.880 --> 24:00.680
Or did you apply other models to this particular problem?

24:00.680 --> 24:03.320
And how did you go about thinking about where to start?

24:03.320 --> 24:09.320
Yeah, so that's how the ImageNet is often a great place to start with if you're looking

24:09.320 --> 24:11.840
with, if you're dealing with sort of natural images.

24:11.840 --> 24:17.480
One challenge in our application domain is that we're looking at satellite images, which

24:17.480 --> 24:23.240
look very, very different from the kind of images that you can find in ImageNet.

24:23.240 --> 24:27.720
They are not object-centric, like there's not a single object in the middle, like you typically

24:27.720 --> 24:28.720
have ImageNet.

24:28.720 --> 24:33.400
They are taken sort of like from this bird's eye perspective, which is very different

24:33.400 --> 24:34.400
from ImageNet.

24:34.400 --> 24:38.880
They have more bands, so it's not just RGB, but you have a whole set of other bands that

24:38.880 --> 24:40.680
you don't typically have in ImageNet.

24:40.680 --> 24:46.960
And so the kind of typical features that you, that the network say convolutional or network

24:46.960 --> 24:53.960
pre-trained on ImageNet is able to discover, do not work well when dealing with satellite

24:53.960 --> 24:54.960
images.

24:54.960 --> 25:00.080
And so what we did was to come up with other transfer learning ideas.

25:00.080 --> 25:04.920
This was actually for a slightly different problem in which we were trying to predict

25:04.920 --> 25:12.520
the distribution of wealth and poverty, in this case, in developing countries.

25:12.520 --> 25:17.760
Again, we were trying to do this using a cheap and conventional data sources.

25:17.760 --> 25:21.840
In this case, we were using higher solution satellite images, while there is very little

25:21.840 --> 25:27.680
data on poverty, there are many African countries that are not taking a nationally representative

25:27.680 --> 25:29.720
survey, maybe in a decade or so.

25:29.720 --> 25:34.000
Satellite images are available in basically every part of the world that they can update

25:34.000 --> 25:35.880
it very frequently.

25:35.880 --> 25:41.080
And they contain a lot of information about various types of socioeconomic outcomes, both

25:41.080 --> 25:48.840
in terms of poverty, wealth, but also, yeah, agriculture outcomes, like we were just

25:48.840 --> 25:52.600
talking about before in the context of copy and prediction.

25:52.600 --> 25:57.240
And so the problem that we were looking at in that paper was that of trying to predict

25:57.240 --> 26:04.920
various types of poverty estimates like asset-based measures of wealth or other measures of poverty

26:04.920 --> 26:05.920
based on income.

26:05.920 --> 26:10.680
Again, using raw satellite images, which are widely available.

26:10.680 --> 26:15.120
And the challenge was that, again, we have very limited training data available to train

26:15.120 --> 26:16.120
these models.

26:16.120 --> 26:18.520
And so we had to do some transfer learning.

26:18.520 --> 26:24.320
And there, the idea was that it turns out that there are satellites that take images of

26:24.320 --> 26:27.080
the Earth both during day and during night.

26:27.080 --> 26:34.720
And so during night, you get to see the amount of nighttime light intensity associated with

26:34.720 --> 26:36.680
essentially every region in the world.

26:36.680 --> 26:42.920
And it turns out that nighttime light intensity is heavily correlated with the level of economic

26:42.920 --> 26:43.920
activity.

26:43.920 --> 26:47.960
If you haven't seen it, you should try to see the different areas in North Korea and South

26:47.960 --> 26:48.960
Korea.

26:48.960 --> 26:53.720
You're going to see that South Korea almost looks like an island at night because North Korea

26:53.720 --> 26:55.040
is so dark at night.

26:55.040 --> 27:00.840
And so the idea there was to see whether we can train a machine learning model to predict

27:00.840 --> 27:07.280
the amount of nighttime light intensity for many, many locations across the world just

27:07.280 --> 27:11.320
by looking at the corresponding daytime images.

27:11.320 --> 27:12.320
And we could do that.

27:12.320 --> 27:16.440
And this is a task for which we have essentially an infinite amount of training data because

27:16.440 --> 27:20.920
these satellites are continuously taking images both during day and during night.

27:20.920 --> 27:25.960
And the hope was that by training the model to solve this task, people discover features.

27:25.960 --> 27:30.600
They are somehow related to the level of economic activity.

27:30.600 --> 27:35.040
And it turns out that it is indeed the case, it turns out that if you train a convolution

27:35.040 --> 27:40.480
neural net to solve this task, and then you sort of try to visualize the features that

27:40.480 --> 27:47.280
the network learns, you discover very features that are very semantically meaningful, like there

27:47.280 --> 27:52.560
is a filter that learns how to recognize roads, other filters try to identify different

27:52.560 --> 27:58.720
types of houses or other features of the landscape, like farmland, whether there are roads with

27:58.720 --> 28:01.960
lots of traffic or not, even swimming pools.

28:01.960 --> 28:06.000
And the nice thing was that this was all discovered in a unsupervised way.

28:06.000 --> 28:11.240
Like we never told the network, you know, look for houses or provide labels of what a road

28:11.240 --> 28:14.240
is or what a house is or what a swimming pool is.

28:14.240 --> 28:20.600
It really discovered these semantically meaningful features by itself, purely by solving this

28:20.600 --> 28:26.560
transfer learning task of trying to predict nighttime-line intensity from data and images.

28:26.560 --> 28:33.320
And then what we did was to use these features that we learned in this transfer learning task

28:33.320 --> 28:38.440
to actually predict the various poverty measures that we cared about.

28:38.440 --> 28:43.960
And because we had such good features, again, we were able to get very high accuracy, even

28:43.960 --> 28:49.680
though we had, there's limited amount of training data corresponding to these poverty metrics

28:49.680 --> 28:51.560
that we cared about predict.

28:51.560 --> 28:54.240
Wow, very, very interesting.

28:54.240 --> 29:00.840
What do you call the name of the paper where you describe the transfer learning technique?

29:00.840 --> 29:02.160
So we had two papers on this.

29:02.160 --> 29:09.120
There was a paper, a triple AI, which is called the transfer learning from the features

29:09.120 --> 29:11.760
for remote sensing and poverty mapping.

29:11.760 --> 29:17.160
And then we had a follow-up paper on science called the combining satellite imagery and machine

29:17.160 --> 29:23.200
learning to predict poverty, where we actually detailed the kind of results that we got in

29:23.200 --> 29:27.400
predicting poverty in across five African countries.

29:27.400 --> 29:32.520
And we showed that it can really outperform all the previously existing techniques by quite

29:32.520 --> 29:34.640
a large margin.

29:34.640 --> 29:40.920
So in that what were without totally losing our place on the crop yield prediction,

29:40.920 --> 29:48.280
can you talk through some of the techniques that went into the transfer learning work?

29:48.280 --> 29:55.120
So that's essentially the key, sort of, just of the transfer learning, trying to find

29:55.120 --> 30:00.080
a task that is somehow correlated with the one you care about, but for which you have

30:00.080 --> 30:01.720
a lot of training data.

30:01.720 --> 30:05.360
We didn't actually use transfer learning for the crop yield prediction.

30:05.360 --> 30:07.640
We just used the dimensionality reduction.

30:07.640 --> 30:13.080
The other thing we used was an idea called semi-supervised learning, which is another approach

30:13.080 --> 30:18.920
that you can use when you have a small amount of labeled training data and potentially

30:18.920 --> 30:24.160
very large amount of unlabeled training data, which is, for example, the case in our applications

30:24.160 --> 30:30.080
where we have a lot of satellite images, but we have a very small amount of labels corresponding

30:30.080 --> 30:38.120
to the amount of soybeans that were produced in different regions or the poverty, sort

30:38.120 --> 30:42.840
of, like, survey-based measures of poverty in developing countries.

30:42.840 --> 30:48.360
And the kind of technique that we use to do semi-supervised learning in this case is a

30:48.360 --> 30:52.000
combination of neural networks with Gaussian processes.

30:52.000 --> 30:58.160
The idea is that we are trying to predict things that have a structure, like we're trying

30:58.160 --> 31:03.960
to predict a distribution of, say, wealth or crop yields across space and time.

31:03.960 --> 31:09.240
And we are expecting these outputs that we're trying to predict to change slowly as a function

31:09.240 --> 31:11.440
of time and space.

31:11.440 --> 31:15.200
And so that's some kind of microbiology that you can incorporate into the model.

31:15.200 --> 31:21.120
And we did this using our Gaussian process, which is a probabilistic model that you can

31:21.120 --> 31:26.360
use to model all sorts of things, but it's very popular in the Geo-statistics community

31:26.360 --> 31:33.160
to sort of model functions that are over space and time, and then you can use this probabilistic

31:33.160 --> 31:38.520
model to not only make predictions, but also to measure the uncertainty that you have

31:38.520 --> 31:43.280
when you make predictions at new locations for which you don't have training data.

31:43.280 --> 31:51.080
And so is the Gaussian process primarily used, especially, as you just mentioned, or is

31:51.080 --> 31:57.520
it also used in time, meaning I've got a label at some point in time, but I don't know

31:57.520 --> 32:01.560
how the value of that label changes over time.

32:01.560 --> 32:05.320
So we apply a Gaussian to that, or both of the above.

32:05.320 --> 32:09.800
Both of the above, yeah, in our case, we were looking both over space and time.

32:09.800 --> 32:15.760
So in general, you can use Gaussian processes over any kind of input space.

32:15.760 --> 32:22.480
They're often used to model functions of the change over space and time.

32:22.480 --> 32:29.600
And the key thing that we did was to combine these with neural networks for each location

32:29.600 --> 32:30.920
in space and time.

32:30.920 --> 32:36.320
We have a corresponding image that is collected by a satellite, and so we somehow want to

32:36.320 --> 32:42.920
include that information to inform the predictions that are made by the discussion process.

32:42.920 --> 32:48.520
And the idea was that we could somehow extract features from an image using a convolution

32:48.520 --> 32:53.640
neural network, and then use these features together sort of the spatial and temporal structure

32:53.640 --> 32:56.000
of the problem to make predictions.

32:56.000 --> 33:00.920
And the nice thing about the Gaussian process is that it not only allows you to make predictions,

33:00.920 --> 33:05.640
but again, it lets you quantify the uncertainty that you have when you make predictions and

33:05.640 --> 33:08.160
new unlabeled data points.

33:08.160 --> 33:14.400
So our idea of doing semi-supervised learning was to sort of join to train the neural network

33:14.400 --> 33:20.240
and the Gaussian process in order to achieve a good fit at the points for which we had actual

33:20.240 --> 33:21.240
labels.

33:21.240 --> 33:27.200
What at the same time, trying to minimize the uncertainty at points for which we don't

33:27.200 --> 33:31.480
have labels, points for which we have the inputs, but we don't have the outputs.

33:31.480 --> 33:36.680
And it turns out that even though we don't have the actual sort of output for this, for

33:36.680 --> 33:41.160
this unlabeled data points, the Gaussian process will still be able to make a prediction and

33:41.160 --> 33:44.120
will still be able to quantify the uncertainty of this prediction.

33:44.120 --> 33:49.520
So we can join to train both of them to actually minimize this measure of uncertainty.

33:49.520 --> 33:55.160
And that's essentially how we use the unlabeled data points together with the labeled ones

33:55.160 --> 33:58.520
to in this semi-supervised learning framework.

33:58.520 --> 34:02.960
And in some sense, it's a formal regularization that is sort of forcing the model to look for

34:02.960 --> 34:09.280
features that are not only useful for the labeled data points, but it's sort of also, they

34:09.280 --> 34:14.080
are also relevant for the unlabeled data points.

34:14.080 --> 34:19.520
And it turns out that by using this framework, the semi-supervised framework we developed,

34:19.520 --> 34:25.640
we were able to further improve the accuracy in both the power-to-tradition task and the

34:25.640 --> 34:27.160
crop-to-tradition task.

34:27.160 --> 34:36.120
Okay, so taking a step back, you've got this image data from the satellites that is, you

34:36.120 --> 34:43.640
know, has pixels, but is multi-model, so it has multiple pixels for each location.

34:43.640 --> 34:50.920
When you talk about making a prediction from a particular point, is that, what's the

34:50.920 --> 34:51.920
granularity there?

34:51.920 --> 34:57.680
Are you predicting the level of Walter Poverty from a pixel or are you somehow aggregating

34:57.680 --> 34:59.880
multiple pixels to form an area?

34:59.880 --> 35:00.880
Yeah, good question.

35:00.880 --> 35:06.120
So we were looking, we were making predictions at the one kilometer or one kilometer sort

35:06.120 --> 35:09.400
of areas, which correspond to multiple pixels.

35:09.400 --> 35:14.440
So for each sort of location, we would collect multiple images that would cover that location,

35:14.440 --> 35:18.320
and then we would sort of aggregate all the information and make a prediction for that

35:18.320 --> 35:19.320
area.

35:19.320 --> 35:25.320
And so you've got all this input data, you're feeding that input data into a convolutional

35:25.320 --> 35:33.880
neural net, which is essentially taking kind of various chunks of the images and translating

35:33.880 --> 35:38.480
them, rotating them, things like that to try to identify what are the salient features

35:38.480 --> 35:41.040
within the images.

35:41.040 --> 35:46.960
And then your, I guess the question that I'm getting at is how and, you know, where do

35:46.960 --> 35:52.640
you marry the Gaussian stuff with the CNN stuff?

35:52.640 --> 35:54.160
Is that a training?

35:54.160 --> 35:56.200
Is that a step that's taken in the training?

35:56.200 --> 35:59.800
Is that a feature of the model architecture?

35:59.800 --> 36:01.080
How do they tie together?

36:01.080 --> 36:03.680
Yeah, you can think of it as both.

36:03.680 --> 36:09.360
So you can sort of think that there is one neural network making prediction at each

36:09.360 --> 36:10.840
different location.

36:10.840 --> 36:16.160
But then all the predictions that are made across different locations are all tied together,

36:16.160 --> 36:19.080
because we know that the outputs are spatially correlated.

36:19.080 --> 36:24.800
So if you take two locations that are close to each other, we know that we sort of expect

36:24.800 --> 36:29.400
the outcomes that we're trying to predict to be more similar to the locations are close

36:29.400 --> 36:30.400
to each other.

36:30.400 --> 36:36.120
And we know that empirically, if you plot a very grand measuring place, there is a large

36:36.120 --> 36:39.240
spatial correlation in these kind of things that we're trying to predict.

36:39.240 --> 36:45.080
As you can imagine, the Gaussian process has an extra layer in your neural network at

36:45.080 --> 36:46.080
the very end.

36:46.080 --> 36:52.040
It's kind of tying together, coupling together all the outputs of these predictions made

36:52.040 --> 36:55.720
by all these various convolutional neural networks.

36:55.720 --> 37:01.920
And by using this, you can sort of exploit this prior knowledge that you have about the

37:01.920 --> 37:06.080
spatial and potentially important dependencies across the outputs.

37:06.080 --> 37:09.240
How many layers did the network end up having?

37:09.240 --> 37:12.800
Yeah, we experimented with several architectures.

37:12.800 --> 37:19.520
I think the one in the science paper, it was based on a BGG network.

37:19.520 --> 37:24.640
More recently, we've been playing with Rasmash of 50 layers, and those tend to work

37:24.640 --> 37:26.160
even better.

37:26.160 --> 37:33.480
And what's the, do you have a sense for the relative performance within without the Gaussian

37:33.480 --> 37:34.480
layer?

37:34.480 --> 37:36.080
Or I imagine that's what you talked about in your paper.

37:36.080 --> 37:43.200
Was that, you know, the, I guess, fundamentally, did the Gaussian layer make this possible?

37:43.200 --> 37:47.160
Or was it adding an incremental boost in performance?

37:47.160 --> 37:50.560
It's adding an incremental boosting performance.

37:50.560 --> 37:55.680
I think you could have gotten some reasonable results, even without the Gaussian process,

37:55.680 --> 38:02.520
but the Gaussian process definitely, definitely helped by maybe improving by maybe 10% or

38:02.520 --> 38:05.120
15% something like that.

38:05.120 --> 38:10.800
Is there anything in particular else that you learned about the process of architecting

38:10.800 --> 38:14.960
networks for this type of problem in the context of this work?

38:14.960 --> 38:27.240
Well, one thing that matters the most is that somehow it's quite interesting how we started

38:27.240 --> 38:32.440
trying to sort of inspire the architectures and try out networks that were sort of losing

38:32.440 --> 38:38.120
inspired by what our domain experts would tell us was important to say in the context

38:38.120 --> 38:42.840
of crop yield prediction, there has been quite a bit of work in the remote sensing literature

38:42.840 --> 38:48.800
in coming up with the handcrafted features and various types of indexes, combination

38:48.800 --> 38:55.400
of various bands that they think are going to be predictive of vegetation growth and

38:55.400 --> 38:58.080
therefore also crop yields.

38:58.080 --> 39:03.560
And it turns out that if you actually train and the neural network to sort of discover

39:03.560 --> 39:10.960
the features by itself just by doing representation learning using a modern machine learning approach

39:10.960 --> 39:16.520
where we let the data speak for itself and try to identify which features are relevant

39:16.520 --> 39:17.520
directly from data.

39:17.520 --> 39:26.120
It turns out that our model ended up using a very, very different kind of different inputs

39:26.120 --> 39:31.000
ended up matter, mattering much more for our model than what was previously thought in

39:31.000 --> 39:32.640
the remote sensing literature.

39:32.640 --> 39:37.640
So some bands that people thought were not particularly important for crop yield prediction

39:37.640 --> 39:42.080
ended up being very, very important for our neural network and vice versa.

39:42.080 --> 39:46.120
So I think it's similar to what's going on, what happened in computer vision, what

39:46.120 --> 39:52.120
people for a long time were handcrafting features and then it turns out that if you sort of

39:52.120 --> 39:56.920
train and to end the neural network you can do an out better than anything sort of people

39:56.920 --> 40:03.600
had the all the kind of kind of crafted features that people had come up with in over the years.

40:03.600 --> 40:08.760
And so something similar was happening in this case that using this modern machine learning

40:08.760 --> 40:12.600
techniques we were able to come up with very different features than what was previously

40:12.600 --> 40:13.600
thought.

40:13.600 --> 40:20.160
And do you attribute the difference between what your models came up with and what domain

40:20.160 --> 40:28.240
experts tended to use with biases on the part of the domain experts with confounding factors

40:28.240 --> 40:32.440
in the model or the use case, any particular insights there?

40:32.440 --> 40:37.720
So unfortunately this neural network is very powerful and making predictions but it can

40:37.720 --> 40:40.720
be hard to really, they're not getting different.

40:40.720 --> 40:45.160
It can be very hard to figure out exactly what they're doing.

40:45.160 --> 40:49.800
We'll talk about that problem quite a bit on the shelf.

40:49.800 --> 40:54.560
So we had the same issues and sort of trying to understand and trying to explain what this

40:54.560 --> 40:57.840
model is doing was very hard.

40:57.840 --> 41:05.560
And so we don't have a good sense of why at the moment is something we're actually actively

41:05.560 --> 41:10.520
researching and really trying to understand why this model capturing and why does it perform

41:10.520 --> 41:12.800
so much better on the previous techniques.

41:12.800 --> 41:18.800
But I think we're going to need probably entirely new techniques to figure out these issues

41:18.800 --> 41:22.560
in a more physical way.

41:22.560 --> 41:29.280
When you describe the approach of using the nighttime imagery to train the model on the

41:29.280 --> 41:35.520
daytime imagery imagery, it reminded me a little bit of another one of your recent papers.

41:35.520 --> 41:43.520
Also AAAI paper from this year on the label-free supervision of neural networks with physics.

41:43.520 --> 41:46.920
And so there are a lot of kind of a lot of parallels there.

41:46.920 --> 41:50.880
But this one, the context in which it's talked about in this paper is kind of an interesting

41:50.880 --> 41:57.160
one where you've got models that you're trying to predict, well, I guess I should allow

41:57.160 --> 42:00.200
you to explain that.

42:00.200 --> 42:03.960
Why don't you go ahead and explain what the focus of that paper was?

42:03.960 --> 42:04.960
Sure.

42:04.960 --> 42:09.680
So I mean, it fits into the general, one of the general themes that I'm excited about,

42:09.680 --> 42:13.600
which is this idea of trying to incorporate the main knowledge into machine learning

42:13.600 --> 42:14.600
systems.

42:14.600 --> 42:23.000
So find ways to provide supervision that are alternative to coming up with millions of

42:23.000 --> 42:24.000
labeled examples.

42:24.000 --> 42:28.680
And so the idea of doing semi-supervised learning, the idea of doing transfer learning,

42:28.680 --> 42:35.480
the idea of doing various dimensionality reduction techniques, fits into this broader

42:35.480 --> 42:36.480
agenda.

42:36.480 --> 42:42.840
I'm trying to see where we can go beyond this labeling, which is really a major bottleneck

42:42.840 --> 42:47.200
and it's really preventing us from applying this machine learning techniques in domains

42:47.200 --> 42:51.760
like the kind of applications we are interested in, this is the ability space where the labels

42:51.760 --> 42:52.760
are just not available.

42:52.760 --> 42:55.760
And I mean, if you wanted more labels, you could not get them.

42:55.760 --> 43:04.480
Like we were a couple of months ago, I said, we were running our model in Somalia, we're

43:04.480 --> 43:08.080
making poverty predictions in Somalia for the World Bank.

43:08.080 --> 43:12.320
And that's a country where they cannot, they were telling me how they cannot even stand

43:12.320 --> 43:17.040
that there are people on the ground to collect data because it's just too dangerous.

43:17.040 --> 43:22.000
And so we are literally looking at problems where getting labels is not possible or is just

43:22.000 --> 43:23.000
too expensive.

43:23.000 --> 43:29.640
So we need to think about different ways to incorporate domain knowledge into machine learning

43:29.640 --> 43:30.640
systems.

43:30.640 --> 43:34.200
And this particular triple AI paper that you mentioned, we were looking at whether we

43:34.200 --> 43:41.600
can use a prior domain knowledge, like knowledge about the laws of physics to supervise object

43:41.600 --> 43:46.880
detectors in that case, supervise, convolutional neural networks and teach them how to recognize

43:46.880 --> 43:52.320
objects by providing sort of this high level description of the kind of things the network

43:52.320 --> 43:53.320
should be looking for.

43:53.320 --> 43:57.440
So even though we don't have precise labels saying, okay, here's the object that you should

43:57.440 --> 43:58.960
be looking for.

43:58.960 --> 44:02.640
One thing that we tried was to see whether it's possible to learn how to recognize these

44:02.640 --> 44:07.280
objects by providing a high level description, like something like a loose description of

44:07.280 --> 44:11.160
the dynamics of the kinematics of the object.

44:11.160 --> 44:18.000
And we showed in that paper that it's possible in some cases, we provide some proof of concept

44:18.000 --> 44:22.480
demonstration that it's possible for example to train a convolutional neural network to

44:22.480 --> 44:30.160
recognize objects moving, falling sort of through the air, just by providing some prior

44:30.160 --> 44:35.560
knowledge about gravity essentially just by saying, well, I know that if an object is falling

44:35.560 --> 44:42.240
and moving through the air, then there's gravity, and so the trajectory will form a parabola.

44:42.240 --> 44:47.440
Just by using this prior knowledge, it turns out it was sufficient to learn how to recognize

44:47.440 --> 44:50.160
objects moving in that video.

44:50.160 --> 44:55.520
And so to be a little bit more concrete in this paper, you use the example of projectile

44:55.520 --> 44:57.840
in particular a pillow.

44:57.840 --> 45:02.440
And from some of the images that you had in the paper, you can imagine a neural network

45:02.440 --> 45:07.760
getting confused between the pillow and the fluorescent lighting, and presumably the knowledge

45:07.760 --> 45:12.080
that you're giving it about the laws of motion, if you will, the physics of a projectile

45:12.080 --> 45:16.400
pillow will help the network figure out where the pillow is.

45:16.400 --> 45:17.400
Exactly.

45:17.400 --> 45:24.160
And then it strikes me that perhaps this is relatable to your research on crop yields.

45:24.160 --> 45:30.760
And for example, you may have knowledge about seasonality in the way, or if not crop

45:30.760 --> 45:37.240
yields the poverty piece, but you may have information about seasonality and how tree

45:37.240 --> 45:41.560
leaf colors change over time, or there's all kinds of things that we know about the

45:41.560 --> 45:47.160
physical world, and one would presume that the more we can incorporate that into our

45:47.160 --> 45:49.600
models, the smarter those models would be.

45:49.600 --> 45:50.600
Correct.

45:50.600 --> 45:57.760
And so how exactly do you, how do you incorporate that into models that an algebraic representation

45:57.760 --> 46:00.800
of some sort, or so, yeah.

46:00.800 --> 46:06.320
So in the triple algorithm, we were only looking at algebraic representation in the case

46:06.320 --> 46:09.600
of physics law, or using logical representation.

46:09.600 --> 46:14.600
We also had another example where we showed that if you know some, you have some prior knowledge

46:14.600 --> 46:20.440
that you can describe using logical forms, something like whenever object A appears in

46:20.440 --> 46:24.600
an image, then object B is also present in an image.

46:24.600 --> 46:31.200
Think this kind of relationship that can be fairly naturally captured using logic in terms

46:31.200 --> 46:37.040
of it's a fairly natural framework for you to model and to write down some prior knowledge

46:37.040 --> 46:38.520
of a particular domain.

46:38.520 --> 46:43.280
That's those are the two things that we that we explore in the triple AI paper.

46:43.280 --> 46:47.480
We are doing some work right now on using simulators.

46:47.480 --> 46:55.200
That's another fairly common way in which we can formulate prior knowledge about domain,

46:55.200 --> 46:59.760
especially in the physical sciences, we might have a rough simulator that gives us a sense

46:59.760 --> 47:03.720
of how a particular physical system behaves.

47:03.720 --> 47:07.440
And the question was whether some of the things we are exploring right now is to see how

47:07.440 --> 47:14.040
to incorporate a simulator and put it together with data on labeled data and see whether

47:14.040 --> 47:19.840
we can combine these things together and use the knowledge that is sort of inherently present

47:19.840 --> 47:24.920
in the simulator to provide supervision to say neural networks.

47:24.920 --> 47:31.440
One of the areas spending some time researching of latest industrial AI and how AI factors

47:31.440 --> 47:37.120
into control and optimization scenarios and robotics and manufacturing and things like

47:37.120 --> 47:38.120
that.

47:38.120 --> 47:42.720
And the use of simulators is a key importance in those areas.

47:42.720 --> 47:45.680
So it's interesting to see you applying that here as well.

47:45.680 --> 47:54.680
So are you are we at the point where you have a how generalizable is the an architecture

47:54.680 --> 48:02.000
that can incorporate these types of rules, meaning is there is the you know, a particular

48:02.000 --> 48:08.840
law of physics, you know, baked deeply into the model architecture or the training process

48:08.840 --> 48:15.440
or, you know, can you envision some kind of architecture or a training regime that you

48:15.440 --> 48:21.760
can feed a somewhat generic kind of rule or law engine that can bake the stuff in.

48:21.760 --> 48:23.160
Yeah, that's a great question.

48:23.160 --> 48:29.040
So at the moment, it's all very much specific to particular constraints and particular forms

48:29.040 --> 48:30.040
of domain knowledge.

48:30.040 --> 48:34.560
At the moment, basically, you have to invest a considerable effort into the engineering

48:34.560 --> 48:39.400
and the right objective function and figure out how to bake in the prior knowledge into

48:39.400 --> 48:40.400
the system.

48:40.400 --> 48:44.360
So it's a different kind of trade off where you're sort of you're still spending and seeing

48:44.360 --> 48:48.480
significant amount of time sort of providing supervision to the system.

48:48.480 --> 48:53.800
But the advantage is that it does not scale linearly in the number of in the size of your

48:53.800 --> 48:54.800
training data.

48:54.800 --> 48:55.800
Right.

48:55.800 --> 49:03.040
So it's an interesting trade off that might be might be advantageous in some situations.

49:03.040 --> 49:08.280
But the dream is to be as you said, sort of come up with a general language that we can

49:08.280 --> 49:13.560
use or a general system that will make it easy to incorporate prior knowledge into

49:13.560 --> 49:14.560
engineering systems.

49:14.560 --> 49:17.040
But we're not there at the moment.

49:17.040 --> 49:18.040
We don't have it.

49:18.040 --> 49:23.240
But I think that that's what we need in order to make sure that people can use the systems

49:23.240 --> 49:27.280
and they can really dramatically reduce the amount of training data that they need to

49:27.280 --> 49:28.600
solve their tasks.

49:28.600 --> 49:34.040
Maybe we're not going to be able to get away with the training data completely, like we

49:34.040 --> 49:38.200
showed in the trip or I paper where we didn't use any label at all.

49:38.200 --> 49:44.040
But maybe and the hope is that it would work like for us for humans where maybe a handful

49:44.040 --> 49:50.480
of examples is enough to learn how to solve an interesting task because and maybe just

49:50.480 --> 49:56.040
a high level description plus a few examples is enough to solve the problem as opposed

49:56.040 --> 50:03.560
to millions of labels, which which is not how we how we learn, I think, right.

50:03.560 --> 50:10.960
So any thoughts on where you see this all going or how you see it evolving over time or

50:10.960 --> 50:14.400
even simply what you're most excited about right now?

50:14.400 --> 50:20.160
Yeah, that's definitely something I'm interested in trying to understand how to put in how

50:20.160 --> 50:26.800
to put in prior knowledge, how to combine it with labels in a most effective way, also

50:26.800 --> 50:31.400
how to do, I mean, a super buzz learning, that's another thing we're playing with this

50:31.400 --> 50:37.600
day is how do you discover structures from data and how far we can push this thing and

50:37.600 --> 50:43.720
really how many, how much, why, how much can we reduce the need for training data and

50:43.720 --> 50:47.960
these are all problems that I'm very excited about thinking about what's the right way

50:47.960 --> 50:52.280
to represent knowledge and what's the right way to put the knowledge into the machine learning

50:52.280 --> 50:57.440
systems and finally how to reverse the process like here we showed how we can put in some

50:57.440 --> 51:00.560
physics and we can make the machine learning system better.

51:00.560 --> 51:06.520
But ideally, I would like to then invert this and try to go from then try to still physics

51:06.520 --> 51:09.800
and try to still knowledge from the from the raw data.

51:09.800 --> 51:14.800
I want my machine learning system to come up with new hypothesis and one is to discover

51:14.800 --> 51:19.760
where have it be just with the end of it is and I think that's what's really exciting

51:19.760 --> 51:24.280
I think we're we're we're so far but I think that would be pretty amazing because then

51:24.280 --> 51:28.640
we can really think about using machine learning in the most physical sciences that's a

51:28.640 --> 51:34.440
space in which we haven't seen as much as we not not as much as in sort of other kind

51:34.440 --> 51:39.120
of application domains I think so I think but there's a lot of potential for using AI

51:39.120 --> 51:42.680
machine learning in the physical sciences because they're doing more and more of high

51:42.680 --> 51:50.080
throughput experiments collecting massive amounts of data and to human time humans are

51:50.080 --> 51:53.400
really the bottleneck trying to analyze this data trying to figure out what is going

51:53.400 --> 51:58.360
on figure trends understand what is interesting what it's not and if we had a way to to put

51:58.360 --> 52:05.320
in that machine learning and any AI systems to help and then sort of make the process easier

52:05.320 --> 52:09.760
trying to automate it a little bit or as much as possible I think I think the benefits

52:09.760 --> 52:11.520
could be could be really huge.

52:11.520 --> 52:14.280
Yeah absolutely absolutely.

52:14.280 --> 52:20.840
So we talked about a bunch of your work in particular several papers if someone wanted

52:20.840 --> 52:27.000
to you know if it's possible to do this kind of get up to speed on your research and

52:27.000 --> 52:32.160
the you know the kinds of things we've discussed are there you know one two or three papers

52:32.160 --> 52:36.480
that would be the kind of key things to you know help them understand what you're up

52:36.480 --> 52:37.480
to.

52:37.480 --> 52:42.040
So if you want to learn about the physics and domain knowledge just the triple AI paper

52:42.040 --> 52:47.520
called supervised supervising neural network with physics and other domain knowledge for

52:47.520 --> 52:53.760
the sustainability applications so there are the papers you mentioned there's the science

52:53.760 --> 52:58.440
paper and that there's the deep Gaussian process for property prediction which is also

52:58.440 --> 53:02.560
triple AI showing discussing how to use the Gaussian processes and the missionality

53:02.560 --> 53:06.040
reductions in to deal with remote sensing data.

53:06.040 --> 53:08.440
So those are the right places to start.

53:08.440 --> 53:15.240
And just to close out if the what's the someone wants to kind of learn more or get in touch

53:15.240 --> 53:20.000
or kind of follow you on a social media if you're out there any particular coordinates

53:20.000 --> 53:21.480
you would point folks to.

53:21.480 --> 53:26.400
Yeah come to my website just to Google my name it's probably the first thing that comes

53:26.400 --> 53:30.840
up with Google it's you can see all my papers or visit my research group website then

53:30.840 --> 53:32.480
you can see all the latest stuff.

53:32.480 --> 53:37.080
Well we'll link directly to it in the show notes so folks won't need to Google it.

53:37.080 --> 53:41.840
But this was a great conversation I really enjoyed talking to you about the work and

53:41.840 --> 53:46.760
I'm super excited about the work you're doing the papers are really interesting and I learned

53:46.760 --> 53:47.760
a ton.

53:47.760 --> 53:48.760
Thanks so much Stefano.

53:48.760 --> 53:49.760
Thanks for having me.

53:49.760 --> 53:50.760
It was a lot of fun too.

53:50.760 --> 53:51.760
Bye bye.

53:51.760 --> 53:52.760
Bye bye.

53:52.760 --> 54:01.280
Alright everyone that's our show for today.

54:01.280 --> 54:05.760
Once again thanks so much for listening and for your continued support.

54:05.760 --> 54:10.200
Don't forget to share your favorite quote from this show to get one of our new stickers.

54:10.200 --> 54:15.160
You can share them via the show notes page via Twitter via our Facebook page or via a

54:15.160 --> 54:18.000
comment on YouTube or SoundCloud.

54:18.000 --> 54:22.680
Please use the hashtag Twimlai on Twitter.

54:22.680 --> 54:28.840
The notes for this show will be up on twimlai.com slash talk slash 15 where you'll find links

54:28.840 --> 54:32.920
to Stefano and the various resources mentioned in the show.

54:32.920 --> 54:59.680
Thanks so much for listening and catch you next time.

