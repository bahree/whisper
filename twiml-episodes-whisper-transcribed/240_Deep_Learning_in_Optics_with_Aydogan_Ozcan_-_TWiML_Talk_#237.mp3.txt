Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
Today we're joined by Ida Wan-Euzjin, professor of electrical and computer engineering at UCLA,
where his research group focuses on photonics and its applications to nano and biotechnology.
In our conversation, we explore his group's research into the intersection of deep learning
and optics, holography, and computational imaging.
We specifically look at a really interesting project to create all optical neural networks,
which work based on diffraction, where the printed pixels of the network are analogous
to neurons.
We also explore some of the practical applications for their research and other areas of interest
for their group, and now on to the show.
Alright everyone, I am on the line with Ida Wan-Euzjin.
Ida Wan is a professor in electrical and computer engineering department at UCLA.
Ida Wan, welcome to this week in machine learning and AI.
Well thanks for having me, Sam.
So you are currently working at the intersection point between optics and deep learning, and
I'm really interested in jumping into this and learning more about the work that's coming
out of your research group, but before we do, tell us a little bit about your background
and how you started working with deep learning.
Sure, sure.
Yes, I work in optics, applications of optics, specifically computational imaging and sensing
techniques, where we're creating new types of imaging systems, new types of microscopes
and sensors, with heavily focusing on applications in biomedical space.
For example, pathology, creating new types of microscopes in pathology, at the same time
different types of sensors for telemedicine, mobile health related applications, and more
and more recently we're interested in the environmental monitoring looking into water
and air quality.
And there's a huge opportunity ahead of us at the intersection of optics and machine
learning that we've been exploiting for the last several years.
Let's first start with essentially using machine learning to look at the images that we
reconstruct and finding specific targets of interest.
Let's say you're looking at certain parasites in drinking water and you create a mobile
microscope and create images of particles inside your observation volume chamber, disposable
cartridge.
And then of course you need some sort of machine learning tool to look at what's captured
there and specifically label, let's say, certain parasites of interest.
Say it starts from that direction of annotating images and looking for specific signatures
that you're looking that you're after.
But more and more recently we're realizing the power of specially deep learning to design
instruments, the design sensors from image labeling, image classification, we're moving
toward data driven designs for optical instrumentation for optical sensors.
In a sense, I've spent easily the last 10 years creating new types of computational reconstruction
methods for different kinds of microscopes, holographic microscopes, for example.
And in the last few years, we're realizing that we're moving from physics driven solutions
to data driven solutions, how data and the framework around deep learning, holistically they're
helping us to essentially better reconstruct images, especially for holography, we've
seen some great examples of that where without any iterations, training purely a neural net
based on input output labels, you can actually reconstruct holograms much better than traditional
methods of solving an inverse problem and that's very, very exciting for actually optics
community at large as a whole.
What does it mean to reconstruct the hologram?
What specifically are we trying to do there?
So one class of mobile microscopes that we've created is actually using diffraction of light.
In fact, holographic shadows of specimen to reconstruct images.
So in this line of work, you're taking, for example, a CMOS image or like the SCCD or
a CMOS image or the same thing that you have at the back of your cell phone and place
specimen right in front of it without any lenses where the specimen can be, let's say, pathologist
sample or a smear and you shine light through it and you capture with that simple configuration
with a simple 510 megapixel imager, the diffraction hologram of a specimen.
So that contains all the three-dimensional information of the specimen, but it needs
to be reconstructed because it doesn't immediately give you the micro scale information of the sample.
It gives you a hologram which is light interference.
The light scattered by the object interferes with the background light giving you some fringes
and a hologram.
It needs to be reconstructed.
So physics is great at providing us tools to do this reconstruction.
But nowadays, one alternative method that we've been exploring and being very successful
at is to use actually holograms as inputs to a neural net that has been trained with
reconstructions, gold standard labels coming from physical reconstruction methods which
are relatively speaking cumbersome in the sense that they're typically using some iterations
to clean up the image and give you something that with reasonable resolution shows you
what's going on in your sample.
Now we're seeing that through data you can actually teach a neural net to reconstruct
the hologram from those interference patterns, from those fringes, to go back to the sample
plane, sample volume to give you all these signatures of the samples.
Better than before, in the sense that this reconstruction is now extremely fast, and
it can also provide some better rejection of some of these interference artifacts that
are coming to holography.
It's just one example where data-driven approaches are providing alternatives to intuition driven
by physics to do every construction.
And so in this case is the your ground truth and image that is image like traditionally
like with a lens in front of the same type of CCD, and then you take an image of the
same subjects without the lens just using the diffractive patterns, and that's how you
produce your labels or is it more involved in that?
You can definitely take that approach and use standard microscopes to create labels.
A better approach is actually to use traditional physical reconstruction methods that take
maybe a bunch of holograms to reconstruct the sample's information.
Different than a traditional lens-based bright field microscope, holography has two different
channels of information.
One of them relates to the amplitude of the sample, amplitude of how the light is scattered,
and the other one is the phase information of the sample.
And that's where holography is very powerful as a coherent imaging modality.
And that's why a better approach to use as gold standard ground truth images is actually
coming from traditional holographic reconstruction solutions.
They provide essentially the physics enabled ground truth.
And after we've done that, we see some very interesting results where it solves the
inverse problem in a different path, but very robust in terms of what you need to see,
what you need to reconstruct at the microscope scale.
For both of these channels that I mentioned, phase and amplitude.
The phase channel is extremely important, especially for transparent samples like jellyfish,
for example, thin samples that do not scatter light as much.
And is the reconstruction that you're targeting?
Is it a visual reconstruction that you might get if you had a traditional microscope
or is it something different that better captures the two types of information that you
have access to with holographic images?
It bears the advantages of holography in the sense that it can look at these two different
information channels that I mentioned, amplitude and phase, and at the same time over a larger
volume.
One advantage of holography is it can see objects at different depths without a physical
focusing mechanism.
You can time reverse the optical field and go to different depths within your sample.
And that gives an advantage to a holographic reconstruction, and it essentially gives
you microscopic features at different planes of your sample.
And that's what the network output gives you.
And we've shown that actually if you trade the neural net with different holograms acquired
at different sample to sensor distances, you can extend a depth of field.
In other words, the neural net can be taught to not only holographically reconstruct objects,
but autofocus them as well.
Which is being very interesting, imagine for example, as a volume of a sample, which contains
several different objects, cells scattered for example in a volume, you can actually
break them all into focus within the reconstruction.
And that's something that we recently shown as a holographic reconstruction doing both standard
hologram to image transformation that physics has been very powerful doing for decades.
But at the same time, merging it with autofocusing so that different parts of the sample come
into focus digitally at the output of the network.
And what do these networks tend to look like? Are they convolutional neural nets with
traditional architectures?
Or are you doing very domain specific things in the various layers of the network?
Well, these are powered by convolutional neural nets.
And we're taking essentially standard architectures, but of course fine tuning them without making
them unnecessarily complicated at the same time.
This is a microscopic imaging modality.
We're also fine tuning some of the parameters in this space to make it learn this transformation
across different spatial features so that it can actually reconstruct larger features at
the same time subcellular features, for example, if you're living into a cell.
And when you're building these networks that essentially map from this kind of raw data
from the imager to the result of something that's trained on the physics-based reconstructions,
is there anything about the networks that you've seen produced and their function that
suggests that the network is learning structure that's analogous to the rules of a physics
that a, you know, the govern the system or is it taking shortcuts that don't necessarily
relate to the physical representation?
Yeah, a fantastic question.
So it learns whatever you teach it to do and it rejects anything else.
Even though that something is physical, I'll give you an example.
We've taught a neural net to do reconstructions of holograms, but these holograms were
planar objects like we were interested in tissues used in pathology, right?
There are thin sections of tissue and our goal was to reconstruct those tissue sections
from their holograms.
This was a plane to plane transformation in a sense that the sample is planar, its hologram
is planar, and we were taking the raw holograms and transforming them back to how the sample
should look like in its phase and amplitude.
And the network was very successful learning kind of like, you know, without understanding
physics, learning the physical principles of this transformation.
However, it's rejecting anything else.
For example, in our experiments, we usually have some dust particles that are seated somewhere
in our optical path above or below the sample, but not on the sample plane.
These dust particles are unavoidable, especially, you know, if you're not having a vacuum type
of an environment.
So in a physical reconstruction, those dust particles appear as some sort of artifact
in the image.
And we understand it.
That's a physical solution.
That's a physical particle creating some sort of an interference pattern, superimpose
on the image of the sample, right?
It's not part of the sample.
It's somewhere else, but it's a physical particle.
The network rejects that, although it appears in the physical reconstruction.
The reason is, the network thinks it's an artifact of holography.
It doesn't seem like it's in focus.
It's an out of focus, you know, dust particle or artifact related to that.
And actually, it's rejecting those kinds of physical, but outside the solution domain
type of pixels.
And that's very powerful in the sense that you see in the physical reconstruction some
artifacts that must be there, but they're actually attacked by the network.
Similar things that you can mention for particles or objects at different depths.
Physics give you a certain solution, and the network sometimes violates that.
But if only it is part of what you've trained it for.
So you said that the physical reconstructions do contain these artifacts from the dust
particles, and these are your ground truth, how then is the network learning to reject
them?
Well, they're rare, right?
So that's the good thing.
The physical reconstruct that, the physics, I mean, the dust particles are obviously happening
rarely.
Like over a large field of view, maybe you have a few places, and by and large, everything
else is plain to plain.
That's why it generalizes to that transformation and attacks anything of that nature.
You're creating these new types of reconstruction methods.
How also you applying deep learning?
Well, I mean, a recent work that we've done on optics deep learning intersection is
actually creation of an optical network, optical neural net that's based on diffraction.
So it's not a traditional deep neural net in the sense of standard convolutional neural
nets with non-denarities like rectified linear units, et cetera, that you find in electronic
neural nets.
This is actually an analogous to that.
It's an optical analog of it where, imagine the input is an object where you shine light,
and behind the input, there is an optical construct.
It's a passive system, meaning you design this optical hardware using a computer, using
deep learning principles.
And once you optimize it, you fabricate it using a 3D printer, if your wavelength is
of interest to that, or using lithography.
You fabricate features to craft essentially a volume.
And that volume as light is penetrating from the input plane into the diffractive layers
is solving essentially a problem, let's say a classification problem.
And that volume is composed of different layers, where every layer is composed of several
pixels, thousands of pixels.
And these pixels are representing what we call as neurons.
The transmittance phase and amplitude of the transmittance of each neuron is a trainable
learnable parameter.
And at the end of the network, there's a light pattern.
And that light pattern is the output of your network that you're trying to solve a problem.
Let's look at, for example, a classification problem.
A simple one, let's look at classification of handwritten digits.
If I input to this network, zeros, like handwritten zero digits, the light that is transmitted
from that object is diffracting through these different layers that are all optimized to
guide the light to a certain detector at the output plane that is assigned to zero.
If I bring a new zero handwritten, it's, if it's correctly built, it's going to actually
channel most of the photons at the output to the correct detector, saying that it's all
optically in fearing that it's a zero.
Same idea applies for other classes.
And it's all optically, essentially, using diffraction, solving this problem that you
asked the network to solve.
This is really fascinating to think of that you can implement a deep neuron that all
in optics.
You mentioned that the pixels are analogous to neurons in a traditional deep neuron network
and that the parameters can be learned.
Are you learning those parameters offline, so to speak, meaning before you're printing
these, this network, or is there somehow, these networks are not, they're mutable,
posts printing?
Great question.
So yes, let's think first, an entirely passive system where it's fabricated and fixed.
Before that fabrication process, you use a computer to finalize the, through deep learning,
through backpropagation, error backpropagation, you finalize the transmittance values of
each one of these neurons.
And then you fabricate it, so it's essentially fixed, but you can also create a hybrid system
where some of these layers are composed of dynamic light modulators, like spatial light
modulators where you can actually change them and make them as part of a learning scheme
or kind of like mend the network's function.
But in the implementation that we had, experimentally, these were trained using a computer in
TensorFlow and then fabricated and then all optically tested with light as input passing
through some input.
One thing that's very important here is that this framework is, it has two branches to
it.
One, you can create something with this kind of layer-to-layer design using linear materials,
linear optical materials, meaning that they are essentially not including any non-linearities.
As a result, if you use no non-linearity in your diffractive layers, this becomes a linear
classifier, meaning that you can have all these different layers of free space propagation
of light and a diffractive layer, some more free space diffraction, some more layer coming
after it and repeating the same process.
All of this can be mathematically squeezed into a single matrix operation, but even using
these linear materials, the network has depth to it and that's where the diffractive
deep network, optical network analogy comes from.
The depth of the network is because of this.
You cannot take a single layer between the input and output planes and generalize to any
function that multiple diffractive layers collectively can produce.
In fact, we've shown that as you add more layers to this system, one after another, all
trainable, your network gets better in its classification and its power efficiency, its
output contrast, per detector, per class improves as the number of layers increases.
That's one aspect that I would like to emphasize.
The other aspect is, before you move on, just to make sure I understand this, are you saying
that adding additional layers inherently introduces non-linearity or that you're using non-linear
materials with non-linear reflective and refractive characteristics to introduce the non-linearity?
Certainly, we can use non-linear materials as part of the diffractive layers to introduce
non-linearity in the system and then it becomes a more sophisticated tool in terms of how
different kinds of functions it can generalize.
You can include non-linearities like non-linear materials, crystals, polymers, semiconductors,
as you fabricate these things as part of the network to introduce non-linearity.
What I was referring to is the fact that even though you don't have any non-linear material
in this system, there is depth to it in the sense that multiple layers perform or have
more degrees of freedom to perform a more general function better than a single diffractive
layer, and that's where, even with the linearity of the material, there is depth and deepness
of the network in its performance.
You've got this essentially a classifier that you've printed just to make this very concrete.
What type of scale are we talking about physically?
Great question.
We tested these experimentally using Terahertz wavelengths. The weigh length of which in
air was about 0.75 millimeters, so it's a big weight length, it's Terahertz.
That's why we were able to use luckily a standard 3D printer, which is transparent at
those weigh length.
We could put many layers, like five layers, one after another, forming a diffractive optical
network made out of, literally, plastic coming from a 3D printer.
The size of this network was on the order of 8 to 9 centimeters.
Obviously, a divisible weight length at shorter weight lengths.
What you're looking at is maybe half a centimeter by half a centimeter type of a network in terms
of width, which would be sufficient in terms of number of neurons, et cetera, that you
can fabricate there with lithography.
So 8 to 9 centimeters sounds very large relative to what I envisioned for this.
That was the weigh length is very large.
The weigh length, as I said, is about 0.7 to 0.8 millimeters.
That's almost about 1,800 microns.
If you go to visible weigh lengths, sub-micron weigh lengths, you're looking at a drastic reduction
in the size.
That's why a few millimeters by a few millimeters would be the size of this kind of a network working
in visible weigh length, let's say in green weigh length, like half a micron weigh length.
That's why everything will be scalable to the weigh length.
In this Terahertz weigh length that you fabricated, the 8 to 9 centimeters is that 8 to 9 centimeters
square for the plane that the light initially hits, and then is there a separate depth for
this or is it 8 to 9?
What dimension is that 8 to 9 millimeters referred to?
Great.
The 8 to 9 centimeters.
Right, right, sorry.
Is the width and the depth of this system is actually layered to layer spacing is only
3 centimeters.
Essentially, you're looking at maybe a cube of 8 to 8 centimeters by maybe about 15 centimeters
in total depth.
The spacing between the layers being 3 centimeters, we also had another network which was much
more compact with a 4 millimeter gap, but you should all convert these to weigh lengths
because that's what is more relevant to discuss because here we use a specific weigh length.
In terms of weigh length, we're looking at layer to layer spacing of less than 50 weigh
lengths, so if your weigh length is let's say a micron, you're talking about 50 micron
at the most layer to layer spacing, sub-micron.
In terms of the width, we're talking about maybe 100 weigh lengths in terms of width and
height of the network, so in that regard, these are really very compact networks that will
scale down with weigh length, obviously, in this example, we've used off the shelf 3D printers
and that's why Terahertz was used to show the fidelity of this thing, this thing with relatively
very inexpensive fabrication methods.
It costs us only a few dollars to print one of these things with 3D printers.
The target of he would be more expensive, but with economies of scale, obviously, the
fabrication of something like this using gloss and using photolithography would be no
different than fabricating, for example, your CMOS imager at the back of your cell phone.
In that regard, it will be also pennies at large scale fabrication.
In terms of visualizing what the layers might look like, should we or could we think of
them kind of as like Fresno lenses or maybe more digital patterns, like QR codes or
is there a way that you can articulate what these things might look like if we were looking
at a layer individually?
Each layer would look like speckle pattern to you.
It wouldn't mean much, but as you come toward the output layer, you will see emergence
of some patterns depending on where you put your detectors.
So essentially, it's a gradual shaping of statistical waveforms coming from different
objects that you've learned to classify.
So, it's very difficult to exactly understand how they work together because there are
multiple layers and your signal is essentially a stochastic signal in the sense that it
can come in different forms.
But what you can see from each layer is that there is a face pattern and it's slowly
emerging at the output to shape itself as if it's facing a bunch of detectors that each
of which is assigned to a class.
If you have 10 classes, then you're looking at 10 detectors at the output so that the
inferences is all optical except the final detection part per detector.
So obviously, you can have more detectors to be on the class.
In fact, in our recent work as a follow-up on this, we've shown the merge of these diffractive
optical layers forming the all optical network with electronic neural nets.
So formed kind of like a hybrid system where the front end was all optical and the backhand
was standard neural nets with the standard non-linearities, et cetera.
So that obviously has some very interesting and appealing features.
And one thing that we've shown is that the input pixels to the electronic neural net can
be compressed by the all optical network if they're both optimized at the same time.
The optical neural net and the electronic neural net, if they are optimized jointly, we
see some very interesting advantages.
But this is something that's unpublished.
We just put it into archives.
And we're seeing some very interesting hybrid systems that can emerge from the same diffractive
neural net concept.
And how is that physically implemented, this mixed mode system?
So this was an analysis, but in an experiment, imagine you take an optical electronic sensor,
let's say 100 pixel by 100 pixel or a few hundred pixel by a few hundred pixel type of design.
In front of it, as if you're putting a lens, you're going to be putting a bunch of diffractive
layers that have been optimized for a certain inference task.
Assuming that there is a very simple neural net that is running beyond the CMOS.
And that's the CMOS or the CCD, the low pixel count CCD or CMOS imager is kind of like
the layer between the optics and the electronic neural net.
And are you using it as an electronic or a dynamic detector?
Is that its role?
Or are you also implementing layers beyond that in the analysis that you did?
We've shown it works as a classifier, just like the all optical network, but a better
classifier.
And it also takes a very primitive neural net that the inference performance, not very deep,
not a lot of trainable parameters, very low power requirement type of a neural net.
You can take something like that and make it work very good.
So in a sense, it will be very useful, especially for low power and mobile systems that need
to frequently, with a very high frame rate, look at some scene or some data.
That's where you don't have the luxury of working with very sophisticated neural nets.
For power reasons, for frame rate reasons, et cetera.
That's where optics can help, because of its speed and its front end, making the rest
after the digitization step to be compact in terms of number of pixels, in terms of
frame rate being fast, and also the depth of the electronic neural net or the complexity
of the electronic neural net becoming kind of much less advanced compared to purely electronic
neural net?
Other experiments were based on an M-ness style database, handwritten digits.
Do you have a sense for the way the physical characteristics of the layers change with the
complexity of the underlying data set, the input data?
We've also tried another data set, which is going to be more complicated.
It's called the fashion M-ness.
We had 10 different classes of fashion products, like sneakers, bags, t-shirts, trousers, et cetera,
et cetera.
It's not as challenging as C-Far-10, for example, but it's more complicated than a fashion
M-ness.
We've also tried that and shown that another network trained for that data set 3D printed
and tested experimentally works nicely to match our predictions or numerical analysis.
In terms of complexity, hard for us to understand what changed from M-ness layers to the fashion
M-ness layers.
As I look at them in front of me, they look similar to me, but of course, all the details
are hidden there in terms of the face profiles of each layer, so it's very difficult to obviously
understand.
Sure.
But there wasn't anything obvious like you needed to go wider or deeper to achieve reasonable
performance.
No.
Of course, there's a huge parameter space that we need to optimize here, and maybe there's
more room to optimize to improve the inference performance for more complicated data sets.
It's still research ongoing in that regard, but what we've shown is that the same 5 layers
with the same number of learnable, trainable parameters worked.
Of course, lower accuracy compared to M-ness because fashion is more difficult.
But essentially, it's the same framework that nicely generalizes for another task.
But how well you can push it further?
It's still a research question.
In fact, in one of our recent work, we have improved.
For example, the paper that the science paper report something like about 92% for M-ness.
We've pushed it to now 98% without using any nonlinear optical materials with some changes
in the way that we optimize the neural net, optical neural net.
So essentially, we've already optimized and improved some of the parameters that we've
been using in designing this diffractive layer set to push it by a good margin from, as
I said, for M-ness, for example, from about 92% with 5 layers now.
We're approaching 98% with also 5 layers, same number of trainable parameters.
It's a lot of different things that you can do to improve the performance of something
like this.
One unique aspect of diffractive layers is it's passive, right?
I mean, once you've fabricated, it doesn't consume any power except the illumination
power and maybe the detectors that you have at the output plane.
Everything else in thin is just material.
It's just you fabricated and it stays.
It's good.
It doesn't consume any power.
But it's bad.
It's static.
If your data changes, you need to reprint it.
The one thing that we've shown is actually you can take a neural net with several layers
and peel off some layers and add new layers, trainable, with respect to the rest of the
static ones and kind of mend and improve the performance of the entire system.
So it's in that sense, it's like a Lego piece.
Kind of like fine tuning applied to a physical network.
Right.
Exactly.
Different layers, you take out some of them or you patch additional layers onto an existing
one.
That's one way of bringing some reconfigurability to the system.
Another way of bringing reconfigurability to the system is replacing some of these layers
with dynamic, electrooptic modulators.
So rather than printing and having the code as part of gloss, permanently, you have a
layer that you can actually change the pixels.
Specialized modulator type of systems.
Of course, you can create a hybrid system where some layers are static, some layers are
dynamic and create some sort of a trade-off between complexity and reconfigurability of
active layers.
This is really fascinating.
How do you see this playing out?
Do you see practical applications for this?
You know, some arbitrary time frame and what might those be or is this kind of a research
direction that's driving you on a broader path that you're not necessarily trying to see
this use practically?
First of all, we're enjoying playing with it.
It's a toy.
And it's making us happy.
So it sounds really fun.
I can definitely see that part of it.
Yeah, it's certainly a good toy that keeps you awake.
It's like a good puzzle, right?
I mean, so many, so many things that you can do with it and try different things.
And it's just at its infancy.
So it's our baby and we're trying to play with it and be happy.
But at the same time, there are so many interesting applications that we foresee.
I believe for defense and security, it has tremendous applications, especially at
longer wavelengths, infrared wavelengths, mid-inferred wavelengths, where some of these
thermal cameras or other types of focal plane arrays, I think those will benefit tremendously
from an optical front end because omega pixel at those wavelengths, omega pixel image
or at those wavelengths is very expensive and not every country has the no-have to fabricate
something like that.
So I think for defense, the applications are enormous.
Overall, I'm very excited about one direction that I've been constantly thinking and that
is to create low power and mobile hybrid systems that are powered at their front end with
some all optical machine learning front end at the backside and inexpensive CMOS or CCD
followed by a very modest low power neural net implementation.
It can go very low power this way, it can be extremely fast this way, frame rates can
be very high and at the same time we're talking about very modest form factor.
So that obviously has to me some very interesting security applications at different parts
of the electromagnetic spectrum.
And I can obviously, because of my background, think enormous amount of biomedical problems
that one can tackle with this kind of front end integrated with optoelectronic sensors
and electronic neural nets.
That's one area and the other area that I'm very excited about is actually nonlinear
materials, meta materials, plasmonics, exotic material systems.
As we discussed this in our paper that we published in science, the nonlinearity aspect
of material science, optical nonlinearity is in material science.
So I think opens up a huge platter of opportunities for enhancing the function of something like
this.
And that's where I think some of these metamaterial basic, exotic structures, plane by plane by
plane, following each other, would really generalize some very sophisticated functions,
maybe coming close to electronic neural nets with favorite, favorite choices of nonlinearities.
We'll see how it goes, but this is another area where I'm very excited about it.
Well, I don't want to thank so much for taking the time to walk us through what you're
up to.
This is really fascinating and it sounds very fun.
So your group must have a good time playing with the stuff.
Thank you.
Really?
Yeah.
We're enjoying ourselves.
I'm enjoying the time.
Yeah.
Thank you so much for having me.
All right, everyone, that's our show for today for more information on I'dawan or any of
the topics covered in this show, visit twomalai.com slash talk slash 237.
As always, thanks so much for listening and catch you next time.
