1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,760
I'm your host Sam Charrington.

4
00:00:23,760 --> 00:00:28,320
This week on the podcast, we're featuring a series of conversations from the Nips conference

5
00:00:28,320 --> 00:00:30,560
in Long Beach, California.

6
00:00:30,560 --> 00:00:34,520
This was my first time at Nips and I had a great time there.

7
00:00:34,520 --> 00:00:37,720
I attended a bunch of talks and of course learned a ton.

8
00:00:37,720 --> 00:00:43,920
I organized an impromptu round table on building AI products and I met a bunch of wonderful

9
00:00:43,920 --> 00:00:48,040
people including some former Twimble Talk guests.

10
00:00:48,040 --> 00:00:52,860
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

11
00:00:52,860 --> 00:00:59,540
take a second right now to subscribe to at twimblei.com slash newsletter.

12
00:00:59,540 --> 00:01:05,220
This week through the end of the year, we're running a special listener appreciation contest

13
00:01:05,220 --> 00:01:09,940
to celebrate hitting one million listens on the podcast and to thank you all for being

14
00:01:09,940 --> 00:01:11,900
so awesome.

15
00:01:11,900 --> 00:01:16,580
Tweet to us using the hashtag Twimble1Mill to enter.

16
00:01:16,580 --> 00:01:21,220
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

17
00:01:21,220 --> 00:01:23,220
other mystery prizes.

18
00:01:23,220 --> 00:01:30,020
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

19
00:01:30,020 --> 00:01:32,220
for the full rundown.

20
00:01:32,220 --> 00:01:36,820
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

21
00:01:36,820 --> 00:01:40,020
of this podcast and our Nips series.

22
00:01:40,020 --> 00:01:45,060
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

23
00:01:45,060 --> 00:01:50,780
sessions, their big news this time was the first public viewing of the Intel Nirvana

24
00:01:50,780 --> 00:01:54,580
neural network processor or NNP.

25
00:01:54,580 --> 00:01:59,540
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

26
00:01:59,540 --> 00:02:04,620
primitives while making the core hardware components as efficient as possible, giving

27
00:02:04,620 --> 00:02:10,100
neural network designers powerful tools for solving larger and more difficult problems

28
00:02:10,100 --> 00:02:14,740
while minimizing data movement and maximizing data reuse.

29
00:02:14,740 --> 00:02:20,580
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

30
00:02:20,580 --> 00:02:22,940
Nirvana.com.

31
00:02:22,940 --> 00:02:28,540
In this episode, I sit down with Timnit Gebrew, postdoctoral researcher at Microsoft Research

32
00:02:28,540 --> 00:02:34,540
in the Fairness Accountability Transparency and Ethics in AI or Fate group.

33
00:02:34,540 --> 00:02:38,580
I've been following Timnit's work for a while now and was really excited to get a chance

34
00:02:38,580 --> 00:02:40,820
to sit down with her at the conference.

35
00:02:40,820 --> 00:02:46,460
We packed a ton into this conversation, especially keying in on her recently released paper,

36
00:02:46,460 --> 00:02:50,780
including deep learning and Google Street View to estimate the demographic makeup of the

37
00:02:50,780 --> 00:02:52,580
US.

38
00:02:52,580 --> 00:02:56,580
Timnit describes the pipeline she developed for this research and some of the challenges

39
00:02:56,580 --> 00:03:02,020
she faced building an end-to-end model based on Google Street View images, census data,

40
00:03:02,020 --> 00:03:04,260
and commercial car vendor data.

41
00:03:04,260 --> 00:03:08,940
We also discussed the role of social awareness in her work, including an explanation of how

42
00:03:08,940 --> 00:03:14,540
domain adaptation and fairness are related and her view on the major research directions

43
00:03:14,540 --> 00:03:16,820
in the domain of fairness.

44
00:03:16,820 --> 00:03:21,940
Timnit is also one of the organizers behind the Black and AI group, which held a very interesting

45
00:03:21,940 --> 00:03:24,740
symposium and poster session at Nips.

46
00:03:24,740 --> 00:03:27,740
I'll link to the group's page in the show notes.

47
00:03:27,740 --> 00:03:38,580
This was a really interesting conversation and one that I'm sure you'll enjoy.

48
00:03:38,580 --> 00:03:40,260
Timnit, welcome to the podcast.

49
00:03:40,260 --> 00:03:41,260
Thank you.

50
00:03:41,260 --> 00:03:42,260
Thanks for having me.

51
00:03:42,260 --> 00:03:43,260
Absolutely.

52
00:03:43,260 --> 00:03:45,780
What is the fate group at Microsoft Research?

53
00:03:45,780 --> 00:03:52,100
Fates stands for Fairness Accountability Transparency and Ethics and AI and it's a very new group

54
00:03:52,100 --> 00:03:57,700
started by Kate Crawford and Hannah Wallach and there's some other people there like Jen

55
00:03:57,700 --> 00:04:03,380
Warman, Vaughan, and some economists and some computational social scientists, so it's

56
00:04:03,380 --> 00:04:08,100
a combination of machine learning people and social science and economics people.

57
00:04:08,100 --> 00:04:12,860
Try to study the societal implications of AI and just make sure that we create algorithms

58
00:04:12,860 --> 00:04:16,140
that are fair, so our research is focused towards that.

59
00:04:16,140 --> 00:04:17,140
Oh wow.

60
00:04:17,140 --> 00:04:22,100
And how did you get interested in fairness and AI in particular and artificial intelligence

61
00:04:22,100 --> 00:04:23,620
in general?

62
00:04:23,620 --> 00:04:27,820
My background is in computer vision and as I was working on, there's a number of things

63
00:04:27,820 --> 00:04:33,020
I've always been interested in social justice and towards the end of my PhD, I saw this

64
00:04:33,020 --> 00:04:41,500
purplica article about a software that was being used by judges to figure out a person's

65
00:04:41,500 --> 00:04:46,940
likelihood of committing a crime again and judges where this was one of the inputs that

66
00:04:46,940 --> 00:04:52,780
they used to figure out how many years they should sentence you to prison.

67
00:04:52,780 --> 00:04:55,140
This being some machine learning algorithm.

68
00:04:55,140 --> 00:04:58,820
That's being sold by North Point and I think it's the name of this startup and that was

69
00:04:58,820 --> 00:05:04,380
very terrifying for me knowing because I had the background to know what kind of biases

70
00:05:04,380 --> 00:05:08,540
we have in the criminal justice system already and what kind of biases we have and how much

71
00:05:08,540 --> 00:05:12,180
discrimination there is in the data, that would be trained for it.

72
00:05:12,180 --> 00:05:19,500
So that was one and then while I was working on my PhD, I kind of figured that my work,

73
00:05:19,500 --> 00:05:24,100
my own work, could be susceptible to this kind of bias as well because my whole work was

74
00:05:24,100 --> 00:05:28,540
trying to show that we can do data mining using images, so large scale computer vision

75
00:05:28,540 --> 00:05:33,680
plus data, you know, like most people use text and social networks and other kinds of

76
00:05:33,680 --> 00:05:38,340
textual data to do data mining and the whole point of my PhD was to show that we could

77
00:05:38,340 --> 00:05:44,540
gain useful societal information using images and so if the ground truth for that that you

78
00:05:44,540 --> 00:05:48,820
use to train is biased, you're going to have, you know, biased conclusions.

79
00:05:48,820 --> 00:05:54,260
So I thought that I should be very cognizant of what kinds of issues could exist with

80
00:05:54,260 --> 00:05:58,260
that type of work, given that my work lies in that type of work.

81
00:05:58,260 --> 00:05:59,260
Okay.

82
00:05:59,260 --> 00:06:02,060
And now you're relatively new at Microsoft, is that right?

83
00:06:02,060 --> 00:06:05,380
I am, yeah, I started in July, yeah, I'm very new.

84
00:06:05,380 --> 00:06:06,380
Awesome.

85
00:06:06,380 --> 00:06:08,380
And you just published your first paper with the group?

86
00:06:08,380 --> 00:06:09,380
Is that right?

87
00:06:09,380 --> 00:06:10,380
No, no, no.

88
00:06:10,380 --> 00:06:13,620
So the my paper that just came out in PNAS is actually from my PhD.

89
00:06:13,620 --> 00:06:14,620
Oh, really?

90
00:06:14,620 --> 00:06:16,620
So this is a project that took four years.

91
00:06:16,620 --> 00:06:17,620
Wow.

92
00:06:17,620 --> 00:06:21,700
So I think, you know, it's more, when I give talks about it, I think people understand

93
00:06:21,700 --> 00:06:25,180
the level, the amount of work it went into it, but it's harder to see a thing from just

94
00:06:25,180 --> 00:06:28,900
from that one paper, but yeah, like that paper took a very long time and it just got

95
00:06:28,900 --> 00:06:29,900
published.

96
00:06:29,900 --> 00:06:30,900
Wow.

97
00:06:30,900 --> 00:06:31,900
So tell us about that paper.

98
00:06:31,900 --> 00:06:36,980
The paper was using Google Street View images to predict demographic characteristics.

99
00:06:36,980 --> 00:06:42,660
So what we did was we detected and classified cars, all the cars and 15 million Google

100
00:06:42,660 --> 00:06:45,820
Street View images across 200 American cities.

101
00:06:45,820 --> 00:06:51,700
And then we were able to use the characteristics of the cars that we detected and classified,

102
00:06:51,700 --> 00:06:57,300
you know, in a particular zip code or precinct and associated that with certain demographic

103
00:06:57,300 --> 00:07:02,940
characteristics like income or political affiliation, like an earlier paper we had even we

104
00:07:02,940 --> 00:07:08,740
did, like we looked at income segregation levels and even like CO2 emission rates.

105
00:07:08,740 --> 00:07:13,780
So we then, you know, once we detected and classified the cars, we represented each

106
00:07:13,780 --> 00:07:18,700
geographic region, like basically for us it would be like a zip code or a precinct,

107
00:07:18,700 --> 00:07:20,860
like a, you know, a voting precinct.

108
00:07:20,860 --> 00:07:26,860
We represented by like the type, the features that of the cars that are in that zip code.

109
00:07:26,860 --> 00:07:31,340
So for example, the percentage of Hondas or the percentage of each make that you have,

110
00:07:31,340 --> 00:07:32,340
that's one feature, right?

111
00:07:32,340 --> 00:07:37,820
Like percentage of triodas or Hondas or like Nisans or whatever percentage of sedans,

112
00:07:37,820 --> 00:07:42,860
you know, other metadata like the average miles per gallon, like efficiency of that particular

113
00:07:42,860 --> 00:07:44,420
zip code, et cetera.

114
00:07:44,420 --> 00:07:49,540
So once we had that information, then we used round truth census data or other data depending

115
00:07:49,540 --> 00:07:53,660
on what we're trying to predict to train another model to go from the car features to

116
00:07:53,660 --> 00:07:55,820
like predicting demographic characteristics.

117
00:07:55,820 --> 00:07:56,820
Okay.

118
00:07:56,820 --> 00:07:57,820
That's the project.

119
00:07:57,820 --> 00:07:58,820
Interesting.

120
00:07:58,820 --> 00:07:59,820
Interesting.

121
00:07:59,820 --> 00:08:03,140
So you mentioned census data as part of your training data set.

122
00:08:03,140 --> 00:08:08,420
When I think of the kinds of things that you're trying to predict for, like the, you know,

123
00:08:08,420 --> 00:08:14,100
the wealth or income of a geographic area, that's already in the census data.

124
00:08:14,100 --> 00:08:17,140
So how would someone use this technique?

125
00:08:17,140 --> 00:08:21,340
Oh, so what we did is, so we had 200 cities in our data set, right?

126
00:08:21,340 --> 00:08:22,340
Okay.

127
00:08:22,340 --> 00:08:25,500
So we used a subset of our cities for training.

128
00:08:25,500 --> 00:08:26,500
Okay.

129
00:08:26,500 --> 00:08:32,220
So we assume we have census data for like, let's say, I think is it 13 percent of, you

130
00:08:32,220 --> 00:08:36,380
know, or we sweep it to see, like, how much training data we need, but like, we assume

131
00:08:36,380 --> 00:08:42,220
we have census data for a small, very small subset of the cities that we have.

132
00:08:42,220 --> 00:08:47,100
And then we train a model using, and then for the rest of the cities, we don't have census

133
00:08:47,100 --> 00:08:48,100
data.

134
00:08:48,100 --> 00:08:49,100
We only have images and cars.

135
00:08:49,100 --> 00:08:53,980
So I guess the way we, someone would use it is if you have census data for some cities

136
00:08:53,980 --> 00:08:59,980
and you want to try to see, like, you know, for other cities, what the data might be, you

137
00:08:59,980 --> 00:09:04,260
could train our model using the census data that you have, and then the cars that are

138
00:09:04,260 --> 00:09:05,780
detected in the other cities.

139
00:09:05,780 --> 00:09:06,780
Okay.

140
00:09:06,780 --> 00:09:07,780
Right?

141
00:09:07,780 --> 00:09:08,940
So that's how we used it.

142
00:09:08,940 --> 00:09:13,020
And, you know, we also did some experiments and trying to do this across time.

143
00:09:13,020 --> 00:09:17,580
So say you have data for past census data for New York.

144
00:09:17,580 --> 00:09:20,460
Like we had it using Google Street View time lapse data.

145
00:09:20,460 --> 00:09:23,380
So for New York, and you have a bunch of images as well.

146
00:09:23,380 --> 00:09:27,300
And you then try to predict what's going to happen in the future, or, you know, like before

147
00:09:27,300 --> 00:09:28,820
you have the census data.

148
00:09:28,820 --> 00:09:30,660
So we did some experiments like that as well.

149
00:09:30,660 --> 00:09:36,060
So, you know, for me, this is kind of like, I don't want people to, like, read too much

150
00:09:36,060 --> 00:09:41,100
into the cars, you know, think for me, it's a proof of concept, basically, like, a new

151
00:09:41,100 --> 00:09:47,300
tool that you can use to do this kind of analysis, like, demography or social science applications

152
00:09:47,300 --> 00:09:51,380
or work, you know, it's like a new tool that is available to researchers, and we want

153
00:09:51,380 --> 00:09:56,100
to show, if one were, you know, say you wanted to study the, I don't know, relationship

154
00:09:56,100 --> 00:09:59,900
between trees or tree species and people's health or something, you know, how would you

155
00:09:59,900 --> 00:10:02,300
go about using images to do that?

156
00:10:02,300 --> 00:10:06,420
Well, now that you saw our paper, you could, like, apply a very similar pipeline to it,

157
00:10:06,420 --> 00:10:07,420
you know?

158
00:10:07,420 --> 00:10:08,420
Got it.

159
00:10:08,420 --> 00:10:10,860
So that's more what I want to take away to be.

160
00:10:10,860 --> 00:10:15,540
It's not specifically about cars, you know, projecting income based on cars.

161
00:10:15,540 --> 00:10:20,580
It's more about, we've got all of this visual data from cameras and sensors and things

162
00:10:20,580 --> 00:10:21,580
like that.

163
00:10:21,580 --> 00:10:25,700
How can we use that as proxy for any other thing that we might want to do?

164
00:10:25,700 --> 00:10:30,020
And for us, like cars, you know, there's, you know, like, there are other things you

165
00:10:30,020 --> 00:10:34,500
could study where the only data you could probably have is probably only visual data,

166
00:10:34,500 --> 00:10:35,500
right?

167
00:10:35,500 --> 00:10:39,140
Like, so for cars, you can argue that that's not necessarily the case you can use DMV

168
00:10:39,140 --> 00:10:40,140
data, right?

169
00:10:40,140 --> 00:10:44,620
But I guess our street view gives you a different perspective, which is you're not looking

170
00:10:44,620 --> 00:10:47,540
at the cars of the people who necessarily live there.

171
00:10:47,540 --> 00:10:52,300
You're just saying, if I were to just walk around the street, you know, what does that street

172
00:10:52,300 --> 00:10:53,300
look like, right?

173
00:10:53,300 --> 00:10:54,500
Like, what kind of cars are driving?

174
00:10:54,500 --> 00:10:56,260
What kind of cars are parked?

175
00:10:56,260 --> 00:10:59,340
That's kind of, and then what does that tell me about the people who live there?

176
00:10:59,340 --> 00:11:00,340
Right.

177
00:11:00,340 --> 00:11:05,100
For me, I'm most excited about the tool, you know, and this pipeline, I'm not, you know,

178
00:11:05,100 --> 00:11:10,020
and I was very, very surprised that our thing actually worked, you know, because there's

179
00:11:10,020 --> 00:11:11,020
a lot of stuff.

180
00:11:11,020 --> 00:11:12,020
I was very surprised.

181
00:11:12,020 --> 00:11:16,860
There's a lot of stuff that could go wrong because the pipeline, there's many, many different

182
00:11:16,860 --> 00:11:17,860
components of it.

183
00:11:17,860 --> 00:11:19,700
Well, can you walk us through the pipeline?

184
00:11:19,700 --> 00:11:20,700
Yeah.

185
00:11:20,700 --> 00:11:25,260
So the biggest thing is a lot of people in AI don't talk about this, but it's data collection

186
00:11:25,260 --> 00:11:26,260
is huge.

187
00:11:26,260 --> 00:11:31,580
So if you want to do a sort of supervised machine learning, and you want to do it in the

188
00:11:31,580 --> 00:11:35,540
real world, right, we're not like talking about a toy data set here.

189
00:11:35,540 --> 00:11:37,820
So we were saying, what is our end goal?

190
00:11:37,820 --> 00:11:42,540
Our end goal is to detect and classify all the cars and 50 million Google's review images

191
00:11:42,540 --> 00:11:45,500
and then to predict, demograph, is using that, right?

192
00:11:45,500 --> 00:11:50,740
So to get to that end goal, we first have to figure out, oh, okay, like how are we going

193
00:11:50,740 --> 00:11:52,540
to, okay, how are we going to get data?

194
00:11:52,540 --> 00:11:54,140
How are we going to get label data?

195
00:11:54,140 --> 00:11:57,780
Like how are we going to label cars and Google's review images?

196
00:11:57,780 --> 00:11:58,780
This is very hard.

197
00:11:58,780 --> 00:12:02,660
What we've been getting the data, is that easily accessible via an API, or did you have

198
00:12:02,660 --> 00:12:03,660
to scrape them?

199
00:12:03,660 --> 00:12:04,660
Google images?

200
00:12:04,660 --> 00:12:05,660
Yeah, they have an API.

201
00:12:05,660 --> 00:12:06,660
Okay.

202
00:12:06,660 --> 00:12:07,660
And these are publicly available images.

203
00:12:07,660 --> 00:12:08,660
Okay.

204
00:12:08,660 --> 00:12:11,980
But so then, you know, we had to be like, okay, so what are all the different types of

205
00:12:11,980 --> 00:12:15,580
cars around, and how we might see Google's review images?

206
00:12:15,580 --> 00:12:16,900
Where do we get that list?

207
00:12:16,900 --> 00:12:17,900
Right.

208
00:12:17,900 --> 00:12:22,620
So we found Edmunds.com and they have all the cars since 1990, so it's about 15,000 types

209
00:12:22,620 --> 00:12:23,620
of cars.

210
00:12:23,620 --> 00:12:24,620
But guess what?

211
00:12:24,620 --> 00:12:30,620
A computer vision algorithm can only, you know, we can only really kind of classify cars

212
00:12:30,620 --> 00:12:33,100
based on what they look on the outside.

213
00:12:33,100 --> 00:12:37,900
And these, you know, a subset of these 15,000 cars look the same, because they don't, they

214
00:12:37,900 --> 00:12:41,820
don't change them, you know, from year to year or from trim to trim or whatever.

215
00:12:41,820 --> 00:12:45,780
So we had to figure out how to cluster the cars that look the same.

216
00:12:45,780 --> 00:12:49,380
So we had a paper on this, it was a CHI paper, it was more of an HCI work.

217
00:12:49,380 --> 00:12:55,740
How do we get our initial subset of classes, where we bucket like cars that look the same

218
00:12:55,740 --> 00:12:57,380
into one class?

219
00:12:57,380 --> 00:13:02,740
And then that, that, that process in and of itself took a few months, by the way, because

220
00:13:02,740 --> 00:13:06,660
you know, you try something, it doesn't work, try something else, you know, so we did

221
00:13:06,660 --> 00:13:07,660
that.

222
00:13:07,660 --> 00:13:08,660
And so this is unsupervised.

223
00:13:08,660 --> 00:13:11,100
You're just clustering the cars that you're seeing in the images.

224
00:13:11,100 --> 00:13:12,100
Yeah.

225
00:13:12,100 --> 00:13:13,100
So we show.

226
00:13:13,100 --> 00:13:18,540
And you could say it's, yeah, so, so what we do is we show, we use Amazon Mechanical Turk

227
00:13:18,540 --> 00:13:20,900
and we show people, it's a graph-based algorithm.

228
00:13:20,900 --> 00:13:23,780
Like we show people images, two images of cars.

229
00:13:23,780 --> 00:13:27,100
So we have example images of cars from Edmonds.com.

230
00:13:27,100 --> 00:13:28,100
Okay.

231
00:13:28,100 --> 00:13:30,660
And so we show people, we say, are these two cars the same or different?

232
00:13:30,660 --> 00:13:34,420
And like we have, we have one of them is from Street View and the others from Edmonds.

233
00:13:34,420 --> 00:13:35,940
No, no, this is all from Edmonds.

234
00:13:35,940 --> 00:13:41,020
So right now, we're not even, okay, we haven't even gotten to getting labeled data right

235
00:13:41,020 --> 00:13:42,020
now.

236
00:13:42,020 --> 00:13:43,020
Right.

237
00:13:43,020 --> 00:13:44,020
We're trying to define what our classes are.

238
00:13:44,020 --> 00:13:45,020
Okay.

239
00:13:45,020 --> 00:13:46,540
What does class one mean?

240
00:13:46,540 --> 00:13:53,340
Class one means 2,500, chord 2,600, chord 2,700, you know, all of them look the same.

241
00:13:53,340 --> 00:13:57,460
So we haven't even started getting data, we're just starting to define what our classes

242
00:13:57,460 --> 00:13:58,460
are.

243
00:13:58,460 --> 00:13:59,460
Okay.

244
00:13:59,460 --> 00:14:00,460
That already takes a lot of time.

245
00:14:00,460 --> 00:14:01,940
That's the first thing you got to do.

246
00:14:01,940 --> 00:14:02,940
Right.

247
00:14:02,940 --> 00:14:06,500
The second one, once you define what your classes are, then for each of those classes,

248
00:14:06,500 --> 00:14:10,980
you have to have labeled data to train your car detector, right?

249
00:14:10,980 --> 00:14:14,500
So that's where we need the experts for Google Street View images.

250
00:14:14,500 --> 00:14:19,820
We also scrape data from like e-commerce sites, like cars.com and Craigslist.com.

251
00:14:19,820 --> 00:14:22,660
But you know, this is why dopamine adaptation exists.

252
00:14:22,660 --> 00:14:29,220
If you just, you know, train a plane like CNN or some sort of supervised machine learning

253
00:14:29,220 --> 00:14:34,980
algorithm on things that look like, you know, cars from Craigslist and try to test it

254
00:14:34,980 --> 00:14:38,820
on cars, you know, try to like detect cars in Google Street View or classify cars in

255
00:14:38,820 --> 00:14:40,540
Google Street View, it's not going to work.

256
00:14:40,540 --> 00:14:42,820
The distribution looks very, very different.

257
00:14:42,820 --> 00:14:49,100
So then there is a whole like, then I had an ICCV paper where we did like, we had, it's

258
00:14:49,100 --> 00:14:50,900
a domain adaptation based paper.

259
00:14:50,900 --> 00:14:51,900
Okay.

260
00:14:51,900 --> 00:14:55,540
So I think actually this data set is a very good domain adaptation data set.

261
00:14:55,540 --> 00:14:59,460
And why don't we do like an in set here on domain adaptation?

262
00:14:59,460 --> 00:15:01,980
What's the 32nd overview of domain adaptation?

263
00:15:01,980 --> 00:15:05,740
So domain adaptation is a subset of what people call transfer learning.

264
00:15:05,740 --> 00:15:10,300
So domain adaptation is like, you have one task, one, one exact task.

265
00:15:10,300 --> 00:15:13,060
And we have something that we call a source domain and a target domain.

266
00:15:13,060 --> 00:15:19,100
So in our example, let's say the source domain is cars from e-commerce sites, Craigslist.com.

267
00:15:19,100 --> 00:15:22,100
And let's say the target domain is cars from like Google Street View images.

268
00:15:22,100 --> 00:15:23,100
Okay.

269
00:15:23,100 --> 00:15:27,860
And so what you try to do in domain adaptation is you assume that you have labeled data in

270
00:15:27,860 --> 00:15:29,340
the source domain.

271
00:15:29,340 --> 00:15:33,900
But in the target domain, you know, in unsupervised adaptation, you assume you have no labeled

272
00:15:33,900 --> 00:15:34,900
data.

273
00:15:34,900 --> 00:15:39,340
So in unsupervised, we would assume that we don't have any images in Google Street View

274
00:15:39,340 --> 00:15:42,300
that are labeled with the types of cars they contain.

275
00:15:42,300 --> 00:15:43,300
That's unsupervised.

276
00:15:43,300 --> 00:15:48,460
In fully supervised adaptation, we would assume that in Google Street View, we have labeled

277
00:15:48,460 --> 00:15:50,900
images for all classes.

278
00:15:50,900 --> 00:15:54,540
And then in semi-supervised, we would assume that in Google Street View, which is our target

279
00:15:54,540 --> 00:15:59,020
domain, we have labeled, you know, data for a subset of our classes.

280
00:15:59,020 --> 00:16:00,020
Okay.

281
00:16:00,020 --> 00:16:03,820
So the idea of domain adaptation is when you're training set and your tests that have different

282
00:16:03,820 --> 00:16:08,900
distributions, how can you best use them, you know, within these different domains,

283
00:16:08,900 --> 00:16:12,380
assuming, you know, making these different assumptions that we just talked about, how can

284
00:16:12,380 --> 00:16:16,740
you best use data in your social domain, I guess, in conjunction with data in your target

285
00:16:16,740 --> 00:16:20,220
domain to maximize your accuracy on the target domain.

286
00:16:20,220 --> 00:16:21,220
Okay.

287
00:16:21,220 --> 00:16:22,220
Right?

288
00:16:22,220 --> 00:16:25,340
So this is a very real, in the real world, this is usually the case because you're never

289
00:16:25,340 --> 00:16:30,580
going to have like camera statistics are different, you know, occlusion or whatever.

290
00:16:30,580 --> 00:16:36,060
So like if you have a training set from like Google images or, you know, you want search

291
00:16:36,060 --> 00:16:37,580
images or something.

292
00:16:37,580 --> 00:16:42,420
And then you're like, you know, you want to apply it, you know, your model to like some

293
00:16:42,420 --> 00:16:43,420
other thing.

294
00:16:43,420 --> 00:16:44,420
Yeah.

295
00:16:44,420 --> 00:16:45,420
That has a different statistic.

296
00:16:45,420 --> 00:16:47,820
You need to know about adaptation techniques, you know.

297
00:16:47,820 --> 00:16:51,980
And when I think about Google Street View images, all those images have, you know, from

298
00:16:51,980 --> 00:16:56,580
the perspective, like they have a very specific kind of look that's different from anything

299
00:16:56,580 --> 00:16:59,620
you'd ever see on Edmonds or any other part side.

300
00:16:59,620 --> 00:17:00,620
Oh, yeah.

301
00:17:00,620 --> 00:17:02,020
Because Edmonds, you know, I'm trying to sell you my car.

302
00:17:02,020 --> 00:17:04,540
So I want to give you the best perspective.

303
00:17:04,540 --> 00:17:07,260
Like you have a really nice resolution.

304
00:17:07,260 --> 00:17:09,180
It's in one car in the middle.

305
00:17:09,180 --> 00:17:12,540
There's no other car, like, occluding this car.

306
00:17:12,540 --> 00:17:17,140
There's no trees or like, you know, I don't know, like there's, it's very, very different.

307
00:17:17,140 --> 00:17:20,140
You don't, you know, there's cars in Google Street View where you only see like, it's

308
00:17:20,140 --> 00:17:21,140
a stage of damage.

309
00:17:21,140 --> 00:17:22,140
Yeah.

310
00:17:22,140 --> 00:17:23,140
Yeah.

311
00:17:23,140 --> 00:17:24,140
Yeah.

312
00:17:24,140 --> 00:17:25,140
Yeah.

313
00:17:25,140 --> 00:17:26,140
There's cars in Google Street View where you only see like a couple of lights or something

314
00:17:26,140 --> 00:17:27,140
like that.

315
00:17:27,140 --> 00:17:28,540
You have this side, you know, view.

316
00:17:28,540 --> 00:17:33,860
So, so that's where, you know, I'm very interested in the domain adaptation problem because

317
00:17:33,860 --> 00:17:34,860
of that.

318
00:17:34,860 --> 00:17:40,020
This project helped me decide what core machine learning and computer vision areas I'm very

319
00:17:40,020 --> 00:17:41,420
interested in.

320
00:17:41,420 --> 00:17:43,580
Because of this, it's domain adaptation.

321
00:17:43,580 --> 00:17:49,340
The second one is data collection, efficient data, like basically what some people call efficient

322
00:17:49,340 --> 00:17:51,780
machine learning, a data efficient machine learning.

323
00:17:51,780 --> 00:17:54,540
So I would say domain adaptation is part of it.

324
00:17:54,540 --> 00:17:56,380
How to efficiently collect data sets.

325
00:17:56,380 --> 00:17:59,900
So when I think of data efficient machine learning, I think of like one shot, few shot

326
00:17:59,900 --> 00:18:00,900
machine learning.

327
00:18:00,900 --> 00:18:01,900
I kind of have anything.

328
00:18:01,900 --> 00:18:04,700
I haven't done too much on a few shot, one shot learning, whatever.

329
00:18:04,700 --> 00:18:05,700
I don't know.

330
00:18:05,700 --> 00:18:09,420
The semantic sometimes kind of like confused me, but you know, but just any, it basically,

331
00:18:09,420 --> 00:18:14,420
so because data collection does not sound fun, I don't think AI people are, you know,

332
00:18:14,420 --> 00:18:18,540
in computer vision, we a lot of us work on it and there's even people doing like a

333
00:18:18,540 --> 00:18:23,820
hybrid computer vision at HCI kind of work, especially in our labs from the very beginning,

334
00:18:23,820 --> 00:18:27,900
like a lot of people were always working on data collection because we don't want to

335
00:18:27,900 --> 00:18:28,900
work on toy problems.

336
00:18:28,900 --> 00:18:31,860
Like, you can't, you can only do so much with MNIST, you know, you can only, you know,

337
00:18:31,860 --> 00:18:36,420
really gain, you can only gain so much insight, you know, to mean into like what kind of problems

338
00:18:36,420 --> 00:18:37,420
we should be solving.

339
00:18:37,420 --> 00:18:41,700
If you're always just using like, you know, readily available data so that you can get

340
00:18:41,700 --> 00:18:45,380
to the next like conference deadline or something like that, you know, I'm saying you should

341
00:18:45,380 --> 00:18:49,220
always spend so much time collecting data, but I'm saying you should do it at least a couple

342
00:18:49,220 --> 00:18:55,380
of times in your life, just to see, you know, where AI is right now if we were wanted

343
00:18:55,380 --> 00:18:57,700
to apply it to the real world, you know.

344
00:18:57,700 --> 00:19:02,140
So this project really, really like, I would say that I mean, I was complaining about it

345
00:19:02,140 --> 00:19:05,020
the whole time I was working on it and that it's over.

346
00:19:05,020 --> 00:19:09,500
It really cemented like what I think is really important to do, to work on.

347
00:19:09,500 --> 00:19:14,420
And actually the issue of bias also came up in this project because in an earlier paper,

348
00:19:14,420 --> 00:19:17,340
you know, we also, we were just trying to see like, okay, we can predict this, we can

349
00:19:17,340 --> 00:19:18,340
predict that.

350
00:19:18,340 --> 00:19:20,980
And one of the things we looked into was crime, crime rates, right?

351
00:19:20,980 --> 00:19:24,180
So, so, and then with crime rates, as you know, the ground truth, whatever ground truth

352
00:19:24,180 --> 00:19:28,180
we were going to have is bias, because all we know is who got arrested for the crime,

353
00:19:28,180 --> 00:19:29,660
who's crime got reported.

354
00:19:29,660 --> 00:19:34,380
So if I say, hey, look, with images, we can, you know, with cars, we can do this, then

355
00:19:34,380 --> 00:19:35,660
that's already bias, right?

356
00:19:35,660 --> 00:19:40,020
So, so basically like, if I'm going to do this type of work, if I'm going to continue

357
00:19:40,020 --> 00:19:44,660
to do this type of work, I have to, I have to also be working on the bit bias and fairness

358
00:19:44,660 --> 00:19:46,460
and other types of issues.

359
00:19:46,460 --> 00:19:50,060
And what's really interesting is that domain adaptation and this whole fairness thing

360
00:19:50,060 --> 00:19:53,740
are very, very related, actually, I just noted.

361
00:19:53,740 --> 00:19:57,540
So like, so some of the techniques that you can even use, that people, some people have

362
00:19:57,540 --> 00:19:59,220
even already used are very related.

363
00:19:59,220 --> 00:20:06,100
So one of the ways in which people do domain adaptation is to say, you know, say, you

364
00:20:06,100 --> 00:20:11,420
know, I want a classifier, let's see your classifier has a primary task, which is to classify

365
00:20:11,420 --> 00:20:14,820
something, maybe the type of card my image, right?

366
00:20:14,820 --> 00:20:22,020
And so I want that classifier to do well, regardless of what domain my image comes from,

367
00:20:22,020 --> 00:20:25,860
whether or not it's Craigslist or, you know, whether it's Craigslist or Google

368
00:20:25,860 --> 00:20:27,060
Street View, right?

369
00:20:27,060 --> 00:20:31,220
So the way some people do this and there's many variations of this is they have another

370
00:20:31,220 --> 00:20:37,180
classifier and the other classifier all it does is it uses the features that its input

371
00:20:37,180 --> 00:20:39,780
is the features learned by the first classifier.

372
00:20:39,780 --> 00:20:45,060
And so the input is those features and the output is, you know, which domain the image

373
00:20:45,060 --> 00:20:48,180
came from Craigslist or Google Street View, right?

374
00:20:48,180 --> 00:20:53,260
So then the first classifier in its job is in addition to accurately classifying the

375
00:20:53,260 --> 00:20:58,460
car, its job is to also confuse the second classifier because if the second classifier

376
00:20:58,460 --> 00:21:04,620
can't tell based on the features learned by the first classifier, which domain the image

377
00:21:04,620 --> 00:21:08,140
came from, then it means you've learned features that are sort of domain invariant,

378
00:21:08,140 --> 00:21:09,140
correct?

379
00:21:09,140 --> 00:21:12,020
Now can you see how you might apply this to fairness?

380
00:21:12,020 --> 00:21:17,980
So say that you want to classify something like, you know, your risk score or something

381
00:21:17,980 --> 00:21:18,980
like this.

382
00:21:18,980 --> 00:21:22,980
You want to say, if my other classifier can identify some class that I don't want to

383
00:21:22,980 --> 00:21:23,980
identify it.

384
00:21:23,980 --> 00:21:24,980
Yeah.

385
00:21:24,980 --> 00:21:28,220
So say you want this to be like, you know, invariant to like your race or something.

386
00:21:28,220 --> 00:21:32,580
So then you can have another classifier that classifies the race of the person.

387
00:21:32,580 --> 00:21:34,780
And then you, this first one confuses that.

388
00:21:34,780 --> 00:21:38,780
You know, now, now, like, there's been work already that does this and, you know, it's

389
00:21:38,780 --> 00:21:43,660
not like a done deal, it's not solved because then there's different fairness criteria

390
00:21:43,660 --> 00:21:47,220
and then like, which criteria do you use, et cetera, et cetera, but but I do think that

391
00:21:47,220 --> 00:21:51,660
these two things, like, you know, these two fields are kind of very related.

392
00:21:51,660 --> 00:21:52,660
Yeah.

393
00:21:52,660 --> 00:21:57,020
And it's so weird to me because I didn't think about that when I got interested in both

394
00:21:57,020 --> 00:21:58,020
of them.

395
00:21:58,020 --> 00:21:59,020
I didn't think about that at all.

396
00:21:59,020 --> 00:22:01,980
And now I'm like, oh, wait a minute, you know, interesting.

397
00:22:01,980 --> 00:22:02,980
Yeah.

398
00:22:02,980 --> 00:22:07,300
Now this, this last thing you were describing, it sounds like it, like the kind of thing

399
00:22:07,300 --> 00:22:09,260
we see in adversarial networks is it?

400
00:22:09,260 --> 00:22:10,260
Yeah.

401
00:22:10,260 --> 00:22:11,260
Yeah, exactly.

402
00:22:11,260 --> 00:22:13,420
I mean, you people have done it with adversarial networks.

403
00:22:13,420 --> 00:22:16,140
So you can implement it with adversarial networks.

404
00:22:16,140 --> 00:22:18,300
You don't have to implement with adversarial networks.

405
00:22:18,300 --> 00:22:22,500
There's other works that use like a different loss function, like, there's work called,

406
00:22:22,500 --> 00:22:25,300
there's the gradient reversal work from a white way back.

407
00:22:25,300 --> 00:22:27,180
I was like 2014ers and like that.

408
00:22:27,180 --> 00:22:29,260
There's also domain confusion loss.

409
00:22:29,260 --> 00:22:35,420
So Judy, who is, well, was opposed to that in our lab, and I had a paper at ICCV where

410
00:22:35,420 --> 00:22:41,780
I used her loss from like a different, a prior paper of hers, which was not adversarial.

411
00:22:41,780 --> 00:22:45,980
But it's very, again, like, it's funny that people independently were working on this

412
00:22:45,980 --> 00:22:51,060
idea by the time Ian came up with his adversarial networks thing, with his GAN thing.

413
00:22:51,060 --> 00:22:54,220
And then once he came up with a GAN thing, they're like, oh, wait a minute.

414
00:22:54,220 --> 00:22:56,260
We could just use GANs for this.

415
00:22:56,260 --> 00:23:00,060
But that's another thing I find interesting is that like this, this idea was kind of

416
00:23:00,060 --> 00:23:03,100
concurrently being thought of by other people.

417
00:23:03,100 --> 00:23:04,100
Interesting.

418
00:23:04,100 --> 00:23:08,340
And so how do you take this forward in your new role at Microsoft?

419
00:23:08,340 --> 00:23:12,420
Oh, so at Microsoft, I'm working a whole bunch of stuff right now.

420
00:23:12,420 --> 00:23:20,220
So Joy Bulwamini from MIT is here, by the way, and she and I have a paper that is hopefully

421
00:23:20,220 --> 00:23:24,980
going to come out at this new fairness conference for this accountability, transparency, and ethics

422
00:23:24,980 --> 00:23:25,980
in AI conference.

423
00:23:25,980 --> 00:23:30,540
It's a whole conference in February, in February, it's in New York, yeah.

424
00:23:30,540 --> 00:23:32,860
So I'm part of this engineering community there.

425
00:23:32,860 --> 00:23:36,660
So we have this paper that's basically doing algorithmic audits.

426
00:23:36,660 --> 00:23:40,100
It's going to come out soon so you can read about it when it comes out.

427
00:23:40,100 --> 00:23:44,700
And then I'm also working on this idea of standardizing.

428
00:23:44,700 --> 00:23:49,540
So you basically want to standardize what kind of information you should put out with

429
00:23:49,540 --> 00:23:54,340
your data sets or pre-trained models or whatever.

430
00:23:54,340 --> 00:23:56,740
So I've been telling everybody about this.

431
00:23:56,740 --> 00:24:02,140
So I used to be a hardware engineer and hardware, we have a data sheet that comes with every

432
00:24:02,140 --> 00:24:03,140
component.

433
00:24:03,140 --> 00:24:08,380
And when you're a circuit designer, you would be very intimately familiar with the data

434
00:24:08,380 --> 00:24:09,380
sheet.

435
00:24:09,380 --> 00:24:14,100
And the designers and all this stuff, before you design something into your model, right?

436
00:24:14,100 --> 00:24:15,100
So that process.

437
00:24:15,100 --> 00:24:16,100
We'll pull down the data set.

438
00:24:16,100 --> 00:24:17,100
We're not doing that.

439
00:24:17,100 --> 00:24:19,380
We're putting in our trainer model on it without thinking about it.

440
00:24:19,380 --> 00:24:24,580
You have an API that someone releases an API that you have to pay for.

441
00:24:24,580 --> 00:24:30,220
Really, you have no information about how you really have zero information about how

442
00:24:30,220 --> 00:24:35,860
are you supposed to, is it supposed to work on this new data set that you're using?

443
00:24:35,860 --> 00:24:37,820
Are there recommended applications?

444
00:24:37,820 --> 00:24:40,500
What's going to happen if you use it the wrong way?

445
00:24:40,500 --> 00:24:42,460
And it's very dangerous.

446
00:24:42,460 --> 00:24:47,580
Because in hardware, at least, I think the reason things were very, it's a mature field,

447
00:24:47,580 --> 00:24:53,340
but things that were standardized because the failure mode is so, it's visible to most

448
00:24:53,340 --> 00:24:54,860
people to everyone, right?

449
00:24:54,860 --> 00:24:59,580
Like, your battery catches fire or something like that, whereas here could be visible to

450
00:24:59,580 --> 00:25:04,420
some people, you know, and might not be like, if face recognition doesn't work for you

451
00:25:04,420 --> 00:25:08,140
because you're black or something like that, it'll be visible, very visible to you.

452
00:25:08,140 --> 00:25:11,340
It won't be visible for other people because they didn't test it out or something like

453
00:25:11,340 --> 00:25:12,340
this.

454
00:25:12,340 --> 00:25:13,340
Right.

455
00:25:13,340 --> 00:25:16,420
Okay, so it's, you know, because probability is involved, it's, you know, might not even

456
00:25:16,420 --> 00:25:19,500
be, you know, formally visible in the class that's affected.

457
00:25:19,500 --> 00:25:20,980
Yeah, exactly.

458
00:25:20,980 --> 00:25:26,980
And so, and, you know, like, we, a lot of people aren't, we're even doing these tests.

459
00:25:26,980 --> 00:25:31,980
So like, the first thing we have to start doing is like doing these audits, audits, you

460
00:25:31,980 --> 00:25:32,980
know?

461
00:25:32,980 --> 00:25:37,700
So stuff like this that I'm working on, I'm also working on, you know, just like, I was

462
00:25:37,700 --> 00:25:42,500
telling you, you know, models that, that are fairer, you know, what is the, the fairness

463
00:25:42,500 --> 00:25:43,500
criterion.

464
00:25:43,500 --> 00:25:45,620
And very, you know, I've learned a lot about this field.

465
00:25:45,620 --> 00:25:48,940
So I, I'm putting you to this field and now I've like gotten embedded in the community

466
00:25:48,940 --> 00:25:50,460
which is nice.

467
00:25:50,460 --> 00:25:52,140
What I mean is to the fairness community.

468
00:25:52,140 --> 00:25:56,860
So yeah, like, so kind of working from the, from the purely machine learning perspective,

469
00:25:56,860 --> 00:26:01,580
you know, like, how can we have account for fairness criterion in our models?

470
00:26:01,580 --> 00:26:05,260
It's very broad, but like, that's one of the things I'm working on, in addition to just

471
00:26:05,260 --> 00:26:10,660
like my regular computer vision kind of work, like domain adaptation blah, blah.

472
00:26:10,660 --> 00:26:14,860
So yeah, so that's how I'm, I'm taking this to my new role at MSR.

473
00:26:14,860 --> 00:26:15,860
Okay.

474
00:26:15,860 --> 00:26:16,860
Interesting.

475
00:26:16,860 --> 00:26:17,860
I'm really interested in this conference.

476
00:26:17,860 --> 00:26:22,460
I'll need to get some info from you about it and check it out.

477
00:26:22,460 --> 00:26:28,100
What are some of the other kind of major research directions in the domain of fairness?

478
00:26:28,100 --> 00:26:32,260
So I think that uncovering bias is one.

479
00:26:32,260 --> 00:26:33,260
Okay.

480
00:26:33,260 --> 00:26:37,900
So Joy and I just did this paper that's coming out where like we were auditing, you

481
00:26:37,900 --> 00:26:43,420
know, commercial gender classification APIs that are sold by people that you have to

482
00:26:43,420 --> 00:26:48,460
pay for and looking at disparities among certain groups by skin color, by gender.

483
00:26:48,460 --> 00:26:51,300
And then, you know, Meg Mitchell just came out with this paper.

484
00:26:51,300 --> 00:26:54,300
So I'm just talking about computer vision, because actually in computer vision, it's

485
00:26:54,300 --> 00:26:55,300
very new.

486
00:26:55,300 --> 00:26:58,580
People have been doing this stuff for like, you know, there is this, I don't know, have

487
00:26:58,580 --> 00:26:59,580
you heard of this?

488
00:26:59,580 --> 00:27:04,580
Man is to programmer as woman is to a filmmaker, that's the kind of like, I've seen a few

489
00:27:04,580 --> 00:27:05,580
various.

490
00:27:05,580 --> 00:27:06,580
Yeah.

491
00:27:06,580 --> 00:27:09,620
So, you know, and so I think in computer vision, that's one of the reasons I wanted to get

492
00:27:09,620 --> 00:27:10,620
into it.

493
00:27:10,620 --> 00:27:12,420
I felt like computer vision people weren't thinking about this.

494
00:27:12,420 --> 00:27:13,420
Where's NLP?

495
00:27:13,420 --> 00:27:14,420
It's been a little bit worse.

496
00:27:14,420 --> 00:27:19,100
And I think the first time Meg is an NLP, even Hannah does some NLP and like, I mean, does

497
00:27:19,100 --> 00:27:20,100
NLP.

498
00:27:20,100 --> 00:27:25,060
And like, even, and also theory people, I feel like in terms of technical people,

499
00:27:25,060 --> 00:27:29,020
theory people were starting to think about it, privacy people, like, send you to work

500
00:27:29,020 --> 00:27:34,020
and other, you know, like, and also from the ethics side, but I felt like, okay, now

501
00:27:34,020 --> 00:27:35,860
deep learning people are talking about it too.

502
00:27:35,860 --> 00:27:40,260
And there's like papers, things like this, but like last year, I felt like, yeah, people

503
00:27:40,260 --> 00:27:44,940
were starting to talk about it, but it wasn't a real, you know, concern.

504
00:27:44,940 --> 00:27:47,980
So now, like, in computer vision, even, we're starting to see some of these work.

505
00:27:47,980 --> 00:27:52,460
So one is uncovering bias, like, you know, what kind of bias exists.

506
00:27:52,460 --> 00:27:55,700
And the second one is how do you mitigate it if you uncover it?

507
00:27:55,700 --> 00:27:57,100
So, you know, there's a lot of work.

508
00:27:57,100 --> 00:28:01,820
So with, I guess the work I just talked about, the word-to-vec work, you know, the first

509
00:28:01,820 --> 00:28:06,420
uncover that bias, and then they tell you some strategies of mitigating that particular

510
00:28:06,420 --> 00:28:07,420
bias.

511
00:28:07,420 --> 00:28:11,340
And then the third one, which I'm very interested in is, I'm interested in all of them,

512
00:28:11,340 --> 00:28:12,340
right?

513
00:28:12,340 --> 00:28:15,980
But the third one is also just like understanding how these things are being used.

514
00:28:15,980 --> 00:28:20,700
So if you have, and how to standardize them, how to have transparency, like, if you have

515
00:28:20,700 --> 00:28:26,460
law enforcement that's using inaccurate face recognition algorithms, where are they

516
00:28:26,460 --> 00:28:27,460
using it?

517
00:28:27,460 --> 00:28:28,460
How are they using it?

518
00:28:28,460 --> 00:28:30,500
Well, you know, we have no idea.

519
00:28:30,500 --> 00:28:35,020
And also just like, you know, there's people who are using your social network data to,

520
00:28:35,020 --> 00:28:39,380
like, then selling it to other people or trying to figure out, like, your credit ratings

521
00:28:39,380 --> 00:28:40,740
and things like their startups, right?

522
00:28:40,740 --> 00:28:42,740
Kathy Neal talks about it in her book.

523
00:28:42,740 --> 00:28:43,740
Have you read this book?

524
00:28:43,740 --> 00:28:44,740
Weapons of Math Destruction.

525
00:28:44,740 --> 00:28:45,740
Okay.

526
00:28:45,740 --> 00:28:46,740
Yeah.

527
00:28:46,740 --> 00:28:47,740
That's hard to get.

528
00:28:47,740 --> 00:28:48,740
And it's on my list.

529
00:28:48,740 --> 00:28:53,980
I mean, and so that's a very important part of the issue actually, because as AI researchers,

530
00:28:53,980 --> 00:28:58,580
a lot of times, I mean, me included, okay, like, I just want to sit in a corner, read my

531
00:28:58,580 --> 00:29:03,100
papers, you know, and I honestly write some code.

532
00:29:03,100 --> 00:29:08,460
That's what I love doing most, still, you know, even though I do this whole, like, social

533
00:29:08,460 --> 00:29:15,620
activism stuff, like, what I enjoy doing is just, just, like, reading papers, thinking

534
00:29:15,620 --> 00:29:17,940
about ideas, writing code, right?

535
00:29:17,940 --> 00:29:23,340
But we have to understand, like, what are the implications of our work are, you know?

536
00:29:23,340 --> 00:29:29,100
And so, like, keeping track of, I just signed this extreme vetting letter against this extreme

537
00:29:29,100 --> 00:29:30,100
vetting initiative.

538
00:29:30,100 --> 00:29:32,260
I don't know if you've heard about it by the DHS.

539
00:29:32,260 --> 00:29:33,260
Yeah.

540
00:29:33,260 --> 00:29:38,660
That was trying to, I didn't even know about it until I was, yeah, until I was, I had no

541
00:29:38,660 --> 00:29:40,860
idea this was going on and it's terrifying.

542
00:29:40,860 --> 00:29:41,860
Yeah.

543
00:29:41,860 --> 00:29:45,420
You know, like, so this kind of stuff, we have to keep track of and we have to make our

544
00:29:45,420 --> 00:29:50,500
voices heard in addition to, you know, working on uncovering bias and mitigating bias.

545
00:29:50,500 --> 00:29:51,500
Right.

546
00:29:51,500 --> 00:29:52,500
Awesome.

547
00:29:52,500 --> 00:29:55,340
Well, I know you've got to run off to a talk.

548
00:29:55,340 --> 00:29:56,340
Yes.

549
00:29:56,340 --> 00:30:00,260
So, let's wrap it up here, but if you have any final words or thoughts or places that

550
00:30:00,260 --> 00:30:04,540
folks should be, you know, looking to keep up with all this information or finding you,

551
00:30:04,540 --> 00:30:05,540
yeah.

552
00:30:05,540 --> 00:30:06,540
Now's the time.

553
00:30:06,540 --> 00:30:10,940
Well, I recently got on Twitter, I was told it's a good thing.

554
00:30:10,940 --> 00:30:11,940
Wow.

555
00:30:11,940 --> 00:30:15,580
Well, yeah, I'm Tim Nick, who's my, my Twitter, and I'm going to have a website.

556
00:30:15,580 --> 00:30:21,180
So, I will be releasing data soon for this monstrous work that I hate that we just discussed

557
00:30:21,180 --> 00:30:26,060
with PNAS paper and to look out for my new, yeah, paper with joy as well.

558
00:30:26,060 --> 00:30:27,060
Okay.

559
00:30:27,060 --> 00:30:28,060
And we're just going to be releasing a couple weeks.

560
00:30:28,060 --> 00:30:29,060
All right.

561
00:30:29,060 --> 00:30:32,620
Well, we'll link you on Twitter and the show notes as well as to your homepage and looking

562
00:30:32,620 --> 00:30:35,940
forward to following this line of research and the stuff you do at Microsoft.

563
00:30:35,940 --> 00:30:36,940
Thank you.

564
00:30:36,940 --> 00:30:44,340
All right, everyone.

565
00:30:44,340 --> 00:30:46,500
That's our show for today.

566
00:30:46,500 --> 00:30:51,660
Thanks so much for listening and for your continued feedback and support.

567
00:30:51,660 --> 00:30:56,780
For more information on Timnett or any of the topics covered in this episode, head on

568
00:30:56,780 --> 00:31:01,100
over to twimlai.com slash talk slash 88.

569
00:31:01,100 --> 00:31:08,020
To follow along with the nip series, visit twimlai.com slash nips 2017.

570
00:31:08,020 --> 00:31:14,660
To enter our Twimlai one mill contest, visit twimlai.com slash twimlai one mill.

571
00:31:14,660 --> 00:31:20,420
Of course, we'd be delighted to hear from you either via a comment on the show notes page

572
00:31:20,420 --> 00:31:25,620
or via a tweet to at twimlai or at Sam Charrington.

573
00:31:25,620 --> 00:31:30,900
Thanks once again to Intel Nirvana for their sponsorship of this series to learn more about

574
00:31:30,900 --> 00:31:37,040
the Intel Nirvana NNP and the other things Intel's been up to in the AI arena, visit intel

575
00:31:37,040 --> 00:31:39,100
Nirvana.com.

576
00:31:39,100 --> 00:31:43,860
As I mentioned a few weeks back, this will be our final series of shows for the year.

577
00:31:43,860 --> 00:31:49,100
So take your time and take it all in and get caught up on any of the old pods you've been

578
00:31:49,100 --> 00:31:50,700
saving up.

579
00:31:50,700 --> 00:31:53,100
Happy holidays and happy new year.

580
00:31:53,100 --> 00:31:55,380
See you in 2018.

581
00:31:55,380 --> 00:32:01,540
And of course, thanks once again for listening and catch you next time.

