WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.600
I'm your host Sam Charrington.

00:23.600 --> 00:28.160
This week on the podcast, we're featuring a series of conversations from the Nips conference

00:28.160 --> 00:30.400
in Long Beach, California.

00:30.400 --> 00:34.360
This was my first time at Nips and I had a great time there.

00:34.360 --> 00:37.600
I attended a bunch of talks and of course learned a ton.

00:37.600 --> 00:43.760
I organized an impromptu roundtable on building AI products and I met a bunch of wonderful

00:43.760 --> 00:47.920
people, including some former Twimble Talk guests.

00:47.920 --> 00:52.720
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

00:52.720 --> 00:59.400
take a second right now to subscribe to at twimblei.com slash newsletter.

00:59.400 --> 01:05.080
This week, through the end of the year, we're running a special listener appreciation contest

01:05.080 --> 01:09.800
to celebrate hitting 1 million listens on the podcast and to thank you all for being

01:09.800 --> 01:11.760
so awesome.

01:11.760 --> 01:16.440
Tweet to us using the hashtag Twimble1Mill to enter.

01:16.440 --> 01:21.080
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

01:21.080 --> 01:23.080
other mystery prizes.

01:23.080 --> 01:29.880
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

01:29.880 --> 01:32.320
for the full rundown.

01:32.320 --> 01:36.680
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

01:36.680 --> 01:39.920
of this podcast and our Nips series.

01:39.920 --> 01:44.920
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

01:44.920 --> 01:50.640
sessions, their big news this time was the first public viewing of the Intel Nirvana

01:50.640 --> 01:54.440
neural network processor or NNP.

01:54.440 --> 01:59.360
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

01:59.360 --> 02:04.480
primitives while making the core hardware components as efficient as possible, giving

02:04.480 --> 02:09.960
neural network designers powerful tools for solving larger and more difficult problems

02:09.960 --> 02:14.600
while minimizing data movement and maximizing data reuse.

02:14.600 --> 02:20.440
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

02:20.440 --> 02:22.720
Nirvana.com.

02:22.720 --> 02:28.440
This time around, I'm joined by Matthew Crosby, a researcher at Imperial College London,

02:28.440 --> 02:31.880
working on the kinds of intelligence project.

02:31.880 --> 02:36.120
Matthew joined me after the Nips symposium of the same name, an event that brought researchers

02:36.120 --> 02:41.880
from a variety of disciplines together towards three aims, a broader perspective on the possible

02:41.880 --> 02:47.480
types of intelligence beyond human intelligence, better measurements of intelligence, and

02:47.480 --> 02:54.000
more purposeful analysis of where progress should be made in AI to best benefit society.

02:54.000 --> 02:59.280
Matthew's research explores intelligence from a philosophical perspective, exploring ideas

02:59.280 --> 03:04.760
like predictive processing and controlled hallucination and how these theories of intelligence

03:04.760 --> 03:08.760
impact the way we approach creating artificial intelligence.

03:08.760 --> 03:13.200
This was a very interesting conversation and one that I'm sure you'll get a kick out

03:13.200 --> 03:14.200
of.

03:14.200 --> 03:23.320
And now on to the show.

03:23.320 --> 03:27.400
Alright everyone, I am here in Long Beach, California at the Nips Conference, and I have

03:27.400 --> 03:33.160
the pleasure of being joined by Matthew Crosby, research associate at Imperial College London.

03:33.160 --> 03:37.160
Matthew, welcome to the podcast, thank you very much.

03:37.160 --> 03:39.880
So why don't we get started by having you tell us a little bit about your background

03:39.880 --> 03:42.840
and how you got involved in AI.

03:42.840 --> 03:49.960
So I was always very, very interested in intelligence as a concept, and I had a fairly mathematical

03:49.960 --> 03:55.920
sort of upbringing and start, and so my first thought was, I'm going to explore intelligence

03:55.920 --> 04:00.680
through mathematics and computer science, and so I worked for a while in high level planning

04:00.680 --> 04:08.040
in AI, and over the course of the time working on that stuff, I sort of got a bit disillusioned

04:08.040 --> 04:12.320
with the fact that AI was less about intelligence than I was hoping it would be.

04:12.320 --> 04:17.560
And of course for me, intelligence always spoke in some sense to human-like intelligence,

04:17.560 --> 04:22.120
to this disability to sort of experience the world and plan in the world, that's why

04:22.120 --> 04:27.280
I was working in planning, but to form ideas about what you're going to do and represent

04:27.280 --> 04:31.160
this picture of the world in front of you, and that was a fundamental part of intelligence.

04:31.160 --> 04:32.160
No, not quite there yet.

04:32.160 --> 04:36.360
Well, it wasn't being covered in the part of the field that I was working on, and then

04:36.360 --> 04:39.800
as I was sort of becoming a bit disillusioned with that, I found these theories in philosophy

04:39.800 --> 04:44.520
which I'd been dabbling in, which suddenly spoke to what I felt, you know, my experience

04:44.520 --> 04:48.640
of the world was like, and what intelligence actually was, and it was the first time

04:48.640 --> 04:53.080
when I thought, no, people are actually explaining this at a level where it's making sense to

04:53.080 --> 04:56.640
me, and it's progressing us further to all the better understanding.

04:56.640 --> 04:57.640
Okay.

04:57.640 --> 05:01.840
So I just completely transitioned, went back and re-learned philosophy so that I could

05:01.840 --> 05:04.840
understand these theories and philosophy of mind better.

05:04.840 --> 05:10.320
And while I was doing that, we were seeing all these advances now in neural networks and

05:10.320 --> 05:14.120
in deep learning and machine learning, where we are actually approaching something that

05:14.120 --> 05:18.680
looks like intelligence in a way that we can talk about, especially philosophically,

05:18.680 --> 05:22.640
that's interesting and like makes sense and does speak to what I was interested in in

05:22.640 --> 05:24.040
terms of intelligence.

05:24.040 --> 05:28.120
So now after making this transition to philosophy, I'm back in a sort of partial role in between

05:28.120 --> 05:34.120
the two, looking at intelligence in AI, but from a more philosophical perspective.

05:34.120 --> 05:38.200
And so how does that translate into a research path for you?

05:38.200 --> 05:43.120
Well, one of the problems with that is that's obviously a giant research project.

05:43.120 --> 05:48.640
So the project I'm on right now is the kinds of intelligence project, which is a sub-project

05:48.640 --> 05:51.640
at the Level Hume Center for the Future for Intelligence.

05:51.640 --> 05:56.840
This is a large 10 million pound project in the UK, mainly based at Cambridge, where

05:56.840 --> 06:01.480
the idea is to take people from multiple disciplines and look at the future of intelligence

06:01.480 --> 06:06.200
in the path we're on towards the future of intelligence and take expertise from AI,

06:06.200 --> 06:12.200
but also from philosophy, political science, literature, and all the broad range of subjects

06:12.200 --> 06:14.840
that could have something to say about intelligence.

06:14.840 --> 06:21.240
And use it to shape research in the future, shape policy decisions, and make sure that

06:21.240 --> 06:26.040
we are moving towards a future that's best for everyone in terms of intelligence.

06:26.040 --> 06:30.720
The kinds of intelligence is also the name of a symposium that you could organize here

06:30.720 --> 06:31.720
at NIPS.

06:31.720 --> 06:34.120
Tell us a little bit about that and the goals for it.

06:34.120 --> 06:39.120
So the goals of the kinds of intelligence project are to map this space of intelligence

06:39.120 --> 06:42.520
so that we can better understand the intelligence landscape.

06:42.520 --> 06:46.760
And the kinds of intelligence symposium and NIPS, the idea was to bring a lot of people

06:46.760 --> 06:50.360
who are at the forefront of different types of intelligence together into the same place

06:50.360 --> 06:54.400
and so we can have this conversation and look at the way the intelligence landscape is

06:54.400 --> 06:59.120
shaped, both from plant cognition to animal cognition to child development, which are all

06:59.120 --> 07:04.240
very important parts of understanding intelligence to obviously being at NIPS, AI and machine learning

07:04.240 --> 07:07.680
and the type of intelligence we can now create.

07:07.680 --> 07:12.360
We had people talking like Demis Hassebus from DeepMind, who's talking about alpha zero,

07:12.360 --> 07:17.920
which is now solving, go and chess and show the levels way beyond human intelligence.

07:17.920 --> 07:22.560
And we also had people talking about plant cognition and the way if you drop certain

07:22.560 --> 07:27.880
plants from a height over time, they can learn a response to curl up in protection.

07:27.880 --> 07:31.480
And falling on the floor, you drop them enough times, eventually they learn to anticipate

07:31.480 --> 07:34.080
what is going to happen and curl up ahead of time.

07:34.080 --> 07:35.080
Oh wow.

07:35.080 --> 07:37.800
So this is a form of learning that doesn't have any neurons involved and it's very alien

07:37.800 --> 07:40.680
to the type of learning that we generally think about.

07:40.680 --> 07:42.080
Huh, interesting.

07:42.080 --> 07:47.880
And so your personal kind of slice through this is from a philosophical perspective and

07:47.880 --> 07:54.320
you mentioned some kind of a body of work or research within philosophy that you stumbled

07:54.320 --> 07:55.320
across.

07:55.320 --> 07:56.320
What is that?

07:56.320 --> 08:01.240
So the general term for this is predictive processing and it's the idea that when we take

08:01.240 --> 08:06.120
in sensory data from the world, we have this huge jumble of messy information coming into

08:06.120 --> 08:07.120
the system, right?

08:07.120 --> 08:12.400
We have 130 million or so photo receptors in each eye, all transducing electromagnetic

08:12.400 --> 08:16.600
information and somehow the brain has to make sense of that and understand it.

08:16.600 --> 08:21.440
And at some point, it's, understands it in a way that we sort of experience the world

08:21.440 --> 08:23.640
and that's how that happened.

08:23.640 --> 08:27.760
But the old sort of very old view was that this information comes into the system and

08:27.760 --> 08:31.720
in a very bottom up way, it gets pieced together more and more and more complicated as it goes

08:31.720 --> 08:36.600
up through the system and eventually you get ideas such as tables and chairs and the

08:36.600 --> 08:39.080
kind of objects that we feel like that we see.

08:39.080 --> 08:42.920
Predictive processing idea sort of turns that on its head and says, we're not in the

08:42.920 --> 08:46.920
process of sort of taking these components of information and putting them together.

08:46.920 --> 08:50.040
We're actively trying to work out what we're going to experience.

08:50.040 --> 08:55.640
We're predicting the incoming sensory information and actively doing so, we're always trying

08:55.640 --> 08:57.480
to work out what's going to happen next in the world.

08:57.480 --> 09:02.120
And by turning it that way around and looking at how we could actively predict, we see

09:02.120 --> 09:07.280
that our experience of the world takes the form of what Annalceta is called a controlled

09:07.280 --> 09:08.280
hallucination.

09:08.280 --> 09:12.000
And this phrase is becoming much more, much more popular nowadays.

09:12.000 --> 09:16.160
So the idea is in a hallucination, you're just making stuff up, right?

09:16.160 --> 09:19.920
Maybe your brain is making stuff up that's not really there and that's what you see,

09:19.920 --> 09:20.920
right?

09:20.920 --> 09:25.440
In a controlled hallucination, you're making stuff up just as you were in a real hallucination,

09:25.440 --> 09:27.520
but it's controlled by the actual sensory data.

09:27.520 --> 09:28.520
It's up.

09:28.520 --> 09:33.000
So there's no real difference from me seeing this table in front of me right now to hallucinating

09:33.000 --> 09:37.640
one except for the fact that there's a ground truth from the sensory data that is binding

09:37.640 --> 09:38.640
it together.

09:38.640 --> 09:42.560
So that hopefully when I see a table there, from your perspective, you're also seeing

09:42.560 --> 09:43.880
a similar table.

09:43.880 --> 09:48.920
So what are the implications of seeing, you know, seeing cognition as this controlled

09:48.920 --> 09:49.920
hallucination process?

09:49.920 --> 09:54.560
There's a huge number of implications from this and I think that's one of the beauties

09:54.560 --> 09:58.360
of the theory and probably one of the potential downfalls of the theory too is that it can

09:58.360 --> 10:00.560
apply it to so many different levels across the brain.

10:00.560 --> 10:06.000
And also in relation to machine learning, we're seeing obviously a huge focus on predictive

10:06.000 --> 10:10.360
algorithms and on generative models, which are generating predictions about the world

10:10.360 --> 10:14.200
or the sensory data or the data that they're being input.

10:14.200 --> 10:18.920
So we can think of this as like a very low level, like in the retina at the back of the

10:18.920 --> 10:25.080
eye, we're doing what is called predictive coding, which is whenever I get say a particular

10:25.080 --> 10:31.040
rod cell is hit by electromagnetic radiation, it has the amount of information of the intensity

10:31.040 --> 10:34.640
of the light that it can transfer up further in the brain, right?

10:34.640 --> 10:38.280
That could be a large number of different values that this takes.

10:38.280 --> 10:42.720
But if instead of transferring that value, I look at what I would expect just by looking

10:42.720 --> 10:48.040
locally at all the values of the rod cells or the cone cells around me, I can take the

10:48.040 --> 10:53.760
average of that and see how much that particular cell is different from that average, then you

10:53.760 --> 10:58.840
will get a much smaller number, which increases the bandwidth that you, well, decreases the

10:58.840 --> 11:01.760
bandwidth that you need to use to send the same amount of information.

11:01.760 --> 11:08.040
And so are you describing a theory of what is actually happening physiologically or are

11:08.040 --> 11:13.880
you describing a modeling approach or this is a serious point happening in the eyes

11:13.880 --> 11:14.880
or something?

11:14.880 --> 11:19.000
A bit like that, yes, but so I was starting at the point where this is actually, yes,

11:19.000 --> 11:22.440
we know this is happening at a neurophysiological level.

11:22.440 --> 11:26.320
And I was going to move on to the complete other side where this approach can be applied

11:26.320 --> 11:31.480
to beliefs and desires and where our beliefs and desires are updated in a similar sort of

11:31.480 --> 11:33.120
predictive format.

11:33.120 --> 11:37.360
But before I move on, one thing with this predictive coding approach to the retina is if

11:37.360 --> 11:42.280
you look at it, it is very much like convolutions that we're seeing in machine learning.

11:42.280 --> 11:45.480
And the way they work is a very similar approach.

11:45.480 --> 11:49.320
They're obviously a bit more focused on invariance in the visual field and how we can apply the

11:49.320 --> 11:52.880
same map at different locations, which is another area of this.

11:52.880 --> 11:58.160
But they could also be used to do a similar approach to predicting local variations and

11:58.160 --> 12:02.960
only transmitting information about that variation as opposed to the raw data itself.

12:02.960 --> 12:08.840
So that's one level of it, but at the other level, we have the idea that our beliefs are

12:08.840 --> 12:13.400
updated in a similar way and our high level understanding of the world is based on these

12:13.400 --> 12:15.400
predictions that we're making.

12:15.400 --> 12:19.160
And comparing to the data coming in and when it's wrong, we have two choices.

12:19.160 --> 12:23.560
We either update our prediction and therefore change our model of the world and see the

12:23.560 --> 12:24.560
world differently.

12:24.560 --> 12:28.000
Or we act in the world and move around the world.

12:28.000 --> 12:31.360
And that might make our prediction that was wrong, turn into a prediction that was

12:31.360 --> 12:32.920
correct.

12:32.920 --> 12:38.000
So for example, if I predict that table is off to my right, there's two ways I can make

12:38.000 --> 12:39.000
that true.

12:39.000 --> 12:43.880
Well, I can turn, I can change my prediction, I can be, no, it's wrong, it's to my left

12:43.880 --> 12:45.120
and then update it that way.

12:45.120 --> 12:49.640
Or I could move my head and that would make the say another way of making the same prediction

12:49.640 --> 12:50.640
right.

12:50.640 --> 12:54.480
And I think this is a way that we could bring actions and interacting with the world

12:54.480 --> 12:59.960
into our understanding in, especially in machine learning and robotic, of how we can incorporate

12:59.960 --> 13:05.040
this sort of predictive approach to the whole brain or the whole way of modeling the world

13:05.040 --> 13:09.320
into a system that's not just understanding the world, but also acting in it and performing

13:09.320 --> 13:10.320
task.

13:10.320 --> 13:13.320
And therefore, being intelligent, hopefully.

13:13.320 --> 13:19.160
Is your research kind of approaching this from a theoretical perspective exclusively

13:19.160 --> 13:24.440
or is it there an experimental element or applied element as well?

13:24.440 --> 13:28.680
So I have been running experiments with predictive coding style on your own network.

13:28.680 --> 13:29.680
Okay.

13:29.680 --> 13:35.920
So you coming out recently based on this structure where you have a hierarchical generative network

13:35.920 --> 13:41.080
and each layer of the hierarchy is just trying to predict anything that the lower layers

13:41.080 --> 13:42.920
have so far failed to predict.

13:42.920 --> 13:47.320
So the first layer tries to predict the world, but it doesn't, it's not strong enough

13:47.320 --> 13:50.280
by itself to fully model everything.

13:50.280 --> 13:51.280
So then what it can't predict.

13:51.280 --> 13:52.280
Can you give an example?

13:52.280 --> 13:53.280
Can you give an example?

13:53.280 --> 13:54.280
Kind of the experiment.

13:54.280 --> 13:57.040
Like what it, what specifically is it trying to predict?

13:57.040 --> 14:03.720
So there's been work on this in experiments from video data, from car images for videos

14:03.720 --> 14:08.200
on top of a car moving around, driving around, you give it the first nine frames and ask

14:08.200 --> 14:09.840
it to predict the 10th.

14:09.840 --> 14:13.720
And then you can get fairly good results at predicting, you know, how the road is going

14:13.720 --> 14:15.800
to have moved and the things that will have come into you.

14:15.800 --> 14:16.800
Okay.

14:16.800 --> 14:19.880
I've been experimenting with this in maze like three dimensional domains working on the

14:19.880 --> 14:25.600
raw pixel inputs and predicting how the maze is going to update or how the pixels are

14:25.600 --> 14:27.400
going to update as I move around this maze.

14:27.400 --> 14:28.400
Okay.

14:28.400 --> 14:34.120
I spoke with a researcher working on something very similar like she was looking at it from

14:34.120 --> 14:37.680
the perspective of like embodied computer vision.

14:37.680 --> 14:44.680
So not just fixed frames, but you know, fixed frames plus the ability to move the orientation

14:44.680 --> 14:51.920
of the viewport, if you will, and one of the sets of experiments or use cases was this

14:51.920 --> 14:55.480
kind of sensor mounted on a car that was changing direction and trying to do the

14:55.480 --> 14:56.480
prediction.

14:56.480 --> 14:57.480
Things like that.

14:57.480 --> 14:58.480
Yeah.

14:58.480 --> 14:59.480
Interesting.

14:59.480 --> 15:03.760
And so you've got this scenario with, you know, the sick, the video case, you've got

15:03.760 --> 15:07.640
this scenario where you've got the camera on the vehicle and you're trying to predict

15:07.640 --> 15:08.800
ahead.

15:08.800 --> 15:13.360
How does that then tie into this hierarchical structure that you were describing?

15:13.360 --> 15:18.760
Well, the idea of the hierarchy is that you'll have the first layer of your network would

15:18.760 --> 15:23.800
be not have enough potential space inside it to fully predict everything that's going

15:23.800 --> 15:24.800
on.

15:24.800 --> 15:29.280
It's going to have space to represent the full function of the mapping of the change of

15:29.280 --> 15:30.880
the inputs over time.

15:30.880 --> 15:35.720
So then anything that it can predict, you're not interested in that anymore because that's

15:35.720 --> 15:36.720
sort of, that's done.

15:36.720 --> 15:40.120
That gets, that gets sort of finished at that layer of the system.

15:40.120 --> 15:43.840
But anything that it doesn't get predicted, this is called the prediction error, will get

15:43.840 --> 15:45.880
sent up to the next layer of the system.

15:45.880 --> 15:50.240
And this is the, if everything's working as intended, this is the parts that are a bit

15:50.240 --> 15:54.320
more complex than could be predicted just really easily, like, you know, something this

15:54.320 --> 15:57.800
pixel change always stays the same, so I'm just going to predict it stays the same.

15:57.800 --> 16:03.000
But then something slowly moving across the visual field or the input space in this video

16:03.000 --> 16:06.680
might be impossible to predict at that lower level, but at a higher level, once you've

16:06.680 --> 16:11.520
removed the easy stuff and you've got access to more machinery for sure, layers deep into

16:11.520 --> 16:13.760
the system, you might be able to predict that.

16:13.760 --> 16:18.720
And the idea is that as you move up the hierarchy, you'll get more high level representations

16:18.720 --> 16:21.600
of elements of the world that are changing.

16:21.600 --> 16:25.040
So the low level would be very simple stuff, the high level could potentially be things

16:25.040 --> 16:29.400
changing, you know, very slowly over time or changing, you know, more modeling, like

16:29.400 --> 16:32.920
the intuitive physics going on behind the domain that you might expect.

16:32.920 --> 16:37.280
Oh, this object's hitting something, so now it's going to change direction.

16:37.280 --> 16:41.920
The lower level might think, oh, it's just going to continue going on in the same direction.

16:41.920 --> 16:45.480
And then when it gets that wrong, the higher level, which hopefully has representation

16:45.480 --> 16:49.880
of the physics, will be able to correct for that and say, no, this is how it's going

16:49.880 --> 16:50.880
to change.

16:50.880 --> 16:56.040
So what you're describing sounds like, you sounds like what I think of a, you know, maybe

16:56.040 --> 17:00.120
a very deep convolutional net, right, that's going to figure out different things at different

17:00.120 --> 17:01.120
levels.

17:01.120 --> 17:06.400
How is it different, or are you doing things to kind of force it to learn certain aspects

17:06.400 --> 17:07.400
in certain layers?

17:07.400 --> 17:13.440
Yes, I think the beauty of the research is that at one level, you could think of it as

17:13.440 --> 17:15.920
just as a very deep convolutional net.

17:15.920 --> 17:20.600
But the one thing that is different is your focusing on this particular prediction

17:20.600 --> 17:23.520
paradigm, which has its own nice properties to it.

17:23.520 --> 17:28.840
So for example, if I, if I give you a noisy input, so if the video is full of noise, and

17:28.840 --> 17:33.880
I'm trying to predict the next frame in a noisy video, yeah, if the noise is unbiased,

17:33.880 --> 17:38.960
so that over time the average of the noise is zero, then the prediction automatically

17:38.960 --> 17:42.000
filters that out at the very bottom layer of the structure.

17:42.000 --> 17:46.440
And that doesn't get passed up to the higher levels, so they see less noisy input.

17:46.440 --> 17:50.280
So you get powerful results just from moving to this prediction paradigm.

17:50.280 --> 17:54.920
And also the second thing that is different is the main propagation from layer to layer

17:54.920 --> 18:00.760
is the errors of the stuff that you can't predict, as opposed to just a more complex

18:00.760 --> 18:03.560
combination of everything you've got so far.

18:03.560 --> 18:10.360
Does that result in a network that has fundamental differences in properties than what you

18:10.360 --> 18:18.080
might see in a typical CNN-like in terms of the density of the weights or the way the

18:18.080 --> 18:20.480
layers are interconnected with one another?

18:20.480 --> 18:21.480
Yeah, I think so far.

18:21.480 --> 18:25.080
I mean, the research is fairly early here, and there will be, I don't really know the

18:25.080 --> 18:26.080
answer to that question.

18:26.080 --> 18:31.600
The answer is going to be yes, but I can tell you in good details exactly how this approach

18:31.600 --> 18:32.600
differs from the others.

18:32.600 --> 18:38.400
And I think the more you incorporate from different areas in machine learning techniques,

18:38.400 --> 18:42.320
the more you are going to find that this approach looks like what we've got.

18:42.320 --> 18:43.320
Right, right.

18:43.320 --> 18:48.280
So the layers are trained into N, you're not training individual layer separately.

18:48.280 --> 18:52.600
You can train the individual layer separately because each layer's input is just the error

18:52.600 --> 18:53.600
from the layer.

18:53.600 --> 18:54.600
Right.

18:54.600 --> 18:55.600
Right.

18:55.600 --> 19:02.040
And so when you, you set, you acknowledge this early, have you had any preliminary results

19:02.040 --> 19:03.520
from this line of research?

19:03.520 --> 19:07.840
There are many results that the idea works.

19:07.840 --> 19:12.520
And the suggestion there is that if this is a good philosophical theory of how the brain

19:12.520 --> 19:17.600
might be working, and then when we do implement it, you know, actually implement it in your

19:17.600 --> 19:19.640
networks, we see positive results.

19:19.640 --> 19:24.480
So that's just a sort of a backup like result to say, yeah, maybe the philosophical theory

19:24.480 --> 19:25.800
is onto something, right?

19:25.800 --> 19:27.000
Maybe this is a good idea.

19:27.000 --> 19:32.240
So it can predict future frames with high accuracy, you know, just when moving around

19:32.240 --> 19:36.320
in the main, but it's still early days to say how well that will be when we move it into

19:36.320 --> 19:40.680
reinforcement learning type of experiments where we can compare to say to the art and say

19:40.680 --> 19:43.680
how that learning that kind of representation helped.

19:43.680 --> 19:44.680
Yeah.

19:44.680 --> 19:49.720
The impression that I get from the conversations I've had recently on this and related topics

19:49.720 --> 19:54.600
is that the, you know, our understanding of the brain and the neurophysiology and our

19:54.600 --> 19:59.040
understanding of the machine learning are, you know, we're kind of one leap frogs the

19:59.040 --> 20:02.320
other and then feeds back, you know, learnings to the other.

20:02.320 --> 20:06.880
And it's kind of this, you know, iterative process, is that your sense as well?

20:06.880 --> 20:12.200
Or as one area like, you know, far ahead of the other and, you know, for example, we

20:12.200 --> 20:16.320
understand the brain a lot more than we do the, you know, the machine learning side and

20:16.320 --> 20:21.160
machine learning continues to pull from that or the other way around or I think we understand

20:21.160 --> 20:23.200
them in very, very different ways.

20:23.200 --> 20:28.240
But there are a lot of people that are sort of on the cusp of between the two disciplines

20:28.240 --> 20:32.080
that are grabbing stuff from one and pulling it into the other and grabbing stuff in the

20:32.080 --> 20:33.080
other direction.

20:33.080 --> 20:38.560
We've seen that work like convolutions, as I mentioned before, across these two disciplines

20:38.560 --> 20:43.120
and they work on both side of the spectrum and what side did they come from, do you know?

20:43.120 --> 20:45.320
I think it depends on your view pit.

20:45.320 --> 20:51.000
So I was at the symposium, Gary Marcus was saying that perhaps Janlequin wasn't aware of

20:51.000 --> 20:55.000
all the predictive coding type work in the retina and how convolutions might be applied

20:55.000 --> 20:58.920
in low-level visual systems and he was just trying stuff and found something that works

20:58.920 --> 21:01.120
that happens to be very, very much related.

21:01.120 --> 21:02.120
Okay.

21:02.120 --> 21:03.120
Yeah.

21:03.120 --> 21:04.120
I think that depends who you ask.

21:04.120 --> 21:05.120
Okay.

21:05.120 --> 21:06.120
Interesting.

21:06.120 --> 21:07.120
Interesting.

21:07.120 --> 21:14.080
So you also mentioned earlier in the kind of the conversation of philosophy, theory of mind

21:14.080 --> 21:15.080
more broadly.

21:15.080 --> 21:18.120
Can you describe that and how that fits into all of this?

21:18.120 --> 21:19.120
Yeah.

21:19.120 --> 21:25.880
So at first, my, my intuitions about why intelligence is interesting are that it involves

21:25.880 --> 21:30.680
introspection and thoughts and the ability to reason about the world and in the way we

21:30.680 --> 21:34.280
do, which is in a sense in a sort of symbolic process, right?

21:34.280 --> 21:37.880
We construct sentences, we have language that's obviously a very key component.

21:37.880 --> 21:42.160
We construct sentences in our heads and we understand things through those sentences

21:42.160 --> 21:43.160
sometime.

21:43.160 --> 21:49.160
And it's really interesting to see how like modern work, it's starting to look at representations

21:49.160 --> 21:54.600
of the world that in machine learning where we can answer questions in natural languages

21:54.600 --> 21:59.880
applied to images, for example, we've seen relation nets where you take in an image

21:59.880 --> 22:03.800
containing, you know, a few objects of different sizes and different locations and you answer

22:03.800 --> 22:08.200
a natural language question such as is the red object to the left of the yellow object

22:08.200 --> 22:09.680
to that kind of question.

22:09.680 --> 22:14.840
And the way they work is, is to try and create a representation inside the network that

22:14.840 --> 22:21.040
understands this relational kind of information, which is moving towards, in, in my opinion,

22:21.040 --> 22:26.320
moving towards a more symbolic or at least potential for a symbolic understanding of the

22:26.320 --> 22:30.080
world inside the standard machine learning algorithm.

22:30.080 --> 22:36.320
My reaction to that from very little kind of reading in linguistics is that like part

22:36.320 --> 22:43.240
of that's not the idea of, you know, thinking and sentences is not universally accepted.

22:43.240 --> 22:44.240
Is that right?

22:44.240 --> 22:48.920
Like, it's, you know, a lot of ways thought is more abstract than sentences, their experiments

22:48.920 --> 22:55.320
about trying to remember the, you know, there's this line of thinking around, you know,

22:55.320 --> 23:01.840
whether the degree to which language impacts thought and, you know, I think there's kind

23:01.840 --> 23:06.560
of this popular belief that, you know, people think in their languages, but it's also been

23:06.560 --> 23:08.240
disproven in a lot of ways.

23:08.240 --> 23:09.240
Yeah.

23:09.240 --> 23:16.000
I don't know too much about that area, but I do think that some kind of, not necessarily

23:16.000 --> 23:20.880
language type processing, but symbolic level processing or processing where we understand

23:20.880 --> 23:26.040
objects as entities that persist over time that we can therefore then label is going to

23:26.040 --> 23:30.440
be necessary as we move towards more general types of intelligence, especially if we end

23:30.440 --> 23:34.840
up on this track where it seems, at least there's a lot of key players in the field right

23:34.840 --> 23:40.400
now pulling towards human like general intelligence, that that's going to be a key component.

23:40.400 --> 23:44.320
And I think as we move towards it in AI machine learning, we will be able to answer those

23:44.320 --> 23:45.320
questions better.

23:45.320 --> 23:49.040
But for now, yeah, I'm not really sure.

23:49.040 --> 23:53.240
What's kind of the future of your particular research, both from the philosophical side

23:53.240 --> 23:55.000
as well as the machine learning side?

23:55.000 --> 23:56.000
Yeah.

23:56.000 --> 23:59.640
So we actually ended up talking about a sort of slightly small part of the research I've

23:59.640 --> 24:03.880
been doing, which I think doesn't really necessarily reflect on that.

24:03.880 --> 24:05.880
Let's dive further into your research.

24:05.880 --> 24:06.880
Okay.

24:06.880 --> 24:12.720
So one thing, Subin said at the symposium was that maybe at the moment we're at a pre-compernican

24:12.720 --> 24:15.720
revolution for our understanding of intelligence.

24:15.720 --> 24:20.240
So obviously in the concurrent revolution, we went from having humans at the center of

24:20.240 --> 24:26.400
the universe with everything revolving around it to humans as no longer at the center.

24:26.400 --> 24:31.320
And our understanding of intelligence seems very human-centric at the moment, or at least

24:31.320 --> 24:35.640
the layperson or the everyday understanding of intelligence.

24:35.640 --> 24:40.960
And we're seeing a sort of move away from this with the, you know, ideas of plant cognition

24:40.960 --> 24:46.600
and also these ideas of AI systems where, as Demis was saying at the symposium, Alpha

24:46.600 --> 24:50.400
Zero playing chess played very alien type of chess to him.

24:50.400 --> 24:52.320
It wasn't a human-like way of playing.

24:52.320 --> 24:54.440
It was a new type of playing.

24:54.440 --> 24:59.480
So when we explore this intelligence landscape, humans are going to be a very, very tiny part

24:59.480 --> 25:01.360
of that giant landscape.

25:01.360 --> 25:05.680
And with AI, because everything is artificial, we have the ability to explore way beyond

25:05.680 --> 25:09.960
the scope of this little area that, well, there's a very tiny area that biological life

25:09.960 --> 25:11.560
potentially exists in this landscape.

25:11.560 --> 25:15.200
And there's an even smaller area of the human life exists in this landscape.

25:15.200 --> 25:21.120
But my particular interest, and I think the big question moving forward for AI is, when

25:21.120 --> 25:25.960
will we create intelligences and what type of intelligences have some kind of moral

25:25.960 --> 25:26.960
patience?

25:26.960 --> 25:28.560
You'd have some kind of, some kind of what?

25:28.560 --> 25:29.560
Moral patience.

25:29.560 --> 25:33.400
So they are, patience in our moral understanding of them.

25:33.400 --> 25:38.800
So they can, it can be ethically correct or wrong to put them in certain situations.

25:38.800 --> 25:44.400
So for example, Nick Bostrom has this term, the mind crime where potentially we could create

25:44.400 --> 25:50.040
conscious artificial entities into some kind of slavery because they're just doing task

25:50.040 --> 25:51.040
for us.

25:51.040 --> 25:55.460
Or we could create entities that just live a life of suffering because they're never

25:55.460 --> 25:59.440
achieving their goals and it actually means something to say that they're suffering.

25:59.440 --> 26:04.760
And I think we need to explore their space of intelligence and compare it to the space

26:04.760 --> 26:06.480
of possible mind.

26:06.480 --> 26:11.600
The type of intelligences, not all intelligences are minds.

26:11.600 --> 26:12.840
We know that, that seems obvious.

26:12.840 --> 26:14.000
But some of them are.

26:14.000 --> 26:18.440
And we don't know right now whether it's just this little tiny, and it seems absurd,

26:18.440 --> 26:22.000
you know, it would be very pre-capernica and to say it's just this little human dot that

26:22.000 --> 26:26.580
is the space of possible minds where we need to map that onto the space of possible

26:26.580 --> 26:27.580
intelligences.

26:27.580 --> 26:31.280
So I see that as the broad research goal that's most important.

26:31.280 --> 26:36.880
So beyond the tiny piece of it, we discover what are some of the other kind of concrete

26:36.880 --> 26:40.080
research areas within that broad umbrella.

26:40.080 --> 26:46.020
So I think that the most concrete question, at least for me, is how can we understand

26:46.020 --> 26:52.800
intelligence in a way where we can then say about certain agents that are intelligent,

26:52.800 --> 26:54.360
whether or not they have a mind.

26:54.360 --> 26:58.360
But obviously that itself is a massively huge question and it's going to take results

26:58.360 --> 27:02.840
from philosophy, from neuroscience, to get to the human understanding of, like, we

27:02.840 --> 27:07.360
know humans have minds, so we can learn about minds from them, and then from AI and from

27:07.360 --> 27:12.520
completely other side of the field to think about what different types of alien intelligences

27:12.520 --> 27:14.560
or artificial intelligence could have.

27:14.560 --> 27:17.600
Do we even have a functional definition of mind?

27:17.600 --> 27:20.520
We don't even have a function with our definition of intelligence.

27:20.520 --> 27:25.320
That was one of the great results I thought of their symposium was we've invited all

27:25.320 --> 27:29.040
these people together to talk about intelligence in different ways and they've brought their

27:29.040 --> 27:30.600
own expertise.

27:30.600 --> 27:36.160
But that expertise, each comes with its own assumptions about what intelligence is, and

27:36.160 --> 27:42.280
we even have this debate there with Alpha Zero now playing Go and Chess better than humans.

27:42.280 --> 27:46.320
If I told you, however, Srendan is a really great chess player, you're automatically think

27:46.320 --> 27:47.680
he's intelligent.

27:47.680 --> 27:51.320
But now there's people saying, oh, Alpha Zero is not intelligent, chess is easy.

27:51.320 --> 27:57.400
Which, I mean, there's not even an agreement on that front of what intelligence is, so

27:57.400 --> 28:01.560
yeah, so minds is going to be harder than intelligence, and we haven't sort of intelligence

28:01.560 --> 28:02.560
yet.

28:02.560 --> 28:08.160
From the various folks involved in the symposium, are there patterns in the way they define

28:08.160 --> 28:10.560
intelligence that are easy to characterize?

28:10.560 --> 28:11.560
Like, is there...

28:11.560 --> 28:12.560
I think so, yeah.

28:12.560 --> 28:13.560
So there's a...

28:13.560 --> 28:17.760
Pedro Domingo's tribes of AI, is there an intelligence version of that?

28:17.760 --> 28:21.200
Yeah, because I think there's definitely a count that's...

28:21.200 --> 28:25.920
Very interesting human-like intelligence, and what they tend to do is define intelligence

28:25.920 --> 28:31.520
in terms of human intelligence, and then automatically assume that AGI, as we move to more general

28:31.520 --> 28:35.120
intelligence, is going to be on a path towards human-like intelligence, because that's the

28:35.120 --> 28:36.960
type they're very, very interesting.

28:36.960 --> 28:40.720
And then on the other side of the spectrum, you've got this idea that intelligence is so

28:40.720 --> 28:44.800
much more broad than as humans we could possibly imagine, so I think you have these sort of

28:44.800 --> 28:46.440
two separate camps.

28:46.440 --> 28:47.440
Okay.

28:47.440 --> 28:48.440
Interesting.

28:48.440 --> 28:52.640
So maybe going kind of circling back to, you know, the future and kind of how you push

28:52.640 --> 28:53.960
all this forward.

28:53.960 --> 28:56.000
How are you thinking about that today?

28:56.000 --> 28:59.960
Having just finished your, you know, pulling together the symposium and bringing together

28:59.960 --> 29:05.360
some of the, you know, folks that are kind of pushing this research forward.

29:05.360 --> 29:10.800
So I'm thinking that it's never too early to start these asking these questions.

29:10.800 --> 29:15.720
It might be too early to expect to have concrete answers to these questions, but it's definitely

29:15.720 --> 29:19.560
the time that we can actually bring these different types of people together to have this

29:19.560 --> 29:20.560
conversation.

29:20.560 --> 29:25.320
Because although everyone has different understandings of intelligence, we're getting results

29:25.320 --> 29:29.040
in these different fields that are comparable, and we can start comparing them and talking

29:29.040 --> 29:30.040
about the issue.

29:30.040 --> 29:35.280
So my feeling is very optimistic that this is the important and right area that we should

29:35.280 --> 29:38.760
be working on, and that we are going to get results.

29:38.760 --> 29:44.080
Now that we're at a state where AGI, you know, is as advanced as it is, and our understanding

29:44.080 --> 29:49.200
of intelligence across the animal kingdom is that it is, but we can stop bringing these

29:49.200 --> 29:50.200
things.

29:50.200 --> 29:51.200
Awesome.

29:51.200 --> 29:52.200
Awesome.

29:52.200 --> 29:54.960
Well, Matt, thanks so much for taking the time to chat with me about what you're up

29:54.960 --> 29:55.960
to.

29:55.960 --> 29:58.240
I wish I had an opportunity to attend the symposium.

29:58.240 --> 30:02.640
It sounds excellent, and I know that you had some really excellent speakers and participants.

30:02.640 --> 30:06.280
So I'm looking forward to keeping up with the work of the group.

30:06.280 --> 30:07.280
Thank you.

30:07.280 --> 30:08.280
Thank you.

30:08.280 --> 30:15.560
All right, everyone, that's our show for today.

30:15.560 --> 30:20.800
Thanks so much for listening, and for your continued feedback and support.

30:20.800 --> 30:25.880
For more information on Matthew or any of the topics covered in this episode, head on

30:25.880 --> 30:30.440
over to twimmelaii.com slash talk slash 91.

30:30.440 --> 30:37.360
To follow along with the NIP series, visit twimmelaii.com slash NIPs 2017.

30:37.360 --> 30:44.000
To enter our twimmel 1 mil contest, visit twimmelaii.com slash twimmel 1 mil.

30:44.000 --> 30:49.800
Of course, we'd be delighted to hear from you, either via a comment on the show news page,

30:49.800 --> 30:54.960
or via a tweet to act twimmelaii or at Sam Charrington.

30:54.960 --> 30:59.440
Thanks once again to Intel Nirvana for their sponsorship of this series.

30:59.440 --> 31:04.160
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in

31:04.160 --> 31:08.400
AI Arena, visit intelnervana.com.

31:08.400 --> 31:13.160
As I mentioned a few weeks back, this will be our final series of shows for the year.

31:13.160 --> 31:18.400
So take your time and take it all in and get caught up on any of the old pods you've been

31:18.400 --> 31:20.040
saving up.

31:20.040 --> 31:22.440
Happy holidays and happy new year.

31:22.440 --> 31:24.720
See you in 2018.

31:24.720 --> 31:37.080
And of course, thanks once again for listening and catch you next time.

