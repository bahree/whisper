WEBVTT

00:00.000 --> 00:16.000
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

00:16.000 --> 00:29.680
All right, everyone. I am here in Vancouver, continuing my coverage and

00:29.680 --> 00:35.040
conversations at the NURPS 2019 conference. And I've got the pleasure of being with

00:35.040 --> 00:39.280
you again, Schmidt Huber. You again is a co-founder and chief scientist of

00:39.280 --> 00:47.360
Nacense, the scientific director at the SWIST AI Lab Insia, as well as a professor of AI at USI

00:47.360 --> 00:54.960
and SUPSI in Switzerland. You again and I had the pleasure of speaking just over a couple of years

00:54.960 --> 01:02.880
ago now, where he hosted me at his lab in Logano, Switzerland, a beautiful setting and a wonderful

01:02.880 --> 01:10.400
conversation that I encourage you to take a listen to. That was Tumel Talk number 44, almost 300

01:10.400 --> 01:18.640
episodes ago. You again is, you know, regarded by many as the father of AI. He is his lab created

01:18.640 --> 01:26.480
the LSTM. That was a big focus of our conversation. The LSTM is a model that, you know, we all use

01:26.480 --> 01:32.400
all the time on our smartphones. And I'm excited to have an opportunity to catch up with you.

01:32.400 --> 01:37.280
You again, welcome to the Tumel AI podcast. It's my pleasure being here. Thank you so much.

01:37.280 --> 01:43.040
So, you know, I really wanted to start this conversation with a bit of an update. What are some

01:43.040 --> 01:48.080
of the things that you've been doing in your research lab and at Nacense? What's been interesting

01:48.080 --> 01:54.160
and exciting for you? Let me give you a couple of examples of what Nacense is doing. That's a B2B

01:54.160 --> 02:02.640
company and now we're entering this age of active AI where you don't just have passive patent

02:02.640 --> 02:07.760
recognition on your smartphone, but where you have machines that learn to become better machines

02:07.760 --> 02:12.720
by translating the data that is coming in through the cameras and other sensors into actions

02:12.720 --> 02:18.960
that manipulate the industrial processes that they are controlling. For example, we have a project

02:18.960 --> 02:25.440
with Festo which is one of the leading robot makers and they have these hands which are being

02:25.440 --> 02:34.640
controlled by air muscles. So you have a compressor which is generating the pressure that you need

02:34.640 --> 02:40.720
to move these fingers around and nobody knows how to control that. And so Nacense is building the

02:40.720 --> 02:49.760
brains that learn to control these soft robot hands that we have there. Or EOS that is leader

02:49.760 --> 02:56.560
in industrial 3D printing where you don't just do little plastic models of something,

02:56.560 --> 03:04.080
but where you really print engines with metal and they have all the patterns for melting the powder

03:04.080 --> 03:10.480
and then letting it incrementally through additive manufacturing turn into engines of airplanes

03:10.480 --> 03:15.360
and stuff like that. And one of these printers costs at least half a million or something.

03:15.360 --> 03:20.720
So they are much more expensive than the standard cheap printers that you can buy in the supermarket.

03:21.360 --> 03:27.280
It already seems clear that you can further improve these printing processes by understanding

03:27.280 --> 03:34.560
better what exactly is the impact of the laser light on the melting process and on the cooling

03:34.560 --> 03:40.400
process and how can you control the thing in a way that improves the quality of the engines that

03:40.400 --> 03:46.560
are coming out. Then other projects with Salt Sashmeet for example they have little drones and

03:46.560 --> 03:54.800
they are inspecting the wind turbines and then how can you automatize that and how can you learn,

03:55.600 --> 04:02.560
make these machines learn to become better quality inspectors. With Audi a while ago we had a

04:02.560 --> 04:09.680
demo project where a little toy Audi is maybe only one eighth-author size of a real Audi.

04:09.680 --> 04:16.320
They learn to park not by imitating a human teacher but just by reinforcement learning just by

04:17.040 --> 04:23.760
maximizing the pleasure signals that come in and minimizing the pain signals that come in.

04:23.760 --> 04:28.480
Whenever one of these little cars bumps against an obstacle then it gets negative reward.

04:28.480 --> 04:36.640
Whenever it reaches the parking lot then it gets a positive reward and as always in reinforcement

04:36.640 --> 04:42.080
learning it's trying to minimize pain and maximize pleasure so it has to translate the incoming

04:42.080 --> 04:50.560
video and the other data from the radar sensors and lead sensors into actions that lead to successful

04:50.560 --> 04:55.680
parking strategies and that was the first time something like that was done in the real world.

04:55.680 --> 05:02.960
These little Audi's they aren't as big as the as the real thing however they have all these

05:02.960 --> 05:10.400
sensors that you have and the big Audi's the radar the cameras they go up to 120 kilometers

05:10.400 --> 05:16.720
an hour so they are pretty fast too and sounds like a nice toy. It's a very nice toy one has

05:16.720 --> 05:25.520
certainly that and a whole bunch of other projects are I'm happy that now that nasons isn't the

05:25.520 --> 05:33.840
second round of financing we don't only have these these investors from the platform companies

05:34.400 --> 05:41.760
on the Pacific Rim but now also old industry. Old industry is investing in us for example shot

05:41.760 --> 05:48.880
is a leading maker of glass and you probably have them on your smartphone because they are owned

05:48.880 --> 05:55.840
by the size foundation and the glass that they make isn't the lenses are the cameras on

05:56.720 --> 06:03.120
hundreds of millions of them okay as smartphones many people don't know that making glass is

06:03.920 --> 06:11.360
difficult and it's about controlling an industrial pipeline where you have

06:11.360 --> 06:20.720
molten glass and it's 1,200 degrees Celsius and you want to remove the bubbles and at the right

06:20.720 --> 06:26.560
moment you want to inject certain substances and you have turbulent mixing that you want to

06:26.560 --> 06:33.760
control all kinds of knobs that you can adjust there and this has been traditionally done by

06:33.760 --> 06:40.400
humans. Shot is a company which is more than a hundred years old and over the decades they have

06:40.400 --> 06:46.000
learned how to do that well however there is still a lot of room for improvement and for making

06:46.000 --> 06:54.240
even better glass through machines that learn to do that so suddenly investors are not just the

06:54.240 --> 07:01.920
platform companies from the Pacific Rim but old industry which is realizing that in this coming

07:01.920 --> 07:08.960
age of active AI suddenly you can do things that go far beyond passive pattern recognition on

07:08.960 --> 07:15.200
this smartphone and they sense this in the middle of that. That's very exciting very exciting you

07:15.200 --> 07:22.640
mentioned a few interesting use cases with the first one that you mentioned the robotic hand calls

07:22.640 --> 07:28.800
to mind the recent news that you may have come across with open AI and their Rubik's Cube and they've

07:29.440 --> 07:36.080
used reinforcement learning to kind of train the model that controls the you know this highly

07:36.080 --> 07:40.880
dimensional robot essentially. Are you also using reinforcement learning as part of that application

07:40.880 --> 07:48.560
or what are the kind of the main techniques that you're applying to that? In fact the open AI

07:48.560 --> 07:57.360
robot hand is driven by the architectures that we have developed in our labs and Munich and

07:57.360 --> 08:04.160
Switzerland over the decades. What is this open AI reinforcement learner doing? It is basically

08:04.160 --> 08:11.680
a big LSTM network which is surrounded by a couple of smaller pre-processing networks, pre-processes

08:11.680 --> 08:18.080
our inputs and then through lots of simulations there's a pretty good simulation of this hand

08:18.720 --> 08:25.120
through reinforcement learning the LSTM learns to control the hand such that it can do interesting

08:25.120 --> 08:33.760
things such as solving the Rubik's Cube in a certain fraction of the times. So yeah of course we are

08:33.760 --> 08:41.120
using the techniques that are now popular in reinforcement learning which is train and LSTM

08:41.120 --> 08:48.480
through reward maximization to do that. However the big difference between what what this

08:48.480 --> 08:54.960
example of open AI is about and what you have in the real world is you don't have a good model

08:54.960 --> 09:04.240
or see real world. You somehow have to deal with this situation that you have very few interactions

09:04.240 --> 09:10.400
examples with the real world from which you have to build a model of the world and then you have

09:10.400 --> 09:17.760
to do mental experiments in this model which is imperfect in many ways but can learn to improve

09:17.760 --> 09:23.280
itself through further data that is coming in through the actions of the robot that is trying to

09:23.280 --> 09:32.880
learn something new. So are you speaking to the relationship between simulation and having the

09:32.880 --> 09:39.200
model operate in the real world and kind of correlating or integrating rather those two such that

09:39.200 --> 09:43.120
you've got some kind of end-to-end training process that integrates both SIM and real.

09:43.120 --> 09:54.320
So at the moment AI works well in the virtual realm for video games like Dota or Starcraft that's

09:54.320 --> 10:00.720
what deep-minded Dota was done by OpenAI. Again you have an LSTM network surrounded by other

10:00.720 --> 10:09.200
networks which is learning through policy gradient methods reward optimization methods to make

10:09.200 --> 10:14.960
some of these weight strong and some of them weaker such that the whole system over time becomes a

10:14.960 --> 10:23.280
better video game player and there you train the system through exposing it through billions and

10:23.280 --> 10:29.920
trillions of video games. This is something that you can do in the virtual realm and the same

10:29.920 --> 10:38.320
principle was applied to the robot hand of OpenAI where they also had lots of simulations

10:38.320 --> 10:43.680
because they did have a pretty good simulation of the hand and then through certain

10:44.720 --> 10:53.840
tricks it was possible to transfer the behavior from the virtual setting to the real world setting

10:53.840 --> 11:03.760
and then one essential ingredient was this excellent simulation of the hand and normally you don't

11:03.760 --> 11:12.000
have that. In almost all industrial processes you don't have that. In the real world you have all

11:12.000 --> 11:20.000
kinds of slippage and it's not well-modeled in any simulation and you have so many things that are

11:20.000 --> 11:30.480
not modeled. How does a baby learn to deal with that? Well the baby is receiving data as

11:30.480 --> 11:36.880
consequence of the actions that it is generating as it is playing with its toys and from this data

11:36.880 --> 11:42.880
it learns to build a model so it doesn't have a given model of the world. No it learns to build this

11:42.880 --> 11:48.800
model and how does it do that? It learns to predict what's going to happen next what is the next

11:48.800 --> 11:56.560
input given what I have done so far and some of the inputs are predictable others are not predictable

11:56.560 --> 12:03.040
and the baby even does more than just predicting the next thing somehow it learns high level abstractions

12:03.040 --> 12:10.800
and it learns to plan ahead in a way that apparently is not the millisecond by millisecond planning

12:10.800 --> 12:16.240
that you would do in a detailed simulation of the world where you will try to come up with a

12:16.240 --> 12:22.400
sequence of actions that leads to a lot of predicted reward in that simulation where you really

12:22.400 --> 12:28.000
have a simulation that millisecond by millisecond simulates the entire world and such a simulation

12:28.000 --> 12:37.120
is totally dependent on being a good simulation and no errors are allowed there while the models that

12:37.120 --> 12:45.040
humans carry around they are more sophisticated in the sense that they acknowledge that certain

12:45.040 --> 12:50.640
things are not known and other things are rather predictable and they don't do this millisecond by

12:50.640 --> 12:57.680
millisecond planning no they do high level planning like for example when you are going to Beijing

12:57.680 --> 13:04.640
you you make a plan for that and you don't plan the trajectories of all your muscle movements

13:04.640 --> 13:11.680
millisecond by millisecond no you say first I have to find a taxi and then once I'm in the taxi

13:11.680 --> 13:18.320
I'm going to the airport and then I'm entering the plane and for nine hours nothing is going to

13:18.320 --> 13:24.960
happen and then I accident so this high level planning is something that humans are really good at

13:24.960 --> 13:31.600
and probably also some of the animals and our robots are not good at that yet however this is

13:31.600 --> 13:39.200
changing and we are making great progress exactly in that realm generally speaking current AI

13:39.200 --> 13:46.720
works well in the virtual realm video games or robots where you have a really good simulation of

13:46.720 --> 13:56.160
the robot and it doesn't work well yet in the physical real world we are so there is an implication

13:56.160 --> 14:04.320
of this that the model that you developed with the hand was not based on simulation it was all based

14:04.320 --> 14:10.880
on training through experimentation on the physical hand itself that's what we generally do

14:10.880 --> 14:19.600
so we try to learn like a little baby a model of the industrial process that we are controlling

14:20.320 --> 14:27.680
and because in most cases there is no good model of this process we have to learn it from experience

14:28.480 --> 14:35.680
so suddenly we need a system that learns to generate experiments that lead to data that can be

14:35.680 --> 14:42.480
used to build a better model when I think about for example you know maybe this doesn't apply to

14:42.480 --> 14:52.320
hands to the extent that applies to like robotic arms but in with robotic arms they're you know I've

14:52.320 --> 14:59.600
often kind of talked to folks or talked about kind of this tension between you know folks that

14:59.600 --> 15:06.240
want to tackle that problem with end deep learning which any model versus folks that want to take

15:06.240 --> 15:12.880
advantage of kind of traditional control systems and you know robot kinematics that you know if

15:12.880 --> 15:19.840
you're able to integrate those traditional models you know that we do kind of know how to get a

15:19.840 --> 15:25.120
robot an actuator from one position you know in a 3D space to another position in a 3D space

15:25.120 --> 15:30.880
because we have kind of models that do that it sounds like your approaches aren't kind of model based

15:30.880 --> 15:37.920
in that perspective you're learning everything from the ground up using reinforcement learning without

15:37.920 --> 15:43.040
the benefit of simulation is that yeah what are the leaps that I'm making there you know tune that

15:43.040 --> 15:50.240
up for me so in industrial applications we use everything that is already there okay so if there's

15:50.240 --> 15:56.960
a reasonable model of the process then of course we are going to use that it's just that the most

15:56.960 --> 16:04.400
exciting applications are those where you don't have that model hmm sure if you have a factory

16:04.400 --> 16:11.520
full of industrial robots then you know exactly where each of these robots is positioned you know

16:11.520 --> 16:22.080
exactly where it has to grip the chair and how to move it exactly at the right position in the car

16:22.080 --> 16:29.680
where it's going to end up so there you have very precise industrial robots and they they are

16:30.480 --> 16:36.240
extremely good at doing what they are doing because everything is totally on a control there's

16:36.240 --> 16:43.280
a perfect model of where everything is located where the tools are where the cars are coming in

16:43.280 --> 16:50.080
where the chairs are they they can do that without cameras even because everything is already

16:50.080 --> 16:55.840
modeled extremely well right but that's not the future of manufacturing the future of manufacturing

16:55.840 --> 17:02.480
is all the messy things all the messy stuff like when you are assembling a smartphone for example

17:02.480 --> 17:09.120
there are all these little parts of this smartphone and at the moment humans have to do that also

17:09.120 --> 17:14.720
there is no robot that is able to do that at least not in the way that humans can do it and what

17:14.720 --> 17:22.400
you really want is a robot that is confronted with a new task and quickly learns through a few

17:22.400 --> 17:29.920
interactions maybe a few demonstrations of a human but also a few self-generated experiments

17:29.920 --> 17:36.960
how this thing works and how to screw in that screw and how to avoid that the screw driver

17:36.960 --> 17:43.120
slipping and all these super complicated important things that are easy for humans but not yet

17:43.120 --> 17:50.640
for current robots and it wants to to learn through a combination of curiosity and imitation of

17:50.640 --> 17:58.800
what the human teachers are showing to it it wants to learn to execute this new behavior quickly

17:58.800 --> 18:05.840
at some point as well as the teacher who is just through a visual demonstration and through

18:05.840 --> 18:12.160
speech and language trying to explain to the robot what it wants to what it should do and then

18:12.160 --> 18:19.280
once it is able to do that to a certain extent you want to have a robot through self-experimentation

18:19.280 --> 18:25.520
to speed up the process and make it more efficient so that it can do it much faster than any human

18:25.520 --> 18:32.320
and with less energy and then once it's really good you want to switch off the learning algorithm

18:32.320 --> 18:39.520
and make a million copies and license it or sell it yeah as we talked about on this show many times

18:39.520 --> 18:46.960
RL is particularly deep RL is historically extremely simple and efficient right and so a lot of

18:46.960 --> 18:52.640
folks will turn the simulation because if they just try to train in the real world they'll destroy

18:52.640 --> 18:59.920
their robots before they converge to any type of model is the work that you're doing they're

18:59.920 --> 19:07.840
focused on the the simple efficiency side of the equation it is the essential thing which we need

19:07.840 --> 19:14.800
for dealing with the real world because there you can have only so many training examples

19:15.440 --> 19:21.920
and self-invented experiments and that's it you can't have a trillion experiments like in video

19:21.920 --> 19:29.120
games right right so that's the big difference and it's really the single most important issue

19:30.400 --> 19:38.640
the the big gap that exists between the learning abilities of humans and our best reinforcement

19:38.640 --> 19:45.920
learning algorithms is really that so from very few training examples you want to pick up all the

19:45.920 --> 19:52.400
things that are necessary to control that process that you're trying to control and it's what

19:52.400 --> 20:00.720
current reinforcement learning algorithms are not so good at what are the specific things that

20:00.720 --> 20:06.320
or specific research or papers that you're working on in your lab that try to get there is it

20:06.320 --> 20:12.160
you know how would you even did they fall under kind of the banner of one shot few shot learning

20:12.160 --> 20:18.080
meta learning curriculum learning imitation learning like are there are they you know one or

20:18.080 --> 20:24.960
combinations of these things or is it an altogether different approach so since we are interested

20:24.960 --> 20:30.160
in universal learning machines it's all of that okay but let me give you a one concrete little

20:30.160 --> 20:36.400
example of something that we published just a couple of days ago okay in an active paper

20:36.400 --> 20:42.640
something which is called upside down reinforcement learning upside down reinforcement learning okay

20:42.640 --> 20:49.040
upside down already intrigued it's called upside down reinforcement learning because it turns

20:49.040 --> 20:53.840
traditionally reinforcement learning on its head traditionally reinforcement learning works like

20:53.840 --> 21:01.520
that you have a neural network or some other adaptive machine which takes in data from the

21:01.520 --> 21:07.760
environment and produces actions which change the environment which means new inputs are coming in

21:07.760 --> 21:15.200
from the environment and there is something you want to maximize which is the cumulative expected

21:15.200 --> 21:23.440
reward so when the when the robot is acting in a good way then it gets reward and it wants to maximize

21:23.440 --> 21:28.320
that by making some of these connections in the network stronger and others weaker such that

21:28.320 --> 21:34.640
actions come out in response to the inputs that lead to a lot of reward success yes exactly

21:34.640 --> 21:39.920
and then the way it's usually done is there is a second network which is a prediction machine which

21:39.920 --> 21:47.120
sees the actions of the first network and the inputs that are coming in from the environment and

21:47.120 --> 21:54.080
it's trying to predict given this policy that is implemented by the first network what is the

21:54.080 --> 22:02.560
expected future reward so some of all the rewards that is going to come in until the end of the

22:02.560 --> 22:10.240
contrial for example and then through this prediction which depends on the actions and the

22:10.240 --> 22:18.080
information from the environment you can find actions that lead to more reward than what you

22:18.080 --> 22:24.400
have so far and then there are simple ways of adjusting the first network such that the actions

22:24.400 --> 22:33.040
that are being generated by it are more likely to lead to a lot of reward so what is very essential

22:33.040 --> 22:39.760
there is this prediction of future expected reward and upside down reinforcement learning turns

22:39.760 --> 22:46.160
all of that around it doesn't have the rewards being predicted by the outputs of such a network

22:46.160 --> 22:55.280
no the rewards are becoming the inputs to a network which produces the output actions and which

22:55.280 --> 23:02.160
also sees the standard inputs coming from the environment and these incoming rewards are

23:02.160 --> 23:12.800
interpreted as commands so there's this extra input field of this control network which is

23:12.800 --> 23:20.000
not the cumulative reward it's the reward that's a result of the previous action no it's

23:20.000 --> 23:27.440
in fact the cumulative the cumulative reward that you would like to get between this time and

23:27.440 --> 23:33.280
this time in the in the simplest case there are two inputs one is a time input which says between

23:33.280 --> 23:40.560
this time and this time I want to get so much reward that's a command that's a command so now

23:40.560 --> 23:46.320
the network sees this command and tries to come up with actions that between this time and this

23:46.320 --> 23:54.320
time lead to so much reward it just tries to obey the command okay so now it's actually unique

23:54.320 --> 24:00.560
its actions and it turns out that the incoming reward is less than what the command

24:01.520 --> 24:07.120
requested so then we have a training example where the network can learn something from that and

24:07.120 --> 24:15.680
it can learn in the following way let's adjust the command retrospectively let's take the

24:15.680 --> 24:22.800
real reward that came in between these two time steps and use that as a command and let's run

24:22.800 --> 24:27.600
the whole thing again and let's train it to generate this action sequence that it already has

24:27.600 --> 24:33.840
generated in response to these modified commands which are now consistent with what the network did

24:33.840 --> 24:42.720
okay so now it learns to generate an action sequence like what it just had generated in response

24:42.720 --> 24:48.880
to these commands and what you really want to achieve is that you generate action sequences

24:48.880 --> 24:54.880
that lead to more reward so you say let's give a new command to the system let's say we now want

24:54.880 --> 25:01.840
to have twice as much reward as last time so new command everything is the same except that

25:01.840 --> 25:09.280
now we've won twice as much reward as a command input so the network is producing outputs

25:09.280 --> 25:17.520
in response to that and maybe the result is that the reward that is really received from the

25:17.520 --> 25:22.960
environment is indeed twice as much because it's somehow was able to internalize then we are happy

25:22.960 --> 25:28.800
then we say on let's try the same thing with four times as much reward and it sounds like a

25:28.800 --> 25:37.040
binary search of the reward space or something like that maybe not binary but you I'm imagining

25:37.040 --> 25:43.760
that you would start with you know an unachievable reward maybe infinite and you the network

25:43.760 --> 25:48.960
achieves much less than that and then you you know it says okay maybe I'll you know go to half

25:48.960 --> 25:54.640
of infinity or whatever half of the large number that I would love to have but the point is from

25:54.640 --> 26:01.680
each of its failures to to satisfy the command it learns something so it learns each time something

26:01.680 --> 26:12.400
new about how to map commands to action sequences and over time it will learn the structure of

26:12.400 --> 26:21.280
this space of parameters that you need to translate these commands into desired behavior and then

26:21.280 --> 26:27.840
you can do very directed exploration because you just as you said you give new commands that you

26:27.840 --> 26:34.160
never have given before and you hope that the network will know enough about the space of these

26:34.160 --> 26:41.760
parameters and about functions mapping reward commands to output actions such that it can

26:41.760 --> 26:50.880
generalize and yield even more desirable behavior and so whenever these expectations fail it still has

26:50.880 --> 26:56.640
a new bunch of training examples from which it can learn to become a better translator of commands

26:56.640 --> 27:04.960
into action sequences so it's not some random exploration though it is a way of using pure

27:04.960 --> 27:11.040
supervised learning to achieve this reinforcement learning objective which is get a lot of reward

27:11.040 --> 27:18.560
and very little time and it's just learning through supervised learning to map these combinations

27:18.560 --> 27:27.760
of cumulative rewards within certain time intervals to action sequences and supervision comes

27:27.760 --> 27:35.600
from it's kind of a binary supervision achieved or failed to achieve the command based on some

27:35.600 --> 27:42.400
sequence of actions yeah and so there is we know for each behavior for each trial of the agent

27:42.400 --> 27:49.760
we know exactly how much reward did we get we know exactly which action sequence was there

27:49.760 --> 27:58.400
and in retrospect we can adjust the reward and time command such that it is compatible with what

27:58.400 --> 28:07.280
really was achieved if you know power play algorithms or hindsight experience replay algorithms

28:07.280 --> 28:13.600
or if you know a rather algorithm of set poor writer's group then you already know a lot about

28:13.600 --> 28:22.000
the principles of what upside down reinforcement learning is about in the paper how did you compare

28:22.000 --> 28:31.200
the performance of this approach to other approaches and maybe before that is do you envision

28:31.200 --> 28:38.880
this approach being applicable to the same or any you know reinforcing any of the kind of

28:38.880 --> 28:45.760
traditional RL problems or it does this approach with the you know the time and the reward as the

28:45.760 --> 28:52.080
inputs is is it particularly you know interesting or useful applied to a specific formulation of the

28:52.080 --> 29:00.240
RL problem yeah so Hu Pech Kumar Srivasava my coworker at Naysans and a former PhD student

29:00.240 --> 29:08.080
of mine and other colleagues at Naysans and also Editsia they came up with variants of what I just

29:08.080 --> 29:14.560
explained and showed that although we at the moment have just little pilot versions of this

29:14.560 --> 29:20.880
simple new type of reinforcement learning it already is able to beat certain traditional

29:20.880 --> 29:28.880
baselines on interesting problems the thing about this upside down reinforcement learning is

29:28.880 --> 29:35.520
that its limitations are exactly the limitations of supervised learning because basically we are

29:35.520 --> 29:42.000
translating reinforcement learning into supervised learning in a way that depends on these deep

29:42.000 --> 29:48.800
networks that have to learn this complicated mapping from reward commands to action sequences

29:48.800 --> 29:56.240
that lead to the rewards that has been requested within two time steps that I also

29:56.960 --> 30:05.120
given through the command and then this is a complex mapping and these many layers because

30:05.120 --> 30:13.440
you change the reward command just a little bit and this might lead to quite different action

30:13.440 --> 30:19.040
sequence that should come out and so on so we know how to train these deep networks and we have

30:19.920 --> 30:25.840
millions of tricks and supervised learning to do that in a good way and RL Pech and the others

30:25.840 --> 30:32.000
use that and use some of these tricks to make that really work but there is so much that we

30:32.000 --> 30:38.880
haven't even exploited yet so many ways of improving supervised learning which can all be applied

30:38.880 --> 30:45.520
to this in principle a very simple type of new reinforcement learning so it sounds like the

30:46.080 --> 30:52.320
when you you mentioned that you included comparisons on traditional benchmarks you know

30:52.320 --> 30:56.640
what categories of traditional benchmarks like the Atari games and that kind of thing or

30:56.640 --> 31:02.960
yeah in the current experiments the result is it works especially well when there are sparse

31:02.960 --> 31:10.160
rewards so when you don't get rewards all the time like in certain video games but only at the

31:10.160 --> 31:18.240
end of the trial which is actually the most challenging situation because if you don't get a lot

31:18.240 --> 31:24.720
of rewards during all your trials then there's not so much from what you can learn something which

31:24.720 --> 31:30.000
means you have this attribution problem of what was it that you did that resulted in the rewards

31:30.000 --> 31:38.160
that she did get the credit assigned problem exactly and some ways the most challenging type of

31:38.160 --> 31:43.280
reward maximization problem however at the moment we have just these pilot experiments and let's

31:43.280 --> 31:49.760
see where that goes at the moment it's also just an active tech report and it's it's useful already

31:49.760 --> 31:56.400
we see that but who knows how much better it's going to get once we use all the tricks of

31:56.400 --> 32:04.320
supervised learning to to improve further the current pilot implementations. And so is that being

32:04.320 --> 32:10.480
presented here in a workshop or something like that at NURBS? Rupesh Kumar Srivasava is actually

32:10.480 --> 32:16.160
going to present that I think tomorrow. Okay awesome awesome well we will definitely look that up

32:16.160 --> 32:23.600
and link to the archive paper in the show notes and I'm looking forward to digging a little bit

32:23.600 --> 32:28.000
deeper and seeing what that's all about sounds really interesting. All right well you again thanks so

32:28.000 --> 32:32.960
much for taking a few minutes to chat with us about you know what's new what you're up to looking

32:32.960 --> 32:39.200
forward to connecting once again in the future. Thank you so much for having me and it was a pleasure

32:39.200 --> 32:47.840
I'm being here. Absolutely thank you. All right everyone that's our show for today. For more

32:47.840 --> 32:55.840
information on today's show visit twomolai.com slash shows. As always thanks so much for listening

32:55.840 --> 33:10.800
and catch you next time.

