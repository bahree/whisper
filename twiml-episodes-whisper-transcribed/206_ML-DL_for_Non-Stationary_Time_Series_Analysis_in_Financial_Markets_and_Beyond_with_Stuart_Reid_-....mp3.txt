Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Before we jump into today's episode, a big thanks to everyone who listened to shared
and commented on our AI platform series.
As always, we love hearing your feedback and we've received a ton of it on these shows.
Stay tuned for part two of the series, early next year, and if you haven't yet, be sure
to sign up for my upcoming series of e-books on the topic, which we'll be releasing soon.
Finally, over the next few weeks, I'll be bringing you great interviews from the AWS
Reinvent Conference, which I'm at right now, NURRIPS and CUBE CON.
And I'd love to connect with any listeners and attendants, so please shoot me a message
via at Sam Charrington on Twitter, email or thetwimlai.com website if you'll be around.
Today we're joined by Stuart Reed, Chief Scientist at Numerical Research.
Numerical, based in Stellanbaugh, South Africa, is an investment management firm that uses
machine learning algorithms to make adaptive, unbiased, scalable, and testable training
decisions for its funds.
In our conversation, Stuart and I dig into the way Numerical uses machine learning and
deep learning models to support the firm's investment decisions.
In particular, we focus on techniques for modeling non-stationary time series, of which
financial markets are just one example.
We start from first principles and look at stationary versus non-stationary time series,
discuss some of the challenges of building models using financial data, explore issues
like model interpretability and much more.
This was a very insightful conversation, which I expect will be useful not just for those
in the Fintechs base.
Enjoy.
All right, everyone.
I am on the line with Stuart Reed.
Stuart is the Chief Scientist at Numerical Research.
Stuart, welcome to this week in machine learning and AI.
Yeah, thanks for having me, Sam, it's really great to be speaking to you.
I've been listening to the podcast for a long time now, so it's great.
And I'm really glad we were able to connect.
This one has taken a while to put together for a variety of reasons.
We initially connected around the time, kind of in the run-up to the deep learning and
dava, which you participated in.
And for whatever reason, it's taken us a bit to connect, but welcome once again.
Yeah, thank you very much.
It was a pretty busy week during the deep learning and dava, so I'm not surprised that
it took a while, but good to finally be speaking to you.
So why don't we get started with a little bit of your background.
You are currently focused on applying AI to finance.
How did you get here?
Yeah, so I think that my interest in technology itself predates my interest in finance.
I was actually the youngest South African to get an amateur radio license when I was 11.
And that's kind of when I got into technology.
But soon afterwards, I discovered finance, started out with the traditional books by Warren
Buffett and Benjamin Graham on security analysis and how do you actually value companies.
And then that slowly shifted into some of the more modern approaches, taken by lacronase
on technologies in D.E. Shaw and AQI.
And that's really quantitative finance.
So that interest is actually what propelled me to study computer science in the first place.
And you're now, again, chief scientist at numerical research, what does numerical research
do?
Yes, a numerical research is a startup financial services provider based in still in
Barcelona, Africa.
We have two investment funds which are run entirely by machine learning algorithms.
So that's what we do.
We're using deep learning algorithms for the most part to predict what is going to happen
in global financial markets, which sounds like the best idea ever, but it's incredibly
challenging for a number of different reasons.
And we will get into those.
The kind of way you framed your presentation as I understand it at the end of was around
the application of machine learning to kind of the broader class of problems that you see
in financial markets, and that is a specific type of time series analysis.
And you made a distinction when we were talking before we got started between stationary
versus non-stationary time series.
Can you elaborate on that?
Yeah.
So, you know, a stationary time series is really one which is sample from a distribution
which doesn't change.
So that fits quite nicely with your traditional machine learning paradigm, which is where
you are fundamentally assuming that your data generating process is constant or very slowly
time varying.
But financial markets are anything but stationary that continuously changing.
In fact, I would go as far as to say that the markets themselves are adversarial.
They don't really want you to succeed.
And this gets into the whole debate of like market efficiency.
So my focus is very much on how can we get deep learning algorithms which were designed
to work in a stationary environment under some reasonably strict assumptions?
How can we get those algorithms in our work in an environment where you can not only experience
a lot of non-stationary but experience extreme shifts in the distribution of the data that
has been generated.
So these kind of points are regime shifts, structural breaks, critical transitions, change
points, as many names for them in the literature, but I prefer change points.
And so what are the main challenges that you see in doing this, do you, are there, you
know, is it just hard or are there distinct challenges that you can point to in trying
to apply machine learning and deep learning to these types of time series?
Okay.
Yeah.
Now there are some very specific challenges, but let me first take a step back and say
that if you are interested in applying deep learning algorithms to financial markets, there
are more problems than just non-stationarity and kind of like this adversarial behavior.
You also have a very challenging problem with signal to noise.
So there's a lot of noise in financial markets.
In fact, most derivatives that are priced these days are priced using random walk models.
And that's kind of where my fascination with randomness comes in, but also markets are
challenging because of this non-stationarity.
But there are a number of different challenges associated with China predict time series
which can abruptly change from one distribution to another one.
And that's, you know, can we actually detect these change points in a timely manner?
And how many of these change points are actually going to occur? Is it a one-soft kind of transition
as you might see in some ecological systems or is it a multi-way kind of transition kind
of like we have in finance where you cycle between maybe a low volatility and a high volatility
regime or between a bear market and a bull market?
Then the next challenge is the duration of that change point.
So you can have change points which are extremely abrupt. Like for example, if we're using change
point analysis or machine learning algorithms to identify the onset of an attack on a network
like network intrusion detection, that's a very instantaneous kind of change in the distribution
in that network activity. Whereas in financial markets, it's actually generally a slightly
slower transition. And that can actually make it harder to detect because it's lots of small
changes which kind of add up and eventually become a very large regime shift.
Also another challenge is the extensiveness of the regime shift. Is this regime shift
or is this change point affecting the entire model which we've trained or is it just affecting
a subset of the model? Is it just affecting the part of the model which is looking at interest
rates or the part of the model which is looking at currencies? And then the other two challenges
is the magnitude. So very large or very small shifts and where those shifts are happening.
And then the certainty around this. So how confident are we that we've identified a real
change in the distribution as opposed to just an outlier or an anomaly in the data?
So those are kind of like the challenges that you're presented with when you're trying
to use deep learning algorithms to predict time series which are non-stationary and can
have these abrupt transitions. And when you're faced with those very distinct challenges,
do you attempt to kind of pick them off one by one or do they kind of together lead you to
a class of solutions to this general problem that has good properties for some subset of the
challenges? That's a very good question. I have definitely taken a very non-linear approach to
this set of problems. In fact, I've only realized that some of them are problems quite recently
and it's kind of like an ongoing area of research for me. But generally what kind of spurred
the interest was that we developed this framework at an numerical and we were training all of these
different models. And one of the early things I noticed is that the best models were generally the
ones which got the financial crisis right, like in terms of their predictions. So that motivated me
to take a look at those models and try and work out what made them get it right. And I think for
the most part it was luck for those models. But then I got really interested in these kind of
like change points because the reality is that significant changes in the distribution of
financial markets, not only affects how you should make your investment decisions, which affects
everybody's retirement savings and their ability to do things, but also policy changes. When we're
talking about how the government should decide to set interest rates or how it should behave in
situations like trade wars, you know, these kind of things. So my approach to it was very much
from an applied perspective. It was like, this is an interesting problem. And then I tried a whole
bunch of different things to try and solve that problem. And then slowly began to see that
it was a very common problem, which actually arises in many different areas, not just finance and
economics, but also in statistical quality control and manufacturing, speech recognition, medical
condition monitoring, you know, disaster prediction. If we're talking about like earthquakes,
network intrusion detection, there's a whole bunch of really interesting ideas. And then through
looking at all of the different applications, kind of piece together the theory and what the
challenges are when you're looking at these kinds of time series. And what's interesting is that
different parts of the literature from different domains are focusing on different subproblems.
Like duration of the change point is a much bigger issue in ecology, for example, than in
network intrusion detection. So I hope that kind of answers the question. It's a very good question.
I have no idea. I'm just kind of going at it and seeing what happens.
You mentioned that one of the ways you or one of the hallmarks of a good model is the ability
to predict the financial crisis, which kind of suggests to me that the main thing you're doing
to test here is kind of back testing against historical financial data. And I guess I'm wondering
is what exactly when we say a model or these models, what exactly are we talking about? Is it one
model that predicts the price of the S&P or some portfolio or do you have models for individual
securities or are you modeling some of the sub components that you mentioned like interest rates
and bond prices and things like that? Like how granular are the models that you're developing?
Yeah, that's a great question. So I think back testing is a little bit of a swear word in finance.
Is it? What did I just walk into?
So no, not at all. I mean, most people call it back testing. I'm not a huge fan of the term
because back testing kind of implies that we have some model. It has some parameters.
And what we do is we pick a whole bunch of different parameters. We run a simulation on
historical data and we see which one did the best. And then generally that's the one that we pick
going forward. And that's a very, very bad way to actually go about finding a good investment
process because inevitably what you're going to do is you're going to curve, but you're going to
you're going to memorize the historical data and you're going to pick very sub optimal parameters
going into the future. So what we prefer to do is really like a walk forward simulation three
time, which arises a whole bunch of additional challenges. One of the challenges that we
have in finance, which doesn't exist in in some areas is this issue of survivorship bias.
So for example, the S&P 500 as you mentioned today, like the constituents of that are not the same
as the S&P 500 from 1980 or 1995. You actually have stocks coming in and coming out and more
recently you have a lot of of stocks going out and and fewer like staying in that S&P for a long
period of time. So we have this kind of like non stationarity, not only in the time series
dimension, but also in the cross section. So like what stocks are we actually looking at at any
particular point in time? And we can't just pick the S&P 500 today and run a simulation on that
because then we've introduced a massive bias. None of the stocks in our simulation can fail,
which is obviously not at all reality and reality. You have, you know, enrons which
which completely fail and world comms which also completely fail. So the process that we've
taken has been quite systematic. It's like how can we construct data sets which are truly
representative of what the market looked like at that point in time? And and as far as what
models we're using go, we started off quite simple. Then we very much focused on the recurrent
models, recurrent deep neural networks. And lately we've actually had a lot of success with
convolutional neural networks for time series analysis which also seems to be picking up in the
literature. And generally we're looking at multi-variant time series prediction. And I think
that that's interesting because there is this whole covariance matrix that you're trying to
model. It's not just about, you know, if we can predict what the stock is going to do into the
future, it's how the stock is going to influence, you know, these other stocks in the universe and
how they evolve through time together as a as a collective. So as far as the number of models go,
we actually want to have many many different models and then we kind of stack them together to
generate better and better predictions. I think in our funds at the moment we've got about
2000 neural networks in production, which is a challenge in and of itself, the engineering
challenges is, you know, distinctly different to the theoretical challenge of how do you actually
get the model to work in the first place. And those are continuously generating a lot of
information, not just predictions, bad measures of their confidences in those predictions,
measures of their errors and the predictions. Yeah, so there's a lot going on.
Yeah. And can you speak it all to the granularity of those individual models or those 2000 models,
all targeting trying to predict the performance of some basket of securities or you,
I'm imagining you modeling different kind of underlying fundamentals. Is that the case?
Yeah, so definitely looking at different things in different models, but also using different
inputs into different models to generate those predictions. We're trying to come up with a very
diverse set of predictions, which we can then ensemble over. As far as the granularity of the data
itself goes, this is another challenge that you get in finance, which maybe you don't get in other
spaces, is that you have some really, really important data, which only comes out quarterly.
You know, if we're talking about like unemployment numbers or we're talking about GDP and, you know,
all of these things obviously have an impact on the markets, but there's people are looking at
them and they're using that to make investment decisions on like which countries they should be
allocating capital to and which sectors in those countries they should be allocating to and then
which stocks. But then you have this very high frequency data as well. So you've got price data,
which is continuously coming at you. You've got volatility data. And what you want is a model
which can actually weigh all of these time series, which may be occurring at very different time
scales together in an unbiased way. Yeah, it's a very hard problem. I'm not sure that we've got a
perfect solution to it yet, but we've tried a lot of different things. Ensembling is the most
obvious approach and it's something that's worked quite well for us. Are you also incorporating in
like natural language processing types of models or are trying to get at information that's
embedded in unstructured text and documents as well or social media things like that?
Yeah, this is a this is an interesting question because I'm actually a little bit of a sentiment
skeptic. That's not to say that, you know, the sentiments that are extracted are wrong. But just
consider the statement sales are down $40 million for the quarter. Now that is clearly a negative
sentiment, right? I mean sales they're down for the quarter. But the reality is that if the
analysts had expected sales to be down 60 million and then they were only down 14, the market would
actually probably rally in that situation. So I think that the challenge with sentiment analysis
is really that there is a lot of context which is very hard to capture. So we've spent quite a
bit of time working on natural language processing and building these kind of like sentiment scores
and including that in our models. And one of the other challenges with using deep neural networks
in finances is obviously the problem of black box, right, is that it's very hard to interpret the
models. And you recently had Sarah on your show. And I really enjoyed the the discussion that she
had about interpretability because some people in machine learning seem to think that interpretability
is a non-issue. But having spoken to many investors and many people who are interested in using
this technology, but from outside of machine learning and and computer science interpretability
is a big problem. So even if we did include these sentiment scores, it's actually quite
challenging to work out if the model is is using that data and if it's using it in an optimal way.
That's kind of like a segue into another like sub conversation. I mean the main technique we're
using there is really ablation studies and sensitivity analysis. But to answer the question,
yeah, we've looked at sentiment. I personally am a little bit of a skeptic, but that's maybe because
I can't tell what my models are doing. So the general approach that you're taking is one of
unsombling lots of models. Some might argue that you know what you're doing is kind of feature
engineering and with the sophisticated and deep enough neural network and enough of the right
data, the network could figure all that stuff out on its own and you should be looking at that.
Have you have you looked at that approach and like how do you respond to to that kind of
approach? I would say that that person is wrong. That's that's how I would respond. Okay, so
this is another interesting discussion. You are very good at asking questions. I'm very impressed.
You know, in the machine learning community, you know, data is considered to be like
unreasonably effective, right? It's like the more data you have, the better your model is going to be.
But my experience in trying to use deep learning and finance has been quite the opposite.
And I think that the analogy that fits at best is really that the markets are kind of like a
haystack. And there are a few needles in there which correspond to signals which you can extract
and actually profit from. And when I get more data, so I go wider, I'm looking at more time
series for more locations and whatnot. Generally, what I'm doing is I'm just throwing more hay
on top of that haystack. So one of the challenges that we have is that if I and we're looking at
about 400,000 independent time series at this stage, if I had to take all 400,000 of those time
series and throw it into one model, it would almost certainly fail. And the main reason why is because
because of two things. One is it's an incredibly wide data set. So you've got a lot of columns,
right? But the other problem is that because this data is inherently non-stationary and now we're
getting back to kind of like my main focus area, you know, a very small subset of that data if we're
looking at, you know, a time horizon is actually relevant for predicting what's going to happen next.
So your data kind of gets wider and wider and wider, but it's not necessarily getting deeper
because, you know, the market fundamentally changed two years ago and, you know, including data from,
you know, 2014 is not actually helping. It actually makes my models worse because it's learning
from a regime which is no longer representative of the data which has been generated now in the
process. So one of the reasons why ensembling is a good approach is because we can actually take
that very wide data set and carve it up into different subsets and give it to smaller models and
then kind of aggregate them and stack them in that way. It's very difficult to do feature engineering
with a very, very big model when your data is more wide than it is deep. I guess that's kind of my
answer to that question, but I accept the criticism. Maybe I just don't have enough data.
One of the things that you talked about in your in Daba presentation and if you're making the
slides available to anyone, they're really, really interesting and I'd encourage folks to
take a look at them and we'd be happy to link to them or post them someplace.
Yeah, I will definitely do that. They're just hyper animated in PowerPoint, so I've just got
to turn them into videos and then I'll do that. Okay. So one of the things that you talked about in
that deck was online learning which makes sense as a way to address this non-stationary
nature of the signals that you're looking at. Can you talk a little bit about the way that you use
online learning and what some of the challenges and discoveries you've made there are?
Yeah. Okay. So there are a lot of intuitions and what I'm working on with my supervisor,
I've actually decided to try postgraduate studies once again is to try and codify these ideas
and publish some of them. But online learning is for anybody who's listening is not familiar.
Really, when you have a machine learning model and it's trained on some data up to a current
point in time and then what we do is we walk forward through time and we're using that model and
we're just updating it on the most recent data which has happened. So there are a number of unique
challenges that you face when you're doing online learning which is when you do kind of one pattern
at a time or incremental learning which is when you have a model you move forward in steps and then
you update on the most recent end patterns or n plus some window size. It's an incremental
kind of batch learning and that's that by the time you have moved from let's say 2002 to 2007
you might have actually forgotten a lot of the stuff that you knew in 2002. The model is actually
forgotten a lot of information which means that when it enters into a regime such as the 2008
financial crisis it actually doesn't know how to deal with that. Online learning is on the other
hand a very good approach because we have these different regimes and if I had to just continuously
include all of the available data and kind of like an expanding dataset fashion then my model would
actually struggle to distinguish between the different regimes unless I had some sort of indicator
of what regime the data was was being stumbled from. That's one of the challenges one of the other
challenges which we were talking about in the office today actually is is for example regularization
and this is I don't know if it's published in literature and I'm not sure if I'm 100% right in
what I'm saying but different parts of the information which we're training our models on
matter at different points in time. So if we have to take a neural network and we have to train
it on let's say you know 2000 time series and the first 500 time series were not relevant from
the period 2002 to 2004. Slowly but surely the regularization term in that neural network would
push those weights very close to zero right but let's say from the period from 2004 to 2006
those first 500 time series which weren't relevant in the past now all of a sudden become
relevant in in the future then we have a challenge which is that you know we've actually pushed all
of these weights very very close to zero and now we actually want to grow those weights again which
which is just something which the models seem to struggle with through time and the other
challenges that maybe the last 1500 time series are now not relevant from 2004 to 2006 and it pushes
all of those weights down to zero. So a lot of the assumptions a lot of the ideas that we can apply
successfully at a particular point in time. So if we just have to train a model on a data set
deployed into production can cause problems when we're doing an online or incremental learning
approach. One of them is regularization so you know that we tend to to bias ourselves towards
you know dropout kind of approaches as opposed to L1 or L2 regularization but there are a lot of
small issues like that. One of the topics that you mentioned early on but we haven't really
died into yet is the types of models that you're using and that seems particularly relevant to
this discussion. You know we think of kind of time series and models with memory and some of the
comments you're making about you know models remembering and forgetting things I tend to think of
recurrent networks but it sounds like you're shifting increasingly to using CNNs. Where does that
memorization element come from in a CNN? Yeah so none of them are particularly good at remembering
data which is very far in the past. So let's say we we have our data and it goes from 2002
all the way through to 2008 and we're training on two years worth of data or four years worth
of data at a time. By the time we reach 2008 we're only really updating our model on data from
2004 you know we've forgotten 2001 2002 we've forgotten what a financial crisis looks like
and whether you're using a convolutional neural network or an LSTM or a grue or you know just a
feed for neural network you're going to run into similar problems because that regime is no longer
in the data that you're training on. So you will forget it. So one of the techniques that I'm busy
developing with my supervisor is kind of this idea of storing historical versions of your models
in some sort of like explicit memory bank and then like let's say you find 2008 and you're like
oh my model is struggling and we can identify that is struggling by looking at you know change point
analysis and we can say all right well there's been a significant change in the data our historical
data from 2007 and 2006 is no longer relevant what do we do either we re-initialize the model
completely at that point in time and we just hope to hold that it learns or what we could do is we
could actually go backwards in time and we could say well is there a point in time historically like
from a long time ago where we learned something we learned to representation which is relevant for
the regime that we're in now I hope that makes sense. No it makes a lot of sense and I've heard of folks
doing similar things from kind of at a model management perspective so you've got some model
in production and you've got kind of a constant evaluation system and it determines that
the models performance has degraded as opposed to you know just immediately triggering a
retrain you know what some folks have done at least is to look at the historical models that have
been in production and kind of test the current regime against those models and see if you know
as opposed to just us moving into new territory we've reverted into territory you know for which we
previously had a model and you know switched to that one precisely so that's exactly kind of the
approach that I've been working on lately although it sounds like in this case at somewhat a lower
level in a sense of at the granularity of kind of the weights of the model or model you know sub
components as opposed to kind of fully rolled models in production or something like that.
Yeah so we're talking about like the weights right so what we would do is we would walk forward
through time and we would actually drop these weights onto some sort of explicit memory so that we
can load them at a later date. Now what I'm thinking and I haven't built anything like this yet but
it's just like an idea that's in my head is really about whether we can model the transitions
between those because you can kind of think of the states that we're dropping onto this memory
as as being really states in like some sort of like Markov process and kind of like where I'm
heading with my researchers whether we can model the transitions between these states and actually
use it for simulation as well but that's just an idea. Interesting interesting.
And so kind of going back to the application of convolutional networks in this case.
So you're you're feeding the the CNN yeah let's call it a frame. Is that frame kind of a
single point in time across a single time series or a single point of time across multiple time
series or a historical frame that contains some time segments across you know one or more
more time series like what what goes into the CNN? Oh yeah so we're we're generally using like
dilated convolutional neural networks which means what? So ones which are preserving the
the temporal structure of the data so we're never feeding in you know historical or future data
into historical notes it's it's always kind of like wave net other than if you've seen the diagrams
on on Google's block. But what's going in is really an image of where you've got your stocks
on you know the on the columns and you've got a number of days at the bottom so we're feeding in
kind of like a picture which is a whole bunch of time series put together you know column by the
amount of time yeah I hope that makes sense. So if it's like 500 stocks and we're looking at you
know 40 days worth of data at a frequency of one data point per day then it would be a 500 by 40
image which is basically going into that convolutional neural network. And so the the difference between
that kind of situation and something like an LSTM is that your memory if you will is kind of limited
by this fixed window as opposed to some some thing that kind of sticks around a varying degrees
indefinitely. Is that right? And what are the implications of that in the way you you model?
Yeah so that is definitely true and I must say that I was surprised myself that the convolutional
neural networks generally perform quite well. I wouldn't have expected that but we don't find a
huge difference in the performance between our best convolutional neural networks and our best
LSTM neural networks or between the best like grooves. Generally they're all performing quite
well what makes a much bigger difference than architecture choices the choice of other hyperparameters
like how much regularization are we are we adding what kind of dropout are we using how many layers
are we using because the challenge with the data that we're looking at is because it's non-stationary
we can't look at all of the historical data we can only look at the historical data that is relevant
so sample from the same regime that we're in which means that if we have a very very over
parameterized network like extremely large or extremely deep and then we shift into a new regime
and we don't have that much data that model actually can't converge with the amount of data that we
have so what matters far more than the choice of architecture in our situation has really been
the hyperparameters that we're choosing for this I mean I'm pretty sure I could tune an LSTM
to beat our best you know convolutional neural network or vice versa I'm not sure if that comes
back to the free lunch theorem or what they generally work differently and because they work
differently they can really come up with predictions which are hopefully uncarlated and then when
combined in an ensemble or in another neural network downstream actually generate better and
better predictions so beyond no free lunch implications would you say that it also has something to
do with the fact that your regime durations are short enough to be kind of captured in your
CNN window as opposed to you know something that might have longer maybe a longer tail.
Yeah perhaps it's it's hard to say this gets back to the interpretability discussion
maybe we should ask Sarah I'm not sure yeah so we we started down this path in talking about
online learning and one of the the challenges that you raised in your slides was this issue of
kind of weight transfer and the ability to kind of capture knowledge and project it forward to
to the next time step how have you kind of dealt with that am I am I capturing that that issue
correctly the weight transfer yeah so I think you know when we're doing online learning we don't
want to continuously reset the model so we don't want to reinitialize it from scratch at every
point in time especially if the model that we had at the previous point in time with the previous
batch is actually relevant for for where we are now in time so in that situation we would like to
transfer the weights from the previous model to the next model but that doesn't hold when we're
talking about a change point so let's say you know something happens and there's a massive
structural break in financial markets right so now the previous model we have is really learned
to representation of the world which is which no longer exists we've we've moved on from that
when we transfer our weights from the previous point in time to the next point in time
we have a few options right so the first option is something we spoke about earlier which is
really going backwards in time and trying to find some optimal weights which work and then
transferring those into the model but another approach would simply be to reinitialize the weights
completely which is extremely detrimental in the situation where you had a false positive so you
made a prediction that there has been a change point that you've shifted from one regime into
the next but you didn't now all of a sudden you've reinitialized your model you've forgotten
everything that you learned previously and all for nothing so one of the things that we've we've
also been playing around with this kind of like partial reinitialization which is this idea that
we take our weights and we actually pass them through some sort of noising function so we add a
little bit of of randomness to those weights at every single point in time to kind of keep them
fresh keep them alive and also give the model the ability to remember some of what it's learned
in the past but not all of it I like to call it kind of like optimal brain damage I know that
there is actually something in machine learning called optimal brain damage and this is not it
but I just I just love the name it's kind of cool it's like basically have a model you hit it
on the head and and you hope that it learned something a little bit better than what I knew in the
past all of these ideas are just different things that we've tried because I think that you know
to understand where we're coming from is you know we you know deep learning is like the solution
right so we we took these models and we applied it and we realized quite quickly that financial
markets don't care how smart to model is they don't care how you know deep the maths is or or how
you know optimal they work on on image net or on speech recognition problems the market is
is this adversarial very complex system that's gotten on stationarity and the cross section in
the time series it experiences these change points which can be partial affecting part of the
model or full affecting you know the full model itself and they occur at kind of like a regular
frequency you would be surprised at how often they occur so all of the ideas that we've tried and all
my whole talk at the deep learning and Darva was really just a presentation of a whole bunch of
tricks that we've tried and some of them have worked particularly well and some of them have not
worked at all I tried to focus on the tricks that have worked as opposed to the tricks that happens
but yeah that's really where all of this is coming from and I think like going forward I'd like
to formalize you know some of those ideas and publish them yeah well one of those tricks that caught
my eye was using reinforcement learning as a way to I guess control the way you ensemble these
models or control the can you talk a little bit about that yeah sure so as I mentioned before what
matters more than the choice of model is the choice of hyper parameters so for example if we're
using early stopping which you know some people don't like I'm quite a fan of it and the main
reason why is because I'm doing continuous learning through time I'm not training my model one
some training at you know maybe three thousand times you know the there's a parameter there which
is the patient so how many epochs are you willing to see without improvement before you just stop
training and there is other parameters like your learning rate and both of these parameters your
patients and your learning rate should probably change if you experience a different regime or if
you enter into a different state if you think that you've moved from you know you know one regime
into another regime it might actually make sense to increase your learning rate because you want to
get into you want to push your weights to a place which matches the new regime quicker or maybe
what you want to do is you want to train for longer because you want to give your model more of an
opportunity to actually fit the new regime I hope that makes sense so you can kind of think of
these as the actions that a reinforcement learning agent can take so it can change these hyperparameters
in the models as we walk forward through time depending on the state which it observes so we're
really using it as kind of like a meta optimization framework around each one of the individual agents
in this in this massive ensemble and that's it's been something that's been quite fun and it works
quite well it's not nearly as sophisticated as some of the reinforcement learning that's coming
out these days but yeah it's it's worked quite well and that's also a better a cheaper approach
than just creating a bigger and bigger ensemble right because what I could you know what you could
argue is well why don't you just create more agents with the you know higher and lower learning rates
or different patients and just grow that ensemble and make it bigger and bigger and the main reason
why I would want to use reinforcement learning instead of just making this ensemble bigger and
bigger is because I only have so many computers right and I have this this thing called a hard drive
pulls up very very quickly when I'm training these models I mean we're producing about 200 gigs
of data a week luckily we can delete a lot of it the next week and we can compress a lot of it
but it's a it's a challenge to actually train massive ensembles like this what are the other
interesting tricks that I came across in your in your slides relates to kind of the challenge of
identifying fundamental patterns in time series data when you can have differences in frequency
and magnitude but kind of the same underlying shape and it can be difficult for networks to
to figure that out and I hadn't come across this notion of dynamic time working before
yeah it's a very traditional technique I think it's been used since the 70s okay there was a paper
which came out I think in 2017 actually proposing and a loss function for neural networks which
incorporates the idea of dynamic time warping but essentially that I mean just to quickly explain
the idea is that if we have two time series which are exactly identical in the sense that they have
the same waveform and they just occur over different intervals like let's say we're looking at audio
because it's mostly been used for speech recognition and I say the word apple and then I say it really
slow apple the waveform of me saying apple is the same but the duration over which it occurred
is different so it's a different time scale and one of the challenges with that is that if I
to do a Euclidean distance between those two waveforms I would say that those two things were very
different when in fact they're actually very similar they're the same thing they just were said
in a different way so the idea behind dynamic time warping is really that you that you drop the one
time series onto the one axis of a matrix and you drop the other time series onto the other
axis of a matrix and then you use a procedure to actually draw a connection between between the
top right hand corner and the bottom left hand corner now I can't remember you know for the life
of me the exact details of the procedure but the idea is that it's a it's a more optimal measure
of similarity between different time series and one of the things that we've been looking at for
that is really in time series clustering so this is this idea of if we have a time series and
we chunk it up into different sub sequences you know can we measure the similarity between those
things and kind of group them together in times time series clustering is another approach to change
point analysis that's one of the tricks which has not been very successful it's very computationally
expensive and so I think that some of the papers which have come out on how to incorporate that
into a loss function is maybe a more interesting approach have you implemented any of those
papers is that what you're doing or are you doing it more procedurally like you mentioned yeah now
we're getting into like some of the more secret source kind of stuff that that we have it with
but yeah loss function engineering is important um I'll just leave it at that okay interesting
interesting yeah when you when I saw that in the slide I envisioned something that you do kind
of as a you know maybe even pre-processing or kind of windowing or something like that but
the idea of doing this building this right into a loss function is kind of interesting
yeah I mean there there are papers out there about it you know I recommend people going Google it
we implemented that a while ago that was probably almost 18 months ago that we looked at that
so we have touched on just a few of the tricks in this set of slides and we're starting to run out
of time you also mention and discuss in the slides time series embeddings embeddings have come a
very hot topic of late can you talk a little bit about how they apply to time series yeah that is
a great question I'm a huge fan of of of order encoder is variational order encoders and also
you know your your traditional kind of dimensionality reduction techniques multi-dimensional scaling
PCA ICA FCA all of these techniques are quite useful at taking time series which have a lot of
redundancy so there's a lot of of correlation and actually reducing it down into a lower
dimensional space which really captures the statistical properties that are relevant so the
reason why we do that is because there is a lot of correlation in financial markets and I think
that I don't want to get into the debate of causation and the best way to to go about that because
I'm not sure but I think that is difficult for the models to really assign importance to the
the different inputs when all of them look very very similar so let's say you had stock A B C
and D and we know for a fact that you know through domain knowledge or whatever that A is influencing
D but maybe A is is also kind of influencing B C as well so maybe the model would look at the
correlations and say well B is maybe the influencer of DNA to apply you know the weights in that way
it's a bad explanation but it's very hard to tease out what is causation and what is correlation
in the data and and financial market data is incredibly correlated so what we've been using the
time series embeddings for is really to reduce this highly correlated space down into another
subspace which has nicest statistical properties to predict and the nice thing about decoding or
auto encoding is that you then have a network that if I had to generate a prediction from this
latent representation I can actually then get it back into the original space so maybe I have like
2000 time series which are all very similar maybe they're the Russell 2000 at this point in time
they're all driven by similar market forces you know of Trump decides to tweet about something
you know they're all going to move in in similar ways and what we can do is we can really squeeze
that down into a latent representation which really captures the salient features which are useful
for prediction but the other nice thing about those features is that they have good statistical
properties especially if we're talking about a variational autoencoder and then what we do is we
decode back into the original space after we've generated our predictions in the latent space
I hope that that makes sense you can do it much more cheaply using principal components
and those kind of more linear techniques but I'm just very partial towards
ordering encoders and variational ordering encoders and so do you end up with essentially a time
series of these or time series in this embedding space or are you is the embedding somehow
are you using that more statically that question makes sense yeah so what you end up with
is another time series but just of much lower dimension and preferably of better statistical
quality which we can then generate our predictions in but it's it's still a time series
and it kind of like preserves the the temporal ordering the ordering of the data
so there are a few architectures that you can use for that and then you would you would use
these embeddings as inputs to your other neural networks the the way you might with other
kinds of embeddings yeah precisely and then it generates a prediction in that space and then
we can actually decode back into the original space because there's no point in predicting let's
say you know the first two principal components or predicting you know this five dimensional
latent representation learned by an autoencoder because I don't know how to make decisions off of
that so we need to be able to actually get it back into the original space so that we can make
decisions about that because it's it's no good knowing you know what's going to happen to the
first principal component or to the first dimension in this latent representation
how does that help me make a constructed portfolio that somebody can invest in
yeah but the main reason for doing that is really just that the statistical properties
that you get out are better than the statistical properties you put in you don't have as many
issues with cointegration and correlation between the time series it doesn't help with regime
shifts which is the main focus of my research but it it has helped in improving the accuracy
of the models very early on in our chat you mentioned that one of the techniques you use to
kind of optimize these models and and kind of explore these tricks is ablation studies
uh is that worth a quick comment before we wrap up? I'm a huge fan of ablation studies yeah but
I mean basically the idea is that if you've architected your neural network in a particular way
you can actually switch off parts of that neural network and measure the deterioration in your
models performance in the same languages whatever loss function you specified and that's particularly
useful when we go to investors or or somebody who would like to understand or have some confidence
in the models that we're using we can say well listen we can't tell you exactly what the functional
form or or the exact decision boundaries look like but what we can tell you is that you know these
are the inputs which are contributing most time model at this point in time and uh and I stress
that at this point in time because the ablation studies again are applied in this kind of like online
learning setting and what's really interesting for me is just how how much they change I mean
you have at certain points in time some variables are just absolutely you know if you took it out of
your model your model would be useless um but then fast forward a year or two into the future I mean
that variable is is it might as well be white noise uh going into the model and some other variable
is now driving you know the performance of the model so what's really interesting to me is really
the dynamics of the neural networks what inputs are mattering at what point in time and uh and then
what I'm particularly interested in and something I haven't spent a lot of time on unfortunately
is then seeing how those kind of measures of are variable importance uh match up to things like
your your business cycle um you know if we're at the end of the bull run you know do we see that
certain variables which we would expect to matter mattering more uh and and really seeing with
or not we can actually test economic theory using neural networks um and and go beyond just
fitting a function uh but actually trying to understand what that function does uh I hope
that makes some sense but I'm a huge fan of that particular approach and I think you know anybody's
interested in in interpretability should check out some of your previous podcasts but that's
the approach that that we found the easiest to implement uh and the most useful from um from an
insights perspective we're not necessarily using that uh for any decision making or to improve the
model but simply to understand what is going on in that model um yeah awesome awesome well Stuart
yeah any uh words of uh advice or pointers or kind of recommended resources for folks that are
interested in the application of uh machine learning and deep learning to these types of time
series whether in finance or any of the other uh domains that you rattled off earlier yeah I
think that um you know there are a lot of really really cool applications in time series analysis
and I think that there are very strong statistical motivations to spend some time looking at change
point analysis and regimeships but also many applied motivations so I'd recommend students who
are listening to this to really you know you know pick up the calls and and and do some research in
it but as far as advice goes I'd say uh you know expect to be unexpected uh you know we expect to be
surprised um because financial markets like I mentioned earlier are almost adversarial in the way
that they behave uh and many of the things which we believe work uh in machine learning I'm not sure
that they do work uh and you start to identify the cracks in the arguments when you apply these
techniques to problems which they were not originally intended to be used for um so yeah I would
just say you're going to be uh surprised at what you see and um and also always be prudent
don't train a model and then see that it's getting 75% accuracy is predicting the S&P 500 and
take all of your money and put it into that model because I guarantee you it is wrong
uh with that level of accuracy uh if you're getting anything above you know 55% accuracy you probably
have a bug um yeah that's just the reality of of the game um awesome yeah we'll start it's a
sombring end sorry to not being like yeah it's great you should go and apply machine learning
to finance is hard but it may save a listen or a ton of money all right I'll I'll be very happy
if that is the case also interesting people who do this full time that's what I'm that's what I'm
suggesting Stewart thanks so much for taking the time it was great chat yeah thank you very much
and it was great to chat to you all right everyone that's our show for today for more information on
Stewart or any of the topics covered in this show visit twimmel ai.com slash talk slash 203 if you're
a fan of the show and you haven't already done so or you're a new listener and you like what you
hear please visit your apple or google podcast app and leave us a five star rating in review your
reviews help inspire us to create more and better content and they help new listeners find the
show as always thanks so much for listening and catch you next time
