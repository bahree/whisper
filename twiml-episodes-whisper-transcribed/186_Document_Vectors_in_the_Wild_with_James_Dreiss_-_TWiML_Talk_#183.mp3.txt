Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A couple of weeks ago I spent some time at the Stratidata conference in New York City, presented
by O'Reilly and Claude Error, and over the course of the next few shows we'll be bringing
you my interviews with some of the awesome speakers that I had the pleasure to meet with.
In this episode of our Stratidata series, we're joined by James Drice, senior data scientist
at International News Syndicate Reuters.
James and I sat down to discuss this talk at the conference, document vectors in the
wild, building a content recommendation system, in which he details how Reuters implemented
document vectors to recommend content to users of their new infinite scroll page layout.
In our conversation we take a look at what document vectors are and how they're used,
how they tested the accuracy of their models, and the future of embeddings for natural
language processing.
Before we move on, I'd like to send a quick shout out to our friends at Claude Error and
Capital One for their continued support of the podcast and their sponsorship of this series.
Claude Error's modern platform for machine learning and analytics optimized for the cloud
that you build and deploy AI solutions at scale efficiently and securely anywhere you want.
In addition, Claude Error Fast Forward Labs' expert guidance helps you realize your AI
future faster.
To learn more, visit Claude Error's machine learning resource center at cloudera.com-ml.
At the NIPPS conference in Montreal this December, Capital One will be hosting a workshop focused
on challenges and opportunities for AI in financial services and the impact of fairness,
explainability, accuracy, and privacy.
A call for papers is open now through October 25th.
For more information on submitting, visit twimmelai.com-c-number-one-NIPPS.
And now onto the show.
All right, everyone, I am in New York for the Stratocomference and I am here with James
Drice.
James is a senior data scientist at Reuters.
James, welcome to this week in machine learning and AI.
Hi, Sam.
It's great to be here.
Thank you very much.
Awesome.
Awesome.
So, James, you've got a background in art history.
You've studied economics at graduate school.
You currently work at Reuters doing data science.
Tell us a little bit about your background.
Yeah, sure.
So I think it is sort of a weird background of the art history and English and sort of
the humanities.
But that's what I studied in undergrad and I sort of kicked around in those worlds for
a while in New York City.
And eventually I started a nonprofit with a friend that was focused on international development.
And that is what led me to grad school.
So I studied econometrics and statistics in London.
And that was sort of my first exposure to, you know, this sort of world of data science
although at the time it wasn't really talked about as much.
And it also first exposed me a little bit to programming.
We use something called STATA, which is like a SAS sort of, you know, statistical software.
But you do do some coding.
So that was a really good sort of skill acquisition period.
And after school, yeah, I learned art.
And there was this great job at the Met Museum in New York that opened up and they were
hiring for predictive analysts.
And that was, so given my art history background and also having studied what I studied, this
was sort of a perfect combination of those two things.
And yeah, that was great.
I mean, it's, you know, you work at the museum sort of ruined me for any future office.
That was awesome.
Yeah, it was really short.
Like, you know, like, whatever, like, I don't really care, like, you know, if you have
any like, Fuseball tables you have, or a bigger like one-coil tower is because, you
know, I'm like, I can go like, look at like five premieres and my, you know, my lunch
break.
So that was amazing.
And was your, was your focus there on kind of operational analysis or marketing analysis,
or was it art related?
It was, it was, yeah, more, more focused on administration and specifically fundraising.
So trying to match donors with the museum, people who really love the museum and have
capacity to give, and yeah, it was interesting because the data there is really, really rich.
You know, I work in web analytics now, but, you know, there it's, you have postal addresses
and, you know, these people really love the museum too.
So it's not, they're happy to work with you.
So that was great.
But yeah, a few years ago, I moved to Raiders and that's where I am now.
So we've done a lot of stuff we had a big redesign in the website last year and built
it out, the design platform and there's been a lot of stuff in NLP, obviously.
And yeah, I'm here giving a talk tomorrow and that's, that's what I'm focusing on.
Fantastic.
And for those who don't know Raiders, what does Raiders do or what is the part of Raiders
that you work with?
Yeah.
So Raiders is a news organization.
It's an international, there's a lot of, do a lot of syndicated news.
So we have reporters all over the world stationed in, you know, many, many different countries.
So wherever there's breaking news report, it will be their end filing stories.
And they go to different outlets.
But we also obviously publish ourselves.
And it's, you know, we've been around for well over 150 years, I think.
Awesome, awesome.
And so the talk that you're giving tomorrow is called Document Vectors in the Wild, building
a content recommendation system.
Tell us a little bit about the motivation for this project.
Sure.
So last year, we decided to redesign the website and to move to something called
an infinite scroll model.
A lot of online publishers have moved to this model.
Basically, it's one where you click on an article and then you read it.
And then you have a string of articles that come after it.
So it's a really, it's a big misdemeanor because it's, you know, it's, of course, it's
not an infinite.
In fact, we just, we wanted to just give five to ten articles for the user.
And yeah, so my job is really to figure out what articles to give to them.
And specifically, so we, we don't have registration on the website, meaning we're really, we're
not doing too much personalization, about 75% of our users have two or fewer articles
that they read.
So it's really a content recommendation system.
So we did some tests previously and found out that if a user is on a page, you know,
if it's a business page or tech page, they're going to probably want more of that content,
which it makes sense.
So we started out thinking, okay, we want to find some more content into the slide programmatically.
And the other challenges were, well, you know, news is breaking and happens quickly.
And editors can always provide tags metadata to the articles.
So essentially, we need to do categorization just purely based on the text of the article.
And maybe some tags as well.
So the challenge is really how do you get at specific sub topics within these larger
topics that we have like tech and business, and how do we find articles about discrimination
and tech, which is not something that would necessarily be tagged by an editor.
Okay.
Yeah.
And so presumably based on the talk title, you did this using document vectors.
Yes.
We've talked quite a bit about word vectors.
What's a document vector?
It's very similar to a word vector.
It's sort of like a sidecar or two word vector.
In fact, the document vector is treated as another word.
So yeah, we, you know, this is like the problem in NLP is that if you just tokenize tags
and you're just looking at the trading the words as these atomic objects, you're losing
so much information, you know, if a build a vector that has the word president and another
one that has the word Trump, you know, those two vectors can't, they don't know that Trump
is president, which is, you know, good for them, but that's, we need to know that sort
of thing.
So, yeah, I mean, word embeddings, which encode semantic meeting, get at this, but the
challenges for us were that we have, the length of our articles are highly variable.
We have really short articles, really long articles.
So doing something like just taking the first hundred words of an article and taking
the embeddings for those words, wasn't really going to cut it.
And you know, having done some research into document vectors, I also knew that performance
in these sorts of tasks was higher than just using, you know, word embeddings that were
cut off at a certain length or average word embeddings or that sort of thing.
So this is something we wanted to explore.
Is that pretty typical for using word embeddings to cut them off at a certain lengthwise
their requirement to do that?
I think, well, if you wanted to compare two articles that were different lengths, you
had to, you know, they had to have similar sizes, the same sizes.
So I'm not sure how wide that is in the industry, but document vectors allow you to obtain
vectors that are the same size, where the articles would be different lengths.
And so how do you go about creating these document vectors?
So word vectors, the way, you know, I'm sure your audience knows how they are graded, but
it's, you know, they're very similar.
Document vectors are very, happen in very similar ways.
Essentially, you know, word vectors are getting at a semantic meeting of words.
I usually call them, I treat them as, you know, their address is basically in this multi-dimensional
space.
So you know that the word dog and the word cat are close together because the model on
which they were trained determined that their co-occurrence happens frequently.
So in the model specifically, it's calculating the probability of co-occurrence and that's
the output layer.
And the middle layer is the embedding layer and that layer is being maximized.
So it's sort of set up as this fake prediction task where you're predicting words from
context or the opposite.
And as a result, you have these weights that are richer with all that semantic information.
Now word, excuse me, document vectors are very similar.
You're doing, the training is virtually the same, but you're just adding an additional
vector at the input layer that is representative of the document.
So you train in a similar way, you have a context window, you sort of scan across the
article, the document, you know, doing the predictions.
And at the end, after you've gone through the entire corpus, you have this additional
vector that is sort of picked up all of the semantic information that was not picked
up by the word vectors.
And you can make these comparisons between articles, between documents, because all the
documents have shared context and words.
So if they have shared context, then the article should be similar.
So you're doing, you're able to do kind of similar vector operations on these documents.
You know, there's the classic word embedding, our word vector examples like, you know,
man, king, woman, queen, kind of thing, you're able to do those similar kinds of things
with documents.
Yeah.
You're able to compare to articles where they don't share any, to any pieces of text that
may not share any words at all in maybe different lengths, and you can get it.
You can know that they have similar meaning, and that's how we got at these subtopics.
There are some sort of peculiarities with them, especially with inference.
So when you want to make a prediction for a new document that you haven't seen before,
you're essentially running a new training step, an additional training step.
Or maybe many 10 to 20 training steps depending on how many of us you had when you're training
the original model.
You're also holding the word vectors and the output weights constant.
So you hold this constant, you do an additional training step, and this new document vector
that was initialized randomly comes to, you know, it essentially picks up all the information
that is present in the context and the output layer.
Can you elaborate on that?
This is when you're during inference time, you're also, there's a training step?
Yeah, there is.
Essentially, so that new vector is initialized randomly, the other weights are held constant.
And it's sort of strange.
Essentially, you're going to have a new document vector that you're obtaining is going
to be a little different each time if you do new references, which is a little strange,
but there are ways of getting to sort of do like sanity checks to know that, okay, this
new document vector that I just inferred should be similar to itself essentially.
So once sort of sanity check is, you have your model, you've trained it, make inferences
on all the training documents that you trained on, and then ensure that those new inferred
document vectors are, their nearest neighbors are themselves.
So if that's the case, then your model should be trained correctly.
And so how did you get from these document vectors to the categories, to the larger topic
categories?
Or to the subcategories?
So they're not really, we don't have labels for the subcategories, it just sort of
happens.
It's a result of cutting the model, yeah.
Clustering in this document space and then manually associating a label with individual
clusters.
Yeah, so we put the unsupervised learning model in production, which is, I think, some
unique and mostly often embeddings are used for downstream tasks as part of the future
of the model, but we're actually using the cluster strictly.
We got at, we tried to ensure that it was accurate and I can get into that, but one
thing that we did, so we're only make, when we rank the article scrolls, we're ensuring
we're just, we're just pulling all tech articles, for instance, and then within tech we rank,
okay, based on the very, the article that these are requested, what are the nearest articles
that are, you know, within the last 24 hours that are also tech, and, you know, there's
also like a popularity filter there as well, so.
So the example you gave earlier where you were interested in a subtopic discrimination
in tech, for example, that subtopic is never explicitly identified.
It's just based on, it's determined based on this, the document space.
Right, exactly.
And we can go back and look at the labels that were applied by editorial after the fact
and get that, that sort of thing, but, you know, similarly to something like LDI, like
it's not, you know, that's not, there's not an automatic assignment of label, like this
is discrimination tech.
You have to sort of, if you're looking at trying to identify subtopics in LDI, you're
really just looking at the words, you know, that were assigned for that topic.
So, it's a similar sort of thing.
You were mentioning some of the testing that you were doing around this.
Yeah, it was interesting, so, you know, measuring performance of unsupervised learning models
is always a little tricky, there's always some eyeballing that goes, goes on, you know,
like something like doing the elbow method for k-means, like that's always been sort of
hard for me.
It's never actually like, you never get like a true elbow, like to determine the right
number of clusters, you know, there should be even like this break and in the graph of
clusters versus how dispersed they are.
But, you know, looking through the literature and also remembering other methods, something
like a silhouette index, which is unsupervised learning metric, where you're essentially comparing
the average distance of a point, a data point, and between it and its own cluster and its
neighboring clusters, and you're trying to get at how close it is between its own cluster
and its neighboring ones, and you're assigning accuracy based on that.
In one of the Doctavec papers, the authors used something called triplet accuracy, where
they took Wikipedia articles and found articles that were in the same topic, and then they
took a randomly drawn non-topic article.
And if the distance between the two similar topic articles was closer than the non-topic
article, then that counted as an accuracy point for the model.
Okay.
So we did something similar.
We knew that we wanted to do subtopics, so, you know, we, we, for the document doctor
model, would take two articles in tech, and one that was not tech.
And if it determined that those two same articles were closer than the one in business or,
you know, sports or something, then that counted as a, as a point to the accuracy.
And we ended up with, I think it was about 77%, accuracy, which is very close to what the
authors who devised the triplet accuracy down as well.
So you're basing this model on accuracy at the category level, but ultimately you're
trying to make inferences at a sub-category level.
Did that play into, did that play into it at all?
In terms of looking at accuracy?
In terms of looking at accuracy or the model, you know, construction or performance?
Yeah, we definitely did some, some sort of, we couldn't have done too many, but yeah,
we looked at, you know, just the performance generally, relying on the editors as well,
to sort of say, like, well, this is a good, you know, this is a good match, this isn't.
But ultimately, the real test of any models when you put in productions who did performance
and if it's performing well, you know, if the topic assigned it is not good, then, you
know, it should not really, it should perform less well than your controller, your other
test branch.
So that was the ultimate test for us.
And so what was the metric for that test?
What are you ultimately trying to drive?
We, so ultimately we want to drive engagement on our site.
There's a lot of ways of, a lot of ways we can optimize, you know, the way users interact
with their website, but engagement is the sort of catch all thing that if you try to get
at that, then you're going to lift all the other ships of, you know, whatever, looking
at ads and engaging with the content itself.
So, so that was ultimately what we were trying to get at in that, their proxies for that.
So specifically how deep an art, a user would get into the article scroll and also how deep
they're getting into the articles themselves.
We did three different test branches, one where we're having similar scrolls and one
where we have, where we have, we have what I was calling dissimilar scrolls.
So I had this sort of theory that users would get sort of tired of reading the same content
so if you were reading an article about like Trump's cabinet was, maybe you want to move
on to something else after you read that article.
So that was the intuition behind that.
So we would, and essentially that's same topic articles just sorted in reverse cosine order,
which essentially what you're obtaining is there's a lot of randomness there.
It sort of doesn't seem, you know, you can't immediately tell that two articles are very
dissimilar.
It's sort of a strange thing.
They're just going to seem like that they were randomly drawn.
So that was one test branch and then as a control, we had top of these articles.
And yeah, the end result was sort of, you know, what we expected, which is good, which
is that similar articles really upformed the scrolls outperform both of these, these
other test branches.
And within topic categories as well, things like sports and culture, those performed
especially well.
Those are sort of on writers, they're a little harder to find on our sites.
So if users are going to those topics, it's likely that, you know, they're going to
be highly engaged.
So that was the intuition behind that as well.
And to what, to what degree did the similar school model outperform the other models?
It was a few fractions of percentage, but it's, you know, when you're looking at statistically
significant results, that's what you would expect.
We were lucky in that, so essentially every article page was a randomized test.
So user comes to the page and they're served one of the three test branches.
And that randomization happens at the article level.
So we ran hundreds of tests.
And that was the real, you know, no matter the, the lift, I mean, that was very important
as well, of course.
But in terms of like winning pages, the similar scrolls blew the other two out of the water.
So that was, that was a very good, similarly about that winning pages.
That the one test branch was significant over the other two in terms of these metrics
that I mentioned.
So that was counted as a winner.
And then if you look at all the pages in aggregate, that was the result.
A couple of random things I remember from flipping through your presentation, there was a reference
to Kung Fifi.
I know.
I was trying to, I was like, I really want to use this, this tweet as an example, you know,
because a lot of people, the wedding events have been around for a while now.
I wanted to sort of, I don't know, live in another bit.
But yeah, I, Kung Fifi, I don't know what would add or subtract, what vectors would add
or subtract to, to equal that.
I think it's this, it's completely nonsense.
So I know, I want to dig into that a little bit more.
I'm glad that you pronounce it the way I'm going to pronounce it, because I'm not entirely
sure.
So I think I pronounce it differently every time I say it, which is not very often.
Well, that's NLP is hard for the story, I mean, this is like, we're getting at this,
like what, you know, pronunciation, meaning, like this is, yeah, I mean, someone described
English to me the other day, and this is any language, but there's a connotative sort
of system, and it really gets at it for me, like in a pithy way, like, it's all built
around these connotations, everything has higher, higher meaning, you know, and they're
also the, the, the meaning may depend on cultural associations, yeah, all sorts of things.
What, what kind of interesting or surprising really things that you have, did you learn
along the way with this project?
Probably the most surprising was that, you know, people were just sort of consumed, they,
they come to a website that, you know, in terms of news, at least, they really go after
one subject.
And I think that that makes sense, and they've seen sort of the development of, like,
the media ecosystem generally, and, you know, it's, they have, like, these one track minds
and they keep delivering on, you know, certain subjects over a certain time period.
So that was, I don't know if it's surprising, but maybe, like, I don't know, but disheartening.
You know, I was hoping that, you know, we want to get a sort of variety, and, you know,
especially when we have, like, these long investigative pieces that, during those
events, have worked on for a while, you know, we want to sort of get at, when is the
best time to serve them?
But I think that is, to do that, you really, that's that personalization.
And we do want to get more into personalization, but it is difficult given that, you know, we're
entirely cookie based, and we don't have registration.
Yeah, really, I think speaks to a lot of what we see with the whole, you know, filter
bubble kind of situation that if you, if you see such strong results by serving up more
of the same.
You know, you want to see the same kind of results and that's just, we're all getting
every time we're the same, right?
Exactly.
Yeah, yeah.
But what can you do?
I don't know.
Talk to the editors about that one.
Yeah, so.
You mentioned inference, stochasticity at some point in your talk.
What was that reference?
Yeah, that was, so I was trying to get this, it's really about, when you make an inference
on a new document vector.
So it's initialized randomly, and if you do the inference multiple times, it's going
to be a bit random each time.
And that was a bit strange to me.
You can look at the variance of the vector, the document vector, but, you know, it's always
going to be a little different.
We tried to get at this by just having a higher number of training iterations.
And that was, I think, especially important because we were working with 100-length vectors
for a number of reasons.
I mean, we looked at it.
I looked at longer vectors, but surprisingly, they didn't really perform too much better.
So given that would have just taken up more memory, we went with the shorter one.
And it was unusual.
I mean, I was actually trying to, I was looking into why shorter vectors, you know, there's
a limit to how long they can be and why shorter vectors tend to perform better.
And it's, no one really has really gotten at that.
And maybe something about overtraining, I don't know.
But just surprising, that's 100 dimensions is commonly used for word vectors.
And I would think that for a document, you'd want kind of a richer space.
And so you'd want a lot more, a lot higher dimension.
Yeah, I agree, but that's not what we saw.
So which was fine.
And in terms of the SOCasticity, there is another source of randomness in the context window.
So we use Gensum for the document vectors and the way the author is simple.
What is that?
It's a topic modeling library in Python, which is really, really terrific, it's very fast.
And yeah, so it handles LDA and word vectors and document vectors.
But the way that they implemented it was, so when you're training, there's a context
window that you can specify.
But it doesn't always draw, say, if you say, okay, well, five words should be the window.
It doesn't always draw five words.
It draws randomly based on how, so the near words, the word, the center of the window
will be, there's a higher likelihood that they'll be drawn.
And this is how the original word detect authors implemented there, the model.
So that's what they did.
So that is also, even if you weren't, you know, even if you could see the document vector
at inference, you would still have the source of randomness.
So, yeah, I mean, you can, another tactic would be to average the vectors, but, you know,
we did the sanity check where you're comparing the inferred vectors to the vectors of training.
And in every case, they were, you know, that you came up as the result where, you know,
the nearest neighbor was the, themselves.
So that was, that was the check and we figured we didn't have to do that.
So.
I still have some fuzziness around the inference thing in this, in this case, what is the,
what's the input and what's the output of this inference step?
So the input is, is all the vectors that were trained previously.
So it's really, it's not too complicated because this is, these are really pretty simple
models.
Mm-hmm.
Yeah, you're just keeping constant the words that were trained.
So you, you're only looking up the words that are in a particular document they were trying
to infer.
So those, you know, those vectors already.
And you know the output, the probability of co-carns for word-to-word pairings with those
two weights established, just their gradient descent, the, the vector that can change,
which is the document vector that was newly initialized, that will change, that will sort
of converge to the result they found in training.
Okay.
Yeah, I don't know why I was, why I was kind of weighing out on that.
So basically you, you trained this set of document vectors based on some number of articles.
I think it was 350,000 or something.
Yeah.
Mm-hmm.
And then you've got some new article that was just someone, some editor just hit Publish
or Submit or whatever.
You've got this new document you're trying to figure out where it fits and you need to
produce a document vector for it and that's the inference step.
Yeah.
Exactly.
But it's quite, another way was, because we're doing this in production, another way was,
you know, just speed at which you can infer.
But it was, it's quite fast, it was less than 10 milliseconds and we do this asynchronously.
So there isn't, you know, if you're going to see a page, the inference has already been
done.
So we're just pulling that from a database, essentially.
And we also are, this, we can also were caching frequently and we can catch at the future
level.
If your request is made for a particular article, I mean, if the request is, okay, we want
to sell in our scroll, if it's not cash, we'll just pull it from the database and it's
very fast.
But after the fact that response is, that's, spring of articles, this is cash so we can
just serve it subsequently to users who come after that first one.
Meaning for a given article, you cash the preferred similar scroll articles, yes, exactly.
And do you, do you cash kind of the exact order or do you cash of, you know, some number
of similar ones in a randomized or something like that?
It's the exact response.
So yeah, there's a, there's a filtering step where, yeah, we're, like, we're only pulling
from our, so the, the tech stack is, and this is sort of one of the more interesting parts
in this whole development is that we had to build out a data science platform.
And it was great because it's, it's just a Python web app and it's serving, knowing
you need users today.
So to get actually, you know, to come upon the realization that you can, you know, build
a Python web app and scale in that way, that was great.
And is it, is it kind of a spoke Python or something like Django or something?
We use a web app framework called Pyramid, which is, it's like Django, except there
are significant differences, one of which is, you can use something called SQL Alchemy,
which is a, allows you to do database queries by writing Python, so ORM.
And that's very powerful.
So that was one of the main reasons for going at that.
Other than that, we use Amazon, AWS, we have a MySQL database that stores all of the
articles.
And then we use RDS, a Redis cache for responses.
And that also stores.
So when we do, we do personalize, to some extent, for like sponsored stories, and, you
know, we serve up sponsored stories based on where users in the world.
So that's a user level feature.
So we do do that.
And there's, Redis is caching those responses as well.
So we're able to scale with the users as well.
Okay.
So you've got this Python based web framework website.
And it sounds like you're saying kind of starting this effort up.
You didn't have a data science stack built up around this, this website around the project.
So what were, what were some of the steps in thinking it went into building up that stack?
It sort of was built out feature by feature.
The original feature was making recommendations for videos.
Because so I started with, at Reuters with one of their apps called Reuters TV, okay.
And we put the Reuters TV player on the website.
And we wanted to, so there's algorithms that go into that.
There's a program, something called a program assembly algorithm that takes all the available
videos that we can show and builds the best program based on the time that the user selects.
And there's other sort of recommendations that go into that.
So we needed to have a system to build that for the website and also for Reuters TV.
Meaning a user goes to the website and to some TV section and they say how long they
intend to watch.
Yeah, that's one of the key features of the app.
Okay.
And that was one of the, that was a really interesting data science problem as well, because it's
actually, so to determine, you know, based on these 25 stories that have three links each
of their variants.
How do you, you know, if user wants to watch a program that's 10 minutes long, how do
you take all of that content and get as close to 10 minutes as possible without going
over?
And how do you also make sure that you're filling the editorial requests of, well, we
want to show X number of world stories and this many culture stories and that sort of
thing.
Kind of like we did been packing kind of thing.
Mm-hmm.
Yeah.
It's called a napsack problem.
And it sort of comes up in, you know, like these coding interviews where you think,
like, I'm never going to have to use this.
Right.
But it was, I used it and it was, and there was some sort of new, what I thought were
new things that went into it.
Like there's a multiple choice component where you're choosing because each main story
has three different variants.
So that was very interesting, but yeah, the framework was really built out, yeah, feature
by feature.
So there's that, there's the scroll model.
Meaning, so you started with this video, we're talking features at that level that, you
know, there's a, the framework has this video optimization feature that scroll optimization
feature, or are you talking about lower level features that support the building of these
different higher level features, I guess.
I guess, I guess both.
I mean, it's all contained there.
So even the models, models are loaded into memory, which we're trying to actually get
away from that and build out another framework where requests were just made to a machine
learning app for, you know, give us a scroll for this article and the rest of the business
logic that was built out during this building, the state of science framework is still there.
So yeah, there's that there's other, I'm trying to think of other data science things
that live on this, this web app.
We do something called a trending people module, so where we're doing name entity recognition
on articles and we're also looking at how articles are trending.
And we have a little module on one of the sites on the website where we show people who
are trending and that's just based on the articles that they've been in and the fact that
we've determined, okay, this person is being referenced and also that they're the subject
of this lead of the article.
Okay.
Yeah.
And are you, are you doing that using that same document toolkit that you mentioned or
how are you doing the any our stuff?
Well, we have some in-house and some using spacey, which is sort of a general really great
NLP library, but Thomson Reuters, which is the larger company of Reuters, they have, they
built on a really terrific name entity system, so for any given article, they will tell
you who's in itch and that's done in, you know, in the typical ways, but from that, there's
also something that I built out was determining, you know, we don't want to just pull out
all the people in an article and show them like, okay, all these people are trending.
The task is really, okay, well, we know these people are in the article, but who are the
main subjects of it?
And that is, that's a task of dependency parsing and saying, okay, well, if you'd like
at the lead of the article, this person is performing the action on this person.
So let's take the subject and say that this person's trending and that's sort of how we
get at that.
And so you've got these different components that you use, you mentioned spacey, and I forget
the name of the other Gremlin?
Jensen.
Jensen, Jensen.
Maybe Jensen, I'm not sure.
Jensen or Jensen?
Jensen again.
Okay.
Perfect.
Yeah.
You describe this kind of data science platform, trying to get at kind of what really
characterizes this platform is at this high, these higher level features that you're offering
through the site or are there lower level components that you've pulled together that
accelerate the ability to make the N plus 1 feature?
Mm-hm.
I think it's probably the features that characterize the web app.
Okay.
Yeah, I mean, just thinking in terms of the end users, that's sort of what they care about.
And who are they?
Are they developers on the web app?
Mm-hm.
Oh, they're, so I built the web app, but they're front-end, mostly, developers.
But yeah, I mean, if there's the sort of the check that goes into the modeling, the lower
level stuff, yeah, I mean, I think down the line that could be something that, you know,
we want to try to build out and Riders is, you know, we have a lot of resources and even
though the team is very small.
That's something that I think that would be good to, you know, if you wanted to think
about contributing to the community, the science community, that would be really terrific.
Mm-hm.
How big is the data science team there?
It's not big.
It's less than 10 people, so we're, you know, trying to grow in different ways.
But yeah, and then the developer team is also not big, but it's good to, you know, be
small and agile in that way.
Cool, anything else that we, that you plan to cover in the presentation that we haven't
already talked about?
I think we got to most of it.
At the end, I do talk about just sort of the future of embeddings.
And I think that there's some question now because they have been outperformed in some
ways by different types of embeddings.
There it's sort of a suite of models.
One of the better known ones is something called Elmo, which is an acronym called Elmo.
Elmo.
Okay.
It's a language model.
It's types of embeddings.
And essentially, it's getting at, essentially, just deeper context.
So, you know, with normal word vectors, you capture the context and of that word in a data
set.
But this actually gets at multiple contexts.
So the, something called a polysemi, which is the multiple meanings of individual words.
So it's, it's, it's more advanced models, more complex.
There are also some sort of structural things about it that allow it to be used in downstream
tasks, a variety of downstream tasks.
And it seems that it's, it's really outperforming in these tasks.
Word embeddings, just sort of an yellow word embeddings.
And I think a lot of the inspiration, this is coming, just lifting this from this guy,
Sebastian Reuters, I could have that, and he developed Elmo and he had a blog post talking
about when his NLP is going to have, it's image net moment, referring to, yeah, it's
really thought provoking and, yeah, getting at, you know, when we, when is NLP really
going to break out and in terms of, you know, a single type of model or a single data
set, that's why he was talking about image net.
And, you know, for computer vision, it was really both like the acceleration that happened
so, swiftly there was because of the data set, and then, and also the models, but, yeah,
and then, but, you know, moving that, doing transfer learning, moving those models and
the weights that were trained on this very rich data set, to other tasks, that was sort
of the image net moment.
And with these embeddings, you know, they're trying to get it, universal embeddings that
can be used in a variety of different tasks downstream, and they're similar to, they're,
you know, they're looking in context, they're, they're sequence models, so you're predicting
words based on context, but it's in a much deeper way and it's, um, my directional
LSTMs. So, you know, given that they, you know, it seems like they've, the metrics are much
higher for these, um, there's a lot of question about, like, you know, what's going to happen
with, with the NLP were embedding some document vectors, so, but, yeah, we'll see.
Yeah, Sebastian and I are, uh, over due to doing interview, we're supposed to be getting
back in touch now, I think.
Fantastic.
Well, I'll tell you much more than I go to.
Yeah.
Anyone listening to this that was intrigued by the comment, um, we should be diving into
that set.
Cool.
I'm looking forward to that.
Awesome.
Well, thanks so much.
It was great to have you on the show.
I appreciate you stopping by and, uh, looking forward to your talk.
Yeah.
Thanks, Simon.
Appreciate it.
All right, everyone, that's our show for today.
For more information on James or any of the topics covered in this show, visit twimlai.com
slash talk slash one 83.
For more information on the Stratidata podcast series, visit twimlai.com slash Stratid and
Y 2018.
As always, thanks so much for listening and catch you next time.
