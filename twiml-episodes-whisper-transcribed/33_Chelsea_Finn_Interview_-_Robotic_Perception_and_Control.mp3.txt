Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week we continue our series on industrial applications of machine learning and AI with
a conversation with Chelsea Finn, a PhD student at UC Berkeley.
Chelsea's research is focused on machine learning for robotic perception and control.
Despite being early in her career, Chelsea is an accomplished researcher with more than
14 published papers in the past two years on subjects like deep visual foresight, model
diagnostic meta learning, visual motor learning, just to name a few.
And we talk about all of these in the show, along with topics like zero shot, one shot
and few shot learning.
I'd also like to give a shout out to Shreyes, a listener who wrote into a request that
we interview a current PhD student about their journey and experiences.
Chelsea and I spend some time at the end of the interview talking about this and she
has some great advice for current and prospective PhD students, but also for independent learners
in the field.
Also, during this part of the discussion, I wonder out loud if any listeners would be
interested in forming a virtual paper reading club of some sort.
I'm not sure yet exactly what this would look like, but please drop a comment in the
show notes if you're interested.
Okay, once again, I'm going to deploy the nerd alert for this episode.
Chelsea and I really dig deep into her research and this conversation gets pretty technical
at times, to the point that I had a hard time keeping up myself.
Thanks again to our sponsor for this series and my industrial AI research, bonsai, bonsai
offers an AI platform that empowers enterprises to build and deploy intelligent systems.
If you're trying to build AI powered applications focused on optimizing and controlling the systems
in your enterprise, you should take a look at what they're up to.
They've got a unique approach to building AI models that let you use high level code
to model the real world concepts in your application.
Really generate, train, and evaluate low level models for your project, using technologies
like reinforcement learning, and easily integrate those models into your applications and systems
using APIs.
You can check them out at bonds.ai, B-O-N-S.ai, and definitely let them know you appreciate
their support of the podcast and now on to the show.
Alright, hey everyone, I've got Chelsea Finn on the line with me.
I'm super excited to have Chelsea here to speak with us.
Chelsea is a PhD student at UC Berkeley and she is co-advised by both Peter Abiel and
Sergei Levine.
By the time this podcast is posted, you'll have heard my interview with Peter and we'll
be digging in a little bit deeper into some of the things we spoke about with Peter with
regards to reinforcement learning, but in particular, we'll focus on Chelsea's research
into topics like deep sensory motor learning and few shot learning and some other things
she's working on.
Chelsea, thanks so much for joining us.
Thanks for having me.
It's great to have you.
Why don't we get started by having you tell us a little bit about how you got interested
in AI and how you got to where you are?
Great, so I did my undergraduate at MIT as an undergrad and I pretty early on decided
that I wanted to go, I wanted major in computer science and once I made that decision, there's
a lot of different things that you can do with computer science, but machine learning and
AI was the thing that I found myself most interested in given how much math it has, unlike
some other areas of computer science, there's a lot of math involved, probability, statistics
and I like that grounding in math and I also find that AI has a lot of very important
applications and I think it's the potential to have a big impact on society.
Absolutely, and I think the further we get with AI, the more of these potential applications
we're seeing in particular some of these industrial use cases where we're using AI to control
robotics and help automate things and that is a big focus of your research, is it not?
Absolutely, I work a lot with real hardware and trying to get robots to learn how to do
tasks and act intelligently, ultimately.
So why don't we talk a little bit about some of the challenges that are involved in doing
that?
So unlike many problems in machine learning, in robotics you have a physical system
that is in the real world and collecting data and the actions that you take affect
the environment and affect the world and that affects what actions you want to take
next.
So you can't just download some data set and process it in a passive way.
You need to actually be collecting data online and then learning from that data and then
repeating essentially.
And when you're collecting that data online, that poses a big challenge, particularly if
you're working with systems that aren't in a lab environment, but in some production
use.
Can you talk a little bit about some of the specific challenges with regard to data collection
and are there techniques or is there research being done that's focused on that particular
slice, how to make that data collection more effective and efficient?
So some of the big success stories in robotics in industrial applications has largely been
in factories and in very controlled settings where you can essentially just pre-program
exactly what motion the robot is going to be doing ahead of time and then just have the
robot repeat that action again and again.
And in these industrial applications, you really don't see robots even using any type of
perception.
They're simply just blindly executing motions.
In machine learning research for robotics, we're trying to move beyond that and I think
that what learning will bring to robotics is the ability to adapt to new environments
and learn to do tasks in very unstructured environments where you don't know what the
world looks like ahead of time and the environment might be dynamic.
A lot of research right now in robotic learning is still in lab environments because that's
where we can set up controlled experiments.
It's more convenient to collect data in your lab than actually putting the robot out into
the real world.
But I think that in the very near future we're going to start seeing more and more research
where robots are actually out in the real world and collecting data because that's where
we'll be able to get the diversity of data that we need to be able to effectively generalize
to new tasks, to new types of objects, etc.
One of the things that I've seen that's been really interesting is been the use of clusters
if you will of robots that are operating in parallel to try to accelerate both data acquisition
and learning.
Are you doing anything with that kind of environment?
Yeah, so I did an internship at Google Brain about a year ago now where I worked on 10
robot arms that were all the same and could collect data in parallel and share their experiences
so that they could more efficiently collect a very large amount of data.
And with that we were able to basically give a bunch of objects to each of the robots
and let them play around with those objects and share their experience with one set of
on one set of objects with another robot who had an experience with the different set
of objects.
We also now have four robot arms here at Berkeley and we might get more that we're just getting
that set up right now and we're soon going to be able to have the capability to set something
up at Berkeley in a similar fashion to what we did at Google.
Oh, that's great.
Is there a name for at use case of the robotics where you're training them in parallel and
you're in the middle of the training transferring knowledge between the different robots?
It sounds like, you know, some version of like active transfer learning or something like
that.
Is there a standard name for that yet?
No, not yet.
We typically just call it large scale robotic learning.
Okay.
Okay.
Well, that's maybe you take a step back when I saw your talk at the rework deep learning
summit.
You started with a particular example that you used in your research and kind of built
your discussion of the, you know, the various challenges and the research you were doing
around this example.
And if I remember correctly, it was, you know, something along the lines of taking, you
know, triangular blocks and putting them in the right holes or something like that.
Can you maybe, you know, walk us through that scenario and then talk us through, you
know, some of what you discussed in your presentation?
Yeah.
So the first thing that I set out to do in my PhD was to try to see if it was possible
to learn a deep neural network that maps from what the robot sees to the actions that
the robot takes and see if we were able to learn, learn a policy that does this for manipulation
skills and be able to do that successfully and one of the first tasks that we experimented
with was inserting a block into a shapeshifting cube and the, I guess, usually when I begin
with that example in my presentations, I talk about for a human, this is very intuitive
to do because over the course of your life, you've learned how to guide your arm such that
the, the block falls into the hole nicely.
But for a computer for a robot, what the robot sees is just a bunch of numbers, a huge array
of numbers in the picture and likewise, the actions that it's taking, maybe the torque
supplied to the joints of the arm is also just a bunch of numbers.
And to be able to map from one set of numbers to another set of numbers, to do that accurately
and to do that in a way that will robustly handle a variety of environments, there's no way
that a hand engineer approach will work.
We need, we need to be able to learn that function and we need things like deep neural networks
to provide a very flexible function in order to be able to effectively do learning in
that scenario.
Yeah, I think that's a, you know, a great way for folks to think about what deep learning
is, you know, thinking about it as a function that, you know, maps from, you know, one set
of, from a set of inputs to a set of outputs, it's interesting that, you know, that we're
able to throw all this data at the problem and have the computer figure out these functions.
Yeah, absolutely.
And actually, one of the challenges in robotics is that typically in deep learning, you gather
a very large data set and then train your neural network on that data set, whereas in robotics,
it's impractical to collect a huge amount of data for a task that you might want to train
because you're actually collecting that data on a real physical system.
And so are there techniques that can be used to, or let's maybe talk a little bit about
the techniques that can be used to address that problem.
Deep learning historically requires lots and lots of data in these industrial environments.
It's difficult to collect lots and lots of data.
What can we do then?
Yeah, so there's been a number of approaches.
One is to, as we talked about before, just get a bunch of robots and have them collect
data in parallel.
Well, actually, one of the challenges with that approach is that if you're having a
lot of robots collect data, you don't want to have a human for every single robot labeling
the data or resetting the environment or providing other means of supervision because
that defeats the point of having the robot there in the first place.
So you can get a lot of data, but you need to algorithm that can learn from the raw data
rather than from labeled data like we see in some of the most successful applications
of deep learning.
Another approach to this problem is to train in simulation where it is very practical
to acquire a lot of data and then try to use what you learned in simulation to be able
to effectively act in the real world, either with zero shot transfer where you don't get
meet where you get zero data in the real world or with or a few shot transfer or just fine
tuning in the real world where you just need less data in the real world than you would
need otherwise if you didn't have that simulated data.
Well, maybe let's talk a little bit about those three things.
So with simulation, maybe walk through the process of using simulator to train a deep
neural net.
Yeah.
So in simulation, we can use algorithms that require a lot of data, specifically reinforcement
learning algorithms that reinforcement learning algorithms are typically very data inefficient
because they don't have the exact input output labels that you have in supervised learning.
You're not just trying to map from one thing to another where you know exactly what the
output should be.
Instead, you get experience in the environment, then you get feedback from the environment
on how well you did.
And that feedback might be delayed.
It might be sparse.
So you might not get it very often or it might not be very detailed.
And so as a result, reinforcement learning algorithms are significantly slower than super
and require significantly more data than supervised learning problems.
It'd be already know that supervised learning problems require a lot of data.
So typically what this kind of the, what an approach like this might look like is train
a policy in simulation using your favorite reinforcement learning method and then take
that policy and try to transfer it into the real world, either just by running it in
the real world and hoping that it works potentially with some modularity to like a vision system
on the robot, like a specific to the robot versus specific to the simulation or a controller
that specific to the robot and specific to the simulation or trying to initialize with
that policy and then fine tune in the real world.
One of the reasons why that transfer doesn't just happen automatically is that one, similarly
vision is usually lower fidelity and not as realistic as the vision that we get from
cameras in the real world.
And physics simulation, the physics in simulators is actually not at all accurate, especially
when you encounter a contact between two different objects.
Modeling contacts and exactly what goes on within that contact is quite complex, so that's
not something that we can accurately model in simulators.
So that's really interesting.
So the specific challenge there is, for example, in simulation, you're modeling a robot manipulator
like a hand that's picking up a block, I can imagine that the physics governing how that
simulator is grasping that block and the coefficients of static friction and dynamic
friction and all the things that determine the way the block will ultimately sit in the
gripper can be quite difficult to model.
Is that basically what you're describing in terms of when you're modeling two bodies?
Yeah, exactly.
And so with the difficulty in modeling that, how do you account for that?
Do you just do a rough approximation and assume that the difference is noise in the system
that your model just needs to account for or are there specific techniques for dealing
with that?
Yeah.
Simulators have various ways to approximate them, then yeah, generally the learning algorithm
doesn't look any different in simulation versus in the real world.
Okay, one conversation that I had that I think I mentioned in the conversation with Peter
as well was a conversation with Stefano Irman over at Stanford who was talking about like
incorporating physics into models and Peter and I talked about that, I think fairly generally,
is that something that comes into play specifically in your work?
And in this issue of the grippers, for example, or the contact between the robot and other
objects.
So you're asking, like, do we try to learn models of the world?
I guess I'm asking, how do you try to incorporate pre-existing knowledge about the way grippers
grip and objects respond to being gripped into your deep learning models?
So some of our algorithms, we do incorporate knowledge about the physical world.
For example, we have a certain type of model usually in one of the algorithms that we use
and that we can run on real robots.
We use a Gaussian mixture model as a prior on a learned time-varing linear model, it
is getting a bit complicated but a bit technical.
But generally a Gaussian mixture model is or at least a prior from a Gaussian mixture
model is a fairly reasonable way to model physics and that oftentimes there are different
modes of physics, whether you're in different types of contact, whether you're in free space
and that sort of model is well suited for that.
Actually, I'd love to have you walk us through the details on that.
Can we start with what is a Gaussian mixture model?
Yeah, so this is one small part of a much larger algorithm for learning policies on the
real robot.
So a Gaussian mixture model is a distribution where there are multiple mixture components
and each mixture component is a Gaussian distribution.
So a normal distribution and then each component also has a weight.
And then the Gaussian mixture model is simply the weighted sum of all of the Gaussians.
And is the Gaussian mixture model are using a mixture to model one specific thing or using
that mixture to model a several phenomena at once?
We're using the mixture to model a mode of dynamics where by dynamics I mean a conditional
distribution of the next state given the current state in action.
So you're basically trying to predict what the next state is going to be given where
you are right now and the action that you take and that conditional distribution will
depend on whether or not you're in contact, whether or not your finger is sliding across
the table versus in static contact or versus whether or not you're in free space.
And so we're using each mixture component of the Gaussian mixture model to model those
different modes of your dynamics.
And so you mentioned a few things in terms of your finger sliding in those things.
Do each of those map one to one to a component of the mixture or is all of that modeled
by the whole of the mixture?
Each of those will typically map to one component, although usually you don't know the number
of components a priori, so you set it to a number that you think is slightly larger
than the actual number of components, just like in k-means or in clustering algorithms.
Okay.
All right.
So you've got this mixture that models some of the physics that rolls up into the broader
model that you're trying to build.
And you said you used that as a prior for fitting a model.
Okay.
And so when you say using that as a priori, you're basically using the output of that Gaussian
mixture model as an input to your deep neural net, is that the right way to think about
it?
So in this case, actually, the dynamics model that we're learning is not a neural network.
We're learning a local time varying linear model where by time varying linear, I mean
that you basically sample a bunch of trajectories on your robot.
And then at every time step in those set of trajectories, you fit a linear model using
linear regression.
And then the Gaussian mixture model is fit to all of the time steps of all of the samples
and that's used as a prior for linear regression, done at every time step.
Okay.
Got it.
Got it.
And then ultimately we use this, well, there's more steps involved, but you use this
this dynamics model that you fit to acquire an optimal policy for a certain version of
your problem.
Okay.
And then when you have an optimal policy, you can at a high level, the top down explanation
is that given a certain manipulation problem, you can decompose your problem into different
instances of the problem, like for a single start position and a single end position.
And then we are solving for the optimal policy for each individual condition using optimal
control and using this linear model that we fit.
And then once we solve each of the individual problems, then we use that for supervised
learning of a deep neural network that can solve all of the instances of the problem.
Okay.
So let me try to paraphrase that to make sure I'm following.
So you've got, it sounds like we are talking here.
You mentioned point A and point B, are we talking about, you know, strictly the problem
of you've got a robot arm, let's say, with, you know, N degrees of freedom, and you're
trying to figure out a path, you know, to translate, you know, to get the gripper from
point A to point B using those motors.
Is that the scope of the problem that we're talking about or have I read into that to
narrowly?
That is a little bit too narrow, so it's not just moving the gripper of the robot, it
could also involve moving objects.
It could be, so this algorithm has also been applied to manipulating objects within a five
fingered hand.
A slightly different version of the algorithm has also been used for a locomotion, robot locomotion.
Okay.
So then maybe taking a step back, it sounds like you've got, you've got something that's,
you're trying to figure out how to get it from point A to point B, and you've got some,
you know, underlying dynamics that you need to model.
And so you use, you train a linear model to tell you basically how to move your motors
to get the thing from point A to point B or state A to state B.
And then once you have those linear models, you're able to use them to generate more data
that you can train deep networks with.
It's basically you're, you're training a data generator.
Essentially.
Yeah.
So the data generator is, is that in any given kind of robot manipulation problem, you
can see your current observation, but you don't know what action you should take.
And what you want to figure out is what action to take, and that's, and so the data generator
is figuring out what action you should take for any given observation.
And then once you have the actions that you should take, you can then just apply play
and supervise learning with your neural network.
Mm-hmm.
Mm-hmm.
Maybe let's take a step back to kind of this, the, you know, the broader problem, which
is the putting the blocks in the right places, a big part of your research is the relationship
between the robot is seeing that problem from a computer vision perspective.
And you're essentially, the intelligence that the robot is acting on is kind of largely
driven by, you know, the manipulation of, or an observation of, you know, the pixels
coming from a camera or a set of cameras.
Can you talk a little bit about the, you know, broadly speaking, the relationship between,
you know, computer vision and the work that you're doing in robotics.
And then, you know, maybe we can drill into some of the specifics.
Yeah, so a lot of the algorithms that we develop for robotics, we aren't necessarily specific
to certain sensory modalities, like tactile or vision.
We want them to work with a fairly wide variety of, of modalities.
Vision is perhaps one of the most interesting because it's, gives you a lot of information
about the environment and it also is one of the most challenging because it is very
high dimensional and not high dimensional in a way that's readily interpretable by a lot
of these algorithms.
One of the other reasons why we use vision a lot is because tactile sensing, while it's
very, can give you a lot of information, good tactile sensors are hard to acquire.
And they're typically either very expensive or very fragile and break a lot.
So a lot of the algorithms that we develop, we try to incorporate, I mean, we use convolutional
neural networks if we're going to use vision, convolutional neural networks are very efficient
and very effective at doing their job, at these localizing objects or inferring things
about the environment, et cetera.
So a lot of my work will be developing algorithms based off of reinforcement learning, imitation
learning, or inverse reinforcement learning, but focusing on algorithms which can scale
to high dimensional inputs like vision.
Okay, you mentioned inverse reinforcement learning.
What's that?
Yeah.
So reinforcement learning first is the problem of given a reward function and the ability
to sample experience from your environment, figure out what the optimal policy is for
that reward function, figure out what actions you should take given a current observation.
Inverse reinforcement learning is essentially the inverse of that.
So in inverse reinforcement learning, you assume that you have rollouts or basically trajectories
from the optimal policy, from an expert, like a human.
And your goal is to figure out what the reward function is.
So your goal is to figure out what the human was trying to accomplish.
And then ultimately, once you figured out what the human was trying to accomplish, then
you also want to learn a policy for yourself that also accomplishes what the human was trying
to accomplish.
So it was an application area that when you're doing imitation learning and you have a human
explicitly move a robot from one position to another, you can then use inverse reinforcement
learning to learn a policy that would produce that same motion.
Yeah, exactly.
So the typical application is given a set of demonstrations from a human, try to figure
out what the human was doing and then figure out how to do it yourself.
And then you said in your previous description, you said not just what the human was doing,
but you kind of characterize it as why the human was doing that, or at least that's what
I read into the way you said it, like the human's intent, is that do you make that distinction
between what the human was doing and the model learning some degree of intent or higher level
purpose, or am I reading too much into it?
No, that's absolutely right.
So the reason why this is interesting is that if a human is doing something, you don't
want to necessarily mimic the exact actions that they do because you might have a different
arm that looks slightly differently, or maybe the way that they're doing it isn't quite
optimal, or you want to be able to generalize what they are doing to new scenarios.
And in those new scenarios, you don't want to do exactly what they did.
You want to achieve what they were trying to achieve.
And so in reverse reinforcement learning, you're adding structure to the problem of imitation
learning, where you're trying to, one, you're assuming that they were acting according
to some reward function, and you're trying to infer what that was and what they were
trying to achieve.
Mm-hmm.
And so bring this, bring all this back to the problem of placing the blocks for us.
Okay.
So in the block scenario, you'd see an example of a human putting the block in the shape
sorting cube, and then you would try to infer the fact that their goal was to get it inside
the cube, not just to take the actions that they were taking.
And then once you infer that, figure out a policy for doing that.
The block example, perhaps, isn't the best example for inverse reinforcement learning,
because providing a reward function for that task is fairly straightforward.
Your goal is to physically get this red block into the shape sorting cube.
But in many other scenarios, it's actually hard to write down what their reward function
should be.
And that's actually one of the big challenges in applying reinforcement learning to real
world scenarios.
So for example, say you want your robot to pour a cup of water from one cup to another
cup.
In that task, you want all of the water to end up in the target cup.
You don't want any water to spill.
You probably want the robot to be somewhat gentle, and encoding those things in a reward
function requires a lot of engineering, like you might need actually like a liquid detector
to be able to detect where the liquid is and to detect if something got wet.
And then also characterizing whether or not the robot was gentle.
And so engineering that reward function may even be more work than engineering the behavior
itself.
And as much easier just to show the robot, this is the task that I want you to do.
But you still need to do you still need to give the robot examples of a failure or of
failures?
Like, to what degree does that come into play as well with supervised learning?
So you've got examples of, hey, I'm successfully getting the cup of water from point A to point
B. And you can label those as successes, but what about labeling?
You know, this is a failure explicitly, you know, even though 80% of the water may have
gotten from point A to point B, does that make any sense?
Yeah.
So typically in inverse reinforcement learning, you only give successful demonstrations,
although thinking about how you could incorporate failed examples would also be, is an interesting
research direction, not a lot of research has been done there.
And actually, perhaps an interesting analogy for people is that for people that are familiar
with generative adversarial networks, you can actually show that the inverse reinforcement
learning objective, that is one of the ones that's most widely used, is mathematically
equivalent to the objective of a discriminator in a generative adversarial network.
So in generative adversarial networks, you're given a data set of images and the goal is
to be able to generate images from that data set or that look like images from that data
set.
And the goal of the discriminator in a generative adversarial network is to figure out
if an image was generated in its fake or if the image came from that data set.
And the reward function that you're trying to learn in inverse reinforcement learning plays
the same role as the discriminator.
So it's trying to, it's generally trying to say that the demonstrations, the example
demonstrations that you got, which is the same as your data set, have high reward.
And things that your policy is trying to generate is try, it has low reward.
So just like in generative adversarial networks, you only get examples of positive data points.
Okay.
Of successful demonstrations.
Okay.
It strikes me that there's, you know, some signal and partial successes to some degree
and it makes me wonder what research is being done out there, you know, in that direction
if anything.
Yeah.
So actually one interesting thing is that humans, even if someone isn't successfully
doing a task, you can typically infer humans can typically infer what they were trying
to accomplish.
Right.
And that's actually one of the things that I'm working on right now is that is an algorithm
which tries to learn from unsuccessful demonstrations that we're still going in the right direction
and still have enough signal to indicate what goal they were trying to achieve.
Okay.
And so what's the approach there?
So the approach that we're trying right now is an approach based on few shot learning
or metal learning.
And I guess another term for metal learning is learning how to learn.
Okay.
And just if I can interject, I think you mentioned also one shot learning previously.
So we've got this, you know, spectrum of learning, if you will.
One shot is, you know, learning on, you know, one example, few shots is learning on a few
examples.
There's also no shot learning, which is learning on no examples.
And then metal learning, it's orthogonal to the end shot issue or is it not?
So a few shot learning and one shot learning are typically achieved using metal learning
algorithms.
Okay.
So metal learning is somewhat of a broader class of algorithms.
Right.
Right.
Okay.
So apologies.
You were saying.
I think you were saying that you were talking about how you're using metal learning or
learning to learn to, you know, solve this problem of learning from failed examples.
Yeah.
So at a high level, what we're working on is being able to show collecting a data set
of examples of demonstrations for different tasks and corrupting some of those demonstrations
with noise and then trying to have a system that learns that the corrupted demonstrations
that learns basically how to do the correct thing from the corrupted demonstrations.
And so in this example, what is the, what is the demonstration?
Are we still talking about the shape sorting cube or is this something else?
And then what, what is the noise?
Are we talking about noise added to sensory input?
Are we talking about noise in some represent or perturbations in some representation of,
you know, the underlying process?
Are we talking about, you know, noise injected into some layer of the deep neural network
or by noise, they just mean noise injected onto the actions.
So the output of the neural network of the demonstrations, so essentially the labels
of the demonstrations, okay, and that will make the demonstrations sub optimal.
Okay.
So basically, you've got some label data that you're training a model on and you just
mess up some of the labels sometimes.
Essentially, although the, we're not training a model in the typical way.
So in this work, we're building on some of my recent work on few shot learning or one
shot learning.
Few shot learning is just kind of the general case where if you could be one to ten or maybe
a little bit more, where we try to learn a representation that's very quickly adaptable
to many different tasks.
And maybe let's dig into the few shot learning issue and talk a little bit about just,
you know, your approach, what you have done in your recent research, but also what others
have done and, you know, a little bit of a background into the problem domain if we
could.
Yeah, absolutely.
So in few shot learning, the goal is to be able to do a new task from only a very small
number of data points from that task.
So an example of this is in the, the visual recognition realm is say that you're given
a picture of a segue and then your goal is given that single picture of a segue, be
able to classify other examples of segues successfully.
And the way that you learn how to do this is get a data set of lots of different types
of objects with a few types of each different object and you learn about the, the variation
across objects.
So you actually learn how to identify objects just from one example.
And so is the data set that you're referring to?
Is that a label data set or that is that an unlabeled data set that you're just learning
a bunch of things from right now methods, well, it's a label data set, okay.
And is it a label data set?
What are the, the nature of the labels, meaning is it labeled with, you know, objects or
meaning this is an orange, this is a cat, or is it, you know, some kind of labels of the
physical attributes of the things that are depicted?
It depends on what you want to do.
If your goal is to do what's called one shot image classification, then you take a standard
image data set that has an image and it's corresponding label and a full data set of
that.
And just like image net, Mness, there's a data set called Omniglott that's very popular
for one shot learning and yeah, that's the nature of the data set that you use for
metal learning.
And so you, you have, let's say image net, you have image net, huge database of labeled
images and, you know, we'll assume that there's no segways in image net and then you're basically
trying to show it a picture of a segway and then you're showing it a labeled picture
of a segway, a single labeled picture of a segway and you want it to be able to identify
subsequent segways.
Yes.
Correct.
And I guess the question that what I'm wondering is why do the labels in image net
matter?
Like what doesn't matter if, you know, we have labels for oranges and these other things.
I'm imagining that what's happening is you're throwing all that data, oh, you're training
a deep neural network against all of that data.
And then, you know, within, you know, the various layers of the neural net, it's kind of
figuring out, you know, textures and colors and geometries and curves and edges and things
like that.
And it's using that to identify, you know, a segway.
What is, what is the fact that the data set is labeled matter in that case?
Yes.
That's a very good question.
It actually doesn't matter that it labeled an orange as an orange or a banana as a banana.
What matters is that it is labeled in the sense that it knows like this set of images is
one type of object.
This set of images is another type of object.
Yeah.
That makes sense.
Okay.
So it's, you're basically, the label is is a way to communicate, you know, clustering or
similarity of like images and it can use that to properly form the internal structure
of your neural net to reflect all these things.
Yes.
Okay.
Have you gotten yet the specifics of how folks are attacking fuchsot learning based
on, you know, having, you know, with image and things like that?
Like are there specific network architectures or specific training techniques or things like
that that lend themselves to building these fuchsot models or meta learning models?
Yeah. So there's actually a few fairly broad classes of techniques for solving this problem.
One of the perhaps easier class of methods to explain is a class of methods that tries
to learn an embedding of images such that when you run nearest neighbor or do like comparisons
in that embedding space, you can very accurately generalize from just one or a few examples.
And just for, for background, an embedding is basically taking a set of images and you
mathematically kind of turning them into vectors that somehow relate one to the other.
So say that again with that as background, you, you, you, you learn an embedding space
such that when you make comparisons in that embedding space, you can generalize well from
just a single example.
So you're kind of learning, you're learning and embedding that kind of maximizes the,
the distance between your examples basically.
Yeah. And then a meta test time or a test time, you're given five examples of, or one
example of five different objects. So five images total. And then your goal is, well,
put those images into your embedding function to get the embedding of each of them.
And then you can just do, when you then get a new image image, then you compare it to
all of those, each of those five embeddings. And the closest one is the, then you assign
the class of that example to the new image.
Mm-hmm. Okay. Yeah, I need to do a show just on embeddings and word to vac and all these
things that I've been meaning to learn more about and haven't really had a chance to dig
into yet.
So you said there are a number of techniques and that's one of them. Are there others that
come to mind? Yeah, I'll talk a bit about the approach that we developed for my most
recent paper. So the method that we developed was largely inspired by fine-tuning. So in
computer vision, if you want to get good results on whatever tasks that you're doing, typically
you'll take a network that was trained on ImageNet, start from that network, start from
the weights of that network, and then fine-tune it on the tasks that you care about.
Mm-hmm. Okay, transfer learning, right? Yeah, exactly. But if you try to do this directly
for one shot learning, where you only fine-tune it on one example is going to overfit a lot.
So it works well for transfer learning with a sufficiently large training set for your,
for your, the tasks that you care about, where sufficiently large isn't as big as ImageNet,
but it's a reasonable size. And so the approach that we take is to actually optimize for a set
of features like ImageNet, such that when you fine-tune on a small number of examples,
you get good performance, good generalization on that task. Okay, and how does that work?
So you can write down this objective. It has a gradient in it that comes from the fine-tuning
procedure. What the objective looks like is essentially you have your, you have your
original weight vector, the features that you're trying to learn. And the updated feature
vector, which is just the fine-tune version of that, and you're trying to minimize the
loss of that updated feature vector with respect to your original parameter vector.
Mm-hmm. I say on home, but I think the, the limits of not being able to have a white
point in front of me is just, you know, we've reached that point. But what I kind of heard
in there was, yeah, if you think to linear regression, right, you are using these gradients
to try to get you to some kind of optimum and you are iterating over or descending, you
know, these gradients, those gradient descent. And what I heard was, you are kind of tweaking
the way you are descending the gradient so as to do something. Is there a way to finish
that sentence? Yeah. So you're, you're tweaking your loss function such that you're optimizing
for their performance after a gradient descent update on that task. And you do this for
a wide variety of tasks. So I guess the, the just is to train for a parameter vector that
can be very quickly adopted for a wide variety of tasks. And we can do this with, just
with gradient based methods, just essentially just with SGD. Okay. So you're, you're basically
changing your loss function so that I mean, it's kind of like a technique, like dropout
and some of these other things where you're doing, you know, funky things to your loss
function to be more impervious to overfitting. Kind of, we're actually optimizing. That, that
kind of was like at the 300 million thousand foot, you know, level. Yeah. You can kind
of see as optimizing for good generalization. Right. Right. And one of the nice things
about this approach is that, well, it sounds kind of complicated, but we actually write
it down. It's incredibly simple. I implemented it in less than a day. And you could apply it
to a few shot classification, like I talked about before and get really good results. But
you could also apply it to a wide range of other few shot learning problems, including
few shot learning of behavior. Okay. So I think I'm just going to take this as a challenge
to myself and anyone else who's listening that wants to dig into this to actually get
your paper and go through it. And, you know, perhaps we'll reconvene after I've done
that and see if I am able to have a coherent conversation about what we're discussing
here. And so in order to facilitate that, what is the name of the paper that you're describing?
Yeah. It's called model agnostic meta learning. Okay. Yeah. Those are the first four words
of the title. Okay. Well, we'll find that paper and we'll make sure that the link is
in the show notes. And anyone else who wants to, you know, dig into this with me, you
know, can just drop a comment in the show notes and we'll kind of exchange notes as we
learn us together. Yeah. I'm also thinking about writing a blog post on it at some point
in the near future. Oh, really? Awesome. Well, if you do that, I would, you know, be happy
to, you know, help in any way review it or ask you more dumb questions or whatever. Okay.
Great. And so since we're talking about some of your, your research and your papers,
any other pointers to papers that, you know, folks can dig into based on the things that
we've talked about, you know, what are the top three, you know, papers that you'd want
folks to take a look at to get a sense for your work? So the first one would be the one
that I just mentioned. The second would be a paper on inverse reinforcement learning called
guided cost learning. Okay. The third, let's see, it depends on how much, how much reading
they want to do. I think that all refer them to the paper that starts with a deep visual
foresight. Okay. That sounds compelling. So that one is, we didn't talk about that much,
but essentially that one's trying to learn a predictive model of video, being able to
predict the future video given the actions that the robot's going to take. Okay. Okay.
Awesome. Awesome. Well, we'll have links to all of those in the show notes. Before we go,
I got a request from one of our listeners a while ago, Shreys, who was about to embark
on his own PhD pursuits. And he asked if we could get a PhD student on and talk about
a little bit about their experiences as a PhD student and what are some things to keep
in mind to be successful in pursuing research in this field. And I was wondering if you would
maybe share some of your thoughts. You are obviously doing amazing research. You've
got two great advisors. What are your secrets to success? Yeah. I think that it's important
to continuously develop and learn throughout your PhD. So reading a lot of papers, especially
these days with archive, having a tremendous number of relevant machine learning papers
every day and working on research skills, learning from others around you. So at the
beginning of my PhD, I started working with a postdoc very closely and I learned a lot
from him on my first project. And the ability, like at the beginning, learn from people
who are more seniored from you is very helpful because doing, like, there's no one perfect
way to do research. I mean, if people, you're trying to solve problems that people haven't
solved before. And that's hard. And the way to approach that is, is different for everyone,
but I think that there's a lot to be learned from people who have been at it for a few
years or more. So I think that learning from others and being open to that is very important.
I've also developed my writing skills a lot in graduate school, depending on where you
want to go. That can be very important, especially if you want to go into academia or if you
want to continue publishing. But let's see. And then I think that work ethic is important,
trying to keep up with the field these days and trying to actually get things to work takes
a lot of work by nature. Research is, is trying to tackle on solve problems. And if those
problems were easy to solve, then they wouldn't be on solve problems. Right. Right. So in other
words, no shortcuts. Yeah. How do you keep up with archive and your paper reading list?
I don't think I necessarily have a good solution, but typically what I do is check archive
every day or every other day and see if there's any relevant papers. I don't, I will read
papers to a varying degree based on how relevant it seems and how good the paper seems. And
I also don't worry too much about missing papers because if it is a really good paper,
then it will rise up through conferences, through publications. I'll see them at different
publication venues or they'll become popular or common knowledge. Right. And so is it a
worthwhile question to ask like how many papers do you read a, you know, day, week month
or what have you or yeah, I read a paper end to end very infrequently. I guess the papers
that I have to review for conferences, I will read end to end. And I will only read
papers, other papers end to end if they are very relevant or if I volunteer to present
it at a group meeting, we have, we have reading group meetings fairly regularly. So that's
another good way to keep up with, keep up with a lot of new papers is find a group and
have someone volunteer to present a paper or kind of present the key findings from a
paper in that group. So then not everyone has to read everyone so we can just read it
and then summarize the points that are relevant to that group. And is there for papers that
are directly relevant to your research, is there a level beyond reading end to end where,
you know, you're actually kind of digging into the math and trying to figure out, you know,
what was some maybe missing steps that were glossed over in the paper or you're implementing
them or things like that? Or do you do that, you know, fairly infrequently as well?
Yeah, so if a paper is along the lines of what I'm working on, then typically I'll want
to compare to that paper or compare to some version of that paper. And so that will
involve grabbing an open source implementation or emailing the author or re-implementing
it myself. And yeah, also thinking about the shortcomings of the paper is important.
So one example is that I'm currently working on a certain application of metal learning
and there was a new paper that came out on a very similar topic and one of the shortcomings
was that the data set required for metal learning was huge. And so one of the benefits
of metal learning is that at test time you can learn from a very small amount of data.
But if you need a ton of data to learn that few shot learner, then it's not going to
be feasible for applying to real robotic systems. And so that's, so that shortcoming is something
that I'm one aware of when I try to, when I'm working on this problem and to something
that I want to address concretely in the work that I do.
Got it. Got it. I'm wondering out loud now, but I am intrigued by the thought of doing
like a virtual paper reading group, like something along the lines of a podcast or an extension
of the podcast. And I wonder if there are any readers or listeners rather that would,
I guess, also be readers would be interested in something like that. So if you are, you
know, shout out in the comments or Twitter or something like that and people are interested
maybe we can find a way to do something. But I guess with that, Chelsea, you have been
very gracious with your time. And I really appreciate you jumping on the, the skypline
here. It's been a really interesting conversation. And I have definitely learned a ton. And, you
know, my brain exploded a little bit, which is also a good sign like, and yeah, just,
you know, thank you. Thanks so much. Yeah. Happy. Awesome. All right. Bye-bye.
All right, everyone. That's our show for today. Thanks so much for listening and for your
continued support, comments, and feedback. We really, really appreciate hearing from you
and we love to incorporate your ideas into the show. I'd also like to thank our sponsor,
Banzai once again. Be sure to check out what they're up to at Banz.ai. And one last reminder,
next week, I'm at the O'Reilly AI Conference in New York City. You can still register using
our discount code, PC Twimble, PC-TW-I-M-L, for 20% off. And if you live in New York or
will be at the event, let's plan to meet up. I'm partnering with the NYAI Meetup to host
a happy hour on Thursday evening after the event. If you'd like more details, please sign
up using the form at twimbleai.com slash NY Meetup and we'll keep you posted. The notes
for this episode can be found at twimbleai.com slash talk slash 29. For more information on
industrial AI, my report on the topic or the industrial AI podcast series, visit twimbleai.com
slash industrial AI. As always, remember to post your favorite quote or takeaway from
this episode and we'll send you a laptop sticker. You can post them as comments to the
show notes page, via Twitter, at twimbleai or via our Facebook page. Thanks again for
listening and catch you next time.
