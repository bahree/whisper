1
00:00:00,000 --> 00:00:05,920
All right everyone welcome to another episode of the Twemble AI podcast. I'm your host Sam

2
00:00:05,920 --> 00:00:12,320
Charrington and today I'm joined by Meg Mitchell, a chief ethics scientist and researcher at Hugging

3
00:00:12,320 --> 00:00:17,280
Face. Before we get into today's conversation be sure to take a moment to head over to Apple

4
00:00:17,280 --> 00:00:23,120
podcasts or your listening platform of choice and if you enjoy the show please leave us a five-star

5
00:00:23,120 --> 00:00:29,200
rating and review. Find us just over a year ago with a friend of the show, another friend of the

6
00:00:29,200 --> 00:00:36,000
show I should say Emily Bender. That was episode 467 for those keeping track where we discussed

7
00:00:36,000 --> 00:00:44,560
your paper on the dangers of stochastic parrots and of course a ton has changed for you since then.

8
00:00:45,520 --> 00:00:52,320
Welcome back to the podcast Meg. How's everything going? You thank you. Yeah it's great to be here.

9
00:00:52,320 --> 00:00:59,120
I should say that the paper wasn't on the dangers of stochastic parrots. It was on the dangers

10
00:00:59,120 --> 00:01:05,680
of large language models comparing the two stochastic. We don't have a fear of parrots.

11
00:01:05,680 --> 00:01:13,840
So the title on the ACM library is on the dangers of stochastic parrots can language models be too big?

12
00:01:14,560 --> 00:01:19,680
I feel like for this one we should just leave Amari as the voice of God coming in and

13
00:01:19,680 --> 00:01:33,760
make us what the title is paper is. In any case Meg welcome back and let's have you reintroduce

14
00:01:33,760 --> 00:01:42,880
yourself to the audience. Yeah so I am a computer scientist with a background in natural language

15
00:01:42,880 --> 00:01:49,760
processing as well as a bit in computer vision and then for the past six or so years I've been

16
00:01:49,760 --> 00:01:56,480
working on ethical AI which brings in a lot of things about natural language processing and

17
00:01:56,480 --> 00:02:04,480
computer vision in terms of issues like foreseeable harms and misuse and things like that.

18
00:02:04,480 --> 00:02:14,640
And I've worked at Microsoft Research, Google, Google Brain and then recently joined hugging

19
00:02:14,640 --> 00:02:22,640
face full-time which is a startup that is an open source community making machine learning

20
00:02:22,640 --> 00:02:30,240
models and data sets generally available. Awesome awesome and are you the first chief ethics

21
00:02:30,240 --> 00:02:36,640
scientist at the company? Well we're a startup so we there aren't exactly

22
00:02:38,400 --> 00:02:45,520
very set in stone titles. You can kind of start coming up with your own titles for things.

23
00:02:45,520 --> 00:02:52,880
I was definitely the first AI ethics computer scientist. I'm excited to share that we're going

24
00:02:52,880 --> 00:03:01,120
to hire a for real ethicist which will be amazing. It's pretty rare to be able to hire an ethicist

25
00:03:01,120 --> 00:03:07,360
in an AI company. I think that that is something that only came into people's attention very recently.

26
00:03:07,360 --> 00:03:13,040
But yeah I mean I came to work at hugging face because I really wanted to have some time to

27
00:03:13,040 --> 00:03:20,000
just code and code open source. At the larger tech companies you find as you sort of rise higher

28
00:03:20,000 --> 00:03:25,440
and higher in the hierarchy you have less and less time to code and it's replaced more and more

29
00:03:25,440 --> 00:03:33,440
by meetings which is sort of the opposite of what you want as you succeed right. Like if you love

30
00:03:33,440 --> 00:03:40,400
coding and you succeed more why would that be taken away in favor of meetings which I absolutely

31
00:03:40,400 --> 00:03:47,040
hate. So after having you know a couple years where basically it was really hard for me to get

32
00:03:47,040 --> 00:03:52,960
coding time in. I really wanted to join hugging face and just sort of not worry about tons of

33
00:03:52,960 --> 00:03:58,880
meetings not worry about tons of sinking across a large organization because it was already small.

34
00:03:58,880 --> 00:04:03,360
I could really just you know get my hands dirty with code and starting to put out some tools

35
00:04:03,360 --> 00:04:09,040
that I really really wanted to prioritize. And then once I got that out of my system a little bit

36
00:04:09,760 --> 00:04:15,360
I was able to start popping up a little more and working more on things like defining the culture

37
00:04:15,360 --> 00:04:22,240
hiring practices, different kinds of processes that we want to put in place for consideration of

38
00:04:22,240 --> 00:04:27,600
what models get shared, what data get shared and things like that. I've been doing a little less

39
00:04:27,600 --> 00:04:35,200
coding but it's still a coding company essentially. So getting to do a mix of defining top down

40
00:04:35,920 --> 00:04:41,040
structures and processes in the company focusing on like diversity and inclusion and ethical

41
00:04:41,040 --> 00:04:47,520
considerations as well as more bottom up just you know pure coding to share with the world.

42
00:04:47,520 --> 00:04:55,040
So it's a nice mix. Got it got it. And the the coding that you're doing is that

43
00:04:55,040 --> 00:05:04,800
been largely focused or tied into the area of ethics or is are they two separate kind of things

44
00:05:04,800 --> 00:05:11,600
that you find ways to connect. Yeah so I would say that all the coding that I'm prioritizing right

45
00:05:11,600 --> 00:05:18,720
now is intimately tied to current discussions around AI ethics. So you'll see that over the past

46
00:05:18,720 --> 00:05:26,320
year so there's been more and more scholarship in the AI ethics world about data and data sets and

47
00:05:26,320 --> 00:05:34,960
things like benchmarking and the fact that the culture of machine learning has generally had

48
00:05:34,960 --> 00:05:41,920
a really laissez-faire approach to data collection and data sharing. But if we're trying to think about

49
00:05:42,640 --> 00:05:49,360
the long-term effects of technology and the long-term effects of things like sharing data

50
00:05:50,000 --> 00:05:54,880
then we need to think very critically about things like data curation instead and what does

51
00:05:54,880 --> 00:06:01,280
data curation mean? It means things like what kind of values do we want to encode in the data

52
00:06:01,280 --> 00:06:07,840
as well as in the data set development process in order to create data sets that foreseeably

53
00:06:07,840 --> 00:06:13,200
can help more than hurt. And so that means things like coming up with measurements,

54
00:06:13,200 --> 00:06:21,040
quantifications of the data that can be inspired by human values. So there's been some work on this

55
00:06:21,040 --> 00:06:27,440
over the past year as well, but I'm trying to really step that up a bit. And so what does that

56
00:06:27,440 --> 00:06:34,000
mean? That means coming up with metrics for the demographic diversity within a data set,

57
00:06:34,000 --> 00:06:39,280
coming up with metrics for stereotyping. How much a data set might run the risk of propagating

58
00:06:39,280 --> 00:06:46,080
harmful stereotypes? Doing things like measuring the naturalness of the language, how contrived it is

59
00:06:46,080 --> 00:06:53,840
versus how much it actually reflects the kind of language that we'd expect to see when a model

60
00:06:53,840 --> 00:06:59,600
trained on it is actually used, that kind of thing. So data development has been a large focus of

61
00:06:59,600 --> 00:07:06,960
mine and particularly through the lens of what can we measure aligned to human values and what we

62
00:07:06,960 --> 00:07:16,320
wanted to develop. Awesome, awesome. I also want to bring up your participation in the Wiki M3L workshop

63
00:07:16,320 --> 00:07:22,480
at iClear. Can you tell us a little bit about that workshop that goes with the workshop and

64
00:07:22,480 --> 00:07:30,240
what you're speaking about there? Yeah, so it's an interdisciplinary workshop that's focusing

65
00:07:30,240 --> 00:07:36,800
on a lot of different topics. I think part of the idea was essentially to bring together people

66
00:07:36,800 --> 00:07:44,480
who are working on different aspects of AI and how those aspects can be really relevant to the

67
00:07:44,480 --> 00:07:52,400
community. So this is joint with Wiki Media and Wiki Media is really passionate about having

68
00:07:52,400 --> 00:08:00,240
everything be open and transparent and so what that means for computer vision and LP,

69
00:08:00,240 --> 00:08:04,960
machine learning, what we should make available, what we shouldn't, what that means in terms of

70
00:08:04,960 --> 00:08:10,240
the uses of the models that we're developing and then how they might actually be used.

71
00:08:12,000 --> 00:08:17,120
All of that is sort of under the umbrella of this workshop. So touching on a lot of different

72
00:08:17,120 --> 00:08:26,000
issues relevant to the socio-technical context of machine learning. And your particular session

73
00:08:26,000 --> 00:08:33,040
at the workshop? So I get to be involved into one is a panel. The panel is looking at

74
00:08:34,000 --> 00:08:40,240
multi-modality which is an area I've worked on to do things like image descriptions for people

75
00:08:40,240 --> 00:08:49,200
who are blind. And then also there's a keynote I'm giving on biases in AI. The session is called

76
00:08:49,200 --> 00:08:56,400
biases in AI and indigenous data sovereignty. So there's another speaker. I believe his name is

77
00:08:56,400 --> 00:09:03,600
Michael Running Wolf. Awesome. And I should mention I mentioned the workshop by its short name Wiki

78
00:09:03,600 --> 00:09:13,040
M3L but that is Wikipedia and multimodal and multilingual research. And so your conversation

79
00:09:13,040 --> 00:09:24,160
at the and presentation at the workshop focus on data governance and biases in AI that sounds

80
00:09:24,160 --> 00:09:29,680
not very far field from the data curation work that we talked about about earlier. What's the

81
00:09:29,680 --> 00:09:36,720
specific angle that you're taking at the workshop? So my plan so far subject to change

82
00:09:38,480 --> 00:09:47,520
still still chatting is to talk about the kinds of assumptions and values that get encoded

83
00:09:47,520 --> 00:09:53,440
when we are creating and sharing data sets. So there's a lot of things you can talk about when

84
00:09:53,440 --> 00:10:00,080
you say biases in AI that means tons of different things. But my particular interest I think

85
00:10:00,080 --> 00:10:08,240
relevant to this workshop is around how we have inclusive sharing of data sets and creation of

86
00:10:08,240 --> 00:10:17,360
models that don't disproportionately underperform on some sub-populations or sort of exploit some

87
00:10:17,360 --> 00:10:23,680
populations in the data collection practices. So this has a little bit more to do with biases

88
00:10:23,680 --> 00:10:30,240
from the perspective of how we're approaching data set collection, what are sort of internal

89
00:10:30,240 --> 00:10:37,200
cognitive biases and cultural biases are doing in our approach to data and the kind of models we

90
00:10:37,200 --> 00:10:42,640
develop as opposed to something that's I think more traditional bias than AI where people think

91
00:10:42,640 --> 00:10:48,560
of things like fairness. I won't be focusing on fairness. I'll be talking more about the context

92
00:10:48,560 --> 00:10:56,240
of of data and how they relate to models and advocating for things like data sovereignty

93
00:10:56,960 --> 00:11:04,560
which is the other part of that session where individuals and communities who are creating the

94
00:11:04,560 --> 00:11:12,560
data have rights for that data as opposed to the current state of the art which is a set

95
00:11:12,560 --> 00:11:20,640
essentially being exploited without consent to have their data used in models that are then

96
00:11:20,640 --> 00:11:26,560
productionized for profit and things like that. So focusing more on the on the data sovereignty side

97
00:11:26,560 --> 00:11:36,160
of things. On the biases side of things it sounds like what you're alluding to is this idea that

98
00:11:36,160 --> 00:11:43,520
as we as practitioners, researchers, a community got and collect these data sets, there are inherent

99
00:11:43,520 --> 00:11:50,560
ways that we think about the world that are influencing the ultimate results of these data sets.

100
00:11:50,560 --> 00:11:55,120
Can you talk a little bit more about the specific examples and how that comes to be?

101
00:11:55,120 --> 00:12:01,680
This is also really relevant to work I've been doing in big science which is this massive effort

102
00:12:01,680 --> 00:12:07,920
international effort with volunteers from a bunch of different countries working on training

103
00:12:07,920 --> 00:12:13,360
this large language model. And I'll just jump in to suggest the folks that want to learn more about

104
00:12:13,360 --> 00:12:20,400
that can check out my recent interview with your colleague Thomas Wolff that's episode 564.

105
00:12:22,080 --> 00:12:27,360
And so in that I'm unsurprisingly working on data as well but this is

106
00:12:27,360 --> 00:12:34,000
data is more and more my thing I think in part because it's generally been less appreciated than

107
00:12:34,960 --> 00:12:42,480
ML work. I mean data collection practices have morphed over the years. You'll see in some of the

108
00:12:42,480 --> 00:12:50,240
first larger data sets that that were developed for use in what was at that time called corpus

109
00:12:50,240 --> 00:12:59,520
linguistics were very carefully balanced made sure to pay attention to the licenses and rights

110
00:12:59,520 --> 00:13:04,240
of the various people who created the data. So we see things like like the brown corpus

111
00:13:05,280 --> 00:13:11,920
coming around in the early 60s actually taking a look at things like fiction and sports and travel

112
00:13:11,920 --> 00:13:15,600
and all these sort of different topics and trying to be really balanced and making sure that they

113
00:13:15,600 --> 00:13:20,480
have agreement from the holders of the data that this is okay to use and distribute things like that.

114
00:13:21,680 --> 00:13:28,000
You know fast forward into the 90s where we're starting to see a switch from corpus linguistics

115
00:13:28,000 --> 00:13:35,120
to computational linguistics. I mean there's still separate fields but there's more and more

116
00:13:36,000 --> 00:13:41,200
changing at that time in the kinds of work that corpus linguists are doing more towards computational

117
00:13:41,200 --> 00:13:47,920
linguistics and what that means is that you start trying to get bigger and bigger data sets

118
00:13:48,480 --> 00:13:55,760
where the size matters a lot more than the quality. So quantity over quality sort of thing and then

119
00:13:56,640 --> 00:14:02,800
then moving into oh I should say but at that time there was still a lot of respect for the rights

120
00:14:02,800 --> 00:14:09,920
and licenses and so a lot of the data collected in the 90s was from newswire because those were

121
00:14:09,920 --> 00:14:18,720
the sources that would allow this public use into the 2000s as you start to have web 2.0 basically

122
00:14:18,720 --> 00:14:26,320
you start to see a lot of content produced online from individuals reflecting more natural language

123
00:14:26,320 --> 00:14:32,720
right so when we work on natural language processing newswire is just some some sort of contrived

124
00:14:32,720 --> 00:14:39,280
kind of way of talking you know it has like a lot of norms you have to abide by whereas regular

125
00:14:39,280 --> 00:14:46,320
conversation or what we're writing online is a lot more reflective of sort of everyday language use

126
00:14:47,200 --> 00:14:52,480
so with kind of the birth of web 2.0 you see people collecting data from the web

127
00:14:53,920 --> 00:15:00,880
sort of mining it to create larger data sets to build models on with less and less attention paid

128
00:15:00,880 --> 00:15:08,240
to what those data sets actually were representing and more attention paid to what can I get that's

129
00:15:08,240 --> 00:15:14,560
relevant to the topic or task that I care about and so with that you see a little bit more of a

130
00:15:14,560 --> 00:15:21,840
loss of rights this also relates to the fact that when we communicate online our data can be

131
00:15:21,840 --> 00:15:28,640
technically owned by the platform right so when we tweet that data is owned by twitter

132
00:15:30,480 --> 00:15:36,320
you know things like that and so even people who are creating really relevant data

133
00:15:36,320 --> 00:15:44,080
and this can be called data labor you know essentially creating the content themselves they don't

134
00:15:44,080 --> 00:15:50,880
end up having as many rights to that actual data where other companies can use that data

135
00:15:50,880 --> 00:15:58,320
it could be further shared to help to help build other sort of models or to help other companies

136
00:15:58,320 --> 00:16:04,160
without the people actually creating the data's without the knowledge of the people actually

137
00:16:04,160 --> 00:16:11,440
creating that data and then even when you are putting in place some things that clearly has

138
00:16:12,720 --> 00:16:20,640
the intention of rights so even when you are putting on things like creative comments licenses

139
00:16:21,280 --> 00:16:30,880
that kind of thing it doesn't actually map to how your data can legally be used by

140
00:16:30,880 --> 00:16:40,320
by anyone at least in practice right so let me jump in and say what I'm hearing is you know this

141
00:16:40,320 --> 00:16:45,760
kind of field of play around creating these large language models and other kind of large models

142
00:16:45,760 --> 00:16:52,160
has evolved quickly and you know there's a lot of rules that haven't been established or conversely

143
00:16:52,160 --> 00:16:58,800
norms that aren't necessarily you know universally beneficial have established I'd love to have you

144
00:16:58,800 --> 00:17:10,080
talk about the specific harms that you see at play like clearly there's this idea of economic

145
00:17:10,080 --> 00:17:17,760
harm in a sense of people are contributing this data other companies are monetizing it in ways

146
00:17:17,760 --> 00:17:23,280
that weren't necessarily disclosed to the people that contributed the data and the benefit isn't

147
00:17:23,280 --> 00:17:28,720
being shared and you could you know argue whether that's theoretical harm or actual harm but

148
00:17:29,600 --> 00:17:36,480
there's this idea of an economic harm there's idea this idea of I should have the right to give

149
00:17:36,480 --> 00:17:41,840
you permission or not I don't know how to phrase what the right phrasing would be for the harm that

150
00:17:41,840 --> 00:17:48,000
is involved in that but those are a couple of examples of harms and I just wanted to to have you

151
00:17:48,000 --> 00:17:53,840
elaborate on that and further develop you know is there a taxonomy of harms that has arisen from

152
00:17:53,840 --> 00:18:00,240
the way that we're collecting these datasets yeah so there there has been work on on taxonomy

153
00:18:00,240 --> 00:18:07,440
of harms I think people in in different groups have come up with their own there isn't as far as I

154
00:18:07,440 --> 00:18:15,920
know a generally agreed upon kind of taxonomy but one of them one of the key issues is around privacy

155
00:18:15,920 --> 00:18:26,560
so when we share content online we have a basic understanding or trust that what we're producing

156
00:18:26,560 --> 00:18:35,200
is not going to not going to be suddenly on some graduate students laptop or something like that

157
00:18:35,200 --> 00:18:43,600
so if you have information about your personal life that can be used to track you if I mean there's

158
00:18:43,600 --> 00:18:50,000
also things like personally identifying information so with just a few characteristics of an

159
00:18:50,000 --> 00:18:55,840
individual you can often identify who they are which can give rise to things like stocking to

160
00:18:55,840 --> 00:19:03,280
things like identity theft as well as just sort of the very the very personal issue of having things

161
00:19:03,280 --> 00:19:11,280
you say be used by other people without you really knowing and potentially even use for technology

162
00:19:11,280 --> 00:19:16,960
that you don't want to support I think Flickr is a really great example where it's been mined for

163
00:19:16,960 --> 00:19:24,960
years for computer vision applications and so some of that some of that data is used for things

164
00:19:24,960 --> 00:19:31,840
like facial recognition but theoretically and I think you know in reality a lot of people whose

165
00:19:31,840 --> 00:19:37,200
faces were mined don't actually want to help build facial recognition and so there's a disconnect

166
00:19:37,200 --> 00:19:43,920
there between what people's intent and expectations are when they're creating the data and how that

167
00:19:43,920 --> 00:19:49,520
data is actually being used in a way where it could potentially even come back and harm them so

168
00:19:49,520 --> 00:19:56,000
for example facial recognition does not work as well for black people that as white people there's

169
00:19:56,000 --> 00:20:01,440
an argument to be made that maybe white people don't want to have their their faces shared as much

170
00:20:01,440 --> 00:20:08,160
if it's going to create that kind of issue there's a concern that facial recognition is used as

171
00:20:08,160 --> 00:20:14,320
a form of discrimination so you know black people tend to be more targeted and so black people who

172
00:20:14,320 --> 00:20:20,240
have their data as part of what's using what is being used to train a facial recognition system

173
00:20:21,200 --> 00:20:26,560
is really not a situation that they ever hoped to be in where it can come back and harm them if

174
00:20:26,560 --> 00:20:33,680
they're targeted so it ends up creating these these problematic kinds of cycles where the beliefs

175
00:20:33,680 --> 00:20:41,600
and intentions of people who are creating the data are not actually met by how different

176
00:20:41,600 --> 00:20:48,720
companies and organizations end up using that data potentially to disrupt their privacy

177
00:20:49,840 --> 00:20:55,280
as well as as well as things like creating technology that could come back to hurt them

178
00:20:55,280 --> 00:21:03,600
time back to your earlier comment about connecting the inherent beliefs of the data set

179
00:21:03,600 --> 00:21:12,960
collectors to the data sets and these these problems is it you know by the inherent beliefs of

180
00:21:12,960 --> 00:21:17,680
the the folks collecting data sets you know are you meeting things like disregard for

181
00:21:18,400 --> 00:21:22,800
you know the the privacy considerations or are there specific more specific things that

182
00:21:22,800 --> 00:21:28,640
you're trying to get out there yeah disregard for but also private information that that can be

183
00:21:28,640 --> 00:21:36,480
used to identify people so you know if you have bad actors malicious actors your data can potentially

184
00:21:36,480 --> 00:21:43,360
be mined to steal your social security number credit card number these are the sorts of things

185
00:21:43,360 --> 00:21:50,240
that end up being stolen all the time as well as you know figuring out personal information about

186
00:21:50,240 --> 00:21:57,280
you which is a kind of fundamental harm when it comes to psychological safety and so when you

187
00:21:57,280 --> 00:22:05,200
think about where we are from a data rights perspective what's the kind of frontier the research

188
00:22:05,200 --> 00:22:14,000
or frontier of the practice is it primarily what we're seeing around you know legislative and

189
00:22:14,000 --> 00:22:21,520
regulatory action or you know how do you think about the state of the world yeah so there has been

190
00:22:21,520 --> 00:22:27,920
a lot more work in the regulatory space on data protection data protection laws I think most

191
00:22:27,920 --> 00:22:35,200
people are familiar with GDPR but a lot of countries very recently have put forward some other

192
00:22:35,200 --> 00:22:43,520
sorts of data protection legislation and and one of the ideas underlying this is that if people

193
00:22:43,520 --> 00:22:49,840
create the data they own the data and that really takes away the profit models of companies like

194
00:22:49,840 --> 00:22:56,640
you know Google that uses this information in order to essentially get people to look at ads more

195
00:22:57,600 --> 00:23:01,920
so you know if they don't get to own that data then you know that opens up

196
00:23:02,960 --> 00:23:08,400
tons of other companies that that can use it or also or it also takes away their ability to use

197
00:23:08,400 --> 00:23:20,320
that data at all and so right and so you see that there is there are ideas around people needing

198
00:23:20,320 --> 00:23:26,720
to consent to have their data being used and so particularly in China's new legislation around

199
00:23:26,720 --> 00:23:34,400
data protection there's this idea that if you have an instance from an individual and that

200
00:23:34,400 --> 00:23:40,640
individual is an anyway identifiable you have to go and find that person and ask for their

201
00:23:40,640 --> 00:23:47,920
consent to use that data so you see a little bit of individual data rights starting to pop up

202
00:23:49,040 --> 00:23:55,920
that said you know geopolitical entities like the EU are also trying to put together things

203
00:23:55,920 --> 00:24:02,320
like data storehouses where that data is open to everyone so it's a little bit of the reverse

204
00:24:02,320 --> 00:24:11,200
where maybe you don't have have rights to it and instead the the ability to use that data can

205
00:24:11,200 --> 00:24:16,960
be distributed amongst everyone in the country in order to you know maximize the overall profit

206
00:24:16,960 --> 00:24:22,720
in the geopolitical entity and so there is a little bit of a push and pull I think right now where

207
00:24:22,720 --> 00:24:29,360
you see some legislation moving in the direction of individuals having some rights over their data

208
00:24:29,360 --> 00:24:37,360
at the same time as you see some legislation coming out that that sort of proposes that an

209
00:24:37,360 --> 00:24:44,240
individual's data should be open for everyone to use rather than an individual's company so I

210
00:24:44,240 --> 00:24:48,240
think there's there's a lot of discussions right now in which way these things should be going

211
00:24:49,600 --> 00:24:54,960
you know I'm obviously personally a proponent of the idea that everybody should own the data

212
00:24:54,960 --> 00:24:59,280
that they create and then they should be able to consent to whether or not it's it's shared more

213
00:24:59,280 --> 00:25:06,240
broadly but you know I might be in the minority there but all the more reason to sort of talk on

214
00:25:06,240 --> 00:25:13,680
these kinds of programs and present at that places like the the wiki m3l workshop I hadn't heard

215
00:25:13,680 --> 00:25:22,560
about the data privacy regulation in China that you referenced and some ways it flies a little bit

216
00:25:22,560 --> 00:25:28,080
in the face of the broader narrative about privacy in China the social credit score on all these

217
00:25:28,080 --> 00:25:33,920
things yeah do you have any additional perspective there I'm really confused by that actually

218
00:25:36,640 --> 00:25:43,760
so I read through I read through their legislation you know not not being a lawyer and you know

219
00:25:43,760 --> 00:25:48,400
sort of ask people who are more legal scholars if my interpretation of what it was saying was

220
00:25:48,400 --> 00:25:54,560
right and it seems like it is right and so I'm really not sure what's going on there I feel like

221
00:25:54,560 --> 00:26:00,880
it might be someone it might be the job of someone who's more knowledgeable about about China and

222
00:26:00,880 --> 00:26:07,200
you know China's government and and what they're trying to sort of get out there I'm really yeah

223
00:26:07,840 --> 00:26:13,200
I'm I'm very confused by it personally it'd be sort of ironic I suppose if China leads the way

224
00:26:13,200 --> 00:26:19,120
an individual data rights but we might be moving in that direction right now well if anyone knows

225
00:26:19,120 --> 00:26:25,760
anyone who's listening to anyone who's listening to this knows anyone let me get I know yes please

226
00:26:25,760 --> 00:26:33,200
I want to understand you know returning to data curation you can talk a little bit about some

227
00:26:33,200 --> 00:26:41,200
of the ways that your work around measurement and quantification helps you kind of think through

228
00:26:41,200 --> 00:26:47,360
and address the kinds of issues that we've been talking about so far yeah so we want to understand

229
00:26:47,360 --> 00:26:54,320
the quality of data and part of what I'm pushing for is prioritizing quality over quantity a little

230
00:26:54,320 --> 00:27:02,320
bit more arguably if you have good quality data you don't need as much as much of the quantity of

231
00:27:02,320 --> 00:27:09,680
data which means that you can be a lot more selective about whether or not you are including

232
00:27:09,680 --> 00:27:14,720
personal information or personally identifying information whether or not you're including

233
00:27:14,720 --> 00:27:20,800
things like stereotypes and biases which can then be laundered through a model and then pushed

234
00:27:20,800 --> 00:27:28,000
back out at the community affecting all kinds of decisions that happen under the hood and so if

235
00:27:28,000 --> 00:27:36,800
you can start to measure for example the strength of association between a term like smile and

236
00:27:36,800 --> 00:27:43,040
woman versus smile and man in some slice of data that you're thinking of then if it's a very

237
00:27:43,040 --> 00:27:48,000
high association and you want to be working towards data that can reflect a more equitable universe

238
00:27:48,800 --> 00:27:54,400
then perhaps you don't want to sample that data directly perhaps you want to do something a

239
00:27:54,400 --> 00:28:00,000
little bit more clever in order to make sure that there isn't such a strong skill so by being able to

240
00:28:00,000 --> 00:28:05,840
to measure these kinds of skews come up with actual measurements that can do this now we can

241
00:28:05,840 --> 00:28:12,800
start quantifying serious issues in the data and using that to inform what we what we do collect

242
00:28:12,800 --> 00:28:18,320
one of the ways that one of the early ways that as a community we've sought to

243
00:28:19,600 --> 00:28:25,360
do a better job around understanding the biases and data sets are kind of like data sheets for

244
00:28:25,360 --> 00:28:35,600
data sets and that workshop that body of work by Timnett and others Timney and the model cards and

245
00:28:35,600 --> 00:28:43,600
it sounds like what you're doing connects to those efforts increasing to sophisticated you know

246
00:28:43,600 --> 00:28:49,120
those things are are dashboards you're increasing the sophistication of the metrics exactly exactly

247
00:28:49,120 --> 00:28:55,760
and actually I was the first author on model cards so I would say you know the so I'm working on

248
00:28:55,760 --> 00:29:03,200
model cards still and it goes to one big part of that is what you evaluate and so that again

249
00:29:03,200 --> 00:29:07,920
goes to the data so I've really been focusing on that aspect of model cards and the kind of things

250
00:29:07,920 --> 00:29:13,280
you want to quantify in the data and I've also been working on data cards which is heavily inspired

251
00:29:13,280 --> 00:29:21,680
by data sheets but also different data cards are shorter they're a card not a sheet so

252
00:29:23,680 --> 00:29:29,520
just make sure I understand that yeah well so data sheets are pretty comprehensive in terms of

253
00:29:29,520 --> 00:29:34,640
all the different decisions that go into creating a data set so they're really wonderful artifacts

254
00:29:34,640 --> 00:29:40,560
to have in terms of auditing the development of machine learning systems they're a little bit

255
00:29:40,560 --> 00:29:46,720
more difficult to do in terms of getting developers to actually implement them so if you're you

256
00:29:46,720 --> 00:29:56,800
know asking someone who who is trying to upload their data sets all the details of you know the

257
00:29:56,800 --> 00:30:01,760
compensation protocols from crowd workers and things like that they're going to be a lot less likely

258
00:30:01,760 --> 00:30:08,480
to to fill it out at all and so you know data cards are trying to get a bit at how can we

259
00:30:08,480 --> 00:30:16,960
incentivize good practices or how can we encourage good practices while still you know while

260
00:30:16,960 --> 00:30:22,080
still being somewhat palatable to people who are who are uploading and and sharing the data set

261
00:30:22,080 --> 00:30:27,760
so it's not as ideal as data sheets it's it's sort of speaking to the cynical reality that a lot

262
00:30:27,760 --> 00:30:35,200
of people don't care and how do we sort of get them to start caring and so so part of that is

263
00:30:35,200 --> 00:30:42,560
creating measurements that can be automatically run on the data set and then be used to fill out

264
00:30:42,560 --> 00:30:48,160
the data card so that goes back again to how to quantify things in the data set like the biases

265
00:30:48,160 --> 00:30:54,800
and skews like the naturalness which side note this is this is related to the zip fee and distribution

266
00:30:54,800 --> 00:31:02,160
of language it's super fun to implement and measure but then you know so but these measurements

267
00:31:02,160 --> 00:31:06,800
are things that can just be run automatically and the more things that you can run automatically

268
00:31:06,800 --> 00:31:13,520
the more buy and there will be to have things like data cards released alongside your data

269
00:31:14,160 --> 00:31:20,800
so we're trying to sort of lower the barrier to having some sort of you know responsibility informed

270
00:31:26,080 --> 00:31:31,760
development of data and sharing of data by providing some automatic quantifications

271
00:31:31,760 --> 00:31:36,880
that speak to values of discrimination that speak to or I should say speak to values of

272
00:31:36,880 --> 00:31:44,080
non-discrimination that speak to ideas around having naturalistic data or data that's balanced

273
00:31:44,080 --> 00:31:50,000
a bunch of variety of topics all these kinds of things that we ideally can have in a perfect data

274
00:31:50,000 --> 00:31:56,000
curated setting but but coming up with those measurements automatically so that people can see

275
00:31:56,000 --> 00:32:02,000
what kind of characteristics their data actually have and then people can actually decide whether

276
00:32:02,000 --> 00:32:09,200
not to use it for the various you know use cases that they that they foresee and so so this data

277
00:32:09,200 --> 00:32:16,400
measurement area of work that I'm digging into more and more is really to help lower the barrier

278
00:32:18,000 --> 00:32:24,080
to create just sort of more responsible AI development life cycles and really being clear about

279
00:32:24,080 --> 00:32:34,400
what the data is encoding on the topic of buy-in the cycle of adoption isn't surprising but it's my

280
00:32:34,400 --> 00:32:44,000
sense that after kind of a while you know a couple of years of not hearing a lot about data sheets

281
00:32:44,000 --> 00:32:50,160
data cards after their proposal and introduction now I'm starting to hear about their use quite

282
00:32:50,160 --> 00:32:55,840
frequently you know at least relatively is that your sense as well yeah well so personally I would

283
00:32:55,840 --> 00:33:03,360
say that my research has generally been around three years ahead of the general pickup but it's

284
00:33:03,360 --> 00:33:09,040
true I mean I've just noticed it again and again which is great for your citations because then it

285
00:33:09,040 --> 00:33:13,200
ends up being that your work is foundational so I think that's generally true for for me with

286
00:33:13,200 --> 00:33:19,840
the model cards work and it's also true for for Tim neat with the data sheets work because you know

287
00:33:19,840 --> 00:33:25,600
when you're working in AI ethics you're thinking very very deeply about issues at a level of nuance

288
00:33:25,600 --> 00:33:33,040
that people who who aren't working on AI ethics day to day don't really see so it's not until you

289
00:33:33,040 --> 00:33:39,840
start seeing the issues that you start paying attention to the AI ethics things and then that's when

290
00:33:39,840 --> 00:33:44,880
you start to see a little bit more of the need to address some sort of nuances and that's when

291
00:33:44,880 --> 00:33:50,480
you get the pickup from work that people who had already been deeply thinking about these things

292
00:33:50,480 --> 00:34:00,160
have put in place for you so ideally we are moving ideally we are defining things before they're

293
00:34:00,160 --> 00:34:05,840
needed so that once they are needed they're already there and available and I would say that's

294
00:34:05,840 --> 00:34:11,200
part of the goal in ethical AI work is to look forward you look towards the future you you try

295
00:34:11,200 --> 00:34:17,760
and use foresight as much as possible and then create the various tools and processes around that

296
00:34:17,760 --> 00:34:23,040
so they're they're ready to go once people are noticing the issues are actually coming up

297
00:34:23,040 --> 00:34:36,720
and I love to hear how the the these you know papers data sheets cards etc which in some sense

298
00:34:36,720 --> 00:34:42,160
you first hear those yeah that makes sense we should do that I I love to hear how those

299
00:34:42,960 --> 00:34:49,680
have become platforms for you know maybe more you know crunchier work around specific measurements

300
00:34:49,680 --> 00:34:56,640
and being able to generate them efficiently and and that kind of thing I am finding interesting

301
00:34:56,640 --> 00:35:05,360
that in your case the this research about the or this paper about model cards has led to

302
00:35:06,640 --> 00:35:15,440
what I perceive as more I said crunchier but like more technical or more concrete research

303
00:35:15,440 --> 00:35:21,200
into the metrics that then populate those cards yeah yeah and you're getting into you mentioned

304
00:35:21,200 --> 00:35:30,080
Zipfian distributions and and other stuff that kind of give kind of fill out this idea of hey

305
00:35:30,080 --> 00:35:36,240
you should know what your model is about and you should publish data about it right yeah yeah so it's

306
00:35:36,240 --> 00:35:42,160
because I would say it's because people are starting to be affected by the problems that we were

307
00:35:42,160 --> 00:35:49,120
for seeing you know a few years ago right so there's the now now unfortunately classic um

308
00:35:50,720 --> 00:35:58,000
guerrilla thing uh the the Google guerrillas issue um where it had tagged people who are black

309
00:35:58,000 --> 00:36:04,640
as guerrillas um and that was massively offensive uh hitting on a lot of historical harm right um

310
00:36:04,640 --> 00:36:11,520
and so one of the one of the solutions to handling something like that is looking at the

311
00:36:11,520 --> 00:36:18,080
association between different skin tones and different labels right so obvious but if you're

312
00:36:18,080 --> 00:36:22,960
not thinking that way if you're not sort of thinking through how these things can happen

313
00:36:22,960 --> 00:36:27,920
then you don't implement those things right and so that's a kind of measurement this sort of a

314
00:36:27,920 --> 00:36:33,760
problematic associations um that we were trying to put in place as we were for seeing like oh

315
00:36:34,640 --> 00:36:39,760
given what we know about the data sets that being that are being collected for language models

316
00:36:39,760 --> 00:36:45,280
language models are going to hate Muslims right and then that was found like years later

317
00:36:45,280 --> 00:36:51,040
but by doing an analysis of the data you know previously it was very clear that that was something

318
00:36:51,040 --> 00:36:57,280
that language models were going to learn to propagate um so as as the public has started to see

319
00:36:57,920 --> 00:37:02,960
the issues that we were for seeing they're sort of turning to see well what can be done what are we

320
00:37:02,960 --> 00:37:08,960
thinking um and it turns out that that one of the key things to be able to do is to document because

321
00:37:08,960 --> 00:37:14,720
as you document you can trace where issues are coming from and address them and this was this was

322
00:37:14,720 --> 00:37:20,400
one of the main takeaways hopefully uh from the stochastic parrots paper although I think people

323
00:37:20,400 --> 00:37:26,560
were maybe a little bit distracted by the the story the massive firing from Google yeah the back

324
00:37:26,560 --> 00:37:32,000
but the back story but you know we actually thought that the the main exciting thing from that

325
00:37:32,000 --> 00:37:38,880
that paper was uh where that was the difficulty in in doing the parrot emoji uh but one of the takeaways

326
00:37:38,880 --> 00:37:46,880
from that paper was that um uh language models are too big um when the data they're using is not

327
00:37:46,880 --> 00:37:53,280
documentable and the reason that that's an issue is because you'll start having really harmful

328
00:37:53,280 --> 00:38:00,080
behavior from language models that you that you don't know where it came from um and so yeah

329
00:38:00,080 --> 00:38:07,280
I wanted to raise that issue yeah I think that was a theme uh I don't recall how explicit it

330
00:38:07,280 --> 00:38:12,240
it was uh but it was a big takeaway from that conversation I referenced earlier with Thomas like

331
00:38:13,040 --> 00:38:16,720
you know when we're talking about these large language models and the training data set essentially

332
00:38:16,720 --> 00:38:23,200
becomes the internet like how do you manage these kinds of issues and that's that's what you're

333
00:38:23,200 --> 00:38:29,520
speaking to now yeah yeah yeah so if you can start measuring things then you can start actually

334
00:38:29,520 --> 00:38:35,600
curating the data um and so I mean this also goes a lot to what sources you should be using so I

335
00:38:35,600 --> 00:38:40,000
think it's now been generally read upon that you shouldn't use reddit data because there's a lot

336
00:38:40,000 --> 00:38:45,920
of toxic and hateful abusive language there and disproportionately directed it at women um and so

337
00:38:45,920 --> 00:38:51,360
but that that took years to establish right so reddit data used to be cool I think it was even used

338
00:38:51,360 --> 00:38:58,240
in the the Delphi uh ethics system that was that was launched through AI2 but uh but yeah so I mean

339
00:38:58,240 --> 00:39:03,600
this is one of the reasons why coming up with quantifications of data is so um so important as well

340
00:39:03,600 --> 00:39:08,880
as paying attention to the demographics of the people creating that data because now you can really

341
00:39:08,880 --> 00:39:14,400
understand what the data is starting to encode which means what the language model will encode so

342
00:39:14,400 --> 00:39:22,720
it turns out that uh the c4 dataset which is um a colossal uh dataset used for training language

343
00:39:22,720 --> 00:39:32,080
models c4 yeah i have to be confused with c4 not to be yeah c4 is nlp c4 is computer vision

344
00:39:32,080 --> 00:39:40,880
so uh not that that really helps I suppose uh but but the c4 dataset um has a lot of data

345
00:39:40,880 --> 00:39:47,520
sampled from Wikipedia and it turns out that Wikipedia um has a lot of writing disproportionately

346
00:39:47,520 --> 00:39:56,080
more writing from uh white north american men um in their in their 20s uh and so in practice what

347
00:39:56,080 --> 00:40:01,600
does this mean it means that black history is basically not represented or represented very

348
00:40:01,600 --> 00:40:06,560
poorly uh and for a while before I started telling people about this black history would redirect

349
00:40:06,560 --> 00:40:13,600
to african-american history uh which hopefully says so much and if it doesn't sense say so much to

350
00:40:13,600 --> 00:40:22,480
you then perhaps you should check your biases yeah you mentioned naturalness earlier as one of

351
00:40:22,480 --> 00:40:29,840
these metrics that you found interesting elaborate on that yeah so as we collect datasets um

352
00:40:29,840 --> 00:40:36,320
and we're trying to measure or rather as we're trying to collect data that reflects natural

353
00:40:36,320 --> 00:40:42,800
language um there becomes a question of how do i know it's natural and so one approach is to say

354
00:40:42,800 --> 00:40:48,400
well i'll be careful with my sampling in order to make sure that the websites amusing or whatever

355
00:40:48,400 --> 00:40:54,000
captures naturalistic language um but there isn't really a useful quantification of that or they're

356
00:40:54,000 --> 00:41:00,000
there generally hasn't been um other than this more qualitative sort of like that looks natural to me

357
00:41:00,560 --> 00:41:06,560
um and it's important and it's important because if you're trying to build models that can understand

358
00:41:06,560 --> 00:41:13,120
natural language so that means uh you know pulling out the the key entities figuring out the content

359
00:41:13,120 --> 00:41:17,520
of a question things like that these tools that really rely on having a reasonable natural

360
00:41:17,520 --> 00:41:21,920
language understanding uh then it's useful to be able to actually measure naturalness

361
00:41:21,920 --> 00:41:27,120
so from here we can look at things like corpus linguistics uh you know going back to brown

362
00:41:27,120 --> 00:41:33,120
corpus in 1964 uh where there were some really cool ideas around what it means to have natural

363
00:41:33,120 --> 00:41:40,720
language um and one of them was noticing that uh that the zippian distribution of so many things

364
00:41:40,720 --> 00:41:49,040
in nature uh also applies to natural language um and so uh the zippian distribution um it's very

365
00:41:49,040 --> 00:41:54,560
hard to explain in one sentence since it's a mathematical concept uh but basically here's a very

366
00:41:54,560 --> 00:42:02,480
like simple oversimplified way of saying it um the most frequent word uh in someone's

367
00:42:02,480 --> 00:42:10,320
document or whatever um will be roughly twice as frequent as the second most frequent word

368
00:42:10,320 --> 00:42:15,760
which will be roughly twice as frequent as the third most frequent word so there's this inverse

369
00:42:15,760 --> 00:42:22,480
relationship between frequency and essentially rank um and so the zippian distribution so what

370
00:42:23,200 --> 00:42:30,080
what zips law states uh is that they'll it'll it'll follow this this sort of general tendency

371
00:42:30,080 --> 00:42:38,640
um and it turns out that there's this parameter we'll call it alpha that um uh that can kind of

372
00:42:38,640 --> 00:42:44,240
um shape what that curve looks like and it turns out that for English that's already been calculated

373
00:42:44,240 --> 00:42:49,600
by by nice corpus linguists and linguists and stuff and we know that alpha is actually like one

374
00:42:49,600 --> 00:42:56,480
um and so different languages have different different kinds of shapes that follow this uh general

375
00:42:56,480 --> 00:43:02,560
tendency um and because they've already been calculated for a ton of languages we can see how well

376
00:43:02,560 --> 00:43:09,120
some collection fits to the ideal zippian distribution um and then the farther out it is for that

377
00:43:09,120 --> 00:43:15,600
language which we can already know uh then we can say like well according to this sort of theory

378
00:43:15,600 --> 00:43:21,120
it is actually moving farther away from what we would expect in natural language um so you know

379
00:43:21,120 --> 00:43:26,160
it's it's a measure it's not the best measure but it's something that uh that corpus linguists

380
00:43:26,160 --> 00:43:31,600
have put forward and is super useful and and personally in in my work on data measurements uh

381
00:43:31,600 --> 00:43:38,240
I found I found that it's really helped to identify uh data sets that are appropriately capturing

382
00:43:38,240 --> 00:43:43,920
you know um uh general conversations and stuff and data sets that for example have a lot of

383
00:43:43,920 --> 00:43:48,720
artifacts or weird mixing of different domains that wasn't well controlled for and things

384
00:43:48,720 --> 00:43:56,560
like that um so it can be really as well you mentioned as uh I think it was kind of a motivation

385
00:43:56,560 --> 00:44:04,640
for this particular measure natural text on the internet versus newswire press releases which

386
00:44:04,640 --> 00:44:12,880
have a different uh a different way of using language but this also kind of rings of thinking about

387
00:44:12,880 --> 00:44:17,120
you know a world where we have these large language models and they're quote unquote polluting

388
00:44:17,120 --> 00:44:24,880
the internet with spam and kind of uh almost that adversarial and anti adversarial uh cat and

389
00:44:24,880 --> 00:44:31,120
mouse game and and are is that part of what you're thinking about here or is that just tangential

390
00:44:31,120 --> 00:44:36,880
yeah I mean I think one thing that might be relevant here is that if we're trying to figure out

391
00:44:36,880 --> 00:44:43,600
whether or not something is synthetically generated um so you know generated from a language model

392
00:44:43,600 --> 00:44:50,800
versus um created by people we have to be thinking about these sort of second order characteristics um

393
00:44:50,800 --> 00:44:56,480
so what can we maybe not tell as we look at things sentenced by sentence um but if we calculate

394
00:44:56,480 --> 00:45:02,880
statistics over patterns uh in a bunch of sentences are there things that we see um that's different

395
00:45:02,880 --> 00:45:07,600
than what we would expect from when when humans are generating it um and so these kinds of

396
00:45:07,600 --> 00:45:12,720
measurements such as naturalness based measurements would help to get at that um and so as we're trying

397
00:45:12,720 --> 00:45:18,720
to figure out you know uh trolls that are spewing hateful and abusive content that are actually

398
00:45:18,720 --> 00:45:22,880
from from systems that are just using large language models and misinformation and all this

399
00:45:22,880 --> 00:45:28,800
other kind of stuff as we can start developing the second order uh kinds of measurements then we

400
00:45:28,800 --> 00:45:34,080
can start being able to a lot more easily make distinctions so what I'm hearing there is that your

401
00:45:34,080 --> 00:45:40,160
focus is on bias and human impact but the cyber security folks may find this interesting too

402
00:45:40,160 --> 00:45:47,520
or the adversarial hopefully hopefully yeah yeah awesome awesome well Meg it has been wonderful

403
00:45:47,520 --> 00:45:52,640
catching up with you and chatting about some of the things that you've been working on and thinking

404
00:45:52,640 --> 00:45:58,640
about thanks so much for joining us yeah thank you for asking me and letting me babble about things

405
00:45:58,640 --> 00:46:12,080
I'm fascinated by I really appreciate it thank you

