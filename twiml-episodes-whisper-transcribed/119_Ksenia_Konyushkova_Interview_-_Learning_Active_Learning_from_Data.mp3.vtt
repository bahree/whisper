WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:38.400
In this episode, I speak with Cassinia Konyashkova, a PhD student in a CV lab at Acole Polytechnic

00:38.400 --> 00:41.960
Ferrahal de Lozan in Switzerland.

00:41.960 --> 00:47.080
Cassinia and I connected at Nips back in December to discuss her very interesting research into

00:47.080 --> 00:51.880
some ways we might apply machine learning to ease the challenge of creating label data sets

00:51.880 --> 00:54.320
for machine learning.

00:54.320 --> 00:59.320
The first paper we discuss is learning active learning from data, which suggests a data-driven

00:59.320 --> 01:04.640
approach to active learning that trains a secondary model to identify unlabeled data points

01:04.640 --> 01:11.360
which, when labeled, would likely have the greatest impact on our primary model's performance.

01:11.360 --> 01:17.000
We also discuss our paper, Learning Intelligent Dialogues for Bounding Box Annotation, in

01:17.000 --> 01:22.800
which she trains an agent to guide the actions of a human annotator to more quickly produce

01:22.800 --> 01:27.360
bounding boxes.

01:27.360 --> 01:31.920
Last week, I attended an investor conference in San Francisco.

01:31.920 --> 01:35.760
Having spent a couple of packed days being peppered with questions about the state of the

01:35.760 --> 01:40.360
AI market and its various players, I wrote about some of the key themes that emerged

01:40.360 --> 01:43.280
for the latest issue of my newsletter.

01:43.280 --> 01:47.440
If you're interested in the commercial side of the AI industry, you'll definitely want

01:47.440 --> 01:49.080
to check it out.

01:49.080 --> 01:55.040
If you don't already subscribed to my newsletter, visit twimmolai.com slash newsletter or just

01:55.040 --> 01:59.280
email info at twimmolai.com asking to be added.

01:59.280 --> 02:02.920
You'll automatically get the latest issue when you sign up.

02:02.920 --> 02:06.760
Quickly, a bit about the next Twimmol online meetup.

02:06.760 --> 02:12.600
On Tuesday, March 13th, Sean Devlin will be doing an in-depth review of reinforcement

02:12.600 --> 02:18.120
learning and presenting the Google Deep Mind paper, playing Atari with deep reinforcement

02:18.120 --> 02:19.520
learning.

02:19.520 --> 02:24.360
Head over to twimmolai.com slash meetup to learn more or register.

02:24.360 --> 02:28.760
All right, let's roll.

02:28.760 --> 02:37.080
All right, everyone, we are here at NIPS and I am with Cassinia Konošková.

02:37.080 --> 02:44.080
Cassinia is a PhD student in the computer vision lab at EPFL in Lozan, Switzerland.

02:44.080 --> 02:46.280
Cassinia, welcome to the podcast.

02:46.280 --> 02:47.280
Thank you.

02:47.280 --> 02:48.280
Nice to meet you today.

02:48.280 --> 02:49.280
Absolutely.

02:49.280 --> 02:51.240
Great to meet you as well.

02:51.240 --> 02:55.040
So why don't we get started by having you tell us a little bit about your background and

02:55.040 --> 02:59.840
how you got involved in computer vision and machine learning, artificial intelligence?

02:59.840 --> 03:05.320
Okay, so it probably all started with my master degree when I just heard this cool name,

03:05.320 --> 03:06.320
machine learning.

03:06.320 --> 03:07.320
Okay.

03:07.320 --> 03:11.680
I had a very little idea of what it is all about, but it sounded exciting, and then I learned

03:11.680 --> 03:16.440
that you can actually recognize Cassinia in the pictures if you use machine learning and

03:16.440 --> 03:19.400
then I thought, great, that's the thing for me.

03:19.400 --> 03:20.400
No, I'm kidding.

03:20.400 --> 03:25.000
No, it wasn't the real motivation, but yeah, it just sounds fun.

03:25.000 --> 03:31.280
But yeah, it sounded like a fun field to be, and it was probably a little bit before,

03:31.280 --> 03:37.640
it became so popular and so famous, but I was likely to be there at the beginning.

03:37.640 --> 03:44.480
So I joined the program in machine learning and algorithms in the University of Helsinki.

03:44.480 --> 03:51.280
And after that, I went from my PhD program in Switzerland at a quality technique for the

03:51.280 --> 03:56.520
Grand de la Sant, and I joined the computer vision lab with Pascal Four, who is famous in

03:56.520 --> 04:00.880
the computer vision community, but as you know, today computer vision very much depends

04:00.880 --> 04:06.560
on machine learning, so it was a great match to combine this two fields that interest

04:06.560 --> 04:07.560
me.

04:07.560 --> 04:08.560
Okay, awesome.

04:08.560 --> 04:09.560
Awesome.

04:09.560 --> 04:14.480
And you're here at NIPS, sharing some of the work you're doing, some of your research.

04:14.480 --> 04:16.560
Can you tell me a little bit about that?

04:16.560 --> 04:17.560
Mm-hmm.

04:17.560 --> 04:18.560
Yes, sure.

04:18.560 --> 04:20.680
So I'm working in the field that is called active learning.

04:20.680 --> 04:21.680
Okay.

04:21.680 --> 04:25.400
So the basic idea is that normally in the traditional machine learning, we have a lot of training

04:25.400 --> 04:31.280
data, which we collect from human experts, and then they provide us with the labels for

04:31.280 --> 04:36.000
this data when we're doing the supervised machine learning, and having this huge annotated

04:36.000 --> 04:41.160
corpus of data, we can train a model, which will help us to predict for the new coming

04:41.160 --> 04:45.920
data what labels were assigned, either it's a classification or a regression task.

04:45.920 --> 04:50.480
The problem that we're dealing with is that we don't have this training data available,

04:50.480 --> 04:56.080
and we're trying to select the data in the optimal way, so that if we annotate it, we can

04:56.080 --> 04:59.760
learn the machine learning model as quickly as possible.

04:59.760 --> 05:05.760
So the idea is to reduce human annotation efforts, and at the same time achieve similar

05:05.760 --> 05:10.080
quality of performance of the machine learning algorithms.

05:10.080 --> 05:15.560
And what I'm doing, I'm presenting the work that is called learning active learning from

05:15.560 --> 05:16.560
data.

05:16.560 --> 05:17.560
Okay.

05:17.560 --> 05:22.520
So we're going even one step further, and instead of trying to construct some specific

05:22.520 --> 05:28.160
algorithm that would select data for us, we're trying to learn an algorithm from the

05:28.160 --> 05:33.160
previous examples of active learning algorithms runs.

05:33.160 --> 05:34.160
Okay.

05:34.160 --> 05:41.720
So for example, the very popular active learning algorithm is called uncertainty sampling.

05:41.720 --> 05:45.920
In this case, we have, for example, suppose we have 10 data points that are labeled for

05:45.920 --> 05:51.000
us, it's very little and we probably cannot predict much based on them, but we can still

05:51.000 --> 05:56.280
have a very rough idea of how the model should be, and then we can construct, for example,

05:56.280 --> 06:01.600
the decision hyperplane, and then we will select points that lie closest to the decision

06:01.600 --> 06:06.720
boundary, and we would think that as the model is the most uncertain right now about this

06:06.720 --> 06:11.680
points, if we annotate these kind of points, they would help to train a better model in

06:11.680 --> 06:13.080
the future.

06:13.080 --> 06:19.360
So instead of labeling the data just uniformly a trend from the very big corpus, we select

06:19.360 --> 06:23.200
the data that would help the model the most.

06:23.200 --> 06:27.680
And this kind of algorithms, like I mentioned, uncertainty sampling, and there are many

06:27.680 --> 06:31.480
other ones that were introduced in the literature.

06:31.480 --> 06:36.920
The problem is that on different applications, they're all competing, some of them are doing

06:36.920 --> 06:40.480
better in this scenario, some are doing better in that scenario.

06:40.480 --> 06:45.520
And for example, in my previous work, I worked on specific algorithm to work with images,

06:45.520 --> 06:51.000
so to take the advantage of the structure and some smoothness assumptions of the images

06:51.000 --> 06:52.720
to annotate data.

06:52.720 --> 06:58.040
But then there is nothing principle there, and we don't know which of the algorithm would

06:58.040 --> 07:03.840
actually perform the best at priority, so we have to test them, and this is all human

07:03.840 --> 07:05.320
annotation costs.

07:05.320 --> 07:11.800
So my idea is that we have some annotated data sets already, and then if we simulate

07:11.800 --> 07:17.680
the runs of this data collection process, we can actually see what kind of points led

07:17.680 --> 07:24.040
to a good behavior of active learning algorithm, and which points didn't improve the quality

07:24.040 --> 07:25.200
much.

07:25.200 --> 07:29.240
And then we can try to predict in the future what kind of points should be selected.

07:29.240 --> 07:35.600
So in particular, I'm dealing with station where I just generate a lot of data myself,

07:35.600 --> 07:40.440
just like some synthetic data sets of very simple shapes, but they still have a lot

07:40.440 --> 07:46.200
of variability of them, some in some cases, if I deal with binary classification, sometimes

07:46.200 --> 07:51.200
the classes overlap a lot, sometimes they're easily separable, two-dimensional shapes

07:51.200 --> 07:52.200
or...

07:52.200 --> 07:56.360
Yes, right now it's just very simple, we can do it with a very kind of cheap procedure

07:56.360 --> 08:02.320
with just two-dimensional data, and then we run active learning on them, just kind of

08:02.320 --> 08:08.000
this data collection, and then we look at the state of the classifier, how the classifier

08:08.000 --> 08:13.680
was trained, and the data points, and we look how much adding each kind of data point

08:13.680 --> 08:16.920
reduced the classification error for us.

08:16.920 --> 08:21.840
So for example, we label two data points, we train a model, we look at what kind of

08:21.840 --> 08:26.760
error this model resulted in, as we know all the ground rules, as this is the simulated

08:26.760 --> 08:27.760
data.

08:27.760 --> 08:32.400
Then we try to select one more data point, and we get three points.

08:32.400 --> 08:37.560
Now in these three points, we can learn a new model, and it results in a new error.

08:37.560 --> 08:41.840
And now we can look at the difference between the error of the model with two points and

08:41.840 --> 08:46.880
the error of the model with three points, and the difference would show us how much adding

08:46.880 --> 08:50.000
this third data point helped the model.

08:50.000 --> 08:57.360
And so now as we go on in training, we memorize the states of the classifier, for example

08:57.360 --> 09:01.920
I'm dealing with something very simple, like random forest classification, I can look

09:01.920 --> 09:07.560
at what was the depth of the tree, what was the variance of the forest, what was the cross

09:07.560 --> 09:12.520
for the data accuracy, and then we look at the data points, for example what was the probability

09:12.520 --> 09:18.000
to belong to a certain class for a certain point, and we have how much in this situation,

09:18.000 --> 09:20.760
this particular data point reduced the error.

09:20.760 --> 09:26.560
So based on this data, after we collected for all synthetic data sets, we can train a regressor

09:26.560 --> 09:31.600
that will predict in the future, based on the state of the classifier, and given data

09:31.600 --> 09:37.040
points from the pool of unlabeled data, which of them would result in the highest reduction

09:37.040 --> 09:41.840
in error, and this is the sample that we select to be annotated.

09:41.840 --> 09:48.280
And we all started with this very simple two-dimensional data, and of course the easiest is to train

09:48.280 --> 09:53.520
it on the similar kind of data, and this is our simplest scenario, then we try to complicate

09:53.520 --> 10:00.600
it more and more, for example we deal with the soil-like problems when we have a classification

10:00.600 --> 10:05.520
where the data is in the form of checkerboard, and imagine so we have like positive class,

10:05.520 --> 10:10.360
negative class, positive class, negative class, or they're kind of repeating in the checkerboard

10:10.360 --> 10:11.680
style.

10:11.680 --> 10:16.840
And this kind of data sets are more tricky for machine learning, usually and active learning

10:16.840 --> 10:18.480
is not an exception.

10:18.480 --> 10:24.240
And if we apply the very famous and very popular uncertainty something algorithm, that

10:24.240 --> 10:30.760
is often difficult to outperform in normal situations, in this case it would perform even

10:30.760 --> 10:36.360
worse than random, because it is confused by this checkerboard structure.

10:36.360 --> 10:42.360
And is the difficulty with this structure because the regions are non-contiguous or is

10:42.360 --> 10:43.360
it something else?

10:43.360 --> 10:48.840
The thing is that for example if we imagine the checkerboard is 2x2, so we have, suppose

10:48.840 --> 10:54.400
we have rent class on the top left, then blue class on the top right, and then blue

10:54.400 --> 11:01.960
class again on the bottom left, 10 red class on the bottom right, suppose that we selected

11:01.960 --> 11:07.920
one sample from the upper red class, and one sample from the upper blue class as our

11:07.920 --> 11:12.880
initial examples, then we construct some kind of decision boundary, and what and so does

11:12.880 --> 11:17.960
something we do, we would try to refine this boundary and it would take a very long time

11:17.960 --> 11:23.200
to discover that there are two other regions of these classes that are in completely different

11:23.200 --> 11:24.200
order.

11:24.200 --> 11:29.240
So what uncertainty something is good at is it's refining boundary when we already have

11:29.240 --> 11:33.160
some reasonable approximation of this boundary.

11:33.160 --> 11:38.960
And this can be a problem in many real data sets when one class can be represented by

11:38.960 --> 11:42.960
some several modalities of examples.

11:42.960 --> 11:46.920
And so in this case we show that then this very popular algorithm performs worse than

11:46.920 --> 11:51.440
just random something, by random something I mean that we would just select data, random,

11:51.440 --> 11:53.440
and then we label everything.

11:53.440 --> 12:00.160
So it's definitely not a very efficient strategy, but in this case it does better than uncertainty

12:00.160 --> 12:01.160
something.

12:01.160 --> 12:05.800
However, if we learn this strategy from the previous examples of this active learning

12:05.800 --> 12:11.920
runs, it figures out that it shouldn't follow the strategy of refining the boundary.

12:11.920 --> 12:15.720
And for example, in this case what it seems that it learns is that when the classifier

12:15.720 --> 12:22.560
is uncertain, it's better not to trust it at all, not to follow any of the predictions

12:22.560 --> 12:27.360
of the classifier, but instead just do some random exploration in the beginning until

12:27.360 --> 12:33.880
we collect enough data on which we can train a better classifier.

12:33.880 --> 12:38.360
And then we would be able to refine the boundary that would be more efficient.

12:38.360 --> 12:44.880
And so this method doesn't involve trying to identify data points that are close to your

12:44.880 --> 12:46.360
decision boundary.

12:46.360 --> 12:48.200
No, not anymore.

12:48.200 --> 12:49.200
Okay.

12:49.200 --> 12:56.160
So as a feature, as we have the regressor, it has some features as an input.

12:56.160 --> 13:01.160
And some of them are features of the classifier that just characterize how certain our classifier

13:01.160 --> 13:06.680
is, how good is cross-validated accuracy, for example.

13:06.680 --> 13:09.480
And then some features they characterize the data points.

13:09.480 --> 13:13.680
And one of the features could be the distance to the boundary, but it can also include some

13:13.680 --> 13:14.680
other features.

13:14.680 --> 13:19.720
For example, we can look at the distance to the other labeled points or distance to the

13:19.720 --> 13:21.120
unlabeled points.

13:21.120 --> 13:26.120
For example, it can characterize for us if the point is an outlier or if it lies in the

13:26.120 --> 13:28.920
dense region of the feature space.

13:28.920 --> 13:34.720
So it includes many more features and including the distance to the boundary.

13:34.720 --> 13:42.600
But then if I study how much each of these features influences the decision of the regressor,

13:42.600 --> 13:47.080
the decision of which point to select by the active learning strategy, the distance to

13:47.080 --> 13:51.240
the boundary is not the first and most important criteria.

13:51.240 --> 13:56.720
It comes maybe like the fourth and the first one was actually the certainty, the confidence

13:56.720 --> 13:58.320
of the classifier.

13:58.320 --> 14:03.960
So the strategy is adaptive as the active learning progresses and we collect more and

14:03.960 --> 14:04.960
more data.

14:04.960 --> 14:10.400
The classifier becomes more and more confident and the strategy adapts accordingly.

14:10.400 --> 14:16.240
And next we try to test this kind of algorithm that was learned from the data on the real

14:16.240 --> 14:20.920
data sets that are very different from these synthetic examples.

14:20.920 --> 14:28.320
In the synthetic examples we just had this two-dimensional features for the data points, but you

14:28.320 --> 14:32.720
cannot go very far with it and it doesn't sound like a very exciting problem in the

14:32.720 --> 14:33.720
learning.

14:33.720 --> 14:38.480
It definitely won't help us to classify cats.

14:38.480 --> 14:46.680
So next we move to the real data sets and I'm a part of human brain project that is

14:46.680 --> 14:53.360
a very big European project with a huge funding trying to address the problem of understanding

14:53.360 --> 14:59.000
how human brain functions and there are many many researchers who work on it from very

14:59.000 --> 15:05.320
different perspectives, biologists, neuroscientists, physicists and all kind of people.

15:05.320 --> 15:10.080
And so I'm also part of this project is trying to understand brain imaging.

15:10.080 --> 15:16.720
For example we're dealing with the electron microscopy scans of red tissue, of red brain

15:16.720 --> 15:21.520
tissue and we're trying to segment some cellular structures in it.

15:21.520 --> 15:27.480
For example we're on the cellular resolution and we can see the mitochondria, the synopsis

15:27.480 --> 15:32.960
between neurons, we can identify membranes or other kinds of things.

15:32.960 --> 15:37.560
So the other application which I'm interested is having MRI scans of the brain and try

15:37.560 --> 15:43.240
to segment brain tumor of different types.

15:43.240 --> 15:49.680
And then I apply the same kind of strategy that I learned on the synthetic data to this

15:49.680 --> 15:55.720
real data sets of much higher dimensionality, much higher complexity.

15:55.720 --> 16:02.160
And also I have a very different data set for example for detecting credit card fraud transactions.

16:02.160 --> 16:08.760
So I have transactions from the bank for which we know that 99% of them are normal transactions

16:08.760 --> 16:13.480
and then we have around 1% or even less of fraud transactions.

16:13.480 --> 16:18.600
And it's also very laborious task to annotate this kind of data because you need to annotate

16:18.600 --> 16:25.320
like thousands of data points before you start having only a dozen of examples of fraud.

16:25.320 --> 16:27.400
And this is only humans they can do it.

16:27.400 --> 16:33.480
And the same goes of course for this biomedical imaging like as a person who's not into biology

16:33.480 --> 16:38.800
or neuroscience it is a very hard task to understand what is going on.

16:38.800 --> 16:44.400
And the time of these people is quite expensive and we would better want them spend it on something

16:44.400 --> 16:48.680
more intelligent rather than annotating data for algorithms.

16:48.680 --> 16:54.120
So these are the applications where active learning is the most interesting in cases when

16:54.120 --> 17:00.360
only very specialized and educated humans can annotate data for us.

17:00.360 --> 17:04.160
And in this case we also apply the same kind of strategy that we learned from synthetic

17:04.160 --> 17:05.160
data.

17:05.160 --> 17:11.000
And surprisingly it works even in this cases it can generalize to very different data sets

17:11.000 --> 17:18.160
and how we understand it is that it learns a very general knowledge about active learning.

17:18.160 --> 17:24.320
For example the rule such as when the classifier is uncertain do random exploration and when

17:24.320 --> 17:27.880
classifier is rather certain refine the boundary.

17:27.880 --> 17:32.760
This should hold no matter how the data set looks like and it would hold in the case of

17:32.760 --> 17:38.480
this simple to dimensional data and the case of very complex electron microscopy imaging

17:38.480 --> 17:39.840
data.

17:39.840 --> 17:43.960
And so is the idea you know when you take a step back and think about this in the context

17:43.960 --> 17:52.360
of a system like the imaging data is the idea that you yeah how does this unfold in practice

17:52.360 --> 17:58.280
like you have some minimal set of training data and then you're envisioning that you

17:58.280 --> 18:06.600
would generate some other set of data and kind of send it out for annotation or okay I see

18:06.600 --> 18:12.680
so okay now example for example let's talk about this electron microscopy segmenting

18:12.680 --> 18:19.080
cellular structures how we imagine it is that the user has a stack of tissues so it's actually

18:19.080 --> 18:25.960
not a real image it's a three-dimensional image so it's kind of a cube and in whatever direction

18:25.960 --> 18:30.760
we look at it we can count it in all directions and we'll get to the images of how the tissue

18:30.760 --> 18:32.280
looked like that.

18:32.280 --> 18:38.240
And so what we ask at the beginning as the user to kind of take a brush and some program

18:38.240 --> 18:45.120
and indicate here is like some part of mitochondria and here is a part of background and then based

18:45.120 --> 18:50.960
on this our model would train some initial classify a very very very rough one that would

18:50.960 --> 18:55.680
just like say okay mitochondria is for example they're darker than background on average so

18:55.680 --> 19:00.800
let's just use this as discriminative feature it would train such a model and then there will

19:00.800 --> 19:05.280
be some kind of grayish part in the tissue and it would think that classify I would think

19:05.280 --> 19:10.400
I'm not sure what it belongs to does it belong to mitochondria or does it belong to the

19:10.400 --> 19:17.360
background and then the active learning would select this as the some special part in the tissue

19:17.360 --> 19:22.240
which we want to be annotated and then it brings the user to this part of the tissue and kind of

19:22.240 --> 19:27.440
highlights this error in which we would like to get more labels and ask the user please get

19:27.440 --> 19:33.360
take a brush again and indicate whether this particular error is background or if this particular

19:33.360 --> 19:38.960
error is mitochondria and then the user annotates it's again and then the procedure iterates

19:39.680 --> 19:45.920
until we're satisfied with results or until some budget of time is reached or something of the type

19:46.560 --> 19:51.840
and do you have that whole system running at this point end to end or you more modeling it from

19:51.840 --> 19:58.080
the data itself so right now I was mostly working just with from the data okay so we already have

19:58.080 --> 20:04.000
the labeled data and we simulate this active learning procedure because in this case we can of

20:04.000 --> 20:10.320
course test many strategies we can study how different parameters influence our algorithms we

20:10.320 --> 20:15.840
can compare to many strategies but we actually have students and master's student who is working

20:15.840 --> 20:24.320
on putting it into a plugin that the neuroscientist would use and I actually know some people who work

20:24.320 --> 20:31.360
in neuroscience and I know one girl whose PhD topic is mostly at identifying mitochondria

20:31.360 --> 20:37.920
and trying to understand how they influence the neural diseases and the human brain and what she

20:37.920 --> 20:45.680
does is a big part of her PhD work she's just she has to annotate this data manually and I believe

20:45.680 --> 20:51.520
that she's very knowledgeable and she can do something more useful at that time if we can provide

20:51.520 --> 20:58.160
her with this plugin that only with you know 100 samples can provide the same quality of

20:58.160 --> 21:04.560
classification as with 10,000 samples for example so in some cases we can reduce the annotation

21:04.560 --> 21:13.200
effort like very significantly so is that you know going from 10,000 required samples to 100 is

21:13.200 --> 21:19.760
that representative of the kind of results you've seen for different scenarios you mean across

21:19.760 --> 21:24.640
different applications right okay it depends on the application how much active learning can

21:24.640 --> 21:31.040
actually improve our results okay yeah so in some cases we can get a very dramatic decrease

21:31.680 --> 21:39.280
in the annotation time in some cases it's not that much but in any case we're reaching the same

21:39.280 --> 21:45.200
quality faster and we can stop the procedure at any time when we reach the budget of time and

21:45.200 --> 21:50.400
the classifier that we constructed at that point of time would be better than what we would construct

21:50.400 --> 21:58.160
if we just sample points at random then the thing that we can look at is how human friendly this

21:58.160 --> 22:05.840
kind of annotation environment is for example in my previous work I worked particularly for images

22:05.840 --> 22:13.520
and I tried to design a particular strategy for the 3D imaging stacks and in this case what we

22:13.520 --> 22:19.520
were saying is that it is very cumbersome if we have to annotate the stack just kind of pixel by

22:19.520 --> 22:24.080
pixel suppose that you are brought like in some part of the stack and ask to annotate what is

22:24.080 --> 22:29.040
here then you brought to some other place you ask to annotate something then some islands and other

22:29.040 --> 22:33.840
and it can actually take quite a lot of time just because of this context switches and you have to

22:33.840 --> 22:40.560
understand what is going on and like it's also trivial to understand how this neurons connect

22:40.560 --> 22:45.120
with each other for example so one thing that we're looking at we're trying to find some

22:45.120 --> 22:51.600
optimal cuts in this 3D plane so we're just cutting them in various directions and we're projecting

22:51.600 --> 22:57.360
data on this direction on these planes on these cuts and we're trying to annotate some batches

22:57.360 --> 23:03.760
of samples instead of asking to annotate one by one whatever samples are you can try to think of

23:03.760 --> 23:09.360
it kind of like maximum uncertainty cuts or something yeah so we had some some uncertainty measure

23:09.360 --> 23:15.760
that wasn't like maybe not a standard one but something special for the images but then here we're

23:15.760 --> 23:22.960
trying to find a cut with a maximum uncertainty and then there are other ways to work on this

23:22.960 --> 23:29.360
direction so in as I see it in active learning there are two components one of them is we want to

23:29.360 --> 23:34.160
design an algorithm that would select the best possible sample but on the other hand we should

23:34.160 --> 23:39.840
still remember that there will be humans annotate data for us so we should make it as convenient and

23:39.840 --> 23:47.520
as easy for humans to label the data and yeah another thing that I've been working on also in

23:47.520 --> 23:54.960
this direction is getting label data for object detection so the problem is very different we just

23:54.960 --> 24:00.400
like want to localize objects in images like I don't know the bottle the TV screen a human

24:00.400 --> 24:06.160
and these kind of things and there are various and for this we need bounding box annotations so we

24:06.160 --> 24:12.640
need someone to draw a bounding box around every object and from this with chain and there are

24:12.640 --> 24:17.840
various ways how we can collect it we can ask the humans to draw it manually this kind of this is

24:17.840 --> 24:22.880
the normal way of doing it but on that the hand we already have some kind of detectors that maybe

24:22.880 --> 24:28.000
not as strong as we would like them to be but we can already try to use the output of these

24:28.000 --> 24:34.000
detectors and show them show it to the users and as the users to correct for the mistakes and

24:34.000 --> 24:39.520
this is what we're talking about mistakes in terms of identifying the bounding boxes or labeling

24:39.520 --> 24:45.040
what's in the bounding boxes or both okay so now I'm talking about identifying the location of

24:45.040 --> 24:50.320
the bounding box because in this scenario we simplify the problem and we say that we already have

24:50.320 --> 24:56.480
image level annotations so we already know that there is a bottle in this image there is a door

24:56.480 --> 25:01.840
in this image and now we want to know where this door is okay so we split the problem in two

25:01.840 --> 25:06.880
separate ones and we deal with the second one where we want to get the coordinates of this

25:06.880 --> 25:13.200
bounding boxes okay and in this case on the one hand if we ask humans to draw an object it is

25:13.200 --> 25:20.640
very fast it is not very fast it takes in the very in kind of in the standard protocols it might

25:20.640 --> 25:28.080
take up to 35 seconds to get a one bounding box per image and we need thousands of them for

25:28.080 --> 25:35.840
every class which results like in years and years of human work and on the other hand if we ask

25:35.840 --> 25:41.680
the users to verify the prediction of the classify it can be very fast it takes around two seconds

25:41.680 --> 25:48.400
for example just to say yes or no if it is a correct identification of the object but it is not

25:48.400 --> 25:54.640
guaranteed to provide a bounding box that we would like to get so if the user answers no we have

25:54.640 --> 26:00.160
to repeat the procedure again and we have to ask the user again where the box was so what we are

26:00.160 --> 26:05.120
trying to do in this work we are trying to balance between these two modalities of data

26:05.120 --> 26:12.240
annotation and trying to decide in what situations it's better to ask to draw a bounding box that is

26:12.240 --> 26:18.400
expensive but guaranteed to produce a label and with which situations it's easier to verify

26:19.280 --> 26:26.560
the detection detected proposals and to result in a positively verified box quicker

26:27.360 --> 26:32.640
and is the methodology the same for this project as the one you previously described?

26:32.640 --> 26:38.960
no it's not exactly the same but what unifies them is that we're still dealing with the problem

26:38.960 --> 26:44.720
of reducing human efforts just approaching it from the other direction so one direction is

26:44.720 --> 26:51.040
select what to annotate and the other direction select how to annotate so what I believe is that

26:51.040 --> 26:57.280
both of them would be important and if we just deal with one of them we can get some good results

26:57.280 --> 27:03.040
on paper and some simulations but in practice both of these directions would be important

27:03.040 --> 27:11.680
interesting I'm curious about how the system would go about directing the annotator to a

27:11.680 --> 27:20.320
specific area meaning you have these you have these data points that are annotated what is it

27:20.320 --> 27:25.360
asking for is it asking for like some coordinate does it know like coordinates of uncertainty or

27:25.360 --> 27:31.600
does it know something else okay so I didn't go into details of the protocol but actually how

27:31.600 --> 27:38.240
we start the task is to segment something to segment mitochondria's and we started from

27:38.240 --> 27:44.880
over-sigmenting images so we use something that is called sleek super pixels or sleek super voxels

27:44.880 --> 27:51.680
so the idea is that it is unsupervised method that can provide segmentation of the image of the

27:51.680 --> 28:00.000
very fine fine grain that is not reflecting actually the boundaries of the object but the uniform

28:00.000 --> 28:06.480
more or less so we get like very small segments in the images that are all more or less uniform

28:06.480 --> 28:13.280
in their appearance and every object would compile would consist for example of 100 of such objects

28:13.280 --> 28:20.880
of this super voxels or super pixels and then what the algorithm the active learning algorithm

28:20.880 --> 28:27.040
identifies it selects which super voxel is the most uncertain and are the super voxels they're

28:27.040 --> 28:32.960
all uniform in shape and size yes this is the property of this sleek super voxels is that a

28:32.960 --> 28:40.160
more or less uniform and of a shape that is close to spherical or circular depending if it's 3D

28:40.160 --> 28:47.520
or 2D stacks and so you can have multiple adjacent super voxels that are the same value essentially

28:47.520 --> 28:54.720
the same color or yes yes so it's still even if there is a very uniform part of the image it would

28:54.720 --> 29:00.240
still try to divide it in some way so that they're still kept of more or less the same size

29:00.240 --> 29:06.240
okay the same shape it depends on the parameters how you specify it's it's unsupervised algorithm

29:06.240 --> 29:11.760
but you need to identify how compact you want them to be how smooth you want them to be this kind

29:11.760 --> 29:17.120
of things but what we're trying to do is like to have them roughly circular small objects

29:17.120 --> 29:22.960
okay and the overlapping no they're not overlapping so it's just kind of assignment of every pixel

29:22.960 --> 29:29.120
of the object to a super pixel okay so we call they're called super pixels because they consist

29:29.120 --> 29:35.680
of multiple pixels or super voxels the same for voxels okay and then what active learning does

29:35.680 --> 29:42.240
it select super voxel or super pixel that needs to be annotated in this case or in case of

29:42.960 --> 29:50.560
this identifying the planes it just identifies the error with several super voxels that are uncertain

29:50.560 --> 29:56.320
so in this case it would be just as if we have an image and then we highlight some kind of round

29:56.320 --> 30:02.800
errors and that we're interested in annotations there okay yeah and there were also suggesting that

30:02.800 --> 30:08.960
if the objects are big enough like mitochondria are rather big and the selected error that we want

30:08.960 --> 30:14.560
to use it to annotate is small we can approximate it that just with two clicks of the mouse just

30:14.560 --> 30:21.040
identifying the boundary of mitochondria if it is in the image so it makes it even faster like this

30:21.040 --> 30:26.080
okay so this is another example of working on this other direction of how convenient it is for

30:26.080 --> 30:32.240
user to annotate the data combines together with the algorithm that selects what to annotate

30:32.240 --> 30:39.280
okay and so if the presumably if you're if the user task is to then identify whether the

30:39.280 --> 30:46.560
boundary is in the image or in the the voxel super voxel then you can kind of go from there to

30:46.560 --> 30:52.240
multiple super voxels with boundaries and determine which is where the mitochondria is is that

30:52.240 --> 30:59.840
what you're saying so how it looks like is that suppose we have a circle that overlaps the mitochondria

30:59.840 --> 31:05.040
bit but because mitochondria is rather big compared to the size of the circle right the boundary

31:05.040 --> 31:10.960
of mitochondria is more or less just a line yeah and we can ask the user to draw this line with two

31:10.960 --> 31:19.360
clicks and to identify on which side the the mitochondria is right and then it would we are able

31:19.360 --> 31:25.040
from this line we're able to go towards the assignment of super voxels to particular classes

31:25.040 --> 31:29.840
so when we have just that one assignment in this one super voxel can tell you

31:29.840 --> 31:37.520
a whole about the whole area exactly yeah okay interesting interesting um well this is

31:37.520 --> 31:42.720
really fascinating work I appreciate you taking the time to to share it with us work and folks

31:42.720 --> 31:50.080
learn more about your research do you have a website up on uh yes I have a website that is

31:50.080 --> 31:57.680
xenia.conishcaller.com and I try to keep it more or less updated about the recent research that

31:57.680 --> 32:03.040
I'm doing for example what I mentioned today the work about bounding boxes I just finished

32:03.600 --> 32:08.800
an internship last Friday at Google Research in Zurich okay and this is a project that we're

32:08.800 --> 32:17.040
working on okay and it's soon to be there and okay before it is going to appear anywhere else

32:17.040 --> 32:20.720
awesome awesome well thank you once again yeah thank you

32:20.720 --> 32:30.080
oh all right everyone that's our show for today for more information on cousinia or any of the

32:30.080 --> 32:37.760
topics covered in this episode head on over to twimlai.com slash talk slash 116 if you haven't

32:37.760 --> 32:43.120
already done so make sure you head on over to iTunes as well to leave us your most gracious

32:43.120 --> 32:49.280
rating and review it helps new listeners finds us which helps us grow and of course

32:49.280 --> 32:55.040
thanks so much for listening and catch you next time

