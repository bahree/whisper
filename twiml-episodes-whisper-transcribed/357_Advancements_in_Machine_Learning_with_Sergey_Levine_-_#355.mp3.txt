Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.
Alright everyone, I am here in Vancouver at the 33rd NURPS conference and I've got the
pleasure of being seated with a repeat Twimal guests. Sergey Levine. Sergey is an assistant
professor in the Department of Electrical Engineering and Computer Science at UC Berkeley.
Sergey, welcome back to the Twimal AI Podcast. Thank you. So your team has been continues to be
very productive and prolific. You've got a dozen papers submitted here or accepted here at the
main conference as well as a number of workshop papers and I would love to use this opportunity to
kind of get caught up with you and hear about what you've been excited about recently. I think it
was July, well not I think, I just checked actually. It was July of 2017 that we last spoke on
deep robotic learning. What have you been up to? What are you excited about?
I guess a lot has happened since then. Broadly speaking, a lot of what my lab has been trying to do
since then is really to try to make it, try to move towards sort of a future where we could have
machines that are out there in the real world learning continues through their own experience.
And while we're doing a lot of very different things, in many ways much of this work is
centered around the components that we believe are necessary to make that happen.
So I could tell you, for example, about some of the things in this conference that I'm pretty
excited about it that I think are giving us sort of a non-trivial step towards that direction.
Absolutely. So one of those things is basically techniques for combining model free reinforcement
learning with planning. So this is something that I think is actually a really big deal.
You know, conventionally when we think about rational decision making, we think about kind of
a planning that is that process, you know, if you're imagining how you're going to get to the
airport, you think, well maybe you'll like get the train or get the taxi, how do you pay for the
taxi, how do you pay for the train, you know, what do you do when you get there, and there's a
sequence of steps that you have in your mind. But the thing that's always been kind of puzzling is
that those steps are, they're a little bit abstract. So you don't plan how you're going to move
every muscle in your body in order to get on the train, you plan through individual steps.
And that has always been the big challenge in marrying planning and learning.
You can't plan over the low level kind of instantiation of your behavior and it seems like
learning doesn't by itself give you these kind of planning type behaviors. So we have actually
two papers in the main conference that study different facets of this problem, essentially using
learning to learn behaviors that achieve goals and then planning over the instantiation of those
goals to achieve much more temporarily extended outcomes. And I'm pretty excited about that because
that gives us some ideas about how reinforcement learning can, it sounds almost provide the abstractions
over which planning can be done. What's the relationship between planning and model-based
approaches where you're integrating in some, you know, prior knowledge about how a task should be
performed? So planning is typically regarded as a very model-based thing. And I think that one of
the, one of the things that's kind of an interesting shift away from the conventional way of thinking
is that conventionally people think that, well, planning is this thing that you do, well,
first people think that it's something that you do that you publish and kind of dusty,
old robotics venues. But they think it's like this thing that you do on top of a very
physics-based, manually designed model, you know, you open up the first textbook right out the
equations. And I think, you know, my group is not the only one working on this. I think this
is actually something that we've seen come up more and more over the past year or two is that
people are thinking, well, can you learn abstractions that you can plan over that are not just purely
predictive models? So these abstractions don't try to predict, you know, if you take this very low
level action, here's what will happen immediately at the next point in time. But they're kind of
higher level abstractions. They're like, if you execute this intention, how will the world change
in some representation that sort of lifts you away from the low level physical grounding?
And by lifting you off from that low level physical grounding, you actually get a much
easier planning problem. So in effect, learning serves to simplify the planning by putting it into
the right abstract representation. And why is that? If the planning still needs to be done at a
low level theory to actually, you know, move the robot or get us from point A to point B.
Yes, exactly. So in effect, the learning kind of takes care of the low level details.
So if you have a model-free learned behavior for, let's say, you know, walking to the door,
then you don't have to plan how you're going to do that. You can plan at the level of, you know,
I want the door open and then kind of, you know, let your body do its thing to get you there.
And I think that's kind of the intuition behind how these methods work. They let the learning
take care of these low level details and then remove them from consideration for the higher level planning.
So it's a kind of a hierarchical approach. I was just going to ask that it sounds like you're
proposing a hierarchical approach in which, as opposed to assuming low level physics-based models,
for example, you are able to, you're maybe learning in both levels of the hierarchy, but the hierarchy
itself simplifies the, doesn't simplify both or just the higher level problem. So the hierarchy
simplifies the higher level problem, but it doesn't, in a sense, simplify the lower level problem
because now the learning component doesn't need to take care of being able to achieve very
temporarily extended goals. So often in reinforcement learning, one of the hardest things is if you have
a task where you don't realize that you've reached a successful outcome until you've performed a
very long sequence of behaviors that are by themselves unrewarding. So in effect, by chunking out
the problem into these intermediate sub-goals, which you can, which the algorithm can basically
invent, so basically make some, you know, here are some places I could go for each place that I've
reached repeatedly. Let me just compartmentalize that into a little skill. So that doesn't require
achieving any temporarily extended goal, like getting to the airport, and then the higher level
planner can go in and say, oh, you know how to reach all these sub-goals. Let me figure out how
to sequence them so that you accomplish your end goal. The notion of hierarchical learning like
this is, is not by any means new, but one of the things that has been sort of coming to fruition
recently is that people have figured out how to abstract away both the behaviors and the states,
right? A planning problem consists of states and the actions, right? So the actions get abstracted
away as these skills and the states get abstracted away via representation learning. And if you have
both of those, then you can get a substantially simplified problem for the higher level planner,
and you can actually get some of these things to work on interesting like image-based tasks,
for example. One of the interesting nuances that came out for me in the way you described that
is when we think about the analogy that you used, I want to get to the airport and I need to take
the bus and the train and the taxi, whatever, that is maybe let's say a top-down kind of plan. I'm
thinking, you know, I had, I start by identifying the intermediate states that's going to get,
that are going to get me to the end goal. But what I thought you just said was that perhaps the
intermediate states can be learned in the process of, you know, you're still specifying the end
goal maybe and intermediate states are learned and, you know, I'm almost thinking like that,
you know, they become kind of a compact or a better optimized representation of memory, you know,
there's a lot of work happening here to try to, you know, in RL to incorporate different memory
schemes so that we're not throwing away what we've learned in the process. This is a little bit.
So it's, so there's always for a hierarchical scheme, there's a top-down in the bottom-up
component and I think it's conventionally actually something that's been a lot, a lot more
common is to think about it mostly as a top-down process. So there's a single task that you want to
accomplish, it's a very complex task and you'll go in and, and sure chunk that task up into pieces.
Yeah. But what has really worked out much better and this is what we use actually in our work
in the main conference too, is that the discovery of the behaviors works very well when it's bottom-up.
Essentially, if you go and try to do something in the world and in the process you've accomplished
accidentally, maybe something else, like maybe, you know, I'm trying to get to the kitchen,
but I happen to have figured out how to reach the bedroom. I'm not thrilled about that, but I can
sort of file that away as something I've learned to reuse later. And this bottom-up discovery of
skills ends up working very well for these kinds of hierarchical methods because you can discover
those skills before you actually know how to solve some really complex tasks. And so is this
analogy to memory one that's useful or? Yeah, I think so. I think in effect, everything that you
experience in the course of solving whatever tasks you have to solve in the world,
even if it's not useful to that task, you sort of file it away in your memory as behaviors that
you can draw on later. And in practice, the way we represent this is not quite so discrete.
It's actually a continuous space of all the possible goals you could reach and you can think of
this as basically filling out that space. So for all the possible states you've seen, can you get
a behavior that reaches them and it's continuous representation not a discrete one? What did you
call this particular kind of line of research? We don't really have a clear name for it, although
maybe we should. So we don't call it hierarchical reinforcement learning because the higher level
here is not really reinforcement learning. So I've been calling it kind of hybridizing planning
on learning. To me, the important bit about is that the learning provides the abstractions to
the planning. So maybe I should come up with a name for it that reflects that. Okay, so that's
that's a couple of the papers that you had. Yeah, so we have two papers on this. We have
one called search on the replay buffer by Benjamin Eisenbach, which applies kind of a non-parametric
graph search approach to it. And then we have another one called learning with goal-conditioned
policies by Serucia Nasseriani, which applies more of a continuous trajectory optimization approach
for the higher level. So where you continue to see optimize over goals. And so are they it sounds
like they're both they're alternate approaches to the higher level optimization problem as opposed
to kind of picking off small pieces of. So the lower level skills actually in both papers are
built by learning goal-conditioned policies with RL. And that's a technique that has been explored
in prior work too. So that's in some sense. It's very important, but it's not really the new part.
Cool. So that's two of the top papers. Yeah. What else? What else? There was another thing
that I could tell you about that is maybe a little bit outside the norm for what we typically do
because it's not quite so focused on autonomous robotic learning, but it's a topic that I think
was very interesting to work on. And maybe something that could be interesting to your listeners too.
So we had a presentation. This was actually an oral presentation by a student who was actually
a visiting student with us named Pim Dehan. And what he worked on was understanding the role of
causality and imitation learning. So you know causality is obviously a big topic. I'm sure you've had
guests on your show talk about it. Not enough though. Not enough. It's something that it took me a
while to understand why this was important. And for that project we really wanted to isolate.
Well, let's pause there. What did you learn after the while? You know, what was your realization
about why it's important? Yeah. So I guess maybe to answer this I should first say why I was not
sure that it was important before. Please. And the reason that I think this is actually a fairly
typical doubt that comes from a reinforcement learning research. And to kind of go back,
you know, inside the onion one more layer maybe, a summary of causality and kind of, yeah,
you know, how do you describe you know causality? Well, so a causal model is a model that
associates causes with effects. Whereas in correlative model is just a model that tries to notice
that, you know, when two things seem to occur together, you can guess in your data what the value
of one of those things will be from the value of the other one. But if the relationship there is
not causal, that doesn't necessarily mean that when you see new things that same relationship will
still hold. So to use an example, it's maybe a bit more relevant for our work. If you imagine
a driving scenario, you have a car, the car in front of you stops that causes you to stop. But if you
stop, the car in front of you will not necessarily stop. It just so happens that if you look at data,
almost always, if your car stops, the car in front of you stops, but the causality goes the other
way, right? So there's a correlation in the data because people don't just stop for no reason,
but you can't exploit that correlation to affect change in the world. And that's tremendously
important for robotics, of course, because the whole point is to do things that affect change in
the world. But you didn't think that was important for a while. I didn't think that was important
for the following reason, which is that if you train predictive models and you use those models to
act, then maybe your models will make mistakes. But when you go and use them to act, if you actually
try stopping to get the guy in front of you to stop and you notice that it isn't working,
then you will update your model. You'll learn that. Yeah, you'll patch up that whole. And that's
why I said earlier that oftentimes we're enforcing learning researchers. Maybe don't worry about
these things so much because once you're in this sort of closed loop scenario where you're constantly
updating your model, maybe those correlations very quickly get fixed up and turn into causal
relationships because otherwise you make mistakes and you have to fix those mistakes.
But we did figure out a particular setting where this is very important that is actually a setting
where the issue even comes up in practice, like in actual practical things that are deployed there
in the world right now, which is imitation learning. So imitation learning is a very simple and very
powerful tool that people use today for things like autonomous driving and many other about
applications where you collect data from a person operating a machine, maybe driving a car,
and then you just treat that as a supervised learning problem. So predict the action of the person
took from the observations, which seems like a great idea like for driving you can get lots and
lots of data doing this. But there are all sorts of correlations that happen like that stopping
example that I gave. Another correlation could be that maybe you turn on the turning signal when
you're about to turn and then you turn. But of course the turn signal is not what caused you to turn.
What caused you to turn is your intention to make a right turn. And that's perhaps a bit risky,
you know, typically your camera and your car will be mounted outside the car, but if it happens
to be inside the car and you can see your turn signal that it might decide, well, I'll just wait
for someone to turn on the turn signal before I turn, which makes sense of course. So there are
all sorts of these correlations that can happen. And the way it shows up in practice is that when
people add more inputs to their system, maybe they'll have a camera, they'll also add a history,
or they'll add a LiDAR, although that's some other sensor. They sometimes find that their
imitation learning system actually does worse. And why does it do worse? Well, because all those
additional sensors that they're adding add more potential correlations, which might result in
this causal confusion. So one of the things that we did with PIM is that we worked on formalizing
this problem, identifying why it happens, where it happens, reproducing it kind of in a little
peach tradition, some toy domains, and then coming up with some simple causal discovery techniques
that could be used to partially address it. So our solution is not perfect, it doesn't fix it
every time. But to me, the thing that's pretty exciting about this work is that it actually
allows us to make a little bit more formal something that people have observed anecdotally
before, which is this phenomena that adding more information to your imitation learning system
makes it do worse. And I'm pretty happy with that work because I think it's also a technique
that people are using in the real world, and if they're not aware of this issue, they might get
into a bit of trouble. Can you give us a sense for the technique itself, how what it's doing?
Yeah, it's actually kind of neat. So this is something that PIM and Dinesh, the LiDAR
authors on this, came up with. The idea is that you can think about discovering causal graphs.
So if you have a bunch of variables, let's say you have a discrete set of variables,
the variables you're predicting are the actions, the variables that you're observing are the
observation variables of states, and some of those state variables are actual causes of the actions,
and some of them are are are spurious, some of them you should not pay attention to, like the
turn signal or whatever. Now the trouble is that in a if your observations are things like
images, you can't just treat every pixels a possible cause. You know, there's a lot more structure
underneath. So what they did is they actually combined a representation learning phase where you
actually take the your observations and you turn them into a smaller set of independent latent
variables with a causal graph discovery phase. So they have one model that learns this representation,
these disentangled variables, and then they have a second phase where they train a model that
takes in those variables and a mask that represents the graph. So the graphs always map inputs to
outputs. So basically you can represent all possible graphs with a vector of bits that says which
edges there and which edges absent. So is the graph learned or the graph is the graph imposed?
It's actually a little weird. It's the model actually simultaneously represents all possible
graphs, which seems really hard because they're exponentially many graphs. But of course the
wonderful thing with neural nets is that they do generalize. So it turns out that you don't have to
have it train on every possible graph to get it to generalize meaningfully to all or most possible
graphs. So you basically randomly delete or remove edges and then train it on on these randomly
selected graphs and then hope that it generalize to the others. And so long as those training graphs
are drawn from the right distribution then you should get generalization if you sample enough graphs.
Now that doesn't let you discover the right graph. It just lets you compactly represent all the
possible graphs. The discovery then requires intervention. So to discover which of those graphs
is correct you have to somehow break the correlations in your data. So you do that by actually
attempting the task but a very very small number of times just so you can figure out which of those
graphs is the right one. And those attempts can have one of two forms either you attempt the task
and you ask for additional human supervision so you say well oh if I were in this intersection
how should you drive or you assume that you have access to a reward function so you try
yourself and then you look at the rewards. Of course in both cases you could learn the task
entirely from scratch if you have enough interventions but the point is that if you have these
pre-trained candidate graphs you can discover the right one with a very very small number of
interventions. Okay I'm trying to wrap my head around all this. It sounds like if I'm understanding
what's happening that the training effort is like some exponential multiplier on the training
effort for not trying to figure out this causality. In other words it's not a lot harder because
you have to it's almost like you are coming up with this graph and then applying like a dropout
kind of thing where you're breaking these connections and you've got to you know in the graph is
potentially you know you have a kind of end by and fully connected thing like it I'm imagining it
exploding. Yeah well so the reason that this ends up being not exponentially difficult is that
there's a bit of regularity so there's a particular variable and you include that variable or exclude
that variable. While in principle the behavior of the resulting model depends a lot on all the other
variables in practice you can generally get a pretty good idea for the for whether that variables
of cause or not just from whether it by itself is excluded or not. So if you exclude for example the
extra turn signal input and things just kind of work okay maybe the right graph is like no turn
signal and some of these other things but just from the fact that you included the turn signal in
these five graphs and excluded in these five graphs that's a really good hint that the turn
signal is should not be included. So while in theory the whole thing is exponentially bad in
practice for most problems that's not usually the case and you can identify these individually
so that's why we can get away with substantially less than exponential time training for this
even though in principle there are exponentially many graphs. Of course you can construct pathological
problems where this would not work. So if there's some very very specific set of edges that works
and everything else fails then through random sampling you probably won't discover that.
Right right. You mentioned that you formulated a toy problem to illustrate this presumably that's
also the problem that you presented your experimental results in the context of tell us a little
bit about those problems. It's actually a very simple toy problem we just took an Atari game
and we drew on the screen a little number that indicates what action was taken at the last
time step. You think this would be completely innocuous because the action you already took that
action and just draw it on the screen but for good Atari playing strategies usually the current
action is strongly correlated with the previous one. So if you're moving the pong paddle down you
probably move it down for several steps. So there's a strong temporal correlation which means that
drawing that previous action as a digit on the screen gives the imitator a very strong hint about
the next action. But of course that's not a causal relationship because the action you took before
is not the cause of your next action. So it turns out that just adding that digit to the screen for
an Atari game completely breaks your ability to imitate an optimal Atari player which is kind of
a disturbing thing to me. Why is that because the agent kind of over indexes on the thing that you
drew or yeah it's because basically learning machines like to be lazy if they can be lazy. So very
lazy strategies to just say hey I know that the next action strongly correlates with the previous
one not always but it's a pretty good clue. So it's very easy for me to read this digit. It's very
hard for me to figure out what's going on in the game. So I'll just start off by looking at that
little hint that you gave me and I can get some decent performance out of that and I'm so happy
with that that I'll kind of not really pay as much attention to everything else and never bother
learning the real task. So it of course doesn't pay attention exclusively to that thing but it
pays so much attention to it that once that relationship is broken once the action is not
actually coming from an optimal policy once it has to play for itself the performance just tanks.
And so since you're optimizing on the ultimate ultimate outcome it's kind of like
in a sense crude sense like a multitask thing it's like find an optimal policy given
the previous action. Well there's an important distinction which is that imitation learning
is not explicitly trying to find the optimal policy. So this is one of the things about
annotation learning that's a little bit of a shortcoming but it's why it's so simple is that
imitation doesn't care about what's good or what's bad it just cares about what the
demonstrator did. Which is great because you can train it from data without all this complexity
of reinforcement learning but it's not so great because of course it can't understand what's
optimal and what's not. Did you do it and can I copy it? I'm still trying to kind of narrow down
what exactly we've done by drawing the number because we already knew the thing that the agent
is trying to imitate is what was done. It seems like it almost changed the problem.
Yeah. But the subtlety there is that the data that you trained the agent on all came from your
expert demonstrator. So while when the agent is playing the game itself that previous action
indicator doesn't really give it any additional information. When it's trying to copy the expert
it does actually give it a bit of additional information because it tells it that the action
that the expert took in the previous state which is probably pretty similar to the current one.
So it is in the otherwise formulated imitation learning problem the agent doesn't have
access to the action itself. It has access to the observation of. Yeah. So it's supposed to
predict the action. So in the same way that an image class far predicts the label. Yeah.
You're noticing predicts the action from you. Yeah. What other interesting things do you
presenting here? So there is one more thing that maybe would be interesting to discuss a little
bit which is some work by a student named Michael Jan or studying model-based reinforcement learning.
Okay. So model-based reinforcement learning is something that we often think about as being
kind of a more efficient but perhaps somewhat more complicated solution to reinforcement learning
problems as opposed to model-free reinforcement learning. Model-based reinforcement learning
first learn how to predict the future and then you use that predictive model to actually
act in the world to accomplish some tasks. And what Michael wanted to understand is can we
actually analyze the degree to which model-based reinforcement learning provably results in a
better policy at each iteration. So there's been a lot of analysis in model-free RL,
particularly for policy gradients. So we did some work for example back in 2014 with John
Schollman on the Trust Research and Policy Optimization algorithm TRPO where we did analyze
that under some circumstances you provably get an improvement in your policy.
Admittedly under some assumptions that are pretty strong like having infinite samples,
but it's a beginnings of a theory. But such a thing did not exist for model-based RL and one of
the things that Michael and the other students on the paper were able to figure out is that you
can actually write down a similar kind of theoretical proof that given enough samples model-based
reinforcement learning will produce an improvement at each iteration and how much an improvement
it achieves depends on the error in the model obviously and also how much you change your policy.
So if you collect some data and then change your policy be totally different now on that totally
different policy the model will probably be very inaccurate so that's no good. So you need to change
the policy by bounded amount and change the model by bounded amount. That's all fairly straightforward.
But there's another term that showed up in the in the bound that we had which is the number of
time steps for which you utilize the model. So if I predict five steps in the future with my model,
I'll get lower error than if I predict 50 steps in the future. That's again pretty obvious.
What was not obvious is that if you actually look at that bound you know the bound is derived
using standard proof techniques that people have used in other RL model free RL work.
If you actually try to find the optimal value for that horizon you end up with the optimal value
being zero. So it says you know the proof says the model will result in a model they don't use.
Yeah. Yeah. The proof says you will improve but the most improvement is obtained by just not
using your model. That doesn't seem to make any sense that defies our intuition. It also defies
our experimental results. So to a degree this is illustrating some shortcomings in the current
theoretical tools that we have. But then one of the things that we did is we tried to
that number the number of time steps is multiplied by some coefficient which theoretically you know
whenever you do theory you derive sort of the most pessimistic coefficients because you want to
always be true. But then we tried to measure that empirically we tried to actually train some models
train some policies and measure those error terms and see what do they look like in reality.
In reality they're not nearly as pessimistic and in reality models seem to generalize a lot better
than the theory would suggest. So then we did something that you shouldn't really do when
you're doing theoretical work which is that we sort of eyeballed what the actual relationship
and the experiments looks like kind of basically did a linear fit. Subtune that term for the
theoretically motivated term in the bound and then we do actually get a trade off that says
that with the actual observed error patterns you should not use zero length roll outs.
You should also not use very long roll outs. You should limit how many time steps you use your
model for and it turns out that if you actually do that if you use your model for only short periods
you actually get a much better algorithm than anything that was done in previous model based
our all work at least at the time when we published the paper since then of course much better
things have come out. So if you arrived at an analytical relationship between the number of
time steps in some characterization of the problem. Well yes except that this is often the case
for kind of theoretical analysis is that there is a relationship and you can derive the optimal horizon
except that it's derived in terms of quantities that are in practice very difficult to measure.
So probably the actionable conclusion is that you should use a length that is not too long
and not too short probably if you're solving a real problem you're going to select that length
empirically. And so does the takeaway is that the the later analytical result kind of nullifies
the initial analytical result but in practice it kind of works the way we would expect anyway.
So that was a lot very hand wavy. How do you how do you how do you attribute value to the you know
these sets of results? Well I think it works the way we expect it to a degree. I do think that
there were some actionable conclusions from it like for example we did end up with sort of after
combining the empirical observation in the theory we ended up with with an algorithm that uses
pretty short rollouts from the model probably you know shorter than what people typically thought
would be suitable and that did end up working very well. I think it also tells us a little bit about
where we should look to next if we're going to develop better theoretical tools so tells us that
you know you can do some of this analysis but the current tools give you a slightly nonsensical
result there's some term if you substitute in the will you empirically measure that term to be
into the theory you get a much more reasonable conclusion so that maybe give us some guidance
where we can look to next. And I think that does it also say that we should all shift our efforts
to model based RL as opposed to model free RL because it's provably better? Not necessarily so
what does the result say? It says that with model based RL it could improve your policy further
than you could with just model free RL but how much further depends on the error in your model?
So in some cases you can get models with this is very obvious statement in some ways in some
case you can get models with very low error in some cases with very high error if you can get a
model with low error then it seems like you should use it if you can get a model but in some case
that's that's very hard to do so if you have very complex let's say image observations maybe you
really just can't get a model that's accurate enough for more than you know what a single time
somewhere even no time steps into the future so then you're still out of luck. And so one thing
that I often do is maybe conflate the idea of model based as like incorporating physical world
models with what you're talking about which sounds different like you know learn models almost
another type of hierarchical RL. Yeah so for this discussion I was just talking entirely about
learned models right that said the the results and the analysis doesn't really distinguish between
them and there's a very blurry line between them so if you if you do have a physics-based model let's
say you have a CAD model of your robot and a physics simulator and maybe you don't know the masses
of some of the links you can view the process of identifying those masses as a learning process
so you're learning a very small number of parameters we typically work with much more generic
models so basically neural nets that predict the future so there you have a very large number of
parameters but in principle the same lesson should apply to both. Does this particular research
parameterize the relative benefit in terms of number of parameters or is that interesting for you?
We typically don't worry too much about the number of parameters because we're more concerned
with sort of how well we can get this thing to work and how efficiently in terms of data.
This is this is maybe like a little bit of a philosophical point you know I kind of trust
the systems people and the architecture people I think they'll build better chips for us and
we'll be able to have lots of parameters and also in general in deep learning research
one of the things we found time and time again is that when you have a huge number of parameters
things seem to just work better and right now people actually starting to understand why
they're starting to understand that over parameterization is actually a very good thing for
optimization so it seems like trying to keep the number of parameters down is not such a rewarding
process so we're more concerned with data because data comes at a cost and final performance.
Okay so this particular result then to kind of summarize is not necessarily
an about face or shift in perspective from the last time we talked that was very much
in the camp of kind of an end-to-end learned approach with a highly parameterized model you
know a deep neural network as your core model you know towards hey this result says that we should
be incorporating more physics-based models into our RL. Yeah it doesn't necessarily say that
although of course if you do have the luxury of having some knowledge about physics you'll probably
have a more accurate model which means that you'll be able to get more improvement at every step
of this process but yeah it's really all about how much error you have. And you've also got some
work being presented here on off-policy RL? Yeah so this is this is an area that I've been
pretty passionate about for a while because I think it's it's important in robotics it's also
very important in many fields outside of robotics so basically when we think about reinforcement
learning we typically think about it as a very active process so if you open up the you know the
Sutton and Bartow textbook you see this diagram the agent interacts with the world and then it
improves its policy and then interacts with the world some more it's fundamentally an online
active thing but if we want models that generalize very well we want to be able to train those
models on large amounts of data because that's where you get your generalization so if you want
you know to have very good computer vision systems as part of your RL policy you need to see lots
of images and it's very hard to have lots of data and to have this fundamentally active online
learning process. If you need to collect data every time you improve your policy and you need
image net size data sets you essentially need to collect something the size of image net
improve your policy then throw it out and then collect it again and that's just not a very scalable
proposition. So what we've been working on a fair bit is this problem of off policy or offline
reinforcement learning is sometimes they're called fully off policy it's also sometimes referred
as battery reinforcement learning people can't seem to agree on the name but the basic idea is
that you have some data and in the most extreme version you're not even collect allowed to collect
any more data the data is all you've got you just have to extract the best policy you can out of it.
In reality we'll probably do something a little in between we'd use the data and then interact
with the world a little bit but let's just say we were not allowed to interact at all that's kind
of the the most rigid formulation. It turns out that a lot of standard RL methods like Q
learning for example while in principle applicable to that setting in practice perform very very
poorly and people have tried this before and they look at these learning curves where you're
using fully off policy data your policy seems to get better and then it gets worse and they look
at it and say well maybe I have an overfitting problem like that that looks like overfitting
why don't I add more data they add more data same thing happens it's like okay what what is this
an overfitting problem that doesn't go away as you add more data. Turns out that it's not an
overfitting problem turns out that what's happening is that the structure of the Q learning algorithm
itself actually causes it to perform very poorly if it's not allowed to interact with the world on
its own and it actually ties a little bit almost to that causality point that I mentioned before yeah
see in Q learning you you're making counterfactual queries people often don't don't realize this
where does the counterfactual query come up it comes up when you calculate a target value right
because in Q learning you're saying you took this action you got this reward and you're going to
land in this state but then you're going to run a different policy not the one that was used in
the data so ask your Q function how good that new policy would be you don't get to actually run
that policy you just have to ask your Q function so that means plug in a different action
and that action that you plug in is not the action that was taken in the data and people say okay
that's okay the Q function will generalize and it will generalize if the distribution matches
the distribution it was trained on so you can plug in a different action that's okay but you can
plug in an action that comes from a different distribution and when you optimize your policy
of course your policy is going to find a different distribution in fact if your policy can find
an action for which your Q function makes a mistake and erroneously predicts a very high value
it will find that because that's what you're asking you to do so essentially your policy ends up
exploiting your Q function essentially comes up with like an adversarial action it fools your
Q function to thinking that it's a good one and then because you don't interact with the world
because you don't actually end up trying that action you never learn that it's actually bad
in the extreme case you could imagine there's some action that was never ever taken in the data
your Q function will make some completely nonsensical prediction for it and if that prediction
is large it's a large number then your policy will just start taking that action yeah
so it's not an overfitting problem it's actually this kind of counterfactual out of distribution
action problem and once you recognize it for what it is then you can actually study
possible solutions and so why does the existence of this counterfactual problem
manifests itself more acutely in offline well because in online learning you would still make
that mistake but then you would go and take that action and then you would add it to your training
action but if you're not allowed to interact with the world then you don't get the opportunity to do
we're actually not the only ones to recognize this there was so there was also some wonderful work
by a student named Scott Fujimoto who also had a paper that studies kind of a similar type of problem
one of the things that seemed to work out really well in our work is the particular formulation
for the constraint that you can use to alleviate that issue and that turns out to work actually very
well for a lot of offline problems so we have the evaluation this conference is kind of on standard
benchmark tasks but now we're looking to see if we can use it for actual robotics tasks another
thing that I think is super exciting about this line of work is that once you have this fully
data driven way of doing RL you could also imagine applying RL to domains where traditionally
online active collection is very very hard like for example medical applications you don't want to
run a reinforcement learning agent interact with the real patients but maybe you can get some logs
you know maybe applications for e-commerce for educational support agents you know decision-making
support that sort of thing these are all areas where you can get data but it's very costly and
dangerous to actually have active interactions the example you use in the in describing the
off policy offline RL was something like an image net if image net is the data that the data set
that you're working on what's an example of the kind of the problem formulation or the thing
that you're trying to learn yeah so I was maybe a little quick in saying that so talking about a
data set generally I meant image net size got it so so you know image net is a giant data set that
we know enables generalization it's not I mean it's not an RL data sets an image classification
got it got it but if you imagine that you need similar generalization RL you probably need a
similar scale so in the order of you know millions of samples sure sure and so are there what are
the data sets you know for which you are a experiment or with which you're experimenting with
off policy yeah so in the paper that we have in the main conference we you know this is just kind
of standard benchmark tasks so we basically took regular RL benchmarks like the the open AI
gym benchmarks and we made them off policy so that's that's not a that's not an application that's
just a you know a little benchmarking procedure and how do you do that do you just have an agent go
do a bunch of stuff and then erase a bunch of the data or yeah we just we just have we just have
some existing agent interact with a task save the data to disk and then pretend as though we're
given that data okay but that's just for testing um one of the things that we've been doing since
then which is not part of this paper but it's actually something we'll be presenting at a workshop
is actually trying to collect such a data set for real uh we actually called it RoboNet uh and
this was actually kind of a joint effort with a number of different universities so uh we had
some folks from from Stanford Chelsea Finn and her lab some folks from University of Pennsylvania
uh some folks from Carnegie Mellon contribute data to this uh large data set of robotic interactions
and one of the things that we did that was a little unusual is that it's actually a data set
collected from multiple different robots so these are all robotic arms and they're all performing
kind of similar tasks relocating objects on table moving things around but they're actually
different robots and one of the things that uh we studied in that work is well first you know
can we collect this data can we make it available to the community but also can we train a model
that actually is robot agnostic so you could imagine whatever robot arm you have you take the
small plug it in and maybe we'll uh control that robot we didn't quite achieve that so we couldn't
get a single model that actually generalizes zero shot to new robots but maybe somebody else we
get to work later but what we did achieve is we managed to use it as effective pre-training so you
can take all the robots but except for one of them pre-trained and then get a new robot get a
little bit of data and then fine tune and that works so far and you know maybe with a data set out
there and available perhaps people can take it and see if they can move towards zero shot
generalization and of course since it's a fixed data set it's kind of an ideal fit for
fully off-ball CRL research. Well Sergei thanks so much for taking the time to chat with us give
us an update on what you're up to sounds like a bunch of really interesting stuff. Thank you.
We'll have lots of homework to do after this podcast. Thank you. All right everyone that's our show
for today. For more information on today's show visit twomolai.com slash shows. As always thanks
so much for listening and catch you next time.
