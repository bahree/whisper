WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.440
I'm your host Sam Charrington. A big thanks to everyone who participated in, voted

00:35.440 --> 00:42.560
in and supported our My AI contest. I'm excited to announce that our first place winner is,

00:42.560 --> 00:54.240
and I get a drum roll, Mohit Nalavati. Mohit sent in the video personal health care applications,

00:54.240 --> 01:01.380
which you can find along with the other entries at Twimbleai.com slash My AI. Second prize

01:01.380 --> 01:09.000
goes to Julian Ford, for his entry, dad and Eleanor. And third prize goes to Matt Taylor

01:09.000 --> 01:16.080
for his poetic and ominously named entry, let Earth not become our tomb. Thank you all

01:16.080 --> 01:22.680
for entering and stay tuned for our next contest. A special shout out to Lighthouse and Anki

01:22.680 --> 01:31.760
for donating the Lighthouse and Cosmo prizes. Check them out at lighthouse.ai and Anki.com.

01:31.760 --> 01:37.320
In this episode, I'm joined by Raja Chitilla, director of intelligence systems and robotics

01:37.320 --> 01:43.000
at Pierre and Marie Curie University in Paris, an executive committee chair of the IEEE Global

01:43.000 --> 01:49.000
Initiative on Ethics of Intelligent and Autonomous Systems. Raja and I had a great chat about

01:49.000 --> 01:54.600
his research, which deals with robotic perception and discovery. We discussed the relationship

01:54.600 --> 01:59.840
between learning and discovery, particularly as it applies to robots in their environments,

01:59.840 --> 02:04.880
and the connection between robotic perception and action. We also dig into the concepts

02:04.880 --> 02:11.000
of affordances, abstract teachings, meta-reasoning, and self-awareness as they apply to intelligent

02:11.000 --> 02:17.720
systems. Finally, we touch on the issue of values and ethics of these systems. Before

02:17.720 --> 02:22.240
we jump into the interview, this is your final reminder that our next Twimmel Online

02:22.240 --> 02:29.840
Meetup is tomorrow, Tuesday March 13th at 5pm U.S. Pacific time. This time, our focus

02:29.840 --> 02:35.000
will be on the Google Deep Mind Paper, playing Atari with deep reinforcement learning,

02:35.000 --> 02:40.920
in a presentation by Sean Devlin. Head on over to twimmelai.com slash Meetup to learn

02:40.920 --> 02:47.680
more or register. Okay, let's get to it.

02:47.680 --> 02:54.120
Alright everyone, I am on the line with Raja Chitilla. Raja is the executive committee chair

02:54.120 --> 03:00.760
of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, as well

03:00.760 --> 03:07.640
as an IEEE Fellow. He's also a professor at Sorbonne University, Pierre and Marie Curie,

03:07.640 --> 03:12.640
and Director of the Institute of Intelligent Systems and Robotics there. Raja, welcome

03:12.640 --> 03:14.960
to this weekend machine learning and AI.

03:14.960 --> 03:20.400
Hello everyone, thanks for inviting me. Absolutely. Why don't we get started by having

03:20.400 --> 03:25.880
you tell us a little bit about your background and how you got involved in Intelligent Systems,

03:25.880 --> 03:33.960
Robotics and Ethics? Well, let's see, I'll make a long story short because I've been

03:33.960 --> 03:43.520
working in robotics and artificial intelligence since a number of years now. Actually, I defended

03:43.520 --> 03:55.160
my PhD on Mobile Robot Navigation in 1981. So that's quite a long ago. And I've been

03:55.160 --> 04:02.280
working therefore in robotics and intelligence systems, intelligent robotics since that

04:02.280 --> 04:12.640
time. Working on several aspects of those problems, including planning, action planning,

04:12.640 --> 04:22.680
motion planning, perception, decision-making, learning and human robot interaction. So,

04:22.680 --> 04:30.800
it's a field that I've been familiar with since a long time and which is evolving very,

04:30.800 --> 04:40.120
very fast, especially in the past few years because the technologies that were developed

04:40.120 --> 04:48.840
over the years have come to some maturation. And we understand much better now, a lot

04:48.840 --> 04:57.880
of processes and problems to be able to integrate them and get them to real world solutions.

04:57.880 --> 05:05.480
And there have been also some progress, of course, which is enabling those technologies

05:05.480 --> 05:11.800
to become much more efficient, which is the increasing computer speed, the availability

05:11.800 --> 05:20.480
of memory, of data, enormous amounts of data. So that fostered the whole field recently.

05:20.480 --> 05:33.480
Great. And what's your current focus? Well, currently I'm focusing on learning processes,

05:33.480 --> 05:43.760
how to get a robot to interact with its environment. This is a basic issue, but the point is that

05:43.760 --> 05:54.720
we want the robot to learn through his or its perception, its physical interaction to

05:54.720 --> 06:03.600
connect those two together without giving it much information a priority, so that it discovers

06:03.600 --> 06:12.880
actually what is its environment made of. We don't want to pre-program things. We don't

06:12.880 --> 06:19.080
want to supervise its learning process. We want this to be unsupervised and we want

06:19.080 --> 06:25.240
the robot to connect its perception and action, because it's the only way, we think, to

06:25.240 --> 06:32.760
actually grasp an understanding of the environment. If you take a perception in general, it's

06:32.760 --> 06:40.960
considered and developed as an observation process. You have images and they process the

06:40.960 --> 06:49.600
images, you make some classification and you come up with identifying objects in the

06:49.600 --> 06:57.040
scene. And this can be based, of course, on learning processes, such as deep learning.

06:57.040 --> 07:08.000
But I think that this, of course, is very efficient and has proven efficiency lately, but

07:08.000 --> 07:15.800
by doing this, the robot doesn't really understand what it's looking at. To understand, it means

07:15.800 --> 07:26.840
that the robot has to develop a knowledge about what it can do, what it's action scope,

07:26.840 --> 07:33.760
its action possibility on the object in this environment. And therefore, it has to connect

07:33.760 --> 07:40.840
its perception with its action capacities. And this cannot be done separately. It has

07:40.840 --> 07:46.920
to be done simultaneously. So this learning process, which is based, of course, on processing

07:46.920 --> 07:52.720
data from sensors, but it's also based on reinforcement learning of interaction with

07:52.720 --> 08:02.000
this environment, would enable the system, the robot, to really connect its action capacities

08:02.000 --> 08:07.960
with its perceptual capacities. And this is what I call really understanding the environment.

08:07.960 --> 08:17.000
Interesting. And now is the robot learning all of this from the ground up, meaning you're

08:17.000 --> 08:23.960
firing up a system that has no a priori knowledge of its capabilities and it's exercising

08:23.960 --> 08:31.360
stepper motors and figuring out what degrees of freedom it has or what's the starting

08:31.360 --> 08:38.360
point for this understanding in your model? Well, that's the main difficulty, excellent

08:38.360 --> 08:45.920
question, I would say. What I want to do is to achieve this process with a minimal initial

08:45.920 --> 08:55.360
information. Now, what is minimal, that's the point. It's not very clear. But if we define

08:55.360 --> 09:04.640
very basic and very specific minimal knowledge that the robot has when it's manufactured, I

09:04.640 --> 09:12.480
would say, what is the initial programming that it has? And then starting from this, develop

09:12.480 --> 09:19.240
the whole process, but reducing more and more these basic knowledge. So for example,

09:19.240 --> 09:25.560
if we start from really zero knowledge, just as you said, exercising the stepping motors,

09:25.560 --> 09:32.280
this will lead to a very big complexity because there are many degrees of freedom. And there

09:32.280 --> 09:39.120
is quite a wide complexity also in the perception of the system. So we have to define a minimal

09:39.120 --> 09:47.800
set of processing capacities. For example, the system is able to determine verticals,

09:47.800 --> 09:56.000
horizontals, diagonals, it's able to identify some intersections, it's able to define some

09:56.000 --> 10:08.480
regions which have a common perceptual background. So these are minimal processing capacities

10:08.480 --> 10:15.360
that the systems come with, come with. And in terms of actions, it is able to move

10:15.360 --> 10:22.320
its arm, for example. So we are not going to start with the basic stepping motor or

10:22.320 --> 10:28.360
it's basic motor, but more a little bit more complex capacities that enable, for example,

10:28.360 --> 10:37.120
to move all arm to touch something. So starting from that, the development of the process

10:37.120 --> 10:46.600
can be made to understand how we connect perception and action together. Then we can get back

10:46.600 --> 10:54.440
to reduce maybe those initial assumptions, which are already small, but what we want

10:54.440 --> 11:00.440
to prove is the process itself. Do you want to prove the process or improve? Approve

11:00.440 --> 11:08.400
that it can work, that we can actually arrive to do this understanding. The system has

11:08.400 --> 11:14.280
to discover what is called affordances of the objects, of the elements of the environment

11:14.280 --> 11:19.880
that it's interacting with. And then we want to manipulate those affordances, which

11:19.880 --> 11:29.160
are those perceptual action representations, so that the system is able to extract more

11:29.160 --> 11:37.080
and more information about what it can do with them through this interaction.

11:37.080 --> 11:48.280
And in order to prove or even measure the robot's understanding of its environment, you

11:48.280 --> 12:00.120
need some kind of representation for how, I guess, for these affordances, how does the

12:00.120 --> 12:07.960
robot represent affordances and how do you measure, I guess, the scope of the affordances

12:07.960 --> 12:15.400
that it has determined with an environment relative to the total number of affordances

12:15.400 --> 12:26.480
or space of affordances? Okay, yeah, very good question. So the point is, some affordances

12:26.480 --> 12:34.040
are discovered initially, because there are some initial simple interactions like pushing

12:34.040 --> 12:44.400
or holding or tapping. And this provides a first base, a representation and affordance.

12:44.400 --> 12:52.840
It's not, of course, the complete representation. So then the process, which is based on reinforcement

12:52.840 --> 13:05.160
learning, tends to build more complex affordances. So this is guided, if I may say, through values,

13:05.160 --> 13:14.800
meaning that the system has a basic set of motivations that translates into rewards,

13:14.800 --> 13:25.400
that it has when it discovers some representation that afford for those motivations and reinforce

13:25.400 --> 13:35.960
therefore, this kind of representation. But by repeating this process with those basic

13:35.960 --> 13:48.360
motivations, the idea is to build upon the basic affordances more complex ones. So let

13:48.360 --> 13:57.960
me give an example. When the robot pushes a given object, well, suppose this pushing will

13:57.960 --> 14:05.800
be actually pushing on a switch and then there is light that as a result. And one of the

14:05.800 --> 14:14.960
motivations of the robot is to have a light, to have a clearer environment. So then this

14:14.960 --> 14:23.080
object that it has pushed on, which is the switch, is identified as a forwarding light.

14:23.080 --> 14:33.760
But the initial representation was just a kind of box or a very simple object, which had

14:33.760 --> 14:39.120
other affordances. For example, the robot could be able to hold it or push it. But now

14:39.120 --> 14:50.400
it's associated with something new, which is light. And so it has a more abstract property,

14:50.400 --> 14:58.240
which is not just its geometrical shape or the fact that it was pushable and so on.

14:58.240 --> 15:06.960
Now it's associated with light. And I can then use it for building up on more complex

15:06.960 --> 15:15.720
motivations, which for example are for fetching other objects I need first to put on the

15:15.720 --> 15:26.400
light. And therefore, this object has become a, I don't really dare to pronounce the word

15:26.400 --> 15:34.560
symbolic, but that's what it is. It's not just described in its perceptual features.

15:34.560 --> 15:42.120
It's more described in some more abstract properties. It's related to this property of light

15:42.120 --> 15:47.840
and the idea is that then we can build upon that again and again.

15:47.840 --> 15:56.560
Yeah, I'm thinking a little bit of Josh Tenenbaum's work at MIT and trying to relate some

15:56.560 --> 16:08.560
of what you're doing to what I've heard of that work. And it's interesting in that I

16:08.560 --> 16:14.480
think what the way I think of what he's trying to do is like come up with this notion of

16:14.480 --> 16:21.320
common sense for like how things work. And what you're doing is starting from almost

16:21.320 --> 16:27.600
no common sense in trying to have the robot figure everything out on the fly. I don't

16:27.600 --> 16:35.440
know if you're familiar with the stuff that he's doing, but I'm curious if you can maybe

16:35.440 --> 16:38.240
help me contextualize how they might fit together.

16:38.240 --> 16:47.240
Yes, of course, I know about Josh Tenenbaum's work and work of many others in this area,

16:47.240 --> 16:54.920
which is not recent, of course. And there have been a lot of efforts and research to try

16:54.920 --> 17:06.600
to provide common sense to robots, to computer systems and so on. And so it's absolutely related.

17:06.600 --> 17:15.680
And I think the approach to building common sense should really be grounded in the physical

17:15.680 --> 17:25.760
interaction with the world. So this is how we build common sense by trying to discover

17:25.760 --> 17:32.320
the effects of our actions, our, I mean, I'm speaking about the robot, of course, the

17:32.320 --> 17:44.120
effects of the actions of the robots action on the work. So the without this knowledge

17:44.120 --> 17:52.040
common sense would be just a very abstract representation. It wouldn't be really something

17:52.040 --> 18:02.680
that the robot is able to sense in its own actions. So that's, I mean, it's, of course,

18:02.680 --> 18:12.680
in this general paradigm of developing common sense. And the approach is developmental again

18:12.680 --> 18:21.240
similarly to what children do. They have some basic capacities, basic knowledge, basic

18:21.240 --> 18:29.920
sensing, basic action capacities. But when they start to experiment with the environment,

18:29.920 --> 18:35.040
this is where they discover that there is something called gravity. Of course, they don't

18:35.040 --> 18:43.680
call it like that. But they discover that when they hold something and they just let it

18:43.680 --> 18:50.520
go, it will drop to the ground. So it means there is a specific effect from their action

18:50.520 --> 18:58.560
that is observed. And the connection between this perception and action is the component,

18:58.560 --> 19:05.120
the major component of what is going to become common sense, which is the description of

19:05.120 --> 19:14.880
the laws of physics of what actually is the effect of actions on the environment. Then we

19:14.880 --> 19:20.800
will need something else, which is generalization, because what I'm speaking about is just the

19:20.800 --> 19:28.000
interaction of the system with its environment. But what what happens when you observe, observe

19:28.000 --> 19:39.320
other agents, for example, doing things and how do you develop from that, also representations

19:39.320 --> 19:47.480
that you can embed with your own actions. That's another step, which is, I think, also

19:47.480 --> 19:52.560
very important. But we are not doing this yet.

19:52.560 --> 20:01.920
As opposed to having the robot learn from base principles, everything about the world,

20:01.920 --> 20:08.160
shouldn't it have the benefit of the knowledge that we have? And for example, are there

20:08.160 --> 20:18.240
ways to or should we move towards ways of representing the laws of physics, for example,

20:18.240 --> 20:25.920
and being able to provide that as a, you know, an input model or something to the robot

20:25.920 --> 20:31.880
that it can kind of discover and use affordances with that knowledge as opposed to needing to

20:31.880 --> 20:34.960
figure that all out from scratch?

20:34.960 --> 20:44.120
If I can answer bluntly, I would say the more knowledge you input to the system, the

20:44.120 --> 20:53.160
less it will be able to understand its environment. It has to, to some point, it has to build this

20:53.160 --> 21:04.960
knowledge by itself. Then it can maybe generalize or learn something that it can relate to its

21:04.960 --> 21:16.240
own experience. But let me know if the following example is good. You can explain to me theoretically

21:16.240 --> 21:27.320
with the laws of physics how I can swim or how I can ride a bike. If I don't do it myself,

21:27.320 --> 21:36.280
I wouldn't really understand what you are talking about. So then if I do it myself and then

21:36.280 --> 21:47.120
I read or I learn the laws of physics about flotation, about the conservation of energy,

21:47.120 --> 21:53.320
of the moments and so on, well, then I can really theorize and understand what's going

21:53.320 --> 22:04.160
on when I ride the bike. So that's the idea. If we cannot anchor or ground on actual experience,

22:04.160 --> 22:12.320
these notions, then we wouldn't really be able to understand these notions. So providing

22:12.320 --> 22:23.040
initial knowledge or information, especially if it's described in theoretical ways or ways that

22:23.040 --> 22:31.160
relate to how the humans actually understand this, I don't believe it will be much of help.

22:31.160 --> 22:39.680
So one of the things that you mentioned earlier that I thought was interesting was that you put all

22:39.680 --> 22:49.040
of this or the robot is exploring all of this in a framework, a values-based framework, right?

22:49.040 --> 22:56.240
The robot has some values or motivation and it strikes me that perhaps this is one of the hooks

22:56.240 --> 23:08.000
into the broader idea of ethics. Is that correct? Yes, well, values. Of course, it relates to some point

23:08.000 --> 23:13.440
to ethics because the values I'm speaking about when you start to interact with the environment are

23:13.440 --> 23:26.960
more values that will in the beginning relate to what is able to answer the motivation of the

23:26.960 --> 23:37.520
robot. For example, just for the sake of simplicity, if the motivation of the robot is to have more power,

23:37.520 --> 23:48.640
to plug into a power device, to have more energy, then this would lead the robot to search for those

23:48.640 --> 24:01.760
actions and those perceptual inputs that can lead it to a better reach this goal, which is

24:01.760 --> 24:08.720
actually achieved this motivation. But then the idea is exactly to make those motivations more

24:08.720 --> 24:15.600
and more complex, starting with basic things like energy, for example, but then you can formulate

24:16.880 --> 24:28.720
motivations in a more complex way because, for example, in order to reach the power plug,

24:28.720 --> 24:38.720
I might have to go to somewhere else because it's not rotated here. Therefore, as another motivation,

24:38.720 --> 24:48.160
there is the motivation to move, to have this motion to reach somewhere else where I can

24:48.960 --> 24:57.520
indeed be able to have my initial motivation fulfilled. This is just an example to say that

24:57.520 --> 25:06.320
we can build more and more global or abstract motivations. I mean, that's my purpose. To the point

25:06.320 --> 25:16.000
that motivations can maybe lead to ethics, in this case, because when I mentioned values,

25:16.000 --> 25:23.360
initially the values are those that are going to lead some to some rewards. So starting from

25:23.360 --> 25:34.720
basic rewards to more complex rewards, is it rewarding to have, for example, a positive

25:34.720 --> 25:46.240
interaction with another agent? How is it rewarding to have an interaction that is satisfying

25:46.240 --> 25:55.120
another agent's own motivations? Now, if you build on that, you can reach a point where this notion

25:55.120 --> 26:03.920
of motivation is going to, and the values guarded, is going to lead the robot to respect some

26:03.920 --> 26:20.000
maybe ethical values. Ethics is built eventually when the Greek philosophers, when Plato, Aristotle,

26:22.000 --> 26:31.680
came up with the notion of ethics. The idea was to live the good life. The good life is what enables

26:31.680 --> 26:42.960
society to hold together, what enables people to live in a society in which they have the ability

26:42.960 --> 26:55.120
to reach a form of happiness. That's what is the initial grounding of ethics. How can we live

26:55.120 --> 27:08.800
together and live the good life? So it means that this comes from an ability to respect some values,

27:08.800 --> 27:20.240
which make this social life possible instead of each one trying to kill their neighbors.

27:20.240 --> 27:28.000
And so this can be translated, although it's not what I'm doing now in terms of scientific

27:28.000 --> 27:35.200
research. This can be translated if we want our robots to respect, to develop an ethical

27:39.280 --> 27:49.920
behavior or an ethical common sense. This can be also input as a motivation of the robot system.

27:49.920 --> 28:01.840
However, personally, I don't believe that at least for some time we can build robots that can

28:01.840 --> 28:10.240
grasp the meaning of some abstract notions, such as human dignity, for example. The notion of

28:10.240 --> 28:22.320
dignity, which also is very important in ethics. I don't believe that we can really define this or

28:23.920 --> 28:32.720
I don't see a way for a robot, which is a machine, to discover what this means exactly. So

28:32.720 --> 28:42.400
there will be some order maybe that we will have to cross one day to try to achieve this. But

28:42.400 --> 28:53.440
today I don't see how we can really define or learn by a robot. These abstract notions of human

28:53.440 --> 29:07.280
dignity, dignity, freedom, human rights. I'm just curious, there are abstractness is a spectrum.

29:07.280 --> 29:12.880
I'm not sure if I'm not sure that it's a two-dimensional spectrum. But I'm wondering,

29:12.880 --> 29:18.000
do you have a sense for what's the most abstract thing we've been able to teach a robot?

29:18.000 --> 29:24.160
Or any examples of things that are kind of towards the abstract side of the spectrum?

29:24.160 --> 29:32.240
Not sure, I can answer this. Well, it's an abstract question.

29:32.240 --> 29:41.520
Yes, no, because it's quite difficult to define what the robot has actually learned when we say

29:41.520 --> 29:52.160
what does the robot know exactly. Of course, I can program the robot with some very abstract

29:52.160 --> 29:58.560
notions. For example, speaking about emotions, I can program a robot that says, I love you,

29:58.560 --> 30:06.080
but I don't think the robot understands what you love is. It's just the result of a specific

30:06.080 --> 30:15.280
programming, some parameters that have been input and that the system can measure and react

30:15.280 --> 30:22.560
according to those values of the values of those parameters. But understanding what love means

30:22.560 --> 30:29.760
exactly, I'm not sure, I'm even sure of the contrary, that a robot can grasp this.

30:29.760 --> 30:36.720
So this is why I'm saying, it depends what you mean by teaching and what you mean by understanding

30:38.160 --> 30:41.760
by a robot system of those abstract notions.

30:42.800 --> 30:50.800
I guess the things that came to mind were, I think we've been able to teach robots

30:51.600 --> 30:59.280
an abstract notion of shape, for example, that is at a higher level than point locations.

30:59.280 --> 31:05.200
And an abstract notion of relative position, like something being on top of another thing that,

31:05.200 --> 31:13.440
again, is abstracted away from stepper motor rotations and point locations and things like that.

31:14.160 --> 31:20.960
And it just occurred to me that there's a spectrum of abstractness. I don't know if those are

31:20.960 --> 31:28.400
pretty close to the physical side of on that spectrum. And maybe we're just kind of stuck around

31:28.400 --> 31:34.240
abstracting these base physical things. And I don't even know how to characterize the line between

31:34.240 --> 31:39.600
that and the next set of things. I was just curious if this was something that you've come across

31:39.600 --> 31:45.840
and thought of. Yeah, well, I still believe that the examples you've provided, which are mostly

31:45.840 --> 31:51.520
geometrical examples, for example, or biological examples, so it's above something else.

31:51.520 --> 31:59.520
I still believe that even if these sound as abstract notions that we have defined,

32:01.360 --> 32:07.840
we, I don't believe we have really made a robot understand what this means exactly.

32:09.600 --> 32:18.400
Of course, you can describe what it means box A on top of box B or whatever

32:18.400 --> 32:32.400
by defining some relationships between the shapes. But this is this really defined in any situation.

32:33.600 --> 32:43.040
Is it that abstract? What what will happen, for example, if you have a robot in outer space

32:43.040 --> 32:50.880
and there is no gravity, what does it mean something on top of something else?

32:52.160 --> 32:57.840
I'm not sure robot can grasp the profound notion of something on top of something else

32:59.600 --> 33:07.840
because it doesn't even have this notion of gravity. And notion of up, down, and so on is

33:07.840 --> 33:16.960
very much related to our relationship with the world. So it's on top because my I'm vertical,

33:16.960 --> 33:23.440
for example, and something is on top of something else because it's following this verticality,

33:23.440 --> 33:33.920
for example. But when you change the referential, I'm not sure the robot can grasp what this means.

33:33.920 --> 33:43.360
So I believe our definitions are actually very much grounded in some knowledge that is not

33:43.360 --> 33:53.120
expressed, not explicit in the robot system. So that kind of begs two questions for me. One is

33:54.160 --> 33:59.440
what does understanding even mean and what is what is the test for robot understanding?

33:59.440 --> 34:10.800
And secondarily, it also causes me to question this idea of having robots learn things from scratch

34:10.800 --> 34:20.480
and in particular, if the robots are going to interact with and communicate with us as humans

34:20.480 --> 34:26.320
and so many fundamental things about the way we perceive the world, like, you know, something

34:26.320 --> 34:32.800
being on top of it is fundamentally connected to our humaneness. In this case, our verticality,

34:32.800 --> 34:39.360
like, don't we have to give the robots a lot in order for them to develop an understanding of

34:39.360 --> 34:48.320
the world that even makes sense to us? Yeah. Well, that's exactly the point. I mean, if we want to

34:48.320 --> 34:57.040
build a robot that is useful, that we want to use tomorrow, that we want to use it for achieving

34:57.040 --> 35:05.360
tasks with which we want to communicate and say, put this book on top of this desk, for example.

35:05.920 --> 35:13.760
Of course, we have to program things into those robots. But again, the robot is just a kind of zombie.

35:13.760 --> 35:18.560
It doesn't understand what it's doing. It's doing it, but it doesn't understand what it's doing.

35:18.560 --> 35:28.480
It's just repeating or executing some program that has embedded what is needed for it to do this

35:28.480 --> 35:38.160
task. If we truly, I mean, from a scientific point of view, not from an utilitarian point of view,

35:38.160 --> 35:45.120
if I may say, if we want really to understand what this means, what does it mean for a system,

35:45.120 --> 35:53.120
for a machine to develop those notions? Well, then it's a different story. And this is what

35:53.120 --> 36:01.760
actually AR robotics in terms of scientific research is trying to deal with. And we are still

36:01.760 --> 36:11.440
miles and years from understanding how we are going to do that. So two different things.

36:11.440 --> 36:16.080
And just to be a little bit more provocative, I would say, my personal

36:17.840 --> 36:26.400
view is that to achieve this understanding, you said what understanding is, to achieve this notion

36:26.400 --> 36:37.520
of understanding. I believe something is necessary. It's the notion of self-awareness.

36:39.200 --> 36:48.640
If the machine does not have this notion, I don't believe it will be able to really understand.

36:48.640 --> 36:58.000
Because understanding is related to me, I mean, the self, being at the center, at the core of a

36:58.000 --> 37:09.680
process of interaction with the environment. And the capacity of reflecting on one's own capacities,

37:09.680 --> 37:16.240
one's own reasoning, reasoning on what I'm doing. Meta reasoning, if you wish.

37:16.240 --> 37:26.320
So I believe that if we are not able to develop these meta reasoning capacities, this self-evaluation

37:26.320 --> 37:36.560
capacity that enables to reflect on one's own place in the environment, in the universe,

37:36.560 --> 37:47.040
in the world, this notion of understanding would be just artificial, not genuine. And which leads us

37:47.040 --> 37:55.280
to this notion of are we able to build self-aware machines, conscious machines, self-aware machines?

37:56.240 --> 38:03.920
That's another issue that is much discussed as well. But I think this is fundamental.

38:03.920 --> 38:14.880
Otherwise, we will always have systems that just act like zombies, not like entities that

38:16.560 --> 38:24.000
grasp what their environment and therefore what they are about, what they are doing, really.

38:25.360 --> 38:32.640
Is there anything that you've come across in the research that is showing promising

38:32.640 --> 38:36.320
results at demonstrating some degree of self-awareness?

38:39.280 --> 38:46.720
There are tons of publications on self-awareness. But frankly, I think we are just

38:48.160 --> 38:56.880
rotating around this notion like the planets are rotating around the sun,

38:56.880 --> 39:03.440
except that the sun we can see it and the consciousness or self-awareness we can't, and we

39:03.440 --> 39:13.440
don't have a clear definition of what it is. So this is actually the basic motivation between my work

39:13.440 --> 39:20.640
is to try to understand if and how, and of course the question is still quite open, if and how

39:20.640 --> 39:28.080
we can develop this notion of self-awareness through the interaction of the system with its

39:28.080 --> 39:36.080
environment, the interaction of the system with other agents, other systems. The idea is that

39:36.080 --> 39:42.400
when you interact with the environment, you have to relate things to you. You have to recognize

39:42.400 --> 39:51.360
yourself in this environment. This is my action, this is my arm doing this, and this is therefore

39:51.360 --> 40:01.520
my evolution, my will, my decision to act in this way. This my places the self in the center.

40:02.720 --> 40:07.520
This means something to me because I can do something with it.

40:07.520 --> 40:17.040
If the system is not able to grasp this relationship between itself and its environment,

40:17.040 --> 40:21.920
it will not have self-awareness. So the self-awareness will be developed through this interaction

40:21.920 --> 40:26.800
with the environment. It strikes me that some of the work that's happening in and around

40:26.800 --> 40:35.680
reinforcement learning, playing Atari games and the like, it strikes me that some of that is

40:35.680 --> 40:44.320
approaching self-awareness. The agent is clearly aware, well, I guess we have to define aware,

40:44.320 --> 40:52.800
but there's a clear relationship between the actions of the agent and the environment and the

40:52.800 --> 41:00.000
whole premise of the reinforcement learning environment is that the agent can manipulate this

41:00.000 --> 41:07.920
environment and maximize some objective. Absolutely. The reinforcement learning, in my opinion,

41:07.920 --> 41:16.960
is a core to this, and this is a technique which is absolutely related to try and

41:16.960 --> 41:24.800
need to build, reinforce this understanding. The point is, of course, that in your

41:24.800 --> 41:31.280
reinforcement learning, you have to learn the value function. It should be something that is

41:31.280 --> 41:40.000
discovered as well. This means that I have to discover what is of value tuning. This gets us back

41:40.000 --> 41:51.680
to motivations. There should be some minimal motivations upon which more complex motivations

41:51.680 --> 41:57.520
are going to be built, but there should be some minimal motivations. However, most systems

41:57.520 --> 42:05.120
doing reinforcement learning are focused on solving a specific problem. You have systems with,

42:05.120 --> 42:10.400
of course, the concept is quite wide and quite general, but you have a system that's going to play

42:10.400 --> 42:16.240
Atari, as you said, or that's going to assemble boxes and so on, with the same process. Again,

42:16.240 --> 42:27.280
the process is core, but the values and the way they translate into achieving the motivations of

42:27.280 --> 42:36.320
the system, that's a big point. For example, you achieve a system that is able to play, say,

42:36.320 --> 42:45.440
a reinforcement learning is going to play Go or to play Atari, whatever, but what about a system

42:45.440 --> 42:52.320
that says, I've had enough playing this game, let's do something else. Then it's going to

42:53.920 --> 42:59.600
actually reason differently. It's not just doing something that it has been designed to do.

42:59.600 --> 43:09.600
It's going to decide something else, which maybe is related to a different motivation. That's

43:11.600 --> 43:24.320
a point in achieving a system that is not just doing what I have or learning to achieve what

43:24.320 --> 43:33.200
I programmed to do. It has to be able from a wider set of motivations or a wider set of

43:36.000 --> 43:49.280
self-reflection capacities to jump, to initiate maybe other decisions that were not explicitly

43:49.280 --> 43:58.880
what I asked it to do at the moment. I'm not sure I'm clear, but the point is if the system is not

43:58.880 --> 44:07.120
able to reflect on its own motivations and to change that, then it will remain prisoner

44:08.320 --> 44:18.000
in the compartment of actions and objectives that the programmer has completely defined.

44:18.000 --> 44:25.360
I'm not sure it's possible to get out of that compartment, but that's the issue. That's the point.

44:26.240 --> 44:34.960
The notion of, I guess we can call it self-agency, that you just describe being able to come up

44:34.960 --> 44:43.280
with its own motivations. Do you think that's inseparable from the idea of self-awareness?

44:43.280 --> 44:52.080
Like, could a machine be both self-aware and yet unable to determine its own motivations?

44:52.880 --> 45:02.400
Well, that's just a belief. I'm not sure. I think it's related, maybe to some degree.

45:04.080 --> 45:10.480
I'm not sure we can define. Self-awareness is not necessarily something you have or we don't.

45:10.480 --> 45:18.880
You might have several degrees of self-awareness. There are studies about this for some animals,

45:18.880 --> 45:32.800
for example, for the dolphins or for some apes, but there might be some degrees of self-awareness

45:32.800 --> 45:42.560
with, so I'm not sure there is only one degree, but eventually, yes, I think that this is related

45:42.560 --> 45:57.040
to this capacity of being able, ultimately, to have a kind of free will to step out of the enclosure.

45:57.040 --> 46:04.960
So, and this might lead more explicitly to the notion of ethics, actually, although we are trying

46:04.960 --> 46:10.800
to, in the artically initiative, to work on the ethical issues of today's autonomous systems,

46:10.800 --> 46:17.280
of today's intelligent systems. Those which don't have those capacities,

46:17.280 --> 46:33.040
how do we reach a development or a design process of those systems so that they are compliant

46:33.040 --> 46:42.000
with human values. So, this is a design and methodology, for example, this is something that

46:42.000 --> 46:50.800
has to be considered and taken into account when people, researchers, engineers, designers

46:50.800 --> 47:00.480
develop such systems, but on the more research scientific side, of course, having an autonomous system

47:00.480 --> 47:12.640
or a system that is capable of reflecting on its own, on its own about ethical considerations.

47:12.640 --> 47:20.640
Well, that will be the ultimate thing, but as I mentioned earlier, I don't believe we are

47:20.640 --> 47:31.360
even close to that. It is certainly a fascinating topic in one with many, many kind of interrelated

47:31.360 --> 47:36.400
layers. You know, we're coming up on the top of the hour, we've hit the top of the hour,

47:36.400 --> 47:42.000
I think we're going to have to schedule a part two at some point to go into the ethics of

47:42.000 --> 47:49.840
autonomous systems. There's a lot there to chat about as well, but I really enjoyed learning about

47:49.840 --> 47:57.760
your research and some of the ways you're approaching this idea of abstract thought and

47:57.760 --> 48:04.160
understanding for robotics. Well, thank you. Yes, I would be delighted to have another chat,

48:04.160 --> 48:10.400
because indeed we just alluded very, very briefly to that, but the conversation was very,

48:10.400 --> 48:19.120
very interesting. Great. Well, thank you so much, Raja. Thank you, Sam. All right, everyone, that's our show

48:19.120 --> 48:25.040
for today. For more information on Raja or any of the topics covered in this episode,

48:25.040 --> 48:32.000
you'll find the show notes at twimlai.com slash talk slash 118. If you have any questions for

48:32.000 --> 48:37.680
Raja, please post them there and we'll make sure to bring them to his attention. If you're new to

48:37.680 --> 48:43.280
the podcast and like what you hear or you're a veteran listener and haven't already done so,

48:43.280 --> 48:49.200
head on over to your podcast app of choice and leave us your most gracious and glowing review.

48:49.840 --> 48:55.520
It helps new listeners find us, which helps us grow. Thanks in advance. And of course,

48:55.520 --> 49:09.040
thanks so much for listening and catch you next time.

