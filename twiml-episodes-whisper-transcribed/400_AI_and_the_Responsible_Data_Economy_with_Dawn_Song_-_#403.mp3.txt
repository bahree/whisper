Hey everyone, just a quick announcement before we get you over to today's show.
I'd like to invite you to join me on Thursday, August 27th for my conversation with Dylan
Urb, co-founder and CEO of PaperSpace, as we discuss machine learning as a software
engineering discipline.
We've been having a ton of fun with these watch parties and of course Dylan and I will
be in the chat answering all of your questions about the relationship and differences between
traditional software development practices and MLOPS and how to scale up and manage machine
learning pipelines.
We'll begin at 1pm pacific time, register at twimmelai.com slash watch 404 for reminders
and updates.
And now on to the show.
Alright everyone, I am on the line with Don Song.
Don is a professor of computer science at UC Berkeley and CEO and founder at OASIS Labs.
Don, welcome to the Twimmelai AI podcast.
Thanks a lot for having me.
Yeah, I'm really looking forward to digging into our conversation.
We will be talking quite a bit about responsible data and what that means.
But before we do, let's start off with a little bit of background.
What got you started in working in AI?
So my undergraduate was in physics and I switched to computer science in my graduate school.
And in my graduate school and also later on as a professor I spent a lot of time actually
focusing in security and privacy, thinking about how to build security systems and so
on.
But at the same time, I've always been really interested in building intelligent machines.
So yes, I'm really glad that I have had the opportunity and the experience to really
try to see how we can make progress in that space.
AI and trying to figure out how to build intelligent machines.
I think it's really probably the ultimate goal.
If we can build intelligent machines, there are so many problems that we can solve and
so on.
So it's just really, really exciting and I would say probably one of the most important
pursuits.
Awesome.
Awesome.
And you're currently working on a startup now, Oasis Labs.
How long have you been working on that?
Oasis Labs is about two years now.
Tell us a little bit about the genesis of Oasis Labs and the problem that you're looking
to solve there.
So at Oasis Labs, we focus on building what we call a platform for a responsible data
economy.
So as we know, Internet has really changed everybody's lives and mostly for the better,
but at the same time, we do see many challenges in particular.
As we know, data is critical.
It's a key driver for a modern economy, but a lot of this data is also really sensitive
and handling the sensitive data poses many challenges for the, on the user side as well
as on the business side.
So for the user side users are losing control of their data, they don't really know what
their data has been used for, how their data has been used and so on.
And also, they are now getting direct sufficient direct benefits from their data.
And on the business side, businesses continue to suffer from large-scale data breaches
and also it's becoming more and more expensive and cumbersome for them to comply with, for
example, privacy regulations like GDPR and CCPA.
And some more importantly, it's still really difficult for business to utilize data due
to data silos and privacy concerns and so on.
So the hope is that we can build a new platform for a responsible data economy that helps
address many of these challenges that helps users to better maintain control of their data
and rise to data and also help businesses to better utilize data, but in a privacy
preserving and responsible way, essentially to enable a new paradigm to address many of
the challenges that I mentioned.
We've talked quite a bit on the podcast over the past few years about differential privacy
and related techniques.
Is that a core piece of this vision of a responsible data platform?
Yeah, that's a great question.
So in order to enable something like what I just mentioned, this responsible data and
responsible data economy, essentially, it needs to address a number of different
questions, different types of questions.
So first of all, we need to ensure that users' rights to data is properly maintained as
well logs and so on.
So for that, we actually utilize blockchain to maintain an immutable ledger for users'
rights to data and also the log of how the data has been utilized.
And then also, we need to ensure that when the data is used, it's used in a way that we
call it a controlled use that actually satisfies users' privacy requirements and their policies
of how their data should be used.
So for that, essentially, we need to address at least two separate types of questions.
One is typically today, for example, when you talk about data markets, the buyer usually
buys a copy of the data.
And they essentially get a raw copy of the data.
And once they buy or get raw access to the data, they essentially, they can do anything
they want with the data.
They can go ahead and resell the data, they can use the data for other purposes that
may not be for the best interests of the data owner and so on.
So that's also one of the big challenges today for data use.
So in contrast, ideally, for responsible data use, what we need to enable is what we
call controlled use.
So in this case, for example, the buyer of the data, they don't just buy the data itself,
what the buyer is the use of the data.
To enable this, essentially, they shouldn't get a copy or a raw access to the data itself.
So what we enable in this platform in this controlled use is enable the buyer or the user
of the data to actually use the data in a confined environment.
You can also think of it as a black box.
So the data will only be computed over in this black box so that the data itself doesn't
leak out and then the buyer or the user of the data doesn't actually ever get raw access
to the data, doesn't get a direct copy of the data.
They can only use the data in this black box confined to environments.
So that's number one.
We call that secure computing.
There are a number of techniques that you can use to enable this secure computing to essentially
you can use it as a way of computing over encrypted data.
So basically, right, so you can use cryptography-based approaches such as homomorphic encryption or
multi-party computation and so on.
So you can use secure hardware which also essentially provides this type of black box-based
black box like confined execution environments so that the data can only be used inside this
black box environment.
Okay.
And a lot of ways it sounds like there's a much lower tech analogy here in like old school
direct marketing.
You have these list aggregators that would collect people's mailing addresses and they
don't want to give the catalog vendors access to their mailing addresses because then
why would they need to license them again?
So instead, the catalog vendors would send them the things that they want to send out.
I think the same thing happens in email as well, right?
So the list company won't give someone the list, they'll say, we'll send your email for
you.
And a lot of ways you're saying, we're not going to give you the data but we'll do your
compute for you.
And now you need to come up with interesting ways to allow people to actually do the compute
on these personal data without giving them access to it.
That's an interesting analogy.
So of course, computing on a data is much more complicated than sending out emails to
your email address and so on, right?
So that requires an entirely new type of technology to enable the secure computing so that you
can only compute on the data you can now actually get access to the data.
So there's one part and the other part is then you want to ensure that the computation
itself on the data also complies with the user's policies of how their data should be utilized.
For example, you want to ensure that when someone uses the data to train a machine learning
model, then in the end, the machine learning model is being used for queries, for inference,
and so on.
You want to ensure that the machine learning model itself doesn't leak, individual usage
information.
And so you mentioned about differential privacy earlier, differential privacy is one example
technology that can help address this problem.
So speaking of which, like the privacy challenges for training machine learning models, I can
give you one example.
This is a recent work that we did in collaboration with the researchers from Google and so on.
And the question here where trying to explore is the following.
As we know, that neural networks has very high capacity and they can, so the question
is whether when you train a machine learning model, it actually remembers a lot about the
original training data and if it does, it can actually exploit this issue and try to actually
learn sensitive information about the original training data in this case, even just through
requiring the machine learning model without even getting a copy of, for example, the parameters
and so on of the model.
Right.
If I remember correctly, there have been papers where they were able to demonstrate that
you can reconstruct images that were part of a training set of the machine learning model.
So that's one example and so the example work that we did is in the language model setting.
So we showed that if you train a language model, for example, using an email data sets,
in our case, it's called the Unreal email data sets.
The Unreal email data sets naturally contains uses social security numbers and critical
numbers.
And we showed that when you train a language model on this email data sets, an attacker
by devising new attacks and just by querying this language model without even knowing the
details of the model, such as the parameters of the model, the attacker actually is able
to recover the original uses credit card and social security numbers that were embedded
in the original data sets.
So this is another example showing that as we train machine learning models, it's really
important to pay attention to privacy protection to use this data.
And in this case, actually, we showed that for this particular case, we actually can have
a very good solution to the problem.
The solution is that instead of a training vanilla language model, if we train a different
language model, then in this case, we can still have pretty good utility, but at the same
time, we can significantly enhance the privacy protection of the user's data in this language
model.
Right.
So essentially, so that's why, as I mentioned, in order to build a platform for a responsible
data economy, we also need to ensure that the computation on data itself doesn't leak
users sensitive information.
And in this case, utilizing technologies like differential privacy can help us to ensure
that the computation results itself doesn't leak sensitive information about individual
users.
OK, in the example that you are describing to what degree are you being fine-grained about
what you consider sensitive information versus what isn't sensitive information?
Everyone's talking about GPT-3.
And there's a very coarse-grained argument that says that what GPT-3 is doing is kind
of remembering all of the text on the internet and kind of regurgitating it in creative ways
based on the prompt.
And so from that perspective, all it's doing is leaking information, but in a constructive
way.
And trying to draw a parallel between that and the data leaking that we're talking about
in this case.
Right.
Yeah, that's a very good question.
And exactly, even in a work that we did, studying the privacy challenges of language
models, one of the main issues here, when the vanilla language model leaks sensitive
information about user's data, it is memorization, it is remembering those credit card numbers
and social security numbers that were infected in the original training data set.
So in this case, what we want is when you train a language model, you want a language
model to really learn essentially how to predict, for example, the next words, the next
character and so on, for the things that's essentially in this probabilistic way, but now
actually remembering all these individual social security numbers and credit card numbers
and so on.
So essentially, it is how to address this memorization issue.
And in our work, we actually showed studies and that's a, this language model, they
do remember these occurrences for social security numbers and credit card numbers in this particular
case.
And also, we proposed the measures of code exposure to actually measure the, like the degree
of the memorization that the language model has essentially has occurred in the language
model.
And for GPT-3, so we actually, we have further extensions of our work that is studying
these type of models and like you said, these really, really large models, they can actually
remember a lot.
And, and oftentimes, these really large models, they are changed in the public data, but
when you actually have private data, it can really potentially, could remember a lot
about sensitive information and that's, these are the kind of issues that we really need
to pay attention to because otherwise, these private data, they can contain both individual
users' sensitive information and also when you're trained to over business data also
can be a lot of proprietary information as well.
Yeah, I think the, the issue that I was trying to form a question around is, it seems to
me a lot easier to figure out how to get a model to not remember or, or not leak or
not even learn from Social Security numbers because those are, you know, very fixed in
nature.
They've got a fixed format.
It's easy to identify them in the training data, it's potentially easy to teach the
model not to, to remember them in some way.
But if you've got a language model and you're trying to maybe fine tune it on a, on a private
data set, there's potentially a ton of sensitive information, you know, say about the inner
workings of a business or, you know, past, you know, deals or contracts or things like
this that it, I'm envisioning a lot of scenarios where it's hard to separate the information
that you want the model to learn from and the information that you want the, the model
to not leak.
Right, that's a very good question.
So when I talked about the solution of linear, differential private language model, so
in this case, the solution is not just particular for, for example, credit current numbers or
Social Security numbers, it's in, right, it's general.
You don't pre-specify the type of sensitive information that you need to protect.
Differential privacy is a very general notion of privacy.
It's not specific to a particular type of sensitive information.
So the idea there is essentially, so when we say, for example, an algorithm is differential
private, it follows the following, a high level definition is essentially, if you consider
you have two, we call it neighboring data sets, where one data set has one more data points,
then the other, for example, one data point about them, that's not in the other data
sets.
And then we compute an algorithm over these two neighboring data sets.
The algorithm is randomized.
And in this case, we say that the algorithm is differential private.
If the computation results of this algorithm over these two neighboring data sets is very
close.
Essentially, the probability distribution of the computation results from this algorithm
over these two neighboring data sets is communistically very close, then what this says is essentially
from the computation results, the attacker wouldn't be able to tell what their sense data
point has been used in the computation or not.
And this is a way to essentially talk about showing that the computation result has not
really leaked much information about, for example, sense data.
So similarly, when you carry over to the machine model, it's similar.
So essentially, from the machine model, the attacker wouldn't be able to tell whether
a particular, for example, social security number has been used in the data sets and
hence, then in this case, you won't.
So in this way, the training machine model can help enhance the privacy protection for
the original training data set.
Okay, do you draw a distinction between techniques like differential privacy that are preventing
information being leaked and techniques that are preventing the network from memorizing
the information in the first place?
I see.
So in this particular case, what the differential privacy, that's, for example, on your training,
the differential privacy machine model, in this case, what you are doing is actually trying
to, it is actually reducing the memorization that the network is doing.
And in our work, we actually showed, as I mentioned, that with our measure of this exposure,
which measures the degree of memorization, we actually showed that when you train a differential
a private language model, in this case, actually, you are reducing this type of memorization.
In the case of differential privacy, typically in an application that involves differential
privacy, you're limited to doing computation or analysis on an aggregate level.
Does that mean that the kinds of applications you'll be able to support in a, this kind
of, in a responsible data platform or scenario or only these kinds of aggregate types of
computations?
That's a very good question.
So I think it can support different types of computations, for example, if usage data
is only used to, like, the computing results is only used for users' own consumption,
then you can essentially do arbitrary computation on users' data and then just review the computation
results to the user's self.
But when you want to compute over multiple usage data and then release the results, the
computation results, then in this case, oftentimes you are already computing some kind of aggregate
sense, or you're actually machine models of a different user's data and then in this
case, yes.
So you actually, you really do need techniques like differential privacy and so on to ensure
that.
So when you are computing over different users' data, then the computing results doesn't
leak sensitive information about individual users.
You mentioned homomorphic encryption.
When you talk a little bit about where that comes into play, I've not spent a lot of
time looking at it, but I understand that in that case, the set of operations that you
can apply that can retain this homomorphic property is somewhat limited.
Does that is that a big barrier?
So homomorphic encryption is one type of cryptographic techniques to enable your two essentially
to computing over encrypted data, and it's one type of solutions to the problem that
I mentioned, the secure computing.
So the goal of the secure computing is following.
We oftentimes talk about this simulation of ideal worlds with trusted third party.
So with the secure computing, the goal is that, let's say we have trusted third party
in this ideal world, and what you do is that you gave data to this trusted third party,
and also you gave an algorithm in this case to the trusted third party.
And the trusted third party will compute this algorithm function, let's say a function
f over your input x, then there's the computing result f of x.
So in this case, only f of x will be revealed, nothing else.
So this is what happens in the ideal world with this trusted third party.
So you trust that this trusted third party will not leak any sensitive information about
your data only, the computing, the computation result f of x is revealed.
But of course, then how to find this trusted third party?
In the past, essentially, essentially people rely on trust of a particular party based
on business contracts, and so on. But of course, that's suboptimal.
So even in the best case, the trusted third party tries to comply to the contract,
their own system may be compromised, and so on.
So essentially, the question is how we can have technical solutions,
ideally even with the provable guarantees to ensure this ideal world,
essentially, be able to simulate this trusted third party.
And in order to do that, essentially, the community has been developing
different types of solutions.
This is a homomorphic encryption, it's one type of solutions,
utilizing cryptography, where you essentially, you encrypt the inputs,
and then you compute over the inputs, and then you generate this encrypted
computing results, so that only the original user is able to decrypt.
And so then they only learn the computing results.
And another way, as I mentioned, is that you can use secure hardware,
where based on certain hardware and software combined solutions,
you simulate this black box environment.
We call it a secure execution environment,
and also called a secure enclave, where, again, you put data into a black box,
and also you put the function of the program into this black box.
And the hardware and software combined solution
ensures that when the program is executing inside this black box,
nothing else, nothing outside this black box,
including the operating system or other applications,
we'll be able to see what's running inside,
or we'll be able to modify what's running inside.
And hence this black box ensures the confidentiality and integrity
of the computation.
And the secure enclave also provides a capability
called remote at a station, so that a remote verifier is able to
remove the verifier.
The right computation has happens on a particular piece of data,
so essentially, it can verify the initial state of the secure enclave
and the program that will be run in the secure enclave.
So with this method, essentially, it's another way,
another practical way to simulate this trusted third party,
to ensure that when the program computes over the data,
nothing else gets leaked.
And there has been various commercial solutions,
including Intel Asjacks and AMD,
different hardware manufacturers have built
their different types of solutions for us.
But however, all these solutions are closed sourced,
and there has been some security issues discovered,
even though they have been patched and so on.
But being a closed source, it's difficult
for the community to really know what kind of security guarantees
that this type of solution can provide.
So as a research project, a Berkeley in my research group,
with other colleagues and so on, we have
been building what's called a keystone secure enclave.
It's an open source secure enclave, essentially
provides an open source version of this black box
that helps you to do this type of secure computing.
And it's built on top of risk 5 open source risk architecture.
And we have demonstrated that you can actually
build machine learning models inside the secure enclave
that you can do inference and other types of computation.
It can be really efficient.
And so on. So in the future, also, I think
there has been discussions, even like GPUs and CPUs
in the future, there can be this notion of a secure enclave.
So that the data is encrypted going into the chip
and only is decrypted inside the chip
and then you compute over the data essentially
in this black box manner.
When you talk about the risk architecture,
is this something that you're able to build
with off-the-shelf components?
Or does it require custom hardware to implement?
So right, so they came with risk 5.
There are chips already off-the-shelf to enable you
to do that.
And right, I was just actually going to sound that.
OK, cool, cool.
You also are active in exploring different adversarial
attacks on machine learning and that whole space
of adversarial machine learning in general.
Can you talk a little bit about some of your work in that area?
Right, yes.
Well, as we tried to deploy machine learning
AI systems in the real world, one issue we've already
discussed is in the privacy and responsible data
used to ensure that the model actually does
a leak-sensely-faint information about users
and also is used in a way that's for users best interests.
And then another important issue is
to ensure that these models are actually not easily
hacked by attackers.
So that relates to the problem of adversarial machine learning
where in adversarial machine learning,
so one example we have said is essentially
looking at how the attackers can actually
feed, for example, modified inputs,
like with adversarial perturbations that
can fool the machine learning system
to give the wrong answers, for example, the wrong predictions.
And one example that we have stated
is in the self-driving car setting,
we're looking at also, well, there's some of these adversarial
example attacks can even happen in the real world.
It's like putting a picker on a stop sign and...
Right, right, right, exactly.
Right, demonstrating that this type of attacks
can even happen in the real world where the attackers
are putting just stickers on stop signs.
It can, for humans, we can still recognize
these stop signs with no problems.
But for the image classification systems,
or these computer vision systems,
they can be very easily thought to give wrong answers
and also to give the target answers that the attacker wants.
And the important part is that this type of attack
can even remain effective as, even from different viewing
distances, different viewing angles, and so on.
Right, so with my collaborators,
we have demonstrated that this is feasible,
and we develop these real world stop signs,
archefic signs, and so on.
And some of these artifacts actually have been
exhibits at the Science Museum in London.
So it's actually quite fun.
Oh, well, to what degree are you seeing or hearing about
kind of real world examples of these types of adversarial attacks?
Are we there yet?
Is it something that people are practically faced with
and worried about today?
That's a very good question.
I think, definitely, we have seen
the attackers try to fool machine learning systems.
So in particular, for example, there are a lot of these clouds
APIs for different machine learning services.
For example, this cloud APIs may try to identify
whether a certain content is deemed safe.
For example, and it's actually very easy for attackers
to write through these type of attacks
to try to fool this type of cloud APIs.
In our own work, we have demonstrated this as well.
And we call this actually Black Box attacks.
So in this case, we don't even need to know the details
of the machine learning model of the cloud API,
including the architecture or the parameters
of these machine learning models, but through Black Box attacks.
And they're from what's called transferability attacks
where we can build a local model and then
try to generate these adversarial examples by just attacking
the local model.
And then due to this transferability phenomena,
they generated the adversary examples
from the local model actually has high likelihood
to actually be successful against the remote victim model
as well.
And we demonstrated that this type of attacks
can be effective for this cloud APIs.
On the work that you were just mentioning
is the ability to generate these effective local models
incorporating some of the parameter and architecture
leakage that we've talked about previously?
So in this case, we assume that we actually
don't really know anything about the remote model,
like what actually, what parameters
it actually uses.
So we just built a separate local model.
And because of this transferability,
the attacks that we found, this local model
has high likelihood to actually succeed on the remote model.
And also, so in the past, we've done work
and also the community has done work studying
in the computer vision fields for this type of attack.
And recently, we've also studied in the natural language
space.
So in this particular case, for machine translation.
So we actually looked at, for example, Google Translate
and a few other these cloud APIs for machine translation.
So first, we showed that by just creating
this cloud APIs for machine translation,
you can call it an imitation attack or model steaming attack.
We can actually build a local model that
has very high essentially performance
that close to this cloud APIs when you evaluate it
over standard benchmark.
So by creating these cloud APIs, we
are able to build these imitation models locally.
And then by developing attacks, these adversarial attacks
on this local imitation model, we
are able to generate these adversarial examples.
As a simple example, we show that if we try to translate,
for example, from English to German,
the English sentence says, as I said,
today is the temperature is very high.
In this case, the language model generates the correct translation.
But we are able to show that by finding essentially,
we are able to find the attack.
In this case, if we just change six Fahrenheit
to seven Fahrenheit, just changing the number.
And otherwise, it's the same sentence.
And then one way to give the sentence to the language model
actually translates into the temperature
is 21 Celsius, for example.
So it gives the wrong translation.
And in this case, we only change the one
character in the original sentence
and show that the result in translation is very different.
And this is just one of many examples
which shows how using different essentially
last functions using different methods,
we can generate different types of attacks
to try to fool the translation model.
And also, when we transfer this attack
to the remote model, for example, to Google Translate
and to other types of APIs, we show
that these attacks can still work.
So these are other examples demonstrating
that when we deploy machine learning systems
in the real world, it's really important
to think about these types of issues as well.
And we call them the integrity attacks,
essentially trying to see how attackers may
fool the machine learning systems to try
to have the machine learning systems to give the wrong answers.
And as we know in the future, more and more critical decisions
will be made by these learning systems
autonomously, almost potentially every minute,
we really need to be very, very careful
about ensuring that these machine learning systems
cannot be easily attacked, they do give the rights,
and so they have high security assurance and so on.
You also have done some interesting work
on the topic of program synthesis,
so trying to use machine learning models
to generate computer programs.
Can you talk a little bit about your work in that area?
Yes, I think the reason I'm working in program synthesis
is because I think program synthesis
is the ideal playgrounds for trying
to build what we call agatic artificial general intelligence.
I think it's really the ultimate task.
Talking about building intelligent machines,
ideally you want this intelligent machine
to actually be able to do the program,
and hence it's actually able to, in the future,
maybe even build itself and so on
to help building programs to solve many real world problems.
Also, I joke with my colleagues in robotics
is that program synthesis is like doing robotics,
but without the physics constraints of nature
without having to obey laws of physics.
So in program synthesis, essentially, you need to solve
many of the similar problems.
You need to understand goals.
You need to be able to decompose problems.
Into some problems, you need to be able
to do very effective search.
The search space of programs is huge.
As we all know, you know,
comparing to playing Go, playing Starcraft
and so on, the search space of programs
is even larger, even for small, simple programs and so on.
So you need to do planning and also,
you need to better understand semantics
to know what you want the programs to do and so on.
So essentially, all the problems that you need to solve
in robotics and also just in general building
education machines, they all fight in program synthesis.
You need to really solve program synthesis.
You really need to solve all these problems.
So it's a really exciting domain,
but also at the same time, it's a domain
that you can experiment much more easily.
You don't need to build robots, you don't, right?
And what you need to do, like what we did,
is oftentimes you just build these synthetic,
you can build synthetic environments.
You can easily also build this program, you know,
analytics or even just execute the program
to know exactly what it does.
And so also from in terms of evaluation,
the experiments, it's much, much easier.
So in the program synthesis space,
in particular program synthesis by Lenny,
it's still a nascent field.
I remember a few years ago,
when we started working in it,
there were very few people actually working in a space.
And then when you look at new ribs,
I clear, I say now, these conferences,
there were very few papers actually
doing program synthesis by Lenny.
But the way you look at the most recent,
for example, new ribs conference and so on.
And I clear conference, you actually see,
now there are specific sessions,
even dedicated to a program synthesis by Lenny.
I think it is a great progress for the community.
But still, we are at a very early age.
The program that in general, the community can synthesize
is still very small.
And in general, typically we focus on the program synthesis
for certain vertical domains,
is there to make progress that way.
And I think also program synthesis can have,
already can have a huge impact in the real world.
So for example, some of the tasks that we have done,
and translating natural language
into simple programs.
And so this includes translating natural language
into if this and that's type of programs.
For example, if it rains tomorrow,
so any text message,
or translating natural language description
into SQL queries.
So this can enable more people who cannot code
to be able to utilize data.
So for example, with a huge database,
people who cannot code,
who cannot write SQL queries,
they may have a lot of questions
that they want to get answers from these large database.
And when we enable this natural language description,
get translated into SQL queries directly,
we can really enable more people to benefit from data,
you know, talking about responsible data,
responsible data economy, and so on.
So I think it's, right,
it's a really exciting domain.
And deep learning has really been hugely helpful in the space.
It's, well, we first started working in the space.
Who was your first paper in program synthesis?
So our first paper in program synthesis
by learning is actually using deep learning
to enable natural language description
to be translated in this,
this then that's called ifTT programs.
Okay.
And at the time, also there has been some other approaches
using more traditional natural language approaches,
like semantic pricing, and so on,
to try to address a problem.
And we were the first one to actually demonstrate
that using deep learning,
you can actually get much better results.
And we were able to get state-of-the-art results
using deep learning, and so on.
And then since then, we have explores, as I mentioned,
translating natural language description into SQL queries
and also building even like language translated,
translating programs written in one program in language
into another, and many other application domains and so on.
And also trying to develop a method that's essentially
make it the learning process more sophisticated
to be able to leverage more about execution semantics
and also better learning from its past mistakes.
So many interesting techniques in the space.
I think the field overall has really flourished
over the last few years.
I imagine you've seen this demo, again,
referencing back to GPT-3.
There's one demo that's kind of making the rounds
of someone who built a web layout generator
that spits out JSX code, and you can tell it,
make me a web page with a red button, a green button,
and a blue button, and it creates the code to do that.
Have you seen that one?
I mean, now the particular, but I see similar applications.
And in the past, actually, we also tried to do
some of this type of applications as well.
I think, yes, you can do that.
Gigantic language models will play a huge role
in the field of program synthesis,
so do you think it's going to be maybe more constrained
techniques that allow us to make big progress there?
That's a very good question.
I think in general for program synthesis to really work,
we need different components, different types of techniques
and so on.
So certainly, for this type of language models,
we know certain network architectures, like a transformer
and so on, they just have been so powerful
they can solve so many different types of problems
in so many different types of applications and so on.
So I think it's different types of network architectures
and certain components are definitely very, very helpful.
But on the other hand, I think with program synthesis,
there's also another reason that I really like the domain
is that for solving problems in this area,
you also really need to understand
semantics in a much deeper level to write,
to know what you want the program to do
and hence what type of programs you should generate.
I don't think just having a transformer itself
can help us solve program synthesis,
but definitely, I think it's an important component.
And that's also why it's a really exciting field
we need to, for the explore and develop
other types of techniques and approaches.
As I mentioned, trying to learn more about the semantics
and also trying to learn from past mistakes,
also trying to understand if the program
that you've generated is now the right one,
you try to identify the type of issues
that it's why it's not working
and try to learn how to fix it,
very much like a how humans program,
how we can crack sense, right, like the programs and so on.
So we hope that the tools for program synthesis
can leverage many of these types of capabilities.
You've also done a bit of work on contact tracing
as it applies to coronavirus and COVID-19.
Can you share a little bit about what you've done in that area?
That's another concrete example
of what we call responsible data use.
So in particular,
you focused on the privacy side there?
Right, right, right, exactly.
As we know, with this pandemic,
it's changed the world.
Nobody has, I think, really, really expected
that it can change the world so fast
and as such a global scale and so on.
And hence, of course, finding solutions
to fight the pandemic is really important.
And of course, data is the key driver
in fighting the pandemic.
Then the question is, as we use data
to fight the pandemic,
how we also at the same time need to ensure
that the data is being used in a privacy-preserving way
and used in a responsible way.
Because otherwise, you can really have huge compromise
of individuals privacy,
which is really sensitive for individuals and so on.
And also, if users are concerned
about their privacy,
this will also limit their participation
in whatever apps that you try to have users to use
or in whatever systems that you are trying to deploy.
And hence, as a concrete example and application
of this approach of building a platform
for a responsible data economy,
we are investigating how we can utilize data
in a more responsible way.
And at the same time to enable, for example,
fasts and effective contact tracing.
So we have been exploring different approaches
and developing different techniques,
including improvements on cryptographic protocols
for more privacy-preserving contact tracing.
That's extension to the Google Apple Exposure Notification
Protocol to provide more privacy protection
as well as enabling what we call a secure distributed
computing fabric to enable this type of contact tracing
work to be done over different data sources,
but not in a centralized manner,
in a more distributed and decentralized manner.
And also, I wanted to mention that, of course,
this is really important and time-tapping.
We are actually launching summits called
responsible data summits.
And we dedicate one day just on responsible data
in the time of pandemic to discuss the various issues
about how we can use data in a more responsible way
while providing effective solutions for fighting pandemic.
And also another day talking about the cutting edge technologies
and also the latest thinking in legal frameworks
for responsible data technology and policy in the real world.
And I think that that can provide more exciting details
for people who are interested.
And responsible data.AI.
Right, great.
And will the, I believe this will be published
after that conference,
but will the videos be available for folks to check out?
Yes, the videos will be online.
And we have a great line of speakers,
including Yushua Benjou, actually we'll talk about how
he uses data to fight pandemic in contact tracing
and in the privacy preserving a responsible way as well,
as well as many other great speakers.
Awesome, awesome.
Well done, thanks so much for taking the time
to share with us a bit about what you're up to.
Great, thanks a lot for having me.
Thank you.
Thanks, thanks a lot.
All right, everyone, that's our show for today.
To learn more about today's guest
or the topics mentioned in this interview,
visit twimmelai.com.
Of course, if you like what you hear on the podcast,
please subscribe, rate, and review the show
on your favorite pod catcher.
Thanks so much for listening and catch you next time.
