Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.
Hey, what's up everyone. Thanks for joining us for today's Twimal AI Podcast
viewing party in Q&A with Lucas B. Wald, founder of Weights and Biases. Of course, I'm your host,
Sam Charrington. While this is a pre-recorded conversation, Lucas and I are waiting for you
in the live chat. There? Or maybe there? And we encourage you to reach out with your questions
while we're all here together. Weights and Biases provides a great lightweight toolkit for
machine learning practitioners and we thank them for sponsoring today's show. We've used it for
quite a while in the Twimal community as part of our various study groups and have found it
super useful for experiment tracking and collaboration. Now that they're offering dataset
versioning and model management capabilities as well, you've got a one-stop shop for managing
and visualizing your complete machine learning pipeline. If you'd like to learn more, Weights and
Biases is extending a special offer to Twimal listeners and viewers, including unlimited private
projects and priority support. For more details, visit wnb.com slash Twimal. It's w-a-n-d-b.com slash
Twimal. And now on to the party. All right, everyone. I am here with Lucas B. Wald. Lucas is the CEO
and co-founder of Weights and Biases. If Lucas is named sound familiar, that's because we
spoke last on the podcast in August of last year, 2019, episode 295 on managing deep learning
experiments. Lucas, welcome back to the Twimal AI podcast. Thanks, Sam. It's great to be back.
Lucas, if folks want to catch your background, they can check out that episode and we'll link to it
in the show notes. But what have you been up to since last August? I know one big thing has
changed for you. Well, I had a baby. Definitely has changed my life in priorities. Maybe more
work relevant, we've been working on improving the product. And I think one of the reasons I
wanted to talk to you come back on your show is we just recently put out a new product called
Weights and Biases artifacts that we're super excited about. Yep, and that's what we'll
spend our time talking about. Tell us a little bit about the problem that you're trying to solve
with artifacts. Totally, yeah. I mean, you always have such good, what did you call it? You have
such good diagrams, sort of like laying out all the different pieces in the industry. I kind of,
I'd love to know how it exactly fits into your diagram because it's always, it's always insightful
to see that. I think you're referring to the the ebook definitive guide to machine learning
platforms. And I broke out kind of at least my view of the ML tooling and platform landscaping into
data acquisition and management, experimentation, and model development, and then model deployment
and management. And so I would think is artifacts in the the middle of those two or the last of
those two or it is a straddle? Well, I think it might kind of straddle. So let me sort of tell you
the problem that we were trying to solve and you tell me how we should, how we should.
So basically what happened was our experiment tracking platform, I mean, we feel super proud of
it. It's been really popular. It's had faster growth than anything that I've been a part of.
And so we're constantly asking our users, what else do you want? What does it feel like it's missing?
And the biggest request was, from what users, we'll look like this is tracking all the experiments
I do and I can compare all the models I build. But in reality, there's kind of other things that are
really important for me to track. Like I want to track my data sets and I want to check the models.
And I also kind of want to, in some cases, like connect steps together in kind of like a pipeline.
And so, you know, we build artifacts as like in an adjacent product that's that's separate because
we think that, you know, we want to keep our experiment tracking a really tight point solution that's
really good. But we also want to make it really easy to kind of track, you know, more things that
that you might care about. And you know, the main things, like I said, are probably data sets,
models and pipelines. It's important to note, I think some people kind of compare us with like,
like looked at my overview and they say, well, how is it different than something like airflow
that like manages pipelines? And I think it's important to say really different, right? It just
tracks it. So what it would do is like, you know, say you, you know, you have some
set of data and you do some transform on it and then maybe you train a model and then maybe you
like, you know, test that model on a couple different data sets and then maybe you do some quantization
and then you deploy it. What W and B artifacts would do is it would basically save all those
steps and save the fact that they're all connected together. So, you know, sometimes a lot like
management is the difference that you're not the state machine or the graph, you're not managing
that, you're not standing that up, they need an airflow or Luigi or something else that is
actually tracking the state of all the objects in this pipeline and you are kind of doing what you,
something similar to what you did with the experiment management product where you're,
you know, capturing metrics of things as they move through this process.
Yeah, and saving it, right? So, you know, if you want to save something like your data set
or your model, we make it really easy for you to save it and version it. But then what a lot of
our users asked for is, hey, we just want to track it. We don't want to necessarily like,
you know, upload these gigantic files to your servers like we have them in a bucket somewhere,
we have them on prem. And so, you know, we let you, you know, save a pointer to those things
if you want to. So, you know, I think, I think model management and data set management is maybe
the best way of thinking about it. I mean, I think what I know for sure is that, you know,
everybody's asking us for this, so it must be, if it's not a category now, I think it's going to
become a category. And there's certainly lots of tools that, you know, there's lots of tools
that do pieces of this. And in some cases, like, you know, have, you know, are more ambitious
than this. And it's really important to us that we work nicely with all these things. I mean,
you have such a great dichotomy of sort of like, you know, sort of n10 platforms and, you know,
point solutions. And, you know, one of our real core values at which advice is to be, you know,
a set of interoperable point solutions that do kind of one thing really well and play nicely
with all the things around it. So, let me try to put that together. The kind of core problem
that you're trying to solve here is you come at it from the perspective of, again,
experiment management where you had folks using your tools to track different experiments that
they were running as part of their model development process. But you found that that
component was used as part of a pipeline where they were getting data from somewhere. They would
run it through multiple, some series of transformations. And then ultimately run an experiment.
And folks wanted to track more of that process so that, yeah, I often talk to folks that are
trying to do kind of providence solve this data providence problem or decision providence even
where you've got a model that makes an inference and you want to kind of go all the way back from
that inference, the decision to, you know, the model that was deployed, you know, the experiment
that said that that model was the best model, the data that, you know, started in a training set
that, you know, allowed you to train up that model that won. And, you know, the data points
to influence, you know, ultimately the data points that influenced this decision. And it sounds
like you're trying to solve more of that problem than you were doing before. Yeah, and I think like,
you know, there's lots of really like interesting, you know, pain points here that people talk
about all the time, like, you know, model explainability and model reproducibility. And those are
obviously there, they're like huge problems. But I think like the sort of core thing that
we see a lot of our users struggling with is just literally knowing, you know, what data set
the model got trained on and what data said actually or what model actually got deployed into
production. So, so that's like the core focus we have. Like, you know, we work with, you know,
work with a couple of companies that do, you know, kind of retail and are trying to build systems
that, you know, automatically, you know, can like detect like, you know, what you're buying as you
walk out like the Amazon store. And, and, you know, one of the things that those companies
all have in common is they're like constantly getting new label data, right? Because they have,
you know, cashiers in their store that are labeling the data live. And they often have,
they always have new products coming in, right? And so, um, what happens with them is that
they really like never train the models on the same, um, data set, right? So it's actually like
every single time it trains is sort of like a different basket of stuff that the models get, um,
trained on. So not just incremental growth of their training data set, but they're, it's just
different. It's just it, well, yeah, actually in that case, it's incremental growth. I'll say,
okay, we have actually have the opposite thing where it's not incremental growth, it's actually
shrinking. So, you know, some of our customers get, um, kind of privacy takedowns, right? Where,
you know, people will say, um, you actually one, one company we're going to talk to,
this is iRobot, right? Where, you know, people say, hey, take my data set out of, you know,
iRobot data set, of course, they do that, right? But then what happens is now your,
your data set has, um, you know, slightly changed, right? So it's not exactly, um,
um, you know, apples to apples, um, maybe, right? Maybe it's okay, but, um, but, you know, like,
I think that the overhead, it sounds simple, maybe to, to track all that, um, but the important
thing is that you really, really have to do it, right? Like, you have to have a system where
you're always tracking it the same way so that when you do this comparison, it's really easy to say,
um, you know, which is which. I'll say, like another issue, um, is like with our, you know,
a ton of vehicle companies, they don't, they have actually so much data, typically,
um, that they, they never trend in all their data, right? So they're, they're actually,
every time they build a model, they're like selecting pieces of it, um, and they're often selecting
different pieces, like, for different purposes. So I guess like, you know, in some cases of
data sets growing, in some cases it's shrinking, in some cases you're like picking and choosing
from different, um, data sets, but in all these cases, um, you know, we think the really like
core need here, or the, the, the pain that we want to solve is just like, we will keep track of what
data sets your, your model was trained on. And then there's another, like, interesting thing,
which is that, um, you know, data sets have these like pipelines where they might get modified,
and those pipelines can change, right? So, um, you know, with our medical, uh, users,
they often have really small data sets, like, you know, some of our, you know, some of our medical
customers, like, they might say, we have this, we have big data, we have like 1000, you know,
but it's like 3000, like, you know, like, um, chest x-rays, and somebody with like a horrible,
you know, it's like a kind of, you know, must have been like a huge feat, um, you know, to,
to get that data. Um, and so like, what they will do, like, a lot of, um, pre-processing, right?
Because every record is so precious, right? So the, the pre-processing steps almost become,
um, maybe more important than the, the sort of like, model architecture itself, right?
So, you know, it's really important for them to, to track that. And we started to see,
we started to see our users actually kind of, as an entrepreneur, this is like a real sign that,
you know, you know, from the fixer. So we still, I use this sort of like, you know, kind of clinging
together, um, you know, they're, they're sort of like, chains of runs, uh, or, you know,
we call them runs, like, experiments in our, in our tool. Um, and so now we've kind of made that,
with, with the artifact product, we made that like a first-class citizen, where you can say,
you know, okay, I have these asynchronous, you know, steps that I do, um, and it might be some,
you know, complicated pre-processing and then a training. And then sometimes even, um, you know,
like, uh, autonomous vehicles are places that come to mind. You know, the model that you train might
not really be the model you deploy, right? Because you might have to do like a whole bunch of, um,
you know, kind of shrinking of your model to like, to get it to actually run. Um, or so you might
have like tests that you do on the model that got trained, but then you also have, you know, more tests
that you do on the model sort of just before it's deployed. And these might even be like different
teams doing it. So, um, you know, connecting all these different steps, um, in a sane way actually
becomes like a, you know, I think a bigger, um, a bigger problem than you might think if you're,
you know, just like start now, you know, training like M-ness to, you know, a couple of times. Uh-huh. And
so the, I'm curious to hear about the things that they were doing before to clooge it together.
What did that look like? So I only used clooge with, with my own product, because that's the only
place that I, I feel like I can actually say for sure it's clooching, right? So, you know,
you know, you know, you know, you know, you know, you know, you know, you know, you could think about
like, you could just sort of like name things. I was envisioning like, you know, what the way
you would see folks managing experiments by putting hyperparameters and filenames. Like, is it?
Yeah. Well, really common hyperparameter in ways and biases is like, you know, the, the
filename of the, um, the training data, right? And so, um, you know, it's like, that's a great thing
to do. For us, as people want to train there, people want to keep track of their training data
and giving them a way to do it. Exactly. And we're like telling people, hey,
use weights and biases because we don't think it's a good idea to use your, um, file system is like
an implicit record of what you did, right? But then, you know, we're like kind of causing people to
do that in some, um, in some cases, right? So that, that, you know, so yeah, so, so, you know,
you can use the file, you can use like a, a Shah, you know, the file if you want to make sure that it,
it doesn't change. And then there's like a whole bunch of, um, there was a whole bunch of like
interesting tools out there that are like, kind of always evolving and improving, right? There's like,
you know, DVC and pack-a-derm and quilt that do, um, you know, really interesting takes on
sort of data set, um, versioning. And we've done, you know, light integrations with these tools
to kind of make them work, um, with what we have. And then, you know, end-to-end platforms, um,
you know, if you really buy into there, pause there for a second. Yeah. We're doing the, so you
saw folks doing the cluages indicating that they wanted to better track training data. Um, and
it sounds like also the light integrations and, and kind of joint customer work you were doing
with the DVC's and pack-a-derms of the light and these other end-to-end platforms kind of indicated
that, um, I was actually going to ask about that. The, the DVC's and the pack-a-derms of the world are
trying to, uh, that's kind of their, that's their world, right? They're trying to help customers
better track the evolution of training data through transformation processes and, and up to,
up to, and in some cases, including training. All right. And so where do, where does what they're
doing end and what you're doing start? Or are they kind of overlapping like our customers having
to make decisions about, um, you know, which part of which product they want to use for,
which part of the process? Like, it seems like there's some overlap there that I'm trying to thank.
Oh yeah. So there's clearly some overlap and I feel like, um, there's probably, you know,
like we're recording this one April, April 2020. I haven't feeling, you know, like June 2020,
the overlap, you know, it could be more or less a different, um, you know, so like, I think that,
you know, I think like one, you know, one thing, I feel super committed to making our product work
with, um, kind of all the best in class tools. It's like really, really important to me. So,
you know, I think that if, um, you know, if, if DVC or pack-a-derm, you know, solved all the
problems that the customers were having around this, you know, we would just, you know, let them and focus on
experiment tracking. I think that, you know, I think the issue, um, I think that the sort of thing
that we keep finding is that, you know, it kind of reminds me of actually, um, hyper parameter search,
which we also didn't expect to roll out a hyper parameter, um, search tool, but we kept, you know,
we kept talking to customers and like, ah, like, I know what should be doing this, but, you know,
it's like, like, I can't quite figure out how to do it. You know, and so, you know, kind of articles,
like, let's make hyper parameter search just so, like, simple. I mean, that's this really easy,
not like a toy, but just like, make actually the powerful, like, make it easy to make it work in
a distributed fashion on lots of data and really reliable. That's sort of like our, um, you know,
point of view is like, let's just like really make it, um, you know, like, like a solid product of
the easy onboarding. And so, kind of felt the same way with this data set versioning stuff, where we
would, you know, we were pointing people to, um, these different products. And I think that they are,
I think they're a pretty steep learning curve, um, honestly. I mean, it's not like a knock on the,
these are really like, you know, it's kind of complicated stuff and it's hard to get it, um,
you know, it's hard to make it like really easy to, to use, but I think like, that's where I think,
like, because we're like totally focused on, um, making ML researchers and practitioners happy,
I think Winston-Biasis has a much more narrow, um, you know, scope here. Like we're, we're, you know,
I think like, you know, DBC is like native virtual, right? So it's like, there's actually a lot of
different situations you might get into and like, you know, they, they use like, um, I mean, they use
Git and I, you know, I love, do I love Git? I don't know, I kind of love Git. I make a little,
we all have the idea of Git until we need to do something monkey. I imagine like 2021 Lucas,
just totally understands Git and, you know, it's like, but like, 2020 Lucas kind of types and commands,
kind of like feels a little afraid, you know, and that's how, you know, I hope I'm not back
to just doing my tools, I can tool for like idiots. I mean, it's a tool for, you know, ML researchers
who want to do ML research, right? Not like, um, and we know that like, some of them are really deep
on Git, but we don't want to necessarily, we want to hide as much complexity as possible and solve
the core pain point, which I think really here is. I think what you're getting at there is that the
DVC and, and Packaderm as examples are, uh, and some of the, and the platforms there are actually
versioning the data and snapshotting the data as it, uh, transitions through a transformation process
and they can, you know, go to any point in time and any place in a process and allow you to see
the data as it was transformed and that's not quite what you're trying to do. Well, I just say,
we do version, we do version actual data itself. Well, in a really simple way, we make copies of it
and we save them. So, you know, like, like, you know, that's why again, like, you know, it's like,
we keep pointers to, to different places your data was at and we let you like, split it into pieces
if, if, if you think those are going to change, but I think we are not as concerned with making
like really kind of sophisticated dipping. Yeah, the sort of thinking being that, um, you know,
space is pretty cheap and the really important thing is that people actually, um, you know, save
this stuff. And then we just, that's like our first. And if, if, you know, of course, if customers
like really, really want like sophisticated dipping and that's like, you know, necessary,
you know, we'll find a way to, to do it for them. But, you know, the product doesn't exist
today. The idea is just like, you know, you tag every state of your data, um, and, and you can
keep uploading it to us if you want to. Um, and of course, if you, if you use the same exact data
in two different places, we're not going to make two, um, you know, copies of it. But I think,
you know, we're not like using Git in the back end, you know, for example. Got it. Got it. So,
you referenced earlier this, um, it's almost a, uh, I think I gave more energy to it, uh,
in the book, the definitive guide to ML platforms. It wasn't quite a throwaway comment or a footnote.
I, I spent some time with it in the last chapter of the book, but it was this idea, um, that I
proposed of the wide versus deep paradox for machine learning, you know, platform and tool vendors.
And this conversation is such a great example of it. Uh, the, the, the general argument for folks
that haven't, um, taken a look at the book is that you've got these end and platforms that are
trying to kind of own the entire machine learning process at an enterprise. Um, and,
you know, if you start from, from that perspective, you know, you can end up with a simplistic
solution that really is just a workflow engine that's very shallow, you know, in functionality
at any particular point. And you've got specialist providers, um, who, you know, any point in that,
in the end process, have gone very deep, for example, waste and biases went very deep on
experiment management as their initial play. And, and the paradox, the paradoxical element of
this was that the, in the end providers, you know, as we have seen, we've got a lot of end-to-end
solutions and, you know, without going deep in any particular area, that tends to get commoditized.
Uh, and the, uh, and so they're getting kind of pushed to specialize or differentiate in, in a
slice or, you know, particular functionality. Uh, and then with the narrow, uh, providers,
the specialists, you know, they're going in, they're finding that, you know, customers have all
these other gaps that they need to fill in order to get value out of the core product. And so they're
being pushed wider. So you've got these two kind of sides kind of, uh, converging in the middle,
perhaps, uh, or in different directions. And this is such a great example of that. Yeah.
Like you started in the middle, you've got the, the data side. I think you, you mentioned, uh,
extending into the hyper parameter optimization piece. Uh, um, are you working on model deployment
management at all? Uh, we, we, we play with the stuff we, we dabble, we dabble in the sense that
I think like what, what I want, I mean, I, I feel like what I want weights and biases to be
note, like what I want our, our take to be is like, you know, we're not, we don't have like a, um,
we don't know like a, like a, we want to make good tools for people doing, um, machine learning
that they're actually going to use and, and really want, right? So that's sort of like our, our,
our process is like, you know, we spend a ton of time with our customers and we ask them,
you know, what do you need? Whereas I feel like, you know, um, I, I feel like there's a surprising
number of companies that it sort of seems like they have sort of like these grand plans that sort of
make sense in a PowerPoint, um, but then don't really fit what people want. So anytime they're sort
of like, you know, business strategy question, and it's like at all in conflict with what, um,
customers are asking for, like, I'm going to do what people are actually asking for, like, you know,
10 times out of 10. And so that's why I think like, you know, we thought when we started, like,
this is just a point solution, we're just going to do experiment tracking, but then people are like,
you know, what I really want you to do is actually, you know, make a hyper parameter,
research thing for me. So we, you know, we did, and then, and then we were like, okay, what are
they really asking for? Um, you know, they're absolutely like, you know, everyone is focused on
just like, I want to be able to keep track of my data sets and models. I think, um, you know,
some people do ask us about, you know, production monitoring or like, you know, CI, CD for, um,
you know, for machine learning. I think this stuff is really interesting and cool. Um, and I don't
think those problems have been been solved yet. So I'm like intrigued. Um, I'm intrigued by
these directions, but I want to make sure that we do, um, well, and I also want to make sure that we are,
like everything we do, you can, um, pick and choose it because I, I guess like, I really don't think
in the end that, um, these kind of all-in platforms are going to be, um, long-term successful. Like,
I think that, um, you know, right now people are confused. I think a lot of VPs are just like,
look, just come in and solve all my ML problems that wants to that, like, feels, um, really good,
but I think the practitioners don't like them because they don't feel like they're learning
transferable skills. And so I think it's, it's actually hard to track the best talent if you
are using, um, you know, the something that doesn't like work with other tools. And I think at the
end, like, well, companies actually really care about and should care about is getting the best talent.
And so I think you have to make, um, tools that make people feel like they have like,
you know, or learning a transferable skill, um, to other companies. And I will say there's a
natural, like, difference in go-to-market too, right? Where, um, you know, if I was selling an ML
platform, you know, I would go into CIOs and I would go to, um, you know, VPs of
machine learning. What are they called? These days, I feel like you have like chief AI officers and
things like that. Um, and it's funny, you know, I, I guess I like, you know, these days, you know,
is anybody, is that chow? Is anybody coined that? That's awesome. If they didn't, it doesn't, I
think the eyes for John Gendres. And he has, I don't know, I, um, I think, definitely, I'm telling
you, somebody out there is that. And then like, I, you know, Tony is like, my network is getting
more and more senior. And I'm getting, um, you know, people are like introducing me to people,
just like, I don't know, I don't know. You know, like, I just want to like talk to the people,
you know, actually making stuff. And, um, you know, get them to want to use it first,
because that's like a bread, butter for, um, you know, the hardest part about, um, ML tools right
now is, is like actually getting people to, um, use the tools, because there's so many tools out
there. That was part of the end, and argument that I was making as well as the end, and,
platform providers, like in order for them to be successful, they'd have to get
everyone to be, you know, they don't have to get folks to be willing to throw away any of the
point investments they made, whether they were internally developed or, you know, they brought
someone else in, whereas, uh, the, you know, specialists would need to fill these, that's,
that was the other side of specialists filling the, the gaps. Um, yeah. It's a interesting.
I wonder how you feel about this. Like I, so this probably happens a few old times, but like every
week, my investors are like, what do you think about this, you know, ML company? And like, I,
I already can prick like what the website is going to say. It's going to like list all the
pain points in ML, right? And like, I know, you know, repressibility, um, explainability, um,
you know, like maintainability, you know, it's like going to be like, we do ML ops, and it's going
to like, and I'm like, I can't evaluate this, because I have no idea what this thing does.
In the same book that you're talking about, I put, uh, uh, I included kind of a way to figure out
what, uh, what a platform is really good at. And to me, I'm sorry, I haven't, I didn't come
across this. I, I really want to know. It's in that same section that you refer to. It's,
it's basically where did they come from? Uh, yeah, makes right. And so, you know, if you,
if you, you know, if they came out of, uh, you know, data storage and snapshotting and now they've
got a, and, and machine learning platform, they're probably going to be focused on the data side
of things. Okay, but it's not in our case. What's that? You know, we're really good at tracking
artifacts. Don't, let's do everyone else. But, you know, so that was one of several criterias,
but I agree. And that's a lot of the problem with, uh, I shouldn't say problem. That is a challenge
with, um, that I see folks faced with when they're looking at these and the end platforms is that
they all talk about solving the same problems, you know, and workflow, uh, increasing innovation
and cycles, you know, decreasing cycle times, increasing, you know, innovation and a number of
experiments and, um, without going deep in, you know, particular areas, it's really difficult
for them to differentiate. I mean, I'll even give you an example. I mean, you probably can't say
the stuff, but, but I can, you know, like, when I, you know, I remember when SageMaker came out,
and it was basically like a way to have like a nice environment to train your models. And,
and my friends told me it was the fastest growing, um, AWS product that they had seen, right? It
was like, it was awesome. It like really solved this, this pain point that everyone had,
everyone like, you know, really wanted to use it. And, and now I'm like baffled, like what,
like what SageMaker does like SageMaker, where begins and ends like, you know, honestly, you could,
it might be useful for you to write it. You know, guys, like we can like figure this out, but
I feel like if I'm confused, the market must be just like utterly baffled because I'm out there,
like every day talking to people, like using these tools, like thinking about it. And I,
I actually could not tell you, um, you know, where SageMaker begins and ends anymore. But I,
I would say to someone, if you want to quickly stand up an environment to run ML models,
I bet you SageMaker is a consolation still. You know, I think, I don't think anyone who follows
AWS is surprised at how it's evolved because, you know, if you've used AWS at all, you go to their
services page and there's like a thousand different modules, right? And, um, you know, they solve,
you know, they're also a very customer-focused, but their strategy or end, their strategy for doing
that is to, uh, come up with a bunch of point solutions and allow folks to kind of string them
together. And they've kind of done that with SageMaker, uh, an ML in general. Um, and to some extent,
or another, for better or for worse, have taken a little bit of the Watson approach where the
brand gets applied to everything, your many things. And it's like, yeah, it's the one thing if Amazon,
if you're listening to this podcast, dude, really, it would make my life easier if you just name them
like different, different things. That's one.
So going back to actually speaking of naming, so artifacts is called artifacts.
Uh, right. We think, what are the artifacts? You know, so the artifact is, so we think of it as a
data set or model management, um, products. And we call it artifacts. And the artifacts here
is essentially a data set or a model. I mean, it really could be like anything that you can think
of as a file. Um, and we kind of thought we had thought up the name artifacts by ourselves,
but then we looked at, you know, Google pipelines actually calls the same thing that we talk about
as artifacts artifacts. And MLflow actually also talks about these things as artifacts. So it does
seem like everyone sort of independently, or we thought we invented it, but, you know, we were
definitely not the first, we have maybe the first that's calling the product that, the first
maybe to call the product that, but certainly we did not, um, you know, we did, you know, we saw it,
we were kind of intrigued to see that other people are calling it the same thing because I think
when I think of artifacts, I think first and foremost about, um, the instantiation of a model.
So like, when I would, you know, outside of this, the context of this conversation, when I'd
ask folks about, uh, the way they manage, managed artifacts, it would be model artifacts,
like some folks would, you know, pickle, uh, python model and store that, that was the,
the pickle file was the artifact. Yeah, yeah, um, folks would kind of, you know, create a doc would
containerize it and the container itself was the artifact. For some folks, the artifact that
they were tracking was a Shah, uh, Gita. Uh, and so I would ask that question to kind of understand,
to understand, um, you know, what was the fundamental currency of whatever system that they were,
um, building, it sounds like for you, an artifact is, well, what is the fun, what are you actually
tracking? It's practically what we see people tracking, like practically what we see them doing,
is tracking usually data sets and, um, models, right? So that's, that's what you should be like
imagining. What are you tracking the data set for the model? Like, do you, do you, is artifacts,
a content management system that is, you know, taking files, putting them in storage, managing that,
or is it, are you tracking pointers to artifacts? Both and I think that's what's key here, right?
So we do both and we also allow you to tag and version them and that's like the big,
that's the big, that's the infrastructure that we've built, right? So we think that people in
different cases might want to save these things on a third party server and might not want to,
right? And, and in fact, even, you know, we have an on-prem version of this and even there,
you might not want to move around like a petabyte, um, data file, um, but in some cases, you actually,
you know, might want to do that just to make sure that it's like completely saved. So, um,
that's what I think a really important feature, it's just super important to our customers is to be
able to kind of handle both cases where it's like, you know, maybe it's appointed to a file or
appointed to a bucket or literally, um, a file. And I think the other important, um, and maybe this
is getting a little in the weeds of the engineering, but, um, I think the way that our users at least
kind of conceive of it is like, you know, experiments, um, have kind of like, in input and output,
typically, right? And the input might be some data sets, say, and the output might be a model,
right? That'll be like sort of the classic kind of experiment that that that we think of,
right? Like a training, you know, ML training run and there the inputs and the outputs are actually
artifacts, right? In our vocabulary, but one of the things that we saw was actually there are
other things that get general. Well, I should say there's other types of experiments that you might
run, um, like a, um, like a data preprocessing step, right? So, you know, data preprocessing, it
actually does have hyper parameters often, right? So like, you know, like maybe, um, if you're doing
like, um, kind of image preprocessing, like, how much you jitter, you know, the images or, um,
you know, with words of information or, yeah, that kind of stuff. Um, and so, you know,
there are the input artifacts is like a data set and the output artifact is like a, you know,
modified, um, data set. But I mean, you know, also as you know, like, um, sometimes in the course
of training and like the real world, there is like other stuff that gets generated, right? There's
like other files that, you know, might matter to you, like, you know, like profiling, like a huge
kind of profiling, um, data set or, um, you know, in the course of model training, right? Like,
you know, so there's, there's all kinds of, um, files that you, that you might generate. The
idea is that, um, in a way, one really nerdy way to think about this is like, you know, the,
the artifacts are like the nodes in the graph and the experiments are like the edges in the graph,
right? So like, you, you think of this sort of directed, um, you know, graph that usually starts
with data sets and ends with like a deployed model. And that is all kinds of chaos, you know,
a long way. Um, you know, I'll say like one thing that's actually been surprisingly important to
some of our, um, users and it's kind of different than, so a lot of people would call this like a
pipeline, right? Like, um, you know, I think people that do this for production, they'd be like,
okay, that's a pipeline. I think one difference about, you know, our artifacts or one design
choice that we made is allowing people to change this type of thing on the fly. So you don't necessarily
have to know exactly, um, you know, what you're going to do with your model, right? You can, you know,
you can make like a game time decision to be like, okay, I'm going to like run some expert experiments
on it. Um, and that's okay, right? So that's, that's a kind of really common thing, um, that we see.
In fact, you might want to like even look at as like a manager, you might want to be like,
like, I have this data set, I just want to see like every model that got trained on it in my company.
And that's actually an easy thing for our tool to do now if you're using our artifacts product,
we can actually, because we, you know, everyone that uses it gets kind of tracked now, you can say,
okay, here's all the people that use that data set, um, or even, you know, you might like have, um,
uh, you might have a model that gets used by, um, you know, other models downstream of it,
right? And so as long as like every model that's like training up the output of a different model,
as long as they declare it, um, you know, you can actually, um, visualize all that in your,
in your company. You were making a point earlier about, um, the, the reason why you didn't like
the terminology pipeline, because it's more dynamic than that is, is that analogous to like,
I'm envisioning, uh, programmatic access to this or predefined, um, you know, access where
you've got some, some pipeline, uh, you know, coded up and you're executing it versus going in via
the UI and, you know, running, kicking things off manually, accessing files manually. Is, is that
the distinction that you're making there? I think we made a design choice that again was just
really informed by your request from, from researchers that we were working with,
let the, the pipeline get defined dynamically, right? So like, you know, production pipelines,
you tend to just sort of like set them and run them and then you might make a new production.
Um, pipeline, whereas like our notion of pipeline, it's like maybe a little bit more like
realistic kind of real world notion, where it's just sort of like that graph and that graph,
you can keep, that graph can keep changing. And it's not a UI thing, like it's like, you know,
you could, you know, you run things at any point, if you run a thing and you declare like,
hey, I'm using, um, you know, dataset xyz, then in our product at any point, if you look at
dataset xyz and you, you query it for like, what's all the stuff downstream of that? You'll see all
those runs. Um, yeah, I think I was getting at, uh, predefined versus ad hoc. Yeah. And it sounds
like that's kind of the distinction that you're, yeah, ad hoc, exactly, exactly. Not
forgetting. Yeah. I hope I'm making sense. I, it's like, you, you're better explain
the stuff than I am. And so maybe another question on that. So, but is the idea then that while
the, um, you know, while you're not locking, you know, while you're not only tracking changes that
happen as the result of a predefined executed pipeline, and you're allowing folks to do ad hoc,
uh, manipulation of the artifacts that you are also tracking the things that they do to,
that they do ad hoc with the artifacts and kind of they become those actions become part of the
graph and thus part of the pipeline. You know, a core design choice, right? It's the artifacts
themselves are immutable. Um, but what you can't add is like essentially like, um, edges to this
graph, right? So if you decide like you built a model and you want to do one extra test on it,
um, you know, before deployment, you can, um, you know, you can, you can basically do that run,
and that'll notify, um, our system and the background that this is happening and we'll keep,
we'll track that all, um, for you. Cool. Is artifacts out? Is it, uh, yeah, folks using it?
Yeah, books are, they're using it. We've been, um, I mean, it'll say we've been like kind of testing
it on early users for quite a long time. So, you know, it's just coming out and, you know,
I'm sure there are some bugs, but it has been, um, you know, beaten on a fair amount of by,
you know, by existing customers. We, we, we've really been trying to use the fact that we have
a lot of users of our first tool to, um, you know, to kind of build it in the conversation with folks.
And historically, and correct me on this, but when I think of weights and biases and the
experiment management tool in particular in the broader context of experiment management,
the focus was very squarely on deep learning as opposed to, uh, you know, tabular data and kind
of traditional machine learning, uh, to what extent does that, if that is, in fact, true?
So what, does that focus kind of carry on to artifacts as well? That's a good question.
I mean, I think we designed it with sort of modern deep learning techniques in mind,
but the line is not so bright. If you know what I'm saying, like, we have a lot of customers
doing, you know, boosted trees and, and, um, you know, I think, I actually, it's a good point.
I think like, um, you know, for a lot of people that are doing kind of boosted trees at big scale,
this, this could be a really relevant, um, you know, think for them. So I think like, I would say
probably most of the users that we have today are doing, um, deep learning or what you would call
deep learning, um, but I don't know. I'm not so opposed to what? Well, I mean, first of all,
I'm not so pure about this. I think, I mean, I think like XG boost is like a awesome tool.
It's like when I started my, you know, I said, well, I started my career before I could boost,
but I've probably made more boosted trees in my life than anything else.
I think scikit-learn is a beautifully done, um, library that I use like all the time for just
my own, um, data analysis. So, um, is it not about about whether deep learning is better than
anything else, but you said deep learning or what you would call deep learning? Is there a deep
learning that like a super secret deep learning or a deep learning? No, no, no, no, no, no, no.
I mean, like, uh, yeah, sorry, I think about like, I think like some of our users they'll be doing,
I think like, you know, I mean, deep learning, you know, as you know, it's like kind of aspirational,
right? So like, you know, something to like enter a company, um, and I really should not name
names here, but like, you know, we'll come in because like somebody's like, oh, I got to do deep
learning. I got to use like, you know, weights and biases for my deep learning thing. But then
most of the company is like using, you know, scikit-learn. And so they, you know, they pick up the
tool and they use it. And I have no, you know, that's great. That's wonderful. I'm not, you know,
I'm not a leadist about this, you know? Right, right. But yeah, I mean, I just honestly,
I think it's probably like, you know, 80% of the people we're talking to are doing,
oh, some kind of neural mat. So it's, it's okay. That are like core user personas is doing deep
learning. Okay. Um, we're not going to ban you from my tool if you use non-neural methods.
And do you have folks or do you anticipate, uh, users that are using artifacts independent
of experiment management? Or do you see this as, uh, side dish to experiment management?
Uh, I guess I view this as an adjacent product. So yeah, you can, you can use this without the
experiment management, but I'm really proud of our experiment management. What I would try to,
I would try to get you to use that. Um, so I mean, right, right now it's, I think that over
lots of 100% because we've really targeted the people doing, you know, using our experiment
management tool, uh, but you know, over time that might change. I think they're pretty similar to
our hyper parameter, um, sweeps, um, is pretty deeply attached to our experiment management. So,
you know, that's similar, um, overlap. Although I think the, the hyper parameter search and the
artifacts is totally independent, you know, you, you could definitely, um, you know, use one or
the other, or neither. Mm hmm. If folks want to, um, take advantage of, uh, artifacts,
is there thinking that they need to do about the way that they process data that they've likely
not done before? Well, look, I think that people should only use software where they have
actual pain points that they can articulate. So, you know, this doesn't happen very often,
but it happens more than, it happens occasionally, and I'm, I'm always kind of baffled by it where
people are like, oh, I want to use your tool, but like, what does it do? You know, it's kind of like,
okay, you probably don't want to use my tool and, you know, let me give you, let me give you an example.
So, um, you know, business process management, right? It's a, it's a similar space. It's like,
essentially, you know, you want to automate some workflow in your business. Maybe it's like
order to cash or something like that. And you may have, you know, 100 people doing it,
you know, you documented the process 10 years ago. The process has evolved for 10 years, and now you
really have no idea how, you know, at a top level, like how your orders are being processed. They
bounce around the organization and the first thing that you need to do to actually implement that
into a tool is to go in and understand what's really being done, right? What are the connections
from one thing to the next? What are the exceptions that pop up? What are the rules that people are
applying implicitly? You know, all of that stuff. And I'm, you know, curious with this question,
are there similar things that you run into where, you know, folks, you know, there are aspects of
the way that their data is being used in this machine learning process that they haven't really
thought about and need to kind of work out before they can implement something like this or,
you know, it's just so, you know, it's all so new and they were doing something, you know,
that was programmatic or rigid enough before that no, they just need to point your tool at some
stuff and it all just works. It's interesting, is it actually a really interesting question because
I think that, you know, my last company was kind of closer to business process management.
And you, you had to buy into a lot to get it working. I think with this,
you know, going, going back to like what I was saying around like pain point, I think the,
you know, you don't have to, you don't have to have like the perfect setup or track everything
to get the benefit, you know, from a tool like this. So like the, I think the point that you would
want to use a tool like our artifacts is when you're asking questions like our customers are asking
like, you know, do we really know exactly what data all the models are trained on or are we worried
about, you know, like our data sets are changing and we're trying to track it. Are we worried that like,
you know, the person who actually writes down what the different data sets are is going to leave
and it's going to be able to know like what they, you know, what they actually were. And I would say
that's actually the most common entry point is around data set, like worrying about data sets,
right? So, you know, like you could get like all that benefit by just tracking the data sets and,
and that is a fairly light instrumentation. It's actually not quite as light as our
experiment management, which we've really worked hard to get down to like under, you know,
five, six minutes. Like this is like slightly more involved. And, but the benefit is bigger in that
you'll for now, for sure, like always know the data sets that got, you know, that you trained on.
And then, you know, down the road, you may, you know, have another concern, which is like, you know,
do I really know, you know, like my models to plan to production, you know, do I really know
exactly what happened to them? Like, do I know like what the preprocessing was? And now you can,
you know, save the model and you can, you know, start to save pipelines, but you don't have to,
it's not like you have to, you know, buy into a huge process to get the benefit. I think on like
other like Senate organizational things that, that teams do, this is really like, this is really some,
I, in fact, I would like kind of encourage teams to adopt it one step at a time versus like trying,
I mean, there's something about like, I think actually, maybe it's that ML is like sort of less,
you know, understood exactly what all the best practices are. Like, you know, my recommendations
always think working ends, you know, and then iterate and improve it. And, you know, part of
improving it is getting better tracking and, you know, getting more organized. And I think that's
the point where, you know, artifacts would make sense to you as a product. And is there a common
pattern that you saw or that you can envision for folks that, you know, aren't ready to move to
a full-on, you know, tool to do this, but, you know, we'll get them part of the way, you know,
from nowhere. Like, what's the, what's the half step or the...
Well, okay, so there's some half steps that you really, you can do. I mean, like, later on,
doing them already, you should be doing what? Well, I mean, this is, I mean, this is
have done, I mean, I'm like, guilty of not doing this, but it's like, you know, if we're creating
your model on a data set, I mean, I don't know, like, I feel like when I have data sets, right,
I start, I mean, I start naming them like, you know, like, XYZ latest, and then like, XYZ,
like, really latest, like, really latest, you know, V2. And it's like, you don't do that. You know,
storage is actually cheap. It's just cheaper than you think. And, you know, that, you know,
that stuff saves you a few seconds or, you know, a day in the short term, and it really bites you
in the medium, the short to medium term. I mean, you know, I think anyone listening that,
that's trained models will kind of understand that. And I think like, you know, like, I mean,
you can make a spreadsheet, and you can, you know, you can be organized about the, you know, all the
model files that you have and like, what they are and what happened to them. And, you know, I think,
like, most, I mean, most people that we talk to are doing some form of this because it's not like,
it's not like a brilliant insight that we had that, you know, you should be organized about your
training, right? So I think most people find some way to do this. And I think like, most experienced
ML practitioners, as they, you know, gain more experience, they get more and more, you know,
kind of paranoid and organized about, you know, about this kind of, except I think where things
like really break down is like, I just, I don't know, I just remember my first job. There was like
this file, like, we had like this file, it was like all the features that we had. And for some
fucking like the TPS file, oh man, I remember that all the features were named like, it was like,
it was like, as though, like, there's some limit on the number, like, letters you could put in the,
the column name, you know what I mean? So they were like, it would literally be like five letter,
you know, five letters in like a number, you know, and like, and it was like, and someone made
like a wiki page of like what they meant, but like they were kind of wrong, you know, because it
wasn't like the person that like made all the features, it was like someone who like kind of tried
to figure it out, you know, like, and I mean, I don't know, like that, I mean, that's like where we
ended up, you know, and like, you know, this is a Yahoo and in 2006, you know, like building,
and it was like, you know, this was like billions of dollars, it was like the relevant engine,
you know, back when Yahoo served like a ton of search traffic, so it was like, there's no joke,
like, we wanted it to be good, you know, but it's just, it's so natural that like, you know,
one person like makes this thing, it's like obvious to them, and they don't document it very well,
and then, you know, when they leave, and another person picks it up, they don't even really know,
like, what's going on? So, you know, I think, I think, you know, documenting and saving is a,
uh, is it really good? Yeah. Cool. Um, well, Lucas, thanks once again for, uh, you know,
joining us, sharing a bit about what you're up to with me and the listeners, and as always,
it's great to catch up with you. Yeah. Real pleasure. Thank you. Thank you.
All right, everyone. That's our show for today. To learn more about today's guest or the
topics mentioned in this interview, visit twimmelai.com. Of course, if you like what you hear on the
podcast, please subscribe, rate, and review the show on your favorite pod catcher. Thanks so much
for listening, and catch you next time.
