1
00:00:00,000 --> 00:00:11,600
Hi everyone, I am joined by Sushil Thomas, who is the VP of engineering for machine learning

2
00:00:11,600 --> 00:00:16,140
at Cladera. A little bit of background on this interview earlier this summer, Cladera

3
00:00:16,140 --> 00:00:20,920
invited me to host a series of roundtable discussions with enterprise data leaders from

4
00:00:20,920 --> 00:00:27,280
a variety of industries to explore their experiences, building the foundation for success with machine

5
00:00:27,280 --> 00:00:33,920
learning and advanced analytics. Sushil and I had a bunch of great conversations in that context

6
00:00:33,920 --> 00:00:40,440
and I invited him here to join me for a deeper conversation on the topics. Sushil, welcome

7
00:00:40,440 --> 00:00:46,040
to the 12 more AI podcast. Thank you Sam, very nice to talk to you again. I had a good

8
00:00:46,040 --> 00:00:51,280
time at that roundtable talk too and I wish we could do this in person, it's kind of insane

9
00:00:51,280 --> 00:00:57,600
in the world we're living in now. It is insane the world we're living in and a little bit

10
00:00:57,600 --> 00:01:02,880
of background on the data leader roundtable program. It was originally envisioned to

11
00:01:02,880 --> 00:01:10,400
be an in-person series of events that we did with folks that were in leadership positions

12
00:01:10,400 --> 00:01:20,800
in machine learning and AI and data and we pivoted that to accommodate the pandemic and

13
00:01:20,800 --> 00:01:26,720
one of the things that I'm curious about and we'll ask you about momentarily is what you're

14
00:01:26,720 --> 00:01:32,720
seeing, how you're seeing folks adapt to the pandemic and those kinds of roles but before

15
00:01:32,720 --> 00:01:40,000
we do that, let's get a little bit of introduction into your background and your role at Cladera.

16
00:01:40,000 --> 00:01:48,640
As you said before, I'm VP engineering for machine learning here at Cladera. We've increased

17
00:01:48,640 --> 00:01:54,560
the scope of that a little bit recently to include a visualization, data visualization platform

18
00:01:54,560 --> 00:02:00,320
as well because it's such a strong part of the machine learning workflow. In my past, so I've

19
00:02:00,320 --> 00:02:06,800
always been very close to data but in different aspects of it, I started out very early working

20
00:02:06,800 --> 00:02:14,000
on Solaris and in Sun Microsystems in the first internet boom. So I saw people serving up

21
00:02:14,000 --> 00:02:20,160
web pages at massive quantity for the first time on these monster sun machines and it was a ton of

22
00:02:20,160 --> 00:02:26,960
fun to work on those challenges moved into a systems storage company called three-power data.

23
00:02:26,960 --> 00:02:32,640
So worked on enterprise storage for a long time where you would run the backbone of these

24
00:02:32,640 --> 00:02:38,560
data centers from a storage systems perspective and that was a really interesting set of

25
00:02:38,560 --> 00:02:44,400
challenges as well where you have like 30, 40 seconds to respond to every single request and you

26
00:02:44,400 --> 00:02:49,280
miss even one and the entire data center is down because there's all these dependencies

27
00:02:49,280 --> 00:02:53,040
built into the stack and different layers. So anyway, that was a lot of fun as well,

28
00:02:53,040 --> 00:02:59,280
dealing with big data but in a different kind of way and then moved from there once I saw the

29
00:02:59,280 --> 00:03:05,280
scale at which these platforms were being built at places like Google and Yahoo very early on

30
00:03:05,280 --> 00:03:12,000
moved into working on distributed systems. So started with a startup called Jovian Data

31
00:03:12,000 --> 00:03:17,840
that's got acquired since by a new start at this point but essentially working on very large

32
00:03:17,840 --> 00:03:22,960
scale data processing to help the early ad networks process through their data and understand their

33
00:03:22,960 --> 00:03:28,320
data. Moved to a company called Aster Data that builds distributed databases got acquired

34
00:03:28,320 --> 00:03:35,280
into Teradata. So saw a lot of the old school data warehousing stuff at Teradata as well and

35
00:03:35,280 --> 00:03:39,120
I have a lot of appreciation for all that technology from seeing it from the inside as well and

36
00:03:39,120 --> 00:03:44,160
seeing how hard it is to do a lot of that stuff. And then of course started my own company.

37
00:03:44,160 --> 00:03:50,880
It's called Arcadia Data. We did visualization and analytics on large data sets. The challenge

38
00:03:50,880 --> 00:03:56,320
there was to get business users into the Hadoop platform which was at the point at a crazy

39
00:03:56,320 --> 00:04:01,440
growth stage. And now at Cloud Data continuing on into the enterprise data cloud,

40
00:04:01,440 --> 00:04:06,400
Arcadia got acquired into Cloud Data. So continuing on with the enterprise data cloud and helping

41
00:04:06,400 --> 00:04:11,440
enterprises work on this hybrid story. They have a bunch of data on prem. They have a bunch of

42
00:04:11,440 --> 00:04:17,600
data in the cloud infinite requirements on what they want to do with that data and so trying to

43
00:04:17,600 --> 00:04:22,400
help them through all of that and working specifically on machine learning AI visualization as a part

44
00:04:22,400 --> 00:04:29,600
of that. Got it. Got it. And in your role as VP of engineering is that primarily focused on

45
00:04:29,600 --> 00:04:36,560
engineering the internal products that are offered to customers or how close do you get to the

46
00:04:36,560 --> 00:04:42,080
the problems that customers are facing from a machine learning lifecycle perspective?

47
00:04:43,120 --> 00:04:49,280
Right. So there's a few groups that that report up to me. One of them is the machine learning

48
00:04:49,280 --> 00:04:54,720
product itself. So we have a product called CML Cloud data machine learning which is a hybrid

49
00:04:54,720 --> 00:04:58,800
platform. It works on prem. It works on the cloud. It's it's very much our product within the

50
00:04:58,800 --> 00:05:03,760
market for a long time that targets machine learning developers and data scientists to actually be

51
00:05:03,760 --> 00:05:08,320
productive and get a lot of the development a lot of the production stuff going. We'll talk about

52
00:05:08,320 --> 00:05:15,120
that a little bit later perhaps in some pieces. And then the other group one of the other groups is

53
00:05:15,120 --> 00:05:21,840
fast-power labs. So this is a machine learning research team. We do applied research towards a bunch

54
00:05:21,840 --> 00:05:27,120
of topics all open source all available. We run webinars every month every few months. We present

55
00:05:27,120 --> 00:05:33,840
a lot of conferences. So that just helps customers like the goal of that organization is to take what

56
00:05:34,480 --> 00:05:39,680
is maybe two years ahead in terms of machine learning theory right in terms of how quickly it's

57
00:05:39,680 --> 00:05:43,840
going to get applied to customers and try to accelerate that process a little bit show people

58
00:05:43,840 --> 00:05:49,360
a glimpse of the future and help them get to solutions on their in their enterprises that that

59
00:05:49,360 --> 00:05:54,240
help with that right that that we can augment them on. And there's a lot of possibilities there.

60
00:05:54,240 --> 00:05:59,280
There's a lot of stuff you're doing in terms of making that even easier especially as it comes to

61
00:06:00,160 --> 00:06:03,920
leveraging all of this stuff on the cloud where everything is very easy, very self-service

62
00:06:03,920 --> 00:06:08,720
and should be trivial to get started. And then the third team that's new is the data visualization

63
00:06:08,720 --> 00:06:13,760
team. We just announced this like basically post the Arcadia acquisition. We've been working on

64
00:06:13,760 --> 00:06:19,200
integrating that technology into cloud-era stack stack as well. So now we've integrated data

65
00:06:19,200 --> 00:06:23,760
visualization into both the machine learning platform and the core data warehousing platform.

66
00:06:23,760 --> 00:06:30,080
And so as machine learning engineers as data scientists you now get to look at the data in a very

67
00:06:30,080 --> 00:06:37,040
very easy way build these end applications that work for end users business users and connect the

68
00:06:37,040 --> 00:06:42,400
sort of the descriptive data analysis that's always been possible with more prescriptive models

69
00:06:42,400 --> 00:06:46,720
and and mesh that data together and and make these beautiful applications for the organization.

70
00:06:48,000 --> 00:06:53,600
Okay, great. But there's a part of that like as to your questions you know we always work very

71
00:06:53,600 --> 00:07:00,320
closely with customers like my strong push on engineering organizations is always that you

72
00:07:00,320 --> 00:07:04,560
cannot just build these products in isolation right so you have to continuously be talking to

73
00:07:04,560 --> 00:07:10,240
customers. If I don't have you know two or three customer meetings every week then I'm really

74
00:07:10,240 --> 00:07:14,560
disappointed essentially and typically it's more than that. And you work in customers at different

75
00:07:14,560 --> 00:07:18,560
phases like so you work with the customers that have never heard of you and never used your products.

76
00:07:18,560 --> 00:07:22,640
You work with customers that are three years into using your products and that know the ins and outs

77
00:07:22,640 --> 00:07:28,240
of of of your product and their business and have very specific suggestions on what you should do

78
00:07:28,240 --> 00:07:36,000
next. So there's like there's there's a high number of customer correlations always ongoing. So

79
00:07:36,000 --> 00:07:42,320
so I mentioned the the data leaders roundtable program earlier and again we wanted to chat a

80
00:07:42,320 --> 00:07:49,600
little bit about that and get your key takeaways and in particular that event took place

81
00:07:49,600 --> 00:07:58,240
earlier in the the COVID pandemic this was earlier in the summer and folks were you know in the

82
00:07:58,240 --> 00:08:04,960
process of adjusting to working from home and getting their teams set up to to work from home.

83
00:08:04,960 --> 00:08:12,080
Now we've got quite a bit more distance under our belts now and a lot of the organizations that

84
00:08:12,080 --> 00:08:18,800
I'm talking to are not planning to return back to offices until sometime mid to late next year

85
00:08:18,800 --> 00:08:25,760
at the earliest yes and I'm just curious in the the conversations that you're having what kind of

86
00:08:25,760 --> 00:08:35,600
impact all of this disruption has been having on on data organizations is it you know has it

87
00:08:35,600 --> 00:08:42,560
been impacting them much at all is it industry by industry or you know our folks still working

88
00:08:42,560 --> 00:08:49,200
through what that mean what this all means yeah so so I think one thing that I didn't know going

89
00:08:49,200 --> 00:08:54,880
in it did surprise me a little bit but maybe in retrospect it's obvious I think one thing is that

90
00:08:55,680 --> 00:09:03,360
data has become so core to organizations now and storing all their data and analyzing it processing

91
00:09:03,360 --> 00:09:08,480
it using it to predict outcomes using it to understand their business is so central to how

92
00:09:08,480 --> 00:09:16,960
organizations work now that at least the cloud era we have seen surprisingly little or no impact

93
00:09:16,960 --> 00:09:23,520
to our customers like even the ones in industries where you would think these industries are substantially

94
00:09:23,520 --> 00:09:28,480
impacted guess what they still have all of that data they still have to worry about securing it

95
00:09:28,480 --> 00:09:34,560
and using it and leveraging it for for future planning stuff the one thing I of course as you

96
00:09:34,560 --> 00:09:40,080
said there's been so many changes at organizations just trying to figure out how to get everyone

97
00:09:40,080 --> 00:09:45,760
working from home and working remotely and some organizations do it better than others but we are

98
00:09:45,760 --> 00:09:51,760
all in this place where no one quite we are still learning right and it's not just the right it's

99
00:09:51,760 --> 00:09:56,160
not just the architecture right it's just it's just living with the pandemic and and all your

100
00:09:56,160 --> 00:10:02,240
employees being a little like their lives being so different right and as that goes on and on and on

101
00:10:02,240 --> 00:10:07,520
I think you have to like continually manage people's expectations and psychology and make sure

102
00:10:07,520 --> 00:10:11,840
work is not a drain on them I think we've all gone through how difficult it is to just stay on

103
00:10:11,840 --> 00:10:18,240
zoom the entire day or stay on video conferencing the entire day it's kind of crazy so I mean there's

104
00:10:18,240 --> 00:10:26,640
all of that but on the other side on the actual challenges of working with data on the on the actual

105
00:10:26,640 --> 00:10:31,760
concept of getting all these things going there's not much has changed all the data teams that I

106
00:10:31,760 --> 00:10:38,240
work with have not you know reduced or changed in size substantially if anything the big differences

107
00:10:38,240 --> 00:10:42,640
I'm hearing of a lot of use cases that are specifically related to the pandemic as you would expect

108
00:10:43,440 --> 00:10:49,280
again I think just highlighting how central data has become to organizations and how much people

109
00:10:49,280 --> 00:10:55,520
start with data I love that notion right like it used to be you know 20 years ago you you want to

110
00:10:55,520 --> 00:11:00,800
decide something about your business you know you want to let's say your retail store and you're

111
00:11:00,800 --> 00:11:05,600
trying to figure out where you open the next 10 locations I think 20 years ago you would have

112
00:11:05,600 --> 00:11:09,840
exact sitting in a room you would have regional leads right you would be discussing you should

113
00:11:09,840 --> 00:11:14,560
be do it right and you would pick like a few locations that people have people think I interesting

114
00:11:14,560 --> 00:11:19,120
people would drive out to it like things like that right and now it's so different the first thing

115
00:11:19,120 --> 00:11:24,400
is like where's the data where are competitors where where is the spend high right and where should

116
00:11:24,400 --> 00:11:28,960
be open and then you start with like shortlisting just based on the data and then maybe you do a

117
00:11:28,960 --> 00:11:34,400
sanity test at the end to just make sure you know you're not building on a on a radioactive site

118
00:11:34,400 --> 00:11:39,200
or something like that at the end but like the the fundamentals are so different in how people

119
00:11:39,200 --> 00:11:48,560
run their businesses now you mentioned that you are seeing some covid related use cases pop up are

120
00:11:48,560 --> 00:11:55,840
these primarily in the industries like health care and the industries that are part of the

121
00:11:55,840 --> 00:12:01,360
response to covid or is it broader than that I actually meant health care because that is so central

122
00:12:01,360 --> 00:12:06,960
like we are we are also you know I think it's natural to try to be particularly supportive to

123
00:12:06,960 --> 00:12:10,960
these industries that are there are trying to help all of us out through this right and you see

124
00:12:10,960 --> 00:12:16,720
that they have a lot of interesting work that's ongoing now and that that work is very compelling

125
00:12:16,720 --> 00:12:20,320
so I actually meant health care cloud has a large health care business and so there's a lot of

126
00:12:21,120 --> 00:12:25,680
companies talking to us about their challenges here and the extra kinds of work and processing

127
00:12:25,680 --> 00:12:28,880
they need to do and seeing if we can help and of course we are more than happy to help

128
00:12:30,080 --> 00:12:36,640
and so yeah so just seeing a lot of those use cases either around planning for for the number of

129
00:12:38,560 --> 00:12:42,640
folks that come into hospitals now and have to be processed and you have to do all of this

130
00:12:42,640 --> 00:12:47,360
planning and forecasting you know things like when do the how many doctors do you need to get

131
00:12:47,360 --> 00:12:51,840
what how do you do the scheduling for this stuff what what your expectations on on workloads

132
00:12:51,840 --> 00:12:56,160
and then of course going all the way into the research side and trying to understand which of these

133
00:12:56,160 --> 00:13:00,480
are which of these possibilities might work towards vaccines which of these are good

134
00:13:01,840 --> 00:13:06,080
treatments options like all of this analysis that's done with the data as well and of course

135
00:13:06,080 --> 00:13:10,960
everything is again just related to data right they have at this point there's a ton of data out

136
00:13:10,960 --> 00:13:14,960
there on people who've come in on the kinds of treatments that we've done on them on remission on

137
00:13:14,960 --> 00:13:18,560
any sort of remission that's happened on on what the outcomes have been and so there's a lot of

138
00:13:18,560 --> 00:13:23,120
analysis you can do to get to a place where where you where you can make better decisions about what

139
00:13:23,120 --> 00:13:30,240
you're doing forward which is the important thing. So for the round table I structured the conversation

140
00:13:30,240 --> 00:13:37,840
broadly in terms of these three core themes people process and technology you know certainly themes

141
00:13:37,840 --> 00:13:45,760
that we talk about a lot and the in technology and I thought we'd spend some time talking through

142
00:13:45,760 --> 00:13:54,160
your key takeaway for you know in those themes from our discussion and as well as your broader

143
00:13:54,160 --> 00:14:04,400
experience talking to the customers you know over the past few months we touched on some of the

144
00:14:04,400 --> 00:14:11,760
impacts of COVID to people but more broadly what are some of the key trends you're seeing in

145
00:14:11,760 --> 00:14:19,200
organizations that are trying to deploy machine learning go further with their ability to

146
00:14:19,200 --> 00:14:28,000
make use of data deploy advanced analytics how are they dealing with the organizational issues

147
00:14:28,000 --> 00:14:35,040
that arise and what are those issues yeah. So the round table was great and I think it was at

148
00:14:35,040 --> 00:14:41,120
least to me very different setting talking to a bunch of data leaders together versus talking

149
00:14:41,120 --> 00:14:46,720
one-on-one to a single organization and maybe a few people at different levels within that organization

150
00:14:46,720 --> 00:14:49,920
of course you've done this a lot more so I'll give you my takeaways but I'd love to hear

151
00:14:49,920 --> 00:14:54,080
your perspective on whether this was different from the others you've done I'm not either also.

152
00:14:54,080 --> 00:14:59,920
So the one thing that was super interesting to me was you know this is obvious when I do the

153
00:14:59,920 --> 00:15:07,200
one-on-one conversations like ML&EI within organizations is is still very very nascent and people

154
00:15:07,200 --> 00:15:12,800
are still figuring out what the right way is to do things and I would contrast this with something

155
00:15:12,800 --> 00:15:18,080
like data warehousing right where this is a very very well understood thing you know exactly how

156
00:15:18,080 --> 00:15:23,440
to set up these organizations you know exactly what to expect at the end of that but for the for

157
00:15:23,440 --> 00:15:30,480
ML&EI it's much more it's much more subtle like no one exactly knows what the expectations are like

158
00:15:30,480 --> 00:15:35,280
normally teams are set up with very very high expectations on this is just going to magically

159
00:15:35,280 --> 00:15:40,400
make everything better the teams are struggling through like we heard that all the time like the

160
00:15:40,400 --> 00:15:47,120
teams are struggling with where is the actual data is the data clean like how do I get rid of all

161
00:15:47,120 --> 00:15:52,560
this noise from the data then what kind of use cases can I do and then continuing on even after

162
00:15:52,560 --> 00:15:56,720
you do the first rare off that work how do you get feedback back into the system how do you

163
00:15:56,720 --> 00:16:01,440
continuously improve these models how do you serve all this stuff at scale and how do you keep

164
00:16:01,440 --> 00:16:07,360
doing this like for new newer use cases that come up while maintaining the older use cases so

165
00:16:07,360 --> 00:16:12,240
that they continue to be relevant and how do you keep all of this going at organizational scale

166
00:16:12,240 --> 00:16:17,840
and so I think industry wide all of us are in the process of figuring a lot of these things out

167
00:16:17,840 --> 00:16:23,040
and what the best practices are here these are not very well written out today and so what I heard

168
00:16:23,040 --> 00:16:29,840
in the roundtable was initially I thought all the participants were a little bit hesitant

169
00:16:29,840 --> 00:16:34,240
because they didn't know if they were the only ones that that were in a little bit of trouble

170
00:16:34,240 --> 00:16:38,960
and that didn't quite know how to solve all these problems right but I think as they heard from

171
00:16:38,960 --> 00:16:44,960
everybody that we are all in the same boat it was a lot more like I think there was a lot more

172
00:16:44,960 --> 00:16:48,960
discussion about the real challenges that everybody hits right because you just don't know if it's

173
00:16:48,960 --> 00:16:52,880
just you but it's just good to know that like everyone's in a very very similar boat there's very

174
00:16:52,880 --> 00:16:57,760
similar set of challenges across the board so that that is really interesting to me like seeing

175
00:16:57,760 --> 00:17:01,360
that dynamic layout and I don't know if you've seen that in the other round tables you've done as well

176
00:17:02,320 --> 00:17:08,160
yeah you know one of the things that jumps out at me on this particular point and in particular

177
00:17:08,160 --> 00:17:16,320
the way you framed it is there's this back and forth in industries one thing that we're grappling

178
00:17:16,320 --> 00:17:23,200
with is thinking about machine learning as an engineering discipline versus thinking about it as

179
00:17:23,200 --> 00:17:31,120
a science and an exploratory process and different organizations take different

180
00:17:32,080 --> 00:17:37,280
different approaches in how they you know synthesize these two perspectives but

181
00:17:38,320 --> 00:17:44,720
in a lot of ways it's it requires synthesizing these two perspectives you can't just approach it

182
00:17:44,720 --> 00:17:54,320
as a traditional engineering task and you can't just approach it as a unstructured exploration

183
00:17:54,320 --> 00:18:01,440
if you hope to achieve any scale and you know I'm curious how that resonates for you as someone who

184
00:18:01,440 --> 00:18:09,040
is you know if you have engineering for machine learning right so it is fascinating we are also

185
00:18:09,040 --> 00:18:15,920
I would say while we have a machine learning product team what the machine learning product

186
00:18:15,920 --> 00:18:23,040
that cloud ourselves does is it helps engineers around all of the problems of machine learning right

187
00:18:23,040 --> 00:18:27,360
so there's the actual machine learning code that you have to write but then there's everything else

188
00:18:27,360 --> 00:18:31,920
there's where's my data there's how do I clean it there's how do I move these models of production

189
00:18:31,920 --> 00:18:38,480
how do I run metrics how do I do ground tracking like all of these problems around the

190
00:18:38,480 --> 00:18:44,800
actual you know 200 500 lines of code that you'll write to train your model and to serve your model

191
00:18:44,800 --> 00:18:49,120
so I think there's a lot around it that's that's what we deal with so so very much the thing

192
00:18:49,120 --> 00:18:54,080
that our product works on is I would say very much the engineering and the science part of it

193
00:18:54,080 --> 00:19:00,080
and less of the art part of it we are very flexible there we let you do anything you want to

194
00:19:00,080 --> 00:19:05,440
particularly because there's this problem where nobody quite knows that this is the one way

195
00:19:05,440 --> 00:19:09,120
in which machine learning is to be done and I don't think that's going to happen but I think what

196
00:19:09,120 --> 00:19:14,480
will happen over time is from my industry perspective these best practices will be will be better

197
00:19:14,480 --> 00:19:20,640
known over time how these organizations the ML data scientists organizations get set up will be

198
00:19:20,640 --> 00:19:27,520
better known over time and there will be like less of that less of the expectation difference

199
00:19:27,520 --> 00:19:31,840
between what is possible and what is achievable which is kind of very prevalent today

200
00:19:31,840 --> 00:19:36,880
you have some execs that will just think this will just work magically I'll just throw two or three

201
00:19:36,880 --> 00:19:41,920
people at this problem and then you find all of these issues around data and what's needed and what

202
00:19:41,920 --> 00:19:46,240
the expectations are on what the what's on the other side and how you to continuously manage this

203
00:19:46,240 --> 00:19:50,560
and stuff like that so we're going to get better than that in fact at loud era you know we have

204
00:19:50,560 --> 00:19:54,880
these ps offerings we work very closely with customers not just on the tech but we also have ps

205
00:19:54,880 --> 00:19:59,040
offerings that that try to get them closer to solving their problems and one of our very popular

206
00:19:59,040 --> 00:20:04,720
ps offerings is a strategy engagement for machine learning which literally boils down to you know

207
00:20:04,720 --> 00:20:09,760
I want more machine learning in my organization how do I get there right and so we do we speak with

208
00:20:09,760 --> 00:20:13,760
execs we speak with business themes that are looking for some of these solutions we try to

209
00:20:14,400 --> 00:20:18,800
meld everyone's expectations together and make suggestions that are right for the organization

210
00:20:18,800 --> 00:20:23,520
on how they should structure their their ML org because that's a challenge too like for many

211
00:20:23,520 --> 00:20:28,640
of these companies that we work with they they don't have a thriving ML practice now they would

212
00:20:28,640 --> 00:20:31,680
love to have one but they don't quite know how to get from where they are to that

213
00:20:32,560 --> 00:20:40,160
yeah one of the recurring themes that struck me from the round tables and for context we were

214
00:20:40,960 --> 00:20:47,680
largely talking to folks from traditional enterprises as opposed to the Facebooks and Google's

215
00:20:47,680 --> 00:20:54,880
of the world and you know no surprise they find it very difficult to compete with those organizations

216
00:20:54,880 --> 00:21:02,480
for talent and so there was a lot of discussion around how they you know groom and upskill and

217
00:21:02,480 --> 00:21:12,000
cross train internal talent from within historical or traditional data organizations to start to

218
00:21:12,000 --> 00:21:20,320
enable the the organization to move more quickly from a machine learning perspective curious

219
00:21:20,320 --> 00:21:26,240
what you've seen working well from that perspective and any takeaways that you had from that part

220
00:21:26,240 --> 00:21:32,080
of that conversation yeah I see a lot of that for sure it's super hard to get

221
00:21:32,880 --> 00:21:38,640
enough machine learning talent enough AI talent also lots of confusion in the industry as you know

222
00:21:38,640 --> 00:21:44,320
around terminology right so you have you have you know teams of data scientists that are really

223
00:21:45,600 --> 00:21:49,360
either data analysts or the like data warehousing folks were just trying to help with

224
00:21:49,360 --> 00:21:53,760
cleaning up the data and getting some basic statistical stuff going you have teams of people that

225
00:21:53,760 --> 00:21:57,680
actually said an IT but it turns out they are amazing data scientists and the use cases they do

226
00:21:57,680 --> 00:22:02,320
a lot more predictive and stuff like that right there's people who have a hub and spoke model right

227
00:22:02,320 --> 00:22:06,320
where they have a central sort of center of excellence these are the guys who set up the best

228
00:22:06,320 --> 00:22:11,760
practices around technologies they use and and and deployment practices and stuff like that

229
00:22:11,760 --> 00:22:16,880
and then business units will have their own individual data scientists that that they work with

230
00:22:16,880 --> 00:22:21,760
for for their actual use cases so yeah I see I see a lot of that also in the industry in terms of

231
00:22:21,760 --> 00:22:27,040
just not there's no standard set of terms on on who these folks are and when you hear someone's

232
00:22:27,040 --> 00:22:30,880
a data scientist or if you hear someone's not a data scientist they could be one it's just like

233
00:22:30,880 --> 00:22:36,800
it's all over the place and in terms of attracting training and retaining people I think that's

234
00:22:36,800 --> 00:22:44,400
a that's a substantial challenge as well I do think it's a very very good idea to to help people

235
00:22:44,400 --> 00:22:49,440
that are in your organization already that work with data on a data day basis and that want to

236
00:22:49,440 --> 00:22:54,560
up level their skills into into machine learning and into AI I I'm a big fan of being very supportive

237
00:22:54,560 --> 00:22:59,840
of that and getting people trained to a level where they want to I guess the good news about it is

238
00:22:59,840 --> 00:23:04,480
everyone who works close to data is interested in understanding more of our machine learning

239
00:23:04,480 --> 00:23:11,120
understanding more about AI and moving closer and closer to to to to being productive there so I

240
00:23:11,120 --> 00:23:15,760
think as a organization the opportunity is that you can actually train your people there because

241
00:23:15,760 --> 00:23:21,120
they want to be trained in in those fields and then the still the tricky bits is just having the

242
00:23:21,120 --> 00:23:26,480
structure around this practice so that you can actually use them productively at the end of that

243
00:23:26,480 --> 00:23:31,280
they can do interesting things while they're learning and then you can get more people in that area

244
00:23:32,880 --> 00:23:38,960
you've talked about some of the different organizational models hub and spoke versus centralized

245
00:23:38,960 --> 00:23:47,280
historically one of the challenges within traditional enterprises and IT is kind of

246
00:23:48,080 --> 00:23:54,080
breaking down silos and enabling organizations to communicate and work together effectively

247
00:23:54,080 --> 00:23:58,560
are there any things that you've seen work better than others or is it

248
00:23:58,560 --> 00:24:06,400
you know very very organizational dependent yeah and it is today it's organizational dependent

249
00:24:06,400 --> 00:24:14,480
I do think if you think about similar things that that organizations have wrestled with before

250
00:24:14,480 --> 00:24:18,880
and the ways in which they've set it up I think you get some clues around what might work better

251
00:24:19,600 --> 00:24:25,520
so in my mind for example if I think about data warehousing and data platforming

252
00:24:26,400 --> 00:24:31,760
it feels like a very centralized model has worked pretty well there for large organizations

253
00:24:31,760 --> 00:24:38,480
where most of your expertise is in a is in a central team they run your your data platforms

254
00:24:38,480 --> 00:24:42,560
and anytime you reach a place where there's a bunch of data silos around the place it's a little

255
00:24:42,560 --> 00:24:47,520
bit of a disaster right because you have to then figure out how to make sense of these these

256
00:24:47,520 --> 00:24:52,080
different deportations of all this data and all of these things in place but that's after many

257
00:24:52,080 --> 00:24:58,080
years of data warehousing practice yeah exactly and I think it's it worked very well there but if I

258
00:24:58,080 --> 00:25:04,000
can take the opposite side on something similar if you look at business analytics as a practice

259
00:25:04,000 --> 00:25:11,040
and BI as a practice it's very much the case that people might build central BI tools and do

260
00:25:11,600 --> 00:25:16,000
large contracts or that there's standardization on sort of what the tool is that you're using

261
00:25:16,000 --> 00:25:22,400
but once you've done that the actual use cases the actual end use cases are typically don't come

262
00:25:22,400 --> 00:25:27,040
from a central organization there's there's a few people that work in marketing and a few people

263
00:25:27,040 --> 00:25:31,840
work in sales and a few people work in product who are actually doing the day-to-day analysis of

264
00:25:31,840 --> 00:25:39,280
what's actually happening using those central platforms and I think that's because the data warehousing

265
00:25:39,280 --> 00:25:45,680
and data platforming is very much a problem of scale and standardization and and reducing costs

266
00:25:45,680 --> 00:25:51,120
and trying to make everything very secure and things like that but business analytics and BI

267
00:25:51,120 --> 00:25:56,480
is very much a problem of what do my users actually need and I need to move really really quickly

268
00:25:56,480 --> 00:26:00,880
really close to the users and these requirements change every day the questions change every day so

269
00:26:00,880 --> 00:26:05,520
so the the folks that work on this problem have to be really close to those business units

270
00:26:05,520 --> 00:26:13,120
so I am biased towards thinking that that's a better model for for ML and AI where the actual outcomes

271
00:26:13,120 --> 00:26:18,560
are really close to the business you need people right next to the business who will work with the

272
00:26:18,560 --> 00:26:22,960
business understand their urgency build the models show them the outcomes do the predictions all

273
00:26:22,960 --> 00:26:29,120
of that stuff and of course at the same time I do very much like this notion of let's standardize

274
00:26:29,120 --> 00:26:33,760
on tools and let's standardize on processes and let's make sure once these things are built out

275
00:26:33,760 --> 00:26:38,400
that there's a standard way in which we manage and monitor and track these things and and augment

276
00:26:38,400 --> 00:26:44,480
them over time right so that sort of that that that I think it's closer to BI and business analytics

277
00:26:44,480 --> 00:26:48,160
then data warehousing but you know that's my opinion we'll find out right we have we have a ways to

278
00:26:48,160 --> 00:26:57,360
go in figuring yeah yeah but you know building on that assuming a future that has data scientists

279
00:26:57,360 --> 00:27:03,440
you know very closely aligned to the line of business and what its needs are and platform

280
00:27:03,440 --> 00:27:08,880
organizations or machine learning engineering or data engineering you know those organizations

281
00:27:08,880 --> 00:27:15,280
that are building out the central capability existing centrally you know what are the

282
00:27:15,280 --> 00:27:23,680
the practices that allow folks to effectively operate that at scale based on these analogies that

283
00:27:23,680 --> 00:27:29,120
you just mentioned what does that life cycle look like what are those the relationships between

284
00:27:29,120 --> 00:27:34,880
those orgs look like in order to allow folks to get things done more quickly yeah and and again

285
00:27:34,880 --> 00:27:40,640
here I think we can take cues from from what's been done previously for analytics as well

286
00:27:40,640 --> 00:27:46,480
essentially the thing that seems to work well is having a standard set of definitions that

287
00:27:46,480 --> 00:27:52,960
are organization wide around the data having a standard set of security practices and access policies

288
00:27:52,960 --> 00:27:59,520
and all that stuff in place that are very strong and very central but then letting the analysts

289
00:27:59,520 --> 00:28:07,760
themselves very liberally work within that framework right so there is typically no standard policy

290
00:28:07,760 --> 00:28:12,880
on this is exactly what your BI dashboard must look like right you must have sort of one

291
00:28:12,880 --> 00:28:18,240
KPI on top and three bar charts in this and that right you want to be you want to be a lot more

292
00:28:18,240 --> 00:28:24,240
flexible on that entire use case and that entire surface area and be a lot more prescriptive

293
00:28:24,240 --> 00:28:29,920
about what you want the tools you want to use and the the practices around access and security

294
00:28:29,920 --> 00:28:35,280
to data so I think there are ways of of slicing this in a very similar way for ML you don't want

295
00:28:35,280 --> 00:28:39,840
to be prescriptive to the data scientists to say that this is the kind of model you'll use this

296
00:28:39,840 --> 00:28:43,600
is the kind of training you'll do this is the kind of hardware you will use to do the training

297
00:28:43,600 --> 00:28:48,400
because I think it's a evolving field and a lot of that is is continuously changing over time

298
00:28:48,400 --> 00:28:53,520
and you want to have that flexibility there at the same time when you talk about how you access

299
00:28:53,520 --> 00:28:58,320
data and when you talk about how you move something to production and how you monitor the

300
00:28:58,320 --> 00:29:03,120
performance of something going ahead and what the standards are for interpretability that I have

301
00:29:03,120 --> 00:29:09,680
to be built into whatever whatever you're publishing and how do you you know manage in a central way

302
00:29:09,680 --> 00:29:14,960
like uptime and all of these things I think there's a strong case to be made that that should be

303
00:29:14,960 --> 00:29:20,320
formalized standardized centralized it's also something that most data scientists I mean

304
00:29:20,320 --> 00:29:24,160
it's a little bit of a hassle for them right like that's something that that hopefully just works like

305
00:29:24,160 --> 00:29:28,080
if I want to get data hopefully it's well understood where that data is and how I get to it

306
00:29:28,080 --> 00:29:33,040
and if it's not there like who do I complain to about right so that it shows up essentially over time

307
00:29:33,040 --> 00:29:39,600
so I think it's all about like I think from the the other angle of people and hiring people

308
00:29:39,600 --> 00:29:44,880
and retaining people it's also important that when you get data scientists in that they work

309
00:29:44,880 --> 00:29:49,680
in a productive environment and one way to make that environment productive is to have very clear

310
00:29:49,680 --> 00:29:54,960
boundaries around a lot of these problem statements so that what they're working on is much more

311
00:29:54,960 --> 00:29:59,440
aligned to what they're good at and what they want to do essentially which is to do the actual

312
00:29:59,440 --> 00:30:05,840
data science build the use cases like that's the exciting bit for them so yeah yeah one of the

313
00:30:05,840 --> 00:30:13,440
recurring challenges that we talked about during the around tables was the challenge to getting

314
00:30:13,440 --> 00:30:23,440
models in a production the I think all of the organizations kind of reference store at least

315
00:30:23,440 --> 00:30:29,600
agreed with this idea that you have many more models that you've tried or that you've experimented

316
00:30:29,600 --> 00:30:36,320
with then have made it into production and some of that is just the natural process of experimentation

317
00:30:36,320 --> 00:30:43,440
but there are also issues that organizations run into when trying to get models into production

318
00:30:43,440 --> 00:30:49,040
you know can you speak to some of the you know the stories that you heard at the round tables and

319
00:30:49,040 --> 00:30:55,200
more broadly the issues that are preventing organizations from effectively fielding their models

320
00:30:55,680 --> 00:31:02,000
yeah absolutely so this is something which is essentially like so you know this as I said there's

321
00:31:02,000 --> 00:31:06,000
a few classes of customers we work with we work with organizations that are really early in their

322
00:31:06,000 --> 00:31:10,400
journey that are trying to build something build an initial practice out on how to use these

323
00:31:10,400 --> 00:31:16,400
technologies there are other organizations who've you know used our product for multiple years

324
00:31:16,400 --> 00:31:22,880
right who have teams of 50 people in that central team and then they have some a large number of

325
00:31:22,880 --> 00:31:27,440
users data scientists in the spoke teams right so we work with fairly large organizations as well

326
00:31:27,440 --> 00:31:33,200
who are pretty far along in their practice the feedback is pretty consistent that production

327
00:31:33,200 --> 00:31:38,080
is still something that's very very very important to them essentially right production ML all of

328
00:31:38,080 --> 00:31:43,200
these issues and there's a category there's a set there's sets of issues around first of all it's

329
00:31:43,200 --> 00:31:46,880
just metrics like what is the standard way in which you're going to publish and look at metrics

330
00:31:47,760 --> 00:31:52,960
there's a whole thing about monitoring how when you deploy this first of all how do you deploy it

331
00:31:52,960 --> 00:31:56,240
what's the platform in which you deploy something to production and then how do you monitor

332
00:31:56,960 --> 00:32:00,320
what's in production over time how do you scale it up and down elastically how do you

333
00:32:00,960 --> 00:32:06,800
do all of that stuff and then the third large thing is governance like if you are so so there's

334
00:32:06,800 --> 00:32:10,800
two challenges right the first challenge is getting your initial monitor production that's

335
00:32:10,800 --> 00:32:14,640
typically a really large challenge if you've not dealt with it if you've not solved those problems

336
00:32:14,640 --> 00:32:19,840
the second thing is one something in is in production how do you change it right because changing

337
00:32:19,840 --> 00:32:27,120
is super complicated it implies that you know how the your model is performing in production now

338
00:32:27,120 --> 00:32:31,920
and you have a theory that the modification is going to make it better like that's one

339
00:32:31,920 --> 00:32:36,000
so that means you have to have good measurement practices before and after and comparison

340
00:32:36,000 --> 00:32:41,680
second there's the entire workflow of this is not a one-off change typically changes could happen

341
00:32:41,680 --> 00:32:45,840
multiple times a week multiple times a day depending on the organization that you are in the type

342
00:32:45,840 --> 00:32:50,080
of model you're building so this is not a one-off problem where you get together and fix it this is

343
00:32:50,080 --> 00:32:56,320
complete like CICD versioning it's like standard software development practices that have to be

344
00:32:56,320 --> 00:33:00,720
applied here and you'll see many many data scientists who are just not familiar with software

345
00:33:00,720 --> 00:33:06,240
development practices at that level because that's not the kind of work that they have done prior and

346
00:33:06,240 --> 00:33:11,280
then there's the challenge of once everything is running in production how do you keep monitoring

347
00:33:11,280 --> 00:33:17,840
that over time how do you compare that to ground truth right how do you run all these metrics not

348
00:33:17,840 --> 00:33:22,880
just on an instant basis in terms of timeliness and how reasonable the predictions are but also

349
00:33:22,880 --> 00:33:27,040
compare to ground truth and understand if you're doing things overall better or worse things like

350
00:33:27,040 --> 00:33:31,920
that and if you've not solved these problems these are massive problems and these are things that

351
00:33:31,920 --> 00:33:36,000
you must solve through standardized tooling right organization wide it's pointless to have

352
00:33:36,640 --> 00:33:41,120
each of your business teams solve these separately it's it's pointless to not have a

353
00:33:41,920 --> 00:33:47,360
org-wide policy on how this stuff is done of course the cloudware product do it other products do it

354
00:33:47,360 --> 00:33:51,680
as well the more important thing from the organizational perspective is just to have a stance on

355
00:33:51,680 --> 00:33:56,880
these things so that your data scientists less spend less spend less time being frustrated about

356
00:33:56,880 --> 00:34:02,960
these practices and more time being productive and what your sense for how far along

357
00:34:02,960 --> 00:34:10,880
organizations are on that journey in Silicon Valley you know we've got as we alluded to a little

358
00:34:10,880 --> 00:34:16,480
earlier like very distinct roles there's the data scientists that's focused on one part of the

359
00:34:16,480 --> 00:34:22,800
process and you know to your earlier point the definitions you know very fairly widely but then

360
00:34:22,800 --> 00:34:28,160
you've got machine learning engineers that tend to be more familiar with concepts like CICD

361
00:34:28,160 --> 00:34:36,080
and DevOps and platform technologies and things like that my sense at least is that in more

362
00:34:36,080 --> 00:34:44,080
traditional organizations that those the distinction and those roles is evolving but yet at the same

363
00:34:44,080 --> 00:34:50,080
time a lot of these same organizations have gone through this process and you know considering

364
00:34:50,080 --> 00:34:55,200
like the evolution in the way they feel web applications there used to be just developers then

365
00:34:55,200 --> 00:35:02,800
they started to you know have platform teams and build out internal platforms and so I'm curious

366
00:35:02,800 --> 00:35:10,080
what you're you're seeing in terms of you know how mature our you know enterprises and along this

367
00:35:10,080 --> 00:35:17,120
in this journey and you know where they need to evolve yeah and as you said it very so much

368
00:35:17,760 --> 00:35:23,200
interestingly if you look at any of these organizations that you would think are doing these

369
00:35:23,200 --> 00:35:27,760
this well right that in the valley right these these orgs that have been doing it maybe

370
00:35:27,760 --> 00:35:33,520
five ten years and have fairly large teams at this point that deal with this what you will see is

371
00:35:33,520 --> 00:35:38,160
that they have their own internal tooling they built for a bunch of this because when they started

372
00:35:38,160 --> 00:35:43,440
out there there's really was not something that worked well for them in this area right so I guess

373
00:35:43,440 --> 00:35:50,080
what's challenging from a from a from a larger industry perspective is if you're not a crazy

374
00:35:50,080 --> 00:35:56,640
Silicon Valley tech company with like 300 data scientists that you've solved for by spending large

375
00:35:56,640 --> 00:36:01,520
amounts of treasure and building custom tooling I think the challenges of just finding that like

376
00:36:01,520 --> 00:36:06,080
figuring out what that process is that works for you at the scale that you're at and I think there's

377
00:36:06,080 --> 00:36:13,440
the other overlapping point here is that you're right of many of these organizations have

378
00:36:13,440 --> 00:36:17,280
standard ways in which they've done web app development and internal product development and

379
00:36:17,280 --> 00:36:22,160
stuff like that for themselves before so I think another challenge is just trying to understand

380
00:36:22,160 --> 00:36:27,680
what's different between that standard web app sort of development and product development versus

381
00:36:27,680 --> 00:36:35,440
like a MLAI model going into production because there are large substantial differences that are

382
00:36:35,440 --> 00:36:40,560
really important to internalize and understand so that you can focus on different aspects of it as

383
00:36:40,560 --> 00:36:46,560
well and as an art that's starting out with this if you're bringing up a team which is like you

384
00:36:46,560 --> 00:36:52,080
know five ten data scientists in size to begin with it's so hard to plan ahead for all of these

385
00:36:52,080 --> 00:36:57,760
issues before you know them or before you hit them and so I think I that is a lot of audience in

386
00:36:57,760 --> 00:37:03,360
terms of where people are with this yeah it's interesting in that it's different enough that you

387
00:37:03,360 --> 00:37:08,960
certainly can't you know take a snapshot of your you know DevOps process and apply it to machine

388
00:37:08,960 --> 00:37:15,200
learning and that leads folks to think that it's you know holy new and magic and it needs some

389
00:37:15,200 --> 00:37:20,000
totally new thing but yet there still is a lot that you can learn about the experience you had

390
00:37:20,000 --> 00:37:26,160
yes you know going through the past ten years of you know DevOps exactly exactly and I think

391
00:37:26,160 --> 00:37:30,000
that's like as you said like that's both good and bad like the good thing is you don't have to

392
00:37:30,000 --> 00:37:33,760
reinvent everything the bad thing is you might think you're already there and you don't understand

393
00:37:33,760 --> 00:37:41,200
the differences well enough and you know the it's it's it's it's also one of these areas in tech

394
00:37:41,200 --> 00:37:47,120
which unlike most of the areas in tech like you know you can have technologists that have been in

395
00:37:47,120 --> 00:37:54,080
the industry for twenty thirty years right and if you asked them about Docker and Kubernetes and

396
00:37:54,080 --> 00:37:59,120
a bunch of these areas that are fairly new right people are fairly familiar with them like it's fine

397
00:37:59,120 --> 00:38:04,640
I understand it it maps to something we've done before you know short we've used VMs for a long

398
00:38:04,640 --> 00:38:09,200
time now like Docker Kubernetes like you know I understand all the stuff right it's straightforward

399
00:38:09,840 --> 00:38:14,240
if you talk to most engineers who've been in around for twenty thirty years and you start talking

400
00:38:14,240 --> 00:38:20,240
what machine learning you will find there's a level of of you know of of discomfort right with

401
00:38:20,240 --> 00:38:26,160
that technology right people are not completely certain how it works what it does what it can actually

402
00:38:26,160 --> 00:38:32,400
do for them there's all these reports in the news about how tech is how ML and AI is used which always

403
00:38:32,400 --> 00:38:38,000
seem so far fetch like suddenly it's beating the best go player in the world right and and it's

404
00:38:38,000 --> 00:38:43,040
like it's it's totally on top of anything that's just later and then you look at what you're doing

405
00:38:43,040 --> 00:38:48,000
and you're looking at even dog fights in the air force there you go right it's it's yeah it's it's

406
00:38:48,000 --> 00:38:53,200
I don't know how that went but yes like even the possibility that is doing that is it's crazy yeah

407
00:38:53,840 --> 00:39:03,440
and and then you look at your organization internally and you're having challenges just

408
00:39:03,440 --> 00:39:08,880
getting a real-time feed of like sales over time right and just understanding like what happened

409
00:39:08,880 --> 00:39:13,360
yesterday versus today and what's different and where the outliers are and it seems so far away

410
00:39:13,360 --> 00:39:17,360
and so I think you see a lot of that discomfort reflected internally in organizations where

411
00:39:17,360 --> 00:39:22,080
people are just not sure how they should move forward and that just also slows down a little bit

412
00:39:22,080 --> 00:39:28,240
of the cadence on on how they can do it yeah one of the no sorry I was just going to say that as

413
00:39:28,240 --> 00:39:33,120
as like the leaders were telling us on the on that round table as well like their challenges

414
00:39:33,120 --> 00:39:37,600
just trying to map those expectations to the teams they're building like making sure everyone

415
00:39:37,600 --> 00:39:43,360
everything moves up and they can keep hiring and and and getting best practices in

416
00:39:43,360 --> 00:39:47,600
as the teams grow to what is the most size and it's just a set of one challenge after another

417
00:39:47,600 --> 00:39:52,560
but that's what makes it fun I guess yeah yeah yeah one of the things that I noticed in the

418
00:39:52,560 --> 00:40:01,600
conversations that we had at the round table is that the the folks that we spoke to were fairly

419
00:40:01,600 --> 00:40:09,040
mature on the the core data you know from a core data maturity perspective just kind of by

420
00:40:09,040 --> 00:40:17,600
selection and I'm wondering that may also be the case with you know in your experience with

421
00:40:17,600 --> 00:40:23,440
the folks that you talked to by virtue of the fact that you're at cladera and I'm curious

422
00:40:23,440 --> 00:40:30,800
your take on you know this idea that you know centralizing your data is a prerequisite to doing

423
00:40:30,800 --> 00:40:36,640
ML and events analytics at scale and you know what exactly that means how mature you you need to be

424
00:40:36,640 --> 00:40:42,960
you mentioned that folks still struggle with some fairly basic things that that is all a shifting

425
00:40:42,960 --> 00:40:51,200
landscape yeah any reflections on that yeah so as you said like certainly cladera works with

426
00:40:51,200 --> 00:40:56,800
the fortune 500 the fortune 2000 and and and these are organizations that have typically got their

427
00:40:56,800 --> 00:41:01,920
data story together over the last 20 years let's say right not just regular data where I was

428
00:41:01,920 --> 00:41:06,080
in kind of data but also they know how to integrate with these big data technologies how to store

429
00:41:06,080 --> 00:41:10,720
massive amounts of data how to secure analyze all of this stuff as well so they are they are

430
00:41:10,720 --> 00:41:16,560
pretty far along but I don't think I think even if you are a smaller company starting out or if

431
00:41:16,560 --> 00:41:24,080
you're you know SMB I think the patterns around data are reasonably well understood now people will

432
00:41:24,080 --> 00:41:29,200
still argue about one tech versus the other but there's a fairly well understood notion of you

433
00:41:29,200 --> 00:41:35,280
know like a data lake you know a warehouse where things are a lot more structured a transactional store

434
00:41:35,280 --> 00:41:41,360
which is maybe you know using like a key value store that that's that for much faster accesses

435
00:41:41,360 --> 00:41:48,800
and I think that high level thing is understood any in fact typically the data silos problem that

436
00:41:48,800 --> 00:41:54,960
you see tends to be in larger companies where the data teams are so disconnected from business

437
00:41:54,960 --> 00:42:00,000
that business feels compelled to to take on like some of that challenge themselves just so they

438
00:42:00,000 --> 00:42:04,800
can get more responsiveness but even then the business teams are working within the constraints

439
00:42:04,800 --> 00:42:08,560
of these well understood patterns right they're setting up their own little data mark somewhere

440
00:42:08,560 --> 00:42:13,920
on the side without talking to central IT but the patterns still are pretty well understood

441
00:42:13,920 --> 00:42:19,920
so I think in terms of the on getting their data story together I would say most organizations are

442
00:42:20,480 --> 00:42:26,720
are in reasonably even shape in terms of solving that that core problem it's of course it's a

443
00:42:26,720 --> 00:42:31,120
much harder challenge to solve for the larger companies which is why it's sort of admirable that

444
00:42:31,120 --> 00:42:36,400
they've done it over the last you know a couple of decades but that is reasonably in good shape

445
00:42:36,400 --> 00:42:41,680
the continuing challenge in both large companies and small companies is okay yes you have data

446
00:42:41,680 --> 00:42:47,680
but if you want to run a particular kind of analysis on it that data has to be transformed in some

447
00:42:47,680 --> 00:42:53,920
way first just to make things efficient and to make things manageable and as those use cases come

448
00:42:53,920 --> 00:43:00,080
in every single day from every single part of your company what happens over three four five years

449
00:43:00,080 --> 00:43:04,800
is you have you know the same data is now like stored in these different forms it's used for these

450
00:43:04,800 --> 00:43:10,400
different use cases as a massive ETL and pipelining thing that you're running to just make sure everything

451
00:43:10,400 --> 00:43:15,760
stays up to date and then this continuously there are new requirements where you to figure out does

452
00:43:15,760 --> 00:43:20,400
this have I already solved this somewhere and that's a hard problem to like that's a hard ask right

453
00:43:20,400 --> 00:43:25,040
because no one quite understands the catalog as well as they showed and things like that

454
00:43:25,040 --> 00:43:29,120
and then if I've solved this somewhere but there are some small changes neither how difficult it

455
00:43:29,120 --> 00:43:35,120
is to make that change so just change management structure management data management at that level

456
00:43:35,120 --> 00:43:39,280
I think is super hard for companies at all levels right it doesn't matter how large you are

457
00:43:39,280 --> 00:43:43,680
the requirements keep changing there's no you cannot have solved this problem completely it's just

458
00:43:43,680 --> 00:43:49,920
not possible yeah I'm just reflecting on you know many years ago talking to folks at you know

459
00:43:49,920 --> 00:43:56,160
cloud era horton works about you know data lakes when we were first starting to talk about this as

460
00:43:56,160 --> 00:44:01,120
an idea and how it was going to going to be the end of ETL we wouldn't need to do ETL anymore

461
00:44:01,120 --> 00:44:09,760
um I think companies are doing a lot more ETL to your point not less yes and uh you know machine

462
00:44:09,760 --> 00:44:15,280
learning is is just one more driver for that yeah it turns out like the data explosion was just step

463
00:44:15,280 --> 00:44:20,880
one step two was the use case explosion right and every use case brought like this massive numbers

464
00:44:20,880 --> 00:44:24,400
of changes that had to be made to how you store and manage this data so that the use case

465
00:44:24,400 --> 00:44:30,880
been more efficient and yeah so I think that's you're right like so ML AI is just another explosion

466
00:44:30,880 --> 00:44:36,400
in terms of like what's possible now and that leads to a bunch of changes you know and the challenges

467
00:44:36,400 --> 00:44:42,640
are so basic right like I was speaking to an organization um a couple of months back where they have

468
00:44:42,640 --> 00:44:48,240
a bunch they have these data scientists their challenge was like I asked them like what what is your

469
00:44:48,240 --> 00:44:52,560
hardware set up look like they describe the hardware set up to me I said like so where are your

470
00:44:52,560 --> 00:44:56,000
GPUs they're like we don't have GPUs I said why don't you have GPUs it makes no sense because

471
00:44:56,000 --> 00:44:59,680
they're it's really advanced they're doing a lot of really interesting work uh clearly they could

472
00:44:59,680 --> 00:45:08,400
use it and the response was like our IT um doesn't like GPUs are not on the approved purchase list

473
00:45:08,400 --> 00:45:15,040
by IIT right so it doesn't matter like I have asked multiple people you have gone through the channels

474
00:45:15,040 --> 00:45:20,160
like it is just not approved like there is no approved GPU that they can purchase and so they were

475
00:45:20,160 --> 00:45:25,600
really excited because I was also talking to them at the time about our hybrid cloud story and how

476
00:45:25,600 --> 00:45:29,600
we work on the cloud and on-prem and all these things and the the only question during the

477
00:45:29,600 --> 00:45:35,760
presentation was okay so in your cloud platform how can I prove it's a GPU and I showed them

478
00:45:35,760 --> 00:45:41,200
there's a slider right and and that's it and then they were like and then the second set of

479
00:45:41,200 --> 00:45:47,120
challenges of okay I have to convince my central data teams that it's okay to move to the cloud

480
00:45:47,120 --> 00:45:50,880
for some of this data that I need they were willing to take that on because they were like okay

481
00:45:50,880 --> 00:45:54,080
it solves all these other problems for us at least that we don't have a little bit right so

482
00:45:54,880 --> 00:46:00,160
I don't know it's that's going to be my question yeah right it's such an interesting world we

483
00:46:00,160 --> 00:46:04,000
live in right and things change all the time and and all these organizations have to try to move

484
00:46:04,000 --> 00:46:09,440
with it and and there's yeah it's it's fascinating yeah well just to wrap things up we started talking

485
00:46:09,440 --> 00:46:16,160
about use cases and then you brought up this very uh you know if we can call it a mundane challenge

486
00:46:16,160 --> 00:46:22,560
that organizations have to deal with and there is this tension uh yes you get this especially when

487
00:46:22,560 --> 00:46:29,760
talking to folks at the leadership levels of data and MLAI organizations between kind of these

488
00:46:30,960 --> 00:46:36,160
inspirational use cases that get folks excited and you know get them all their funding

489
00:46:37,200 --> 00:46:45,520
and the you know the mundane get on first base low hanging fruit that still abounds in many many

490
00:46:45,520 --> 00:46:54,080
organizations and I'm curious your you know perspective on how the organizations that you talk to

491
00:46:55,600 --> 00:47:01,760
you know manage the you know the tension between these two types of use cases and you know if there

492
00:47:01,760 --> 00:47:08,000
are any you know stories that you've heard about how folks have you know have moved forward given

493
00:47:08,000 --> 00:47:15,840
how that right so I think the the one thing I've seen that's um that's changed recently is that

494
00:47:15,840 --> 00:47:22,640
MLAI techniques are becoming a standard part of the typical business analysis business

495
00:47:22,640 --> 00:47:27,600
intelligence that organizations do as well so I guess what's happening is it's it's genuinely

496
00:47:27,600 --> 00:47:31,840
getting you know this is a very overuse term but those those requirements are getting a lot more

497
00:47:31,840 --> 00:47:36,160
democratized and a lot more people are thinking about how can I do more of this just because

498
00:47:36,160 --> 00:47:40,720
us the state that the technology is in so from an organizational perspective I think there are two

499
00:47:40,720 --> 00:47:47,360
things happening there continues to be this high level pie in the sky sort of thing off like

500
00:47:47,360 --> 00:47:53,760
here's this amazing thing that we can do right with AI and ML there's also this on the ground

501
00:47:53,760 --> 00:47:59,680
consistent movements towards more and more advanced analytics and more and more interesting

502
00:47:59,680 --> 00:48:05,360
insights and predictions that can be made from the data today that's also an ongoing movement

503
00:48:05,360 --> 00:48:11,200
I certainly think that that latter part is the way for most organizations to get started

504
00:48:11,200 --> 00:48:15,360
just be a little more grounded in where you are try to make that a little bit better try to add

505
00:48:15,360 --> 00:48:20,720
in these things that makes sense and then sure you can have the the visionary thought on on what

506
00:48:20,720 --> 00:48:26,240
else you can do and you can spend you know some of your cycles on that but it's important to just

507
00:48:26,240 --> 00:48:32,080
up level org wide uh what you can do with all of your data and and how you can understand your

508
00:48:32,080 --> 00:48:37,120
business by using all of these techniques rather than just you don't go for the one or two crazy

509
00:48:37,120 --> 00:48:42,320
ideas which you know which work as we know sometimes also so yeah it's it's impossible to say don't

510
00:48:42,320 --> 00:48:49,120
do them either so great great well so she was great catching up with you thanks so much for taking

511
00:48:49,120 --> 00:48:55,840
a time to reflect on this series of round tables with me yeah very nice talking to you as well

512
00:48:55,840 --> 00:49:07,760
Sam thanks for your time thank you

