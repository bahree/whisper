WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:36.040
Today we continue our train AI series with a conversation with Sarah Ayrney, director

00:36.040 --> 00:39.300
of data science at Salesforce Einstein.

00:39.300 --> 00:44.200
Sarah and I sat down at the train AI conference to discuss her talk notes from the field,

00:44.200 --> 00:48.680
the platform people and processes of Agile Data Science.

00:48.680 --> 00:52.920
Sarah and I dig into the concept of Agile Data Science, exploring what it means to her

00:52.920 --> 00:57.360
and how she's seen it done at Salesforce and other places she's worked.

00:57.360 --> 01:01.520
We also dig into the notion of machine learning platforms, which is a keen area of interest

01:01.520 --> 01:02.600
for me.

01:02.600 --> 01:06.640
We discuss some of the common elements we've seen in M.O. platforms and when it makes

01:06.640 --> 01:10.720
sense for an organization to start building one.

01:10.720 --> 01:14.600
As you may know by now, we're celebrating the second anniversary of the podcast this

01:14.600 --> 01:15.600
week.

01:15.600 --> 01:20.320
Have you found this podcast useful? If so, we want to hear how.

01:20.320 --> 01:26.480
Submit your written comments via the page at twimble.ai slash 2av or leave us a voicemail

01:26.480 --> 01:32.160
at 636-735-3658.

01:32.160 --> 01:35.840
We'll be sharing a few of your stories with your permission, of course, and a special

01:35.840 --> 01:39.240
podcast episode celebrating the Twimble community.

01:39.240 --> 01:45.560
Again, the second anniversary page is at twimble.ai slash 2av.

01:45.560 --> 01:49.960
Finally, a shout out to our friends over at Figure 8 for their sponsorship of this week's

01:49.960 --> 01:50.960
series.

01:50.960 --> 01:55.320
Figure 8 provides the essential human and aloop AI platform for data science and machine

01:55.320 --> 01:56.880
learning teams.

01:56.880 --> 02:01.520
The Figure 8 software platform trains, tests, and tunes machine learning models to make

02:01.520 --> 02:04.000
AI work in the real world.

02:04.000 --> 02:08.880
Learn more at www.figure-8.com.

02:08.880 --> 02:14.720
This episode was recorded live on site at the train AI conference, so there is some unavoidable

02:14.720 --> 02:21.720
background noise, and now on to the show.

02:21.720 --> 02:22.560
All right, everyone.

02:22.560 --> 02:27.640
I am at the train AI conference, and I've got the pleasure to be seated across from Sarah

02:27.640 --> 02:28.960
A. Ernie.

02:28.960 --> 02:32.080
Sarah is Director of Data Science with Salesforce Einstein.

02:32.080 --> 02:34.680
Sarah, welcome to this week in machine learning and AI.

02:34.680 --> 02:36.440
Thank you so much for having me.

02:36.440 --> 02:42.160
I'm really excited to jump into the topic of Agile Data Science, which is what you were

02:42.160 --> 02:46.240
speaking about here at the conference today, but before we do that, tell us a little bit

02:46.240 --> 02:51.320
about your background and how you got into ML and AI and data science.

02:51.320 --> 02:52.320
Absolutely.

02:52.320 --> 03:00.040
Actually, I started off way back in undergrad when I had the opportunity to work on my

03:00.040 --> 03:02.720
two passions, biology and computer science together.

03:02.720 --> 03:08.400
They opened up a major bioinformatics, and back then it was an ascent field, but I was

03:08.400 --> 03:13.640
able to do some research in cancer biology, kind of on leukemia and detecting patterns,

03:13.640 --> 03:21.200
and then spent some time doing remote research in Caltech on DNA binding and trying to understand

03:21.200 --> 03:25.560
if there were patterns we could recognize there, and while I didn't have a specific path

03:25.560 --> 03:31.440
and decided to go to grad school for biomedical informatics, kind of building on that.

03:31.440 --> 03:35.280
The lab that I ended up joining was a AI lab, although it was completely focused on

03:35.280 --> 03:40.000
bioinformatics, it still was generally part of that department, and so I got a little bit

03:40.000 --> 03:47.120
more exposure, and started doing things again related to DNA and genomics, but as any

03:47.120 --> 03:52.360
grad student at Stanford started company on the side.

03:52.360 --> 03:58.880
That's not cliche at all, but it was actually with a good purpose, at the time, actually

03:58.880 --> 04:04.320
Obama had announced the stimulus grants, and it was over one of these late-night conversations

04:04.320 --> 04:08.840
with professors that we realized there was this opportunity to support other researchers

04:08.840 --> 04:14.200
that didn't necessarily have all of the things required in order to complete and apply for

04:14.200 --> 04:18.400
these grants, where they might have really strong background in some biological portion,

04:18.400 --> 04:20.920
but didn't have the informatics side of it.

04:20.920 --> 04:24.600
So originally we thought, well, we would offer consulting services to them and kind of

04:24.600 --> 04:31.480
stick to our field, and turns out that space of consulting is a lot bigger, and our group

04:31.480 --> 04:33.880
ended up doing things well outside of that.

04:33.880 --> 04:39.680
So I did some work predicting what websites people would be interested in browsing, and

04:39.680 --> 04:44.240
kind of completely went off the rails from the biomedical background, but all related

04:44.240 --> 04:46.960
to the stimulus grants are totally a field.

04:46.960 --> 04:51.640
No, it was, it's almost as if you put out a press release on the topic, and it just

04:51.640 --> 04:56.480
snowball from there, and people kind of hear that you're available and around.

04:56.480 --> 05:01.600
So no, it ended up coming through other avenues, it was very interesting.

05:01.600 --> 05:04.880
There were a lot of things that originally started off really around grant writing,

05:04.880 --> 05:08.560
and helping startups that wanted to apply for these small business innovation research

05:08.560 --> 05:09.560
grants.

05:09.560 --> 05:10.560
SRS, yeah.

05:10.560 --> 05:11.560
So, yeah.

05:11.560 --> 05:12.560
So people...

05:12.560 --> 05:13.560
Been there.

05:13.560 --> 05:17.440
So it's interesting, because I think people apply to them when they hear about them,

05:17.440 --> 05:21.640
and if they're not from an academic background, don't necessarily know how to write these

05:21.640 --> 05:25.840
grants that will be received well by an academic audience, and so our experience on kind

05:25.840 --> 05:29.320
of academic grant writing was already very helpful.

05:29.320 --> 05:33.680
The informatics part was maybe only a portion of it, and then eventually it moved outside

05:33.680 --> 05:39.440
of the healthcare realm, and I realized I had just passion and data in general.

05:39.440 --> 05:44.280
And although when I was graduating, I still knew that I cared about healthcare and doing

05:44.280 --> 05:48.760
something in that space, so I was completely ready to let go of it.

05:48.760 --> 05:53.360
I wanted to leave it a little bit open, and so I joined a software company where I was

05:53.360 --> 05:56.880
actually more or less a data scientist for hire.

05:56.880 --> 06:04.880
I worked on the particular distributed database that the company Greenblum actually was selling,

06:04.880 --> 06:09.800
and working with my customers with the expertise in biomedical and chromatics, I'd be able

06:09.800 --> 06:13.000
to speak their language, but still really be doing raw data science.

06:13.000 --> 06:17.880
And a lot of the time, it wasn't necessarily in my area of expertise, but it was somebody

06:17.880 --> 06:19.880
who spoke their language.

06:19.880 --> 06:24.240
And there again, over time, I drifted from that background, started doing work on jet

06:24.240 --> 06:28.480
plane engines and banking fraud, and it was just all over the place.

06:28.480 --> 06:30.960
So that was great, and it was exciting.

06:30.960 --> 06:36.960
But what I really wanted to do was deploy models at crazy scale to my customers, and so

06:36.960 --> 06:44.080
Salesforce really offers that opportunity to build models in this really amazing platform

06:44.080 --> 06:49.400
that allows me to just reach a lot of people, so that they are empowered to use AI.

06:49.400 --> 06:53.480
And I can do the work around in innovative data science on the other end.

06:53.480 --> 07:01.000
Before we jump into the agile data science element, Einstein, and a lot of ways Einstein

07:01.000 --> 07:04.840
is starting to feel like Watson, for me, is like this umbrella brand that covers so many

07:04.840 --> 07:07.720
things, like in your words, what is Einstein?

07:07.720 --> 07:11.800
So the purpose of Einstein really is just to democratize AI.

07:11.800 --> 07:17.440
So Salesforce has many customers, and any customer, regardless of size and industry, should

07:17.440 --> 07:18.800
be able to harness that power.

07:18.800 --> 07:24.120
And so the goal is to allow our customers to use AI to have better interactions with

07:24.120 --> 07:25.920
our customers to make them more predictive.

07:25.920 --> 07:29.920
And so we have products, some packaged apps that I've worked on like predictive leads

07:29.920 --> 07:35.280
scoring that allow sales rep to understand which of the sales leads are likely to convert

07:35.280 --> 07:39.640
to an opportunity based on the data that they have that have historically converted.

07:39.640 --> 07:44.160
So while it might sound like a big umbrella, there are some very specific use cases for

07:44.160 --> 07:46.000
which we have packaged apps.

07:46.000 --> 07:49.800
And then there's a product that I'm currently working on the Einstein prediction builder,

07:49.800 --> 07:56.120
which is really intended to, again, democratize AI by allowing our admins to set up things

07:56.120 --> 08:01.320
that they want to predict, explicitly some prediction that we don't know they'll want,

08:01.320 --> 08:04.840
but they would like to have to augment what they're currently able to have their users

08:04.840 --> 08:05.840
do.

08:05.840 --> 08:15.080
So we have, you know, Salesforce admin, I do some kind of forecast, and I want to predict

08:15.080 --> 08:17.600
the close date on a deal or something like that.

08:17.600 --> 08:24.120
And I can train using my own historical data and maybe some of yours and so now it is

08:24.120 --> 08:25.120
absolutely.

08:25.120 --> 08:32.200
We are, we are very strict on kind of focusing on single tenant, being able to leverage

08:32.200 --> 08:33.200
its own data.

08:33.200 --> 08:38.640
So only my data, but the example is the kind of thing that I might be able to predict

08:38.640 --> 08:39.640
using this platform.

08:39.640 --> 08:40.640
Yeah.

08:40.640 --> 08:43.360
Well, we're moving forward on right now in prediction builder are binary classifications

08:43.360 --> 08:49.480
and regressions and the goal is to allow you to build these predictions and we're working

08:49.480 --> 08:55.920
on making the UI really intuitive and allow the admins to do this in a way that's making

08:55.920 --> 09:01.120
it easy to set up and then following on, allowing them to inject those results directly

09:01.120 --> 09:06.800
back because sort of going back to my talk a bit today, there's this part around AI

09:06.800 --> 09:11.400
and certainly when I was working at my previous job, where we spend a lot of time building

09:11.400 --> 09:15.640
models without thinking about how to get them in front of our customers in the end.

09:15.640 --> 09:18.880
And so I spent all this time building a model thinking it's really great and then I'm

09:18.880 --> 09:22.720
ready to push live to production and that doesn't go anywhere.

09:22.720 --> 09:26.960
It just kind of sits with me and that's what's amazing with Einstein is really that it's

09:26.960 --> 09:27.960
right there.

09:27.960 --> 09:32.360
You're going to be able to have it immediately available when it's done building.

09:32.360 --> 09:36.320
So agile data science, what does that mean for you?

09:36.320 --> 09:42.880
Yeah, well, my talk today focused around that concept and insisting that it's of course

09:42.880 --> 09:46.680
around people in process to make it possible, but there really is a platform underlying

09:46.680 --> 09:48.320
that enables it.

09:48.320 --> 09:55.000
So pivotal, my previous company, there is a practice pivotal labs that is all about agile

09:55.000 --> 09:56.400
and extreme programming.

09:56.400 --> 10:00.320
And when we were part of that team, it was really important for us to try and understand

10:00.320 --> 10:03.640
how to integrate into that process.

10:03.640 --> 10:09.120
And agile has been around, it's meant to be adaptive and to work, but there certainly

10:09.120 --> 10:13.480
is a mindset that comes with how to approach it, how to have sprints, how to have stories,

10:13.480 --> 10:15.720
how to be able to go live and test things.

10:15.720 --> 10:19.840
And it doesn't necessarily translate perfectly in data science.

10:19.840 --> 10:25.840
So what's interesting is when we think about for us trying to push our changes live, it's

10:25.840 --> 10:31.480
important to have a way to not feel once you've built a model that is just going nowhere.

10:31.480 --> 10:36.640
It needs to be connected, so really thinking MVP with what it means to actually add any

10:36.640 --> 10:38.040
model to a product.

10:38.040 --> 10:43.680
And that's exactly where I was focusing was a model that is part of a product.

10:43.680 --> 10:48.080
And then secondarily, making it possible to run experiments so that you're not moving

10:48.080 --> 10:53.680
off into a corner and doing it elsewhere and then figuring out how to translate it over.

10:53.680 --> 10:58.360
So really that you have frameworks in place and for Salesforce, we have this platform that

10:58.360 --> 11:04.240
makes it possible for data scientists to use services to get access to what they need

11:04.240 --> 11:12.200
and to share algorithms, to share future engineering steps, and to allow us to run experiments

11:12.200 --> 11:16.880
without having access to data, but with understanding how performance the results might be at

11:16.880 --> 11:18.560
the end.

11:18.560 --> 11:28.080
So setting aside what the customer might use, how do you kind of express agile internally

11:28.080 --> 11:31.800
and the way you do data science within Salesforce?

11:31.800 --> 11:37.560
Yeah, so the team that I run, the prediction builder team, we have multiple models in production,

11:37.560 --> 11:42.640
but we use one underlying code base that automates the entire future engineering process.

11:42.640 --> 11:44.880
So it's AutoML that we use.

11:44.880 --> 11:50.520
And the platform team as a whole is responsible for having services around, you know, safe

11:50.520 --> 11:55.000
access to data around making compute scalable and possible.

11:55.000 --> 11:59.560
And these elements around automated machine learning are really where we get to implement

11:59.560 --> 12:02.680
that agile methodology really cleanly.

12:02.680 --> 12:07.400
So when you have these shared repositories, what that means is if you have models in production

12:07.400 --> 12:12.440
and you can continuously monitor their performance, which is really, really critical, you're able

12:12.440 --> 12:17.560
to understand where there may be a problem, where models are not performant and be able

12:17.560 --> 12:21.320
to go ahead and iterate and come up with solutions to it.

12:21.320 --> 12:25.800
So some of the examples that I gave was, you know, you see a drop in AUROC and you decide

12:25.800 --> 12:29.160
that you actually want to do something about it and, you know, assuming you have access

12:29.160 --> 12:33.840
to the data, you can do a deep dive and understand what has happened.

12:33.840 --> 12:38.080
Understand if there are some underlying elements, maybe there are fields that are not being

12:38.080 --> 12:40.040
processed correctly.

12:40.040 --> 12:44.000
Salesforce has some very unique problems around leakage detection.

12:44.000 --> 12:49.240
Turns out that when you have a pull of data, that is from a moment where they're setting

12:49.240 --> 12:50.880
up the predictions.

12:50.880 --> 12:55.200
We don't know what came before a certain, yes, no event.

12:55.200 --> 13:00.240
So an easy example is if you were trying to predict churn and the customer has a field

13:00.240 --> 13:04.320
called Reason for Churn and they don't explicitly exclude it, we actually need to detect that

13:04.320 --> 13:10.520
problem because if we're now using Reason for Churn in our model, we'll have a problem.

13:10.520 --> 13:14.600
So those types of things would appear up front in a holdout set that we're doing extremely

13:14.600 --> 13:17.760
well, but over time you would see there's a degradation in performance because of course

13:17.760 --> 13:18.760
you're not doing a good job.

13:18.760 --> 13:23.640
So many of the new data sets that were in that scoring open set and so it's that kind

13:23.640 --> 13:28.360
of monitoring and identifying of problems to which we then have to create resolutions.

13:28.360 --> 13:34.360
So if it's introducing a new test to find leakers, if it's coming up with a new way of processing

13:34.360 --> 13:40.360
text, if it's, you know, at some point you can imagine wanting to use sentiment as just

13:40.360 --> 13:44.720
a feature that you engineer out of any text field that comes in.

13:44.720 --> 13:49.280
It's really having that shared repository being able to add that new feature.

13:49.280 --> 13:54.680
So taking the time, building a new way of engineering it, adding it on to that library or

13:54.680 --> 13:59.680
that service, and then iterating, that's sort of one portion.

13:59.680 --> 14:02.800
There's like the user story around implementation.

14:02.800 --> 14:07.120
There's also really that platform though around how do you now run an experiment?

14:07.120 --> 14:12.240
So if you have a new feature that you're adding, and in our case you can have so many

14:12.240 --> 14:17.520
customers that are building models using your underlying AutoML, you would need to understand

14:17.520 --> 14:21.760
what the impact is of adding that feature to any one of your customers.

14:21.760 --> 14:25.960
And so being able to do that in a way that doesn't require us to look at data, but allows

14:25.960 --> 14:29.200
us to run an experiment at scale to understand what the impact would be.

14:29.200 --> 14:33.480
Are there any regressions that happen for lack of a better term in our model performance

14:33.480 --> 14:35.160
across the board?

14:35.160 --> 14:42.440
So to make sure I understand that you've got these customers, customer data, it's all

14:42.440 --> 14:44.160
single tenant.

14:44.160 --> 14:47.960
It sounds like you're not testing against it, but you are.

14:47.960 --> 14:52.040
What exactly are, how exactly are you doing that or what are you doing?

14:52.040 --> 14:57.920
Our team is not able to do is so in general, we can't look at the data, so trust is our

14:57.920 --> 15:01.720
number one value across the board it sells for us, and so we cannot request access that

15:01.720 --> 15:02.720
is not our data.

15:02.720 --> 15:07.920
But can you deploy models to it that don't return the data and test statistically?

15:07.920 --> 15:11.160
Yeah, so you can understand it on aggregate level.

15:11.160 --> 15:12.160
What is your performance?

15:12.160 --> 15:14.360
Is there a change in performance?

15:14.360 --> 15:19.120
And we do have customers that we might pilot a product with where we can work and understand

15:19.120 --> 15:20.680
what's going on.

15:20.680 --> 15:25.000
They can request investigations, but it's not a system where we need to be able to go

15:25.000 --> 15:28.280
in and look at every single model.

15:28.280 --> 15:33.240
Similarly, although Salesforce is at such a massive scale, any company will have multiple

15:33.240 --> 15:35.160
models in production at some point.

15:35.160 --> 15:38.680
I don't believe any company sets out to have exactly one model.

15:38.680 --> 15:43.480
And it's unlikely that at some point, every model is going to have such a heavy data science

15:43.480 --> 15:48.440
component that rather than monitoring and alerting to a degradation performance, you would

15:48.440 --> 15:52.400
want to be able to just have multiple models in production and probably a small number

15:52.400 --> 15:55.720
of data scientists supporting those efforts and iterating.

15:55.720 --> 16:01.000
So having a platform where you have a shared set of services for engineering and for understanding

16:01.000 --> 16:05.840
that if you add a feature or if you change an algorithm or you decide to tune parameters

16:05.840 --> 16:10.240
in a certain way, understanding the impact across the board becomes really critical.

16:10.240 --> 16:14.160
And so having those monitoring services there, although it's really important for Salesforce

16:14.160 --> 16:17.160
is extremely important for any company that's out there.

16:17.160 --> 16:22.920
I'm envisioning, when you say platform, I'm envisioning something analogous to like Uber's

16:22.920 --> 16:28.080
published quite a bit on Michelangelo there, internal platform, and some other companies

16:28.080 --> 16:29.080
have.

16:29.080 --> 16:33.200
Netflix talks about their platform a lot too, it's really great.

16:33.200 --> 16:37.560
LinkedIn, I mean, a lot of companies have spent time, I think what's so unique about

16:37.560 --> 16:44.000
what Salesforce does is our platform is around running multiple models for our customers.

16:44.000 --> 16:49.120
And it's really around taking what all of these companies have done and the number of

16:49.120 --> 16:52.880
data scientists and engineers that go into building those platforms and making it accessible

16:52.880 --> 16:58.040
to our customers so that they can benefit from what it is that we have done and be able

16:58.040 --> 17:04.480
to access a few clicks instead and sort of know that you have that data science prowess

17:04.480 --> 17:08.440
behind it and people that are paying attention to constantly improving and making things

17:08.440 --> 17:10.440
better on your behalf.

17:10.440 --> 17:16.480
So the first of these elements is a repository and you've mentioned a couple of things that

17:16.480 --> 17:19.280
might be in that repository, models and features.

17:19.280 --> 17:20.680
What else is in that repository?

17:20.680 --> 17:26.720
So in general, and again, this will vary by companies that are building for their purposes.

17:26.720 --> 17:30.320
If it's, for example, a single application that you have, there's always going to be an

17:30.320 --> 17:32.080
element around data.

17:32.080 --> 17:33.080
So how is it there?

17:33.080 --> 17:34.080
You're storing data.

17:34.080 --> 17:35.080
How are you indexing it?

17:35.080 --> 17:36.080
How do you access it?

17:36.080 --> 17:39.120
Data scientists don't want to be paying attention to like what folder is this living in

17:39.120 --> 17:41.640
or what table or whatever the history is.

17:41.640 --> 17:46.760
So what we do is we build APIs to let us request data in a certain way so that it can be

17:46.760 --> 17:52.760
again automated, there exists a customer that wants to use the services and therefore being

17:52.760 --> 17:57.840
able to reference by APIs and running code against that is then possible.

17:57.840 --> 18:03.320
And although again, not everyone is operating at Salesforce scale, no data scientist necessarily

18:03.320 --> 18:07.200
wants to spend time thinking about how data moves in and out.

18:07.200 --> 18:12.080
But those services need to be there to make it possible and to make a possible index

18:12.080 --> 18:17.280
what's happening, look for drifts, understand how to monitor that scores are being pushed

18:17.280 --> 18:18.280
back.

18:18.280 --> 18:20.240
The scores have changed over time.

18:20.240 --> 18:25.880
All of these elements around data services, monitoring services and then actual modeling

18:25.880 --> 18:27.640
and feature engineering as well.

18:27.640 --> 18:30.360
So interestingly enough, this topic is timely for me.

18:30.360 --> 18:35.080
I just organized an event last week where we spent quite a bit of time talking about

18:35.080 --> 18:40.400
this notion of, you know, machine learning platforms and agile data science and things

18:40.400 --> 18:47.400
of that like and one of the questions that came up is, you know, for a company that is

18:47.400 --> 18:55.480
not necessarily on a platform like Salesforce, but is doing data science, you know, when

18:55.480 --> 19:00.840
do they start to collect some of this stuff into a platform?

19:00.840 --> 19:03.280
When does it make sense to build a platform?

19:03.280 --> 19:09.040
If you're a, you know, an internet company that's probably earlier than if you're a traditional

19:09.040 --> 19:14.400
enterprise, how do you know when it's time to start consolidating some of this functionality,

19:14.400 --> 19:19.040
building repositories, building, monitoring, framework, stuff like that?

19:19.040 --> 19:21.720
So monitoring is from the start.

19:21.720 --> 19:25.640
Like anytime you have an application, you're, you're going to monitor everything that's

19:25.640 --> 19:27.480
happening on those apps.

19:27.480 --> 19:31.360
So as soon as you're wanting to add a model to it, you have to have monitoring around

19:31.360 --> 19:32.360
your models.

19:32.360 --> 19:38.600
It's to treat a model any different from another feature of your app.

19:38.600 --> 19:43.360
And if does your, your, your data scientists is service because they want to know what's

19:43.360 --> 19:48.400
happening, not by always requesting access and moving over, they need to be alerted to

19:48.400 --> 19:51.160
an issue the same way anyone else would be.

19:51.160 --> 19:57.160
Also for allowing them to triage the problem more quickly, I understand that in general,

19:57.160 --> 20:05.080
there might be a question about waiting until you have multiple models maybe or some opportunities

20:05.080 --> 20:10.960
for sharing, but I argue and that was sort of one of my take homes on the talk today.

20:10.960 --> 20:15.440
Sort of my main point is to plan for multiple models always.

20:15.440 --> 20:21.160
I cannot imagine an organization that doesn't commit to having more than one model.

20:21.160 --> 20:26.080
And by doing so, really doing some upfront planning, certainly at my last job, there

20:26.080 --> 20:31.640
were many companies that were kind of in this position of having a lot of smaller organizations

20:31.640 --> 20:34.320
within that could have benefited from sharing, from understanding.

20:34.320 --> 20:37.400
And of course, there's always a question of investment.

20:37.400 --> 20:42.960
And I think it's a struggle when you don't invest time in the platform, I was just talking

20:42.960 --> 20:47.040
with several people today about the fact that there's an expectation that you should

20:47.040 --> 20:51.000
have good return on investment for your modeling and for the data scientists to hire and everything

20:51.000 --> 20:52.240
that's there.

20:52.240 --> 20:57.040
And of course with that, they want to have the normal agile processes in place of having

20:57.040 --> 21:00.520
waste assess the risk and how much longer and like, when am I going to see something

21:00.520 --> 21:06.600
from it? And as a data scientist, and certainly coming from graduate school, we think of it

21:06.600 --> 21:08.800
as like, oh, we're entering a research project.

21:08.800 --> 21:14.200
We have absolutely no idea if this will work and, you know, let us go work on it for three

21:14.200 --> 21:17.080
months, six months a year and we'll get back to you.

21:17.080 --> 21:23.840
And that may be one way to approach it, but inevitably that leads to all around frustration,

21:23.840 --> 21:26.960
both on kind of the business that is trying to get something.

21:26.960 --> 21:29.640
And also the data scientists, who then once they're ready, don't necessarily know how

21:29.640 --> 21:31.160
to go to production.

21:31.160 --> 21:37.480
And so if we really want a more agile approach as a whole, you must build platforms to support

21:37.480 --> 21:41.880
your data scientists' ability to iterate, ability to understand how well something is

21:41.880 --> 21:45.400
performing and ability to fail early.

21:45.400 --> 21:50.600
Because if we don't commit to having data pipelines, monitoring services, waste to deploy

21:50.600 --> 21:55.080
models from the start, we will inevitably find ourselves in the position of every single

21:55.080 --> 22:01.040
time starting over, rather than building for multiple apps from the beginning.

22:01.040 --> 22:10.560
Do you feel like data science is standardized enough across organizations and use cases

22:10.560 --> 22:12.280
that we can do that, right?

22:12.280 --> 22:18.520
Like a classic problem is, you know, building a platform before you're, you know, the problem

22:18.520 --> 22:20.920
that the platform is trying to solve, right?

22:20.920 --> 22:25.040
You build all kinds of features, oh, I need a, you know, model repository with versioning

22:25.040 --> 22:26.040
that does XYZ.

22:26.040 --> 22:30.560
This is what the API looks like, and then you go to actually build something and it doesn't,

22:30.560 --> 22:34.000
you know, it's like you're building the bridge from both sides and they don't meet in the

22:34.000 --> 22:35.000
middle.

22:35.000 --> 22:36.000
Oh, I love that.

22:36.000 --> 22:41.800
There's like a famous bridge between Germany and Switzerland that every time I drive by

22:41.800 --> 22:45.800
with my dad who lives there, he always talks about how the German side and the Swiss side

22:45.800 --> 22:47.600
agreed to build it.

22:47.600 --> 22:52.680
And when they came to me in the middle, they were off by a huge number because Switzerland

22:52.680 --> 22:58.560
measures from elevation from the Mediterranean and Germany from the North Sea.

22:58.560 --> 23:00.960
And that was the reason why that happened.

23:00.960 --> 23:02.560
It's like very funny.

23:02.560 --> 23:04.600
It's like starting a race at zero and one.

23:04.600 --> 23:08.360
No, there's like a, there are, you can find blogs about this on that.

23:08.360 --> 23:09.360
It's very funny.

23:09.360 --> 23:11.840
But I totally understand what you're saying.

23:11.840 --> 23:15.040
And absolutely at the outset, I, and I should have done the same here.

23:15.040 --> 23:17.560
I level set my talk with saying, look, there are different ways of doing this.

23:17.560 --> 23:18.560
Different types of models.

23:18.560 --> 23:24.800
And my examples are, there are models that are informing strategic decisions and my classic

23:24.800 --> 23:29.800
examples are from, you know, back in my biomedical informatics days, they're, you know, trying

23:29.800 --> 23:32.240
to identify the next drug target to go after.

23:32.240 --> 23:36.160
And so we're doing a bunch of data science to understand where to head next with our research.

23:36.160 --> 23:39.120
And they're kind of these others that are like doing automated decisions and they're

23:39.120 --> 23:42.920
ones that are like augmenting an app and maybe informing something.

23:42.920 --> 23:46.320
So like predictive leads going, we'll dislate, convert.

23:46.320 --> 23:50.560
And you're absolutely right, depending on what it is that you're building, you would want

23:50.560 --> 23:54.280
to focus on the different elements first.

23:54.280 --> 23:58.760
But across the board, there will always be things that make sense like how to bring your

23:58.760 --> 24:05.960
data in a way that's access controlled in a way that is also accessed via APIs.

24:05.960 --> 24:08.720
I cannot think of a reason to not want to do that.

24:08.720 --> 24:13.440
There's no reason to have to like, imagine what folder or table or scheme, it's a, it's

24:13.440 --> 24:15.120
painful to think of it that way.

24:15.120 --> 24:19.040
There's a corollary there that the data lake is not enough.

24:19.040 --> 24:21.360
Look, it's, it's all iterative.

24:21.360 --> 24:25.600
You know, we all started on a path together on a journey.

24:25.600 --> 24:31.560
And what I really hope and the hope of my talk was to share how we're, we're moving forward

24:31.560 --> 24:32.960
on this.

24:32.960 --> 24:36.720
And, and to learn from each other with it.

24:36.720 --> 24:45.000
So I, I know that again, during my consulting days where I was asked to do these projects

24:45.000 --> 24:49.160
and then kind of convince the organizations that what we had done was good enough.

24:49.160 --> 24:51.960
And now let's, let's figure out how to build an app.

24:51.960 --> 24:55.560
That was interesting because it was almost like the companies needed convincing that data

24:55.560 --> 24:58.280
science was going to do something good.

24:58.280 --> 25:02.800
And if we can all agree that there will be value in models, if only we accepted it, can't

25:02.800 --> 25:07.080
we start by saying that we're looking to build a use case into something.

25:07.080 --> 25:11.600
And let's come up with some, let's create a roadmap for it and then decide what to execute

25:11.600 --> 25:14.960
against first in an MVP mindset.

25:14.960 --> 25:20.320
And the other element is that that agile request that we have of data scientists does need

25:20.320 --> 25:24.320
to come with that commitment to support them and to support the differences between data

25:24.320 --> 25:26.800
scientists and software developers.

25:26.800 --> 25:31.160
They have a lot of things in common around wanting to monitor their models or their, their

25:31.160 --> 25:35.960
services around wanting to, you know, understand how to improve around trying out new technologies

25:35.960 --> 25:41.400
or algorithms, but they just have slightly different ways that those need to be solved.

25:41.400 --> 25:44.520
And platform commitment needs to be there to make that possible.

25:44.520 --> 25:46.320
Let's take it from a software development perspective.

25:46.320 --> 25:47.320
I'm a software engineer.

25:47.320 --> 25:51.600
I'm building some kind of component or service, microservice, whatever.

25:51.600 --> 25:54.160
It's part of agile, part of DevOps.

25:54.160 --> 25:57.160
I don't know, part of whatever you want to call it, like the current best practices.

25:57.160 --> 26:03.200
I'm also building the tests and responsible for the tests that, you know, would ultimately

26:03.200 --> 26:06.520
monitor the performance of my service, right?

26:06.520 --> 26:12.520
A big part of DevOps is I also own that service and production and I'm responsible for

26:12.520 --> 26:17.280
its upkeep, so it's in my, it's to my advantage to build the tests that will allow me to

26:17.280 --> 26:19.120
be able to do that.

26:19.120 --> 26:25.560
Data sciences is a bit different at most places, I think, because there's this bit of a,

26:25.560 --> 26:29.920
I don't know if you want to call it a wall, that may be too strong, but there's, you know,

26:29.920 --> 26:33.600
the person who's building the models isn't the person that's putting that in the model

26:33.600 --> 26:39.320
into production and is responsible for, they're responsible for different elements of it,

26:39.320 --> 26:40.320
right?

26:40.320 --> 26:44.280
Data sciences is responsible for the performance of that model over time.

26:44.280 --> 26:51.160
Are they also building those monitoring tests or is the engineering team that's implementing

26:51.160 --> 26:54.240
the model, building those overall monitoring tests?

26:54.240 --> 26:56.040
Like, where does that sit?

26:56.040 --> 27:02.880
Those, I think those are hard questions to answer as a whole, I think, in the abstract

27:02.880 --> 27:03.880
meaning.

27:03.880 --> 27:04.880
Yeah.

27:04.880 --> 27:05.880
Yeah.

27:05.880 --> 27:06.880
So that's elsewhere for what is that?

27:06.880 --> 27:12.360
Well, so I think one thing is to consider that I guess this, this concept of like completely

27:12.360 --> 27:14.680
isolated teams is, is difficult.

27:14.680 --> 27:19.760
I think it's important for data science and engineering to sit extremely closely in,

27:19.760 --> 27:25.040
in prediction builder, for example, we have a very hybrid team, we have, we have a designer,

27:25.040 --> 27:31.680
we have front end, we have data scientists, and our data scientists are very good engineers

27:31.680 --> 27:38.840
as well, in the sense that they've learned how to write code or at least work on these

27:38.840 --> 27:42.560
libraries that are able to run in production.

27:42.560 --> 27:50.320
And part of that is because of the way we have built that platform, we can run that code

27:50.320 --> 27:54.080
and run experiments using that code and make changes.

27:54.080 --> 27:57.680
And if you just think about it as adding a component in and not necessarily what you're

27:57.680 --> 28:02.880
describing, which is kind of completely offline building and then, okay, it looks good on

28:02.880 --> 28:06.880
the sample data and then what you're saying throwing it over the wall, but instead having

28:06.880 --> 28:11.440
the ability to run it in those same environments and make sure that it's performant.

28:11.440 --> 28:15.480
And of course, having a team that's dedicated to understanding, you know, scalability concerns

28:15.480 --> 28:21.360
of those exist, to make sure that those things are all very intertwined and that monitoring

28:21.360 --> 28:25.640
happens in the same way that we have the same alerts for, you know, SLAs are not met,

28:25.640 --> 28:29.920
well, your SLAs are not met for models aren't running fast enough, they're not reaching

28:29.920 --> 28:35.240
completion fast enough, you're not, you know, pushing back scores, you've seen a drop

28:35.240 --> 28:38.680
in the number of scores, you've seen your valuations have changed.

28:38.680 --> 28:44.840
I mean, you can kind of hijack some of the same tools, or you can build your own, but

28:44.840 --> 28:50.960
I would suggest looking at how suitable existing infrastructure is and existing tools that

28:50.960 --> 28:56.160
exist for the rest of your app and seeing how many of those could also now support data

28:56.160 --> 28:57.920
scientists as well.

28:57.920 --> 29:04.320
I think there is a desire across the board as well to integrate these teams into stop

29:04.320 --> 29:08.960
treating them as completely isolated and I really think part of it is by introducing them

29:08.960 --> 29:13.720
to that same culture, the same agile processes, then maybe they wouldn't feel the need to

29:13.720 --> 29:19.800
feel so separate as well, but you really do need to support them in that change.

29:19.800 --> 29:25.080
And I think that's maybe an interesting segue to people in process, like we've talked

29:25.080 --> 29:29.360
a lot about technology, that's only one leg of the stool, so to speak, you've certainly

29:29.360 --> 29:36.920
touched on people in process throughout, but did you have a specific message in your talk

29:36.920 --> 29:38.880
around the people aspect of all this?

29:38.880 --> 29:44.280
Yeah, so some of it was really around my own journey from starting an extremely research

29:44.280 --> 29:48.480
oriented world, and to be honest, back in grad school, I wish that somebody had introduced

29:48.480 --> 29:54.920
agile to me, I would have benefited from really not spending time on obsessively thinking

29:54.920 --> 30:01.280
about one particular problem and instead creating an MVP and iterating, and probably would

30:01.280 --> 30:06.560
have also helped in terms of when do I publish a paper if you think of that as your quote

30:06.560 --> 30:08.280
unquote release.

30:08.280 --> 30:14.800
So it was around that journey on acknowledging the similarities that I have with developers

30:14.800 --> 30:22.560
around the needs that I have, how they are served by either a process or by a monitoring

30:22.560 --> 30:29.520
tool or tests that they write, and how those aren't identical for me, but there are things

30:29.520 --> 30:34.200
that I want that, again, bring the platform in, if I could only modify those elements

30:34.200 --> 30:39.240
I might be able to get, but also a little bit around what it means to have a user story

30:39.240 --> 30:47.760
for a data scientist, so sort of acknowledging my asks and my desire to want to push live,

30:47.760 --> 30:52.360
thinking then next on the process side, given that there's a platform in place, how can

30:52.360 --> 30:57.720
I now go through identifying opportunities where the team focuses on things that are

30:57.720 --> 31:00.240
near-term and things that are long-term.

31:00.240 --> 31:05.920
So we have a backlog of things that we want to achieve, we have investigations that are

31:05.920 --> 31:10.320
ongoing, and those investigations follow a certain pattern, which is identify a source

31:10.320 --> 31:15.440
cause, and from that, create user stories around solutions that you want to implement,

31:15.440 --> 31:21.640
and those are sometimes very, very tangible, very easy, tuning a hyper parameter, trying

31:21.640 --> 31:26.880
to understand if you could add a new feature, maybe there's a bug, something I give an example

31:26.880 --> 31:31.800
of like a text field that's blank, should be treated as a null, something very explicit

31:31.800 --> 31:37.000
like that, very tangible, and you can have this running backlog of those items, but we

31:37.000 --> 31:43.400
also acknowledge that at any stage there could be a lot of open opportunities.

31:43.400 --> 31:48.320
Maybe you want to try new word embeddings, or you want to try a segmented model, and

31:48.320 --> 31:54.520
those are larger problems that require longer-term efforts, and to kind of balance out those

31:54.520 --> 32:00.480
near-term data science focused and require data science chops to identify the issues against

32:00.480 --> 32:04.880
the longer-term or innovative opportunities, we have architects on the team that are

32:04.880 --> 32:10.840
able to take on those bigger meteor problems to understand how do we tackle using something

32:10.840 --> 32:15.880
completely different, and choose when to promote that into something that will push into

32:15.880 --> 32:16.880
production.

32:16.880 --> 32:21.120
So you do need to have a large enough team to support both sides of that.

32:21.120 --> 32:27.840
And so is this, are you identifying a role that's specifically a data science architect,

32:27.840 --> 32:32.880
as opposed to like a traditional engineering architect, or developer architect?

32:32.880 --> 32:37.920
So yeah, I guess the role of architect is very specific, which is somebody who's at the

32:37.920 --> 32:44.520
level of solving these meteor problems, not necessarily the architect that you're mentioning,

32:44.520 --> 32:50.240
but they do have a solid understanding and ability to work and communicate with a engineering

32:50.240 --> 32:51.640
architect.

32:51.640 --> 32:59.920
So yeah, is this architect on the data science side, is that even the right way to say it,

32:59.920 --> 33:05.560
is someone that's come up through data science, what I've not heard much of the notion

33:05.560 --> 33:09.320
of a data science architect, if that's what we're even calling this?

33:09.320 --> 33:13.560
Yes, I mean, that's sort of the term we use, and maybe that's too specific to us, but

33:13.560 --> 33:20.360
it is the folks that serve that role are individuals that have strong machine learning chops and

33:20.360 --> 33:24.960
strong engineering chops to the sense that they can understand, you know, where something

33:24.960 --> 33:29.240
would fail, where there might be a problem with scalability, where there might be a whole

33:29.240 --> 33:32.120
new paradigm that needs to be introduced.

33:32.120 --> 33:37.400
And at times, folks need to come together and learn, but there are people that are essentially

33:37.400 --> 33:42.840
given the space and time to be able to go after these other problems, as opposed to the ones

33:42.840 --> 33:45.520
that are more focused on the MVP near term.

33:45.520 --> 33:50.840
That we have kind of our productivity zone and our incubation zone, and yeah, a team

33:50.840 --> 33:53.280
needs to be large enough to support it, but it's important.

33:53.280 --> 33:54.280
Makes perfect.

33:54.280 --> 34:01.240
Yeah, and how about on the process side, are there observations that you've made from

34:01.240 --> 34:07.760
a process perspective that makes this all more efficient, more smoothly flowing?

34:07.760 --> 34:14.160
Yeah, aside from actually using, you know, your traditional backlog and stories, what

34:14.160 --> 34:20.560
we do that, I think, again, borrowing these techniques from elsewhere, really around

34:20.560 --> 34:25.040
doing prioritizations around, you know, what is the value of going after a certain element

34:25.040 --> 34:29.440
versus an ease of implementation to allow us to triage where to spend our time?

34:29.440 --> 34:31.360
And value is something really hard to assess.

34:31.360 --> 34:34.440
How do I know what the value is going to be of introducing a new algorithm?

34:34.440 --> 34:38.200
How do I know what the value is going to be of, you know, maybe going after a new way

34:38.200 --> 34:44.960
to process texts or engineering features, what I can do, or, you know, something like

34:44.960 --> 34:50.800
a segmented model, what I can do is look at my models and how they're currently failing,

34:50.800 --> 34:55.520
what, you know, are there a lot of, a lot more text fields in one of my models, and therefore

34:55.520 --> 34:59.720
I believe that if I make a change, it would have a lot of impact.

34:59.720 --> 35:04.520
And sort of measuring where I believe there would be a big shift as one way to prioritize

35:04.520 --> 35:09.360
my backlog, and again, around the ease of implementation really focusing around what

35:09.360 --> 35:16.480
will be a longer term and more risky element that I need to work on, but I won't have

35:16.480 --> 35:19.920
the same kind of immediate need to shift.

35:19.920 --> 35:27.880
Have you made any attempts to, you know, taking all these, you know, the technology approach,

35:27.880 --> 35:30.720
platform approach, the people approach, the process approach?

35:30.720 --> 35:38.160
Have you made any attempts to try to characterize the net advantage over not doing these things?

35:38.160 --> 35:43.480
I mean, I'm imagining they've all evolved organically, and so the answer is probably

35:43.480 --> 35:50.160
no, but you know, certainly there's some degree of overhead associated with these different

35:50.160 --> 35:51.160
steps.

35:51.160 --> 35:54.280
How do you continue to justify it?

35:54.280 --> 36:00.360
Yeah, so the most obvious, I guess, metric that you could use is how quickly your models

36:00.360 --> 36:02.960
are improving over time, and then how quickly you're able to ship.

36:02.960 --> 36:08.760
So I know at previous companies, there might be like the actual how often we have new releases

36:08.760 --> 36:09.840
that could be used.

36:09.840 --> 36:12.840
What's interesting is in data science, of course, it's a little bit challenging because if

36:12.840 --> 36:15.760
I release a new model, that could impact my end customers.

36:15.760 --> 36:21.520
So, but what we can do is understand how much improvement we're getting on in performance,

36:21.520 --> 36:25.600
because we do have models and we do have summary stats on how they're performing and we can

36:25.600 --> 36:28.840
see how they've changed over time.

36:28.840 --> 36:33.480
And of course, it's a bit tricky when you're in an early phase or in a company that has

36:33.480 --> 36:38.320
a smaller number of models in production, but if you can see how quickly you're making

36:38.320 --> 36:43.280
progress or improving outcomes or, you know, in the instance of something more explicit

36:43.280 --> 36:48.920
like getting customer feedback, you know, we might have like a time to resolution that

36:48.920 --> 36:52.680
their improvements there, that those are metrics that you can capture at the end of your

36:52.680 --> 36:57.760
sort of unwilling to say, like, you know, how quickly am I releasing as you're a measure?

36:57.760 --> 37:01.440
Any other topics that you touched on in your talk that we haven't covered yet?

37:01.440 --> 37:09.360
Well, I think overall, at Salesforce, we have this really cool AutoML pipeline that we've

37:09.360 --> 37:15.840
been developing that really focuses around future engineering and, you know, automating

37:15.840 --> 37:18.200
the process of selecting models.

37:18.200 --> 37:24.920
And I know that I think that term AutoML is something that isn't always so well received

37:24.920 --> 37:29.880
by data scientists as a whole, I think I've noticed that at times there's this question

37:29.880 --> 37:34.520
around, oh, is like this automation way of data scientists, essentially.

37:34.520 --> 37:36.320
And it actually isn't at all.

37:36.320 --> 37:41.560
And the reason why I think it's such an important concept for everyone to introduce is that what

37:41.560 --> 37:47.560
AutoML really represents is harnessing your data scientists, repeated future engineering

37:47.560 --> 37:52.200
techniques, repeated analyses, finding opportunities to do these things.

37:52.200 --> 37:56.360
And instead of having everybody come up with a new, you know, like, stop word removal,

37:56.360 --> 38:01.120
or language class virus sentiment, now share those.

38:01.120 --> 38:07.360
Work together on saying, I would like to use, you know, this shared library and let's

38:07.360 --> 38:12.040
instead focus together on how to automate that process and identifying ways to, you know,

38:12.040 --> 38:16.360
produce more features and instead focus on the fun, hairy problems, like, what's the

38:16.360 --> 38:19.440
next set of features that I want to engineer?

38:19.440 --> 38:21.320
So it's not really such a scary space.

38:21.320 --> 38:26.840
It's actually very empowering, I feel, for data scientists to consider what they can contribute

38:26.840 --> 38:27.840
there.

38:27.840 --> 38:28.840
It's been a great chat.

38:28.840 --> 38:29.840
Thank you so much.

38:29.840 --> 38:31.800
Anything else you'd like to mention before we close out?

38:31.800 --> 38:34.840
No, I'm so appreciative of this opportunity.

38:34.840 --> 38:35.840
Thank you so much.

38:35.840 --> 38:36.840
All right.

38:36.840 --> 38:37.840
Thank you.

38:37.840 --> 38:43.440
All right, everyone, that's our show for today.

38:43.440 --> 38:48.640
For more information on Sarah or any of the topics covered in this episode, head over

38:48.640 --> 38:53.760
to twimmel.ai slash talk slash 143.

38:53.760 --> 38:57.000
Thanks again to figure eight for their sponsorship of this episode.

38:57.000 --> 39:04.440
To follow along with the train AI series, visit twimmel.ai slash train AI 2018.

39:04.440 --> 39:09.720
And finally, show us some love for the podcast's second anniversary and share how it's been

39:09.720 --> 39:15.600
helpful to you over at twimmel.ai slash 2 AV.

39:15.600 --> 39:21.240
Thank you so much for listening and catch you next time.

