WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twomel AI Podcast.

00:13.400 --> 00:21.320
I'm your host Sam Charrington.

00:21.320 --> 00:26.960
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded

00:26.960 --> 00:31.800
earlier this month at the 33rd annual NURRIPS conference.

00:31.800 --> 00:35.920
If you've been waiting for the Twomel pendulum to swing from workflow and deployment back

00:35.920 --> 00:39.920
over to AI and ML research, this is your time.

00:39.920 --> 00:45.200
We've got some great interviews in store for you over the upcoming weeks.

00:45.200 --> 00:49.280
Before we move on, I want to send a huge thanks to our friends at Shell for their support

00:49.280 --> 00:53.840
of the podcast and their sponsorship of this NURRIPS series.

00:53.840 --> 00:58.600
Shell has been an early adopter of a wide variety of AI technologies to support use cases

00:58.600 --> 01:04.920
across retail, trading, new energies, refineries, exploration, and many more, and is doing

01:04.920 --> 01:08.680
some really interesting things, but don't take it from me.

01:08.680 --> 01:14.720
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.

01:14.720 --> 01:19.240
They have a very deliberate strategy of using AI right across their operation from the

01:19.240 --> 01:22.800
drilling operations to safety.

01:22.800 --> 01:27.800
Last year, the company established the Shell.AI Residency Program, a two-year full-time

01:27.800 --> 01:32.680
program, which allows data scientists and AI engineers to gain experience working on

01:32.680 --> 01:37.080
a variety of AI projects across all Shell businesses.

01:37.080 --> 01:40.960
If you're in a position to take advantage of an opportunity like this, I'd encourage

01:40.960 --> 01:46.560
you to hit pause now and head over to Shell.AI to learn more.

01:46.560 --> 01:55.560
Once again, that's Shell.AI, and now on to the show.

01:55.560 --> 01:56.560
All right, everyone.

01:56.560 --> 02:02.240
I am here in Vancouver, British Columbia at NURRIPS, and I've got the pleasure of being

02:02.240 --> 02:04.520
seated with Mohamed Siddhamid.

02:04.520 --> 02:08.840
Mohamed is a machine learning and AI R&D manager at Shell.

02:08.840 --> 02:11.280
Mohamed, welcome to the Trumbo AI platform.

02:11.280 --> 02:12.280
Thank you, Sam.

02:12.280 --> 02:14.280
It's really excited to be here today.

02:14.280 --> 02:19.400
Chile, Vancouver, we're very delighted to be in Europe this year.

02:19.400 --> 02:20.760
It's a pleasure meeting you.

02:20.760 --> 02:25.600
It's also an exciting time to see all the latest and greatest in the Culling Edge technology

02:25.600 --> 02:26.600
in this field.

02:26.600 --> 02:27.840
Yeah, absolutely.

02:27.840 --> 02:30.760
I am also very excited to be here.

02:30.760 --> 02:35.000
This is my third NURRIPS, and in fact, I love Vancouver.

02:35.000 --> 02:41.640
The Vancouver Convention Center is, I can't think of one that is more beautiful and in

02:41.640 --> 02:46.560
his scenic context, so I'm really excited to be here.

02:46.560 --> 02:47.560
And happy to see you.

02:47.560 --> 02:53.880
Again, you, of course, were at Tomlcon AI Platforms, and it's wonderful to get a chance

02:53.880 --> 02:59.800
to actually get you on a microphone and get a sense for what you're up to at Shell.

02:59.800 --> 03:02.800
Shell actually has a couple of papers here at the conference, right?

03:02.800 --> 03:03.800
We do.

03:03.800 --> 03:08.520
Actually, we have a couple of conference papers being presented this year.

03:08.520 --> 03:15.680
We have one actually deal with least squares for seismic interpretation and the other papers,

03:15.680 --> 03:18.120
which is basically, we call it the facies net.

03:18.120 --> 03:23.560
This is a new architecture we designed for our kind of well-liked classifications and

03:23.560 --> 03:24.560
rock typing.

03:24.560 --> 03:29.320
So it's part of the geophysics community and how we are applying machine learning for

03:29.320 --> 03:30.320
physical systems.

03:30.320 --> 03:31.320
Awesome.

03:31.320 --> 03:32.320
Awesome.

03:32.320 --> 03:37.200
And so for this conversation, we're going to dive into the facies net paper.

03:37.200 --> 03:40.920
But before we do that, the least squares paper, tell me a little bit more about that one.

03:40.920 --> 03:46.820
That is actually our kind of internal R&D approach for how we use machine learning and deep

03:46.820 --> 03:52.360
learning specifically to actually generate an equivalent kind of workflow processes to

03:52.360 --> 03:56.160
the physics-based approach for full-wave inversion.

03:56.160 --> 04:02.920
If you think about how the geophysicist historically been doing the all imaging using seismic

04:02.920 --> 04:10.080
data to try to identify different types of zonal interest areas, so we actually identified

04:10.080 --> 04:15.360
a huge breakthrough in terms of time-saving, computational simplification relative to the

04:15.360 --> 04:17.480
conventional approach.

04:17.480 --> 04:20.800
So you mentioned phase inversion.

04:20.800 --> 04:22.400
What is that specifically?

04:22.400 --> 04:28.720
That's typically like when people actually have seismic surveys to measure what is the

04:28.720 --> 04:34.920
prospects of finding hydrocarbon on the subsurface, there is an imaging process that actually

04:34.920 --> 04:40.240
generate seismic images from this kind of seismic data.

04:40.240 --> 04:45.520
And this is a process of taking all this data and trying to generate a high-resolution image

04:45.520 --> 04:50.280
where you can pinpoint where is your kind of area of interest look like.

04:50.280 --> 04:53.200
Historically, it's been a very computationally intensive.

04:53.200 --> 04:59.520
This is required almost a month amount of effort, and now a traditional, high-performance

04:59.520 --> 05:02.560
computing has been applied to these types of problems.

05:02.560 --> 05:07.800
And that's quite frankly the highest investments in the energy business, like it's not very

05:07.800 --> 05:15.600
common to see a data center with almost 20, 30 beta-flops of computation cycles in a data

05:15.600 --> 05:18.200
center in an oil and gas company.

05:18.200 --> 05:24.960
And that's actually one of the things we rely on, even for training our deep reinforcement

05:24.960 --> 05:28.320
learning models for this type of applications.

05:28.320 --> 05:32.840
I don't think we got to the inversion, the phase inversion, where does phase inversion

05:32.840 --> 05:34.720
fit into that scenario?

05:34.720 --> 05:42.360
So this is typically when we take the migration of waves into the subsurface of Earth and

05:42.360 --> 05:47.440
trying to reconstruct an image based on these kind of reflectivity of sound waves that

05:47.440 --> 05:49.720
goes to the penetrate the subsurface.

05:49.720 --> 05:55.800
So basically you reconstruct the whole image based on these reflectivity of sound waves.

05:55.800 --> 05:56.800
Oh, got it.

05:56.800 --> 06:03.000
And so this paper, then, with all that as context, is showing what?

06:03.000 --> 06:09.920
Basically, it's showing how we can computationally efficiently reconstruct the whole problem using

06:09.920 --> 06:11.800
a deep learning framework approach.

06:11.800 --> 06:18.960
So relying on sequence models to be able to generate these type of image construction and

06:18.960 --> 06:26.480
using image recognition to be able to classify what the type of interesting zone of areas

06:26.480 --> 06:27.480
within these images.

06:27.480 --> 06:28.480
Oh, nice.

06:28.480 --> 06:29.480
Nice.

06:29.480 --> 06:34.080
And you mentioned reinforcement learning is reinforcement learning a part of this particular

06:34.080 --> 06:37.360
problem, or something that's used more broadly or in other areas.

06:37.360 --> 06:42.920
So actually, we're using it in different areas, typically one of the issues regarding the

06:42.920 --> 06:46.680
space that we work on is the limited of label data.

06:46.680 --> 06:52.880
So it's a very high and certain environment, and typically to be able to confound it in

06:52.880 --> 06:56.880
a well-defined simulation is not applicable all the time.

06:56.880 --> 07:02.880
So we're trying to complement that with a novel approach of defining what is the physical

07:02.880 --> 07:07.560
boundaries of the system look like in order to be able to use reinforcement learning.

07:07.560 --> 07:08.560
Okay.

07:08.560 --> 07:09.560
Interesting.

07:09.560 --> 07:10.560
Interesting.

07:10.560 --> 07:16.360
And so the other paper that is presented here, the FacesNet paper, that's one that you

07:16.360 --> 07:17.360
worked on personally.

07:17.360 --> 07:18.360
Indeed.

07:18.360 --> 07:21.040
Yeah, this is actually a very exciting paper.

07:21.040 --> 07:26.720
We had our kind of whole team, including one of our summer intern from Stanford, the

07:26.720 --> 07:31.360
earth she worked on this paper extensively with our extended team.

07:31.360 --> 07:35.840
And we're very excited actually to come up with a very novel way of designing a new architecture

07:35.840 --> 07:42.960
for how we use sequence model and recurrent neural networks to be able to basically break

07:42.960 --> 07:48.880
out the benchmark, the best in practice standard in the field for how you basically classify

07:48.880 --> 07:51.360
these different type of rock faces.

07:51.360 --> 07:52.360
Okay.

07:52.360 --> 07:58.040
So this paper, like the one we discussed a second ago, is also kind of focus on this exploration

07:58.040 --> 07:59.040
problem.

07:59.040 --> 08:00.040
Indeed.

08:00.040 --> 08:01.040
That's correct.

08:01.040 --> 08:06.400
So you think about the whole life cycle of how quickly you can maturate your hydrocarbon

08:06.400 --> 08:07.560
discovery.

08:07.560 --> 08:12.280
This is typically several months could be 9 to 12 month timeframe.

08:12.280 --> 08:17.120
And by introducing how we can apply machine learning to this type of a problem, we're actually

08:17.120 --> 08:21.680
aiming to reduce that what we call cycle time by almost 50%.

08:21.680 --> 08:26.600
That's why we try trying to tackle the problem by identifying these bottleneck areas, which

08:26.600 --> 08:32.360
really is a computational intensive or require massive amount of data and human effort to

08:32.360 --> 08:37.760
be able to use human judgment to classify and make some sort of an inference.

08:37.760 --> 08:43.480
And so this paper, you're proposing a new network architecture here or?

08:43.480 --> 08:49.120
We actually proposing a new architecture by combining two different type of approaches.

08:49.120 --> 08:55.520
We're using actually a sequence model to be able to actually detect what is the sequence

08:55.520 --> 08:57.680
of these different phases type.

08:57.680 --> 09:02.920
And for people who might not be familiar with that, typically on the on the subsurface

09:02.920 --> 09:08.320
of earth, they are kind of an alternation of sequence between, for example, clean sand,

09:08.320 --> 09:14.600
it could be a clay, a dirty sand, and different types of phases type or lithology types that

09:14.600 --> 09:16.280
usually we penetrate.

09:16.280 --> 09:21.960
And you can think of this as typically a way of identifying where is the area of interest

09:21.960 --> 09:25.200
that you would like to target in your targeting zone.

09:25.200 --> 09:33.000
And the images that are forming your input data are, I'm envisioning them as cross sections,

09:33.000 --> 09:41.640
I guess, that are or a stack of these different types of phases that appear in a sequence.

09:41.640 --> 09:46.880
And you're trying to determine or be able to predict the different layers.

09:46.880 --> 09:51.600
The novel way actually we deal with this, typically the features are representing geological

09:51.600 --> 09:56.560
features, this could be something like a gamma ray, neutron prosity, permeability, and

09:56.560 --> 09:57.560
so forth.

09:57.560 --> 10:02.120
But what we actually discovered that for the conventional method that trying to use machine

10:02.120 --> 10:06.720
landing to solve this type of a problem, they quickly ran into an issue of not taking

10:06.720 --> 10:11.720
into consideration the sequence nature and overlapping by trying to predict the next

10:11.720 --> 10:16.200
type of label of a phase without taking into consideration the previous one.

10:16.200 --> 10:20.960
So what we have done is we actually converted these different well logs.

10:20.960 --> 10:27.280
It's basically like a prosity log using neutron gamma rays to actually as spectrograms.

10:27.280 --> 10:32.880
So basically now you're generating an image with a texture feeling that represent the sequencing

10:32.880 --> 10:33.880
based on depth.

10:33.880 --> 10:39.920
So we're using a short term Fourier transfer to be able to convert all these kind of log

10:39.920 --> 10:43.680
measurements into a spectrogram with a texture representation.

10:43.680 --> 10:46.320
So now you have an image to train based on.

10:46.320 --> 10:53.960
The spectrogram-like image is the output of the thing that you are, you know, that this

10:53.960 --> 10:55.840
paper is talking about or it's the input.

10:55.840 --> 10:56.840
That's the input.

10:56.840 --> 11:02.200
So the nice thing is usually we have a geologist and petrophysicist who actually extract

11:02.200 --> 11:07.560
a labeled sample based on very limited amount of course.

11:07.560 --> 11:13.040
These are sample being extracted from the earth and they were able to actually delineate

11:13.040 --> 11:15.440
what each type of label represents.

11:15.440 --> 11:20.240
So we have some of a small amount of labeled data by subject matter experts.

11:20.240 --> 11:26.240
But the input for this model is the spectrogram image that we actually generate based on

11:26.240 --> 11:27.240
the measurements.

11:27.240 --> 11:28.240
Okay.

11:28.240 --> 11:33.160
And the spectrogram image is based on the same type of imaging that we were speaking about

11:33.160 --> 11:36.120
with the previous paper, the sonar type imaging.

11:36.120 --> 11:37.120
These are different.

11:37.120 --> 11:38.120
Okay.

11:38.120 --> 11:42.320
This is actually one of the, to my knowledge, this is the first time we actually convert

11:42.320 --> 11:47.840
some of the well-locks data that actually think, can think about it as a depth-based measurement

11:47.840 --> 11:54.000
to reflect what is your gamma transfer in earth to an actual image and use that as a

11:54.000 --> 11:56.080
training input for our model.

11:56.080 --> 11:57.080
Oh, got it.

11:57.080 --> 12:02.720
So you've got these walls in place, you've got something that is going kind of up and

12:02.720 --> 12:09.360
down the walls, a sensor head and you're taking measurements of, you say gamma rays and

12:09.360 --> 12:10.360
neutral ones.

12:10.360 --> 12:11.360
Right.

12:11.360 --> 12:14.120
So exactly your spot on.

12:14.120 --> 12:15.840
So these are the typical measurements.

12:15.840 --> 12:20.360
Every time a well has been drilled, you actually take a logging measurement.

12:20.360 --> 12:25.280
So basically you like to measure there is a stivity around the well, which is basically

12:25.280 --> 12:29.840
give you a different kind of representation of what type of rock around that.

12:29.840 --> 12:34.440
If there is a high reflectivity, this is likely to be a sand, it could be a clay, it could

12:34.440 --> 12:38.840
be some sort of a mixture, like a cemented sand or so forth.

12:38.840 --> 12:45.120
So in this particular situation, we have about six different type of what we call physical

12:45.120 --> 12:46.440
properties.

12:46.440 --> 12:52.160
We use it as an input for our model after converting it to these spectrograms.

12:52.160 --> 12:57.560
And the matter of the outcome which we're trying to predict is what type of faces we're

12:57.560 --> 12:59.120
actually looking for.

12:59.120 --> 13:02.320
In this particular situation, we have five different labels.

13:02.320 --> 13:04.880
So it's a multi-class classification.

13:04.880 --> 13:10.640
But that's the unique aspect, historically what most of the state of the art technology

13:10.640 --> 13:16.920
in that field, people are able to actually predict at most two or three classes.

13:16.920 --> 13:22.880
And they kind of misses the majority of other relevant classes because of either the distribution

13:22.880 --> 13:24.480
factor is very low.

13:24.480 --> 13:29.480
And you basically, the high distribution of a class that's dominant is typically to be

13:29.480 --> 13:31.640
classified correctly.

13:31.640 --> 13:37.120
And this is actually something we're very careful about to be able to use our balanced accuracy

13:37.120 --> 13:41.440
as a measure of evaluation, not the general accuracy for the model.

13:41.440 --> 13:46.960
Maybe for a little bit more context, what does being able to predict the facial type allow

13:46.960 --> 13:48.320
you to do?

13:48.320 --> 13:55.720
So this is actually allow the whole field development plan to be able to develop your field, how

13:55.720 --> 14:02.360
you basically determine where is the chance, the likelihood of finding the hydrocarbon

14:02.360 --> 14:04.320
deposition in certain area?

14:04.320 --> 14:06.080
Where do you place your future well?

14:06.080 --> 14:09.840
So this is based solely on exeploration wells.

14:09.840 --> 14:15.960
Usually you have only about five or six wells being drilled at the early phase of development

14:15.960 --> 14:17.480
for your field.

14:17.480 --> 14:22.720
And the intent here is you need some sort of high accuracy, high fidelity that would give

14:22.720 --> 14:27.200
you confidence to design the plan for future field development plans.

14:27.200 --> 14:33.000
So for example, in certain areas you might be drilling up to two, three hundred wells

14:33.000 --> 14:37.120
and you need to be able to identify where exactly you place that well.

14:37.120 --> 14:41.400
So you'd be only interested in one type of facial type.

14:41.400 --> 14:47.040
So this kind of clean sand that's where the reservoir is placed and being able to predict

14:47.040 --> 14:52.180
that area with that type of faces with high accuracy has a huge implication down the

14:52.180 --> 14:53.180
field for your field development.

14:53.180 --> 14:59.180
Well, you know, where this paper fits into the process as being a little bit further

14:59.180 --> 15:07.920
along than the previous paper, the previous paper, you are doing kind of, you know, sonar-based

15:07.920 --> 15:13.320
geological studies to try to get a lay of the land generally before you might put these

15:13.320 --> 15:19.680
exploratory wells in and then, you know, your next phase and I'm sure I'm dramatically

15:19.680 --> 15:25.480
familiar with this, but the next phase might be, hey, we'll deploy a few wells to get

15:25.480 --> 15:31.480
a sense for, you know, what it actually looks like down there and get some centers underground

15:31.480 --> 15:37.780
and then that then gives you the information that you need to figure out where to deploy

15:37.780 --> 15:43.160
your next set of investments which might be much larger in scale and much more expensive.

15:43.160 --> 15:44.160
Absolutely.

15:44.160 --> 15:45.160
You're exactly correct.

15:45.160 --> 15:49.400
So the first paper we talked about is part of concepts, election evaluation.

15:49.400 --> 15:55.680
So how do you basically have some sort of economic assessment evaluation and confidence that

15:55.680 --> 16:01.280
there is a likelihood of potential value in that prospect.

16:01.280 --> 16:06.960
The second paper we talked about is a decision has been made to go with a specific field,

16:06.960 --> 16:12.920
but you need to be able to maximize your development plan down the road in the future.

16:12.920 --> 16:21.600
And so the innovation in this paper again is the a kind of transformation of the features

16:21.600 --> 16:27.920
into this image structure, Graham, this image type and then applying sequence models

16:27.920 --> 16:34.640
to that image type to get state-of-the-art level prediction of the facial, the multi-class

16:34.640 --> 16:35.640
facial classifier.

16:35.640 --> 16:36.640
Exactly.

16:36.640 --> 16:43.840
Okay, let's talk a little bit about the development of the model and kind of what drove the

16:43.840 --> 16:46.800
eventual direction of that.

16:46.800 --> 16:52.520
So it sounds like one of the key inspirations here was that, hey, this is kind of a sequential

16:52.520 --> 16:56.760
a problem that might lend itself to a sequential type of approach.

16:56.760 --> 17:00.120
Did you discover that or is that where you started?

17:00.120 --> 17:04.040
Actually, we had an iterative kind of experiment.

17:04.040 --> 17:08.480
This is part of an ongoing research project in our portfolio.

17:08.480 --> 17:13.720
And we're very kind of focused in terms of trying different architectural design, but

17:13.720 --> 17:19.280
we actually have a brief conceived notion that one of the limitations of the published

17:19.280 --> 17:25.760
literature in the field is you actually overlooking some of the sequence aspect of how these

17:25.760 --> 17:29.680
faces are sequence in the physical system.

17:29.680 --> 17:33.640
Well too, for example, speech recognition where you need to be able to predict what is

17:33.640 --> 17:38.480
for example the next word or the next sentence based on the sequence of your previous text.

17:38.480 --> 17:44.280
So that actually inspires to think about how do you construct a sequence model that would

17:44.280 --> 17:46.120
take into account?

17:46.120 --> 17:52.320
What is this kind of overlapping nature of different faces that the way it actually manifested

17:52.320 --> 17:55.200
itself in a physical system?

17:55.200 --> 18:02.080
So we looked into the recurrent neural network architecture and we tried to determine how

18:02.080 --> 18:07.720
we can actually leverage this sort of spectrogram into that notion.

18:07.720 --> 18:12.160
But in that particular situation, the one of the primary limitation is it will only allow

18:12.160 --> 18:17.040
you to train on a single kind of image texture structure.

18:17.040 --> 18:23.400
And this is typically have a limited amount of information from a practical standpoint

18:23.400 --> 18:27.160
for two physicists and geologists to make that sort of an inference.

18:27.160 --> 18:32.920
So think about one physical property, I mentioned like for example Gamma Ray, has a very good

18:32.920 --> 18:35.520
indication of what face is type is that.

18:35.520 --> 18:41.480
So by taking that initial measurement, that first kind of input feature, converting it

18:41.480 --> 18:47.800
into a spectrogram and using it as an input, now we're able to generate a sequence model

18:47.800 --> 18:52.120
that's only solely based on one feature representation.

18:52.120 --> 18:57.560
But we actually went the step ahead and we said, now we need to think about training based

18:57.560 --> 19:02.000
on multiple images, multiple of these kind of spectrograms.

19:02.000 --> 19:08.120
And in this case, we actually have to adjust our network structure to have an encoder, decoder

19:08.120 --> 19:13.960
up front to be able to take these kind of seek multiple input features set and use it

19:13.960 --> 19:18.200
as a feed forward to our recurrent neural network.

19:18.200 --> 19:21.800
So that's actually one of our kind of a breakthrough moment of saying, huh?

19:21.800 --> 19:26.280
Now we are able to include additional feature representation.

19:26.280 --> 19:31.600
And it's also very kind of dynamic in certain areas where you want to generalize this model.

19:31.600 --> 19:37.000
You might not have the same set of input features, but our architecture is very flexible to allow

19:37.000 --> 19:41.960
people to say, for example, you have feature X, that's not part of the existing input for

19:41.960 --> 19:42.960
the model.

19:42.960 --> 19:49.560
It's very easy to incorporate that and use the encoder decoder layer to be able to actually

19:49.560 --> 19:51.520
use that as an additional input.

19:51.520 --> 19:58.640
So the input to the encoder decoder layer is multiple images, each corresponding to one

19:58.640 --> 20:02.600
of these features, gamma rays, neutrons, etc.

20:02.600 --> 20:12.840
And so you're transforming these N dimensions of images to some kind of single space and

20:12.840 --> 20:18.440
then using that as your feature set for the sequence model.

20:18.440 --> 20:19.440
Correct.

20:19.440 --> 20:20.440
That's exactly.

20:20.440 --> 20:28.120
We have initially a five hidden layer for doing basically the encoding for encoding decoding.

20:28.120 --> 20:34.840
And the output is fed into a sequence model RNN with two layers.

20:34.840 --> 20:40.560
And then the last layer is just a soft knacks, which actually output, which kind of faces

20:40.560 --> 20:46.400
is that output, whether this is a clean sound or a dirty sound or cement and so forth.

20:46.400 --> 20:54.240
The spectrogram images say one of the images, the gamma ray, one, is it a single channel

20:54.240 --> 20:56.200
image or a multi channel image?

20:56.200 --> 20:59.240
It's a multi channel image and you think about this.

20:59.240 --> 21:04.760
This is part of the construction of the depth sequence that's on the initial gamma ray.

21:04.760 --> 21:11.680
So you will have basically the input when you generate the spectrogram is going to create

21:11.680 --> 21:17.760
this kind of a multi channel image that was used as a feature input for our model.

21:17.760 --> 21:25.600
Meaning the channels are the time series of measurements or because I thought we would

21:25.600 --> 21:29.560
have gotten away from the time series by doing the Fourier transforming and getting to

21:29.560 --> 21:30.560
afraid the domain.

21:30.560 --> 21:31.560
That's correct.

21:31.560 --> 21:33.440
What would the channels represent then?

21:33.440 --> 21:35.640
So this is a 3D channel.

21:35.640 --> 21:42.240
So it's a 2D image, but we actually projected as a 3D input for the model itself.

21:42.240 --> 21:47.360
And so when we think about using images as inputs, the obvious thing that comes to mind

21:47.360 --> 21:54.320
is a CNN or some type of model like that or the sequence models that you're using, CNN

21:54.320 --> 21:59.480
type of sequence model or is there some kind of, you know, did you explore that direction

21:59.480 --> 22:00.480
or?

22:00.480 --> 22:08.640
For that direction, for this particular particular application, this is a bidirectional recurrent

22:08.640 --> 22:10.040
neural network.

22:10.040 --> 22:16.160
And the reason being that is especially when you have a limited amount of training set

22:16.160 --> 22:20.920
for a number of these images, the performance and the stability of the model is actually

22:20.920 --> 22:26.560
much, much more stable and able to converge rather than expecting, for example, a CNN

22:26.560 --> 22:29.120
with a very large amount of label data.

22:29.120 --> 22:34.680
So one of the primary reasons that dictated the selection of the architecture we used

22:34.680 --> 22:39.840
is how much amount of data we have and how much labeling actually accurate labeling

22:39.840 --> 22:41.320
for these data is available.

22:41.320 --> 22:42.320
Got it.

22:42.320 --> 22:43.560
Makes sense.

22:43.560 --> 22:47.960
How do you construct your performance evaluation?

22:47.960 --> 22:53.760
You mentioned that you're looking at your bias weighted metric.

22:53.760 --> 22:56.440
Did you have to iterate to that or is it obvious what to do there?

22:56.440 --> 23:01.320
In fact, we looked at different types of loss evaluation functions.

23:01.320 --> 23:08.320
So we looked into the general accuracy and the balanced accuracy, F1 score and so forth.

23:08.320 --> 23:12.840
And one of the things that's typically reported in the literature, people actually look at

23:12.840 --> 23:13.840
the accuracy.

23:13.840 --> 23:20.280
But as I mentioned before, the drawback for that is you always correct with the predominant

23:20.280 --> 23:23.280
class in your kind of outcome.

23:23.280 --> 23:28.320
And in this particular situation, we're very kind of focused about taking that kind of

23:28.320 --> 23:30.040
balanced accuracy in place.

23:30.040 --> 23:36.400
So we came up with our drive metric based on multiple performance indicators to come

23:36.400 --> 23:42.600
up with more human interpretable and also provide some rationalization for the human

23:42.600 --> 23:46.360
expert to interpret that and take it into consideration forward.

23:46.360 --> 23:52.000
But it's been a more an evaluation and to compare that as a benchmarking mechanism,

23:52.000 --> 23:59.600
we actually compared our faces net performance against several of the state of our technologies.

23:59.600 --> 24:04.560
Most people use, for example, naive based classification, random forest and other types

24:04.560 --> 24:08.840
of model to be able to actually do the classifications.

24:08.840 --> 24:14.000
And actually the paper have a very decent representation almost to close to a nine or ten

24:14.000 --> 24:18.560
different algorithms that's been reported and showing the performance difference.

24:18.560 --> 24:19.560
Okay.

24:19.560 --> 24:21.080
And what was the performance difference?

24:21.080 --> 24:22.720
How did you deal with it?

24:22.720 --> 24:26.120
It's quite significant, actually.

24:26.120 --> 24:30.680
The highest accuracy rate that's being reported is about 72%.

24:30.680 --> 24:35.280
So we're actually above like 75% accuracy.

24:35.280 --> 24:39.400
And this is across all the five different classes.

24:39.400 --> 24:43.480
The most actually people get it right for like three classes out of five.

24:43.480 --> 24:50.920
So we are able to accurately, correctly classify more number of faces type than just focusing

24:50.920 --> 24:56.160
on the high representation of classes available in your training set.

24:56.160 --> 24:57.160
Okay.

24:57.160 --> 24:58.160
Okay.

24:58.160 --> 24:59.160
Cool.

24:59.160 --> 25:01.200
Where does this direction of research take you?

25:01.200 --> 25:05.360
What's next and pursuing this work?

25:05.360 --> 25:08.400
One thing actually we're very passionate about.

25:08.400 --> 25:12.920
We always active to outsource some of our architecture.

25:12.920 --> 25:17.120
This is not the first paper we outsource the code to the community.

25:17.120 --> 25:21.520
In fact, we have a GUDNN outsource library.

25:21.520 --> 25:25.760
It's a model that we developed for the GUE science community.

25:25.760 --> 25:28.880
And it's been contributed to our public GitHub.

25:28.880 --> 25:35.080
So we're kind of actively trying to drive this as a mainstream application for machine

25:35.080 --> 25:39.520
learning within the physics community in general.

25:39.520 --> 25:41.320
And so is this one already up on GitHub?

25:41.320 --> 25:43.560
It's actually on its way in GitHub.

25:43.560 --> 25:52.280
As we speak, as the paper is being presented, this should be available for people to actually

25:52.280 --> 25:57.240
stop using and actually contribute to expansion of this approach.

25:57.240 --> 26:00.200
Who are the folks that you expect to be using stuff like this?

26:00.200 --> 26:09.080
Is it a primarily academic research labs or other energy companies or it's a joint actually.

26:09.080 --> 26:15.080
Because quite frankly, we're very optimistic about the level of collaboration between

26:15.080 --> 26:17.120
industry and academics.

26:17.120 --> 26:23.440
As I mentioned, the outcome of this results is a combination between our internal R&D research

26:23.440 --> 26:29.600
team and also some of our people from the academia who were part of that research project.

26:29.600 --> 26:34.800
Moving forward, the intent here is to make these as part of industry standard.

26:34.800 --> 26:40.720
For example, across there's a consortium of open subsurface data universe platform for

26:40.720 --> 26:47.520
experimentation and trial that's been signed up by more than 83 different academic institutions

26:47.520 --> 26:53.720
and industry partners to work on improving different types of or developing new different

26:53.720 --> 26:59.160
type of algorithms for different sorts of application, whether this is for image-based

26:59.160 --> 27:05.160
applications or an structured based type of modeling to be able to speed up and accelerate

27:05.160 --> 27:11.440
some of the bottlenecks, especially for computational time, more about human intensive efforts

27:11.440 --> 27:15.360
that need to be put in place for sorting these types of problems.

27:15.360 --> 27:22.400
Along with the code, do you also publish a data set to support folks researching in this

27:22.400 --> 27:27.520
era or are there well-known or already-existent data sets in the space?

27:27.520 --> 27:32.680
With good news, there's a lot of open source data sets available for that.

27:32.680 --> 27:38.120
The beauty about this approach is enabling people to actually take these kind of generally

27:38.120 --> 27:43.520
publicly available feature sets of physical properties and converting them into these

27:43.520 --> 27:48.840
kind of texture-based images to try to apply our architecture for these type of problems.

27:48.840 --> 27:55.480
This is actually a common challenge that facing most of the previous research in this area,

27:55.480 --> 28:02.520
how you can use a very sparse representation of non-humogeneous feature sets.

28:02.520 --> 28:07.480
The correlation between these geological features is very crucial to be able to correctly

28:07.480 --> 28:12.360
predict what type of faces that expect it as an outcome.

28:12.360 --> 28:18.320
Using this kind of an image-based approach for training, take some of this kind of deficiency

28:18.320 --> 28:20.800
or limitation out of the equation.

28:20.800 --> 28:26.760
Even how important the image creation part of this is, the Fourier transform is that specific

28:26.760 --> 28:32.280
code that you're providing in the repository as well, or is it something that you just

28:32.280 --> 28:40.600
need to explain how to do it, run your data through MATLAB's FFT routine or presumably

28:40.600 --> 28:43.160
you have to tell people how to do that part also.

28:43.160 --> 28:49.760
This is actually in our situation, we recommend people to use the standard techniques that's

28:49.760 --> 28:55.080
being described in the paper, because this is part of the feature, like pre-processing,

28:55.080 --> 29:00.480
like they can be processing, and it varies from one type of a problem to another.

29:00.480 --> 29:06.520
For example, if we're trying to apply this for example sensor data that's coming from

29:06.520 --> 29:13.080
satellites or vibration data that's measured over time, then it might require a different

29:13.080 --> 29:17.680
type of parameterization for that short time for your transformation to be able to generate

29:17.680 --> 29:21.600
the same representation of important features.

29:21.600 --> 29:26.360
But doing it is not a mystery, you just explain how it can be done.

29:26.360 --> 29:27.360
It's a standard process.

29:27.360 --> 29:33.280
But it sounds like you can apply this general technique beyond geological sciences to a

29:33.280 --> 29:34.280
bunch of other areas.

29:34.280 --> 29:37.080
Can you talk through some of the examples you just mentioned?

29:37.080 --> 29:45.360
One of the other examples we also envision using this for is for any type of equipment

29:45.360 --> 29:52.640
kind of abnormality detection, a common example for that if you have an equipment that have

29:52.640 --> 29:57.960
a different type of vibration rate over time, then usually you would have a sensor measurement

29:57.960 --> 30:03.240
over time for how that vibration signal is being captured.

30:03.240 --> 30:09.080
If you actually convert that using the Fourier transform, you would be able to generate an

30:09.080 --> 30:15.400
image representation for how that kind of vibration feature looked like over time and being

30:15.400 --> 30:20.880
able to use that as a representation input for model development is also a possibility.

30:20.880 --> 30:28.280
Yeah, I would expect for this kind of, for like this predictive maintenance thing

30:28.280 --> 30:32.400
where we're talking about where we have a time series of, you know, sensor readings

30:32.400 --> 30:42.680
that the FFT part of that would be fairly standard, right, to get a frequency domain feature.

30:42.680 --> 30:46.800
But the image part of that is the part that I'm still trying to wrap my head around

30:46.800 --> 30:47.800
a little bit.

30:47.800 --> 30:54.680
If you think about the main issue with any type of kind of anomaly detection is usually

30:54.680 --> 30:59.920
the difficulty of determining what is, what can institute an anomaly because over time

30:59.920 --> 31:07.320
they might be a concept drift from what is normal, but that still haven't actually materialized

31:07.320 --> 31:10.640
to the point of being an anomaly, it's kind of a continuation.

31:10.640 --> 31:17.120
Being able to actually represent that kind of change in the distribution over time would

31:17.120 --> 31:23.360
give you a different kind of representation for how things really evolve before you can

31:23.360 --> 31:29.880
actually use a static threshold of determining by exceeding that type of an upper bound

31:29.880 --> 31:32.520
tree you already fall into that space.

31:32.520 --> 31:37.240
So it's, I'm not saying that's the only way of doing it, but if you want, for example,

31:37.240 --> 31:43.600
to account for normal variation within the boundary before it reaches an extreme outlier

31:43.600 --> 31:48.720
level, then that's a way of attenuating that signal to noise ratio to make it more

31:48.720 --> 31:52.040
useful and more generalizable as well.

31:52.040 --> 31:56.360
And so from that perspective, is it fair to think of the spectrogram as, you know, not

31:56.360 --> 32:00.840
so much your processing in image, but you just have a bunch of vectors of frequency components

32:00.840 --> 32:02.320
relative to time?

32:02.320 --> 32:07.520
That's actually a fair representation, and really what you want it to do is you want

32:07.520 --> 32:13.600
to representation that time frequency domain in a more kind of an image kind of structure

32:13.600 --> 32:19.720
to be able to determine which part of that image is more relevant more salient to provide

32:19.720 --> 32:23.040
more informative features set for training your model.

32:23.040 --> 32:28.720
Yeah, I'm still struggling with within the, like when I think about an image, I'm thinking

32:28.720 --> 32:32.800
about things like convolutions that are taking, you know, that are operating in kind of

32:32.800 --> 32:36.840
multiple dimensions and kind of aggregating and things like that.

32:36.840 --> 32:39.560
How does that play out in the RNN scenario?

32:39.560 --> 32:41.240
Or does it play out in the encoder scenario?

32:41.240 --> 32:43.400
I guess it plays out in the encoder, right?

32:43.400 --> 32:44.400
Exactly, right.

32:44.400 --> 32:47.600
And kind of, if you think about it, these are images that's captured over multiple period

32:47.600 --> 32:48.600
of time.

32:48.600 --> 32:54.720
Kind of depth-based is reflecting, taking an image a shot at each kind of depth-based approach.

32:54.720 --> 32:58.840
So you can have, every have a feet, you would have a different image generated, and you

32:58.840 --> 33:03.360
would be able to see a transition as if you're having a moving object from point A to point

33:03.360 --> 33:04.360
B.

33:04.360 --> 33:09.640
Every time you take a shot, that frame would represent a moving aspect or an adjustment

33:09.640 --> 33:12.040
to the image that's being captured.

33:12.040 --> 33:13.040
That makes sense.

33:13.040 --> 33:15.520
Thank you for helping me get there.

33:15.520 --> 33:16.520
Thank you.

33:16.520 --> 33:20.240
But I'd love to hear you kind of talk a little bit about some of the other things you're

33:20.240 --> 33:22.080
working on there.

33:22.080 --> 33:27.440
I've had your colleague, Dan, on the podcast a couple of times, talking about some of the

33:27.440 --> 33:36.040
work that you're doing in the platform space, and you attended the Twilcon platforms conference,

33:36.040 --> 33:38.400
kind of in furtherance of that.

33:38.400 --> 33:41.000
So clearly, you're building platforms to support lots of different things.

33:41.000 --> 33:43.600
What are some of the other things that you're working on there?

33:43.600 --> 33:45.720
I'm glad you actually raised that question.

33:45.720 --> 33:51.400
I think people typically, when they think about how you operationalize these type of models,

33:51.400 --> 33:55.960
they think about maybe I'm getting a very kind of high-accurate model.

33:55.960 --> 34:00.360
It's performed very well on my training data and testing data set.

34:00.360 --> 34:06.640
But we're actually very kind of thinking about and very focused on the idea, how do you

34:06.640 --> 34:11.760
make sure this is deployable and scaled to a large application use?

34:11.760 --> 34:19.280
And that's why we kind of designing and building this kind of expandable open architecture platform

34:19.280 --> 34:26.040
based on Kubernetes, to be able to actually commissioning the same type of development

34:26.040 --> 34:31.720
and training environment and being able to push it to different types of deployment mechanism.

34:31.720 --> 34:39.720
Currently, we have ways of deploying models within, for example, our central data warehouse

34:39.720 --> 34:42.640
or typically we push things to the cloud.

34:42.640 --> 34:47.640
We have a very likely distributed kind of an environment for hosting these applications.

34:47.640 --> 34:53.560
But the key ingredient here is how do we actually go from point A to point B because in order

34:53.560 --> 34:59.840
to be able to provide real insights in a very timely manner, the amount of data acquisition

34:59.840 --> 35:05.680
and data transformation is very crucial to be able to handle in a more systematic way.

35:05.680 --> 35:10.840
And that's why all the pipelines and feeding of our crucial kind of feature sets that

35:10.840 --> 35:17.400
are required for any type of a model is something we spend quite a bit of time of designing

35:17.400 --> 35:22.920
building and making sure that everybody have the access to this in order to be able to

35:22.920 --> 35:25.520
using it in a day-to-day operation.

35:25.520 --> 35:30.600
So that's the primary essence of designing the platform.

35:30.600 --> 35:35.240
One of the things we're also very passionate about is how we can right now have a very

35:35.240 --> 35:40.200
large kind of a library of models that's actually reusable.

35:40.200 --> 35:42.760
We hardly tend to start things from scratch.

35:42.760 --> 35:49.320
We have a library of dealing with things about ingesting a large amount of different types

35:49.320 --> 35:55.000
of data structure, whether it's time series or unstructured or large seismic volumes and

35:55.000 --> 35:56.160
so forth.

35:56.160 --> 36:03.160
And then being able to very quickly scale these ups and run them in a massive pedal, I think

36:03.160 --> 36:09.240
the largest kind of number of models, we have about close to 500,000 models for different

36:09.240 --> 36:14.400
predictive subsystem units across the whole organization, being running in real time.

36:14.400 --> 36:20.680
So someone could go to some catalog and choose FaciusNet and just drop it into a pipeline

36:20.680 --> 36:21.680
somewhere?

36:21.680 --> 36:22.680
Exactly.

36:22.680 --> 36:26.080
So they will specify, for example, what is your feature set look like and where is actually

36:26.080 --> 36:27.400
located?

36:27.400 --> 36:31.200
You specify, for example, the volume that you're going to be trained on and that would give

36:31.200 --> 36:36.000
you some recommendation about what kind of GPUs you would require and give you also an estimate

36:36.000 --> 36:39.800
about what time it actually takes to complete that job.

36:39.800 --> 36:44.600
And we're putting that in perspective as a benchmark for what is a conventional approach

36:44.600 --> 36:47.000
and workflow would take typically.

36:47.000 --> 36:52.440
And I think we're very pleased to see that we're making a significant amount of progress

36:52.440 --> 36:54.760
on reducing that cycle of time.

36:54.760 --> 36:59.240
Also so that you announced AI Fellows program, can you talk a little bit about that?

36:59.240 --> 37:00.240
Yes.

37:00.240 --> 37:06.080
Actually, our shell.ai residency program is one of the exciting programs we're happy

37:06.080 --> 37:08.600
to announce recently.

37:08.600 --> 37:12.800
This is actually one of the programs we announced last year in NURPS.

37:12.800 --> 37:17.760
We have happily now have our first cohort of AI residents being on board.

37:17.760 --> 37:23.760
This is a two-year program where we invite people who are PhD or postdocs or some people

37:23.760 --> 37:32.880
with an industry experience in machine learning and AI to come and work with us across different

37:32.880 --> 37:39.520
type of challenging problem, basically defining what is the next wave of solving these challenging

37:39.520 --> 37:46.520
problems, whether this happened to be in the energy space or how the whole energy transition,

37:46.520 --> 37:52.440
new parts of the business, to be able to work on these problems for two to three different

37:52.440 --> 37:53.440
rotations.

37:53.440 --> 37:57.880
So currently we have people who are working on the hydrogen business, how we optimize

37:57.880 --> 38:03.760
the generation of hydrogen across the whole of our new energy platform mix, people who

38:03.760 --> 38:08.760
are working on the interesting projects in the exploration side, people working on the

38:08.760 --> 38:13.360
trading and asset optimization, people working on predictive maintenance.

38:13.360 --> 38:16.240
And the goal here is it's a two-way collaboration.

38:16.240 --> 38:21.080
It gives people the opportunity to shape the direction and innovation within machine learning

38:21.080 --> 38:28.640
application across different types of domains, but at the same time we provide enough training

38:28.640 --> 38:36.080
challenges and support for people to publish, collaborate and openly challenge the status

38:36.080 --> 38:37.560
goal of how we do things.

38:37.560 --> 38:41.280
You said you have your first cohort in place or are you out there recruiting your second

38:41.280 --> 38:42.280
cohort?

38:42.280 --> 38:47.720
And actually we currently actively are looking for the brilliant minds to come and create us.

38:47.720 --> 38:55.180
So always welcoming people in our space and we encourage people to check out shell.ai

38:55.180 --> 38:59.480
residency program on our shell website, very kind of interesting.

38:59.480 --> 39:05.720
We have a lot of Q and A's and a little bit kind of a videos about the existing residents,

39:05.720 --> 39:08.960
what they're working on and what's their experience so far.

39:08.960 --> 39:09.960
Awesome.

39:09.960 --> 39:10.960
Awesome.

39:10.960 --> 39:14.600
Well, Mohammed, it's so great to catch up with you as always.

39:14.600 --> 39:17.200
Any kind of parting words about your experience as far in Europe?

39:17.200 --> 39:20.920
We're just kind of getting started, but it's an amazing, actually.

39:20.920 --> 39:26.000
The last couple of days I had been very fortunate to participate in different affinity groups.

39:26.000 --> 39:30.920
Actually, this year also we sponsored a woman in ML workshop, but I actually had a chance

39:30.920 --> 39:35.480
to stop by Black and AI and other type of affinity groups.

39:35.480 --> 39:36.480
It's really interesting.

39:36.480 --> 39:40.800
Lots of interesting ideas, the poster session yesterday was fantastic, and I'm looking

39:40.800 --> 39:46.240
forward to actually spend the next few days and getting to see more about these kind of

39:46.240 --> 39:48.200
outstanding ideas coming through.

39:48.200 --> 39:49.200
Awesome.

39:49.200 --> 39:50.200
Awesome.

39:50.200 --> 39:51.200
Same here.

39:51.200 --> 39:52.200
Well, thanks once again.

39:52.200 --> 39:53.200
Thank you Sam.

39:53.200 --> 39:54.200
Really pleasure talking to you.

39:54.200 --> 39:55.200
Thank you.

39:55.200 --> 39:56.200
My pleasure.

39:56.200 --> 39:57.200
Cheers.

39:57.200 --> 39:58.200
All right, everyone.

39:58.200 --> 40:00.240
That's our show for today.

40:00.240 --> 40:06.760
For more information on today's guest or our NURPS podcast series, head over to twimmolaii.com

40:06.760 --> 40:10.080
slash NURPS 2019.

40:10.080 --> 40:13.880
Thanks once again to Shell for sponsoring this week's series.

40:13.880 --> 40:19.960
Check out the Shell.ai Residency program by typing Shell.ai into your browser's address

40:19.960 --> 40:20.960
bar.

40:20.960 --> 40:23.200
Thanks so much for listening.

40:23.200 --> 40:52.160
Happy Holidays and catch you next time.

