All right, everyone. I am here with Andrea Benino. Andrea is a research scientist at DeepMind.
Andrea, welcome to the Twomol AI podcast. Thanks, Sam. Thanks for having me. So you are
working on artificial general intelligence at DeepMinds. Tell us a little bit about where your
interest in AGI comes from. My interest in AGI comes from my background. I'm actually a neuroscientist
who decided to study how the brain works and in particular memory, how we create our memory,
why we create our memory. And in particular, I'm interested in a subset of the memory field,
which is called a episodic memory. So episodic memory, those memory that relates to some
episode that you experience in the past in a specific location, sometimes, or with specific people.
So they are also referred as autobiographical memories.
An episodic is in contrast to what other kinds of memory?
Another example could be semantic memory. So semantic is like, you know, the meaning of something,
for example, you know, what is a bicycle that could be a semantic memory. But if I ask you to remind
you to remember a specific when when you when you cycle with someone, then maybe now you're
thinking about a specific episode in your life that would be an episodic memory. So you relate
something to a specific moment in time. It's also described as the what when and where of a specific
it went. To some degree, the relationship between memory and intelligence is kind of an obvious one
in the sense that, you know, we we use our memory and our prior experiences and interacting with
the world, making decisions and all that. But is there a kind of broader significance to memory
and the development of AGI? Yeah, I think as you say, it's kind of an obvious one, right? So
we live in a world that is consistent. So if we gain some experience in the world, then we want
to reuse that experience if we don't want to relearn every time from scratch. So you can already
see how that's viable. But something that I actually study over my PhD is that episodic memory,
it's also way to so it's something that enables generalization. In particular, we study how
let's say if you have if you experience two events A and B that are related together. And later
in time, you experience other two events like B and C that are again related together. You are
brain without you doing any effort directly relate A to C together, such that you relate,
you do this kind of inference through your episodic memory. So if you want to give like if I can tell
you like a more precise example would be like, if you see someone going out with a dog in the
morning, and then you see the same dog with a different person in the afternoon, immediately your
brain is going to try to connect the two people that we are going out with the dog. And that's the
kind of inference that episodic memory we know support. So that's that's quite important, right?
Right, right. And what is the the history of memory, you know, in this effort to kind of get us
closer to AGI? How have how have we used memory in the field to facilitate intelligence? Okay, so
this is I think it's a nice question because I think in some sense the kind of memory I'm talking
about is still an open question. How we do it properly in in AI? We have different forms of memory
currently, which we know how to to play with in particular recurrent networks both RNN and the
STM are so-called working memory. So it's kind that gives you like imagine a white white board where
you can write something and reason about that and then erase because it won't stay there forever.
It's like kind of a canvas where you can you can make predictions. That's that's what we know.
Then we have also something called memory augmented neural networks. In that case we we basically give
a neural networks an external memory where they can write a previous computation that they performed
and then read back from there and reuse previous computation and this gets a little bit closer to
the kind of memory I hinted before but we are not there yet and then we have retrieval augmented
models which are those models that basically go back in a if you like in a table where we store
almost everything we have seen in the past like a dictionary if you like and then they try to
look up things but most of the time they look up and they they they use that look up to answer
but they don't consolidate back the knowledge into the into the weights if you like of the system
so they need to do the look up all the time which in some sense it's a waste we don't do as
as I said before we don't do that we kind of make sense of what we are to and use it later.
Right so in that lot of case you can think of it as kind of this fixed boundary between the memory
and the computation in a sense and the computation isn't ever updated the way we think about or
the way the model thinks about its inputs is never updated with regard to the things that it
learns in a memory it's just checking the memory constantly with each each input. Yeah sometimes
there are models for I can I that comes now to my mind a model where that computation is updated
but it's very difficult to scale up those models because they require a lot of computation so we
don't know yet how to make that model big to a scale where they're actually useful to target
a very complex problem and also we have finally the last one will be a very recent
advancement in in AI which is just something that everyone knows I'm sure transformer
this model have shown some of the properties of this kind of memory in language there are what
the sometimes they show a few short learning which is a prerogative of like the memory models I'm
talking about but that happens in language only not in other domains so I'm not sure if it's
big that then that raised the question it's because of the model capable of doing that or it's
because of this specific domain where you train the model that allows that so no one has done that
experiment yet but I think it could be interesting and can you drill down on that how should we
think about transformers from a memory perspective and exhibiting these properties that you
mentioned first of all I think most of the transformer that we use now in language
transformer excel and it's a specific instantiation of transformer whereby you add
a memory and a sort of external memory at each layer of the transformer so we already know
that that's necessary in some sense to overcome some of the limits like not having recurrence for instance
so I would say they are limited in terms of memory also because given that we do this
all to all comparison over the sequence we cannot process very long context there are now papers
trying to deal with that and reduce the complexity to to linear size but still I don't think
we are at the point where let's say we can process several books and ask inference questions
about books without like do like this sort of external retrieval all the time you know oftentimes
there are ideas that we might want to apply in the context of neural networks and a big challenge
is yeah are they differentiable you know so that we can apply techniques like gradient descent
is is memory you know typically differentiable is it typically undifferentiable and like how does
how does that plan to you touch the perfect the perfect downside of like these kind of external
memory where we do what is called k-nears neighbor retrieval and that's a non-differentiable operation
so that obviously has some limitations on on then what you can get and again I don't think
with the current system we know how to backprop or to do backpropagation of a very large memory
which again it's an open problem I think it's a very nice problem to tackle and we should probably
stop now I think we might be able to do it where what is this how does the size of the memory play
into whether we can backprop over it or not because most of the time you use a soft max operation
over the dimension of the memory and you know it becomes quickly impractical to send gradients
over a very large large memory and that's because it's computationally infeasible so it doesn't fit
in memory and also when you get with very large soft maxes also you have problem with gradients
there's another aspect of memory that comes to mind you know we've talked about specific
kind of features and you know architectures that are used to emulate memory but oftentimes one
of the critiques of deep learning is that the networks themselves like remember stuff and you know
that becomes a problem it causes problems with generalization are there ways that
that can be harnessed more directly to achieve some of your goals for episodic memory
yeah okay yes memorization it's definitely an issue like overfitting it's it's it's an issue although
yeah most of what we do as well is like kind of memory so we enlarge our our data set of experience
as we grow right so in some sense we tend to overfeed as well in most of what we do so I don't
think that's that's a huge problem that's also there was actually a nice paper last year called
direct feed to nature which I recommend your listen to to look up from Williamson which basically
poses these the same question you are posing to me and it basically answers in this way so we
we as we grow as we have grown as a species through evolution and also doing our life we kept
enlarging the kind of the the amount of experience that we used to grow our our our brain our network
so in some sense we kept memorizing it's just that we don't know where we start from scratch
and also we have this ability of generalizes that I think it's still a little bit missing from
from deep learning although we shouldn't we should recognize this this generalization
is not that we can generalize to everything right so we have in some sense limited as well
in generalization maybe neural networks are slightly more limited than us but we still we already
see some example of generalization which are starting to to emerge in neural networks and I think
that's a good thing and something we cannot neglect so for instance I had a paper couple of
in two thousand eighty so three years ago in nature where we actually use
and we and view the neural network with representation like the one like the one we have in the brain
in the apocampus and that way the agent was able to travel that was an obligation task the agent
was able to take shortcut and traverse part of the environment that was previously blocked
and the agent was able to do that with the right representation so I don't think the problem is
the back propagation of the model themselves it might be a problem how we train that in terms of
both data we we use in the representation that we force to emerge you mentioned this paper
that implemented something akin to the representation that's used in the apocampus what is that
representation how does how does that work and how the paper we had these we studied these
these things called grid cell so in the apocampus grid cells yes so in the apocampus we have
so it's kind it's a memory machine of the brain but it's also the spatial machine right you can
see also spatial as memory but that's not going to that that's how I feel that I don't want to go
into but basically we have the two cells that are probably the three cells that are more known
like head direction cell so those are cell that fires every time you look in a certain direction
but I'm talking about allocentric direction so every time you face north there will be a cell
firing with a certain probability distribution over north and the same for the other the other
allocentric direction and the north is not the cardinal north the north is the let's say is the
one in with which relate to a certain reference point in the environment so let's say okay
then we have play cell those are neurons that fires every time you are in a particular location
independently aware you're looking at and then we have grid cells which are these
visually and mathematically beautiful cells that basically fires following an hexagonal lattice
they have a 60 degree offset and they are very beautiful and no and there have been several theories
that try to motivate the reason why we have that and one of these was because we can basically
we can use that to calculate shortcut to calculate the shortest vector between two points
and we manage to do two things in that paper first of all we manage to make the representation
emerge in our neural network trained to do path integration so to do our navigation task
and secondly we use these those representations in our in our enforcement learning agent
and we prove through through several ablation as well that the world that was the only agent
able to take shortcut and if we lesion let's say some grid cell the ability of the agent to take
shortcut just wet down so it was kind of an empirical paper to prove what the grid cell are for
is it is the idea in terms of the implementation that i'm imagining you're like adding
signing cosine elements to your loss function or something like that no no no no no no no no no no it's
data driven that our goal was really to be super data driven and we achieved we achieved that by
actually a specific arc it was a recurring architecture two things were really important one was
to introduce a dropout such that no or not all yours were able to fire the same time and the
second one was introduced noise in the gradients and that basically helped if those are two things
we always want to do yeah yeah particularly uh specific to this problem that's that's that's
probably why people like so much that kind of paper because it was kind of a general approach
to to make that one of the although i have to say it was kind of difficult to analyze but
you know one of the reason why noise helps because it helps you moving away from certain solution
in the landscape of the loss and our our our our way to work was to help the network
going down the solution we were we liked so grid cell but that wasn't too difficult actually
to achieve okay that's why i think it it was a nice piece of work yeah interesting interesting um
so we were i'm trying to speak a memory i'm trying to know how we got here actually
in a little the ponder stuff i guess you invited me to talk about the ponder the ponder
the ponder anything inspired by memory exactly from the study that i mentioned at the beginning
the this thing about related doing inference associative inference that was part of my PhD and
one of the paper that came out during that time was disability of the hippocampus to basically
do this recursive ponder before actually being able to do this sort of inference so you see people
when you ask people people when you ask i remember doing this experiment practically with with
people and when when you ask them to late a and c to tell your story about a c they think more
they really spend more time in thinking compared to a b and b c and that's and that's that's
how i got inspired because you know our brain works so the same mechanism because when we did
then FMI study right the same mechanism was involved it's just that the the answer was going out
of the apocampus and then back into the apocampus few times and but then was processed by the same
system if you like and i think that's a nice probability to have in an angry then
does that mean i'm probably you know taking this too far but you know when i hear you say that i
i make associations like the you know the brain knows how to do scanning and it doesn't have
something like an inner join okay no the the problem there is that i what does it mean the brain
no i guess from a computational perspective then i could ask you what's the loss that the brain
is minimizing then to do that which i don't know i don't know the answer maybe i don't know maybe
it could be uncertainty reduction because at the end of the day we want to get better prediction we
want to be able to i think better predict the model the world because it's then it will be less
uncertain and so less risky but i think there are people like much better than me that could explain
that yeah yeah no i think you that was going in maybe a slightly different direction i was
inferring from the what i heard you say was that when we ask people to do these kind of associative
types of tasks or inferences you know where they need to get from a to b and b to c you know that
takes longer which kind of suggests that there's not some built-in associative thing in the brain no
because in the in the fmi study that we did we saw that also if we do longer inference we are
still able to do that it just takes more time even more time so i think that the algorithm we
applies the same which is the ability of making associations is just that the longer the jump
that i ask you to do in doing this sort of associative mechanism the more you need to do you do
it a bit hierarchically right you put i don't know the things that are just two steps separated you
calculate that then three steps and then you might be able to put together two and three and do five
so we went down this particular rat hole in trying to provide some context for ponder net which
you know i'll have you explain but i still don't really see the connection with memory when i
when i read the abstract for ponder net i think about things like you know hyper parameter
tuning and like early stopping like the way we train networks and like pulling that into the network
as opposed to anything having to do with memory and the stuff we were just talking about yeah
so tell me more about the connection two way answer on about the connection the first one is the
more high level maybe and wavy if you like it's generalization so we believe that mechanism like
ponder net that can get you a little bit far in terms of generalization compared to not doing
ponder in deep neural networks and that's the kind of things that i we discussed before right so
one of the benefit of memory is you're giving you the ability to to have the right input to generalize
i guess let me put it this way the second is that a previous work to ponder net was another work
i did during my PhD which was called memo and was a memory of mental neural network where we did
exactly we studied this mechanism of recirculation so you you the network speed out an answer
you compare it with previous answer and you only give the final answer to a certain problem when
you are satisfied so you though you were already like already implementing this sort of ponder
mechanism but there it was really really early stage because we use basic reinforcement learning
to train to train a Bernoulli variable that was basically saying go stop and we saw that
as was really hard to train and very noisy in terms of variance so we decided to do something more
principled and that's that's what led us to to to ponder net okay so taking a step back what's the
problem that you're trying to solve with ponder net yeah i think that's the first line of the
abstract which basically says that the normally neural networks so the amount of computation that we
spend in neural network grows with the size of the input but not with the complexity of the problem
but as we just mentioned like few minutes ago that's not how we reason right the more complex
the problem the more we spend time on it and that's that's what we wanted to get a session
with this this work and also we wanted to to make it fairly general such that it could be applied
to any architecture that was there to be architectural and mystic got it and so the size of the input
we know what that means you were talking about like feature dimensionality yeah we know the
problems that come along with that complexity of the problem what exactly does that mean and how do
we measure that okay so i think empirically again we have like the the example i like in the
paper is that it takes more to divide than to sound so that's exactly the same problem
mathematically speaking but for some reason we spend more time dividing than doing some nation
it's fair to say that we're we're talking about the computational complexity of a given problem
um as opposed to some conceptual type of complexity yeah yeah yeah yeah so if you have to apply
the same algorithm to uh so the algorithm is the same it's just that the specific
instantiation where you have to apply it requires more more compute more compute time
so it's like a computer right so it's the same the same you apply the same function but in
some some circumstances it takes more time so the computer thinks more that's not how neural networks
work it's they imply the same amount of thinking for each input i can give you another example in
a sentence for instance when we read we don't spend the same amount of time of gazing each single
world in a sentence so that's been proved in psychology so we tend to focus our attention
few words in the sentence that are more important to process the whole thing so that's another
that's another practical example okay okay um and so you want to create a neural network that
um you know would you describe it as kind of budgets it's you know computational investment
in solving a problem according to the inherent complexity of the problem is that a way to think of it
yeah i would say it's a fair description and so how did you do that in ponder net
okay so important first of all this is based on our previous work called adaptive computation time
and the the problem there is that they are they they directly minimize the number of pondering
step so the number of step the the network took whereas in our case what we did was to make
this probabilistic so to be to be a slightly more practical on this for each time step in the
sequence we calculate the prediction the probability of alting and the next step so the probability
of alting is just a Bernoulli random variable which tells you the probability of alting at this
particular step given that you have not altered the previous step and then from there what we can do
then is to calculate a probability distribution by basically multiplying the probability at each
time step in order to form a proper distribution a proper geometric distribution once we have that
what we can do is to basically wait each single so we calculated the loss for each prediction in
the sequence that we made and then the loss is then weighted by the probability of having
altered that particular step and that's a critical difference between us and act because in act
they instead output a weighted average of the prediction so they don't output a specific
prediction but a weighted average of the whole prediction and that creates a bias integrated
whereas in our case we can basically just really take the exact loss at each time step
given given the decision of alting and then we take the particular training time we take just
the particular step that has at threshold that basically so past the threshold that we decide for
alting whereas at test time we just sample from the probability that from the probability distribution
that we know okay I mentioned earlier that it calls to mind you're talking about holding here
calls to mind early stopping is the idea that early stopping is kind of like you know we're trying
to conserve training time is the idea here that we're trying to conserve inference time like we
have exactly we're trying to make a prediction yeah instead of going through the whole thing let's keep
going until we're sure what the answer is and then stop early and this is an approach to getting
there yeah I think our hypothesis was that you know you can let's say you want to implement an
algorithm on your phone you can train it in a you know in the cloud no problem a training time
like the amount of commitment for a few people no problem but then at in fact it's time you want
to be quick and that's and that's and that's important but now I'm laughing a little bit but
you know if done proper I think this could also reducing the amount of resources that you
spend a training time because we actually have an experiment in the paper where we see that
the total number of gradient updates for pondernet as more than other methods
even the same final performance so these could also help reducing the amount of resources
that for certain people I guess okay and then circling back the the connection to memory here
is it in storing these probabilities and the Bernoulli variables that kind of stuff or is there
a different connection yeah I guess the we left the connection with memory maybe a little bit
behind because okay however you might think I remember one tweet that actually also inspired
these work a little bit from and I can't party that was basically say one of the limitation of
transformer is actually they spend the amount the same amount of compute for each token in the
sequence so if we treat broadly speaking like transformer has a form of preliminary memory right
you can you can think of applying these on top of transformer and see if that could help right
maybe spending different amount of compute per point in the sequence got it got it got it so
I understand it's a bit stretchy but yeah it's it's it's kind of it's an analogy of some sort
it's not necessarily an implementation of a memory system that we're talking about here
is pondernet is a specific network architecture as opposed to a technique that you could apply
to different architectures or is it the latter it's a technique so it's indeed if you see in the
paper the step function what we call S in the paper could be anything could be an RNN
a CNN of transformer okay on a real agent whatever as long as you return so as long as you
add these extra unit that calculates the probability ofality you can apply it to everything that
that was important for us and indeed in the paper we do that so we applied it three different
architectures okay and how did you evaluate the results so we use one task from the ACT paper
called the parity task so you have a string of of 1 and 0 or 1 and minus 1 whatever and you
need to calculate the parity of that string could so could be either positive odd or even right
and the good thing that we could that that's a nice task because you can also train on
parity up let's say in our case up to 48 integers but then test up to twice as long the length
to test these a bit of extrapolate and indeed we see that our network extrapolates much better than
baselines the other methods then we applied it to a reasoning task which is called a Bobby and
basically you have 20 tasks which you train all in parallel it's a language task where you get
ask questions actually can I can I pause you and go back to parity I'm trying yeah yeah I'm
trying to work this through my head like okay so I guess the first thought that occurred to me is
it was some number of bits that you're trying to calculate the parity for it's not like you're
trying to end the bits together and as soon as you hit a zero or something like that you know the
answer right you still you need to look at all of the bits yes and the input for parity yeah but
there's the premises that independent of that particular fact inside the network
you can still stop early relative to going through some number of computations
and still answering the right question the question correctly we have a baseline we pick this
task exactly because we know this is a task that like a normal RNN is having trouble
okay doing got it that's a that's a kind of a well-known issue in this sort of literature okay got
it yeah on the topic of transformers which have come up a couple times you have you did a workshop
at ICML talking about transformers and reinforcement learning can you talk about taking two topics
that people are really excited about and combining them together tell us a little bit about the
goals of that work so the I think I'm really got inspired by the birth work where they have
their these known causal masking and you know in in RL we we keep using LSTM essentially for doing
most of the task but we know that they suffer from what it's called emergency bias so they tend
to pay attention only to the last bit of the sequence that they are trained on which to be honest
for most of the environment that we we use nowadays it's fine but if you grow in size and the
context you want to pay attention to is quite long then they start to suffer so one option
would be to use transformer because we know that they can end a long term the long context longer
context than LSTM however the problem in RL is that the reward are sparser most of the time and the
gradient being shown to be noisy so it's difficult to train so many ways so what we did in that work
was basically to to generalize the birth training to which you know is done on token so those are
like categorical numbers so that you can apply softmax on the other side we generalize
the the birth masking to real value numbers input so to basically features so we send the
features from the CNN into the into the transformer we mask some of them and then on the opposite side
we basically use a contrastive approach to reconstruct the the masking so we give
the like some negative taken from the same sequence and and and positive and the network has to
discriminate and and also what we did in that work was to combine LSTM and transformer in the
same architecture and and the good thing about that is that it helps you reducing the size
of the the transformer to gain in speed so because the and we let the agent actually learn when
to use transformer sorry when to use a LSTM alone or when to combine transformer and the LSTM
such that in some task it can basically avoid the extra complexity of the transformer and just
use the LSTM whereas in other more complex tasks it will focus on using both. Got it and so in a sense
you there are echoes of the pondernet paper in that you're trying to manage the computation
or let the agent manage the computational investment based on an assessment of complexity.
In some sense yes through the RL gradients there so that was really the agent by playing with
the environment that was deciding what to use because yeah it's although I would love to actually
have agents that stop and ponder that I think will be nice. What types of problems did you
experiment with this paper? Three three different domains the whole of Atari suite
deep mind control suite which I'm particularly proud because that's something that normally
with policy methods like the one that we had there we didn't do so much work and then deep
deep mind laboratory which is a suite of 30 task 3d complex task that you play all at the same
time so it's also free of multi task escape that was the task we escape.
And from a performance perspective what kind of results did you see was this you know promising
enough to keep poking around that or was it you know really good performance that you know it's
kind of challenges state of the art. First of all we always improve on the
efficiency massively compared to baseline and in many domains especially in DM lab so in the
deep mind laboratory we actually also got state of the art performance okay which was so then
that's good because that was the more complex domain where we played so I think and I think that
was kind of not an issue but you know something we could have done slightly better to focus more
and more on complex domain because I think that's where this kind of method we shine so
like complex architecture probably will benefit more from like complex method although I still think
it's something I quite passionate about I think there's lots of stuff we can do to improve
transformer and memory in general in reinforcement learning especially in relation to the
length of the context that we can process that's something I think important and kind of a bottleneck
in my opinion. Meaning the approach you took in this work of you know coupling the LSTM
and the transformer and allowing the agent to choose sounds like you're saying you know that
that's kind of a beginning place but there's a lot more a lot more to be done there. I think so
I think so. As for us we have different sort of memory as I said before we have very long term
memory we have shorter memory I think I'm not the first one to say these there are few papers out there
already and they argue and I argue that agents should be equipped with this sort of different
timescale memory. Awesome awesome. Andrea thanks so much for taking the time to share a bit about
what you're working on. It's been a pleasure Sam and again thanks a lot for for
inviting me. Actually I know. Absolutely thank you so much.
