1
00:00:00,000 --> 00:00:05,960
We're excited to kick off our coverage of TwilmoCon AI platforms with an interview

2
00:00:05,960 --> 00:00:12,480
recorded today on the TwilmoCon stage featuring none other than Andrew Ng, founder of Landing

3
00:00:12,480 --> 00:00:17,320
AI among so many other ventures.

4
00:00:17,320 --> 00:00:21,920
Before we get to the show though, I'd like to acknowledge TwilmoCon sponsors.

5
00:00:21,920 --> 00:00:26,400
We couldn't have done this without our founding sponsors who supported us in a big way as

6
00:00:26,400 --> 00:00:31,760
we pulled this event together. A huge thanks to all of our friends at Sigopt, IBM and

7
00:00:31,760 --> 00:00:33,760
DotScience.

8
00:00:33,760 --> 00:00:38,680
We're also grateful to CloudDera who joined us as a gold sponsor. CloudDera was our first

9
00:00:38,680 --> 00:00:43,240
ever sponsor for this podcast and we thank them for their continued support.

10
00:00:43,240 --> 00:00:47,600
Our silver sponsors include Fiddler Labs and Virtua, two innovators you'll undoubtedly

11
00:00:47,600 --> 00:00:52,680
be hearing more about in the near future. And last but not least, we are overwhelmed by

12
00:00:52,680 --> 00:00:58,520
the outpouring of support by our community sponsors including Apple, Etsy, Figurate,

13
00:00:58,520 --> 00:01:05,120
Georgian Partners, Imerit, LinkedIn, Logical Clocks, Neuromation, NYU Future Labs,

14
00:01:05,120 --> 00:01:11,280
Valahai and Waits and Biasis. Thanks so much to all of these great companies. Please be

15
00:01:11,280 --> 00:01:17,640
sure to check them out at twilmocon.com slash sponsors and thank them for supporting us.

16
00:01:17,640 --> 00:01:23,320
And now on to the show.

17
00:01:23,320 --> 00:01:30,120
My first guest really needs no introduction but I will try anyway. Through the podcast,

18
00:01:30,120 --> 00:01:36,600
I get to have a lot of conversations with a lot of incredible people and learn about

19
00:01:36,600 --> 00:01:43,400
how they got their start in machine learning. And it probably wouldn't surprise you that

20
00:01:43,400 --> 00:01:50,720
many of the folks that I talk to got their start with my first guest courses, his Stanford

21
00:01:50,720 --> 00:01:58,320
University courses, Coursera and more recently deep learning AI. In fact, I got my start

22
00:01:58,320 --> 00:02:02,680
in the space taking one of his courses and I can't think of an individual who's helped

23
00:02:02,680 --> 00:02:10,040
more people enter this space than our first guest, Andrew Ang. Andrew?

24
00:02:10,040 --> 00:02:18,520
So it's great to see all of you here. It is great to have you here. So my first question

25
00:02:18,520 --> 00:02:26,120
is, I don't know if you saw it on the document that had your autograph but I lost 4.85 points

26
00:02:26,120 --> 00:02:30,520
on homework assignments in your course. What was that all about?

27
00:02:30,520 --> 00:02:35,040
Yeah, I'm sorry, I think you are perfect. It must have been a bug and it's a great

28
00:02:35,040 --> 00:02:43,680
question. Awesome, awesome. So you also launched your most important learning machine,

29
00:02:43,680 --> 00:02:47,760
Nova, back in February. How's fatherhood treating you?

30
00:02:47,760 --> 00:02:53,600
Yes, so Nova is now seven months old. But you know, there's actually a story about how

31
00:02:53,600 --> 00:02:58,160
we chose her name. We wanted her to have the initials, you know, neural networks taking

32
00:02:58,160 --> 00:03:03,360
off. So we wanted her initials to be NN, so Nova. But one layer deeper than that, you know,

33
00:03:03,360 --> 00:03:08,000
thinking, all right, have a new baby. I think every person on the planet is a unique person,

34
00:03:08,000 --> 00:03:14,000
is a unique human being. So people on, no one is a number. So we gave her the middle name,

35
00:03:14,000 --> 00:03:19,600
Athena. So the full name is Nova Athena. Does the initials are NAN?

36
00:03:23,040 --> 00:03:27,040
That doesn't burden, she'll have to carry it for her whole life. Nice, nice.

37
00:03:27,840 --> 00:03:32,720
You are prolific. You are up to so many things. Can you give us an overview of what you're

38
00:03:32,720 --> 00:03:37,440
working on nowadays? Yeah, the teams, the rise of AI, the rise of machine learning means there

39
00:03:37,440 --> 00:03:42,560
are a lot of pieces needed for it to reach this full potential. So right now, the three teams,

40
00:03:43,200 --> 00:03:48,240
swimming most time with them, leading our landing AI, which is helping companies jumpstart AI

41
00:03:48,240 --> 00:03:55,440
adoption, deep learning.ai, which is our educational producers content, a lot of it is on Coursera,

42
00:03:55,440 --> 00:04:00,880
also a weekly newsletter called The Batch, which is subscribed to get weekly news about machine

43
00:04:00,880 --> 00:04:06,160
learning, and then also AI funds, which is a startup studio that builds AI power startups from

44
00:04:06,160 --> 00:04:10,720
scratch. The rise of machine learning creates a lot of new opportunities. So building these three

45
00:04:10,720 --> 00:04:15,680
teams which work together in the ecosystem, trying to build many pieces that, you know, let's build

46
00:04:15,680 --> 00:04:21,200
AI power future. Let's dig into landing AI a bit. I associate that with doing work in the

47
00:04:21,200 --> 00:04:25,840
manufacturing space. Give us an overview of the company and what it's up to. Is that the case?

48
00:04:25,840 --> 00:04:33,920
So I think I saw with my own eyes how an injection of modern AI can make a company much more

49
00:04:33,920 --> 00:04:40,240
effective and valuable. I think, you know, building up Google Brain and then leading AI at by-do,

50
00:04:40,240 --> 00:04:45,120
I saw, right, with my own eyes, a couple of great companies become modern AI companies,

51
00:04:45,120 --> 00:04:48,960
have become much more effective and valuable along the way. But if you look at what we've done

52
00:04:48,960 --> 00:04:54,000
in machine learning world, I think we've transformed the software internet sector. So many of the

53
00:04:54,000 --> 00:04:59,360
companies represented, you know, at this conference and many companies in Silicon Valley and Beijing,

54
00:04:59,360 --> 00:05:04,160
even outside the top small handful, you kind of have a giving a lot of traction in AI.

55
00:05:04,960 --> 00:05:09,840
I think the next step for AI is first to transform all of the other industries as well,

56
00:05:09,840 --> 00:05:16,880
outside software internet. And so landing AI works with many companies from manufacturing to

57
00:05:16,880 --> 00:05:22,880
agriculture, to healthcare to others. And we can act as a partners outsource chief AI officer

58
00:05:22,880 --> 00:05:28,720
to help a partner build a AI function, channel team, develop IP. And all of those we help partners,

59
00:05:29,600 --> 00:05:34,480
we will cook for you, but also teach you how to cook so that after a couple of years, you can

60
00:05:34,480 --> 00:05:40,960
insource the function and be an AI-enabled business in your vertical, which we think can

61
00:05:40,960 --> 00:05:45,200
help a lot of people and help a lot of companies become more effective and more valuable.

62
00:05:45,200 --> 00:05:51,760
And frankly, we are go, there's more to life than your financials, but I think our impact,

63
00:05:51,760 --> 00:05:55,840
we hope actually has a mature impact on the market cap of companies we work with.

64
00:05:56,480 --> 00:06:01,120
Can you give us an example of some of the types of problems that you're helping customers solve?

65
00:06:02,240 --> 00:06:07,440
Let's see, one of our engagements has been with a large agriculture machine in your company.

66
00:06:07,440 --> 00:06:13,280
And I think if you can help a company, you know, reposition from being a traditional agriculture

67
00:06:13,280 --> 00:06:18,640
company to being an AI-enabled agriculture machinery company, then you can build smart agriculture

68
00:06:18,640 --> 00:06:24,800
machinery. We're going to have the same farmer, same farm, but with automation suggestions for how

69
00:06:24,800 --> 00:06:29,840
to control the machine better, you get more crop from the same farm, the same farm. And so this

70
00:06:29,840 --> 00:06:35,760
is direct impact on the farmer as well as not surprisingly on the company building these types of

71
00:06:35,760 --> 00:06:44,320
things. I think, you know, I was just in Latin America actually last week in Colombia visiting

72
00:06:44,320 --> 00:06:49,360
companies in different industries as well, from logistics to manufacturing. And what I'm seeing

73
00:06:49,360 --> 00:06:56,800
is that there's very strong CEO level interest to help companies in all sorts of industry sectors

74
00:06:56,800 --> 00:07:00,720
become AI-enabled. And it's not that, it's not that if you're a, you know,

75
00:07:00,720 --> 00:07:04,240
manufacturing company, you want to become an AI company. It's just that, you know, let someone

76
00:07:04,240 --> 00:07:10,240
else do that. But I think in the future, an AI-enabled manufacturing company can be much more effective

77
00:07:10,240 --> 00:07:16,640
and valuable than one that doesn't. Maybe one lost disruptive ways to technology was to rise

78
00:07:16,640 --> 00:07:23,600
to the internet. And we saw that if you have a shopping mall plus a website, you know, yes,

79
00:07:23,600 --> 00:07:29,200
everyone has to build a website, but that doesn't turn you into Amazon. Or if you're a taxi company

80
00:07:29,200 --> 00:07:35,200
and you build a website, you know, you're not an internet company, instead Uber, Lyft, Grab,

81
00:07:35,200 --> 00:07:43,760
you know, DD are true internet companies. AI arguably is as disruptive as the internet. And so

82
00:07:43,760 --> 00:07:49,200
there will, and then it changes the core of how different companies will compete. What are the

83
00:07:49,200 --> 00:07:52,640
things that help you build a defensive business? What are the things that generate value?

84
00:07:52,640 --> 00:07:56,000
Where do you play? Where do you not play? What is the new strategy? And I think companies able

85
00:07:56,000 --> 00:08:01,200
to figure that out will become, you know, what will survive and thrive. One of the myths we

86
00:08:01,200 --> 00:08:05,520
tell Silicon Valley, which is not true, is that whenever there's a disruptive technology, it's always

87
00:08:05,520 --> 00:08:09,920
a style that's that wouldn't. That's just not true. Where they rise the internet, some startups

88
00:08:09,920 --> 00:08:15,120
that did well include Google, Facebook, and Amazon, but some incumbents that did well include

89
00:08:15,120 --> 00:08:20,000
Microsoft and Apple, which were not internet companies, but became great internet companies.

90
00:08:20,000 --> 00:08:24,400
So what the rise of modern machine learning, and the exciting work that many of you in this

91
00:08:24,400 --> 00:08:29,680
community are doing to, you know, land these technologies, to bring them to fruition, I think

92
00:08:29,680 --> 00:08:34,880
this is very, that the races again on, where their great opportunities for startups,

93
00:08:34,880 --> 00:08:40,240
but incumbents also have a lot of advantages. And if they play their cards right, they can become

94
00:08:40,240 --> 00:08:46,560
very valuable, very effective AI-enabled businesses in their verticals. So what's involved in

95
00:08:46,560 --> 00:08:52,160
playing those cards right? One of the hardest things for companies to embrace AI is to scope

96
00:08:52,160 --> 00:08:59,040
the right set of projects. And so we spend a lot of time, I think we've actually become very

97
00:08:59,040 --> 00:09:03,520
good at working with companies to figure out what you should and should not use machine learning

98
00:09:03,520 --> 00:09:08,400
to do. I think, you know, some people advise a good company is one that's not small.

99
00:09:08,400 --> 00:09:13,440
Maybe actually here's story. When I was, some companies tried to do the biggest, most glamorous

100
00:09:13,440 --> 00:09:18,320
project as probably number one, and that's usually a mistake. At least the failure that then causes

101
00:09:18,320 --> 00:09:22,800
you lose faith, actually says to company back, because you need to regain the faith.

102
00:09:22,800 --> 00:09:29,760
One story, early days of Google Brain, people in Google, where Sophie didn't know how to use deep

103
00:09:29,760 --> 00:09:36,000
learning, or even skeptical about it. So my first internal customer was the Google speech team.

104
00:09:36,000 --> 00:09:39,840
It wasn't the most, you know, it's not web search advertising, right? Speech recognition is a

105
00:09:39,840 --> 00:09:44,720
nice project, but it's not web search advertising. But by making Google speech more accurate,

106
00:09:44,720 --> 00:09:50,720
it helped other teams within Google gain faith in our ability to deliver results. It also taught

107
00:09:50,720 --> 00:09:56,880
the company how to use, you know, deep learning. I remember when I first GPU server, it was just

108
00:09:56,880 --> 00:10:01,440
a server sitting under some guy's desk with a nest of wires. But that told us important lessons

109
00:10:01,440 --> 00:10:07,920
about how to train models on GPUs. After the first successes, second internal customer was Google maps,

110
00:10:07,920 --> 00:10:14,000
where we use a OCR, photo OCR to read house numbers to more accurately place houses on Google maps,

111
00:10:14,000 --> 00:10:18,560
the improved quality map data. So only after those two successes that I then started the most

112
00:10:18,560 --> 00:10:25,200
serious conversation with the advertising team. So, so one lesson from this is I think start small.

113
00:10:25,200 --> 00:10:29,040
It's more important that your first project comes something like speech recognition, you know,

114
00:10:29,040 --> 00:10:35,280
back in the day to help the company learn what it feels like and then to use that to build momentum.

115
00:10:36,400 --> 00:10:41,200
And then I think it's important to form cross functional teams with machine learning experts

116
00:10:41,200 --> 00:10:47,120
and business application experts to brainstorm projects together. One tip I offer a lot of

117
00:10:47,120 --> 00:10:54,080
the companies kind of approach it. Often the number one project that the CEO gets excited about

118
00:10:54,080 --> 00:11:00,320
does actually not the project you should work on. So I recommend to companies to brainstorm at

119
00:11:00,320 --> 00:11:05,440
least half a dozen projects and spend a few weeks deeply evaluating, is it technically feasible,

120
00:11:05,440 --> 00:11:10,400
is it actually valuable and do that before investing, you know, several few months worth of resources

121
00:11:10,400 --> 00:11:17,280
to do that. And I think it's also a quick answer. Several months ago published online

122
00:11:17,280 --> 00:11:22,240
an AI transformation playbook which talks about the sequence of steps from scoping pilot projects

123
00:11:22,240 --> 00:11:26,320
to building a team, to providing training, to thinking through your strategy, to even

124
00:11:26,320 --> 00:11:31,520
communications, including some of the pre IPO companies you work with, you know, value,

125
00:11:32,480 --> 00:11:39,440
mature, communicating their AI value thesis creation clearly. But that type of AI transformation

126
00:11:39,440 --> 00:11:42,160
playbook which can find online tell the company to become AI Naval.

127
00:11:42,160 --> 00:11:48,720
Yeah, one of the things that I find fascinating in speaking to folks that are leading these

128
00:11:48,720 --> 00:11:55,520
efforts is the challenge of managing the portfolio of projects. They need to start small and kind of

129
00:11:55,520 --> 00:12:02,560
pick off the easy wins, but they also need to maintain that vision, the overall promise of AI

130
00:12:02,560 --> 00:12:09,920
and what it can lead to so that they can continue to drive enthusiasm. Do you see that as well?

131
00:12:09,920 --> 00:12:18,080
That's interesting. I think the panel of the company, some of the things that is important

132
00:12:18,080 --> 00:12:24,880
is like go-de-locks principle for AI, right, to not be over the optimistic. So HGI is not around

133
00:12:24,880 --> 00:12:34,000
the corner, at least unless Elon Musk has a secret lab somewhere. But also not to. But also not be too

134
00:12:34,000 --> 00:12:40,080
pessimistic because there's a lot it can do if you under-ame then your competitors or someone else,

135
00:12:40,080 --> 00:12:48,720
you know, you're missing out on opportunities. So I think it's important that not just engineers

136
00:12:48,720 --> 00:12:54,480
know how to do this, but that company management and executives also learn how to do this.

137
00:12:54,480 --> 00:12:59,040
Actually, we're running an event in Columbia. We run these events around the world called

138
00:12:59,040 --> 00:13:05,680
Pine AI, but it was running an event in Columbia and I met this engineer who came up to me and

139
00:13:06,400 --> 00:13:11,680
she said, Hey Andrew, I love your AI for everyone course. And this is a very technical person,

140
00:13:11,680 --> 00:13:15,920
right, AI for everyone was scope for a non-technical audience and she said, love your AI

141
00:13:15,920 --> 00:13:20,240
for everyone course, not for myself, but I'm getting tons of non-engineers to take it and just

142
00:13:20,240 --> 00:13:25,520
making them much easier for me to work with. And I find that in a company, if the management

143
00:13:25,520 --> 00:13:29,840
structure, the product managers, the executives, even the C suite executives have a basic on the

144
00:13:29,840 --> 00:13:33,440
survey AI, then the machine learning team or the data science team is actually much better

145
00:13:33,440 --> 00:13:41,360
set up for success. When organizations do make that commitment and initial investment, spin up

146
00:13:41,360 --> 00:13:48,480
teams, how well do you think enterprises are doing and getting value out of the other end of

147
00:13:48,480 --> 00:13:54,240
their ML investments? I think it's really hard. We've seen the launch companies, you know,

148
00:13:54,240 --> 00:13:59,680
the mature, relatively sophisticated technology companies are getting pretty good at building

149
00:13:59,680 --> 00:14:06,480
and deploying machine learning systems, but I think a couple of challenges. One, AI grew up

150
00:14:06,480 --> 00:14:12,320
in consumer internet companies, which just have a lot of data, 100 million users or billion users,

151
00:14:12,320 --> 00:14:18,960
you have a lot of data. In other industries, you often don't have that much data. So if you're

152
00:14:18,960 --> 00:14:25,280
manufacturing plant, doing visual inspection, using computer vision to tell if smartphone is

153
00:14:25,280 --> 00:14:29,280
scratch or not, you don't have a million pictures of scratch smartphones because you just

154
00:14:29,280 --> 00:14:35,760
fortunately did not manufacture a million scratch smartphones. So I think how do you get

155
00:14:36,400 --> 00:14:41,680
machine learning to work with small data instead of big data is an important technical challenge.

156
00:14:41,680 --> 00:14:46,240
And then it turns out, some of the processes we use in big companies, I mean, so when I was deploying,

157
00:14:46,240 --> 00:14:52,320
actually, actually, so, you know, deploying a launch speech recognition system, right? Well,

158
00:14:52,320 --> 00:14:57,840
what happens? You deploy the model and then the world gives you data that's different than what you

159
00:14:57,840 --> 00:15:03,120
had in your test set, stored in your hot disk, right? So again, in the early days, the conversations

160
00:15:03,120 --> 00:15:08,400
we go like this, the machine learning is to say, wow, look, I do so well on the test set,

161
00:15:08,400 --> 00:15:13,040
and then the business people will say, no, look, the customers are doing these crazy things,

162
00:15:13,040 --> 00:15:18,320
they're talking a car, there's background noise, your speech recognition engine doesn't work.

163
00:15:18,320 --> 00:15:22,880
And then the machine learning says, look, I did so well on the test set. What are you talking about?

164
00:15:24,000 --> 00:15:29,120
And I think, I think, you know, the machine learning world, I think now, more machine learning

165
00:15:29,120 --> 00:15:33,520
people realize our job is not to do well on the test set you have on the hot disk. The job is to

166
00:15:33,520 --> 00:15:39,680
build a product, move the product or the business forward. But if you look at how we use to solve

167
00:15:39,680 --> 00:15:44,400
these problems, what we used to do was, if we ship a speech system and for whatever reason,

168
00:15:44,400 --> 00:15:50,400
the performance degrades, then we would monitor it, alarm it, you know, and then page at UT,

169
00:15:50,400 --> 00:15:55,360
whatever, and then someone like me would go, hey, you know, you 20 engineers, there's a problem,

170
00:15:55,360 --> 00:15:59,920
please go and fix it. Now, the launch companies could do that. Every time there's a, you know,

171
00:15:59,920 --> 00:16:04,480
stick them a problem for a major product, we could find 20 engineers to then go and, you know,

172
00:16:04,480 --> 00:16:09,920
page at UT people and fix it. But for other applications where you just cannot afford to do that,

173
00:16:10,640 --> 00:16:15,680
we need better, more systematic ways to monitor, alarm, mitigate. So I think the whole

174
00:16:15,680 --> 00:16:21,200
machine learning world, including I think many of you, you know, coming up with better, I guess

175
00:16:21,200 --> 00:16:27,040
in this community, MLO seems to be a growing term to figure the processes to manage that.

176
00:16:27,040 --> 00:16:33,920
So many organizations, including many of the folks here, are in this transition point that I

177
00:16:33,920 --> 00:16:40,800
described earlier. They've launched successful proof of concepts, they've generated excitement

178
00:16:40,800 --> 00:16:45,840
within their organizations, and now they need to scale up so they can get more models out into

179
00:16:45,840 --> 00:16:53,040
production. What have you seen working in those organizations that are similarly situated,

180
00:16:53,040 --> 00:16:57,360
that are scaling up their ability to, you know, drive real value out of these models?

181
00:16:58,880 --> 00:17:04,960
I think that, I know that this conference talks about platforms, and I think platforms are

182
00:17:04,960 --> 00:17:10,400
important, but the hopes of the technical aspects of scaling these up, I think what makes

183
00:17:10,400 --> 00:17:18,560
scaling up hard is even more the people, business aspects of it, although the technical aspects

184
00:17:18,560 --> 00:17:24,320
of it are really important to, you know, actually after Sam and I were chatting a couple days ago,

185
00:17:24,320 --> 00:17:29,440
and after our conversation, it was very interesting. I went back and reread the 30-year-old paper

186
00:17:29,440 --> 00:17:36,240
by Fred Brooks titled No Silver Bullet, and it made the point that even as we improve

187
00:17:36,240 --> 00:17:41,760
programming languages, software engine is still hot, right? Now I'm really glad that I could

188
00:17:41,760 --> 00:17:48,880
code in Python and not, you know, Fortran or Assembly or something. I think we're all happy about that.

189
00:17:50,000 --> 00:17:55,680
Even though we now have Python and these fancy tools, TensorFlow PyTorch, software engine

190
00:17:55,680 --> 00:18:02,880
is still really hot, and that's because the tools we have did not remove the essential complexity,

191
00:18:02,880 --> 00:18:06,160
this is Fred Brooks' term, the essential complexity of software engineering, which is a

192
00:18:06,160 --> 00:18:11,120
thing through clearly what you want to do to write the specification, to then express it, and then

193
00:18:11,120 --> 00:18:18,000
to test it. And the hot part of software engineering is not, you know, do you use a go-to statement,

194
00:18:18,000 --> 00:18:23,440
or do you use a fancy wild loop or whatever. It is thinking through clearly what is the problem,

195
00:18:23,440 --> 00:18:28,720
and what are the steps needed to solve it. Now, one of the beautiful things about

196
00:18:28,720 --> 00:18:35,040
modern machine learning is that it removed essential complexity from a task. As in, you know,

197
00:18:35,040 --> 00:18:40,160
10 years ago, if you're building a computer vision system, it was really complicated. You would

198
00:18:40,160 --> 00:18:46,560
download these, now, maybe, arguably, obsolete algorithms, you know, sift, surf, hog,

199
00:18:46,560 --> 00:18:52,160
have these features, then you do something crazy about colonization, where you feed it through,

200
00:18:52,160 --> 00:18:56,160
you know, some software library, say OpenCV, you find it doesn't work, the walk the image,

201
00:18:56,160 --> 00:19:00,160
well, it's just crazy complicated set of steps. And you did it on an octave.

202
00:19:01,120 --> 00:19:06,480
Oh, yeah. Actually, OpenCV, which you'll see at the time, I think.

203
00:19:06,480 --> 00:19:14,320
But what deep learning allowed us to do is to say, forget all these steps, let's get a lot of data,

204
00:19:14,320 --> 00:19:19,120
trade in your network on it, and then, and so the workflow changed dramatically with the rise

205
00:19:19,120 --> 00:19:25,600
of deep learning, which is why we're so many more things now. Now, I think just as I'm happy to

206
00:19:25,600 --> 00:19:31,360
use Python instead of, you know, Fortrad, we've been seeing a lot of improvement in developer tools

207
00:19:31,360 --> 00:19:37,760
for building a deploy machine learning. Some of this removes, you know, accidental complexity,

208
00:19:37,760 --> 00:19:42,960
Fredbroxist term. And I think we need more thought on how to remove the essential complexity

209
00:19:42,960 --> 00:19:48,640
at the work we still have, which is, and a lot of my work is meeting with a company, meeting with,

210
00:19:48,640 --> 00:19:53,600
you know, CEOs, leaders, brainstorming, what are the things actually valuable for the business,

211
00:19:53,600 --> 00:19:59,440
thinking about where do you get data, and do they have data, and how do you organize the data,

212
00:19:59,440 --> 00:20:05,520
and then those things are essential complexity things that are still difficult to, to relieve with

213
00:20:05,520 --> 00:20:11,440
today's tools. Which is not to say tooling is not important. The game Python is great, but I think

214
00:20:11,440 --> 00:20:16,400
the heart of what makes a machine learning problem difficult is still thinking through clearly,

215
00:20:16,400 --> 00:20:21,760
what does the problem want to solve, and where to get the data, even though I'm grateful for the

216
00:20:21,760 --> 00:20:26,000
much better ways we have today for expressing the neural network architecture I want to train.

217
00:20:26,000 --> 00:20:32,240
Talking about that, what you refer to as accidental complexity versus kind of essential complexity,

218
00:20:33,040 --> 00:20:43,200
very similar theme is the core mission and motivation behind Airbnb's platform team. They

219
00:20:43,200 --> 00:20:51,920
refer to it as, as their mission is being eliminating the incidental complexity of machine learning,

220
00:20:51,920 --> 00:20:57,680
so that their engineers can focus on the unavoidable complexity that essential complexity that you

221
00:20:57,680 --> 00:21:04,240
refer to, and that has kind of power their development of a whole set of, you know, platforms and

222
00:21:04,240 --> 00:21:11,360
tools to allow them to move more quickly. And when we spoke about this a couple of days ago,

223
00:21:11,360 --> 00:21:17,200
you made an interesting point about one of the success factors being an organization's ability

224
00:21:17,200 --> 00:21:23,760
to iterate quickly. Are there particular things that you've seen organizations do that allow

225
00:21:23,760 --> 00:21:29,440
them to iterate very quickly and do more experiments, understand what's working, what's not, you know,

226
00:21:29,440 --> 00:21:34,880
with a shorter time like? Yeah, so here's one example of a slightly unusual process that some of

227
00:21:34,880 --> 00:21:40,880
my teams use. We're all used to agile, you know, two-week sprints. Some of my teams use a one-day

228
00:21:40,880 --> 00:21:46,000
sprint, so there were up those like this. We wake up in the morning and look at the experiment results

229
00:21:46,000 --> 00:21:51,280
that we had run overnight, and then we do our analysis, gap analysis, our analysis to figure out,

230
00:21:51,280 --> 00:21:55,200
you know, what are the shortcomings of the algorithm? So we do that in the morning. And then,

231
00:21:55,200 --> 00:21:59,520
based on that, we brainstorm what to do, do collect more data, chase their architecture,

232
00:21:59,520 --> 00:22:03,600
our regularization, whatever, brainstorm a bunch of stuff, and then different tasks,

233
00:22:03,600 --> 00:22:07,600
different team members take on different tasks. I get more data. You try that regularization

234
00:22:07,600 --> 00:22:12,560
hyperparameter. You try that, you know, data augmentation, whatever. We write code in the afternoon,

235
00:22:12,560 --> 00:22:18,720
or write code in the morning through, you know, afternoon evening. And then, we run the experiment,

236
00:22:18,720 --> 00:22:23,760
next experiment overnight. And then the next morning, we wake up and look at experiments from

237
00:22:23,760 --> 00:22:29,200
last night, and then iterate the next day, our analysis, plan what to do, run code overnight.

238
00:22:29,200 --> 00:22:34,240
So now, this workflow isn't for everyone. It tends to work best for when the machine learning

239
00:22:34,240 --> 00:22:40,000
job takes, you know, like four or five hours to train, because that workflow fits in well,

240
00:22:40,000 --> 00:22:43,440
with this type of cadence where you code during the day and run experiments overnight.

241
00:22:43,440 --> 00:22:49,280
But I found that if you could, in agile development, we were used to this idea that you should

242
00:22:49,280 --> 00:22:54,640
replan every two weeks, or whatever is your sprint cycle. But in machine learning, it turns

243
00:22:54,640 --> 00:23:00,000
out a lot of workflow of building a machine learning system. It feels more like debugging than

244
00:23:00,000 --> 00:23:08,160
development. And as you know, it's a key insight. And with that, and so this daily sprint cycle,

245
00:23:08,160 --> 00:23:13,280
we found to be good practice for quickly driving down errors of certain types of workflows.

246
00:23:14,160 --> 00:23:18,880
I think a lot of us are still making up, you know, are still indenting these new processes.

247
00:23:18,880 --> 00:23:25,200
I think of how long did it take our community? We went through a lot of versions of version control,

248
00:23:25,200 --> 00:23:33,760
right? From emailing each other, code, email, to get, you know, I guess CVS version get, right?

249
00:23:33,760 --> 00:23:37,840
So I think we're still in the early phases of making up the processes that are suitable for

250
00:23:37,840 --> 00:23:45,200
machine learning workflows. Yeah, I think the kind of contemporary version of emailing files for

251
00:23:45,200 --> 00:23:50,560
version control is probably putting hyper parameters in file names to track experiments.

252
00:23:54,400 --> 00:24:00,160
Yeah, and actually, let me describe to you another, another strange process that we use,

253
00:24:00,160 --> 00:24:04,720
which is talking about version control. So we're pretty good tools for editing and

254
00:24:05,360 --> 00:24:11,040
versioning code. I think we're still, you know, indicating some companies are working on this,

255
00:24:11,040 --> 00:24:18,000
but I think there's still nascent tools for editing and versioning data. And, you know,

256
00:24:18,000 --> 00:24:24,160
and this is one thing you, I think there's a gap between what is done in academia versus what,

257
00:24:24,160 --> 00:24:27,280
you know, we need to do it to build production machine learning systems. Here's one example.

258
00:24:27,280 --> 00:24:34,160
Say we have a test set, train a system, it does poorly on the test set. What do you do? Well,

259
00:24:34,160 --> 00:24:39,840
sometimes we go in and edit the test set, right? Because we, now you can't do that. If you go

260
00:24:39,840 --> 00:24:44,400
to the publisher paper, it's not legit to edit the test and look, I edited the test set and got

261
00:24:44,400 --> 00:24:49,680
great results. You don't want to publish that paper. But from a, from a, from a business production

262
00:24:49,680 --> 00:24:53,200
point of view, sometimes you realize the test set labels are wrong. They don't, they're not

263
00:24:53,200 --> 00:24:57,120
longer business objective. And so you go and edit the test set. And I think that needs to become

264
00:24:57,120 --> 00:25:01,120
part of our accepted workflow. And, and then we need better tools for doing the editing and

265
00:25:01,120 --> 00:25:06,560
versioning it and having multiple people collaborate on editing data. I think these are things that,

266
00:25:06,560 --> 00:25:11,040
I think I feel like, you know, some of my teams, land AI, deep learning AI, I find we're making

267
00:25:11,040 --> 00:25:15,200
about our own processes, but hopefully as we as a committee learn from each other and come up

268
00:25:15,200 --> 00:25:20,640
much better ways to do this. One of the challenges I see is that academia has a much

269
00:25:20,640 --> 00:25:26,160
easy time working on certain problems where progress is, you know, measurable and repeatable.

270
00:25:26,160 --> 00:25:30,720
And if every team edits their test set and changes the labels in a different way, it's,

271
00:25:30,720 --> 00:25:34,960
you know, it's much harder to benchmark, right, who's better at editing test set labels.

272
00:25:35,840 --> 00:25:41,360
So, so there are certain category problems that I think are difficult to study systematically.

273
00:25:41,360 --> 00:25:48,000
And so for good or bad reason, they get less attention in academia. Maybe one, one of the example,

274
00:25:48,000 --> 00:25:52,960
I was at ICML International Conference of Machine Learning, there was a lot of discussion

275
00:25:52,960 --> 00:25:59,200
on robustness, but maybe what happens is a lot of times is let's say, let's say we train the

276
00:25:59,200 --> 00:26:04,800
deep learning system to diagnose from X-rays. And we train on high quality, high-res, X-ray images

277
00:26:04,800 --> 00:26:10,000
taken off of a Stanford University X-ray machine. And we post your paper saying this does really well.

278
00:26:10,000 --> 00:26:15,440
But if you ship your model to, you know, all the hospital down the street with a blurrier X-ray

279
00:26:15,440 --> 00:26:20,240
machine, whether radiologists has a different protocol for how the organ, the patient, it doesn't

280
00:26:20,240 --> 00:26:26,400
generalize, right, doesn't generalize well. So I think we know that this robustness or

281
00:26:26,400 --> 00:26:33,360
generalization problem, it is a problem. But how do you systematically study how well your

282
00:26:33,360 --> 00:26:39,040
learning algorithm can do on a brand new distribution of data, you know, that the algorithm's never seen,

283
00:26:39,040 --> 00:26:46,000
right, so that's just been a relatively more difficult problem to study systematically. So even

284
00:26:46,000 --> 00:26:50,480
though this is a problem that we've seen production all the time, and we have, you know, practical

285
00:26:50,480 --> 00:26:56,800
solutions solving it, I think the amount of attention in academia to this problem is underweighted.

286
00:26:56,800 --> 00:27:02,480
And there are some academic papers, but they tend to be framed in, you know, like domain adaptation

287
00:27:02,480 --> 00:27:07,600
or framed in certain ways that makes it easier to study systematically, whereas sometimes the world

288
00:27:07,600 --> 00:27:13,600
fills you some random data, and so it's been more difficult to formulate benchmarks to drive for

289
00:27:14,160 --> 00:27:22,160
systematic study of some of these issues. Along the lines of robustness when we were chatting

290
00:27:22,160 --> 00:27:27,600
a couple of days ago, you mentioned some interesting things that we're happening at landing around

291
00:27:27,600 --> 00:27:32,880
managing the risk of machine learning deployments once they get in the production. Can you elaborate

292
00:27:32,880 --> 00:27:39,360
on that for us? Yeah, so we've been doing, I feel like my team's doing a lot of strange things

293
00:27:41,920 --> 00:27:48,880
one of the processes I've used before is called the FME analysis, it actually does a set of

294
00:27:48,880 --> 00:27:53,280
processes that grew up in the hardware world where, you know, if you're building an airplane and you

295
00:27:53,280 --> 00:27:58,080
want to make sure the airplane is saved, then you do an analysis on the risk of what could go wrong,

296
00:27:58,080 --> 00:28:04,160
and typically you categorize, what's the chance of this thing going wrong, what's the chance of,

297
00:28:04,160 --> 00:28:10,320
you know, this error on not moving or something, what is the severity, how bad is it if it

298
00:28:10,320 --> 00:28:15,440
does happen, and how detectable is it, right? So those are the three things you try to categorize

299
00:28:15,440 --> 00:28:19,920
each risk again. So one of the things we've been doing is when we look through our machine learning

300
00:28:19,920 --> 00:28:24,320
projects, when you think about how to deploy these projects, you know, in a factory or elsewhere,

301
00:28:24,320 --> 00:28:31,360
my teams tend to, we like going through a very rigorous exercise where we, you know, pre-mortem

302
00:28:31,360 --> 00:28:35,520
or think through all the risks and have a team discuss in debate, but each of the things we could

303
00:28:35,520 --> 00:28:40,160
go wrong, what's the probability, what's the severity, and what's the detectability,

304
00:28:40,160 --> 00:28:45,040
so that we could do a better job seeing around corners and plan for things rather than be surprised

305
00:28:45,040 --> 00:28:52,160
by them later. I think we're seeing a lot of startups, a lot of, there are a lot of machine learning

306
00:28:52,160 --> 00:28:58,240
projects that end up at the proof of concept stage, but unable to go beyond, and I think, you know,

307
00:28:59,600 --> 00:29:06,160
landing AI, putting a lot of thought into how to see around corners so you can

308
00:29:06,160 --> 00:29:12,080
take all these things all the way to deployment and production. So I think we've become pretty good

309
00:29:12,080 --> 00:29:18,640
at that due to processes like these. Are there specific examples of issues that this, the

310
00:29:18,640 --> 00:29:25,280
figure mode and effects analysis process has allowed you to avoid? I would say the one number one

311
00:29:25,280 --> 00:29:33,360
most common bucket is planning for the ways that the real world test sets may be different than

312
00:29:33,360 --> 00:29:37,920
the test sets stored on your heart disc, and whether the consequences, what's your mitigation,

313
00:29:37,920 --> 00:29:42,960
what's your escalation process, and what's your response time, so you can give reasonable SLAs

314
00:29:43,600 --> 00:29:47,680
for what happens to the shipping machine and the system, the world does something crazy and then what?

315
00:29:47,680 --> 00:29:54,240
How broadly applicable do you think this, this way of thinking about deploying

316
00:29:55,280 --> 00:30:00,080
models and AI products is, do you think it's something that everybody should be, you know,

317
00:30:00,080 --> 00:30:06,000
pulling up the Wikipedia page and figuring out how to do, or is it very specific to the kinds of

318
00:30:06,000 --> 00:30:10,800
scenarios you're looking at at landing? I think, I think, my analysis, and then you can look

319
00:30:10,800 --> 00:30:17,440
along with Wikipedia, is a relatively heavyweight process, but maybe for most of my life, most of parts

320
00:30:17,440 --> 00:30:26,720
that I work on, I feel like, for most parts that work on, I tend to like to identify the risks

321
00:30:26,720 --> 00:30:34,080
up front, you know, actually when those ads on Polsera, one of my direct reports gave me,

322
00:30:34,080 --> 00:30:39,040
you know, gave me direct feedback, and she said, hey Andrew, I find my one-on-ones would

323
00:30:39,040 --> 00:30:43,200
you very depressing, because every time we meet, you want to talk about all the things that could

324
00:30:43,200 --> 00:30:47,040
go wrong, but look at all the things going right, why are you always, you know, so depressing,

325
00:30:47,040 --> 00:30:52,880
and then I actually told her, yes, I understand, vision for the bright future, but I tend to spend

326
00:30:52,880 --> 00:30:56,400
most of my time planning for the scenarios that could go wrong so that we don't hit those,

327
00:30:56,400 --> 00:31:02,560
then she was okay after, I gave her that context, but most of my project planning, you know,

328
00:31:02,560 --> 00:31:07,600
is the discipline of having a vision for a bright future, but then also being very explicit in

329
00:31:07,600 --> 00:31:12,400
listing out all the things that could go wrong, so that, so that hopefully we're not, you know,

330
00:31:12,400 --> 00:31:17,440
as surprised as something, so to avoid those outcomes. Having just planned an event,

331
00:31:17,440 --> 00:31:21,760
I can definitely relate to trying to anticipate all the things that could possibly go wrong.

332
00:31:21,760 --> 00:31:28,000
Yeah, and just say, I think, you know, this community, I find the work being done in this

333
00:31:28,000 --> 00:31:33,520
community, really exciting. I think over the past few years, you know, machine learning research

334
00:31:33,520 --> 00:31:38,480
has really raised ahead, our ability to build machine learning models has really raised ahead,

335
00:31:38,480 --> 00:31:45,760
but if this is a set of things, if it's a small set of things needed to do a good machine learning

336
00:31:45,760 --> 00:31:53,440
model, the set of software you need to write to a super product is so much bigger than the work

337
00:31:53,440 --> 00:32:00,800
of building the machine learning model, so the advances in our ability to build high accuracy

338
00:32:00,800 --> 00:32:05,520
machine learning models has raised ahead, and this whole technology improvement is driving a

339
00:32:05,520 --> 00:32:11,040
lot of value, but there are also a lot of other things that I think our whole community is still

340
00:32:11,040 --> 00:32:17,920
figuring out, and I think, you know, the, the ability of organizations like, like you guys

341
00:32:17,920 --> 00:32:23,120
told us now, as well as all of us in this community sharing our learnings with each other,

342
00:32:23,120 --> 00:32:28,160
I think we have important work to do to move the machine learning world forward.

343
00:32:28,160 --> 00:32:32,880
And I think this is especially important if you want machine learning to have an impact

344
00:32:32,880 --> 00:32:38,960
outside the software into that industry. I feel like, you know, the rise of open source tools,

345
00:32:38,960 --> 00:32:46,640
things like TensorFlow, PyTorch, many others, well, the rise of archive papers, right,

346
00:32:46,640 --> 00:32:51,680
knowledge can download for free. That's done a lot to hold machine learning break outside

347
00:32:51,680 --> 00:32:57,200
the software into the industry, which it's going to be a big part of the next wave of

348
00:32:57,200 --> 00:33:02,560
where machine learning needs to go. Well, Andrew, thank you so much for joining us,

349
00:33:02,560 --> 00:33:06,720
and helping me kick off our very first Twimmelcon AI platforms.

350
00:33:06,720 --> 00:33:08,320
That's good. Thank you, Zaz. Thank you.

351
00:33:17,360 --> 00:33:23,040
All right, everyone. That's our show for today. For more information about this and every show,

352
00:33:23,040 --> 00:33:29,680
visit twimmelai.com. Thanks again to all the great sponsors of Twimmelcon AI platforms.

353
00:33:29,680 --> 00:33:35,360
Head over to twimmelcon.com slash sponsors to learn more about each and every one of them.

354
00:33:36,080 --> 00:33:41,440
While you're there, be sure to click on over to the Twimmelcon news page to follow along with

355
00:33:41,440 --> 00:33:56,240
all of the updates coming out of Twimmelcon. Thanks so much for listening and catch you next time.

