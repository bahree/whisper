1
00:00:00,000 --> 00:00:15,880
Hello and welcome to another episode of Twimal Talk, the podcast where I interview interesting

2
00:00:15,880 --> 00:00:20,840
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:20,840 --> 00:00:23,240
I'm your host Sam Charrington.

4
00:00:23,240 --> 00:00:29,000
This week we continue our industrial AI series with Sergei Levine, an assistant professor

5
00:00:29,000 --> 00:00:34,040
at UC Berkeley, whose research focus is deep robotic learning.

6
00:00:34,040 --> 00:00:38,800
Sergei is part of the same research team as a couple of our previous guests in the series,

7
00:00:38,800 --> 00:00:44,720
Chelsea Finn and Peter Rebel, and if the response we've seen to those shows is any indication,

8
00:00:44,720 --> 00:00:47,480
you're going to love this episode.

9
00:00:47,480 --> 00:00:52,480
Sergei's research interests and our discussion focus in on how robotic learning techniques

10
00:00:52,480 --> 00:00:58,760
can be used to allow machines to autonomously acquire complex behavioral skills.

11
00:00:58,760 --> 00:01:03,160
We really dig into some of the details of how this is done, and I found that my conversation

12
00:01:03,160 --> 00:01:08,040
with Sergei filled in a lot of gaps for me from the interviews with Peter and Chelsea.

13
00:01:08,040 --> 00:01:11,800
By the way, this is definitely a nerd alert episode.

14
00:01:11,800 --> 00:01:15,680
Before we jump into the show, I'd like to thank everyone who's taken the time to enter

15
00:01:15,680 --> 00:01:18,400
our AI conference giveaway.

16
00:01:18,400 --> 00:01:23,360
You all know that one of my favorite things to do is to give away free stuff to listeners,

17
00:01:23,360 --> 00:01:28,320
and we've been fortunate to be able to give away tickets to the O'Reilly AI conference,

18
00:01:28,320 --> 00:01:33,200
to lucky Twomo listeners since the very first event in the series last year.

19
00:01:33,200 --> 00:01:38,200
Well, we've got a couple of exciting updates for those of you who want in on this opportunity.

20
00:01:38,200 --> 00:01:44,000
First, we're making it even easier to enter our ticket giveaway for the San Francisco event,

21
00:01:44,000 --> 00:01:48,720
and second, we're giving away two tickets now, not just one.

22
00:01:48,720 --> 00:01:55,560
To enter the contest in 30 seconds or less, just hit pause right now, and visit TwomoAI.com

23
00:01:55,560 --> 00:01:59,200
slash AISF right from your phone.

24
00:01:59,200 --> 00:02:05,640
Finally, a quick thank you to our sponsors for the Industrial AI series, Banzai, and

25
00:02:05,640 --> 00:02:08,440
Wise.io at GE Digital.

26
00:02:08,440 --> 00:02:10,800
By now, you know a bit about Banzai, right?

27
00:02:10,800 --> 00:02:15,640
You've heard me mention their AI platform, which lets enterprises build and deploy intelligent

28
00:02:15,640 --> 00:02:16,640
systems.

29
00:02:16,640 --> 00:02:21,040
Well I actually spent some time in the Banzai offices in Berkeley last week learning more

30
00:02:21,040 --> 00:02:26,480
about that platform and recording an interview with their co-founder and CEO Mark Hammond.

31
00:02:26,480 --> 00:02:30,200
It was a great conversation and I'm really looking forward to getting it up on the

32
00:02:30,200 --> 00:02:32,800
podcast in a few weeks.

33
00:02:32,800 --> 00:02:37,720
In the meantime, I'll reiterate that if you're trying to build AI-powered applications

34
00:02:37,720 --> 00:02:43,920
focused on optimizing and controlling the physical systems in your enterprise, whether robots,

35
00:02:43,920 --> 00:02:48,920
or HVAC systems, or supply chains, you should take a look at what they're up to.

36
00:02:48,920 --> 00:02:53,000
They've got a unique approach to building AI models that lets you model the real world

37
00:02:53,000 --> 00:02:57,880
concepts in your application, automatically generate, train, and evaluate low level

38
00:02:57,880 --> 00:03:03,040
models for your project using technologies like reinforcement learning, and easily integrate

39
00:03:03,040 --> 00:03:07,240
those models into your applications and systems using APIs.

40
00:03:07,240 --> 00:03:13,800
You can check them out at Banz.ai slash TwomoAI, and definitely let them know you appreciate

41
00:03:13,800 --> 00:03:17,400
their support of the podcast and this series.

42
00:03:17,400 --> 00:03:22,760
Last week, I announced wise.io at GE Digital as a sponsor for this series as well.

43
00:03:22,760 --> 00:03:27,360
Wise.io was among the first companies I began following in what I call the machine learning

44
00:03:27,360 --> 00:03:31,000
platform space back in 2012-2013.

45
00:03:31,000 --> 00:03:36,360
I've since interviewed co-founder Josh Bloom here on the show and mentioned the company's

46
00:03:36,360 --> 00:03:39,760
subsequent acquisition by GE Digital.

47
00:03:39,760 --> 00:03:45,120
At GE Digital, the wise.io team is focused on creating technology and solutions that enable

48
00:03:45,120 --> 00:03:51,680
advanced capabilities for the industrial internet of things, making infrastructure more intelligent

49
00:03:51,680 --> 00:03:55,520
and advancing the industry's critical to the world we live in.

50
00:03:55,520 --> 00:03:59,920
I want to give a hearty thanks and shout out to the team at wise.io at GE Digital for

51
00:03:59,920 --> 00:04:03,960
supporting my industrial AI research and this podcast series.

52
00:04:03,960 --> 00:04:08,520
Of course, you can check them out at wise.io.

53
00:04:08,520 --> 00:04:10,520
And now onto the show.

54
00:04:10,520 --> 00:04:21,960
Hey everyone, I am on the line with Sergei Levine.

55
00:04:21,960 --> 00:04:29,080
Sergei is an assistant professor at UC Berkeley in the EECS department and I'm super excited

56
00:04:29,080 --> 00:04:30,520
to have him on the show.

57
00:04:30,520 --> 00:04:31,520
Hi Sergei.

58
00:04:31,520 --> 00:04:32,520
Hello.

59
00:04:32,520 --> 00:04:33,520
How are you doing?

60
00:04:33,520 --> 00:04:35,520
I'm doing well.

61
00:04:35,520 --> 00:04:36,520
Wonderful.

62
00:04:36,520 --> 00:04:41,960
How about we start by having you introduce yourself and talk a little bit about your background

63
00:04:41,960 --> 00:04:48,160
and how you got interested in your current area of research and what that is?

64
00:04:48,160 --> 00:04:49,160
Sure.

65
00:04:49,160 --> 00:04:54,240
So I actually started off in graduate school working on computer graphics and particularly

66
00:04:54,240 --> 00:04:55,240
in computer graphics.

67
00:04:55,240 --> 00:04:59,920
I was really interested in simulating virtual humans, simulating virtual characters.

68
00:04:59,920 --> 00:05:03,600
And the trouble is that if you want to simulate very realistic virtual humans, one of the

69
00:05:03,600 --> 00:05:06,640
things you have to do is you have to simulate intelligence because humans are intelligent

70
00:05:06,640 --> 00:05:08,960
and machines by default aren't.

71
00:05:08,960 --> 00:05:13,960
So a lot of my work turned out to be essentially artificial intelligence work in computer graphics

72
00:05:13,960 --> 00:05:17,400
to get these virtual characters to behave in ways that look plausible.

73
00:05:17,400 --> 00:05:21,720
So from there, I decided that, well, if I have some methods that work reasonably well

74
00:05:21,720 --> 00:05:25,720
in computer graphics, I can create some plausibly realistic virtual humans, perhaps those are methods

75
00:05:25,720 --> 00:05:28,240
that are also applicable, for example, to robotics.

76
00:05:28,240 --> 00:05:32,520
So I did a postdoc after that in robotics, turns out that a lot of the stuff works well

77
00:05:32,520 --> 00:05:34,000
for robots as well.

78
00:05:34,000 --> 00:05:38,800
And a lot of that led to my current work in reinforcement learning and deep learning.

79
00:05:38,800 --> 00:05:39,800
Fantastic.

80
00:05:39,800 --> 00:05:46,280
I noticed on your website that you've got a paper, except that you're speaking at a computer

81
00:05:46,280 --> 00:05:51,720
animation conference, are you still fairly active in the video domain?

82
00:05:51,720 --> 00:05:52,800
Not as much in recent years.

83
00:05:52,800 --> 00:05:57,040
So I think my last paper there was in 2012.

84
00:05:57,040 --> 00:06:00,640
I am giving a guess lecture this summer actually at SCA, that's a symposium on computer

85
00:06:00,640 --> 00:06:04,640
animation to talk about some of the recent progress in deep reinforcement learning.

86
00:06:04,640 --> 00:06:09,000
So actually, since I moved to robotics, actually, a lot of this technology has made actually

87
00:06:09,000 --> 00:06:13,280
a big impact in graphics, and that's really right about now and this past year that's

88
00:06:13,280 --> 00:06:14,600
been registering a lot.

89
00:06:14,600 --> 00:06:19,960
So they invited me to come give a talk to them about how some of this stuff is going.

90
00:06:19,960 --> 00:06:20,960
Fantastic.

91
00:06:20,960 --> 00:06:21,960
Fantastic.

92
00:06:21,960 --> 00:06:27,080
So as you know, we recently had on the show Peter Rebel and Chelsea Finn, who are your

93
00:06:27,080 --> 00:06:34,240
colleagues there at Berkeley, and the conversations I had with those guys were really, really interesting.

94
00:06:34,240 --> 00:06:39,720
And let's maybe take a minute to talk about the research that you're doing in a little

95
00:06:39,720 --> 00:06:43,000
bit more detail and we can dive in deeper.

96
00:06:43,000 --> 00:06:44,000
Sure.

97
00:06:44,000 --> 00:06:48,920
So the area that I work in can be broadly categorized as robotic learning.

98
00:06:48,920 --> 00:06:53,640
So I'm interested in developing algorithms and models that can allow robots to autonomous

99
00:06:53,640 --> 00:06:58,520
who learn very large and complex repertoire of behaviors so that they can take on more

100
00:06:58,520 --> 00:07:02,320
and more of the functionality that we associate with intelligent human beings so that they

101
00:07:02,320 --> 00:07:06,080
can do all the things that are dangerous and pleasant or for other reasons undesirable

102
00:07:06,080 --> 00:07:07,840
for people to do themselves.

103
00:07:07,840 --> 00:07:12,960
And to me, this problem is not just a problem that has a lot of interesting practical implications,

104
00:07:12,960 --> 00:07:16,880
it's also something that I think can serve as a really valuable lens and artificial

105
00:07:16,880 --> 00:07:22,240
intelligence because in the end, we have only one proof of existence of true intelligence

106
00:07:22,240 --> 00:07:24,960
of human beings and human beings are embodied.

107
00:07:24,960 --> 00:07:30,000
So we don't just exist sort of in the ether thinking abstract thoughts, we actually have

108
00:07:30,000 --> 00:07:34,120
a body we interact with the world and the nature of that interaction is very central to

109
00:07:34,120 --> 00:07:37,240
shaping who we are and how and how we reason about things.

110
00:07:37,240 --> 00:07:41,680
So I think that dealing with systems that are embodied systems like robots gives us a very

111
00:07:41,680 --> 00:07:47,480
valuable perspective in understanding how we might be able to construct artificial intelligence.

112
00:07:47,480 --> 00:07:55,880
So more so than some of the non-physical applications of machine learning in AI, including other

113
00:07:55,880 --> 00:07:58,560
deep learning applications like gameplay.

114
00:07:58,560 --> 00:08:05,400
Well, so the thing about other applications of AI is that oftentimes, especially in

115
00:08:05,400 --> 00:08:10,200
things like computer vision, speech recognition, so on, we work with just the perception

116
00:08:10,200 --> 00:08:11,200
half the equation.

117
00:08:11,200 --> 00:08:15,200
So we think about how we can take in data and produce a particular answer.

118
00:08:15,200 --> 00:08:17,760
But the nature of intelligence is much more complex than that.

119
00:08:17,760 --> 00:08:21,880
It's about taking in information, reasoning about it, making decisions, thinking about

120
00:08:21,880 --> 00:08:25,000
the outcomes of those decisions and so on and so on.

121
00:08:25,000 --> 00:08:28,240
Now you mentioned game playing, which has some elements of this.

122
00:08:28,240 --> 00:08:33,080
But one thing that game playing won't let you do is it won't let you tackle the full

123
00:08:33,080 --> 00:08:37,560
complexity and diversity of the real world because the real world is characterized not

124
00:08:37,560 --> 00:08:42,520
just by sequential nature, but also by its diversity, by the sheer number of unexpected

125
00:08:42,520 --> 00:08:47,240
things that might happen in a natural interaction, which computer vision has dealt with for decades,

126
00:08:47,240 --> 00:08:50,240
but without handling the decision making and the game playing handles the decision making

127
00:08:50,240 --> 00:08:53,840
but without handling so much of diversity.

128
00:08:53,840 --> 00:09:00,760
So to what degree is your research and robotic learning kind of integrative across all

129
00:09:00,760 --> 00:09:02,000
these different fields?

130
00:09:02,000 --> 00:09:10,920
Are you specifically focused on pulling together some of the state of the art research from

131
00:09:10,920 --> 00:09:18,160
these various fields or is your domain within robotic learning kind of established and you're

132
00:09:18,160 --> 00:09:19,560
heading down a path that way?

133
00:09:19,560 --> 00:09:22,680
I don't know if that question makes any sense, but if you kind of get a sense from where

134
00:09:22,680 --> 00:09:23,680
I'm going.

135
00:09:23,680 --> 00:09:24,680
I think I see where you're going.

136
00:09:24,680 --> 00:09:28,920
This is actually a very good question and something that for robotics has been sort of

137
00:09:28,920 --> 00:09:34,720
one of these big tensions over the years is that it's often been very tempting for researchers

138
00:09:34,720 --> 00:09:39,760
to think of robotics as fundamentally a systems or integration exercise.

139
00:09:39,760 --> 00:09:43,440
So if you have, let's say, a very effective computer vision system and you have a very

140
00:09:43,440 --> 00:09:48,080
effective, let's say, planning system, well, maybe building an intelligent robot is just

141
00:09:48,080 --> 00:09:52,400
a matter of welding those species together, connecting up the wires and seeing it work.

142
00:09:52,400 --> 00:09:56,320
And a lot of people have hoped for exactly this that by making progress independently in

143
00:09:56,320 --> 00:10:00,400
different domains, we'll get closer and closer to intelligent robots.

144
00:10:00,400 --> 00:10:03,320
Unfortunately, reality hasn't quite panned out that way.

145
00:10:03,320 --> 00:10:07,160
And a lot of robotics will actually lament that if they take sort of the latest image

146
00:10:07,160 --> 00:10:10,640
net train model and put it on their robot and try to use it for object detection in the

147
00:10:10,640 --> 00:10:14,440
wild, it'll actually do a pretty terrible job because the biases that are present in

148
00:10:14,440 --> 00:10:18,000
the kind of data sets that those models are trained on don't really reflect what a robot

149
00:10:18,000 --> 00:10:21,360
will see from its cameras in natural environments.

150
00:10:21,360 --> 00:10:25,560
So I actually think that in order to really get this right, we need to draw on the lessons

151
00:10:25,560 --> 00:10:30,000
in the state of the art models in, you know, game playing, vision and so on.

152
00:10:30,000 --> 00:10:32,680
But at some point, we have to kind of do a lot of that ourselves.

153
00:10:32,680 --> 00:10:37,520
We have to take the lessons, but not necessarily the technical components themselves.

154
00:10:37,520 --> 00:10:40,640
And for that reason, I've actually been a really big advocate of end to end training for

155
00:10:40,640 --> 00:10:45,360
robotic learning where we set up models that include both perception and control and

156
00:10:45,360 --> 00:10:49,680
act to train together to perform the particular task the robot needs to handle instead of relying

157
00:10:49,680 --> 00:10:53,520
on integration of existing components.

158
00:10:53,520 --> 00:10:58,680
In taking a look at your research, I came across a really interesting example of the

159
00:10:58,680 --> 00:11:03,960
effect you're describing, the particular research was where you were training a robot arm.

160
00:11:03,960 --> 00:11:09,920
I think it was a backster robot to tie knots in a rope.

161
00:11:09,920 --> 00:11:15,880
And some of the comments associated with the research on the, I think there was a

162
00:11:15,880 --> 00:11:22,600
get-hud page about it was that, hey, we trained this system on a, I think it was a red

163
00:11:22,600 --> 00:11:23,600
rope.

164
00:11:23,600 --> 00:11:27,720
And, you know, we're working hard to make it work with a white rope also that's a little

165
00:11:27,720 --> 00:11:28,720
bit stiffer.

166
00:11:28,720 --> 00:11:33,800
And we trained it on a background that was a green background and, you know, that doesn't,

167
00:11:33,800 --> 00:11:37,760
we found that that doesn't generalize to other backgrounds.

168
00:11:37,760 --> 00:11:43,760
This is a conversation point that came up with Peter as well, this notion of mastery versus

169
00:11:43,760 --> 00:11:44,760
generalization.

170
00:11:44,760 --> 00:11:48,880
Can you talk a little bit about that and how your research is taking that issue on?

171
00:11:48,880 --> 00:11:49,880
Yeah, absolutely.

172
00:11:49,880 --> 00:11:55,360
So, the backster paper that you're referring to there, what we did is we actually had a robot

173
00:11:55,360 --> 00:11:59,520
practice tying knots, but of course, it was one robot and it was practicing tying knots

174
00:11:59,520 --> 00:12:00,720
in one particular rope.

175
00:12:00,720 --> 00:12:04,480
So the resulting system could do really well at tying knots in that rope, it could kind

176
00:12:04,480 --> 00:12:08,280
of tie knots in ropes that looked a little similar and it pretty much broke down if you

177
00:12:08,280 --> 00:12:13,000
gave it something, you know, a rope that was too thick or too thin or something like that.

178
00:12:13,000 --> 00:12:19,440
But here's the thing that in robotics, there's like oftentimes when we run experiments,

179
00:12:19,440 --> 00:12:22,360
the experiment is the entirety of the data collection process.

180
00:12:22,360 --> 00:12:26,320
So if you imagine an experiment in computer vision, you take all of ImageNet, you train

181
00:12:26,320 --> 00:12:29,040
your model on it and you show its performance.

182
00:12:29,040 --> 00:12:33,040
In robotics, an experiment basically amounts to generating an entire new data set, training

183
00:12:33,040 --> 00:12:35,160
your model on it and then observing its performance.

184
00:12:35,160 --> 00:12:39,240
So of course, if you're generating an entire data set every time, if you have one robot,

185
00:12:39,240 --> 00:12:42,720
just a little bit of time, it's not going to generalize very far.

186
00:12:42,720 --> 00:12:47,120
We did actually try to study at one point what would happen is if we scaled up the style

187
00:12:47,120 --> 00:12:52,280
of technique, we did this actually in partnership with Google, which has quite a

188
00:12:52,280 --> 00:12:55,920
bit more resources as far as deploying large numbers of robots.

189
00:12:55,920 --> 00:12:59,880
And we tried to see actually, like if we run data collection at the scale of something

190
00:12:59,880 --> 00:13:05,600
like ImageNet, can we actually get robotic skills that generalize effectively?

191
00:13:05,600 --> 00:13:11,000
So what we did there is we set up, we called this the ARM farm by analogy to server farm.

192
00:13:11,000 --> 00:13:15,880
We set up a cluster of about 14 robots and we had them basically working day and night

193
00:13:15,880 --> 00:13:18,000
to practice grasping objects.

194
00:13:18,000 --> 00:13:21,840
So we chose grasping because it's something they can do to pretty much any object and

195
00:13:21,840 --> 00:13:25,520
it's also very important for a lot of other robotic manipulation tasks.

196
00:13:25,520 --> 00:13:29,600
And we had them running day and night like this and they collected about 800,000 grasps,

197
00:13:29,600 --> 00:13:31,880
each grasped had maybe five to ten images.

198
00:13:31,880 --> 00:13:35,480
So the total size of the data set was about on the same order of magnitude as ImageNet.

199
00:13:35,480 --> 00:13:39,760
And there we did find that actually the resulting networks that you train on that really large

200
00:13:39,760 --> 00:13:43,200
data set, they do actually generalize effectively to new objects that are completely different

201
00:13:43,200 --> 00:13:44,840
than what they've seen before.

202
00:13:44,840 --> 00:13:49,560
In fact, when you do learning at this larger scale, you can observe some really interesting

203
00:13:49,560 --> 00:13:51,560
or emergent behavior.

204
00:13:51,560 --> 00:13:55,400
One of the things that we were thinking as we did this work as well, grasping is a very

205
00:13:55,400 --> 00:13:56,400
geometric behavior.

206
00:13:56,400 --> 00:14:01,000
So probably the first thing that these systems will learn about is the geometry of objects

207
00:14:01,000 --> 00:14:04,680
in the world still learn that you need to put the finger on one side, put the finger on

208
00:14:04,680 --> 00:14:06,720
the other side and so on.

209
00:14:06,720 --> 00:14:11,240
What we saw, which surprised us a little bit, is that in the earlier stages of training,

210
00:14:11,240 --> 00:14:16,440
when you have maybe 100,000 grasps before we collected the full data set of a million,

211
00:14:16,440 --> 00:14:21,680
in the early stages of training, the network actually didn't pay as much attention to geometry,

212
00:14:21,680 --> 00:14:24,760
but what it did do is it paid a lot of attention to material properties.

213
00:14:24,760 --> 00:14:28,720
It recognized right away that if something was really soft, then it could pinch it and

214
00:14:28,720 --> 00:14:30,040
pick it up really easily.

215
00:14:30,040 --> 00:14:32,560
But if something was rigid, then it couldn't do that.

216
00:14:32,560 --> 00:14:36,640
And this is completely different from how conventional, manually designed grasping systems

217
00:14:36,640 --> 00:14:40,320
tend to work because when you manually design a grasping system, you're going to use

218
00:14:40,320 --> 00:14:44,480
some sort of geometric motion planning and you're going to completely ignore the material

219
00:14:44,480 --> 00:14:45,480
properties.

220
00:14:45,480 --> 00:14:48,800
It's really interesting to us and that's sort of underscored, I think, the value that

221
00:14:48,800 --> 00:14:52,440
you get from using learning through trial and error because you actually learn about

222
00:14:52,440 --> 00:14:55,720
the patterns that are really present in the world rather than the ones that your analytic

223
00:14:55,720 --> 00:14:59,240
model thinks are important.

224
00:14:59,240 --> 00:15:04,320
Are there any other emergent behaviors that you observed in that set of experiments?

225
00:15:04,320 --> 00:15:05,320
Let me see.

226
00:15:05,320 --> 00:15:08,600
So that was the only one that we could pin down in the sense that we could actually measure

227
00:15:08,600 --> 00:15:12,240
it, like we could actually put different objects in front of it and quantify that, yes,

228
00:15:12,240 --> 00:15:14,680
it was really employing the strategy.

229
00:15:14,680 --> 00:15:18,920
And formally, there were a few things that it did tend to do pretty consistently that

230
00:15:18,920 --> 00:15:20,400
I can kind of speculate a little bit about.

231
00:15:20,400 --> 00:15:22,520
I just don't have the hard numbers for it.

232
00:15:22,520 --> 00:15:26,360
Intended to figure out, for example, that if you have something like a brush that you

233
00:15:26,360 --> 00:15:31,000
should pick up the brush by the stiff part rather than the flexible bristles, which is

234
00:15:31,000 --> 00:15:35,440
nice, it tended to figure out that center of masses of objects really matter, especially

235
00:15:35,440 --> 00:15:37,640
for awkwardly shaped objects.

236
00:15:37,640 --> 00:15:40,000
So those were some of the things that it picked up on.

237
00:15:40,000 --> 00:15:42,680
There were also a few mistakes that actually made that we're kind of amusing.

238
00:15:42,680 --> 00:15:48,520
So it just so happened that a lot of the soft things in our training objects were brightly

239
00:15:48,520 --> 00:15:52,840
colored because we bought, you know, we wanted to buy small items of clothing and small items

240
00:15:52,840 --> 00:15:54,680
of clothing, our children's clothing.

241
00:15:54,680 --> 00:15:55,680
And children's clothing.

242
00:15:55,680 --> 00:15:57,000
So we brightly colored.

243
00:15:57,000 --> 00:16:01,480
So it had this association of things that were brightly colored were soft.

244
00:16:01,480 --> 00:16:03,880
And in our test set of objects, we had a pink stapler.

245
00:16:03,880 --> 00:16:07,320
And that pink stapler was just impossible for it to pick up because it was just convinced

246
00:16:07,320 --> 00:16:10,800
that this pink stapler was a soft fuzzy thing and it could just pinch it.

247
00:16:10,800 --> 00:16:16,160
So that's a good example, actually, of the kind of funny data set biases that you can

248
00:16:16,160 --> 00:16:21,080
get that will actually affect you even in real world tasks like this.

249
00:16:21,080 --> 00:16:22,080
Interesting.

250
00:16:22,080 --> 00:16:23,080
Interesting.

251
00:16:23,080 --> 00:16:31,560
When I hear you describe the examples, an example like the pink stapler, it makes me wonder,

252
00:16:31,560 --> 00:16:37,840
you know, to what extent is it possible to layer the traditional object recognition types

253
00:16:37,840 --> 00:16:43,360
of technologies into a model like this, like, should it be able to recognize the stapler

254
00:16:43,360 --> 00:16:50,640
first and then have some higher level abstraction that we're also training on in addition to

255
00:16:50,640 --> 00:16:52,040
just the raw pixels?

256
00:16:52,040 --> 00:16:53,360
Is that something you look at?

257
00:16:53,360 --> 00:16:54,760
Yeah, that's a very good question.

258
00:16:54,760 --> 00:16:57,400
That's actually something that we've thought about a lot here.

259
00:16:57,400 --> 00:17:03,200
So one of the big things that you get out of traditional approaches to dog detection,

260
00:17:03,200 --> 00:17:04,680
it's not actually the models themselves.

261
00:17:04,680 --> 00:17:05,680
It's the data.

262
00:17:05,680 --> 00:17:12,000
There's very large and extremely diverse data sets, label data sets of objects with bounding

263
00:17:12,000 --> 00:17:14,840
boxes, segmentation, and so on.

264
00:17:14,840 --> 00:17:16,920
And it would be really nice to try to use that.

265
00:17:16,920 --> 00:17:20,400
But at the same time, you want to avoid losing the benefit of end-to-end training.

266
00:17:20,400 --> 00:17:24,560
So if you simply run a bounding box detector on what the robot is seeing and then ask

267
00:17:24,560 --> 00:17:27,960
it to pick things up, well, it's not just the bounding box that matters for the grasp,

268
00:17:27,960 --> 00:17:30,800
it has to also understand something about what's in that bounding box.

269
00:17:30,800 --> 00:17:33,920
So you don't want to lose the benefit of the end-to-end training, but at the same time,

270
00:17:33,920 --> 00:17:38,200
you want to somehow get more out of all these auxiliary sources of information.

271
00:17:38,200 --> 00:17:41,600
One of the things that we've been working on a little bit, and this isn't out yet,

272
00:17:41,600 --> 00:17:46,400
but this will be released in probably a couple of weeks, is some work on semi-supervised

273
00:17:46,400 --> 00:17:51,560
learning of robotic skills, where we combine experience from the robot's point of view that

274
00:17:51,560 --> 00:17:56,160
includes the actions that it took and the observations that it saw with kind of a weekly

275
00:17:56,160 --> 00:17:57,960
labeled image data set.

276
00:17:57,960 --> 00:18:02,160
And weekly labeled in the sense that that data set just tells you, does the image contain

277
00:18:02,160 --> 00:18:03,800
the object that the robot needs to use?

278
00:18:03,800 --> 00:18:07,920
If the robot is learning, for example, how to put a cap on a bottle, the weekly labels

279
00:18:07,920 --> 00:18:11,080
might say, does this image contain a bottle or not?

280
00:18:11,080 --> 00:18:14,080
And the idea is that the robot itself, when it's interacting with the world, maybe it

281
00:18:14,080 --> 00:18:18,280
only gets to interact with a few instances of those objects.

282
00:18:18,280 --> 00:18:22,040
So it can use those few instances to understand the physics of the behavior, but it's not

283
00:18:22,040 --> 00:18:26,080
really enough for it to really generalize to understand what the entire class of objects

284
00:18:26,080 --> 00:18:27,400
of this type looks like.

285
00:18:27,400 --> 00:18:32,880
So the weekly labeled data is there to basically show it, what can this skill be applied to?

286
00:18:32,880 --> 00:18:37,160
And the important thing when incorporating this weekly label data is not to lose the benefit

287
00:18:37,160 --> 00:18:38,160
of the intent training.

288
00:18:38,160 --> 00:18:42,760
So in this technique that we develop, we're actually including the weekly label data and

289
00:18:42,760 --> 00:18:46,520
the robot's own experience at the same time in a joint training procedure, rather than

290
00:18:46,520 --> 00:18:50,600
actually splitting things up into components and then trying to wire them up together as

291
00:18:50,600 --> 00:18:52,440
in the more kind of conventional systems approach.

292
00:18:52,440 --> 00:18:54,280
And that turns out to work very well.

293
00:18:54,280 --> 00:18:57,760
Under the hood, the method has kind of an intentional flavor to it, so it basically

294
00:18:57,760 --> 00:19:01,560
learns what kind of objects to pay attention to from the weekly label data and then uses

295
00:19:01,560 --> 00:19:06,520
that attentional mechanism to perform the task at test time.

296
00:19:06,520 --> 00:19:09,400
How do you express weakness in this model?

297
00:19:09,400 --> 00:19:14,160
Well, when I say weekly labeled, I just mean that the images have a label that only tells

298
00:19:14,160 --> 00:19:16,840
you whether the object you care about is present or not.

299
00:19:16,840 --> 00:19:22,680
So you can think of this as a person telling the robot, here are the things that you can

300
00:19:22,680 --> 00:19:24,920
execute this skill on.

301
00:19:24,920 --> 00:19:29,320
So here, lots of pictures of the thing that you can do this task too, and here are all

302
00:19:29,320 --> 00:19:32,440
the pictures of things that you cannot do this task too.

303
00:19:32,440 --> 00:19:33,440
Right.

304
00:19:33,440 --> 00:19:34,440
Right.

305
00:19:34,440 --> 00:19:42,400
And is there a general approach to incorporating in kind of higher level abstractions, higher

306
00:19:42,400 --> 00:19:50,360
level abstractions into models like this, meaning, you know, in the case of a, and going

307
00:19:50,360 --> 00:19:55,800
back to the stapler example, you know, we could do the object detection and determine that,

308
00:19:55,800 --> 00:20:00,600
hey, this is a stapler, but there's also, you know, there are other neural nets that or

309
00:20:00,600 --> 00:20:06,920
other examples that can do geometry detection and things like that and orientation detection.

310
00:20:06,920 --> 00:20:13,280
And I guess the question that I'm trying to get at is it sounds like the general approach

311
00:20:13,280 --> 00:20:18,800
to applying deep learning in this model is, you know, let's just collect a bunch of data

312
00:20:18,800 --> 00:20:23,000
and, you know, throw it at and train on a bunch of data.

313
00:20:23,000 --> 00:20:27,600
And if there are important features, you know, the model will figure it out, the network

314
00:20:27,600 --> 00:20:28,600
will figure it out.

315
00:20:28,600 --> 00:20:34,560
And what I'm curious about is, is that do you, hey, I guess what are the, you know, what's

316
00:20:34,560 --> 00:20:38,520
the, is there an analytical foundation to that assertion?

317
00:20:38,520 --> 00:20:44,200
And if not, are there other ways that folks are looking at incorporating in abstractions

318
00:20:44,200 --> 00:20:53,440
or features into, you know, these models to help them, you know, both generalize and train faster?

319
00:20:53,440 --> 00:20:56,160
So I think there's perhaps a little more to it than that.

320
00:20:56,160 --> 00:21:01,680
So it used to be that when we thought about kind of the, the previous generation, generational

321
00:21:01,680 --> 00:21:06,680
machine learning models, the way that we would imagine using them is exactly when you describe

322
00:21:06,680 --> 00:21:10,080
that we say, okay, we have some edge detector, we have a pose detector, we have some kind

323
00:21:10,080 --> 00:21:14,840
of thing that will analyze local geometry, we'll plug that into the downstream module and

324
00:21:14,840 --> 00:21:16,640
so on and so on.

325
00:21:16,640 --> 00:21:22,520
The thing about deep learning is that the model itself, you know, it's good for making

326
00:21:22,520 --> 00:21:26,080
predictions, but there's nothing kind of unique or special about it.

327
00:21:26,080 --> 00:21:30,360
You can actually have the same model perform multiple tasks.

328
00:21:30,360 --> 00:21:35,080
And that's often not actually that much harder than stapling together two models that each

329
00:21:35,080 --> 00:21:40,760
perform those tasks. So if you want a model that can, you know, segment an image and detect

330
00:21:40,760 --> 00:21:44,240
poses of objects, you could train two separate models and then combine their outputs or

331
00:21:44,240 --> 00:21:46,760
it can just train one model that does both of those tasks.

332
00:21:46,760 --> 00:21:51,360
And the latter is often not actually that much harder, but it has a substantial benefit

333
00:21:51,360 --> 00:21:55,000
which is when you train a single model to perform multiple tasks, it can actually learn

334
00:21:55,000 --> 00:22:00,400
internal representations that share the knowledge that's contained in those two tasks.

335
00:22:00,400 --> 00:22:06,760
So if you were to ask me how I would consider combining, let's say, a object pose detector

336
00:22:06,760 --> 00:22:11,240
and a grasping system, I would much rather train a single model that predicts both pose

337
00:22:11,240 --> 00:22:16,480
and grasp than to take a pose predictor and feed its output into a grasp predictor.

338
00:22:16,480 --> 00:22:19,440
And the reason for that is that the data already has all the information.

339
00:22:19,440 --> 00:22:22,240
There's nothing, you know, magical that's contained in the model that's not already contained

340
00:22:22,240 --> 00:22:24,840
in the data and it's possible to train these joint models.

341
00:22:24,840 --> 00:22:30,040
So I might as well take both data sets and train one model that'll benefit from the shared

342
00:22:30,040 --> 00:22:33,520
structure in both of those tasks, then train two completely destroyed models and then

343
00:22:33,520 --> 00:22:36,000
try to stable them together afterwards.

344
00:22:36,000 --> 00:22:37,000
Right.

345
00:22:37,000 --> 00:22:38,000
Right.

346
00:22:38,000 --> 00:22:47,160
Have you run into situations where there's there are pre-existing models trained on inaccessible

347
00:22:47,160 --> 00:22:48,160
data?

348
00:22:48,160 --> 00:22:52,800
I guess I'm maybe I'm kind of chasing the chasing the tail of the scenario a little bit,

349
00:22:52,800 --> 00:22:56,880
but it sounds like, you know, there may be some corner case where it makes sense to do

350
00:22:56,880 --> 00:23:00,920
that if you don't have access to the data, but you do have access to the model.

351
00:23:00,920 --> 00:23:06,720
But I get the point that in general, the data is the data and if you can train one model

352
00:23:06,720 --> 00:23:12,280
that can build these internal representations, it's much more efficient than trying to

353
00:23:12,280 --> 00:23:17,840
engineer one model that can solve part of the problem and another model that uses that

354
00:23:17,840 --> 00:23:20,680
to solve the thing that you're actually trying to do.

355
00:23:20,680 --> 00:23:21,680
Yeah.

356
00:23:21,680 --> 00:23:25,200
Basically, it's a lot easier for us to compose data sets than it is to compose models.

357
00:23:25,200 --> 00:23:27,200
Right.

358
00:23:27,200 --> 00:23:33,280
So, one of the challenges that comes up that you've spent some time looking at is the

359
00:23:33,280 --> 00:23:37,400
efficiency of training these deep learning models.

360
00:23:37,400 --> 00:23:40,680
Sample efficiency in particular is one of the ways you talk about that.

361
00:23:40,680 --> 00:23:43,880
Can you talk a little bit about that problem and the things you've done there?

362
00:23:43,880 --> 00:23:44,880
Right.

363
00:23:44,880 --> 00:23:48,080
So, I assume you're referring specifically to sample efficiency for deep reinforcement

364
00:23:48,080 --> 00:23:49,880
learning algorithms.

365
00:23:49,880 --> 00:23:50,880
That's correct.

366
00:23:50,880 --> 00:23:55,760
So, deep reinforcement learning algorithms are kind of a funny creature.

367
00:23:55,760 --> 00:24:00,520
Deep learning, like standard deep learning with gradient descent, it's a common perception

368
00:24:00,520 --> 00:24:02,640
that it's inefficient.

369
00:24:02,640 --> 00:24:07,360
And in some sense, it is like we can build very good object detectors, but we need maybe

370
00:24:07,360 --> 00:24:11,520
millions of images to train them, which might seem like a lot, but if you consider what

371
00:24:11,520 --> 00:24:15,280
that model is really doing, it's reasoning about pixels, edges, everything from those pixels

372
00:24:15,280 --> 00:24:20,360
and edges all the way to complex higher level concepts, that's actually pretty sophisticated.

373
00:24:20,360 --> 00:24:23,480
With deep reinforcement learning, though, things get a lot worse.

374
00:24:23,480 --> 00:24:28,880
So, if you look at the kind of sample complexity for learning to play, let's say, a simple

375
00:24:28,880 --> 00:24:33,680
video game like Pong, and there you're going to be looking at millions or even tens of

376
00:24:33,680 --> 00:24:38,680
millions of images for a task with visual diversity that's nowhere near where we see

377
00:24:38,680 --> 00:24:41,440
in conventional, let's say, computer vision data sets.

378
00:24:41,440 --> 00:24:47,440
So, visually it's very simple, physically it's very simple, but you need a lot of samples

379
00:24:47,440 --> 00:24:52,240
to learn that task, and those samples involve actively interacting with an environment.

380
00:24:52,240 --> 00:24:55,440
How it happens to be a simulated environment, so you can run it much faster than real

381
00:24:55,440 --> 00:24:59,760
time on a server, but still something here seems a little out of whack.

382
00:24:59,760 --> 00:25:05,240
Something here is a lot worse than perhaps it should be.

383
00:25:05,240 --> 00:25:08,120
And what's the intuition for why that is the case?

384
00:25:08,120 --> 00:25:09,800
There are a couple of reasons for it.

385
00:25:09,800 --> 00:25:13,440
The short version is that we don't fully understand, but the long version is that there are

386
00:25:13,440 --> 00:25:16,920
a few things that are being done that could perhaps be done differently.

387
00:25:16,920 --> 00:25:21,480
Now, if I knew exactly the answer to this, then of course, I would have a much more efficient

388
00:25:21,480 --> 00:25:26,560
algorithm to give you, but it's possible to guess a few things here.

389
00:25:26,560 --> 00:25:31,800
One of the things is that reinforcement learning provides a much weaker signal than supervised

390
00:25:31,800 --> 00:25:32,800
learning.

391
00:25:32,800 --> 00:25:37,200
So in reinforcement learning, even though it's gradient-based optimization, you don't

392
00:25:37,200 --> 00:25:40,000
really have gradients of the thing that you really care about.

393
00:25:40,000 --> 00:25:44,960
You're sort of estimating them in this very peculiar way, depending on the reinforcement

394
00:25:44,960 --> 00:25:49,040
learning algorithm that you use, so you essentially get a lot less information from every gradient

395
00:25:49,040 --> 00:25:50,040
step.

396
00:25:50,040 --> 00:25:54,400
A lot of reinforcement learning algorithms also tightly couple the collection of data in

397
00:25:54,400 --> 00:25:58,200
the environment and the updating of the model, which is very different from supervised

398
00:25:58,200 --> 00:25:59,200
learning.

399
00:25:59,200 --> 00:26:01,880
So in supervised learning, you first collect a large data set, and then you take many, many

400
00:26:01,880 --> 00:26:04,200
gradient steps on that large data set.

401
00:26:04,200 --> 00:26:09,160
In reinforcement learning, you often interleave collection of data and updating the model

402
00:26:09,160 --> 00:26:12,080
because you need to collect data that agrees with your model.

403
00:26:12,080 --> 00:26:15,480
So if you're learning a policy, you'd like to collect the kind of experience that that

404
00:26:15,480 --> 00:26:18,760
policy will actually see, and you want to do this iteratively.

405
00:26:18,760 --> 00:26:22,000
So that means that you're often throwing out lots of data from old policies that you

406
00:26:22,000 --> 00:26:25,760
can no longer use because your policy has changed, and that prevents you from reusing

407
00:26:25,760 --> 00:26:26,760
old data.

408
00:26:26,760 --> 00:26:28,920
So that can be very harmful for sample efficiency.

409
00:26:28,920 --> 00:26:33,080
In fact, some of the most inefficient methods, methods like policy gradient, that are very

410
00:26:33,080 --> 00:26:36,840
convenient to use in simulation, they're often the most inefficient in the real world

411
00:26:36,840 --> 00:26:39,160
because they can't reuse data.

412
00:26:39,160 --> 00:26:42,800
So we need to look at methods that can reuse old data, these are sometimes called off-policy

413
00:26:42,800 --> 00:26:44,320
algorithms.

414
00:26:44,320 --> 00:26:48,640
Before we go there, can you elaborate on the throwing out of the data?

415
00:26:48,640 --> 00:26:54,240
Is this something that the algorithm is doing as part of the way it's constructed, or

416
00:26:54,240 --> 00:26:58,440
is this something that we're doing manually to tell us a little bit more about what we

417
00:26:58,440 --> 00:26:59,440
mean by that?

418
00:26:59,440 --> 00:27:03,680
Oh, so that's just how a lot of on-policy, policy gradient algorithms work.

419
00:27:03,680 --> 00:27:09,160
So these algorithms will operate as following, they will collect experience from the current

420
00:27:09,160 --> 00:27:14,480
policy, they will compute a gradient descent direction on that experience, they will take

421
00:27:14,480 --> 00:27:19,000
that gradient step, update the policy, and now they need more data from the latest policy,

422
00:27:19,000 --> 00:27:20,000
which has not been updated.

423
00:27:20,000 --> 00:27:24,040
So they have to throw out all the old data and collect a new batch of data.

424
00:27:24,040 --> 00:27:29,280
So if you want a mental picture of what this looks like, if you have a robot that lets

425
00:27:29,280 --> 00:27:33,440
say it's learning to walk, it'll try to walk a couple of times, update its behavior, try

426
00:27:33,440 --> 00:27:36,520
to walk a couple more times, and so on and so on, and that's the reinforcement learning

427
00:27:36,520 --> 00:27:37,520
process.

428
00:27:37,520 --> 00:27:41,120
But you have to remember that each time it changes the behavior like that, it has to

429
00:27:41,120 --> 00:27:44,320
basically collect new experience because you need to understand how well its current

430
00:27:44,320 --> 00:27:45,600
policy is really doing.

431
00:27:45,600 --> 00:27:46,600
Right.

432
00:27:46,600 --> 00:27:50,240
So that can get really, really expensive in terms of the amount of time it needs to spend

433
00:27:50,240 --> 00:27:52,280
collecting experience.

434
00:27:52,280 --> 00:27:56,520
So if you're running stuff in a simulator on a server farm somewhere, then it's okay

435
00:27:56,520 --> 00:27:59,680
you can paralyze all that and everything is reasonable.

436
00:27:59,680 --> 00:28:03,880
But if that's a real physical system that's actually executing those trials, that can

437
00:28:03,880 --> 00:28:06,040
get extremely tight consuming.

438
00:28:06,040 --> 00:28:07,040
Mm-hmm.

439
00:28:07,040 --> 00:28:12,680
Okay, and you're about to talk about some of the ways we can get beyond this.

440
00:28:12,680 --> 00:28:13,680
Right.

441
00:28:13,680 --> 00:28:16,200
So one of the things we can do is we can look at off-policy algorithms.

442
00:28:16,200 --> 00:28:22,280
So these are algorithms that can supplement their training with data from other policies.

443
00:28:22,280 --> 00:28:24,760
So what can you learn from other policies?

444
00:28:24,760 --> 00:28:28,920
Well, intuitively, one of the things that you can learn is you can learn about predicting

445
00:28:28,920 --> 00:28:35,440
future events because the rules of physics and so on, they will hold true regardless

446
00:28:35,440 --> 00:28:38,080
of which policy you're executing.

447
00:28:38,080 --> 00:28:43,000
And the kind of future events that you can predict can range all the way from very detailed

448
00:28:43,000 --> 00:28:46,400
where you're actually predicting, let's say, the entirety of your future observations.

449
00:28:46,400 --> 00:28:49,960
And this is sometimes called model-based reinforcement learning.

450
00:28:49,960 --> 00:28:54,560
Or all the way to something fairly abstract, like the future rewards that you will see.

451
00:28:54,560 --> 00:28:58,880
And this is actually a type of model-free reinforcement learning that's sometimes referred

452
00:28:58,880 --> 00:29:03,080
to as value function estimation or Q-learning that also falls up this category.

453
00:29:03,080 --> 00:29:05,360
But they're all kind of prediction-style methods.

454
00:29:05,360 --> 00:29:09,880
So on the one extreme, you're predicting the entirety of your future sensory observations

455
00:29:09,880 --> 00:29:13,360
and on the other extreme, you're predicting something very abstract, like rewards that

456
00:29:13,360 --> 00:29:15,200
you will see in the future.

457
00:29:15,200 --> 00:29:18,840
And that tends to be more efficient because that allows you to incorporate data from other

458
00:29:18,840 --> 00:29:22,040
policies, including your own past policies.

459
00:29:22,040 --> 00:29:28,200
Hmm. And so, can you talk a little bit about those policies and how they differ from

460
00:29:28,200 --> 00:29:29,200
one another?

461
00:29:29,200 --> 00:29:30,200
Yeah.

462
00:29:30,200 --> 00:29:33,640
So I can talk a little bit about the model-based reinforcement learning because I feel

463
00:29:33,640 --> 00:29:37,480
like this is something that perhaps hasn't gotten quite as much attention in the research

464
00:29:37,480 --> 00:29:41,240
community in recent years because there's been a lot of excitement about model-free reinforcement

465
00:29:41,240 --> 00:29:42,240
learning.

466
00:29:42,240 --> 00:29:46,160
The model-based reinforcement learning, it's perhaps not as far along because the prediction

467
00:29:46,160 --> 00:29:50,680
problem that is trying to solve is a lot harder, but it has a lot of problems for dramatically

468
00:29:50,680 --> 00:29:53,960
improving sample efficiency for two reasons.

469
00:29:53,960 --> 00:29:58,440
The first reason is the one I mentioned that you can use data from other policies.

470
00:29:58,440 --> 00:30:02,320
But the second reason, which is perhaps a little more subtle, is that a model-based reinforcement

471
00:30:02,320 --> 00:30:06,640
learning, every sample has a lot more bits of supervision.

472
00:30:06,640 --> 00:30:10,880
So if you imagine what you're doing when you're, let's say, predicting a value function,

473
00:30:10,880 --> 00:30:16,480
you're predicting one scalar value that's a function of your current observation or state.

474
00:30:16,480 --> 00:30:20,040
When you're predicting everything that will happen in the future, maybe you're predicting

475
00:30:20,040 --> 00:30:24,640
future images that you will see, there are many more bits of supervision in that prediction

476
00:30:24,640 --> 00:30:25,640
problem.

477
00:30:25,640 --> 00:30:29,480
So every single sample actually carries a lot more bits of supervision, and that means

478
00:30:29,480 --> 00:30:32,640
that your model can learn a lot more from each of those samples.

479
00:30:32,640 --> 00:30:35,840
Now the flip side of the coin is that your model is now trying to solve a much harder problem.

480
00:30:35,840 --> 00:30:39,640
It doesn't have to predict just a single scalar value, just a predict an entire image.

481
00:30:39,640 --> 00:30:44,240
So it's sort of a little bit unclear how that shakes out, but potentially the benefit

482
00:30:44,240 --> 00:30:47,360
in sample complexity can actually be quite substantial there.

483
00:30:47,360 --> 00:30:51,880
We've done a little bit of work on model-based reinforcement learning for vision-based tasks,

484
00:30:51,880 --> 00:30:55,920
section on real physical robots, and this is some work that we did that also involved

485
00:30:55,920 --> 00:31:00,120
actually paralyzing data collection across multiple robots, but at a much smaller scale.

486
00:31:00,120 --> 00:31:04,960
So with the grasping, I mentioned that we needed about 800,000 grasped attempts.

487
00:31:04,960 --> 00:31:08,560
For the model-based reinforcement learning, we actually trained a video prediction model

488
00:31:08,560 --> 00:31:13,000
for pushing objects around on a table with about 50,000 pushes.

489
00:31:13,000 --> 00:31:16,920
And that was actually effective for generalizing to new objects and pushing them in new directions

490
00:31:16,920 --> 00:31:17,920
and so on.

491
00:31:17,920 --> 00:31:21,720
Simply by predicting what the robot will see in the future, and then taking the actions

492
00:31:21,720 --> 00:31:25,120
for which that model predicts the kind of outcomes that you want.

493
00:31:25,120 --> 00:31:28,920
So that was already a lot more efficient and it ran on real physical systems.

494
00:31:28,920 --> 00:31:32,640
Now the downside is that because the prediction problem there is so hard, the predictions

495
00:31:32,640 --> 00:31:34,040
were very short range.

496
00:31:34,040 --> 00:31:38,560
So the robot could only execute behaviors maybe with a horizon of two to three seconds.

497
00:31:38,560 --> 00:31:40,680
So these weren't complex behaviors.

498
00:31:40,680 --> 00:31:43,400
And that's because the prediction problem is so hard, but hopefully as we get better

499
00:31:43,400 --> 00:31:46,720
and better video prediction models, which is a very active area of research right now,

500
00:31:46,720 --> 00:31:49,960
these methods will get better and better.

501
00:31:49,960 --> 00:31:52,880
Is that the inverse reinforcement learning problem?

502
00:31:52,880 --> 00:31:56,520
No, this is the model-based reinforcement learning problem.

503
00:31:56,520 --> 00:32:02,320
So when I looked at the, again going back to the backster robot video, it talked a little

504
00:32:02,320 --> 00:32:07,840
bit about this inverse RL where you are, it sounded like you're doing the same thing.

505
00:32:07,840 --> 00:32:12,440
You've got your rope in one state, you have the human move it to another state, and then

506
00:32:12,440 --> 00:32:16,840
you're looking at the action, the robot action that it would take to get it from one state

507
00:32:16,840 --> 00:32:19,760
to another and producing the inverse of that.

508
00:32:19,760 --> 00:32:28,040
Or that becomes the action that the robot takes to move the rope into the rope to a position

509
00:32:28,040 --> 00:32:32,280
that's required to imitate what the human did.

510
00:32:32,280 --> 00:32:37,120
So that's the inverse RL, how are what you just described sounded very similar to that.

511
00:32:37,120 --> 00:32:39,120
So I think what you mean is actually inverse dynamics.

512
00:32:39,120 --> 00:32:40,120
Inverse dynamics.

513
00:32:40,120 --> 00:32:41,120
Okay.

514
00:32:41,120 --> 00:32:44,480
So when you have a model-based reinforcement learning problem, there's actually different

515
00:32:44,480 --> 00:32:46,800
ways that you can represent your predictive model.

516
00:32:46,800 --> 00:32:50,280
The most common way is to build what's called a forward dynamics model.

517
00:32:50,280 --> 00:32:54,480
So forward dynamics means that you're predicting from the present to the future.

518
00:32:54,480 --> 00:32:57,520
So you're looking at your current observation, your current action, and you're predicting

519
00:32:57,520 --> 00:32:59,960
what the next observation will look like.

520
00:32:59,960 --> 00:33:05,040
Inverse dynamics means you're predicting from the future to the action.

521
00:33:05,040 --> 00:33:09,520
So that means that you're looking at your current observation, your future observation,

522
00:33:09,520 --> 00:33:12,200
and you're predicting what action will get you from one to the other.

523
00:33:12,200 --> 00:33:13,200
Right.

524
00:33:13,200 --> 00:33:17,000
So I've got the rope in position A, I've got the rope in position B, what's the action

525
00:33:17,000 --> 00:33:19,200
that's required to get it from A to B?

526
00:33:19,200 --> 00:33:20,200
Exactly.

527
00:33:20,200 --> 00:33:24,440
So it's just another kind of predictive model and they have different pros and cons.

528
00:33:24,440 --> 00:33:27,960
So with a forward model, you can run it forward many steps because you can basically

529
00:33:27,960 --> 00:33:32,400
recursively apply it to its own predictions, but you have to work a little harder to get

530
00:33:32,400 --> 00:33:33,400
the action.

531
00:33:33,400 --> 00:33:37,320
With the inverse model, the action comes right out of the model, but it's difficult to

532
00:33:37,320 --> 00:33:40,800
chain it together because you don't know what the following observation will be because

533
00:33:40,800 --> 00:33:43,440
the model has some predictive observations and predictive actions.

534
00:33:43,440 --> 00:33:47,040
So inverse models are perhaps a little easier to use, they're a little easier to train,

535
00:33:47,040 --> 00:33:50,680
but they're a little harder to use for longer term planning.

536
00:33:50,680 --> 00:33:51,680
Okay.

537
00:33:51,680 --> 00:33:52,680
Okay.

538
00:33:52,680 --> 00:33:57,640
So in the discussion about sample efficiency, one of the things that I came across was

539
00:33:57,640 --> 00:34:01,360
mirror descent, guided policy search, can you talk a little bit about that and where

540
00:34:01,360 --> 00:34:02,360
that fits in?

541
00:34:02,360 --> 00:34:03,560
Sure.

542
00:34:03,560 --> 00:34:11,560
So mirror descent, guided policy search is a technique for optimizing a very complex policies

543
00:34:11,560 --> 00:34:16,040
like deep neural network policies by only using supervised learning to train the policy

544
00:34:16,040 --> 00:34:17,040
itself.

545
00:34:17,040 --> 00:34:19,680
And that sounds a little bit funny because if we're doing reinforcement learning, well,

546
00:34:19,680 --> 00:34:21,680
that's not supervised learning.

547
00:34:21,680 --> 00:34:25,360
So mirror descent, guided policy search sort of plays this game where it tries to figure

548
00:34:25,360 --> 00:34:29,960
out what is the supervision that I can give to a supervised learning algorithm such that

549
00:34:29,960 --> 00:34:35,600
when it trains some complex policy, that policy will do the right thing.

550
00:34:35,600 --> 00:34:39,720
So it's like if you know that you're that only supervised learning is ever allowed to

551
00:34:39,720 --> 00:34:43,240
touch the neural net, what can you give to the supervised learning algorithm so that it

552
00:34:43,240 --> 00:34:47,280
does the right thing for solving a reinforcement learning problem?

553
00:34:47,280 --> 00:34:51,840
And the way that the algorithm works is something like this that you're going to basically

554
00:34:51,840 --> 00:34:58,120
have a model based teacher that's going to generate training data for your supervised

555
00:34:58,120 --> 00:35:03,040
learning algorithm, so that model based teacher is, it's a kind of model based URL method,

556
00:35:03,040 --> 00:35:04,680
but it's not a deep model based URL method.

557
00:35:04,680 --> 00:35:08,360
It's just a, you can think of it almost like a, like a non-parametric algorithm.

558
00:35:08,360 --> 00:35:11,520
So it'll look at a few different trajectories that you took, figure out how to improve each

559
00:35:11,520 --> 00:35:15,680
of those individual trajectories by themselves without reasoning about any policies.

560
00:35:15,680 --> 00:35:19,120
And then that will generate training data so that your neural network can be trained

561
00:35:19,120 --> 00:35:21,320
with supervised learning to do better.

562
00:35:21,320 --> 00:35:24,480
So instead of reinforcement learning, which looks at you at the parameters of your model

563
00:35:24,480 --> 00:35:28,120
and says, how do I change these parameters to be better, in this mirror to send guided

564
00:35:28,120 --> 00:35:32,320
policy search, it actually looks at the trajectories that you executed, fit some model figures

565
00:35:32,320 --> 00:35:36,080
out how those trajectories should be improved, and then as those improvements as training

566
00:35:36,080 --> 00:35:38,520
data for regular supervised learning.

567
00:35:38,520 --> 00:35:42,420
That way the neural net is only ever trained with supervised learning and standard back

568
00:35:42,420 --> 00:35:43,420
problem.

569
00:35:43,420 --> 00:35:44,420
Okay.

570
00:35:44,420 --> 00:35:47,360
But at a very high level, the reason that this procedure is efficient has a lot to do with

571
00:35:47,360 --> 00:35:51,440
why model based our algorithms are efficient, because it really is a kind of model based

572
00:35:51,440 --> 00:35:55,200
on real algorithms, it's just one that, under the hood, uses standard supervised learning

573
00:35:55,200 --> 00:35:58,560
to train the policy neural network.

574
00:35:58,560 --> 00:36:04,720
So there's another interesting paper I came across, and that was the one on policy sketches.

575
00:36:04,720 --> 00:36:09,160
Can you talk a little bit about that work and what the goals are and what the results

576
00:36:09,160 --> 00:36:10,160
were?

577
00:36:10,160 --> 00:36:11,160
Yeah, I'd be happy to talk about that.

578
00:36:11,160 --> 00:36:14,280
So that was worked by a student named Jacob Andreas, together with Professor Dan Klein,

579
00:36:14,280 --> 00:36:18,620
who's another professor here at UC Berkeley, Jacob and Dan, they both worked on natural

580
00:36:18,620 --> 00:36:20,400
language processing.

581
00:36:20,400 --> 00:36:26,800
So the premise in that paper is that we'd like to see how symbolic descriptions of tasks,

582
00:36:26,800 --> 00:36:30,800
you can think of these as very, very simplified natural language, how symbolic descriptions

583
00:36:30,800 --> 00:36:33,880
of tasks can be used to improve learning.

584
00:36:33,880 --> 00:36:38,600
And the key ingredient there is that we'd like to basically see how symbolic descriptions

585
00:36:38,600 --> 00:36:42,400
can improve learning without assuming that those symbolic descriptions are grounded.

586
00:36:42,400 --> 00:36:46,120
So without assuming that the agent already understands what the symbols mean.

587
00:36:46,120 --> 00:36:49,400
So if you, let's say go to a foreign country, let's say you don't speak French and you go

588
00:36:49,400 --> 00:36:54,720
to France, and someone tells you, in French, how to, let's say, make a piece of furniture

589
00:36:54,720 --> 00:36:55,960
out of wood.

590
00:36:55,960 --> 00:36:58,480
And then they tell her how to make another piece of furniture out of wood, and then they

591
00:36:58,480 --> 00:37:00,560
tell you how to make a bench out of wood.

592
00:37:00,560 --> 00:37:03,400
Well, listening to those descriptions, you'll probably notice some common patterns.

593
00:37:03,400 --> 00:37:07,200
You'll probably notice that some words repeat, and if you hear enough of these descriptions

594
00:37:07,200 --> 00:37:11,440
and you actually perform those tasks and you kind of understand physically what it means,

595
00:37:11,440 --> 00:37:15,120
you'll find those patterns, even if you don't actually speak the language, and eventually

596
00:37:15,120 --> 00:37:18,880
when you hear a new phrase describing new items that you can construct out of wood, for

597
00:37:18,880 --> 00:37:23,200
example, you might be able to put the pieces together and figure that out more quickly.

598
00:37:23,200 --> 00:37:24,840
So that was kind of the idea that we were working with.

599
00:37:24,840 --> 00:37:29,960
So what Jacob did is he constructed this sort of simplified version of a Minecraft video

600
00:37:29,960 --> 00:37:30,960
game.

601
00:37:30,960 --> 00:37:32,240
So it's like a little crafting video game.

602
00:37:32,240 --> 00:37:34,880
It was simplified because we didn't want to deal with vision, we just wanted to deal

603
00:37:34,880 --> 00:37:38,240
with kind of simple kind of top-down navigation problems.

604
00:37:38,240 --> 00:37:41,680
And it had these tasks that were like, you know, pick up the wood, or chop it on the

605
00:37:41,680 --> 00:37:46,680
tree, pick up the wood, make the chest, for example, chop it on the tree, get the wood,

606
00:37:46,680 --> 00:37:52,120
make a boat, or, you know, grab the coal, put it in the oven, and so on.

607
00:37:52,120 --> 00:37:55,480
And there was a long list of these different tasks that the agent could perform that were

608
00:37:55,480 --> 00:37:59,200
constructed out of these symbolic verbs, essentially.

609
00:37:59,200 --> 00:38:02,840
And the agent would be given a set of these tasks, it would learn them.

610
00:38:02,840 --> 00:38:05,560
And the symbolic descriptions would just be given as an additional input.

611
00:38:05,560 --> 00:38:08,600
So they would result in some decomposition of the neural net, but there's actually

612
00:38:08,600 --> 00:38:09,920
different ways you could do that.

613
00:38:09,920 --> 00:38:12,800
But essentially they would be provided as an input to the agent without telling it what

614
00:38:12,800 --> 00:38:13,960
those symbols really mean.

615
00:38:13,960 --> 00:38:17,240
And just by learning the different tasks with the different symbolic descriptions, it

616
00:38:17,240 --> 00:38:21,200
could actually figure out how to then use new symbolic descriptions to solve new tasks

617
00:38:21,200 --> 00:38:23,040
more quickly.

618
00:38:23,040 --> 00:38:24,040
Interesting.

619
00:38:24,040 --> 00:38:31,520
It's funny when we talk about learning objects, object detection, and images, you know,

620
00:38:31,520 --> 00:38:37,800
the amount of data that is required to train a neural network to figure out what an object

621
00:38:37,800 --> 00:38:43,680
represents seems so large compared to our ability as humans to do it.

622
00:38:43,680 --> 00:38:48,520
This is an example where I would need at least a million examples of the French sentence.

623
00:38:48,520 --> 00:38:53,360
So not knowing if I didn't know French, you know, I can imagine needing a ton of examples

624
00:38:53,360 --> 00:38:58,360
of training examples for myself to be able to figure out the language and then how to

625
00:38:58,360 --> 00:39:00,920
put that together to make some furniture.

626
00:39:00,920 --> 00:39:04,520
But you know, if you spoke Spanish, you'd probably figure it out much more quickly.

627
00:39:04,520 --> 00:39:05,520
And that's true.

628
00:39:05,520 --> 00:39:06,520
Ah, this is true.

629
00:39:06,520 --> 00:39:10,360
And I think that actually there's something to that as far as how the learning-based systems

630
00:39:10,360 --> 00:39:11,960
can work better.

631
00:39:11,960 --> 00:39:14,720
I talked before about multitask learning.

632
00:39:14,720 --> 00:39:19,680
And one of the things that distinguishes humans from these learned models that humans

633
00:39:19,680 --> 00:39:23,760
are actually always doing multitask learning, we're always doing multiple things at once.

634
00:39:23,760 --> 00:39:27,600
We're looking for things in our environment, doing something, we're worrying about we're

635
00:39:27,600 --> 00:39:31,160
going to have for dinner, we're worrying about some other stuff, we're observing some

636
00:39:31,160 --> 00:39:35,360
interesting, you know, car that we see on the road over there, we're always doing many,

637
00:39:35,360 --> 00:39:36,840
many things.

638
00:39:36,840 --> 00:39:41,360
And perhaps a lot of our efficiencies actually down to this fact that we're never learning

639
00:39:41,360 --> 00:39:43,480
anything truly from scratch.

640
00:39:43,480 --> 00:39:47,520
Because we're learning so many things all at once, any new thing that we have to do,

641
00:39:47,520 --> 00:39:52,240
we get a broad basis of knowledge in which to draw to figure out that new thing.

642
00:39:52,240 --> 00:39:57,160
So in a sense, perhaps what we're doing is we're actually extremely broad kind of multitask

643
00:39:57,160 --> 00:39:58,160
learners.

644
00:39:58,160 --> 00:40:01,800
And maybe that's a big part of how we get that efficiency.

645
00:40:01,800 --> 00:40:05,320
And what's the relationship between multitask and transfer learning?

646
00:40:05,320 --> 00:40:08,840
Well, monthly task learning is one of the ways to get transfer learning.

647
00:40:08,840 --> 00:40:13,600
Right, so in multitask learning, we're learning multiple things in parallel.

648
00:40:13,600 --> 00:40:20,240
And in transfer learning, we are transfer learning is a broader idea that includes taking

649
00:40:20,240 --> 00:40:23,560
pre-trained models and using them, applying them to other things.

650
00:40:23,560 --> 00:40:29,280
I guess the direction that, you know, the curiosity that has been peaked is like, how do

651
00:40:29,280 --> 00:40:36,160
we combine all of these things to, you know, make our ability to train these models even

652
00:40:36,160 --> 00:40:37,160
faster?

653
00:40:37,160 --> 00:40:42,120
Right, well, so one of the things we've been looking at quite a bit actually is how we

654
00:40:42,120 --> 00:40:47,080
can use past experience to accelerate future learning so that we've worked on this in the

655
00:40:47,080 --> 00:40:50,760
context of reinforcement learning, supervised learning, and so on.

656
00:40:50,760 --> 00:40:54,680
There are a number of ways you can approach that problem, but they all sort of boil down

657
00:40:54,680 --> 00:40:59,800
to some version of looking at your past experience, breaking it up into, you know, little

658
00:40:59,800 --> 00:41:03,240
pieces of training data, little pieces of validation data, trying to build your model

659
00:41:03,240 --> 00:41:06,960
such that when it sees that little training data, it'll do well in that little validation

660
00:41:06,960 --> 00:41:10,840
data and do this many, many, many times so that you get a model that's basically good at

661
00:41:10,840 --> 00:41:13,760
quickly adapting to small training sets.

662
00:41:13,760 --> 00:41:17,000
There are different ways that you can construct these types of models that, you know, many

663
00:41:17,000 --> 00:41:21,920
other groups and end us have looked at, but that's sort of the big picture setup.

664
00:41:21,920 --> 00:41:24,200
These are sometimes called metal learning algorithms.

665
00:41:24,200 --> 00:41:28,920
I think that's actually an extremely promising direction for the future to really take deep

666
00:41:28,920 --> 00:41:33,200
learning methods beyond this regime of always relying on really gigantic data sets.

667
00:41:33,200 --> 00:41:37,840
And I think it goes hand in hand with multitask learning that basically that the way that we

668
00:41:37,840 --> 00:41:43,320
can get to the kind of efficiency that we've seen humans is by solving many tasks, solving

669
00:41:43,320 --> 00:41:47,840
those tasks in a metal learning context so that we're using the our past experience of

670
00:41:47,840 --> 00:41:51,360
solving old tasks to accelerate the solving of new tasks.

671
00:41:51,360 --> 00:41:54,800
And then when we encounter new tasks that we hadn't seen before, we'll generalize and

672
00:41:54,800 --> 00:41:58,240
quickly adapt to them.

673
00:41:58,240 --> 00:42:05,400
And does multitask learning necessarily imply a single network across all of the tasks or

674
00:42:05,400 --> 00:42:07,680
are there variations there?

675
00:42:07,680 --> 00:42:08,680
There are definitely variations.

676
00:42:08,680 --> 00:42:13,160
So one of the things that we've studied actually as well as several other groups is how we

677
00:42:13,160 --> 00:42:15,080
can construct actually modular networks.

678
00:42:15,080 --> 00:42:19,080
So networks that will have some components that are shared and some components that are

679
00:42:19,080 --> 00:42:21,200
distinct between tasks.

680
00:42:21,200 --> 00:42:24,680
And the nice thing when you build modular networks, actually the policy sketches paper you

681
00:42:24,680 --> 00:42:27,640
mentioned is an instance of this that also had modular networks.

682
00:42:27,640 --> 00:42:28,960
And you have modular networks.

683
00:42:28,960 --> 00:42:34,160
One of the things that you can observe is that there will actually be kind of interfaces

684
00:42:34,160 --> 00:42:36,840
that emerge naturally between different modules.

685
00:42:36,840 --> 00:42:41,880
So in a robotic context, let's say you might have a module for perception and maybe you

686
00:42:41,880 --> 00:42:46,480
have one module for a color camera and a different module for LiDAR.

687
00:42:46,480 --> 00:42:51,880
And then you have a module for actuation for a robot with four links and a different module

688
00:42:51,880 --> 00:42:54,080
for actuation for robot with three links.

689
00:42:54,080 --> 00:42:55,760
And you can mix and match any combination of these.

690
00:42:55,760 --> 00:42:58,560
You can say, OK, here's a LiDAR robot with four links.

691
00:42:58,560 --> 00:43:01,280
Here's a RGB camera robot with three links.

692
00:43:01,280 --> 00:43:03,480
Train different combinations of these modules.

693
00:43:03,480 --> 00:43:07,080
And then you can actually find that you might get generalization to new combinations of

694
00:43:07,080 --> 00:43:08,680
sensors and robots.

695
00:43:08,680 --> 00:43:13,160
And you can figure out that bottleneck between the two modules actually constitutes a kind

696
00:43:13,160 --> 00:43:15,160
of a learned interface.

697
00:43:15,160 --> 00:43:18,640
Because different modules, they have to basically adopt the common interface because they don't

698
00:43:18,640 --> 00:43:22,960
know who's going to be downstream from them or who's going to be upstream.

699
00:43:22,960 --> 00:43:26,080
And at the systems level, what are the implications of that?

700
00:43:26,080 --> 00:43:32,320
Is it then easy to take one of these modules and drop it into another system or does it

701
00:43:32,320 --> 00:43:33,480
not quite work like that?

702
00:43:33,480 --> 00:43:35,400
Well, I think that's part of the hope.

703
00:43:35,400 --> 00:43:36,840
So I think we haven't seen that yet.

704
00:43:36,840 --> 00:43:40,720
But in the long run, that's, I think, one of the really interesting things about modular

705
00:43:40,720 --> 00:43:47,080
network designs is that perhaps it could actually be possible to use this as a way to combine

706
00:43:47,080 --> 00:43:51,720
the benefit of intent learning with the benefit of modularity to be able to actually train

707
00:43:51,720 --> 00:43:55,320
up some component, let's say, if you're doing autonomous driving, you train up a particular

708
00:43:55,320 --> 00:43:59,160
vision component on one car, maybe supplement it with image net data, and then you just drop

709
00:43:59,160 --> 00:44:00,440
it into a different car.

710
00:44:00,440 --> 00:44:05,600
But then that different car has its own modules for, let's say, controlling the acceleration

711
00:44:05,600 --> 00:44:07,000
or something like that.

712
00:44:07,000 --> 00:44:09,720
So I think that that's part of the hope we're not quite there yet.

713
00:44:09,720 --> 00:44:14,200
This work is still in fairly early stages, but I think that's definitely a really exciting

714
00:44:14,200 --> 00:44:16,720
place that this kind of stuff could go.

715
00:44:16,720 --> 00:44:25,040
The scenarios you just described were all end-to-end trained, at least in the initial system,

716
00:44:25,040 --> 00:44:29,640
they're end-to-end trained as opposed to training module at a time, is that, right?

717
00:44:29,640 --> 00:44:30,640
Right.

718
00:44:30,640 --> 00:44:34,360
So that's actually the, that's the nice thing about modular neural networks as opposed

719
00:44:34,360 --> 00:44:38,520
to modular anything else is that neural networks can be composed.

720
00:44:38,520 --> 00:44:42,720
So if you have a modular neural network, you can still train the whole thing, a combination

721
00:44:42,720 --> 00:44:44,440
of multiple modules end-to-end.

722
00:44:44,440 --> 00:44:48,760
Now when I say end-to-end, there could be different ends, so end-to-end could mean that your

723
00:44:48,760 --> 00:44:53,640
vision system is simultaneously trained on image net recognition and feeding the right

724
00:44:53,640 --> 00:44:57,640
visual representation to a downstream control module to perform some task.

725
00:44:57,640 --> 00:44:59,160
Yeah, it's interesting.

726
00:44:59,160 --> 00:45:05,480
So the general question that I want to get out here is, I think the basis that you've

727
00:45:05,480 --> 00:45:11,240
laid out for end-to-end robotic learning makes a ton of sense.

728
00:45:11,240 --> 00:45:16,080
At the same time, when I talk to folks in industry about how they're using neural networks

729
00:45:16,080 --> 00:45:20,480
in deep learning, and I present this vision of, hey, we're just going to have this one

730
00:45:20,480 --> 00:45:26,040
uber neural network that can figure everything out, invariably, I get back some reaction

731
00:45:26,040 --> 00:45:28,680
that's like, no, no, no, we don't do it like that at all.

732
00:45:28,680 --> 00:45:29,680
It doesn't work.

733
00:45:29,680 --> 00:45:30,680
It's too hard.

734
00:45:30,680 --> 00:45:32,960
How do you account for the gap there?

735
00:45:32,960 --> 00:45:37,320
Do you see similar things, I guess, for one, and how do you account for that gap?

736
00:45:37,320 --> 00:45:42,720
Well, I think one way to look at it is it's a little bit like the difference between gasoline

737
00:45:42,720 --> 00:45:44,840
cars and electric cars.

738
00:45:44,840 --> 00:45:50,240
So it's very difficult to, right now, or maybe even like five years ago, to make the case

739
00:45:50,240 --> 00:45:54,200
that, well, everybody should have electric cars because gasoline cars are really good.

740
00:45:54,200 --> 00:45:57,800
We've been designing them and improving them for almost a century.

741
00:45:57,800 --> 00:46:04,160
So of course, you build the first electric car that's not going to be as nice as a gasoline

742
00:46:04,160 --> 00:46:08,080
car that's benefiting from all those decades of engineering, but I think the technology

743
00:46:08,080 --> 00:46:09,080
is progressing.

744
00:46:09,080 --> 00:46:12,280
So I think the reason that you hear, especially, you know, it's certainly in robotics that

745
00:46:12,280 --> 00:46:17,000
we get this a lot, that you'd like to be able to use something like a manually designed

746
00:46:17,000 --> 00:46:21,160
motion planner on top of your learned computer vision system because that motion planner is

747
00:46:21,160 --> 00:46:25,600
really good, like it's benefited from decades of development.

748
00:46:25,600 --> 00:46:27,880
So that's, I think, maybe the explanation.

749
00:46:27,880 --> 00:46:30,400
Now, the solution, I'm not sure.

750
00:46:30,400 --> 00:46:32,720
I think there are a couple of possible solutions.

751
00:46:32,720 --> 00:46:36,920
One solution is that, well, maybe we should see what makes that really nice, manually designed

752
00:46:36,920 --> 00:46:38,600
component work so well.

753
00:46:38,600 --> 00:46:43,000
Can we incorporate that into a learning system or can we use it as part of an intent system?

754
00:46:43,000 --> 00:46:46,920
So can we use that manually designed, let's say, controller, differentiate through it,

755
00:46:46,920 --> 00:46:50,320
compute gradients and use those to improve our vision system?

756
00:46:50,320 --> 00:46:54,000
Or maybe we just need to make a little more progress in reinforcement learning to the

757
00:46:54,000 --> 00:46:58,760
point where we can replace that component without a loss of capability.

758
00:46:58,760 --> 00:46:59,760
Right.

759
00:46:59,760 --> 00:47:00,760
Right.

760
00:47:00,760 --> 00:47:01,760
Great.

761
00:47:01,760 --> 00:47:03,800
Any other things that you wanted to cover?

762
00:47:03,800 --> 00:47:07,120
No, I think that's everything on my hand.

763
00:47:07,120 --> 00:47:08,120
Fantastic.

764
00:47:08,120 --> 00:47:09,120
Fantastic.

765
00:47:09,120 --> 00:47:13,880
Well, what's the best way for folks to catch up with you to learn more about what you're

766
00:47:13,880 --> 00:47:14,880
up to?

767
00:47:14,880 --> 00:47:19,200
I know you've got a webpage on the Berkeley site that will include a link to in the show

768
00:47:19,200 --> 00:47:20,200
notes.

769
00:47:20,200 --> 00:47:23,840
Are there any other ways for folks that you'd like folks to get in touch with you?

770
00:47:23,840 --> 00:47:25,880
I think my website is a good place to start.

771
00:47:25,880 --> 00:47:30,400
Also for anybody who is interested in learning about deep reinforcement learning, we do have

772
00:47:30,400 --> 00:47:35,360
a course that I and Chelsea and our colleague John Showman taught at UC Berkeley, and that

773
00:47:35,360 --> 00:47:36,680
all that material is online.

774
00:47:36,680 --> 00:47:40,320
So if you search for Berkeley deep reinforcement learning course, you can find all those

775
00:47:40,320 --> 00:47:41,320
lectures.

776
00:47:41,320 --> 00:47:42,320
That can also be a good resource.

777
00:47:42,320 --> 00:47:45,720
But yeah, for getting in touch with me, definitely my website will be a place to start.

778
00:47:45,720 --> 00:47:46,720
Fantastic.

779
00:47:46,720 --> 00:47:51,040
Well, I'll make sure I include the link to the course in the show notes as well.

780
00:47:51,040 --> 00:47:53,360
And thank you so much for being on the show.

781
00:47:53,360 --> 00:47:54,360
Thank you.

782
00:47:54,360 --> 00:47:57,360
Thanks, sir.

783
00:47:57,360 --> 00:48:00,560
All right, everyone.

784
00:48:00,560 --> 00:48:02,880
That's our show for today.

785
00:48:02,880 --> 00:48:07,960
Thanks so much for listening and for your continued feedback and support.

786
00:48:07,960 --> 00:48:12,720
For the notes for this episode, to ask any questions or to let us know how you like the

787
00:48:12,720 --> 00:48:20,480
show, leave a comment on the show notes page at twomolai.com slash talk slash 37.

788
00:48:20,480 --> 00:48:26,720
For more information on industrial AI, my report on the topic or the industrial AI podcast

789
00:48:26,720 --> 00:48:31,480
series, visit twomolai.com slash industrial AI.

790
00:48:31,480 --> 00:48:34,280
The report is complete and it's beautiful.

791
00:48:34,280 --> 00:48:37,360
And I'll be notifying folks who sign up at that page.

792
00:48:37,360 --> 00:48:40,040
How they can receive a copy of it shortly.

793
00:48:40,040 --> 00:48:46,920
Once you're done with this show, take 30 seconds to head over to twomolai.com slash AISF

794
00:48:46,920 --> 00:48:52,840
to enter our giveaway for a free ticket to the AI conference in San Francisco in September.

795
00:48:52,840 --> 00:48:55,920
You could be one of two lucky winners.

796
00:48:55,920 --> 00:48:58,160
Thanks again for listening and catch you next time.

