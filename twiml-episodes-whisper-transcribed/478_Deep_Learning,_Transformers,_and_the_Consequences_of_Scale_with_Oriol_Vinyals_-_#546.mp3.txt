All right, everyone. I am here with Ariel Vinyals.
Ariel is the lead of the deep learning team at DeepMind.
Ariel, welcome to the Twomo AI podcast.
Hey, Sam. It's great to be here. I'm a big fan of the show.
Thanks so much. I'm really looking forward to chatting with you.
This conversation is long overdue. I'd love to get started
by having you share a little bit about your background
and introduce yourself to our audience.
Yeah, absolutely. I mean, this could go a long time,
so I'll try to keep it maybe at a maybe more
recency bias, I guess. But yeah, I've been in the field of
machine learning deep learning since it was quite not as
popular as it is today. So it's been definitely a fun journey.
I'm happy to maybe link this back at the end of our conversation
a little bit. But you know, in the nutshell, I've been
maybe the main passion I've had always is with sequence modeling.
So I started in speech recognition where a lot of deep learning
early days things were going on. And then I transition from
doing my PhD in California in Berkeley to joining Google
Brain in very early days, worked a lot on actually natural
language processing. We had maybe one of the works that
you know, links forward to many of the research that I'm doing
these days is the sequence to sequence work on machine
translation that we did back in the day. And then you know,
my passion for sequences kind of was developing.
At some point it was a good time to actually go back to Europe. I'm
originally from Spain. So I moved to London in 2016 where I joined
deep mind. And since then, you know, I learned a lot actually
about the reinforcement learning. One of the, you know, most fun projects
I've had, you know, a pleasure to work with a large team of people is the
Alpha Star project, you know, involving creating an agent to play
Starcraft. But actually in that creation, a lot of the sequence
modeling background came back as we use a lot of, you know, LSTM's
transformers, all these kind of models that are very popular
thanks to their performance in NLP. And these days, I'm mostly just
focusing on, you know, leading the deep learning team and trying to
just do the usual things that we like to do in deep learning, which is
try to unlock, you know, state of the art or new horizons and benchmarks
in many modalities and as many modalities as we can.
As deep learning has matured, you know, folks often get a lot more
specialized, you know, leading the scope of deep learning,
writ large seems like a very big scope.
Yes, indeed. I mean, it was fun because the way the way it all started, right,
it was literally a lot of like proving yourself in very specific fields,
even very specific applications in those fields. So the very first
results that caught people's attention were actually on speech
recognition. So that started actually mostly in, you know,
thanks to the Toronto group led by, of course, Jeff Hinton.
I got to learn that through my internships at Microsoft Research,
where a lot of action was happening. And then, you know, maybe the most
obviously notable moment that most people would refer to is the image
net moment that really happened almost three, four days, you know,
days, no years, these days, time passes very differently.
But yeah, three or four years after that breakthrough,
the image net moment occurred. And then from there on, it's been an adventure
of basically trying to find where are the frontiers
of what deep learning models can or cannot do. And, you know,
through the years, it turns out that by applying more or less the same
methodology, you know, through gradient descent, new
architectures, but refining ideas, adding the right kind of
inductive biases to our models, et cetera, you end up
then transferring to more and more areas that involve, of course, natural
language, machine translation, then now all sorts of like
language modeling, multimodal language, vision,
generative models. One of the recent successes, of course, of the year was
alpha fold, in which essentially all these tools just were applied to
these very different domains. So expanding to more and more domains
of science in general has been now, you know, it's the day to day and the way
to think, perhaps when you're, you know, thinking more deep learning
in 2021 and beyond. I'd love to have you elaborate a little bit on that
when I kind of think about the field, I tend to think about it
in terms of, you know, there's this, you know, set of work that's been like
applying deep learning to new application areas. There has been, you know,
applying a deep learning approach to, you know,
kind of technical field. That's not a great way to put it, but like,
okay, we've got graph machine learning. Like, how do we do that in deep learning?
You know, we've got reinforcement learning. How do we do that with deep learning?
That kind of thing. And then there's been, you know, a lot of energy, just how
do we make deep learning more computationally efficient and, you know,
make it easier to train that kind of thing? Are you working across all of those
areas or do you even, you know, think about it similarly? How do you kind of,
you know, think about that taxonomy? Yeah, I mean, it's one thing that I tend to,
I found it useful throughout my career. And this, I think, the level of
generality has increased over the years. But the, what you try to do mostly if you're
a deep learning researcher, having been in the field for abilities to try to
just identify commonalities across, you know, modalities or problem settings.
And in a way, I have this sort of argument. And I've given more, obviously,
more detailed technical talks at, you know, like recently about what I call the
deep learning toolbox, right? And I think that is a reasonably, you know,
you can see quite a few examples on one of, you know, some of the major successes
recently on applying these toolbox approach. And by a toolbox, I mean,
we have architectures, um, tricks of the trade on how to train the models like,
you know, like optimization methods, how to use the hardware more efficiently.
Like this is all part of this gigantic toolbox. And then when you're faced with a new problem,
right? If you're truly like embracing sort of the deep learning approach,
you're applying almost always the same first principle, which is you learn everything.
Everything is learned and to end from the output back to the input by training a set of
learnable weights through gradient descent in general. And then you just pick a mix and match
from the toolbox. The precise elements beat, oh, I have a sequence. Okay, I'm going to use a
transformer. It's a very long sequence. Maybe I'll use an LSTM. I have vision. I use
compolutions, et cetera, et cetera. So you're mixing and matching all these components.
And then when faced with a new problem, like folding proteins, which obviously has some
similarities, but in the end is like at the input, you have a sequence of letters, right?
The amino acids that form the protein. And then the output are like these
sequence and ordered set almost of 3D coordinates, right? So when you face with this problem,
you say, okay, I have some data that maps these inputs to these outputs that people in the
community has built over the years, very expensive to do because you need
crystallography and some methods to generate this training data. But then with the deep learning
mindset, you just go at the problem and then iterate over ideally a very nice, you know,
training, validation split, as usually is done in machine learning. Then you start playing with
details that matter a lot actually to unlock performance to attack this particular new modality,
right? So I think it's the right framing to think about what is the next modality? What is a
challenge? Is it a very large graph that we currently are at the, you know, we cannot quite do,
because yeah, we have graph neural nets, but do they scale properly and so on? And that is
generally how then you see a lot of papers at Newribs and other conferences kind of tackle
these new challenges. And what's beautiful about this is that even though there is a theory of
deep learning, that's a field I've gotten a bit into like just because it's just fascinating.
But in general, how the field has advanced is there is a problem out there. You take it, you cannot
change it, but you apply this first principle of end-to-end learning, learnable model, powerful
model, and then hopefully you enable something that that field was not, you know, maybe looking
at at the time. Although nowadays many people have heard about deep learning and it's quite
permitting everywhere. It's mostly you don't have to prove yourself in the maybe it's the same way
that we had to by going one field at a time almost as we were doing in 2008, 2009, up to maybe
2014-15, where things really start taking over and people start paying attention, given the successes
where too many may be to ignore. Two, three years ago, a lot of work was happening
in the area of just getting the basic machinery, working, tweaking the optimizer,
tweaking learning rates, all this kind of stuff. Are you still involved in that kind of work?
Do you think that's accelerating as more people are coming in or slowing down as we've gotten the
basic machinery working? Yeah, it's a good question. I mean, going to kind of all the modalities,
right? The ultimate modality if you extrapolate is, well, all the data is just a sequence of
bytes, right? You can always represent any data structure, input or output as a sequence of
bytes. And that is a very kind of romantic, almost way to think about machine learning,
at least supervised learning problem. Kind of a grand unified theory of deep learning or something?
Yeah, yeah, exactly. So in that sense, what has happened is that sequence models have evolved
enough that we have transformers, which might not be the last iteration over the ultimate model,
right? That will rule all the modalities with the same sort of applying the same model,
the same formula, right? The toolbox maybe just reduces to this one model. We're not quite there yet,
but that is one way to see it. And indeed, as the field is advancing, I think the details
are refined enough that more people can just access these toolbox without being maybe
necessary an expert and see successes reasonably in a reasonable, simple way. Obviously, this goes
hand in hand with the fact that our software has also tremendously advanced with all the frameworks
that exist currently and open source, and et cetera, that exist to kind of lower the barrier
of entry to the field. But I would say that, yes, it's easier to get the details right
more so than it was three years ago. Although, I mean, I think there's still quite a lot of research
to be done indeed. Just kind of hinging off of the, you know, this point that you made around,
you know, sequences and transformers and how we're, you know, almost there. Do you have a feeling
or a bet on kind of what, you know, do we get there? What, you know, is transformers the thing
that gets us there? Is it something that's a slight evolution of transformers? You know, what
what dimensions do you think get us there if you think we get there? Yeah, I think transformers
have evolved, you know, in a very natural, cool way from obviously what has come before transformers,
LSTMs, attention mechanisms, et cetera. But indeed, there are also a lot of limitations and
maybe one way, like there's a very computational sort of framing of machine learning, which is to say,
well, we want to have any modality to any modality. Our model needs to not make many assumptions
over the underlying data. So it kind of adapt to all the tasks that we might be interested in
tackling. I think transformers do quite well here. An obvious challenge that if you look at the
probably the amount of papers tackling this, this probably going to be a few, is the fact that
transformers is still requires kind of, it's very symmetric in the way that it looks at all the data.
I mean, if you think of language modeling, for example, it just looks at every single piece of text,
right? And when you're reading a book, that's not how you're kind of ingesting this information
as you read chapter after chapter. So there's some beautiful work, I think, still to be done in
in the memory mechanism being a bit more hierarchical. Maybe that's lose inspiration from how we,
you know, we think we work in terms of ingesting information and, you know, compressing the information
we ingest. We don't remember every single detail. That being said, computers might not necessarily
need to operate like we do, right? So it is possible that, well, okay, like that's not we cannot do
that, but well, the machines can can can do it. So maybe that is fine. But I think computationally,
there are there are definitely challenges that may make transformers have definitely some limits
on the amount of information they can process effectively in parallel as they ingest these sequences
of information or bytes. It sounds like you're suggesting a kind of a higher level attention
mechanism that more more broadly shapes the way that the transformers learning from the data that
it's presented. Yeah, I think, I mean, there again, and this exists, it just that then it's the
matter of what details how the optimization can be made to work. But I think the indeed some more
forms of hierarchical memory from course to find lots of these ideas existed in computer vision
for years as well. So I think there's going to be a good mix of ideas and iterative processes
until maybe the next model that feels maybe more efficient for for other tasks we might not even
be thinking about. That's that's what I was saying. There's a you need to push the envelope. So
usually it goes hand in hand with a new task that we cannot even dream of doing right now because
yeah, the limitations of the state of the art models. But I think hierarchy in memories, one of my
beds, definitely some work, you know, we've done and we might mean the whole research community is
doing that I think is I'm definitely keeping an eye on. You know, also related to
transformers and the impact that we've seen there is the work specifically happening around
large language models. What kind of work are you doing there? Yeah, I mean large language models
is a very fascinating field that you could you could think there's been a huge paradigm shift
or maybe there's been non-depending on how far you zoom in, zoom back in the past, right? Because
you know, if you look even even in the 50s, Shannon like was already intrigued by the fact that
well, if you have, you know, statistical pieces of text, you could you could actually generate
text from end-gram models. And then, you know, you have to go forward to maybe the neuro-language model
error until we start to scaling this up with the data and the models and GPUs, etc. to start
unlocking indeed state-of-the-art machine translation, which already feels a bit magical because
in machine translation, we're asking the model to translate sentences that definitely have not been
in, are not in the training set. As we know, most of the sentences we add are
they're probably unique. Otherwise, we would not be talking to one another because we could always
predict what is some going to say, what is Oriol going to say. So, but what had, but thanks to
these kind of mix of components, I think the last definitely 10 years, last five years, and then
transformers being the last maybe ingredient that has been added to this mix toolbox, you know,
build that I was mentioning. We've gotten to language models that, yeah, we can sample from and
we can query, but it starts to feel like, oh, this is, if we were talking to someone, right,
behind the scenes, like going back to obviously Turing and Turing test ideas, although maybe that
is not that useful these days, but just thinking like that, you have this pre-trained language model
that you can start treating like an entity that can obviously utter any text, and that's powerful
if it's good enough. And I think that's what changed. We were doing this all along, I would argue,
in NLP for many years, many people have obviously tried to model language. So the basic principle
exists. I mean, you could always query the model to, you know, ask it a question and see how it
reacts. Does it know about color of the sky, et cetera? But recently, thanks to scale, there's
what it's true is that the performance has been so good. And like the change from like the,
you know, millions to billions of parameters has triggered like now, okay, a new dream almost of
what else could we do if we kept scaling on the one side? And also like, look, it's unbelievable
that this that felt like maybe at that end or like sure, we could use language models in a more
complex system that was doing, you know, the traditional chatbot building with rules and so on.
Now, this actually feels like it's in a state that for some applications and with lots of
caveats actually can actually be used and feels like a very powerful tool. And indeed, I mean,
open AI being probably the one that has pioneered this more notably has shown some demonstrations,
right? So at DeepMind, we are definitely looking into this. I mean, the space of large language
models is extremely exciting. And actually, if you look a bit at the history and perhaps
things like you see in projects such as the one I was involved with, which is a StarCraft,
the way I always thought about StarCraft coming from where I come from is that it's just modeling
sequences of words. These words don't are not like English words. They're like instructions on,
you know, that you send to the game engine, like move this piece here and it's a very rich language.
It's an API like language. But you could already start seeing the power of these methods beyond
language to like decision making and agents. And I think this is there's a lot of interesting
parallels that we are already witnessing by like, you know, maybe a like human level, like go
actually the very first steps in AlphaGo where indeed as well similar to modeling,
precise modeling of the probability of the next word in Go, the next word is just a two-dimensional
like position of where you would put the next, you know, piece. But in general, this principle has
been there since ever and then the all that it took is for the performance to be at the level of,
wow, you can really play a game or reply to almost anything you can ask these models and you will
get somewhat sensible replies sometimes enough that this speaks now the interest of many more
and the field is indeed expanding a lot which is super welcome. So you mentioned Starcraft
once again and you are you've got a workshop paper at Nurebs that is a follow-up to the Alpha
Star work. That was maybe 2019 the Alpha Star. Can you maybe give us a refresher on that work and
then talk about what you are presenting at Nurebs this year? Sure, yeah, I mean the I think DeepMine
as a company obviously had its own different stages where, you know, the very early beginnings
on Atari just showing that deep reinforcement learning has to prove itself, right? It's a bit like
deep learning but now a bit more specific. So a way to prove itself is actually indeed to master
ever more complicated domains or environments as we call them in reinforcement learning instead
of being datasets you are optimizing a reward but ultimately actually a lot of the deep learning
kind of main components or ideas apply. So you saw this kind of kind of one at a time, right? Almost
as a curriculum of increasingly, you know, domains like from Atari to Go and Chess and then,
you know, ultimately I would say Starcraft being, you know, much more complex in many different
dimensions as a as a game or a video game. That's what we kind of were doing at DeepMine and maybe
that's the Alpha Star project that regarded Starcraft 2 which is a popular real-time strategy game
was the end of that sort of sequence of demonstrations of, well, these deep parallel principles
really apply to all the domains that we have found interesting in these sense of
games that are complex. They require like a synchronous thinking, partial observability,
all the right kind of interesting properties that may be the same way computer vision went
from emnis which was very interesting until, you know, it became soft and then we moved over,
right? So this is kind of a parallel. I like to kind of visualize almost in this complexity versus
the kind of game that DeepRL was stacking. And Alpha Star essentially did this through an approach
that was not purely deep reinforcement learning. It actually employed imitation learning or offline
reinforcement learning thanks to the massive amount of games that when humans play one another
I recorded anonymously by the company that makes the game. So there's a huge wealth of sequences
of observations and actions, right? That as I said, you could see as a bit of a language
just that expresses moves in the game, right? And then that was kind of the first seed of Alpha
Star, how we reach ground master level at the game was, well, let's take a look at these sequences
that we have at scale. There's actually millions of sequences. So we have a lot of data which is
great in deep learning the more data, the better. And then we learn to imitate which essentially
is applying the same principles as language modeling. So given all the words that we've seen and to
until some point, we're trying to predict the next work or in the case of StarCraft, we're trying
to predict the next move, which is just kind of a complex object like move this unit
onto this position in the map, right? But it's actually very similar to modeling language.
And that first agent was reasonably good. It was actually better than most humans in terms of
the median performance. But then we took it to the next level by then initializing a self kind of
play system, a multi agent system in fact, of many agents that developed different skills and
different strategies in the game. And they played each other for many years actually, like there's
like each agent kind of plays a StarCraft over like 150 years or so. That's what we did in the
nature paper. And that achieves from this median level or above median level to really like the top,
the very top top level of play that then we verified and we published, as you said, indeed in the
in the end tail of 2019. So that was very cool. And obviously our goal was can we just really crack
this game, right? So we took this kind of hybrid approach of initializing the model to imitate
humans and then taking off by just doing self play. And in a way, imitate how humans have discovered
the game by playing online against each other. Recently, we were both motivated by first actually
trying to pose StarCraft or imitating human moves as a challenge for those who study offline
around, which is this area of reinforcement learning where you're not allowed to interact with
the environment. You you can observe kind of agents or people interacting with environments,
but you're not allowed because maybe it's not practical to perhaps go there and to plug your agent
in the environment and try to see if you get a reward. So that's a fascinating area. And what we
thought is look, we have one of the richest offline around data sets, right? Millions of trajectories
from humans of all levels playing the game would it would be great to first try to open this as a
challenge for the community. So one thing we're working on very hard these days actually is to,
you know, finalizing open sourcing. So, you know, anyone can just go download the the code and
just train their own agent based on these imitation principle only. So we're only looking at the
the very kind of first beginning of the whole Alpha Star agent. But at the same time, we were
wondering, look, can we push the performance of these agents? We didn't care to do it at the time
because we knew that multi-agent and self-play and reinforcement learning were going to be successful
at making the agent better. But could we do that? Only imitating human moves. And that is kind of
almost the same if you think of language modeling that you're only imitating next word prediction.
You're never training these models further, but by themselves are quite good. And the answer is
you can push performance. We definitely have beaten like the agent that we published, the initial
agent that was learned to imitate. With some further refinements, we use Mu0, there are details
in the paper that folks can go and read. But with some ideas, very key ideas using further than
just imitating the next move. So from the offline RL community, we're able to beat that first Alpha
Star agent that was very good and see that, of course, what yielded the nature of publication
performance by, I think, over 90% of the time. So there is performance to be unlocked, and this
is only the beginning. We tried a few ideas, but I think this is a very fruitful resource for those
who are interested in this field of offline reinforcement learning to maybe go and tackle
this challenge and hopefully with the source code available. And just the fact that it's such a fun
game to observe and a cool domain, people might go and obviously then take it on and try to
get to a next level of performance just with imitating human moves, which is fascinating to me at
least. Do you think of at least in this context the offline setting and imitation as synonymous?
It seems like they largely are, but that you could also envision other types of processing of
the data set that has some benefit beyond just the imitation itself. Yeah, so I think, I mean,
there's a lot of names. I mean, people call this supervised learning, behavioral cloning,
imitation learning. I think what's powerful about, if you restrict yourself to, I will not interact
with the environment, but I am able to see like what agents interacting with the environment achieved.
I think that the powerful and what offline are really poses is imitation is one part, right? So
imitating the actions very well is what language modeling regards with and it's very important.
But there is also the reward that we observe in a game like StarCraft, who won the game? And you can
also predict the winner, right, offline. You can say, look, I mean, we have this game to players
playing. We have every single action they took. Let's try to imitate that to understand that
very well, like we understand language. But also there is a fact that one of them won. And if we
can model that precisely with a value of action, which is obviously a crucial element of most RL,
then we can even in an offline setting try to find an action that is not just human like, but
maximizes the probability according to our own estimate of the value and maximizes the
probability of winning. And in fact, one of the best performance agents we showed uses Mu0,
which is offline. We never use self play, but Mu0 basically tries to model based on
simulating actions that you may take in the future and then pick those that maximize reward
or value, but according to your own estimates, right. And that extra step we didn't take at the
time. But many people in offline RL obviously are studying not only actions, but also estimating
the reward or value. We found that already enabled this level of performance that we didn't
unlock at the time we were doing the project. But I'm sure there is more on how you train the values,
how do you train the actions, how, what the losses are. There's a lot of toolbox actually
components to be discovered perhaps. And, you know, the usage of benchmarks is critical to advancing
this. So there are quite a few benchmarks in offline RL already. I think StarCraft poses an
interesting one given its complexity in action space and the fact that it's partial observable
and some properties that make, you know, this a unique environment like many others.
So what degree are you seeing, you know, the learnings from the work that deep mind and others
are doing around games kind of translate to, you know, real world, non-game scenarios.
Yeah, that's a great question because I think we're seeing quite a lot like, for instance,
like actually like thinking of Alpha Star and then Alpha Fold. A lot of the work we did in Alpha
Star early days, right. We, you know, transformers just had come up. So we started investing
or seeing maybe good performance with transformers. Maybe the first project that we saw that was Alpha
Star actually. And then transformers and self-attention and some further tools that were developed
specifically thinking about the protein folding problem were developed. And, you know, there's
loose inspiration, right, by the fact that, hey, we know that there's this group that found this
model that, you know, a different research group that the great part of research is you take at
learnings from not only your own company, but of course, any other research institution. And,
you know, loosely speaking, right, there is always, you can find always these connections, right,
that from one project learnings, then it goes to others and so on. From games and reinforcement
learning, actually, I see a shift now with many great examples of just applying the same
areal techniques to other places. There's some that we are applying in language, right,
machine translation, for example. It's difficult, like we have a few works. I'm not sure they're
quite there in terms of breaking the state of the art, but certainly, you know, there, you know,
because you have this, again, going back to maybe the beginning of the conversation,
these deep learning approach that the tools must be generic, then anything you discover in any
specific domain, because you tried not to be very domain specific, then they will naturally
translate to other domains, ideally, right. And perhaps one of the things that
in StarCraft, maybe it's a very simple idea, but I think this one has a lot of potential when
you mix this idea of imitating humans with reinforcement learning refinement, is that we added
distillation laws, which is a pressure of the areal model to actually look still even if it wants to
change the actions because reinforcement learning is a different laws and it's doing different things.
You make some pressure for the model to always sort of imitate a little bit the policy that
it started from that imitates humans. And that was critical there. And in fact, in a lot of now
language model applications, for instance, this principle, I see it kind of sprinkled around,
like because it's quite natural to not want the model to diverge. And this comes from obviously
all their work on model distillation, knowledge distillation, et cetera. So it's, as I said,
there's always these tools that ideally you purpose for some particular reason initially,
but the consequences or the applications later on along the line, they're going to be unprecedented.
And I mean, Transformers is a great example of having been developed for machine translation
now suddenly, they're folding proteins. Even the authors of these papers, it's just sometimes
hard to believe probably, like, how is this happening? And it's a bit random and you need a
bit of lag and the right titles in the papers. And there's a lot of interesting actually randomness
in the field, which makes nearly such an interesting conference, actually, and of all the other
conferences in ML. But yeah, it's cool to think that these are basically in the end tools that we
want to apply generally anywhere. And many times we see these successes transfer over very
surprising areas that even the original inventors did not anticipate. One of the historical challenges
with reinforcement learning, deep reinforcement learning in particular is the sample inefficiency.
You know, that gives rise to topics like fuchsad or related to topics like fuchsad and one-top
learning. You've got a poster at Nureps that is looking at multimodal fuchsad learning
with language models. Is that in the RL context or separate from RL?
That's not in the RL context. But it is definitely in the imitation learning context. And then it's
it's proposing a way to try to leverage a large amount of data and a big language model that we
trained, right? Only on language. But then can this language model with a little bit of extra training
be then tuned to be able to talk about images that it takes as its inputs, right? And I mean,
this is fascinating because it links to fuchsad learning, which is an area that actually has
one of the very first works I did when I joined in mine. There was all these metal learning
papers coming from from the groups and I was very impressed. So one of the works we did was working
on fuchsad learning. What happened with the work is that we propose a new method that I think
it's reasonably simple. But but actually the benchmark proposing that paper is what made,
you know, this this paper more maybe widely known for the benchmark it proposed, which is called
Mini ImageNet. And you know, Mini ImageNet is the task of fuchsad learning where you know,
you just have a few images from one class you've never trained and you know, the the goal is to
classify it better at that chance, right? And you know, there's been a lot of work following up
nicely defined clear benchmark for the community. And the cool thing here is that
without even training at all to do Mini ImageNet or fuchsad learning for images, these language
models, right? Just through imitating just generic knowledge that I found on the internet about
people talking to one another and so on. And with a little bit of fine tuning to understand correlations
between images and language, namely captioning, image captioning is what we pre-trained these models
with. But by freezing the language component, you can then reuse this model to not only do image
captioning, but also Mini ImageNet was one of the tasks that we present in the paper. And it's
impressive that it's not only doing fuchsad learning better than chance, but it's never trained
with the purpose of doing fuchsad learning. You're in a way borrowing the capabilities of fuchsad
learning from the language modeling. And then with a little bit of vision mixed in, you're able to do
fuchsad tasks that you were not even thinking when you were designing the training setting. And like
what you were doing at the time, we were doing at the time with fuchsad learning for image classification,
where we were training the models to do fuchsad learning for image classification. And then they
were good at that. But now it's like, almost you don't train them to do this particular task,
but they actually generalize over different tasks that involve language and vision in different
degrees, of course, of accuracy, not very high. But this is actually a lot of work that I see in
the vision and language intersection is going this direction. And it's quite exciting to see all
the amazing work that appears as well at Neurib's and computer vision conferences like. But it
doesn't have RL, although you could always use RL to fine tune the models further. But in general,
that has not been adopted yet too much, I guess, in these communities. We spoke earlier about
transformers as kind of this, you know, innovation frontier, you know, for lack of a better term,
and kind of where a lot of the activity is in the toolkit. And, you know, this poster is an example
of, you know, transformers, you know, plus, you know, transformers trained with language. And
in parallel, there's this broader conversation about foundation, the foundational models in the
community. You know, what's your take on that? Do you think that, you know, language is going to
provide this, you know, substrate that we'll be able to do a lot of non-language things with. And
that's going to be a, you know, broadly applied tool in the toolbox. Yeah, I mean, I honestly think
looking at what's been going on. And even taking some inspiration from not only like transformers,
but also ideas around unsupervised learning, self-supervised learning, another big area that's
been exploding lately, it does feel that, you know, again, going back to the very beginning of
deep learning, right? You have this principle, right? Like the traditional deep learning, maybe,
let's see what the next generation deep learning could look like. But traditional one is this,
as I said, you take the data set of input outputs and you have these toolbox, you mix and
match components, and then off you go, you train your model and it does reasonably well at many
tasks. But this is quite unsatisfying, right? If you ask many of those who have been in the field
for a long time, and obviously the ones that are not probably also find it annoying, the annoying
bit is the ways are randomly initialized, which is beautiful in a way. But it's also like quite annoying.
I mean, this, this is very wasteful, right? We're, we're starting to train from scratch,
and we're throwing away a lot of energy used to train, you know, fine weights that we had for
a different task that's somewhat related. And so this weight reusability, I think is what these
foundational models and this way of thinking is getting at, which is, look, not only we want to
take tools, we might want to take the tools with the weights associated with them, which poses a very
interesting challenge of, can we initialize parts of the network with maybe a language modeling
component, but practice and sadly this is at the frontier, it's still not very clear that we found
the way to use this idea of pre-trained weights in a successful way, meaning that we start from
the pre-trained weights rather than random weights, and we achieve better performance. Although
more exceptions to start to appear in the field, such as the one from self-supervised learning,
in which you train features that look at image statistics that seem superfluous maybe initially,
but these features happen to be quite good at classifying or doing all sorts of image tasks if
you use them as pre-trained weights. But I do believe that these probably has to be part of an
answer for the next generation deep learning that not only reuses the tools, but reuses the weights,
but it is extremely tricky. Again, the field has been looking at this problem not, you know,
definitely before deep learning was deep learning, but it is very natural to think about that, and
also again, maybe looking a bit at how we learn, we don't always like start from a fresh brain when
we learn something new, right? There's always an accumulation of learning capabilities, and that
feels like a big gap in the way we do deep learning. Despite the fact that you train these networks
from scratch, and they argue that protein folding or translation and the components are the same,
but the weights, no, right? And that, I think I believe this will be as we do research and find
ways to make these weights useful and beating the performance without, you know, training from
scratch, et cetera. I believe this is definitely a way forward, and, you know, it's exciting because,
I mean, we tried, I mean, we, definitely we tried many, many things, and, you know, it's, it's
hard. It's not, it's, you know, it feels intuitive, and it's one of these things that neural nets don't
seem to want to do too well without, like, a lot of effort. So there must be a way, and I mean,
finding it is definitely in the future. I'm sure many people are excited about this.
So another thing I wanted to ask about is a panel that you're going to be on, also at NERPs,
that is talking about a topic that's somewhat related to what we've talked about thus far,
but is specifically focused on, you know, the consequences of the level of scale that we've achieved,
and that we, that, you know, the way we're approaching machine learning with transformers is
requiring. Tell us a little bit about that panel and some of the questions that it is exploring.
Yeah, I mean, I think the panel and the question about scale, there's a more profound question,
which links to actually in the whole field, or what it, what it, what it, what makes research good
or interesting. And I find that that is a fascinating evolving topic, right? Definitely.
I actually kind of recalled one of my, I think my very first NERPs paper has a theorem and a proof,
because at the time, you know, you had to put a theorem and a proof in a NERPs paper for it
to be accepted, right? And it's just that's what you had to do. The paper actually talks a bit about
actually deep learning with SBMs, which was a very, you know, obviously popular model at the time,
super vector machines. But, you know, the question, and I think a challenge is, um, scale is,
is definitely permeating into research. And there is, I think one, one of the main aspects that
will be discussing the panel, or we discussed in the panel is the fact that many research works
or papers might be required or asked naturally by reviewers. Hey, like, can you try this idea at
scale, right? If you look at reviews, there's many reviews that are actually public. I really applaud
the usage of systems like open review that has a more transparent way to show how the review process
work. And it's quite great for people that are new in machine learning, perhaps the most. But
you often see that, well, does this model scale or not, right? And, you know, that is that a fair
question to ask. Should we always aim to scale up our methods? I wish, as we show, like,
results at the conference, like, near ribs. And I think the answer is, I mean, absolutely not.
But the question is, if a paper claims, um, claims to have discovered a new tool or a new
advance on an existing tool, um, the real question, perhaps, and that, that's where it gets maybe
a bit tricky is in which data sets or in which benchmarks are you trying this idea on? And I think what
the community has evolved towards is that, let's say, in computer vision, if you don't show something
works on image net, um, it's probably a bit inconclusive based on, let's say, other very popular
data sets that are smaller and does a bit easier to to run on, like, CIFAR or MNIST, um, right? And
there's still a lot of good work and insights discovered on those data sets. But it is natural
that you scale up, right? These experiments to image net. Otherwise, I mean, the field is full,
like, it's very big right now. There's another big change is that there are lots of good papers
very well written. And there's a bit, you know, challenging signal signal to noise ratio to understand
or slice out what is a meaningful contribution or what needs more work in terms of showing
empirically that some method works. And here, scale plays a big role. I think the role is that
it's fair to ask maybe to run on image net certain models. And then a very interesting follow-up
question is can everyone in the world scale to that scale of data set, right? I mean, image net is
not, um, perhaps the most large one, you know, if you think of it of hardware and we can train
image net. I mean, some there's some results that can train image net in 10 seconds, of course,
using a lot of parallelism. So it feels like in this sense, it's more accessible, but I mean,
the real question is, is it truly accessible? And that creates, I think, a challenge and part of
the panel, right? I'm discussing about accessibility to scale and is research at scale the only or is
is there interesting research at scale? I think the answer is yes. But is the only research we should
look for at scale? And I think absolutely not. And one very beautiful example that I can think of
again related to transformers is that at the time we were working on machine translation,
at Google, we had this sequence, the sequence paper in which we use an 8,000 dimensional long,
like LSTM basically to do machine translation. And we achieved almost a state of the art numbers
with a technique that was completely different than what people had been doing at the time,
which got I guess people's attention. And it was perfected further. And most of, if not all,
the machine translation systems now, they ran with some sort of neural network underneath.
But Montreal had less GPUs. It's not that we use a lot of GPUs. We use a GPUs for like 20 days.
So it was a painful experiment to run, right? You had to wait 20 days and it's a GPUs. They all
fit in one machine. But of course, it's reasonably large scale. But I think Montreal took a different
approach. And instead of scaling up to 8,000 layers, 8,000 units, they only could use 1000 because
maybe they run it on a single GPU. But what they did, which was I think that's maybe the most
transformative thing. Nobody intended is to invent attention. So the attention paper came maybe
perhaps because a scale was not as available to a group in Montreal as it was for us at Google.
And I think this is just a lesson that we need to learn that sometimes by just being resource
constrained, very good research has happened, right? I mean, I'm not saying that then all
please be happy and never try to scale up. It's not like that. But indeed, there are examples and
it should inspire creativity and a different way of thinking, which ultimately might create
a pattern shift. And absolutely as a community, we need to be very careful, of course, at
discarding ideas because they don't scale. And it would obviously had been a mistake to discard
that idea that created attention, which of course, followed up with self-attention transformers
and who knows what comes next. But that attention principle maybe was the key tool that we keep
seeing being quite useful. And it was invented by a lack of like scaling up to achieve better
results. They had to invent these very intuitive attention mechanism, which for translation made
a lot of sense. You look at, you know, you look at the sentence and you attend over the words
that look like, oh, yeah, I'm going to translate this word. And that beautiful principle now is
folding proteins, right? It's like unbelievable. But it is what it is. Yeah. So that's one aspect
in the panel. And I think maybe another big challenging question, I think, is I don't think
we have many answers. Although there were some interesting parallels with different fields,
like, for instance, the large collider that exists at CERN, the question about, I mean,
companies obviously have access to resources like compute. And I think it is our duty to do the
best we can in terms of research, publishing, and scaling up as responsibly as we can. But the
real question is, how can you make that kind of research available, right? To more people. And
you know, a very interesting thought is should, you know, who should kind of invest on the
maybe building a computer for academic research. CERN is a very interesting example
because the physics community kind of came together and decided, well, we need to build this
device. It's very expensive. It actually uses a lot of energy. I mean, it's quite an interesting
place to be actually. I was very lucky to be very recently in the bizarre real world, you know,
experience that I had during the pandemic. But that is an interesting model. And there are
challenges. It's not like, oh, we should just build a super computer like CERN and, you know,
just have people access it in the same way that you do access CERN by writing grants. And
it's a very interesting system, actually. But the problem there is that happened with a lot of
consensus that, you know, physics is a, for many years, like they said, oh, we need to test these
things and to do that, we need this device. So a very, another question we're discussing
the panel is, of course, it might be too early is scaling up is something that, I mean, it has
happened in machine learning, actually, forever. It's not new. You look at the history of data sets.
There's a scaling up trend, of course, of everything, thanks to more laws and so on. But the
question is, do we know, do we feel, you know, assured enough that it's investing, like, let's say,
public money from different governments that maybe could form a coalition similar to CERN,
is that the right moment to do it, right? Do we know that's the way to go? And that's a good
good question. My answer to that is scaling up will be part of the solution, but it's not the
solution, right? In terms of building intelligence, I think it's inconceivable to me that
we need to scale up just because of the amount of the sheer amount of learning at the planetary,
like, species level that happened, if you think of the amount of parallelism, years, etc., that,
you know, it got us to be intelligent beings. So just the existing proof tells me that scaling
probably will be part of the solution, but it's not the only thing that will be, like, required,
right? So, anyways, the panel, I guess, no more spoilers, but it touches all these very interesting
questions and a lot of them, of course, are inspired indeed by what we were discussing before on
scaling up language models and their foundational model capabilities that they seem to exhibit,
and, you know, if that is true, like, when is the time to think very carefully about how to,
you know, get access to the community, similar to the hardware that was being made accessible
with CERN or another example is obviously the Hubble telescope, right? Very, you know,
huge endeavors from to build those things. So, yeah, very interesting questions. I don't know if
you have any thoughts or any solutions to these either, or what do you think? You know, my,
what occurred to me was when you talked about kind of humans as the existence proof of the
requirement for scale, it prompted me to think about, I think we've demonstrated that
scale is required to advance knowledge, but it's not clear that, you know, that that's the same
as advancing intelligence. And, you know, certainly, you know, there's, you know, there's a degree
to knowledge that becomes kind of common knowledge and, you know, we take for granted,
but yeah, it's not obvious the impact of, you know, our scale is a species on our intelligence
per se. I mean, I guess you could argue that our scale as a species has, you know,
facilitated, you know, for example, nutrition, like, you know, we've, we've industrialized farming
and we become stronger and that's, you know, made our brains bigger and, you know, that's,
that has advanced intelligence significantly, but I'm not sure that that's the same kind of scale
argument. Yeah, I really like, by the way, your, your, yeah, your knowledge. I think definitely
from a knowledge standpoint, I mean, we, we store all the knowledge in different formats,
right? Like, over time, without which, you know, things would be much harder. So even in that sense,
accessing all the knowledge and learning how to access that feels like a natural thing we need
to investigate from a machine learning standpoint. And what maybe, you know, these language models
are getting at the very first stages of they know, or they have the knowledge in an imperfect way,
et cetera, but they have the knowledge that exists in a particular corpus. And, but then, yeah,
from there to intelligence, I agree. Like, that's what I'm saying. I think it's part of the puzzle,
but definitely not the whole puzzle indeed. Yeah, awesome, awesome. Well, Ariel, it has been
wonderful catching up, chatting a little bit about all the things you're working on,
particularly with regard to nirips. You are also involved in a new to machine learning panel.
We're not going to have a chance to talk about that, but for those who are new to the field,
I encourage you to seek out that panel. And I'm sure there'll be lots of interesting tidbits to
learn from there. Thanks so much, Ariel, for taking the time to chat. Excellent. Sam, it's been a
pleasure over, long overdue and looking forward to him back in a few years. And we'll see if we change
the discussion topics or it will, it will be like about the same, the same line of thinking about
scaling up and so on. But the field is fascinating. So looking forward to our next chat. Absolutely,
thanks so much. Thank you.
