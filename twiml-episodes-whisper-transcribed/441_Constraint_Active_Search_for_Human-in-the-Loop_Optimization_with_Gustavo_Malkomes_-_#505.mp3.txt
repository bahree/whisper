All right, everyone. I'm here with Gustavo Malcoms. Gustavo is a research engineer at Intel
by way of their recent acquisition of Sigopt. Gustavo, welcome to the Twomol AI podcast.
Thank you very much for having me in the show. Hey, I'm really looking forward to
digging into our interview. But of course, I'd like to have you share a little bit about your
background and give our audience an opportunity to get to know you better. Tell us a little bit
about your journey to machine learning and AI. Awesome, absolutely. Well, first, I'm originally from
Brazil. I'm a computer science by training. And when I did my undergrad, I went to a very good
school in my region. But machine learning wasn't a strong topic. But fortunately, the internet was
very generous with machine learning. We had the opportunity to basically watch Professor
Angel Ng to talk about machine learning on YouTube 12 years ago, I think. So my friends and I
created a study group to study machine learning. And maybe I can say that he was my first
instructor to the topic. After that, I pursue, I work in many machine learning projects. I decided
to do my PhD in the US. I remember checking all the faculties that I could work with that worked
with machine learning and counting how many publications they had on Isomal and Neroops.
I don't recommend this to be a metric to consider when you're choosing between grad schools.
But seven years ago, or something, I was super excited about working with people who actually
did machine learning. And that's one of the things that I actually evaluate. It seems silly to say
now. And then that's how I began my journey into the field. And you ended up for grad school
here in St. Louis at Wash You. Is that right? Yeah, exactly. I went to Wash You under the
supervision of Professor Romanger Net. I was actually very fortunate to work with many intelligent
researchers throughout America here. Of course, my advisor being one of them. But also Professor
Ben Mosley, which is now at CMU, Professor Kylian Weinemberg, who is now at Cornell.
So I was very happy with all the collaborations that I did for out of America here. And I continue
to do so. And you mentioned your school in Brazil, where was that? It was the University, the
Federal University of Seattle. In Brazil, the Federal universities are typically very good.
And that's basically it. And that's North East, like Fort Eliza. Exactly. Good to know that.
We had like the World Cup in my city, too. One of the big games was there. Of course,
as a Brazilian, I don't want to remember the World Cup in Brazil, because we lost.
That's 721 to Germany. Yeah. Nice. Nice. And for your graduate work, what did you study?
What was your dissertation on? Of course, yeah. So as I said, like I worked with different
topics in machine learning. Mass scale, clustering was one of them, multi-agents. But I felt in
love with this topic that I like to call active learning. So my dissertation was about
how we can use active machine learning to create better tools for machine learning itself.
It's kind of like an automated machine learning scenario. And specifically, I think that my
many areas of expertise is how we can actually make decisions under the face of uncertainty.
That's what I typically call active learning. Any kind of decision tool, any kind of decision
problem that we have to solve, where we want to gather new data to accomplish a specific task.
And I've done this for a hearing project, where we improve the performance of a screening test
for audiometry. That was very, very cool. It's under the same setting. To do model selection
with the same pipeline, where you actively select models to train. And of course, with Bayesian
optimization, which is what I've been doing since grad school, but also in my work at SIGOPT,
how we can make effective decisions to optimize functions very, very fast, specifically black box
functions. And we'll be digging into that topic in quite a bit of detail as we talk through your
spotlight paper at ICML, which is beyond the Pareto efficient frontier, constraint active search
for multi-objective experimental design. It is a long name and we'll unpack that name. But before
we do, you reference active learning. You also reference it in the paper. But my sense is that you
think of it in a slightly different way than is commonly construed. So when I think of active
learning, I think of machine learning approach or algorithm that takes an active approach to data
selection so that the model can be trained in a more sample efficient way to put it simply. But
you think of active learning in a much more broad way, I think? Can you kind of connect the two?
Yeah, of course, absolutely. You're perfectly right. Most famous examples of active learning are
when you have a model and you want to select training data to create this model faster.
When I mean by create, I mean, achieve some accuracy faster than if I used a whole data set.
I basically want to make smart decisions about my training samples to avoid waste of resources.
I think in real life, there are like many opportunities where we can use the same setting,
the same tools and the same mechanism to solve more broad problems. So in the case of optimization,
the data that we're going to collect are is basically parameter configurations that we can test
and evaluate if they are helpful for our application, which the goal is not to improve accuracy,
but it will be to, well, it could ultimately be, but typically will be to maximize the function.
So any kind of sequential decision making tool, we can also call sequential decision making,
but train all machine learning. I like to think that active learning is as broad as supervised learning,
reinforcement learning. We can think about this very general framework that any time that we are
collecting data, we can do so in an efficient way to achieve a specific goal. So three examples
are, well, one tip collective learning where your goal is to give information about your model
faster, that ultimately would be the case if I'm trying to learn this decision boundary,
very, very fast, typical case, Bayesian optimization where we want to optimize functions.
So the data is the parameter configurations and the goal is to find highest values.
And another example is drug discovery where you want to find new components or biology,
you want to find new chemicals to achieve some properties and or perhaps even more broadly
experimental design where you want to understand the whole of the physical phenomena very, very fast.
You can say that active learning is kind of science more generally. We are all,
or every time building models and collecting new data to validate those models. So that's kind of
my general idea for that. I know that I talk about different subjects, but I'm happy to clarify
any of them. You mentioned the, you mentioned this chemical engineering scenario example,
and that was one of the background problems or motivations for this paper. Can you tell us a
little bit about the problem that you were trying to solve that led to the work discussed in the
paper? Yeah, of course. So first, this is a joint work with my colleagues at TIGOPT, Harvey Chen,
Eric Lee, and Michael McCourt. In Harvey and Mike, they had this collaboration with
members of the University of Pittsburgh, the laboratory of advanced materials at Pittsburgh,
directed by Professor Paul Lu at the University of Pittsburgh, and they have been collaborating
with this idea of creating new materials using machine learning. It's specifically, they are
interested on creating new types of glasses that can improve the performance of, for example,
solar panels. The idea is to have durable, anti-reflective, anti-soiling, self-cleaning glasses
for solar modules. Basically, the problem is you want to improve the efficiency of solar panels
by changing the properties of the glass. You don't want, for example, glass or reflection in the glass,
because the lights will, the light bulb bounces, will basically bounce off of the glass, and you
basically will be losing energy. Also, if the glass is dirt, that's a problem too, because it
blocks the light to coming to the solar panel, and you also lose efficiency. What the researchers
from the University of Pittsburgh they work with is creating like small structures, nano
structures on the top of this glass, to change the properties of light, and therefore,
improve the performance of the solar panel. The idea is, and this is very common in many
different material sciences and other types of science work, is they have numerical simulations,
they construct designs, they can fabricate designs using computer simulation,
and using the software they can undercentrate ops for ultimately creating a structure in real life.
What we have observed is that this is very different. It's very different to create
new material in the computer versus creating a new material in the real life.
And the reason for that is, in the numerical simulation, we don't have all the properties.
We cannot perfectly simulate everything. There's a correlation. We can't get a lot of this,
but ultimately, the equipment that we're going to use to produce the materials will be
slightly different. We don't have like infinite precision, or something different happens in real
life, because that's how real life is. That being that the simulation is producing a set of candidates,
but you ultimately have to fabricate some small number of those candidates and see how they
perform in the real world. Perfect. That's exactly it. We have this human in the loop. That's a very
common term right now that basically requires the machine learning tool to assist the decisions
that the human expert that knows everything about the problem, knows everything about the domain,
can ultimately do and select. I think that's the main inside that we want to bring to this
paper, talking in a real high level. There is a development scenario, there is a production
scenario, and there's probably a discrepancy. And ultimately, there is a human in the process,
doing supervision of the whole decision making, and being powered by the experimentation process
that we do offline, but which eventually will be very costly to do in real life.
Let's maybe return to the title of the paper and start to unpack some of that so we can dig deeper
into what you've actually done. Again, the paper is beyond the Pareto efficient frontier
constraint active search for multi-objective experimental design. It sounds like the problem
that you're seeking to tackle is multi-objective experimental design. How does that relate to the
scenario that you just outlined for the materials? Yeah, so material, experimental design is basically
the field that do with smart decisions about designs, and the designs could be like many different
things. It could be actually a whole experimentation pipeline in a lab. In our case, just to give like
think, just to give with a real example, it's going to be the structure that we want to really
create on top of the class. The column, the box size, all the properties that we can actually
use to create those structures. That's the experimental design part.
Multi-metric, it's because in many problems in real life, it's very, very hard to find one thing
that we care about. So there are competitive objectives. If we change all the light to reflect
on this glass, another property will be different. Transparency, oil contact, cleanness, all those
things really related to the subject will change. And ultimately, they will be competitive.
That, what typically makes the optimization problem a little bit harder.
And one thing to highlight here, which is very general, is the way that we're doing experimentation
is really for smart decisions. We don't have access. We are solving an optimization problem,
which is black box. We will really run the simulation to get an output. But we don't have access
to gradients, for example. So it's really like a black box that we put inputs and we receive
outputs back. We want to design experiments, basically designing put configurations that will give
good results on the outside here, on the the Y-values. If you're talking about the optimization,
we're trying to optimize, minimize or optimize one of those goals. Now, the Pareto efficient
frontier, it's basically how we try to solve multi optimization problems, multi metric optimization
problems. We traditionally want to find output values that can dominate the other ones.
The hypothesis here is that we cannot compare the two metrics. For example, risk and return,
if we're talking about finance. And there's no really like solution for this. It's really
dependent on the user to decide the level of risk and the level of return it's willing to take.
So in that sense, all the configurations that we can return to the customers that
dominate the other ones. So if I can improve my return without reducing the risk, that's better
than a suboptimal configuration that has the same risk level, but a lower return.
I would want this, it doesn't make sense. So I would actually have the dominate solution,
which has a higher return, but the same level of risk. That's how we construct the Pareto
efficient frontier. And this makes a lot of sense if you're probably choosing maybe stocks. But in
problems that we're trying to understand the metric values and the parameters, we really want to
focus on the search aspect, what I mean by this. For a scientist, for example,
they will be very happy if we give them a material that has very nice properties.
But they will be very confused if we change the parameters that create this design,
it's lily, and the results is really bad. So they're looking for really a broader sense of what's
happening here, not only with the metric values, but also with the parameters. Ultimately,
they want these stable parameters that can lead to consistent results. Why? Because as I said,
the production and the development and the production settings won't be a perfect match.
They really want consistent results. That's the motivation for why we're talking about an
alternative to this multi-objective solution for experimental design. It's because people care
about both the metric values, but the parameter values. They want this stability. They want to
understand the problem they're trying to solve. Let me try to replay that so that I make sure I'm
understanding it and we're all on the same page. You've got this note of the Pareto,
Efficient Frontier. We know it from finance and economics and problems like that. And that says
that in the case of a two-dimensional relationship, there's a line that is your
optimal trade-off among these values. And what you're suggesting is that in the real world,
for example, when you're designing a material, a value on this Pareto, Efficient Frontier,
might be optimal from the perspective of the metric, but it might be unstable. You know,
if we think about kind of the chart of the optimal, like a curve, it might be sitting on a spike.
And so if you shift one of your parameters, your metric might drop off precipitously,
whereas something that is theoretically less optimal might be sitting on a broader base.
And I think ultimately, the result is it would be easier to produce in the real world because
it's not quite as unstable. Is that the idea? That's the idea. If you think about, like,
I think when we're talking about optimization, it's very common to think about climbing a mountain,
right? Of course, in our scenario here, we don't have derivatives,
but we can theoretically find blocks. Yeah, because it's a black box, but you can theoretically
think about a surface of function values that exist. And we can pretend that this is like a mountain.
And the metric values will be basically the height of this mountain.
Right. So in a multimetric problem, it's kind of like we're trying to find the optimal of
the function values between two mountains, let's say there is a mountain number one, which is one
metric, there's a mountain number two, which is the second metric. In a multi metric optimization
problem, we want to find really the peak of the mountain, the highest value possible.
And the Pareto-efficient Frontier will be basically aligned between those two mountains
on paramere space. That's exactly the Pareto-efficient Frontier on metric values, how they will be
translated to the paramere space. It's going to be a line. If I navigate towards the outer mountain,
I'm going to reduce the metric values here, but I'm going to improve the metric values here.
Okay. So what we are trying to, yeah, to jump in there, the
dimensionality of the parameter space is like the numbers, the number of dimensions of your mountain
essentially, and then the number of metrics is the number of mountains. In some sense, yes.
Okay. That's a very good analogy. Yeah. So in this case here, we're talking about two and two,
but it could be, you know, in a more, like in a higher, in a more complicated surface
in the world, will be like a three-dimensional shape. But anyway, the point is, if we have two
metrics, you know, there is one optimal value here, another optimal value here. The Pareto-efficient Frontier
was going to be aligned that trades those two off. Now, what we are saying is, okay, this is only true
during our development setting. You find the optimal values here, doing your numerical simulation.
But what if the mountain shifts a little bit? The line was going to be completely different. So,
do I really want to return to the to the expert, to the human, only the points that could be,
you know, sub-optimal in the real life, because the the mountains can shift a slightly.
The answer is no. We probably want to give them like a more information. We want to give them
enough information so that even if the mountains change a little bit, you will still be comfortable
with the results that you get. Does that make sense? Yeah, I guess it raises a question for me.
Are we trying to, is the idea fundamentally that we're going to be less restrictive in the values
that we return? We're going to be looser and allow the human and elope to, you know, because we
know there's a human and elope, they can determine ultimately what's the best point for them. Or is it
that somehow that the algorithm that you've created also incorporates stability and returns
better points that aren't on the Pareto efficient frontier? That's a distinction I'm getting
on. Yeah, it's definitely different. I totally agree with that. And I would say it's the former.
Okay. In the let the the second option, the latter could be our next paper.
But it's more on the former. In the sense that we do we will do exactly what you describe. We're
going to lose the definition of best to not really the peak and top of the mountain. But you know,
to altitude that is good enough. So we're going to have like the whole shape of the top of the
mountain. And we're going to offer this to the user somehow. That's the part that it's the problem
formulation aspect of this. We are generalizing something called Bayesian optimization,
which really cares about the highest values and the experimental design, which wants to understand
the whole mountain, all the shapes and you know configurations of the mountain.
Sorry. Constrain active search, which is exactly the problem that we are proposing to solve.
It's really a search problem in the sense that we want to find structures in the parameter space
in the executive learning pipeline, which are constrained, constrained by metric values.
Specifically in the paper, we say that there is a region of satisfaction or a sex,
sex, sex factory region where it's the parameter configurations that lie above the metric values.
You can also think about this feasible region, but typically feasibility means that we have
constraints on the parameters. And for some reason, we don't want the parameters that are not
feasible, right? Here, we are really losing the problem from the optimization to something that
is not the best, but it's a set set set, it's a set's fight of constraints. So feasibility typically
speaks to the parameters satisfaction. In this case, you're more speaking to the metric or the
objective. Exactly. We want to load the definition of what is best on the metrics. And again,
this is black box. We don't have this beforehand. So again, with the example of the mountain, we want
the whole shape of the top of the mountain, not necessarily the peak best value.
And thinking about it in the context of the mountain, what's the relationship between
the satisfying construct that you're creating and the idea of global optimal versus local
optimal, that kind of thing? Is the thing that connects those the idea of stability that we talked
about earlier? In some sense, yes. Because the optimal values will be within the satisfactory
region. So we're just expanding the notion of what it's good to more values based on the constraints
that the user gave us on the metric values. So for example, suppose that we want to find a
high-performing machine learning models. And we care about accuracy, but also inference time.
I want to give you a list of different models and diversity he hears the key for a constraint
active search that have that satisfy your constraint on accuracy above 80% and inference time,
less in a minute, let's say, hopefully way less than that. But I'm going to give you an option,
the solution for this problem is going to be a list of options that you can trade them off,
both in terms of the metric values, but also one of the choices of hyperparameters that you have.
For example, number of trees, if you're talking about exuboost, and all kind of like
any kind of parameter that you think is important for creating a machine learning model,
which are the hyperparameters.
Okay, okay. And so you've created this, you've kind of redefined or defined a new problem
constrained active search. Do you also share any insights into how to solve the problem?
Yeah, absolutely. That's the main difference between the workshop paper that we have published
like a couple of months ago, less here. And the, you know, in a full publication that I
said, well, it's the fact that we have a solution for this problem. So I hope it's very clear
that like why I'm stressing the problem is because it's a different mentality. We're changing the
focus from getting the best possible values to getting high performing values, high performing
models, high performing designs, which means that I can accept in a continuous space a bunch of
new points. And if I, I can change slightly the same configuration and or parameter and get
the same results. So that's something that it doesn't make sense mathematically. So you want to
incorporate something which is diversity. And I think this is really, really important for many
real life applications, especially in, and I can talk more about this next in the context of
developing machine learning models, developing real materials in real life. But to the technical
aspect of the work, we use something called Bayesian decision theory to derive an algorithm
for solving this problem. And as I said, the main property of this problem is we want to find a
set of solutions in continuous space. So we want those solutions to be different in primary space.
We don't want to lead to the same design. If I show this to a scientist, you know, a box of,
you know, this nano structure that I've changed like one nanometer up or or left or, you know,
the cone in the height of the cone or the shape. Lively they would say like, yeah, this is the
same thing. I don't care about that. I can't even produce this difference. That's also important.
But anyway, the way that we focus on solving tackling this problem is defining something called
utility function, which defines how our active learning, active search algorithm
will select the next configuration. So to give you an example in optimization, what we typically
care is if I have a configuration that has this value here, I care about new configurations that
can improve the value that I have observed. So this leads to a policy that is expected improvement.
How much I can improve over over the last of function value. We don't care about the best values
per se. We care about two things in this problem. We want to satisfy the constraints. That's the very
first important thing. And two, we want the solutions, the parameters that we select, the designs
that we select to be different. So if I choose two new configurations that satisfy the constraints
but they're very close to each other, I don't want this. So we define something called the
neighborhood of a point. So any configuration that we choose in primary space will basically
induce a volume on primary space. And this volume is exactly what I want to increase. I want to
increase the coverage of my configurations where coverage I'm defining as the volume on
primary space that is potentially above the threshold. That's something complicated. I
definitely recommend you to see the details or your audience to see the details on the paper.
But that's the intuition. We want points above the constraints that satisfy all the constraints.
And we want points that are not too close to each other so that they can lead to diversity.
And you said that this is a complicated part but when you talk about the threshold is that a
threshold, meaning the union of the volumes around the individual points and the degree to which
that coverage your entire space or as the threshold specific to each point in the primary space.
The threshold that I mentioned was specifically to the metric values.
Okay. So basically is the level if you go back to the mountain again.
Right. And I'm going to use my fingers here to point out and I definitely recommend you to check
out the paper which we have better visualizations for that. But if we talk about the top of the
mountain, we are chopping off the top of this mountain in some region. So the shape could be
really pointy or the shape could be like really wide. So the threshold really defines the shape
here, the level that we want to cut off the mountain. Now the the coverage that I was talking
about is really like if I want to place a point here in this side of the this is funny.
If I want to place a point on the top of this, I basically want to solve a packing problem.
I want to know how many configurations I can place on the top of this mountain.
Got it. In the way that it covers the whole
primary space. Got it. Got it. Does that make sense?
Packing or a tessellation, you want to pick your points to maximize the coverage of the
I guess this part of the mountain that you've cut off. Exactly. Exactly.
And you define the points by some kind of radius around the points.
Right. In our algorithm specifically, we have a parameter that we call radios are
which defines in your application, how much you you care about distance. So
basically this parameter is telling me that if you find two configurations that are too close
according to this distance, that's exactly what this parameter is measuring. I don't care
about this. This does increase my knowledge about the problem. Yeah. So those radios define the
minimal level of proximity that you can have between two points. And again, the top of this
mountain, this region that we want to cover is really the region of parameter configurations
that yields high performing values. Right. Right. Right. And
is there just out of curiosity, I'm wondering to what degree you could
characterize the total volume of the top of the mountain. And could you
could you like back your way into R by saying you want, you know, 20 points and you know the
volume of the top of the mountain. And so like divide the volume of the mountain by R.
I'm sorry, divide the volume of the mountain by the number of points and kind of back into your
R value. Or do we are there things there that we don't know or can't figure out or are really
difficult. Right. That's an excellent question. But the setting is we don't know.
Basically, we construct, um, probabilistic models that talk about the surfaces of the
objective functions that we want. Basically, and the surrogate model, the probabilistic model that
we have is going to give us a notion of uncertainty of the shape of that we are trying to solve for.
So nothing we didn't know for sure. We are actually discovering this through experimentation.
We are increasing our knowledge or our ability to model the surface of the metric, metric values.
And also really the region of satisfaction. So in reality, the utility that I just described
is true, fixed the mountain shape, if you will. But what we really implement is the expected
gain in utility, which takes in consideration our uncertainty around the surface.
Okay. So if you're asking about how many points I can place, it's really through experimentation.
The more points you give us, the better to solve the problem.
There is a rule of thumb of, you know, 20 to 30 times the dimension of gain put.
It's a good number. Um, if we really have good priors to match our problem, but theoretically,
this could be way, you might need more, way more points.
And you've mentioned Bayesian optimization previously. And you know, now we're talking about surrogate
models and priors. Can you talk about where Bayesian optimization fits into this and how?
And where the surrogate models come from?
Exactly. Yeah, absolutely. So I think it's, that's, let's think about like the complexity of those
problems. If I want to solve a single problem, just one metric, I'm going to do Bayesian optimization
to find the highest, the best values, what I'm going to do. I'm going to create a surrogate model
that does this inference over the function values that I haven't observed yet. And I'm going to
create an acquisition function, which, as I said, could be, it's basically a policy that tell us
which point I'm going to choose next. When I'm talking about multi-matric problems, I can also
use Bayesian optimization. I can also use Bayesian optimization with constraints. But again,
everything that I'm focused here is to construct surrogate models, one for each metric.
And trying to find the hypervolume of the Pareto Frontier. Again, the Pareto Frontier is this region
of dominate points. The hypervolume, it's a metric that tells me if I increase the, the,
the values, I will be doing a better job at finding the Pareto Frontier.
The next step for this is, at least in my mind, well, there is a different problem that is
important to highlight, which is called level set estimation. It's when I'm not interesting on the,
the best values per se, but I'm interesting on finding the threshold really where the function
values would change from the level that I stipulate, basically the constraints in this case.
What we argue is constraint active search. It's a generalization of this.
In hindsight, if you knew the best value, something that we don't know, but if you place the
highest value, highest function value on the constraint active search formulation, we would
recover Bayesian optimization. Because again, the definition will be just the peak of the mountain.
I just want to return the best values. So I recover Bayesian optimization with this
constraint active search formulation. The opposite of this is if I knew the minimum value of all
the metrics, I could plug this in in my constraints in constraint active search, and I will recover
a problem that is experimental design. It's really, I want to know the function values everywhere.
Constraint active search is something in the middle. You choose the thresholds the way that you wish.
And of course, we recommend you to do so in a conservative way because everything about the
function we are learning on the fly. So if you start with, yeah, I was just going to say I'm not
sure. I fully follow that, but I think in essence, what you're saying is that there's a relationship
between this problem that you've articulated here constraint active search and other things that
we've studied for a long time, Bayesian optimization, level set up estimation and experimental design.
That's a digest. Yeah. Okay. Okay.
And first, did we get through the kind of the explanation of the mechanism? I think we
did get through it at least conceptually in talking about the volumes. If there's any pieces that
we have not covered, jump in with those, but I'm starting to think about, okay, how do you
assess performance here and measure the effectiveness of the method and what that even
means in this context? Precisely. So for the algorithm, really, it's hard to talk more in details
like in more details about it because you have to know some mathematical definitions of neighborhood.
I think we covered the high level idea in a good sense, which is really trying to solve this
packing problem in the high performing region of the primary space. Got it. But ultimately, we define
utility function and we do something that we do in Bayesian decision theory. We select the action
that maximizes my expected utility. And the utility is this volume that we define. The uncertainty
on this utility is on the real region of satisfaction, which is, you know, the thresholds
because we're using probabilistic models, we can compute what is the probability of any configuration
to lie above the thresholds. And that's defined basically as a whole set of points in expectation
that will be within the region of satisfaction. Okay. What was your next question? Sorry.
So then what's what does evaluation look like? Exactly. So in our experiments, we were we tried to
be very honest. We didn't try to, but basically we had multiple ways of looking how about the
quality of those results. So we consider like four metrics. One of them is just the number of positive
points that you get, the number of points above the threshold. If your problem is really,
really difficult, that's something that you might care. But if you just optimize this metric,
you might not get diversity, which I think I said, it's a very important thing in our problem.
The second, the second angle that we can look at this is really what about the hypervolume?
What about the quality of the points that we get above the threshold? Are they
dominate points or not? This is what the hypervolume is measuring. It's a standard metric for
optimization problems. But for our problem, what we really care is diversity of the points that lie
above the threshold that satisfy the constraints. So we measure it in two ways. The very first one
is using the field distance, which is a typical measure of diversity in many areas.
It basically the radius of the largest ball you can place on the points that cover them. It's
the largest radio that you can use to cover all the points. That's kind of the definition of full
full field distance. And a new criteria that we use to tackle this problem, which is coverage
recall, which is really how much volume I was able to recover from this satisfactory region.
This is only possible to compute if we know the ground truth, of course. But it's definitely
something that measures the success of constraint-active search.
Yeah. And what kind of results did you see? Oh, yeah. We basically do. And maybe to jump in,
are there, is the problem that you've defined close enough to existing problems that there are
benchmarks that are relevant? Or is it mostly evaluation, mostly self-referential?
Well, our goal with this experimentation setup was to show those formatrix and really highlight
that different problems, different strategies are solving different things. So our algorithms,
the algorithms that be proposed to solve constraint-active search. We actually proposed two or three
alternatives. And they all do very, very well on coverage recall and field distance.
Specifically, does the main algorithm that we propose, expected coverage improvement,
is doing really well and minimizing the field distance and maximizing the coverage recall,
which is exactly what we want. If we look at the angle of optimization, you will see that
Bayesian optimization with multiple matrix will actually be the best because it's the only
algorithm that is really trying to do that. And if we look at the number of sheer number of positive
points, there's another line of research called active search, which optimizes the number of positive
points. The downside of that approach is diversity is compromised. And this is true for both problems.
Bayesian optimization doesn't have as much diversity as constraint-active search. Active search
doesn't have the same ability to optimize hyper-volume, nor increase diversity as active search.
It just finds a bunch of positive points and they are close together.
Right. Restating Bayesian optimization will give you the points that are optimal we talked about,
but not necessarily stable or diverse. Active search just gives you back a bunch of positive points,
and so creates a lot of noise for the human and the loop to go through. Because the points are
cluster together, it's not particularly meaningful distinctions. And so the diversity formulation
and constraint-active search produces something that is much more useful in a real-world human
and a loop scenario. Exactly. And I would like to dive deeper on that. Why do we think this is
very useful? Just in benefit of active search, this is typically a solution that it's offered
for discrete points. So if you have discrete points, maybe you should do active search,
and they're very rare to find. In this scenario where we have parameters that are continues,
then constraint-active search will be the best trade-off between finding good values,
kind of the hyper-volume and diversity. Now if we go back to how this is really useful in practice
when we have a human and loop, the reason for that is if we actually solve optimization problems
and we offer experts the decision on the Pereiro-efficient frontier, they typically will think that they
can choose metric values in the specifically in this case of design. They would try to see metric
values that, you know, satisfy their preferences in some way. They will look at the function values here.
Oh yeah, this configuration is really good. And I see a couple of points nearby. They have the same
performance of this one. But the problem that they could face is when they map back to find what exactly
what kind of how I'm going to produce this design. Maybe those metric values that were close
here in the metric space could be really all over the place on the pyramid space. And this
doesn't increase their confidence about the ability to produce a specific design with this height
or this width. On the opposite, constraint-active search will provide not only points on the
Pereiro frontier, but also a bunch of points in, they're not, they're like
dominated by the points in the Pereiro frontier, but they are a bunch of points that in the
primary space, they will offer you insights on how I can actually produce and create this design.
That's one of the things we think this is important. The other one, it's really respect to the
application. For example, some of those metrics that they care to, so basically we have this
humidity loop. We want to use this experimentation to guide them their choices. Eventually they
will choose five or ten designs to create to produce in real life. They will fabricate the
designs. Diversity plays a really important role here because when we actually compute
the designs, you can further measure properties of those designs. If they are all very different,
then you can further do like a more expensive test to really find your best solution.
Now, we can also translate this to machine learning in real life.
Ultimately, we have business metrics that we care about, and the validation metrics that we use
are a good proxy of what we can do during production, but maybe they're not perfect. What we
wanted is offer users the possibility of having very different models so that they can use those
models in a more complicated testing setting. For example, as a shadow of a current
system or in an AB scenario where they want to evaluate things that they couldn't during development
because the data is different. Having the ability to test more different models in the semi-production
system is really, really helpful for them to make sure that the models that they created using all
the metrics developed previously are actually helping them to achieve the business goals.
So, returning to this theme of performance and development versus performance in the real world,
but here in the broader machine learning application as opposed to the engineering application.
Yes, exactly. Awesome, awesome. My next question would be to ask about where you go from here
in future directions, but you already said that it's returning back to the objective and trying to
understand stability for some of these points as one potential direction. Any other thoughts there?
Well, this is a new formulation, so we are excited to invite researchers for a new community to
think about different properties, different ideas. In this current formulation, we have this
parameter R, which is something that is a function of your application. In real life, we can probably
design a schedule for this parameter. We start with very large R values that will do a lot of
exploration, and then we reduce the R values, so we can actually find a bunch of models more
similar in some sense. We have some also theoretical understanding of the problem. We prove that
this algorithm has a bound on the field distance, which means that it's able to converge in the sense
of covering the satisfactory region, but you can also think about developing all the results,
which we haven't done in this paper about how many other theoretical results, about the number
of points that I need to at least get a notion of convergence closer to the results that we get
on Bayesian organization. For example, yeah. Awesome, awesome. Bogusavo, thanks so much for
taking the time to share a bit about your paper. Very interesting stuff. Thank you very much.
We really appreciate it. That was a great conversation.
