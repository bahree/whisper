1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:35,980
This week's shows are drawn from some of the great conversations I had at the recent

5
00:00:35,980 --> 00:00:40,960
Nvidia GPU technology conference and they're brought to you by Dell.

6
00:00:40,960 --> 00:00:44,840
If you caught my tweets from GTC, you may already know that one of the announcements this

7
00:00:44,840 --> 00:00:49,320
year was a new reference architecture for data science work stations powered by high

8
00:00:49,320 --> 00:00:54,480
NGPUs and accelerated software such as Nvidia's Rapids.

9
00:00:54,480 --> 00:00:58,960
Dell was among the key partners showcased during the launch and offers a line of workstations

10
00:00:58,960 --> 00:01:03,040
designed for modern machine learning and AI workloads.

11
00:01:03,040 --> 00:01:07,720
To learn more about Dell precision workstations and some of the ways they're being used by customers

12
00:01:07,720 --> 00:01:12,320
in industries like media and entertainment, engineering and manufacturing, healthcare

13
00:01:12,320 --> 00:01:20,440
and life sciences, oil and gas and financial services, visit Dell EMC.com slash precision.

14
00:01:20,440 --> 00:01:23,040
Alright everyone, I am on the line with Gerald Kwan.

15
00:01:23,040 --> 00:01:27,040
Gerald is an assistant professor at UC Davis.

16
00:01:27,040 --> 00:01:30,280
Gerald, welcome to this week in machine learning and AI.

17
00:01:30,280 --> 00:01:32,800
Thanks for the invitation to interview with you.

18
00:01:32,800 --> 00:01:33,800
Absolutely, absolutely.

19
00:01:33,800 --> 00:01:38,560
I'm looking forward to diving into some of the work that you've been doing around applying

20
00:01:38,560 --> 00:01:41,640
deep learning to genomics.

21
00:01:41,640 --> 00:01:46,320
Before we do that, I'd love to hear a little bit about your background and how you started

22
00:01:46,320 --> 00:01:50,360
working in the intersection of those two fields.

23
00:01:50,360 --> 00:01:57,280
So I guess my career in computational biology started out in my first year undergrad actually

24
00:01:57,280 --> 00:02:02,560
where I was initially slated to enter a computer science undergraduate program and about

25
00:02:02,560 --> 00:02:07,720
two weeks before I started, I got a piece of paper in the mail from my university telling

26
00:02:07,720 --> 00:02:12,320
me that they started a new bioinformatics program and were wondering if I was interested

27
00:02:12,320 --> 00:02:13,320
in joining.

28
00:02:13,320 --> 00:02:17,200
And actually initially, I initially turned them down because I thought the idea of kind

29
00:02:17,200 --> 00:02:21,440
of studying biology with computers was, you know, it didn't seem that interesting.

30
00:02:21,440 --> 00:02:27,160
But after a couple semesters, I just happened to meet a professor doing some really interesting

31
00:02:27,160 --> 00:02:31,160
kind of high throughput data analysis, looking at proteins in their kind of three dimensional

32
00:02:31,160 --> 00:02:36,600
structures and he was interested in kind of predicting these 3D structures from their

33
00:02:36,600 --> 00:02:40,680
kind of linear protein sequence and I was just kind of peaked my interest in from there

34
00:02:40,680 --> 00:02:43,720
I just kind of, yeah, I never really looked back.

35
00:02:43,720 --> 00:02:48,400
And so do you currently sit in which department at Davis?

36
00:02:48,400 --> 00:02:54,160
So I'm in the department of Lecler and Cell Bio, which is a little bit of a, yeah, it's

37
00:02:54,160 --> 00:02:59,040
kind of an interesting, interesting place to be because I, you know, my undergrad gain

38
00:02:59,040 --> 00:03:05,360
was kind of in computer science bioinformatics and then I took a brief break and went to biochemistry,

39
00:03:05,360 --> 00:03:09,960
had to do masters in biochemistry before I went back to computer science for a PhD.

40
00:03:09,960 --> 00:03:13,680
And I think this kind of background where I have a little bit of biology and a little bit

41
00:03:13,680 --> 00:03:18,160
of computer science background kind of appealed to my department and so that's kind of how

42
00:03:18,160 --> 00:03:19,160
I ended up here.

43
00:03:19,160 --> 00:03:20,160
Awesome.

44
00:03:20,160 --> 00:03:21,160
Awesome.

45
00:03:21,160 --> 00:03:26,480
And so you were recently at the NVIDIA GTC conference where you gave a presentation

46
00:03:26,480 --> 00:03:34,520
on deep domain adaptation and generative models for single cell genomics.

47
00:03:34,520 --> 00:03:40,800
What's the main challenge that you're talking about in that presentation?

48
00:03:40,800 --> 00:03:47,440
Well, so basically in the field of kind of molecular biology, you know, these kind of genomics

49
00:03:47,440 --> 00:03:52,560
technologies are the biologist's way of kind of getting really high resolution snapshots

50
00:03:52,560 --> 00:03:55,800
of different kinds of like human tissues and so on.

51
00:03:55,800 --> 00:04:00,360
And the work that I spoke with specifically is kind of on this emerging field called

52
00:04:00,360 --> 00:04:04,680
single cell genomics where you could actually do these measurements on individual cells

53
00:04:04,680 --> 00:04:09,240
where it's kind of historically you had to kind of use like really big tissues in order

54
00:04:09,240 --> 00:04:12,080
to get enough sample to generate this data.

55
00:04:12,080 --> 00:04:16,240
And so it's kind of exciting because you know, because you can do measurements on these

56
00:04:16,240 --> 00:04:22,240
on these single cells, you can generate vast orders of magnitude more data than you could

57
00:04:22,240 --> 00:04:23,240
previously.

58
00:04:23,240 --> 00:04:28,320
And so now, whereas before when you look at kind of human data, you could only generate

59
00:04:28,320 --> 00:04:33,200
data for say like a couple hundred tissues or samples, now we can generate like millions

60
00:04:33,200 --> 00:04:35,480
of data for millions of samples.

61
00:04:35,480 --> 00:04:40,440
And so that it's really exciting because you can now we can sort of apply a lot of the

62
00:04:40,440 --> 00:04:44,000
kind of methods from like deep learning and so on because we have this large amount

63
00:04:44,000 --> 00:04:46,760
of data that we we didn't previously have.

64
00:04:46,760 --> 00:04:53,160
What are some of the things that you're trying to understand through these analyses or diseases

65
00:04:53,160 --> 00:04:57,760
that you're trying to identify or fight?

66
00:04:57,760 --> 00:05:00,360
We try to study a couple of diseases of interest.

67
00:05:00,360 --> 00:05:05,000
So these include things like cancer and Alzheimer's disease and schizophrenia.

68
00:05:05,000 --> 00:05:09,000
And so the basic premise is that you know, if you want to kind of understand how say

69
00:05:09,000 --> 00:05:15,400
a disease like schizophrenia works, you know, you kind of want to take say samples from

70
00:05:15,400 --> 00:05:19,680
brain tissues from sort of so-called healthy normal people and you want to take them

71
00:05:19,680 --> 00:05:23,680
from these, you know, say people with schizophrenia, and then you want to somehow be able to

72
00:05:23,680 --> 00:05:28,960
compare, you know, the data that you get from both of these groups of patients in order

73
00:05:28,960 --> 00:05:34,000
to figure out what changed when you know, when schizophrenia happens at different stages.

74
00:05:34,000 --> 00:05:39,840
And so again, in sort of this, in our work that we presented at the GTC, the tool that

75
00:05:39,840 --> 00:05:47,600
we developed was specifically designed to basically build models of genomic data in both

76
00:05:47,600 --> 00:05:52,680
kind of normal people and schizophrenia people and then kind of use techniques from domain

77
00:05:52,680 --> 00:05:57,520
adaptation to kind of understand how the two are related.

78
00:05:57,520 --> 00:06:02,480
And this is maybe a more domain specific question.

79
00:06:02,480 --> 00:06:10,720
But for something like schizophrenia, it strikes me as more of a systemic problem than

80
00:06:10,720 --> 00:06:14,520
a, you know, something that you would see in a single cell.

81
00:06:14,520 --> 00:06:19,480
How does what's the connection?

82
00:06:19,480 --> 00:06:20,480
So you're right.

83
00:06:20,480 --> 00:06:25,320
You know, complex diseases like schizophrenia have, you know, there's many factors that

84
00:06:25,320 --> 00:06:28,000
contribute to the development of schizophrenia.

85
00:06:28,000 --> 00:06:33,200
In our particular work, we look at, we try to look at whole regions of the brain and how

86
00:06:33,200 --> 00:06:36,960
these whole regions may change between normal people with schizophrenia.

87
00:06:36,960 --> 00:06:42,160
And so the reason why this single cell stuff is really interesting is because previously

88
00:06:42,160 --> 00:06:46,000
if you wanted to, like we know that in the brain there's like lots of different types

89
00:06:46,000 --> 00:06:48,400
of cells and tissues in there.

90
00:06:48,400 --> 00:06:52,520
And so previously when people did these kind of studies, they could only take say an entire

91
00:06:52,520 --> 00:06:57,560
brain kind of chop it up and then look at a snapshot of the whole brain and compare

92
00:06:57,560 --> 00:07:00,920
a whole normal versus whole schizophrenia.

93
00:07:00,920 --> 00:07:05,760
But now because we can measure things at single cell, we can take, we can measure like millions

94
00:07:05,760 --> 00:07:11,400
of cells in a single normal person, millions of cells in a single schizophrenia person,

95
00:07:11,400 --> 00:07:15,560
and then try to compare and say, okay, of these million cells I got from one normal, one

96
00:07:15,560 --> 00:07:19,880
schizophrenic person, which subset of these million cells is actually changing because

97
00:07:19,880 --> 00:07:26,360
not all, like not, you know, not everything in the brain will change when you get schizophrenia.

98
00:07:26,360 --> 00:07:31,520
And so we've talked about genomics here is the data that you're fundamentally looking

99
00:07:31,520 --> 00:07:38,760
at, is it sequence data or is it some kind of imaging data or a combination of the two?

100
00:07:38,760 --> 00:07:49,280
So the data that sort of comes from the biologist comes in the form of like DNA or RNA sequence

101
00:07:49,280 --> 00:07:53,200
from these patients, but that kind of gets converted in this kind of data preprocessing

102
00:07:53,200 --> 00:07:59,480
steps such that the data that we look at essentially is just, it just blows down to matrices

103
00:07:59,480 --> 00:08:04,440
where like columns of these matrices correspond to different cells from a patient and rows

104
00:08:04,440 --> 00:08:09,440
represent kind of different features of the cells that are measured through genomics.

105
00:08:09,440 --> 00:08:17,440
So a feature, an example of a feature being like a known sequence or something, the existence

106
00:08:17,440 --> 00:08:21,000
of a known sequence or a snip or something like that?

107
00:08:21,000 --> 00:08:25,120
So it actually corresponds to like a gene and so the idea is that when you look at a cell

108
00:08:25,120 --> 00:08:31,600
within, you know, a patient, you can measure, you can make many different types of measurements

109
00:08:31,600 --> 00:08:36,600
on the cells, but one common type of measurement is called a gene expression measurement.

110
00:08:36,600 --> 00:08:42,640
So the idea is that, you know, each cell has DNA in it, but DNA by itself usually acts

111
00:08:42,640 --> 00:08:44,360
as kind of just a storage mechanism.

112
00:08:44,360 --> 00:08:50,600
So for DNA to do anything, it actually, large parts of it get converted to this molecule

113
00:08:50,600 --> 00:08:51,600
called RNA.

114
00:08:51,600 --> 00:08:56,360
And so what we're measuring, what each features measuring is how much of this kind of active

115
00:08:56,360 --> 00:09:00,480
RNA molecule gets produced from each part of the DNA.

116
00:09:00,480 --> 00:09:04,680
And so that's how we get like a vector for each cell where each component corresponds

117
00:09:04,680 --> 00:09:08,840
to how much activity we see from a given part of the genome.

118
00:09:08,840 --> 00:09:09,840
Okay.

119
00:09:09,840 --> 00:09:15,080
And so how does, where does domain adaptation and generative models, where do those come

120
00:09:15,080 --> 00:09:19,160
into your workflow here?

121
00:09:19,160 --> 00:09:24,760
So we're, the GTC talk was, was basically discussing two different projects.

122
00:09:24,760 --> 00:09:30,240
So in terms of domain adaptation, the problem we were trying to solve there is that essentially

123
00:09:30,240 --> 00:09:37,680
the problem is when you kind of look at, when you look at these features of cells in normal

124
00:09:37,680 --> 00:09:44,440
versus schizophrenia people, the, and you do say, so one of the, one of the most common

125
00:09:44,440 --> 00:09:48,360
first tasks that people do when they, when they get this kind of data is they, they will

126
00:09:48,360 --> 00:09:53,200
do some, you know, dimensionality reduction and they'll sort of visualize the cells that

127
00:09:53,200 --> 00:09:55,680
they get from these normal schizophrenia people.

128
00:09:55,680 --> 00:10:00,400
And the first thing that you notice when you do this visualization is that all of the

129
00:10:00,400 --> 00:10:06,200
cells that you collect from normal people separate very distinctly from the cells that you

130
00:10:06,200 --> 00:10:08,040
get from schizophrenia people.

131
00:10:08,040 --> 00:10:12,280
Now this is kind of a problem because the, you know, your underlying goal is, you know,

132
00:10:12,280 --> 00:10:17,240
you have these different types of cells represented in the normal and the schizophrenia people.

133
00:10:17,240 --> 00:10:23,160
And you ideally want to kind of in an unsupervised way match the right cell types from the normal

134
00:10:23,160 --> 00:10:27,800
schizophrenia people so that you can figure out for each type of cell, how are they different

135
00:10:27,800 --> 00:10:29,800
across the two populations.

136
00:10:29,800 --> 00:10:37,240
And so we use, we basically perform domain, domain adaptation to take these cells that kind

137
00:10:37,240 --> 00:10:42,120
of look very different overall between normal schizophrenia people and kind of merge them

138
00:10:42,120 --> 00:10:47,320
together such that cells of the same type basically look very similar to each other.

139
00:10:47,320 --> 00:10:51,800
And so we can kind of match them across, across these two groups of people.

140
00:10:51,800 --> 00:10:56,760
So the adaptation really is in basically building model to say, okay, given this is what my data

141
00:10:56,760 --> 00:11:00,480
from the normal people look like, given this is what the data from the schizophrenia people

142
00:11:00,480 --> 00:11:01,480
look like.

143
00:11:01,480 --> 00:11:08,240
Can I kind of match them such that the distribution of my cells in this feature space basically

144
00:11:08,240 --> 00:11:09,240
overlap?

145
00:11:09,240 --> 00:11:16,600
And so when you say the data looks very different between these two groups, in what sense?

146
00:11:16,600 --> 00:11:21,080
I mean, in some sense, that's what you want if you're trying to build a classifier that

147
00:11:21,080 --> 00:11:26,840
can determine whether a given cell indicates schizophrenia, for example.

148
00:11:26,840 --> 00:11:30,840
But it sounds like that's not the part of the process that you're working on here.

149
00:11:30,840 --> 00:11:37,480
So I guess just to be more clear, there are both kind of very large scale differences

150
00:11:37,480 --> 00:11:40,240
between normal and schizophrenia people.

151
00:11:40,240 --> 00:11:46,000
And then there's very kind of smaller changes that are specific to each type of cell that

152
00:11:46,000 --> 00:11:48,240
is different between normal and schizophrenia people.

153
00:11:48,240 --> 00:11:55,720
And so in a typical kind of analysis, you want to first characterize what are those very

154
00:11:55,720 --> 00:12:01,640
big changes that cause these two different types of cells from these two types of people

155
00:12:01,640 --> 00:12:05,160
to reside in different regions of this feature space.

156
00:12:05,160 --> 00:12:10,000
And then conditioned on understanding what those big changes are, you want to then take

157
00:12:10,000 --> 00:12:14,360
away the effect of that big change such that now you can look at individual cell types

158
00:12:14,360 --> 00:12:18,280
and then ask, okay, for these individual groups of cells that are common between normal

159
00:12:18,280 --> 00:12:22,200
and schizophrenia, what's different about them?

160
00:12:22,200 --> 00:12:25,000
What's the approach for doing that?

161
00:12:25,000 --> 00:12:32,120
There's been some previous work published, basically these class of models called associative

162
00:12:32,120 --> 00:12:35,200
domain adaptation have previously been developed.

163
00:12:35,200 --> 00:12:39,760
And so in the associative domain adaptation problem, the problem that are trying to solve

164
00:12:39,760 --> 00:12:45,440
is that they basically envision that you have, say, two different data sets where you're

165
00:12:45,440 --> 00:12:47,480
trying to perform the same task.

166
00:12:47,480 --> 00:12:51,280
So they assume that you're trying to do classification in dataset one and classification in dataset

167
00:12:51,280 --> 00:12:52,280
two.

168
00:12:52,280 --> 00:12:55,720
And they assume that you have, you know, you have the same labels that you're trying to

169
00:12:55,720 --> 00:12:58,200
classify in both datasets.

170
00:12:58,200 --> 00:13:04,120
Now what the assumption that the original associative domain adaptation makes is that although the

171
00:13:04,120 --> 00:13:08,600
labels that you're trying to classify are the same, the distribution of the data is different.

172
00:13:08,600 --> 00:13:13,480
So given your feature space, the data in one dataset sits in a different region of the

173
00:13:13,480 --> 00:13:15,480
feature space than the other.

174
00:13:15,480 --> 00:13:22,320
And so this, their domain adaptation approach was designed to basically try to essentially

175
00:13:22,320 --> 00:13:27,960
learn kind of common features that are predictive of the same labels across these two different

176
00:13:27,960 --> 00:13:29,360
datasets.

177
00:13:29,360 --> 00:13:36,040
And so our approach is basically similar in the sense that we have, you know, we have data

178
00:13:36,040 --> 00:13:42,640
in sort of two different regions of the feature space, corresponding to normal and schizophrenia.

179
00:13:42,640 --> 00:13:48,560
But in our case, we're trying to do an unsupervised analysis, so we're trying to group cells that

180
00:13:48,560 --> 00:13:52,640
come from these two different domains, but we think represent the same cell type.

181
00:13:52,640 --> 00:14:00,600
And so we basically modify their domain app, original supervised domain adaptation approach

182
00:14:00,600 --> 00:14:05,480
to do unsupervised clustering across two datasets.

183
00:14:05,480 --> 00:14:12,600
So the way that the approach works is that it's a deep neural network and it looks, it

184
00:14:12,600 --> 00:14:15,600
looks for all intents and purposes like an autoencoder.

185
00:14:15,600 --> 00:14:21,600
So, you know, the first half of the network takes the data in the original feature space

186
00:14:21,600 --> 00:14:26,000
and it projects it down to a low dimensional manifold.

187
00:14:26,000 --> 00:14:29,600
And then the second half of the network takes the low dimensional manifold and projects

188
00:14:29,600 --> 00:14:32,560
it back into some high dimensional space.

189
00:14:32,560 --> 00:14:38,240
But kind of unlike a typical autoencoder where the loss function that you're optimizing,

190
00:14:38,240 --> 00:14:42,960
you know, can be something like C squared loss, sorry, squared error.

191
00:14:42,960 --> 00:14:47,880
What are and what what our loss function is and what the other domain adaptation loss

192
00:14:47,880 --> 00:14:53,760
function is, is it it actually, what we try to do is we try to find a low dimensional

193
00:14:53,760 --> 00:15:01,920
embedding of the different cells such that the marginal distribution of the cells in the

194
00:15:01,920 --> 00:15:06,640
latent space overlap between the two datasets as much as possible.

195
00:15:06,640 --> 00:15:12,400
And so there's no kind of notion of kind of squared loss or reconstruction loss that

196
00:15:12,400 --> 00:15:13,800
we're trying to optimize here.

197
00:15:13,800 --> 00:15:19,440
It's really just trying to match the marginal distributions of the cells in the low dimensional

198
00:15:19,440 --> 00:15:22,280
embedding space between the two different datasets.

199
00:15:22,280 --> 00:15:28,920
And so the embedding space that you're creating when you're trying to overlap the two, that's

200
00:15:28,920 --> 00:15:33,240
essentially the domain adaptation aspect of this, is that right?

201
00:15:33,240 --> 00:15:34,240
Exactly.

202
00:15:34,240 --> 00:15:38,680
Yeah, we're essentially adapting like one domain to another.

203
00:15:38,680 --> 00:15:44,960
We're distorting one set of cells to match the marginal distribution of the other.

204
00:15:44,960 --> 00:15:52,080
So where does the domain adaptation fit in in your overall kind of end to end experimentation?

205
00:15:52,080 --> 00:15:57,840
Well, so what we do is we take the two the cells from the two different groups.

206
00:15:57,840 --> 00:16:04,160
We do this adaptation of to bring them into the same region of the future space.

207
00:16:04,160 --> 00:16:08,800
And that then allows us to match for an individual cell by cell basis.

208
00:16:08,800 --> 00:16:14,920
We can say, okay, this cell in the normal person corresponds to exactly this normal cell

209
00:16:14,920 --> 00:16:17,080
in the schizophrenic person.

210
00:16:17,080 --> 00:16:22,200
And then we can, that then allows us downstream to then look at those individual cells paired

211
00:16:22,200 --> 00:16:26,700
across normal schizophrenic people and ask the question, how are they different in

212
00:16:26,700 --> 00:16:28,680
terms of the original features?

213
00:16:28,680 --> 00:16:35,720
And when you're talking about looking at these different types of cells pre and post adaptation,

214
00:16:35,720 --> 00:16:44,760
are you taking the output of the domain adaptation and using that to train a model or is it for

215
00:16:44,760 --> 00:16:46,520
more manual analysis?

216
00:16:46,520 --> 00:16:50,560
So one thing that I didn't quite discuss it as I said that we, you know, we trained

217
00:16:50,560 --> 00:16:55,720
this network to learn and embedding a shared embedding space for both kind of the normal

218
00:16:55,720 --> 00:17:00,200
and schizophrenic people, but what I didn't talk about is that the other half of the network

219
00:17:00,200 --> 00:17:06,160
is still a decoder and it's a decoder in the traditional sense where we learn the encoder

220
00:17:06,160 --> 00:17:07,160
separately.

221
00:17:07,160 --> 00:17:12,640
And then after we learn the shared embedding space, we train decoders to take cells from

222
00:17:12,640 --> 00:17:18,360
this shared embedding space and project them back into the normal versus schizophrenic

223
00:17:18,360 --> 00:17:19,360
data set.

224
00:17:19,360 --> 00:17:22,760
And so this is important for the following reason.

225
00:17:22,760 --> 00:17:29,400
So the reason why domain adaptation is necessary for this kind of genomic analysis is that

226
00:17:29,400 --> 00:17:32,480
in the ideal biological experiment, you would be able to take a normal cell, you would

227
00:17:32,480 --> 00:17:38,080
be able to kind of assay it's, you know, make those high dimensional measurements, you

228
00:17:38,080 --> 00:17:43,880
would then be able to kind of give it schizophrenic and give it schizophrenia in some sense and then measure

229
00:17:43,880 --> 00:17:45,600
its activity again.

230
00:17:45,600 --> 00:17:49,600
And so in that sense, you'd be able to take exactly the same cell but kind of before and

231
00:17:49,600 --> 00:17:55,560
after you've applied this phenotype, this disease phenotype, and then see exactly what change

232
00:17:55,560 --> 00:17:59,640
occurred in that cell do do this, do this stimulus.

233
00:17:59,640 --> 00:18:04,880
And so this isn't possible in real biology because to kind of, you know, make these high

234
00:18:04,880 --> 00:18:08,760
dimensional measurements, you have to kill the cell and so you can measure the same cell

235
00:18:08,760 --> 00:18:09,760
twice.

236
00:18:09,760 --> 00:18:15,720
But because we can train decoders which take any cell in this shared embedding space, project

237
00:18:15,720 --> 00:18:21,880
it to normal or schizophrenia, we can now kind of simulate, you know, what would happen

238
00:18:21,880 --> 00:18:26,000
if you took one cell and you gave it either, you know, you made it either normal cell or

239
00:18:26,000 --> 00:18:27,480
schizophrenia cell.

240
00:18:27,480 --> 00:18:33,200
And so now we can treat these projections as paired data and then ask on a personal basis,

241
00:18:33,200 --> 00:18:36,560
you know, how do they, how do they differ between between these two conditions?

242
00:18:36,560 --> 00:18:41,000
And so, so getting back to original question, we don't, you know, the downstream analysis

243
00:18:41,000 --> 00:18:45,720
is still kind of somewhat manual but we are kind of trying to do something a little bit

244
00:18:45,720 --> 00:18:50,800
smarter by using these decoders to kind of simulate these biological experiments that

245
00:18:50,800 --> 00:18:51,800
we can't do.

246
00:18:51,800 --> 00:18:52,800
Right.

247
00:18:52,800 --> 00:18:53,800
Right.

248
00:18:53,800 --> 00:19:00,120
But ultimately you're not trying to like train a classifier to determine, you know, given

249
00:19:00,120 --> 00:19:09,280
a cell, whether it's normal or schizophrenic or not, it's more using the domain adaptation

250
00:19:09,280 --> 00:19:18,320
to allow you to more kind of manually compare the characteristics of these two types of

251
00:19:18,320 --> 00:19:25,120
cells absent the kind of broad spectrum differences between the two.

252
00:19:25,120 --> 00:19:30,760
So that's the domain adaptation piece and then you, you're using generative models as

253
00:19:30,760 --> 00:19:32,480
a part of this as well.

254
00:19:32,480 --> 00:19:33,480
Where do they come in?

255
00:19:33,480 --> 00:19:34,480
Right.

256
00:19:34,480 --> 00:19:38,920
And so the idea behind these generative models is that one of the kind of really hot research

257
00:19:38,920 --> 00:19:45,720
areas in biology right now is to kind of try to understand how cells work together.

258
00:19:45,720 --> 00:19:46,720
Right.

259
00:19:46,720 --> 00:19:50,560
And so, you know, there's this kind of this big focus on, okay, can we, you know, do these

260
00:19:50,560 --> 00:19:54,520
genomic measurements on these individual cell single cell levels?

261
00:19:54,520 --> 00:19:58,480
But part of the thing is that when you look at, like, you know, a disease like cancer,

262
00:19:58,480 --> 00:20:03,520
for example, cancer is not really just kind of a collection of individual cancer cells

263
00:20:03,520 --> 00:20:04,520
doing their own thing.

264
00:20:04,520 --> 00:20:07,480
They're kind of, there's a lot of cross talk between them.

265
00:20:07,480 --> 00:20:11,160
They're kind of helping each other out to kind of, you know, metastasize and so on.

266
00:20:11,160 --> 00:20:17,040
And so the idea of our generative model is that we kind of want to, is that, you know,

267
00:20:17,040 --> 00:20:23,680
it's very easy nowadays to take cells in isolation and, you know, measure their genomics profile

268
00:20:23,680 --> 00:20:25,440
to see what they're doing.

269
00:20:25,440 --> 00:20:29,920
And it's also easy to take a collection of them together when they're working together

270
00:20:29,920 --> 00:20:33,560
and measure as a whole, you know, what they're doing.

271
00:20:33,560 --> 00:20:38,000
And so now what we're trying to do is we're trying to develop models where generative models

272
00:20:38,000 --> 00:20:43,840
where we have generative models for each individual cell based on measurements we make on single

273
00:20:43,840 --> 00:20:45,000
cell level.

274
00:20:45,000 --> 00:20:49,960
And then these component generative models essentially inform a very larger generative

275
00:20:49,960 --> 00:20:54,400
model which try to explain what happens when you put them all together, if that makes

276
00:20:54,400 --> 00:20:55,400
sense.

277
00:20:55,400 --> 00:20:58,600
And so we're basically learning these nested generative models to say, okay, what can

278
00:20:58,600 --> 00:21:04,920
we learn about cells in isolation and then can we explain how they work together and why

279
00:21:04,920 --> 00:21:08,760
that's different from just some of the individual components itself.

280
00:21:08,760 --> 00:21:10,600
Wow, there's a lot there.

281
00:21:10,600 --> 00:21:17,880
So these nested generative models, are they end to end trained or are you training them

282
00:21:17,880 --> 00:21:25,000
on a cell by cell basis and then kind of unsombling or aggregating to create the system

283
00:21:25,000 --> 00:21:26,000
level?

284
00:21:26,000 --> 00:21:32,560
We're basically training them end to end by optimizing.

285
00:21:32,560 --> 00:21:40,760
So basically, for example, the component generative models that kind of handle what generating

286
00:21:40,760 --> 00:21:44,880
individual cell types look like, those are influenced both by our data on the individual

287
00:21:44,880 --> 00:21:49,760
cells themselves as well as kind of the cells all together, whereas other components that

288
00:21:49,760 --> 00:21:54,680
take these, whereas the other pieces of the bigger model that take the individual components

289
00:21:54,680 --> 00:22:00,200
and somehow add them together, those are only influenced by the data on kind of the bulk

290
00:22:00,200 --> 00:22:01,760
cells, all of the cells together.

291
00:22:01,760 --> 00:22:06,240
And so we kind of try to optimize them all, we try to optimize the parameters of all

292
00:22:06,240 --> 00:22:09,640
of these parts of the network at the same time.

293
00:22:09,640 --> 00:22:16,440
And so is the starting the underlying data set that you're using to build up these generative

294
00:22:16,440 --> 00:22:17,440
models?

295
00:22:17,440 --> 00:22:23,000
Is it the same type of data that we discussed previously for the domain adaptation piece?

296
00:22:23,000 --> 00:22:29,160
Right, so the parts, the type of data used to train the individual generative models

297
00:22:29,160 --> 00:22:35,240
of each cell type are the same type of data as we just talked about with the domain adaptation.

298
00:22:35,240 --> 00:22:41,840
The type of data that we collect to measure how they're working together is actually from

299
00:22:41,840 --> 00:22:43,000
an older technology.

300
00:22:43,000 --> 00:22:46,880
So kind of genomics technologies that don't work on single cell levels, but they work

301
00:22:46,880 --> 00:22:50,840
on measuring genomics of a collection of cells together.

302
00:22:50,840 --> 00:22:56,960
And so we're kind of mixing data then from a newer generation and in older generation.

303
00:22:56,960 --> 00:23:00,240
What type of gen approach did you use for this?

304
00:23:00,240 --> 00:23:04,520
So we actually, we started out with variational auto encoders actually.

305
00:23:04,520 --> 00:23:08,560
We tried different types of different variants of variational auto coders, it turns out for

306
00:23:08,560 --> 00:23:13,600
this specific problem, in terms of performance, you know, this specific variant didn't seem

307
00:23:13,600 --> 00:23:16,040
to make such a big difference.

308
00:23:16,040 --> 00:23:21,400
So we are trying to, we are also trying more certainly to kind of combine.

309
00:23:21,400 --> 00:23:25,320
So I mean, the number of people have looked at this performance, basically use like deep

310
00:23:25,320 --> 00:23:32,160
graphical models, where again, like the generative models are, you know, they use proper probability

311
00:23:32,160 --> 00:23:35,360
distributions, but they also use neural networks in there as well.

312
00:23:35,360 --> 00:23:43,160
So the variational auto encoder, you started this project using variational auto encoders

313
00:23:43,160 --> 00:23:49,960
for your generative models, but then did you ultimately evolve to using more of a

314
00:23:49,960 --> 00:23:51,960
GAN type of a model?

315
00:23:51,960 --> 00:23:57,280
We actually have tried, we've tried GANs in the past, we found they were a little bit

316
00:23:57,280 --> 00:23:58,640
more difficult to train.

317
00:23:58,640 --> 00:24:04,320
And so part of the issue here, when, you know, part of the issue we have in the field of

318
00:24:04,320 --> 00:24:09,280
computation biology is that we're building these models in part to try to understand

319
00:24:09,280 --> 00:24:13,720
something about biology into these, but also a lot of these kind of problems.

320
00:24:13,720 --> 00:24:16,800
So this, the problem that we're trying to solve with these generative models is what's

321
00:24:16,800 --> 00:24:18,640
called deconvolution.

322
00:24:18,640 --> 00:24:23,600
And this, the problem of deconvolution arises in many different areas of biology as well.

323
00:24:23,600 --> 00:24:29,000
And so part of what we're trying to do is also develop kind of usable software than

324
00:24:29,000 --> 00:24:33,520
Ben Shindtis, Ben Shindtis biologists who may not have a lot of experience training

325
00:24:33,520 --> 00:24:38,000
neural nets, for example, can easily just kind of pick up and apply to their own problems.

326
00:24:38,000 --> 00:24:45,840
And so we found it using, at least, you know, when comparing VAE's versus GANs, the VAE

327
00:24:45,840 --> 00:24:50,440
based models were easier for other people to train and use on their own data, whereas

328
00:24:50,440 --> 00:24:55,520
the GANs seem to be much harder to get to work out of the box.

329
00:24:55,520 --> 00:24:56,520
Got it.

330
00:24:56,520 --> 00:25:01,920
And so I guess from a more practical perspective, we, our current models are based on VAE's

331
00:25:01,920 --> 00:25:05,880
just because it's easier to give to other people to use on their own problems.

332
00:25:05,880 --> 00:25:12,360
And so you've got the domain adaptation piece and the generative models.

333
00:25:12,360 --> 00:25:18,680
Are those, do those then come together as part of this pipeline, or are they just both

334
00:25:18,680 --> 00:25:25,200
tools in your tool bag that you used to study these cells and the conditions that create

335
00:25:25,200 --> 00:25:26,720
them?

336
00:25:26,720 --> 00:25:35,680
So they're basically two related but distinct tools that we're using on similar problems.

337
00:25:35,680 --> 00:25:43,640
So we're basically part of a project called the human cell Atlas, whose goal is in part

338
00:25:43,640 --> 00:25:49,480
to kind of characterize human tissues and organs at the single cell level and characterize

339
00:25:49,480 --> 00:25:52,800
how they look under normal and disease conditions.

340
00:25:52,800 --> 00:25:59,120
And so both of these tools, you know, one which enables comparison of data collected

341
00:25:59,120 --> 00:26:04,720
under different experiments, which is the domain adaptation and the other deconvolution,

342
00:26:04,720 --> 00:26:11,560
which is a tool to help us understand how do, you know, wire human tissues, not just collections

343
00:26:11,560 --> 00:26:13,320
of their individual components.

344
00:26:13,320 --> 00:26:19,520
I sort of see both of these tools as, you know, ways to study this, to study the same problem

345
00:26:19,520 --> 00:26:20,520
essentially.

346
00:26:20,520 --> 00:26:26,320
Are there examples you can share of insights that these tools ultimately led to?

347
00:26:26,320 --> 00:26:32,680
Up to now, we've basically been working mostly on kind of the development aspect of this

348
00:26:32,680 --> 00:26:38,040
tool, so we've been developing it and kind of validating that it works on, you know,

349
00:26:38,040 --> 00:26:39,440
datasets with known ground truth.

350
00:26:39,440 --> 00:26:44,720
And so now we're kind of in the, we're in the process of applying these tools in the

351
00:26:44,720 --> 00:26:45,800
number of different ways.

352
00:26:45,800 --> 00:26:51,520
And so to give an example of, you know, maybe a more practical application, this tool,

353
00:26:51,520 --> 00:26:56,800
we're working, you know, one, in one problem we're looking at, we're trying to understand

354
00:26:56,800 --> 00:27:04,120
how, for example, these malaria parasites, you know, transmit themselves across, you

355
00:27:04,120 --> 00:27:06,840
know, across various populations.

356
00:27:06,840 --> 00:27:12,000
And so, you know, the thing about malaria is that malaria is kind of an interesting parasite

357
00:27:12,000 --> 00:27:18,280
because it has this like life cycle where for single, basically for single parasite,

358
00:27:18,280 --> 00:27:25,000
at the end of this life cycle, it has to make a decision, you know, do I asexually reproduce

359
00:27:25,000 --> 00:27:30,280
or do I sexually reproduce and therefore, you know, which therefore leads to transmission.

360
00:27:30,280 --> 00:27:36,520
And so kind of understanding, you know, if you can get a good grasp of how these malaria

361
00:27:36,520 --> 00:27:41,960
parasites decide when and how to transmit to other hosts, then you could potentially

362
00:27:41,960 --> 00:27:43,840
develop therapies to stop it.

363
00:27:43,840 --> 00:27:49,400
And so that's in this specific problem we were working with some other scientists to,

364
00:27:49,400 --> 00:27:55,840
to basically compare malaria parasites that were undergoing kind of sexual or asexual

365
00:27:55,840 --> 00:28:00,680
reproduction and doing this domain adaptation to then ask the question, you know, at what

366
00:28:00,680 --> 00:28:05,920
point do these two different, do these malaria parasites, you know, going down two different

367
00:28:05,920 --> 00:28:07,720
fates change.

368
00:28:07,720 --> 00:28:13,360
And so by applying this tool, we identified a small set of genes, which now we think

369
00:28:13,360 --> 00:28:19,160
if we kind of delete these genes from the malaria or somehow inhibit them from working,

370
00:28:19,160 --> 00:28:21,640
we might be able to stop their transmission.

371
00:28:21,640 --> 00:28:26,080
But this kind of work is still, the downstream application of these tools is still kind

372
00:28:26,080 --> 00:28:30,240
of something that we're now just getting into now that we've kind of established that

373
00:28:30,240 --> 00:28:31,240
the tools are working.

374
00:28:31,240 --> 00:28:37,240
Well, certainly that the application to malaria could be hugely impactful if we're able

375
00:28:37,240 --> 00:28:44,120
to identify and kind of stop that whatever the sequence is, yeah, the transmission, obviously

376
00:28:44,120 --> 00:28:48,840
the transmission, but whatever the mechanism is that is causing the transmission, whether

377
00:28:48,840 --> 00:28:53,120
there are other things that you covered in your presentation that it would make sense

378
00:28:53,120 --> 00:28:55,560
to jump into.

379
00:28:55,560 --> 00:28:59,520
So I think, you know, one of the things that we briefly touched on that we're really

380
00:28:59,520 --> 00:29:08,280
excited about is kind of the use of the use of machine learning to do what people sometimes

381
00:29:08,280 --> 00:29:10,960
call like multimodal data integration.

382
00:29:10,960 --> 00:29:17,240
And so the idea here is that sort of in biology again, a lot of people including, including,

383
00:29:17,240 --> 00:29:22,480
you know, my research lab, you know, we do a lot of analyses at kind of like the DNA level,

384
00:29:22,480 --> 00:29:24,480
which is kind of very low level.

385
00:29:24,480 --> 00:29:29,760
And we, you know, our goal is always to somehow tie what happens at the low level to, you

386
00:29:29,760 --> 00:29:33,920
know, the big things like, you know, are you going to get the disease or not?

387
00:29:33,920 --> 00:29:39,920
But you know, tying these low level events to these high level things like disease incidents

388
00:29:39,920 --> 00:29:43,360
is sometimes really hard because there's a lot of steps between, you know, things that

389
00:29:43,360 --> 00:29:46,920
happen at DNA level and things that happen at, you know, the whole human level.

390
00:29:46,920 --> 00:29:53,840
So nowadays there's kind of more and more interest in people collecting data from the same kinds

391
00:29:53,840 --> 00:29:59,480
of cells or tissues that both kind of like the DNA level and at the whole cell level and

392
00:29:59,480 --> 00:30:03,240
at the kind of tissue level and then trying to link all of these things together such

393
00:30:03,240 --> 00:30:07,160
that we can instead of just trying to predict, okay, if something happened, if this event

394
00:30:07,160 --> 00:30:10,160
happens at DNA, you know, am I going to get disease?

395
00:30:10,160 --> 00:30:13,600
Now we can try to predict, okay, if something happens at the DNA level, does that somehow

396
00:30:13,600 --> 00:30:18,600
change how my, like, tissues are organized or how my brain is, like, structured and then

397
00:30:18,600 --> 00:30:20,640
how does that then kind of impact the season?

398
00:30:20,640 --> 00:30:26,760
So I think, you know, for example, we're working with some people at the Allendants

399
00:30:26,760 --> 00:30:31,640
too for brain science where they're kind of, they're doing some really cool experiments

400
00:30:31,640 --> 00:30:36,840
where they can take certain types of brain cells and they can use like electrophysiology

401
00:30:36,840 --> 00:30:42,720
or, you know, they can take pictures of the cell to look at their shape and then also

402
00:30:42,720 --> 00:30:45,400
measure what's happening at the genome level.

403
00:30:45,400 --> 00:30:51,880
And so now we're trying to build models which can basically tie together things at the

404
00:30:51,880 --> 00:30:59,240
DNA level, at the kind of electrophysiology level and at the imaging level to try to get

405
00:30:59,240 --> 00:31:04,600
a better understanding of how do events at the DNA change our risk of disease.

406
00:31:04,600 --> 00:31:10,200
And so this is, you know, this is kind of exciting because, you know, now we, you know, there's

407
00:31:10,200 --> 00:31:15,720
obviously tons and tons of work in the computer vision field for doing things like, you know,

408
00:31:15,720 --> 00:31:19,600
doing all things related to image processing and, you know, understanding, understanding

409
00:31:19,600 --> 00:31:20,600
images.

410
00:31:20,600 --> 00:31:24,800
And so now we can, you know, our goal over the next few years is to try to incorporate,

411
00:31:24,800 --> 00:31:30,040
you know, what's happening in, like, computer vision with, you know, our work in sort of genomics

412
00:31:30,040 --> 00:31:34,120
along with, you know, what people have been doing, modeling neuroscience data and try

413
00:31:34,120 --> 00:31:38,560
to put it all together to try to kind of really understand what's happening in biology

414
00:31:38,560 --> 00:31:39,560
and the human cell.

415
00:31:39,560 --> 00:31:44,760
Well, very interesting work, Gerald, thanks so much for taking the time to chat with us

416
00:31:44,760 --> 00:31:45,760
about it.

417
00:31:45,760 --> 00:31:46,760
Yep, definitely.

418
00:31:46,760 --> 00:31:47,760
Thanks.

419
00:31:47,760 --> 00:31:53,320
All right, everyone, that's our show for today.

420
00:31:53,320 --> 00:31:59,880
For more information on any of the shows in our GTC 2019 series, visit twimmaleye.com

421
00:31:59,880 --> 00:32:02,760
slash GTC19.

422
00:32:02,760 --> 00:32:05,560
Thanks again to Dell for sponsoring this series.

423
00:32:05,560 --> 00:32:10,320
Be sure to check them out at dellemc.com slash precision.

424
00:32:10,320 --> 00:32:37,080
As always, thanks so much for listening and catch you next time.

