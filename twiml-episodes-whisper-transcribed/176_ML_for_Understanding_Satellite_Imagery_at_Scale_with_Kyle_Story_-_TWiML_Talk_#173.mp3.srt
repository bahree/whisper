1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,480
I'm your host Sam Charrington.

4
00:00:31,480 --> 00:00:36,880
Today we're joined by Kyle's story, computer vision engineer at Descartes Labs.

5
00:00:36,880 --> 00:00:41,920
Kyle and I caught up after his recent talk at the Google Cloud next conference titled

6
00:00:41,920 --> 00:00:47,320
How Computers See the Earth, a machine learning approach to understanding satellite imagery

7
00:00:47,320 --> 00:00:48,960
at scale.

8
00:00:48,960 --> 00:00:52,520
Kyle and I discussed some of the interesting computer vision problems he's worked on

9
00:00:52,520 --> 00:00:58,480
at Descartes, including custom object detectors and the company's geo-visual search engine.

10
00:00:58,480 --> 00:01:02,520
We cover everything from the models they've developed and the platform they've built

11
00:01:02,520 --> 00:01:06,200
to the key challenges they've had to overcome in scaling them.

12
00:01:06,200 --> 00:01:07,200
Enjoy.

13
00:01:07,200 --> 00:01:10,760
Alright everyone, I am on the line with Kyle's story.

14
00:01:10,760 --> 00:01:15,160
Kyle is a computer vision engineer at Descartes Labs.

15
00:01:15,160 --> 00:01:18,160
Kyle, welcome to this weekend machine learning and AI.

16
00:01:18,160 --> 00:01:19,680
Thank you so much for having me on.

17
00:01:19,680 --> 00:01:20,680
How are you?

18
00:01:20,680 --> 00:01:21,680
I am wonderful.

19
00:01:21,680 --> 00:01:28,400
You are joining an esteemed group of physicists who have been on this podcast.

20
00:01:28,400 --> 00:01:34,800
It seems like there are quite a number of folks that come out of cosmology like you and

21
00:01:34,800 --> 00:01:40,520
astronomy and particle physics and make their way to machine learning.

22
00:01:40,520 --> 00:01:42,200
How did you do that?

23
00:01:42,200 --> 00:01:50,480
Yeah, it's actually a fairly natural transition so my background is in cosmology as you said

24
00:01:50,480 --> 00:01:54,720
which is a branch of astrophysics and so I spent almost a decade in graduate school

25
00:01:54,720 --> 00:02:01,280
and then as a postdoc at Stanford working on a mostly on it with data from a telescope

26
00:02:01,280 --> 00:02:06,800
that's in Antarctica where we were measuring what's called the cosmic microwave background.

27
00:02:06,800 --> 00:02:10,240
So this is radiation left over from the big bang.

28
00:02:10,240 --> 00:02:14,840
You can think of it as an afterglow of the big bang and you can actually map it and what

29
00:02:14,840 --> 00:02:21,840
you see in those maps are basically a baby picture of the early universe or variations

30
00:02:21,840 --> 00:02:27,160
in density, about 100,000 years after the big bang is the furthest back you can see

31
00:02:27,160 --> 00:02:34,520
in time and so I spent a decade studying this using the information and the statistics

32
00:02:34,520 --> 00:02:41,840
in those data to try to understand the universe as a system, model the universe as a whole

33
00:02:41,840 --> 00:02:47,520
and then relatively recently this past fall came out of academia to join day cart labs

34
00:02:47,520 --> 00:02:53,880
and basically instead of looking upwards using a satellite I'm now using a telescope on

35
00:02:53,880 --> 00:02:59,160
the ground and kind of inverted that problem so I'm now using data from satellites to look

36
00:02:59,160 --> 00:03:06,080
downwards at the earth and so it's a lot of there are a lot of similarities and a number

37
00:03:06,080 --> 00:03:12,360
of important differences as well so in one sense they are both remote sensing problems

38
00:03:12,360 --> 00:03:20,080
so they are both trying to learn something about phenomenon purely based on the light

39
00:03:20,080 --> 00:03:28,200
that you see or the signals that you're receiving from that source and then on the machine

40
00:03:28,200 --> 00:03:34,840
learning side before we go too far I've got to ask have you did you ever get to spend

41
00:03:34,840 --> 00:03:40,360
time in Antarctica like I'm envisioning I forget the movie but like the you know the

42
00:03:40,360 --> 00:03:45,320
one camp where you go basically under I don't know if it's on the ground probably not

43
00:03:45,320 --> 00:03:51,200
under under eyes but you go into this into this encampment was it anything like that

44
00:03:51,200 --> 00:03:58,080
it's it's not quite like that I have spent just shy of a year of time total in Antarctica

45
00:03:58,080 --> 00:04:03,480
that was spread across six different trips and so that the telescope that I worked at

46
00:04:03,480 --> 00:04:08,920
is aptly named the South Pole telescope and it is literally at the South Pole and so

47
00:04:08,920 --> 00:04:14,000
I would go there during the austral summer season to maintain the telescope set it up

48
00:04:14,000 --> 00:04:20,120
for observations and then most of our observations were taken during the austral winter so when

49
00:04:20,120 --> 00:04:25,280
you're at the pool there is one day and one night per year so I like to say that I've

50
00:04:25,280 --> 00:04:32,120
spent almost a year in Antarctica and I've never seen the sunset just kind of interesting

51
00:04:32,120 --> 00:04:37,880
but it it's it's a base that's funded by the National Science Foundation and the South

52
00:04:37,880 --> 00:04:42,080
Pole Station is fairly small during the summer it usually has a hundred to a hundred and

53
00:04:42,080 --> 00:04:47,640
fifty people who live there we do have a building that's heated so kind of very small

54
00:04:47,640 --> 00:04:52,760
dormitory style rooms and a cafeteria and then where there works we just work all the

55
00:04:52,760 --> 00:04:59,400
time but it's it's a really interesting environment to be in it's just a super cool place to

56
00:04:59,400 --> 00:05:05,200
be and I really value having had the opportunity to spend time down there it sounds like an

57
00:05:05,200 --> 00:05:11,960
incredible experience so you were recently at the Google next conference talking a little

58
00:05:11,960 --> 00:05:20,200
bit about the work you're doing at Descartes and one of the things that you made sure to

59
00:05:20,200 --> 00:05:25,920
touch on is really the types of problems that you encounter the computer vision problems

60
00:05:25,920 --> 00:05:31,160
that you encounter at Descartes and some of the unique characteristics of those problems

61
00:05:31,160 --> 00:05:34,080
can you maybe review those for us?

62
00:05:34,080 --> 00:05:41,840
Sure so the types of problems that we're working on in Descartes labs is trying to understand

63
00:05:41,840 --> 00:05:50,160
the world using machine learning and using satellite imagery and other geospatial or like

64
00:05:50,160 --> 00:05:57,560
georeference data sets so it's it's really is a big data problem and I know that's kind

65
00:05:57,560 --> 00:06:03,240
of a you know thrown around as a phrase that people like to use but the the size and scale

66
00:06:03,240 --> 00:06:10,800
of the data are are quite large and because of that it really does require automated analysis

67
00:06:10,800 --> 00:06:16,440
approaches and this is where the machine learning comes in is trying to instead of just

68
00:06:16,440 --> 00:06:21,480
saying we have a bunch of pictures let's set somebody down in front of computer to go

69
00:06:21,480 --> 00:06:28,440
look it through of all of those pictures that that doesn't really scale to the globe right

70
00:06:28,440 --> 00:06:34,760
so that's where that's where we take a machine learning approach where you say let's instead

71
00:06:34,760 --> 00:06:42,000
build an algorithms that can detect either objects or patterns and then we can automatically

72
00:06:42,000 --> 00:06:49,160
scale these to large regions so and just to give you a concrete or two concrete examples

73
00:06:49,160 --> 00:06:54,640
one of the places that Descartes labs started as a company was trying to understand corn

74
00:06:54,640 --> 00:07:01,360
production in the United States so being able to measure and predicts corn production

75
00:07:01,360 --> 00:07:07,040
over the the Midwest United States and then another example just to have something concrete

76
00:07:07,040 --> 00:07:13,080
is mapping infrastructure in a higher resolution is satellite imagery so for example being

77
00:07:13,080 --> 00:07:17,720
able to find just simply find buildings be able to map make maps of buildings and you'd

78
00:07:17,720 --> 00:07:23,920
be surprised how incomplete open source data sets are even on on basic infrastructure

79
00:07:23,920 --> 00:07:31,120
like this so the idea with that latter example is we don't have good lists of where the

80
00:07:31,120 --> 00:07:37,960
nuclear reactors are or the power stations or things like that so we need to actually regenerate

81
00:07:37,960 --> 00:07:44,240
those lists using satellite imagery so some of the specific types of buildings like nuclear

82
00:07:44,240 --> 00:07:50,800
power plants are pretty well listed but even just mapping you know residential buildings

83
00:07:50,800 --> 00:07:57,920
and industrial buildings and simply just knowing where they are the fast open source data

84
00:07:57,920 --> 00:08:02,520
set that I'm aware of his open street map and so some of your listeners may be familiar

85
00:08:02,520 --> 00:08:09,480
with that and it's an incredible data source but even in the United States when I'm looking

86
00:08:09,480 --> 00:08:14,680
at that data set I can go and pull up and see entire neighborhoods that just are blank

87
00:08:14,680 --> 00:08:20,720
just don't have anything filled in and that's largely because that community has primarily

88
00:08:20,720 --> 00:08:25,280
been based on people hand labeling things and largely volunteer effort and it's really

89
00:08:25,280 --> 00:08:31,240
impressive what they've been able to do but being able to scale that to an up-to-date

90
00:08:31,240 --> 00:08:35,720
map of buildings across the United States or even better across the world is really

91
00:08:35,720 --> 00:08:42,040
just not it's not up to the task of hand labeling all of you know segmenting out each building

92
00:08:42,040 --> 00:08:47,720
in a neighborhood and so so when you step outside of the United States of course it the

93
00:08:47,720 --> 00:08:53,800
problem changes as well in that there's just a lot less information in a lot of countries

94
00:08:53,800 --> 00:08:59,960
around the United States and so by training convolutional neural nets or other machine-land

95
00:08:59,960 --> 00:09:06,480
approaches we can take a first pass at being able to map out where buildings are and that

96
00:09:06,480 --> 00:09:11,400
provides an incredible amount of information that would just be unfeasible to achieve

97
00:09:11,400 --> 00:09:16,600
in a so person sitting in front of a computer screen hand labeling type of way.

98
00:09:16,600 --> 00:09:24,280
I think many listeners of the podcasts may be familiar with for example the planet

99
00:09:24,280 --> 00:09:32,600
data set on Kaggle kind of understanding the Amazon from space and I know for those

100
00:09:32,600 --> 00:09:39,240
of us who have worked through the fast AI course you kind of spend some time playing

101
00:09:39,240 --> 00:09:46,680
with that data set and learning to you know automatically classify tiles that are forested

102
00:09:46,680 --> 00:09:52,600
from tiles that contain water features things like that and so we may like come to this

103
00:09:52,600 --> 00:09:58,040
conversation say oh yeah sounds easy pretty standard stuff like bring us to the real world

104
00:09:58,040 --> 00:10:03,200
what's the you know what are some of the challenges that you deal with that you know the

105
00:10:03,200 --> 00:10:10,480
folks doing this on Kaggle don't have to worry about yeah sure so I so I come at this

106
00:10:11,120 --> 00:10:17,600
from you know from my physics background as it's all about the data and the information

107
00:10:17,600 --> 00:10:24,480
that's in that data and so the stepping away from machine learning a little bit at first

108
00:10:24,480 --> 00:10:32,480
you simply need to be able to access the data in an efficient way so that's and you know these

109
00:10:32,480 --> 00:10:37,200
datasets are quite large you know when you when you come to a Kaggle competition that that step

110
00:10:37,200 --> 00:10:42,320
has pretty much already been taken care of for you but there was a huge amount of effort in being

111
00:10:42,320 --> 00:10:49,440
able to get the satellite imagery and then have it quickly accessible in you know a way that you

112
00:10:49,440 --> 00:10:54,800
can incorporate into even starting to take a machine learning approach and so this is actually

113
00:10:54,800 --> 00:11:00,240
one of the things that Descartes labs realized fairly early on was that a lot of the power for

114
00:11:00,240 --> 00:11:05,200
being able to work with these datasets is going to come from having a platform that allows

115
00:11:05,200 --> 00:11:11,840
uniform access and quick access to these types of data so we one of the things that we have done

116
00:11:11,840 --> 00:11:17,440
and it has been a primary goal and continues to be a primary objective of the company is to

117
00:11:17,440 --> 00:11:25,520
build out a platform that provides access quick access to a huge amount of data we do so this is

118
00:11:25,520 --> 00:11:30,960
why we are at the Google next conference we've built out this entire platform within Google Cloud

119
00:11:30,960 --> 00:11:37,440
so it's so we pretty much don't own any hardware beyond our own laptops and are working almost

120
00:11:37,440 --> 00:11:43,200
exclusively within the cloud and what that gives us is being able to host a large amount of data

121
00:11:43,200 --> 00:11:48,880
in a way that we can access very quickly so that's that's one problem then another so then a more

122
00:11:48,880 --> 00:11:55,920
machine learning specific problem is the is the ground truth question right so when you come to

123
00:11:55,920 --> 00:12:02,560
a cargo competition oftentimes you have a lot of well labeled ground truth and how basically

124
00:12:02,560 --> 00:12:07,200
how much ground truth you have will dictate what types of algorithms you're able to use and then

125
00:12:07,200 --> 00:12:14,160
what types of problems you're able to solve so those are at least just two problems that

126
00:12:14,160 --> 00:12:20,560
present present themselves when you start dealing with questions in the real world on that first

127
00:12:20,560 --> 00:12:29,360
point the platform talk about I'm curious what that means for you and or rather I'm curious what

128
00:12:29,360 --> 00:12:36,880
that all that that encompasses you know is the platform you know just some set of Google Cloud

129
00:12:36,880 --> 00:12:46,720
services that you're using off the shelf or have you built a lot of capability on top of kind of

130
00:12:46,720 --> 00:12:53,440
raw level hardware that you're using via Google Cloud you know walk me through like what the

131
00:12:53,440 --> 00:13:00,720
the elements of this platform are sure so it's all the above everything you mentioned okay

132
00:13:00,720 --> 00:13:09,120
uh the so the first step is getting access to these data the so there are a number of openly

133
00:13:09,120 --> 00:13:15,360
available sources like Landsat or Sentinel generally government sources and they all have their own

134
00:13:15,360 --> 00:13:23,360
unique ways of accessing the data so in principle it's it's open freely available but each one has

135
00:13:23,360 --> 00:13:28,800
it you know each one has a slightly different format or has slightly different image registering

136
00:13:28,800 --> 00:13:35,040
or has been tiled in a different way so the first big step is ingesting all of those data and

137
00:13:35,040 --> 00:13:40,880
then hosting them in the cloud so figuring out how to access the data stream pulling out all of

138
00:13:40,880 --> 00:13:47,040
that data in uploading it into the cloud the next the the next step that the platform accomplishes

139
00:13:47,040 --> 00:13:54,800
is tiling those images across the globe so in so I as a because I'm working within the you know

140
00:13:54,800 --> 00:13:59,760
within our platform I don't have to think too carefully about how to stitch together a bunch of

141
00:13:59,760 --> 00:14:05,440
different images in order to make a picture that covers the region that I'm interested in that's

142
00:14:05,440 --> 00:14:12,560
all handled in an automatic way so a predetermined way of tiling the data uh that just simply removes

143
00:14:12,560 --> 00:14:18,880
one of the steps that I would otherwise have to deal with then there's the search question right

144
00:14:18,880 --> 00:14:24,480
so let's say I want to let's continue working with the building model example since that's the one

145
00:14:24,480 --> 00:14:32,080
that I've personally worked on directly so let's say I want to collect a set of imagery over

146
00:14:32,080 --> 00:14:39,760
California to use as training data how do I figure out which exactly which images I need to pull

147
00:14:39,760 --> 00:14:45,280
from you know from the giant database so we have a search capability that has where each

148
00:14:45,280 --> 00:14:52,160
each image that gets uploaded has a list of metadata and then we can using the google's big query

149
00:14:52,160 --> 00:14:57,280
we can or actually this is the last search we can search through that metadata and pull down

150
00:14:57,280 --> 00:15:03,200
exactly which images over which region over what time in a really efficient way and then turning

151
00:15:03,200 --> 00:15:09,760
those images from so it's from whatever format compress format they've been saved into into an

152
00:15:09,760 --> 00:15:15,840
actual image that I can applaud on my computer and then do things with and so this actually brings

153
00:15:15,840 --> 00:15:23,200
up a point that maybe we can get into a little bit later of how we think about these types of analysis

154
00:15:23,200 --> 00:15:30,720
and what I mean by that is thinking about imagery instead of something you pull up on your computer

155
00:15:30,720 --> 00:15:35,600
and then you flip through images kind of hand-level things or look at things thinking about that

156
00:15:35,600 --> 00:15:41,760
imagery as input to algorithms input to machine-learning algorithms and when you when you you start

157
00:15:41,760 --> 00:15:48,400
thinking in in those terms it's you definitely take a different approach to how you work with that

158
00:15:48,400 --> 00:15:54,240
data how you store it on disk how you make it available interesting so for example is that

159
00:15:54,240 --> 00:16:02,720
latter point meaning it as opposed to storing the images on disk as you know in a native image

160
00:16:02,720 --> 00:16:08,880
format your form your storing them as like TensorFlow records or things like that close it's that's

161
00:16:08,880 --> 00:16:16,320
right yes so we actually chose to store the majority of our data in jpeg 2000 and it's a the

162
00:16:16,320 --> 00:16:26,240
reason for doing so is it that format basically decomposes the images by scale so and and then

163
00:16:26,240 --> 00:16:32,080
allows you to use the correlations between different bands so these images you know can have

164
00:16:32,080 --> 00:16:38,160
spectral bands like red, green, blue, and year and red, swirr etc so you can take advantage of

165
00:16:38,160 --> 00:16:42,480
some of the fact that these bands are correlated to be able to compress them significantly

166
00:16:43,120 --> 00:16:51,120
and so when you say store them by scale is that in a sense of like a progressive image where you've

167
00:16:51,120 --> 00:16:58,560
got kind of a low resolution version of the entire tile and then like progressively more detailed

168
00:16:59,280 --> 00:17:04,880
tiles on a yeah yeah so it it allows you to do that without having to save duplication duplicate

169
00:17:04,880 --> 00:17:10,800
copies of the data right so if you're so if then you so this is one way that we're able to zoom in

170
00:17:10,800 --> 00:17:16,160
and out you know so on the visual side when a person is actually looking at it being able to zoom

171
00:17:16,160 --> 00:17:23,200
in and out quickly is taking advantage of that kind of layered structure of the way that the data

172
00:17:23,200 --> 00:17:32,800
is saved and then sticking to this theme of platform you kind of talked about these three steps I

173
00:17:32,800 --> 00:17:43,840
guess this ingest and the indexing and then the way you manage and manipulate these images I don't

174
00:17:43,840 --> 00:17:48,560
know if I captured exactly as the way you described it but you know looking at like the ingest

175
00:17:48,560 --> 00:17:54,480
side is that are you what are the different ways that you're getting the imagery like are you

176
00:17:54,480 --> 00:18:02,400
I don't recall if Google has a you know ship a hardware box type of feature like a AWS snowball

177
00:18:02,400 --> 00:18:09,520
but is that an issue for you are you mostly getting them online and landing them in cloud storage

178
00:18:09,520 --> 00:18:15,120
or something similar yeah it's the second it's the second option so we're pretty much always

179
00:18:15,120 --> 00:18:22,480
getting them online and then depending on the most of the data sources that we're interested in

180
00:18:22,480 --> 00:18:28,320
are data sources that are continually updating right because when you're thinking about what

181
00:18:28,320 --> 00:18:32,320
types of problems you actually want to use these data for a lot of it is going to be looking for

182
00:18:32,320 --> 00:18:38,320
change or monitoring what's what's new on the surface of the earth and so it's important to have

183
00:18:38,320 --> 00:18:45,360
as you know it's close to real-time data as the sources can provide so what so we're stepping

184
00:18:45,360 --> 00:18:51,600
a little bit outside my specific expertise but what those in general look like are pipelines

185
00:18:51,600 --> 00:18:58,240
that are built within the cloud that can at check for new imagery as that new imagery becomes

186
00:18:58,240 --> 00:19:05,200
available pulled into the cloud and then using the you know using parallel parallel compute they

187
00:19:05,200 --> 00:19:12,320
can process those imagery filter out all the metadata get that get that all set up and then store

188
00:19:12,320 --> 00:19:18,880
it into our cloud storage system so it can then be searched and accessed by a user like myself

189
00:19:18,880 --> 00:19:26,160
and then so from a machine learning platform once you've got all of this imagery in place what are

190
00:19:26,160 --> 00:19:31,600
some of the types of models that you're building and the specific problems that you're trying

191
00:19:31,600 --> 00:19:38,880
to solve so let's say a little bit in general about how I think about the data and then we can

192
00:19:38,880 --> 00:19:46,560
talk about some specific problems so with these data and there are kind of three dimensions sort of

193
00:19:46,560 --> 00:19:53,920
that are that provide at least me a useful framework for thinking about how to put together analyses

194
00:19:53,920 --> 00:19:59,920
by that I mean a spatial dimension a spectral dimension a temporal dimension so the spatial

195
00:19:59,920 --> 00:20:07,120
dimension is just where are you in xy and perhaps z position on the surface of the earth the

196
00:20:07,120 --> 00:20:14,080
spectral dimension is for each image you may have taken data in different bands or different

197
00:20:14,080 --> 00:20:21,760
frequencies so optical frequencies red and blue like a ccd camera would take near infrared imagery

198
00:20:21,760 --> 00:20:28,880
there's thermal bands so that that type of spectral information can be really important

199
00:20:28,880 --> 00:20:33,680
and then there's the temporal dimension so what I mentioned about seeing how things change

200
00:20:33,680 --> 00:20:39,440
over time for a given place on the earth and then given these three you can combine them in

201
00:20:39,440 --> 00:20:43,600
different ways to address different types of problems so to be concrete about that

202
00:20:43,600 --> 00:20:50,000
usually a computer vision problem where I've spent most of my time working is generally combining

203
00:20:50,000 --> 00:20:56,640
spatial and spectral information so you're trying to say find where things are on the earth by

204
00:20:57,520 --> 00:21:04,240
looking at these images you can also combine spectral and temporal information to say given a

205
00:21:04,240 --> 00:21:12,880
place I know so let's say like a crop field if I want to measure how that field is doing over time

206
00:21:12,880 --> 00:21:17,680
I'm probably going to be primarily using spectral and temporal information and then obviously

207
00:21:17,680 --> 00:21:23,120
you'll pull all these all together in different in different ways so should I jump into a

208
00:21:24,080 --> 00:21:31,680
specific example or two sure please yeah so on the computer vision side where one of the places

209
00:21:31,680 --> 00:21:38,080
that we're kind of starting is with mapping what you can see on the ground so this is related to

210
00:21:38,080 --> 00:21:44,480
your earlier comment about the the cargo competition on planet data so that you can think of that as

211
00:21:46,320 --> 00:21:51,520
basically making masks of where's forest where's water where other types of things like that

212
00:21:53,600 --> 00:21:59,200
taking that to the next step would be like like the building example that I said so being able to

213
00:21:59,200 --> 00:22:05,520
define an algorithm that you can run over imagery and it will return result of that algorithm

214
00:22:05,520 --> 00:22:12,880
will be a probability map where each just says for each pixel are you in a building or not so

215
00:22:12,880 --> 00:22:17,920
this is a semantic segmentation problem and we can use pretty pretty standard semantic segmentation

216
00:22:17,920 --> 00:22:25,280
tools and then there are also if you don't need to segment out entire image you just want to say

217
00:22:25,280 --> 00:22:33,280
where are all of the x in the world so an example of something we've done is found electric

218
00:22:33,280 --> 00:22:40,160
substations right so saying we even in the United States we do not there are not comprehensive

219
00:22:40,160 --> 00:22:46,400
lists of all the electric substations across the United States but you can see them in high

220
00:22:46,400 --> 00:22:51,600
resolution satellite imagery so in this case we were working with the freely available

221
00:22:51,600 --> 00:22:59,280
nape that's the national agriculture imagery program and so we can use an object detection approach

222
00:22:59,280 --> 00:23:05,440
to find where all the substations are if you want I can go into more detail about what that looks like

223
00:23:05,440 --> 00:23:12,720
yeah please sure so for let's just keep running with the the electric substation example

224
00:23:13,920 --> 00:23:20,080
for that one there are so from a machine learning approach you have you need to define your data

225
00:23:20,080 --> 00:23:25,280
you define your inputs they need to find the model architecture train and then run that model

226
00:23:25,280 --> 00:23:31,280
right this is kind of the life cycle of these types of problems so for here the input data as I

227
00:23:31,280 --> 00:23:39,040
said is high resolution imagery from nape so this is either one meter or 60 centimeters on a side

228
00:23:39,040 --> 00:23:45,680
per pixel so it's enough to actually be able to see the kind of the structure of an electric

229
00:23:45,680 --> 00:23:51,040
substation where you see the the wires coming off and you see the the stands that are holding

230
00:23:51,040 --> 00:23:56,560
pulling all the wires and the transformers and things like that there are enough substations

231
00:23:56,560 --> 00:24:02,880
that are labeled in open street map and so for this problem that makes it makes the ground truth

232
00:24:02,880 --> 00:24:09,200
fairly fairly straightforward we can just grab the locations of substations in open street map and

233
00:24:09,200 --> 00:24:12,560
for training we don't need to know where all of them are we just need enough to be able to train

234
00:24:12,560 --> 00:24:21,920
a good model and then you know using the platform I can create images and then I can burn those polygons

235
00:24:21,920 --> 00:24:28,800
that say where the the known substations are into a similar target image so I now have an image

236
00:24:28,800 --> 00:24:36,400
and a target pair that I can use for a supervised supervised learning approach and then we picked

237
00:24:36,400 --> 00:24:42,720
ssd model it's a single shot multi box detector model as our object detection model of choice

238
00:24:43,520 --> 00:24:48,400
and one of the reasons for this is also kind of an interesting that we might talk a little bit more

239
00:24:48,400 --> 00:24:58,640
about is this is taking advantage of transfer learning ideas so with ssd the so in two sentences

240
00:24:58,640 --> 00:25:03,120
for listeners who aren't familiar with it the way that algorithm works in general is you take an

241
00:25:03,120 --> 00:25:10,720
image you split it up into a set of predefined boxes for each of those small boxes you use a

242
00:25:10,720 --> 00:25:17,120
compost from a neural net to create a set of features and then the final step is basically

243
00:25:17,120 --> 00:25:22,480
predicting whether or not each box contains the object you care about so in my case an electric

244
00:25:22,480 --> 00:25:30,720
substation and we don't necessarily have enough examples to train something all the way from scratch

245
00:25:30,720 --> 00:25:38,960
but we can take advantage of in this case ResNet to be able to generate those features and then we

246
00:25:38,960 --> 00:25:43,920
only have to train you to fine tune those those features and train a final classification stage

247
00:25:43,920 --> 00:25:50,160
so even with a relatively small number of examples we're able to train an effective algorithm to

248
00:25:50,160 --> 00:25:57,680
locate these electric substations and so when you say ResNet ResNet trained on what image net or

249
00:25:57,680 --> 00:26:05,440
yeah exactly yeah exactly so actually I'm sorry I misspoke this one is the VGG network

250
00:26:06,160 --> 00:26:13,040
we do use ResNet in in similar contexts but in other models yeah so this one is using VGG as

251
00:26:13,040 --> 00:26:19,840
the neural net to produce features and then we are just fine tuning the last I guess training the

252
00:26:19,840 --> 00:26:24,800
last classification step to say whether or not each box contains the object you care about

253
00:26:24,800 --> 00:26:33,600
with VGG and ResNet and the like typically these networks are trained particularly the ones that

254
00:26:33,600 --> 00:26:38,720
are pre-trained on image net and the like are typically looking at relatively small

255
00:26:39,600 --> 00:26:46,880
image sizes and like three channel images you know 240 pixel square or something along those lines

256
00:26:47,520 --> 00:26:53,360
or what approaches do you use to apply those to these you know super high resolution images that

257
00:26:53,360 --> 00:27:01,680
you have yeah so for for this particular model we used 512 by 512 images and week so one

258
00:27:01,680 --> 00:27:08,160
one advantage even though we're trying to use this over large regions the earth is still in

259
00:27:08,160 --> 00:27:12,880
in some sense an embarrassingly parallel problem right as long as your objects are small enough that

260
00:27:12,880 --> 00:27:21,120
they fit into a tile you can chop up a region into you know almost an arbitrary a number of large

261
00:27:21,120 --> 00:27:28,240
number of small tiles so it so it still works quite well we even even with the constraints that you

262
00:27:28,240 --> 00:27:34,960
said and then in this case we are just taking so you mentioned the three bands that's actually

263
00:27:34,960 --> 00:27:39,680
that is an important difference between a lot of standard computer vision problems where you're

264
00:27:39,680 --> 00:27:45,680
just using a you know RGB image versus what I've out what I've mentioned here with satellite imagery

265
00:27:45,680 --> 00:27:52,560
in this case we are still just taking the I think I think I'm using an NIR and R&G bands in this

266
00:27:52,560 --> 00:28:00,080
case but in other approach and other algorithms that we worked on we have used basically data

267
00:28:00,080 --> 00:28:05,680
compression so you know PCAD composition things like that to be able to compress a larger number

268
00:28:05,680 --> 00:28:11,760
of bands down into three bands that will then work with these pre-trained feature generators

269
00:28:11,760 --> 00:28:20,240
huh that's interesting so in that case you've got you know some larger number of bands you've

270
00:28:20,240 --> 00:28:25,520
got your you know I don't know if for the satellite images the color bands a visual bands or

271
00:28:25,520 --> 00:28:31,280
three or more but you've got some number of visual bands and then you've got like these infrared

272
00:28:31,280 --> 00:28:38,480
bands how many total do you start within some of those cases it depends on the satellite so for

273
00:28:38,480 --> 00:28:45,200
example the sentinel satellite has 12 bands that extend from visible to a near infrared all the

274
00:28:45,200 --> 00:28:51,120
up into sphere bands that are sensitive to thermal emission so it yeah it can vary a lot

275
00:28:51,840 --> 00:28:59,920
so you've got you've got 12 bands and you're able to use PCA or some some kind of alternate

276
00:28:59,920 --> 00:29:09,600
projection of these bands down to three bands and yet a model that's trained on kind of visual

277
00:29:10,480 --> 00:29:18,880
image net objects still works yes and no it's it's it's surprising I mean because I'm

278
00:29:18,880 --> 00:29:24,800
imagining that the these visual I guess in one sense like that you know the the result of this PCA

279
00:29:24,800 --> 00:29:33,120
thing I'm not imagining that to really mean anything visually right and so your model that's been

280
00:29:33,120 --> 00:29:37,840
trained on this highly visual data you know in one sense I wouldn't expect to work on the other

281
00:29:37,840 --> 00:29:43,040
sense like all it's doing is learning a bunch of different kinds of patterns that's what you're

282
00:29:43,040 --> 00:29:47,920
counting on is that it learns a bunch of different types of patterns that themselves aren't necessarily

283
00:29:47,920 --> 00:29:53,760
visual so maybe it should work yeah exactly and it's it's a lot of trial and error right it's a lot of like

284
00:29:53,760 --> 00:30:01,360
hey this I don't know this might work let's you know let's do a small test and just see what happens

285
00:30:01,360 --> 00:30:08,240
and in some cases this type of transfer of compression has worked well and then in other in

286
00:30:08,240 --> 00:30:13,760
other cases I hopefully we'll talk about a geobigial search in a little bit there was insufficient

287
00:30:13,760 --> 00:30:19,600
and we had to do some further fine tuning to be able to get effective features so it just you

288
00:30:19,600 --> 00:30:25,040
know it's it's it's a lot of a lot of trial and error and a lot of simply digging into the data and

289
00:30:25,760 --> 00:30:31,120
doing a few tests seeing what works seeing what doesn't and then moving on from there you mentioned

290
00:30:31,120 --> 00:30:38,720
that you to get your your ground truth you start with this open street map data that has identified

291
00:30:38,720 --> 00:30:45,040
some number of electrical substations how many approximately are there that are identified

292
00:30:45,040 --> 00:30:51,040
so in the the ground truth set that I ended up with I had about a few thousand okay so it kind of

293
00:30:51,040 --> 00:30:55,600
gives you a gives you an order of magnitude yeah so you've got a few thousand of these electrical

294
00:30:55,600 --> 00:31:05,520
substations and you said you burn them into the images meaning you like your yeah just simply saying

295
00:31:05,520 --> 00:31:14,080
that I the output from OSM is a polygon that has lat long coordinates for to trace out a boundary

296
00:31:14,080 --> 00:31:19,360
and I'm just simply turning that into an actual image of zeros and ones ones with inside that

297
00:31:19,360 --> 00:31:26,880
polygon and zeros outside that line up with the corresponding image from the satellite and is

298
00:31:26,880 --> 00:31:35,360
that type of training data arrangement while you know we're on you've you've essentially colored on

299
00:31:35,360 --> 00:31:42,560
a pixel by pixel basis is that kind of what SSD is looking for so then for SSD there is one final

300
00:31:42,560 --> 00:31:49,920
step of then drawing a bounding box around the the ground truth object and so I guess I we

301
00:31:49,920 --> 00:31:56,240
probably could skip the burning the raster step to make it slightly more efficient but then the

302
00:31:56,240 --> 00:32:02,240
actual training is done against the coordinates of the well I guess at that point they're actually

303
00:32:02,240 --> 00:32:07,520
in pixel coordinates so we do need to go through the image to to get the pixel coordinates of the

304
00:32:07,520 --> 00:32:14,640
bounding box for the object of interest did that make sense yeah I mean it sounds like you could

305
00:32:14,640 --> 00:32:24,400
potentially do some kind of math going from a polygon from the OSM data set to pixel coordinates but

306
00:32:24,400 --> 00:32:30,720
the way you did it is by just manipulating the pixels on the image themselves yeah that's right and

307
00:32:30,720 --> 00:32:35,200
ends you know with a lot of these things it's a question of where do you invest your time

308
00:32:35,920 --> 00:32:41,120
absolutely and yeah so something like that yeah you could probably make that better but there's

309
00:32:41,120 --> 00:32:46,960
a thousand other things that are more important to to put my time in at my desk into

310
00:32:46,960 --> 00:32:58,480
so you've trained this model and I'm curious like are you with you mentioned some places you

311
00:32:59,120 --> 00:33:07,520
use VGG other places you use ResNet are you you know is it kind of you you know for some projects you

312
00:33:08,400 --> 00:33:13,520
do have you built up in an intuition around where you use specific things that works or is it

313
00:33:13,520 --> 00:33:19,280
more like kind of looking at the the rings in a tree like the you know this project is older and

314
00:33:19,280 --> 00:33:23,600
so you use VGG which was state of the art at the time and now if you were to do it again you'd use

315
00:33:23,600 --> 00:33:30,480
ResNet yeah there's there's a lot of that right exactly kind of exactly what you said you yeah

316
00:33:31,120 --> 00:33:38,400
you build up models as you go and then as new tools come come online and depending whether or not

317
00:33:38,400 --> 00:33:44,160
it's worth going back to fix up a model we'll dictate whether or not you go back and try out new

318
00:33:44,160 --> 00:33:48,960
tools or new feature generation I hope we'll talk about GeoVigil search in a bit and that's an

319
00:33:48,960 --> 00:33:54,400
example of that where I we've done something that's worked pretty well but then are now the process

320
00:33:54,400 --> 00:33:59,440
of going back using new tools using new data trying to make it better okay well let's do that before

321
00:33:59,440 --> 00:34:06,400
we do that have we covered all of the the important bits on the the substation identification

322
00:34:06,400 --> 00:34:11,920
but maybe one last thing just because it really with this being the google next series and relating

323
00:34:11,920 --> 00:34:18,480
to the cloud an important part of these models is once you're done with the model you need to be able

324
00:34:18,480 --> 00:34:24,720
to run it efficiently over large regions and so it's you know not machine learning specific

325
00:34:24,720 --> 00:34:31,360
necessarily but it's definitely cloud related so as I mentioned a few minutes ago the earth is

326
00:34:32,160 --> 00:34:35,760
somewhat of an embarrassingly barrel problem where you can just chop it up into a lot of little

327
00:34:35,760 --> 00:34:42,800
pieces and this really lends itself very naturally to a cloud environment so once we have a model

328
00:34:42,800 --> 00:34:48,960
then say I want to run over the United States I can just grab all of the imagery that to cover

329
00:34:48,960 --> 00:34:55,280
the entire United States chop it up into a little 512 by 512 pixel pieces and then hand a set of

330
00:34:55,280 --> 00:35:03,120
those and the model to a worker in the cloud and then I can spawn effectively an arbitrary number

331
00:35:03,120 --> 00:35:08,880
of workers to go process all of these data through the model and then save those results directly to

332
00:35:08,880 --> 00:35:17,360
the cloud so just by by working in that highly parallel fashion that's how we're able to automate

333
00:35:17,360 --> 00:35:22,320
analyses and run them in a recently you know a reasonably quick amount of time so like I've been

334
00:35:22,320 --> 00:35:27,840
able to I mentioned the building detector I was able to run that over the entire state of California

335
00:35:28,480 --> 00:35:32,720
kind of overnight and so I come back and work in the morning and I have a map of all the buildings

336
00:35:32,720 --> 00:35:40,320
across the state so that's just the kind of the final piece of the model life cycle and I even with

337
00:35:40,320 --> 00:35:49,520
that final you know that specific piece and you know the the constraints you mentioned of taking

338
00:35:49,520 --> 00:35:54,960
this model handing it to something that lives in the cloud and having it run in a distributed fashion

339
00:35:54,960 --> 00:36:03,360
and even within the confines of Google's cloud offerings there are still probably I can think of

340
00:36:04,080 --> 00:36:10,000
five ways off the top of my head to do that specific thing like do you get involved in the

341
00:36:10,640 --> 00:36:17,360
infrastructure pieces of putting that all together or maybe more generally how have you

342
00:36:17,360 --> 00:36:24,400
architected this distributed execution or training is it like is it one of the higher level you know

343
00:36:24,400 --> 00:36:32,720
the machine learning engine type services or is it all running on just cloud engine workers

344
00:36:32,720 --> 00:36:41,040
that you guys build up and manage or is it running on Kubernetes yeah so we so the engineers and

345
00:36:41,040 --> 00:36:49,120
our team have built up a system in Kubernetes that are using primarily preemptible VMs and then

346
00:36:49,760 --> 00:36:55,680
being then they have we've written our own task scheduling software that will and then

347
00:36:55,680 --> 00:36:59,520
so we've written our own task and new software that will launch jobs into the into

348
00:36:59,520 --> 00:37:05,840
criminal VMs all of the work environment so capturing the code and then all you know all the

349
00:37:05,840 --> 00:37:11,120
different dependencies things like that using Docker and then inside of Kubernetes and so then

350
00:37:11,120 --> 00:37:18,240
you can set up a environment in each of those workers run the code save your results to the cloud

351
00:37:18,240 --> 00:37:25,440
and then break that all down so it's all it's all Kubernetes-based we're now working on getting

352
00:37:25,440 --> 00:37:31,920
Istio which is one of the new monitoring packages out of Google to be able to monitor all of these

353
00:37:31,920 --> 00:37:39,200
processes in an effective and effective way so and then we're we we're not using the

354
00:37:39,920 --> 00:37:45,200
ML engine yet but it's something that we're looking into and are you also looking at the

355
00:37:45,920 --> 00:37:52,000
Qflow package for Kubernetes yeah so that is that that's also something that we're looking into

356
00:37:52,000 --> 00:37:57,440
at this point so far we're running you're basically running our own models we're largely

357
00:37:57,440 --> 00:38:02,960
cares models but yeah Qflow is something that we're starting to look into now how long has the

358
00:38:02,960 --> 00:38:10,400
platform been around how long have you been working with Kubernetes so the company in total has

359
00:38:10,400 --> 00:38:18,080
been around for about three years so we're pretty pretty company and I'm not on the engineering team

360
00:38:18,080 --> 00:38:22,560
but we've been working with Kubernetes for for a while so I think most of that time

361
00:38:22,560 --> 00:38:30,080
cool yeah well we won't drill into necessarily the Kubernetes details here but I may be interested

362
00:38:30,080 --> 00:38:36,320
in talking to someone on the engineering side as well just for background we talk through the

363
00:38:37,040 --> 00:38:42,480
substation detection problem and you've been chomping at the bit to tell us about the geovisual

364
00:38:42,480 --> 00:38:48,160
search tell us about that problem sure yeah so this kind of chomping at the bit just because I think

365
00:38:48,160 --> 00:38:56,080
it's cool and interesting and a little bit unique so the idea with visual search is to be able to

366
00:38:56,640 --> 00:39:03,040
click on anywhere on the earth and then be able to see other areas that look like this so at a high

367
00:39:03,040 --> 00:39:10,240
level you can think of it as kind of a generalist model that hasn't been trained to find anything

368
00:39:10,240 --> 00:39:16,960
specific but is able to you know is able to search over a large region for lots of different

369
00:39:16,960 --> 00:39:24,160
types of things on so then the way it actually works under the hood is kind of cool so maybe I'll

370
00:39:24,160 --> 00:39:30,400
take a few minutes to walk through that now yeah please cool so the the way it works under the hood

371
00:39:30,400 --> 00:39:35,520
pretty actually fairly similar to many of the ideas that we've been talking about so far in this

372
00:39:35,520 --> 00:39:42,400
podcast take a set of data so in our first pass we've worked with either Nape that I've mentioned

373
00:39:42,400 --> 00:39:49,120
before over the United States or compositive Landsat over the entire globe chop that imagery up

374
00:39:49,120 --> 00:39:58,560
into a bunch of 128 by 128 images and then use a ResNet based feature generator to create a set of

375
00:39:58,560 --> 00:40:05,440
features and then simply search for similarity between features for each tile so so when you click

376
00:40:05,440 --> 00:40:11,680
on a tile it's doing a similarity search to say what other tiles have similar visual features

377
00:40:11,680 --> 00:40:17,440
so this this touches a number of the things that we've already mentioned on this podcast so far

378
00:40:17,440 --> 00:40:23,680
the first one being whether or not the ResNet features are sufficient so in this case they we found

379
00:40:23,680 --> 00:40:30,480
that the features that are generated by ResNet were not good enough to be able to have a really

380
00:40:30,480 --> 00:40:36,800
clean visual search and so we did actually do some fine fine tuning of those weights basically

381
00:40:36,800 --> 00:40:43,840
for Nape where it's high resolution enough to you'll see a lot of objects we took an approach of

382
00:40:43,840 --> 00:40:49,840
grabbing a bunch of different objects from osm again from the open street map and then doing a

383
00:40:50,800 --> 00:40:57,600
supervised fine tune training of the features to be able to match to osm objects that just kind of

384
00:40:57,600 --> 00:41:03,760
cleaned up what types of features that the network was generating and so specifically to

385
00:41:03,760 --> 00:41:12,000
drill in on that point for a second what you mean is that the features that the pre-trained

386
00:41:12,000 --> 00:41:21,440
presumably on ImageNet model was had as opposed to the architecture itself you weren't ever

387
00:41:21,440 --> 00:41:26,720
training from scratch this was all we're still talking about pre-trained ImageNet models exactly

388
00:41:26,720 --> 00:41:33,120
that's exactly right so yeah taking advantage of all of that that computation and pre-training

389
00:41:33,120 --> 00:41:40,480
that went into creating the ImageNet based ResNet weights and then just fine tuning those

390
00:41:40,480 --> 00:41:48,000
and I'm curious can you provide us some scope or order of magnitude of the fine tuning effort

391
00:41:48,000 --> 00:41:54,160
just to get a sense for you know how much you benefit from someone else doing the heavy lifting

392
00:41:54,160 --> 00:42:00,480
of training ImageNet I don't really have a good way of quantifying that at the moment yes

393
00:42:00,480 --> 00:42:11,840
are so you've you start with the pre-trained ResNet model on ImageNet you you fine tune and then

394
00:42:11,840 --> 00:42:19,440
when you talk about using the features of this ResNet model are you basically chopping off the

395
00:42:19,440 --> 00:42:26,480
classifier at the end and using that last layer or you're using intermediate layer activations or

396
00:42:26,480 --> 00:42:33,840
something like that yeah so we're we're basically just chopping off that last classification layer

397
00:42:35,040 --> 00:42:43,200
and then an additional step there is if we store all of those features the features numbers in

398
00:42:43,200 --> 00:42:48,720
their full size I would take up a future amount of disk space so we've actually binarized those

399
00:42:48,720 --> 00:42:55,760
features by injecting a bunch of noise at the end of the training process to basically force the

400
00:42:55,760 --> 00:43:02,000
those features to choose between either zero one and then in the you know in the final network we

401
00:43:02,000 --> 00:43:11,200
threshold it so the output of running a tile through the trained network is a list of 512 bits

402
00:43:11,200 --> 00:43:18,800
zero ones and that's so that's at the cost of the you'd be basically the precision of being

403
00:43:18,800 --> 00:43:25,120
able to differentiate between so we lose a little bit of differentiation power but we gain a lot

404
00:43:25,120 --> 00:43:31,040
in both speed for the search later and in terms of having it take up a reasonable amount of

405
00:43:31,040 --> 00:43:37,600
disk space can you elaborate on on that process the noise injection process and how that translates

406
00:43:37,600 --> 00:43:44,880
to allowing you to isolate these individual pixels yeah sure so at the so basically at the

407
00:43:44,880 --> 00:43:50,240
the last step we injected noise during training with an apple too that was comparable to the

408
00:43:50,240 --> 00:43:56,560
width of the layers activation function and so this means that the the network needs to either

409
00:43:56,560 --> 00:44:01,920
basically either decide you know the activation needs to either be one or zero otherwise there's

410
00:44:01,920 --> 00:44:06,800
enough noise to kind of destroy the information that the layer is trying to pass on so it forces

411
00:44:06,800 --> 00:44:13,120
the network to learn to output either very close to one or very close to zero and then once we have

412
00:44:13,120 --> 00:44:18,800
that then we can just once that's trained then we can in the final model we can just add a

413
00:44:18,800 --> 00:44:25,520
thresholding step does that make sense and you're injecting that noise at the last layer itself as

414
00:44:25,520 --> 00:44:32,240
opposed to in the input is by manipulating the image or that's right at the okay yeah it's just at

415
00:44:32,240 --> 00:44:38,000
that last step oh it's interesting um is there a name for that technique not that I'm aware of

416
00:44:38,000 --> 00:44:44,640
they're they're very well maybe okay I'm not aware of a specific name okay at that last layer of

417
00:44:44,640 --> 00:44:52,080
resonant how many features are there so we're we're ending up with 512 features per input image

418
00:44:52,880 --> 00:44:57,120
so you're what you're what you're trying to do is it is not you're not trying to necessarily

419
00:44:57,120 --> 00:45:04,080
reduce the number of features you're trying to basically compress them on off yeah that's right

420
00:45:04,080 --> 00:45:10,720
got it exactly okay and then just to complete because it's also interesting idea for the

421
00:45:10,720 --> 00:45:18,240
Landsat dataset this same fine tuning process but instead of using osm uh there aren't really

422
00:45:18,240 --> 00:45:23,360
uh enough useful information in osm around the across the entire globe so we used took an auto

423
00:45:23,360 --> 00:45:31,520
encoder approach basically um and use that as a way to to fine tune the um satellite imagery

424
00:45:31,520 --> 00:45:38,880
future generation for the Landsat dataset and what's the relationship between the fine tuning process

425
00:45:38,880 --> 00:45:45,760
and osm or the you know we'll get to the auto encoder but how does the osm data help you with

426
00:45:45,760 --> 00:45:51,920
the fine tuning it just makes the the features that you're generating more responsive to the

427
00:45:51,920 --> 00:45:56,080
the types of things you've seen satellite imagery so it kind of exactly what you mentioned

428
00:45:56,080 --> 00:46:02,000
earlier where image net is trained on you know a bunch of pictures that may or may not

429
00:46:02,000 --> 00:46:07,040
have the size and shapes of the types of things that you're interested in in satellite imagery

430
00:46:07,040 --> 00:46:13,040
this just allows us to fine tune those weights a little bit to uh make them more descriptive about

431
00:46:13,040 --> 00:46:20,160
the the types of visual features that we see you know in satellite imagery so does that mean

432
00:46:20,160 --> 00:46:26,640
I mean you've got satellite imagery in your original data um it is what you're doing

433
00:46:26,640 --> 00:46:31,760
are using osm to give you landmarks so that you can train on specific tiles that you know are

434
00:46:31,760 --> 00:46:36,800
interesting as opposed to training on a bunch of ocean or something like that or is it more

435
00:46:36,800 --> 00:46:42,960
nuanced than that it's partially that but it's more recognizing the fact that

436
00:46:42,960 --> 00:46:49,360
up overhead picture of you know a city park and a golf course that has doesn't look exactly the

437
00:46:49,360 --> 00:46:54,960
same as a picture of a cat or a dog and so the the features that a network learns to consider

438
00:46:54,960 --> 00:47:00,640
interesting from one of those images won't be exactly the same as the features that a network

439
00:47:00,640 --> 00:47:05,360
learns to consider interesting from the other one and so it's it's really more accounting for

440
00:47:05,360 --> 00:47:10,560
that type of difference knowing that we're going to be applying it to satellite imagery so trying

441
00:47:10,560 --> 00:47:17,040
to just fine tune a little bit uh what information the network is pulling out of those images

442
00:47:17,040 --> 00:47:24,400
okay uh yeah so you're you're doing this before you take off the classifier and you're training

443
00:47:25,360 --> 00:47:32,160
on osm tiles because you've got labels for them exactly got it I missed that part I was thinking

444
00:47:32,160 --> 00:47:38,320
we'd already chopped off the the classifier and so how do you use an auto encoder to do that

445
00:47:38,320 --> 00:47:44,800
you're basically doing it in more of an unsupervised kind of way yeah exactly that's right so because

446
00:47:44,800 --> 00:47:53,440
the Solanza imagery is a lot lower resolution 15 15 meters per pixel instead of one meter per pixel

447
00:47:53,440 --> 00:47:59,120
so the labeling from the osm simply wasn't as useful and then additionally there weren't as many

448
00:47:59,120 --> 00:48:06,000
labels around other go in other continents around the globe and so we still need to so we use an

449
00:48:06,000 --> 00:48:12,720
auto encoder to uh basically compress those images so compress the final image net features down

450
00:48:12,720 --> 00:48:19,920
into the same number of 512 feature bits so you so in that case the auto encoder is we're basically

451
00:48:19,920 --> 00:48:27,680
using it as an compression algorithm to get from the output image net features to the small number

452
00:48:27,680 --> 00:48:33,840
of binary features that we want to use for the visual search cool so yeah so then the result of

453
00:48:33,840 --> 00:48:41,520
that process is we've now tiled up the United States and the earth into small tiles and we've trained

454
00:48:41,520 --> 00:48:48,560
a algorithm to create visual features for each of those tiles and so now the the search part of

455
00:48:48,560 --> 00:48:56,800
the visual search comes in uh so the basic idea there is to you want to look for tiles that are

456
00:48:56,800 --> 00:49:03,680
close in feature space and so we took kind of a two-step approach to making this work first

457
00:49:03,680 --> 00:49:10,400
we used a hamming distance as kind of a first pass filter so this is just simply looking at the

458
00:49:10,400 --> 00:49:14,160
difference you're just lining up the bits and looking for differences between bits and that

459
00:49:14,160 --> 00:49:21,120
allows you to zoom into a smaller number of of candidates that may be visually close to the tile

460
00:49:21,120 --> 00:49:31,120
that you're searching for and then as a final step for the Landsat data the that first first step

461
00:49:31,120 --> 00:49:37,920
was enough to limited it down to where we could simply do a direct search of tag because it's just

462
00:49:37,920 --> 00:49:45,200
comparing bit by bit a group for search over all the the close images and then return return the

463
00:49:45,200 --> 00:49:52,320
image that's closest in feature space for the NAPE data set which starts with about two billion

464
00:49:52,320 --> 00:49:58,560
of these little tile images that was it still a director works but it's too slow for interactive

465
00:49:58,560 --> 00:50:03,840
use so we needed to do something a little more creative to be able to make this so you could click

466
00:50:03,840 --> 00:50:09,600
on something on a website and have have information come back to you in real time and so we used

467
00:50:09,600 --> 00:50:17,280
the approximation method called bit sampling which is basically takes a set of 32 hash functions

468
00:50:17,280 --> 00:50:26,000
and then it hashes sets of bits to reduce down that 512 feature bit vector into smaller chunks

469
00:50:26,000 --> 00:50:33,440
and then looks for images that have similar outputs of that hash function and then you can do a

470
00:50:33,440 --> 00:50:40,960
group for a search of that second filtered set of images did did that make sense it does I'm

471
00:50:41,760 --> 00:50:50,000
curious whether the notion of using some kind of projecting this feature space into some kind of

472
00:50:50,000 --> 00:50:56,240
embedding layer is that does that make sense in this context and using like the distance in

473
00:50:56,240 --> 00:51:03,600
an embedding layer to do image similarity that I think that approach could make sense yeah this

474
00:51:03,600 --> 00:51:09,760
this type of search is basically what we landed on first but an embedding approach could also work

475
00:51:09,760 --> 00:51:20,480
well okay interesting so yes so then the output of that is we now have a search capability where

476
00:51:20,480 --> 00:51:27,040
you can click on a little tile and get back results in other tiles that look visually similar

477
00:51:27,040 --> 00:51:32,400
basically in real time it's you know public available on our website at search.dakerlabs.com

478
00:51:32,400 --> 00:51:40,720
you can go play around with it if you want and so it forms a it's not a it's as I said in intro

479
00:51:40,720 --> 00:51:45,040
it's kind of a generalist model that's looking for things that look visually similar

480
00:51:45,040 --> 00:51:52,960
and then if so for some types of problems that's enough for other types of problems where you

481
00:51:52,960 --> 00:51:57,200
you want a higher precision recall in the objects that you're detecting or the objects aren't

482
00:51:57,200 --> 00:52:04,080
quite as visually striking then you need to take a more you know expensive more but more standard

483
00:52:04,080 --> 00:52:09,040
computer vision approach with like the electric substation as I mentioned before for example

484
00:52:09,040 --> 00:52:16,160
I'm curious with the the geovisual search have you developed a way to

485
00:52:17,120 --> 00:52:25,920
characterize the accuracy of the end to end process like you're you know the performance metrics

486
00:52:25,920 --> 00:52:30,880
that you would typically look at for a training process are you know don't really apply because

487
00:52:30,880 --> 00:52:37,040
you're kind of shifting the the use domain and I'm wondering if there's a way to measure

488
00:52:37,040 --> 00:52:44,320
the extent to which this feature similarity maps to what people think is similar or what people

489
00:52:44,320 --> 00:52:49,280
want to see like you know is it figuring out built you know tiles that have the same number of

490
00:52:49,280 --> 00:52:54,000
buildings or tiles that have some you know maybe there's something that's not really you know

491
00:52:54,000 --> 00:53:01,040
that doesn't jump out at us that causes feature similarity but it's not but it's not necessarily

492
00:53:01,040 --> 00:53:05,600
what an analyst wants to see when they're clicking around how do you characterize that performance

493
00:53:05,600 --> 00:53:12,960
so we haven't done anything quantitative in what you described you certainly could say I have

494
00:53:12,960 --> 00:53:18,400
a class I care about let's see whether or not a visual search you you could calculate a precision

495
00:53:18,400 --> 00:53:24,800
recall for if you had basically a labeled data set that you could check against and here it's

496
00:53:25,360 --> 00:53:31,520
you know we we haven't done that type of quantification of the process and because that'll also

497
00:53:31,520 --> 00:53:37,600
depend a lot on which object you choose you know for example so we're still we thought about

498
00:53:37,600 --> 00:53:43,840
this more as just kind of a general first pass search rather than a quantifiable like precision

499
00:53:43,840 --> 00:53:47,760
recall toward right right if that makes sense but yeah it's I mean it's definitely the type of

500
00:53:47,760 --> 00:53:53,920
thing that you you could do we just haven't performed that exercise with any specific examples

501
00:53:53,920 --> 00:54:02,000
and you mentioned that that's you know once a user cares about that type of precision you would lean

502
00:54:02,000 --> 00:54:08,160
more on the type of process that we described earlier where you're modeling around a specific

503
00:54:08,160 --> 00:54:14,480
type of entity or object and it sounds like you know presumably you what you have found is that

504
00:54:14,480 --> 00:54:19,440
you know when people click around in this map they tend to get results that are appealing to them

505
00:54:19,440 --> 00:54:26,800
that are that satisfy the base promise of the system yeah exactly so it's it's it's it's kind

506
00:54:26,800 --> 00:54:32,880
of an interesting I think of it as an interesting starting place or you can even say if if I'm

507
00:54:32,880 --> 00:54:39,760
thinking about investing time and therefore money into a more exact model do you will are there

508
00:54:39,760 --> 00:54:44,160
enough features that will look you know visually distinctive do I have a good chance of being able

509
00:54:44,160 --> 00:54:51,120
to build an effective model but then yeah for for a solution where you really want a quantified

510
00:54:51,120 --> 00:54:56,560
precision then taking one of the other approaches or object detection or semantic segmentation

511
00:54:57,280 --> 00:55:01,920
and building a specialist model for the exact thing you're looking for has almost always

512
00:55:01,920 --> 00:55:07,680
been more much more effective great well just as we kind of wind down I appreciate you being

513
00:55:07,680 --> 00:55:12,080
flexible with time here run over a little bit but this has been really interesting are there any

514
00:55:12,080 --> 00:55:17,280
other things that you'd want to leave folks with I just thanks for having me on it's been

515
00:55:17,280 --> 00:55:22,160
fun to discuss these things and if people want to play around with some of these things go check

516
00:55:22,160 --> 00:55:26,400
us out awesome well thank you Kyle thanks so much for taking the time to chat with me about this

517
00:55:26,400 --> 00:55:36,160
stuff all right thank you all right everyone that's our show for today for more information on Kyle

518
00:55:36,160 --> 00:55:43,680
or any of the topics covered in this episode head over to twimmol ai.com slash talk slash 173

519
00:55:43,680 --> 00:56:07,840
as always thanks so much for listening and catch you next time

