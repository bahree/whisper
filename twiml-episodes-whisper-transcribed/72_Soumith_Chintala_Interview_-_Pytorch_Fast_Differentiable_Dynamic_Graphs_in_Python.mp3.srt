1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,120
I'm your host Sam Charrington.

4
00:00:23,120 --> 00:00:27,960
A big thanks to everyone who participated in last week's Twimble Online Meetup, and

5
00:00:27,960 --> 00:00:30,760
to Kevin T from SIGUP for presenting.

6
00:00:30,760 --> 00:00:34,840
You can find the slides for his presentation in the Meetup Slack Channel, as well as

7
00:00:34,840 --> 00:00:36,880
in this week's show notes.

8
00:00:36,880 --> 00:00:41,280
Our final Meetup of the Year will be held on Wednesday, December 13th.

9
00:00:41,280 --> 00:00:46,480
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

10
00:00:46,480 --> 00:00:48,640
for our discussion segment.

11
00:00:48,640 --> 00:00:54,280
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

12
00:00:54,280 --> 00:01:00,760
the paper understanding deep learning requires rethinking generalization, by Shi Huang Zhang

13
00:01:00,760 --> 00:01:04,120
from MIT and Google Brain and others.

14
00:01:04,120 --> 00:01:09,240
You can find more details and register at twimbleia.com slash Meetup.

15
00:01:09,240 --> 00:01:13,560
If you receive my newsletter, you already know this, but Twimble is growing and we're

16
00:01:13,560 --> 00:01:19,280
looking for an energetic and passionate community manager to help expand our programs.

17
00:01:19,280 --> 00:01:23,680
This position can be remote, but if you happen to be in St. Louis, all the better.

18
00:01:23,680 --> 00:01:27,400
If you're interested, please reach out to me for additional details.

19
00:01:27,400 --> 00:01:31,360
I should mention that if you don't already get my newsletter, you are really missing

20
00:01:31,360 --> 00:01:36,600
out and should visit twimbleia.com slash newsletter to sign up.

21
00:01:36,600 --> 00:01:42,320
Now the show you are about to hear is part of our Strange Loop 2017 series, brought to

22
00:01:42,320 --> 00:01:45,160
you by our friends at Nexusos.

23
00:01:45,160 --> 00:01:50,080
Nexusos is a company focused on making machine learning more easily accessible to enterprise

24
00:01:50,080 --> 00:01:51,080
developers.

25
00:01:51,080 --> 00:01:55,600
Their machine learning API meets developers where they're at, regardless of their mastery

26
00:01:55,600 --> 00:02:00,840
of data science, so they can start cutting up predictive applications immediately and

27
00:02:00,840 --> 00:02:03,680
in their preferred programming language.

28
00:02:03,680 --> 00:02:08,080
It's as simple as loading your data and selecting the type of problem you want to solve.

29
00:02:08,080 --> 00:02:12,920
Their automated platform trains and selects the best model fit for your data and then outputs

30
00:02:12,920 --> 00:02:14,520
predictions.

31
00:02:14,520 --> 00:02:18,840
To learn more about Nexusos, be sure to check out the first episode in this series at

32
00:02:18,840 --> 00:02:27,080
twimbleia.com slash talk slash 69 where I speak with co-founders Ryan Sevy and Jason Montgomery.

33
00:02:27,080 --> 00:02:32,520
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

34
00:02:32,520 --> 00:02:37,680
learning in your next project at nexosos.com slash twimble.

35
00:02:37,680 --> 00:02:43,000
In this show, I speak with Sumit Shintala, a research engineer in the Facebook AI Research

36
00:02:43,000 --> 00:02:44,320
Lab.

37
00:02:44,320 --> 00:02:48,800
Sumith joined me at Strange Loop before his talk on PyTorch, the deep learning framework.

38
00:02:48,800 --> 00:02:53,360
In this conversation, we discussed the market evolution of deep learning frameworks, different

39
00:02:53,360 --> 00:02:58,240
approaches to programming deep learning frameworks, Facebook's motivations for investing in

40
00:02:58,240 --> 00:03:00,200
PyTorch and much more.

41
00:03:00,200 --> 00:03:03,320
This was a fun interview and I hope you enjoy it.

42
00:03:03,320 --> 00:03:06,320
And now on to the show.

43
00:03:06,320 --> 00:03:18,280
Hey everyone, I am here at the Strange Loop Conference in St. Louis and I'm with Sumith

44
00:03:18,280 --> 00:03:24,640
Shintala, who is a research engineer at Fair, the Facebook AI Research Lab.

45
00:03:24,640 --> 00:03:30,280
And Sumith is giving a talk here at the conference tomorrow and graciously agreed to spend some

46
00:03:30,280 --> 00:03:34,760
time with us to talk a little bit about what he's up to and about its talk.

47
00:03:34,760 --> 00:03:35,760
So welcome, Sumith.

48
00:03:35,760 --> 00:03:39,760
Hi Sam, nice to be here at Strange Loop for the first time.

49
00:03:39,760 --> 00:03:42,760
Absolutely, absolutely, and welcome to St. Louis.

50
00:03:42,760 --> 00:03:46,280
So why don't we get started by having you tell us a little bit about your background

51
00:03:46,280 --> 00:03:48,760
and how you got into AI research?

52
00:03:48,760 --> 00:03:49,760
Sure.

53
00:03:49,760 --> 00:03:50,760
Let's see.

54
00:03:50,760 --> 00:03:58,800
About eight years ago, I wanted to be a digital artist trying to make VFX for movies and

55
00:03:58,800 --> 00:03:59,800
stuff.

56
00:03:59,800 --> 00:04:06,960
I didn't internship there and as soon as I went into that internship a weekend, I realized

57
00:04:06,960 --> 00:04:12,760
that I was terrible at this, I was not an artist by any standards.

58
00:04:12,760 --> 00:04:18,040
And then I had to find second choices in life and I was looking at my interests and one

59
00:04:18,040 --> 00:04:23,560
of the things that struck to me was I was a decent programmer since I was a young age.

60
00:04:23,560 --> 00:04:29,880
And I kind of liked the whole computer vision, angle, object detection, you know, these

61
00:04:29,880 --> 00:04:36,200
show picture and some machine shows tells you, oh, there's a person here, there's a cat

62
00:04:36,200 --> 00:04:37,960
here, that just fascinated me.

63
00:04:37,960 --> 00:04:44,840
So I went down that line, I did a small internship at a research lab in India at this place

64
00:04:44,840 --> 00:04:52,400
called Triple IT and like there I did a little bit of random stuff, you know, as an undergrad

65
00:04:52,400 --> 00:04:58,080
you explore all kinds of things and then I tried a little bit of face detection and a

66
00:04:58,080 --> 00:05:03,520
little bit of taking a bunch of pictures of a monument and then stitching them together

67
00:05:03,520 --> 00:05:04,520
in 3D.

68
00:05:04,520 --> 00:05:05,520
Okay.

69
00:05:05,520 --> 00:05:07,240
And I had no idea what I was doing.

70
00:05:07,240 --> 00:05:12,880
I was just like trying to put together things based on various tools and snippets and

71
00:05:12,880 --> 00:05:18,680
you know how you call programmers stack over flow bots, you just take snippets and you're

72
00:05:18,680 --> 00:05:26,040
trying to put together something and I was guilty as charged and then I got an opportunity

73
00:05:26,040 --> 00:05:33,120
to come to CMU and Pittsburgh just doing more exploration, trying to figure out what

74
00:05:33,120 --> 00:05:34,720
I want to do.

75
00:05:34,720 --> 00:05:38,160
Here I got to play robot soccer.

76
00:05:38,160 --> 00:05:45,040
So program robots to play soccer autonomously, autonomously, so you basically like flashed

77
00:05:45,040 --> 00:05:50,040
a robot with your program and then they just play soccer.

78
00:05:50,040 --> 00:05:55,160
And I've seen pictures of a variety of scenarios of these, the humanoid ones.

79
00:05:55,160 --> 00:05:58,840
Yes, the ones I worked on were these humanoid ones.

80
00:05:58,840 --> 00:05:59,840
Okay.

81
00:05:59,840 --> 00:06:07,680
They were called Naoki robots and they were cute, and they were, I mean, I came in with

82
00:06:07,680 --> 00:06:13,040
the expectation that we figured out so much in robotics, we must be pretty good.

83
00:06:13,040 --> 00:06:18,680
And we couldn't make them stand properly, like they would just stand, they would walk

84
00:06:18,680 --> 00:06:21,960
and they would fall down and that's the state of the art.

85
00:06:21,960 --> 00:06:23,960
If you could make them walk without falling down.

86
00:06:23,960 --> 00:06:28,480
We've probably all seen the video, the Boston robotics robots that like tries to turn

87
00:06:28,480 --> 00:06:32,000
the door, the door not pan, and then just fall over totally.

88
00:06:32,000 --> 00:06:33,760
I saw that one.

89
00:06:33,760 --> 00:06:40,800
So that like shocked me because I was like, oh, robotics was so much more advanced than

90
00:06:40,800 --> 00:06:42,200
it is.

91
00:06:42,200 --> 00:06:48,840
And it also, I saw opportunity there, I'm like, oh, this film is still kind of open.

92
00:06:48,840 --> 00:06:53,960
And we were trying to do the whole algorithm of the robot playing soccer with vision.

93
00:06:53,960 --> 00:06:58,440
Like, oh, can you identify where the ball is and walk towards it and stuff.

94
00:06:58,440 --> 00:07:03,760
And that part as well was very, very rudimentary, not working very well.

95
00:07:03,760 --> 00:07:12,280
It would sort of look for an orange pixel in your image and try to make that pixel bigger.

96
00:07:12,280 --> 00:07:16,600
And even something as stupid as that, it wasn't working very well.

97
00:07:16,600 --> 00:07:17,760
That's pretty funny.

98
00:07:17,760 --> 00:07:26,360
So I did a bunch of random stuff there and I had a good time at CME and I decided I want

99
00:07:26,360 --> 00:07:32,480
to go into artificial intelligence, computer vision, robotics.

100
00:07:32,480 --> 00:07:37,600
And then I applied to a bunch of places, CME being one of them and I didn't get accepted

101
00:07:37,600 --> 00:07:38,600
anywhere.

102
00:07:38,600 --> 00:07:45,000
And I was looking for late applications and stuff and that also combined with someone

103
00:07:45,000 --> 00:07:48,440
who there, like where there was computer vision.

104
00:07:48,440 --> 00:07:55,360
And I saw this web page by this guy called Jan LeCouven and he had a like, janky web

105
00:07:55,360 --> 00:08:03,320
page with some object detection stuff going on, but I was like, hey, it's NYU, I'll give

106
00:08:03,320 --> 00:08:04,320
it a shot.

107
00:08:04,320 --> 00:08:09,600
So I applied to NYU and a couple of other places last minute because I got rejected from

108
00:08:09,600 --> 00:08:12,800
all of my top schools I wanted to go to.

109
00:08:12,800 --> 00:08:15,000
No offense, Jan.

110
00:08:15,000 --> 00:08:18,160
I told them the story right away.

111
00:08:18,160 --> 00:08:24,000
And I got accepted at NYU, I wasn't super happy with myself because I was like, oh,

112
00:08:24,000 --> 00:08:28,080
I can do so much better if I worked harder.

113
00:08:28,080 --> 00:08:30,080
And then it gets even more hilarious.

114
00:08:30,080 --> 00:08:35,080
I come in to NYU, I emailed Jan before and I'm like, hey, I did a little bit of work

115
00:08:35,080 --> 00:08:37,760
here and they're at CMU and Triple ID.

116
00:08:37,760 --> 00:08:43,440
I want to work with you, try to do more object detection research and he replied immediately.

117
00:08:43,440 --> 00:08:47,640
He was like, hey, let's meet, once you get here, let's meet this day at this time.

118
00:08:47,640 --> 00:08:53,960
I go, Jan's in his office and it's like, hey, listen, do you know anything about

119
00:08:53,960 --> 00:08:54,960
neural networks?

120
00:08:54,960 --> 00:08:55,960
You know what?

121
00:08:55,960 --> 00:08:56,960
I do my kind of research.

122
00:08:56,960 --> 00:09:04,000
I'm like, nope, I only heard the term neural networks at once and I have no idea what

123
00:09:04,000 --> 00:09:05,000
you're doing.

124
00:09:05,000 --> 00:09:09,720
And so he went on to explain to me how neural networks work.

125
00:09:09,720 --> 00:09:14,960
This was in 2010, neural networks weren't hot and Jan Likun had a lot of time on his

126
00:09:14,960 --> 00:09:15,960
head.

127
00:09:15,960 --> 00:09:21,080
So that relationship went well, he introduced me to one of his PhD students, Pierre

128
00:09:21,080 --> 00:09:27,640
Cervine, who was now at Google, and Pierre and I, I was Pierre's like understudy.

129
00:09:27,640 --> 00:09:33,480
I worked on many things there, like I implemented neural networks, we built like Pierre built

130
00:09:33,480 --> 00:09:39,280
this deep learning framework called eBLearn and I was kind of helping out on that and that

131
00:09:39,280 --> 00:09:44,720
made me understand more about how neural networks work also got me stronger on the engineering

132
00:09:44,720 --> 00:09:46,200
side of things.

133
00:09:46,200 --> 00:09:53,000
That's roughly how I entered the field in my two years at NYU, we published one paper

134
00:09:53,000 --> 00:09:58,920
and another paper on the work we did got published in another conference.

135
00:09:58,920 --> 00:10:03,800
And then in 2012 May, I graduated, I couldn't find a job in deep learning.

136
00:10:03,800 --> 00:10:04,800
Wow.

137
00:10:04,800 --> 00:10:11,080
2012 December was when the whole deep learning boom started.

138
00:10:11,080 --> 00:10:16,920
So 2012 May, I was going to go accept a job at Amazon as a test engineer.

139
00:10:16,920 --> 00:10:17,920
Wow.

140
00:10:17,920 --> 00:10:19,920
You're like, why did I waste the last two years of my life?

141
00:10:19,920 --> 00:10:21,760
It was just so frustrating.

142
00:10:21,760 --> 00:10:25,360
I was trying not to give up, I was still super interested in the field, but you know,

143
00:10:25,360 --> 00:10:27,360
you have practical constraints, right?

144
00:10:27,360 --> 00:10:32,320
Like you're working at that, you need to think of all these things.

145
00:10:32,320 --> 00:10:39,240
So in the last minute, I think I had to accept the Amazon offer by Saturday and on Wednesday

146
00:10:39,240 --> 00:10:46,800
Jan, like, and Jan, at that day, I don't even remember why I met Jan the day, he was like,

147
00:10:46,800 --> 00:10:47,800
oh, where are you going?

148
00:10:47,800 --> 00:10:53,160
And I told him I couldn't find a job anywhere else and I go to Amazon and he's like, oh,

149
00:10:53,160 --> 00:10:57,960
just yesterday, one of the companies I co-founded got fresh funding and they're looking to

150
00:10:57,960 --> 00:10:58,960
hire engineers.

151
00:10:58,960 --> 00:10:59,960
Oh, wow.

152
00:10:59,960 --> 00:11:02,880
So that was a conversation that happened on Wednesday.

153
00:11:02,880 --> 00:11:07,120
I went and gave my interviews on Thursday in Princeton, New Jersey.

154
00:11:07,120 --> 00:11:14,400
And on Friday, I signed for Musami and they were doing music and deep learning on phones.

155
00:11:14,400 --> 00:11:15,400
Okay.

156
00:11:15,400 --> 00:11:18,520
So that was a company that was like, like a shazam kind of thing?

157
00:11:18,520 --> 00:11:24,640
No, it was basically you, you want to, if someone's playing music, you should be able

158
00:11:24,640 --> 00:11:31,120
to transcribe it live onto sheet and if someone takes a picture of sheet, then you have

159
00:11:31,120 --> 00:11:33,400
to be able to play it back for them.

160
00:11:33,400 --> 00:11:39,520
So it was like a full cycle, I want to hear, I want to play, so it was like a tool for

161
00:11:39,520 --> 00:11:40,520
musicians.

162
00:11:40,520 --> 00:11:41,520
Okay.

163
00:11:41,520 --> 00:11:42,920
Does that exist for guitar tab?

164
00:11:42,920 --> 00:11:43,920
Do you have any idea?

165
00:11:43,920 --> 00:11:44,920
There it should.

166
00:11:44,920 --> 00:11:45,920
And it would be awesome.

167
00:11:45,920 --> 00:11:54,520
They do, but it's not a solved problem to decouple, when you play multiple notes together.

168
00:11:54,520 --> 00:12:00,040
If you play a single note at a time, it's very easy, but if you play like five or six

169
00:12:00,040 --> 00:12:06,920
chords at a time, like decoupling and understanding which of those exactly maps to what you played,

170
00:12:06,920 --> 00:12:07,920
okay.

171
00:12:07,920 --> 00:12:09,640
It's still like an ongoing problem.

172
00:12:09,640 --> 00:12:13,560
So I spent time at Musami for a couple of years.

173
00:12:13,560 --> 00:12:16,680
We were building all kinds of mobile stuff.

174
00:12:16,680 --> 00:12:21,200
I mean, we wanted the whole thing to run on phones, so I was training new on networks

175
00:12:21,200 --> 00:12:27,600
on sheet music and it would be called music optical recognition or more.

176
00:12:27,600 --> 00:12:32,240
And then the company kind of had to fold at some point.

177
00:12:32,240 --> 00:12:37,400
In the meanwhile, while it was at Musami, I always started actively maintaining torch,

178
00:12:37,400 --> 00:12:43,320
which was the deep learning framework that was one of the bigger ones at that time.

179
00:12:43,320 --> 00:12:49,920
And I eventually wanted to get out of Musami because we're not sure how the business

180
00:12:49,920 --> 00:12:52,200
set of things was going.

181
00:12:52,200 --> 00:12:58,800
And then Jan started at Facebook six months before that and they were using torch as their

182
00:12:58,800 --> 00:13:00,040
main deep learning framework.

183
00:13:00,040 --> 00:13:05,840
And so they needed good engineers to maintain and develop torch.

184
00:13:05,840 --> 00:13:11,720
And so by the time I was joining Facebook, I was the only maintainer of torch.

185
00:13:11,720 --> 00:13:12,720
So it was perfect.

186
00:13:12,720 --> 00:13:18,040
They just got the engineer who will help like had the side of things.

187
00:13:18,040 --> 00:13:24,640
So I came to Facebook and there were so many smart people that I just learned so much from.

188
00:13:24,640 --> 00:13:27,440
I was also interested in research.

189
00:13:27,440 --> 00:13:33,600
And so I ended up going down this path of generative adversarial networks where we were trying

190
00:13:33,600 --> 00:13:36,680
to synthesize images.

191
00:13:36,680 --> 00:13:42,280
So the neural network kind of just synthesizes images from nothing or like a training or

192
00:13:42,280 --> 00:13:43,600
for what use cases.

193
00:13:43,600 --> 00:13:46,600
This was more of an unsupervised learning use case.

194
00:13:46,600 --> 00:13:51,680
So an unsupervised learning, one of the things you do is generation.

195
00:13:51,680 --> 00:14:00,440
And the motivation there is that if you can generate something, you have generally good concepts

196
00:14:00,440 --> 00:14:03,280
about how that process works.

197
00:14:03,280 --> 00:14:09,480
So the motivation is that if we can do a really good image generation neural network, we

198
00:14:09,480 --> 00:14:14,200
can take parts of that neural network and bootstrap other neural networks which are

199
00:14:14,200 --> 00:14:17,520
doing computer vision tasks to get better performance.

200
00:14:17,520 --> 00:14:22,600
So we could take parts of this neural network and then make it work on a different task

201
00:14:22,600 --> 00:14:26,120
like dog versus clats classification.

202
00:14:26,120 --> 00:14:30,440
And without having as much data, you would still get where you could accuracy.

203
00:14:30,440 --> 00:14:33,840
So that's the whole unsupervised semi-supervised learning motivation.

204
00:14:33,840 --> 00:14:39,320
I worked on a few things on the adversarial network side and then coming to...

205
00:14:39,320 --> 00:14:41,880
And this was back in 2012, 2013?

206
00:14:41,880 --> 00:14:43,880
No, 2014 I joined Facebook.

207
00:14:43,880 --> 00:14:44,880
Oh, got it.

208
00:14:44,880 --> 00:14:46,880
2014, 2015, 2016.

209
00:14:46,880 --> 00:14:53,040
Because the GANs in general have been more, you know, past two, three years, I think,

210
00:14:53,040 --> 00:14:54,880
in the year 2015, 2016.

211
00:14:54,880 --> 00:14:55,880
Yeah.

212
00:14:55,880 --> 00:15:03,240
Coming to what I'm going to talk about tomorrow, what happened was torch has been an aging

213
00:15:03,240 --> 00:15:04,800
design in general.

214
00:15:04,800 --> 00:15:09,240
It's been seven years since the previous release of torch came out.

215
00:15:09,240 --> 00:15:14,360
So it was becoming more flexible, you know, as the field changes, there's this concept

216
00:15:14,360 --> 00:15:19,200
for researchers who use the tools that they have available to them best.

217
00:15:19,200 --> 00:15:21,280
And they push those tools to the limit.

218
00:15:21,280 --> 00:15:27,360
And the new tools come that will then, again, make the researchers more flexible and exploring

219
00:15:27,360 --> 00:15:28,680
new things.

220
00:15:28,680 --> 00:15:32,320
So torch was reaching its limits of flexibility.

221
00:15:32,320 --> 00:15:35,920
So we wanted to develop a new tool.

222
00:15:35,920 --> 00:15:41,920
And so we worked on it for four years, started off as an intern project and then we kept

223
00:15:41,920 --> 00:15:42,920
developing it.

224
00:15:42,920 --> 00:15:45,240
And we released it earlier this year.

225
00:15:45,240 --> 00:15:47,320
It's called PyTorch.

226
00:15:47,320 --> 00:15:52,520
And it's what I'm going to be talking about tomorrow, it's strange, look, I'll be talking

227
00:15:52,520 --> 00:15:58,600
about PyTorch, how it came about, what engineering challenges we faced.

228
00:15:58,600 --> 00:16:05,240
PyTorch generally is a fairly slow language, but it's the most popular language for machine

229
00:16:05,240 --> 00:16:10,120
learning, for, you know, statistics, like all kinds of things.

230
00:16:10,120 --> 00:16:15,880
So the most obvious choice to build something in was PyTorch, because all of the users

231
00:16:15,880 --> 00:16:16,880
were familiar with it.

232
00:16:16,880 --> 00:16:17,880
We have a huge ecosystem.

233
00:16:17,880 --> 00:16:21,520
And the barrier to entry is smaller than the C++.

234
00:16:21,520 --> 00:16:26,480
Yeah, you have so many tutorials and it's very easy to learn and stuff.

235
00:16:26,480 --> 00:16:27,480
Yeah.

236
00:16:27,480 --> 00:16:33,680
So you had that upside, but the downside was deep learning is one of those high performance

237
00:16:33,680 --> 00:16:34,680
computing spaces.

238
00:16:34,680 --> 00:16:35,680
Right.

239
00:16:35,680 --> 00:16:39,080
Every, every second, every millisecond matters.

240
00:16:39,080 --> 00:16:46,240
But PyTorch is slow, like how do you make some packages really fast, but taking a constraint

241
00:16:46,240 --> 00:16:49,080
that the users want to use it from PyTorch.

242
00:16:49,080 --> 00:16:55,600
So generally, like how we worked around these challenges in various ways, I'm just going

243
00:16:55,600 --> 00:16:57,080
to talk about that.

244
00:16:57,080 --> 00:17:01,640
Some of the things we did was we moved the most critical parts into C. We made a large

245
00:17:01,640 --> 00:17:05,320
part of the implementation lock free.

246
00:17:05,320 --> 00:17:09,320
Well, let's make sure not to kind of breeze by these topics, because we really want to

247
00:17:09,320 --> 00:17:11,240
dive into some of these.

248
00:17:11,240 --> 00:17:12,240
Sure.

249
00:17:12,240 --> 00:17:16,800
But, you know, one of the things that is maybe an interesting place to start is, and I've

250
00:17:16,800 --> 00:17:22,000
talked about this, I think possibly on the podcast, definitely in my newsletter, just

251
00:17:22,000 --> 00:17:27,920
the idea that it's actually kind of interesting here in your story and how, you know, in a

252
00:17:27,920 --> 00:17:31,880
lot of ways, it's like all about timing and mistiming and timing windows and things like

253
00:17:31,880 --> 00:17:32,880
that.

254
00:17:32,880 --> 00:17:39,320
I think PyTorch has kind of popped up on the scene, if you will, at a time when I think

255
00:17:39,320 --> 00:17:44,720
a lot of people in Crown TensorFlow was like the heir apparent to the deep learning framework

256
00:17:44,720 --> 00:17:45,720
world.

257
00:17:45,720 --> 00:17:46,720
Right.

258
00:17:46,720 --> 00:17:52,680
And, you know, I wonder if you're just hearing your story, if like your experiences with,

259
00:17:52,680 --> 00:17:58,880
you know, the timing cycle of machine learning and deep learning, like if that influences

260
00:17:58,880 --> 00:18:04,920
your perspective on this kind of the market evolution of, you know, tools and, you know,

261
00:18:04,920 --> 00:18:10,400
where do you see what you see the opportunity is for PyTorch and just kind of where you

262
00:18:10,400 --> 00:18:11,760
think things are going.

263
00:18:11,760 --> 00:18:12,760
Right.

264
00:18:12,760 --> 00:18:17,560
So TensorFlow popped up was a December 2015, I think.

265
00:18:17,560 --> 00:18:24,720
It took the whole deep learning world by a bang and Google put so much effort evangelizing

266
00:18:24,720 --> 00:18:25,720
TensorFlow.

267
00:18:25,720 --> 00:18:33,080
I mean, from Sundar Pichai to like pretty much everyone was like, hey, this is TensorFlow,

268
00:18:33,080 --> 00:18:36,880
this is what Google is going to be about for the next few years, right.

269
00:18:36,880 --> 00:18:43,800
So that, and, you know, they're a huge team and they've been putting great effort into

270
00:18:43,800 --> 00:18:48,800
generally making sure everyone are covered by TensorFlow, whether it's a data scientist

271
00:18:48,800 --> 00:18:52,800
or deep learning researcher or like a production engineer.

272
00:18:52,800 --> 00:18:55,880
And like what Google did was amazing.

273
00:18:55,880 --> 00:19:03,800
They raised the bar for engineering of a deep learning framework so far high.

274
00:19:03,800 --> 00:19:08,800
Until then, like if you think about it, Torch, Tiano Cafe, these were the dominant deep

275
00:19:08,800 --> 00:19:10,720
learning frameworks before TensorFlow.

276
00:19:10,720 --> 00:19:18,200
They were all like, they all started as one man, grad student projects.

277
00:19:18,200 --> 00:19:22,040
And you could totally see that in them.

278
00:19:22,040 --> 00:19:24,640
The quality control was really bad.

279
00:19:24,640 --> 00:19:27,640
They were not planned properly.

280
00:19:27,640 --> 00:19:35,280
If you want to install any of those back in 2014, you would spend a day, maybe more.

281
00:19:35,280 --> 00:19:38,160
You'd install that dependency, this dependency.

282
00:19:38,160 --> 00:19:45,000
The TensorFlow, it made the engineering bar very high, they were like, we're not screwing

283
00:19:45,000 --> 00:19:46,000
around here.

284
00:19:46,000 --> 00:19:47,000
Right.

285
00:19:47,000 --> 00:19:50,320
We want to make the best product out there for people.

286
00:19:50,320 --> 00:19:54,520
And they went with a Tiano style programming model.

287
00:19:54,520 --> 00:19:55,520
Yeah.

288
00:19:55,520 --> 00:20:01,320
So a Tiano style programming model is very, very low level, which means if you want to

289
00:20:01,320 --> 00:20:09,240
write something like a convolution neural network, then you'd spend writing so much

290
00:20:09,240 --> 00:20:11,200
boilerplate code.

291
00:20:11,200 --> 00:20:15,680
And also like the TensorFlow style model, it's called symbolic, which means that like you

292
00:20:15,680 --> 00:20:18,960
create a graph and then you run it later.

293
00:20:18,960 --> 00:20:26,040
The problem with that is if you want to debug anything, you'd have to use the tooling

294
00:20:26,040 --> 00:20:27,440
given by TensorFlow.

295
00:20:27,440 --> 00:20:31,280
Like you can, for example, debug your code by itself.

296
00:20:31,280 --> 00:20:36,120
You have to run your model in this other virtual machine.

297
00:20:36,120 --> 00:20:40,920
And then you can set breakpoints in that virtual machine using TensorFlow's own tools.

298
00:20:40,920 --> 00:20:47,320
Meaning because it's not standard language x Python, in the case of PyTorch, it's being

299
00:20:47,320 --> 00:20:53,560
interpreted by some virtual machine that knows how to read this graph and schedule execution

300
00:20:53,560 --> 00:20:54,560
against this graph.

301
00:20:54,560 --> 00:21:01,640
So the only way to develop debugging and other tooling for it is via that virtual machine.

302
00:21:01,640 --> 00:21:02,640
That is correct.

303
00:21:02,640 --> 00:21:03,640
Okay.

304
00:21:03,640 --> 00:21:08,360
So the upsides to it are that this virtual machine can be as big or small as possible.

305
00:21:08,360 --> 00:21:11,320
You can ship it into phones, you can ship it into anything.

306
00:21:11,320 --> 00:21:16,600
The downside is that now where you define your network was in your Python VM.

307
00:21:16,600 --> 00:21:19,440
And you're running your network in a different VM, there's disconnect.

308
00:21:19,440 --> 00:21:25,640
So as a developer, you always have to keep thinking about how the behavior is something

309
00:21:25,640 --> 00:21:31,240
in the TensorFlow virtual machine maps back to what you wrote in the Python code.

310
00:21:31,240 --> 00:21:35,880
So anyone kind of on the enterprise side that has some Java experience, you know, knows

311
00:21:35,880 --> 00:21:40,840
how exactly how different the right ones run anywhere is from the practice of needing to

312
00:21:40,840 --> 00:21:45,720
know the internals of your heap size and garbage collection strategies and stuff like that.

313
00:21:45,720 --> 00:21:46,720
Exactly.

314
00:21:46,720 --> 00:21:53,360
And also give you, or at least that the ecosystem, if not an individual developer, the flexibility

315
00:21:53,360 --> 00:21:58,000
to decouple the VM from the your code base.

316
00:21:58,000 --> 00:22:03,320
So meaning, you know, you could write your code using TensorFlow and Python and but have

317
00:22:03,320 --> 00:22:08,520
the VM written in, you know, go or whatever the fast concurrent, highly concurrent language

318
00:22:08,520 --> 00:22:13,960
flavor of the day is and even port that over to, you know, distributed models or HPC

319
00:22:13,960 --> 00:22:14,960
or.

320
00:22:14,960 --> 00:22:20,600
So the upside is that the VM can now be written and rewritten in many other things.

321
00:22:20,600 --> 00:22:27,880
The downside is that you have to have a build a whole ecosystem around your VM so that

322
00:22:27,880 --> 00:22:29,960
users don't feel depraved by tooling.

323
00:22:29,960 --> 00:22:30,960
Yeah.

324
00:22:30,960 --> 00:22:32,520
And Tiano was also like that.

325
00:22:32,520 --> 00:22:36,440
That's the whole programming model of symbolic where you define a symbolic model and then

326
00:22:36,440 --> 00:22:38,800
you compile it and then you run it somewhere.

327
00:22:38,800 --> 00:22:44,480
The torch model has always been imperative, which is you don't really have a separate VM.

328
00:22:44,480 --> 00:22:49,000
You just declare things and you don't even like you just like you just write one plus

329
00:22:49,000 --> 00:22:54,480
two and it just executed like there's no like separation between declaration and execution.

330
00:22:54,480 --> 00:22:55,480
Yeah.

331
00:22:55,480 --> 00:22:58,160
We wanted to extend the same thing to PyTorch.

332
00:22:58,160 --> 00:23:03,840
So you just write arbitrary imperative code and that is your neural network itself.

333
00:23:03,840 --> 00:23:08,640
That also lends itself to I think at least in the data science community, there's a lot

334
00:23:08,640 --> 00:23:14,520
of popularity and flexibility around like Jupyter notebooks as a primary you are having

335
00:23:14,520 --> 00:23:21,000
an imperative execution lends itself to being able to yeah, you can basically see your

336
00:23:21,000 --> 00:23:22,480
execution as it goes.

337
00:23:22,480 --> 00:23:24,880
You can print things and all of that.

338
00:23:24,880 --> 00:23:27,560
That's one of the biggest upsides.

339
00:23:27,560 --> 00:23:32,640
So yeah, while we're building PyTorch, we wanted to continue the torch model, the imperative

340
00:23:32,640 --> 00:23:40,040
style, the dynamic nature of things and we wanted to build it in a way that it also reaches

341
00:23:40,040 --> 00:23:43,080
a very high bar of engineering.

342
00:23:43,080 --> 00:23:47,280
So that's been our core philosophy.

343
00:23:47,280 --> 00:23:55,200
And so for Google, I think the way most people have interpreted their strategy behind diving

344
00:23:55,200 --> 00:24:04,680
deep into TensorFlow is they foresee this world where AI workloads, whether they're training

345
00:24:04,680 --> 00:24:07,920
or inference workloads are going to drive a ton of compute.

346
00:24:07,920 --> 00:24:13,720
Their business model, their non advertising business model is heavily geared towards providing

347
00:24:13,720 --> 00:24:16,520
compute via the Google Cloud.

348
00:24:16,520 --> 00:24:22,200
And so if they can own the model in which they can basically own the VM for AI, then they

349
00:24:22,200 --> 00:24:25,640
can be the best place to run AI workloads, right?

350
00:24:25,640 --> 00:24:31,400
What's the Facebook motivation for investing so heavily in PyTorch and tooling?

351
00:24:31,400 --> 00:24:36,520
Is it just not to be controlled by Google or is there more to it?

352
00:24:36,520 --> 00:24:41,600
So Facebook's motivation is to fold.

353
00:24:41,600 --> 00:24:48,520
For Facebook AI research, which is the 100 odd researchers and in general for the community

354
00:24:48,520 --> 00:24:50,280
of AI.

355
00:24:50,280 --> 00:24:56,200
We have a single point agenda at fair, which is to try to solve AI.

356
00:24:56,200 --> 00:24:59,440
And for that, we're building the best tools out there.

357
00:24:59,440 --> 00:25:03,520
And we keep them open just because there's nothing secret to it.

358
00:25:03,520 --> 00:25:08,320
We want to build the best possible AI and we keep publishing about how we're trying to

359
00:25:08,320 --> 00:25:09,840
make progress.

360
00:25:09,840 --> 00:25:13,400
Our motivation is not really on the business model so much.

361
00:25:13,400 --> 00:25:18,040
It's more like, hey, we're trying to solve this very challenging problem.

362
00:25:18,040 --> 00:25:25,680
And you see this manifest in various ways in the Facebook product as a second-hand effect.

363
00:25:25,680 --> 00:25:29,640
Like the product teams are not sitting with the AI researchers and saying how do we improve

364
00:25:29,640 --> 00:25:31,680
Facebook as a product.

365
00:25:31,680 --> 00:25:38,840
Facebook AI research, the people are independently working on their AI research.

366
00:25:38,840 --> 00:25:44,120
But as we build and publish these things, the product teams look at our research and

367
00:25:44,120 --> 00:25:49,520
they're like, oh, we can implement this thing in our product in this way.

368
00:25:49,520 --> 00:25:53,040
And that's just going to be a better product experience for everyone.

369
00:25:53,040 --> 00:25:59,920
Some examples are we've had the accessibility interface improve quite a bit recently about

370
00:25:59,920 --> 00:26:05,800
a year, a year and a half ago, where now if you're a user who is blind or near blind,

371
00:26:05,800 --> 00:26:10,640
you can view Facebook like you can basically touch Facebook as you would and it will tell

372
00:26:10,640 --> 00:26:12,600
you what's going on.

373
00:26:12,600 --> 00:26:17,720
Or if you touch the picture, it would just not tell it's would say a picture posted

374
00:26:17,720 --> 00:26:19,360
by this person.

375
00:26:19,360 --> 00:26:23,800
But if you touch a picture now, it actually tells you, oh, it's a picture where a boy is

376
00:26:23,800 --> 00:26:29,440
playing with a cat and it's very descriptive and similarly we're trying to do the same

377
00:26:29,440 --> 00:26:31,400
for videos as well.

378
00:26:31,400 --> 00:26:36,200
That's one manifestation, others are where you want to make language barriers.

379
00:26:36,200 --> 00:26:43,960
So let's say I have 500 friends, like some of them I met in various places and trips.

380
00:26:43,960 --> 00:26:49,160
Like if one of my good friends writes a huge post in Chinese, I still want to be able

381
00:26:49,160 --> 00:26:51,080
to know what it's about, right?

382
00:26:51,080 --> 00:26:56,560
So we have the translate feature embedded into Facebook, all powered by a Facebook research.

383
00:26:56,560 --> 00:27:00,720
And you see these manifest in various other producty ways.

384
00:27:00,720 --> 00:27:08,360
And I think you see that evolve as well, like if I remember correctly, the Facebook products

385
00:27:08,360 --> 00:27:14,120
only relatively recently switched to switched the way they did translation to neural translation.

386
00:27:14,120 --> 00:27:17,600
I think there was a blog post on a fair blog maybe two months ago.

387
00:27:17,600 --> 00:27:19,080
So very, very interesting.

388
00:27:19,080 --> 00:27:20,880
Okay, so your talk.

389
00:27:20,880 --> 00:27:24,680
So kind of walk us through the main points of your talk.

390
00:27:24,680 --> 00:27:31,880
Basically you're at the high level, you've got PyTorch, it's inherently built around Python,

391
00:27:31,880 --> 00:27:37,240
but you need to find ways to overcome the limitations of Python and kind of how you do

392
00:27:37,240 --> 00:27:38,240
that.

393
00:27:38,240 --> 00:27:40,120
Was that the main thrust of the talk or?

394
00:27:40,120 --> 00:27:45,120
Yeah, so it's mostly just a general engineering talk about PyTorch.

395
00:27:45,120 --> 00:27:46,120
Okay.

396
00:27:46,120 --> 00:27:48,960
So I'm going to give like an overview of PyTorch how it works.

397
00:27:48,960 --> 00:27:49,960
Okay.

398
00:27:49,960 --> 00:27:52,960
A strange little audience doesn't necessarily know about deep learning computations, right?

399
00:27:52,960 --> 00:27:55,320
More of the developer focus audience.

400
00:27:55,320 --> 00:28:01,920
Like a lot of diversity in the programming models we use and like strange loop has everyone

401
00:28:01,920 --> 00:28:02,920
under the sun.

402
00:28:02,920 --> 00:28:03,920
Yeah.

403
00:28:03,920 --> 00:28:05,920
So, which is part of what makes it great, right?

404
00:28:05,920 --> 00:28:06,920
It's awesome.

405
00:28:06,920 --> 00:28:08,200
Like this is my first strange loop.

406
00:28:08,200 --> 00:28:13,440
I've just seen the sessions so far and like looking at the sessions lined up for tomorrow,

407
00:28:13,440 --> 00:28:14,440
it's awesome.

408
00:28:14,440 --> 00:28:20,160
So interesting factoid, the guy who founded Strange Loop, a guy named Alex Miller, Strange

409
00:28:20,160 --> 00:28:25,760
was a group out of a meetup he had here in town called Lambda Lounge that looked at

410
00:28:25,760 --> 00:28:31,960
like functional programming languages and at a startup that I was at years ago, this

411
00:28:31,960 --> 00:28:39,120
was probably eight years ago, maybe we basically hosted, incubated this meetup, they would

412
00:28:39,120 --> 00:28:40,120
meet in office.

413
00:28:40,120 --> 00:28:43,400
So I've been sitting there late at night trying to get finished with my work and hearing

414
00:28:43,400 --> 00:28:47,840
people talk about monads and stuff that I just had no clue about.

415
00:28:47,840 --> 00:28:48,840
Nice.

416
00:28:48,840 --> 00:28:58,560
I didn't realize St. Louis had a big developer community and this is all new to me.

417
00:28:58,560 --> 00:29:04,880
So I'll be talking a little bit about PyTorch, start off with how to relate it to other

418
00:29:04,880 --> 00:29:10,480
things people know and then a little bit of deep learning workloads, the general challenges,

419
00:29:10,480 --> 00:29:12,320
why they're very different from let's say.

420
00:29:12,320 --> 00:29:16,880
Let's say those in turn, how should people think about PyTorch and relating it to things

421
00:29:16,880 --> 00:29:17,880
that they know?

422
00:29:17,880 --> 00:29:23,760
Let's say you're, I mean, the most common thing everyone knows about is JavaScript, right?

423
00:29:23,760 --> 00:29:27,320
So let's look at JavaScript.

424
00:29:27,320 --> 00:29:32,440
If you're trying to build a compiler for JavaScript, the things you most care about is JavaScript

425
00:29:32,440 --> 00:29:40,200
code, which is very branchy, has a lot of control flow and it's very, very like cash

426
00:29:40,200 --> 00:29:41,880
and sensitive.

427
00:29:41,880 --> 00:29:46,360
If you're building a very high performance compiler for JavaScript, you will build it

428
00:29:46,360 --> 00:29:53,320
in a way that you'll try to optimize for like branch prediction and like try to get

429
00:29:53,320 --> 00:29:58,600
traces and do trace compilation, you're tracing, like for example, if you look at

430
00:29:58,600 --> 00:30:05,120
Chrome's JavaScript compiler, V8, yeah, V8, it's tracing jit and or you can look at

431
00:30:05,120 --> 00:30:10,880
Lua jit as another example, it's another tracing jit and they do, we'll quickly trace

432
00:30:10,880 --> 00:30:16,840
through upcoming code and then if something's compatible, we'll run time code generator

433
00:30:16,840 --> 00:30:20,840
really quickly and these are all in the order of nanoseconds even because they're very,

434
00:30:20,840 --> 00:30:28,640
very small computations and they're very branchy and doing something like loop hoisting or

435
00:30:28,640 --> 00:30:34,520
strength reduction, these are the things that would really go well with such workloads.

436
00:30:34,520 --> 00:30:35,520
Okay.

437
00:30:35,520 --> 00:30:41,040
If learning, what you do is you do operations on tensors, let's say like, and eventual

438
00:30:41,040 --> 00:30:48,560
matrices and usually you're doing a computation between A and B and A and B are not two

439
00:30:48,560 --> 00:30:55,720
integers, they're 2,000 integers or 200,000 integers and so tensors is this intervention

440
00:30:55,720 --> 00:31:00,680
on matrix and when you're doing these calculations, you're basically doing a lot of multiplications

441
00:31:00,680 --> 00:31:08,360
or like any kind of point wise or reduction or some kind of convolvingy, like a moving

442
00:31:08,360 --> 00:31:13,560
window kind of operations, these are the most common things in deep learning.

443
00:31:13,560 --> 00:31:19,520
So when you're trying to make these like something like this more efficient, you look

444
00:31:19,520 --> 00:31:25,640
at how fast you can, how fast, like how much you can parallelize each of these operations

445
00:31:25,640 --> 00:31:32,760
individually and it turns out almost all of these operations are bandwidth, so these

446
00:31:32,760 --> 00:31:40,480
operations can one as fast as how fast your memory bandwidth is, like how fast you can

447
00:31:40,480 --> 00:31:46,240
get it in and out of the CPU because inside the CPU, you're just doing a small multiplication

448
00:31:46,240 --> 00:31:52,440
or like an exponential but it's still much more expensive to get 200,000 numbers into

449
00:31:52,440 --> 00:31:55,360
the CPU and out of the CPU.

450
00:31:55,360 --> 00:32:01,920
So the way you would do optimization when building such things, let's say you're building

451
00:32:01,920 --> 00:32:04,120
a compiler for these things.

452
00:32:04,120 --> 00:32:09,600
You would try to fuse a bunch of these point wise operations, a bunch of these reduction

453
00:32:09,600 --> 00:32:15,960
operations into an inner loop and then what you would do is you would get these tensors

454
00:32:15,960 --> 00:32:20,760
in instead of doing one operation, putting it back out in a result and doing another

455
00:32:20,760 --> 00:32:26,640
operation, putting the result back out, you try to get the tensor in, do seven operations

456
00:32:26,640 --> 00:32:32,440
at once and then get all like the result of the seven of them out because that would

457
00:32:32,440 --> 00:32:35,640
make it more compute bond rather than bandwidth bond.

458
00:32:35,640 --> 00:32:36,640
Interesting.

459
00:32:36,640 --> 00:32:42,960
So taking a step back to the analogy that JavaScript is primarily to say you build these high

460
00:32:42,960 --> 00:32:47,760
performance, compilation and execution environments by understanding the property of the language

461
00:32:47,760 --> 00:32:51,600
that you're working with and optimizing around that.

462
00:32:51,600 --> 00:32:59,120
And we can do that here by, in this example, you would take code that is fundamentally written

463
00:32:59,120 --> 00:33:06,040
in a very iterative serial kind of way but maybe parallelize or unfold those loops.

464
00:33:06,040 --> 00:33:07,040
Yeah.

465
00:33:07,040 --> 00:33:08,440
I don't know if that's the right way of thinking about it but.

466
00:33:08,440 --> 00:33:09,960
Say tile the loops.

467
00:33:09,960 --> 00:33:10,960
What's that tile the loops?

468
00:33:10,960 --> 00:33:11,960
Tiling.

469
00:33:11,960 --> 00:33:15,760
It's called tiling because you can, you can break the computation to, oh, I have 200,000

470
00:33:15,760 --> 00:33:16,760
of these.

471
00:33:16,760 --> 00:33:21,760
I'll just make tiles of 20 and send them to like 20 different processors.

472
00:33:21,760 --> 00:33:22,760
Okay.

473
00:33:22,760 --> 00:33:23,760
Interesting.

474
00:33:23,760 --> 00:33:24,760
That was very cool.

475
00:33:24,760 --> 00:33:29,600
Especially with GPUs, with GPUs, you have 3,000 cores on your GPU.

476
00:33:29,600 --> 00:33:30,600
Right.

477
00:33:30,600 --> 00:33:35,680
So you want to like break this computation down and feed those into all the separate processors

478
00:33:35,680 --> 00:33:38,320
and then like get the results back.

479
00:33:38,320 --> 00:33:41,440
But they don't fundamentally change the bandwidth issue.

480
00:33:41,440 --> 00:33:42,440
They don't.

481
00:33:42,440 --> 00:33:43,840
The bandwidth issue is still exist.

482
00:33:43,840 --> 00:33:44,840
Yeah.

483
00:33:44,840 --> 00:33:49,720
So I'm going to be talking a little bit about the GIT that we built into PyTorch.

484
00:33:49,720 --> 00:33:51,960
We built a just in time compiler.

485
00:33:51,960 --> 00:33:55,320
It's also a tracing GIT but it's of a very different kind.

486
00:33:55,320 --> 00:33:58,320
Art tracing is not in the order of nanoseconds.

487
00:33:58,320 --> 00:34:00,280
It's in the order of microseconds.

488
00:34:00,280 --> 00:34:01,280
Okay.

489
00:34:01,280 --> 00:34:04,200
But that's completely fine because general deep learning workloads are in the order of

490
00:34:04,200 --> 00:34:05,200
milliseconds.

491
00:34:05,200 --> 00:34:06,200
Right.

492
00:34:06,200 --> 00:34:12,360
And the kind of optimizations passes we write as well are, as I said, more like fusion

493
00:34:12,360 --> 00:34:19,600
and batching by batching, I mean, let's say you do computation x, y, z.

494
00:34:19,600 --> 00:34:23,440
But between x, computation x and z, they're shared operations.

495
00:34:23,440 --> 00:34:26,920
Let's say x also does multiplies and z also does multiplies.

496
00:34:26,920 --> 00:34:31,720
So the tensors involved in x and tensors involved in z are very small.

497
00:34:31,720 --> 00:34:38,120
What you do is you create a multiply operation, combine the tensors that are involved there

498
00:34:38,120 --> 00:34:41,800
and then after the result comes out separate them on.

499
00:34:41,800 --> 00:34:43,960
So this is called dynamic batching.

500
00:34:43,960 --> 00:34:44,960
Yeah.

501
00:34:44,960 --> 00:34:50,560
So we've been writing this GIT that's very new for us, like as in not many people have

502
00:34:50,560 --> 00:34:52,640
worked generally in this direction.

503
00:34:52,640 --> 00:34:55,360
Tens of flow is building one called XLA.

504
00:34:55,360 --> 00:34:56,960
It's a compiler.

505
00:34:56,960 --> 00:35:01,280
And the kind of optimizations they're doing as well are very similar in nature.

506
00:35:01,280 --> 00:35:06,120
Like everyone's exploring now these like how to make tensor computations faster.

507
00:35:06,120 --> 00:35:10,600
We're just taking the just in time approach and they're taking the head of time analysis

508
00:35:10,600 --> 00:35:11,840
approach.

509
00:35:11,840 --> 00:35:20,200
And I think of GIT and this probably comes from really JavaScript as something that is

510
00:35:20,200 --> 00:35:26,840
more relevant to interactive types of workloads than batch, but deep learning is primarily

511
00:35:26,840 --> 00:35:31,040
a batch workload, at least the training part of it.

512
00:35:31,040 --> 00:35:32,240
Depends on how you see it.

513
00:35:32,240 --> 00:35:35,600
Like we want to keep the interactiveness because remember what we talked about, people

514
00:35:35,600 --> 00:35:41,800
like to keep that interactive Pythani iPad on on book style programming model.

515
00:35:41,800 --> 00:35:47,040
We want people to keep that flexibility, but that's not really where you need like the

516
00:35:47,040 --> 00:35:48,040
super high performance.

517
00:35:48,040 --> 00:35:55,360
No, that's every like that's people do interactively.

518
00:35:55,360 --> 00:36:01,600
They're programming for AGP's like this is this is the norm and deep learning.

519
00:36:01,600 --> 00:36:06,600
So you're backed by your super powerful GPU.

520
00:36:06,600 --> 00:36:12,600
And I'll talk a little bit about this in the talk tomorrow about how you can your PyTorch

521
00:36:12,600 --> 00:36:16,360
sensors can just be transferred to the GPU and you operate on them.

522
00:36:16,360 --> 00:36:21,840
And all the operations are now being done on the GPU with very high performance, but you're

523
00:36:21,840 --> 00:36:23,800
doing this in a very interactive way.

524
00:36:23,800 --> 00:36:24,800
Okay.

525
00:36:24,800 --> 00:36:27,880
It is a little bit of a mind shift for folks that have been kind of immersed in a tensor

526
00:36:27,880 --> 00:36:32,600
flow oriented or a batch oriented world where you kind of create this job.

527
00:36:32,600 --> 00:36:38,120
You send it off to the cloud or wherever to train and then you check on it a few days.

528
00:36:38,120 --> 00:36:39,120
Yeah.

529
00:36:39,120 --> 00:36:45,160
We are the worst nightmare for hardware developers because they're all building solutions

530
00:36:45,160 --> 00:36:46,160
around.

531
00:36:46,160 --> 00:36:51,360
Oh, let's say you build this model beforehand and then you give it to our hardware.

532
00:36:51,360 --> 00:36:52,360
Yeah.

533
00:36:52,360 --> 00:36:58,320
Let's say it takes 30 minutes, but it will map your model in the most effective way to

534
00:36:58,320 --> 00:36:59,640
our hardware.

535
00:36:59,640 --> 00:37:06,040
And after that 30 minutes is done, if you pump any images in or any inputs in, it'll

536
00:37:06,040 --> 00:37:08,120
be like super fast.

537
00:37:08,120 --> 00:37:11,000
And we're like, well, that's kind of dumb in our model.

538
00:37:11,000 --> 00:37:15,520
Like we'll give you a different model in every single iteration.

539
00:37:15,520 --> 00:37:16,520
What are you going to do?

540
00:37:16,520 --> 00:37:17,520
Right, right.

541
00:37:17,520 --> 00:37:19,000
Interesting.

542
00:37:19,000 --> 00:37:24,720
So speaking of hardware and hardware developers, I don't know if this is something that you're

543
00:37:24,720 --> 00:37:29,640
close to it all, but Facebook also is very involved in this OCP Open Compute projects.

544
00:37:29,640 --> 00:37:31,920
I was involved in the Big Sur and the Big Basin.

545
00:37:31,920 --> 00:37:32,920
Don't worry you.

546
00:37:32,920 --> 00:37:33,920
Yeah.

547
00:37:33,920 --> 00:37:34,920
Okay.

548
00:37:34,920 --> 00:37:42,320
So do you see, as I understand it, OCP has primarily been oriented around kind of off the shelf

549
00:37:42,320 --> 00:37:47,440
stuff like system architecture as opposed to, you know, board level architecture or anything

550
00:37:47,440 --> 00:37:53,480
like that, but do you see a future where OCP takes on like this bandwidth problem to try

551
00:37:53,480 --> 00:37:59,080
to make hardware that's more suitable for these kinds of workloads or are those other

552
00:37:59,080 --> 00:38:00,080
people's problems?

553
00:38:00,080 --> 00:38:06,560
I think OCP is one of those huge industry-wide efforts, right?

554
00:38:06,560 --> 00:38:11,480
I think it's truly possible that under OCP you'll get something that's like a custom

555
00:38:11,480 --> 00:38:13,160
ASIC.

556
00:38:13,160 --> 00:38:15,880
And I'm not sure how that will happen in general.

557
00:38:15,880 --> 00:38:20,880
Because there's so many players and so many people who can contribute to OCP just two

558
00:38:20,880 --> 00:38:29,880
days ago, I think, or yesterday, Nvidia released an open spec with very log files and HDL

559
00:38:29,880 --> 00:38:36,560
files off a chip that does convolutional neural networks, like a high performance convolutional

560
00:38:36,560 --> 00:38:37,560
neural network-based thing.

561
00:38:37,560 --> 00:38:41,600
I think I saw the headline for that is like, this is Nvidia's TVU.

562
00:38:41,600 --> 00:38:42,600
Right.

563
00:38:42,600 --> 00:38:45,320
I mean, that was the click-bady headline.

564
00:38:45,320 --> 00:38:48,560
So they basically put it out there.

565
00:38:48,560 --> 00:38:54,840
You can take that very log and HDL is basically the code for fabricating these chips.

566
00:38:54,840 --> 00:38:55,840
Yeah.

567
00:38:55,840 --> 00:39:02,960
You will have to still like remap, very log into actual process, your manufacturing under,

568
00:39:02,960 --> 00:39:07,680
but that's a mechanical process that usually, like you then, you usually give your HDL

569
00:39:07,680 --> 00:39:14,600
files to someone and then they'll spend, they had a company will spend remapping whatever

570
00:39:14,600 --> 00:39:17,680
you have to the process most effectively.

571
00:39:17,680 --> 00:39:21,520
But yeah, it's a fairly mechanical process at the point.

572
00:39:21,520 --> 00:39:26,000
So now, Nvidia just released this open chip.

573
00:39:26,000 --> 00:39:31,880
And that's totally one of the candidates, for example, for open compute.

574
00:39:31,880 --> 00:39:35,800
Like, open compute is all about, okay, everything is open.

575
00:39:35,800 --> 00:39:39,240
You can refer to this as your own, right?

576
00:39:39,240 --> 00:39:44,800
So I think we're still not sure about what's going to come into OCP in that direction.

577
00:39:44,800 --> 00:39:46,680
Like I personally don't know the details.

578
00:39:46,680 --> 00:39:47,680
Right.

579
00:39:47,680 --> 00:39:48,680
So we'll see.

580
00:39:48,680 --> 00:39:50,440
We'll see how it goes.

581
00:39:50,440 --> 00:39:51,440
Okay.

582
00:39:51,440 --> 00:39:59,000
So you also mentioned that parts of the pie torch engine are written in C makes me think

583
00:39:59,000 --> 00:40:04,720
a little bit of, I can, in just kind of the regular Python community, there's, I'm

584
00:40:04,720 --> 00:40:09,320
or incorrectly, there's like pi pi and sithon and all these different implementations of

585
00:40:09,320 --> 00:40:10,320
the Python interpreter.

586
00:40:10,320 --> 00:40:12,160
Is it the same general idea there?

587
00:40:12,160 --> 00:40:14,360
Not at all.

588
00:40:14,360 --> 00:40:21,200
Pi pi is a replacement for the default Python implementation, which is called CPITON.

589
00:40:21,200 --> 00:40:22,200
Okay.

590
00:40:22,200 --> 00:40:28,720
So Python is a programming language and they have a base implementation of that programming

591
00:40:28,720 --> 00:40:33,560
language that the language developers develop on, which is called CPITON.

592
00:40:33,560 --> 00:40:40,520
It's written in C and if you type Python into your desktop, usually that's what it is.

593
00:40:40,520 --> 00:40:41,520
CPITON.

594
00:40:41,520 --> 00:40:48,160
And CITON is just a very cute way of writing extensions to Python, like to see Python

595
00:40:48,160 --> 00:40:49,160
mostly.

596
00:40:49,160 --> 00:40:50,160
Okay.

597
00:40:50,160 --> 00:40:53,760
Pi pi is a replacement interpreter for CPITON.

598
00:40:53,760 --> 00:40:54,760
For CPITON.

599
00:40:54,760 --> 00:40:56,200
It's written in Python.

600
00:40:56,200 --> 00:40:57,520
That can interpret Python.

601
00:40:57,520 --> 00:41:01,040
I'm not sure what it's written in, but I don't think it's written in Python.

602
00:41:01,040 --> 00:41:02,040
Okay.

603
00:41:02,040 --> 00:41:04,720
The implementation is probably not in Python.

604
00:41:04,720 --> 00:41:05,720
Okay.

605
00:41:05,720 --> 00:41:11,760
Pi pi is a just in time interpreter for Python.

606
00:41:11,760 --> 00:41:14,760
It's probably written in part assembly and part C or whatever.

607
00:41:14,760 --> 00:41:15,760
Okay.

608
00:41:15,760 --> 00:41:16,760
Okay.

609
00:41:16,760 --> 00:41:21,880
So the idea of being the significance of it is not its implementation, but just in time

610
00:41:21,880 --> 00:41:24,320
versus not just in time.

611
00:41:24,320 --> 00:41:25,320
Yeah.

612
00:41:25,320 --> 00:41:26,320
Okay.

613
00:41:26,320 --> 00:41:29,720
Pi Torch is just a CPITON extension.

614
00:41:29,720 --> 00:41:35,200
Like all the C bits we have are, the equivalent you can find is NumPy.

615
00:41:35,200 --> 00:41:38,920
NumPy is 90% written in C.

616
00:41:38,920 --> 00:41:39,920
Yeah.

617
00:41:39,920 --> 00:41:40,920
Okay.

618
00:41:40,920 --> 00:41:43,280
But it has, it's a CPITON extension.

619
00:41:43,280 --> 00:41:46,560
That is, it's not an independent C library that you can run.

620
00:41:46,560 --> 00:41:50,560
It's like heavily integrated into the CPITON API and stuff.

621
00:41:50,560 --> 00:41:53,280
So Pi Torch is just like a Python extension.

622
00:41:53,280 --> 00:41:54,280
Okay.

623
00:41:54,280 --> 00:41:59,080
But there's recently been an announcement of, you know, one of the first like, I think

624
00:41:59,080 --> 00:42:06,480
kind of broadly publicize Pi Torch wins, if you will, is like fast.ai, deciding to rewrite

625
00:42:06,480 --> 00:42:10,040
all of their courseware and into Pi Torch.

626
00:42:10,040 --> 00:42:12,240
That was a very pleasant surprise.

627
00:42:12,240 --> 00:42:13,240
I wasn't.

628
00:42:13,240 --> 00:42:18,640
As soon as Pi Torch came out, I think they've tried it and they found it really effective,

629
00:42:18,640 --> 00:42:22,040
especially for teaching and the barrier of entry.

630
00:42:22,040 --> 00:42:26,960
So they've switched over to Pi Torch from TensorFlow.

631
00:42:26,960 --> 00:42:29,960
And we're supporting them in any way we can.

632
00:42:29,960 --> 00:42:30,960
Yeah.

633
00:42:30,960 --> 00:42:31,960
Yeah.

634
00:42:31,960 --> 00:42:32,960
It's interesting.

635
00:42:32,960 --> 00:42:38,000
I think if, you know, if there's anything that this community benefits from is options and

636
00:42:38,000 --> 00:42:43,360
especially options that, you know, have major, you know, both major companies behind them

637
00:42:43,360 --> 00:42:49,760
pushing them forward, but also that are open to community engagement and community contributions.

638
00:42:49,760 --> 00:42:55,920
And so it's definitely great to see from a kind of industry observer point of view that,

639
00:42:55,920 --> 00:43:02,320
you know, we've got, you know, what's starting to look like a second kind of really strong

640
00:43:02,320 --> 00:43:07,320
contender at a time when, you know, again, I think a lot of people said, oh, yeah, it's

641
00:43:07,320 --> 00:43:08,880
just, you know, it's TensorFlow.

642
00:43:08,880 --> 00:43:12,160
So congrats for your part in that.

643
00:43:12,160 --> 00:43:14,760
And thanks so much for taking the time to sit down with me.

644
00:43:14,760 --> 00:43:15,760
Oh, my pleasure.

645
00:43:15,760 --> 00:43:16,760
Appreciate it.

646
00:43:19,760 --> 00:43:21,240
All right, everyone.

647
00:43:21,240 --> 00:43:23,320
That's our show for today.

648
00:43:23,320 --> 00:43:28,280
Thanks so much for listening and for your continued feedback and support.

649
00:43:28,280 --> 00:43:33,440
For more information on to me or any of the topics covered in this episode, head on over

650
00:43:33,440 --> 00:43:37,560
to twomlai.com slash talk slash 70.

651
00:43:37,560 --> 00:43:45,040
To follow along with our strange loop 2017 series, visit twomlai.com slash ST loop.

652
00:43:45,040 --> 00:43:50,760
Of course, you can send along your feedback or questions via Twitter to act twomlai or

653
00:43:50,760 --> 00:43:55,840
at Sam Charrington, or leave a comment right on the show notes page.

654
00:43:55,840 --> 00:43:59,200
Thanks again to Nexosis for their sponsorship of the show.

655
00:43:59,200 --> 00:44:06,200
Check out twomlai.com slash talk slash 69 to hear my interview with the company founders

656
00:44:06,200 --> 00:44:13,800
and visit nexosis.com slash twimmel for more information and to try their API for free.

657
00:44:13,800 --> 00:44:23,800
Thanks again for listening and catch you next time.

