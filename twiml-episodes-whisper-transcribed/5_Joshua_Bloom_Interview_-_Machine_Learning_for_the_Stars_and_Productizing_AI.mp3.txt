Hello everyone and welcome to another episode of Twimmel Talk, the podcast where I interview
interesting people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I think you all are really going to get a kick out of this show.
My guest this time is Joshua Bloom, professor of astronomy at the University of California
Berkeley and co-founder and chief technology officer of machine learning startup Wise.io.
I was in California last week and Josh graciously agreed to host me in his company's office
for this interview.
We had a wonderful discussion and as you might have guessed, if you happen to have noticed
the length of this episode, we covered quite a lot of ground, but I promise you that you'll
find this 84 minute interview to be jam packed with great information, ideas and war stories.
In this show, you'll learn how Josh and his research group at Berkeley pioneered the
use of machine learning for the analysis of images from robotic infrared telescopes.
We talk extensively about the challenges they faced in doing this and some of the results
they achieved.
We also discussed the founding of his company Wise.io, which uses machine learning to help
customers deliver better customer support, but that wasn't where the company started and
you'll hear why and how they evolved to serve that market.
We talk about his company's technology stack and data science pipeline in fair detail
and discuss some of the key technology decisions they've made in building their product.
We also discussed some interesting open research challenges in machine learning and AI.
Of course, I'll be linking to Josh and the various things we mentioned on the show in the
show notes, which you'll be able to find at twimmelai.com slash talk slash five.
That's twimelai.com slash T-A-L-K slash the number five.
And now onto the interview, hey everyone, I am here at the Wise.io offices with the
CTO, Joshua Bloom, and we got a great conversation lined up for you, and we'll start with Josh.
Why don't you introduce yourself to the audience here.
Great, so this is Josh and I am CTO and one of the co-founders of Wise.io.
I'm also a professor at UC Berkeley in the astronomy department.
One of the important things that we'll touch on today is how does somebody go from astronomy
and teaching to building an AI application company.
I think a big part of the origin story of course of the company is my history, but I
think it also has some interesting lessons for how we think about AI and production
systems and why having diverse backgrounds is pretty important these days.
Yeah, that's a lot of good stuff to talk about there.
Why don't we start by learning a little bit more about you and your background and how
you got to where you are.
So I was trained as a physicist and an astronomer, went to Harvard as an undergrad and caught
a bit of the research bug over the summers working in Los Alamos, then went to Cambridge
England to do a masters and back to Caltech where I did my PhD all in the context of astronomy
and then back to Harvard where I was a postdoc.
All the while working on what we could broadly term time-domain astrophysics, understanding
the variable sky and why things do what they do explosively, cataclysmically or otherwise.
And while there's a deep interest in understanding the origins of those events and how they're
connected to other things that we study in the universe, I got more and more interested
over time in the informatics of actually just doing the science, the statistics on variable
sources and the presence of noise.
And then as I became a faculty member at Berkeley, I started looking ahead to really a series
of new surveys, particularly imaging surveys of large swaths of the night sky and one
of the great interests for myself and many others was in finding new events, essentially
new explosions or new variable eruptive stars that hadn't been known about before and doing
that as quickly as possible.
Now the traditional way in which that was done and in some cases is still done today is
that as you acquire more data, you linearly hire more grad students that scales with the
total number of images that you're getting and you need to sift through.
And as I was becoming involved in some of those projects, I wound up realizing that as time
went on, that really wouldn't scale anymore.
And we needed to find effectively a replacement for domain experts who otherwise would have
been looking at and apining on data using alternate techniques.
And about 79 years ago, I stumbled upon machine learning as a real interesting potential avenue.
And at the time, machine learning really hadn't been applied to anything in astronomy in
the variable sky, sort of in a time to mean context.
There had been a number of studies in using machine learning to do special types of inference
on the static sky and understanding, sort of demographics of stars and galaxies and
their distribution in space.
So we really felt like there wasn't a lot of precedent in us applying some of the capabilities
to this data, but we wound up realizing it was sort of an imperative.
And one of the things that people who know astronomers would probably say about them is
they tend to like to use tools that help them and seek those tools out, be those new types
of detectors.
So CCDs, for instance, were something that astronomers adopted almost as soon as they were invented.
And obviously statistical techniques and computational techniques, astronomers are willing to try
things out to solve their problem.
The classic example I go back to is Galileo who said, hey, there's this new thing that's
been invented to look at the horizon for ships coming towards us.
What if I just took it and pointed it to the stars?
What could I do with that?
And so our use of the telescope was essentially a co-option of the use of a technology that
had been built for other purposes.
And so that kind of precedes a pace throughout the history of astronomy.
And so the idea of essentially a fairly new technique into the fold is not at all unusual.
Do you remember how you stumbled across machine learning?
Yeah, so part of it was just asking the question, if I've got a bunch of data and I need to
decide, is this part of the sky interesting or not, is this event new or not, what type
of event could this be, very quickly you wind up realizing this is a classification problem
of some sort.
And talking to people at UC Berkeley in the stats department as I was starting to introduce
some of these interesting challenges became very clear that machine learning and particularly
supervised learning would be a fertile ground for us to start exploring.
But one of the challenges that I saw is that even though we're in a very rich and fertile
environment at UC Berkeley, and there's a lot of crosstalk between departments and individuals
within departments, it was very hard to get even the kind of the language on both sides
up and running where both sides understood the methodological folks who deeply understood
what machine learning was and what it could be used for, and then people like myself from
the physical side of even learning to ask the right questions.
So thankfully wind up getting a group from the stats department and those folks from
computer science together with me and my postdocs and we were able to get a proposal together
the National Science Foundation funded that allowed us to basically start building out new
ways of doing inference on astronomy data.
That turned out to be a very fruitful place for us, for me in particular, to learn about
the landscape of what other techniques were out there that we hadn't been taught in
school.
Can you tell us a little bit about from that initial discovery what the research arc looked
like, what were some of the first things you started exploring and how that evolved
over time?
Yeah, so I really just started looking at toy amounts of data that we already had in
the can, and we could start applying these different techniques to, and looking for tools
that would be useful for us, and really the best thing out there the time that we started
was something called WECCA, which was a, and still is, a collection of machine learning
algorithms that one can apply in a sort of gooey graphical way, all kind of written
in Java, and really that was our original playground in benchmark, and used that as a launching
off point to start understanding what are these different modeling techniques that are being
exposed to here?
What does a support vector machine, what does it mean when people say random forest, and
use that as a way to sort of educate myself and those in our group, and started seeing
some interesting results, right?
We started seeing accuracies that were better than what you could get from random, and then
as we poked farther and farther, one ended up seeing how far can we take these algorithms,
how well does one of them work relative to the others to get the kinds of answers that
we want?
How do we build in a loss function, which turns out to be very important to get good answers,
because in the case of what we did, when we're discovering something in the sky, it's
not easy to articulate that loss function, and by that I mean, what is the cost of
missing an interesting place in the sky?
It means that you don't get to do new science, versus what's the cost of saying everything
in the sky is interesting, which means you burn all of your follow-up resources, and
starting then to think about context-aware classification, now just not in the context
of really just resources, but now time constraints, making an inference about something that could
be of interest, may be more important than waiting to get another couple of data points
and saying something with even more confidence.
So understanding how to calibrate confidence in probabilities, doing this in the presence
of sort of missing data and irregularly sampled data in time, all of these also started
wind up showing to us that there were parts of the machine learning sphere, at least
in the academic world, that were not often exposed to the kinds of data that we were
exposed to.
And so noisy data, for instance, know whenever, when they talk about the iris data set,
they don't say, you know, the pedal of this is red plus or minus purple.
So even just having uncertainties in your features, let alone your labels, became an interesting
challenge, and we wanted realizing perhaps there were some new techniques that we needed
to start innovating on, to even do the kinds of inference we wanted to do with our data.
So you mentioned the loss function and needing to wrap your arms around what that means, can
you succinctly describe how you grapple that?
What did you end up, how did you approach it and what did you end up coming up with for
the types of data that you were looking at?
One of the things we wind up realizing is that one person's loss function is not the
same as another person's loss function, and so to get traction on your answers, one needs
to at least be clear about what it is that you're optimizing for.
And at least give people the ability to imbue their own loss functions, if for instance
you're producing a catalog of different types of variable stars on the sky.
We have a specific notion of what it means to get something wrong about say a very minority
class versus a majority class, and I wouldn't say that we solved that problem by any stretch,
but at least we were trying to be clear about what our assumptions were of the loss function
and articulate what it is that we're optimizing for.
When people are doing AI or machine learning in a production environment, there is always
going to be an optimization of some sort, and the typical one people will go to without
knowing exactly what the business value is, or scientific value is of the answer, is
you go for some notion of an accuracy, and then when you get a level deeper in that,
you say, well, what I really want to do is I want to minimize false positives at a false
negative rate of 0.1, and then that is an implicit statement of what your loss function is,
and you hope that by defining it that way, and by optimizing on it that way, that you're
actually getting very close to an optimization of the result of what you're emitting out
of your modeling.
And so you're primarily looking at image-oriented data over time.
Are there other fields where you've seen them adopt the same types of approaches to
what you were working with?
Well, one of the nice things is you can work at the sensor level data, which is effectively
photoelectrons in a CCD and counting those up as a function of position in X, Y, and
then trying to map that back in the sky.
So that's what you might call noisy image sensor data, and we worked at that level.
And then we also worked at a metadata level, which was now, let's use traditional astronomy
techniques to extract the brightness of a star as a function of time.
So we got ourselves out of the image plane and into the time domain, and then we're basically
working with effectively tabular data.
And again, there are lots of different models and feature engineering approaches that one
can take to all of that.
I wouldn't say that there was a common thread in our work, across a bunch of these different
sort of sub-questions, other than say that over time we wind up realizing that there
were only really a couple of different machine learning models that did as well or better
than everything else.
And so even though, for instance, support vector machines are very popular because they
have some great sort of theoretical, provable properties, they tend to be kind of unwieldy
and for dealing with the kinds of data we were working with, which is heterogeneous, noisy,
dirty, sparse, missing, and multi-class, where you needed to also get probabilities out
that you could then calibrate models like support vector machines really fall short for
practical purposes.
And so we wound up recognizing in our group, and I think that was validated in a conference
that we ran at UC Berkeley on essentially streaming inference with machine learning.
It was a sort of week-long conference that involved folks from Netflix, folks from Google,
and then domain experts, everything from biomedical to physics.
A number of people would stand up, give their talk, and say, yeah, and we wound up realizing
that decision forest pretty much always won.
Now this was in 2012 before the resurgence of deep learning, I bet if we ran this conference
again half of the talks would be about how that's a better algorithm as it were.
But it was pretty eye-opening.
And it was one of the things that we took to heart as we wound up starting the company
is a recognition that to exceed, to produce value sort of very generally, the algorithm
itself is not necessarily the key.
In some sense, the way I view this now is that algorithms and their accuracy that they
can produce and their ability to optimize them around a loss function is really only
table stakes for the utility of these in a real environment.
So yes, you need to use a model that's very, very accurate and potentially can be retrained
and gets slightly better.
But as most data scientists or most people at work in machine learning workflows will
say almost all of that work is in feature engineering.
And if you're a deep learning person, you'll say almost all of that work is in figuring
out what the shape of the network should be iterating over that.
On that note, before we jump into what you're doing at the company today, what were some
of the results you saw out of your research on the astronomy side?
So we looked at a couple of different realms.
One was looking at large catalogs of variable stars and coming up with probabilistic classifications
of what type of variable stars they were, what was the physics that drove them.
And we did that in a bootstrap way, starting with effectively a few hundred known classes
and few hundred or a few thousand known labels.
And then extrapolated that to tens of thousands, hundreds of thousands of variable stars and
produced probabilistic catalogs.
One of the things I became adamant about as we were doing that was producing a catalog
where you say, hey, this object in the sky is of this type with this probability is effectively
useless unless it's then used for some new kind of science.
And one of the things that I became, I won't say frustrated with, but I noticed often
is that people started using, not just in astronomy, but in many other fields, machine
learning as an end to itself saying, I'm going to apply machine learning to this data.
I'm going to get a result until that result itself is novel or until that becomes a stepping
stone to another result, which becomes novel, it's sort of an empty exercise.
And so what we want to be saying is what can we do with this probabilistic catalog that
couldn't have been done with any other means.
And so one of the things we did is we looked for very strange types of stars that had certain
properties and then followed those up with big telescopes and actually wrote science papers
with those.
So we use that as a launching off point.
In a real time environment where we actually were looking at images as they were streaming
off of telescopes in Southern California off of Palomar Mountain, every 60 seconds or so,
we would basically get transferred up to Lawrence Berkeley National Lab and we'd apply our machine
learning to that to find new interesting objects in the sky and then populate databases
of, you know, for tonight here are the interesting objects.
And then also had another machine learning code which would go into those databases and
periodically make statements about what types of objects those wind up might be, what
they could be.
And we went up having, I think of order of 100, maybe 200 papers that came out of that,
a refereed papers, which again, the machine learning part of that was really the stepping
stone to discovery, the other parts of the machine learning were the stepping stones
to initial inference.
And obviously in the end, you needed people to actually write the paper, but we tried
it to the grad students, right?
It all goes back to grad students, exactly.
But I really thought about kind of removing people from the real time inference loop and
getting as far up the inference stack as we could.
We even got to the point where we were finding interesting objects in the sky without any
humans in the loop, identifying that not only is it a new object, but it's something
we probably should be following up.
And we were ishing alerts to robotic telescopes to go follow those up.
So by the time people woke up in the morning, we not only had the discovery, we not only
had the initial inference, we then also had real follow-up, scientific follow-up.
One of the, I think, great achievements of the work that we and others did in one of
our collaborations was to build this production system that had real consumers on the other
side of that, and when it was broken or was wrong or didn't take feedback properly, you
know, we'd get nasty emails from our collaborators and say, your thing didn't work for an hour.
You kind of screwed my science while I was at the telescope.
So having an end user really keeps you heavily focused on making sure the things that it
needs to do does it right and robustly.
But because we were discovering things even faster than a whole army of grad students
would have been able to pour over all of these images, we were able to find, for instance,
the nearest type one, a supernova that had been found in 25 years and get a whole bunch
of people looking at that part of the sky and taking lots of data that led to a bunch
of papers in nature and science.
Not because that object wouldn't have been found by even amateurs because it got so bright
you could have seen it with binoculars eventually.
But because the interesting science happened hours after the event blew up, right?
The event happened.
And so it wind up also driving home for me the need for not only something that's working
and is robust, et cetera, but where it's able to make statements quickly and do it in
a way that's reliable.
Interesting.
So I'm sure that that has led you to some interesting perspectives on the relationship
between this technology and society and jobs and stuff like that.
I'm hearing parallels to a lot of people kind of projecting that as AI is deployed, shifting
shifts in the job market will take place that put a lot of people out of work.
But I'm also hearing in your example kind of the counter-argument you often hear that
really what it does is it empowers people and allows people to do different things that
I don't necessarily want to go deep into the society stuff at this point.
But yeah, it's certainly a valid concern.
What we do in our company at WiseIO is help customers support agents become more efficient
at their work by suggesting answers of how they can respond to an incoming inquiry,
by automatically triaging, incoming increase or tickets, emails, et cetera, that is getting
them to the right person or the right group who's going to be the best at answering that
question.
And then in some cases, we will automatically respond to incoming increase.
So when you write into e-commerce site and say my package didn't arrive, there's a
growing chance that us or somebody else may be answering what looks like a bespoke
question of yours with what looks like a bespoke answer.
But in the end, it's just a templatized response that we ourselves are using.
For us to be able to do that, obviously, we can talk more deeply about how that works
from an AI perspective.
We have to get very confident in what our answers are.
But what does this mean on one side to your question about labor displacement companies
don't need to hire as many support agents?
So where would they have gone?
The other side of that is that the agents that they do have become better and more tuned
at working in some of the harder parts of what their own products are about and what
their customer complaints are about, in a way they wouldn't have been able to because
they would have been distracted by the mundane.
So if you say, how do I reset my password and there's a person or sets of people that
have to look at that email and decide how to respond, that's time that those people are
not spending on really complex problems where empathy is required as well.
So we think of it as our product and what we do as a way of freeing people to work on
the things that people are uniquely suited at that machines really aren't going to be
that good until somebody solves the turing test.
Chat bots are not going to be able to understand people in the subtle ways that they need to.
But we can take a lot of easy stuff off the table.
So there was certainly a concern as we were starting to roll this out that we were part
of this labor displacement movement, but we heard time and time again from our customers
that their support agents became more and more happy, the more involved we were.
There was an entire team in Asia who had been tasked with basically reading an incoming
inquiry or a ticket and then deciding who else should be reading this to solve the problem.
And because our triage capability came into play, they effectively deprecated a 20-person
team, one of our clients, off of triage because we're effectively automatically triaging
now.
And we were worried what's going to happen to these people, they have families to feed.
And we got a whole bunch of really great quotes from them saying because they had been reassigned
to actually work these support tickets rather than push them along, they were much more
happy in their job.
That's fantastic.
So we jumped right into what you're doing at Wise.Oat.io, but the transition is a fascinating
one as well.
How do you get from astrophysics to software company doing CRM stuff?
And I know there was an intermediate step there as well.
So going back to the original part of the conversation, we had recognized in the team
that I had built and the people I had worked with that A, we had some great sort of technical
orthogonality, some were good software engineering, some good at ML, some UI, et cetera.
And B, that what we had learned to do of recognizing the importance of putting AI into production
and having real end users give real feedback in potentially a real-time loop was something
we at the time didn't see anyone else doing.
We knew obviously that the Googles and the LinkedIn's and the Netflix's of the world had
this kind of baked in to their overall data flow, but we certainly weren't seeing companies
helping other companies do it.
And one of my now co-founders had more or less, while he was between jobs, figured out how
to make one of the algorithms that we liked a lot, these decisions for our scale very,
very well, at least on a single machine in a multi-core environment.
And so we realized that we might have some interesting firepower and given that there
seemed at the time to be so much emphasis on massive-scale machine learning.
It was certainly a pre-spark era, but it was very much in the Hadoop heyday.
It looked like most of the interesting ML that was starting to come out and some of the
other companies that were coming out were really focused around helping the, you know,
I won't say exit scale, but certainly pediscale level, Google scale, amount of data companies
bring machine learning into their workflows.
So we thought about sort of skating to a place, you know, using the analogy that's heavily
overused to the part of where the puck was going to be, which was helping smaller companies
and mid-sized scale companies bring machine learning into production environments.
And that was the impetus for starting the company.
What the domain was going to be?
We didn't know who the customer was going to be and who the buyer was going to be.
We didn't know.
We were, I'd say blissfully ignorant about all of the business challenges that we would
wind up encountering over the next couple of years in bringing this to market.
And when we emerged out of our first accelerator, I gave a talk, our demo day was the alchemist
accelerator where I said we're going to be GitHub for data scientists and produce some
interesting UIs of interactions to help data scientists like ourselves more easily build
models that they could then push into a production environment.
We wind up seeing over the next couple of months the challenges of selling products like this
into data science teams, first data science teams were few and far between.
And the ones that existed were either too sophisticated to believe they could build it
all themselves or not sophisticated enough to get a large enough budget to pay for the
things that we wanted to provide them from a tooling perspective.
All the while we were building out our underlying platform to be able to do exactly that, to
be able to build templated machine learning models against certain types of data for
certain types of use cases.
And then even though you built it at a laptop level, push it into the cloud and have, you
know, in Amazon or other compute frameworks, the scalability to be able to serve large
numbers of customers around the same use cases where what's emerged for us is the difference
between a customer is not new code, it's just a config file if they're using that same
use case.
So all of that to say that we evolved, you could call it a pivot, if you'd like, but
I think of it as a series of of pivots to a place where we wind up seeing in customer
support, a lot of data, a lot of manual work and some really nice CRM systems with open
APIs and a fairly fixed schema.
So the sales forces and Zendes and service now's of the world really are the data lake and
the transactional layer for doing customer support and related activities.
And we thought if we could build now an intelligent system on top of that and do all the things
that I mentioned in powering these agents to become more efficient of their job and make
the whole support desk more efficient at serving customers, we would solve a bunch of
pain points.
And that as we wind up going into the market and started leading with products that could
be more or less installed by a non-technical user and could be used by a non-technical
user, wind up getting a lot of feedback that indeed we were solving some pain points.
There's obviously the efficiency question of needing less head count, but there's also
some really interesting customers of ours who are growing very quickly.
And one of them said to us that if the CEO had given him an infinite budget, he wouldn't
be able to hire good customer support agents quick enough.
So helping them capture all the internal knowledge was something that it turns out machine
learning actually does quite well at.
What were some of the biggest challenges in going from product direction focused on
generalize tools and platforms to one focused on a very specific application area?
I mean interestingly, it was all the things that we hadn't thought about which was product
management and how do you get structured feedback from customers?
What does it mean to build an MVP, roll that out, iterate on it, etc.
A lot of kind of lean startup 101 stuff was something that we hadn't really been thinking
of when we started the company and certainly didn't have frankly a lot of expertise in.
And then as we started scaling, it was a recognition that there are large parts of a machine
learning pipeline that don't naturally scale.
So figuring out ways to containerize the parts that need special attention from PhD levels
and data science and abstract that away from other parts of our engineering group that
don't need to know about what's happening deeply but need to be able to ask predictions
of some other part of the stack, restfully in a services oriented way.
We just want to realizing that what worked for us from a scaling perspective, compute
scaling perspective also wound up being what we needed to do from an organizational and
HR perspective, when we hire front-end engineers and middleware engineers who create it writing
scripts against databases and managing reticues, etc.
Those folks don't need to know about machine learning.
They need to know that there is a contract between their part of the stack and somewhere
deeper in the stack that if I ask you for a prediction for this client, for this model,
I'm expecting to get it back in this format on this time scale and if I don't, then
our contract's broken.
But likewise, I'm going to hand to that deeper part of the stack that's going to be providing
those predictions effectively some data and JSON or otherwise that will have a fixed schema
so that the group that built that machine learning pipeline knows that this column is going
to be of type date time, this column is going to be an int and it's going to join using
these four indices on some other data.
So once we wind up realizing that we could lock down the schema for a given use case, it
meant that we could write data science pipelines against data we hadn't seen before.
You need to see it once to make sure it's all working and make sure it crossvalidates
in an offline sense and it has the kinds of accuracy properties that you want.
But then it means that we could basically start spinning up new customers where they get
the base template that operates on their data and when we need to make changes, those
can happen really from a more or less technical person than somebody with a PhD in statistics.
So there were a bunch of challenges around that and as we started solving those, it just
sort of fell out that our stack really mimics what our organization looks like.
Can you talk a little bit about the data that you typically see in a customer environment?
I'm imagining just loads of trouble tickets but I imagine as well that there's ancillary
data supporting data as well and you mentioned that there's lots of it.
Can you talk about the size you typically see, those kinds of things?
Yeah, so our typical customer is doing a quarter of five to 20,000 tickets a month and
we need to be working with companies that are achieving that level of volume A because
the price points are reasonably high and so it's typically the companies that have those
large volumes that are willing to pay for what we do and B because the machine learning
models are built specifically for and on their data and we don't use a common model for
instance across our customers.
So we need a lot of training data for a given customer.
Now this isn't again, this is not pediscale amounts of data.
We're talking sort of tens to hundreds of gigabytes at the per month level per customer.
The data is indeed a lot of human interaction from emails, web forums, even chat and there's
also a lot of metadata.
So what is the value of this customer, what products are they using, how often have they
been emailing so there's a time series component to this as well and we've had to build these
pipelines that are generic enough that we can then apply them to other use cases but
specific enough that they give good enough accuracies that wind up rivaling what humans
can do.
And so oftentimes our goal is to get to not, we don't call it accuracy, we call it matching
capability because oftentimes when a human's labeling something and saying it belongs
to this bucket or this person should answer it or it should have been answered with this
template, they oftentimes can be wrong.
So we want to get all that experience.
We want to get ourselves to that kind of level of quality, let's say.
So it's mostly from a from a featureization perspective, we're doing lots of natural
language processing, getting it to the point of sort of rectangularized data where each
row is a different instance and each column is a set of features and then we have a bunch
of labels of what the answers are.
So we're working almost entirely in a supervised sense where we know from past data to particularly
closed tickets what the actual right answer is quote unquote.
We've got a couple of unsupervised models that we also wind up running where we wind up
discovering for instance that there is a grouping and semantic space of outgoing tickets
that is how agents are responding that don't look like templates that are sanctioned by
the company which means that they're coming up with their own templated responses and
potentially even sharing those with other agents.
So we have a dashboard for instance, we show our customers the one that was running the
support desk of potentially new templates that they can use because obviously if there's
a new issue for instance with a product then agents who are on the ground have to figure
out a way to answer it and if it's a recurring problem within a couple of emails that wind
up essentially having the right answer that they've already pre-formulated.
So that's an unsupervised problem.
And do you see in that last example a feature place for generative types of AI approaches
or is that more do you think of when you hear that is that like the technology you know
looking for chasing the problem kind of thing?
Yeah it's a good question.
We've shied away from the generative component and in fact we make that a big part of our
sales pitch of to say you are a potential client really know the voice that you want to
speak in and speak with your customers and it's who is it for us to come in and say we're
going to auto-generate at the character level you know CNNs or something a bespoke answer
the way that you know Google inbox does well if it gives you five different words sure
all sounds good I'll see you then or how about Friday.
Those are fine but because we're really focused on not just getting results into the hands
of agents where they can actually see in a UI sense within the dashboards they normally
see what our predictions are and consume it in a way that they like to.
We also want to take a lot of these tickets off the table in an automatic sense.
The only way our customers get comfortable with that is if we're basically showing to
them in an offline way here's our accuracies for these types of templates so we're going
to send every now and then somebody says I'm very unhappy with what you've done we're
going to said thanks for your feedback when it should have gone a different path but
we're only going to do that you know 1% of the time at this at this level of false positive
and once we can do that then our customers essentially can turn on a specific macro for
us to auto respond with.
The idea that we're going to auto respond without any agents in the loop to something like
that to potentially I rate customer is pretty challenging.
So we don't certainly want to rule it out but we certainly don't think about ourselves
as producing gendered advances in a bespoke way.
We're just more or less turning all of our problem into multi-class classification problems
of what of the hundreds or potentially thousands of canned responses is the right one to answer
with.
And just so I understand the comment that you made a second ago in terms of sending
out a given response a small percentage of the times are you describing an errors type
of situation or you describing a feature where you're like an exploration type of feature.
Good point.
So we try not to do we there's an explore exploit component to what we do in a multi-armed
bandit sense that's typically not you know expose or a knob that's tunable by our customers.
So that will happen and some of that will happen naturally in the case of auto response
we hold back 10% of the ones that work we know what the answer is or we believe we know
with a certain threshold of confidence and then compare after the fact whether an agent
who wound up now having to see it because we did not a respond gave the same response
we did.
There's an exploration where with the agents job is now to do the exploration implicitly.
No that the one I was pointing out is ones where we are essentially wrong.
And that gets back to the question of the loss function of what does it mean to be wrong?
Right.
If one of the canned responses is I'm so sorry for your loss I will refund your entire
vacation you know in the amount of $10,000 the cost of being wrong of that is very very
high.
But if somebody is mad and says my vacation got ruined because of something you did I
want my 10k back and we say thanks for your feedback it's being wrong on that side is
not nearly as bad as being wrong on the other side of that.
And so we give and empower our customers to basically make the decision about you know
let's do the easy stuff where the cost of being wrong is not a big deal.
But because and that's for the automatic response but for the recommended types of responses
if our first canned responses here's your money back and an agent looks at that and says no
that's crazy the right answer is farther down the list.
They'll select that and that becomes the feedback that our model you know our models went
up getting better as they wind up learning over time.
What are some of the most interesting challenges that you've run into in putting together this
kind of hybrid you know ML plus human solution like in one of the things that pops to my
mind is you know just user experience user interface like are there challenges there
that are interesting or you know what surprised you the most in trying to fill these types
of solutions.
Certainly because this we're getting into the space and the face of agents who do this
all the time when we first started releasing our products we didn't have a good training
program for them.
And so when they would see even though what we thought was an intuitive set of responses
in the form of widgets that would show up on their on their desktop you know they didn't
know how to consume it and they didn't know how to use it as effectively as we thought
they should you know there's all the mundane stuff around UIs like responsiveness and somebody
saying well doesn't look like your products working because now there are no responses
and we'd say well that's because you've already responded and you're bringing up a new
ticket you bring up an old ticket that already has a whole conversation and we're only getting
involved in at least for now in the first part of the conversation what's the first response
you should do okay so then we weren't showing the results and so how can we you know
to modify our widgets so that the agents understand we're not showing it for a purpose it's
not that our system is is broken and then realizing also that many agents wanted parts
of our UI that and UX were generally that doesn't have anything to do with ML so they wanted
keyboard shortcuts because we thought everyone would just click on stuff but high velocity
support desk wanted just to use the keyboard so having to build that in for a set of customers
because they essentially is like the keyboard had some disease or the mouse had a disease
on it they didn't want to touch it getting feedback from the UI itself back into our system
making sure that we're getting the right metrics back making sure that the KPIs that
we're measuring or that we're aligned with the KPIs that our customers wanted I think
one of the hardest things for us and it frankly continues to be a challenge is really
just thinking about how fault tolerant ML needs to happen and again Google going back to
Google inbox for those of you that have used it it makes a couple of suggestions about
how you could respond to an email if you don't want to use those you don't use it so
I would call that a great fault tolerant ML experience and the same thing in a spam
filter within your within your mail system it'll say we think this is spam if it's not
move it over and then later on we'll figure out how not to call these things spam anymore
that are like that that sort of fault tolerance where you're also getting feedback either
implicitly or explicitly is just something we've had to build up over time but I think
more broadly that that kind of approach needs to be built into any AI system in a production
environment unless the AI outputs that you're building are going to be consumed entirely
by machines you need to have some level of understanding of who it is that's going
to be consuming it what are their concerns and how can they give you feedback so that
your models one of getting better over time can you talk a little bit about the algorithms
that you're employing and the the tool chain the pipelines what does all that look like
yeah so we we stay out of what we call the algorithms arms race internally a we're not
really selling the the platforms other data scientists it gives us the freedom to focus
on parts of the of the pipeline that we we find most important all of our algorithmic
sort of learning parts and then prediction parts are built in C++ and then surface
back out into Python which is where the data science team winds up working we have our
own notion of what a pipeline needs to be and the data science team works entirely within
that the confines of what that pipeline ought to be which is some sort of pre filtering
so for instance if a if a ticket is from a voicemail don't predict on it or don't use
it for a build so you get rid of those that have this in this column this value then there's
the data transformation parts of that in the joining across multiple across multiple
datasets if that's needed and then the featureization which we'll often use open source tools
for that in the Python ecosystem pandas is that we use very regularly and then once we
want to realizing that we've created a bottleneck which typically will happen not so much in
time but in ram usage we'll wind up rewriting other people's algorithms or code so that
we create you know a ram efficient pipeline and then once the featureization happens basically
the learning winds up happening in the C++ layer and we've built a whole bunch of hyper
parameter optimizations and feature selection capabilities and then post process capabilities
to get calibrated probabilities out of a multi-class problem so we have a bunch of pieces
that we've been building up that are not in the open world and something that we've decided
not to open source for now that allow us to work efficiently so we think of it as high
velocity data science and building out a template for the first time but then because the
models have to rebuild every single day for every single customer on you know cloud infrastructure
which is not super cheap we needed to make the cost of doing that as small as possible
and what we want up realizing is that open source tools you know that many people use
like the psychic learns and the Tories slash dotes of the world or even Spark ML were vastly
more costly to run even if you could do it in the same amount of time which we think
we're much much faster than most of those tools because of the ram requirements needed
on multiple machines or even a large single machine in Amazon the cost of building a model
just was x percent higher and x being you know in the thousands so having a ram efficient
speed efficient and obviously again getting back to the original conversation about table
six highly accurate set of algorithms which produce the kinds of answers we want that
we could then get into and modify if we needed to was kind of where we went up settling
as where we needed to spend our kind of R&D slash engineering time now one of the areas
that many of the machine learning platform companies have focused on is trying to close
this gap between data science and production yep and in essence eliminate the hey I've got
this model that kind of works though it over the wall to developers and have it implemented
and it sounds like you guys have maybe embraced that and you're using that as a way to build
out the models in C++ for presumably for performance are there ways that you've been compensated
for that in terms of automation tooling or do you just accept that or you know even you
know we just have the you know the best people on both sides of that fence that can deal
with the you know the existence of the gap like how do you maintain a level of efficiency
and innovation in terms of the development pipeline not the machine learning pipeline
so that it all works for you yeah so there's definitely this separation of concerns which
again is both an organizational one and then is also a computational one to the level
where we think we often talk about what we call the organizational API of who within
this stack is the customer of who and so for instance the people who are the sort of
core ML and algorithms folks in the company are working in C++ and surfacing the great
results back into Python layer their customer is the data science team the customer of
the data science team is the people working on our architecture who have to maintain you
know this the scalable robust infrastructure and you know their customers are the people
working in the middle where and their customers are the ones who are in the UI and so each
of them have a set of contracts of what it is that each part of that stack is looking
for and and how in fact they're supposed to engage with each other and that's become
very very helpful for us because you know what you find is that when you put somebody in
a box they figure out a way to innovate very highly within that box so if there is a very
strong contract of what data is expected to come in and what data is expected to come
out and everything in between there is really up to you to decide how to do this well and
efficiently that's where for instance our data science team and implementation team will
wind up working and building out a new template they can work at their laptop or glorified
laptop level on a toy data set get some confidence that the pipeline is working offline accuracies
look good and the whole thing is going to work and they once they're comfortable with
that they literally are just pushing a new version of a of a Docker image into our registry
which then farther upstream from something they ever have to think about from a production
sense once a new build winds up getting kicked off for that customer for that type of template
the new the new image will just get pulled and it'll just get built with the config file
for that customer and so the data science team can wind up working within their confines
and of course we have a whole testing suite to make sure that if they build something
new they're not going to break something downstream from them gain confidence in that and
then they're literally just pushing the results of what they're doing on a semi weekly
basis into the Docker registry that becomes the latest template for let's say triage and
then all the customers in production are automatically migrated to that so having the
data science team be able to push stuff into production without having to be on the
op side of things nor have to think about the architecture has a has really freed us
up in great ways I think to innovate and likewise when they need a new beller and or whistle
from the the core algorithm folks because they say this part of our entire a build chain
is really inefficient they can ask of the people working on that to improve it and they
go through their own testing suite and I think we're at 300,000 regression tests in our
core ML we're also testing against every open source algorithm on customer data to make
sure that we're staying as efficient or more on all these different axes before we
cut a release then the data science team can just pull essentially Python egg from our
registry and use that in their system so having those separations has been great obviously
if you're abstracting everybody from what the ends use cases are there can be a huge danger
but it's the job of people like myself to make sure that everyone is is focused and innovating
towards the right set of goals. Oh great great I'm glad that Docker came up you guys
published a you publish and maintain a set of Docker images for data science tools I've
come across that my impression is that in general Docker adoption within the data science
machine learning community is not particularly high is that years as well.
Certainly haven't heard of many other companies using it in the ways that we are but it
seems like such a natural way to literally containerize and abstract the work of one
part of an organization from the other so long as they you know that container will respond
with a slash build predict endpoint you know feedback endpoint etc in the way that everyone
expects it to. I think that's a I think that's a wonderful way to do abstraction so and
then obviously it also helps you wind up achieving scale because for us scale is not you know
can we serve you know a billion of our customers with the same app it's instead well we've
got a new customer that we just spin up more containers to do the builds and the predicts
for that customer and if we need more compute capability that's elastically scaling for
us for free on top of Amazon. So I think of a very natural way to separate concerns you
know from a stack perspective and also a very natural way to do what is for a company
that's serving lots and lots of customers a very embarrassingly parallel type of of
compute. Interesting I got into a conversation on Twitter or Reddit or someplace where
someone was kind of griping about just the dependency hell with Python and pandas and trying
to come to terms with managing different versions of you know different tooling versions
and things like that and I suggested I might have even pointed to your docker repository
and the response was now I want to make this simpler not more complex and obviously you
find it to be simpler can you give folks that aren't familiar with docker and containers
like your 30 second you know docker for data science pitch and where they can learn
more about it. Yeah so docker is a way of basically explicitly specifying what not
only your let's say Python requirements are which you can do with a simple file but
also what the entire OS shall be for running whatever scripts you're going to need and
once you build that and you can confidence that that image is doing what it ought to you
can essentially very rapidly turn a container on that is the almost instant instantiation
of that entire OS plus that script and all of the dependencies built inside of that and
you can hand somebody a link to the docker hub registry or if you maintain your own
private registry explicit URI to that explicit version of that explicit image and more
less guarantee that when they run that with whatever data is contained inside of that
or whatever will be pulled over so long as it's the same you'll get the same answer out.
So I tend to think from a data science workflow and then getting back to you know just
doing science more generally docker is a very nice framework for reproducibility and
so the idea that now I'm not I don't have to share a machine with you or an Amazon
machine image with you I'm just handing you effectively a docker file and says if
you run this you're going to want to get the same answer that I got but again because
I don't think doing the types of work that we do and wise and in some cases what we do
on the science side of things as the you know the final result is not what comes out
of the docker image or container it's not okay here's a report of what my ROC curve is
going to be my false positive versus false negative curve and then let me write a paper
about that it needs to be for us at least in a production environment just now I've produced
a prediction that now needs to get consumed by something that's farther downstream.
So docker is quite nice in that sense as well because you can also now connect docker
containers explicitly using something like a docker compose on there's many other tools
out there as well so that containers talk to other containers and you allow each container
to have again its own separated concern from the other ones but still pull the results
and push results to the other the other ones around in addition some containers can just
contain data and you can build databases around that data so now it allows you to build
up a very lightweight version of what might be your entire stack and do this in a way
that's programmable so we found that to be incredibly useful for testing purposes.
So as your GitHub the place that someone can go to learn more about what you're doing
or. Yeah so we've got a public docker registry that you can go to the docker registry
in search for YZO or you can go to GitHub slash YZO and see our other public projects
that we've pushed out so there's one around docker and data science which in that case
because we're not releasing any of our internal tools we're basically building up a container
with open source tools that we find are really useful for doing lots of different types
of data science. The other major project which we have up there in GitHub that's open
is something we call Paratax which started as just sort of a weekend hack from one of
our engineers Damian Eeds who wanted to see what it would be like to read data from
disk in parallel just to see what kind of speedups you could wind up getting and it turns
out pretty much every open source tool out there doesn't read in parallel and the ones
that do are explicitly parallelized like over multiple machines but if you just made
multiple use of the multi-core environment how well would you get and we want to getting
100,000 x speedups over some of the other tools that are out there and importantly also
use vastly less memory that Paratax is not yet in our production environment but we thought
it would be a good example of kind of showing off the philosophies that we try to adhere to
within the company of creating efficiencies that isn't just the one thing like around accuracy
but you know around how fast can you read data and how big is your model on disk all these
other aspects of what it means to do machine learning that has nothing to do with the algorithm
once you're happy and you've reached some level of plateau with the algorithm accuracy all
that you're left to do is optimize all these other pieces of that pipeline and so a lot of our
engineering over the last year in particular has moved away from just optimizing accuracy to
you know things like creating interpretability around the models that we build
making the models smaller on disk making the other parts of the featureization pipeline
be more ram-efficient and once you start sort of playing whack-a-mole with let's just say ram usage
you want to find really interesting parts of your entire pipeline that very few people
want to talk about just you know again reading data which is should be the easiest part of your
entire tool chain is vastly inefficient and you know whacking that mole turns out you save a whole
bunch of Amazon costs because now you need a smaller ram machine that's great that's great
you mentioned interpretability have you spent a lot of time working on that and what were the
drivers for that we have spent a lot of time on that you know sort of one of what we think of
as our trade secret one of our trade secrets around getting back to the question of UI and UX
for end users we we were asked often at least in the early days well why are you getting the answer
that you're getting and you can't say well you know it's a thousand dimensional feature space
and there's covariance between all of these and you know the model importance is over the entire
thing you know says that this is the most important feature I don't know why we said for this one
what the answer is but that answer is what what's called in in the financial services world reason
codes turns out to be really important some some some places it's actually regulatory required
that you tell somebody why you got the answer that you got even if it's a machine learning black box
and so some of our early R&D effort was around how to make at the instance level so an individual
prediction level how do we make these models interpretable by saying these are the important
features and these are what's driving this specific prediction so as an example if you're
working on customer churn and you want to predict somebody going to churn in 90 days from now it's
a use case that we've also used on our platform but not something we go to market with necessarily
two customers can have an identical probability of churning but one of them may be churning because
they haven't really used your product and they haven't done the training videos and the other one
may be churning because there's a high probability they're going to go bankrupt in the first case
that's something you can do something about and the second case you know you're kind of SOL
and so even though they're identical and what their predictions are and their and their
probabilities of those predictions coming to pass one is actionable and one isn't and so it's
not just people gaining kind of a warm fuzzy about why did you get these predictions and does it
jive with my you know feeling about why that that could be okay which is critically important
it also then starts tying into next best action and because I think again a important part of
machine learning and production is to drive value if the value isn't the prediction in of itself
then the prediction in of itself is really just there to drive the next thing that happens
and so next best action is heavily coupled with you know the the importance is around which
features are driving the prediction okay um you mentioned value and that's a great transition
to one of the things that I really wanted to dig into with you and that is the this blog post
that you wrote about cost optimized AI that I've incidentally mentioned on the podcast a couple
of times do you have time to go into that of course so I guess the first you know it's it's actually
come up several times in our conversation already this notion of cost and value but was there a
specific thing that prompted you to I really got to write this down now would drove that
so that was a bit of an intellectual journey I was wondering to be really frank why the hell are
all these companies building these neuromorphic chips and all these specialized hardware to do deep
learning where where you know because I think much of the world's data and much of the world's
value in data is tied up and I'll use the word or quote unquote small data or medium data
non massive scale Google scale data Facebook scale data I was wondering why all these people
are starting to build these very specialized pieces of hardware when you know deep learning I
think magnanimously one could say or charitable is incredibly good at a large number of of inference
problems but not very good at a large probably even larger space of inference problems that may be
changing over time as people start applying it to these new realms but the place where deep learning
winds up shining is in really large amounts of data right because effectively what you're doing
is turning millions or even billions of knobs to optimize a model and to do that credibly without
overfitting when it's lots and lots of data so so I want to ask you this question of myself why are
people doing this and why isn't what we already have out there and even just the GPU land good
enough and if you look at the the plot which I have in my blog post of the efficiency sort of
gigaflops per watt right which is something of if I put this amount of energy in which has this
amount of cost how many computations can I get out that efficiency has been growing over time
but it's nowhere near what some of these other chips or the specialized pieces of hardware can do
for these specific types of calculations and those themselves are nowhere near what the human
brain can do right which is of order if I remember right about 10 to the five gigaflops per watt
so your your brain is a you know 30 watt supercomputer unrivaled at least for now by anything
else it's out there and anything that else is out there is likely going to take megawatts or
hundreds of megawatts to get anywhere close the computational capability incidentally I don't know
if you've come across it but there there's a parallel to using DNA for storage and the the storage
density per per unit energy is incredible in DNA yeah something like you know the the drop of
of uh you know in a teaspoon or something it can take all the world's data as it's it's
it's incredible um so so getting back to this you know that's an obvious that's an obvious one
and I started thinking about it when alpha go um had uh it's big um set of results uh the national
or international championships and you want to looking at the computational capability that it took
to win those um those competitions it's just huge thousands and thousands of computers thousands
and thousands of GPUs the amount of power required there uh was several orders of magnitude larger
than what was going on in uh you know the the the champions head that they was playing against
so I was thinking about that sort of vast gulf and I wound up realizing that the companies that
are pushing towards these specialized pieces of hardware is because they realized that uh for a
given amount of time and a given amount of data because these algorithms are all basically
saturating on near perfect answers uh the only thing left to do is to get more uh energy
efficient um machine learning uh for building and and that the step after energy efficiency when
it comes down to it is really cost efficiency um and and so my my take away on on that part was
that people are building these chips because that's sort of the last frontier of squeaking out
and eaking out the last amount of dollars uh coming out of the system for the number of dollars
going into the system um and then it's taking a step back from that it went up realizing that uh
or or at least realizing for myself it's probably obvious to most out there that because machine
learning is optimization that you're a good optimizer will find the optimal answer by definition
that if you're not writing down your uh your the function that you're trying to optimize um to get
a minimum of or maximum of uh in terms that actually matter then you're creating by definition a sub sub
optimal answer or system and now that system doesn't just involve you know as my algorithm
more optimal at getting an accuracy better than yours but now translating the accuracy into
well let's go back to our loss function what's the cost of being wrong you know and saying this
thing is uh a and this thing is b um translating that to a business term is something that's critical
and almost everybody knows that that's uh important but then you want to realizing well um if I'm
going to build a model what if it takes me 12 days to build one of these models right to get an
accuracy which is only epsilon better than one that takes me to 10 seconds uh and what if
you know I can build a model that may take 12 days and the accuracy is much higher than one
took me less time but uh the labor costs are very different so I had to spend more data science time
building one versus the other and what about the opportunity costs of those data scientists not
working on another problem in your business that may be more important and when you wind up
couching the problem that way um you get out of just again focused on accuracy in the algorithm
to what is my cost of doing the entire pipeline and now the entire pipeline isn't just
running a uh a machine learning model in production for this specific use case
but how does that couple to all the other things you're doing in your business um are you hiring a
data science team to do this and then paying pensions or you're going to do a third party to do
this and just write a check one time um and then you know where the societal benefits of all this
and you know it becomes unwieldy at some point if you're actually being very honest about what's
the cost of doing this but at least if people I just wanted people to start thinking about as we
started thinking about within our company that accuracy is the table stakes and let's assume that
you all have your good algorithm that's going to do well is it going to have strong scaling
properties so that if you needed to get the model built uh you know an x amount of time that you
could just have n number of machines that get you x divided by n amount of time on the clock
because maybe you need that model built very quickly very often um and then you know questions
around the pipeline and and RAM usage and AWS costs in the end as a as a small uh start up
when you start getting down to the breast tax of what's our revenue um and what's our cost of
good soul was our cogs the cogs component is really what is it cost to build a model and predict
and until we were able to boil down the fact that the cost per prediction for one of our customers
is x and we're going to be making x times some number um you know everything else is sort of move
if you're losing you know every time you make a prediction effectively hand over fist
uh then you've got something wrong that's unsustainable so i i started thinking about it as we
were going through the exercise of what's our cost of doing business and the cost of doing business
is running an ai system in a cloud with real customers right um and the labor part we can get
but all the other pieces they're in the end there's an amazon bill and because we put it all inside
of amazon you know and we know how much money we're taking in we can see how those two things
relate to each other so you you started with the question and and kind of ran through the thought
exercise what's next there whether it's you or someone else that does it do you do you see this
evolving or uh you know co-evolving with someone else thinking about you know analytical frameworks
for thinking about this or you know tools you know whether that's a spreadsheet or um you know
it almost lends itself to machine learning algorithm trying to figure out how to deploy resources
to you know do the machine learning yeah it's a great question and you know one of the nice
things about blog posts is you admit it out to the world and you hope somebody runs you hope somebody
runs with it um it's been helpful in focusing for me in my own thoughts and as we drive those
sorts of efficiencies in in our company and then again more broadly um you know in doing science
doing astrophysics uh choosing the right tools um choosing the right skill sets and people
choosing the right problems to work on or not work on those those are very obvious uh sort of
outcomes from me having thought about it and framed it that way one of the challenges is and I
think people may wind up being able to pick uh pieces up of this uh and work with it is coming up
with articulations of um essentially what is the conversion term uh between that item in the
in the entire optimization uh equation and dollars so one I'll throw out there that I don't
know the answer to is uh what's the dollar value of interpretability um and once somebody starts
getting some handle on that then optimization takes its wonderful you know uh told or or approach
uh or at least shall lead to a great outcome which is you know once you once you can really put a
dollar cost to all these different pieces then I think you can do a real honest to goodness
optimization so I know what the dollar costs for instance of needing a ram machine of this size
versus that size on Amazon okay great um but what's the real dollar cost of and can I know
what how much time it's going to take for a data science team to build up this template
from scratch and then push that into production uh and how many people do I really need on that
is it good to have one data scientist or multiple ones right um and so all those things I think wind
up becoming really interesting over time uh ones people wind up potentially even wind up agreeing
upon what this equation what's in ban for this equation and what's not obviously out of scope is
uh you know what's the probability that you know my machine learning algorithm is going to start
world world three right probably not worth talking about right right something smaller than that
smaller in scope at the company level um is probably worth starting to get some clear
understanding around so now we've maybe come back full circle to graduate students sounds like
there are a lot of interesting uh research parts research questions in here for PhD student or
something yeah I think in the you know for for those in computer science thinking about systems
optimization uh who are also interested in machine learning this is um hopefully some fertile
ground to start to start thinking um the other statement which hopefully is clear from what we've
been talking about is that doing machine learning for machine learning sake really doesn't make sense
it's it's probably the last thing you want to do if somebody hands you data uh you do it because
you have to do it it's painful and to run it in a production environment um given all the crazy
bugaboo's that um many many people have talked about there's a great paper from the folks at Google
by Dee Scully is the first author called machine learning is the high interest credit card of
technical debt um my last interview yes I'm not surprised it's an important paper it's got I think
no equations in it but it's a whole bunch of important lessons about how machine learning
systems tend to be very different than typical engineering systems um so so so there's a lot in
there uh to get right a lot of bugaboo's there that people who haven't done this before
tend to get wrong but what you wind up realizing is that once you realize machine learning or
or more broadly AI is the right set of tools to apply to the problem uh that you have
what you'll often wind up finding I think is at the graduate student level um in terms of graduate
student projects they can be working on is that it's still very much early days for the for the
types of algorithms pipelines etc in dealing with real world data um I've often said to my my
colleagues on campus that um real data is not doing uh sentiment analysis on twitter right and yet
many many many papers saying my scaling algorithms but then your scaling algorithm uh will wind up
using that as a toy data set the real world is not uh toy data sets yes we need to have benchmark
data to have a lingua franca of who's doing better in these different axes right but when you
wind up getting exposed to real questions uh you wind up realizing that all the stuff that people
know out there in the academic world that people write about and do kegel blog posts about are not
what you really need if you're being truly honest about what needs to get optimized hmm that's
great so how does one manage being ctl for you know the high growth startup and you know being
a astrophysics professor it's becoming increasingly common to see folks particularly in the machine
learning community have uh professor real posts and do um academic or do uh you know work in these
research labs and things like that but yeah so i i uh been on a what's called an industry leave
for for a number of years um and so it's allowed me to have also that separation of concern so
so uh not getting not getting paid by the university and i have been uh health care has made it
easier for me to spend all my time as need be on the uh on the on the company um while still
maintaining uh the kinds of links that i think are important um as i you know start thinking about
coming back into the university setting um obviously a number of things i've picked up and
management thing you know ideas and and and capabilities um and then also thinking about how to
evaluate uh new technologies when is it appropriate to bring this into your toolkit or when is
it appropriate to wait um those become really practical uh uses that you know i can take i can take
with me but then also again recognizing that as i was saying before there's a whole uh interesting
set of problems out there that are not being addressed by um pure academic R&D research
means that i can also start uh you know looking for those white spaces to actually do some
pure academic research around those um i'm particularly interested in questions around
interpretability and how you put metrics on interpretability um and that's something that i think
i benefit from having come from uh you know felt the pain of customers asking about that
right um that uh you know at least have a fresh lens on that um doesn't mean i'll solve any of
those problems but at least i'll i'll have a direction of potential interest um so it's uh it's
certainly a challenge but i think uh despite the challenges the the benefits to both myself
the company and the university and my students at the university uh are are far up way all the
gray hairs that i wind up getting i'm i'm teaching a uh data science um class essentially a
python ecosystem data science class right now it's uh aimed at graduate students
and the things that i've seen in the uh in the business world have really helped me
hone that that class and i'm directly giving back to the students uh from that from those
learnings and is that a MOOC or is that available only to uh it is not a MOOC um other incarnations of
that class that i've done in the past uh are probably online somewhere in the iTunes sphere or
elsewhere um that can also be found on github all the material uh and then we'll hopefully post
some of the of the lectures online as well okay great uh so if folks want to learn more about the
the company or get in touch with you what's what are the best ways for them to find you guys um
easiest is uh drop me an email um and you can find that by googling around
uh um so i'll i'll add that as a little bit of a bar that if you really fun to find me you'll have
to do a little bit of work uh you can tweet at me so that's prof jsp uh it's my twitter handle
and uh and we can do a direct message um maybe that's probably the best way to to get at me
great great uh well i really appreciate you taking the time it's great to finally meet you
in person and uh i really enjoy the conversation i think folks will will enjoy it as well and get
a lot out of it great well thanks so much thanks for your interest great thanks
all right everyone that's it for today's interview thanks so much for listening
i haven't asked you all to do this in a while but if you enjoyed this episode of the show
please please please do these two things first share it with your friends on twitter
facebook good old email or however you share cool things with your friends second reach out
let me know how you like the show who you'd like to hear on it and how i can make it better for you
you can reach me on twitter at at twimmel ai and at sam charrington and you can email me directly
from the contact page on the twimmel ai.com site thank you so much for your support and catch you next time
you
