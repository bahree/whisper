Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.
Alright everyone, I am here with Ben Green. Ben is a PhD candidate in Applied Math at Harvard
and Affiliate at the Berkman Client Center for Internet and Society, also at Harvard,
and a research fellow at the AINow Institute at NYU. Ben is joining me for our continued
conversations coming out of the 33rd NURBS conference here in Vancouver. Ben, welcome to the
Tumel AI Podcast. Thanks so much for having me. I'm excited for our conversation.
I am as well. So, as I just said, your degrees are going to be very soon in Applied Math,
but you have applied that application of math to largely an exploration of the intersection of
technology, AI, and social good. Tell us a little bit more about your background.
Yeah, so my background is primarily computer science and data science, but I always,
even going back to my undergraduate years, had a really strong interest in urban policy
and urban government and urban planning. And so, as I started out the PhD, a lot of my emphasis
was on how can we use, you know, the tools of artificial intelligence and data science to
improve society and participated in the UChicago data science for social good program and did some
other work really thinking about how can I as a data scientist contribute to society? I spent
a year working for the city of Boston as a data scientist in the middle of my PhD. But in the
course of doing that work from originally from a more technical perspective, increasingly also
came to see the broader governance political social questions that were at the heart of these
technological endeavors and often were overlooked or ignored and also played a significant role in
shaping the impacts of these systems in some of the projects that I worked on, whether it was
building machine learning algorithms in the city of Boston or for the city of Memphis to help them
prioritize various types of inspections and investments. Often what I found was that the key
factor that shaped the impacts was not the technology itself, but the broader policy,
government, political environment in which that technology was being embedded. And so that shaped
a lot of my thinking on how to integrate technology into these broader social contexts and how to
think about the ways in which what are very well-intentioned efforts to use technology for good can
overlook some key factors and end up failing to achieve those those social goals. And your affiliation
with the applied math department is that I imagine there are several places that you could have
plugged in your research interests at a place like Harvard. Is that the selection of a particular
advisor or is applied math? How does applied math fit into that broader research agenda?
Yeah, so definitely the really the computer science perspective which is sort of where my
more more day-to-day home is at Harvard is both very much a lot of my work is sort of with is all
sort of within the realm of computer science both thinking about those tools in the past more
work on building those building AI systems for various social applications, running different
types of more recently. I've been running a variety of human computer interaction audits to
understand how people interact with algorithms in practice. And but also my work is very much
than about stepping outside of the typical modes of thinking within the field bringing in other
perspectives from science and technology studies and philosophy and government and thinking about
what and law and what those perspectives can do to inform our understanding of artificial
intelligence and its impacts and how to develop it. So my work is definitely very multi-disciplinary
and I've worked with you know even within Harvard at many different departments and with many
different people but the core focus has always been on the application of data and algorithms
in society. So here at NERPS you're presenting a paper called Good Isn't Good Enough
at the AI for Social Good Workshop. Tell us a little bit about the paper and what your objectives
are there. Yeah so this paper is so it's just a it's a short workshop paper so definitely not
a fully in-depth discussion of these topics but it really emerged out of my own experiences
and some of the other broader examples I was seeing of these efforts to do social good
that were well intentioned but often sort of not thinking of the full of the full picture of
what it actually means to do good. And recognizing in particular that efforts to do any sort of
technology for social good are about somehow shaping society somehow changing society for the better
and that's of course an incredibly complex topic and the two things that I really point out
that I have seen missing in the majority of efforts to use AI for social good is first what I
would think of as a normative theory a sort of grounded definition of what good actually means
typically most groups will talk about AI for social good and the social good part is sort of
taken for granted what that might mean but of course as you can sort of if you step out of the AI
space and just think about our broader social political world there are many different definitions
of what's good and many nuances within that type of debate so there was often a lack of sort of
a normative discussion about what are we trying to accomplish and then the second part was a lack of
what I would call a theory of change a theory or sort of a grounded justification for how the
particular technological approach being taken is an effective means to getting to the social good
and whatever that end may be and so a lot of the time even in cases where perhaps the
an important problem is recognize the particular mode in which technologists go about trying to
solve that problem may not be the most effective way of achieving that end if you take the social good
or the social impact as the as the ultimate goal here and think about the technology as a means
to achieving that goal and so both of those things are I think pretty significant challenges
certainly one not ones that cannot be overcome but the types of things that really need to be
incorporated into these into these discussions I like to think about it and in some sense this is
really what the goal of the paper is to do is I like to think about it in terms of rigor
right that when we talk about AI for social good what we're actually doing is really expanding
what we're trying to accomplish with an AI system right we're not simply saying we want to build
a tool that can efficiently predict this or you know efficiently analyze this data but we're
trying to build a tool so that it can achieve or advance this social outcome and what I'm trying
to bring as a sense of here are things that we're overlooking failing in many cases to think about
and trying to frame that as a lack of rigor in these efforts that we're actually not thinking
about factors that are incredibly important in shaping those outcomes and that to the same extent
that we would never accept a system that hadn't done an analysis on some sort of hold out test
dataset we also shouldn't be accepting systems for integration into societal contacts that also
hasn't done some sort of analysis of well what will the impacts of this system be in practice how
is it actually going to affect the system that we're trying to impact here and bringing in more of
those types of sociotechnical analyses into what it means to build and evaluate these types of
systems. The way you've framed those two components establishing kind of a normative definition
for good and theoretical framework for understanding the implications of a particular technology
sound like ideals. Are you holding these up as hurdles that should be overcome in order to
before folks embark on their AI for social good projects or more like conversations that we
need to be having? Yeah, I think it's a bit of both. I wouldn't view those as being in
contrast with one and other. It's definitely I would say centrally about conversations. There's
certainly not a single answer to any of these things both within the field and without the field
philosophers and activists have been debating questions about what is good and how do you get there
for forever essentially? Are there examples from other fields where good or even
something you know as broad as good has been successfully defined and used as a foundation for
kind of further work? What would this be? Ground that we're breaking in in AI? So it's definitely not
ground that's being broken in AI. I would say again the point is not that there are other fields.
It's less about there are other fields that have come up with the definition of good, but there are
other fields that have figured out how to incorporate these sorts of conversations into what it means
to do this practice. So one of the places that I've been looking and this is a paper that I just
finish and will be published at the 2020 Fat Star Conference Fairness Accountability and Transparency,
my co-author and I we look to the law and we look to debates in the law in the 20th century
about how they moved from a system that was very formalistic that said the law has these rules.
We can just deduce from these rules how do we get to the right outcome? And there were a number
of challenges to the law a shift from what was known as legal formalism to what was known as
legal realism that said legal practitioners whether lawyers or judges or scholars need to figure out
how to reason about the law in terms of the impacts that it's actually having in real people's
lives. It's not just about did you apply the principle of liberty correctly, but how will
this application of the law affect people's liberty in their real lives? That's what the
realism is doing here. And so this paper is about shifting from what we might call algorithmic
formalism to a mode of algorithmic realism, recognizing that the law is a place while typically
viewed as being in conflict with technology. And there certainly are conflicts in many ways. The
law is another mechanism for having a structure to a mechanism to structure our decision making
and to structure the rules by which we allocate decisions and resources. And in many ways that's
what AI want applied to city governments or healthcare systems or any number of different
contexts are doing right. It's providing a way to distribute to make decisions to manage
discretion. And so I think that by taking for example some of those lessons of the law which
have always been thinking about how do we understand or not always but have for many years been
thinking about how do we emphasize the the quality of illegal analysis not in terms of it's sort of
whether it not it fits the ideal theory or the principles within ideal theory but how does
it affect society. And I think in many ways that's where we should be moving to in these contexts
of building AI for society right. The goal is not to just build these systems for these systems
own sake but because we're trying to advance some sort of social benefit or social outcome.
And so is the paper is it prescriptive in the sense that it tells a practitioner who buys into
this vision what to do next? Yeah absolutely I mean we're certainly not prescriptive as saying
you know here's all of the answers it's a broad a broad shift but yeah so we we provide and I
think you know and these sort of things that orient we call them orientations that we're putting
forward are very much also aligned with what I recommend at the end of this AI for social good paper
here in the rips but you know one is really thinking about what are those normative commitments
that you want to embody within your application what might you be taking for granted as things that
are good or not good if you're not interrogating those things. How do you bring in a more interdisciplinary
approach recognizing the variety of different perspectives that might come to bear on that
system or the ways in which the interactions outside of what we would think of as the algorithm
itself can affect the impacts of that system so expanding beyond what are the technical
specifications in terms of accuracy and efficiency but what happens when you put this into the middle
of a healthcare system and you need doctors to be working with it in practice. How does you know
what what happens there and that's just as central and then really engaging with the context of
the problem and ultimately from what what we call an approach of agnosticism right recognizing
that the AI can be built in many different ways and that the AI is just one way of approaching
a social problem that if the goal ultimately is not about building the system the goal is about
advancing some social outcome that requires a different sort of stance of understanding okay well
what type of system should I build what is the best contribution that I as an engineer can make
into this problem and how might that be different from building the sophisticated system that
sounds really cool from an engineering perspective but may not actually be the best data analytic
tool for pushing the ball forward on this problem so you know there's a variety of questions
and prompts and sort of things like that to start start thinking about and then there are
you know getting at what that looks like in practice at every single one of those decision points
is certainly an area for further work and one that other people also are thinking about a lot.
A big part of your work is kind of thinking about and providing a framework for thinking about
kind of unintended consequences and the application of technology to social good
are there you know specific examples of these that are most salient for you.
Yeah so you know I think there are a couple I would say the one that I always come back to because
this is the focus of my dissertation work are algorithms in the criminal justice system things
like predictive policing and risk assessments and these are systems that often are deployed
under the idea that these are effective ways of achieving some form some type of criminal
justice reform right of taking this system that is broken in a number of ways and helping to
improve it and I think that that's an example of where thinking about what the typical sort of
we can build an algorithm for this problem approach where that can go wrong because I think
that ultimately what we're doing with these systems is not just improving a particular
aspect of the system with these algorithms but actually affecting the broader landscape of what
it means to do reform what sorts of shifts that we're taking. So the example that comes to mind
I'm sure for many who follow this base is like the compass system that was written about
extensively in a public overport maybe a couple of years ago now you're in 2016 anything.
Oh, 2016, it's been a while. Yeah but that's a system that is being you know developed and
promoted by a you know commercial entity and it's not at all clear that their goals are you know
reform or social good for that matter you know is that the example that comes to mind for you
or are there others and you know certainly there are folks that are you know whose primary goal is
you know social good you know that is not necessarily one of them. Yeah I mean it definitely
varies the the landscape is sort of complex and they're definitely just companies trying to
make money out of this and then I think also definitely computer scientists and other folks who
are genuinely excited about these tools as as a way of improving the system. You know so two two
different very different types of unintended consequences that play out there that I have looked
at in my research. One being again going back to this human computer interaction component
where a lot of my work and other folks have done similar work is looking at okay well you have
this system that seems like it's able to make these decisions what happens when you actually put
it into the context are people using these predictions in the way that you would expect and what
what I and others have found is that people can in can inject their own different types of biases
in terms of how they respond to these systems or they might ignore the recommendations or only
use the recommendations in an unexpected and sort of particular way that has you know that
benefits one group versus another so that's a particular type of other specific examples of that
that come to mind. Yeah so so in my work I've looked at how this is lay people not judges but how
lay people can be more likely to be influenced by higher recommendations of risk from an algorithm
when that defendant is black compared to when that defendant is white so they might be more
susceptible to using it in certain directions in certain cases so they'll have a higher they'll
be pulled in a higher risk prediction for black defendants in a lower risk prediction for white
defendants and then others like me being the the groups that you studied or the research that
you're referring to demonstrates that that folks are more willing to defer to the authority of
an algorithm that they don't understand in the case of a black defendant than a white defendant
is that the essence of it? Yeah so it's that they they differ in different ways depending on
the defendants right so if essentially if the algorithm is telling you to increase your score
you're more likely to follow that recommendation for black if the defendant is black and if the
algorithm is telling you to decrease your score you're more likely to follow that if the defendant is
white so what you know through the point and sort of the you know how this this is this clear
example of how we might think about the system we analyze we can audit the system on its own terms
a great deal but what we're actually doing is incorporating this AI system into a very complex
social political government context and so how it actually gets used in that context may lead it to
have very different impacts even just in terms of the decisions that are made then we might expect
from doing a analysis of the algorithms recommendations on their own in terms of accuracy and false
positives and those sorts of things so you know this is adding even just more dimensions to something
like what the pro-publica article was pointing out in terms of disparate false positive predictions
and saying e above and beyond those types of analyses we can also study how these algorithms will
affect decision-making when you put them closer or in their in their real context also makes me think
of the use of facial recognition technology by law enforcement groups Amazon has been central to
many aspects of this conversation and when I talk to them they defer to you know documenting
proper use of these systems you're suggesting that improper use of of these systems is in fact
like systematic and so maybe it's not enough to just point to the documentation is that a conclusion
that you're making or I would say yeah I think I think you know that's a useful framework for
thinking about some of these systems and facial recognition is a good example here there are questions
of both is the is the actual use proper right so do we get proper improper use in practice so that's
one way of breakdown is that what the we get improper use in practice the other would be that
proper use would itself be troubling right and I think that many of the systems I would I think
there are a lot of systems facial recognition being one of them that proper use is itself troubling
right so and similarly with with risk assessments I would make the same argument that there are
definitely breakdowns that can happen in practice things like the types of bias human biases I was
talking about a minute ago but then there's also the question of you know what happens if the system
is perfect right so with facial recognition there's been great work showing that these systems
are flawed and biased in a variety of ways but the answer there is not to say well we just need to
have proper use and proper systems that are perfectly accurate right because the way that facial
recognition systems are primarily being used is you know law enforcement and corporate surveillance
right so having a system that just works better and is better able to document people is not necessarily
the outcome that we actually would want and that's a place where the proper use is itself a problem
and because why because their perfection and successful use promotes further surveillance and
that and of itself is kind of taken as negative or because there are specific outcomes that are
that you predict or see based on their their use yeah it's definitely that it's you know it's
about thinking again not it's about taking the the system like facial recognition in the broader
context and how it gets used right so if the primary use of these tools are you know think about
law enforcement being able to have better records of where everyone is going in the city more
ability to track people and that to me and to many others as an incredibly dangerous prospect the
idea that you are fully tracked just by stepping foot into you know downtown Vancouver where we are
right now right and of course that type of surveillance typically has the the most significant harms
for minorities in the lower and lower classes and so the idea that we would just want to make those
systems more accurate and more technically sound is only enhancing the ability for those broader
types of systems to be developed so I think that thinking through I think it's really important
for folks who are trying to think about the social consequences of AI to sort of think about it
in this just in this way of you know where problems occurring because of improper use and where
would proper use itself be bad and I think that's a really helpful heuristic for thinking through
what types of challenges we want to make to these systems so you know in my in my work on risk
assessments there are all sorts of improper use critiques to make or sort of improper engineering
critiques to make about flaws in the training data or flaws in the human interactions but then
there are also critiques we made of even a perfect system what itself be bad right and we need to
we need to be able to think about that relationship between those different types of harms and
different types of breakdowns and I think that doing that can inform a lot of both our understanding
of these systems but also our work as computer scientists to understand these systems and actually
understand what are the types of flaws that we should be actively working to fix and what are the
types of flaws that can prompt us to do a larger process of reimagining what types of systems
we're building reimagining what problems we're choosing to work on in the first place and so
this is you know and this comes back to you know something I talk about that comes up a lot
in my conversations I am often sort of advocating for a position of what I would call a political
orientation among computer scientists about thinking about these types of questions that's
obviously an uncomfortable thing as a scientist and an engineer who has been sort of taught
that you are to work from an objective remove and a position of neutrality and I think one of the
key things in sort of thinking about how to do that is to understand what we mean when we talk
about objectivity and to think about how there are many types of decisions that sort of fly
under the radar of what we would consider objective and that we can play with those right so when I
say you know political the point is not to make up data or to make up results that would advance
an empirical finding that you would like to show but to think about what types of problems are you
choosing to work on in the first place who are you choosing as your domain experts who are the
partners that you're trying to work with and build systems for those are all ultimately incredibly
what I would call political questions right that choosing choosing a problem to work on
is in many cases you know that's projecting a vision of a good society or a vision of where
society has gone wrong and then you know building systems within that doesn't necessarily
require rejecting the typical modes that we would use when building and evaluating AI systems
but that's one way that we can think about stepping back from our typical process and recognizing
the decisions that we're making often without realizing it about what we're going to work on
and who we're going to work with but those can be incredibly consequential and simply choosing a
slightly different problem or a slightly different partner can have really big consequences and
can very much shape what the type of system you're building is or the types of impacts that you're
going to have so we've maybe straight a little bit from kind of establishing a basis for what we
mean by good or social good and a theory of the relationship between technology and social good
I think we've kind of spoke to talk a little bit about the first part of that all of this is
definitely very central to exactly those those questions right yeah right right but the you know
the latter of those two points kind of a theory for you know the relationship between technology
and social impact you know what does that what does that look like yeah yeah you know in some
ways it's like what we've been talking about like what you know you do you know you do x that has
implications a and b and kind of just thinking through that but when you talk about it as kind of
a theory and you you may have said formalism or I'm like kind of implying that like you know how
and you said rigor like how how rigorous can we get yeah yeah yeah um yeah so that you know I
I like to talk about it as in terms of a theory of change right and this is something that definitely
comes out of more I guess political activist circles right that you're thinking about okay how do we
tie our actions now to the outcomes that we want to get to in the medium or long term right how do
we think of any particular intervention and making the impacts that we want to make and that's
you know that's an incredibly hard question but one that many different areas of uh you know people
in government political activists and social scientists and others have been thinking about a lot
and in sort of a broader sense and then there's also a great deal of work in STS and critical
algorithm studies and philosophy of science and other fields that are looking at how our
technologies that are built and put into an applied social context actually affecting the world
and I think that by starting to pull all of that together we can start to have some some better
ways of thinking about you know okay if I'm trying to achieve you know x goal how can I
think about what type of system I can build to achieve that goal or how can I think about the ways
that the system that I built that seems to advance that goal could end up sort of splintering
off in any different number of ways and a lot of you know great work in law for example has been
looking at how laws meant to protect against discrimination or protect civil rights can end up
getting wielded and used in unexpected ways that actually have impacts totally counter to what
the developers uh or the creators of that law were intending so you know it's an incredibly
it's a complex area but I think that starting to you know look to some of those areas uh you know
we can look at for example okay how have particular types of anti-discrimination laws and efforts
been effective and been ineffective and how can we uh I think a lot of the work on fairness for
example today uh mirrors a lot of the typical modes of anti-discrimination work in the past
that have not necessarily been as effective as they were intended to be and I've written some
about that and Anna Lauren Hoffman at the University of Washington has a great paper on that
so that's an example of how we can look to some of these other fields and other areas and historical
contexts uh both for coral areas of how certain types of efforts have uh gone wrong or been
effective and also the lessons learned in terms of what it means to uh advance a particular
social project in a way that is is robust but but then to get to get more granular here uh you know
I think a couple of the things that that computer scientists can do you know I think that thinking
about the socio-technical context is super important here uh one of the ways that these unintended
impacts go wrong as we've been discussing has been you know you put it into context and it doesn't
actually get used in uh the ways you might expect and I think that part of this is if we can if
we think about the work that we're doing from this perspective of uh impact and theories of change
those types of things can shift from unintended impacts to things that we've thought about ahead of
time so taking those you know human interaction audit studies and making that an integral part
of building a system that's going to be used in an applied context or thinking about what is the
what is the policy domain that I'm working in and how have other efforts in the past and today
been successful at trying to challenge some of the issues that I'm challenging or been effective
at moving the ball in a certain direction so there are I think sort of there's this larger
conversation to be had and a lot of that is then you know talking to domain experts and talking
to people outside the field but then also I think there are just ways of having a more socio-technical
orientation that can take what today would be called unintended consequences and bringing them
into the design process earlier in terms of okay how do we prepare to test if that consequence
is going to happen and how do we make that central to what it means to evaluate this system rigorously
to bring that back into the conversation a lot of what I'm hearing here is going back to
as practitioners we should be thinking more about these issues you know I wonder you know for
practitioners you know do we know how to think about these issues and do we need more
or yeah obviously we should be thinking more about these issues but do we need more you know structure
frameworks rigor you know do we all have to go back and get degrees in STS or other things in order
to kind of reason through these issues it seems like at some point you know it's it's very
reductionist but like you know we need a checklist or you know some kind of map or some kind of
you know or at least you know a list of you know required reading or something like that to kind
of narrow the scope yeah do you agree with that or do you think that you know we're just being
lazy and we should just think no no no you know the you know and I think one thing that's really
important to to to say on that last point is like the goal of this is not to say you computer
scientists are evil or malevolent is right but to say that the at its foundations this is not
the type of thing that the field is trying to think about right so you know even going back to
the types of things that would be taught in an algorithms 101 class or like your canonical AI
or other CS textbooks and these sorts of things don't come up so I think it really does come back
to this question of education and training and sort of the broader culture and norms within the
field so these are you know these are certainly things that are hard really I would say long-term
shifts for the field to be to be building but I think that starting to think about how to
incorporate these other other disciplines into CS education that that or not just incorporating
but also expanding the boundaries of what it means to study computer science right that studying
computer science means not just studying you know algorithms and their efficiency and what's
an MP hard problem but also studying STS to understand how algorithms are affecting society
and studying at least some form of you know the policy or social domain where you're maybe
interested in applying right like if you're interested in the criminal justice system taking
classes in those areas or finding ways to have rich aspects of those fields or courses brought
into a computer science training world so you know and there are a lot of efforts to to do that
there is you know efforts to incorporate ethics training into courses there's an effort at Harvard
to do that led by Barbara Gross there are efforts at a lot of different universities now there
these sort of AI ethics or variants on that type of class and so and that's this you know that's
sort of the right start to be moving on is how do we expand the training that you're receiving
as a computer scientist but also in maybe a broader sense how do we change what it means to be a
computer scientist through that training so that these types of questions are not external to your
professional responsibility or to the types of things that might come up in peer review if you're
a researcher but are the among the fundamental questions that are going to be missed so I think
there's you know a lot of work to be done at both the scholarship level thinking about what is this
even look like and then at the education level thinking about how do we take these broader lessons
and incorporate that into my undergraduate and graduate curricula for what it means to study
machine learning in a different way than we would have studied machine learning even five years ago
right and also referring back to an earlier part of the conversation implications for
those people that are activists or trying to affect change within not just education but
but practice a way to kind of constructively look back at you know at social change across
you know other fields within and out and without technology and identify you know what's working
what's not working and use that to rethink the way you know we engage in conversations around
AI for social good mm-hmm yeah and you know again like none of this is to say that we can't
you know that that the types of tools or the types of methods that we have that are you know
what I would call them more sort of algorithmic formalist methods are bad or should be completely
thrown away or that every single person needs to do every single one of these things but I think
particularly as the field becomes more oriented around questions of social good and social impact
and thinking about these applied domains that only becomes more and more central so you know it's
one thing to think about building if you're a systems architect and not thinking about the social
impact I mean there are always social impacts and always ways to be thinking about that every
about ethics and architecture that everyone should be having but I think particularly where this
really comes to the fore is in the shift in the field towards applied work towards social good
which is both an incredibly exciting move for the field to be making but also a move that
requires you know a sense of humility and responsibility for what that means and a recognition
that that means incorporating new types of knowledge production and expertise and opening up
the bounds of what expertise even means or who is an expert yeah into the field also now also
well Ben thanks so much for sharing a bit of what you're up to yeah thank you so much I really
enjoyed this thank you same all right everyone that's our show for today to learn more about today's
guest or the topics mentioned in this interview visit twimmel ai.com of course if you like what you
hear on the podcast please subscribe rate and review the show on your favorite pod catcher thanks
so much for listening and catch you next time
