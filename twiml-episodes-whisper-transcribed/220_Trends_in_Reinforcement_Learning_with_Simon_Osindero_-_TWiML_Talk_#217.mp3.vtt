WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

00:35.920 --> 00:40.640
to present to you our first ever AI rewind series.

00:40.640 --> 00:45.040
In this series I interview friends of the show for their perspectives on the key developments

00:45.040 --> 00:49.720
of 2018 as well as a look ahead at the year to come.

00:49.720 --> 00:54.320
We'll cover a few key categories this year, namely computer vision, natural language

00:54.320 --> 00:59.200
processing, deep learning, machine learning and reinforcement learning.

00:59.200 --> 01:03.840
Of course we realize that there are many more possible categories than these, that there's

01:03.840 --> 01:08.800
a ton of overlap between these topics and that no single interview could hope to cover

01:08.800 --> 01:12.240
everything important in any of these areas.

01:12.240 --> 01:17.120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

01:17.120 --> 01:26.080
by commenting on the series page at twimbleai.com slash rewind 18.

01:26.080 --> 01:31.640
In this episode of our AI rewind series we introduce a new friend of the show Simon Ocindero,

01:31.640 --> 01:37.200
staff research scientist at DeepMind to discuss trends in deep reinforcement learning in 2018

01:37.200 --> 01:38.520
and beyond.

01:38.520 --> 01:42.600
We've packed a bunch into the show as Simon walks us through many of the important papers

01:42.600 --> 01:48.200
and developments seen this year in areas like imitation learning unsupervised RL, meta-learning

01:48.200 --> 01:49.680
and more.

01:49.680 --> 01:50.680
Enjoy.

01:50.680 --> 01:56.200
Alright everyone, I'm on the line with Simon Ocindero.

01:56.200 --> 01:59.560
Simon is staff research scientist at Google Monde.

01:59.560 --> 02:02.320
Simon, welcome to this week in machine learning and AI.

02:02.320 --> 02:03.320
Hi, sir.

02:03.320 --> 02:04.320
Good to see you.

02:04.320 --> 02:05.320
Awesome.

02:05.320 --> 02:06.320
It is great to have you on the show.

02:06.320 --> 02:15.880
You are joining us for our end of 2018 beginning of 2019 kind of reflections and predictions

02:15.880 --> 02:22.920
series and you're unique in this series and that the other guests that we've featured

02:22.920 --> 02:28.160
in this series are all kind of guests that are returning back to the podcast, but you

02:28.160 --> 02:33.920
are our new guests in this series.

02:33.920 --> 02:37.080
We wanted to catch up with you because we've been trying to get you on the show for a while.

02:37.080 --> 02:42.000
We just saw one another at Nourips and this seemed like a great opportunity.

02:42.000 --> 02:47.760
For our project here, we're going to talk through some of the interesting developments

02:47.760 --> 02:53.080
in your particular area of focus which is deep reinforcement learning.

02:53.080 --> 02:59.280
But you've identified a bunch of papers that caught your eye as being significant this

02:59.280 --> 03:00.280
year.

03:00.280 --> 03:05.600
Yeah, that's right, I guess when you asked me this, I was kind of trying to think of what

03:05.600 --> 03:11.880
was interesting and just the rate of publications at the moment is so high that given that and

03:11.880 --> 03:17.000
the number of subfields that I see developing in deeper all these days, it was kind of

03:17.000 --> 03:18.080
hard to come up with a short list.

03:18.080 --> 03:24.040
So I figured maybe what we do is touch on a couple of high level topic areas and I'll

03:24.040 --> 03:28.320
try and pull out from each of them a couple of the papers that I thought were interesting

03:28.320 --> 03:29.320
or that caught my eye.

03:29.320 --> 03:34.160
Actually, even doing that, there's probably going to be some focus areas that we could touch

03:34.160 --> 03:38.240
on that I don't know if we'll have time to but maybe we'll do that in another show.

03:38.240 --> 03:39.240
Awesome.

03:39.240 --> 03:41.440
So what's the first area you have for us?

03:41.440 --> 03:47.920
So the first step is what I'm going to call grouping together, sort of imitation learning,

03:47.920 --> 03:56.080
learning from demonstration, warm starts and curricula, so kind of this idea that we don't

03:56.080 --> 04:01.160
always need to learn from scratch without any information and in fact, doing that when

04:01.160 --> 04:07.840
rewards are very sparse is kind of difficult and so we can leverage other types of knowledge.

04:07.840 --> 04:13.040
So in particular, maybe demonstrations, a small number of demonstrations from other agents,

04:13.040 --> 04:17.320
some of those might be humans, another way of thinking about this problem is to think

04:17.320 --> 04:24.040
about curricula, so either over problems or model complexity and essentially starting

04:24.040 --> 04:28.880
off doing something simple and then bootstrapping from the solution to a similar problem to a

04:28.880 --> 04:30.760
more complicated one.

04:30.760 --> 04:36.640
With regards to imitation learning, I typically see this in the context of reinforcement

04:36.640 --> 04:41.920
learning, are they linked inseparably or can you do one without the other?

04:41.920 --> 04:48.400
Yeah, I mean, for sure you can do reinforcement learning without imitation and actually I guess

04:48.400 --> 04:52.600
depending on what you mean by imitation, you don't necessarily have to do it in a reinforcement

04:52.600 --> 04:53.600
context.

04:53.600 --> 04:58.240
Are there examples that come to mind of imitation learning outside of the reinforcement learning

04:58.240 --> 04:59.240
context?

04:59.240 --> 05:04.240
I mean, I guess depending on how you think about it, you can actually imagine even some of

05:04.240 --> 05:13.560
the way in which we train some language models is essentially being imitation learning,

05:13.560 --> 05:18.720
the kind of teacher forcing aspect of sequence model learning, you can kind of do that as

05:18.720 --> 05:20.280
a kind of imitation.

05:20.280 --> 05:26.800
All right, so the first paper on your list of important papers in this space comes out

05:26.800 --> 05:33.920
of deep mind is the playing exploration games by watching YouTube paper and actually by

05:33.920 --> 05:40.560
the time this interview is published, my interview with Nando to Freitas, one of the authors

05:40.560 --> 05:44.040
of that paper from Nureps, will have been published.

05:44.040 --> 05:47.760
So folks can take a listen to that one.

05:47.760 --> 05:48.760
Great.

05:48.760 --> 05:53.200
Yeah, what were the highlights of that paper for you?

05:53.200 --> 05:58.000
I guess a couple of things, and Nando's already said about this, I'll be relatively brief,

05:58.000 --> 06:03.600
but it's kind of nice that we have observational demonstration data rather than necessarily

06:03.600 --> 06:04.600
providing.

06:04.600 --> 06:08.400
So I guess there's a couple of ways that you could give an agent demonstrations, one is

06:08.400 --> 06:12.840
just letting it watch another agent and observing the world, the other is literally telling

06:12.840 --> 06:18.320
it action sequences that another agent have done that have been successful.

06:18.320 --> 06:23.400
So this is a kind of nice example in that it's purely observational, and so the hope is

06:23.400 --> 06:28.800
that by generalizing this technique, we really could take it from, in this case, it was

06:28.800 --> 06:32.880
looking at a video game that you can imagine wanting to scale this up and just having an

06:32.880 --> 06:38.160
agent watch, people doing stuff in YouTube and learn to do stuff in the real world.

06:38.160 --> 06:41.480
So it's kind of got a lot of potential in that direction.

06:41.480 --> 06:42.480
Okay.

06:42.480 --> 06:43.480
Cool.

06:43.480 --> 06:46.000
How about the next paper on your list?

06:46.000 --> 06:52.200
So the other one was another deep mind paper and similar kind of idea, so this is the

06:52.200 --> 06:56.480
observant look further, achieving consistent performance on Atari.

06:56.480 --> 07:05.040
And again, it's using demonstration, in this case, they do get to see the actions, but

07:05.040 --> 07:10.360
they basically learn to get good performance on some of these hard-expression gains with

07:10.360 --> 07:13.640
just a single demonstration trajectory.

07:13.640 --> 07:19.680
And the kind of idea there is that you basically, in your replay buffer, you have the experience

07:19.680 --> 07:25.720
of the agents generated, but you also get to populate that with experience that a demonstrators

07:25.720 --> 07:31.400
provided, and it turns out that if you do that and a couple of other tricks in terms of

07:31.400 --> 07:36.760
how you do the training, you can also get extremely good results from a very limited amount

07:36.760 --> 07:38.760
of demonstration data.

07:38.760 --> 07:44.960
So when you say a single demonstration trajectory?

07:44.960 --> 07:50.800
Yeah, so imagine a video game just kind of playing through it once, and that's all you

07:50.800 --> 07:51.800
get to see.

07:51.800 --> 07:57.080
So the reason that's kind of interesting is that, you know, a single, I mean, I guess it

07:57.080 --> 08:01.440
depends on the game, but a single playthrough wouldn't necessarily capture all the variability

08:01.440 --> 08:05.320
that you get when you're playing the game, you know, if you start to do the things a little

08:05.320 --> 08:10.960
differently at the beginning, then things can kind of diverge, but in this game, and I

08:10.960 --> 08:13.840
guess it's true of a lot of environments, you know, if you're starting from scratch and

08:13.840 --> 08:17.920
you don't know anything, then there's lots and lots of things that you could end up doing.

08:17.920 --> 08:22.920
And so even just seeing one demonstration of roughly the right thing to do can be extremely

08:22.920 --> 08:23.920
helpful.

08:23.920 --> 08:28.600
How is the imitation part of this paper approached?

08:28.600 --> 08:37.000
Yeah, so like I said, they're using the building known as Technica DQFD, and so DQ learning

08:37.000 --> 08:38.840
from demonstration.

08:38.840 --> 08:46.040
And so the idea there is you have this replay buffer, which is basically transitions that

08:46.040 --> 08:47.040
you've seen in the world.

08:47.040 --> 08:52.960
So you have your current state, the action you did, where you ended up next, and associated

08:52.960 --> 08:59.560
with rewards, and in regular Q learning, or you would essentially populate that buffer

08:59.560 --> 09:00.920
just with your own experience.

09:00.920 --> 09:04.160
And so when you're starting out, you maybe don't really know how the world works.

09:04.160 --> 09:09.960
And so it's put part of the reason that this is interesting and kind of why highlighting

09:09.960 --> 09:15.240
this as an area is, there's a lot of problems where just getting the agent off the ground

09:15.240 --> 09:19.880
and starting to do something simple, can actually end up being the hardest part, but once

09:19.880 --> 09:23.840
you're starting to see rewards and figuring out how the world works, then learning tends

09:23.840 --> 09:26.200
to progress reasonably.

09:26.200 --> 09:29.560
So in terms of learning curves, a lot of the time on some of these tough environments,

09:29.560 --> 09:34.360
you'll basically see flat lining for a very long time, because the agent is essentially

09:34.360 --> 09:39.600
just randomly exploring until it figures out something interesting, and then it starts

09:39.600 --> 09:41.040
to get a signal that it can learn from.

09:41.040 --> 09:47.800
And so I think that's another reason why just a very small amount of demonstration data

09:47.800 --> 09:54.640
or instruction or just something to kind of kick learning off the ground and end up accelerating

09:54.640 --> 09:59.400
learning and help us learn on problems that otherwise would be very hard to do from scratch.

09:59.400 --> 10:01.280
Were there other papers in the scatigory?

10:01.280 --> 10:05.240
Yeah, I guess there was another one that we had in Europe, so I was actually, so this

10:05.240 --> 10:08.960
was kickstarting deep reinforcement learning.

10:08.960 --> 10:14.480
And that's a slightly different approach, so it's not using demonstrations per se anymore.

10:14.480 --> 10:21.520
The idea is we have a free-training teacher agent, so you could imagine getting that in

10:21.520 --> 10:27.320
a couple of different ways, so maybe that would be a simple, easy-to-train agent that

10:27.320 --> 10:31.560
doesn't necessarily get the best performance, but it's easy to train in a reasonable time

10:31.560 --> 10:36.280
or maybe you have lots of different tasks in an environment you want to do, and you might

10:36.280 --> 10:41.640
be able to have a teacher that specializes on a single task, which is typically easier

10:41.640 --> 10:45.320
than learning a whole bunch of things, but you have a student who wants to be able to learn

10:45.320 --> 10:51.560
to do many different things, and the kind of, the approach we have there is we are able

10:51.560 --> 10:55.840
to have the student experience in the world, and we can also query the teacher in a sense

10:55.840 --> 10:58.240
he asks, what would you do if you were in the situation?

10:58.240 --> 11:05.320
So here we have access to the actions, and the idea is we'd like the student to match

11:05.320 --> 11:09.760
the teacher's behaviour, but we don't just want to end up completely copying the teacher

11:09.760 --> 11:14.880
because there can be no point in that, so we also have a mechanism that essentially

11:14.880 --> 11:19.480
over time allows the student to figure out how much attention it should pay to its teacher

11:19.480 --> 11:25.720
and how much attention it should pay to its own experience, so over time the student

11:25.720 --> 11:29.560
trusts its own experience more and more, listens to the teacher less and less, so you

11:29.560 --> 11:34.000
can end up with a student rather exceeding the performance of the teacher, and we kind

11:34.000 --> 11:40.400
of see this as being useful in a couple of different ways, so particularly right now

11:40.400 --> 11:44.400
in terms of the kind of nuts and bolts of experimental cycles, you often want to try out

11:44.400 --> 11:49.360
a lot of different ideas and architectures, and if for each one of those you are having

11:49.360 --> 11:55.400
to start from scratch each time that can be costly in terms of computation, but also can

11:55.400 --> 12:00.000
just be slow, so we kind of see this as a nice way to shorten experimental cycles if

12:00.000 --> 12:04.200
you're wanting to iterate through different aging architectures pretty quickly, another

12:04.200 --> 12:11.600
benefit that we see is, sometimes if you have a really big agent it's just a lot of

12:11.600 --> 12:16.240
parameters to train, so getting that off the ground initially can be kind of hard, and

12:16.240 --> 12:20.480
so we've actually seen really nice progress taking very simple agents that are quick

12:20.480 --> 12:25.000
to train, but they flatline it, not great performance, but by having that as a teacher

12:25.000 --> 12:30.360
we can then get one of these much, much bigger agents off the ground that then ultimately

12:30.360 --> 12:35.640
gives us much better performance, so it's, yes, as a technique it's something that we're

12:35.640 --> 12:37.440
using quite a lot internally.

12:37.440 --> 12:42.000
One of the contexts that you explained this in is in terms of like multi-task, or at least

12:42.000 --> 12:45.480
an agent trying to learn multi-task, I don't know if it's formerly multi-task learning,

12:45.480 --> 12:51.880
but what's the role of the teacher in that multi-multiple task scenario?

12:51.880 --> 12:58.160
Yes, so in that scenario it would be, often what we'd like is a single agent that can

12:58.160 --> 13:03.520
do lots and lots of things, and that training that transcript can be kind of challenging,

13:03.520 --> 13:07.440
what's often more manageable is you kind of subdivide the problem space into smaller

13:07.440 --> 13:13.800
and smaller chunks, so you are learning to do just one of those tasks, maybe even breaking

13:13.800 --> 13:19.120
a single task into smaller chunks, so learning to do a small part of the overall problem,

13:19.120 --> 13:23.880
you can get a teacher agent that specializes on that little bit of the overall problem

13:23.880 --> 13:29.080
space, and if you then have many different teachers that specialize in different parts between

13:29.080 --> 13:33.040
them that able to do most of the things you care about, but like I say what you'd really

13:33.040 --> 13:38.320
like is a single agent that has competency to do everything in the domain, and so in a

13:38.320 --> 13:42.280
situation like that you could build these simple agents that kind of specialize in parts

13:42.280 --> 13:48.120
of the problem space, and then use those different teachers to then transfer their knowledge

13:48.120 --> 13:53.920
to a student that is learning from all of them, and as I say over time it can figure out

13:53.920 --> 13:58.640
how much attention to pay to the different teachers and get good performance, so in the

13:58.640 --> 14:06.640
paper we have a set of different tasks that we want to do in the 3D DM lab environment,

14:06.640 --> 14:14.920
and so you know they can like navigation kind of tasks, avoidance kind of tasks, gathering

14:14.920 --> 14:19.720
tasks, all sorts of things like that, and we have teacher agents that are specialized

14:19.720 --> 14:24.480
for each one of those, and then we can build, but what we're really interested in, and

14:24.480 --> 14:30.000
this is kind of moving towards general intelligence, we really want one agent that can do lots and

14:30.000 --> 14:34.160
lots of things and can also learn to do new things, and so one way of getting that off

14:34.160 --> 14:40.200
the ground is to have these simpler, less general agents that specialize in particular parts

14:40.200 --> 14:44.640
of the problem space, and then distill their knowledge into the student to get the student

14:44.640 --> 14:46.320
off the ground.

14:46.320 --> 14:54.320
To be clear, so maybe an example, if you say you have, you're trying to train an agent,

14:54.320 --> 15:01.800
and you've got four kind of things that you want it to learn, and you create three of

15:01.800 --> 15:09.360
these teacher agents, are the teacher agents increasing performance in the three categories

15:09.360 --> 15:15.720
for which you have teacher agents, or is the lack of teacher agent allowing the agent

15:15.720 --> 15:20.680
to learn more about the one where you don't have a teacher agent?

15:20.680 --> 15:29.680
In this setting, I guess it's a little orthogonal to that, so in terms of, the teacher should

15:29.680 --> 15:37.360
never be too limiting in the sense that we have this mechanism, in the people are using

15:37.360 --> 15:41.960
population-based training, but you could imagine doing other things that essentially tries

15:41.960 --> 15:45.920
to meta-learn how much attention the page of the teacher.

15:45.920 --> 15:50.040
If the teacher is useful, if it's giving you learning progress on the thing that you

15:50.040 --> 15:55.160
care about, great, as that becomes less and less useful, you pay less and less attention

15:55.160 --> 15:56.160
to the teacher.

15:56.160 --> 16:01.600
So far, in these conversations, what's been interesting is that I could spend the entire

16:01.600 --> 16:07.720
time talking about one thing that kind of pulls me in that direction, but we'll try not

16:07.720 --> 16:12.400
to do that here, and so where there are other papers in this category, do you want to move

16:12.400 --> 16:13.680
on to the next one?

16:13.680 --> 16:19.240
So we had another paper mix and match agent curricula for reinforcement learning, and

16:19.240 --> 16:26.280
there, rather than having a teacher that's kind of pre-trained, it's kind of as if you

16:26.280 --> 16:35.360
have a companion agent that is trained jointly with the one you really care about, so in practice

16:35.360 --> 16:44.720
how we implement that, we have a mixed-your-policy, which is two different policies that we kind

16:44.720 --> 16:50.640
of blend together, and the idea there is maybe a much simpler policy would be easier to train

16:50.640 --> 16:57.160
to imagine something that has far fewer possible actions that it could take, and so often

16:57.160 --> 17:01.200
that's similar to learn, but what we'd really like is an agent that has access to many,

17:01.200 --> 17:02.360
many actions.

17:02.360 --> 17:08.400
One thing you could do is initially train an agent with few actions and then use the behavior

17:08.400 --> 17:12.600
of that agent to train the one that has many, many actions, and in this particular setup,

17:12.600 --> 17:17.760
we do that in a continuous way where the learning is shared between both different types

17:17.760 --> 17:24.280
of policy, and we have a term that essentially encourages the information to flow in a sense

17:24.280 --> 17:30.240
the simpler and faster learning agent to the complex, more solely learning agent, and again,

17:30.240 --> 17:36.040
we have that processes orchestrated, is done automatically with this method population-based

17:36.040 --> 17:42.040
training, which essentially tries a whole bunch of different values for how strongly we're

17:42.040 --> 17:48.200
coupling behavior and preferentially follows the ones that are giving the best empirical

17:48.200 --> 17:49.200
performance.

17:49.200 --> 17:54.600
So the next category that you identified was what?

17:54.600 --> 18:01.720
So this one is, I guess what I call unsupervised RRR, or you could think of it as self-supervision

18:01.720 --> 18:09.720
or different kinds of interesting auxiliary tasks, so I think this is sort of a general

18:09.720 --> 18:16.960
direction that lots of people are interested in, which is the information in a reward signal

18:16.960 --> 18:22.680
probably isn't enough to train the scale of agents that we're interested in in a reasonable

18:22.680 --> 18:23.680
amount of experience.

18:23.680 --> 18:28.920
It's not very information rich, but there actually is a ton of other stuff in the environment

18:28.920 --> 18:31.640
that we can learn from.

18:31.640 --> 18:39.040
And so leveraging that along with a reward signal is something that I think we're going

18:39.040 --> 18:40.040
to see a lot more.

18:40.040 --> 18:48.240
So you've probably heard Jan Lecun talk about the cherry analogy with a cake, so it's kind

18:48.240 --> 18:54.880
of getting at that, but the point is that there's all sorts of things that we can learn

18:54.880 --> 18:57.280
from in the reinforcement learning context.

18:57.280 --> 19:03.720
And so examples of this, not from this year, but previously, would be the unreal paper

19:03.720 --> 19:09.240
where you're essentially doing a whole bunch of auxiliary prediction tasks in addition to

19:09.240 --> 19:11.560
trying to learn the policy and value.

19:11.560 --> 19:14.600
One of the papers from this year that I thought was particularly interesting in that sense

19:14.600 --> 19:21.360
is this one unsupervised control through non-parametric discrimination rewards.

19:21.360 --> 19:30.080
And the basic idea is wanting to kind of learn to discover and achieve goals without

19:30.080 --> 19:31.080
any supervision.

19:31.080 --> 19:36.760
So it appears to be similar to what happens if you have a young kid, you're essentially

19:36.760 --> 19:40.920
just learning what you can control in the world, learning what you can do.

19:40.920 --> 19:46.080
But no one's, you do have some supervision, but a lot of learning is just figuring out,

19:46.080 --> 19:49.680
what can I affect, what can I control in the world, can I set myself arbitrary goals

19:49.680 --> 19:52.080
and then achieve them?

19:52.080 --> 19:54.760
And so it's kind of taking a shot at that problem.

19:54.760 --> 19:56.760
Does that kind of exist?

19:56.760 --> 20:04.680
Yeah, how did they kind of encode that knowledge that they're making not relative to the

20:04.680 --> 20:05.680
loss function?

20:05.680 --> 20:07.680
Yeah, that's a great question, actually.

20:07.680 --> 20:14.240
So they let's learn a loss function, so there's a couple of clever tricks that they do.

20:14.240 --> 20:23.000
So you start out behaving, let's say, randomly, and you know that states that you visited

20:23.000 --> 20:27.160
in that random paper are achievable, or you know, something closer that should be achievable

20:27.160 --> 20:29.040
because you've achieved that before.

20:29.040 --> 20:35.040
So this is drawing on some of the ideas from, there's a paper last year on hindsight experience

20:35.040 --> 20:39.360
replay, which essentially is, you know, maybe I was trying to do one thing.

20:39.360 --> 20:42.320
I didn't necessarily do that, but I did a whole bunch of things.

20:42.320 --> 20:46.520
And so you can learn about the count of factuals.

20:46.520 --> 20:48.560
When you're acting, you do something.

20:48.560 --> 20:56.640
And so learning about what you did is an alternative to learning about what you wanted to do.

20:56.640 --> 20:58.680
So that's kind of one element.

20:58.680 --> 21:06.480
The other element is how you figure out how well you achieve those goals, and they use

21:06.480 --> 21:11.840
an approach that's a little similar to the discriminator in again, so that the basic

21:11.840 --> 21:16.360
idea is, well, yeah, okay, the three components, you have a goal condition policy.

21:16.360 --> 21:22.640
So you have a policy that gets to see its current state, and it also gets given a goal state

21:22.640 --> 21:25.080
that it's trying to achieve.

21:25.080 --> 21:28.880
And under ideal conditions, you kind of plug both those in, and it will get you to the

21:28.880 --> 21:31.280
goal.

21:31.280 --> 21:34.600
But then there's the question that I think, well, you're saying initially, how do you figure

21:34.600 --> 21:36.800
out if you achieve your goal or not?

21:36.800 --> 21:40.640
And that's where this sort of bootstrap teacher comes in.

21:40.640 --> 21:47.000
So the idea is, imagine the goal that I'm trying to achieve, and imagine a whole bunch of

21:47.000 --> 21:48.520
other possible states that I could be in.

21:48.520 --> 21:52.760
So there's the kind of, there's the target and a whole bunch of distractors.

21:52.760 --> 21:59.440
What I'm going to do is try and learn a classifier that distinguishes between those distractors

21:59.440 --> 22:02.520
and the goal that I was trying to achieve.

22:02.520 --> 22:05.680
And the way that that's trained, it's a little subtle.

22:05.680 --> 22:11.280
So there's two things that we can plug in there, because if you've told what I was saying,

22:11.280 --> 22:13.400
you might worry there's a bit of a secularity.

22:13.400 --> 22:19.440
So one of them is to plug in some of those hindsight goals, so given some initial state

22:19.440 --> 22:24.120
and just a random trajectory, there'll be states that I go through.

22:24.120 --> 22:29.000
And so I could, after the fact, say, hey, that state that I ended up passing through,

22:29.000 --> 22:31.200
that wasn't my goal.

22:31.200 --> 22:35.280
And so the nice thing about that is you know that you achieved it because you were literally

22:35.280 --> 22:40.640
in that state, you can use that to kind of get this classifier a little off the ground.

22:40.640 --> 22:46.600
Now other data, the situation between that goal that you reached and these distractors,

22:46.600 --> 22:50.600
but then it's also bootstrapped a little bit, and there you essentially end up saying,

22:50.600 --> 22:53.720
wherever you've got to, when you were following this particular goal, I'm going to treat

22:53.720 --> 22:58.080
that from the point of view of the classifier as if that was the ground truth.

22:58.080 --> 23:00.720
Which sounds like it might be a little unstable, because initially you're not going to be

23:00.720 --> 23:06.080
achieving those goals, but what that ends up doing is it sort of gives you a scoring

23:06.080 --> 23:11.440
function that is kind of smooth in the sort of meaningful space of task achievement over

23:11.440 --> 23:12.440
time.

23:12.440 --> 23:17.440
So, you know, I don't necessarily want to need to be pixel perfect to say that I've achieved

23:17.440 --> 23:21.240
that goal and there might be different ways of achieving the same thing.

23:21.240 --> 23:29.720
And so by plugging the agent's behavior in it in that way, they're able to learn an

23:29.720 --> 23:34.400
intrinsic reward function that is effective at kind of critiquing how well they achieve

23:34.400 --> 23:36.240
these arbitrary self-derived goals.

23:36.240 --> 23:37.240
Yeah, yeah.

23:37.240 --> 23:43.040
So, I think that this kind of area of self-supervision or unsupervised RL is going to be something

23:43.040 --> 23:48.680
we're going to see a lot more of figuring out good ways of how we can set prediction

23:48.680 --> 23:53.560
or control tasks for ourselves in order to expand the knowledge and capabilities of an

23:53.560 --> 23:54.920
agent in a particular environment.

23:54.920 --> 24:02.840
And so, is this general category is it trying to help us overcome the difficulty of creating

24:02.840 --> 24:05.400
loss functions for RL?

24:05.400 --> 24:07.640
Yeah, you could do it.

24:07.640 --> 24:13.000
It's doing that in one way.

24:13.000 --> 24:17.160
So in terms of a lot of problems that we might be interested in, so imagine you know,

24:17.160 --> 24:22.720
a kind of robotic and control problem, we're sort of trying to move away from hand-engineering

24:22.720 --> 24:23.720
reward function.

24:23.720 --> 24:30.400
So, in that setting, you might, what you really care about is the task done or not.

24:30.400 --> 24:35.160
But just training on that is very sparse, so you basically get to know if you've done

24:35.160 --> 24:39.040
the task or not pretty much at the end.

24:39.040 --> 24:42.720
And there might be lots of different ways of achieving that and to the extent that you

24:42.720 --> 24:48.080
try and hand design or reward function, you're also in some sense hand designing a solution

24:48.080 --> 24:50.320
which were kind of trying to get away from it.

24:50.320 --> 24:53.000
If it was easy for us to design solution, then maybe we wouldn't need to learn it in

24:53.000 --> 24:54.000
the first place.

24:54.000 --> 24:55.000
Right.

24:55.000 --> 24:56.000
Right.

24:56.000 --> 25:00.920
So, yeah, this is kind of heading in this direction of how can I build a system that can

25:00.920 --> 25:03.920
learn to control the environment in lots of different interesting ways?

25:03.920 --> 25:09.320
Because if I can do that, then when you give me your problem, if I know how to do all

25:09.320 --> 25:12.680
sorts of things, then essentially when you give me your problem, I just need to figure

25:12.680 --> 25:17.280
out all the many things that I'm capable of, which one is the one that fits your particular

25:17.280 --> 25:18.280
problem?

25:18.280 --> 25:20.080
And so that then becomes a lot easier.

25:20.080 --> 25:21.560
So what's the next category?

25:21.560 --> 25:25.640
So, the next category, learning to learn or meta-learning.

25:25.640 --> 25:36.040
So, this idea of how do we use learning to improve the process of learning itself?

25:36.040 --> 25:43.200
And I'd sort of, this kind of blends in a little bit, especially in the RL setting with

25:43.200 --> 25:49.200
how do we learn to acquire experience that is useful for learning, or to us to be care

25:49.200 --> 25:51.920
about, which is, you know, more or less exploration.

25:51.920 --> 25:57.120
So, I had three papers from this topic that I thought would be good to highlight.

25:57.120 --> 26:06.960
So, the first one is called Meta-Reenforcement Learning Exploration Strategies.

26:06.960 --> 26:09.520
And there's a couple of elements to it.

26:09.520 --> 26:17.920
So, it builds on some meta-learning ideas from previous papers.

26:17.920 --> 26:24.240
So, there's a paper model-agnostic meta-learning, mammal that folks went ahead of where the

26:24.240 --> 26:29.520
idea there is, you're trying to find model parameters such that, given arbitrary new problems,

26:29.520 --> 26:31.520
you can rapidly adapt to solve them.

26:31.520 --> 26:39.000
This one is building on top of that, and you have a policy that has latent variables.

26:39.000 --> 26:46.280
And in this case, those latent variables are going to add, so, episode scale, noise

26:46.280 --> 26:48.200
or stochasticity to your behavior.

26:48.200 --> 26:52.840
But there's other ways you could do that conditioning, the kind of the point is, there's a latent

26:52.840 --> 27:00.480
variable that different settings of that cause your model to, cause your agent to behave differently.

27:00.480 --> 27:06.760
And so, by setting those parameters, you effectively can get different types of exploratory

27:06.760 --> 27:08.760
behavior.

27:08.760 --> 27:19.320
So, they essentially try to learn an initial distribution of these latent variables, such

27:19.320 --> 27:26.840
that for new problems from the domain of interest, if you sampled on that, you quickly produce

27:26.840 --> 27:31.560
behaviors that allow you to gain experience, that allow you to solve those new problems

27:31.560 --> 27:32.560
rapidly.

27:32.560 --> 27:41.280
So, rather than having to have stochasticity at each time step, you get to explore in

27:41.280 --> 27:44.640
a structured way across the scale of an episode.

27:44.640 --> 27:45.640
Okay.

27:45.640 --> 27:47.920
So, that was one of the ones.

27:47.920 --> 27:55.640
Another one, this paper, Evolved Policy Gradients, and there, it's a very similar metal learning

27:55.640 --> 28:03.200
flavor, there the idea is, can we evolve a reward functions or an intrinsic reward functions

28:03.200 --> 28:10.680
such that if I optimize, according to that reward function, I get good performance on the

28:10.680 --> 28:11.680
task that I care about.

28:11.680 --> 28:16.280
So, again, imagine one of these settings where you basically, all I care about is, did

28:16.280 --> 28:18.080
you do the thing that I wanted you to do or not?

28:18.080 --> 28:20.120
Did you complete the task successfully?

28:20.120 --> 28:25.720
And, you know, as I was saying before, hand designing, excuse me, a dancer reward function

28:25.720 --> 28:29.760
that tells you, you know, for each state action transition, how good was that?

28:29.760 --> 28:34.200
It's kind of hard, but maybe that's a function rather than specifying it, maybe we can learn

28:34.200 --> 28:35.200
that function.

28:35.200 --> 28:39.560
And that's what this paper is trying to do, and it's using evolutionary methods to do

28:39.560 --> 28:40.560
that.

28:40.560 --> 28:47.800
So, basically, you have an in-loop where you're using regular policy gradient learning,

28:47.800 --> 28:53.800
an absolute where you're using evolutionary search to figure out what a good function would

28:53.800 --> 28:54.800
be.

28:54.800 --> 29:00.200
So, I plug in a function, you get to train with that function, I see how well you did

29:00.200 --> 29:05.520
training with that reward function, and I'm slowly shifting my search space of functions

29:05.520 --> 29:09.240
towards ones that when I optimize them, give me good performance on the thing that I

29:09.240 --> 29:10.240
really care about.

29:10.240 --> 29:18.360
The evolved policy gradients paper is out of OpenAI and the Meta-reinforcement learning paper

29:18.360 --> 29:20.440
that I remember correctly is out of Berkeley.

29:20.440 --> 29:22.440
That's it, yeah.

29:22.440 --> 29:23.440
Okay.

29:23.440 --> 29:28.040
Yeah, I should think both of them, they might be both the OpenAI Berkeley collaborations,

29:28.040 --> 29:29.040
but.

29:29.040 --> 29:30.040
Oh, really?

29:30.040 --> 29:31.040
Okay.

29:31.040 --> 29:32.040
Yeah.

29:32.040 --> 29:33.040
Okay, interesting.

29:33.040 --> 29:34.040
Yeah.

29:34.040 --> 29:36.240
This, the meta-learning space is a really interesting one that I'm starting to hear a lot

29:36.240 --> 29:37.240
about.

29:37.240 --> 29:41.560
Yeah, yeah, I think there's a ton of interesting potential there.

29:41.560 --> 29:47.040
The other paper I was going to mention in that topic real quick was this one is another

29:47.040 --> 29:54.400
deep-mine paper, Meta-gradient reinforcement learning, and yes, similar themes again in

29:54.400 --> 29:57.760
terms of having two scales of optimization.

29:57.760 --> 30:03.960
Here we're asking ourselves that there are parameters that essentially influence the

30:03.960 --> 30:11.040
return that we're learning on that can affect the learning dynamics, so the discount, how

30:11.040 --> 30:17.400
much I discount rewards into the far future is a property of the algorithm that affects

30:17.400 --> 30:18.400
the learning dynamics.

30:18.400 --> 30:24.440
So even if there is a horizon that I truly care about, it might be the case that I get

30:24.440 --> 30:29.960
better empirical performance by using something that's different from that.

30:29.960 --> 30:37.640
And so this paper is combining regular policy gradient with essentially a second set of

30:37.640 --> 30:44.240
rollouts that try to, again, ask the question, if these meta-parameters of my learning were

30:44.240 --> 30:47.920
different, would I be making better learning progress?

30:47.920 --> 30:55.600
And so it's a way, for instance, of automatically and dynamically tuning things like the lambda

30:55.600 --> 31:04.000
or your discount, moving on, and I'll try and condense this section a little bit since

31:04.000 --> 31:07.960
we're probably going to get tied on time if we kind of go into a lot of detail.

31:07.960 --> 31:12.840
But we've been trying to hold back my many questions.

31:12.840 --> 31:20.640
So yeah, another kind of interesting area is exploration, learning to explore, and dealing

31:20.640 --> 31:30.080
with different types of uncertainty, so I guess this, you can think of two types of uncertainty.

31:30.080 --> 31:34.800
And so, folks like you've been talking a lot about, sort of, epistemic versus alliotoric

31:34.800 --> 31:35.800
uncertainty.

31:35.800 --> 31:39.400
So epistemic uncertainty is basically represents your lack of knowledge or data about

31:39.400 --> 31:40.400
the world.

31:40.400 --> 31:42.400
So it's like uncertainty in your model parameters.

31:42.400 --> 31:47.800
So whereas alliotoric, you can kind of think of that as like fundamental randomness in

31:47.800 --> 31:52.560
the world, so stuff that you just can't predict no matter how much data you see.

31:52.560 --> 31:57.520
And maybe that's because it's too many random, or I guess other people might argue that there's

31:57.520 --> 32:00.640
the things that are just kind of outside the capability of a model class, but it kind

32:00.640 --> 32:02.480
of boils down to the same thing.

32:02.480 --> 32:08.240
So yeah, there's a couple of papers that are closely related, actually, that touch

32:08.240 --> 32:09.240
on this.

32:09.240 --> 32:16.600
So there's a really nice paper at New York's in Osborne, John Aslanneeds and Albon Kissiro,

32:16.600 --> 32:21.080
customized prior functions for the DBRL.

32:21.080 --> 32:25.320
And there's a couple of really nice tricks in what they're doing.

32:25.320 --> 32:29.640
This is also kind of building a prior work that I've seen reused in a couple of different

32:29.640 --> 32:30.640
places.

32:30.640 --> 32:37.120
So this notion that we can use a sort of bootstrap ensemble of functions to try and represent

32:37.120 --> 32:38.120
uncertainty.

32:38.120 --> 32:45.640
So that's basically just rather than having a single function, if I train lots of copies

32:45.640 --> 32:52.720
of a function with different visualizations, and with, in this case, different prize,

32:52.720 --> 32:57.960
which I'll touch on briefly in a second, I can then look at to what degree the different

32:57.960 --> 33:01.200
models in the ensemble agree or disagree.

33:01.200 --> 33:04.840
So if there's a lot of variance in the models predictions, then in some sense that's a reflection

33:04.840 --> 33:05.840
of uncertainty.

33:05.840 --> 33:13.800
And it turns out you can kind of make that connection tight on the certain conditions

33:13.800 --> 33:16.680
and in general, it seems like a good heuristic.

33:16.680 --> 33:24.640
So the idea here is we're going to do that with our Q functions and then build on top

33:24.640 --> 33:31.080
of that for learning the notion of the randomised prior, essentially boils down to, I'm going

33:31.080 --> 33:36.040
to have a prior distribution over the parameters of our function class.

33:36.040 --> 33:39.440
And for each member of the ensemble, I'm going to sample from that prior and instead

33:39.440 --> 33:44.840
of effectively doing weight decay towards zero as a kind of regularization, I'm going

33:44.840 --> 33:51.560
to do weight decay towards those initially sampled parameters.

33:51.560 --> 33:57.120
You can kind of think about as starting off with some random function from your prior

33:57.120 --> 34:01.480
beliefs, and then learning a correction function top of that to model what you actually

34:01.480 --> 34:02.480
see in the world.

34:02.480 --> 34:08.320
And so it turns out, if you do that and you don't look the right way, you can basically

34:08.320 --> 34:13.200
sample when you're acting, you can sample a function from your ensemble, act according

34:13.200 --> 34:18.360
to that gather experience, participants in a buffer, bootstrap, update all the different

34:18.360 --> 34:20.160
members of the ensemble.

34:20.160 --> 34:28.440
And it deals with that uncertainty about the world in the right way that leads to good

34:28.440 --> 34:30.960
exploratory behaviour.

34:30.960 --> 34:32.840
So yeah, I thought that was a really cool paper.

34:32.840 --> 34:39.800
There was another paper, I think OpenAI and you'd most of Edinburgh folks, so Yoruba,

34:39.800 --> 34:46.200
Harrison Edward, David Storky, and all the claim of exploration by random network distillation

34:46.200 --> 34:53.280
that take a similar sort or the, you can actually draw a fairly tight connection between what

34:53.280 --> 35:00.520
they do and the paper I just described, although it's maybe not so obvious immediately.

35:00.520 --> 35:06.200
What they're doing is they're coming up with a function that, so you have a random

35:06.200 --> 35:10.760
function, and what you're going to try and do is predict the outputs of that random

35:10.760 --> 35:14.640
function, using a function that you're learning, and you're going to use your prediction error

35:14.640 --> 35:19.840
as a sign of unfamiliarity, and then you get to plug that in as an intrinsic award.

35:19.840 --> 35:26.160
So the idea being, you know, if you're in discrete states, then you can use some kind

35:26.160 --> 35:30.800
of account-based measure of how unfamiliar you are in a certain part of the world, and

35:30.800 --> 35:35.320
kind of give bonuses for exploring new places, that gets harder in much bigger statespaces

35:35.320 --> 35:38.000
where you can't enumerate everything.

35:38.000 --> 35:42.560
And so this is another way of approaching that.

35:42.560 --> 35:48.000
How do I measure how uncertain I am about this part of state space?

35:48.000 --> 35:54.200
And so yeah, essentially what you do, you have this random function, you're trying to

35:54.200 --> 35:58.560
learn something that predicts the outputs of that random function when applied to the

35:58.560 --> 36:02.360
state, and your prediction error, you're going to use as an intrinsic award.

36:02.360 --> 36:09.760
So places that you've visited lots and lots, you kind of know what the random function

36:09.760 --> 36:13.240
is going to behave there, and so you should have relatively low prediction error, places

36:13.240 --> 36:17.520
that you haven't visited that much, and you're going to have a higher prediction error,

36:17.520 --> 36:21.840
and so you'll be incentivized to go find those and explore those more.

36:21.840 --> 36:29.920
And so they use that approach on Montezuma, which is this hard expression game, and they

36:29.920 --> 36:32.080
get some really nice results using that.

36:32.080 --> 36:40.160
Is Montezuma still considered to be, you know, unplayable, you know, with high performance

36:40.160 --> 36:47.520
by RL agents, or have we kind of cracked that with this and other recent agents?

36:47.520 --> 36:50.520
Yeah, I mean, it's still pretty challenging.

36:50.520 --> 36:57.680
Yeah, with a person like this and other recent agents, I think in this paper, they get super

36:57.680 --> 36:59.480
human performance.

36:59.480 --> 37:00.480
Okay.

37:00.480 --> 37:01.480
Yeah.

37:01.480 --> 37:02.480
Okay, interesting.

37:02.480 --> 37:11.640
So these are always to kind of structure the structure, the exploration space, and

37:11.640 --> 37:18.040
kind of create different types of ensembles and train across those ensembles.

37:18.040 --> 37:19.040
That's right.

37:19.040 --> 37:21.960
Okay, very cool.

37:21.960 --> 37:26.640
And then the other one that I guess I'll just mention briefly that is sort of in the category,

37:26.640 --> 37:33.080
but as long as this is dealing with the other type of uncertainty, was a paper implicit

37:33.080 --> 37:41.000
quantile networks for distribution RL that was, will that be Geogos Grobski, David Silver

37:41.000 --> 37:47.480
and Ren Yunus, and it's sort of building on some of these previous ideas in distribution

37:47.480 --> 37:53.840
RL were the kind of the ideas, rather than just predicting the expected return, I'm going

37:53.840 --> 37:58.280
to try and predict the distribution of possible returns.

37:58.280 --> 38:00.760
And so that could happen.

38:00.760 --> 38:05.480
So if there is sort of intrinsic uncertainty in the environment, then even if I'm following

38:05.480 --> 38:10.920
the same policy, how the world turns out could end up having variability in it, and rather

38:10.920 --> 38:14.960
than just kind of model the mean of that, you can also try and capture properties of the

38:14.960 --> 38:15.960
distribution.

38:15.960 --> 38:23.080
That's potentially interesting for two different reasons, one, it's a richer prediction problem,

38:23.080 --> 38:26.800
so you can sort of think of predicting that distribution as helping in the same way

38:26.800 --> 38:29.080
that auxiliary tasks do.

38:29.080 --> 38:35.760
And then the other is having those sorts of estimates are themselves useful in some

38:35.760 --> 38:36.760
cases.

38:36.760 --> 38:41.200
So, you know, if you want to be risk sensitive for instance, then you might prefer something

38:41.200 --> 38:48.400
that even though the mean might be higher, you might prefer a policy that has a lower

38:48.400 --> 38:54.800
mean, but also a lower variance, so you kind of avoid worse but rare outcomes.

38:54.800 --> 38:58.560
So, that was the one for that one.

38:58.560 --> 39:04.560
The other area that I thought I'd mention briefly, and again, I think this is an area

39:04.560 --> 39:10.000
that we'll see more and more interesting developments over the coming years, is in model-based

39:10.000 --> 39:15.240
reinforcement learning, so this was another one from New York's this month, sample-efficient

39:15.240 --> 39:21.320
reinforcement learning with stochastic ensemble value expansion, and they have an algorithmic

39:21.320 --> 39:26.200
approach that they get the acronym Steve out of.

39:26.200 --> 39:32.880
And that was, I think, mostly Google brain folk, so Jake and Bookmoon don't know, I have

39:32.880 --> 39:39.080
to Joyce Chalker, Eugene Bervdo, and Holly Lakeley, and it's another one actually where

39:39.080 --> 39:47.480
they're using this trick of using a bootstrapped ensemble to help us capture model uncertainty.

39:47.480 --> 39:51.960
And here, they're actually using it in a model of the environment.

39:51.960 --> 39:56.840
So, in addition to learning our policy and value, we're also going to be trying to learn

39:56.840 --> 40:05.400
about how the world works, so given a particular state in an action, what is going to be my,

40:05.400 --> 40:09.360
what state will I transition into, and what will be the associated award?

40:09.360 --> 40:15.320
And if you have a really good model of the environment, then you can learn much more quickly

40:15.320 --> 40:22.080
because you can not only use the experience that you gather in the world, but you can also

40:22.080 --> 40:26.800
essentially simulate in your model what would happen if you did different things.

40:26.800 --> 40:32.440
But the problem, or one of the problems with that, is that if your model is inaccurate,

40:32.440 --> 40:37.640
then you can run into all sorts of trouble, and so what they do in this paper, they,

40:37.640 --> 40:40.720
because they're learning this model with an uncertainty estimate, they essentially

40:40.720 --> 40:48.720
are able to use the measure of model certainty to figure out how far to trust a model, their

40:48.720 --> 40:49.720
environment model.

40:49.720 --> 40:57.200
So, you essentially roll out your internal environment model to the point where you're essentially

40:57.200 --> 41:03.800
waiting how much you trust your model at different prediction depths into the future by how

41:03.800 --> 41:09.160
much uncertainty there is associated with that, and then at some point you end up bootstrapping.

41:09.160 --> 41:14.600
So, it's kind of a neat way of combining model based and model free RL.

41:14.600 --> 41:21.040
So, the other thing that, or one other area, and this is probably the last one that we'll

41:21.040 --> 41:26.000
touch on for today, I'll maybe just kind of be shout out to some of the other ones that

41:26.000 --> 41:29.400
we probably wouldn't have as much time to talk about.

41:29.400 --> 41:36.920
And that's this kind of trend of seeing sort of, ways in which DPRRL are being scaled

41:36.920 --> 41:45.200
up, so moving to kind of like these very large models, lots and lots of data distributed

41:45.200 --> 41:46.200
training systems.

41:46.200 --> 41:52.600
So, there's a couple of papers from this year that are doing that, so there was three,

41:52.600 --> 42:00.480
or there was a comment to mind as a three from demide and one from OpenAI, so we had this

42:00.480 --> 42:05.760
paper importance weighted at learner architectures in Paula, and so the set up there is, you

42:05.760 --> 42:12.760
have lots and lots of actors gathering experience, they send that experience to a smaller collection

42:12.760 --> 42:18.680
of learners, but even the learners can be distributed, and one of the kind of subtleties

42:18.680 --> 42:24.520
of that is, in that kind of set up, the experience that comes from the actors might be a little

42:24.520 --> 42:28.920
allowed of data, so there's a little bit of op-policy correction, but it essentially

42:28.920 --> 42:36.800
allows you to use extremely large batches and very big models, and still kind of get

42:36.800 --> 42:40.200
through a reasonable number of model updates in a short space of time, so that's kind

42:40.200 --> 42:42.480
of one direction for scaling.

42:42.480 --> 42:47.760
The others, I'll just mention these, I guess briefly, so there was a paper distributed

42:47.760 --> 42:55.560
prioritized experience replay, and that's essentially a kind of value learning analog of the

42:55.560 --> 43:00.040
impulsify I just mentioned, so again lots of active generating experience coming into

43:00.040 --> 43:06.320
a shared experience, a replay, and then learners pulling off on top of that.

43:06.320 --> 43:14.120
Another similar paper had an approach, D4PG, so there was all demide, and then the OpenAI

43:14.120 --> 43:19.480
data, I think there's not a paper on it, I'm not sure, but they kind of detail it in

43:19.480 --> 43:20.640
their blog post.

43:20.640 --> 43:26.800
They have a similar scheme, I think, to Impala where they have essentially lots of distributed

43:26.800 --> 43:35.800
workers generating experience, and then a small number of beefy large batch learners pulling

43:35.800 --> 43:40.040
from experience from the actors and generating parameter updates, and there's a couple of

43:40.040 --> 43:43.440
things there, as I said one, it sort of like allows you to kind of crunch through a lot

43:43.440 --> 43:48.280
more experience, and in some ways it's kind of just by scaling up in that way, it's able

43:48.280 --> 43:55.680
to address some of the inefficiencies of RL, so earlier we were talking a little bit

43:55.680 --> 44:00.600
about wanting to kind of pull as many different learning signals as we can out of the environment,

44:00.600 --> 44:06.160
particularly if reward is sparse or explosion is difficult, and there's sort of lots of

44:06.160 --> 44:10.160
different ways of tackling that, some are kind of more algorithmic, but also just by

44:10.160 --> 44:15.120
scaling things up and pushing a lot more data through, that's another way we're addressing

44:15.120 --> 44:20.240
that, and it also helps some of the kind of stability issues from the variants of the

44:20.240 --> 44:23.600
learning signals that you get a lot of these environments, so again I think that's another

44:23.600 --> 44:27.600
trend that we'll see just kind of being built on more and more, and you know, so it's

44:27.600 --> 44:32.200
somehow in computer vision kind of things have been scaling up and up over the past like

44:32.200 --> 44:33.200
five or six years.

44:33.200 --> 44:37.760
So yeah, maybe if I'll just mention some kind of other interesting areas that we work

44:37.760 --> 44:42.720
up time to dive into, but there's been some kind of interesting developments in hierarchy

44:42.720 --> 44:49.720
car, RL, multi-agent, there's been some neat work on sort of leveraging different types

44:49.720 --> 44:57.120
of memories in agents, and also just the kind of folding in of the sort of broader advances

44:57.120 --> 45:03.600
from machine learning in general in terms of better ways of doing representation learning,

45:03.600 --> 45:08.000
kind of some modeling, all those other kind of techniques, kind of feeding into DPRL.

45:08.000 --> 45:14.960
So just a quick review of the topics that we did cover here, imitation learning, unsupervised

45:14.960 --> 45:22.640
RL, meta-learning, exploration, model-based RL, and scaling up, and those are just a

45:22.640 --> 45:27.160
few of the ones that you were able to think of.

45:27.160 --> 45:34.240
So what would you say about the kind of the rate of growth of RL as a field?

45:34.240 --> 45:36.240
Yeah, super exciting.

45:36.240 --> 45:46.000
I mean, there's a lot of difficult problems that we don't yet have good ways to solve,

45:46.000 --> 45:50.480
but there's a ton of creative solutions coming out.

45:50.480 --> 46:00.720
Beyond papers, did you, did any other kind of broader developments in open source or on

46:00.720 --> 46:05.040
the commercial side, anything jump out at you?

46:05.040 --> 46:06.040
Yeah, couple.

46:06.040 --> 46:14.320
So I guess the commercial side, I've tracked a little less of late in terms of open source.

46:14.320 --> 46:21.320
There's a kind of background in terms of like the kind of continued development, both sort

46:21.320 --> 46:23.320
of like the TensorFlow and PyTorch frameworks in the sporting ecosystems.

46:23.320 --> 46:27.600
There've been a couple of things that there that I thought were pretty nice.

46:27.600 --> 46:35.760
So these like TF Hub, kind of having a kind of repository for models, and now they're

46:35.760 --> 46:37.760
kind of similar efforts.

46:37.760 --> 46:42.360
So there's a couple of frameworks that I saw at Neuros that I like.

46:42.360 --> 46:50.960
So there's dopamine, which is out of, I think, Google Brain Montreal, lucid, such a dopamine

46:50.960 --> 46:56.560
is a kind of a nice way of making some of these, yeah, with, with definitely lucid, and

46:56.560 --> 47:01.840
then there's also something from Uber Model 2, the kind of combination of those, it's kind

47:01.840 --> 47:08.200
of nice and that I think it'll maybe open up some opportunities to kind of play with

47:08.200 --> 47:10.920
deep RL with a lower entry bar.

47:10.920 --> 47:16.320
So with the model zoo they essentially have, I think, three or four different flavors

47:16.320 --> 47:22.200
of agent, pre-trained on all the games in the Atari Suite that you can just essentially

47:22.200 --> 47:28.480
download, and lucid is this kind of visualization framework.

47:28.480 --> 47:33.840
And so with the two of those, you can start to play with some of these pre-trained agents

47:33.840 --> 47:38.360
without necessarily having to have the resources to train them yourselves, which I think

47:38.360 --> 47:44.000
is probably something that will be helpful for new people wanting to get into the field,

47:44.000 --> 47:50.360
to kind of get a bit of a warm start, and then also, I think in general releasing pre-trained

47:50.360 --> 47:56.840
agents that people can build off of and poke is a nice trend, similar to some of the stuff

47:56.840 --> 48:03.480
we saw, again, sort of looking back at computer vision with making some of the pre-trained models

48:03.480 --> 48:07.760
on the ImageNet available that helped a lot of folks kind of get a foothold there, and

48:07.760 --> 48:11.520
then also then accelerated the progress in that field.

48:11.520 --> 48:19.920
With these tools and with RL models in general are the same kinds of things possible, like

48:19.920 --> 48:24.840
kind of taking off the last few layers of a network and fine-tuning, that same kind

48:24.840 --> 48:26.320
of transfer learning.

48:26.320 --> 48:34.240
I guess it would depend on the agent, not so much in what's available right now, the

48:34.240 --> 48:39.560
transfer between different Atari games, for instance, isn't huge, but just in terms of

48:39.560 --> 48:42.120
having something that can get you started.

48:42.120 --> 48:47.560
For instance, the kickstarting stuff that we were talking about earlier in the conversation,

48:47.560 --> 48:53.680
if you have pre-trained agents that have some decent behavior that can dramatically shorten

48:53.680 --> 48:58.720
your experience cycle time if you want to play with your own modifications of those architectures,

48:58.720 --> 49:04.760
eventually you might end up wanting to not use that pre-training to kind of get a clear

49:04.760 --> 49:09.320
look at innovations that you're looking at, but if you don't necessarily have access

49:09.320 --> 49:15.320
to a ton of compute resources, then leveraging things that are pre-trained is a really nice

49:15.320 --> 49:17.120
way of making it more accessible.

49:17.120 --> 49:18.120
Okay.

49:18.120 --> 49:23.640
So, no direct transfer in the way we might see with CNNs, but you can use these agents

49:23.640 --> 49:25.560
to kind of get things kicked off.

49:25.560 --> 49:32.560
Yeah, I think so, I guess, transfer in RL is an interesting conversation that we could

49:32.560 --> 49:38.840
have on the side, but yeah, it would depend on what task you want to transfer to, and

49:38.840 --> 49:43.480
there's a lot of subtleties there, so even things like training and simulation and deploying

49:43.480 --> 49:47.960
in the real world, you see some transfer there, so I guess that's one place where you could

49:47.960 --> 49:54.800
kind of think of that as being pre-training and fine-tuning, but yeah, I kind of think

49:54.800 --> 50:00.800
of that as a little differently than like I said, or at least in its current state, the

50:00.800 --> 50:04.920
model zero is just focused on Atari, so it's more kind of entry level, but I think that's

50:04.920 --> 50:09.120
probably still something that folks entering the field will find useful.

50:09.120 --> 50:10.120
Okay.

50:10.120 --> 50:11.280
Cool.

50:11.280 --> 50:19.600
And so, given all that we've seen happen in 2018, what are your kind of thoughts and

50:19.600 --> 50:25.440
predictions on what we'll see in 2019 and the near future beyond that?

50:25.440 --> 50:27.240
Yeah, prediction that was tough.

50:27.240 --> 50:37.120
I think a lot of the areas that I highlighted to me feel like they're particularly active

50:37.120 --> 50:43.520
frontiers, so yeah, I think we'll see more folks trying to come up with different ways

50:43.520 --> 50:51.000
of doing agent-based RL with more self-supervision, I think the kind of trend, sort of meta-learning,

50:51.000 --> 50:56.800
learning to learn, transfer learning, continual learning, so one agent learnings to do many,

50:56.800 --> 50:59.400
many different things through a lifetime.

50:59.400 --> 51:05.960
I trends will see, yeah, likewise, model-based RL, I feel like models have probably been a

51:05.960 --> 51:12.400
little less prominent in recent years, sorry, like in very models, definitely have been

51:12.400 --> 51:18.480
some papers that look to that, but I feel like that's an interesting area for growth,

51:18.480 --> 51:21.520
so yeah, those are a couple that come to mind.

51:21.520 --> 51:25.200
Any final thoughts before we wrap up?

51:25.200 --> 51:31.760
Yeah, I think a super exciting time in the field is kind of tons of groups, both from

51:31.760 --> 51:37.040
academia and industry that are doing really great work, and so yeah, it's a super exciting

51:37.040 --> 51:42.280
time to be doing research in the field, and then also, I guess, through the broader machine

51:42.280 --> 51:46.960
learning community, there's a lot of great efforts in terms of just working to expand

51:46.960 --> 51:51.200
that the pool of folks who can participate in AI research and who can kind of contribute

51:51.200 --> 51:54.760
to some of these problems that we're looking at, so yeah, I'm super optimistic for the

51:54.760 --> 51:55.760
future.

51:55.760 --> 52:00.440
Well Simon, thanks so much for taking the time to chat with us about RL, there's a ton

52:00.440 --> 52:06.720
of stuff here, and we'll try to get links to the various papers that you mentioned in

52:06.720 --> 52:08.760
the show notes, but thank you.

52:08.760 --> 52:17.320
All right, everyone, that's our show for today.

52:17.320 --> 52:22.920
For more information on Simon or any of the topics covered in this show, visit twimlai.com

52:22.920 --> 52:26.000
slash talk slash 217.

52:26.000 --> 52:33.960
You can also follow along with our AI rewind 2018 series at twimlai.com slash rewind 18.

52:33.960 --> 52:38.640
As always, thanks so much for listening and catch you next time.

52:38.640 --> 52:39.640
Happy Holidays.

