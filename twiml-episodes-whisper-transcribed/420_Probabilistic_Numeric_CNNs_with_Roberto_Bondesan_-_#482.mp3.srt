1
00:00:00,000 --> 00:00:17,440
All right, everyone. I am here with Roberto Bandizan. Roberto is an AI researcher at Qualcomm.

2
00:00:17,440 --> 00:00:20,080
Roberto, welcome to the Twomo AI podcast.

3
00:00:20,080 --> 00:00:26,960
Hi, Sam. Thank you. I'm really looking forward to our chat. We'll be talking about your paper

4
00:00:26,960 --> 00:00:35,040
at ICLR, probabilistic numerical, numeric CNNs. But before we dig into that, I'd love to hear

5
00:00:35,040 --> 00:00:41,600
you share a little bit about your background and how you came to work in AI. Sure. So my background

6
00:00:41,600 --> 00:00:49,440
is in physics, and that's how I got into AI by basically applying deep learning to some physics

7
00:00:49,440 --> 00:00:56,800
problems. So before joining Qualcomm, I was working on characterizing new phases of matter and

8
00:00:56,800 --> 00:01:03,920
their potential for quantum computation. So phase of matter is basically one of the states

9
00:01:05,280 --> 00:01:10,800
you can find matter or materials around you. And one classical example is provided by the

10
00:01:10,800 --> 00:01:17,360
Easy Model. So in the Easy Model, you have some binary units, some bits which can be either

11
00:01:17,360 --> 00:01:25,520
up or down. And you can find this system into phases. One order phase, which is at a low

12
00:01:25,520 --> 00:01:31,360
temperature, where all these units, we call them spins, point in the same direction, and a high

13
00:01:31,360 --> 00:01:37,680
temperature disorder phase, where these units point in random directions. It turns out that

14
00:01:37,680 --> 00:01:43,200
characterizing phases of matter is a very challenging problem. And so researchers in the field

15
00:01:43,200 --> 00:01:51,600
they started to use AI for that. So that's how I got into AI. And as I was, you know, just learning

16
00:01:51,600 --> 00:01:58,080
about the techniques that were being developed, if this was around 2017-2018, for these problems,

17
00:01:58,080 --> 00:02:03,360
I got very interested in AI per se, and then I decided a career shift and to move

18
00:02:03,360 --> 00:02:10,800
in there to do research in AI. At this point, it was also when the paper by Taco Cohen and Max

19
00:02:10,800 --> 00:02:16,720
Welding were my colleagues at Qualcomm. Their paper on spherical CNNs came out. And so at that

20
00:02:16,720 --> 00:02:23,760
point, I understood that, you know, studying neural networks using insights from physics was a very

21
00:02:23,760 --> 00:02:30,800
useful and interesting thing. And that's what I wanted to work on. So did you get, were you familiar

22
00:02:30,800 --> 00:02:37,120
with the spherical CNN paper before you came to work with Taco and Max? Yes, yes, definitely.

23
00:02:37,120 --> 00:02:43,840
That's indeed one of the links that brought me to Qualcomm, yeah. That's fantastic. So tell us

24
00:02:43,840 --> 00:02:52,320
a little bit about your research interests. Yeah, so after I joined Qualcomm, I got involved

25
00:02:52,320 --> 00:03:00,080
into quite a few exciting projects. So one of them has to deal with the neural data compression.

26
00:03:00,080 --> 00:03:09,040
So here, the problem is that you want to compress data to, you know, send them over to some receiver.

27
00:03:09,040 --> 00:03:15,120
And you can actually use neural networks in particular, some forms of auto encoder to do that.

28
00:03:15,120 --> 00:03:19,920
So that's a quite, you know, interesting problem from the theoretical point of view. It involves

29
00:03:19,920 --> 00:03:24,320
generative modeling and things like that. But it is also super impactful because, you know,

30
00:03:24,320 --> 00:03:30,960
it can immediately translate into different ways to compress data. So that's a very interesting

31
00:03:30,960 --> 00:03:39,040
direction. Otherwise, I'm also very excited about Quantum AI. So after joining Qualcomm, I also

32
00:03:39,040 --> 00:03:45,040
had the chance to develop some ideas around quantum deep learning. So this is the, you know,

33
00:03:45,040 --> 00:03:50,400
intersection between quantum computation and the NDI. And so here is where also my background

34
00:03:50,400 --> 00:03:56,720
in quantum thesis was quite useful to, you know, get started quickly on this field, which is

35
00:03:56,720 --> 00:04:01,440
a rapidly developing field, which I think is one of the most interesting, you know, directions,

36
00:04:01,440 --> 00:04:10,480
which can disrupt AI in the future. And finally, more recently, I got interested in also applying

37
00:04:10,480 --> 00:04:15,680
machine learning to combinatorial optimization problems. So, you know, in industry, we have a lot

38
00:04:15,680 --> 00:04:21,840
of combinatorial optimization problems to solve. And so the potential of using machine learning

39
00:04:21,840 --> 00:04:26,800
to, you know, improve on the classical techniques is very interesting. And again,

40
00:04:26,800 --> 00:04:32,800
this is a very interesting, you know, area between, you know, very theoretical mathematical work,

41
00:04:32,800 --> 00:04:39,040
which, you know, is interesting to me and also very impactful work. So, yeah, in summary,

42
00:04:39,040 --> 00:04:47,760
I had the chance, you know, of both doing impactful work for society, you say at large,

43
00:04:47,760 --> 00:04:56,160
but also, you know, being able to do some long-term research like quantum AI. So I'm very excited

44
00:04:56,160 --> 00:05:02,560
about, you know, 15s that are going on at Qualcomm. I know. Nice. And when you say combinatorial

45
00:05:02,560 --> 00:05:07,200
optimization, these are problems like traveling salesmen and map coloring and that kind of thing,

46
00:05:07,200 --> 00:05:13,120
like classical combinatorial problems. Yeah, precisely. So, in fact, you know, in the last few

47
00:05:13,120 --> 00:05:22,240
years, maybe three, four years, there's been quite, you know, good progress on using deep learning

48
00:05:22,240 --> 00:05:29,280
for these problems. So, you know, traveling salesmen indeed is the standard paradigm of combinatorial

49
00:05:29,280 --> 00:05:33,840
optimization problem, right? When you have a salesman, we need to find the optimal tool to visit

50
00:05:33,840 --> 00:05:39,120
the series of cities. And so it has, of course, also direct applications in industry, for example,

51
00:05:39,120 --> 00:05:44,560
vehicle routing, right? Where a company has to deliver stuff and so we need to find the optimal

52
00:05:44,560 --> 00:05:51,040
route. And so, yeah, traveling salesmen is certainly a great example. And it also historic,

53
00:05:51,040 --> 00:05:55,680
I just driven most of the research combinatorial optimization and it is also in the intersection

54
00:05:55,680 --> 00:06:02,640
with machine learning. But also, I'm thinking about other problems like cheap design or, you know,

55
00:06:02,640 --> 00:06:08,000
some problems, combinatorial problems in wireless. So, these are also problems that are, you know,

56
00:06:08,000 --> 00:06:14,160
hard. So, in fact, there are some empty complete, you know, problems there. And, you know,

57
00:06:14,160 --> 00:06:17,200
these are also very interesting application areas for us.

58
00:06:18,400 --> 00:06:24,080
So, and cheap design, that would be things like routing traces on a circuit board or on a

59
00:06:24,080 --> 00:06:28,800
chip. Yeah, so there are indeed a few stages of cheap designs and all of them, in fact,

60
00:06:28,800 --> 00:06:34,480
involve different combinatorial optimization problems. And, yeah, so as you say, routing,

61
00:06:34,480 --> 00:06:40,880
so finding the optimal connections between the different logical gates for memories on a

62
00:06:40,880 --> 00:06:47,440
cheap canvas. And also, you can think about indeed also placing these components on a cheap

63
00:06:47,440 --> 00:06:52,960
in the optimal way to minimize area and stuff like that. Indeed, this is all very interesting

64
00:06:52,960 --> 00:06:56,960
combinatorial problems where, you know, I could disrupt.

65
00:06:56,960 --> 00:07:03,120
And on the wireless side, I'm imagining that's like frequency allocations and things like that,

66
00:07:03,120 --> 00:07:07,840
or what are some of the applications there? So, I have in mind, you know, things like

67
00:07:08,640 --> 00:07:16,320
some coding problems where, you know, you basically have sent some signal over and then you want to,

68
00:07:16,960 --> 00:07:24,880
you know, retrieve what was the bit string that was sent. But, you know, your signal has been

69
00:07:24,880 --> 00:07:30,080
corrupted by a noisy channel and you want to retrieve that, that these strings, that's one

70
00:07:30,080 --> 00:07:36,160
example of things that one could do or some other error correcting problems and things like that,

71
00:07:36,160 --> 00:07:43,040
yeah. So, there's a bit of a relationship between the combinatorics and kind of information

72
00:07:43,040 --> 00:07:48,240
theoretical types of problems and oppression, which is where you spend a bulk of your time.

73
00:07:48,240 --> 00:07:53,840
Yeah, absolutely. It is also fair to say indeed that all of these problems, you know,

74
00:07:53,840 --> 00:08:01,600
compression, also other problems that we work on, like quantization. These are all combinatorial

75
00:08:01,600 --> 00:08:07,520
nature. So, these are certainly possible use cases for this nesting field of ML for combinatorial

76
00:08:07,520 --> 00:08:14,720
optimization. Yeah. Nice, nice. It's an important area. Yeah. So, you're probabilistic numeric CNN's

77
00:08:14,720 --> 00:08:23,680
paper. What's the problem area that you're addressing there? Sure. So, there, the problem area

78
00:08:23,680 --> 00:08:33,680
is the application of deep learning to, you know, signals that are not necessarily

79
00:08:34,560 --> 00:08:41,440
sampled on a grid. Think about, you know, in many applications of deep learning, you, for

80
00:08:41,440 --> 00:08:48,400
example, want to model some time series and these time series, you know, have not been,

81
00:08:48,400 --> 00:08:55,760
you know, sampled uniformly. So, that's one possible, you know, use case for the models we

82
00:08:55,760 --> 00:09:02,320
want to develop in probabilistic numeric CNN's. But more generally, the motivation there

83
00:09:02,320 --> 00:09:10,080
is really, you know, trying to think about the signals that we want to model, you know,

84
00:09:10,080 --> 00:09:14,480
in machine learning, in their continuous formulation, not in their discrete formulation,

85
00:09:14,480 --> 00:09:21,280
which is, you know, the natural formulation you observe, you know, when you measure something.

86
00:09:21,280 --> 00:09:29,120
And so, basically, just to unpack a little bit of the title, right? So, the title starts

87
00:09:29,120 --> 00:09:34,960
with probabilistic numeric. So, the inspiration for this work is really this field of probabilistic

88
00:09:34,960 --> 00:09:39,680
numeric. So, you certainly have a media with a convolution on your networks, so I do not need to

89
00:09:39,680 --> 00:09:44,960
introduce that. But so, let me just spend a couple of words on probabilistic numerics, which

90
00:09:44,960 --> 00:09:52,000
might not be familiar to everyone. So, probabilistic numeric is a recent field in statistics,

91
00:09:52,000 --> 00:09:57,840
which tries to, I mean, not necessarily recently, but recently, I think there's been a

92
00:09:57,840 --> 00:10:06,800
quite big developments. And so, it tries to quantify the uncertainty that a numerical program

93
00:10:06,800 --> 00:10:14,160
has, due to the discreteness of the sampling procedure of its input. To make this concrete,

94
00:10:14,160 --> 00:10:19,760
let's think about the problem of computing a numerical integral. You know, the function that you

95
00:10:19,760 --> 00:10:25,120
want to integrate analytically, you can have evaluated everywhere on a continuous range in its

96
00:10:25,120 --> 00:10:31,760
domain, but necessarily, you need to sample that function on a discrete sample of points,

97
00:10:31,760 --> 00:10:38,240
because your memory and time to compute that integral is finite. And so, probabilistic numeric

98
00:10:38,240 --> 00:10:44,480
tells you a way to derive uncertainty from this sampling procedure. And the way it is done

99
00:10:44,480 --> 00:10:51,840
is via Bayesian inference. So, in Bayesian inference, one has an agent, a machine learning model,

100
00:10:51,840 --> 00:10:59,520
that has a prior. Here, the prior will be over the set of possible functions that you want to

101
00:10:59,520 --> 00:11:06,240
integrate. And in technical terms, it is a Gaussian process. So, a Gaussian prior on this set of

102
00:11:06,240 --> 00:11:11,680
functions. And then, upon measuring that function, you want to integrate on some points in your

103
00:11:11,680 --> 00:11:18,640
domain, you update your prior to a posterior. And this allows you to immediately translate to some

104
00:11:18,640 --> 00:11:24,960
uncertainty on the result of your integral. So, now, your numerical program will not just return

105
00:11:24,960 --> 00:11:30,080
a number, which will return a probability distribution. And this probability distribution will be

106
00:11:30,080 --> 00:11:35,520
picked around some value, and there will be an uncertainty. So, that's your discretization error.

107
00:11:36,400 --> 00:11:40,640
So, we thought that this is quite interesting also for machine learning. And so, we,

108
00:11:41,360 --> 00:11:47,200
indeed, start from the same philosophical standpoint, you know, the images that we want to model

109
00:11:47,200 --> 00:11:52,000
in machine learning, the time series that we want to model, are in fact, continuous signal.

110
00:11:52,000 --> 00:11:57,920
And so, we necessarily need to discretize them because we want to put them in a computer.

111
00:11:57,920 --> 00:12:03,120
And, but, you know, this procedure will come with some uncertainty with some errors. And so,

112
00:12:03,120 --> 00:12:09,440
it is important to quantify those. And so, that's basically the motivation behind this work.

113
00:12:09,440 --> 00:12:22,160
If I could replay that to make sure I'm understanding with classical numerical programs, like I'm thinking

114
00:12:22,160 --> 00:12:28,400
back to Fortran, numerical computing and undergrad, like you've got some function, and you want to

115
00:12:28,400 --> 00:12:33,760
compute an integral for it. And you just, you do that. There are established outcomes for doing that.

116
00:12:33,760 --> 00:12:40,560
What probabilistic layers on top of that is allowing you to look at your, the function that you're

117
00:12:40,560 --> 00:12:47,920
integrating, not as a single function, but as a distribution of functions. And what you have then,

118
00:12:49,520 --> 00:12:57,120
you know, then your kind of classic quantization error now becomes a probabilistic quantization error.

119
00:12:57,120 --> 00:13:03,920
Yeah, that's a good summary. Thanks. And maybe just to clarify, so here we are really looking at the

120
00:13:03,920 --> 00:13:10,080
quantization, right, in the domain of the function. So, the value that the function takes

121
00:13:10,800 --> 00:13:17,120
are still continuous. And so, that's the kind of quantization we are looking at. And so, indeed,

122
00:13:17,120 --> 00:13:27,680
in our probabilistic numeric CNN, so we start from this idea. And then we develop on top in your

123
00:13:27,680 --> 00:13:34,400
network. I also want to interrupt to say that when you say the quantization is in the domain of

124
00:13:34,400 --> 00:13:40,880
the function, meaning as opposed to the range, which is your think about your vertical amplitude,

125
00:13:40,880 --> 00:13:45,600
here we're talking about you're taking different points in time that may or not be, may or may

126
00:13:45,600 --> 00:13:51,280
not be, well, are not uniform. And so, that's where your quantization is coming in. So, you've got a

127
00:13:51,280 --> 00:13:57,440
time series, but you're not getting data in every second or millisecond or whatever it comes

128
00:13:58,880 --> 00:14:05,120
irregularly. And you're trying to figure out quantization error based on that irregularity.

129
00:14:05,840 --> 00:14:12,800
That's precisely it. Yeah. Okay. Yeah. All right. Cool. So, and indeed, maybe just to give you a little

130
00:14:12,800 --> 00:14:24,320
more about the paper, so we develop the idea of using probabilistic numeric for deep learning.

131
00:14:24,320 --> 00:14:31,600
So, the first step in this procedure is that we start indeed from a regular sample time series,

132
00:14:31,600 --> 00:14:38,240
for example, or even from an image, which has been sub-sampled in a regular way. And what we do

133
00:14:38,240 --> 00:14:43,680
is that we interpolate that. So, we interpolate that in a probabilistic way. So, like it, is that

134
00:14:44,240 --> 00:14:52,480
like a probabilistic numerical programs do. And that gives us my posterior distribution over

135
00:14:52,480 --> 00:14:59,280
our input. And what we do then on this posterior distribution is that we apply a neural net. So,

136
00:14:59,280 --> 00:15:04,560
now this posterior distribution is a distribution of a continuous functions. And so, the technical

137
00:15:04,560 --> 00:15:11,280
contribution that we make in this paper is to devise a neural network on continuous functions.

138
00:15:11,280 --> 00:15:18,640
So, typically, your CNN, we lack on vectors, right? There's some array of numbers.

139
00:15:18,640 --> 00:15:24,560
Here, our probabilistic numeric CNN is defined directly in the continuum. And that turns out

140
00:15:24,560 --> 00:15:30,800
to be quite powerful. And also unlocks, you know, new models and new mechanisms for learning.

141
00:15:30,800 --> 00:15:38,080
Which have, what does that exactly mean? I think, yes, I'm so used to thinking about the input

142
00:15:38,080 --> 00:15:44,160
to a CNN being a vector. I'm not even sure how to unpack it being continuous.

143
00:15:44,160 --> 00:15:50,560
Right. So, indeed, you're not going to store that function in your computer, because by definition,

144
00:15:50,560 --> 00:15:59,200
indeed, you're going to need to, you know, have, you know, an infinite number of points if you

145
00:15:59,200 --> 00:16:04,880
want to store all the values. What you're going to store is just some function of form,

146
00:16:04,880 --> 00:16:10,080
some code that allows you to evaluate that function, right? And so, that's somehow the input to,

147
00:16:10,080 --> 00:16:16,720
to your, you know, neural network. And so, to be precise, indeed, about what happens,

148
00:16:16,720 --> 00:16:22,880
we still have a neural network which works by interliving linear, nonlinear layers. But now,

149
00:16:22,880 --> 00:16:27,760
and so the nonlinear layer, you can actually morally understand that it's going to be very

150
00:16:27,760 --> 00:16:32,800
similar to what you're used to do at each point of your function applied nonlinearity. But now,

151
00:16:32,800 --> 00:16:40,320
the real, you know, new part of the work is about the linear layer. So, we devise actually a

152
00:16:41,520 --> 00:16:46,560
new convolutional layer, which is defined in terms of a linear PD, the partial differential

153
00:16:46,560 --> 00:16:52,720
equation. So, this partial differential equation is a linear operation on an input function,

154
00:16:52,720 --> 00:17:00,480
which is the input sum out to the PD, namely the value that you have the initial condition

155
00:17:00,480 --> 00:17:09,200
to your differential equation. So, what happens is that, you know, if you want to impose actually

156
00:17:09,200 --> 00:17:14,960
translation equivalence that you have, you know, in convolutional layer, this restricts the forms

157
00:17:14,960 --> 00:17:20,400
of differential equations that you can input in your neural network. And interestingly,

158
00:17:20,400 --> 00:17:27,040
you know, one of the simplest things you can do is to use the kind of generalized diffusion

159
00:17:27,040 --> 00:17:32,880
equation. So, diffusion, you know, is a process from physics, which you can understand, you know,

160
00:17:32,880 --> 00:17:37,600
for example, when you have a glass of water, you put some dye into it and this dye diffuses

161
00:17:38,240 --> 00:17:44,320
over time. And so, similarly here, you know, we have our image, which is encoded, you know,

162
00:17:44,320 --> 00:17:50,640
in some function. And that function gets blurred, similar to the diffusion process over time.

163
00:17:50,640 --> 00:17:57,280
So, that's really what we mean, you know, by the layer on continuous functions. So,

164
00:17:57,280 --> 00:18:03,600
it is defined formally. And it turns out that for certain choices, indeed, of layers,

165
00:18:03,600 --> 00:18:09,600
we can do computations analytically. So, we can actually propagate these, you know,

166
00:18:09,600 --> 00:18:15,920
functional forms in our code analytically. And so, that's a pretty cool.

167
00:18:17,840 --> 00:18:25,040
And so, we can, you know, ultimately, we can devise a practical procedure to, you know,

168
00:18:25,040 --> 00:18:32,320
start from our input signal, which was, you know, this sub-sampled signal, then interpolate it,

169
00:18:32,320 --> 00:18:38,400
then we apply this convolutional layer as PDEs. We interleave with some non-linearities.

170
00:18:38,400 --> 00:18:43,520
And what we get out, after some of these layers, and perhaps the pooling and so on,

171
00:18:44,400 --> 00:18:49,200
we get out, you know, a prediction. Like, you know, we want to classify this time serious,

172
00:18:49,200 --> 00:18:55,760
for example, this input image. And so, we want to get out a class label, right? As we do usually,

173
00:18:55,760 --> 00:19:00,800
but on top of that, we also get out an uncertainty. And actually, this uncertainty is also there

174
00:19:00,800 --> 00:19:06,400
at T-EV intermediate layer. And it's really an uncertainty that is related to the, you know,

175
00:19:06,400 --> 00:19:12,880
entity, the input signal didn't have maybe information in certain regions of space or time.

176
00:19:14,000 --> 00:19:18,720
So, in this way, we know, you know, we can characterize indeed what is the error that we make.

177
00:19:18,720 --> 00:19:24,320
And more interesting, we can also choose where to sample the signal in order to reduce uncertainty.

178
00:19:25,440 --> 00:19:30,000
So, these are all the interesting applications that we can think about with this model.

179
00:19:30,000 --> 00:19:42,080
Is that, is that ladder point choosing where to sample? Is that a, like a byproduct of

180
00:19:42,960 --> 00:19:47,280
going through the process in the same direction, or is it more like going through the process

181
00:19:47,280 --> 00:19:52,160
backwards? I don't know if that question makes it. It's going to the process backwards, you're right,

182
00:19:52,160 --> 00:19:59,840
it's a small, you basically, you know, find a certain uncertainty and this uncertainty will be a

183
00:19:59,840 --> 00:20:04,320
function of where your value to your input. So, you can compute some kind of derivative of that

184
00:20:04,320 --> 00:20:09,760
to the spread to the inputs to minimize the uncertainty. And that's, you know, that can be

185
00:20:09,760 --> 00:20:15,600
useful, you know, when it is, for example, costly to get data points. You can optimize the points

186
00:20:15,600 --> 00:20:20,560
that are most informative. Or, you know, when you have maybe some data on meshes and things like that,

187
00:20:20,560 --> 00:20:24,880
you know, where discretization errors are important. So, there are a lot of interesting, you know,

188
00:20:24,880 --> 00:20:33,760
use cases. So, in this paper, actually, we focus mostly on a benchmarking this model on a couple

189
00:20:33,760 --> 00:20:42,640
of data sets. So, one is the super pixel classification of images. So, super pixel, you know, is just

190
00:20:42,640 --> 00:20:50,560
an image, which is sub-sampled, but again, the points are not on a grid. Before we get to the

191
00:20:50,560 --> 00:20:58,400
benchmark, another question about the architecture here. So, you, one of the key innovations or

192
00:20:58,400 --> 00:21:06,080
contributions, it sounds like it's this PDE layer. Yes. And PDE's arise in physics all the time,

193
00:21:06,080 --> 00:21:12,720
like you can, I'm imagining the inspiration of that was thinking about the problem, like the

194
00:21:12,720 --> 00:21:17,280
closed form problem and how you might solve it. And then, you know, that involves PDE's.

195
00:21:19,920 --> 00:21:26,560
Yes. So, PDE, yeah, good. Sorry. No, no, I was, I was going to, you know, but then you get to that

196
00:21:26,560 --> 00:21:32,400
so that your PDE, you have this PDE layer that you think needs to be involved in here, but it,

197
00:21:32,400 --> 00:21:39,120
you have the constraints of translation, invariance from CNNs, and then suddenly you're like,

198
00:21:39,120 --> 00:21:44,880
okay, diffusion is the answer. And like, where did that come from? Was that? Did you,

199
00:21:46,400 --> 00:21:55,120
did you recognize diffusion as like a translational independence by thinking about a glass of water,

200
00:21:55,120 --> 00:22:02,240
or is that like a known physics thing? Yeah. So, yeah. So, the, the way we got there, and actually,

201
00:22:03,360 --> 00:22:09,360
I would like at this point to amend the one of the big omissions that I've done in the beginning,

202
00:22:09,360 --> 00:22:14,640
which is not to acknowledge that the first author of this paper is Mark Fincy, who was doing

203
00:22:14,640 --> 00:22:20,400
an internship with us last summer. So, he's really the main driver driving force in this project.

204
00:22:20,400 --> 00:22:27,600
And so, you know, Mark came up with this proposal, and I guess it was a bit of a mixture of two things.

205
00:22:27,600 --> 00:22:32,160
One thing was intuition, and the other thing may become from physical reasoning, and the other

206
00:22:32,160 --> 00:22:37,200
thing was just a mathematical formalism. So, we wrote down, you know, the most general,

207
00:22:37,840 --> 00:22:45,360
basically, local, you know, linear layer in the former PDE, and then, you know, basically,

208
00:22:45,360 --> 00:22:51,280
this turned out to be diffusion when you imposed translation in variance.

209
00:22:51,280 --> 00:22:57,280
And actually, we also, you know, did something a bit more general. So, we also consider the,

210
00:22:57,280 --> 00:23:01,520
you know, symmetries like rotation and things like that. So, we thought a little bit about

211
00:23:01,520 --> 00:23:07,360
spherical CNN at the beginning. So, there is, you know, interest in the community in characterizing

212
00:23:07,360 --> 00:23:11,840
equipment against under more general symmetries. And so, it turns out that, you know,

213
00:23:11,840 --> 00:23:16,960
beautifully, also, in this context, we can get, you know, PDE's, which are, you know,

214
00:23:16,960 --> 00:23:22,160
equivalent under more general transformations, like rotations at things like that.

215
00:23:22,160 --> 00:23:28,160
And that's actually quite interesting, I believe, because, you know, one of the problems with,

216
00:23:28,160 --> 00:23:34,640
you know, getting to work, also, the equivalent under rotations, say, is that you necessarily need

217
00:23:34,640 --> 00:23:41,840
to discretize things on a lattice. And so, at that point, you know, the rotation for a certain angle

218
00:23:41,840 --> 00:23:47,360
becomes, you know, pretty tricky to get it to work well and necessarily, you know, you will have

219
00:23:47,360 --> 00:23:53,360
some error, which is due to the, basically, mesh of your lattice and so on. In this context,

220
00:23:53,360 --> 00:23:58,560
we avoided this problem. So, our model is defining the continuum, and you know, it is basically

221
00:23:58,560 --> 00:24:05,280
equivariant under arbitrary rotations. So, that's a pretty cool, I think, feature,

222
00:24:06,000 --> 00:24:11,440
and also equivalent on the arbitrary translation. So, that's, I think, a pretty cool feature too.

223
00:24:12,000 --> 00:24:16,880
Right. Yeah. I'll just interject really quickly that the, this whole idea of

224
00:24:16,880 --> 00:24:23,760
equivalence and spherical CNNs and gauge equivalence is a big focus of the AI research team

225
00:24:23,760 --> 00:24:30,400
there at Qualcomm. And for folks that want to dig in more, that's probably the best place to

226
00:24:30,400 --> 00:24:35,200
start as the first interview I did with Max Welling on gauge equivalence CNNs, or we talk about

227
00:24:35,200 --> 00:24:40,960
a lot of this, what equivalence is and why it's important. And we'll drop a link to that in the show

228
00:24:40,960 --> 00:24:49,680
notes. Yeah, thanks. I also listened to that. It was a great episode. Yeah. Awesome. Awesome.

229
00:24:49,680 --> 00:24:56,560
So, you were talking about benchmarking? Yeah, indeed. So, I was talking about the fact

230
00:24:56,560 --> 00:25:03,440
that we benchmarked on a couple of data sets. So, the first one was this super pixel images. So,

231
00:25:03,440 --> 00:25:09,920
you start from an image. And, you know, suppose it is defined on a grid and then you sub-sample it.

232
00:25:09,920 --> 00:25:14,720
So, you take away some of these points. It's such a way that then it becomes, you know,

233
00:25:14,720 --> 00:25:20,640
the grid structure is lost. And, you know, at this point, your regular CNN will not work well

234
00:25:20,640 --> 00:25:27,200
for this data type. There are a few other competitors out there, but it turns out that our model

235
00:25:27,200 --> 00:25:34,480
basically established a new state of the art for this task. So, three times reduction in the

236
00:25:34,480 --> 00:25:43,600
test error. And so, this was quite encouraging. And we also applied, you know, the model to

237
00:25:43,600 --> 00:25:49,200
medical time series. So, in this case, you know, you can think about, you know, patients

238
00:25:49,200 --> 00:25:56,960
goes to the hospital. And then, you know, for example, the doctor measures, you know, blood pressure,

239
00:25:56,960 --> 00:26:01,760
things like that. And this is done at the regular times, right? So, this is also a good, you know,

240
00:26:01,760 --> 00:26:06,160
case of a regular time series. And then, based on these measurements, you want to predict,

241
00:26:06,160 --> 00:26:12,160
you know, if the patient will recover things like that. And in fact, so, this is our,

242
00:26:12,160 --> 00:26:18,400
these are pretty important data sets to look at. And so, we also applied our model to these

243
00:26:18,400 --> 00:26:28,480
data sets and show competitive results there too. Yeah, so, we basically, you know, think that

244
00:26:28,480 --> 00:26:37,520
this point of view is very powerful. And, yeah, in fact, we have a few, you know, future directions

245
00:26:37,520 --> 00:26:47,040
in mind that came after this, this work. And what are those? Right. So, one of the interesting

246
00:26:47,040 --> 00:26:53,840
directions for this work, in my opinion, is to connect it to quantum computation. In fact,

247
00:26:54,960 --> 00:27:01,120
one of the promising platforms for quantum computation is a so-called quantum optical computer.

248
00:27:01,120 --> 00:27:09,600
So, here, optical means that you use light as the, you know, basically unit of information that

249
00:27:09,600 --> 00:27:16,320
you want to manipulate. And it turns out that, you know, there are states of light that, you know,

250
00:27:16,320 --> 00:27:24,400
people know how to build in a lab, which are closely related to Gaussian processes.

251
00:27:24,960 --> 00:27:29,440
So, there is a beautiful connection here between states of light and Gaussian processes.

252
00:27:29,440 --> 00:27:37,600
And they immediately disperse a connection also between, you know, our model and a possible

253
00:27:37,600 --> 00:27:46,960
generalization to a quantum model, so a quantum neural network. And so, I mean, started about this

254
00:27:46,960 --> 00:27:54,640
because, you know, it seems to me a very natural, basically, way to encode the data via this

255
00:27:54,640 --> 00:28:00,880
relationship between Gaussian processes and certain states, Gaussian states of light. So,

256
00:28:00,880 --> 00:28:06,080
that's, I think, a very natural way to encode data in a quantum computer, which operates on

257
00:28:06,640 --> 00:28:13,280
with optical elements. And therefore, you know, there is also a nice way to interpret, basically,

258
00:28:13,280 --> 00:28:18,080
the probabilistic numeric, new and network from this point of view of quantum information,

259
00:28:18,080 --> 00:28:25,520
quantum optics, quantum mechanics. So, there is a big, basically, direction here that I'm

260
00:28:25,520 --> 00:28:30,560
excited about, which spurred out of this, of this paper, actually, and this new way to think

261
00:28:30,560 --> 00:28:34,240
about inputs to neural networks. Think about neural networks.

262
00:28:35,520 --> 00:28:41,200
Is the primary connection there thinking about continuous functions, or is there also this

263
00:28:41,200 --> 00:28:49,760
aspect of missing or irregularly sample data? Yeah, I would say both. So, the fact that indeed,

264
00:28:49,760 --> 00:28:58,720
you have continuous function relates to what, you know, physicists call, basically, kind of model

265
00:28:58,720 --> 00:29:03,920
that physicists use, which is called quantum field theory. So, the quantum field theory is also

266
00:29:03,920 --> 00:29:09,360
a continuous field. And so, your continuous field classically, right, which is your function,

267
00:29:09,360 --> 00:29:13,440
which is continuous, becomes now a quantum function, so it becomes a quantum field.

268
00:29:14,080 --> 00:29:18,400
And this is quite exciting, I think, because this, you know, these quantum fields are relevant

269
00:29:18,400 --> 00:29:24,800
for describing, you know, elementary particles. So, this kind of experiments that you see,

270
00:29:24,800 --> 00:29:32,000
you know, in this, they collide as a lecture and so on. So, this, you know, continuous formulation

271
00:29:32,000 --> 00:29:38,080
allows you to make a very interesting connection between very different fields, which are described

272
00:29:38,080 --> 00:29:43,920
in a very similar formalism. And so, this is a very interesting thing. And also the, you know,

273
00:29:43,920 --> 00:29:50,560
sampling, the irregularly sample nature of the inputs is also, I think, naturally captured

274
00:29:50,560 --> 00:29:56,880
in terms of these Gaussian states of light that I was talking about. So, yes, I would say both,

275
00:29:57,440 --> 00:30:03,840
I, in my opinion, are very natural candidates, you know, that allow you to, I think, propose

276
00:30:03,840 --> 00:30:12,960
the interesting models for quantum neural networks. And what are some of the, you talked about kind

277
00:30:12,960 --> 00:30:22,960
of the, the, going back to the benchmarks, the sub sampled images and the, the healthcare time

278
00:30:22,960 --> 00:30:33,760
series, is there also a compression application for this paper as well? No, I would say that

279
00:30:33,760 --> 00:30:40,560
a compression was not our main focus, but I can see maybe where you're going. So, if you can

280
00:30:40,560 --> 00:30:46,240
condense, you know, your input in some mean and covariance of the Gaussian, that's maybe also

281
00:30:46,240 --> 00:30:51,520
way to think about if compressed it to a few numbers. So, that's an interesting spin that I didn't

282
00:30:51,520 --> 00:30:57,360
think about. So, it was not really our, our focus here, but yeah, it might be. Got it.

283
00:30:57,360 --> 00:31:05,680
Cool. So, again, this is a paper that you're presenting at ICLR. What else's Qualcomm

284
00:31:05,680 --> 00:31:13,600
have going on at the conference? Sure. So, another paper is from my colleagues,

285
00:31:14,560 --> 00:31:23,840
Tis, Farazindal, and Iris Halben and Taco Cohen. And so, this paper is about adaptive

286
00:31:23,840 --> 00:31:30,320
neural compression. So, here, the, so we thought a little bit before, you know, about what is the

287
00:31:30,320 --> 00:31:36,320
idea of neural compression? So, neural codex. And so, typically, there is a problem, which is the

288
00:31:36,320 --> 00:31:41,360
problem that, you know, you want to have a small neural network, because you want, you know, to,

289
00:31:41,360 --> 00:31:48,160
to have a low computational burden to do, to do this codec process. But at the same time, a low

290
00:31:48,160 --> 00:31:54,160
neural network, low complexity neural network, we will typically, you know, not generalize well.

291
00:31:56,160 --> 00:32:04,800
And so, the idea here of the authors in this paper that we present at ICLR was to do adaptive

292
00:32:04,800 --> 00:32:10,400
or fine-tuned compression. So, the idea is that, okay, you have trained your models on training

293
00:32:10,400 --> 00:32:15,280
data, but now you deploy it. But it turns out that, you know, the test data can be different

294
00:32:15,280 --> 00:32:20,720
from the training data. As I said, you can suffer from generalization problems. But you can,

295
00:32:20,720 --> 00:32:25,520
you know, imagine now that in the scenario where, for example, your sender, you know, is some,

296
00:32:26,480 --> 00:32:32,080
you know, at your sender side of the data, you can, you're willing to spend compute time.

297
00:32:32,080 --> 00:32:36,880
And, you know, you're sending this data to this compressed data to some low power devices,

298
00:32:36,880 --> 00:32:42,480
like mobile phones. And in this scenario, it makes sense, you know, that at your sender time,

299
00:32:42,480 --> 00:32:51,200
you can fine-tune your, your, basically, codec on the test data. And then, on top of sending,

300
00:32:51,200 --> 00:32:58,320
you know, the transmitted image or video to your mobile phone, you also send some bits that are

301
00:32:58,320 --> 00:33:05,120
related to the difference in the weights of your, you know, adaptive neural network codec.

302
00:33:05,840 --> 00:33:11,440
And so, the authors in this show that, you know, you can indeed reserve some bandwidth

303
00:33:11,440 --> 00:33:18,720
for this delta in the weights, on top of the bandwidth for the stream of the video that you want

304
00:33:18,720 --> 00:33:25,280
to send. And it turns out that if you do that, if you jointly optimize the model to do the best

305
00:33:25,280 --> 00:33:31,360
possible thing, so to minimize the rate, the number of bits if you transmit and optimize the

306
00:33:31,360 --> 00:33:36,400
reconstruction accuracy, you actually can do better if you do this procedure, you know,

307
00:33:36,400 --> 00:33:41,360
of sending over some of the bits for your daily weights, then if you're just in occupied

308
00:33:41,360 --> 00:33:45,520
the whole bandwidth for your, for your stream. So that's a pretty up-promising, I think,

309
00:33:45,520 --> 00:33:50,640
direction, which can have a few interesting, you know, applications, a direct application,

310
00:33:50,640 --> 00:33:58,960
in fact, also for PACOM. Now, I think this one was counterintuitive for me. I maybe misremembering

311
00:33:58,960 --> 00:34:05,440
the information theory, but I thought like Nyquist or Hammond or Heming or something like that

312
00:34:05,440 --> 00:34:11,600
said that it doesn't matter how you chop up your channel, you know, if you have a fixed bandwidth,

313
00:34:11,600 --> 00:34:18,000
there's some certain maximum throughput that you'll be able to get, but here you're kind of

314
00:34:18,000 --> 00:34:23,680
splitting your channel into kind of in-band and out-of-band or something like that and getting

315
00:34:23,680 --> 00:34:30,800
better results. Yeah, so here the idea is really that, you know, you want to basically send

316
00:34:30,800 --> 00:34:39,440
the certain number of bits, right? That's your somehow, the rate that you're willing to

317
00:34:40,880 --> 00:34:48,240
send. So that's somehow the amount of information that you would like to send. And at that

318
00:34:48,240 --> 00:34:54,400
point, you would like to do, you know, the best possible job given that constraint. So what is the,

319
00:34:54,400 --> 00:35:03,200
you know, choice of my codec, you know, encoder, right, to give me the best description of my input

320
00:35:03,200 --> 00:35:09,840
in such a way that when I decode it, I get the highest reconstruction quality. And so that's the

321
00:35:09,840 --> 00:35:17,040
setup. And so in this setup, you know, what we showed is that you can actually reserve some of

322
00:35:17,040 --> 00:35:25,680
these bits that you transmit for the, you know, weights. And so that was a new idea that, you know,

323
00:35:25,680 --> 00:35:31,920
people have not thought about. And yeah, in this setting, this is beneficial. But you're right,

324
00:35:31,920 --> 00:35:38,320
there are certain some terrific bounds to the, you know, rate distortion performance that you

325
00:35:38,320 --> 00:35:43,040
can achieve, but you know, we are certainly within these bounds. And yeah.

326
00:35:43,040 --> 00:35:49,280
Mm hmm. Okay. So yeah, we're talking about the different, the, the thing that I was thinking

327
00:35:49,280 --> 00:35:56,080
about it applies to the theoretical bounds, but I would not say that. So we have some ability to

328
00:35:56,080 --> 00:36:03,120
operate within that envelope. I would say closer to the boundary with this procedure than without.

329
00:36:03,760 --> 00:36:11,680
Got it. Got it. Cool. Any other papers? Qualcomm AR research papers at ICLR?

330
00:36:11,680 --> 00:36:20,480
Yeah. Yeah, certainly. There are a few other papers. I can highlight a paper by my colleague,

331
00:36:21,200 --> 00:36:29,600
Pim, the one who is a PhD student of Max. And he has a paper on mesh CNN. So here the idea

332
00:36:31,200 --> 00:36:40,240
is that, you know, you have tasks or meshes like, you know, shape segmentation or 3D shape

333
00:36:40,240 --> 00:36:46,240
reconstruction and things like that. And so you would like to use a graph neural network for

334
00:36:46,240 --> 00:36:51,920
these tasks, but graph neural networks suffer from the problem that they are, they are oblivious

335
00:36:51,920 --> 00:36:56,480
to geometry. So by definition of the graph structure, they do not have information about the

336
00:36:56,480 --> 00:37:02,960
geometry. So in particular, if you have, you know, a vertex with two edges connected to it,

337
00:37:02,960 --> 00:37:07,840
you know, it doesn't matter if you move these edges around, basically because the graph neural

338
00:37:07,840 --> 00:37:13,920
networks do not, convolution neural networks do not see the angle between these edges. And so the

339
00:37:13,920 --> 00:37:21,040
idea of this mesh CNN is to use, you know, gauge equivariance tools to build this geometry

340
00:37:21,040 --> 00:37:26,160
into graph neural networks. It turns out that if you do that, you get, you know, much better results

341
00:37:26,160 --> 00:37:42,320
for these tasks or meshes. So we have also other works on, you know, temporal localization

342
00:37:42,320 --> 00:37:58,000
of actions. And also we have a, we are organizing together with UC Irvine and Disney research.

343
00:37:58,000 --> 00:38:05,360
We are organizing a workshop at ICLR, a workshop on neural compression. So that's a certain

344
00:38:05,360 --> 00:38:12,240
and excited opportunity to, you know, get together with the experts in information theory, communication,

345
00:38:12,240 --> 00:38:18,480
and deep learning to indeed afford the advance of this effort of getting better codex using neural

346
00:38:18,480 --> 00:38:28,720
networks. Nice, nice. So you've talked, we spoke earlier on kind of where you saw the

347
00:38:28,720 --> 00:38:37,200
probabilistic numeric research going kind of more broadly when you think about your research area

348
00:38:37,200 --> 00:38:42,240
and the area of your team. What, what are you most excited about? Where do you see that going?

349
00:38:42,960 --> 00:38:48,720
Yeah. So indeed, we spoke earlier and I was hinting at quantum neural networks. So this is

350
00:38:48,720 --> 00:38:55,760
certainly something that I believe would be a drive for innovation in AI in the future. You know,

351
00:38:55,760 --> 00:39:02,080
recent years, last couple of years, I've seen a tremendous, you know, excitement in the quantum

352
00:39:02,080 --> 00:39:08,160
computing community, you know, with the supremacy experiments. So we are really at an era where

353
00:39:08,160 --> 00:39:14,080
we are starting, you know, to seriously think about, you know, these things. And so it's really

354
00:39:14,080 --> 00:39:19,360
timely, I think, to get serious about, you know, thinking about how can you use quantum

355
00:39:19,360 --> 00:39:25,840
information of quantum computation to enhance machine learning. So that's I think a very exciting place

356
00:39:25,840 --> 00:39:33,840
to be. It is true that, you know, it is still a recent deal and, you know, there is certainly a lot

357
00:39:33,840 --> 00:39:41,760
to do and it is still an open question to figure out what's the best way to use quantum computers

358
00:39:41,760 --> 00:39:47,280
for machine learning. So that's why I think it's a very exciting area. So I've been thinking

359
00:39:47,280 --> 00:39:53,760
a little bit about this over the last year. And so one of the things also, I've been thinking

360
00:39:53,760 --> 00:40:00,640
about was the problem, you know, of benchmarking these models. So we talked a little bit about

361
00:40:00,640 --> 00:40:06,240
this direction where you can use the quantum optical computers and the relation to, you know,

362
00:40:06,240 --> 00:40:11,200
probabilistic numerics, CNN and so on. But, you know, in general, the problem here is that we do

363
00:40:11,200 --> 00:40:16,880
not have these devices to run the quantum neural networks as scales that we would like right now,

364
00:40:16,880 --> 00:40:22,240
right? So what do we do? Certainly, we can indeed base on intuition and small experiments,

365
00:40:22,240 --> 00:40:28,880
figure out what are the most promising models. And I think that I worked on was to also try to

366
00:40:28,880 --> 00:40:34,960
find an interpolation between, you know, your classical neural net and a quantum neural net.

367
00:40:34,960 --> 00:40:42,400
So basically, we came up with this quantum deformed neural networks, which is, you know,

368
00:40:42,400 --> 00:40:51,120
is some work I did also with Maxwell. And so the idea here is that, you know, you can think

369
00:40:51,120 --> 00:40:57,280
about your classical neural network as embedded in a quantum computer. And in fact, the architecture

370
00:40:57,280 --> 00:41:02,480
we are thinking about now is the qubit architecture, which is an interesting relation, you know,

371
00:41:02,480 --> 00:41:07,200
to binary neural nets, because you know, a qubit is basically the, you know, quantum equivalent

372
00:41:07,200 --> 00:41:12,480
of a bit. And so there is a relation with quantization of neural net, so that you can also explore

373
00:41:12,480 --> 00:41:18,480
using quantum mechanics and so on. But the most interesting thing is that indeed we managed to

374
00:41:18,480 --> 00:41:23,600
map this binary neural net or probabilistic binary neural net. In fact, on a quantum computer,

375
00:41:23,600 --> 00:41:31,520
using qubits. And then we started to deform this model, so to introduce gates, which, you know,

376
00:41:31,520 --> 00:41:37,920
use pure quantum effects like entanglement and superposition. And so we do that in a way that,

377
00:41:37,920 --> 00:41:43,840
you know, we slow interpolate between a regime, which we can simulate classically, which is basically

378
00:41:43,840 --> 00:41:50,400
the classical neural net regime. And the regime, you know, which is basically intractable classically,

379
00:41:50,400 --> 00:41:56,480
which will require a quantum computer. And along the way, there is some, some, you know,

380
00:41:56,480 --> 00:42:01,520
intermediate regime where you can still do some classical simulations using some tools from

381
00:42:01,520 --> 00:42:07,200
quantum physics, which are called tensor networks. And basically, this allows you to start from a good,

382
00:42:07,200 --> 00:42:12,320
you know, prior somehow for your model, which is this classical network, the format by doing

383
00:42:12,880 --> 00:42:17,760
this, indeed, these modifications. And then you can, you know, use the classical simulations of

384
00:42:17,760 --> 00:42:23,920
the quantum model to learn that. So that you can implement as a differential program. And so we

385
00:42:23,920 --> 00:42:30,240
show actually some modest gains with respect to, you know, the classical neural net by introducing

386
00:42:30,240 --> 00:42:35,840
this quantum gates. And we can actually provide, you know, the first example where you can simulate

387
00:42:36,480 --> 00:42:41,920
quantum model at the scale of, you know, real world data. So that was interesting for us.

388
00:42:42,560 --> 00:42:46,720
But yeah, more generally, you know, there are many, indeed, the different directions at the moment

389
00:42:46,720 --> 00:42:51,600
are also related to optimization problems. So we talked about computer optimization, right?

390
00:42:51,600 --> 00:42:56,800
And machine learning and also, you know, quantum computing is also an exciting area for exploring

391
00:42:56,800 --> 00:43:02,560
new algorithm for combinator optimization. So yeah, to summarize, indeed, the quantum AI, I think,

392
00:43:02,560 --> 00:43:08,240
is a very interesting direction. And also, I also think that the machine learning for combinator optimization

393
00:43:08,240 --> 00:43:14,800
is a very interesting direction. So this is also pretty recent. And I believe that here we will see,

394
00:43:14,800 --> 00:43:22,560
you know, large scale adoption of this technique, because it has been shown recently that, you know,

395
00:43:22,560 --> 00:43:29,760
you can actually enhance your classical solvers for, you know, integer linear programs or stuff

396
00:43:29,760 --> 00:43:35,040
like that, which, you know, people use routinely for solving their problems in the industry. You can

397
00:43:35,040 --> 00:43:42,480
actually, you know, augment with neural networks, these solvers in such a way that the decisions

398
00:43:42,480 --> 00:43:48,480
that these solvers make are better informed and basically are faster. And the idea here is that,

399
00:43:48,480 --> 00:43:54,080
you know, you can adapt, basically, your, your solver to the data distribution that you really

400
00:43:54,080 --> 00:44:00,240
care to solve, you know, a delivery company which will routinely solve, you know, the driving

401
00:44:00,240 --> 00:44:05,200
assessment problem in the same city, you know, if you not want to deal with the most arbitrary

402
00:44:05,200 --> 00:44:08,960
hard instances of a traveling assessment problem. And this is where machine learning, I think,

403
00:44:08,960 --> 00:44:14,000
we really put an edge. So it will allow you, you know, to tailor your optimization algorithm

404
00:44:14,000 --> 00:44:19,760
to the instances that you care about. And ultimately, in ultimately this, you know, leads to,

405
00:44:20,320 --> 00:44:26,240
you know, better performance for combinator optimization solvers. And, you know, and also

406
00:44:26,240 --> 00:44:31,680
potentially, you know, it allows you to discover new strategies, like using reinforcement learning

407
00:44:31,680 --> 00:44:38,800
as we have seen, you know, you know, alpha-go alpha-fold. So super excited, I think. Awesome. Awesome.

408
00:44:38,800 --> 00:44:44,880
Overbirdo, thanks so much for sharing a bit about what you're working on. And what some of the

409
00:44:44,880 --> 00:44:50,080
folks in your team are working on at ICLR. It's been really great chatting with you.

410
00:44:50,080 --> 00:45:01,040
Thank you, Sam. Pleasure for me, too. Thank you.

