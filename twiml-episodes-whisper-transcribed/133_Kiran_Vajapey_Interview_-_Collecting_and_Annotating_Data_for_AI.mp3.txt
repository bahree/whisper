Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode I'm joined by Karen Vajepet, a human computer interaction developer at
Figure 8.
In this interview, Karen shares some of what he's learned through his work developing applications
for data collection and annotation at Figure 8 and earlier in his career.
We explore techniques like data augmentation, domain adaptation, and active and transfer
learning for enhancing and enriching training data sets.
We also touch on the use of ImageNet and other public data sets for real world AI applications.
If you like what you hear in this interview, Karen will be speaking at my AI Summit,
which takes place April 30th and May 1st in Las Vegas, and I'll be joining Karen at
the upcoming Figure 8 Train AI Conference.
May 9th and 10th in San Francisco.
You can learn more about the AI Summit at Twimbleai.com slash AI Summit.
At Train AI, you'll find a host of amazing speakers like Gary Casparov, Andre Carpathy,
Marty Hurst, and many more.
And you'll have the opportunity to receive hands-on AI, machine learning, and deep learning
training through real world case studies on practical machine learning applications.
For more information on Train AI, head over to www.figure-8.com slash train-ai.
And be sure to use the code Twimbleai for 30% off of registration.
A huge thanks to Figure 8 for sponsoring this episode of the podcast.
Alright, let's roll.
Alright everyone, I am on the line with Karen Vajapay.
Karen is a human computer interaction developer at Figure 8.
And he'll also be speaking at my upcoming AI Summit on the topic of data collection
management and annotation for machine learning and AI.
And in addition, both Karen and I will be at Figure 8's upcoming Train AI Conference
in May.
Karen, welcome to this week in machine learning and AI.
Hey Sam, thanks for having me.
It is great to have you on the podcast.
Why don't we get started by having you tell us a little bit about your background?
Yeah, definitely.
So I guess we can start, I did my undergrad in bioengineering at UCSD.
During this time, I actually did some research with one of my professors.
And in over the course of that research, I kind of realized that I didn't want to do
bioengineering because it was not applied.
So that's when I actually transitioned into computer science and then went for a master's.
I did my master's in computer science at Cornell Tech.
And it was at Cornell Tech where actually did my first machine learning and modern analytics
courses where we actually learned how to use clustering and kind of linear regression
techniques to actually do data analysis.
And while I was there, I actually had a co-op with this startup called Appio where they
gave myself and my teammate a whole bunch of mobile data on people entering and exiting
vehicles.
And they wanted to train a machine to actually do a detection of when people were entering
and exiting vehicles based off of accelerometer data.
So it was kind of here where I really understood that data drives a lot of these applications.
After I did my master's, I went on to found a company called Oyster with a couple people
that had met at Cornell.
And here I was the chief product officer and I was in charge of basically developing
a product that would allow our customers who were developing a talent marketplace where
we could match PhDs and people in academia with jobs in industry.
And the way that we decided to go about doing this was to actually develop a platform
where people could give us their skill set in a very granular way.
And then we could match these individuals with the companies based off of the type of
skills that those companies were looking for.
And while we were developing this product, we didn't really realize the type of data that
we were collecting once we started, but over the course of a few months, we actually had
about 10,000 PhDs that had signed up with our website.
And they had given us each about on average of 15 skills each.
So we had a database of about 150,000 skill sets that people had indicated to us.
And what we decided to do was actually look into this data and try to understand how
we could make the most out of it.
And so if you look at LinkedIn, one of the things that they've actually been trying
to do is develop a skill graph where they want to map skills across industries and understand
what sort of jobs require, what sort of skills.
And so what we actually realized that as we were collecting this data, it was very structured
data, which was a key factor in allowing us to implement what we did.
And we built out a machine learning tool that we could actually predict an individual's
skill set based off of one or two skills that they would give us.
And so this was really cool because we could start to build out kind of people information
based off of one or two skills that someone might give us and understand what industry
they would fall into and how well they may perform within this industry.
So after about a year of working at Oyster, just like all businesses, we kind of needed
to make some money and we weren't getting traction on the business side as much as we
would have liked to.
So I actually went on to a startup called Icentium.
And Icentium is based in New York City and what they do is they've developed a deterministic
natural language processing system to take tweets and assign scores to these tweets
and then use this information to actually make predictions in the stock market.
So what my job was at Icentium was to actually write the tools that would use this AI to
make those predictions in the stock market.
And myself being a bioengineering computer scientist, I had actually no information about
the stock market or really any understanding of the way that it works.
So what I did know is that data drives applications.
And so what I did starting at Icentium is tried to understand first what is this natural
language processing really doing and what they had built was a completely rule based system
where there was actually no machine learning integrated into it.
They actually had a team of PhDs who are writing rules based on both the English language
as well as the financial industry that would assign scores to individual tweets.
Then what we would do is we would actually take millions of these tweets and aggregate
scores for given entities in the stock market.
So say you want to predict the price of Apple, you might look at all the tweets about Apple
over the last three months and determine what trends you are seeing in the sentiment
and then make a prediction based off of what the crowd is saying.
Now the one catch with this is that it wasn't necessarily always the tweets about Apple
that were driving the stock price of Apple.
So we actually had to look at tweets from different entities and across industries to really
get the best predictions on these stocks.
And so that was interesting to see that even though this data doesn't seem like it's
correlated, when you actually look at it from a holistic point of view, there are actually
key factors across industries that may be driving trends in different industries.
So that was really cool.
So after about a year and a half at Isentium, that's when I joined Figure 8.
And now what I'm doing is I'm actually developing tools that allow companies to create data that
they need to develop machine learning and AI applications.
Looking back at your time at Isentium, do you think if they did it all over again?
They'd use the same rule based approach to determining sentiment and all that kind of
stuff?
Or was there something specific to what they wanted to do that really required that approach?
So I think there's pros and cons to both systems.
So one of the pros really with a rule based system is that you're really writing all of
the rules, you're telling the computer exactly what it should be doing.
But the problem with that is that if you want it to be complete, you have to write every
single rule.
So if you're trying to make a natural language processing system, you essentially have
to write every single rule in the English language for that computer to really understand
English.
And the benefit with machine learning is that you can actually take a whole bunch of
existing data that may be labeled or not, right?
And then use this data to train a system to have a rough understanding of what the English
language might be.
So I'd say depending on what your use case is, you might want to go for a machine learning
approach if you just need good enough to kind of get you to the next phase.
Whereas if you're kind of in a targeted space like Isentium was with finance, it kind of
made more sense to build out a rule based system that could capture the intricacies of
this industry.
So your role now is on or your title at least is human computer interaction.
When I think of HCI, I think of user interfaces and user device experiences, is that a big
part of your role today?
Yeah, that's correct.
So I'm actually working on the machine learning team as an HCI developer.
So my role is really around developing the applications that our contributors and
our customers will use to kind of develop the data that they need to build into their
machine learning and AI applications.
And so a typical application that you'd work on would be something like a tool that someone's
using to create bounding boxes or label data?
Yeah, correct.
So we have contributors all around the world and they're all using our platform to create
this data for the customers.
And so the tools that those contributors are using are very essential in creating this
data because the customers, they want to get the most accurate data.
And if these tools are really hard for the contributors to use, then they may get tired
while they're doing the tasks or they just may not get the most accurate data.
So it's actually a very crucial component to make sure that these applications that the
contributors are using are very user friendly, especially because we're catering to people
all around the world so you don't necessarily know what their background is or how much
technical knowledge they have.
So it has to be kind of something that they can just pick up and run with.
So both in your prior experience at at Figure 8, you've come across a lot of AI implementations.
What are some of the main data challenges that you see organizations running into when
they're implementing AI?
Yeah, definitely.
So I think a big challenge that companies face at least from my experience is that people
don't necessarily know how the best way to use their data.
So they have a whole bunch of data, they don't necessarily know what it really means.
So to give you an example, when we were at Icentium, I spoke about how you may not use just
Apple sentiment to determine Apple's stock price.
It sounds very simple, but it actually took us quite a while to learn that and the only
that we really did was digging into the data and understanding how trends in this sentiment
are changing over time and how sentiment across entities kind of affect each other.
So when we were developing these models, we were actually start to play around with smoothing
algorithms.
So instead of just aggregating all the data for a given time period, we actually want to
smooth that data to give us a better signal over time because especially in finance, you
don't want to make a trade every two hours, you kind of want there to be some stability.
So you could say, all right, I'm going to buy it here and I'm going to sell it X amount
of time later as opposed to having buy sell, buy sell frequently.
While that may be a use case in high frequency trading, that wasn't necessarily the use case
that we're trying to cater to, especially when we're basing our predictions off of social
media, which is kind of longer than a couple second turn over time.
And so a really important factor of what we were developing was to really understand how
the data can change over time and how we can apply smoothing methods and change the
way that we were kind of shifting data over time.
You've collected a bunch of data.
You've started to do some analysis to understand what that data can tell you.
But even the specifics of the given problem that you're trying to go after, it sounds
like in some of your experiences that, or we see often that that changes over time,
are there practices that an organization can follow that will allow it to be able to
more quickly map its data to the use cases as they evolve?
Yeah, for sure.
So I think it's one of the important things is that an organization should kind of set
a goal of understanding what they're trying to become and what they want to ultimately
do with their data.
And by really planting a goal, it doesn't necessarily mean that that's the only endpoint
that you're going to hit, but it allows you to kind of have a frame of reference as you
kind of develop your strategies and as you build out your AI.
And so you could set a goal for two years from now of becoming an organization that does
a certain type of data processing, right?
And over those two years, you want to make sure that you're building your data sets and
building your algorithms to kind of achieve that goal, right?
And along the way, you'll certainly run into changes in your data or changes in behavior
of customers that might alter your ultimate goal, but by having some sort of state that
you're trying to achieve, that will really allow you to develop your tools to kind of hit
that point.
Do you have a framework for thinking about the different ways that organizations can collect
data to allow them to power AI use cases?
Certainly, so again, I think it really depends on the particular use case, right?
So when you think about image labeling, there are many different ways that you can label
an image.
For example, you can have an image of, say, a meadow with some sheep on it, right?
And you could just label this meadow with sheep.
And so that's kind of some sort of structured data where you have an image and you have an
associated caption, but you might want to get more granular, right?
And you want to actually now put boxes around each of those sheep, right?
And then label those individual boxes as sheep and maybe label tree and maybe label sun,
right?
So now you actually have more insight into the content of that picture, as well as the
specific objects and what the computer could learn based off of those regions of interest.
And then going further, you could do something like semantic segmentation where you're actually
labeling every single pixel in the image with an associated label.
So it'll really depend on the needs of your technology.
And if you are just trying to give like a general categorization of an image, you don't necessarily
need to get that pixel level segmentation.
But if you're doing something like autonomous driving where this is critical situation that
could mean life or death in certain situations, you want to make sure that your AI is really
as knowledgeable as possible about its surroundings.
Yeah, I'm wondering even before you get to labeling, if there's a set of things that organizations
should be thinking about in terms of the collection, like if you, you know, some organizations
will have a data set that they've collected just based on their interactions with their
clients, you know, whether that's financial data or user-submitted content, but they
may want to do something with machine learning around data that, you know, they haven't already
collected.
Are there, you know, have you run into that kind of situation where an organization needs
additional data that they don't already have and how to organizations typically approach
that particular challenge?
Yeah, so kind of one of the problems that I've seen is you may have a lot of data.
Say you're trying to do a surveillance camera and you want the surveillance camera to detect
objects, right?
So you want to see people, you want to see cars.
And so what you'll go and do is say, okay, well, let's go online and we'll find a
bunch of open source images of vehicles and people, right?
And a lot of these will come up as portraits of people and maybe like dealerships will
have a lot of pictures of their cars.
And so you might take a lot of these images, you'll go and train a model and then realize
that it's actually doing a very poor job of detecting these objects in surveillance video.
And the reason for this is probably because when you look at a portrait and you look at
a person in a surveillance camera, they look very different.
And so us as humans might understand that yes, this is a human and this is a human, but
the computer doesn't necessarily know the difference between the two, right?
So they'll look at the portrait and say, okay, I understand a human to have a head that's
this size relative to their body and they have arms, legs and you can see their facial
features.
Whereas on a surveillance camera, they may just appear as kind of like a spec that's moving
around.
So if that's the case, you're going to actually have to go and collect that type of footage.
So you really want to make sure that the data that you're using to train your system is
similar to the type of data that you're going to use to run through and get predictions
out of it.
There have been a number of attempts to try to do things like domain augmentation, to
kind of massage existing data so that it looks more like the domain that you're trying
to make predictions off of.
Is that have you worked with that at all?
Yeah.
So you can definitely do like sort of data augmentation to enhance your data set.
So for example, if you're trying to do a speech recognition system, right, you want this
to be robust in many different situations.
And if it's going to be deployed on a mobile device, there's going to be a lot of ambient
noises.
There's going to be a lot of static.
So if you're collecting very clean data, say in a recording studio, you could use this
data and they'll be great, but your machine may not be robust enough to actually understand
what people are saying in these noisy situations.
So in that case, you could actually take these clean data samples and simulate noise into
those.
So you can add in background noises or horns or cars and then use that data to train.
So now you've kind of used the same data and you just expanded your data set through
this augmentation.
Yeah.
And I've kind of mushed together two terms data augmentation and domain adaptation, but
you kind of refer to both of them there.
I guess one question I have is around, you know, when folks are trying to build, again,
AI for commercial applications, we see a lot of the use of these public data sets in
research, like ImageNet and others, how often do you see folks using those data sets
in commercial applications?
So these sort of data sets are actually really great starting points for your AI and
technology because from these data sets, there's actually a lot of pre-trained models that
you can just start out with.
So generally, depending on your data, it may not be the perfect application for your organization.
But by having this starting point, you can actually do things like transfer learning
and active learning to really improve your models and kind of get the results that you're
looking for.
And the nice thing is that you don't have to go and pre-trained this model on millions
of examples, right?
If you're using ImageNet and something like a YOLO detector, you have this YOLO model
that's pre-trained, that has millions of images that it's learned from.
And you can basically run like a transfer learning and remove the last couple layers
of that YOLO model and then retrain it with your specific data.
So it's going to have all of the rules that it learned from the previous data set
around general images and edges, edge detection kind of pixel level.
And then when you do the transfer learning and really retrain those last couple layers,
that's when your applications going to get the knowledge that it needs that's specific
to your application.
Okay, and for those that heard YOLO and aren't familiar with it, YOLO is you only look
once, right, the object detector.
Yeah, that's correct.
And actually, we'll be doing an online meetup focusing on YOLO, not this one that's coming
up tomorrow actually, but the May meetup will be all about YOLO.
You mentioned transfer learning are there.
Things that you find that folks really struggle with around transfer learning that isn't
obvious from the things that you might find online or the kind of easily accessible write-ups.
So I think again, a really big part of it is actually going to be the data that's going
into this.
So when you do transfer learning, you actually really do need the right data that's similar
to the type of data that you're looking to predict to retrain this model.
Because again, it has this knowledge of all the previously trained examples.
But if those examples are different than the type of data that you're looking for, then
it's not going to perform very well.
So for example, if you're looking at detecting cancer in X-ray images, right, this is a very
specific use case that there actually isn't a whole lot of data that's publicly available.
But you could do something like take this YOLO model, do transfer learning.
So maybe you want to remove the last two layers of your model, and then take all of the
X-ray imagery that you may have collected over your work or that might be open source and
use that to retrain this model.
So that those last two layers can gain the knowledge that's specific to this use case.
And then in that process, you'll ultimately develop a neural net that's suited to your
use case.
When you're working with customers that are building out these kinds of applications,
and taking into account all of these types of factors, how to, one of the questions
I hear all the time is, how much data am I going to need for this?
How do you advise them and walk them through starting to understand what the requirements
will be?
So I think the best way to really go about that is just very iteratively, right?
So if you have a thousand images of this X-ray data for cancer detection, then use those
thousand, right?
And once you've trained your model, you really get an understanding of how well that works
or not.
So you could use a thousand images and that could get you up to like an 85% accuracy.
And maybe you don't need like 95, 99% because all you want this model to do is just run
a general overview.
And so if you can get a confidence level of a patient above 50%, then you know, okay,
well, there's a, this person person has a 50% risk.
So we're going to actually pay more attention to them.
And you could maybe filter out a lot of false positives that way.
We started off talking about some of the tooling aspects of data collection and data labeling.
Do folks tend to use or reuse kind of off the shelf tools to label data sets or is there
a big advantage in customizing the tools for a particular, the needs of a particular
data set or application?
What's the right way to think about that?
So I think here at Figure 8, right, we're catering to a large number of customers that
have very different use cases.
So when we develop our tooling, we want to make sure that it's really applicable to a lot
of those use cases so that our customers can really get the most benefit out of it.
But there's also many organizations that have very, very specific use cases, right?
So maybe what they're looking to do is label just the tires in cars, right?
And so they may not need a whole bounding box tool or pixel segmentation, but they may
develop their own tool that allows for this very specific use case.
In those cases, you may want to have a specific tooling that works well with your application.
But for the more generalized use cases, there are definitely a number of tools out there
that can help you get up and running pretty quickly without having to develop your own
stack from end to end.
As an HCI focus developer, are there ways that you think about testing the interfaces
and the tooling to determine how effective it is?
You mentioned user fatigue earlier.
How do you determine, how do you quantify user fatigue and try to test for it and test
design around it?
Well, we just kind of run it with people.
Luckily for us, we have thousands and thousands of people that use our platform every day.
So as we're developing these tools, we kind of just schedule sessions regularly so that
we can work with them and we'll sit in like an hour long session and just say, here's
the tool, right?
And give them the tool without any instructions and see if they can use it.
And inevitably they'll stumble on it and so we'll kind of give them some advice and
give them some direction and then that will help them get through the task.
And it's really through this user testing process that we actually make the biggest steps
and improvements in our tools because these are the people that are going to be using it.
And so we want to make sure that they actually know how to use it, right?
So if we're sitting internally and kind of just developing these tools in a sandbox without
actually running it with real people, then ultimately what we're going to give the contributors
and the customers isn't going to be what they want, right?
So kind of as we develop these tools, we try to make sure that as frequently as possible,
we can run these tests with new features anytime there's updates and so and then get really
qualitative and quantitative feedback on these tools that we're developing.
Can you give us some examples of things that you've observed in these kinds of tests and
how they've translated into new ways of thinking about or implementing the tooling?
Definitely.
So I think one big thing, right, is icons.
So we see icons everywhere and there's a lot of things that we've started to understand
just from having these across platforms.
And so for example, when you see a trash can, you know that that's a delete, right?
But in certain cases, you may have multiple things working on the screen that if you push
this trash can, what is it really going to delete, right?
Is it going to remove an annotation?
Is it going to delete your image?
So it's kind of things around this where you say, all right, well, do you understand these
icons, right?
And you'll kind of have them walk you through what they think each icon is going to do.
And if three out of four people are saying this icon is supposed to do something that you're
not intending it to do, then you know immediately that, okay, well, I need to change the icon
or I need to make it more clear as to what is going to happen when they click on this.
So it goes all the way down to those little details where you see people doing interactions
that you didn't intend.
And so you kind of understand that, okay, well, these people are trying to use this tool
in a specific way.
So I either try to cater it so that it allows them to do their interactions better or I make
it more clear that what I'm intending to do should be done in a certain way.
Is the implication from that example that a lot of the core things you observe are not
specific to the ML and AI use cases and are more general UI, UX types of issues?
Yeah.
So really with the AI and ML, a lot of that has to do with the actual data itself, right?
So if you're looking to put bounding boxes and images, right, the data that you need
is kind of regions of interest in an image.
So the way that you generate that data, that's actually going to be, can be done very differently,
right?
Like you can have a machine kind of predict these boxes and then just give those back
to the customer, you can have people draw boxes around these images or the objects and
the images and give that to the customer.
And so there's different ways that you can accomplish or create the same sort of data
that's used in AI and ML.
So when we do a lot of this HCI testing, it's actually on a tool level where we see the
people's interactions with the tools that are going to allow them to create the data that
they need.
And one of the initiatives that we've been working on over the last few months is to really
integrate ML and AI into these tooling.
And so an example of that is we have this bounding box tool where customers or contributors
can go in and put boxes around different objects of interest based on the customer need.
But what we want to do as figurate and as a platform for data is we want to make this
interaction with the contributors and the tooling as easy as possible.
And so part of that could actually be, instead of giving the contributor contributors an
image and asking them to label all the objects, we could actually do a prediction on that
image and display the image and the predictions to the contributors and tell them, hey, instead
of drawing boxes around every single one, can you confirm which boxes are correct and
which ones are wrong?
So now instead of having to go through and really annotate every single object, they're
kind of just saying yes, no, yes, no, making that interaction a lot easier and ultimately
getting the customer better and cheaper data.
And it sounds like you just described a capture system.
Yeah.
So capture actually does that and that's exactly their goal is really to use machine learning
to develop their tools.
What other things have you learned about the tooling process that have surprised you like
once you got the tools into the hands of end users?
So one of the major challenges that we've seen is we're building a completely web-based
platform, right?
So people aren't installing any software, they're kind of just going online and they're
running these tools and again, they're all around the world.
And so you actually don't really know what these users' environments are.
So these contributors could be using any web browser, they could be on any sort of device,
they could be on any bandwidth, internet connection and these tools have to be consistent across
all of those things.
So as a developer, I'm really trying to capture as many environments as possible, but you
really can never get every single one.
And that's where the user testing comes in and it's really important because we'll actually
go and work with people and we have a number of BPO's around the world and we'll work
with people in each of those to understand what their environment is, what sort of tooling
or browsers they're using, what their internet speeds might be like, and then we'll kind
of build our tools to cater to as many of these as possible.
Earlier in the conversation you mentioned active learning, can you talk a little bit about
how you use that to support this process?
Yeah, definitely.
So with active learning, the idea is that you have semi-supervised data.
So you have some of your data may be labeled and some of the data isn't labeled, right?
And so what you can do is use this already labeled data to train a rough deep learning
model.
And then once you have this rough model, you can start to run these unlabeled instances
through your neural net and based on the confidence of each of these predictions, you may
use some of these to retrain your model.
So if you have a thousand images that are already labeled and they have whatever caption
that you need, you can use these images, train a model and say you have 100,000 images
that aren't labeled, you might take chunks of a thousand images and run them through
this model and they each may have a confidence.
So if you're trying to predict vehicles in images, they might come out with one image
has a confidence of 20% that there's a vehicle and another one might have a confidence that
there's 85% that there's a vehicle in the image.
And so what you'll do is you'll take all of these images that got 85% confidence and
you'll just assume that there's a car in there and you'll use that to go and retrain
this model with, say, another thousand images.
And then you'll iteratively do this as you improve your and continue to improve your
deep learning models.
Can you speak to the specific kinds of results you've seen with doing active learning for
these types of models?
Yeah, so for some of the like bounding box use cases, we'll see customers may have, they'll
run jobs with contributors.
So they'll have maybe like 100,000 images that these contributors have manually labeled.
So they're confident that the labels in these images are accurate.
And then they'll go and train this model.
And when they try to run new images through the model may be producing like 60 or 70%
accuracy of like false positives and actual accuracy of the boxes on the objects.
And as you continue to build active learning and run through iterations of the data, you'll
see this, this accuracy go up like 70 to 80, 85%.
And it's, and it's really like an infinite loop that you're constantly training these cycles.
Interesting.
So we've talked about, we've covered a bunch of ground in terms of just the data collection
and annotation process are there.
And you think of things that we have not spent enough time on and that we should dig into
a little bit?
Well, I'd say another important aspect of AI and ML is really understanding when is the
right time to use AI and ML.
So there may be certain applications where you're trying to do this, but it isn't actually
the right fit for your organization.
So for example, something like 3D printing, right?
A lot of the 3D printing that we see is really based off of specific models that people
are building and these models may be custom for a given individual.
And so it's really difficult to understand where you might integrate ML or AI into these
organizations.
And so you have to kind of take a step back and look at what is the ultimate goal of your
organization and is ML and AI really going to help you achieve that goal.
And if the answer is yes, then you definitely should then go in and dig into your data and
understand really what is my data and how can I pull insights out of it that are going
to help us achieve that goal.
And if not, then there may be other applications that you should move towards that would help
you really achieve your organization's goal.
Awesome.
Well, that definitely brings us full circle.
Anything else that you'd like to leave the audience with?
For anyone listening, please come check out Train AI.
Our website is figure-8.com slash train-ai.
You'll see a whole host of speakers talking about similar topics we chatted about today
and more.
There's also an executive briefing and a hands-on machine learning course on the first
day May 9th.
So come check it out.
Yes, definitely check it out and be sure to use the code Twimble AI for 30% off of
registration.
Karen, thanks so much.
It was great chatting with you.
It was great speaking with you too, Sam.
Thanks for having me.
Thank you.
All right, everyone.
What's our show for today for more information on Karen or any of the topics covered in this
episode?
You'll find the show notes at twimbleai.com slash talk slash 130.
Thanks again to figure-8 for their continued support of the podcast and their sponsorship
of this episode.
Thanks so much for listening and catch you next time.
