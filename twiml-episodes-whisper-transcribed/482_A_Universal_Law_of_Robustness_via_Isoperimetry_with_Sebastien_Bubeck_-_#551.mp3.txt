All right, everyone. I'm here with Sebastian Boobeck. Sebastian is a senior principal research manager at Microsoft. Sebastian. Welcome to the Twimal AI podcast.
Thanks, Sam. Hey, I'm looking forward to digging into our conversation and particularly excited to introduce your paper outstanding paper award recipient at this year's Neurips to our audience.
The paper is a universal law of robustness via isoparametry.
Before we do that, I'd love for you to share a little bit about your background and how you came to work in machine learning.
Sure, yeah, I can do that. So I started my journey into research by doing mathematics. I was a mathematics student and I was interested at the time back in 2005 to 2006 in the mathematics of the brain.
And I was trying to look what this would look like, who was studying these things. And I discovered that basically this didn't exist. But while I was looking for it, I noticed this emerging field at the time of machine learning.
I mean, not quite emerging. It already had a long history, but it was not as popular as it is today.
And I got interested in a statistical machine learning series because this really combined, you know, my interest in probability series and statistics for mathematics.
And you know, the things which are closer to what the brain does, you know, for example, learning.
So I got into this and I did my PhD in Lille in Ria in France. And there I was studying a very specific and particular problem called the multi-unbended problem, which maybe we can talk about at some point.
It's a very special problem in machine learning series, which has to do with the exploration, exploitation, trade of, you know, that humans have to solve every day.
You know, do you go to a new restaurant or do you sample, you know, do you go to your usual place.
So that's what I did for my PhD. And then after that, I did a postdoc in Barcelona, where I worked more on classical statistics, high dimension of statistics.
And you know, we're going to talk about this best paper award at NERIPS, where we use this concept of isoparametry, which is one of the fundamental concept of high dimensional geometry and high dimensional statistic.
So, you know, when I did my postdoc, it was the seed of, you know, this new project that I have been working on.
After the postdoc, I moved to Princeton as an assistant professor in the OR department. There, I worked more on convex optimization. So, you know, you know, you know, in a person research, people care a lot about optimization.
And of course, you know, they care about the multi-unbended problem that I was working on. And they also cared about high dimensional statistics.
But optimization is really a core topic, you know, and so there I learned, you know, convex optimization, and I wrote a monograph on this topic. So, that's what I did back then.
And then I moved to MSR to Microsoft Research in 2014 and I've been there since, you know, I'm enjoying myself tremendously there.
And there, you know, there is also a long journey in terms of research topics that we can get into.
Sure, sure. So, tell us about the focus of your research at MSR.
Sure. Yeah. So, when I joined MSR back in 2014, the team was very heavily focused on probability theory. So, you know, I was going back to my roots and it was really very, very exciting.
And there, at the time, I worked a lot on random graphs and statistics on random graphs. So, by the way, what all I mean is, you know, in statistics, you have to deal with data.
And in statistics on random graph, what your data looks like is it's a network. For example, you know, the Facebook graph or the Twitter graph of connection.
And what you want is to build, you know, the very basic building blocks of statistics, but for this type of network data.
And I did a couple of projects back then on this topic and it was, you know, very exciting. Then I moved to, you know, basically the marriage of two of the topics that I worked on previously.
So, I told you that I was working on convex optimization, which is one of the, you know, foundational building blocks of machine learning.
And also the multi-unbended problem. And at the time, there was this great open question of, can you merge the two? Can you do convex optimization for bandits, which is called bandits, convex optimization.
And this was an open problem for, you know, more than 10 years. It was open since 2004. And we worked on it with a bunch of people and we managed to solve it, you know, after a few years of work. And this was really exciting because it was combining everything.
And the ability, convex optimization, bandits, and all of these things.
After that, I got interested more and more into these longstanding open problems. And as I was emerged also in a group that was interested in theoretical computer science.
And I also started to look at this whole problem in online decision making, which we can get into that go by the name, magical test systems and case server. Doesn't matter what the exact problem is. But these are longstanding questions that were able to solve using the same technology as the one we developed for bandits learning and for convex optimization.
It was a really exciting time. And then came, you know, the watershed moment of alpha go and all the excitement around our network, which I had been following, of course, since, you know, the early days in 2011 when the first paper of the new wave of neural network came out.
And I realized that there is really something to be done there. And that was the start of my journey to neural network series.
Awesome. Let's maybe talk a little bit about convex optimization and your work there. What are you trying to accomplish with convex optimization? What's the setting?
Sure. So the beauty of convex optimization is that so what do you want to do? You have a function that depends on many, many variables.
For example, you know, in neural network world, you have a neural network model where the number of parameters is, let's say, the number of neurons, also the number of connections that you have.
In fact, it's not exactly like that, but you know, let's make this simplification. And when you set the parameters of those neurons, you know, the function, the model computes a certain function, which could be good.
You know, for example, it could be good as I don't know, recognizing cats in images or recognizing dogs or whatever you want.
And the question of optimization is how to find, you know, the setting of parameters. Now, neural network learning is the prototypical case of a non convex problem, meaning that it's not very structured.
On the contrary, in convex optimization, you have a problem, which is very structured, meaning the interplay between the different variables is extremely constrained. And the typical example there is linear regression.
If you just have a very simple linear model, then this is a convex problem. And the question becomes, what is the most efficient algorithm to optimize a convex function?
You know, how do you find the setting of the linear model as quickly as possible? And there is a ton of work going on in that area.
And in the last decade, people got really interested in to it because they realize that the basics of machine learning is really like convex optimization.
You're doing this algorithm course to cast a gradient descent, which is, you know, very well understood from the 80s or even before in the convex optimization literature.
And that's also how I got into that field. And then, you know, yeah, you develop basically the goal is to develop new algorithms faster optimization methods.
Got it. And so talk a little bit about your, well, first talk a little bit about bandits multi-armed bandits and then your application of convex optimization to that.
Sure, sure, sounds good. So the bandit problem goes like this. It's very simple. So the most important application and the reason why it was revived.
So this is a topic that goes back to more than a century ago in direct trials. But the modern application is at placement on the internet.
So the setting is very simple. You have a user that comes to your website and you have maybe 10 ads that you can show to that user.
And depending on which ad you show to the user, the user is more or less likely to click on the ad.
And of course, if the user clicks, you're going to get more money. So you want to show the ad that, you know, the user is going to click on.
But at the beginning, you have no idea which ad is going to be popular with which user.
So you have to explore the space of possibility. You have to show different ads and kind of learn about which ad work for which kind of users.
And as you start to learn and get a sense of what's going on, you want to exploit.
You want to start showing, you know, the ad that you believe to be better for that person.
So you have to move smoothly between this exploration phase to this exploitation phase.
And the bandit problem is a mathematical formulation of this, which asks what is the optimal way to do this smooth transition between exploration and exploitation.
And basically, the answer is that the optimal thing to do is not to transition from one to the other, but it's to blend the two at the same time.
So that's what the bandit problem is about.
And now in bandit conducts optimization, what you do is a following.
So here I was telling you that, you know, you just have like 10, 10 different ads to show.
But what if instead of a finite set of ads that you can show, you have a linear model, you have some, you know, machine that's going to take, you know, some set of parameters.
And then decide, you know, based on this set of parameters, which ad to show.
So now you want to move instead of deciding which ad to show, you're going to decide the setting of the parameters.
And this is exactly the optimization problem. And then it becomes bandit conducts optimization.
And is the, is that that formulation where you've got the machine and you're setting the parameter is that reflective of a particular set of use cases or is that more machinery that allows you to apply to apply convex optimization.
Right. So, so this program is really more theoretical in nature.
But understanding, you know, the limit of what you can do with what's called bandit feedback. So by bandit feedback, what I mean is you set the parameters.
And you don't know yet what's going to happen with this setting of parameters. You actually have to make an experiment, you know, test the set of parameters.
And then you receive feedback. And then if you want to know, okay, but what about this other set of parameters, when you have to do yet another experiment.
So this kind of back and forth between setting the parameters and doing experiment, this is what's called bandit feedback. And this is what makes a, you know, problem difficult.
And we want to understand, you know, what is the scope of problems that you can solve with this limited type of feedback.
And bandit conducts optimization is, you know, probing those limits. It's not necessarily giving you an algorithm that you're going to implement and, you know, get better, you know, practical things out of it, but it's really more mathematical in nature.
And you also mentioned metrical test systems and case server problem. Can you talk a little bit about that one as well?
Sure, absolutely. So maybe I will explain the case server problem.
Or actually, yeah, maybe let's do the case server problems. And we try to come up like MMK cues and that kind of thing.
Right, right, right, right, absolutely. So this is all related, you know, it's all one, one, you know, all of these work, originate in the OR literature.
So the operational research literature has developed all of these problems. They are all related.
So in the case server problem, what you do is the following. You have K servers.
And what's going to happen is that they live in a space. Maybe let's think about the K taxi problem. It's a simplest thing.
So you have K taxis. And you're, they are located in a city. And then there is a user that comes and request a taxi.
You know, it's literally the Uber problem. So, you know, the user requests a taxi and you as a, you know, designer of the system, you have to choose which taxi you're going to send to that user.
Okay. So maybe you choose that taxi. Okay. So this taxi travels there. And then it takes a user somewhere else. Then also user comes and you have to choose which taxi to send.
Okay. Now, after many, many days, you can ask yourself, okay, as, you know, the designer of the system.
It took me a certain time to, you know, satisfy all those requests to, you know, make sure that every user got that taxi.
But in hindsight, if I look at everything that has happened, there was a better way to do it.
And what you ask yourself is can you design an algorithm, which is going to be competitive with the best possible thing in hindsight.
Meaning that no matter what is the sequence of users that come, you're almost always very close to being optimal.
And the difficulty is very simple to imagine. Imagine that you have two servers. Okay. And that there is, you know, maybe one, one user.
So the two server are, let's say, on the right hand side of the city. Okay. Far, far east in the city.
And then you have a user, you know, that comes on the west side. Okay. So you choose one taxi, you send it there.
Okay. Now there is another user that comes just in the neighbor next to this one. What do you do? Do you send the same taxi? Yeah, maybe.
But then maybe those two users, they keep being requesting, you know, taxis back and forth.
So you keep sending this one taxi back and forth. What's the right thing would have been to recognize that you should have brought in the second taxi, you know, from the east side of the city.
And now you have two taxes there. And they can, you know, manage those two clients very, very efficiently.
So the question is again, it's a kind of exploration expectation. And that's why it's related to bandits. And that's why I was excited about it.
Is that you have to recognize when is it time to bring in this second taxi, you know, even though it's far away.
And you have this one, you know, that can keep going back and forth. It's in fact much better to bring in the second taxi.
So that's, that's okay. So the problem and magical task system in certain kind of generalization of it.
And the way I describe it, it's very combinatorially nature. So it might seem far away from convexity and convex optimization.
But the beauty is that we were able to bring in tools from convex optimization to attack this problem.
Got it, got it. And in that problem, as you described it, there is an element of it that is related to the kind of the feedback delay, you know, when went after you make your choices.
There's an element of it that also has, you're comparing your decision at any point with, with hindsight, which is kind of perfect information about the scenario as it's played out over multiple time steps.
And I guess I'm curious, which, I guess I'm curious about the relationship between, you know, is there a relationship between kind of a simple, you know, gratification delay or, or attribution delay or feedback and this, you know,
overall hindsight of the entire scenario and how those play out. So the beauty of, you know, the two problems that I told you about the bandit program and K server is that they are really about understanding how to deal with uncertainty.
As you're making your decision, you don't know what the future is going to look like. So you have to hedge against whatever can happen into the future.
And the question is this is really, these problems are really about what is the optimal way to deal with uncertainty, which is, of course, you know, something that we have to deal with all the time and it's maybe, you know, the basis of intelligence, which is, you know, kind of really exciting.
Yeah, I think of, so I studied a little bit of of killing theory and the like and grad school and for many years, I would bore my wife about kind of, you know, whenever we'd go into a grocery store or a bank or whatever, you know, trying to explain the theory of which line to get into.
But there's always that okay, how long do I wait for this line before ditching to the next line and that's kind of what we're talking about here is this is exactly what we're talking about is exactly what we're talking about.
Yeah, very practical, very practical problems.
And so you're the approach that you've described as far is analyzing these systems and optimizing these systems, I don't think we've yet talked about, you know, these systems are kind of governed by parameters, we're not necessarily learning parameters or applying machine learning to optimizing these systems yet.
Not really, so the thing is the same tools are being used to set those parameters in those problems that I described and to set the parameters in current machine learning system.
So the reason why it's the same set of tools remains a mystery today because, you know, what I have described to you right now, it's all convex, okay, so convex meaning, you know, nice like there is a relation, there is a structure between the different parameters.
Well, for modern machine learning in neural networks, there is no relation between the different parameters, you know, how to the parameters of two different neurons interact with each other.
There is nothing nice, there is nothing nice about it, yet we use the same tools, you know, something like gradient descent of stochastic gradient descent.
And this is, it's not really clear why, why is that case, but these are kind of two different applications of the same set of tools.
So a little bit of the space that your, or the context for your paper, which in some ways starts to, you know, it's asking some of the questions that we've been asking for a long time about generalization in neural networks and why that works.
So, you know, maybe we'll use that as a segue to have you introduce the paper and the space that you see it fitting into.
Sure, yeah. So, you know, so far I have told you about optimization and how, you know, we want to do it with many, many parameters, like having this, you know, large number of parameters, which mathematically you call high dimension, you know, just many parameters means that you live in high dimension.
Now, in neural networks, there is something really crazy that happened is that we have been using many, many parameters, many more parameters than we ought to, meaning that, you know, to solve certain problems, we know that there is a boundary limit on the number of parameters that you need.
You don't need more parameters than that.
Yet, you know, modern neural networks, they use many, many more than that. And we don't, we don't really understand why and what this paper is about, it's putting, you know, forward on hypothesis, you know, which is backed up by a mathematical theorem and some, you know,
eristic calculation on existing data set, but it's a hypothesis. We don't know if it's, you know, the end of the story, but it puts forth a hypothesis as to why we need many more parameters than we thought, classically, we needed.
This is the context of that paper.
And in order to give you some rough numbers, so you have something in mind, in 2011, when they had the paper, you know, neural network by Jeff Inton, a subscriber and Christchurch, which was the first, you know, paper of the new wave of neural network, their neural network at the time was using 60 million parameters.
And it was to solve a problem with roughly 10 million data points. And the point is, classically, we know that if you have 10 million data points, you don't need more than 10 million parameters.
So the fact that they were using 60 million parameters, so six times more than what the classical theory would tell you sounded really crazy at the time. And it wasn't clear if, you know, maybe it was a hack, maybe, you know, after a few more years of work, we would realize, oh, no, actually just six million parameters is enough, you know, maybe you could reduce it by a factor of 10.
But in fact, history unfolded completely differently, and instead of reducing the number of parameters, we have kept increasing it more and more and more. And you know, the more recent breakthrough in AI is the GPT-3 language model from OpenAI, and that model is using hundreds of billions of parameters.
And, you know, it's to solve problems using a language data set, which is in the billions of parameters. So now we're talking about 100 fold, bigger than what the classical theory would tell you.
So we move from 10x to 100x, and who knows where we're going is, it seems that there is no end in sight. So really, the question is, why is it the case? And the paper gives some plausible explanation.
And I referenced generalization and the idea that with a certain amount of parameters, the training process and the network will memorize the data set, and then you get issues like overfitting, does that play into this paper as well?
Okay, so I said that you never want to use more parameters than the number of data points. And indeed, one reason why we say that is because if you were to use more parameters than number of data points, then you run the risk of memorization of just overfitting your data and not learning any core concept.
You're not going to be able to generalize to unseen data. You're just going to fit, you know, whatever data set has been given to you. But this is not at all what happens with those neural networks, those neural networks, they do generalize.
Now, in our paper with Mark Serke, what we obtain is a result that shows that overparametrization is necessary to achieve something. Now, what is this something? This something is not directly generalization. What this something is is that you can generalization, sorry, overparametrization is necessary.
If you want to fit the data and fit it smoothly, what does it mean fit it smoothly? It means that if I give you an example in your data set that you've seen and you have fitted, you know what is a value there.
Then you want your model to be such that if you move a little bit away from this data point, then the prediction does not change too much. This is what we call by being smooth.
And what we have proved, you know, the law of robustness is the statement that if you want to fit a data set smoothly, then you need more parameters than the number of training points.
You know, so classical theory tells you you have to have less parameters if you want to generalize. And what we say is that no, no, no, actually you need more parameters if you want to fit smoothly.
Now, there is a question and it remains an open question. What is the relationship between generalization and fitting smoothly?
Fitting smoothly is a is simple from a mathematical point of view because you're just saying, okay, I have these data points and I want to pin down the value of those data points.
And I want my function to move, you know, to be very smooth, which is in mathematical language, we just say lip sheets, you know, it's something you learn in first year of undergrad.
So it's kind of a very classical concept and it's we understand it very well.
Generalization on the other end is a much more pure concept, like, mathematically, it's not as well defined.
So how do you move from one to the other? We don't know yet.
And is there a relationship to between the smoothness and differentiability?
Yes, exactly, exactly. So in fact, yeah, very simply put, you know, smoothness just means that you are differentiable and that your differential, the gradient is not too large, is not to be.
This is exactly equivalent.
And I guess the question there was that seems like a low bar.
Yes, for thinking about that Craig, like for deep learning, like that's kind of the low bar, right?
Absolutely. I agree with you. Yeah, absolutely. This seems like the bare minimum you would like to have.
And so that's exactly the point why this result is surprising is that you're asking the bare minimum and immediately you run into the issues that you need over parametrization.
All the parametrization is necessary. So you kind of, you know, shed these slides that, oh, actually, if you just have this simple prerequisite, you know, there's the rata, then you need over parametrization.
Got it, got it, got it.
And so the paper plays in this space of, you know, what I think about is kind of generalization over fitting and the parameterization, all that stuff.
But then there's a separate, you know, thread, I guess, that is related to adversarial robustness.
How does the paper relate to adversarial robustness?
Right, right, right. So, so, okay, so let me just remind the listeners what, you know, the adversarial example phenomenon is.
It's a phenomenon that you take a trained neural network and you know, it's performing very, very well. You give it an image, you recognize it, there is a dog, a cat in it.
But then there is this, this very funny behavior that you're able to slightly perturb the image and make it predict whatever you want.
So you can just change a few pixels and then suddenly it's going to say, you know, that there is a monkey on the image of a dog.
So this is a very weird phenomenon and suddenly not something we see in humans. I mean, you know, sometimes people refer to optical illusion as an equivalent to adversarial examples.
But I want to emphasize optical illusion and adversarial examples are extremely different.
First of all, adversarial example applies to almost all input images. So those neural networks, you know, they are very sensitive to their input for almost any image.
Humans are sensitive to the input only on very special images. So there is a big difference between optical illusion and adversarial example.
Now, again, what is adversarial example? Adversarial example is this phenomenon that if you take an image, maybe in your data set and you move it a little bit, then the output changes dramatically.
So this is exactly being non-smooth, being not lipcious. So you see immediately the relation to the law of robustness. The law of robustness is telling you that you need to have a large model if you want to be smooth.
So what the law of robustness, I think, says is that you will have adversarial examples if you have a model which is too small.
And you know, this could be maybe the answer to why we don't have adversarial robust neural networks yet. It's maybe we're just training two small models.
Yeah, they are just not being enough. Exactly. So we give one example. So, you know, using the law of robustness, you can actually make real exact calculations.
And we did a calculation for the image net data set, which is the one you know that was used by Hinton and his student in 2011 for the 60 million parameters neural net. So remember, it was a data set of size 10 million.
And there's a law of robustness suggests that maybe we need between a billion and 10 billion parameters if you want to be adversarial robust.
The current state of the art model on this problem of size 500 million roughly. Okay. So basically, you know, 10x compared to 10 years ago.
But what we're saying is you're missing yet another 10x. So then you should scale it up by another factor 10x. And then maybe you will manage to be adversarial robustness.
In a sense, you can maybe think of it as there's a ton of work happening in the adversarial robustness community that is coming up with various tricks to try to make these networks more robust.
What your paper perhaps suggests is that there's a brute force approach, which is just make the networks 10 times bigger or much bigger than they are.
Yeah. So I want to emphasize something which is very, very important. So this paper is really an impossibility result.
It's telling you you cannot do it if you're too small. It's not quite telling you that you can do it if you're big enough.
Exactly. Exactly. So I do believe that's a case, actually. But you know, I don't want to miss other people to.
Yeah, I don't know if I have a great answer to this. And this is what you know, I'm working on like many, many other people, which is that this overpowermentization.
So again, the law of robustness shows that it's necessary. But I think there is more to it.
The English that it also, you know, some work. Okay. So there has been some work in the last decade showing that if you overpowerment rise, you also make maybe the landscape, the optimization landscape to be nicer.
So maybe when you add more parameters, the optimization becomes easier and it's going to be easier to find, you know, a good model.
So that's one possible reason. And also just, you know, experimentally, like it seems like people keep training bigger networks and they just keep working better and better.
So just, you know, it's an experimental fact.
My sense of part of the reason why the paper was awarded the outstanding paper award was the approach used in the proof.
And I think that's related to the ISO parametry assumption.
Can you talk a little bit about that? What it means and how it played into the way you address the problem.
Right, right, right. So, you know, ISO parametry, it is very basic question, you know, that goes back to the engines, maybe probably the Greeks, that you want to know, you know, for example, in 3D space, you know, what is going to be the shape of a given volume that has the largest surface.
You know, you, you know, this, this is a very, again, experimental fact because you can have a bubble of soap, for example, and see what shape it's going to take and it's going to take, you know, the shape, which has a maximum surface for a given volume.
And no one behold what is going to be this shape is going to be a ball, okay, a sphere.
And the isoparametric phenomenon is basically asking this question in higher dimensional spaces, okay. And coming back to, you know, optimization, the beauty is that these high dimensional spaces are just about, you know, function of many, many parameters.
And when you translate it into functional languages, instead of, you know, geometric shape language, what basically isoparametry tells you is that a function of many, many independent variables is going to be very close to a constant function, which is not what you would expect at all.
You know, you imagine that if you have many, many variables, many parameters, then it's very far from being constant. But isoparametry tells you that in a very precise sense, actually, if you have many of those, you know, variables, which are set independently of each other, then it's going to be close to a constant function.
What specifically do you mean when you say it is going to be close to a constant function, meaning if you sample it, you know, many, many times, exactly, exactly what I mean is if you sample the variables independently, then I can predict what is the values that you're going to see up to a certain error.
And I can characterize the error exactly as a function of the dimension. So as I have more and more variable, my error bar is going to get smaller and smaller.
And so this is a phenomenon of isoparametry. As you have more, you know, variables, I can predict more and more accurately what is going to be the value of the function.
And we leverage this fact to say that, oh, so then that means that if you have too few parameters, you just cannot fit, you know, smoothly. And this is basically what the proof does.
Meaning if you have too few parameters, your error bars are too large for you to ensure the smooth fit.
Exactly, exactly right. Exactly. So this is what, what, how, how the paper proves it. And you know, I think, so I don't know exactly why the paper was a world, but I think one of the main reason was was really that despite a decade of work, you know, and an incredible interest
burst from industry and academia, we still don't really know why we need to overparameterize. And this is the first paper that, you know, doesn't show in a contrived setting that, you know, overparameterization can help.
But it's actually a pretty broad setting, like it doesn't make much assumption. And out of not very much, you get this, you know, fact that you need overparameterization.
So this is, I think, one of the reason why it was a world and the proof itself, you know, one of the beauty of the paper is that the proof is simple.
You know, it's something you can teach in less than an hour, probably an hour, you know, in a graduate course. So that's nice. That's something that doesn't happen too often.
And you talked about the degree of the constraints imposed by the assumptions here, and we talked a little bit about the isopermetry.
And it's, you know, simplicity and multiple dimensions, but, you know, why is that not a, you know, a constraining constraint, if you all right, right.
So, so the serum as any mathematical serum has assumptions. And, you know, those assumptions do not map on to reality perfectly. Right. In fact, obviously far from me. One of the crucial assumption in the serum for the law of robustness is that you have what's called labor noise.
Similarly, you know, if you take a simple example, let's say you take a large, you know, data set of images, then the law of robustness is not going to apply directly to this specific data set, but it's going to apply if you train.
And for each image, maybe you flip the label, you know, whether it's a dog or cat with formality, I don't know, point one. Okay. So with very small probability one, maybe, you know, maybe not even point one, maybe point zero one. Okay. So maybe one percent of the time you just change the label in your training.
And if you do have this label noise, then the law of robustness is going to apply. Now, of course, in reality, you don't have this label. You know, data sets are just clean. And, you know, nobody is messing with the label. I mean, why, why would you do that?
So there is, this is one of the few discrepancies between the assumption of the serum and what happens in real life. You know, I just gave you one with this label noise.
There are, you know, maybe one or two more. Okay. And going beyond those assumption is something, you know, that you can do potentially, you know, so there could be refinement, like stronger version of the law of robustness.
But it's certainly going to be more difficult mathematically. And I don't expect you can get to strictly stronger result and, you know, still be able to teach the proof in an hour that that would not be my guess.
Got it, got it. And what's the, what's the summary of the, you know, what's the 30 second version of the hour long teaching of the proof? Like what's the general approach you take? Is there a way to clarify that?
Yes, yes. So the general approach is to relate the problem to isoparametry. So that's, that's, you know, the general approach. So basically the general approach is to recognize that neural networks don't matter in this problem.
You can take any space of function. So take any space of function. And then what you're going to do is that you're going to reason about each individual function.
And you're going to say that each individual function in your space of function, it has a very low probability of being a good function for your data set because of the label noise.
So you're going to take into account the label noise. And you do this calculation of what is the priorities that the fixed function is going to fit your data sets mostly.
So the calculation, you know, appears to the, to, to isoparametry. And then you can generalize using classical tools to the whole space of function. So that's, you know, the 30 seconds version was approved.
Got it. Awesome.
And so we've talked a little bit about, you know, how the paper, you know, aids are intuition and understanding of the relationship between parameters and models. And it may have some implication into the way we think about adversarial robustness.
Are there other ways that you see the paper playing into kind of the practical practicalities.
Yeah. So to me, you know, what really is the take away message of this paper to me is that, you know, those large models are necessary and we won't be able to get away from them.
But more so than that also is that it really gets you into a scale size, a number of parameters, why think the current training methods are just not, you know, appropriate anymore.
So you can just imagine, you know, take it to the limit, like if you have a, a neural network, which is trillions and trillions of parameters.
And you know, you show to this neural network a single image with a cat on the image, are you really going to update trillions of parameters based on just a single image, which is what current training method would do.
So I think this is not reasonable. And what you would imagine is that, you know, the single image, maybe it's going to, you know, sparsely update the set of parameters.
Maybe I don't know for this single image, I'm going to choose a thousand parameters and update this thousand parameters.
And currently there is basically no training method that does that. There is nothing that does this kind of sparse update.
So there is a few words that, you know, I've recognized this issue, but that came much more from an engineering perspective, just saying, you know, how do we scale up to trillions of parameters, you know, using our form of GPUs or TPUs.
What is the training method that we're going to do because we just cannot, you know, store everything on the single GPU.
So it was a very engineering perspective. And what I'm saying is that I think there is a more philosophical angle on this question, like a more mathematical angle.
And this is, I think, you know, where the law for vastness leads you. I don't have the answer yet, but, but I think this is where you know, the good questions reside.
Awesome. Awesome. Well, Sebastian, thanks so much for joining us and sharing a bit about the paper. And once again, congratulations.
Sure. Thanks a lot.
