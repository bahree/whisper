1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,880
I'm your host Sam Charrington.

4
00:00:31,880 --> 00:00:36,800
While at this past NURBS, I attended the second annual Black NAI workshop, which gathered

5
00:00:36,800 --> 00:00:41,680
participants from all over the world to showcase their research, share experiences and support

6
00:00:41,680 --> 00:00:43,080
one another.

7
00:00:43,080 --> 00:00:47,560
This week, we continue our Black NAI series with interviews with some of the great presenters

8
00:00:47,560 --> 00:00:49,280
from the workshop.

9
00:00:49,280 --> 00:00:55,440
Today I'm joined by Justice Ammo, a PhD student at Dartmouth, Thayer School of Engineering.

10
00:00:55,440 --> 00:00:59,720
Justice presented his work on an optimized recurrent unit for ultra low power acoustic

11
00:00:59,720 --> 00:01:03,400
event detection.

12
00:01:03,400 --> 00:01:08,200
In our conversation, we discuss his goal of bringing low cost, high efficiency wearables

13
00:01:08,200 --> 00:01:10,720
to market for monitoring asthma.

14
00:01:10,720 --> 00:01:15,920
We explore the many challenges of using classical machine learning models on microcontrollers,

15
00:01:15,920 --> 00:01:20,800
and how he went about developing models optimized for constrained hardware environments.

16
00:01:20,800 --> 00:01:25,360
We'd also like to wish Justice the best of luck as he should be defending his PhD any day

17
00:01:25,360 --> 00:01:26,360
now.

18
00:01:26,360 --> 00:01:27,360
Enjoy.

19
00:01:27,360 --> 00:01:32,040
Alright everyone, I am on the line with Justice Ammo.

20
00:01:32,040 --> 00:01:37,120
Justice is a PhD student at the Thayer School of Engineering at Dartmouth.

21
00:01:37,120 --> 00:01:40,080
Justice, welcome to this week in machine learning and AI.

22
00:01:40,080 --> 00:01:43,480
Thank you Sam, I'm happy to talk to you today about my wake.

23
00:01:43,480 --> 00:01:49,000
Fantastic, so to kind of lead us into what you're up to, I would love if you share a little

24
00:01:49,000 --> 00:01:53,600
bit about how you got started working in machine learning and AI.

25
00:01:53,600 --> 00:01:57,080
Yes, I'm happy to share a bit about my background.

26
00:01:57,080 --> 00:02:02,760
So I was born and raised in Ghana and came to Dartmouth for undergrad.

27
00:02:02,760 --> 00:02:07,560
So this was in like 2009, I mean, undergrad when I was coming in, I always wanted to do

28
00:02:07,560 --> 00:02:08,560
electronics.

29
00:02:08,560 --> 00:02:10,760
So that's where I started.

30
00:02:10,760 --> 00:02:12,160
Pretty much embedded system tried.

31
00:02:12,160 --> 00:02:15,360
How do you build devices that was always fascinating to me?

32
00:02:15,360 --> 00:02:21,720
So I finished undergrad becoming sort of good at building devices for embedded systems.

33
00:02:21,720 --> 00:02:28,280
And then when I stayed a year after undergrad to do research, one of my mentors in undergrad

34
00:02:28,280 --> 00:02:34,040
was he had this research project and he was like, hey, I think you really do good on

35
00:02:34,040 --> 00:02:35,040
this.

36
00:02:35,040 --> 00:02:41,960
He was interested in building a wearable monitor for monitoring asthma, like monitoring

37
00:02:41,960 --> 00:02:42,960
the lungs, right?

38
00:02:42,960 --> 00:02:47,640
Like if you can listen to what is going on in the lungs, then maybe you can better advise.

39
00:02:47,640 --> 00:02:52,720
So he asked me to come on board and I was excited to come on board and just explore that project

40
00:02:52,720 --> 00:02:54,440
for a year.

41
00:02:54,440 --> 00:02:56,280
And it really came down to two things.

42
00:02:56,280 --> 00:03:01,600
One was how do you build the hardware that can really listen to, you know, can acquire

43
00:03:01,600 --> 00:03:03,560
the sound really well, right?

44
00:03:03,560 --> 00:03:07,480
And then the second phase is once you have that hardware in place, right?

45
00:03:07,480 --> 00:03:09,880
How do you detect these sounds well enough?

46
00:03:09,880 --> 00:03:15,760
How do you detect sounds and symptoms like coughing, whizzing or even when someone is

47
00:03:15,760 --> 00:03:16,760
panting?

48
00:03:16,760 --> 00:03:18,560
How do you detect them and do that very well?

49
00:03:18,560 --> 00:03:24,760
And so that one year I did, we came pretty far on, like, device in a good hardware.

50
00:03:24,760 --> 00:03:27,880
And so I started looking at, you know, how do we do detection?

51
00:03:27,880 --> 00:03:30,240
What are the algorithms that exist in for detection?

52
00:03:30,240 --> 00:03:34,720
That's what brought me to classifiers and machine learning and looking at that.

53
00:03:34,720 --> 00:03:38,600
And so after that one one year, we're all excited about the work as I say, hey, let's

54
00:03:38,600 --> 00:03:43,480
explore this as a PAD work and I start going to work on it.

55
00:03:43,480 --> 00:03:48,720
And it was mainly from the standpoint of your, okay, how do I, now that I have some hardware,

56
00:03:48,720 --> 00:03:50,840
how do I make the signal really great?

57
00:03:50,840 --> 00:03:55,160
And then how do I really detect these sounds very well?

58
00:03:55,160 --> 00:04:00,720
So started from traditional classifiers, like, you know, decision trees, hidden macaw models

59
00:04:00,720 --> 00:04:03,600
because those were the things that people were doing around that time.

60
00:04:03,600 --> 00:04:08,600
And then all the way meeting to, you know, deep learning, looking at, like, convenants

61
00:04:08,600 --> 00:04:11,400
and then more recently into recurrent.

62
00:04:11,400 --> 00:04:16,000
So that's kind of been the very broad journey, yeah, of mine.

63
00:04:16,000 --> 00:04:22,880
One thing that jumps out at me hearing the story is that you started with the hardware

64
00:04:22,880 --> 00:04:29,560
and then went on to think about the software, is that it stretched me as the opposite of

65
00:04:29,560 --> 00:04:34,960
the way that won my purchase, is that like an academic thing or was there something novel

66
00:04:34,960 --> 00:04:40,040
about the way you were approaching the hardware that you thought was the right thing for this

67
00:04:40,040 --> 00:04:41,040
application?

68
00:04:41,040 --> 00:04:46,240
Yeah, actually, it's interesting you point that out because like, I really started out

69
00:04:46,240 --> 00:04:47,720
interested in hardware, right?

70
00:04:47,720 --> 00:04:49,680
How do I build really good hardware?

71
00:04:49,680 --> 00:04:54,720
And the point wasn't just, you know, build whatever hardware, like, you know, build the

72
00:04:54,720 --> 00:05:01,280
best hardware, I was really constrained for very efficient but affordable hardware.

73
00:05:01,280 --> 00:05:05,720
The initial boost, where, like I mentioned, I'm from Ghana, so our initial boost, where

74
00:05:05,720 --> 00:05:10,880
how do we build devices that can be used in, you know, and the resource communities,

75
00:05:10,880 --> 00:05:15,080
communities where, you know, they cannot afford to buy really expensive devices, right?

76
00:05:15,080 --> 00:05:21,640
In areas that you can have doctors to do diagnosis, can use some of these tools to, you know,

77
00:05:21,640 --> 00:05:22,640
better do diagnose.

78
00:05:22,640 --> 00:05:25,120
So that was really the real motivation.

79
00:05:25,120 --> 00:05:30,520
And so when I was building hardware, even in building the hardware, I realized that,

80
00:05:30,520 --> 00:05:34,240
you know, hardware is expensive, software is cheap, right?

81
00:05:34,240 --> 00:05:37,120
So there is this trade off you can do, right?

82
00:05:37,120 --> 00:05:46,160
You can build a just okay hardware and do more of the signal processing and machine learning

83
00:05:46,160 --> 00:05:48,440
so that you don't have to use, you know, the best sense.

84
00:05:48,440 --> 00:05:52,520
So you can have really cheap hardware and then software is much cheaper, so, you know,

85
00:05:52,520 --> 00:05:53,960
you can duplicate it, right?

86
00:05:53,960 --> 00:05:57,840
So that's where it really, like I followed that trajectory, where I built a hardware

87
00:05:57,840 --> 00:06:01,920
that was like, you know what, this hardware is good enough, I could spend more time making

88
00:06:01,920 --> 00:06:06,000
the best hardware that would be very expensive, but I think this is affordable.

89
00:06:06,000 --> 00:06:10,600
And so now, how do I use software to bridge that gap to go the extra mile, right?

90
00:06:10,600 --> 00:06:15,920
And so that's where it came in and machine learning became a really good answer to that,

91
00:06:15,920 --> 00:06:16,920
right?

92
00:06:16,920 --> 00:06:23,600
And if you have data that is not like 100% like, you know, clean, you can still use advanced

93
00:06:23,600 --> 00:06:29,760
patent recognition tools to extract information from that and make really reasonable inferences

94
00:06:29,760 --> 00:06:30,760
of that.

95
00:06:30,760 --> 00:06:34,760
So it was like that practical approach that brought me into machine learning.

96
00:06:34,760 --> 00:06:41,520
And so before we jump into the machine learning elements, can you give us an overview

97
00:06:41,520 --> 00:06:48,000
of the hardware and the various components? It sounds like acoustics, maybe some microphones

98
00:06:48,000 --> 00:06:50,680
or something like that.

99
00:06:50,680 --> 00:06:54,160
And, you know, what is the processing capability?

100
00:06:54,160 --> 00:06:56,640
What are the various components of the hardware system?

101
00:06:56,640 --> 00:07:02,880
So as I said, I'm interested in this application, we're interested in basically escortating

102
00:07:02,880 --> 00:07:04,240
sounds from the body.

103
00:07:04,240 --> 00:07:06,760
So that's like what a status group does, right?

104
00:07:06,760 --> 00:07:11,280
And so you want an acoustic sensor that you can collect sound from the body.

105
00:07:11,280 --> 00:07:19,040
You could use really advanced microphones, like MEMS, but then those are really, they

106
00:07:19,040 --> 00:07:20,640
can be more expensive.

107
00:07:20,640 --> 00:07:26,880
But also, when you think of collecting data from the body, you can, and you can use pz

108
00:07:26,880 --> 00:07:30,400
electric transducers that have contact sensors, right?

109
00:07:30,400 --> 00:07:34,600
And so when you have that compared to a microphone, it would only pick up sounds that is coming

110
00:07:34,600 --> 00:07:35,960
from the patient.

111
00:07:35,960 --> 00:07:40,520
So already you start off eliminating a lot of the external background sounds, right?

112
00:07:40,520 --> 00:07:46,960
So a huge part of the hardware system is a contact pz electric transducer, also a pz

113
00:07:46,960 --> 00:07:49,360
electric transducer can generate power.

114
00:07:49,360 --> 00:07:51,320
And so it's not that power hungry.

115
00:07:51,320 --> 00:07:56,040
You don't have to, it's going to be really good for low power applications, right?

116
00:07:56,040 --> 00:08:00,720
And so from there, from the pz electric transducer to your raw sensor, you have some analog

117
00:08:00,720 --> 00:08:04,200
front end secretary to condition the signal, right?

118
00:08:04,200 --> 00:08:07,960
You're looking for things like cove sounds and we sounds.

119
00:08:07,960 --> 00:08:10,680
But a person is going to be wearing this most of the time, right?

120
00:08:10,680 --> 00:08:12,680
And people speak all the time.

121
00:08:12,680 --> 00:08:17,280
So you want to make sure that you're actually filtering out a lot of this, you know, the

122
00:08:17,280 --> 00:08:20,880
speech sounds and all of the other possible background sounds.

123
00:08:20,880 --> 00:08:23,680
So that's what a lot of the analog front end does.

124
00:08:23,680 --> 00:08:28,400
So these are things like low pass, high pass, notch filters, that kind of thing?

125
00:08:28,400 --> 00:08:30,000
Exactly, exactly.

126
00:08:30,000 --> 00:08:35,280
So you have, you know, a couple of stages of, you know, classical analog filters where

127
00:08:35,280 --> 00:08:40,280
you are really in the, in the kind of the range that we are interested in, you're looking

128
00:08:40,280 --> 00:08:43,520
for sounds between 100 heads and two kilohertz, right?

129
00:08:43,520 --> 00:08:48,480
So you're attenuating anything outside of that band, that band and sort of really amplifying

130
00:08:48,480 --> 00:08:49,760
the signal there.

131
00:08:49,760 --> 00:08:53,400
Because in that way, you can capture the re sounds, the cove sounds and the things that

132
00:08:53,400 --> 00:08:55,200
are of interest, right?

133
00:08:55,200 --> 00:08:56,200
Yeah.

134
00:08:56,200 --> 00:09:01,000
So once you condition that signal, then now you come to, you know, you need a microprocessor

135
00:09:01,000 --> 00:09:07,280
to sample the, to sample the analog signal and then, you know, maybe preprocess and identify

136
00:09:07,280 --> 00:09:09,720
what events actually okay, right?

137
00:09:09,720 --> 00:09:12,680
And so there you have a lot of options for microcontrollers.

138
00:09:12,680 --> 00:09:16,800
But if you think of something that is going to be wearable for, you know, there's something

139
00:09:16,800 --> 00:09:21,800
that someone will wear for maybe a whole day, that leaves you with only ultra low power

140
00:09:21,800 --> 00:09:22,800
microcontrollers, right?

141
00:09:22,800 --> 00:09:28,440
You cannot think of some of the fancy, you know, and for even M7, you have to go to something

142
00:09:28,440 --> 00:09:30,480
that is really, really low power.

143
00:09:30,480 --> 00:09:36,200
So the huge specification of this was to go for the lowest power microcontroller.

144
00:09:36,200 --> 00:09:38,720
So we are using an ARM M0, right?

145
00:09:38,720 --> 00:09:43,760
It's like the lowest power, the most energy efficient ARM microcontrollers are out there,

146
00:09:43,760 --> 00:09:44,760
right?

147
00:09:44,760 --> 00:09:48,560
So once you go there, that's where all the constraints begin to happen because you don't

148
00:09:48,560 --> 00:09:54,480
have, you don't have floating point operations, you don't have DSP instructions.

149
00:09:54,480 --> 00:09:56,200
So you are really constrained, right?

150
00:09:56,200 --> 00:09:59,800
And you still have to be able to get inference and working at that level.

151
00:09:59,800 --> 00:10:03,760
That's, I guess, pretty much sort of the set up for the hardware.

152
00:10:03,760 --> 00:10:11,720
And so that led you down the path of trying to figure out how to get the various types

153
00:10:11,720 --> 00:10:18,320
of software that you wanted to have working on this hardware, you had to work within kind

154
00:10:18,320 --> 00:10:21,400
of these constraints.

155
00:10:21,400 --> 00:10:28,880
You've got some constraints that are set up by the choice of microcontroller.

156
00:10:28,880 --> 00:10:34,680
Are you also thinking about power envelope and that kind of thing, or is that all inferred

157
00:10:34,680 --> 00:10:37,160
by the microcontroller choice?

158
00:10:37,160 --> 00:10:41,360
So actually, the ultimate and fundamental constraint is power, right?

159
00:10:41,360 --> 00:10:44,200
Power is always, right?

160
00:10:44,200 --> 00:10:45,200
Exactly.

161
00:10:45,200 --> 00:10:47,080
So you start out with power.

162
00:10:47,080 --> 00:10:50,480
And if you really look at the power range, you are talking about, right?

163
00:10:50,480 --> 00:10:52,040
To put it in perspective.

164
00:10:52,040 --> 00:10:58,800
So I like smartphones around the thousand milliwatts, even some of these fun embedded systems

165
00:10:58,800 --> 00:11:03,080
are around hundreds of milliwatts, but we are talking of tens of milliwatts.

166
00:11:03,080 --> 00:11:05,680
So that already really constrains you.

167
00:11:05,680 --> 00:11:10,640
And when you look at all the microprocesses available within that low, ultra-low power

168
00:11:10,640 --> 00:11:11,640
range, right?

169
00:11:11,640 --> 00:11:17,120
You realize they have so many things in common, like they don't have, you know, they don't

170
00:11:17,120 --> 00:11:22,440
have floating point operations, the clocks are normally much lower.

171
00:11:22,440 --> 00:11:25,640
We are looking at things less than 48 megahertz.

172
00:11:25,640 --> 00:11:30,280
And so the constraints become very apparent for that range.

173
00:11:30,280 --> 00:11:36,120
I guess what I was curious about was, you know, certainly within a given power constraint,

174
00:11:36,120 --> 00:11:38,960
you choose your microcontroller.

175
00:11:38,960 --> 00:11:46,920
But I guess I was envisioning a scenario where you want to do things at the software level

176
00:11:46,920 --> 00:11:52,320
to even further manage the power consumption given a specific microcontroller.

177
00:11:52,320 --> 00:11:56,800
Are you having to deal with that kind of thing or did you just constrain the power by choosing

178
00:11:56,800 --> 00:12:00,360
the microcontroller and then you can go hog while you're with?

179
00:12:00,360 --> 00:12:01,360
No.

180
00:12:01,360 --> 00:12:06,360
So that's a perfect question actually, because even with the microcontroller, you have

181
00:12:06,360 --> 00:12:11,400
in place, if you just run it all the time, you're not going to make your power budget.

182
00:12:11,400 --> 00:12:20,000
So you have to do this, you know, event pre-event detection task that can, you know, your

183
00:12:20,000 --> 00:12:23,000
microcontroller is mostly in a very low power state.

184
00:12:23,000 --> 00:12:27,240
And you wake it up when you think that the events you are seeing correspond to, you need

185
00:12:27,240 --> 00:12:29,000
to make an inference on it.

186
00:12:29,000 --> 00:12:33,480
So you have all of those like low levels sort of software already implemented.

187
00:12:33,480 --> 00:12:37,840
It's kind of like the very first layer of your entire detection pipeline, right?

188
00:12:37,840 --> 00:12:41,960
This wake up webbit has an sound event okay and at all, right?

189
00:12:41,960 --> 00:12:44,160
Before you wake up, other things to be processing.

190
00:12:44,160 --> 00:12:46,480
So you have that block link.

191
00:12:46,480 --> 00:12:52,640
It sounds like a low level version of like the wake word for an Alexa type of device.

192
00:12:52,640 --> 00:12:54,840
Exactly, exactly, exactly.

193
00:12:54,840 --> 00:12:55,840
Yeah.

194
00:12:55,840 --> 00:13:00,320
But putting like the whole pipeline is very, very interesting because you have to get all

195
00:13:00,320 --> 00:13:04,160
these blocks in place in order to make all your specifications.

196
00:13:04,160 --> 00:13:10,360
And so you have the hardware in place, I'm imagining that the traditional way of solving

197
00:13:10,360 --> 00:13:17,840
this problem is using kind of time-tested digital signal processing algorithms.

198
00:13:17,840 --> 00:13:21,200
Is that the direction that you'd first go with this or is there something else?

199
00:13:21,200 --> 00:13:22,200
Yeah, yeah.

200
00:13:22,200 --> 00:13:28,080
So when I started my research very early on, right, the traditional way was, you know what

201
00:13:28,080 --> 00:13:34,240
you get your signal and you do like a smart signal processing as you can.

202
00:13:34,240 --> 00:13:37,720
So you're doing things like, you know, what is the best feature extraction that you can

203
00:13:37,720 --> 00:13:38,720
do, right?

204
00:13:38,720 --> 00:13:43,840
So people have come up with so many sort of kind of fifth features that once you extract

205
00:13:43,840 --> 00:13:47,480
those features, it makes it so easy to fail your event, right?

206
00:13:47,480 --> 00:13:51,640
So I'm imagining things like FFTs would come into play here.

207
00:13:51,640 --> 00:13:53,640
Exactly, exactly, exactly.

208
00:13:53,640 --> 00:13:59,120
So FFTs being fast for your transform form is a way to kind of extract the frequency

209
00:13:59,120 --> 00:14:02,640
components of an incoming signal.

210
00:14:02,640 --> 00:14:04,440
Exactly, exactly.

211
00:14:04,440 --> 00:14:10,320
So doing something like that and even other sort of steps on top of the FFTs, so get

212
00:14:10,320 --> 00:14:16,920
something like the spectrails, centroid, the fundamental frequency and all of these things

213
00:14:16,920 --> 00:14:22,240
make it much, much easier so that now you can just use a very simple, say, like, you

214
00:14:22,240 --> 00:14:26,360
know, thresholding to detect when an event has happened, right?

215
00:14:26,360 --> 00:14:29,400
So that was kind of like the assistant work, right?

216
00:14:29,400 --> 00:14:33,840
How do you extract the salient features from this so that classification becomes so,

217
00:14:33,840 --> 00:14:35,040
so much easier, right?

218
00:14:35,040 --> 00:14:36,040
Okay.

219
00:14:36,040 --> 00:14:40,040
But of course, the handicap of that is, what is the best feature, right?

220
00:14:40,040 --> 00:14:46,080
So FFTs spend so many years sort of identifying the best features for say, curve detection

221
00:14:46,080 --> 00:14:47,680
for whiz detection.

222
00:14:47,680 --> 00:14:50,320
And I started out with looking at all of that.

223
00:14:50,320 --> 00:14:54,960
And that's when the promise of deep learning became really exciting because in deep learning

224
00:14:54,960 --> 00:14:58,240
now, you don't have to hunt engineer features, right?

225
00:14:58,240 --> 00:15:02,440
You can do the learning of both the features and the classifier jointly.

226
00:15:02,440 --> 00:15:07,680
So that's actually what pushed me into that direction that, hey, what I'm sitting here

227
00:15:07,680 --> 00:15:12,480
trying to hand crab the best feature sets, but it can actually be automated as part of

228
00:15:12,480 --> 00:15:13,480
the process.

229
00:15:13,480 --> 00:15:16,400
So how about we look at that and see how that compares?

230
00:15:16,400 --> 00:15:21,240
And so as a, you know, someone who's equally excited about deep learning, I kind of get

231
00:15:21,240 --> 00:15:31,120
that as a direction, but I wonder if you've got, you know, a very well defined kind of body

232
00:15:31,120 --> 00:15:35,560
of work for that's, you know, there's already handing you the features, kind of telling

233
00:15:35,560 --> 00:15:37,400
you the way to do it.

234
00:15:37,400 --> 00:15:40,080
What's the real advantage that deep learning is giving you?

235
00:15:40,080 --> 00:15:41,080
Yeah.

236
00:15:41,080 --> 00:15:46,360
So one of the key things to start realizing was these models with that, these kind

237
00:15:46,360 --> 00:15:50,600
of like handcrafted features were there, but one of the areas that they really struggled

238
00:15:50,600 --> 00:15:55,920
to do with was how do you handle, you know, the temporal nature of the data, right?

239
00:15:55,920 --> 00:15:56,920
Okay.

240
00:15:56,920 --> 00:15:59,240
That means it's pretty key.

241
00:15:59,240 --> 00:16:04,760
So a typical part actually has very key characteristics.

242
00:16:04,760 --> 00:16:07,120
You have about three phases.

243
00:16:07,120 --> 00:16:13,120
So there's a phase phase that is sort of the explosive phase.

244
00:16:13,120 --> 00:16:19,240
You literally have like these three phases that are very distinctive to the core, right?

245
00:16:19,240 --> 00:16:23,080
And the properties in any of these three states are very different, right?

246
00:16:23,080 --> 00:16:28,320
But then most of the previous methods were all sort of pretty much like think of them as

247
00:16:28,320 --> 00:16:33,640
a static way of approaching and not really taking advantage of the temporal properties.

248
00:16:33,640 --> 00:16:36,840
So they could only do so well, right?

249
00:16:36,840 --> 00:16:42,120
And people had now started using things like hidden Markov models to try to model those

250
00:16:42,120 --> 00:16:46,920
temporal patterns and those were sort of like working very well, but then hidden Markov

251
00:16:46,920 --> 00:16:52,040
models are pretty, you know, basically algorithms to start dealing with, right?

252
00:16:52,040 --> 00:16:59,800
And so a good segue was, hey, and I started out also like implementing HMMs for this application.

253
00:16:59,800 --> 00:17:05,160
And at that level, you've already gone into the complexity that, you know, you can now

254
00:17:05,160 --> 00:17:09,920
use some of that even deep learning models that these days much easier than, you know,

255
00:17:09,920 --> 00:17:12,480
a full blown hidden Markov model, right?

256
00:17:12,480 --> 00:17:18,600
So you quickly run into the area where you realize that this traditional approach was

257
00:17:18,600 --> 00:17:21,120
not doing the work quite right.

258
00:17:21,120 --> 00:17:26,720
And congestion becomes even much more complex when you start thinking of different patients,

259
00:17:26,720 --> 00:17:32,480
or main patient to patient variability, so the sounds, the cough sounds sound very different

260
00:17:32,480 --> 00:17:34,000
from different patients.

261
00:17:34,000 --> 00:17:37,920
And when they have different conditions, it makes it sound like, you know, very different.

262
00:17:37,920 --> 00:17:43,120
So you quickly run into the issue where you just feel like, no, I need better models

263
00:17:43,120 --> 00:17:44,840
to really capture this.

264
00:17:44,840 --> 00:17:48,600
How far down the path of the traditional models did you go?

265
00:17:48,600 --> 00:17:49,760
Did you try?

266
00:17:49,760 --> 00:17:54,640
Did you have a working HMM that you could later compare to deep learning?

267
00:17:54,640 --> 00:17:55,640
Yeah.

268
00:17:55,640 --> 00:17:59,000
So that was actually one of my very first papers.

269
00:17:59,000 --> 00:18:04,720
I did a whole survey on all of these traditional algorithms that folks have been using.

270
00:18:04,720 --> 00:18:09,520
They had decision trees, you had your SVM, you had the HMM with like, you know, Gaussian

271
00:18:09,520 --> 00:18:11,640
make some model observations.

272
00:18:11,640 --> 00:18:16,200
And then you had like the, you know, the early neural networks I started working on.

273
00:18:16,200 --> 00:18:20,320
So the first one I started being was to use convolutional neural networks.

274
00:18:20,320 --> 00:18:25,880
And they compared very well, they compare like, you know, much better than the HMM, right?

275
00:18:25,880 --> 00:18:27,200
Which was very, very impressive.

276
00:18:27,200 --> 00:18:33,440
And at that time, I was using the convolutional neural net on top of the spectrogram, right?

277
00:18:33,440 --> 00:18:36,480
I was using, yeah, on top of the spectrogram.

278
00:18:36,480 --> 00:18:41,280
So it was being able to learn both the, you know, the temporal and spatial in terms of

279
00:18:41,280 --> 00:18:47,720
the frequency components of the sound and helping you make decisions, like make good inferences

280
00:18:47,720 --> 00:18:49,040
based off on that.

281
00:18:49,040 --> 00:18:54,400
So I started off actually validating that, hey, actually, these deep learning approaches

282
00:18:54,400 --> 00:18:59,280
work much better than the traditional, you know, approaches, those are some of my

283
00:18:59,280 --> 00:19:03,560
anyways, okay, before jumping or into it, okay.

284
00:19:03,560 --> 00:19:11,480
So I'm imagining the early work into CNN's, you, you validated all this, but you were

285
00:19:11,480 --> 00:19:15,960
doing this on desktop computers and then you had to figure out, okay, how do I get this

286
00:19:15,960 --> 00:19:18,960
thing to actually run on the devices?

287
00:19:18,960 --> 00:19:21,240
Exactly, exactly, exactly.

288
00:19:21,240 --> 00:19:25,680
Before we dig into into that, I just remember a question I had from your earlier description

289
00:19:25,680 --> 00:19:28,440
of the hardware.

290
00:19:28,440 --> 00:19:37,800
You mentioned that you were using traditional analog stages to condition the signal.

291
00:19:37,800 --> 00:19:42,120
One of the great things, and you mentioned this about deep learning is that you don't

292
00:19:42,120 --> 00:19:51,560
have to do a lot of feature engineering and often you find that by giving the network,

293
00:19:51,560 --> 00:19:55,840
you know, as much data and not doing as much pre-processing as you might otherwise do,

294
00:19:55,840 --> 00:20:00,840
you can find patterns that haven't traditionally been used, like maybe a cough has like these

295
00:20:00,840 --> 00:20:05,840
really high frequency components that no one really thought about, but that can be predictive.

296
00:20:05,840 --> 00:20:08,640
Did you explore that angle at all?

297
00:20:08,640 --> 00:20:13,440
Yeah, so I did, I did with sort of the first combination, right?

298
00:20:13,440 --> 00:20:21,120
So this was around, I started this week, around 2014, 2015 using the combination, and

299
00:20:21,120 --> 00:20:27,160
so I was using like piano at that time, and I started visualizing the features it was

300
00:20:27,160 --> 00:20:28,760
learning, right?

301
00:20:28,760 --> 00:20:33,560
Of course, some of them were, there were some that looked really interesting, so if you

302
00:20:33,560 --> 00:20:38,560
look at the spectrogram, normally in speech, you have these harmonics, right?

303
00:20:38,560 --> 00:20:43,720
Where you would have these kind of distinct fine lines in the spectrum, but cough is just

304
00:20:43,720 --> 00:20:45,600
a burst of energy, right?

305
00:20:45,600 --> 00:20:47,920
So it's quite over the entire place, right?

306
00:20:47,920 --> 00:20:50,440
It's like the whole place is just full of energy.

307
00:20:50,440 --> 00:20:54,400
And you could see that what the network was actually doing was for the cough.

308
00:20:54,400 --> 00:21:00,400
It was really activated by like a broad range across the spectrum.

309
00:21:00,400 --> 00:21:05,920
For us, like for the speech, it was mostly activated by these sort of lines, right?

310
00:21:05,920 --> 00:21:10,200
And in that space, I thought I was discriminating between cough and speech.

311
00:21:10,200 --> 00:21:15,200
So you could definitely see that it was learning this notion, and was being activated by the

312
00:21:15,200 --> 00:21:20,760
fact that in cough, you have a burst of energy spread across a wide range of frequency,

313
00:21:20,760 --> 00:21:22,760
and it would highlight those.

314
00:21:22,760 --> 00:21:27,000
Given that, why is it still important to do the conditioning?

315
00:21:27,000 --> 00:21:33,080
Oh, so the reason why it's actually important to do the conditioning is because of the sense

316
00:21:33,080 --> 00:21:34,680
that we are using, right?

317
00:21:34,680 --> 00:21:41,480
So like I mentioned, I'm using this piezoelectric transducer, which is, it's not like you

318
00:21:41,480 --> 00:21:46,360
know, your best microphone is actually a good, it's used for like guitar pickups.

319
00:21:46,360 --> 00:21:47,960
So the sense is not great to begin with.

320
00:21:47,960 --> 00:21:51,600
So your signal is actually really bad to start out with.

321
00:21:51,600 --> 00:22:00,680
So the conditioning that we do on there is one to sort of make sure that hey, your signal

322
00:22:00,680 --> 00:22:06,040
is at least clean enough that it's even audible when you hear that, you know, a person

323
00:22:06,040 --> 00:22:11,040
can discern something because without that conditioning, you can even hear the different

324
00:22:11,040 --> 00:22:17,800
events that okay, but yeah, but I think what was really critical is you do have a point

325
00:22:17,800 --> 00:22:23,360
because we could have done more conditioning on there to get a really great signal, right?

326
00:22:23,360 --> 00:22:25,720
But that would have defeated the point.

327
00:22:25,720 --> 00:22:31,240
And so because you could have added more stages, just like really good operational amplifiers,

328
00:22:31,240 --> 00:22:33,480
but we really did the minimal.

329
00:22:33,480 --> 00:22:36,960
So we have like a one stage filter, for instance, right?

330
00:22:36,960 --> 00:22:41,240
A minimum processing so that you can use the rest of the deep learning pipeline to take

331
00:22:41,240 --> 00:22:42,840
advantage of that, right?

332
00:22:42,840 --> 00:22:48,800
So we do sort of really use the benefit of the not having to clean up the signal too much,

333
00:22:48,800 --> 00:22:50,800
except we do do a little cleaning.

334
00:22:50,800 --> 00:22:51,800
Got it, got it.

335
00:22:51,800 --> 00:22:55,040
Yeah, that was the intuition that I was trying to explore.

336
00:22:55,040 --> 00:22:56,040
Yeah.

337
00:22:56,040 --> 00:22:57,040
Awesome.

338
00:22:57,040 --> 00:23:02,480
So for your latest paper, which you presented at NURP set the the black and AI workshop,

339
00:23:02,480 --> 00:23:10,280
which is all about an optimized recurrent unit for ultra low power acoustic event detection.

340
00:23:10,280 --> 00:23:15,520
Tell us about that paper and the key challenges you're trying to solve there.

341
00:23:15,520 --> 00:23:16,520
Yeah.

342
00:23:16,520 --> 00:23:17,520
So that's great.

343
00:23:17,520 --> 00:23:20,600
And I think it follows very well, we kind of like what we've been discussing up on

344
00:23:20,600 --> 00:23:21,600
to now.

345
00:23:21,600 --> 00:23:25,600
So like I mentioned, I started with convolutional neural nets and the comments are great,

346
00:23:25,600 --> 00:23:26,600
right?

347
00:23:26,600 --> 00:23:27,600
We all love them.

348
00:23:27,600 --> 00:23:28,600
They do awesome.

349
00:23:28,600 --> 00:23:33,760
And even these days there is an argument that hey, recurrent neural nets are no longer

350
00:23:33,760 --> 00:23:39,040
necessary, because if you see some of Google's work with on like wave net, you see these

351
00:23:39,040 --> 00:23:44,480
direct solutions are doing great for speech recognition and like speech synthesis.

352
00:23:44,480 --> 00:23:48,640
So why do we care about recurrent net at all?

353
00:23:48,640 --> 00:23:53,040
And the motivation for that was first recurrent net when you're thinking of like really

354
00:23:53,040 --> 00:23:59,880
constrained sort of applications where you don't have a lot of time for you can't admit

355
00:23:59,880 --> 00:24:01,280
a lot of late things, right?

356
00:24:01,280 --> 00:24:06,680
In combination, if you think about it, you literally have to buffer up the entire event

357
00:24:06,680 --> 00:24:11,200
so you can apply the confidence, and you have to apply windows, but recurrent net have

358
00:24:11,200 --> 00:24:17,120
this natural nature where they can process data one time step at a time, right?

359
00:24:17,120 --> 00:24:18,120
So that's really great.

360
00:24:18,120 --> 00:24:21,080
It's great for application because it's just like a digital filter.

361
00:24:21,080 --> 00:24:25,560
You can implement it in such a way that a sample comes in, you process it, the next sample

362
00:24:25,560 --> 00:24:30,160
comes in, you process it, and you have really, really low latency, right?

363
00:24:30,160 --> 00:24:33,240
So that was one of the big promises for starting this, right?

364
00:24:33,240 --> 00:24:36,360
But then once we started, okay, we have this recurrent net.

365
00:24:36,360 --> 00:24:39,360
We can train them and they can do very well on the task.

366
00:24:39,360 --> 00:24:41,320
How do we put them on the device?

367
00:24:41,320 --> 00:24:48,920
Then it's like, wow, we open a whole box of challenges because it's interesting.

368
00:24:48,920 --> 00:24:54,040
And there are a lot of people working on deploying deep learning to the edge.

369
00:24:54,040 --> 00:24:58,920
And you know, you have, you know, really great hardware systems, like the Nvidia drive.

370
00:24:58,920 --> 00:25:06,120
You have like Jetson, which is like this small embedded system board by Nvidia that you can

371
00:25:06,120 --> 00:25:09,440
true, you know, neural nets on there and they run just fine, right?

372
00:25:09,440 --> 00:25:14,840
But when you look at all of these hardware, you are just above our power budget, right?

373
00:25:14,840 --> 00:25:19,880
We are running at either tens of thousands of milliwatts or thousands, and we are literally

374
00:25:19,880 --> 00:25:25,440
looking for something that's within, you know, tens of milliwatts, right?

375
00:25:25,440 --> 00:25:31,560
Something that looked very promising was this library, it's called, like, new tensor or

376
00:25:31,560 --> 00:25:33,360
micro tensor.

377
00:25:33,360 --> 00:25:38,760
It's supposed to be a really lightweight tensor tool like we that can run on certain embedded

378
00:25:38,760 --> 00:25:39,760
boards.

379
00:25:39,760 --> 00:25:44,800
But then when you look at that, it's still above the power range of these really, really

380
00:25:44,800 --> 00:25:48,240
ultra-low power, like, you know, microcontrollers, right?

381
00:25:48,240 --> 00:25:54,480
And so the challenges, can we take the classical models and put them on these microcontrollers?

382
00:25:54,480 --> 00:25:58,320
And the first thing realized that no, if you look at the specifications on the, you

383
00:25:58,320 --> 00:26:04,520
are thinking of, you know, memory of, you're thinking of memory of like less than 32 kilobytes,

384
00:26:04,520 --> 00:26:08,560
you don't have floating point operations, that's like a no, no, right?

385
00:26:08,560 --> 00:26:13,920
So the real challenges was, well, how do we look at the, you know, some of these recurring

386
00:26:13,920 --> 00:26:14,920
neural networks?

387
00:26:14,920 --> 00:26:20,360
I was looking at particularly the gated recurring unit, now can I optimize it all the way down

388
00:26:20,360 --> 00:26:23,520
so that it can run on my target device, right?

389
00:26:23,520 --> 00:26:25,280
So that was the motivation, right?

390
00:26:25,280 --> 00:26:29,880
But when you look at the current systems, they are limited precisely in terms of power,

391
00:26:29,880 --> 00:26:30,880
right?

392
00:26:30,880 --> 00:26:35,120
And if we can come up with a way of optimizing them, such that you can run on this unit,

393
00:26:35,120 --> 00:26:36,920
then that would be great, right?

394
00:26:36,920 --> 00:26:42,920
So once that problem was formulated, I run into the, you know, the main areas in which

395
00:26:42,920 --> 00:26:45,040
I can start optimizing them, yeah.

396
00:26:45,040 --> 00:26:46,040
Okay.

397
00:26:46,040 --> 00:26:53,120
And just to get a sense of the scope of this, you said you're trying to optimize this gated

398
00:26:53,120 --> 00:26:58,920
recurrent unit all the way down, does that mean you're like writing it in assembly and,

399
00:26:58,920 --> 00:27:03,680
you know, and writing it to, you know, to run on this physical device at the instruction

400
00:27:03,680 --> 00:27:04,680
level?

401
00:27:04,680 --> 00:27:05,920
Like how far down are we going here?

402
00:27:05,920 --> 00:27:06,920
Oh, okay.

403
00:27:06,920 --> 00:27:10,320
So I'm writing it to run just at C level, right?

404
00:27:10,320 --> 00:27:11,320
Okay.

405
00:27:11,320 --> 00:27:18,320
So it's just, yeah, see, not assembly, I think with me, but just at C, right?

406
00:27:18,320 --> 00:27:24,840
But that means, you cannot just, you know, grab your TensorFlow model and just run it on

407
00:27:24,840 --> 00:27:25,840
TensorFlow Lite, right?

408
00:27:25,840 --> 00:27:30,760
That means you're going to have to write everything from scratch in a very optimized way.

409
00:27:30,760 --> 00:27:37,600
And so this, how did you approach this process of writing the, of kind of writing it to run

410
00:27:37,600 --> 00:27:38,600
on the device?

411
00:27:38,600 --> 00:27:44,440
Was it, and we, you talked a little bit about challenges, but was it was, was it just

412
00:27:44,440 --> 00:27:50,440
kind of work or were there's particular problems that you ran into that you had to overcome?

413
00:27:50,440 --> 00:27:51,440
Yeah.

414
00:27:51,440 --> 00:27:57,160
So there were, there were actually like three main areas I identified that, you know,

415
00:27:57,160 --> 00:28:01,760
of like optimization that could really make this feasible.

416
00:28:01,760 --> 00:28:07,760
So just sort of stat, and a lot of these had been, you know, already resets and explored,

417
00:28:07,760 --> 00:28:08,760
right?

418
00:28:08,760 --> 00:28:10,280
In isolation, but they had been done.

419
00:28:10,280 --> 00:28:12,600
That's one of the great things about the deep learning community.

420
00:28:12,600 --> 00:28:16,400
There are so many researchers working on incredible things.

421
00:28:16,400 --> 00:28:21,200
And so just taking the time to look at all the work you would see that a lot of people

422
00:28:21,200 --> 00:28:25,560
have worked on, how do you really quantize which, right?

423
00:28:25,560 --> 00:28:27,600
So there was a lot of work there.

424
00:28:27,600 --> 00:28:34,800
And how do you make which run on, run, how do you see, maybe build new networks without

425
00:28:34,800 --> 00:28:37,040
even using a lot of multiplication?

426
00:28:37,040 --> 00:28:39,960
So there were a lot of these, you know, research out there.

427
00:28:39,960 --> 00:28:45,360
And when I really sat down and started like, you know, thinking of implementing, I did

428
00:28:45,360 --> 00:28:51,440
this in steps by the four main areas that I did was, the first was sort of in the gated

429
00:28:51,440 --> 00:28:53,120
recurring unit.

430
00:28:53,120 --> 00:28:58,720
So if you think of the gated recurring units, actually an optimized form of the LSTM unit,

431
00:28:58,720 --> 00:28:59,720
right?

432
00:28:59,720 --> 00:29:02,160
Where, you know, in LSTM, you have a lot of gates.

433
00:29:02,160 --> 00:29:05,280
In the gated recurring unit, you just have two gates.

434
00:29:05,280 --> 00:29:08,320
You have your reset gate and your data gate.

435
00:29:08,320 --> 00:29:13,880
So I was like, wait, if we can optimize LSTM to goo, then we can optimize goo even down,

436
00:29:13,880 --> 00:29:14,880
right?

437
00:29:14,880 --> 00:29:17,160
What happens if we just use one gate, right?

438
00:29:17,160 --> 00:29:21,200
And there's a lot of, there were some, there are some, a few weights out there actually

439
00:29:21,200 --> 00:29:29,240
from 2017, 2017, 2018, there were a few researchers who had just woken on this idea of, hey, what

440
00:29:29,240 --> 00:29:33,960
if we true out one of the gates, particularly the reset gate?

441
00:29:33,960 --> 00:29:40,200
So two main papers stood out, there was, there's one paper from Banger's Group, which said

442
00:29:40,200 --> 00:29:45,680
in speech recognition applications, you can actually omit the reset gate.

443
00:29:45,680 --> 00:29:52,880
And because speech doesn't change so much, it doesn't change too quickly, like you don't

444
00:29:52,880 --> 00:29:59,440
have these sudden discontinuities, it tends to work just as okay as having a group.

445
00:29:59,440 --> 00:30:04,880
And so I took that and sort of tried to apply that to my case of like, okay, maybe I can

446
00:30:04,880 --> 00:30:08,640
get rid of the reset gate.

447
00:30:08,640 --> 00:30:12,760
And when I, in my first simulations, when I did it, I realized that for my application,

448
00:30:12,760 --> 00:30:14,800
it actually tends to work well.

449
00:30:14,800 --> 00:30:19,760
And yes, conditection is different from speech because you do have, you know, sudden changes

450
00:30:19,760 --> 00:30:26,360
when the event happens, but then I validated that you can actually omit the reset gate

451
00:30:26,360 --> 00:30:29,200
and get it to work as much, right?

452
00:30:29,200 --> 00:30:34,760
The next, once I was able to do that, so you cut down one gate and get that expensive

453
00:30:34,760 --> 00:30:41,080
by the way, because it gets means you need weights, you need a lot of weights you need,

454
00:30:41,080 --> 00:30:45,120
you need a lot of computations to do the dot product, like do the weight and the activation

455
00:30:45,120 --> 00:30:46,120
function.

456
00:30:46,120 --> 00:30:51,760
So just by removing one gate, it's like you cut down the network by 33% in terms of memory

457
00:30:51,760 --> 00:30:53,960
and, you know, computation.

458
00:30:53,960 --> 00:30:59,120
Like, so once I got rid of the gates, the next, the main area that everyone thinks of

459
00:30:59,120 --> 00:31:04,640
when you're thinking of, you know, making optimizing neural net is quantization, right?

460
00:31:04,640 --> 00:31:06,240
So I have to do quantization.

461
00:31:06,240 --> 00:31:09,880
But what was popular out there was 8-bit quantization.

462
00:31:09,880 --> 00:31:16,720
I think even today you can use TensorFlow or even PyTorch and you can get 8-bit weights equivalent

463
00:31:16,720 --> 00:31:18,920
of whatever network you train, right?

464
00:31:18,920 --> 00:31:22,920
But I knew 8-bit wasn't going to cut it for my publication, so we have to go all the

465
00:31:22,920 --> 00:31:25,360
way down to just three bits, right?

466
00:31:25,360 --> 00:31:27,320
Three-bit quantization for the network.

467
00:31:27,320 --> 00:31:34,840
So I also did a lot of experiments and simulations to ensure that with three weights, I can still

468
00:31:34,840 --> 00:31:39,840
sort of train the network and have it run, well, and the trick there was to do what people

469
00:31:39,840 --> 00:31:46,520
have done in the past, which is during training, you apply the quantization as kind of like

470
00:31:46,520 --> 00:31:52,280
functions within your computational graph so that you are simulating the quantization process

471
00:31:52,280 --> 00:31:53,280
while you are training.

472
00:31:53,280 --> 00:32:00,520
And the next trick is learning about it and shifting the weights to the right place.

473
00:32:00,520 --> 00:32:04,320
Something else too with the quantization was rather than just quantizing to three-bit

474
00:32:04,320 --> 00:32:11,320
weights, you could quantize them such that they are just, you know, integer exponents of

475
00:32:11,320 --> 00:32:14,760
two or like, you know, the inverse of integer exponents of two.

476
00:32:14,760 --> 00:32:18,640
And if you did it that way, then you don't need multiplications anymore because you can

477
00:32:18,640 --> 00:32:24,320
replace all multiplications with, you know, with base shifts, right?

478
00:32:24,320 --> 00:32:29,640
So to recap the two main areas, one was true, I will show a way one of the gates and then

479
00:32:29,640 --> 00:32:36,080
you know, cut down the network computation and memory by 33% do this kind of exponential

480
00:32:36,080 --> 00:32:41,600
with quantization all the way down to three-bit and that saves you a lot of memory, but also

481
00:32:41,600 --> 00:32:46,280
enables you to just do base shifts and no multiplications, which you can do on the

482
00:32:46,280 --> 00:32:49,160
ultra-loop power microcontroller, right?

483
00:32:49,160 --> 00:32:56,200
And then the other main area of optimization was we don't have, we don't have two team

484
00:32:56,200 --> 00:32:58,880
points units on the microcontroller.

485
00:32:58,880 --> 00:33:04,800
So to get rid of every floating point of operation and replace that with integer operations, right?

486
00:33:04,800 --> 00:33:09,440
So the whole network on the microcontroller has to be implemented in such a way that you

487
00:33:09,440 --> 00:33:11,640
are just using integer operations, right?

488
00:33:11,640 --> 00:33:17,840
So all the activation functions, all the, you know, the linear transformations, the

489
00:33:17,840 --> 00:33:21,640
weak multiplication and the dot product, they all have to be done in such a way that

490
00:33:21,640 --> 00:33:27,400
you're operating within integer, you know, framework, I was using like 16-bit integers and

491
00:33:27,400 --> 00:33:31,960
then you are applying all the necessary collecting so you don't overflow, right?

492
00:33:31,960 --> 00:33:38,800
So those were the main sort of key areas of quantization, the integer arithmetic throughout,

493
00:33:38,800 --> 00:33:44,800
train out the single gate, like, train out the reset gate, and then also using fast activation

494
00:33:44,800 --> 00:33:47,240
because those are ourselves, right?

495
00:33:47,240 --> 00:33:49,080
Using what was that last one?

496
00:33:49,080 --> 00:33:50,080
Fast activations?

497
00:33:50,080 --> 00:33:55,560
Yeah, I have to, like, you know, so in the traditional, you know, recurring unit or

498
00:33:55,560 --> 00:34:01,560
updated recurring unit, you have the sigmoid and a time each and those are expensive on

499
00:34:01,560 --> 00:34:02,560
the microcontroller.

500
00:34:02,560 --> 00:34:06,600
Whenever you have to do an exponential on the microcontroller, if you are thinking all

501
00:34:06,600 --> 00:34:09,280
child do power, they become very painful.

502
00:34:09,280 --> 00:34:15,240
And so what I did was to use soft sign function instead, you can have like, soft sign variance

503
00:34:15,240 --> 00:34:19,880
of the time each and a soft sign and it works just as well, right?

504
00:34:19,880 --> 00:34:24,560
So these are the four main areas I had to, I had to bring together in order to really

505
00:34:24,560 --> 00:34:26,040
pull this off.

506
00:34:26,040 --> 00:34:31,160
One of the things that occurs to me here is a lot of times when we're trying to get networks

507
00:34:31,160 --> 00:34:36,120
to converge, we're doing things like drop out and other things that are like adding noise

508
00:34:36,120 --> 00:34:41,960
which kind of helps with regularization, but your process is inherently adding noise.

509
00:34:41,960 --> 00:34:49,400
So does that change the way you do regularization or eliminate the need to do regularization

510
00:34:49,400 --> 00:34:51,040
when you're trying to train the network?

511
00:34:51,040 --> 00:34:57,480
Yeah, so I don't do any additional regularization apart from these methods that I'm talking

512
00:34:57,480 --> 00:34:58,480
about.

513
00:34:58,480 --> 00:35:02,600
And almost every one of them you can actually think of as a regularization and you see

514
00:35:02,600 --> 00:35:03,600
that effect.

515
00:35:03,600 --> 00:35:10,640
So if I train on a really big network, it actually trains, you are able to convey to

516
00:35:10,640 --> 00:35:17,200
about the same or even it's slightly better than what you would with like a full precision

517
00:35:17,200 --> 00:35:22,600
network, right, but then when you go like really when you are training a very, very small

518
00:35:22,600 --> 00:35:29,080
network, the full precision will, you know, do better, but yes, you do see the regularization

519
00:35:29,080 --> 00:35:34,840
effect of these and you don't need to do any additional external regularization.

520
00:35:34,840 --> 00:35:40,240
I think one thing also that I have to point out is you see the whole simulation, the quantization

521
00:35:40,240 --> 00:35:46,480
but also the fixed point in the gyrismatic, it pretty much means you are sort of clipping

522
00:35:46,480 --> 00:35:50,520
how high the outputs of certain nodes can go, right?

523
00:35:50,520 --> 00:35:55,920
So nothing can go above the highest number in say 16 bit into the presentation.

524
00:35:55,920 --> 00:35:58,240
So it's almost like you're doing with clipping, right?

525
00:35:58,240 --> 00:36:02,320
You're doing all of these things that you traditionally do with regularization.

526
00:36:02,320 --> 00:36:05,120
In fact, really strict regularization.

527
00:36:05,120 --> 00:36:10,480
So it ends up training very well and then you're able to get a network that function.

528
00:36:10,480 --> 00:36:18,360
So you made a comment about how for large networks, it works as well or better than,

529
00:36:18,360 --> 00:36:23,960
than non, you know, networks where you're not doing these four things.

530
00:36:23,960 --> 00:36:29,000
How broadly does that generalize, is that, you know, just for this specific problem?

531
00:36:29,000 --> 00:36:35,000
Or, I mean, we always care about, you know, compute and usually care about power.

532
00:36:35,000 --> 00:36:40,880
Like could you just take this and apply it to something totally, you know, a totally

533
00:36:40,880 --> 00:36:41,880
different kind of problem?

534
00:36:41,880 --> 00:36:45,680
And do you think it would work or there are specific things about your problem that

535
00:36:45,680 --> 00:36:48,800
allows you to make these compromises?

536
00:36:48,800 --> 00:36:55,720
So actually in my paper, in the work that I presented in the paper, I actually wrote

537
00:36:55,720 --> 00:37:02,320
a paper on this that would be coming out for you become, like maybe later this year.

538
00:37:02,320 --> 00:37:06,520
What we realized was we tried on two additional like tasks, right?

539
00:37:06,520 --> 00:37:10,640
One was recognizing spoken digits, right?

540
00:37:10,640 --> 00:37:13,000
And you see that it still waits, right?

541
00:37:13,000 --> 00:37:18,080
In like, when you're trying to Google has this speech command dataset, it pretty much has

542
00:37:18,080 --> 00:37:25,480
like one second audio recording of sounds like on or, and you have 20,000 flat examples,

543
00:37:25,480 --> 00:37:26,480
right?

544
00:37:26,480 --> 00:37:31,320
And the way it's just as well, where you begin to see it hurting is when you look at the

545
00:37:31,320 --> 00:37:36,560
test as we consider this was called like a band sounds where you have really, really long

546
00:37:36,560 --> 00:37:37,560
recordings.

547
00:37:37,560 --> 00:37:41,720
You have recordings that are about four seconds long, right?

548
00:37:41,720 --> 00:37:47,560
And sequences that are about 100 plus, you know, instances that you begin to see it hurting.

549
00:37:47,560 --> 00:37:51,320
I think one of the reason why it begins hurting is because when you don't have the reset

550
00:37:51,320 --> 00:37:57,560
gate, you are pretty much now like you're limiting the long-term memory of the network, right?

551
00:37:57,560 --> 00:38:02,960
Like the very advantage of the group and then LSTM, like your handicrafting, the network

552
00:38:02,960 --> 00:38:07,480
is that because it can't really remember for too long, right?

553
00:38:07,480 --> 00:38:13,120
So I think these optimizations make it very suitable for applications like, you know,

554
00:38:13,120 --> 00:38:18,480
keywords, potten, or where you are, literally detection sounds that are very short, you

555
00:38:18,480 --> 00:38:21,400
know, or not very short, but relatively short.

556
00:38:21,400 --> 00:38:27,240
So, you know, anything one second, two second and below, then it becomes really promising.

557
00:38:27,240 --> 00:38:31,400
And then once you are looking at really long sequences, like you would in speech recognition,

558
00:38:31,400 --> 00:38:33,560
then it would not be ideal, right?

559
00:38:33,560 --> 00:38:38,640
And so you mentioned two additional tasks was one the digits and the other was the speech

560
00:38:38,640 --> 00:38:40,440
commands or were those one?

561
00:38:40,440 --> 00:38:45,080
Yeah, those were one the speech commands has the spoken digits.

562
00:38:45,080 --> 00:38:47,440
The test was this a band sounds.

563
00:38:47,440 --> 00:38:52,240
If the data says it's available online and they have, it's actually a really difficult

564
00:38:52,240 --> 00:38:53,240
task.

565
00:38:53,240 --> 00:38:59,240
You have like recordings of environments, you have things like child playing, car honking,

566
00:38:59,240 --> 00:39:02,280
and you know, you're supposed to identify these classes.

567
00:39:02,280 --> 00:39:07,880
You have about 20 classes or 10 classes, 10 to 20 classes, one of those.

568
00:39:07,880 --> 00:39:11,960
And sometimes it's really even hard for a person to discriminate between them because

569
00:39:11,960 --> 00:39:15,960
you know, the sounds are long and then the context is also very broad, right?

570
00:39:15,960 --> 00:39:21,200
So those, that was a very difficult task for our model.

571
00:39:21,200 --> 00:39:28,080
So we're performing maybe 10 to 15% ways than using a full precision, like, you know,

572
00:39:28,080 --> 00:39:29,080
green network.

573
00:39:29,080 --> 00:39:30,080
Okay.

574
00:39:30,080 --> 00:39:33,440
What was that data set called again?

575
00:39:33,440 --> 00:39:35,560
Abans sounds data set.

576
00:39:35,560 --> 00:39:41,520
So I think if I remember correctly, it's from, yeah, they have two.

577
00:39:41,520 --> 00:39:49,400
There's the Abans sound 8K and then like a more Abans sound like with even longer recordings

578
00:39:49,400 --> 00:39:52,200
and we have a paper out there on it.

579
00:39:52,200 --> 00:40:00,000
I think it's a data set in taxonomy for Abans sound research was published in an ACM conference

580
00:40:00,000 --> 00:40:01,360
on multimedia.

581
00:40:01,360 --> 00:40:04,800
So it's a data set that has been out there since 2015.

582
00:40:04,800 --> 00:40:05,800
I'm sorry.

583
00:40:05,800 --> 00:40:06,800
Yeah.

584
00:40:06,800 --> 00:40:07,800
All right.

585
00:40:07,800 --> 00:40:12,280
So you've developed this GRU kind of all the way down.

586
00:40:12,280 --> 00:40:18,840
You have demonstrated that it works for your task and reasonably well for other tasks.

587
00:40:18,840 --> 00:40:22,200
Are you done then or is there more work that you had to do here?

588
00:40:22,200 --> 00:40:23,200
Yeah.

589
00:40:23,200 --> 00:40:30,120
So I think right now for, and I actually had deployed it to the device and have it like,

590
00:40:30,120 --> 00:40:35,280
you know, run on examples in the data set and get like the decent results.

591
00:40:35,280 --> 00:40:38,040
I think one key area to was late, right?

592
00:40:38,040 --> 00:40:41,440
You need this to not just fit, but be fast enough.

593
00:40:41,440 --> 00:40:47,440
And that also we were meeting the right spec actually where I'm going about 60 to 70 times

594
00:40:47,440 --> 00:40:53,480
faster than if you were to just manually put a GRU onto any of these devices.

595
00:40:53,480 --> 00:40:57,520
And so while they all work and everything is like great, you've been able to embed

596
00:40:57,520 --> 00:40:58,520
the model.

597
00:40:58,520 --> 00:41:02,880
The entire pipeline that I talked about, right, also needs to be in place, right?

598
00:41:02,880 --> 00:41:09,200
So right now, it means, you know, our our GRU, our optimized recovery unit can, when

599
00:41:09,200 --> 00:41:11,840
you train it, they can't really detect these sounds.

600
00:41:11,840 --> 00:41:16,480
But then once you put it, once you start thinking, you know, okay, I have a whole streaming

601
00:41:16,480 --> 00:41:17,840
input, right?

602
00:41:17,840 --> 00:41:23,880
How do I ensure that I'm telling off my microprocessor, like at the right time, I'm waking up quickly

603
00:41:23,880 --> 00:41:26,240
enough so that I don't miss the events.

604
00:41:26,240 --> 00:41:32,520
And then my, you know, recurring neural network is also like, it's also processing the incoming

605
00:41:32,520 --> 00:41:33,520
sequence.

606
00:41:33,520 --> 00:41:38,080
And and finally also, how do I, so you get some outputs from your recovery net, right?

607
00:41:38,080 --> 00:41:43,920
But how do you smoothen over those posterior probabilities and actually record the event

608
00:41:43,920 --> 00:41:44,920
at what time?

609
00:41:44,920 --> 00:41:49,120
So those are that sort of additional work, that's what I've been working on, sort of completing

610
00:41:49,120 --> 00:41:51,000
that whole pipeline, right?

611
00:41:51,000 --> 00:41:56,720
Because I don't see the, the RNA is one block in that whole pipeline and you want to get

612
00:41:56,720 --> 00:42:00,520
the whole pipeline working, so this can be used in real time, right?

613
00:42:00,520 --> 00:42:04,400
So that's kind of like the work that I'm finishing on right now.

614
00:42:04,400 --> 00:42:05,400
Nice.

615
00:42:05,400 --> 00:42:10,480
And you're finishing up your PhD, what's next for for you in this project?

616
00:42:10,480 --> 00:42:11,480
Yeah.

617
00:42:11,480 --> 00:42:16,080
I'm very excited, I'm hopefully going to be defending in a month.

618
00:42:16,080 --> 00:42:17,080
Oh wow.

619
00:42:17,080 --> 00:42:20,800
And yeah, so very exciting things.

620
00:42:20,800 --> 00:42:27,400
But actually, my advice and I have found that a company called Clewis to commercialize

621
00:42:27,400 --> 00:42:32,560
these, this Asma monitoring device that I've been talking about, this world would device

622
00:42:32,560 --> 00:42:34,120
for monitoring the lungs, right?

623
00:42:34,120 --> 00:42:37,800
And the company is called Clewis and we are very excited about it.

624
00:42:37,800 --> 00:42:43,160
We just, I was just awarded an SBIR grant to sort of see that too.

625
00:42:43,160 --> 00:42:47,480
So I think for the next year, that's what I'll be, that's what I'll be fully dedicated

626
00:42:47,480 --> 00:42:54,080
to trying to bring the technology out of the lab and into the hospitals to actually use

627
00:42:54,080 --> 00:42:55,720
in patients on patients.

628
00:42:55,720 --> 00:42:56,720
Nice, nice.

629
00:42:56,720 --> 00:43:03,440
And so I imagine that this will have to go through the, kind of the FDA approvals, is this

630
00:43:03,440 --> 00:43:07,200
subject to like clinical trials and all of those kinds of things?

631
00:43:07,200 --> 00:43:08,200
Yes.

632
00:43:08,200 --> 00:43:11,560
So, eventually, it's going to go to FDA.

633
00:43:11,560 --> 00:43:17,080
We've actually been sort of prepping on the right materials and sort of lining up the

634
00:43:17,080 --> 00:43:18,520
right studies we have to do.

635
00:43:18,520 --> 00:43:23,120
I have done some preliminary studies at the hospital on patients' population of even

636
00:43:23,120 --> 00:43:29,800
up to 30, actually testing on patients, but we have to do like a really full-blown study.

637
00:43:29,800 --> 00:43:36,040
But for the next year, we are hoping to work with some medical companies who are already,

638
00:43:36,040 --> 00:43:42,760
you know, doing clinical trials for their respiratory therapeutics, and they need data, they need

639
00:43:42,760 --> 00:43:47,640
data on, you know, have these patients responding to particular drugs, right?

640
00:43:47,640 --> 00:43:53,000
And often, the available data is just subjective reports from patients, oh, I think these

641
00:43:53,000 --> 00:43:57,200
recalls COVID-19, but if they could have something that's more objective, huh?

642
00:43:57,200 --> 00:44:02,720
They were within X, RS throughout this week, and that reduced by 10% the next week, or

643
00:44:02,720 --> 00:44:06,600
their cough went down by this, they'll be really, really, you know, actionable for them

644
00:44:06,600 --> 00:44:08,080
and really valuable to them.

645
00:44:08,080 --> 00:44:14,680
So we are talking with some pharmaceuticals about, you know, the need and positioning the

646
00:44:14,680 --> 00:44:17,200
device in such a way that it can fulfill that need.

647
00:44:17,200 --> 00:44:18,200
Awesome.

648
00:44:18,200 --> 00:44:23,880
And will you also be getting into physical manufacturer of the devices themselves?

649
00:44:23,880 --> 00:44:24,880
Yes.

650
00:44:24,880 --> 00:44:30,840
So, for some of the studies that I did, I had to do like batch sort of manufacturing of

651
00:44:30,840 --> 00:44:36,000
these processes of these devices, because, you know, you're running a small study where

652
00:44:36,000 --> 00:44:41,440
you have maybe 50 subjects, you still have to have a lot of, you know, devices, and you

653
00:44:41,440 --> 00:44:45,400
have to have like packaging down how they're going to arrive at the hospital, how do you

654
00:44:45,400 --> 00:44:49,760
get the information in and out, and so I have had to do that, and I anticipate I'm going

655
00:44:49,760 --> 00:44:55,440
to have to do much more of that, like in this year, once I finished school.

656
00:44:55,440 --> 00:44:56,440
Wow.

657
00:44:56,440 --> 00:44:57,440
So, yeah.

658
00:44:57,440 --> 00:44:58,440
It's exciting.

659
00:44:58,440 --> 00:45:02,880
It sounds like a really exciting project, and you will certainly have your hands full

660
00:45:02,880 --> 00:45:05,200
over the coming months.

661
00:45:05,200 --> 00:45:08,560
I really appreciate you taking the time to share it with us.

662
00:45:08,560 --> 00:45:09,560
Oh, thank you.

663
00:45:09,560 --> 00:45:10,560
Well, this was fun.

664
00:45:10,560 --> 00:45:11,560
It's taking a while.

665
00:45:11,560 --> 00:45:12,560
Fantastic.

666
00:45:12,560 --> 00:45:13,560
Thank you.

667
00:45:13,560 --> 00:45:18,680
All right, everyone, that's our show for today.

668
00:45:18,680 --> 00:45:24,240
For more information on justice or any of the topics covered in this show, visit twimlai.com

669
00:45:24,240 --> 00:45:27,280
slash talks slash 2 3 0.

670
00:45:27,280 --> 00:45:34,240
For more information on our Black and AI series, visit twimlai.com slash Black and AI 19.

671
00:45:34,240 --> 00:46:01,000
As always, thanks so much for listening, and catch you next time.

