1
00:00:00,000 --> 00:00:05,860
Hey everybody, Sam here. We've got some great news to share, and also a favorite

2
00:00:05,860 --> 00:00:12,120
to ask. We're in the running for this year's People's Choice Podcast Awards, in both the

3
00:00:12,120 --> 00:00:17,680
People's Choice and Technology categories, and we would really appreciate your support.

4
00:00:17,680 --> 00:00:23,760
To nominate us, you'll just head over to twimlai.com slash nominate, where we've linked to and

5
00:00:23,760 --> 00:00:29,440
embedded the nomination form from the award site. There, you'll need to input your information

6
00:00:29,440 --> 00:00:35,200
and create a listener nomination account. Once you get to the ballot, just find and select

7
00:00:35,200 --> 00:00:41,280
this week in machine learning and AI on the nomination list for both the Adam Curry People's

8
00:00:41,280 --> 00:00:49,120
Choice Award and the this week in tech technology category. As you know, we really, really appreciate

9
00:00:49,120 --> 00:00:55,200
each listener and would love to share in this accomplishment with you. Remember that url is

10
00:00:55,200 --> 00:01:02,240
twimlai.com slash nominate. Feel free to hit pause and take a moment to nominate us now.

11
00:01:14,160 --> 00:01:20,240
Hello and welcome to another episode of Twimletalk, the podcast why interview interesting people,

12
00:01:20,240 --> 00:01:26,160
doing interesting things in machine learning and artificial intelligence. I'm your host Sam

13
00:01:26,160 --> 00:01:39,440
Charrington. If you follow us on Twitter, you might have seen this announcement, but as of last week,

14
00:01:39,440 --> 00:01:47,360
the pod is available on both the new Google Podcast app and wait for it, Spotify. If you've

15
00:01:47,360 --> 00:01:52,160
been waiting on that update, make sure you give us a quick search and subscribe on either or both

16
00:01:52,160 --> 00:01:58,640
of these platforms. Spotify in particular was a long time in the works and ultimately required

17
00:01:58,640 --> 00:02:04,240
a big shift in the way the podcast is published, so you may notice other changes as well. Let us know

18
00:02:04,240 --> 00:02:12,720
if something seems amiss. In this episode, I'm joined by Ajibuso Zhang, PhD student in the Department

19
00:02:12,720 --> 00:02:19,360
of Statistics at Columbia University. In this interview, Ajie and I discuss two of her recent papers,

20
00:02:19,360 --> 00:02:26,400
the first and accepted paper from this year's ICML conference titled Noison, Unbiased Regularization

21
00:02:26,400 --> 00:02:33,040
for Recurrent Neural Networks, which as the name implies, presents a new way to regularize RNNs

22
00:02:33,040 --> 00:02:40,960
using noise injection. The second paper, an ICLR submission from last year titled Topic RNN,

23
00:02:40,960 --> 00:02:46,880
a recurrent neural network with long-range semantic dependency. They've used an RNN-based

24
00:02:46,880 --> 00:02:52,160
language model designed to capture the global semantic meaning relating words and a document

25
00:02:52,160 --> 00:02:58,080
via latent topics. We dive into the details behind both of these papers and I learn a ton

26
00:02:58,080 --> 00:03:00,000
along the way. Enjoy.

27
00:03:00,000 --> 00:03:11,040
All right, everyone. I am on the line with Ajibuso Zhang. Ajie is a PhD student in the Department

28
00:03:11,040 --> 00:03:16,640
of Statistics at Columbia University. Ajie, welcome to this weekend machine learning in AI.

29
00:03:16,640 --> 00:03:23,680
Hi, thanks for having me. Absolutely. So Ajie and I are actually catching up for the second time.

30
00:03:23,680 --> 00:03:31,520
We did have a chance to chat back in December at NIPS, but wanted to get caught up on all of her

31
00:03:31,520 --> 00:03:38,080
latest work, including a paper that she's got accepted to the ICML conference. So Ajie, before

32
00:03:38,080 --> 00:03:43,840
we dive in all that, why don't you talk a little bit about your background and how you got involved

33
00:03:43,840 --> 00:03:48,720
in machine learning research from a statistical perspective?

34
00:03:48,720 --> 00:03:55,200
Yeah. So I started with machine learning pretty early. I did my undergrad in France at Telecom

35
00:03:55,200 --> 00:04:01,440
Paritek, where I specialize in statistical learning theory. They call it a theoretical

36
00:04:01,440 --> 00:04:10,240
advantage statistic. So my background is in a mix of math and CS. After my second year at Telecom,

37
00:04:10,240 --> 00:04:16,320
I had the opportunity to go to Cornell for an exchange program, which I did and worked more

38
00:04:16,320 --> 00:04:22,000
on the staff side there. And after Cornell, I decided that I wanted to find a meaningful

39
00:04:22,000 --> 00:04:28,640
application of statistics. So I went to the World Bank and worked on building models for assessing

40
00:04:28,640 --> 00:04:34,400
market and counterparty risk, with the vision that if I do my job well, then the World Bank is

41
00:04:34,400 --> 00:04:41,040
going to keep it stripper a rating and poor countries will be able to get loans at very low rate

42
00:04:41,040 --> 00:04:46,480
because they don't have access to the market. So I did that for a bit more than a year. And then

43
00:04:46,480 --> 00:04:54,800
I started my PhD at Columbia around 2014, August 2014 to be more precise. And yeah, I've been

44
00:04:54,800 --> 00:05:00,960
working on the 36th department ever since. Okay. Can you talk a little bit more about that work

45
00:05:00,960 --> 00:05:04,880
at the World Bank and the types of models that you were building there?

46
00:05:04,880 --> 00:05:12,560
Yes, there's different ways we were using stats to assess risk. There's models for value

47
00:05:12,560 --> 00:05:20,960
at risk. There's model for potential future exposure. All these models basically tell the World Bank

48
00:05:20,960 --> 00:05:28,640
what are the risk if they enter in a financial agreement with a counterparty. Those counterparty

49
00:05:28,640 --> 00:05:36,400
are, for example, the ones in the financial market. And if they are able to assess the risk in

50
00:05:36,400 --> 00:05:42,320
entering into that financial transaction, then they'll be able to not incur any defaults from

51
00:05:42,320 --> 00:05:47,200
their counterparties. And they will be able to keep their triple a rating, which they are being

52
00:05:47,200 --> 00:05:53,440
evaluated on. I believe every year by standard imports and other like fish and more,

53
00:05:53,440 --> 00:05:59,600
there are different shops that assess rating for these types of institutions. And keeping the

54
00:05:59,600 --> 00:06:05,680
triple a rating is key because then they can get these rates at, they can get these money from the

55
00:06:05,680 --> 00:06:12,480
market at very low rates and also lend money to poor countries at even low rates. So they are

56
00:06:12,480 --> 00:06:18,800
acting like an intermediary between poor countries and the financial market. Okay. So my work was to

57
00:06:18,800 --> 00:06:23,840
assess value at risk and potential future exposure for the World Bank.

58
00:06:24,800 --> 00:06:31,360
What were some of the data sources upon which you were building those types of models?

59
00:06:31,360 --> 00:06:40,160
Those were internal internal data sets. Okay. So yeah, there is a repository where they store every

60
00:06:40,880 --> 00:06:47,520
every transaction they have, either with the poor countries that they lend money to and also to

61
00:06:47,520 --> 00:06:54,320
the transactions they have with the financial market institutions. Okay. Yeah. Interesting. Interesting.

62
00:06:54,320 --> 00:07:06,160
And so then you went over into your PhD program and what's the defining thread that is guiding

63
00:07:06,160 --> 00:07:14,160
your research on the PhD? I like working with probabilistic graphical models and I like also working

64
00:07:14,160 --> 00:07:20,720
with neural networks because I think they are very flexible. And I like applications involving

65
00:07:20,720 --> 00:07:26,160
sequential data because I think there's a lot of challenges in modeling sequential data. You need to

66
00:07:26,160 --> 00:07:32,000
account for the fact that they can be high dimensional. You need to account for all the dependencies

67
00:07:32,000 --> 00:07:38,800
in the data. And I find that interesting from a scientific point of view. So that's mainly what's

68
00:07:38,800 --> 00:07:44,720
guiding my research right now, finding ways to improve learning with sequential data.

69
00:07:44,720 --> 00:07:51,280
And I've looked at things like context representation, regularization, and scalable learning with

70
00:07:51,280 --> 00:08:00,480
variational inference. Okay. And the paper that you have accepted at ICML is focused on one of those

71
00:08:00,480 --> 00:08:06,800
areas regularization in particular. Maybe we can start by having you talk a little bit about

72
00:08:06,800 --> 00:08:16,320
regularization generally and different techniques for doing it and kind of what kind of results you've

73
00:08:16,320 --> 00:08:23,520
seen applying traditional regularization to RNNs. And then we can talk a little bit about this

74
00:08:23,520 --> 00:08:30,800
new technique. Yeah. So, recreational networks are the main family of models for sequential data.

75
00:08:30,800 --> 00:08:36,400
And they tend to be very flexible. They tend to have high capacity because when you have, for

76
00:08:36,400 --> 00:08:44,080
example, texts, you have the recreational network that represent text as using a hidden state.

77
00:08:44,080 --> 00:08:50,960
And basically you project each word in your data into that low dimensional space. And that low

78
00:08:50,960 --> 00:08:56,240
dimensional phase, you also use it to do prediction for the next word that you're going to observe.

79
00:08:56,240 --> 00:09:02,320
And those parameters, those weight matrices that you use to do these projections used to be

80
00:09:02,320 --> 00:09:07,840
very high dimensional. And so you end up easily with models that are that have very high capacity

81
00:09:07,840 --> 00:09:15,200
and that tend to memorize data. And so it becomes important to find ways to regularize those models.

82
00:09:15,200 --> 00:09:22,160
And the paper that I'm going to present at ICML lies in the line of work that used noise injection

83
00:09:22,160 --> 00:09:28,480
as a way to regularize these models. Dropout is one one very famous noise injection

84
00:09:28,480 --> 00:09:34,800
regularization technique. And what we are proposing with noisy is an alternative to dropout that

85
00:09:34,800 --> 00:09:42,240
does the noise injection in a different way. So maybe talk through how dropout works as a

86
00:09:42,240 --> 00:09:48,560
baseline for starting to think about this problem. So in dropout for recreational networks,

87
00:09:48,560 --> 00:09:54,800
you would let's take the LSTN. You would multiply, let's say you have your current observation,

88
00:09:54,800 --> 00:10:01,120
your current word, XT. You would have to multiply that observation with some Bernoulli noise.

89
00:10:01,120 --> 00:10:08,000
And you would also have to multiply your hidden state with some Bernoulli noise. And you will

90
00:10:08,000 --> 00:10:13,760
condition on that noise observation and noise hidden state to compute your new hidden state

91
00:10:13,760 --> 00:10:19,040
before predicting the next word. And then practically what that means is that you're

92
00:10:19,040 --> 00:10:26,160
essentially forgetting or not using some of the weights or zeroing out some of the weights,

93
00:10:26,160 --> 00:10:31,360
is that right? Yes, it's equivalent to basically, like you say, zeroing out some of the weights.

94
00:10:31,360 --> 00:10:37,680
So you are reducing the capacity of the network in doing that, which is essentially what regularization

95
00:10:37,680 --> 00:10:45,040
is. And so what's different about the approach you're proposing in the noise in paper?

96
00:10:45,040 --> 00:10:52,480
In the noise in paper, the motivation was that we want to do regularization while still keeping

97
00:10:52,480 --> 00:10:58,880
the properties of the model we are regularizing intact. In the sense that we want to regularize

98
00:10:58,880 --> 00:11:07,440
an LSTM while not altering the LSTM's properties. And we define that in with the term unbiased

99
00:11:07,440 --> 00:11:14,320
noise injection. And so the way unbiasedness works is that if you take the conditional expectation

100
00:11:14,320 --> 00:11:21,200
of any hidden state in your sequence, in your sequential representation, then you recover

101
00:11:21,760 --> 00:11:26,640
the hidden state of the underlying R and then you are regularizing. And you don't get that

102
00:11:26,640 --> 00:11:34,640
with dropout. And so what way does dropout in what way is it biased or does it not preserve this

103
00:11:34,640 --> 00:11:43,040
unbiased property? It's because of the way you are injecting the noise. So in noise in we compute

104
00:11:43,040 --> 00:11:50,640
the hidden state before doing prediction as usual, but we multiply the finite hidden state with noise.

105
00:11:50,640 --> 00:11:56,800
In dropout, you would multiply the noise with the data and the previous hidden state and you

106
00:11:56,800 --> 00:12:02,480
condition on those to compute your new hidden state, which uses some nonlinearities. So if you do

107
00:12:02,480 --> 00:12:08,000
expectation of that hidden state, you wouldn't recover the previous underlying hidden state because

108
00:12:08,000 --> 00:12:14,320
of the nonlinearities. Whereas in noise in we compute everything, we condition on the previous

109
00:12:14,320 --> 00:12:19,520
observation and the previous hidden state and compute the new hidden state. And before we use

110
00:12:19,520 --> 00:12:25,680
that hidden state for prediction, we multiply it or add some noise to it such that when you take

111
00:12:25,680 --> 00:12:31,040
its expectation, you recover the underlying hidden state. And we found that to be very useful

112
00:12:31,040 --> 00:12:37,920
in regularizing these LSTMs. Is it fair to simplify this as saying that dropout adds a noise

113
00:12:37,920 --> 00:12:45,040
before calculating the hidden state and noise and adds a noise after calculating the hidden state

114
00:12:45,040 --> 00:12:52,080
and doing that, it doesn't bias the network? Yes, that's a good summary of what the difference is.

115
00:12:52,720 --> 00:13:00,560
You talk about in your paper some performance improvements. I'm curious in your use of the term,

116
00:13:00,560 --> 00:13:09,600
you know, biased. Have you identified like more qualitative differences between the way these two

117
00:13:09,600 --> 00:13:16,080
regularization methods perform beyond just the performance? Are there observations you can make

118
00:13:16,080 --> 00:13:21,840
about the way dropout affects the results of a network? Yes, we have not we have not looked at

119
00:13:21,840 --> 00:13:28,400
the qualitative differences. But one thing that was interesting in that paper was that given

120
00:13:28,400 --> 00:13:34,800
under the unbiased noise injection definition, we can consider actually an LSTM that's

121
00:13:34,800 --> 00:13:41,280
regularized with dropout as a new model, as defining a new model class for sequential data.

122
00:13:41,280 --> 00:13:47,760
And we apply the noise in regularization on top of that. And we also found improvements.

123
00:13:47,760 --> 00:13:54,000
We didn't, we didn't unfortunately do any qualitative comparison between dropout and noise

124
00:13:54,000 --> 00:13:58,480
and I think that would be an interesting thing that I should look at next. Could you elaborate on

125
00:13:58,480 --> 00:14:04,480
that last comment you made though? On dropout as defining a model class? It sounds like what you're

126
00:14:04,480 --> 00:14:12,800
saying is that and you hinted at this earlier that dropout and maybe this is comes directly from

127
00:14:12,800 --> 00:14:20,160
changing the expectation of this hidden state, but you can think of LSTM with dropout as almost

128
00:14:20,160 --> 00:14:27,520
its own class of model where as noise in applied to an LSTM, it sounds like you're saying preserve

129
00:14:27,520 --> 00:14:34,160
some fundamental LSTMness or some fundamental thing? Yes, in the sense that the expectation of

130
00:14:34,160 --> 00:14:38,720
your noisy hidden state is the same as the hidden state of the thing you're regularizing

131
00:14:38,720 --> 00:14:44,480
under strong and biasness. Are there other things that you learned about applying regularization

132
00:14:44,480 --> 00:14:52,160
to LSTMs and RNNs in the process of exploring this noise and approach? Yeah, interesting.

133
00:14:52,160 --> 00:14:58,240
Usually when you want to regularize a model, you will have to basically use less parameters.

134
00:14:59,360 --> 00:15:05,120
So one way of doing that in iron and has been to tie the weights of your inputs,

135
00:15:05,680 --> 00:15:10,320
the embedding matrix from the input and the embedding matrix from the output right before you do

136
00:15:10,320 --> 00:15:17,280
prediction. That's an effective way of doing it, but another way is in using noise,

137
00:15:17,280 --> 00:15:23,120
but using noise wouldn't reduce the number of parameters. It would reduce the capacity of your

138
00:15:23,120 --> 00:15:28,800
network, not by reducing the number of parameters, but by reducing the amount that it encodes in the

139
00:15:28,800 --> 00:15:36,240
data using noise. So you have this network that has a very high capacity while also being regularized

140
00:15:36,240 --> 00:15:42,400
because you are injecting noise in the procedures such that the network doesn't try to memorize the

141
00:15:42,400 --> 00:15:52,400
data. No, I'm following you. So one question that occurs to me then is are there implications

142
00:15:52,400 --> 00:16:02,320
on the computational complexity here in applying a noise-based approach as opposed to tying your

143
00:16:02,320 --> 00:16:09,760
inputs and outputs in terms of the training time specifically? I'm responding primarily to the

144
00:16:10,240 --> 00:16:15,920
you saying that you have access to more parameters when you're using a noise-based approach

145
00:16:15,920 --> 00:16:22,080
than with the other approach you described. Yes, what I mean is that you don't have to reduce your

146
00:16:22,080 --> 00:16:28,400
number of parameters. All you have to do is use noise such that your weights will memorize less

147
00:16:28,400 --> 00:16:35,440
of the data than usual. So in terms of training time of course if you have if I give you two

148
00:16:35,440 --> 00:16:42,240
equivalent networks where they both have the same number of parameters but where one uses noise

149
00:16:42,240 --> 00:16:47,840
injection and the other one uses weight time but they both have the same number of parameters then

150
00:16:47,840 --> 00:16:56,080
the training time will will be comparable. Okay. Yeah, the only slight difference will be in the time

151
00:16:56,080 --> 00:17:01,760
you take to multiply a noise with the hidden state. Right. Yeah. No, it's not particularly significant.

152
00:17:01,760 --> 00:17:09,600
Yeah, yeah, yeah, exactly. To test this you tested it against a couple of benchmark data sets. Can

153
00:17:09,600 --> 00:17:16,800
you describe your evaluation process? Yes, we wanted to because it's a regularization method and

154
00:17:16,800 --> 00:17:22,480
because we applied it to language we used two benchmark data sets. In the literature we used

155
00:17:22,480 --> 00:17:29,920
the pantry bank and the week it takes two from Salesforce and so we wanted to assess the method

156
00:17:29,920 --> 00:17:36,400
as being a regularization method. So we compared it to the deterministic unregularized LSTM

157
00:17:36,400 --> 00:17:42,400
and which we so obviously that adding noise helps in regularizing so you get a better performance

158
00:17:42,400 --> 00:17:50,800
as defined by a perplexity but we also compared it to an LSTM regularized with dropout to assess the

159
00:17:50,800 --> 00:17:57,280
importance of having this unbiasedness and we found also that it does better when you ensure that

160
00:17:57,280 --> 00:18:04,240
your regularization method is unbiased and we also built on this unbiasedness notion that

161
00:18:04,240 --> 00:18:09,600
since dropout is biased we can consider it as a new model class for sequences and we also

162
00:18:09,600 --> 00:18:16,240
compared to an LSTM regularized with dropout only as the new model and an LSTM regularized with

163
00:18:16,240 --> 00:18:21,600
dropout augmented with this noise in regularization and we also found that that helps too.

164
00:18:23,280 --> 00:18:34,480
And so just to back up on the testing with these two data sets you basically created an RNN

165
00:18:34,480 --> 00:18:39,840
or trained an RNN on these data sets using these various regularization methods and then

166
00:18:39,840 --> 00:18:45,920
were you giving it a word and asking it to predict the next word and using that to

167
00:18:46,640 --> 00:18:50,560
you know that was your basis for evaluation or is it something else?

168
00:18:50,560 --> 00:18:52,080
Yes, next word prediction.

169
00:18:52,080 --> 00:18:53,040
Next word prediction.

170
00:18:53,040 --> 00:18:59,280
Okay, and then you get the perplexity as a measure of how good your language model is doing.

171
00:18:59,280 --> 00:19:05,600
In the paper you talk about you mentioned earlier Bernoulli noise but you tested this with

172
00:19:05,600 --> 00:19:11,760
a bunch of different noise distributions. Can you talk a little bit about what you saw there?

173
00:19:11,760 --> 00:19:17,600
Yes, what we wanted to assess there was let's not only restrict ourselves to Bernoulli and see

174
00:19:18,240 --> 00:19:24,320
what the effect is when you use any type of noise distributions because there are many and they

175
00:19:24,320 --> 00:19:30,400
all have different properties, some have heavy details and others some are more skewed and others

176
00:19:30,400 --> 00:19:36,560
and things like that so we wanted to assess how that would impact the effectiveness of noise in

177
00:19:36,560 --> 00:19:42,560
but what we found was that the only thing that matters from the noise distribution is its second

178
00:19:42,560 --> 00:19:49,600
moment so the variance and that you can use any noise distribution as long as you use the same

179
00:19:49,600 --> 00:19:55,760
variance you will get the same performance and we show that also theoretically in the paper

180
00:19:55,760 --> 00:20:01,360
by deriving the actual objective function and we found that it depends on yeah the second

181
00:20:01,360 --> 00:20:11,440
moment of the noise. Did you come to any conclusions as to specific either problems or properties

182
00:20:11,440 --> 00:20:20,640
of data sets or other characteristics where you know this performs better than you know say drop

183
00:20:20,640 --> 00:20:27,520
out or some other method or is it does it appear to be broadly applicable to you know whenever

184
00:20:27,520 --> 00:20:34,560
you're using an RNN? Yes actually yes it would be we only looked at text data but it's applicable

185
00:20:34,560 --> 00:20:39,920
to any that's a sequence data and I'm working on extending it on that I'm also working on

186
00:20:39,920 --> 00:20:46,800
extending it to other architectures because ultimately it's not only useful for RNNs because the

187
00:20:46,800 --> 00:20:52,560
procedure is applicable to any type of neural networks so you can use this with convolutional

188
00:20:52,560 --> 00:20:58,640
neural networks or also feed forward neural networks. Okay and how far have you gotten with that?

189
00:20:58,640 --> 00:21:07,360
How how easily does it apply to for example a CNN? There's actually there's actually work on

190
00:21:07,360 --> 00:21:14,640
at nips 2017 the past nips that had something similar to what we were doing in the feed forward

191
00:21:14,640 --> 00:21:22,400
context but there what they were using was this importance waiting procedure and they optimize

192
00:21:22,400 --> 00:21:30,160
the low bound we are not using any of that and and when I say no it is applicable to any type of

193
00:21:30,160 --> 00:21:36,240
neural network it's because what the only thing that it requires is that if you give me a hidden

194
00:21:36,240 --> 00:21:41,840
state which is a low dimensional representation of your data from any neural network then I can

195
00:21:41,840 --> 00:21:47,120
regularize this neural network by just multiplying that hidden state with noise before using it

196
00:21:47,120 --> 00:21:54,800
to do prediction so in CNNs it would be yes do your usual CNN hidden state computation and then

197
00:21:54,800 --> 00:22:00,560
once you hand me that I will regularize with noise in by just multiplying it with noise or adding

198
00:22:00,560 --> 00:22:06,400
it with noise so additive or multiplicative noise injection. Are there any challenges to applying

199
00:22:06,400 --> 00:22:14,720
this approach to any to RNNs in particular or to to any other extending it to other types of

200
00:22:14,720 --> 00:22:22,480
networks? No it's actually very it's actually very simple and we we didn't one thing is that you

201
00:22:22,480 --> 00:22:28,400
have to choose the variance of your noise distribution that's the only thing that it requires and so

202
00:22:28,400 --> 00:22:33,840
you will need to do grid search on that because it depends on that parameter you need to tune the

203
00:22:33,840 --> 00:22:42,000
variance of your distribution according to the data you want to fit and so yes that's what the

204
00:22:42,000 --> 00:22:46,480
one one disadvantage might be that you will need to do grid search on that parameter.

205
00:22:47,520 --> 00:22:54,400
Yeah okay one thing that I also wanted to mention is that it has a an ensemble interpretation

206
00:22:54,400 --> 00:23:02,240
like dropout dropout is usually interpreted as getting predictions from an infinite number of

207
00:23:02,240 --> 00:23:08,640
neural networks we also found that interpretation is the method. Can you explain that elaborate on

208
00:23:08,640 --> 00:23:15,920
that interpretation? Basically when you inject noise and you want to maximize the likelihood you

209
00:23:15,920 --> 00:23:23,760
are basically marginalizing out all the noise you've injected into the network and that marginalization

210
00:23:23,760 --> 00:23:30,080
you can look at it as averaging the prediction of many neural network and that's basically what

211
00:23:30,080 --> 00:23:37,280
ensemble is and it's been known traditionally to actually have regularization effect which is

212
00:23:37,280 --> 00:23:43,280
another explanation explanation for why noise injection does regularization. Right we also talked

213
00:23:43,280 --> 00:23:51,840
about another paper this one you presented at ICLR last year on topic RNNs what was that one about?

214
00:23:51,840 --> 00:24:01,200
It was about combining recurrent networks and topic models for better sequence model in the sense

215
00:24:01,200 --> 00:24:08,080
that you are able to get the right context at each time step when doing prediction. The motivation

216
00:24:08,080 --> 00:24:15,280
for that work was that we found that RNNs and topic models were very complimentary RNNs are very

217
00:24:15,280 --> 00:24:23,200
good at encoding the local dependencies in the sequence so in language that would be syntax so RNNs

218
00:24:23,200 --> 00:24:29,760
are very good at detecting syntax and modeling syntax but they have a problem when you go further

219
00:24:29,760 --> 00:24:34,160
back in the sequence they have problems with long term dependencies because of the usual

220
00:24:34,160 --> 00:24:41,680
vanishing exploding gradients and what we noticed was that actually we don't need to have an RNN

221
00:24:41,680 --> 00:24:49,440
with many time steps because as you go further the dependencies in text are not sequential dependencies

222
00:24:49,440 --> 00:24:55,680
they are semantic dependencies and these words in the document are related semantically with

223
00:24:56,960 --> 00:25:03,120
because they belong to the same team and topic models are known as these probabilistic

224
00:25:03,120 --> 00:25:08,960
probabilistic models that can detect teams in data and so it made sense to use topic models

225
00:25:08,960 --> 00:25:13,680
to get the teams and then condition on those when doing prediction with the RNN.

226
00:25:13,680 --> 00:25:20,720
And so when you say topic models are you referring to things like embedding spaces and word

227
00:25:20,720 --> 00:25:28,560
to veck and the like or something else? I'm referring to something else. So what is a topic model?

228
00:25:28,560 --> 00:25:35,360
A topic model is like a family of graphical model that takes a bunch of documents and tells you

229
00:25:35,360 --> 00:25:42,320
the teams that they discuss. It tells you each document which topics they discuss in which

230
00:25:42,320 --> 00:25:48,320
proportion and it tells you what these topics are. So a topic in the topic model literature is

231
00:25:48,320 --> 00:25:57,600
defined as a distribution of a word of a word and each document is expressed as a distribution

232
00:25:57,600 --> 00:26:06,720
over those topics. And so what you end up with is a distribution of a word that's your topic matrix

233
00:26:06,720 --> 00:26:12,080
at least of distribution of a word and you also end up with a distribution of a topic for each

234
00:26:12,080 --> 00:26:18,800
document. Is your distribution of words is this I guess what comes to mind is LDA is this

235
00:26:18,800 --> 00:26:26,320
something that you might use in LDA to do exactly what it is? There's LDA and there's approximations

236
00:26:26,320 --> 00:26:32,640
to LDA using neural networks. So these are neural topic models and use that version in the topic

237
00:26:32,640 --> 00:26:38,560
ironing paper. We use the neural topic model where you basically represent your document as a

238
00:26:38,560 --> 00:26:46,560
bag of word as in LDA and project that high dimensional bag of word representation using an MLP

239
00:26:46,560 --> 00:26:54,160
to get a distribution to get a document distribution. Now what I do in topic ironing is I represent that

240
00:26:54,160 --> 00:27:02,480
document distribution with a Gaussian and I condition on that in the softmax as an additional

241
00:27:02,480 --> 00:27:08,960
by-ster depending on whether I need to predict the word that needs this topic content or a word

242
00:27:08,960 --> 00:27:14,960
that does not need the topic content. And how do you determine for a given word whether you need

243
00:27:14,960 --> 00:27:20,400
to use the topic content or not? I let the iron and because they are highly flexible I let the

244
00:27:20,400 --> 00:27:25,760
iron and tell me whether the next word should be a quote-unquote stop words. Those are words that

245
00:27:25,760 --> 00:27:32,080
don't need any that don't have any semantic meaning. Okay so based on stop words. Yes. Okay.

246
00:27:32,080 --> 00:27:37,440
So the iron and will tell me whether that the next word needs the topic content or not. Yeah binary

247
00:27:37,440 --> 00:27:49,680
classification. Got it. So this is interesting to me and that it seems like a an NLP type of

248
00:27:49,680 --> 00:27:55,760
application of the idea of I guess this theme that I've seen in other places where hey we've got

249
00:27:55,760 --> 00:28:02,800
these RNN or we've got these neural networks they're great for you know basically approximating

250
00:28:02,800 --> 00:28:09,440
any relationship if we throw enough data at them but you know hey there are also these more

251
00:28:09,440 --> 00:28:17,440
traditionally determined relationships that we've figured out. So for example it strikes me as

252
00:28:17,440 --> 00:28:27,840
analogous to in robotics you know using a strict neural network approach to go from visual input

253
00:28:27,840 --> 00:28:35,360
to motor output you know or an approach that combines the neural network with traditional

254
00:28:35,360 --> 00:28:40,640
like control system modeling or something like that. It seems like it's very much thematically

255
00:28:40,640 --> 00:28:48,080
uh in those kind of along those lines and that's exactly what it is there's this line of work of

256
00:28:48,080 --> 00:28:54,000
planning to combine privacy grabs for modeling and neural networks and and I think that's an

257
00:28:54,000 --> 00:28:59,760
interesting line of work which I'm working on because they're very complimentary like when we are

258
00:28:59,760 --> 00:29:05,360
using ease of these frameworks we are trying to learn from high dimension of data and neural networks

259
00:29:05,360 --> 00:29:12,080
give you a way to design these highly flexible likelihood whereas graphical models allow you to

260
00:29:12,080 --> 00:29:17,040
basically represent the hidden structure you want to learn from data in an interpretable way

261
00:29:17,600 --> 00:29:24,000
so you can combine the two together to benefit from the best of both worlds. Right right and uh

262
00:29:24,000 --> 00:29:32,640
that approach is always um I always find that uh compelling you know but then I I've also talked

263
00:29:32,640 --> 00:29:40,320
to folks that you know we might describe as deep learning purists that say you know it's ultimately

264
00:29:40,320 --> 00:29:45,520
a waste of time the neural networks configure everything out if you throw enough data and compute

265
00:29:45,520 --> 00:29:50,160
at them and I'm just curious how like if you thought about it like that and how you might respond to

266
00:29:50,160 --> 00:29:56,880
that. There are still there's that might be true in some context if you have too much compute and you

267
00:29:56,880 --> 00:30:04,000
have too much data but that's not the case for many people and and there are times where you

268
00:30:04,000 --> 00:30:09,280
actually care about uh these latent variables there are times where you want to look at them

269
00:30:09,280 --> 00:30:16,000
and say oh this uh this is how these things depend together and most of the time when you say

270
00:30:16,000 --> 00:30:21,040
I can just use a neural net and throw compute and data at it and it will give me what I want that's

271
00:30:21,040 --> 00:30:27,120
when you only care about performance about getting the state of the art number but there are times

272
00:30:27,120 --> 00:30:31,840
where you actually use this model to better understand the data you're dealing with to

273
00:30:32,640 --> 00:30:39,280
encode uncertainty to make help people in decision-making understand better their data and

274
00:30:39,280 --> 00:30:44,400
everything and they're just using a neural network to get better performance wouldn't mean much.

275
00:30:44,400 --> 00:30:52,720
So for you the the two counter arguments are limited resource scenarios and

276
00:30:53,680 --> 00:30:58,240
you know broadly speaking explainability and insight that you're able to get from this train

277
00:30:58,240 --> 00:31:06,240
system. Yes exactly. Continuing on in the topic RNN work was there any interesting insights in

278
00:31:06,240 --> 00:31:11,680
the way that you combine the topic models and the RNNs where there are different ways that you

279
00:31:11,680 --> 00:31:18,560
could have done that but you you know went down a specific path or yes there are different ways

280
00:31:18,560 --> 00:31:24,800
you can combine that. Actually the topic RNN worked you can look at it in a more general way and

281
00:31:24,800 --> 00:31:29,840
say that what we are trying to do is at the end of the two model sequence data you need to have

282
00:31:29,840 --> 00:31:35,520
a model to capture the local dependencies so that's that will be your syntactic model and then you

283
00:31:35,520 --> 00:31:41,840
will have a model that will capture the global dependencies and that would be your semantic model

284
00:31:41,840 --> 00:31:47,840
and you have also to decide on how you want to combine them so you have three degrees of freedom

285
00:31:47,840 --> 00:31:52,480
with it you have how to decide on the same tactic how to decide on the semantic how you decide

286
00:31:52,480 --> 00:32:00,000
on combining them. In topic RNN the idea was so cool and motivating to me that I tried

287
00:32:00,000 --> 00:32:06,400
in for any of those three things I tried the simplest thing so for the syntactic model I tried

288
00:32:06,400 --> 00:32:12,080
an RNN which was the main thing that was used for sequences and for the topic model I used the

289
00:32:12,080 --> 00:32:17,040
neural topic model version because those are efficient to learn ways you just project a bag of

290
00:32:17,040 --> 00:32:21,680
what representation for a neural network to get a latent representation of the document distribution

291
00:32:21,680 --> 00:32:28,320
and in combining them I just said let me use the usual software max but add this topic information

292
00:32:28,320 --> 00:32:34,000
as an additional biester but you can actually use all these three things in different ways

293
00:32:34,000 --> 00:32:39,360
and that's what's cool about it because it somehow defines a new class of models for dealing with

294
00:32:39,360 --> 00:32:45,280
these sequences. Did you train these simultaneously or did you train your semantic model and your

295
00:32:45,280 --> 00:32:51,520
syntactic model separately? Yes the cool thing is that you want to define all these three things

296
00:32:51,520 --> 00:32:58,000
in the way you combine them in such a way that you can afford joint training. I always prefer

297
00:32:58,000 --> 00:33:04,000
joint training because then you are letting your model also the ability to decide what

298
00:33:04,000 --> 00:33:09,680
what it want to use from each component so we did joint training from the topic model and the

299
00:33:10,240 --> 00:33:15,680
RNN we trained the two things together because the whole pipeline is fully differentiable.

300
00:33:15,680 --> 00:33:22,560
And how did you evaluate the performance of this approach? The first thing we tried it as a

301
00:33:22,560 --> 00:33:28,240
language model so we say let's see how it improves on existing language models so we did for next

302
00:33:28,240 --> 00:33:36,880
word prediction where we got good results but we also said let's see this as a feature extractor

303
00:33:36,880 --> 00:33:42,560
and we applied it to document classification more specifically we applied it to sentiment analysis

304
00:33:42,560 --> 00:33:48,560
so we use the IMDB dataset which is a bunch of reviews and labels of those reviews whether they

305
00:33:48,560 --> 00:33:55,520
are negative or positive and we use topic RNN to extract features from the reviews and use

306
00:33:55,520 --> 00:34:02,160
those features in a binary classification model and we achieved a very nice results not only

307
00:34:02,160 --> 00:34:08,720
in terms of classification error rate but also in how it managed to discover the features for

308
00:34:08,720 --> 00:34:14,960
the negative reviews and the positive reviews so we had this nice clustering of the features from

309
00:34:14,960 --> 00:34:19,360
the topic RNN of the negative reviews and the positive reviews and that was very exciting.

310
00:34:19,360 --> 00:34:26,800
Interesting and the features in this context would be words like you know stinks horrible

311
00:34:26,800 --> 00:34:32,880
that kind of thing or something else. The features we learned was that we use the because

312
00:34:32,880 --> 00:34:38,640
topic RNN is a generative model for tech for for any sequence data we use the reviews alone

313
00:34:38,640 --> 00:34:45,520
and trained that as a language model as usual and we derived the features by taking the last hidden

314
00:34:45,520 --> 00:34:51,680
state of the RNN component of topic RNN and the document distribution output by the topic model

315
00:34:51,680 --> 00:34:57,200
component of topic RNN and we concatenated them together to say this represents the future for

316
00:34:57,200 --> 00:35:06,960
this document. The last hidden state of the topic model RNN and the so you're trying to get the kind

317
00:35:06,960 --> 00:35:14,080
of your latent state of both your semantic and syntactic models and kind of smush them together

318
00:35:14,080 --> 00:35:21,440
to represent some feature. Yes exactly. Okay. Yeah. Interesting. That turned out to be effective at

319
00:35:21,440 --> 00:35:27,600
telling you where the negative reviews and the positive reviews lie in this data manifold.

320
00:35:28,640 --> 00:35:33,920
I don't know if if this makes sense given like dimensionality and stuff like that but

321
00:35:33,920 --> 00:35:42,480
qualitatively what did those features look like? Did they look like in terms of visualization?

322
00:35:42,480 --> 00:35:49,600
Visualization or like is it a one hot encoded you know an encoded vector of like where negative

323
00:35:49,600 --> 00:35:55,840
words were identified for negative features and positive words were identified for positive

324
00:35:55,840 --> 00:36:00,240
features or something like that or is it something totally different? It's something it's at the

325
00:36:00,240 --> 00:36:07,200
document level not at the word level. Okay. Yeah. It's at the document level when you concatenate

326
00:36:07,200 --> 00:36:13,600
these two things and project them in two dimension then you will see a nice clustering of negative

327
00:36:13,600 --> 00:36:19,840
documents and positive documents together. We don't go at the word level. One thing that's

328
00:36:19,840 --> 00:36:26,720
interesting is that the IMDB has a lot of sarcasm on it so it's actually it's actually very hard

329
00:36:26,720 --> 00:36:33,360
to capture the sentiment just using the review because of that. So this was this was a cool encouraging

330
00:36:33,360 --> 00:36:39,760
result. And so why do you think that this worked better for presumably it did capture some of

331
00:36:39,760 --> 00:36:48,560
that sarcasm? I think the way we use the features, the way we we said let's combine the document

332
00:36:48,560 --> 00:36:54,800
distributions as given by the neural topic model and the final hidden state of the RNN. That final

333
00:36:54,800 --> 00:37:02,320
hidden state usually captures you know how when you when you write a review say oh this movie was

334
00:37:02,320 --> 00:37:11,120
was the actors were great then and then but it sucked because of it. Right. So the whole sentiment is

335
00:37:11,120 --> 00:37:16,800
actually captured in that last part. Okay. So I am guessing that's why it did well because

336
00:37:16,800 --> 00:37:25,040
because you exploit that sarcasm when you actually just look at the most recent information. How do

337
00:37:25,040 --> 00:37:30,880
you see this work being applied? I think it can be used for any we looked on their sentiment

338
00:37:30,880 --> 00:37:36,160
classification. The word prediction thing is not a real task. We were just using it to evaluate

339
00:37:36,160 --> 00:37:41,680
it as a language model. But I think it can be used for any document classification task in

340
00:37:41,680 --> 00:37:47,920
there are many applications to that. Not only sentiment analysis but yeah any document classification

341
00:37:47,920 --> 00:37:53,200
you can use Topikarin and to get to get features it proved to be very effective in the case of

342
00:37:53,200 --> 00:37:58,880
sentiment but you can use it for any document classification task. Say you wanted to classify

343
00:37:58,880 --> 00:38:07,600
documents or like tag documents based on content. Would you have to come up with your own feature

344
00:38:07,600 --> 00:38:14,560
representation or would you use this concatenation that you've described but then do something else

345
00:38:14,560 --> 00:38:20,960
to get at the last mile of how you want to classify? Where does the customization have to take

346
00:38:20,960 --> 00:38:26,480
place in order to apply this to another type of classification problem? The general thing is that

347
00:38:26,480 --> 00:38:32,720
you will have to use both the global representation and the local representation you need both.

348
00:38:32,720 --> 00:38:38,320
And from the local representation side you can decide to use the last hidden state but you can do

349
00:38:38,320 --> 00:38:43,840
anything you want there. You can use an averaging of the different hidden state or you can use a

350
00:38:44,480 --> 00:38:50,800
I don't know like attention. It depends on what you can do. You can build a classifier on top

351
00:38:50,800 --> 00:38:57,440
of Topikarin and I did that actually for medical data and we found again there a nice clustering

352
00:38:57,440 --> 00:39:06,160
of patients according to their diseases. And so yes the customization is in how you use the

353
00:39:06,160 --> 00:39:12,800
hidden state of the RNN part. Okay. Yeah. Well you're working on some really interesting things and

354
00:39:12,800 --> 00:39:18,160
I appreciate you taking the time to share some of them with us. Are there any other things you

355
00:39:18,160 --> 00:39:32,480
wanted to touch on? That would be it. Thank you for taking the time. Thank you for having me.

356
00:39:35,920 --> 00:39:42,160
All right everyone that's our show for today. For more information on Ajay or any of the topics

357
00:39:42,160 --> 00:39:49,600
covered in this episode head on over to twomlai.com slash talk slash 160. If you didn't hit pause

358
00:39:49,600 --> 00:39:54,400
and nominate us for the podcast people's choice awards at the beginning of the show I'd like to

359
00:39:54,400 --> 00:40:01,680
encourage you to jump over to twomlai.com slash nominate right now and send us your love and your vote.

360
00:40:01,680 --> 00:40:12,960
So as always thanks so much for listening and catch you next time.

