WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.240
I'm your host Sam Charrington.

00:31.240 --> 00:36.320
Alright everyone, I am on the line with Genevara Allen, Genevara is an associate professor

00:36.320 --> 00:41.240
of statistics in the Department of Statistics, Computer Science and Electrical and Computer

00:41.240 --> 00:46.800
Engineering at Rice University, as well as Founder and Director of the Rice Center for

00:46.800 --> 00:53.080
Transforming Data to Knowledge and an Investigator with the Neurological Research Institute

00:53.080 --> 00:55.080
with the Baylor College of Medicine.

00:55.080 --> 00:58.120
Genevara, welcome to this week in machine learning and AI.

00:58.120 --> 01:00.000
Thank you, thanks for having me.

01:00.000 --> 01:07.320
Why don't we jump right in and have you introduce yourself to our audience, you work kind

01:07.320 --> 01:13.920
of at the intersection of stats, as well as medicine, how do those connect for you and

01:13.920 --> 01:19.600
how did you arrive at this, your research interests?

01:19.600 --> 01:27.640
Sure, so my work, my research interests are to build statistical and machine learning

01:27.640 --> 01:34.920
tools to help scientists make discoveries from their big and large complex data sets.

01:34.920 --> 01:41.080
I particularly like working in neuroscience and genetics, but I'm interested in lots

01:41.080 --> 01:43.320
of scientific problems.

01:43.320 --> 01:47.440
And I took a little bit of a roundabout track to get here.

01:47.440 --> 01:55.000
I went to undergrad interested in music, actually, but took a statistics class and really liked

01:55.000 --> 01:56.000
it.

01:56.000 --> 02:02.640
It felt like it made sense, it's kind of the, to me, it's kind of the mathematical basis

02:02.640 --> 02:06.880
of making decisions, right, kind of data-driven decisions.

02:06.880 --> 02:12.880
And I really liked that a lot, got into statistics and as an undergrad, I actually worked

02:12.880 --> 02:18.080
at the Baylor College of Medicine, which is across the street from Rice University, where

02:18.080 --> 02:25.080
I was doing my undergrad as a data analyst in a biomedical research lab, but also did

02:25.080 --> 02:30.520
some research in statistics and then went to grad school at Stanford and statistics,

02:30.520 --> 02:36.200
which that department is heavily influenced by machine learning and computer science.

02:36.200 --> 02:41.400
So while in grad school, I took quite a bit of classes in machine learning and applied

02:41.400 --> 02:49.760
math and optimization, and that kind of launched my research at the intersection.

02:49.760 --> 02:50.760
It's interesting.

02:50.760 --> 02:56.240
I'm pretty interdisciplinary using kind of statistics, machine learning, optimization techniques,

02:56.240 --> 02:59.560
but I'll apply the solve scientific problems.

02:59.560 --> 03:04.840
You gave a talk at the AAAS conference, was it last month?

03:04.840 --> 03:05.840
It was last month.

03:05.840 --> 03:09.240
Okay, AAAS being the American Association for the Advancement of Science.

03:09.240 --> 03:15.720
On the topic of reproducibility in machine learning, that sounds like from what I read,

03:15.720 --> 03:21.760
the reaction to your talk caught you a little bit by a surprise.

03:21.760 --> 03:28.120
Maybe we'll start by talking about the reaction and really dig into kind of your thoughts

03:28.120 --> 03:29.600
on the topic.

03:29.600 --> 03:31.240
Absolutely.

03:31.240 --> 03:37.720
So I gave a talk at the AAAS meeting and a press briefing.

03:37.720 --> 03:46.600
And I titled my talk, can we trust data driven discoveries, or more specifically, can we

03:46.600 --> 03:54.600
trust discoveries made using machine learning techniques from large scientific data sets?

03:54.600 --> 04:03.080
And I really ask this question as to be a bit provocative, but my core interest and my

04:03.080 --> 04:08.400
research is actually on how to answer these questions, kind of what are some of the new

04:08.400 --> 04:15.400
research directions and directions we can go to help make scientists give scientists tools

04:15.400 --> 04:20.400
so that they can better make inferences from their really big data using machine learning

04:20.400 --> 04:22.040
techniques.

04:22.040 --> 04:27.760
So I gave a talk and outlined some of these new research approaches, some general approaches

04:27.760 --> 04:32.120
and some new research from my own group as well.

04:32.120 --> 04:40.480
So I was certainly a surprise to say the least at some of the response online.

04:40.480 --> 04:44.760
And I'd love to take the opportunity to dig into this deeper.

04:44.760 --> 04:51.840
I think that there are a lot of people that were very excited about what I said and said,

04:51.840 --> 04:55.400
hey, these are good points or at least these are things that we should have a conversation

04:55.400 --> 04:58.440
about or start a discussion on.

04:58.440 --> 05:04.160
Those were quite upset and said I was machine learning public enemy number one, which I would

05:04.160 --> 05:08.200
disagree with slightly because I love machine learning.

05:08.200 --> 05:09.200
It's what I do.

05:09.200 --> 05:12.400
I think machine learning is awesome and want to promote machine learning.

05:12.400 --> 05:14.240
And that's certainly not what I was doing.

05:14.240 --> 05:20.800
But I would love to raise the point of that there's a lot of research directions in

05:20.800 --> 05:21.800
this area.

05:21.800 --> 05:26.480
And perhaps today I can highlight a couple of those and perhaps sparks some ideas for other

05:26.480 --> 05:28.600
machine learning researchers.

05:28.600 --> 05:29.600
Let's do that.

05:29.600 --> 05:37.560
But where I like to start having seen your slides from that presentation is where you

05:37.560 --> 05:38.560
started.

05:38.560 --> 05:46.120
And in particular, you gave a lot of examples, very focused on medicine and biological

05:46.120 --> 05:47.120
science.

05:47.120 --> 05:52.840
We've talked here on the podcast about the reproducibility problems in ML broadly.

05:52.840 --> 06:01.200
But I'd love to understand the way you contextualize them in the medicine domain.

06:01.200 --> 06:02.360
Absolutely.

06:02.360 --> 06:07.800
So I should say biomedical data is really broad.

06:07.800 --> 06:12.080
There's lots of different types of data out there and machine learning techniques that

06:12.080 --> 06:13.960
people apply to it.

06:13.960 --> 06:19.800
So I can give a couple of examples, but there's lots of situations where there's pretty large

06:19.800 --> 06:24.280
and complex data sets over the past, say, five, 10 years, 20 years.

06:24.280 --> 06:30.560
There's been a lot of new technologies developed to measure, say, nearly every aspect of our genome

06:30.560 --> 06:38.840
or to image brains and record from neurons at scales larger than we could ever have imagined

06:38.840 --> 06:40.160
five years ago.

06:40.160 --> 06:47.360
And so all of this is really exciting, but the data is quite big to analyze this data and

06:47.360 --> 06:50.600
really kind of make discoveries from this data.

06:50.600 --> 06:56.120
You need to use, it's, you can't necessarily just use classical statistics and classical

06:56.120 --> 06:57.120
techniques.

06:57.120 --> 07:00.920
A lot of machine learning techniques are used to really dive into this data and understand

07:00.920 --> 07:03.040
the data.

07:03.040 --> 07:10.040
And the question I would ask is, are the discoveries that we make from using techniques

07:10.040 --> 07:12.200
in this data, are they reproducible?

07:12.200 --> 07:19.640
So a specific example is, and this has actually been a well studied example.

07:19.640 --> 07:29.520
So in cancer genomics, people are very interested in finding subgroups of patients that have

07:29.520 --> 07:38.920
are genomically similar, have genomically similar tumor profiles, but and also have clinical

07:38.920 --> 07:39.920
differences.

07:39.920 --> 07:42.520
Cancer, cancer subtypes.

07:42.520 --> 07:48.480
And these are very promising in precision medicine because they're useful in precision

07:48.480 --> 07:53.920
medicine, where say we want to find groups of patients that are genomically similar and

07:53.920 --> 07:59.840
develop jug targets that can specifically target their genomic signatures.

07:59.840 --> 08:04.320
So it's important to find these groups of patients that have these similar genomic

08:04.320 --> 08:11.320
signatures and one very successful, very well-known case study of this is in breast cancer, where

08:11.320 --> 08:17.120
there's very famous breast cancer subtypes that have been very well validated.

08:17.120 --> 08:22.000
These were found using clustering techniques, pretty standard clustering techniques actually

08:22.000 --> 08:24.440
just hierarchical clustering.

08:24.440 --> 08:29.480
And the, there's a very strong signal and separation between groups of patients based

08:29.480 --> 08:38.200
on their genomic profiles, and these subgroups have been very well validated by many studies

08:38.200 --> 08:42.840
in many clinical settings, and now they're actually developing drugs that are targeting

08:42.840 --> 08:46.680
specific for different subgroups of breast cancer right now.

08:46.680 --> 08:53.600
So that's a success story of using machine learning in biomedicine, and after that a lot

08:53.600 --> 08:59.280
of people for various different cancer types say lung cancer, colorectal cancer or ovarian

08:59.280 --> 09:02.320
cancer have asked the same questions.

09:02.320 --> 09:07.240
Can we find these subgroups that have where their tumors are genomically similar so that

09:07.240 --> 09:10.720
we can treat them in a similar manner with similar drugs?

09:10.720 --> 09:16.040
And the problem is that one, you know, one paper will come out and it'll make a big splash

09:16.040 --> 09:22.720
and say here's a definitive, you know, subtyping and clustering of, you know, this particular

09:22.720 --> 09:26.880
cancer type, and then another paper comes out maybe two years later on another cohort

09:26.880 --> 09:31.600
of patients that has a different clustering and maybe even totally different numbers of

09:31.600 --> 09:37.920
groups anywhere from three subtypes or groups to seven subtypes or groups.

09:37.920 --> 09:43.240
And they haven't necessarily been consistent and reproduced, and several of the people

09:43.240 --> 09:48.120
that have tried to do meta-analyses on these studies have said, look, you know, some

09:48.120 --> 09:54.760
of these clustering results that we found, these proposed subtypes, don't in fact, are

09:54.760 --> 09:57.600
not reproducible, do not necessarily hold up.

09:57.600 --> 10:03.680
So this is just one example from cancer genomics, but I think it's reminiscent of kind of a

10:03.680 --> 10:05.440
larger problem.

10:05.440 --> 10:11.320
Now I should caution that my research is not necessarily on, I don't study the extent

10:11.320 --> 10:17.720
of the reproducibility crisis, I'm focused more on developing methods to solve problems

10:17.720 --> 10:21.720
related to this, but I do think this is an open area for research.

10:21.720 --> 10:27.120
I do think it's a problem in biomedical studies, and I've certainly heard lots of anecdotal

10:27.120 --> 10:32.560
evidence that there's a problem from both scientists who are reading these papers and say,

10:32.560 --> 10:37.200
look, I tried to reproduce this lab's finding from their big data set, I have data just

10:37.200 --> 10:40.800
like it, I tried to reproduce it, I, you know, couldn't.

10:40.800 --> 10:45.320
I hear this actually quite a bit a lot from scientists that I work with.

10:45.320 --> 10:51.960
I hear this also from other machine learning and data science researchers in the biomedical

10:51.960 --> 10:59.160
domain, who seem well aware of the problem, and I do think it warrants further investigation.

10:59.160 --> 11:02.680
Okay, so a couple of clarifying questions.

11:02.680 --> 11:09.800
The first is you talked about the breast cancer studies that have been validated, that

11:09.800 --> 11:18.640
originated from using statistical clustering techniques, and then some other results that

11:18.640 --> 11:25.400
have not been validated and were contradictory results have found, just to make sure the

11:25.400 --> 11:30.760
statistical techniques in both, I guess there are a couple of ways to ask this question

11:30.760 --> 11:36.480
A, are you kind of categorizing the clustering techniques and using the first example as

11:36.480 --> 11:41.840
machine learning, you know, and or, you know, are the clustering, are the techniques used

11:41.840 --> 11:46.840
in some of these other studies, are they the same, and the issue is that, you know, what

11:46.840 --> 11:52.000
worked here is trying to be used in other places and it's not working, or, you know, are

11:52.000 --> 11:54.520
there different types of techniques being used?

11:54.520 --> 11:57.560
Okay, so that's a great question.

11:57.560 --> 12:02.480
So first, I don't like to necessarily call techniques, statistics techniques, machine learning

12:02.480 --> 12:07.560
techniques, data science techniques, I actually think there are a lot of the same toolkits.

12:07.560 --> 12:13.680
It's just a data scientist toolkits, and we should probably be using all of these techniques.

12:13.680 --> 12:17.840
But most of the techniques that discovered the breast cancer subtypes are very, very, very

12:17.840 --> 12:24.520
simple algorithms, hierarchical clustering and k-means, very standard clustering algorithms.

12:24.520 --> 12:32.640
And I characterize the reproducibility issue here as not just, you know, a scientist, you

12:32.640 --> 12:37.280
know, took a new data set and applied the exact same pipeline and the exact same techniques

12:37.280 --> 12:39.440
and couldn't reproduce the results.

12:39.440 --> 12:45.680
But it's also, if something is truly, I think our goal, especially when it comes to analyzing

12:45.680 --> 12:51.780
scientific data, our goal is to make some type of scientific discovery that actually has

12:51.780 --> 12:56.840
underlying principles, in this case, that actually if we're trying to find subgroups based

12:56.840 --> 13:05.240
on similarity of genomic profiles, there should be some actual underlying biology that links

13:05.240 --> 13:06.240
these groups.

13:06.240 --> 13:13.440
So there's some type of scientific truth that we're trying to discover, and we don't want

13:13.440 --> 13:21.200
necessarily, or our goal should be to have techniques that we can apply, that we understand

13:21.200 --> 13:28.360
but whether it's likely to be a finding is likely to be reproducible on a future data

13:28.360 --> 13:33.880
set, not just when applying the exact same kind of technique and pipeline, but is going

13:33.880 --> 13:39.960
to be lead to kind of a reproducible, true discovery, perhaps if you alter the pipeline

13:39.960 --> 13:43.480
a little bit, apply a slightly different clustering technique.

13:43.480 --> 13:50.680
So it's really kind of a robustness to like the particular choices of the data analyst,

13:50.680 --> 13:52.040
taking with the data.

13:52.040 --> 13:58.840
So it's a little bit more than just exactly, I think stage one is you should be able to

13:58.840 --> 14:05.000
exactly apply someone else's pipeline on a new data set and hopefully get similar results,

14:05.000 --> 14:08.560
kind of the training to test that type thing.

14:08.560 --> 14:13.440
But a step beyond that is if you change the pipeline just a little bit, if another analyst

14:13.440 --> 14:19.360
applies a very reasonable clustering technique or dementia reduction technique, can you still

14:19.360 --> 14:27.560
recover the same type of scientific result under reasonable perturbations to your methods?

14:27.560 --> 14:31.880
Do you contend that kind of reproducibility by definition means that you should be able

14:31.880 --> 14:38.880
to apply the same pipeline to different data sets that are exploring different diseases?

14:38.880 --> 14:42.920
Why are those, why do those need to go hand in hand?

14:42.920 --> 14:43.920
Ah, okay.

14:43.920 --> 14:49.320
So I'm not necessarily saying apply the same pipeline to data sets on different diseases.

14:49.320 --> 14:54.800
Let's think about this and we're talking about data set, you've got the same fundamental

14:54.800 --> 14:55.800
type of data.

14:55.800 --> 14:56.800
The same.

14:56.800 --> 15:01.640
Imagine you've got a training data set and say if you're doing, if your goal in machine

15:01.640 --> 15:06.960
learning is doing prediction and prediction has been super well studied and we know or

15:06.960 --> 15:12.360
a lot of people are doing research on how to do reproducible predictive models, right?

15:12.360 --> 15:15.840
We've got to, in the fundamental premises, we've got a training data set and we've got

15:15.840 --> 15:17.760
a test data set, right?

15:17.760 --> 15:22.920
We've got prediction error that measures how close our predictions on the test data

15:22.920 --> 15:27.000
set are to the true labels or outcome on that data set, right?

15:27.000 --> 15:33.200
So we've got very well understood principles for measuring kind of the reproducibility or

15:33.200 --> 15:38.440
the more broadly, like the generalizability of our findings when it comes to prediction

15:38.440 --> 15:40.600
as the goal.

15:40.600 --> 15:48.240
And what I think the other kind of big goal of machine learning techniques is data driven

15:48.240 --> 15:50.480
discovery or inference.

15:50.480 --> 15:59.880
And I would define this as, say, trying to find insights from big data, right?

15:59.880 --> 16:05.080
So we're not interested necessarily in, and this isn't a distinction between unsupervised

16:05.080 --> 16:06.800
learning and supervised learning.

16:06.800 --> 16:11.960
It's more of a distinction of, in prediction, are you interested in just the predicted labels

16:11.960 --> 16:15.680
or outcomes and it's kind of a black box that got you there and you don't care what got

16:15.680 --> 16:16.680
you there?

16:16.680 --> 16:22.680
Or are you interested in what's underneath that black box that insights from the data?

16:22.680 --> 16:28.680
So this could be, say, if you're doing supervised learning, say you're interested in which features

16:28.680 --> 16:34.480
are associated with outcome and your inference question is what are the important features

16:34.480 --> 16:40.240
associated with my outcome or it could be this fine groups or structure in my data set,

16:40.240 --> 16:44.680
fine patterns, fine interactions between features and so forth.

16:44.680 --> 16:51.560
So those insights from data, we have a lot of fantastic machine learning methods that

16:51.560 --> 16:56.640
have been developed to find all sorts of really great insights from big data.

16:56.640 --> 16:57.640
It's really awesome.

16:57.640 --> 16:59.560
There's been so much development in this area.

16:59.560 --> 17:03.240
This is actually one of my primary areas of research.

17:03.240 --> 17:11.760
But there's been less research until recently on asking, can we get that type of same generalized

17:11.760 --> 17:19.160
ability and reproducibility of our data-driven discoveries as we can for prediction?

17:19.160 --> 17:24.960
So for example, in prediction, we assume that our training and our test data come from

17:24.960 --> 17:25.960
the same population.

17:25.960 --> 17:28.440
It's the same data set, right?

17:28.440 --> 17:35.360
And our goal is to learn some type of machine learning model on a training data set that's

17:35.360 --> 17:39.920
going to generalize well on future data set, which is the test data set that comes from

17:39.920 --> 17:41.800
the same population.

17:41.800 --> 17:47.520
And if you over train to your training data set, you of course over fit and your model will

17:47.520 --> 17:51.120
not generalize well and you're going to have poor predictions on your test data.

17:51.120 --> 17:56.680
So imagine even you can do the same thing with data-driven discovery if that's your primary

17:56.680 --> 18:01.760
goal and say, OK, we've got a test, we've got a training data set, and we've got a test

18:01.760 --> 18:03.200
data set.

18:03.200 --> 18:08.920
And what if we use our training data set to find a particular type of pattern or group

18:08.920 --> 18:10.880
or important features?

18:10.880 --> 18:16.920
If you, there's a lot of potential to say over fit and I'm saying over fit and quotes

18:16.920 --> 18:22.760
to your training data and find particular kind of discoveries that are inherent in your

18:22.760 --> 18:27.200
training data that are probably not going to generalize to a test data set that comes

18:27.200 --> 18:29.600
from the same population.

18:29.600 --> 18:37.320
And so the question is, is how can we build new data-driven discovery techniques or perhaps

18:37.320 --> 18:44.840
kind of other techniques to make sure that the discoveries we make are generalizable

18:44.840 --> 18:50.400
to kind of future test data sets in the context of data-driven discoveries?

18:50.400 --> 18:55.200
You actually hit on my second question, which was I was kind of keying off of your use

18:55.200 --> 19:04.120
of the term scientific discovery here and really wondering about like what's special about

19:04.120 --> 19:10.120
that relative to other uses of machine learning in other places that this reproducibility

19:10.120 --> 19:11.360
problem comes up.

19:11.360 --> 19:17.440
And I think that the idea of prediction versus inference really captured that, captured

19:17.440 --> 19:19.480
that really nicely.

19:19.480 --> 19:24.320
So we're talking about, you know, the way we typically think about kind of building these

19:24.320 --> 19:29.440
models, you've got some data set and you kind of split it into your training data and

19:29.440 --> 19:32.640
your test data and you build a model.

19:32.640 --> 19:38.800
And in some ways, you want to apply that to, you know, another data set, you know, totally

19:38.800 --> 19:43.760
different data set, meaning the same kind of data, obviously, the same, you know, underlying

19:43.760 --> 19:51.560
population, but a data that's been collected, you know, maybe differently and that you can,

19:51.560 --> 19:57.440
you kind of massage to kind of fit the need of your original pipeline.

19:57.440 --> 20:01.920
We look at kind of the way we typically report these results like in academic papers where

20:01.920 --> 20:06.600
almost always using the same data set, oftentimes it's kind of the same data set that everyone

20:06.600 --> 20:08.560
else uses.

20:08.560 --> 20:14.400
And so, you know, there's this reproducibility across different data sets.

20:14.400 --> 20:21.200
Yeah, I'm, I'm perhaps talking a little bit less about reproducibility across different

20:21.200 --> 20:29.920
data sets of different types, but more asking with, if your goal is to do inference or data

20:29.920 --> 20:37.240
driven discovery instead of prediction, are there good ways of assessing how generalizable

20:37.240 --> 20:43.320
your results are from an analysis of a particular data set?

20:43.320 --> 20:50.880
So in particular, let's take an example of just imagine you've got a data set that someone

20:50.880 --> 20:54.880
gives you a big, could be scientific data, could be any data, someone gives you a big data

20:54.880 --> 21:00.520
set and you split it into two-thirds training and one-third test data.

21:00.520 --> 21:07.880
So if you were doing prediction, there's great metrics on you build, you build a model

21:07.880 --> 21:13.920
that predicts some labels on your test data and there's great metrics to measure the success

21:13.920 --> 21:18.360
of your labels, misclassification error, all sorts of other things.

21:18.360 --> 21:22.080
We have kind of metrics to measure prediction error.

21:22.080 --> 21:27.680
If we're doing data driven discovery, for example, suppose you find clusters or groups

21:27.680 --> 21:30.240
on your training data set.

21:30.240 --> 21:37.360
How do you measure on your test data set, whether you found those same groups, do you apply

21:37.360 --> 21:43.920
your same cost-oring pipeline on the test data and group those observations and then see

21:43.920 --> 21:48.760
how they overlap, there's many different ways to define this and some people have proposed

21:48.760 --> 21:53.520
this in the literature, but it's not even clear what metrics are to measure different

21:53.520 --> 21:57.520
types of reproducibility of different types of data driven discoveries.

21:57.520 --> 22:05.920
So I think one of the first research questions in this area or an open question would, if

22:05.920 --> 22:12.520
you do use the training and test set split, what are possible metrics to measure kind

22:12.520 --> 22:20.920
of the accuracy of, say, clustering results or pattern recognition results or interactions,

22:20.920 --> 22:24.120
important interactions between features or things like this.

22:24.120 --> 22:26.960
We have very clear metrics when it comes to prediction.

22:26.960 --> 22:29.840
We've got great stuff in the prediction realm.

22:29.840 --> 22:34.600
What about these kind of inference on data or data driven discovery realm?

22:34.600 --> 22:41.200
So you're not even trying to address this broader issue of reproducibility and kind of

22:41.200 --> 22:46.600
that disconnect between what you're specifically looking at and this kind of where I was taking

22:46.600 --> 22:52.760
it is leading me to ask, do we even have good names for like these different types of reproducibility

22:52.760 --> 22:53.760
issues yet?

22:53.760 --> 22:57.400
Yeah, that's actually a really great question.

22:57.400 --> 22:58.560
I don't think we do.

22:58.560 --> 23:05.800
I think we need to somehow expand our terminology because I don't know the right words to always

23:05.800 --> 23:07.880
talk about this.

23:07.880 --> 23:12.200
Everybody, when you say computational reproducibility, everybody knows what that is, that we should

23:12.200 --> 23:17.080
document our code and all of the steps so that you can fully computationally reproduce

23:17.080 --> 23:22.320
the results on your given training data set that you analyzed, right?

23:22.320 --> 23:24.960
We know what computational reproducibility is.

23:24.960 --> 23:28.880
We know what generalizability of prediction results are, right?

23:28.880 --> 23:31.040
We've studied that very well.

23:31.040 --> 23:36.480
I'm not really sure how to, I don't know that there's a good definition or good terminology

23:36.480 --> 23:43.160
to talk about how we generalize results for data driven discoveries or make sure, I like

23:43.160 --> 23:45.400
to call this, you know, make sure they're reproducible.

23:45.400 --> 23:48.600
Generalization is really making sure that the results are reproducible.

23:48.600 --> 23:55.040
All right, so some of the techniques that we do have for doing this, walk us through

23:55.040 --> 23:56.040
that.

23:56.040 --> 23:57.040
Absolutely.

23:57.040 --> 24:03.360
So, the first and easiest one, you can split your data into a training and test data split,

24:03.360 --> 24:07.680
and you can do kind of, you can build a pipeline.

24:07.680 --> 24:12.680
I like to say your data analysis pipeline needs to be automated in some way so that you

24:12.680 --> 24:20.000
can apply it without human interaction on your test data set and see, do the, quote, discoveries

24:20.000 --> 24:24.240
that I made or inferences that I've drawn from my training data set, are they the same

24:24.240 --> 24:25.640
on my test data set?

24:25.640 --> 24:27.240
So, that's one check.

24:27.240 --> 24:32.240
It's not a great check because you really only have one data set, and if they don't exactly

24:32.240 --> 24:36.280
match the question as well, is it because of the random split that I made in my training

24:36.280 --> 24:41.280
test data set or is there something wrong with my pipeline and my methods, or is the

24:41.280 --> 24:43.600
discovery not there?

24:43.600 --> 24:49.040
So you can repeatedly split the data into training and test data sets and do this over and

24:49.040 --> 24:50.040
over again.

24:50.040 --> 24:53.880
The challenge is you might get slightly different discoveries, right?

24:53.880 --> 24:58.600
If your goal is say, feature selection, every time you re-splip your data, you might select

24:58.600 --> 24:59.880
slightly different features.

24:59.880 --> 25:04.520
So in the end, what is the true discovery from this method?

25:04.520 --> 25:10.080
So, there has been some work on this, there are some challenges with that approach, but

25:10.080 --> 25:16.040
an approach that's an extension of this that I think is perhaps one of the easiest approaches

25:16.040 --> 25:22.520
for practitioners to apply right now is the stability principle.

25:22.520 --> 25:28.400
And the idea of the stability principle is very simple, and it's a stable discovery is

25:28.400 --> 25:31.280
likely to be a reproducible discovery.

25:31.280 --> 25:37.320
The idea is to take your training data set and randomize your training data set in a way

25:37.320 --> 25:41.440
that would mimic if you had future test data sets.

25:41.440 --> 25:47.760
So for example, if you have IID observations in a training data set, you can perhaps bootstrap

25:47.760 --> 25:51.800
your data or subsample your data as a specific example.

25:51.800 --> 25:57.760
In other situations, you might want to add a slight noise, maybe even the plus noise in

25:57.760 --> 25:59.320
certain situations.

25:59.320 --> 26:04.960
So you're randomizing your data and you do this repeatedly for say 500 a thousand times

26:04.960 --> 26:11.560
and you apply your data analysis pipeline to draw your inferences and make your discoveries

26:11.560 --> 26:14.400
on every one of the randomized data sets.

26:14.400 --> 26:21.200
And then you aggregate the results over all of the randomizations and the discoveries

26:21.200 --> 26:25.560
that are stable, meaning those discoveries that appeared in all of the different randomized

26:25.560 --> 26:31.880
data sets or a high proportion of them, are likely to be reproducible discoveries.

26:31.880 --> 26:40.960
So why is it that doing this type of analysis using randomized noise as opposed to randomly

26:40.960 --> 26:47.240
generating your trained test split, why is the former more efficient, or if efficiency

26:47.240 --> 26:51.720
is not the right word, why is it easier to do in practice?

26:51.720 --> 26:59.360
I think the training and test sets are certainly doable in practice.

26:59.360 --> 27:03.120
There's a challenge of how you aggregate all the results over all of those different

27:03.120 --> 27:06.160
training and test sets splits at the end.

27:06.160 --> 27:10.640
So maybe you split your data and every time you split your data, you get slightly different

27:10.640 --> 27:15.800
discoveries that have different generalization properties on the test data set.

27:15.800 --> 27:21.000
The stability principle, I actually think, takes that randomly splitting your data over

27:21.000 --> 27:22.000
and over again.

27:22.000 --> 27:27.240
And it's very similar because you can, for example, subset your data many times.

27:27.240 --> 27:31.640
You can just repeatedly, randomly sample two-thirds of your data over and over again.

27:31.640 --> 27:37.920
It's very similar to data splitting, but the goal is just to aggregate all of the discoveries

27:37.920 --> 27:42.640
you made over the different randomization, which ones kind of always appeared.

27:42.640 --> 27:47.880
It's kind of a robustness measure that if we kind of randomize and shake up our data

27:47.880 --> 27:52.760
over and over again and the discovery, whatever that discovery is, if it's always there,

27:52.760 --> 27:57.560
it's probably got a pretty strong signal and it's probably going to be generalizable.

27:57.560 --> 28:01.600
I still don't understand why you can't do that with the data splitting.

28:01.600 --> 28:10.440
Like in both cases, you ultimately have a bunch of test and train or a bunch of train

28:10.440 --> 28:15.320
data sets and you're training a model and then you're looking at the features and you're

28:15.320 --> 28:20.680
trying in the case of stability principle, you're looking at the features and looking

28:20.680 --> 28:27.680
at maybe taking a majority or which one, which features rank highest on the importance

28:27.680 --> 28:29.600
over a bunch of different runs.

28:29.600 --> 28:32.640
Why couldn't you do the same thing on the data splitting side?

28:32.640 --> 28:34.800
You absolutely can.

28:34.800 --> 28:41.640
So absolutely and again, subsampling for stability principle is essentially the same as data

28:41.640 --> 28:42.640
splitting.

28:42.640 --> 28:47.920
Yeah, so they're essentially the same thing if you're aggregating the results over them.

28:47.920 --> 28:54.040
The challenge with data splitting is again, a matter of, do we have a metric for when

28:54.040 --> 28:57.080
we apply our discovery technique to the test data set?

28:57.080 --> 29:03.240
Do we have a metric to understand whether it's a good discovery or it's generalizable or

29:03.240 --> 29:08.080
not and we don't, like we do for prediction error if our goal is prediction.

29:08.080 --> 29:13.240
But with the stability principle, we do have firmer metrics.

29:13.240 --> 29:17.680
I'm not sure they're firmer metrics, it's just an indication if you aggregate overall

29:17.680 --> 29:22.560
the different randomizations, you just, it's some measure of kind of the uncertainty in

29:22.560 --> 29:25.800
your discovery or the robustness of your discovery.

29:25.800 --> 29:31.360
So I'm not sure it's a, I mean, you can always come up with metrics, you could probably

29:31.360 --> 29:33.360
do the same for data splitting.

29:33.360 --> 29:36.440
So both of these techniques are very similar to each other.

29:36.440 --> 29:37.440
Okay.

29:37.440 --> 29:38.440
Yeah.

29:38.440 --> 29:44.960
So there are, I did want to highlight, there are several other techniques that some machine

29:44.960 --> 29:50.320
learning and statistical researchers have started to look at, but are potentially really

29:50.320 --> 29:53.080
interesting directions to go.

29:53.080 --> 29:56.520
And the first is an area called post-selection inference.

29:56.520 --> 30:02.920
And the goal here is to use your training data set to make some type of discovery for

30:02.920 --> 30:05.640
some type of machine learning procedure.

30:05.640 --> 30:12.520
And then say, can you do classical statistical inference on that discovery to say, you know,

30:12.520 --> 30:13.520
whether it's true or not?

30:13.520 --> 30:15.240
I say classical statistical inference.

30:15.240 --> 30:19.320
I mean, you know, hypothesis testing, confidence regions and so forth.

30:19.320 --> 30:26.560
And the idea behind this approach is that if we say, mine a big data set using feature

30:26.560 --> 30:31.320
selection, clustering, pattern recognition, any, any kind of techniques we would use

30:31.320 --> 30:37.640
to analyze a big data set, we're actually exploring that data set to find our model,

30:37.640 --> 30:39.160
to find our hypothesis.

30:39.160 --> 30:44.680
So it's essentially an exploratory analysis situation in which we don't have an a priori

30:44.680 --> 30:47.400
defined hypothesis in advance.

30:47.400 --> 30:51.960
We let our machine learning method find our hypothesis for us.

30:51.960 --> 30:58.120
And then since our machine learning method found our hypothesis, it's a big challenge to

30:58.120 --> 31:02.880
say, then say, now that we've used machine learning to find the hypothesis, we now want

31:02.880 --> 31:08.120
to conduct traditional statistical inference on this hypothesis.

31:08.120 --> 31:12.760
The challenge is all of the classical inference techniques that we know and love don't quite

31:12.760 --> 31:20.160
apply because they assume that you have an a priori hypothesis in advance and that data

31:20.160 --> 31:25.000
was collected specifically to address that a priori hypothesis.

31:25.000 --> 31:28.720
So new techniques need to be developed in this area.

31:28.720 --> 31:34.800
And so far, there have been some techniques developed for feature selection in this area

31:34.800 --> 31:41.960
that say allow you to apply a feature selection method to a training data set and on the same

31:41.960 --> 31:48.760
training data set, then say test the coefficients from a model of the features that you selected.

31:48.760 --> 31:53.160
So this is an area again called post-selection inference and it's potentially exciting

31:53.160 --> 32:01.440
because it's especially exciting for scientific researchers because in science to get published

32:01.440 --> 32:06.160
in a lot of the big journals, you definitely need p-values and confidence regions.

32:06.160 --> 32:07.600
Those are very important.

32:07.600 --> 32:14.360
And so I do see this as a potential potentially big contribution to science that kind of

32:14.360 --> 32:18.960
marry traditional statistical inference with machine learning methods.

32:18.960 --> 32:25.480
You made a comment about features of your training data as opposed to features of your model.

32:25.480 --> 32:26.800
Can you elaborate on that?

32:26.800 --> 32:27.800
Yeah.

32:27.800 --> 32:33.920
And in the post-selection inference paradigm, the goal would be to not necessarily need

32:33.920 --> 32:37.600
to have a test, a training and test set split.

32:37.600 --> 32:45.600
But instead, use your training data just as both the data for which you're going to find

32:45.600 --> 32:51.080
your model or select your model, meaning you're going to find the hypothesis you want to

32:51.080 --> 32:55.280
test and then test that hypothesis on the same data.

32:55.280 --> 33:01.120
So the advantage of this approach over say stability and data splitting is it tends to

33:01.120 --> 33:07.480
be more efficient in terms of using the available data and using the available samples.

33:07.480 --> 33:12.880
The downside is the math is a bit harder, but that gives math and stats people plenty

33:12.880 --> 33:14.080
to do.

33:14.080 --> 33:20.240
And so is there something about the way that the math is done that you can elaborate on

33:20.240 --> 33:27.400
that gets us beyond like when we typically think about using the same data for training

33:27.400 --> 33:28.400
and evaluation.

33:28.400 --> 33:33.920
It's kind of double dipping and you tend to have overfitting problems.

33:33.920 --> 33:37.680
What about this particular technique allows us to avoid that?

33:37.680 --> 33:38.680
Yeah.

33:38.680 --> 33:47.360
So a core part of this technique says, can you take say whatever procedure you applied?

33:47.360 --> 33:54.480
And you have some statistical tests that you want to conduct and you have a test statistic.

33:54.480 --> 34:00.280
The question is can you take that test statistic and can you decompose it into the part of

34:00.280 --> 34:07.520
the test statistic that is depending on your machine learning method or your method you

34:07.520 --> 34:15.000
use to select your model or select your hypothesis and the part that's not dependent on the machine

34:15.000 --> 34:16.800
learning method that you use.

34:16.800 --> 34:23.160
And then you condition on, you basically conduct conditional inference.

34:23.160 --> 34:30.360
So you basically say, can I derive the null distribution of my test statistic conditional

34:30.360 --> 34:33.960
on the results of the machine learning procedure?

34:33.960 --> 34:39.080
So that's essentially what's going on behind the scenes and you need to calculate these

34:39.080 --> 34:44.760
conditional distributions, sometimes conditional and very complicated machine learning procedures

34:44.760 --> 34:47.480
which is the mathematical challenge.

34:47.480 --> 34:52.160
And so when you talk about this mathematical challenge, are we talking about computational

34:52.160 --> 34:54.960
challenge or an analytical challenge?

34:54.960 --> 34:56.360
More analytical challenge.

34:56.360 --> 34:57.360
Okay.

34:57.360 --> 35:06.080
So that kind of suggests to me that this particular approach is maybe harder to scale across

35:06.080 --> 35:10.240
a variety of different problems or it may be even a variety of different data sets, is

35:10.240 --> 35:11.240
that case?

35:11.240 --> 35:12.240
Yeah.

35:12.240 --> 35:17.240
So you're exactly right here, stability and data splitting, the first two I talked about

35:17.240 --> 35:21.080
are pretty generalizable to any situation.

35:21.080 --> 35:26.680
This post-selection inference paradigm, so far is only applicable in limited settings

35:26.680 --> 35:31.000
and it might, it's not even clear that you're going to be able to generalize this to say

35:31.000 --> 35:38.120
all machine learning methods, that's currently unclear, but it is an active area of research.

35:38.120 --> 35:44.480
And so maybe going back to where we started your talk and the reaction to your talk, what

35:44.480 --> 35:50.720
did all that say to you about where kind of your community, the community that you are

35:50.720 --> 35:58.880
speaking to of science as opposed to machine learning practitioners, where they are in terms

35:58.880 --> 36:06.160
of their awareness of these issues and where they need to be?

36:06.160 --> 36:15.760
Yeah, so it seems like there are sometimes machine learning researchers, there's a lot

36:15.760 --> 36:22.840
of great techniques being developed that are really cool, that can do just super fascinating

36:22.840 --> 36:24.800
things with data.

36:24.800 --> 36:30.600
At the same time, sometimes the way machine learning is used by practitioners is very

36:30.600 --> 36:35.400
different from the latest and state of the art machine learning techniques that are being

36:35.400 --> 36:38.200
developed by machine learning researchers.

36:38.200 --> 36:44.960
And I think there is a bit of a disconnect between how kind of machine learning researchers

36:44.960 --> 36:49.520
view how machine learning should be used and what it should be used for and how people

36:49.520 --> 36:55.000
use it in practice and particularly how scientists are using machine learning techniques to

36:55.000 --> 37:00.600
make inferences and discoveries from their very large complex data sets.

37:00.600 --> 37:04.960
And so I do think it's important to pay attention to how people are using machine learning

37:04.960 --> 37:11.440
techniques and science and particularly and part of my goal in my research is how can

37:11.440 --> 37:19.000
we improve machine learning techniques to make them easier for scientists to use or describe

37:19.000 --> 37:24.080
kind of underlying principles to scientists in very clear terms that they can understand.

37:24.080 --> 37:29.120
For example, scientists are now very aware and practitioners of machine learning all

37:29.120 --> 37:33.080
know that if you're doing prediction, you have to have a separate test set.

37:33.080 --> 37:37.960
It's not okay to just train a model on a training data set and report your training error.

37:37.960 --> 37:42.320
So that's very well understood amongst practitioners.

37:42.320 --> 37:46.920
I think we can definitely do a better job when it comes to inference and data driven discovery

37:46.920 --> 37:55.640
on perhaps developing the best ways to go about this and also making sure that those methods

37:55.640 --> 38:02.600
get turned into practical tools that everybody can use to help analyze their data.

38:02.600 --> 38:10.160
So a lot of the work that's happening in the broader machine learning community and maybe

38:10.160 --> 38:17.400
this is just the kind of with a commercial lens on, but it's focused on, I guess not

38:17.400 --> 38:23.840
even commercial, you know, the open source community, things like TensorFlow and PyTorge

38:23.840 --> 38:29.080
on the deep learning side and the toolkits like psychic learning Python, like a lot of

38:29.080 --> 38:36.640
this activity is, you know, we talk about this idea of democratizing machine learning,

38:36.640 --> 38:38.600
like making it more accessible.

38:38.600 --> 38:42.920
Do you're finding suggests that this is wrong headed or, you know, we're not ready for

38:42.920 --> 38:43.920
that yet?

38:43.920 --> 38:44.920
Oh, no.

38:44.920 --> 38:45.920
I mean, that's fabulous.

38:45.920 --> 38:51.760
That's what we should do with research is make it open source and everybody have access

38:51.760 --> 38:52.760
to it.

38:52.760 --> 38:59.120
At the same time, I do think it's important that we're educating users and practitioners

38:59.120 --> 39:03.640
about how best to apply these techniques in practice and what are really, what are

39:03.640 --> 39:09.880
good underlying principles and simple principles that they can use and follow to make sure they're

39:09.880 --> 39:12.600
getting robust discoveries.

39:12.600 --> 39:21.320
And again, I think in a lot of comments too in the news articles following my talk alluded

39:21.320 --> 39:26.720
to this, the problem isn't necessarily that the machine learning techniques are bad

39:26.720 --> 39:29.560
or not correct and it's not at all.

39:29.560 --> 39:31.960
The machine learning techniques are great.

39:31.960 --> 39:33.960
It's often in how they're applied.

39:33.960 --> 39:40.960
And so I do think we have a responsibility to think carefully about how we're educating

39:40.960 --> 39:45.600
people on how to apply these techniques and what the principles are for doing so.

39:45.600 --> 39:46.600
Got it.

39:46.600 --> 39:54.080
So you see the first and foremost, there's a problem of education in particular around

39:54.080 --> 39:57.680
the appropriateness of these techniques.

39:57.680 --> 39:58.680
Yeah.

39:58.680 --> 40:03.280
And it was like you said earlier, you asked, you know, is there even a terminology to talk

40:03.280 --> 40:07.760
about reproducibility when it comes to inference or data to other discoveries?

40:07.760 --> 40:09.760
And I don't think there is.

40:09.760 --> 40:13.040
We don't even have terms to necessarily talk about this yet.

40:13.040 --> 40:19.280
So how can we expect practitioners and users to understand how to appropriately apply

40:19.280 --> 40:21.800
these techniques in these situations?

40:21.800 --> 40:27.600
So I would love to see more machine learning and statistical researchers get into this

40:27.600 --> 40:34.840
area and hash out what's a robust terminology to talk about this, what's the correct mathematical

40:34.840 --> 40:39.440
paradigm to write down for these types of problems and let's start studying them and

40:39.440 --> 40:45.120
making sure we get the word out to practitioners to use the machine learning techniques appropriately

40:45.120 --> 40:51.560
so that they can do more generalizable discoveries and hence do better science.

40:51.560 --> 40:52.560
Awesome.

40:52.560 --> 40:53.560
Awesome.

40:53.560 --> 40:57.960
Well, Jennifer, thank you so much for taking the time to chat with us about this really

40:57.960 --> 41:03.720
interesting work and conversation and I appreciated, you know, I obviously struggled with that

41:03.720 --> 41:08.920
terminology myself in trying to kind of narrow in on what we were talking about here.

41:08.920 --> 41:14.400
But I think there is definitely a lot of work to be done in this space and thank you for

41:14.400 --> 41:15.400
doing it.

41:15.400 --> 41:21.400
Well, thank you very much for having me on.

41:21.400 --> 41:24.280
Alright everyone, that's our show for today.

41:24.280 --> 41:28.960
If you like what you've heard here, please do us a huge favor and tell your friends about

41:28.960 --> 41:30.120
the show.

41:30.120 --> 41:34.200
And if you haven't already hit that subscribe button yourself, make sure you do so you

41:34.200 --> 41:38.200
don't miss any of the great episodes we've got in store for you.

41:38.200 --> 41:41.520
As always, thanks so much for listening and catch you next time.

