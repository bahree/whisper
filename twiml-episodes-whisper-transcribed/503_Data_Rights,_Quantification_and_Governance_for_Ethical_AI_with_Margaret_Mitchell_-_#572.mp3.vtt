WEBVTT

00:00.000 --> 00:05.920
All right everyone welcome to another episode of the Twemble AI podcast. I'm your host Sam

00:05.920 --> 00:12.320
Charrington and today I'm joined by Meg Mitchell, a chief ethics scientist and researcher at Hugging

00:12.320 --> 00:17.280
Face. Before we get into today's conversation be sure to take a moment to head over to Apple

00:17.280 --> 00:23.120
podcasts or your listening platform of choice and if you enjoy the show please leave us a five-star

00:23.120 --> 00:29.200
rating and review. Find us just over a year ago with a friend of the show, another friend of the

00:29.200 --> 00:36.000
show I should say Emily Bender. That was episode 467 for those keeping track where we discussed

00:36.000 --> 00:44.560
your paper on the dangers of stochastic parrots and of course a ton has changed for you since then.

00:45.520 --> 00:52.320
Welcome back to the podcast Meg. How's everything going? You thank you. Yeah it's great to be here.

00:52.320 --> 00:59.120
I should say that the paper wasn't on the dangers of stochastic parrots. It was on the dangers

00:59.120 --> 01:05.680
of large language models comparing the two stochastic. We don't have a fear of parrots.

01:05.680 --> 01:13.840
So the title on the ACM library is on the dangers of stochastic parrots can language models be too big?

01:14.560 --> 01:19.680
I feel like for this one we should just leave Amari as the voice of God coming in and

01:19.680 --> 01:33.760
make us what the title is paper is. In any case Meg welcome back and let's have you reintroduce

01:33.760 --> 01:42.880
yourself to the audience. Yeah so I am a computer scientist with a background in natural language

01:42.880 --> 01:49.760
processing as well as a bit in computer vision and then for the past six or so years I've been

01:49.760 --> 01:56.480
working on ethical AI which brings in a lot of things about natural language processing and

01:56.480 --> 02:04.480
computer vision in terms of issues like foreseeable harms and misuse and things like that.

02:04.480 --> 02:14.640
And I've worked at Microsoft Research, Google, Google Brain and then recently joined hugging

02:14.640 --> 02:22.640
face full-time which is a startup that is an open source community making machine learning

02:22.640 --> 02:30.240
models and data sets generally available. Awesome awesome and are you the first chief ethics

02:30.240 --> 02:36.640
scientist at the company? Well we're a startup so we there aren't exactly

02:38.400 --> 02:45.520
very set in stone titles. You can kind of start coming up with your own titles for things.

02:45.520 --> 02:52.880
I was definitely the first AI ethics computer scientist. I'm excited to share that we're going

02:52.880 --> 03:01.120
to hire a for real ethicist which will be amazing. It's pretty rare to be able to hire an ethicist

03:01.120 --> 03:07.360
in an AI company. I think that that is something that only came into people's attention very recently.

03:07.360 --> 03:13.040
But yeah I mean I came to work at hugging face because I really wanted to have some time to

03:13.040 --> 03:20.000
just code and code open source. At the larger tech companies you find as you sort of rise higher

03:20.000 --> 03:25.440
and higher in the hierarchy you have less and less time to code and it's replaced more and more

03:25.440 --> 03:33.440
by meetings which is sort of the opposite of what you want as you succeed right. Like if you love

03:33.440 --> 03:40.400
coding and you succeed more why would that be taken away in favor of meetings which I absolutely

03:40.400 --> 03:47.040
hate. So after having you know a couple years where basically it was really hard for me to get

03:47.040 --> 03:52.960
coding time in. I really wanted to join hugging face and just sort of not worry about tons of

03:52.960 --> 03:58.880
meetings not worry about tons of sinking across a large organization because it was already small.

03:58.880 --> 04:03.360
I could really just you know get my hands dirty with code and starting to put out some tools

04:03.360 --> 04:09.040
that I really really wanted to prioritize. And then once I got that out of my system a little bit

04:09.760 --> 04:15.360
I was able to start popping up a little more and working more on things like defining the culture

04:15.360 --> 04:22.240
hiring practices, different kinds of processes that we want to put in place for consideration of

04:22.240 --> 04:27.600
what models get shared, what data get shared and things like that. I've been doing a little less

04:27.600 --> 04:35.200
coding but it's still a coding company essentially. So getting to do a mix of defining top down

04:35.920 --> 04:41.040
structures and processes in the company focusing on like diversity and inclusion and ethical

04:41.040 --> 04:47.520
considerations as well as more bottom up just you know pure coding to share with the world.

04:47.520 --> 04:55.040
So it's a nice mix. Got it got it. And the the coding that you're doing is that

04:55.040 --> 05:04.800
been largely focused or tied into the area of ethics or is are they two separate kind of things

05:04.800 --> 05:11.600
that you find ways to connect. Yeah so I would say that all the coding that I'm prioritizing right

05:11.600 --> 05:18.720
now is intimately tied to current discussions around AI ethics. So you'll see that over the past

05:18.720 --> 05:26.320
year so there's been more and more scholarship in the AI ethics world about data and data sets and

05:26.320 --> 05:34.960
things like benchmarking and the fact that the culture of machine learning has generally had

05:34.960 --> 05:41.920
a really laissez-faire approach to data collection and data sharing. But if we're trying to think about

05:42.640 --> 05:49.360
the long-term effects of technology and the long-term effects of things like sharing data

05:50.000 --> 05:54.880
then we need to think very critically about things like data curation instead and what does

05:54.880 --> 06:01.280
data curation mean? It means things like what kind of values do we want to encode in the data

06:01.280 --> 06:07.840
as well as in the data set development process in order to create data sets that foreseeably

06:07.840 --> 06:13.200
can help more than hurt. And so that means things like coming up with measurements,

06:13.200 --> 06:21.040
quantifications of the data that can be inspired by human values. So there's been some work on this

06:21.040 --> 06:27.440
over the past year as well, but I'm trying to really step that up a bit. And so what does that

06:27.440 --> 06:34.000
mean? That means coming up with metrics for the demographic diversity within a data set,

06:34.000 --> 06:39.280
coming up with metrics for stereotyping. How much a data set might run the risk of propagating

06:39.280 --> 06:46.080
harmful stereotypes? Doing things like measuring the naturalness of the language, how contrived it is

06:46.080 --> 06:53.840
versus how much it actually reflects the kind of language that we'd expect to see when a model

06:53.840 --> 06:59.600
trained on it is actually used, that kind of thing. So data development has been a large focus of

06:59.600 --> 07:06.960
mine and particularly through the lens of what can we measure aligned to human values and what we

07:06.960 --> 07:16.320
wanted to develop. Awesome, awesome. I also want to bring up your participation in the Wiki M3L workshop

07:16.320 --> 07:22.480
at iClear. Can you tell us a little bit about that workshop that goes with the workshop and

07:22.480 --> 07:30.240
what you're speaking about there? Yeah, so it's an interdisciplinary workshop that's focusing

07:30.240 --> 07:36.800
on a lot of different topics. I think part of the idea was essentially to bring together people

07:36.800 --> 07:44.480
who are working on different aspects of AI and how those aspects can be really relevant to the

07:44.480 --> 07:52.400
community. So this is joint with Wiki Media and Wiki Media is really passionate about having

07:52.400 --> 08:00.240
everything be open and transparent and so what that means for computer vision and LP,

08:00.240 --> 08:04.960
machine learning, what we should make available, what we shouldn't, what that means in terms of

08:04.960 --> 08:10.240
the uses of the models that we're developing and then how they might actually be used.

08:12.000 --> 08:17.120
All of that is sort of under the umbrella of this workshop. So touching on a lot of different

08:17.120 --> 08:26.000
issues relevant to the socio-technical context of machine learning. And your particular session

08:26.000 --> 08:33.040
at the workshop? So I get to be involved into one is a panel. The panel is looking at

08:34.000 --> 08:40.240
multi-modality which is an area I've worked on to do things like image descriptions for people

08:40.240 --> 08:49.200
who are blind. And then also there's a keynote I'm giving on biases in AI. The session is called

08:49.200 --> 08:56.400
biases in AI and indigenous data sovereignty. So there's another speaker. I believe his name is

08:56.400 --> 09:03.600
Michael Running Wolf. Awesome. And I should mention I mentioned the workshop by its short name Wiki

09:03.600 --> 09:13.040
M3L but that is Wikipedia and multimodal and multilingual research. And so your conversation

09:13.040 --> 09:24.160
at the and presentation at the workshop focus on data governance and biases in AI that sounds

09:24.160 --> 09:29.680
not very far field from the data curation work that we talked about about earlier. What's the

09:29.680 --> 09:36.720
specific angle that you're taking at the workshop? So my plan so far subject to change

09:38.480 --> 09:47.520
still still chatting is to talk about the kinds of assumptions and values that get encoded

09:47.520 --> 09:53.440
when we are creating and sharing data sets. So there's a lot of things you can talk about when

09:53.440 --> 10:00.080
you say biases in AI that means tons of different things. But my particular interest I think

10:00.080 --> 10:08.240
relevant to this workshop is around how we have inclusive sharing of data sets and creation of

10:08.240 --> 10:17.360
models that don't disproportionately underperform on some sub-populations or sort of exploit some

10:17.360 --> 10:23.680
populations in the data collection practices. So this has a little bit more to do with biases

10:23.680 --> 10:30.240
from the perspective of how we're approaching data set collection, what are sort of internal

10:30.240 --> 10:37.200
cognitive biases and cultural biases are doing in our approach to data and the kind of models we

10:37.200 --> 10:42.640
develop as opposed to something that's I think more traditional bias than AI where people think

10:42.640 --> 10:48.560
of things like fairness. I won't be focusing on fairness. I'll be talking more about the context

10:48.560 --> 10:56.240
of of data and how they relate to models and advocating for things like data sovereignty

10:56.960 --> 11:04.560
which is the other part of that session where individuals and communities who are creating the

11:04.560 --> 11:12.560
data have rights for that data as opposed to the current state of the art which is a set

11:12.560 --> 11:20.640
essentially being exploited without consent to have their data used in models that are then

11:20.640 --> 11:26.560
productionized for profit and things like that. So focusing more on the on the data sovereignty side

11:26.560 --> 11:36.160
of things. On the biases side of things it sounds like what you're alluding to is this idea that

11:36.160 --> 11:43.520
as we as practitioners, researchers, a community got and collect these data sets, there are inherent

11:43.520 --> 11:50.560
ways that we think about the world that are influencing the ultimate results of these data sets.

11:50.560 --> 11:55.120
Can you talk a little bit more about the specific examples and how that comes to be?

11:55.120 --> 12:01.680
This is also really relevant to work I've been doing in big science which is this massive effort

12:01.680 --> 12:07.920
international effort with volunteers from a bunch of different countries working on training

12:07.920 --> 12:13.360
this large language model. And I'll just jump in to suggest the folks that want to learn more about

12:13.360 --> 12:20.400
that can check out my recent interview with your colleague Thomas Wolff that's episode 564.

12:22.080 --> 12:27.360
And so in that I'm unsurprisingly working on data as well but this is

12:27.360 --> 12:34.000
data is more and more my thing I think in part because it's generally been less appreciated than

12:34.960 --> 12:42.480
ML work. I mean data collection practices have morphed over the years. You'll see in some of the

12:42.480 --> 12:50.240
first larger data sets that that were developed for use in what was at that time called corpus

12:50.240 --> 12:59.520
linguistics were very carefully balanced made sure to pay attention to the licenses and rights

12:59.520 --> 13:04.240
of the various people who created the data. So we see things like like the brown corpus

13:05.280 --> 13:11.920
coming around in the early 60s actually taking a look at things like fiction and sports and travel

13:11.920 --> 13:15.600
and all these sort of different topics and trying to be really balanced and making sure that they

13:15.600 --> 13:20.480
have agreement from the holders of the data that this is okay to use and distribute things like that.

13:21.680 --> 13:28.000
You know fast forward into the 90s where we're starting to see a switch from corpus linguistics

13:28.000 --> 13:35.120
to computational linguistics. I mean there's still separate fields but there's more and more

13:36.000 --> 13:41.200
changing at that time in the kinds of work that corpus linguists are doing more towards computational

13:41.200 --> 13:47.920
linguistics and what that means is that you start trying to get bigger and bigger data sets

13:48.480 --> 13:55.760
where the size matters a lot more than the quality. So quantity over quality sort of thing and then

13:56.640 --> 14:02.800
then moving into oh I should say but at that time there was still a lot of respect for the rights

14:02.800 --> 14:09.920
and licenses and so a lot of the data collected in the 90s was from newswire because those were

14:09.920 --> 14:18.720
the sources that would allow this public use into the 2000s as you start to have web 2.0 basically

14:18.720 --> 14:26.320
you start to see a lot of content produced online from individuals reflecting more natural language

14:26.320 --> 14:32.720
right so when we work on natural language processing newswire is just some some sort of contrived

14:32.720 --> 14:39.280
kind of way of talking you know it has like a lot of norms you have to abide by whereas regular

14:39.280 --> 14:46.320
conversation or what we're writing online is a lot more reflective of sort of everyday language use

14:47.200 --> 14:52.480
so with kind of the birth of web 2.0 you see people collecting data from the web

14:53.920 --> 15:00.880
sort of mining it to create larger data sets to build models on with less and less attention paid

15:00.880 --> 15:08.240
to what those data sets actually were representing and more attention paid to what can I get that's

15:08.240 --> 15:14.560
relevant to the topic or task that I care about and so with that you see a little bit more of a

15:14.560 --> 15:21.840
loss of rights this also relates to the fact that when we communicate online our data can be

15:21.840 --> 15:28.640
technically owned by the platform right so when we tweet that data is owned by twitter

15:30.480 --> 15:36.320
you know things like that and so even people who are creating really relevant data

15:36.320 --> 15:44.080
and this can be called data labor you know essentially creating the content themselves they don't

15:44.080 --> 15:50.880
end up having as many rights to that actual data where other companies can use that data

15:50.880 --> 15:58.320
it could be further shared to help to help build other sort of models or to help other companies

15:58.320 --> 16:04.160
without the people actually creating the data's without the knowledge of the people actually

16:04.160 --> 16:11.440
creating that data and then even when you are putting in place some things that clearly has

16:12.720 --> 16:20.640
the intention of rights so even when you are putting on things like creative comments licenses

16:21.280 --> 16:30.880
that kind of thing it doesn't actually map to how your data can legally be used by

16:30.880 --> 16:40.320
by anyone at least in practice right so let me jump in and say what I'm hearing is you know this

16:40.320 --> 16:45.760
kind of field of play around creating these large language models and other kind of large models

16:45.760 --> 16:52.160
has evolved quickly and you know there's a lot of rules that haven't been established or conversely

16:52.160 --> 16:58.800
norms that aren't necessarily you know universally beneficial have established I'd love to have you

16:58.800 --> 17:10.080
talk about the specific harms that you see at play like clearly there's this idea of economic

17:10.080 --> 17:17.760
harm in a sense of people are contributing this data other companies are monetizing it in ways

17:17.760 --> 17:23.280
that weren't necessarily disclosed to the people that contributed the data and the benefit isn't

17:23.280 --> 17:28.720
being shared and you could you know argue whether that's theoretical harm or actual harm but

17:29.600 --> 17:36.480
there's this idea of an economic harm there's idea this idea of I should have the right to give

17:36.480 --> 17:41.840
you permission or not I don't know how to phrase what the right phrasing would be for the harm that

17:41.840 --> 17:48.000
is involved in that but those are a couple of examples of harms and I just wanted to to have you

17:48.000 --> 17:53.840
elaborate on that and further develop you know is there a taxonomy of harms that has arisen from

17:53.840 --> 18:00.240
the way that we're collecting these datasets yeah so there there has been work on on taxonomy

18:00.240 --> 18:07.440
of harms I think people in in different groups have come up with their own there isn't as far as I

18:07.440 --> 18:15.920
know a generally agreed upon kind of taxonomy but one of them one of the key issues is around privacy

18:15.920 --> 18:26.560
so when we share content online we have a basic understanding or trust that what we're producing

18:26.560 --> 18:35.200
is not going to not going to be suddenly on some graduate students laptop or something like that

18:35.200 --> 18:43.600
so if you have information about your personal life that can be used to track you if I mean there's

18:43.600 --> 18:50.000
also things like personally identifying information so with just a few characteristics of an

18:50.000 --> 18:55.840
individual you can often identify who they are which can give rise to things like stocking to

18:55.840 --> 19:03.280
things like identity theft as well as just sort of the very the very personal issue of having things

19:03.280 --> 19:11.280
you say be used by other people without you really knowing and potentially even use for technology

19:11.280 --> 19:16.960
that you don't want to support I think Flickr is a really great example where it's been mined for

19:16.960 --> 19:24.960
years for computer vision applications and so some of that some of that data is used for things

19:24.960 --> 19:31.840
like facial recognition but theoretically and I think you know in reality a lot of people whose

19:31.840 --> 19:37.200
faces were mined don't actually want to help build facial recognition and so there's a disconnect

19:37.200 --> 19:43.920
there between what people's intent and expectations are when they're creating the data and how that

19:43.920 --> 19:49.520
data is actually being used in a way where it could potentially even come back and harm them so

19:49.520 --> 19:56.000
for example facial recognition does not work as well for black people that as white people there's

19:56.000 --> 20:01.440
an argument to be made that maybe white people don't want to have their their faces shared as much

20:01.440 --> 20:08.160
if it's going to create that kind of issue there's a concern that facial recognition is used as

20:08.160 --> 20:14.320
a form of discrimination so you know black people tend to be more targeted and so black people who

20:14.320 --> 20:20.240
have their data as part of what's using what is being used to train a facial recognition system

20:21.200 --> 20:26.560
is really not a situation that they ever hoped to be in where it can come back and harm them if

20:26.560 --> 20:33.680
they're targeted so it ends up creating these these problematic kinds of cycles where the beliefs

20:33.680 --> 20:41.600
and intentions of people who are creating the data are not actually met by how different

20:41.600 --> 20:48.720
companies and organizations end up using that data potentially to disrupt their privacy

20:49.840 --> 20:55.280
as well as as well as things like creating technology that could come back to hurt them

20:55.280 --> 21:03.600
time back to your earlier comment about connecting the inherent beliefs of the data set

21:03.600 --> 21:12.960
collectors to the data sets and these these problems is it you know by the inherent beliefs of

21:12.960 --> 21:17.680
the the folks collecting data sets you know are you meeting things like disregard for

21:18.400 --> 21:22.800
you know the the privacy considerations or are there specific more specific things that

21:22.800 --> 21:28.640
you're trying to get out there yeah disregard for but also private information that that can be

21:28.640 --> 21:36.480
used to identify people so you know if you have bad actors malicious actors your data can potentially

21:36.480 --> 21:43.360
be mined to steal your social security number credit card number these are the sorts of things

21:43.360 --> 21:50.240
that end up being stolen all the time as well as you know figuring out personal information about

21:50.240 --> 21:57.280
you which is a kind of fundamental harm when it comes to psychological safety and so when you

21:57.280 --> 22:05.200
think about where we are from a data rights perspective what's the kind of frontier the research

22:05.200 --> 22:14.000
or frontier of the practice is it primarily what we're seeing around you know legislative and

22:14.000 --> 22:21.520
regulatory action or you know how do you think about the state of the world yeah so there has been

22:21.520 --> 22:27.920
a lot more work in the regulatory space on data protection data protection laws I think most

22:27.920 --> 22:35.200
people are familiar with GDPR but a lot of countries very recently have put forward some other

22:35.200 --> 22:43.520
sorts of data protection legislation and and one of the ideas underlying this is that if people

22:43.520 --> 22:49.840
create the data they own the data and that really takes away the profit models of companies like

22:49.840 --> 22:56.640
you know Google that uses this information in order to essentially get people to look at ads more

22:57.600 --> 23:01.920
so you know if they don't get to own that data then you know that opens up

23:02.960 --> 23:08.400
tons of other companies that that can use it or also or it also takes away their ability to use

23:08.400 --> 23:20.320
that data at all and so right and so you see that there is there are ideas around people needing

23:20.320 --> 23:26.720
to consent to have their data being used and so particularly in China's new legislation around

23:26.720 --> 23:34.400
data protection there's this idea that if you have an instance from an individual and that

23:34.400 --> 23:40.640
individual is an anyway identifiable you have to go and find that person and ask for their

23:40.640 --> 23:47.920
consent to use that data so you see a little bit of individual data rights starting to pop up

23:49.040 --> 23:55.920
that said you know geopolitical entities like the EU are also trying to put together things

23:55.920 --> 24:02.320
like data storehouses where that data is open to everyone so it's a little bit of the reverse

24:02.320 --> 24:11.200
where maybe you don't have have rights to it and instead the the ability to use that data can

24:11.200 --> 24:16.960
be distributed amongst everyone in the country in order to you know maximize the overall profit

24:16.960 --> 24:22.720
in the geopolitical entity and so there is a little bit of a push and pull I think right now where

24:22.720 --> 24:29.360
you see some legislation moving in the direction of individuals having some rights over their data

24:29.360 --> 24:37.360
at the same time as you see some legislation coming out that that sort of proposes that an

24:37.360 --> 24:44.240
individual's data should be open for everyone to use rather than an individual's company so I

24:44.240 --> 24:48.240
think there's there's a lot of discussions right now in which way these things should be going

24:49.600 --> 24:54.960
you know I'm obviously personally a proponent of the idea that everybody should own the data

24:54.960 --> 24:59.280
that they create and then they should be able to consent to whether or not it's it's shared more

24:59.280 --> 25:06.240
broadly but you know I might be in the minority there but all the more reason to sort of talk on

25:06.240 --> 25:13.680
these kinds of programs and present at that places like the the wiki m3l workshop I hadn't heard

25:13.680 --> 25:22.560
about the data privacy regulation in China that you referenced and some ways it flies a little bit

25:22.560 --> 25:28.080
in the face of the broader narrative about privacy in China the social credit score on all these

25:28.080 --> 25:33.920
things yeah do you have any additional perspective there I'm really confused by that actually

25:36.640 --> 25:43.760
so I read through I read through their legislation you know not not being a lawyer and you know

25:43.760 --> 25:48.400
sort of ask people who are more legal scholars if my interpretation of what it was saying was

25:48.400 --> 25:54.560
right and it seems like it is right and so I'm really not sure what's going on there I feel like

25:54.560 --> 26:00.880
it might be someone it might be the job of someone who's more knowledgeable about about China and

26:00.880 --> 26:07.200
you know China's government and and what they're trying to sort of get out there I'm really yeah

26:07.840 --> 26:13.200
I'm I'm very confused by it personally it'd be sort of ironic I suppose if China leads the way

26:13.200 --> 26:19.120
an individual data rights but we might be moving in that direction right now well if anyone knows

26:19.120 --> 26:25.760
anyone who's listening to anyone who's listening to this knows anyone let me get I know yes please

26:25.760 --> 26:33.200
I want to understand you know returning to data curation you can talk a little bit about some

26:33.200 --> 26:41.200
of the ways that your work around measurement and quantification helps you kind of think through

26:41.200 --> 26:47.360
and address the kinds of issues that we've been talking about so far yeah so we want to understand

26:47.360 --> 26:54.320
the quality of data and part of what I'm pushing for is prioritizing quality over quantity a little

26:54.320 --> 27:02.320
bit more arguably if you have good quality data you don't need as much as much of the quantity of

27:02.320 --> 27:09.680
data which means that you can be a lot more selective about whether or not you are including

27:09.680 --> 27:14.720
personal information or personally identifying information whether or not you're including

27:14.720 --> 27:20.800
things like stereotypes and biases which can then be laundered through a model and then pushed

27:20.800 --> 27:28.000
back out at the community affecting all kinds of decisions that happen under the hood and so if

27:28.000 --> 27:36.800
you can start to measure for example the strength of association between a term like smile and

27:36.800 --> 27:43.040
woman versus smile and man in some slice of data that you're thinking of then if it's a very

27:43.040 --> 27:48.000
high association and you want to be working towards data that can reflect a more equitable universe

27:48.800 --> 27:54.400
then perhaps you don't want to sample that data directly perhaps you want to do something a

27:54.400 --> 28:00.000
little bit more clever in order to make sure that there isn't such a strong skill so by being able to

28:00.000 --> 28:05.840
to measure these kinds of skews come up with actual measurements that can do this now we can

28:05.840 --> 28:12.800
start quantifying serious issues in the data and using that to inform what we what we do collect

28:12.800 --> 28:18.320
one of the ways that one of the early ways that as a community we've sought to

28:19.600 --> 28:25.360
do a better job around understanding the biases and data sets are kind of like data sheets for

28:25.360 --> 28:35.600
data sets and that workshop that body of work by Timnett and others Timney and the model cards and

28:35.600 --> 28:43.600
it sounds like what you're doing connects to those efforts increasing to sophisticated you know

28:43.600 --> 28:49.120
those things are are dashboards you're increasing the sophistication of the metrics exactly exactly

28:49.120 --> 28:55.760
and actually I was the first author on model cards so I would say you know the so I'm working on

28:55.760 --> 29:03.200
model cards still and it goes to one big part of that is what you evaluate and so that again

29:03.200 --> 29:07.920
goes to the data so I've really been focusing on that aspect of model cards and the kind of things

29:07.920 --> 29:13.280
you want to quantify in the data and I've also been working on data cards which is heavily inspired

29:13.280 --> 29:21.680
by data sheets but also different data cards are shorter they're a card not a sheet so

29:23.680 --> 29:29.520
just make sure I understand that yeah well so data sheets are pretty comprehensive in terms of

29:29.520 --> 29:34.640
all the different decisions that go into creating a data set so they're really wonderful artifacts

29:34.640 --> 29:40.560
to have in terms of auditing the development of machine learning systems they're a little bit

29:40.560 --> 29:46.720
more difficult to do in terms of getting developers to actually implement them so if you're you

29:46.720 --> 29:56.800
know asking someone who who is trying to upload their data sets all the details of you know the

29:56.800 --> 30:01.760
compensation protocols from crowd workers and things like that they're going to be a lot less likely

30:01.760 --> 30:08.480
to to fill it out at all and so you know data cards are trying to get a bit at how can we

30:08.480 --> 30:16.960
incentivize good practices or how can we encourage good practices while still you know while

30:16.960 --> 30:22.080
still being somewhat palatable to people who are who are uploading and and sharing the data set

30:22.080 --> 30:27.760
so it's not as ideal as data sheets it's it's sort of speaking to the cynical reality that a lot

30:27.760 --> 30:35.200
of people don't care and how do we sort of get them to start caring and so so part of that is

30:35.200 --> 30:42.560
creating measurements that can be automatically run on the data set and then be used to fill out

30:42.560 --> 30:48.160
the data card so that goes back again to how to quantify things in the data set like the biases

30:48.160 --> 30:54.800
and skews like the naturalness which side note this is this is related to the zip fee and distribution

30:54.800 --> 31:02.160
of language it's super fun to implement and measure but then you know so but these measurements

31:02.160 --> 31:06.800
are things that can just be run automatically and the more things that you can run automatically

31:06.800 --> 31:13.520
the more buy and there will be to have things like data cards released alongside your data

31:14.160 --> 31:20.800
so we're trying to sort of lower the barrier to having some sort of you know responsibility informed

31:26.080 --> 31:31.760
development of data and sharing of data by providing some automatic quantifications

31:31.760 --> 31:36.880
that speak to values of discrimination that speak to or I should say speak to values of

31:36.880 --> 31:44.080
non-discrimination that speak to ideas around having naturalistic data or data that's balanced

31:44.080 --> 31:50.000
a bunch of variety of topics all these kinds of things that we ideally can have in a perfect data

31:50.000 --> 31:56.000
curated setting but but coming up with those measurements automatically so that people can see

31:56.000 --> 32:02.000
what kind of characteristics their data actually have and then people can actually decide whether

32:02.000 --> 32:09.200
not to use it for the various you know use cases that they that they foresee and so so this data

32:09.200 --> 32:16.400
measurement area of work that I'm digging into more and more is really to help lower the barrier

32:18.000 --> 32:24.080
to create just sort of more responsible AI development life cycles and really being clear about

32:24.080 --> 32:34.400
what the data is encoding on the topic of buy-in the cycle of adoption isn't surprising but it's my

32:34.400 --> 32:44.000
sense that after kind of a while you know a couple of years of not hearing a lot about data sheets

32:44.000 --> 32:50.160
data cards after their proposal and introduction now I'm starting to hear about their use quite

32:50.160 --> 32:55.840
frequently you know at least relatively is that your sense as well yeah well so personally I would

32:55.840 --> 33:03.360
say that my research has generally been around three years ahead of the general pickup but it's

33:03.360 --> 33:09.040
true I mean I've just noticed it again and again which is great for your citations because then it

33:09.040 --> 33:13.200
ends up being that your work is foundational so I think that's generally true for for me with

33:13.200 --> 33:19.840
the model cards work and it's also true for for Tim neat with the data sheets work because you know

33:19.840 --> 33:25.600
when you're working in AI ethics you're thinking very very deeply about issues at a level of nuance

33:25.600 --> 33:33.040
that people who who aren't working on AI ethics day to day don't really see so it's not until you

33:33.040 --> 33:39.840
start seeing the issues that you start paying attention to the AI ethics things and then that's when

33:39.840 --> 33:44.880
you start to see a little bit more of the need to address some sort of nuances and that's when

33:44.880 --> 33:50.480
you get the pickup from work that people who had already been deeply thinking about these things

33:50.480 --> 34:00.160
have put in place for you so ideally we are moving ideally we are defining things before they're

34:00.160 --> 34:05.840
needed so that once they are needed they're already there and available and I would say that's

34:05.840 --> 34:11.200
part of the goal in ethical AI work is to look forward you look towards the future you you try

34:11.200 --> 34:17.760
and use foresight as much as possible and then create the various tools and processes around that

34:17.760 --> 34:23.040
so they're they're ready to go once people are noticing the issues are actually coming up

34:23.040 --> 34:36.720
and I love to hear how the the these you know papers data sheets cards etc which in some sense

34:36.720 --> 34:42.160
you first hear those yeah that makes sense we should do that I I love to hear how those

34:42.960 --> 34:49.680
have become platforms for you know maybe more you know crunchier work around specific measurements

34:49.680 --> 34:56.640
and being able to generate them efficiently and and that kind of thing I am finding interesting

34:56.640 --> 35:05.360
that in your case the this research about the or this paper about model cards has led to

35:06.640 --> 35:15.440
what I perceive as more I said crunchier but like more technical or more concrete research

35:15.440 --> 35:21.200
into the metrics that then populate those cards yeah yeah and you're getting into you mentioned

35:21.200 --> 35:30.080
Zipfian distributions and and other stuff that kind of give kind of fill out this idea of hey

35:30.080 --> 35:36.240
you should know what your model is about and you should publish data about it right yeah yeah so it's

35:36.240 --> 35:42.160
because I would say it's because people are starting to be affected by the problems that we were

35:42.160 --> 35:49.120
for seeing you know a few years ago right so there's the now now unfortunately classic um

35:50.720 --> 35:58.000
guerrilla thing uh the the Google guerrillas issue um where it had tagged people who are black

35:58.000 --> 36:04.640
as guerrillas um and that was massively offensive uh hitting on a lot of historical harm right um

36:04.640 --> 36:11.520
and so one of the one of the solutions to handling something like that is looking at the

36:11.520 --> 36:18.080
association between different skin tones and different labels right so obvious but if you're

36:18.080 --> 36:22.960
not thinking that way if you're not sort of thinking through how these things can happen

36:22.960 --> 36:27.920
then you don't implement those things right and so that's a kind of measurement this sort of a

36:27.920 --> 36:33.760
problematic associations um that we were trying to put in place as we were for seeing like oh

36:34.640 --> 36:39.760
given what we know about the data sets that being that are being collected for language models

36:39.760 --> 36:45.280
language models are going to hate Muslims right and then that was found like years later

36:45.280 --> 36:51.040
but by doing an analysis of the data you know previously it was very clear that that was something

36:51.040 --> 36:57.280
that language models were going to learn to propagate um so as as the public has started to see

36:57.920 --> 37:02.960
the issues that we were for seeing they're sort of turning to see well what can be done what are we

37:02.960 --> 37:08.960
thinking um and it turns out that that one of the key things to be able to do is to document because

37:08.960 --> 37:14.720
as you document you can trace where issues are coming from and address them and this was this was

37:14.720 --> 37:20.400
one of the main takeaways hopefully uh from the stochastic parrots paper although I think people

37:20.400 --> 37:26.560
were maybe a little bit distracted by the the story the massive firing from Google yeah the back

37:26.560 --> 37:32.000
but the back story but you know we actually thought that the the main exciting thing from that

37:32.000 --> 37:38.880
that paper was uh where that was the difficulty in in doing the parrot emoji uh but one of the takeaways

37:38.880 --> 37:46.880
from that paper was that um uh language models are too big um when the data they're using is not

37:46.880 --> 37:53.280
documentable and the reason that that's an issue is because you'll start having really harmful

37:53.280 --> 38:00.080
behavior from language models that you that you don't know where it came from um and so yeah

38:00.080 --> 38:07.280
I wanted to raise that issue yeah I think that was a theme uh I don't recall how explicit it

38:07.280 --> 38:12.240
it was uh but it was a big takeaway from that conversation I referenced earlier with Thomas like

38:13.040 --> 38:16.720
you know when we're talking about these large language models and the training data set essentially

38:16.720 --> 38:23.200
becomes the internet like how do you manage these kinds of issues and that's that's what you're

38:23.200 --> 38:29.520
speaking to now yeah yeah yeah so if you can start measuring things then you can start actually

38:29.520 --> 38:35.600
curating the data um and so I mean this also goes a lot to what sources you should be using so I

38:35.600 --> 38:40.000
think it's now been generally read upon that you shouldn't use reddit data because there's a lot

38:40.000 --> 38:45.920
of toxic and hateful abusive language there and disproportionately directed it at women um and so

38:45.920 --> 38:51.360
but that that took years to establish right so reddit data used to be cool I think it was even used

38:51.360 --> 38:58.240
in the the Delphi uh ethics system that was that was launched through AI2 but uh but yeah so I mean

38:58.240 --> 39:03.600
this is one of the reasons why coming up with quantifications of data is so um so important as well

39:03.600 --> 39:08.880
as paying attention to the demographics of the people creating that data because now you can really

39:08.880 --> 39:14.400
understand what the data is starting to encode which means what the language model will encode so

39:14.400 --> 39:22.720
it turns out that uh the c4 dataset which is um a colossal uh dataset used for training language

39:22.720 --> 39:32.080
models c4 yeah i have to be confused with c4 not to be yeah c4 is nlp c4 is computer vision

39:32.080 --> 39:40.880
so uh not that that really helps I suppose uh but but the c4 dataset um has a lot of data

39:40.880 --> 39:47.520
sampled from Wikipedia and it turns out that Wikipedia um has a lot of writing disproportionately

39:47.520 --> 39:56.080
more writing from uh white north american men um in their in their 20s uh and so in practice what

39:56.080 --> 40:01.600
does this mean it means that black history is basically not represented or represented very

40:01.600 --> 40:06.560
poorly uh and for a while before I started telling people about this black history would redirect

40:06.560 --> 40:13.600
to african-american history uh which hopefully says so much and if it doesn't sense say so much to

40:13.600 --> 40:22.480
you then perhaps you should check your biases yeah you mentioned naturalness earlier as one of

40:22.480 --> 40:29.840
these metrics that you found interesting elaborate on that yeah so as we collect datasets um

40:29.840 --> 40:36.320
and we're trying to measure or rather as we're trying to collect data that reflects natural

40:36.320 --> 40:42.800
language um there becomes a question of how do i know it's natural and so one approach is to say

40:42.800 --> 40:48.400
well i'll be careful with my sampling in order to make sure that the websites amusing or whatever

40:48.400 --> 40:54.000
captures naturalistic language um but there isn't really a useful quantification of that or they're

40:54.000 --> 41:00.000
there generally hasn't been um other than this more qualitative sort of like that looks natural to me

41:00.560 --> 41:06.560
um and it's important and it's important because if you're trying to build models that can understand

41:06.560 --> 41:13.120
natural language so that means uh you know pulling out the the key entities figuring out the content

41:13.120 --> 41:17.520
of a question things like that these tools that really rely on having a reasonable natural

41:17.520 --> 41:21.920
language understanding uh then it's useful to be able to actually measure naturalness

41:21.920 --> 41:27.120
so from here we can look at things like corpus linguistics uh you know going back to brown

41:27.120 --> 41:33.120
corpus in 1964 uh where there were some really cool ideas around what it means to have natural

41:33.120 --> 41:40.720
language um and one of them was noticing that uh that the zippian distribution of so many things

41:40.720 --> 41:49.040
in nature uh also applies to natural language um and so uh the zippian distribution um it's very

41:49.040 --> 41:54.560
hard to explain in one sentence since it's a mathematical concept uh but basically here's a very

41:54.560 --> 42:02.480
like simple oversimplified way of saying it um the most frequent word uh in someone's

42:02.480 --> 42:10.320
document or whatever um will be roughly twice as frequent as the second most frequent word

42:10.320 --> 42:15.760
which will be roughly twice as frequent as the third most frequent word so there's this inverse

42:15.760 --> 42:22.480
relationship between frequency and essentially rank um and so the zippian distribution so what

42:23.200 --> 42:30.080
what zips law states uh is that they'll it'll it'll follow this this sort of general tendency

42:30.080 --> 42:38.640
um and it turns out that there's this parameter we'll call it alpha that um uh that can kind of

42:38.640 --> 42:44.240
um shape what that curve looks like and it turns out that for English that's already been calculated

42:44.240 --> 42:49.600
by by nice corpus linguists and linguists and stuff and we know that alpha is actually like one

42:49.600 --> 42:56.480
um and so different languages have different different kinds of shapes that follow this uh general

42:56.480 --> 43:02.560
tendency um and because they've already been calculated for a ton of languages we can see how well

43:02.560 --> 43:09.120
some collection fits to the ideal zippian distribution um and then the farther out it is for that

43:09.120 --> 43:15.600
language which we can already know uh then we can say like well according to this sort of theory

43:15.600 --> 43:21.120
it is actually moving farther away from what we would expect in natural language um so you know

43:21.120 --> 43:26.160
it's it's a measure it's not the best measure but it's something that uh that corpus linguists

43:26.160 --> 43:31.600
have put forward and is super useful and and personally in in my work on data measurements uh

43:31.600 --> 43:38.240
I found I found that it's really helped to identify uh data sets that are appropriately capturing

43:38.240 --> 43:43.920
you know um uh general conversations and stuff and data sets that for example have a lot of

43:43.920 --> 43:48.720
artifacts or weird mixing of different domains that wasn't well controlled for and things

43:48.720 --> 43:56.560
like that um so it can be really as well you mentioned as uh I think it was kind of a motivation

43:56.560 --> 44:04.640
for this particular measure natural text on the internet versus newswire press releases which

44:04.640 --> 44:12.880
have a different uh a different way of using language but this also kind of rings of thinking about

44:12.880 --> 44:17.120
you know a world where we have these large language models and they're quote unquote polluting

44:17.120 --> 44:24.880
the internet with spam and kind of uh almost that adversarial and anti adversarial uh cat and

44:24.880 --> 44:31.120
mouse game and and are is that part of what you're thinking about here or is that just tangential

44:31.120 --> 44:36.880
yeah I mean I think one thing that might be relevant here is that if we're trying to figure out

44:36.880 --> 44:43.600
whether or not something is synthetically generated um so you know generated from a language model

44:43.600 --> 44:50.800
versus um created by people we have to be thinking about these sort of second order characteristics um

44:50.800 --> 44:56.480
so what can we maybe not tell as we look at things sentenced by sentence um but if we calculate

44:56.480 --> 45:02.880
statistics over patterns uh in a bunch of sentences are there things that we see um that's different

45:02.880 --> 45:07.600
than what we would expect from when when humans are generating it um and so these kinds of

45:07.600 --> 45:12.720
measurements such as naturalness based measurements would help to get at that um and so as we're trying

45:12.720 --> 45:18.720
to figure out you know uh trolls that are spewing hateful and abusive content that are actually

45:18.720 --> 45:22.880
from from systems that are just using large language models and misinformation and all this

45:22.880 --> 45:28.800
other kind of stuff as we can start developing the second order uh kinds of measurements then we

45:28.800 --> 45:34.080
can start being able to a lot more easily make distinctions so what I'm hearing there is that your

45:34.080 --> 45:40.160
focus is on bias and human impact but the cyber security folks may find this interesting too

45:40.160 --> 45:47.520
or the adversarial hopefully hopefully yeah yeah awesome awesome well Meg it has been wonderful

45:47.520 --> 45:52.640
catching up with you and chatting about some of the things that you've been working on and thinking

45:52.640 --> 45:58.640
about thanks so much for joining us yeah thank you for asking me and letting me babble about things

45:58.640 --> 46:12.080
I'm fascinated by I really appreciate it thank you

