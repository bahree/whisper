1
00:00:00,000 --> 00:00:16,880
Hello everyone and welcome to another episode of Twimmel Talk, the podcast where I interview

2
00:00:16,880 --> 00:00:22,200
interesting people, doing interesting things in machine learning and artificial intelligence.

3
00:00:22,200 --> 00:00:26,080
I'm your host Sam Charrington.

4
00:00:26,080 --> 00:00:28,720
I think you all are really going to get a kick out of this show.

5
00:00:28,720 --> 00:00:34,640
My guest this time is Joshua Bloom, professor of astronomy at the University of California

6
00:00:34,640 --> 00:00:41,640
Berkeley and co-founder and chief technology officer of machine learning startup Wise.io.

7
00:00:41,640 --> 00:00:46,440
I was in California last week and Josh graciously agreed to host me in his company's office

8
00:00:46,440 --> 00:00:48,560
for this interview.

9
00:00:48,560 --> 00:00:52,880
We had a wonderful discussion and as you might have guessed, if you happen to have noticed

10
00:00:52,880 --> 00:00:58,240
the length of this episode, we covered quite a lot of ground, but I promise you that you'll

11
00:00:58,240 --> 00:01:05,760
find this 84 minute interview to be jam packed with great information, ideas and war stories.

12
00:01:05,760 --> 00:01:09,400
In this show, you'll learn how Josh and his research group at Berkeley pioneered the

13
00:01:09,400 --> 00:01:14,920
use of machine learning for the analysis of images from robotic infrared telescopes.

14
00:01:14,920 --> 00:01:19,320
We talk extensively about the challenges they faced in doing this and some of the results

15
00:01:19,320 --> 00:01:21,000
they achieved.

16
00:01:21,000 --> 00:01:26,000
We also discussed the founding of his company Wise.io, which uses machine learning to help

17
00:01:26,000 --> 00:01:31,480
customers deliver better customer support, but that wasn't where the company started and

18
00:01:31,480 --> 00:01:35,880
you'll hear why and how they evolved to serve that market.

19
00:01:35,880 --> 00:01:40,880
We talk about his company's technology stack and data science pipeline in fair detail

20
00:01:40,880 --> 00:01:45,680
and discuss some of the key technology decisions they've made in building their product.

21
00:01:45,680 --> 00:01:51,000
We also discussed some interesting open research challenges in machine learning and AI.

22
00:01:51,000 --> 00:01:56,080
Of course, I'll be linking to Josh and the various things we mentioned on the show in the

23
00:01:56,080 --> 00:02:01,600
show notes, which you'll be able to find at twimmelai.com slash talk slash five.

24
00:02:01,600 --> 00:02:09,520
That's twimelai.com slash T-A-L-K slash the number five.

25
00:02:09,520 --> 00:02:23,520
And now onto the interview, hey everyone, I am here at the Wise.io offices with the

26
00:02:23,520 --> 00:02:29,960
CTO, Joshua Bloom, and we got a great conversation lined up for you, and we'll start with Josh.

27
00:02:29,960 --> 00:02:33,080
Why don't you introduce yourself to the audience here.

28
00:02:33,080 --> 00:02:39,560
Great, so this is Josh and I am CTO and one of the co-founders of Wise.io.

29
00:02:39,560 --> 00:02:45,520
I'm also a professor at UC Berkeley in the astronomy department.

30
00:02:45,520 --> 00:02:51,240
One of the important things that we'll touch on today is how does somebody go from astronomy

31
00:02:51,240 --> 00:02:56,960
and teaching to building an AI application company.

32
00:02:56,960 --> 00:03:02,760
I think a big part of the origin story of course of the company is my history, but I

33
00:03:02,760 --> 00:03:10,040
think it also has some interesting lessons for how we think about AI and production

34
00:03:10,040 --> 00:03:16,600
systems and why having diverse backgrounds is pretty important these days.

35
00:03:16,600 --> 00:03:20,640
Yeah, that's a lot of good stuff to talk about there.

36
00:03:20,640 --> 00:03:25,560
Why don't we start by learning a little bit more about you and your background and how

37
00:03:25,560 --> 00:03:27,720
you got to where you are.

38
00:03:27,720 --> 00:03:35,840
So I was trained as a physicist and an astronomer, went to Harvard as an undergrad and caught

39
00:03:35,840 --> 00:03:41,920
a bit of the research bug over the summers working in Los Alamos, then went to Cambridge

40
00:03:41,920 --> 00:03:48,440
England to do a masters and back to Caltech where I did my PhD all in the context of astronomy

41
00:03:48,440 --> 00:03:52,520
and then back to Harvard where I was a postdoc.

42
00:03:52,520 --> 00:04:00,120
All the while working on what we could broadly term time-domain astrophysics, understanding

43
00:04:00,120 --> 00:04:07,360
the variable sky and why things do what they do explosively, cataclysmically or otherwise.

44
00:04:07,360 --> 00:04:14,320
And while there's a deep interest in understanding the origins of those events and how they're

45
00:04:14,320 --> 00:04:19,280
connected to other things that we study in the universe, I got more and more interested

46
00:04:19,280 --> 00:04:27,720
over time in the informatics of actually just doing the science, the statistics on variable

47
00:04:27,720 --> 00:04:30,120
sources and the presence of noise.

48
00:04:30,120 --> 00:04:38,480
And then as I became a faculty member at Berkeley, I started looking ahead to really a series

49
00:04:38,480 --> 00:04:46,640
of new surveys, particularly imaging surveys of large swaths of the night sky and one

50
00:04:46,640 --> 00:04:52,680
of the great interests for myself and many others was in finding new events, essentially

51
00:04:52,680 --> 00:04:57,680
new explosions or new variable eruptive stars that hadn't been known about before and doing

52
00:04:57,680 --> 00:05:00,800
that as quickly as possible.

53
00:05:00,800 --> 00:05:06,360
Now the traditional way in which that was done and in some cases is still done today is

54
00:05:06,360 --> 00:05:12,200
that as you acquire more data, you linearly hire more grad students that scales with the

55
00:05:12,200 --> 00:05:16,680
total number of images that you're getting and you need to sift through.

56
00:05:16,680 --> 00:05:22,280
And as I was becoming involved in some of those projects, I wound up realizing that as time

57
00:05:22,280 --> 00:05:24,280
went on, that really wouldn't scale anymore.

58
00:05:24,280 --> 00:05:31,280
And we needed to find effectively a replacement for domain experts who otherwise would have

59
00:05:31,280 --> 00:05:36,080
been looking at and apining on data using alternate techniques.

60
00:05:36,080 --> 00:05:43,200
And about 79 years ago, I stumbled upon machine learning as a real interesting potential avenue.

61
00:05:43,200 --> 00:05:51,440
And at the time, machine learning really hadn't been applied to anything in astronomy in

62
00:05:51,440 --> 00:05:54,920
the variable sky, sort of in a time to mean context.

63
00:05:54,920 --> 00:06:00,600
There had been a number of studies in using machine learning to do special types of inference

64
00:06:00,600 --> 00:06:08,120
on the static sky and understanding, sort of demographics of stars and galaxies and

65
00:06:08,120 --> 00:06:10,600
their distribution in space.

66
00:06:10,600 --> 00:06:19,520
So we really felt like there wasn't a lot of precedent in us applying some of the capabilities

67
00:06:19,520 --> 00:06:23,800
to this data, but we wound up realizing it was sort of an imperative.

68
00:06:23,800 --> 00:06:27,720
And one of the things that people who know astronomers would probably say about them is

69
00:06:27,720 --> 00:06:36,560
they tend to like to use tools that help them and seek those tools out, be those new types

70
00:06:36,560 --> 00:06:37,560
of detectors.

71
00:06:37,560 --> 00:06:46,080
So CCDs, for instance, were something that astronomers adopted almost as soon as they were invented.

72
00:06:46,080 --> 00:06:50,760
And obviously statistical techniques and computational techniques, astronomers are willing to try

73
00:06:50,760 --> 00:06:54,000
things out to solve their problem.

74
00:06:54,000 --> 00:06:59,280
The classic example I go back to is Galileo who said, hey, there's this new thing that's

75
00:06:59,280 --> 00:07:05,520
been invented to look at the horizon for ships coming towards us.

76
00:07:05,520 --> 00:07:07,960
What if I just took it and pointed it to the stars?

77
00:07:07,960 --> 00:07:08,960
What could I do with that?

78
00:07:08,960 --> 00:07:15,000
And so our use of the telescope was essentially a co-option of the use of a technology that

79
00:07:15,000 --> 00:07:16,880
had been built for other purposes.

80
00:07:16,880 --> 00:07:21,320
And so that kind of precedes a pace throughout the history of astronomy.

81
00:07:21,320 --> 00:07:28,920
And so the idea of essentially a fairly new technique into the fold is not at all unusual.

82
00:07:28,920 --> 00:07:32,320
Do you remember how you stumbled across machine learning?

83
00:07:32,320 --> 00:07:41,480
Yeah, so part of it was just asking the question, if I've got a bunch of data and I need to

84
00:07:41,480 --> 00:07:48,000
decide, is this part of the sky interesting or not, is this event new or not, what type

85
00:07:48,000 --> 00:07:53,320
of event could this be, very quickly you wind up realizing this is a classification problem

86
00:07:53,320 --> 00:07:55,760
of some sort.

87
00:07:55,760 --> 00:08:01,880
And talking to people at UC Berkeley in the stats department as I was starting to introduce

88
00:08:01,880 --> 00:08:08,080
some of these interesting challenges became very clear that machine learning and particularly

89
00:08:08,080 --> 00:08:14,320
supervised learning would be a fertile ground for us to start exploring.

90
00:08:14,320 --> 00:08:19,000
But one of the challenges that I saw is that even though we're in a very rich and fertile

91
00:08:19,000 --> 00:08:24,160
environment at UC Berkeley, and there's a lot of crosstalk between departments and individuals

92
00:08:24,160 --> 00:08:30,080
within departments, it was very hard to get even the kind of the language on both sides

93
00:08:30,080 --> 00:08:34,560
up and running where both sides understood the methodological folks who deeply understood

94
00:08:34,560 --> 00:08:39,360
what machine learning was and what it could be used for, and then people like myself from

95
00:08:39,360 --> 00:08:44,640
the physical side of even learning to ask the right questions.

96
00:08:44,640 --> 00:08:51,560
So thankfully wind up getting a group from the stats department and those folks from

97
00:08:51,560 --> 00:08:57,720
computer science together with me and my postdocs and we were able to get a proposal together

98
00:08:57,720 --> 00:09:03,480
the National Science Foundation funded that allowed us to basically start building out new

99
00:09:03,480 --> 00:09:06,760
ways of doing inference on astronomy data.

100
00:09:06,760 --> 00:09:11,480
That turned out to be a very fruitful place for us, for me in particular, to learn about

101
00:09:11,480 --> 00:09:15,640
the landscape of what other techniques were out there that we hadn't been taught in

102
00:09:15,640 --> 00:09:16,640
school.

103
00:09:16,640 --> 00:09:21,920
Can you tell us a little bit about from that initial discovery what the research arc looked

104
00:09:21,920 --> 00:09:25,320
like, what were some of the first things you started exploring and how that evolved

105
00:09:25,320 --> 00:09:26,320
over time?

106
00:09:26,320 --> 00:09:34,160
Yeah, so I really just started looking at toy amounts of data that we already had in

107
00:09:34,160 --> 00:09:39,480
the can, and we could start applying these different techniques to, and looking for tools

108
00:09:39,480 --> 00:09:45,120
that would be useful for us, and really the best thing out there the time that we started

109
00:09:45,120 --> 00:09:51,680
was something called WECCA, which was a, and still is, a collection of machine learning

110
00:09:51,680 --> 00:09:57,880
algorithms that one can apply in a sort of gooey graphical way, all kind of written

111
00:09:57,880 --> 00:10:05,200
in Java, and really that was our original playground in benchmark, and used that as a launching

112
00:10:05,200 --> 00:10:10,320
off point to start understanding what are these different modeling techniques that are being

113
00:10:10,320 --> 00:10:11,320
exposed to here?

114
00:10:11,320 --> 00:10:15,320
What does a support vector machine, what does it mean when people say random forest, and

115
00:10:15,320 --> 00:10:19,760
use that as a way to sort of educate myself and those in our group, and started seeing

116
00:10:19,760 --> 00:10:21,920
some interesting results, right?

117
00:10:21,920 --> 00:10:25,600
We started seeing accuracies that were better than what you could get from random, and then

118
00:10:25,600 --> 00:10:29,720
as we poked farther and farther, one ended up seeing how far can we take these algorithms,

119
00:10:29,720 --> 00:10:35,520
how well does one of them work relative to the others to get the kinds of answers that

120
00:10:35,520 --> 00:10:36,520
we want?

121
00:10:36,520 --> 00:10:43,040
How do we build in a loss function, which turns out to be very important to get good answers,

122
00:10:43,040 --> 00:10:49,560
because in the case of what we did, when we're discovering something in the sky, it's

123
00:10:49,560 --> 00:10:55,560
not easy to articulate that loss function, and by that I mean, what is the cost of

124
00:10:55,560 --> 00:10:58,040
missing an interesting place in the sky?

125
00:10:58,040 --> 00:11:02,920
It means that you don't get to do new science, versus what's the cost of saying everything

126
00:11:02,920 --> 00:11:09,320
in the sky is interesting, which means you burn all of your follow-up resources, and

127
00:11:09,320 --> 00:11:17,280
starting then to think about context-aware classification, now just not in the context

128
00:11:17,280 --> 00:11:24,680
of really just resources, but now time constraints, making an inference about something that could

129
00:11:24,680 --> 00:11:30,840
be of interest, may be more important than waiting to get another couple of data points

130
00:11:30,840 --> 00:11:33,920
and saying something with even more confidence.

131
00:11:33,920 --> 00:11:39,240
So understanding how to calibrate confidence in probabilities, doing this in the presence

132
00:11:39,240 --> 00:11:46,800
of sort of missing data and irregularly sampled data in time, all of these also started

133
00:11:46,800 --> 00:11:54,200
wind up showing to us that there were parts of the machine learning sphere, at least

134
00:11:54,200 --> 00:11:58,720
in the academic world, that were not often exposed to the kinds of data that we were

135
00:11:58,720 --> 00:12:00,520
exposed to.

136
00:12:00,520 --> 00:12:05,200
And so noisy data, for instance, know whenever, when they talk about the iris data set,

137
00:12:05,200 --> 00:12:10,880
they don't say, you know, the pedal of this is red plus or minus purple.

138
00:12:10,880 --> 00:12:15,840
So even just having uncertainties in your features, let alone your labels, became an interesting

139
00:12:15,840 --> 00:12:19,680
challenge, and we wanted realizing perhaps there were some new techniques that we needed

140
00:12:19,680 --> 00:12:25,320
to start innovating on, to even do the kinds of inference we wanted to do with our data.

141
00:12:25,320 --> 00:12:32,000
So you mentioned the loss function and needing to wrap your arms around what that means, can

142
00:12:32,000 --> 00:12:35,760
you succinctly describe how you grapple that?

143
00:12:35,760 --> 00:12:39,360
What did you end up, how did you approach it and what did you end up coming up with for

144
00:12:39,360 --> 00:12:42,520
the types of data that you were looking at?

145
00:12:42,520 --> 00:12:46,680
One of the things we wind up realizing is that one person's loss function is not the

146
00:12:46,680 --> 00:12:54,760
same as another person's loss function, and so to get traction on your answers, one needs

147
00:12:54,760 --> 00:12:59,360
to at least be clear about what it is that you're optimizing for.

148
00:12:59,360 --> 00:13:05,360
And at least give people the ability to imbue their own loss functions, if for instance

149
00:13:05,360 --> 00:13:11,720
you're producing a catalog of different types of variable stars on the sky.

150
00:13:11,720 --> 00:13:17,160
We have a specific notion of what it means to get something wrong about say a very minority

151
00:13:17,160 --> 00:13:24,840
class versus a majority class, and I wouldn't say that we solved that problem by any stretch,

152
00:13:24,840 --> 00:13:28,800
but at least we were trying to be clear about what our assumptions were of the loss function

153
00:13:28,800 --> 00:13:34,640
and articulate what it is that we're optimizing for.

154
00:13:34,640 --> 00:13:40,800
When people are doing AI or machine learning in a production environment, there is always

155
00:13:40,800 --> 00:13:47,960
going to be an optimization of some sort, and the typical one people will go to without

156
00:13:47,960 --> 00:13:54,560
knowing exactly what the business value is, or scientific value is of the answer, is

157
00:13:54,560 --> 00:13:58,960
you go for some notion of an accuracy, and then when you get a level deeper in that,

158
00:13:58,960 --> 00:14:03,920
you say, well, what I really want to do is I want to minimize false positives at a false

159
00:14:03,920 --> 00:14:12,760
negative rate of 0.1, and then that is an implicit statement of what your loss function is,

160
00:14:12,760 --> 00:14:18,720
and you hope that by defining it that way, and by optimizing on it that way, that you're

161
00:14:18,720 --> 00:14:25,280
actually getting very close to an optimization of the result of what you're emitting out

162
00:14:25,280 --> 00:14:28,440
of your modeling.

163
00:14:28,440 --> 00:14:34,480
And so you're primarily looking at image-oriented data over time.

164
00:14:34,480 --> 00:14:40,080
Are there other fields where you've seen them adopt the same types of approaches to

165
00:14:40,080 --> 00:14:41,800
what you were working with?

166
00:14:41,800 --> 00:14:46,680
Well, one of the nice things is you can work at the sensor level data, which is effectively

167
00:14:46,680 --> 00:14:51,400
photoelectrons in a CCD and counting those up as a function of position in X, Y, and

168
00:14:51,400 --> 00:14:53,120
then trying to map that back in the sky.

169
00:14:53,120 --> 00:14:58,160
So that's what you might call noisy image sensor data, and we worked at that level.

170
00:14:58,160 --> 00:15:02,760
And then we also worked at a metadata level, which was now, let's use traditional astronomy

171
00:15:02,760 --> 00:15:07,400
techniques to extract the brightness of a star as a function of time.

172
00:15:07,400 --> 00:15:13,720
So we got ourselves out of the image plane and into the time domain, and then we're basically

173
00:15:13,720 --> 00:15:16,840
working with effectively tabular data.

174
00:15:16,840 --> 00:15:21,800
And again, there are lots of different models and feature engineering approaches that one

175
00:15:21,800 --> 00:15:23,680
can take to all of that.

176
00:15:23,680 --> 00:15:29,120
I wouldn't say that there was a common thread in our work, across a bunch of these different

177
00:15:29,120 --> 00:15:34,840
sort of sub-questions, other than say that over time we wind up realizing that there

178
00:15:34,840 --> 00:15:41,000
were only really a couple of different machine learning models that did as well or better

179
00:15:41,000 --> 00:15:42,480
than everything else.

180
00:15:42,480 --> 00:15:48,080
And so even though, for instance, support vector machines are very popular because they

181
00:15:48,080 --> 00:15:53,000
have some great sort of theoretical, provable properties, they tend to be kind of unwieldy

182
00:15:53,000 --> 00:15:59,480
and for dealing with the kinds of data we were working with, which is heterogeneous, noisy,

183
00:15:59,480 --> 00:16:05,680
dirty, sparse, missing, and multi-class, where you needed to also get probabilities out

184
00:16:05,680 --> 00:16:11,400
that you could then calibrate models like support vector machines really fall short for

185
00:16:11,400 --> 00:16:12,720
practical purposes.

186
00:16:12,720 --> 00:16:18,720
And so we wound up recognizing in our group, and I think that was validated in a conference

187
00:16:18,720 --> 00:16:23,840
that we ran at UC Berkeley on essentially streaming inference with machine learning.

188
00:16:23,840 --> 00:16:30,440
It was a sort of week-long conference that involved folks from Netflix, folks from Google,

189
00:16:30,440 --> 00:16:35,600
and then domain experts, everything from biomedical to physics.

190
00:16:35,600 --> 00:16:39,320
A number of people would stand up, give their talk, and say, yeah, and we wound up realizing

191
00:16:39,320 --> 00:16:42,840
that decision forest pretty much always won.

192
00:16:42,840 --> 00:16:47,720
Now this was in 2012 before the resurgence of deep learning, I bet if we ran this conference

193
00:16:47,720 --> 00:16:55,120
again half of the talks would be about how that's a better algorithm as it were.

194
00:16:55,120 --> 00:16:57,040
But it was pretty eye-opening.

195
00:16:57,040 --> 00:17:01,480
And it was one of the things that we took to heart as we wound up starting the company

196
00:17:01,480 --> 00:17:11,560
is a recognition that to exceed, to produce value sort of very generally, the algorithm

197
00:17:11,560 --> 00:17:14,240
itself is not necessarily the key.

198
00:17:14,240 --> 00:17:19,440
In some sense, the way I view this now is that algorithms and their accuracy that they

199
00:17:19,440 --> 00:17:25,640
can produce and their ability to optimize them around a loss function is really only

200
00:17:25,640 --> 00:17:33,280
table stakes for the utility of these in a real environment.

201
00:17:33,280 --> 00:17:39,080
So yes, you need to use a model that's very, very accurate and potentially can be retrained

202
00:17:39,080 --> 00:17:40,680
and gets slightly better.

203
00:17:40,680 --> 00:17:45,360
But as most data scientists or most people at work in machine learning workflows will

204
00:17:45,360 --> 00:17:50,000
say almost all of that work is in feature engineering.

205
00:17:50,000 --> 00:17:53,680
And if you're a deep learning person, you'll say almost all of that work is in figuring

206
00:17:53,680 --> 00:17:58,880
out what the shape of the network should be iterating over that.

207
00:17:58,880 --> 00:18:02,720
On that note, before we jump into what you're doing at the company today, what were some

208
00:18:02,720 --> 00:18:07,440
of the results you saw out of your research on the astronomy side?

209
00:18:07,440 --> 00:18:11,480
So we looked at a couple of different realms.

210
00:18:11,480 --> 00:18:18,520
One was looking at large catalogs of variable stars and coming up with probabilistic classifications

211
00:18:18,520 --> 00:18:23,560
of what type of variable stars they were, what was the physics that drove them.

212
00:18:23,560 --> 00:18:31,120
And we did that in a bootstrap way, starting with effectively a few hundred known classes

213
00:18:31,120 --> 00:18:33,280
and few hundred or a few thousand known labels.

214
00:18:33,280 --> 00:18:37,560
And then extrapolated that to tens of thousands, hundreds of thousands of variable stars and

215
00:18:37,560 --> 00:18:41,040
produced probabilistic catalogs.

216
00:18:41,040 --> 00:18:45,600
One of the things I became adamant about as we were doing that was producing a catalog

217
00:18:45,600 --> 00:18:51,440
where you say, hey, this object in the sky is of this type with this probability is effectively

218
00:18:51,440 --> 00:18:57,320
useless unless it's then used for some new kind of science.

219
00:18:57,320 --> 00:19:02,280
And one of the things that I became, I won't say frustrated with, but I noticed often

220
00:19:02,280 --> 00:19:08,400
is that people started using, not just in astronomy, but in many other fields, machine

221
00:19:08,400 --> 00:19:12,960
learning as an end to itself saying, I'm going to apply machine learning to this data.

222
00:19:12,960 --> 00:19:19,720
I'm going to get a result until that result itself is novel or until that becomes a stepping

223
00:19:19,720 --> 00:19:28,320
stone to another result, which becomes novel, it's sort of an empty exercise.

224
00:19:28,320 --> 00:19:32,640
And so what we want to be saying is what can we do with this probabilistic catalog that

225
00:19:32,640 --> 00:19:34,400
couldn't have been done with any other means.

226
00:19:34,400 --> 00:19:39,600
And so one of the things we did is we looked for very strange types of stars that had certain

227
00:19:39,600 --> 00:19:44,800
properties and then followed those up with big telescopes and actually wrote science papers

228
00:19:44,800 --> 00:19:45,800
with those.

229
00:19:45,800 --> 00:19:48,760
So we use that as a launching off point.

230
00:19:48,760 --> 00:19:52,800
In a real time environment where we actually were looking at images as they were streaming

231
00:19:52,800 --> 00:20:00,680
off of telescopes in Southern California off of Palomar Mountain, every 60 seconds or so,

232
00:20:00,680 --> 00:20:05,800
we would basically get transferred up to Lawrence Berkeley National Lab and we'd apply our machine

233
00:20:05,800 --> 00:20:11,520
learning to that to find new interesting objects in the sky and then populate databases

234
00:20:11,520 --> 00:20:14,880
of, you know, for tonight here are the interesting objects.

235
00:20:14,880 --> 00:20:18,200
And then also had another machine learning code which would go into those databases and

236
00:20:18,200 --> 00:20:23,880
periodically make statements about what types of objects those wind up might be, what

237
00:20:23,880 --> 00:20:25,320
they could be.

238
00:20:25,320 --> 00:20:32,040
And we went up having, I think of order of 100, maybe 200 papers that came out of that,

239
00:20:32,040 --> 00:20:36,920
a refereed papers, which again, the machine learning part of that was really the stepping

240
00:20:36,920 --> 00:20:41,120
stone to discovery, the other parts of the machine learning were the stepping stones

241
00:20:41,120 --> 00:20:43,120
to initial inference.

242
00:20:43,120 --> 00:20:49,240
And obviously in the end, you needed people to actually write the paper, but we tried

243
00:20:49,240 --> 00:20:50,760
it to the grad students, right?

244
00:20:50,760 --> 00:20:54,840
It all goes back to grad students, exactly.

245
00:20:54,840 --> 00:21:01,040
But I really thought about kind of removing people from the real time inference loop and

246
00:21:01,040 --> 00:21:04,160
getting as far up the inference stack as we could.

247
00:21:04,160 --> 00:21:08,160
We even got to the point where we were finding interesting objects in the sky without any

248
00:21:08,160 --> 00:21:12,600
humans in the loop, identifying that not only is it a new object, but it's something

249
00:21:12,600 --> 00:21:14,280
we probably should be following up.

250
00:21:14,280 --> 00:21:17,640
And we were ishing alerts to robotic telescopes to go follow those up.

251
00:21:17,640 --> 00:21:21,840
So by the time people woke up in the morning, we not only had the discovery, we not only

252
00:21:21,840 --> 00:21:28,560
had the initial inference, we then also had real follow-up, scientific follow-up.

253
00:21:28,560 --> 00:21:33,840
One of the, I think, great achievements of the work that we and others did in one of

254
00:21:33,840 --> 00:21:40,840
our collaborations was to build this production system that had real consumers on the other

255
00:21:40,840 --> 00:21:46,680
side of that, and when it was broken or was wrong or didn't take feedback properly, you

256
00:21:46,680 --> 00:21:52,360
know, we'd get nasty emails from our collaborators and say, your thing didn't work for an hour.

257
00:21:52,360 --> 00:21:54,840
You kind of screwed my science while I was at the telescope.

258
00:21:54,840 --> 00:22:03,680
So having an end user really keeps you heavily focused on making sure the things that it

259
00:22:03,680 --> 00:22:06,680
needs to do does it right and robustly.

260
00:22:06,680 --> 00:22:11,600
But because we were discovering things even faster than a whole army of grad students

261
00:22:11,600 --> 00:22:16,080
would have been able to pour over all of these images, we were able to find, for instance,

262
00:22:16,080 --> 00:22:23,000
the nearest type one, a supernova that had been found in 25 years and get a whole bunch

263
00:22:23,000 --> 00:22:26,880
of people looking at that part of the sky and taking lots of data that led to a bunch

264
00:22:26,880 --> 00:22:30,040
of papers in nature and science.

265
00:22:30,040 --> 00:22:34,400
Not because that object wouldn't have been found by even amateurs because it got so bright

266
00:22:34,400 --> 00:22:38,520
you could have seen it with binoculars eventually.

267
00:22:38,520 --> 00:22:44,480
But because the interesting science happened hours after the event blew up, right?

268
00:22:44,480 --> 00:22:45,720
The event happened.

269
00:22:45,720 --> 00:22:50,800
And so it wind up also driving home for me the need for not only something that's working

270
00:22:50,800 --> 00:22:56,800
and is robust, et cetera, but where it's able to make statements quickly and do it in

271
00:22:56,800 --> 00:22:59,520
a way that's reliable.

272
00:22:59,520 --> 00:23:00,520
Interesting.

273
00:23:00,520 --> 00:23:08,840
So I'm sure that that has led you to some interesting perspectives on the relationship

274
00:23:08,840 --> 00:23:15,920
between this technology and society and jobs and stuff like that.

275
00:23:15,920 --> 00:23:23,920
I'm hearing parallels to a lot of people kind of projecting that as AI is deployed, shifting

276
00:23:23,920 --> 00:23:29,760
shifts in the job market will take place that put a lot of people out of work.

277
00:23:29,760 --> 00:23:35,920
But I'm also hearing in your example kind of the counter-argument you often hear that

278
00:23:35,920 --> 00:23:40,840
really what it does is it empowers people and allows people to do different things that

279
00:23:40,840 --> 00:23:45,880
I don't necessarily want to go deep into the society stuff at this point.

280
00:23:45,880 --> 00:23:52,200
But yeah, it's certainly a valid concern.

281
00:23:52,200 --> 00:24:00,240
What we do in our company at WiseIO is help customers support agents become more efficient

282
00:24:00,240 --> 00:24:07,440
at their work by suggesting answers of how they can respond to an incoming inquiry,

283
00:24:07,440 --> 00:24:15,440
by automatically triaging, incoming increase or tickets, emails, et cetera, that is getting

284
00:24:15,440 --> 00:24:18,880
them to the right person or the right group who's going to be the best at answering that

285
00:24:18,880 --> 00:24:19,880
question.

286
00:24:19,880 --> 00:24:24,280
And then in some cases, we will automatically respond to incoming increase.

287
00:24:24,280 --> 00:24:31,000
So when you write into e-commerce site and say my package didn't arrive, there's a

288
00:24:31,000 --> 00:24:37,120
growing chance that us or somebody else may be answering what looks like a bespoke

289
00:24:37,120 --> 00:24:41,360
question of yours with what looks like a bespoke answer.

290
00:24:41,360 --> 00:24:48,240
But in the end, it's just a templatized response that we ourselves are using.

291
00:24:48,240 --> 00:24:52,200
For us to be able to do that, obviously, we can talk more deeply about how that works

292
00:24:52,200 --> 00:24:54,520
from an AI perspective.

293
00:24:54,520 --> 00:24:57,680
We have to get very confident in what our answers are.

294
00:24:57,680 --> 00:25:04,960
But what does this mean on one side to your question about labor displacement companies

295
00:25:04,960 --> 00:25:07,720
don't need to hire as many support agents?

296
00:25:07,720 --> 00:25:09,960
So where would they have gone?

297
00:25:09,960 --> 00:25:16,000
The other side of that is that the agents that they do have become better and more tuned

298
00:25:16,000 --> 00:25:21,680
at working in some of the harder parts of what their own products are about and what

299
00:25:21,680 --> 00:25:25,800
their customer complaints are about, in a way they wouldn't have been able to because

300
00:25:25,800 --> 00:25:27,840
they would have been distracted by the mundane.

301
00:25:27,840 --> 00:25:33,040
So if you say, how do I reset my password and there's a person or sets of people that

302
00:25:33,040 --> 00:25:37,280
have to look at that email and decide how to respond, that's time that those people are

303
00:25:37,280 --> 00:25:42,400
not spending on really complex problems where empathy is required as well.

304
00:25:42,400 --> 00:25:51,760
So we think of it as our product and what we do as a way of freeing people to work on

305
00:25:51,760 --> 00:25:55,800
the things that people are uniquely suited at that machines really aren't going to be

306
00:25:55,800 --> 00:26:01,200
that good until somebody solves the turing test.

307
00:26:01,200 --> 00:26:06,400
Chat bots are not going to be able to understand people in the subtle ways that they need to.

308
00:26:06,400 --> 00:26:10,880
But we can take a lot of easy stuff off the table.

309
00:26:10,880 --> 00:26:14,240
So there was certainly a concern as we were starting to roll this out that we were part

310
00:26:14,240 --> 00:26:19,760
of this labor displacement movement, but we heard time and time again from our customers

311
00:26:19,760 --> 00:26:26,000
that their support agents became more and more happy, the more involved we were.

312
00:26:26,000 --> 00:26:32,640
There was an entire team in Asia who had been tasked with basically reading an incoming

313
00:26:32,640 --> 00:26:38,680
inquiry or a ticket and then deciding who else should be reading this to solve the problem.

314
00:26:38,680 --> 00:26:45,400
And because our triage capability came into play, they effectively deprecated a 20-person

315
00:26:45,400 --> 00:26:50,800
team, one of our clients, off of triage because we're effectively automatically triaging

316
00:26:50,800 --> 00:26:51,800
now.

317
00:26:51,800 --> 00:26:56,040
And we were worried what's going to happen to these people, they have families to feed.

318
00:26:56,040 --> 00:27:01,040
And we got a whole bunch of really great quotes from them saying because they had been reassigned

319
00:27:01,040 --> 00:27:05,280
to actually work these support tickets rather than push them along, they were much more

320
00:27:05,280 --> 00:27:06,600
happy in their job.

321
00:27:06,600 --> 00:27:08,600
That's fantastic.

322
00:27:08,600 --> 00:27:15,600
So we jumped right into what you're doing at Wise.Oat.io, but the transition is a fascinating

323
00:27:15,600 --> 00:27:16,600
one as well.

324
00:27:16,600 --> 00:27:22,280
How do you get from astrophysics to software company doing CRM stuff?

325
00:27:22,280 --> 00:27:25,800
And I know there was an intermediate step there as well.

326
00:27:25,800 --> 00:27:32,760
So going back to the original part of the conversation, we had recognized in the team

327
00:27:32,760 --> 00:27:38,880
that I had built and the people I had worked with that A, we had some great sort of technical

328
00:27:38,880 --> 00:27:45,920
orthogonality, some were good software engineering, some good at ML, some UI, et cetera.

329
00:27:45,920 --> 00:27:53,840
And B, that what we had learned to do of recognizing the importance of putting AI into production

330
00:27:53,840 --> 00:28:00,600
and having real end users give real feedback in potentially a real-time loop was something

331
00:28:00,600 --> 00:28:03,480
we at the time didn't see anyone else doing.

332
00:28:03,480 --> 00:28:08,360
We knew obviously that the Googles and the LinkedIn's and the Netflix's of the world had

333
00:28:08,360 --> 00:28:14,280
this kind of baked in to their overall data flow, but we certainly weren't seeing companies

334
00:28:14,280 --> 00:28:16,840
helping other companies do it.

335
00:28:16,840 --> 00:28:24,520
And one of my now co-founders had more or less, while he was between jobs, figured out how

336
00:28:24,520 --> 00:28:29,040
to make one of the algorithms that we liked a lot, these decisions for our scale very,

337
00:28:29,040 --> 00:28:33,080
very well, at least on a single machine in a multi-core environment.

338
00:28:33,080 --> 00:28:39,080
And so we realized that we might have some interesting firepower and given that there

339
00:28:39,080 --> 00:28:45,520
seemed at the time to be so much emphasis on massive-scale machine learning.

340
00:28:45,520 --> 00:28:50,800
It was certainly a pre-spark era, but it was very much in the Hadoop heyday.

341
00:28:50,800 --> 00:28:54,520
It looked like most of the interesting ML that was starting to come out and some of the

342
00:28:54,520 --> 00:28:59,520
other companies that were coming out were really focused around helping the, you know,

343
00:28:59,520 --> 00:29:05,520
I won't say exit scale, but certainly pediscale level, Google scale, amount of data companies

344
00:29:05,520 --> 00:29:08,280
bring machine learning into their workflows.

345
00:29:08,280 --> 00:29:13,680
So we thought about sort of skating to a place, you know, using the analogy that's heavily

346
00:29:13,680 --> 00:29:19,440
overused to the part of where the puck was going to be, which was helping smaller companies

347
00:29:19,440 --> 00:29:25,960
and mid-sized scale companies bring machine learning into production environments.

348
00:29:25,960 --> 00:29:30,720
And that was the impetus for starting the company.

349
00:29:30,720 --> 00:29:32,120
What the domain was going to be?

350
00:29:32,120 --> 00:29:36,400
We didn't know who the customer was going to be and who the buyer was going to be.

351
00:29:36,400 --> 00:29:37,560
We didn't know.

352
00:29:37,560 --> 00:29:42,200
We were, I'd say blissfully ignorant about all of the business challenges that we would

353
00:29:42,200 --> 00:29:46,840
wind up encountering over the next couple of years in bringing this to market.

354
00:29:46,840 --> 00:29:55,080
And when we emerged out of our first accelerator, I gave a talk, our demo day was the alchemist

355
00:29:55,080 --> 00:30:01,760
accelerator where I said we're going to be GitHub for data scientists and produce some

356
00:30:01,760 --> 00:30:08,120
interesting UIs of interactions to help data scientists like ourselves more easily build

357
00:30:08,120 --> 00:30:12,040
models that they could then push into a production environment.

358
00:30:12,040 --> 00:30:17,160
We wind up seeing over the next couple of months the challenges of selling products like this

359
00:30:17,160 --> 00:30:22,480
into data science teams, first data science teams were few and far between.

360
00:30:22,480 --> 00:30:26,160
And the ones that existed were either too sophisticated to believe they could build it

361
00:30:26,160 --> 00:30:30,400
all themselves or not sophisticated enough to get a large enough budget to pay for the

362
00:30:30,400 --> 00:30:35,040
things that we wanted to provide them from a tooling perspective.

363
00:30:35,040 --> 00:30:40,560
All the while we were building out our underlying platform to be able to do exactly that, to

364
00:30:40,560 --> 00:30:46,000
be able to build templated machine learning models against certain types of data for

365
00:30:46,000 --> 00:30:48,160
certain types of use cases.

366
00:30:48,160 --> 00:30:53,880
And then even though you built it at a laptop level, push it into the cloud and have, you

367
00:30:53,880 --> 00:31:02,120
know, in Amazon or other compute frameworks, the scalability to be able to serve large

368
00:31:02,120 --> 00:31:07,800
numbers of customers around the same use cases where what's emerged for us is the difference

369
00:31:07,800 --> 00:31:12,680
between a customer is not new code, it's just a config file if they're using that same

370
00:31:12,680 --> 00:31:13,840
use case.

371
00:31:13,840 --> 00:31:19,360
So all of that to say that we evolved, you could call it a pivot, if you'd like, but

372
00:31:19,360 --> 00:31:25,000
I think of it as a series of of pivots to a place where we wind up seeing in customer

373
00:31:25,000 --> 00:31:35,800
support, a lot of data, a lot of manual work and some really nice CRM systems with open

374
00:31:35,800 --> 00:31:38,880
APIs and a fairly fixed schema.

375
00:31:38,880 --> 00:31:46,520
So the sales forces and Zendes and service now's of the world really are the data lake and

376
00:31:46,520 --> 00:31:52,040
the transactional layer for doing customer support and related activities.

377
00:31:52,040 --> 00:31:57,760
And we thought if we could build now an intelligent system on top of that and do all the things

378
00:31:57,760 --> 00:32:01,280
that I mentioned in powering these agents to become more efficient of their job and make

379
00:32:01,280 --> 00:32:06,640
the whole support desk more efficient at serving customers, we would solve a bunch of

380
00:32:06,640 --> 00:32:07,640
pain points.

381
00:32:07,640 --> 00:32:12,240
And that as we wind up going into the market and started leading with products that could

382
00:32:12,240 --> 00:32:17,120
be more or less installed by a non-technical user and could be used by a non-technical

383
00:32:17,120 --> 00:32:22,280
user, wind up getting a lot of feedback that indeed we were solving some pain points.

384
00:32:22,280 --> 00:32:26,320
There's obviously the efficiency question of needing less head count, but there's also

385
00:32:26,320 --> 00:32:31,600
some really interesting customers of ours who are growing very quickly.

386
00:32:31,600 --> 00:32:36,040
And one of them said to us that if the CEO had given him an infinite budget, he wouldn't

387
00:32:36,040 --> 00:32:40,640
be able to hire good customer support agents quick enough.

388
00:32:40,640 --> 00:32:45,440
So helping them capture all the internal knowledge was something that it turns out machine

389
00:32:45,440 --> 00:32:49,360
learning actually does quite well at.

390
00:32:49,360 --> 00:32:54,480
What were some of the biggest challenges in going from product direction focused on

391
00:32:54,480 --> 00:33:00,400
generalize tools and platforms to one focused on a very specific application area?

392
00:33:00,400 --> 00:33:05,720
I mean interestingly, it was all the things that we hadn't thought about which was product

393
00:33:05,720 --> 00:33:11,200
management and how do you get structured feedback from customers?

394
00:33:11,200 --> 00:33:17,040
What does it mean to build an MVP, roll that out, iterate on it, etc.

395
00:33:17,040 --> 00:33:21,960
A lot of kind of lean startup 101 stuff was something that we hadn't really been thinking

396
00:33:21,960 --> 00:33:28,400
of when we started the company and certainly didn't have frankly a lot of expertise in.

397
00:33:28,400 --> 00:33:36,120
And then as we started scaling, it was a recognition that there are large parts of a machine

398
00:33:36,120 --> 00:33:40,600
learning pipeline that don't naturally scale.

399
00:33:40,600 --> 00:33:47,400
So figuring out ways to containerize the parts that need special attention from PhD levels

400
00:33:47,400 --> 00:33:54,200
and data science and abstract that away from other parts of our engineering group that

401
00:33:54,200 --> 00:33:59,000
don't need to know about what's happening deeply but need to be able to ask predictions

402
00:33:59,000 --> 00:34:03,680
of some other part of the stack, restfully in a services oriented way.

403
00:34:03,680 --> 00:34:09,800
We just want to realizing that what worked for us from a scaling perspective, compute

404
00:34:09,800 --> 00:34:14,880
scaling perspective also wound up being what we needed to do from an organizational and

405
00:34:14,880 --> 00:34:23,760
HR perspective, when we hire front-end engineers and middleware engineers who create it writing

406
00:34:23,760 --> 00:34:29,080
scripts against databases and managing reticues, etc.

407
00:34:29,080 --> 00:34:31,680
Those folks don't need to know about machine learning.

408
00:34:31,680 --> 00:34:37,680
They need to know that there is a contract between their part of the stack and somewhere

409
00:34:37,680 --> 00:34:43,040
deeper in the stack that if I ask you for a prediction for this client, for this model,

410
00:34:43,040 --> 00:34:48,000
I'm expecting to get it back in this format on this time scale and if I don't, then

411
00:34:48,000 --> 00:34:49,400
our contract's broken.

412
00:34:49,400 --> 00:34:53,640
But likewise, I'm going to hand to that deeper part of the stack that's going to be providing

413
00:34:53,640 --> 00:35:00,040
those predictions effectively some data and JSON or otherwise that will have a fixed schema

414
00:35:00,040 --> 00:35:06,520
so that the group that built that machine learning pipeline knows that this column is going

415
00:35:06,520 --> 00:35:11,920
to be of type date time, this column is going to be an int and it's going to join using

416
00:35:11,920 --> 00:35:14,920
these four indices on some other data.

417
00:35:14,920 --> 00:35:21,200
So once we wind up realizing that we could lock down the schema for a given use case, it

418
00:35:21,200 --> 00:35:28,320
meant that we could write data science pipelines against data we hadn't seen before.

419
00:35:28,320 --> 00:35:32,360
You need to see it once to make sure it's all working and make sure it crossvalidates

420
00:35:32,360 --> 00:35:37,200
in an offline sense and it has the kinds of accuracy properties that you want.

421
00:35:37,200 --> 00:35:42,040
But then it means that we could basically start spinning up new customers where they get

422
00:35:42,040 --> 00:35:49,000
the base template that operates on their data and when we need to make changes, those

423
00:35:49,000 --> 00:35:56,720
can happen really from a more or less technical person than somebody with a PhD in statistics.

424
00:35:56,720 --> 00:36:00,840
So there were a bunch of challenges around that and as we started solving those, it just

425
00:36:00,840 --> 00:36:08,760
sort of fell out that our stack really mimics what our organization looks like.

426
00:36:08,760 --> 00:36:14,240
Can you talk a little bit about the data that you typically see in a customer environment?

427
00:36:14,240 --> 00:36:20,160
I'm imagining just loads of trouble tickets but I imagine as well that there's ancillary

428
00:36:20,160 --> 00:36:25,040
data supporting data as well and you mentioned that there's lots of it.

429
00:36:25,040 --> 00:36:28,360
Can you talk about the size you typically see, those kinds of things?

430
00:36:28,360 --> 00:36:40,640
Yeah, so our typical customer is doing a quarter of five to 20,000 tickets a month and

431
00:36:40,640 --> 00:36:46,280
we need to be working with companies that are achieving that level of volume A because

432
00:36:46,280 --> 00:36:49,960
the price points are reasonably high and so it's typically the companies that have those

433
00:36:49,960 --> 00:36:55,560
large volumes that are willing to pay for what we do and B because the machine learning

434
00:36:55,560 --> 00:37:03,400
models are built specifically for and on their data and we don't use a common model for

435
00:37:03,400 --> 00:37:05,200
instance across our customers.

436
00:37:05,200 --> 00:37:09,040
So we need a lot of training data for a given customer.

437
00:37:09,040 --> 00:37:12,080
Now this isn't again, this is not pediscale amounts of data.

438
00:37:12,080 --> 00:37:19,240
We're talking sort of tens to hundreds of gigabytes at the per month level per customer.

439
00:37:19,240 --> 00:37:27,120
The data is indeed a lot of human interaction from emails, web forums, even chat and there's

440
00:37:27,120 --> 00:37:28,400
also a lot of metadata.

441
00:37:28,400 --> 00:37:35,000
So what is the value of this customer, what products are they using, how often have they

442
00:37:35,000 --> 00:37:41,760
been emailing so there's a time series component to this as well and we've had to build these

443
00:37:41,760 --> 00:37:46,680
pipelines that are generic enough that we can then apply them to other use cases but

444
00:37:46,680 --> 00:37:53,520
specific enough that they give good enough accuracies that wind up rivaling what humans

445
00:37:53,520 --> 00:37:54,520
can do.

446
00:37:54,520 --> 00:38:00,640
And so oftentimes our goal is to get to not, we don't call it accuracy, we call it matching

447
00:38:00,640 --> 00:38:04,880
capability because oftentimes when a human's labeling something and saying it belongs

448
00:38:04,880 --> 00:38:09,000
to this bucket or this person should answer it or it should have been answered with this

449
00:38:09,000 --> 00:38:11,480
template, they oftentimes can be wrong.

450
00:38:11,480 --> 00:38:13,280
So we want to get all that experience.

451
00:38:13,280 --> 00:38:18,920
We want to get ourselves to that kind of level of quality, let's say.

452
00:38:18,920 --> 00:38:23,560
So it's mostly from a from a featureization perspective, we're doing lots of natural

453
00:38:23,560 --> 00:38:30,240
language processing, getting it to the point of sort of rectangularized data where each

454
00:38:30,240 --> 00:38:35,760
row is a different instance and each column is a set of features and then we have a bunch

455
00:38:35,760 --> 00:38:37,640
of labels of what the answers are.

456
00:38:37,640 --> 00:38:44,520
So we're working almost entirely in a supervised sense where we know from past data to particularly

457
00:38:44,520 --> 00:38:48,640
closed tickets what the actual right answer is quote unquote.

458
00:38:48,640 --> 00:38:53,760
We've got a couple of unsupervised models that we also wind up running where we wind up

459
00:38:53,760 --> 00:39:01,000
discovering for instance that there is a grouping and semantic space of outgoing tickets

460
00:39:01,000 --> 00:39:06,200
that is how agents are responding that don't look like templates that are sanctioned by

461
00:39:06,200 --> 00:39:11,640
the company which means that they're coming up with their own templated responses and

462
00:39:11,640 --> 00:39:13,640
potentially even sharing those with other agents.

463
00:39:13,640 --> 00:39:17,600
So we have a dashboard for instance, we show our customers the one that was running the

464
00:39:17,600 --> 00:39:22,720
support desk of potentially new templates that they can use because obviously if there's

465
00:39:22,720 --> 00:39:27,920
a new issue for instance with a product then agents who are on the ground have to figure

466
00:39:27,920 --> 00:39:32,520
out a way to answer it and if it's a recurring problem within a couple of emails that wind

467
00:39:32,520 --> 00:39:36,880
up essentially having the right answer that they've already pre-formulated.

468
00:39:36,880 --> 00:39:39,160
So that's an unsupervised problem.

469
00:39:39,160 --> 00:39:46,760
And do you see in that last example a feature place for generative types of AI approaches

470
00:39:46,760 --> 00:39:51,160
or is that more do you think of when you hear that is that like the technology you know

471
00:39:51,160 --> 00:39:54,000
looking for chasing the problem kind of thing?

472
00:39:54,000 --> 00:39:55,360
Yeah it's a good question.

473
00:39:55,360 --> 00:40:00,840
We've shied away from the generative component and in fact we make that a big part of our

474
00:40:00,840 --> 00:40:07,760
sales pitch of to say you are a potential client really know the voice that you want to

475
00:40:07,760 --> 00:40:14,440
speak in and speak with your customers and it's who is it for us to come in and say we're

476
00:40:14,440 --> 00:40:19,920
going to auto-generate at the character level you know CNNs or something a bespoke answer

477
00:40:19,920 --> 00:40:25,120
the way that you know Google inbox does well if it gives you five different words sure

478
00:40:25,120 --> 00:40:29,720
all sounds good I'll see you then or how about Friday.

479
00:40:29,720 --> 00:40:37,280
Those are fine but because we're really focused on not just getting results into the hands

480
00:40:37,280 --> 00:40:41,960
of agents where they can actually see in a UI sense within the dashboards they normally

481
00:40:41,960 --> 00:40:46,000
see what our predictions are and consume it in a way that they like to.

482
00:40:46,000 --> 00:40:50,480
We also want to take a lot of these tickets off the table in an automatic sense.

483
00:40:50,480 --> 00:40:55,040
The only way our customers get comfortable with that is if we're basically showing to

484
00:40:55,040 --> 00:41:01,240
them in an offline way here's our accuracies for these types of templates so we're going

485
00:41:01,240 --> 00:41:05,440
to send every now and then somebody says I'm very unhappy with what you've done we're

486
00:41:05,440 --> 00:41:09,440
going to said thanks for your feedback when it should have gone a different path but

487
00:41:09,440 --> 00:41:15,320
we're only going to do that you know 1% of the time at this at this level of false positive

488
00:41:15,320 --> 00:41:20,120
and once we can do that then our customers essentially can turn on a specific macro for

489
00:41:20,120 --> 00:41:21,920
us to auto respond with.

490
00:41:21,920 --> 00:41:26,160
The idea that we're going to auto respond without any agents in the loop to something like

491
00:41:26,160 --> 00:41:30,800
that to potentially I rate customer is pretty challenging.

492
00:41:30,800 --> 00:41:35,160
So we don't certainly want to rule it out but we certainly don't think about ourselves

493
00:41:35,160 --> 00:41:39,280
as producing gendered advances in a bespoke way.

494
00:41:39,280 --> 00:41:43,520
We're just more or less turning all of our problem into multi-class classification problems

495
00:41:43,520 --> 00:41:48,320
of what of the hundreds or potentially thousands of canned responses is the right one to answer

496
00:41:48,320 --> 00:41:49,320
with.

497
00:41:49,320 --> 00:41:55,960
And just so I understand the comment that you made a second ago in terms of sending

498
00:41:55,960 --> 00:42:03,880
out a given response a small percentage of the times are you describing an errors type

499
00:42:03,880 --> 00:42:11,880
of situation or you describing a feature where you're like an exploration type of feature.

500
00:42:11,880 --> 00:42:12,880
Good point.

501
00:42:12,880 --> 00:42:18,320
So we try not to do we there's an explore exploit component to what we do in a multi-armed

502
00:42:18,320 --> 00:42:26,600
bandit sense that's typically not you know expose or a knob that's tunable by our customers.

503
00:42:26,600 --> 00:42:30,920
So that will happen and some of that will happen naturally in the case of auto response

504
00:42:30,920 --> 00:42:36,040
we hold back 10% of the ones that work we know what the answer is or we believe we know

505
00:42:36,040 --> 00:42:41,000
with a certain threshold of confidence and then compare after the fact whether an agent

506
00:42:41,000 --> 00:42:45,920
who wound up now having to see it because we did not a respond gave the same response

507
00:42:45,920 --> 00:42:46,920
we did.

508
00:42:46,920 --> 00:42:52,440
There's an exploration where with the agents job is now to do the exploration implicitly.

509
00:42:52,440 --> 00:42:59,600
No that the one I was pointing out is ones where we are essentially wrong.

510
00:42:59,600 --> 00:43:04,120
And that gets back to the question of the loss function of what does it mean to be wrong?

511
00:43:04,120 --> 00:43:05,120
Right.

512
00:43:05,120 --> 00:43:10,600
If one of the canned responses is I'm so sorry for your loss I will refund your entire

513
00:43:10,600 --> 00:43:17,120
vacation you know in the amount of $10,000 the cost of being wrong of that is very very

514
00:43:17,120 --> 00:43:18,440
high.

515
00:43:18,440 --> 00:43:22,760
But if somebody is mad and says my vacation got ruined because of something you did I

516
00:43:22,760 --> 00:43:29,520
want my 10k back and we say thanks for your feedback it's being wrong on that side is

517
00:43:29,520 --> 00:43:32,400
not nearly as bad as being wrong on the other side of that.

518
00:43:32,400 --> 00:43:37,120
And so we give and empower our customers to basically make the decision about you know

519
00:43:37,120 --> 00:43:41,440
let's do the easy stuff where the cost of being wrong is not a big deal.

520
00:43:41,440 --> 00:43:46,720
But because and that's for the automatic response but for the recommended types of responses

521
00:43:46,720 --> 00:43:51,160
if our first canned responses here's your money back and an agent looks at that and says no

522
00:43:51,160 --> 00:43:54,800
that's crazy the right answer is farther down the list.

523
00:43:54,800 --> 00:43:59,280
They'll select that and that becomes the feedback that our model you know our models went

524
00:43:59,280 --> 00:44:02,800
up getting better as they wind up learning over time.

525
00:44:02,800 --> 00:44:09,560
What are some of the most interesting challenges that you've run into in putting together this

526
00:44:09,560 --> 00:44:17,840
kind of hybrid you know ML plus human solution like in one of the things that pops to my

527
00:44:17,840 --> 00:44:22,240
mind is you know just user experience user interface like are there challenges there

528
00:44:22,240 --> 00:44:28,640
that are interesting or you know what surprised you the most in trying to fill these types

529
00:44:28,640 --> 00:44:30,640
of solutions.

530
00:44:30,640 --> 00:44:37,320
Certainly because this we're getting into the space and the face of agents who do this

531
00:44:37,320 --> 00:44:43,600
all the time when we first started releasing our products we didn't have a good training

532
00:44:43,600 --> 00:44:44,600
program for them.

533
00:44:44,600 --> 00:44:48,840
And so when they would see even though what we thought was an intuitive set of responses

534
00:44:48,840 --> 00:44:53,520
in the form of widgets that would show up on their on their desktop you know they didn't

535
00:44:53,520 --> 00:44:57,360
know how to consume it and they didn't know how to use it as effectively as we thought

536
00:44:57,360 --> 00:45:04,440
they should you know there's all the mundane stuff around UIs like responsiveness and somebody

537
00:45:04,440 --> 00:45:08,520
saying well doesn't look like your products working because now there are no responses

538
00:45:08,520 --> 00:45:12,360
and we'd say well that's because you've already responded and you're bringing up a new

539
00:45:12,360 --> 00:45:17,200
ticket you bring up an old ticket that already has a whole conversation and we're only getting

540
00:45:17,200 --> 00:45:21,320
involved in at least for now in the first part of the conversation what's the first response

541
00:45:21,320 --> 00:45:25,880
you should do okay so then we weren't showing the results and so how can we you know

542
00:45:25,880 --> 00:45:30,760
to modify our widgets so that the agents understand we're not showing it for a purpose it's

543
00:45:30,760 --> 00:45:39,320
not that our system is is broken and then realizing also that many agents wanted parts

544
00:45:39,320 --> 00:45:50,040
of our UI that and UX were generally that doesn't have anything to do with ML so they wanted

545
00:45:50,040 --> 00:45:54,200
keyboard shortcuts because we thought everyone would just click on stuff but high velocity

546
00:45:54,200 --> 00:46:00,320
support desk wanted just to use the keyboard so having to build that in for a set of customers

547
00:46:00,320 --> 00:46:04,840
because they essentially is like the keyboard had some disease or the mouse had a disease

548
00:46:04,840 --> 00:46:13,320
on it they didn't want to touch it getting feedback from the UI itself back into our system

549
00:46:13,320 --> 00:46:19,160
making sure that we're getting the right metrics back making sure that the KPIs that

550
00:46:19,160 --> 00:46:24,520
we're measuring or that we're aligned with the KPIs that our customers wanted I think

551
00:46:24,520 --> 00:46:30,480
one of the hardest things for us and it frankly continues to be a challenge is really

552
00:46:30,480 --> 00:46:35,820
just thinking about how fault tolerant ML needs to happen and again Google going back to

553
00:46:35,820 --> 00:46:41,560
Google inbox for those of you that have used it it makes a couple of suggestions about

554
00:46:41,560 --> 00:46:46,200
how you could respond to an email if you don't want to use those you don't use it so

555
00:46:46,200 --> 00:46:53,160
I would call that a great fault tolerant ML experience and the same thing in a spam

556
00:46:53,160 --> 00:46:57,880
filter within your within your mail system it'll say we think this is spam if it's not

557
00:46:57,880 --> 00:47:01,960
move it over and then later on we'll figure out how not to call these things spam anymore

558
00:47:01,960 --> 00:47:07,640
that are like that that sort of fault tolerance where you're also getting feedback either

559
00:47:07,640 --> 00:47:13,520
implicitly or explicitly is just something we've had to build up over time but I think

560
00:47:13,520 --> 00:47:21,280
more broadly that that kind of approach needs to be built into any AI system in a production

561
00:47:21,280 --> 00:47:26,840
environment unless the AI outputs that you're building are going to be consumed entirely

562
00:47:26,840 --> 00:47:33,560
by machines you need to have some level of understanding of who it is that's going

563
00:47:33,560 --> 00:47:38,640
to be consuming it what are their concerns and how can they give you feedback so that

564
00:47:38,640 --> 00:47:44,960
your models one of getting better over time can you talk a little bit about the algorithms

565
00:47:44,960 --> 00:47:51,960
that you're employing and the the tool chain the pipelines what does all that look like

566
00:47:51,960 --> 00:47:58,240
yeah so we we stay out of what we call the algorithms arms race internally a we're not

567
00:47:58,240 --> 00:48:03,720
really selling the the platforms other data scientists it gives us the freedom to focus

568
00:48:03,720 --> 00:48:11,880
on parts of the of the pipeline that we we find most important all of our algorithmic

569
00:48:11,880 --> 00:48:18,120
sort of learning parts and then prediction parts are built in C++ and then surface

570
00:48:18,120 --> 00:48:23,640
back out into Python which is where the data science team winds up working we have our

571
00:48:23,640 --> 00:48:30,160
own notion of what a pipeline needs to be and the data science team works entirely within

572
00:48:30,160 --> 00:48:34,960
that the confines of what that pipeline ought to be which is some sort of pre filtering

573
00:48:34,960 --> 00:48:41,160
so for instance if a if a ticket is from a voicemail don't predict on it or don't use

574
00:48:41,160 --> 00:48:46,720
it for a build so you get rid of those that have this in this column this value then there's

575
00:48:46,720 --> 00:48:51,920
the data transformation parts of that in the joining across multiple across multiple

576
00:48:51,920 --> 00:49:00,360
datasets if that's needed and then the featureization which we'll often use open source tools

577
00:49:00,360 --> 00:49:07,640
for that in the Python ecosystem pandas is that we use very regularly and then once we

578
00:49:07,640 --> 00:49:12,320
want to realizing that we've created a bottleneck which typically will happen not so much in

579
00:49:12,320 --> 00:49:19,960
time but in ram usage we'll wind up rewriting other people's algorithms or code so that

580
00:49:19,960 --> 00:49:27,920
we create you know a ram efficient pipeline and then once the featureization happens basically

581
00:49:27,920 --> 00:49:32,840
the learning winds up happening in the C++ layer and we've built a whole bunch of hyper

582
00:49:32,840 --> 00:49:38,440
parameter optimizations and feature selection capabilities and then post process capabilities

583
00:49:38,440 --> 00:49:46,080
to get calibrated probabilities out of a multi-class problem so we have a bunch of pieces

584
00:49:46,080 --> 00:49:51,240
that we've been building up that are not in the open world and something that we've decided

585
00:49:51,240 --> 00:49:59,160
not to open source for now that allow us to work efficiently so we think of it as high

586
00:49:59,160 --> 00:50:03,920
velocity data science and building out a template for the first time but then because the

587
00:50:03,920 --> 00:50:09,440
models have to rebuild every single day for every single customer on you know cloud infrastructure

588
00:50:09,440 --> 00:50:16,560
which is not super cheap we needed to make the cost of doing that as small as possible

589
00:50:16,560 --> 00:50:23,160
and what we want up realizing is that open source tools you know that many people use

590
00:50:23,160 --> 00:50:31,840
like the psychic learns and the Tories slash dotes of the world or even Spark ML were vastly

591
00:50:31,840 --> 00:50:37,400
more costly to run even if you could do it in the same amount of time which we think

592
00:50:37,400 --> 00:50:42,560
we're much much faster than most of those tools because of the ram requirements needed

593
00:50:42,560 --> 00:50:48,240
on multiple machines or even a large single machine in Amazon the cost of building a model

594
00:50:48,240 --> 00:50:57,200
just was x percent higher and x being you know in the thousands so having a ram efficient

595
00:50:57,200 --> 00:51:01,080
speed efficient and obviously again getting back to the original conversation about table

596
00:51:01,080 --> 00:51:06,560
six highly accurate set of algorithms which produce the kinds of answers we want that

597
00:51:06,560 --> 00:51:11,920
we could then get into and modify if we needed to was kind of where we went up settling

598
00:51:11,920 --> 00:51:18,040
as where we needed to spend our kind of R&D slash engineering time now one of the areas

599
00:51:18,040 --> 00:51:24,680
that many of the machine learning platform companies have focused on is trying to close

600
00:51:24,680 --> 00:51:32,960
this gap between data science and production yep and in essence eliminate the hey I've got

601
00:51:32,960 --> 00:51:38,280
this model that kind of works though it over the wall to developers and have it implemented

602
00:51:38,280 --> 00:51:46,280
and it sounds like you guys have maybe embraced that and you're using that as a way to build

603
00:51:46,280 --> 00:51:56,040
out the models in C++ for presumably for performance are there ways that you've been compensated

604
00:51:56,040 --> 00:52:02,320
for that in terms of automation tooling or do you just accept that or you know even you

605
00:52:02,320 --> 00:52:06,440
know we just have the you know the best people on both sides of that fence that can deal

606
00:52:06,440 --> 00:52:13,960
with the you know the existence of the gap like how do you maintain a level of efficiency

607
00:52:13,960 --> 00:52:19,680
and innovation in terms of the development pipeline not the machine learning pipeline

608
00:52:19,680 --> 00:52:24,880
so that it all works for you yeah so there's definitely this separation of concerns which

609
00:52:24,880 --> 00:52:32,920
again is both an organizational one and then is also a computational one to the level

610
00:52:32,920 --> 00:52:38,720
where we think we often talk about what we call the organizational API of who within

611
00:52:38,720 --> 00:52:44,480
this stack is the customer of who and so for instance the people who are the sort of

612
00:52:44,480 --> 00:52:49,920
core ML and algorithms folks in the company are working in C++ and surfacing the great

613
00:52:49,920 --> 00:52:59,400
results back into Python layer their customer is the data science team the customer of

614
00:52:59,400 --> 00:53:07,200
the data science team is the people working on our architecture who have to maintain you

615
00:53:07,200 --> 00:53:13,720
know this the scalable robust infrastructure and you know their customers are the people

616
00:53:13,720 --> 00:53:18,800
working in the middle where and their customers are the ones who are in the UI and so each

617
00:53:18,800 --> 00:53:24,000
of them have a set of contracts of what it is that each part of that stack is looking

618
00:53:24,000 --> 00:53:30,240
for and and how in fact they're supposed to engage with each other and that's become

619
00:53:30,240 --> 00:53:35,400
very very helpful for us because you know what you find is that when you put somebody in

620
00:53:35,400 --> 00:53:41,000
a box they figure out a way to innovate very highly within that box so if there is a very

621
00:53:41,000 --> 00:53:45,000
strong contract of what data is expected to come in and what data is expected to come

622
00:53:45,000 --> 00:53:49,680
out and everything in between there is really up to you to decide how to do this well and

623
00:53:49,680 --> 00:53:54,920
efficiently that's where for instance our data science team and implementation team will

624
00:53:54,920 --> 00:54:00,040
wind up working and building out a new template they can work at their laptop or glorified

625
00:54:00,040 --> 00:54:06,240
laptop level on a toy data set get some confidence that the pipeline is working offline accuracies

626
00:54:06,240 --> 00:54:11,200
look good and the whole thing is going to work and they once they're comfortable with

627
00:54:11,200 --> 00:54:17,320
that they literally are just pushing a new version of a of a Docker image into our registry

628
00:54:17,320 --> 00:54:21,720
which then farther upstream from something they ever have to think about from a production

629
00:54:21,720 --> 00:54:27,000
sense once a new build winds up getting kicked off for that customer for that type of template

630
00:54:27,000 --> 00:54:31,640
the new the new image will just get pulled and it'll just get built with the config file

631
00:54:31,640 --> 00:54:37,720
for that customer and so the data science team can wind up working within their confines

632
00:54:37,720 --> 00:54:42,160
and of course we have a whole testing suite to make sure that if they build something

633
00:54:42,160 --> 00:54:47,680
new they're not going to break something downstream from them gain confidence in that and

634
00:54:47,680 --> 00:54:52,520
then they're literally just pushing the results of what they're doing on a semi weekly

635
00:54:52,520 --> 00:54:58,800
basis into the Docker registry that becomes the latest template for let's say triage and

636
00:54:58,800 --> 00:55:04,400
then all the customers in production are automatically migrated to that so having the

637
00:55:04,400 --> 00:55:07,960
data science team be able to push stuff into production without having to be on the

638
00:55:07,960 --> 00:55:13,440
op side of things nor have to think about the architecture has a has really freed us

639
00:55:13,440 --> 00:55:21,640
up in great ways I think to innovate and likewise when they need a new beller and or whistle

640
00:55:21,640 --> 00:55:28,280
from the the core algorithm folks because they say this part of our entire a build chain

641
00:55:28,280 --> 00:55:35,160
is really inefficient they can ask of the people working on that to improve it and they

642
00:55:35,160 --> 00:55:39,400
go through their own testing suite and I think we're at 300,000 regression tests in our

643
00:55:39,400 --> 00:55:46,440
core ML we're also testing against every open source algorithm on customer data to make

644
00:55:46,440 --> 00:55:51,600
sure that we're staying as efficient or more on all these different axes before we

645
00:55:51,600 --> 00:55:56,320
cut a release then the data science team can just pull essentially Python egg from our

646
00:55:56,320 --> 00:56:05,040
registry and use that in their system so having those separations has been great obviously

647
00:56:05,040 --> 00:56:10,720
if you're abstracting everybody from what the ends use cases are there can be a huge danger

648
00:56:10,720 --> 00:56:17,800
but it's the job of people like myself to make sure that everyone is is focused and innovating

649
00:56:17,800 --> 00:56:25,440
towards the right set of goals. Oh great great I'm glad that Docker came up you guys

650
00:56:25,440 --> 00:56:32,440
published a you publish and maintain a set of Docker images for data science tools I've

651
00:56:32,440 --> 00:56:40,200
come across that my impression is that in general Docker adoption within the data science

652
00:56:40,200 --> 00:56:46,000
machine learning community is not particularly high is that years as well.

653
00:56:46,000 --> 00:56:51,600
Certainly haven't heard of many other companies using it in the ways that we are but it

654
00:56:51,600 --> 00:57:00,440
seems like such a natural way to literally containerize and abstract the work of one

655
00:57:00,440 --> 00:57:05,400
part of an organization from the other so long as they you know that container will respond

656
00:57:05,400 --> 00:57:10,680
with a slash build predict endpoint you know feedback endpoint etc in the way that everyone

657
00:57:10,680 --> 00:57:18,440
expects it to. I think that's a I think that's a wonderful way to do abstraction so and

658
00:57:18,440 --> 00:57:25,040
then obviously it also helps you wind up achieving scale because for us scale is not you know

659
00:57:25,040 --> 00:57:30,800
can we serve you know a billion of our customers with the same app it's instead well we've

660
00:57:30,800 --> 00:57:35,640
got a new customer that we just spin up more containers to do the builds and the predicts

661
00:57:35,640 --> 00:57:39,600
for that customer and if we need more compute capability that's elastically scaling for

662
00:57:39,600 --> 00:57:47,320
us for free on top of Amazon. So I think of a very natural way to separate concerns you

663
00:57:47,320 --> 00:57:52,320
know from a stack perspective and also a very natural way to do what is for a company

664
00:57:52,320 --> 00:57:58,320
that's serving lots and lots of customers a very embarrassingly parallel type of of

665
00:57:58,320 --> 00:58:03,840
compute. Interesting I got into a conversation on Twitter or Reddit or someplace where

666
00:58:03,840 --> 00:58:14,240
someone was kind of griping about just the dependency hell with Python and pandas and trying

667
00:58:14,240 --> 00:58:18,120
to come to terms with managing different versions of you know different tooling versions

668
00:58:18,120 --> 00:58:24,240
and things like that and I suggested I might have even pointed to your docker repository

669
00:58:24,240 --> 00:58:28,600
and the response was now I want to make this simpler not more complex and obviously you

670
00:58:28,600 --> 00:58:35,240
find it to be simpler can you give folks that aren't familiar with docker and containers

671
00:58:35,240 --> 00:58:41,160
like your 30 second you know docker for data science pitch and where they can learn

672
00:58:41,160 --> 00:58:52,720
more about it. Yeah so docker is a way of basically explicitly specifying what not

673
00:58:52,720 --> 00:58:59,920
only your let's say Python requirements are which you can do with a simple file but

674
00:58:59,920 --> 00:59:06,760
also what the entire OS shall be for running whatever scripts you're going to need and

675
00:59:06,760 --> 00:59:14,880
once you build that and you can confidence that that image is doing what it ought to you

676
00:59:14,880 --> 00:59:21,680
can essentially very rapidly turn a container on that is the almost instant instantiation

677
00:59:21,680 --> 00:59:28,600
of that entire OS plus that script and all of the dependencies built inside of that and

678
00:59:28,600 --> 00:59:36,080
you can hand somebody a link to the docker hub registry or if you maintain your own

679
00:59:36,080 --> 00:59:45,720
private registry explicit URI to that explicit version of that explicit image and more

680
00:59:45,720 --> 00:59:51,160
less guarantee that when they run that with whatever data is contained inside of that

681
00:59:51,160 --> 00:59:56,280
or whatever will be pulled over so long as it's the same you'll get the same answer out.

682
00:59:56,280 --> 01:00:02,600
So I tend to think from a data science workflow and then getting back to you know just

683
01:00:02,600 --> 01:00:12,040
doing science more generally docker is a very nice framework for reproducibility and

684
01:00:12,040 --> 01:00:17,400
so the idea that now I'm not I don't have to share a machine with you or an Amazon

685
01:00:17,400 --> 01:00:21,960
machine image with you I'm just handing you effectively a docker file and says if

686
01:00:21,960 --> 01:00:29,400
you run this you're going to want to get the same answer that I got but again because

687
01:00:29,400 --> 01:00:34,320
I don't think doing the types of work that we do and wise and in some cases what we do

688
01:00:34,320 --> 01:00:41,040
on the science side of things as the you know the final result is not what comes out

689
01:00:41,040 --> 01:00:47,600
of the docker image or container it's not okay here's a report of what my ROC curve is

690
01:00:47,600 --> 01:00:52,320
going to be my false positive versus false negative curve and then let me write a paper

691
01:00:52,320 --> 01:00:57,960
about that it needs to be for us at least in a production environment just now I've produced

692
01:00:57,960 --> 01:01:02,320
a prediction that now needs to get consumed by something that's farther downstream.

693
01:01:02,320 --> 01:01:08,000
So docker is quite nice in that sense as well because you can also now connect docker

694
01:01:08,000 --> 01:01:13,240
containers explicitly using something like a docker compose on there's many other tools

695
01:01:13,240 --> 01:01:18,640
out there as well so that containers talk to other containers and you allow each container

696
01:01:18,640 --> 01:01:23,800
to have again its own separated concern from the other ones but still pull the results

697
01:01:23,800 --> 01:01:28,600
and push results to the other the other ones around in addition some containers can just

698
01:01:28,600 --> 01:01:35,240
contain data and you can build databases around that data so now it allows you to build

699
01:01:35,240 --> 01:01:41,640
up a very lightweight version of what might be your entire stack and do this in a way

700
01:01:41,640 --> 01:01:48,040
that's programmable so we found that to be incredibly useful for testing purposes.

701
01:01:48,040 --> 01:01:51,960
So as your GitHub the place that someone can go to learn more about what you're doing

702
01:01:51,960 --> 01:02:00,720
or. Yeah so we've got a public docker registry that you can go to the docker registry

703
01:02:00,720 --> 01:02:07,880
in search for YZO or you can go to GitHub slash YZO and see our other public projects

704
01:02:07,880 --> 01:02:13,640
that we've pushed out so there's one around docker and data science which in that case

705
01:02:13,640 --> 01:02:19,240
because we're not releasing any of our internal tools we're basically building up a container

706
01:02:19,240 --> 01:02:24,120
with open source tools that we find are really useful for doing lots of different types

707
01:02:24,120 --> 01:02:31,080
of data science. The other major project which we have up there in GitHub that's open

708
01:02:31,080 --> 01:02:37,320
is something we call Paratax which started as just sort of a weekend hack from one of

709
01:02:37,320 --> 01:02:43,520
our engineers Damian Eeds who wanted to see what it would be like to read data from

710
01:02:43,520 --> 01:02:50,080
disk in parallel just to see what kind of speedups you could wind up getting and it turns

711
01:02:50,080 --> 01:02:55,040
out pretty much every open source tool out there doesn't read in parallel and the ones

712
01:02:55,040 --> 01:03:00,720
that do are explicitly parallelized like over multiple machines but if you just made

713
01:03:00,720 --> 01:03:06,000
multiple use of the multi-core environment how well would you get and we want to getting

714
01:03:06,000 --> 01:03:12,400
100,000 x speedups over some of the other tools that are out there and importantly also

715
01:03:12,400 --> 01:03:19,920
use vastly less memory that Paratax is not yet in our production environment but we thought

716
01:03:19,920 --> 01:03:25,600
it would be a good example of kind of showing off the philosophies that we try to adhere to

717
01:03:25,600 --> 01:03:31,760
within the company of creating efficiencies that isn't just the one thing like around accuracy

718
01:03:31,760 --> 01:03:38,480
but you know around how fast can you read data and how big is your model on disk all these

719
01:03:38,480 --> 01:03:43,360
other aspects of what it means to do machine learning that has nothing to do with the algorithm

720
01:03:44,000 --> 01:03:49,520
once you're happy and you've reached some level of plateau with the algorithm accuracy all

721
01:03:49,520 --> 01:03:56,160
that you're left to do is optimize all these other pieces of that pipeline and so a lot of our

722
01:03:56,160 --> 01:04:02,160
engineering over the last year in particular has moved away from just optimizing accuracy to

723
01:04:02,160 --> 01:04:07,840
you know things like creating interpretability around the models that we build

724
01:04:08,560 --> 01:04:14,480
making the models smaller on disk making the other parts of the featureization pipeline

725
01:04:15,200 --> 01:04:22,800
be more ram-efficient and once you start sort of playing whack-a-mole with let's just say ram usage

726
01:04:22,800 --> 01:04:28,080
you want to find really interesting parts of your entire pipeline that very few people

727
01:04:28,080 --> 01:04:32,880
want to talk about just you know again reading data which is should be the easiest part of your

728
01:04:32,880 --> 01:04:41,680
entire tool chain is vastly inefficient and you know whacking that mole turns out you save a whole

729
01:04:41,680 --> 01:04:47,280
bunch of Amazon costs because now you need a smaller ram machine that's great that's great

730
01:04:48,320 --> 01:04:54,320
you mentioned interpretability have you spent a lot of time working on that and what were the

731
01:04:54,320 --> 01:04:59,920
drivers for that we have spent a lot of time on that you know sort of one of what we think of

732
01:04:59,920 --> 01:05:08,080
as our trade secret one of our trade secrets around getting back to the question of UI and UX

733
01:05:08,080 --> 01:05:15,360
for end users we we were asked often at least in the early days well why are you getting the answer

734
01:05:15,360 --> 01:05:21,200
that you're getting and you can't say well you know it's a thousand dimensional feature space

735
01:05:21,200 --> 01:05:27,840
and there's covariance between all of these and you know the model importance is over the entire

736
01:05:27,840 --> 01:05:33,760
thing you know says that this is the most important feature I don't know why we said for this one

737
01:05:33,760 --> 01:05:40,240
what the answer is but that answer is what what's called in in the financial services world reason

738
01:05:40,240 --> 01:05:46,720
codes turns out to be really important some some some places it's actually regulatory required

739
01:05:46,720 --> 01:05:51,840
that you tell somebody why you got the answer that you got even if it's a machine learning black box

740
01:05:53,120 --> 01:05:59,920
and so some of our early R&D effort was around how to make at the instance level so an individual

741
01:05:59,920 --> 01:06:06,400
prediction level how do we make these models interpretable by saying these are the important

742
01:06:06,400 --> 01:06:14,080
features and these are what's driving this specific prediction so as an example if you're

743
01:06:14,080 --> 01:06:19,200
working on customer churn and you want to predict somebody going to churn in 90 days from now it's

744
01:06:19,200 --> 01:06:25,520
a use case that we've also used on our platform but not something we go to market with necessarily

745
01:06:27,200 --> 01:06:33,200
two customers can have an identical probability of churning but one of them may be churning because

746
01:06:33,200 --> 01:06:37,280
they haven't really used your product and they haven't done the training videos and the other one

747
01:06:37,280 --> 01:06:41,920
may be churning because there's a high probability they're going to go bankrupt in the first case

748
01:06:41,920 --> 01:06:46,560
that's something you can do something about and the second case you know you're kind of SOL

749
01:06:47,760 --> 01:06:53,200
and so even though they're identical and what their predictions are and their and their

750
01:06:53,200 --> 01:07:00,560
probabilities of those predictions coming to pass one is actionable and one isn't and so it's

751
01:07:00,560 --> 01:07:05,280
not just people gaining kind of a warm fuzzy about why did you get these predictions and does it

752
01:07:05,280 --> 01:07:12,160
jive with my you know feeling about why that that could be okay which is critically important

753
01:07:12,160 --> 01:07:19,600
it also then starts tying into next best action and because I think again a important part of

754
01:07:19,600 --> 01:07:26,240
machine learning and production is to drive value if the value isn't the prediction in of itself

755
01:07:26,240 --> 01:07:32,160
then the prediction in of itself is really just there to drive the next thing that happens

756
01:07:32,160 --> 01:07:39,600
and so next best action is heavily coupled with you know the the importance is around which

757
01:07:39,600 --> 01:07:46,800
features are driving the prediction okay um you mentioned value and that's a great transition

758
01:07:46,800 --> 01:07:52,160
to one of the things that I really wanted to dig into with you and that is the this blog post

759
01:07:52,160 --> 01:07:58,000
that you wrote about cost optimized AI that I've incidentally mentioned on the podcast a couple

760
01:07:58,000 --> 01:08:06,000
of times do you have time to go into that of course so I guess the first you know it's it's actually

761
01:08:06,000 --> 01:08:12,560
come up several times in our conversation already this notion of cost and value but was there a

762
01:08:12,560 --> 01:08:19,520
specific thing that prompted you to I really got to write this down now would drove that

763
01:08:19,520 --> 01:08:29,760
so that was a bit of an intellectual journey I was wondering to be really frank why the hell are

764
01:08:29,760 --> 01:08:35,600
all these companies building these neuromorphic chips and all these specialized hardware to do deep

765
01:08:35,600 --> 01:08:43,840
learning where where you know because I think much of the world's data and much of the world's

766
01:08:43,840 --> 01:08:49,200
value in data is tied up and I'll use the word or quote unquote small data or medium data

767
01:08:49,200 --> 01:08:55,760
non massive scale Google scale data Facebook scale data I was wondering why all these people

768
01:08:55,760 --> 01:09:01,840
are starting to build these very specialized pieces of hardware when you know deep learning I

769
01:09:01,840 --> 01:09:10,720
think magnanimously one could say or charitable is incredibly good at a large number of of inference

770
01:09:10,720 --> 01:09:18,480
problems but not very good at a large probably even larger space of inference problems that may be

771
01:09:18,480 --> 01:09:24,320
changing over time as people start applying it to these new realms but the place where deep learning

772
01:09:24,320 --> 01:09:31,360
winds up shining is in really large amounts of data right because effectively what you're doing

773
01:09:31,360 --> 01:09:37,200
is turning millions or even billions of knobs to optimize a model and to do that credibly without

774
01:09:37,200 --> 01:09:43,440
overfitting when it's lots and lots of data so so I want to ask you this question of myself why are

775
01:09:43,440 --> 01:09:52,320
people doing this and why isn't what we already have out there and even just the GPU land good

776
01:09:52,320 --> 01:10:00,880
enough and if you look at the the plot which I have in my blog post of the efficiency sort of

777
01:10:00,880 --> 01:10:06,560
gigaflops per watt right which is something of if I put this amount of energy in which has this

778
01:10:06,560 --> 01:10:13,520
amount of cost how many computations can I get out that efficiency has been growing over time

779
01:10:14,720 --> 01:10:19,520
but it's nowhere near what some of these other chips or the specialized pieces of hardware can do

780
01:10:19,520 --> 01:10:24,240
for these specific types of calculations and those themselves are nowhere near what the human

781
01:10:24,240 --> 01:10:30,000
brain can do right which is of order if I remember right about 10 to the five gigaflops per watt

782
01:10:30,000 --> 01:10:36,400
so your your brain is a you know 30 watt supercomputer unrivaled at least for now by anything

783
01:10:36,400 --> 01:10:41,680
else it's out there and anything that else is out there is likely going to take megawatts or

784
01:10:41,680 --> 01:10:46,240
hundreds of megawatts to get anywhere close the computational capability incidentally I don't know

785
01:10:46,240 --> 01:10:53,040
if you've come across it but there there's a parallel to using DNA for storage and the the storage

786
01:10:53,040 --> 01:11:00,240
density per per unit energy is incredible in DNA yeah something like you know the the drop of

787
01:11:00,240 --> 01:11:05,840
of uh you know in a teaspoon or something it can take all the world's data as it's it's

788
01:11:05,840 --> 01:11:13,040
it's incredible um so so getting back to this you know that's an obvious that's an obvious one

789
01:11:13,040 --> 01:11:20,400
and I started thinking about it when alpha go um had uh it's big um set of results uh the national

790
01:11:20,400 --> 01:11:26,400
or international championships and you want to looking at the computational capability that it took

791
01:11:26,400 --> 01:11:34,160
to win those um those competitions it's just huge thousands and thousands of computers thousands

792
01:11:34,160 --> 01:11:41,680
and thousands of GPUs the amount of power required there uh was several orders of magnitude larger

793
01:11:42,480 --> 01:11:49,200
than what was going on in uh you know the the the champions head that they was playing against

794
01:11:49,200 --> 01:11:56,480
so I was thinking about that sort of vast gulf and I wound up realizing that the companies that

795
01:11:56,480 --> 01:12:02,880
are pushing towards these specialized pieces of hardware is because they realized that uh for a

796
01:12:02,880 --> 01:12:09,200
given amount of time and a given amount of data because these algorithms are all basically

797
01:12:09,200 --> 01:12:17,120
saturating on near perfect answers uh the only thing left to do is to get more uh energy

798
01:12:17,120 --> 01:12:23,520
efficient um machine learning uh for building and and that the step after energy efficiency when

799
01:12:23,520 --> 01:12:32,400
it comes down to it is really cost efficiency um and and so my my take away on on that part was

800
01:12:33,040 --> 01:12:38,640
that people are building these chips because that's sort of the last frontier of squeaking out

801
01:12:38,640 --> 01:12:45,360
and eaking out the last amount of dollars uh coming out of the system for the number of dollars

802
01:12:45,360 --> 01:12:52,080
going into the system um and then it's taking a step back from that it went up realizing that uh

803
01:12:52,080 --> 01:12:58,000
or or at least realizing for myself it's probably obvious to most out there that because machine

804
01:12:58,000 --> 01:13:05,680
learning is optimization that you're a good optimizer will find the optimal answer by definition

805
01:13:05,680 --> 01:13:11,200
that if you're not writing down your uh your the function that you're trying to optimize um to get

806
01:13:11,200 --> 01:13:19,520
a minimum of or maximum of uh in terms that actually matter then you're creating by definition a sub sub

807
01:13:19,520 --> 01:13:26,800
optimal answer or system and now that system doesn't just involve you know as my algorithm

808
01:13:26,800 --> 01:13:31,920
more optimal at getting an accuracy better than yours but now translating the accuracy into

809
01:13:31,920 --> 01:13:36,960
well let's go back to our loss function what's the cost of being wrong you know and saying this

810
01:13:36,960 --> 01:13:43,600
thing is uh a and this thing is b um translating that to a business term is something that's critical

811
01:13:43,600 --> 01:13:50,000
and almost everybody knows that that's uh important but then you want to realizing well um if I'm

812
01:13:50,000 --> 01:13:55,600
going to build a model what if it takes me 12 days to build one of these models right to get an

813
01:13:55,600 --> 01:14:01,840
accuracy which is only epsilon better than one that takes me to 10 seconds uh and what if

814
01:14:01,840 --> 01:14:08,080
you know I can build a model that may take 12 days and the accuracy is much higher than one

815
01:14:08,080 --> 01:14:14,240
took me less time but uh the labor costs are very different so I had to spend more data science time

816
01:14:14,240 --> 01:14:19,920
building one versus the other and what about the opportunity costs of those data scientists not

817
01:14:19,920 --> 01:14:25,120
working on another problem in your business that may be more important and when you wind up

818
01:14:25,120 --> 01:14:31,600
couching the problem that way um you get out of just again focused on accuracy in the algorithm

819
01:14:31,600 --> 01:14:36,880
to what is my cost of doing the entire pipeline and now the entire pipeline isn't just

820
01:14:37,760 --> 01:14:42,160
running a uh a machine learning model in production for this specific use case

821
01:14:42,160 --> 01:14:47,840
but how does that couple to all the other things you're doing in your business um are you hiring a

822
01:14:47,840 --> 01:14:53,200
data science team to do this and then paying pensions or you're going to do a third party to do

823
01:14:53,200 --> 01:14:59,040
this and just write a check one time um and then you know where the societal benefits of all this

824
01:14:59,040 --> 01:15:03,520
and you know it becomes unwieldy at some point if you're actually being very honest about what's

825
01:15:03,520 --> 01:15:08,720
the cost of doing this but at least if people I just wanted people to start thinking about as we

826
01:15:08,720 --> 01:15:14,400
started thinking about within our company that accuracy is the table stakes and let's assume that

827
01:15:14,400 --> 01:15:20,000
you all have your good algorithm that's going to do well is it going to have strong scaling

828
01:15:20,000 --> 01:15:25,760
properties so that if you needed to get the model built uh you know an x amount of time that you

829
01:15:25,760 --> 01:15:31,520
could just have n number of machines that get you x divided by n amount of time on the clock

830
01:15:31,520 --> 01:15:36,800
because maybe you need that model built very quickly very often um and then you know questions

831
01:15:36,800 --> 01:15:45,120
around the pipeline and and RAM usage and AWS costs in the end as a as a small uh start up

832
01:15:46,560 --> 01:15:52,480
when you start getting down to the breast tax of what's our revenue um and what's our cost of

833
01:15:52,480 --> 01:15:59,840
good soul was our cogs the cogs component is really what is it cost to build a model and predict

834
01:16:00,480 --> 01:16:06,640
and until we were able to boil down the fact that the cost per prediction for one of our customers

835
01:16:06,640 --> 01:16:13,520
is x and we're going to be making x times some number um you know everything else is sort of move

836
01:16:13,520 --> 01:16:17,760
if you're losing you know every time you make a prediction effectively hand over fist

837
01:16:17,760 --> 01:16:23,520
uh then you've got something wrong that's unsustainable so i i started thinking about it as we

838
01:16:23,520 --> 01:16:28,480
were going through the exercise of what's our cost of doing business and the cost of doing business

839
01:16:28,480 --> 01:16:34,960
is running an ai system in a cloud with real customers right um and the labor part we can get

840
01:16:34,960 --> 01:16:40,960
but all the other pieces they're in the end there's an amazon bill and because we put it all inside

841
01:16:40,960 --> 01:16:45,520
of amazon you know and we know how much money we're taking in we can see how those two things

842
01:16:45,520 --> 01:16:52,080
relate to each other so you you started with the question and and kind of ran through the thought

843
01:16:52,080 --> 01:16:57,840
exercise what's next there whether it's you or someone else that does it do you do you see this

844
01:16:57,840 --> 01:17:03,680
evolving or uh you know co-evolving with someone else thinking about you know analytical frameworks

845
01:17:03,680 --> 01:17:11,280
for thinking about this or you know tools you know whether that's a spreadsheet or um you know

846
01:17:11,280 --> 01:17:16,160
it almost lends itself to machine learning algorithm trying to figure out how to deploy resources

847
01:17:16,160 --> 01:17:20,640
to you know do the machine learning yeah it's a great question and you know one of the nice

848
01:17:20,640 --> 01:17:24,720
things about blog posts is you admit it out to the world and you hope somebody runs you hope somebody

849
01:17:24,720 --> 01:17:31,680
runs with it um it's been helpful in focusing for me in my own thoughts and as we drive those

850
01:17:31,680 --> 01:17:38,960
sorts of efficiencies in in our company and then again more broadly um you know in doing science

851
01:17:38,960 --> 01:17:46,080
doing astrophysics uh choosing the right tools um choosing the right skill sets and people

852
01:17:46,080 --> 01:17:52,240
choosing the right problems to work on or not work on those those are very obvious uh sort of

853
01:17:52,240 --> 01:17:57,440
outcomes from me having thought about it and framed it that way one of the challenges is and I

854
01:17:57,440 --> 01:18:04,560
think people may wind up being able to pick uh pieces up of this uh and work with it is coming up

855
01:18:04,560 --> 01:18:14,800
with articulations of um essentially what is the conversion term uh between that item in the

856
01:18:14,800 --> 01:18:22,640
in the entire optimization uh equation and dollars so one I'll throw out there that I don't

857
01:18:22,640 --> 01:18:31,360
know the answer to is uh what's the dollar value of interpretability um and once somebody starts

858
01:18:31,360 --> 01:18:39,680
getting some handle on that then optimization takes its wonderful you know uh told or or approach

859
01:18:39,680 --> 01:18:45,600
uh or at least shall lead to a great outcome which is you know once you once you can really put a

860
01:18:45,600 --> 01:18:50,960
dollar cost to all these different pieces then I think you can do a real honest to goodness

861
01:18:50,960 --> 01:18:56,080
optimization so I know what the dollar costs for instance of needing a ram machine of this size

862
01:18:56,080 --> 01:19:05,040
versus that size on Amazon okay great um but what's the real dollar cost of and can I know

863
01:19:05,040 --> 01:19:09,600
what how much time it's going to take for a data science team to build up this template

864
01:19:09,600 --> 01:19:15,760
from scratch and then push that into production uh and how many people do I really need on that

865
01:19:15,760 --> 01:19:22,000
is it good to have one data scientist or multiple ones right um and so all those things I think wind

866
01:19:22,000 --> 01:19:27,040
up becoming really interesting over time uh ones people wind up potentially even wind up agreeing

867
01:19:27,040 --> 01:19:32,400
upon what this equation what's in ban for this equation and what's not obviously out of scope is

868
01:19:32,400 --> 01:19:37,360
uh you know what's the probability that you know my machine learning algorithm is going to start

869
01:19:37,360 --> 01:19:42,320
world world three right probably not worth talking about right right something smaller than that

870
01:19:43,040 --> 01:19:48,880
smaller in scope at the company level um is probably worth starting to get some clear

871
01:19:48,880 --> 01:19:53,600
understanding around so now we've maybe come back full circle to graduate students sounds like

872
01:19:53,600 --> 01:19:58,480
there are a lot of interesting uh research parts research questions in here for PhD student or

873
01:19:58,480 --> 01:20:04,080
something yeah I think in the you know for for those in computer science thinking about systems

874
01:20:04,080 --> 01:20:09,040
optimization uh who are also interested in machine learning this is um hopefully some fertile

875
01:20:09,040 --> 01:20:15,920
ground to start to start thinking um the other statement which hopefully is clear from what we've

876
01:20:15,920 --> 01:20:22,960
been talking about is that doing machine learning for machine learning sake really doesn't make sense

877
01:20:22,960 --> 01:20:29,360
it's it's probably the last thing you want to do if somebody hands you data uh you do it because

878
01:20:29,360 --> 01:20:35,120
you have to do it it's painful and to run it in a production environment um given all the crazy

879
01:20:35,120 --> 01:20:41,280
bugaboo's that um many many people have talked about there's a great paper from the folks at Google

880
01:20:41,280 --> 01:20:47,840
by Dee Scully is the first author called machine learning is the high interest credit card of

881
01:20:47,840 --> 01:20:53,600
technical debt um my last interview yes I'm not surprised it's an important paper it's got I think

882
01:20:53,600 --> 01:20:59,600
no equations in it but it's a whole bunch of important lessons about how machine learning

883
01:21:00,480 --> 01:21:08,320
systems tend to be very different than typical engineering systems um so so so there's a lot in

884
01:21:08,320 --> 01:21:13,760
there uh to get right a lot of bugaboo's there that people who haven't done this before

885
01:21:13,760 --> 01:21:18,480
tend to get wrong but what you wind up realizing is that once you realize machine learning or

886
01:21:18,480 --> 01:21:24,960
or more broadly AI is the right set of tools to apply to the problem uh that you have

887
01:21:25,680 --> 01:21:31,520
what you'll often wind up finding I think is at the graduate student level um in terms of graduate

888
01:21:31,520 --> 01:21:38,880
student projects they can be working on is that it's still very much early days for the for the

889
01:21:38,880 --> 01:21:47,360
types of algorithms pipelines etc in dealing with real world data um I've often said to my my

890
01:21:47,360 --> 01:21:58,080
colleagues on campus that um real data is not doing uh sentiment analysis on twitter right and yet

891
01:21:58,080 --> 01:22:03,440
many many many papers saying my scaling algorithms but then your scaling algorithm uh will wind up

892
01:22:03,440 --> 01:22:09,520
using that as a toy data set the real world is not uh toy data sets yes we need to have benchmark

893
01:22:09,520 --> 01:22:13,840
data to have a lingua franca of who's doing better in these different axes right but when you

894
01:22:13,840 --> 01:22:19,200
wind up getting exposed to real questions uh you wind up realizing that all the stuff that people

895
01:22:19,200 --> 01:22:25,760
know out there in the academic world that people write about and do kegel blog posts about are not

896
01:22:25,760 --> 01:22:30,560
what you really need if you're being truly honest about what needs to get optimized hmm that's

897
01:22:30,560 --> 01:22:36,640
great so how does one manage being ctl for you know the high growth startup and you know being

898
01:22:36,640 --> 01:22:42,880
a astrophysics professor it's becoming increasingly common to see folks particularly in the machine

899
01:22:42,880 --> 01:22:50,480
learning community have uh professor real posts and do um academic or do uh you know work in these

900
01:22:50,480 --> 01:22:57,120
research labs and things like that but yeah so i i uh been on a what's called an industry leave

901
01:22:57,120 --> 01:23:03,040
for for a number of years um and so it's allowed me to have also that separation of concern so

902
01:23:03,760 --> 01:23:08,400
so uh not getting not getting paid by the university and i have been uh health care has made it

903
01:23:08,400 --> 01:23:15,200
easier for me to spend all my time as need be on the uh on the on the company um while still

904
01:23:15,200 --> 01:23:20,960
maintaining uh the kinds of links that i think are important um as i you know start thinking about

905
01:23:20,960 --> 01:23:26,240
coming back into the university setting um obviously a number of things i've picked up and

906
01:23:26,240 --> 01:23:32,320
management thing you know ideas and and and capabilities um and then also thinking about how to

907
01:23:32,320 --> 01:23:38,320
evaluate uh new technologies when is it appropriate to bring this into your toolkit or when is

908
01:23:38,320 --> 01:23:45,440
it appropriate to wait um those become really practical uh uses that you know i can take i can take

909
01:23:45,440 --> 01:23:52,160
with me but then also again recognizing that as i was saying before there's a whole uh interesting

910
01:23:52,160 --> 01:23:58,240
set of problems out there that are not being addressed by um pure academic R&D research

911
01:23:58,800 --> 01:24:04,880
means that i can also start uh you know looking for those white spaces to actually do some

912
01:24:04,880 --> 01:24:10,320
pure academic research around those um i'm particularly interested in questions around

913
01:24:10,320 --> 01:24:16,000
interpretability and how you put metrics on interpretability um and that's something that i think

914
01:24:16,000 --> 01:24:21,840
i benefit from having come from uh you know felt the pain of customers asking about that

915
01:24:21,840 --> 01:24:28,000
right um that uh you know at least have a fresh lens on that um doesn't mean i'll solve any of

916
01:24:28,000 --> 01:24:34,240
those problems but at least i'll i'll have a direction of potential interest um so it's uh it's

917
01:24:34,240 --> 01:24:40,960
certainly a challenge but i think uh despite the challenges the the benefits to both myself

918
01:24:40,960 --> 01:24:48,480
the company and the university and my students at the university uh are are far up way all the

919
01:24:48,480 --> 01:24:56,080
gray hairs that i wind up getting i'm i'm teaching a uh data science um class essentially a

920
01:24:56,080 --> 01:25:01,840
python ecosystem data science class right now it's uh aimed at graduate students

921
01:25:01,840 --> 01:25:07,840
and the things that i've seen in the uh in the business world have really helped me

922
01:25:08,560 --> 01:25:13,840
hone that that class and i'm directly giving back to the students uh from that from those

923
01:25:13,840 --> 01:25:20,960
learnings and is that a MOOC or is that available only to uh it is not a MOOC um other incarnations of

924
01:25:20,960 --> 01:25:27,040
that class that i've done in the past uh are probably online somewhere in the iTunes sphere or

925
01:25:27,040 --> 01:25:34,000
elsewhere um that can also be found on github all the material uh and then we'll hopefully post

926
01:25:34,000 --> 01:25:40,400
some of the of the lectures online as well okay great uh so if folks want to learn more about the

927
01:25:40,400 --> 01:25:45,040
the company or get in touch with you what's what are the best ways for them to find you guys um

928
01:25:45,040 --> 01:25:50,560
easiest is uh drop me an email um and you can find that by googling around

929
01:25:50,560 --> 01:25:56,320
uh um so i'll i'll add that as a little bit of a bar that if you really fun to find me you'll have

930
01:25:56,320 --> 01:26:03,200
to do a little bit of work uh you can tweet at me so that's prof jsp uh it's my twitter handle

931
01:26:03,200 --> 01:26:09,360
and uh and we can do a direct message um maybe that's probably the best way to to get at me

932
01:26:09,360 --> 01:26:13,760
great great uh well i really appreciate you taking the time it's great to finally meet you

933
01:26:13,760 --> 01:26:19,520
in person and uh i really enjoy the conversation i think folks will will enjoy it as well and get

934
01:26:19,520 --> 01:26:23,040
a lot out of it great well thanks so much thanks for your interest great thanks

935
01:26:28,960 --> 01:26:33,360
all right everyone that's it for today's interview thanks so much for listening

936
01:26:34,000 --> 01:26:39,040
i haven't asked you all to do this in a while but if you enjoyed this episode of the show

937
01:26:39,040 --> 01:26:45,680
please please please do these two things first share it with your friends on twitter

938
01:26:45,680 --> 01:26:52,320
facebook good old email or however you share cool things with your friends second reach out

939
01:26:52,320 --> 01:26:57,600
let me know how you like the show who you'd like to hear on it and how i can make it better for you

940
01:26:57,600 --> 01:27:05,200
you can reach me on twitter at at twimmel ai and at sam charrington and you can email me directly

941
01:27:05,200 --> 01:27:18,640
from the contact page on the twimmel ai.com site thank you so much for your support and catch you next time

942
01:30:05,200 --> 01:30:06,160
you

