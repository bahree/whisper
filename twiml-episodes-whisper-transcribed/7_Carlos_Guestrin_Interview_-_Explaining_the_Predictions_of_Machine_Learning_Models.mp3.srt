1
00:00:00,000 --> 00:00:16,120
Hello everybody and welcome to another episode of Twimble Talk, the podcast where I interview

2
00:00:16,120 --> 00:00:21,640
interesting people doing interesting things in machine learning and artificial intelligence.

3
00:00:21,640 --> 00:00:27,200
I'm your host Sam Charrington.

4
00:00:27,200 --> 00:00:31,360
The recording you're about to hear is the first of a handful of interviews I was fortunate

5
00:00:31,360 --> 00:00:36,960
enough to be able to record live in New York City from the O'Reilly AI and Stratoconferences

6
00:00:36,960 --> 00:00:39,000
that I attended last week.

7
00:00:39,000 --> 00:00:43,000
I'll be sharing these interviews on the podcast over the next several weeks and I think

8
00:00:43,000 --> 00:00:45,640
you'll really, really enjoy them.

9
00:00:45,640 --> 00:00:51,920
I'm especially excited to lead off this series with an interview with Carlos Gastron.

10
00:00:51,920 --> 00:00:56,960
Now if that name sounds familiar it's because I've discussed Carlos's work on the show a number

11
00:00:56,960 --> 00:01:04,000
of times, most recently when I discussed Apple's acquisition of his company Tury back in August.

12
00:01:04,000 --> 00:01:09,080
In addition to Carlos's new role at Apple, he's also the Amazon professor of machine learning

13
00:01:09,080 --> 00:01:12,280
at the University of Washington.

14
00:01:12,280 --> 00:01:17,760
Earlier this year Carlos, along with one of his PhD students Marco Ribeiro and postdocs

15
00:01:17,760 --> 00:01:23,600
Samir Singh, published some very, very interesting research into the explainability of machine

16
00:01:23,600 --> 00:01:26,000
learning algorithms.

17
00:01:26,000 --> 00:01:31,000
My conversation with Carlos is focused on this research and the paper that the group recently

18
00:01:31,000 --> 00:01:36,640
published called Why Should I Trust You, Explaining the Predictions of Any Classifier.

19
00:01:36,640 --> 00:01:39,880
This paper has been on my reading list for a while and I encourage you to take a look

20
00:01:39,880 --> 00:01:40,880
at it.

21
00:01:40,880 --> 00:01:45,720
Of course, you'll find links to Carlos and the paper in the show notes, which you can

22
00:01:45,720 --> 00:01:50,200
find at twimmelai.com slash talk slash seven.

23
00:01:50,200 --> 00:01:54,320
A quick note about the background noise and this and the other onsite recordings, they're

24
00:01:54,320 --> 00:01:59,640
not too bad considering the noisy caverns in which they were recorded, but some of you

25
00:01:59,640 --> 00:02:02,720
might find the murmurs and bumps a bit annoying.

26
00:02:02,720 --> 00:02:07,280
If you find yourself in this camp, please accept my apologies.

27
00:02:07,280 --> 00:02:16,480
And now on to the show.

28
00:02:16,480 --> 00:02:22,400
So hey everyone, I'm here at the strata conference in New York City and I happen to find Carlos

29
00:02:22,400 --> 00:02:26,360
Gestren, who we've talked about on the podcast before.

30
00:02:26,360 --> 00:02:31,480
He's the Amazon professor of machine learning at the University of Washington and we've known

31
00:02:31,480 --> 00:02:33,200
each other for a bit.

32
00:02:33,200 --> 00:02:34,880
So Carlos, say hi.

33
00:02:34,880 --> 00:02:40,440
Hi, thanks for having me here and it was great running into you, it's a great event.

34
00:02:40,440 --> 00:02:41,680
Yeah, absolutely, absolutely.

35
00:02:41,680 --> 00:02:47,680
In fact, I think we probably had a briefing like right at this very table a year or two

36
00:02:47,680 --> 00:02:48,680
ago.

37
00:02:48,680 --> 00:02:52,760
And I think we met at this event in this very place.

38
00:02:52,760 --> 00:02:54,360
Where's that room over there?

39
00:02:54,360 --> 00:02:55,360
Yeah.

40
00:02:55,360 --> 00:02:56,360
Yeah.

41
00:02:56,360 --> 00:02:57,360
Yeah.

42
00:02:57,360 --> 00:03:02,560
So I guess I'll say very briefly to the audience, we're not in the most convenience spot

43
00:03:02,560 --> 00:03:03,560
for podcasting.

44
00:03:03,560 --> 00:03:08,080
So if there's the occasional trolley rolling by, just try to block that out because if you

45
00:03:08,080 --> 00:03:12,280
want some lunch, it's right behind us.

46
00:03:12,280 --> 00:03:16,240
But I'm sure you won't remember that at all because we're going to have a great conversation

47
00:03:16,240 --> 00:03:26,840
here, first of all, congratulations on the acquisition of Toree, Neh, Dada, Neh, Graflab

48
00:03:26,840 --> 00:03:27,840
by Apple.

49
00:03:27,840 --> 00:03:28,840
That was amazing.

50
00:03:28,840 --> 00:03:31,200
Yeah, we're very excited to work with Apple.

51
00:03:31,200 --> 00:03:32,200
It's great.

52
00:03:32,200 --> 00:03:33,200
Awesome.

53
00:03:33,200 --> 00:03:34,200
Awesome.

54
00:03:34,200 --> 00:03:39,840
So why don't we just start with introductions, like introduce yourself, talk a little bit about

55
00:03:39,840 --> 00:03:40,840
your background.

56
00:03:40,840 --> 00:03:43,640
I think a lot of people kind of know what you've been up to.

57
00:03:43,640 --> 00:03:45,520
But sure, I'm happy to share.

58
00:03:45,520 --> 00:03:51,280
So well, I'm Carlos, Carlos Gaston, me working machine learning for a long time.

59
00:03:51,280 --> 00:03:55,640
So I was a professor at Carnegie Mellon for about eight years and then at University of

60
00:03:55,640 --> 00:03:58,480
Washington since about 2012.

61
00:03:58,480 --> 00:04:03,080
And I've been excited about machine learning for a long time and worked on many areas of

62
00:04:03,080 --> 00:04:05,320
machine learning.

63
00:04:05,320 --> 00:04:09,480
Most recently, a couple of years have been exciting to me, really around dealing with

64
00:04:09,480 --> 00:04:12,520
a big data and the two sides of that.

65
00:04:12,520 --> 00:04:16,600
So on one side, algorithms for machine learning that scale to very large data sets.

66
00:04:16,600 --> 00:04:20,760
So how can you scale up to deal with tons of tons of data.

67
00:04:20,760 --> 00:04:24,280
And the second side is what I think about is the human side of machine learning.

68
00:04:24,280 --> 00:04:26,800
So how can a human understand large data sets?

69
00:04:26,800 --> 00:04:32,000
How can a human understand what machine learning algorithm is doing and bringing some kind

70
00:04:32,000 --> 00:04:33,960
of human perspective into the mix.

71
00:04:33,960 --> 00:04:38,320
So I think about those two sides, a computer perspective and a human perspective of machine

72
00:04:38,320 --> 00:04:41,000
learning in large data sets.

73
00:04:41,000 --> 00:04:44,800
And I imagine there's also a fair amount of overlap and intersect between those two.

74
00:04:44,800 --> 00:04:46,080
And of course, right?

75
00:04:46,080 --> 00:04:52,200
So the bigger your data, in a sense, the harder it is to figure out how to make it work,

76
00:04:52,200 --> 00:04:54,640
but it's also hard to figure out what's going wrong with that.

77
00:04:54,640 --> 00:04:59,360
So debugging and machine learning algorithm that requires you to run in a cluster with tons

78
00:04:59,360 --> 00:05:03,200
of machines is just almost an impossible task.

79
00:05:03,200 --> 00:05:06,920
And honestly, the way I think about it is that there's no machine learning without humans

80
00:05:06,920 --> 00:05:07,920
in the loop.

81
00:05:07,920 --> 00:05:08,920
Right.

82
00:05:08,920 --> 00:05:15,120
And this incredibly intelligent application, they're going to be self-sufficient, but

83
00:05:15,120 --> 00:05:18,920
we'll always have humans be part of the process at some point.

84
00:05:18,920 --> 00:05:23,760
And so making that more human is a very important part of machine learning.

85
00:05:23,760 --> 00:05:28,840
It's been understudied in my field, but it's something that I'm very excited to engage

86
00:05:28,840 --> 00:05:29,840
in as well.

87
00:05:29,840 --> 00:05:30,840
Yeah.

88
00:05:30,840 --> 00:05:34,480
So there's humans in the loop in lots of places, actually.

89
00:05:34,480 --> 00:05:41,480
And one of the places that humans are most certainly in the loop is on the back end of a machine

90
00:05:41,480 --> 00:05:43,600
learning recommendation.

91
00:05:43,600 --> 00:05:51,200
And your group has done a lot of interesting work very recently, at least, on explainability.

92
00:05:51,200 --> 00:05:54,920
Can you talk a little bit about how you've arrived at that research area?

93
00:05:54,920 --> 00:05:55,920
Yeah, yeah.

94
00:05:55,920 --> 00:06:01,080
So just in one sentence, we're interested in being able to provide more transparency to

95
00:06:01,080 --> 00:06:02,080
machine learning.

96
00:06:02,080 --> 00:06:05,640
We'll explain why a machine learning model makes a particular prediction, or why it behaves

97
00:06:05,640 --> 00:06:06,960
a certain way.

98
00:06:06,960 --> 00:06:12,720
Now, we fell into this topic kind of interestingly in various ways.

99
00:06:12,720 --> 00:06:17,800
So for example, in academia, we're working with various folks in application domains,

100
00:06:17,800 --> 00:06:20,720
and we said, oh, come use machine learning, solve this problem, it's going to be awesome,

101
00:06:20,720 --> 00:06:22,360
we're going to change your life.

102
00:06:22,360 --> 00:06:28,080
And their response was like, sounds great, but why should I trust this model?

103
00:06:28,080 --> 00:06:29,080
What is it doing?

104
00:06:29,080 --> 00:06:30,080
Right.

105
00:06:30,080 --> 00:06:34,560
And I was like, it's got great accuracy.

106
00:06:34,560 --> 00:06:35,560
So that's one side.

107
00:06:35,560 --> 00:06:36,920
That's somehow unsatisfying.

108
00:06:36,920 --> 00:06:37,920
It's unsatisfying.

109
00:06:37,920 --> 00:06:39,520
Yeah, no, it's not enough.

110
00:06:39,520 --> 00:06:40,520
It's really not enough.

111
00:06:40,520 --> 00:06:42,000
We can talk about why it's not enough.

112
00:06:42,000 --> 00:06:49,520
And then on the other side, once we build a company around machine learning, three day

113
00:06:49,520 --> 00:06:54,360
to graph lab, we start working for a lot of companies that brought machine learning

114
00:06:54,360 --> 00:06:55,360
to production.

115
00:06:55,360 --> 00:06:59,360
And there was always a step that nobody talked about, but it was very fundamental.

116
00:06:59,360 --> 00:07:05,520
They trained the model to do something, recommendations or whatever, predictor and predict fraud.

117
00:07:05,520 --> 00:07:08,560
And you want to deploy it as a service every time you swipe your credit card, it makes

118
00:07:08,560 --> 00:07:09,920
a prediction about fraud.

119
00:07:09,920 --> 00:07:13,480
You don't just make that happen out of the box, you want to make sure that model is working

120
00:07:13,480 --> 00:07:15,840
well and is doing things for the right reason.

121
00:07:15,840 --> 00:07:18,520
Because if it's not, you're going to get fired.

122
00:07:18,520 --> 00:07:19,520
Right.

123
00:07:19,520 --> 00:07:22,760
You really want to understand why that thing is behaving the way it is.

124
00:07:22,760 --> 00:07:24,360
So by talking to those.

125
00:07:24,360 --> 00:07:28,760
And not just that, we've talked about on the podcast before in Europe, there's legislation

126
00:07:28,760 --> 00:07:33,440
that's coming down the line that mandates explainability for machine learning and predictive

127
00:07:33,440 --> 00:07:34,440
applications.

128
00:07:34,440 --> 00:07:37,040
Yes, there's legislation in Europe.

129
00:07:37,040 --> 00:07:43,200
And not just that, even in the US, for certain application domains in the financial sector,

130
00:07:43,200 --> 00:07:47,800
they mandate certain models you are allowed to use versus others because they believe those

131
00:07:47,800 --> 00:07:52,080
models to be more interpretable or more believable or something.

132
00:07:52,080 --> 00:07:58,320
And so for certain tasks in that sector, you have to use a particular kind of model.

133
00:07:58,320 --> 00:08:04,200
And that just blocks a lot of the high accuracy models you might want to use.

134
00:08:04,200 --> 00:08:05,960
So it is a real issue.

135
00:08:05,960 --> 00:08:09,520
And I think that issue gets bubbled up in three areas.

136
00:08:09,520 --> 00:08:12,880
So one is just kind of general user model.

137
00:08:12,880 --> 00:08:16,240
How can they gain trust that service is doing things for the right reason?

138
00:08:16,240 --> 00:08:23,800
So if I go to a movie content recommendation in Netflix, I want to know that I got recommended

139
00:08:23,800 --> 00:08:28,040
Lord of the Rings because I also like Star Wars, that gives me a sense that that thing is

140
00:08:28,040 --> 00:08:31,400
doing the right things, recommending things that make sense to me.

141
00:08:31,400 --> 00:08:35,640
And they can begin to gain a relationship of trust with that artificial intelligence

142
00:08:35,640 --> 00:08:36,840
system underneath.

143
00:08:36,840 --> 00:08:40,000
So that's kind of a more personal consumery thing.

144
00:08:40,000 --> 00:08:46,400
But if you think about from the decision that it's really important, a really high change,

145
00:08:46,400 --> 00:08:52,160
like a doctor making a decision about the treatment of a patient, there you want transparency.

146
00:08:52,160 --> 00:08:58,800
So if you have a system that says the patient is going to have cancer with 90% probability,

147
00:08:58,800 --> 00:09:02,600
most doctors are going to ignore that system because they might not trust it.

148
00:09:02,600 --> 00:09:07,680
And also because there's a holistic approach to medicine that we want to have where it's

149
00:09:07,680 --> 00:09:09,800
not enough to just make that one prediction.

150
00:09:09,800 --> 00:09:16,080
But if the system were to say, you know, this patient is likely to have cancer because

151
00:09:16,080 --> 00:09:22,440
if you look at their MRI results, you see this lump, and if you look at this related cases,

152
00:09:22,440 --> 00:09:28,400
they were diagnosed in the same way, and if you look at this latest study, this all corroborates

153
00:09:28,400 --> 00:09:32,440
to evidence, then I can gain a more holistic view and I can gain trust in the system.

154
00:09:32,440 --> 00:09:33,840
So that's the second piece.

155
00:09:33,840 --> 00:09:38,000
It's kind of getting deeper insights as to what's happening in that prediction.

156
00:09:38,000 --> 00:09:42,960
And the third way, which is more kind of personal for me, is as a data scientist, I want

157
00:09:42,960 --> 00:09:45,600
to be able to make the models always better.

158
00:09:45,600 --> 00:09:49,200
And I want to understand when it's working, what's not working, so they can improve it.

159
00:09:49,200 --> 00:09:54,520
And so those are the areas, public perception of machine learning, making more informed

160
00:09:54,520 --> 00:09:59,600
decisions, not just the prediction for machine learning, and improving the models through

161
00:09:59,600 --> 00:10:00,600
feedback.

162
00:10:00,600 --> 00:10:04,120
And so transparency and explanation are going to be indispensable to make that happen.

163
00:10:04,120 --> 00:10:08,080
And unfortunately, as a field, we haven't invested enough in that topic.

164
00:10:08,080 --> 00:10:09,080
Right.

165
00:10:09,080 --> 00:10:10,080
Right.

166
00:10:10,080 --> 00:10:14,400
But you guys have started to invest in this and you was it a month or two ago, you published

167
00:10:14,400 --> 00:10:15,400
a paper there?

168
00:10:15,400 --> 00:10:22,880
Yeah, the paper came out just a couple months ago, and it's a system called Lyme, that

169
00:10:22,880 --> 00:10:28,880
my student, Marco Ribeiro, and postdoc Samir Singh, wrote, Samir is now a professor you see

170
00:10:28,880 --> 00:10:31,040
or find.

171
00:10:31,040 --> 00:10:36,840
And we wrote this paper based on the feedback we're hearing and the need to do something

172
00:10:36,840 --> 00:10:37,840
more in this area.

173
00:10:37,840 --> 00:10:40,480
And there being some other works in the kind of explainability machine learning, but what

174
00:10:40,480 --> 00:10:49,040
was unique about the perspective that Marco and Samir brought into the world is how they

175
00:10:49,040 --> 00:10:50,520
approach the problem.

176
00:10:50,520 --> 00:10:54,720
So a lot of the working machine learning has been about finding models they're transparent

177
00:10:54,720 --> 00:10:55,720
or explainable.

178
00:10:55,720 --> 00:10:56,720
Right.

179
00:10:56,720 --> 00:10:57,680
Like we talked about in the financial sector.

180
00:10:57,680 --> 00:11:00,880
So these models have to be simple, so this somebody can understand it.

181
00:11:00,880 --> 00:11:05,280
But the problem with that is that simple models tend to be inaccurate.

182
00:11:05,280 --> 00:11:09,760
And so you're compromising accuracy for explainability.

183
00:11:09,760 --> 00:11:13,000
And that's what I think is the wrong compromise to have.

184
00:11:13,000 --> 00:11:17,440
In fact, when you were making the comment that you drawing the analogy with Netflix earlier,

185
00:11:17,440 --> 00:11:21,720
I was thinking to myself, you know, I'd actually kind of rather that Netflix recommends

186
00:11:21,720 --> 00:11:26,360
movies that I want to see and not tell me how it got that, then recommend movies that

187
00:11:26,360 --> 00:11:28,400
I kind of don't really like.

188
00:11:28,400 --> 00:11:33,040
But it says it's read, this is the reason, so, so, so, so, right.

189
00:11:33,040 --> 00:11:38,320
So accuracy is still a good, but for me, the way I'm thinking about it is accuracy is

190
00:11:38,320 --> 00:11:40,240
number one.

191
00:11:40,240 --> 00:11:44,400
You want to have high accuracy for the right reasons, but that's the main thing.

192
00:11:44,400 --> 00:11:48,200
Otherwise, you're not going to be able to solve the image processing task that we've seen

193
00:11:48,200 --> 00:11:50,200
solved really well deep learning today.

194
00:11:50,200 --> 00:11:53,520
If you don't have the most accurate models, if I want to use a very simple model, like

195
00:11:53,520 --> 00:11:59,280
a shallow decision tree, you're never going to be able to detect objects in an image.

196
00:11:59,280 --> 00:12:00,280
So what's the point?

197
00:12:00,280 --> 00:12:01,280
Right.

198
00:12:01,280 --> 00:12:03,360
You know, we're not going to build that kind of artificial intelligence system.

199
00:12:03,360 --> 00:12:11,920
So what we did was say, okay, let's take accuracy as a requirement.

200
00:12:11,920 --> 00:12:14,760
And so that means that we want to be able to give a data scientist the flexibility to

201
00:12:14,760 --> 00:12:17,280
choose any model they want.

202
00:12:17,280 --> 00:12:23,560
And the question is, can we provide an approach that can explain the prediction for any model?

203
00:12:23,560 --> 00:12:29,680
That was like the question, and I think it's a really beautiful question.

204
00:12:29,680 --> 00:12:34,120
And you know, the way that the work came together was really interesting.

205
00:12:34,120 --> 00:12:37,040
Of course, it's only scratching the surface of the possibility.

206
00:12:37,040 --> 00:12:43,640
But what basically, Marcus Mierde, was come up with a system that says, okay, I want

207
00:12:43,640 --> 00:12:45,400
to explain a particular prediction.

208
00:12:45,400 --> 00:12:48,600
Why did you like that movie or why does this patient have cancer?

209
00:12:48,600 --> 00:12:51,760
And the way we're going to explain it is in a simple way that's good just for this

210
00:12:51,760 --> 00:12:52,760
prediction.

211
00:12:52,760 --> 00:12:56,440
And we're going to do it by highlighting the pieces of the input that we believe the

212
00:12:56,440 --> 00:13:01,120
model was most important for the model to make this decision.

213
00:13:01,120 --> 00:13:07,720
So for the doctor's example, it is this particular area of the MRI, this particular studies

214
00:13:07,720 --> 00:13:10,040
are going on, this related cases.

215
00:13:10,040 --> 00:13:11,720
So that's kind of a small explanation.

216
00:13:11,720 --> 00:13:16,280
For the recommendation system analytics that you're unhappy with, it might be that the

217
00:13:16,280 --> 00:13:20,400
underlying system is very complex and very accurate, but the explanation we have to give

218
00:13:20,400 --> 00:13:23,760
you, it's kind of very simple, but somehow has to be faithful.

219
00:13:23,760 --> 00:13:24,760
Right.

220
00:13:24,760 --> 00:13:29,840
It behaves like the model for this particular prediction, it's not like the model only

221
00:13:29,840 --> 00:13:36,080
uses, you know, a lot of the rings for everything, but for this prediction, that's what was

222
00:13:36,080 --> 00:13:37,080
the model.

223
00:13:37,080 --> 00:13:38,080
Right.

224
00:13:38,080 --> 00:13:39,600
And so that's how we went about doing that.

225
00:13:39,600 --> 00:13:46,200
And, you know, we did a bunch of user studies that really showed that this can be very

226
00:13:46,200 --> 00:13:47,840
powerful and can be used in various ways.

227
00:13:47,840 --> 00:13:48,840
So it's pretty cool.

228
00:13:48,840 --> 00:13:50,480
What was the nature of the user studies?

229
00:13:50,480 --> 00:13:58,200
So one of the really cool user studies that Marco designed, let me just step back and say,

230
00:13:58,200 --> 00:14:01,520
it's really hard to evaluate explanations because it's a subjective thing, right?

231
00:14:01,520 --> 00:14:04,680
How do you even figure out it's doing anything interesting?

232
00:14:04,680 --> 00:14:05,680
Right.

233
00:14:05,680 --> 00:14:12,320
And maybe if it's better, I want to get into kind of how it works and what the research

234
00:14:12,320 --> 00:14:13,320
actually showed.

235
00:14:13,320 --> 00:14:18,120
If it's better to do that first, we can do that first and encircle back to the user studies.

236
00:14:18,120 --> 00:14:20,960
It is, you're the boss.

237
00:14:20,960 --> 00:14:25,720
So let's talk about how it works, let's start from there.

238
00:14:25,720 --> 00:14:32,000
So the way it works is to say, I want to have a particular prediction, I want to explain

239
00:14:32,000 --> 00:14:35,200
why it was made, why did the model make the prediction the same way.

240
00:14:35,200 --> 00:14:39,320
And the way we're going to explain it is, let's look at the behavior of the system, the

241
00:14:39,320 --> 00:14:42,800
behavior of this complex model around this prediction.

242
00:14:42,800 --> 00:14:45,280
So not everywhere, but around that.

243
00:14:45,280 --> 00:14:49,280
So for this particular patient, I'm going to try to explain why the model thought it

244
00:14:49,280 --> 00:14:50,280
had cancer.

245
00:14:50,280 --> 00:14:54,080
So let's look at patients around that, some that were predicted to have cancer by the

246
00:14:54,080 --> 00:14:57,040
model, some that were not predicted to have cancer by the model.

247
00:14:57,040 --> 00:15:03,680
And fit a simple explanation that explains the difference between those similar patients.

248
00:15:03,680 --> 00:15:07,840
It doesn't explain the difference between every patient, but just patients that kind of

249
00:15:07,840 --> 00:15:09,000
like you.

250
00:15:09,000 --> 00:15:16,040
So patients that had most of the same characteristics as you that have cancer had this thing going

251
00:15:16,040 --> 00:15:19,040
on, and patients with similar characteristics as you that don't have cancer had these

252
00:15:19,040 --> 00:15:20,320
other things going on.

253
00:15:20,320 --> 00:15:24,440
And that gives me a lot of insights for this particular decision, and that's more or

254
00:15:24,440 --> 00:15:25,440
less how it works.

255
00:15:25,440 --> 00:15:32,160
So as a course approximation of what I'm hearing, take the example of that you use in your

256
00:15:32,160 --> 00:15:36,720
course, predicting real estate prices based on a bunch of different variables, it almost

257
00:15:36,720 --> 00:15:44,320
sounds like you might have a model that's a regression model that's predicting based on

258
00:15:44,320 --> 00:15:53,400
house size, and the Lyme system is almost a reverse regression that's going the other

259
00:15:53,400 --> 00:15:56,680
way, like predicting what the inputs might be based on the output.

260
00:15:56,680 --> 00:16:02,840
In a sense, so one way to think about it is house prices are very complex, depends on

261
00:16:02,840 --> 00:16:07,920
the neighborhood you live in, the characteristics of the house, everything around it.

262
00:16:07,920 --> 00:16:13,320
So there is a simple explanation why your house costs showing you $5 million, right?

263
00:16:13,320 --> 00:16:15,360
That's how much your house costs.

264
00:16:15,360 --> 00:16:18,160
And so, can you give me a raise to it?

265
00:16:18,160 --> 00:16:20,160
So I can pay for my house.

266
00:16:20,160 --> 00:16:24,360
I'll double what I pay you right now.

267
00:16:24,360 --> 00:16:28,400
So why did your house cost five million?

268
00:16:28,400 --> 00:16:30,200
Why did the house cost certain amount?

269
00:16:30,200 --> 00:16:34,920
Who knows, really complex, the models can be very complex, but your specific house can

270
00:16:34,920 --> 00:16:38,920
explain why the model predicted just $5 million, that's very doable.

271
00:16:38,920 --> 00:16:44,200
I can look at your house and I look at similar houses that were like it, and I can fit locally

272
00:16:44,200 --> 00:16:49,720
a simple model with only a few variables, they were most important, that's basically,

273
00:16:49,720 --> 00:16:55,200
let's say linear, and it says around your house, the variables that were most important

274
00:16:55,200 --> 00:17:04,520
were square footage and zip code and number of bathrooms, that's really important for

275
00:17:04,520 --> 00:17:12,280
$5 million house, for $500,000 house, it might be a different thing is more important.

276
00:17:12,280 --> 00:17:18,480
And so that's the kind of thing that the model would say, right, so that's kind of how

277
00:17:18,480 --> 00:17:25,000
it works, it provides the key pieces of the input, they were most distinctive for particular

278
00:17:25,000 --> 00:17:30,920
prediction, and as I said, this is one way to do explanations which scratch in the surface,

279
00:17:30,920 --> 00:17:34,800
there's all sorts of other ways you can imagine doing it, and I think there's a lot of opportunity

280
00:17:34,800 --> 00:17:35,800
to do even more.

281
00:17:35,800 --> 00:17:40,640
But one of the challenges is how do you figure out if this makes any sense whatsoever, if

282
00:17:40,640 --> 00:17:44,640
these kinds of explanations are good, if the algorithm is working at all, like there's

283
00:17:44,640 --> 00:17:50,880
so many dimensions that this can go wrong, how can you even figure out if this is at all

284
00:17:50,880 --> 00:17:57,800
reasonable, right, and Marco Samir and I spent a lot of time buying your head against

285
00:17:57,800 --> 00:18:04,200
the ball to figure out how can you even test this, how can you even measure it, how do you

286
00:18:04,200 --> 00:18:11,720
explain my explanations, and Marco had a couple brilliant ideas that really surprised

287
00:18:11,720 --> 00:18:19,520
me, and so let me give you one of them, so he wanted to know if explanations are good

288
00:18:19,520 --> 00:18:23,880
and intuitive, that would mean that somebody who is not a machine learning expert, a layperson

289
00:18:23,880 --> 00:18:30,360
could look at it and make good decisions from it, so that's what he thought, so how can

290
00:18:30,360 --> 00:18:36,640
we test that hypothesis, so he did two tests of validated hypothesis in a brilliant way,

291
00:18:36,640 --> 00:18:44,000
so the first one was, if I can interrupt you, it sounds like the aim of the research was

292
00:18:44,000 --> 00:18:50,280
not just to spit out, like in order list of features in terms of, you know, waiting or

293
00:18:50,280 --> 00:18:56,680
importance in the output, but more generating human readable description, am I reading

294
00:18:56,680 --> 00:18:58,480
too much into this, or is that correct?

295
00:18:58,480 --> 00:19:02,560
No, no, no, no, no, it was a human interpretable description, interpretable, not necessarily

296
00:19:02,560 --> 00:19:07,800
readable, okay, that's one way to explain, just can be many things, right, we explored

297
00:19:07,800 --> 00:19:17,160
human readable explanations, the visualizations, we explored different ways to explain things,

298
00:19:17,160 --> 00:19:21,920
but yeah, so here's the experiment that he did, which was pretty brilliant, so he took

299
00:19:21,920 --> 00:19:31,720
a data set and just a little background, which is kind of a fun story that I just told,

300
00:19:31,720 --> 00:19:36,160
there's this famous data set called the 20 News Groups Data Set, like 20 News Groups

301
00:19:36,160 --> 00:19:46,040
Data Set has been around for about 30 years in a machine learning community, and it's

302
00:19:46,040 --> 00:19:49,280
from news groups, which you might not know what they are, but they're something they're

303
00:19:49,280 --> 00:19:53,840
called forums, now called Facebook pages, right, they have a topic and people talk about

304
00:19:53,840 --> 00:20:00,160
the topic and they post things, and the data set was famous because the idea was given

305
00:20:00,160 --> 00:20:04,640
the text of the posting, can you predict whether this was about Christianity or atheism

306
00:20:04,640 --> 00:20:09,960
or hockey or computers, whatever the topic was, and basically any modern machine learning

307
00:20:09,960 --> 00:20:18,120
approach gets 94% accuracy, so everybody used this data set in their classes, like I

308
00:20:18,120 --> 00:20:21,960
used in my class, I said, oh machine learning is so cool, I guess 94% accuracy, when

309
00:20:21,960 --> 00:20:27,000
Marco run his explanations on that, it turns out that the main features being used are

310
00:20:27,000 --> 00:20:34,360
things like the email address of the poster, okay, so sam at gmail.com, always post

311
00:20:34,360 --> 00:20:41,800
in the, I don't know what your interest are, sam, but then let's say podcast, news group,

312
00:20:41,800 --> 00:20:49,480
podcasting, yeah, recta podcasting, clearly, and obviously it's a great predictor, but

313
00:20:49,480 --> 00:20:53,680
it doesn't generalize to somebody else, and so it's not a good feature, it's a good

314
00:20:53,680 --> 00:20:59,000
feature for you, but not a good feature for the world, and so if you remove that, those

315
00:20:59,000 --> 00:21:05,880
kinds of features, the accuracy went down from 94% to only 57%, so this data set that

316
00:21:05,880 --> 00:21:11,600
everybody has used for decades, a machine learning is so well, actually it was great so

317
00:21:11,600 --> 00:21:15,960
well, and you were able to see that from the explanations, so the question that he wants

318
00:21:15,960 --> 00:21:21,720
to ask going back to the user study was, as an expert he discovered this with the explanation,

319
00:21:21,720 --> 00:21:26,040
can somebody who's not a machine learning expert discovered this and improved the performance

320
00:21:26,040 --> 00:21:32,320
of a machine learning system, so he took this data set and there was only getting 57%

321
00:21:32,320 --> 00:21:36,720
accuracy, and then clean the data, as much as scrub, scrub, scrub, scrub, remove all

322
00:21:36,720 --> 00:21:41,960
this bad features, then retrain the model and using what to get about 70% accuracy,

323
00:21:41,960 --> 00:21:48,960
by removing all the bad features like sam at gmail.com, and then coming up with a new model

324
00:21:48,960 --> 00:21:54,480
or new model training on that clean data set, so that's the go sound, clean data, and

325
00:21:54,480 --> 00:22:00,240
it was the dirty data, and the question was, using explanations could mechanical turkers

326
00:22:00,240 --> 00:22:05,560
who know nothing about machine learning, identify bad features, we said, look at explanation,

327
00:22:05,560 --> 00:22:09,560
just cross out things that you think should be relevant for this system, we didn't say

328
00:22:09,560 --> 00:22:13,840
anything else, just cross out things that you think are irrelevant, and we thought, okay,

329
00:22:13,840 --> 00:22:22,520
could crossing it out get performance of close to Marko's gold standard, from non experts.

330
00:22:22,520 --> 00:22:30,880
So you ran the line system, the explanation system, against the dirty data set, you came

331
00:22:30,880 --> 00:22:35,280
up with these explanations that included things like emails that should be irrelevant,

332
00:22:35,280 --> 00:22:38,800
and you asked if turkers, yes, turkers if they could figure that out.

333
00:22:38,800 --> 00:22:44,080
Figure out what parts of the explanation, in the sense what features, you thought should

334
00:22:44,080 --> 00:22:47,640
not have been used as parts of this decision, and how did that go?

335
00:22:47,640 --> 00:22:54,480
So after just three rounds of mechanical turkers crossing things out, they were able to

336
00:22:54,480 --> 00:22:58,720
get better accuracy than Marko's gold standard data set.

337
00:22:58,720 --> 00:23:03,560
So they were able to clean the data better than Marko did.

338
00:23:03,560 --> 00:23:04,960
What is around in this case?

339
00:23:04,960 --> 00:23:13,600
Around was showing the, so adding into details, but we showed the explanations to a number

340
00:23:13,600 --> 00:23:19,360
of mechanical turkers, they were able to cross it out and retrain the model, then we showed

341
00:23:19,360 --> 00:23:24,160
the new model to different types of mechanical turkers, the crossing things out and we showed

342
00:23:24,160 --> 00:23:25,160
it again.

343
00:23:25,160 --> 00:23:30,160
So we just did that three times, three iterations with non experts, but looking at explanations,

344
00:23:30,160 --> 00:23:34,600
they were able to find all sorts of problems with data, clean it, and get better performance

345
00:23:34,600 --> 00:23:38,880
than Marko did, like sitting down and like trying to clean the data himself.

346
00:23:38,880 --> 00:23:42,720
Which was surprising, so that means that non experts, this is just an example, so just

347
00:23:42,720 --> 00:23:51,640
a non-experts are able to understand explanations of complex machine learning system and provide

348
00:23:51,640 --> 00:23:55,920
some feedback to that system that can be used to improve the performance of that system.

349
00:23:55,920 --> 00:23:57,680
Which was really surprising.

350
00:23:57,680 --> 00:23:59,920
Wow, that's very cool.

351
00:23:59,920 --> 00:24:01,160
It was very cool.

352
00:24:01,160 --> 00:24:06,520
And then the second user, so then you know, Marko was bold and then went to come up even

353
00:24:06,520 --> 00:24:13,840
more interesting for this, so Mark, and the second one was also really exciting.

354
00:24:13,840 --> 00:24:18,200
So here's what he wanted to ask.

355
00:24:18,200 --> 00:24:22,440
When you train a machine learning model, you usually train it on some data and you evaluate

356
00:24:22,440 --> 00:24:29,320
it on some data that you hold out, it's called a test set, so that you don't kind of get

357
00:24:29,320 --> 00:24:32,240
a biased prediction on how well the model do.

358
00:24:32,240 --> 00:24:37,280
So you can imagine some models might do well on the training set, but don't do well on

359
00:24:37,280 --> 00:24:40,760
the test data set, so we want to throw out those models.

360
00:24:40,760 --> 00:24:45,160
And some models will do well on the test set, and then you want to keep those models.

361
00:24:45,160 --> 00:24:46,560
So that's what you typically do.

362
00:24:46,560 --> 00:24:50,800
If you go back to the training news groups data set, if we had just looked at the training

363
00:24:50,800 --> 00:24:56,000
news groups with the email address in the test set, you do well on the test set, so you

364
00:24:56,000 --> 00:24:57,000
think you're doing well.

365
00:24:57,000 --> 00:24:58,000
But it wouldn't be.

366
00:24:58,000 --> 00:24:59,000
We didn't validate very well.

367
00:24:59,000 --> 00:25:04,360
But if you had tested on some other data set, they didn't have some as email.com, then

368
00:25:04,360 --> 00:25:06,960
it would have them badly, then it would be able to throw it out.

369
00:25:06,960 --> 00:25:11,240
So here's the experiment that Marko did, which I thought was brilliant.

370
00:25:11,240 --> 00:25:19,320
He split the data into a training set and a test set, and he trained a bunch of models,

371
00:25:19,320 --> 00:25:23,880
a lot of models on different random subsets of that input data, of the training data.

372
00:25:23,880 --> 00:25:29,120
And some models did well on the training data, and some models did badly on the training

373
00:25:29,120 --> 00:25:30,120
data.

374
00:25:30,120 --> 00:25:32,480
He threw out everything that did badly on the training data, because we only want to keep

375
00:25:32,480 --> 00:25:34,520
high accuracy models.

376
00:25:34,520 --> 00:25:37,760
So he kept only things that were accurate on the training data.

377
00:25:37,760 --> 00:25:41,200
And then he looked at the test data set, and some models did well on the test data

378
00:25:41,200 --> 00:25:44,600
set, and some models did badly on the test data set.

379
00:25:44,600 --> 00:25:48,960
And then he said, oh, the model is still now, I mean, this is pretty standard, what any

380
00:25:48,960 --> 00:25:50,200
data scientists would do.

381
00:25:50,200 --> 00:25:51,200
Yeah, yeah.

382
00:25:51,200 --> 00:25:52,200
Pretty standard.

383
00:25:52,200 --> 00:25:54,200
Well, Z-Watt sticks against the wall.

384
00:25:54,200 --> 00:25:55,200
Yeah, exactly.

385
00:25:55,200 --> 00:25:56,200
Yeah.

386
00:25:56,200 --> 00:25:57,200
Now he did the following.

387
00:25:57,200 --> 00:25:58,200
Okay.

388
00:25:58,200 --> 00:26:02,160
He took mechanical turkeys with no nothing about machine learning, and he showed them explanations

389
00:26:02,160 --> 00:26:06,560
for the models that did, they didn't say anything about the process, and it was all

390
00:26:06,560 --> 00:26:09,000
randomized and blind and everything, right?

391
00:26:09,000 --> 00:26:14,000
So the explanations for models that were doing well on the test set, and models that did

392
00:26:14,000 --> 00:26:20,360
badly on the test set, and they both looked equally good on the training set.

393
00:26:20,360 --> 00:26:21,360
Yeah.

394
00:26:21,360 --> 00:26:27,320
And the test set, you mean the validation.

395
00:26:27,320 --> 00:26:30,840
Actually, it was a hidden, in this case, it was a hidden test set, but it came out of

396
00:26:30,840 --> 00:26:31,840
all this way.

397
00:26:31,840 --> 00:26:32,840
It was a hidden test set.

398
00:26:32,840 --> 00:26:36,840
So there was something that you wouldn't do as a data scientist, but the test, he held

399
00:26:36,840 --> 00:26:43,040
off some additional data, and he ran both, he ran lots of models, did well on the training

400
00:26:43,040 --> 00:26:50,040
set, and he picked out some that did badly on the test set, and did well on the test set.

401
00:26:50,040 --> 00:26:55,560
And then showed explanations for all those models to make out code turkeys and ask, which

402
00:26:55,560 --> 00:27:00,080
model do you think is going to be better in the real world, based on the explanations of

403
00:27:00,080 --> 00:27:01,840
why they're making their predictions?

404
00:27:01,840 --> 00:27:02,840
Yeah.

405
00:27:02,840 --> 00:27:08,080
And they were asked to pick between one, between two.

406
00:27:08,080 --> 00:27:09,080
Yeah, between two.

407
00:27:09,080 --> 00:27:11,280
And you were comparing them with the coin flip, right?

408
00:27:11,280 --> 00:27:12,280
Yeah.

409
00:27:12,280 --> 00:27:13,280
Compared to the coin flip.

410
00:27:13,280 --> 00:27:14,280
And they did better than a coin flip.

411
00:27:14,280 --> 00:27:16,880
So a coin flip gets 50% accuracy, 87% accuracy.

412
00:27:16,880 --> 00:27:18,720
Wow.

413
00:27:18,720 --> 00:27:26,680
So totally untrained, unwashed mechanical Turk masses are basically creating, you know,

414
00:27:26,680 --> 00:27:28,280
doing feature engineering on models in their head.

415
00:27:28,280 --> 00:27:29,480
So the first part was feature engineering.

416
00:27:29,480 --> 00:27:31,800
The second part was like model selection, basically.

417
00:27:31,800 --> 00:27:32,800
Right.

418
00:27:32,800 --> 00:27:36,240
They were able to look at explanations and figure out this model is stupid.

419
00:27:36,240 --> 00:27:37,240
Yeah.

420
00:27:37,240 --> 00:27:39,200
Even though on the training set, it looked great.

421
00:27:39,200 --> 00:27:40,200
Yeah.

422
00:27:40,200 --> 00:27:42,200
But in the real world, there's going to be bad.

423
00:27:42,200 --> 00:27:43,200
Wow.

424
00:27:43,200 --> 00:27:44,200
That's pretty amazing.

425
00:27:44,200 --> 00:27:46,200
And that was amazing to me.

426
00:27:46,200 --> 00:27:50,360
And the fact that we can do that, as I said, we're only scratching the surface here,

427
00:27:50,360 --> 00:27:57,080
but the fact we can do that, to me says humans will be in the loop in the long run.

428
00:27:57,080 --> 00:28:01,480
There are insights who have, I mean, humans in the loop, because even kind of the statistical

429
00:28:01,480 --> 00:28:05,760
problems that underlie this question, like if we discuss for a long time, we can talk

430
00:28:05,760 --> 00:28:09,720
about why there's a relevant statistical.

431
00:28:09,720 --> 00:28:13,920
Humans might be able to pick those out and they'll be able to do better feature engineering.

432
00:28:13,920 --> 00:28:18,040
They'll be able to understand problems they're going in the data, even on train folks.

433
00:28:18,040 --> 00:28:24,520
And now, if you imagine doing this to get more insight for the doctors or for systems

434
00:28:24,520 --> 00:28:27,120
in the real world, I think you could do really amazing things.

435
00:28:27,120 --> 00:28:30,840
So it's pretty exciting to me to start exploring this further and further.

436
00:28:30,840 --> 00:28:31,840
That's very cool.

437
00:28:31,840 --> 00:28:32,840
Let me ask you this.

438
00:28:32,840 --> 00:28:36,640
This is kind of in the weeds question, but were the features, what you might think of as

439
00:28:36,640 --> 00:28:39,000
natural features or engineered features?

440
00:28:39,000 --> 00:28:40,000
Yeah.

441
00:28:40,000 --> 00:28:45,640
That's the really interesting or a really interesting question.

442
00:28:45,640 --> 00:28:50,080
So the underlying models used the engineered features.

443
00:28:50,080 --> 00:28:54,200
So for example, he also showed this was good for deep learning models for images, which

444
00:28:54,200 --> 00:28:58,560
used really, you know, learn complex features of the data.

445
00:28:58,560 --> 00:29:01,720
But the way that he explained was from pieces of the input.

446
00:29:01,720 --> 00:29:07,400
So the assumption that he made was the input is interpretable.

447
00:29:07,400 --> 00:29:11,920
And by selecting pieces of images or part of the text, I used to look at that and say,

448
00:29:11,920 --> 00:29:13,840
oh, this makes sense.

449
00:29:13,840 --> 00:29:17,920
If we looked at like the seventh player of a neural network, I can say, oh, this is like

450
00:29:17,920 --> 00:29:19,720
a human being like, what is that?

451
00:29:19,720 --> 00:29:20,720
Yeah.

452
00:29:20,720 --> 00:29:22,040
And why do I care?

453
00:29:22,040 --> 00:29:24,760
And so that's why we biased towards this approach.

454
00:29:24,760 --> 00:29:29,160
It doesn't mean that in long run we want to invent something better that looks at the features

455
00:29:29,160 --> 00:29:32,600
because the problem might be down in the features, it might be down in the weeds.

456
00:29:32,600 --> 00:29:34,840
And that's kind of where the research should go.

457
00:29:34,840 --> 00:29:38,040
But as a first step, we looked at pieces of the input.

458
00:29:38,040 --> 00:29:42,600
And it's totally model independent, like you can work on neural network models.

459
00:29:42,600 --> 00:29:43,600
Yeah, yeah.

460
00:29:43,600 --> 00:29:44,600
We've done this with deep neural networks.

461
00:29:44,600 --> 00:29:46,400
We've done this with boosted decision trees.

462
00:29:46,400 --> 00:29:48,600
We've done this with lots of different kinds of models.

463
00:29:48,600 --> 00:29:49,600
Wow.

464
00:29:49,600 --> 00:29:50,600
Wow.

465
00:29:50,600 --> 00:29:51,600
So I know we need to get you off to your next session.

466
00:29:51,600 --> 00:29:53,440
Where can folks learn more about this?

467
00:29:53,440 --> 00:29:57,080
So if you just search for my name and line, you'll find our paper.

468
00:29:57,080 --> 00:29:58,680
It was a KDD this year.

469
00:29:58,680 --> 00:29:59,680
Line LIME.

470
00:29:59,680 --> 00:30:00,680
LIME?

471
00:30:00,680 --> 00:30:01,680
Okay.

472
00:30:01,680 --> 00:30:06,840
We'll find a GitHub project that Marco has been putting together, we hope it's some of

473
00:30:06,840 --> 00:30:08,840
these ideas.

474
00:30:08,840 --> 00:30:12,640
But yeah, it's been a pretty exciting work.

475
00:30:12,640 --> 00:30:18,320
And you just keep tracking, you know, Marco, Samir, there'll be a lot more in the pipeline.

476
00:30:18,320 --> 00:30:19,320
There's a really cool thing.

477
00:30:19,320 --> 00:30:20,320
That's awesome.

478
00:30:20,320 --> 00:30:21,320
That's great.

479
00:30:21,320 --> 00:30:24,320
And if folks want to reach out to you, you're on Twitter or what's the best way to get in touch

480
00:30:24,320 --> 00:30:25,320
with you?

481
00:30:25,320 --> 00:30:26,320
I'm on Twitter, Gastrin.

482
00:30:26,320 --> 00:30:27,320
It's my last name.

483
00:30:27,320 --> 00:30:28,760
It's the handle.

484
00:30:28,760 --> 00:30:29,760
Okay.

485
00:30:29,760 --> 00:30:30,760
And so reach out.

486
00:30:30,760 --> 00:30:31,760
Awesome.

487
00:30:31,760 --> 00:30:32,760
Give me some feedback.

488
00:30:32,760 --> 00:30:36,880
We're also, as you know, on a Coursera teaching machine learning and that's another place

489
00:30:36,880 --> 00:30:38,880
that I interact with folks.

490
00:30:38,880 --> 00:30:39,880
Yep.

491
00:30:39,880 --> 00:30:40,880
And it's a great course.

492
00:30:40,880 --> 00:30:41,880
I highly recommend it.

493
00:30:41,880 --> 00:30:42,880
Very case study focused.

494
00:30:42,880 --> 00:30:43,880
I really enjoyed it.

495
00:30:43,880 --> 00:30:44,880
Okay.

496
00:30:44,880 --> 00:30:45,880
Thanks.

497
00:30:45,880 --> 00:30:46,880
Great.

498
00:30:46,880 --> 00:30:47,880
All right.

499
00:30:47,880 --> 00:30:48,880
Thanks so much, Carlisle.

500
00:30:48,880 --> 00:30:49,880
Yep.

501
00:30:49,880 --> 00:30:50,880
I'll do a handshake here.

502
00:30:50,880 --> 00:30:51,880
On the audio.

503
00:30:51,880 --> 00:30:52,880
Thanks.

504
00:30:52,880 --> 00:31:01,400
All right, everyone, that's it for today's show.

505
00:31:01,400 --> 00:31:03,400
Thank you so much for listening and for your continued support.

506
00:31:03,400 --> 00:31:04,880
A quick story.

507
00:31:04,880 --> 00:31:09,560
If you follow me on Twitter, you know that I recently called out an iTunes review that

508
00:31:09,560 --> 00:31:12,000
I'm actually particularly proud of.

509
00:31:12,000 --> 00:31:18,040
In this review, a user that originally rated the podcast a two out of five based on their

510
00:31:18,040 --> 00:31:23,520
disappointment with the switch to the interview format came back and revised that review to

511
00:31:23,520 --> 00:31:28,520
a four, noting that the interviews were getting better and that the format was really starting

512
00:31:28,520 --> 00:31:29,920
to grow on them.

513
00:31:29,920 --> 00:31:32,320
Now don't get me wrong, please.

514
00:31:32,320 --> 00:31:37,480
I really, really, really appreciate those of you that left five star reviews on iTunes.

515
00:31:37,480 --> 00:31:40,640
And I hope the rest of you go run and do that right now.

516
00:31:40,640 --> 00:31:45,400
But it also felt great to see that in spite of his initial misgivings, the shows just kept

517
00:31:45,400 --> 00:31:49,160
getting better and the user eventually came around.

518
00:31:49,160 --> 00:31:51,920
That kind of feedback is great to read.

519
00:31:51,920 --> 00:31:56,000
Thanks to everyone who stuck with the show through the transition and I hope you're continuing

520
00:31:56,000 --> 00:31:57,760
to learn a ton.

521
00:31:57,760 --> 00:32:03,480
Please join the conversation by commenting on the show notes at the twimmalei.com website

522
00:32:03,480 --> 00:32:09,600
or by reaching out to me on Twitter where you can find me at at Sam Charrington or at

523
00:32:09,600 --> 00:32:10,800
Twimmalei.

524
00:32:10,800 --> 00:32:15,920
All right everyone, thanks again for listening and catch you next time.

