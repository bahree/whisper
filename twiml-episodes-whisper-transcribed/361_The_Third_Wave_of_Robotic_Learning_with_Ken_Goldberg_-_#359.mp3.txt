Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.
Alright everyone, I am on the line with Ken Goldberg. Ken is a professor of engineering at UC Berkeley.
Ken, welcome to the Twimal AI Podcast. Thank you, Sam. Pleasure to be here.
Yeah, it is great to finally get you on this show. We've been talking about this for a bit.
You know, I meant to ask you before we started. Last time you were, you mentioned you were working on a book.
Maybe we'll, did I get my remembering that right? Well, I think I've been thinking about that for a while,
but I'm also thinking about it and more right now, an article. Okay. Okay. Okay. Well, we'll, we'll get to the article.
I think I first came across you and some of your work in the context of DexNet.
I saw that at a Siemens Innovation Fair last year, and I think we exchanged some tweets and stuff like that.
But, you know, I would really love for you to introduce yourself to the audience and share a little bit about your background
and how you came into working in robotics and AI. Okay, great.
I, well, first since you mentioned Twitter, I should mention my Twitter handle, which is at Ken underscore Goldberg.
And I've been trained very well by my daughter to post there at least once a day.
So, I've got, I've actually found it very interesting channel. So, so I am posting technical things as well as,
as updates about things that I'm finding out, which is that I'm learning about, which is, I find very useful.
So, my background is that I was, I went to University of Pennsylvania and then went to Carnegie Mellon for PhD.
I was at USC for four years and then moved to Berkeley, where I've been for now 25 years.
I, here I run a lab. The, we, we called the Auto Lab for Automation Science and Engineering.
And we have approximately 30 students doing research in there. And we're doing work.
There's, there's, there's postdocs, graduate students and a good number of undergrads.
And we're also associated with other labs like the Berkeley AI Research Lab and the RISE Lab and Citrus and other programs at Berkeley.
Our particular lab is interested in, in, in doing research on, on robotics.
Basically, an algorithmic approach is to robotics and specifically in the last few years,
we've been focusing on learning methods for, for imitation learning, deep learning and reinforcement learning
for control of robots and applications from grasping, as you mentioned, which is a primary one
that I've been working on for, for 35 years, to surgery, surgical assistance,
assistance to human surgeons for, for robotics and home robots to, especially for seniors and,
in, who are, who prefer to live at home. And the last area is very new and we can talk about it later
is, uh, is agriculture. And we have a new approach to polyculture farming that we're exploring
using deep learning. So one thing that I thought was really interesting, uh, in looking at your bio
is in spite of the fact that you are a highly accomplished roboticist, you start your, your
bio starts with Ken Goldberg as an artist. Uh, and so your art clearly must be very important to you.
I actually saw some sketches behind you. Um, and I'm curious, uh, you know, I'm, I'm curious about
Ken as an artist and, you know, how, if, if it all that ties into your work, it's not the usual
fare of, uh, this podcast, but, um, then I saw some of us, you were a filmmaker as well. Um,
is that your art? Like, tell us about that. Okay. Well, actually, I've wanted to be an artist
when I was a kid and I, I, I, I, I basically, my mother said, listen, you can be an artist
after you become an engineer. So, um, she, she, she, she was very wise. And I think it was, it
was a, it was a good choice for me because I actually love both. Art is something that I, I take
very seriously. I think it's often underrated by, by many people, especially engineering engineers,
who think of it as, as lightweight. It's actually just the opposite. Trying to produce something
that's meaningful in the art world is extremely difficult and demanding. So I've spent a lot of
time studying art. I have made a series of installations and projects that almost always involve
technology in some way, but they're also commenting on the role of technology in society. So probably
best known piece is a project is a project called the telegarden that my students and I set up
in the very early, very big early years of the internet. So it's 1995 that we, we connected a
industrial robot arm to the web interface at the time, which was mosaic browser. And we built
an interface that would allow you from your screen, from anywhere, from your laptop.
There were no cell phones at the time, but you could, you could log in and control this thing.
I think I remember this. Right. Yeah, it was a very fun project. We thought, well, it's kind of
curious, you know, what, what, who would use it if anyone? And we got thousands of people coming in
and, and, and moving the robot, but the, the part of what was made in artwork was the context
because it was sitting inside a garden, a real physical garden, so you could plant and water
seeds remotely. And then we got tens of thousands and we estimate that over the, the time that
project was, was that robot was available online, which was approximately nine years. It was
visited and over a hundred thousand people participated in the, in the project. That's awesome.
That's awesome. And again, kind of the technology and art coming together.
Right. So that was the thing Sam, because one of the ideas was that I, I don't think I would have
pursued that if I had just stuck with my research plans at the time, but because this came out
and it offered a way to reach at the time when I saw as a potentially very broad audience,
I started putting effort into this and then I, there was a fantastic team of students who worked
on it and then we, we, I was thrilled with the, the idea that you could take a robot and you could
put it into the hands essentially of a potentially millions of people. And then there were,
there was a proof of concept. There was all the user interface questions. There, it turned out
that there were lots of interesting theoretical questions that came out of that. So after that
project, we did a series of subsequent projects and then had an NSF grant to develop versions of
this. We have a patent related to this. So yeah, it really grew into a whole new direction of
research that that really started with art. Awesome. Awesome. And so tell us a little bit about your
research interests nowadays more broadly. So we're still doing art and I can come back to that.
There's a new project. But the, the, the lab right now has been, been very, very focused on robot
learning. And especially as, as I know your, your listeners are very aware, there's been a huge
revolution in the past decade. And so we've been, we were interested in this before the, the advances
in deep learning started. But now it's really has become a huge focus for us. So in particular,
we have this, we've been working in robot grasping for many years. And then when deep learning came
out, we saw an opportunity to apply it. I can tell you that story if you, if you like, how we do it?
Maybe start from the perspective of the grounding on the challenges associated with grasping.
Like we see these pictures of, you know, whether they're, you know, robot hands or more industrial
types of robots or prostheses. And, you know, they can grasp like we've seen, we've all seen,
you know, pictures of that. But maybe it's harder than it looks or, you know, maybe the,
we want to have the opportunities that, you know, we've not yet figured out.
Oh, good. Okay. So I can, I can answer that partly. I, I've realized that only in the last
few years, the part of the reason I believe I went into this field was that I myself as a kid was
a, was incredibly clumsy. I still, I still am. I mean, you know, anyone would throw me a ball. I
would drop it. And so, you know, I was a last kid getting picked for any sports games or anything
like that. And it was just that I, I think that me, unconsciously made me interested in,
in trying to figure this thing out. Like how, how do you grasp things? And many years later,
when I was in undergraduate, I joined a laboratory at the University of Pennsylvania. And they were
studying various aspects of tactile sensing and, and, and I built a very simple hand
with another student. And we started really exploring this, this question of how do you grasp things?
And it is fundamentally difficult for robots. I like to say that robots
remain incredibly clumsy today. They're much better than they were. But industrial
arms, if you give them novel objects, they will drop them with a fairly high frequency.
And this is a problem because what you really want is you want, um, you want to be able to pick
up anything that's put in front of you. And the application, the big application that's
growing enormously right now is e-commerce. So you want to be able to take objects,
you know, every order is different. So you want to take things from bins and pack them,
lift them out of the bin, grasp them and put them into boxes or bags for shipment. And that
turns out to be a bottleneck right now for robotics. And Amazon's been holding competitions to,
for teams to figure out how to do this for a few years now, right? Right. So they had a very
interesting competition called Amazon picking challenge and picking is the word for grasping
out of a bin. And they had, and it pushed the field forward in a very, very constructive way.
And there were, there was progress made. But however, they stopped doing that about three years ago.
Oh, really? Okay. And the, but what the good news is that it did bring a number of researchers
into this, got a number of researchers interested. And now there's really a great, great number of
researchers working on this. Now we had been working on it for, as I mentioned, 35 years. I mean,
since I was an undergrad. And so about seven or eight years ago, we were looking at new ways
of grasping and, and, and we were working with Google and talking with them about what we,
we called the Dexterity Network. And the idea there was to use an analogy with machine vision.
And you're familiar with ImageNet. And I'm sure you've talked about it on the, on the podcast.
So ImageNet really transformed machine learning by having a very large data set of labeled images.
And it seemed that you get to a critical mass. Enough labeled images that then you can train
a system and it could start to generalize to new images. So the question for us is, could we do
something analogous in grasping by assessing a very large data set of three-dimensional objects,
three-dimensional models, CAD models, and then labeling them with grasps and specifically robust
grasps. So then we can start to learn from those examples. A robust graph meaning robot actually
has the object and it's not in some precarious position. It's, it's fingers are in the right
place, so to speak. Right. And actually, let me, I'm glad you asked that question because that,
that's actually very important. So by robust, we mean robust to the following uncertainty in sensing,
control, and physics. So this is a fundamental reason why robots are still clumsy.
Because of this uncertainty, what do I mean? Well, let's start with, with perception.
Just even if you have the highest resolution camera available, and you look at a scene,
you still don't know the precise geometry of where everything is in that scene. And there has
now been advances in depth sensors, which I can talk about because I'm a big believer in those.
I think those are really game-changing, but they still don't completely solve this problem. Because
if there's anything reflective or transparent on the surface that causes the light to react
in unpredictable ways. And so you, you don't get, it doesn't register as an exact correct
position of where that surface really is. And you primarily referring to LiDAR and connect
types of sensors, or is there something else? No, that's what I'm referring to. And those are,
there's now a whole new genre of such cameras coming out that are lower in cost, higher resolution,
higher frame rate, more reliable for various reasons. And so I'm, I'm very excited about that.
But it doesn't solve the problem. It actually facilitates making progress.
So the perception is still a huge problem. I'm looking down at the scene, even right now at my
desk, and I have a sense of where things are. But a robot doesn't actually really figure out
exactly where things are in space. Second of all, the control. And by that I mean that robot,
we move its gripper. And is that, I mean, that even that is an interesting question, right? Because
certainly if it's a single two dimensional image, then yeah, we understand why the robot doesn't
can't figure out where things are. But now when you're introducing in stereoscopic images and
LiDAR and all these things, like we should be able to give the robot enough data point that it
should be able to figure it out. But there's, you know, there's still something missing, is it,
is it that we have kind of surpassed our capacity from a kind of compute per unit time perspective,
or is it that, you know, locating objects on a flat surface requires some fundamental human
intelligence that we haven't transferred to robots yet, or is it something totally different?
A little bit of all of those. I mean, one of it is that you would think, right, if you put
multiple cameras and then you use stereo and depth sensors, that that would, would help you.
But remember, every time you add another modality, another sensor, you actually add more complexity.
And you end up with more cases where you have contradictions between what these different
sensors are telling you. So in fact, what you get is these, the results are that you, you actually
have system will often have two aspects of its own sensory apparatus telling you two different
things, right? And that's a problem. So then it doesn't know what to trust. So that,
throwing more, more, more sensors does not solve the problem. The other is you mentioned computation.
So that I actually think you're, you're right, we are, we are, computation is not necessarily the
bottleneck. Right now we have very fast computing and we can distribute it over many, many, many
processors. But the, it is challenging when you have to build statistical models because of the
residual uncertainty, you don't actually know. So you want essentially to say, here's where I
think it is. And here's my confidence level on top of that. So there's a really that you're
actually out, you're dealing with a statistical model of the environment. And that is actually fairly
high dimensional. And you have to, there is some computational complexity there. The, the other
thing though is that humans and animals, by the way, seem to cope very well with the problem like
grasping and interacting with the physical world. Because we bring to it a sort of inherent
understanding, a deeper understanding about the, the nature of objects. And so this is very subtle.
We don't really, I can't describe this exactly. It's, it's, it's, it's intuitive to us how to make
things up. But it's very hard for us to, to, to formalize that intuition and put, give that to a robot.
So that, so the sensing is still a problem. And anyone who says, oh, that's a solve problem,
doesn't really know the problem. Right. And this is true, by the way, for self-driving cars,
and every, every other application where you want to perceive the environment. And by the way,
it matters for sensing, for, for grasping, very critically, because for a car, maybe an inch,
one way or another, doesn't matter. Although it does, if you're on the edge of a cliff. But for
grasping, one inch makes all the difference. Even a half, even less. Yeah. Exactly. A millimeter,
or less, can make the difference between holding something and dropping it. So that's why it
matters to get this exactly right. It's very subtle. And that's all the sensing. That's in the
control aspect I was starting to say, which is that the robot has to now get its manipulator and
its gripper and its fingertips to the precise position and space consistent with what it's,
what it believes is happening in from its sensors. For a robust grasp. So, so wait, I'll come back
to robots. Right. We're almost, the third one, the third one is, is, is physics. So one of the
things that we don't know often know, if I look at an object, I don't know exactly where the center
of massive that object is. Very importantly, I don't know its frictional properties. And friction
is still an immense unknown for science. I like to say that we can predict the motion of a,
of an asteroid, a billion miles away, far better than we can predict the motion of pushing a
pencil across a table. Because you actually can't predict the latter. Is that, is that, is that,
that's hard to believe? Like, try it. Put a pencil down. I thought I did that in McKenna's class
many years ago. Oh, yeah. For a fiction of static friction, a fiction of dynamic friction.
I apply a force and that tells me how far I go. No, those are, what are you talking about, Ken?
Exactly. Exactly. Cool arm friction, right? The law of cool arm friction. Well,
unfortunately, it doesn't actually work that way. So it's a, it's a reasonable approximation.
But look at just pushing, take a pencil, put it on your, on your desk and put your finger in the,
hold it's horizontally and then put your finger in the middle approximately, start pushing it.
At a certain point, it's going to rotate off your finger. And you don't know where that's going
to happen. And if you do it again, it'll do it somewhere different. Every time you do it,
it's somewhere different. Why? What's going on? Because it matters. Because really, what's going
on under there is a chaotic system. And it has to do with the tackle. It's a little dirty. Right.
One lot of out of true, one microscopic grain of sand of anything under there is going to
cause it to behave extremely differently than if that, that sand wasn't there. So, and we can't
blend itself to some kind of statistical probabilistic distribution of how the pen's going to tumble.
Right. But here's a thing is that in fact, that's another example where the statistics is not
necessarily going to help you because it's not going to be a nice Gaussian distribution of where
it's going to end up. It actually will end up in possibly a very, a very multimodal distribution.
So, it's very, it may be a non-parametric distribution of where the pen will end up in space.
And it's very hard to model or predict. It's going to change every time there's a little bit of
moisture or dust on your table. And you can't even if you just want a robot to push pens.
You're out of luck, out of luck. Pencil pushers, okay. Robots can't do it. So, one of the things
that's really interesting is that these three elements together conspire to make robotics
grab more robot grasping extremely difficult. The uncertainty in the perception, uncertainty in
control, and uncertainty in physics. Those are, those are fundamental. Now, what I, let's come back
to robust, what I mean by robust. What I mean to that by that is that can I look for a grasp
that'll be robust, that'll be insensitive to my uncertainty in those three elements.
What I mean by that is I want to grasp that even if my perception is slightly off,
even if my control is slightly off, and even if the physics is slightly off,
I'll still be able to pick the object up successfully. That's a robust grasp.
So, an example, you know, if you pick up a glass of wine, for example, you put your hand under it,
you sort of, you put your fingers apart, you scoop up around the stem, and then you lift.
Now, that's robust grasp, because even if the glass isn't quite where you thought it was,
even if your hand isn't quite where you thought it was, and even if the thing is slippery,
you're still going to be able to pick it up. That's a robust grasp. So, it turns out that for most
objects, there are grasps that are more or less robust, and what we're trying to do is get a robot
to learn that quality, that robustness. And we can generate that by using the physics that you're
talking about, that you learned, and actually it goes all the way back to centuries of beautiful
mechanics of understanding the physics and forces and torques, wrenches and space,
that characterize what happens if we know everything, then what we do is perturb that statistically.
And we say if we can, if it's robust, it works for all these statistical perturbations with
high probability, then we say it's a robust grasp. Now, before we make our way back to Dexnet,
which is where we start here, you brought up an interesting point about the kind of marriage
of physics and statistics. When I talk to some of your frequent collaborators,
Sergey Levine, Peter Abile, come to mind, the impression I get from them, maybe less so now,
than years ago, was that hey, we should throw away all the physics and just get the centers and
let the computers learn how all the physics is going to work. When I talk to more traditional
roboticists, they are more interested in preserving everything that we've learned in the past
couple of centuries via physics. I get the, well, you tell me where are you in that kind of
with feed in both worlds? Good. I like that you ask that. I call the old physics, the classic
physics, the first wave of robot grasping. That's really dominated and still very, very
common in the robotics conferences and journals. It's very well grounded. It's beautiful mathematics,
beautiful theory. That's the first wave. The second wave is really robot learning,
which Peter, Sergey, and many others are very excited about today, which is purely data-driven
approaches that say forget about the physics, but let's just learn it. Let's just learn it from
observation purely. Actually, I'm an advocate of what I call the third wave and that is to synthesize
those two, to use the physics where it's appropriate and use learning where it's appropriate.
That those combination is exactly what we need, figuring out where that combination is is the
challenge. That's really the story behind Dexnet. Dexnet you assembled this data set of CAD,
models, and grasps, and then what did you do with it? What we did was we applied the first wave,
the physics and the statistics to lots and lots of simulated models. We just basically
simulated and said, are these robust to simulations and perturbations of each of the candidate
grasps? We checked for many, many combinations and found grasps that are robust. We used that
whole first wave, all that beautiful theory there. Then we took the examples that were generated
and used those to train a deep learning system to be able to generalize to new examples. One
thing that's important is that we did this with three-dimensional models. I occurred to be that
if we're going to try and model the perception part, what we really care about is the three-dimensional
arrangement of the surfaces and the points in space. I don't care about the color of things or
the texture on things. In fact, that's a distraction. We asked, could we do this with just pure depth
sensors? Another nice thing about depth sensing is that you can simulate it very nicely.
Simulating color images actually turns out to be very hard, and we can talk more about why,
but in simulation, you can say, you know the points in space perfectly. There you know everything,
by the way, there's no uncertainty at all, but what you can do is now add a little bit of noise
to what you would say is, what would a depth sensor see if it looked at this scene?
Now you can say, okay, but I know we're talking about the depth sensor. The simulation or the
imagery? This is a depth sensor. The depth sensor is creating an image, but it's a depth image,
if you would. That's all it sees. It's throwing out what we call RGB color information, and it's
only using the depth information. Now I have an arrangement of points in space, and then I know what
a grasp, the successful grasp, when that arrangement of points corresponds to a successful grasp or
not, because I'm using the physics and statistical model of the sensor. How do you even represent that,
is that you have for a cylinder you've got four ideal points on one side and one ideal point on
the other side, and you do like some kind of distance metric of the fingers to those points or something, or like...
How do you do that? So representing that sounds hard. Well actually, so if I have a cylinder,
I can look at two points on opposite sides of the cylinder. I can generate friction cones around
those, and I can just check if those two friction cones intersect, then actually by the first
way of grasping, I can check if that grasp is going to succeed, or not. So that's fairly...
That's very fairly understood. If I knew everything, then I could do a check that would tell me
if it succeeds or not. But since I don't know everything, what I would do is I say, well here,
if I change it ever so slightly in all these different ways, if it still works, well then it's
robust. Now what we do is we start with the perfect model of these objects, then we say,
okay, we know what the good grasps are, because we've done... The robust grasps are for those
perfect models, and then we say, let's make a... Let's pretend we actually have a fairly noisy sensor,
which we do in practice, and so we can simulate that noise, just add noise to all the little points
that you would detect with your depth camera. So now you have a noisy pattern of points in space,
and you know what the true robust grasp was for that pattern of points. So that is the input to...
That's one example of a robust grasp, and you run that through, and you actually have already
computed the probability of success. So the output is just a scalar number from zero to one,
which is the quality. We call it the probability that that grasp will succeed. That's one example.
Now we generate millions of these examples, and we can do this very fast. Actually, you can generate
the examples overnight. Then we say, okay, now we have a nice data set. It's not quite as big as
ImageNet, right? But it's pretty nice-sized, and by the way, I have to give it both positive and
negative examples, so I have to give it a whole range of qualities, so I can learn that whole function.
Now what I do is... Now I'll put that out into the field, where I'm taking new depth images,
from a real camera, of objects it's never seen before, and candidate grasps for that object,
and then it'll tell me which one, basically, it can quickly evaluate the grasps quality.
And then what I do is I try a number of different grasps, again, synthetically, on that depth map,
and it tells me this is the one with highest quality. So now what I do is execute that one,
that is defined, we consider that the optimal grasp, the one with highest quality, and we execute it,
here's the thing, it works remarkably well, far better than we thought. So that was a big surprise
for us. That idea that you could train it very fast, you could generate lots of examples,
you could train it on that work relatively fast, again, another overnight, but then the result
was it did generalize in surprising ways. Now it's not perfect, and it's not perfect, I will say,
and it's really important, and by the way, I really believe it's our duty as roboticists to
point out the limitations and limits of our work. So we're able to get up to the 90 plus success
rate, 90 plus percent success rates, but it depends on the nature of the objects. So if the objects
are all fairly well-behaved, like cylinders and cuboids, then it's fairly easy to do well,
but it's when you have more complex geometries that the system, many systems have trouble,
and this is where our system was relatively good. Again, getting above 90, but not close to 100.
And so the generalization that you're referring to, specifically generalization and shapes,
as opposed to grippers, sensors, and any of other pieces of the system.
Good questions to them. So that's actually a great point. We have to retrain this. If you change
the gripper, I have to generate a new data set for that for your gripper, and then retrain a
new neural network. But the framework should apply. And if you change the sensor, same story.
I have to change the generation of the samples, get a new training set, retrain the network,
and then use that. So in fact, interestingly, just a quick story. When we first announced
DexNet, we got picked up in some news reports, and we were contacted by industry.
And they quickly taught us that actually we want to use this, but we use suction cups,
not grippers. And we sat and thought about that, and never forget we were sitting in the
lap, and suddenly it was like a light bulb went off, and we said, wait a second, we can take the same
exact framework and apply it to suction. And so instead of a two-point grasp, we're looking for
a single point grasp. And that's the suction point, and you want that to be robust.
So kind of flat, kind of bigger than the suction cup itself, that kind of thing.
That's right. But we can formalize that in a paper. We actually started looking around for
literature, like how do you define the quality of a suction grasp? And amazingly, there was a gap
in the literature there. Wow. I mean, we looked and we could not find it. And to this day, I don't
know if someone who's really nailed that question. But we developed a model that looks at the,
basically, the points around the suction cup, how far they deviate from planar and a spring model
that would give you an estimate of how well the seal would be achieved. And then we also
looked at the object in general and how what wrenches would be created as you lifted that object,
because it has to do with the center of mass and the angle of the surface you're making contact
with. So you could come up with the exactly the same physics that you were talking about earlier,
again, a first wave type of approach. But then we perturbed it by all the same ideas. So we don't
know the exact position of the surface. We don't know the exact position of the suction cup.
We don't know the exact physics of the suction and the frictions. But then we did the same thing.
So then we have a model of suction cups. And again, it worked remarkably well.
Nice. Nice. And so Dex that was a few years ago, a couple like 17 or when, well, it was 2017
that Jeff, and I really want to give credit to Jeff Moller. This was his PhD work. And he,
most of the ideas, I mean, he was, he's still, he's now running a startup called the ambidextrous
laboratories. And Jeff is a brilliant student and really a great engineer. He worked out a lot of
this and deserves a huge amount of the credit. So my role was essentially guiding him toward
toward taking a, basically teaching a, a TA in a class that was on, on the mechanics of,
of grasping because that's where he picked up the physics aspect of it. And then in our discussions
we basically on Dexnet, et cetera, that we started thinking, okay, how can we start put this
together? But he had the idea of applying it to heaps of objects, which I, I was skeptical
that that was going to work. And then it was very surprised what it did. Awesome. Awesome. So one of
the, the things that we kind of hinted on earlier, it's related to the, you know, first wave,
second wave, third wave. I don't know if this is something that you'll want to comment on. But,
you know, I think a lot of the way we think about robotics or maybe just some of the way we think
about robotics is influenced by like the general, why don't I always forget their name, not general
dynamics. Boss dynamics. The boss dynamics. Yeah. Videos. Right. As a roboticist, like, you know,
in particular, roboticist that is, you know, thinking about things from the perspective of learning
and AI, like what's your take on those? My perspective is that actually Mark Raybert was my first
research advisor, Carnegie Mellon. Oh wow. And I was, I used to hang out in that lab where they
were doing the, the running machine, which was at that, like, at that time, just a single Pogo
actuator. And it was, it was, it was beautiful research. I think what I always, one thing that attracted
me to Mark was that he would always spend time showing the bloopers and show the thing running
and then he would show it wiping out. And it was really nice because he would always say,
well, look at, you know, it's, it works, but not always. And somehow, what I'm worried about,
and is that, for example, boss dynamics, when it showed, showed the, the robot doing the backflip
that got, you know, millions of views, at most, at least, even the backflip. Yeah. Now,
that's remarkable. And it's beautiful. But if you stay, if you watch to the end of that video,
it goes black for a little while. And if you stick with it, it then goes to a, it shows you a blooper
where it just totally wipes out. Yeah. In fact, is that's very common. All right. So, but they,
they, they, they sort of put that delay. And most people didn't see that. So when I give talks,
I always, I always show that blooper. I say, this is what our world is. Right. This is so important
to convey. So I have a huge respect for Mark and his, his, his team at Boston Dynamics. But I
think that they, at least the way that the videos sometimes are portrayed or that, hey, we're,
we've solved these problems. And robots are now capable of, you know, all kinds of agility,
like, like, as good as, as a gymnast. And the reality is that it's not, we're nowhere near that.
Yeah. Yeah. Well, which is interesting. So you're saying that we're not near the agility. There's
a whole, another level of it or layer, which is, at least as I understand it, like the self-directed
ness, it's not like someone said, robot, do a backflip. And it took 400 times, but it did 10 backflips.
It's more, much more choreograph than that. Someone is plotting out the, the points, you know,
in which it's doing the backflip, that kind of thing. Like, can you, is that your impression as well?
Like, what's your take on that? You mean in terms of how well is it crafted or fine-tuned?
Mean more how, how autonomous is it, I think? Oh, right. I'm getting out like, or how is it,
you know, the flip side of that being how orchestrated, orchestrated is it?
Flip side. Good. Yeah. It's a, no, the, the, the, the, it's very, I would say, let's see,
it's a little difficult to say, but I would say orchestrated is probably the right word.
There's a lot of results like, you know, where you look at something like OpenAI's hand doing a
Rubik's cube. And what's equally important to point out is that despite this huge and very
impressive engineering effort, that it still drops that cube very often, and that the motions are
very, the imprecise and inefficient. And we're, we're making progress, Sam. I mean, that's the thing
I want to say. I don't want to, I don't want to cast a negative shadow on, on, on research. But
I also think it's really important to understand that it's not nearly as fast as the, the, the public
beliefs and the press often leads people to believe by, by highlighting the successes and not
talking about the failures or the limits. Yeah. So that's what I was getting to where I believe
that there's a lot of, I do believe our field is, is somewhat overhyped. I do. I think AI in
general and I think robotics, we have, we are making wonderful progress, but let's put it in
context. And so don't over raise the expectations, because I do worry about a robotics winter,
where we're going to, where people say, wait a second, you told us you're going to have self-driving
cars and now we're still waiting for them. And this whole thing is a bad idea. So we're just going
to cancel all the projects. That is a real danger. You worry about them more than a robot uprising.
Far more. I'm not worried about a robot uprising. I totally not.
So now you're working on some, some other interesting things since the, or beyond the,
the grasping work, a couple of things that I came across were telemedicine and some activity
there, agriculture. And tell us about some of the, the more recent things you're up to.
Okay. Well, one area that I've been interested in for many years is, is, is, is robot assisted
surgery. And you may have seen sometimes hospitals who advertise that you have a robot, they're doing
surgical, you know, they have a surgical robot. Well, what they mean by that, it's intuitive,
is the, is they, by far, the, the biggest, biggest company in this space is they have a system
that a human can operate a human surgeon, but that reflects the actual motions of the human
in the body of the patient. And this has some great advantages because you mean like haptic
feedback kind of actually that's, it doesn't have haptic feedback, which is interesting, but it can,
basically take the motions I make with my hands, right? And surgeon is operating with, think of
it as VR. And then looking down the reflection one way from the doctor to the exact surgical studio
as opposed to back. Exactly. In fact, it's very, the only thing coming back is video from inside
the body. And they do this very minimally invasive, you know, just a couple of small holes in the
abdomen, which is very important for recovery and reduces infection and speeds up healing,
because often times the big problem is you have a big scar and that's, it, it has a lot of side
effects. So, so this is a great, a huge breakthrough in surgery. It's, it's fantastic, but the,
but it's important to keep in mind that it's been completely controlled by a human every step
of the way. So it's like a, a very, very expensive puppet. Now, one thing that's interesting,
though, is it has the capability of offering a little bit of autonomy. And I think of this as,
you know, the levels of, of autonomy in driving, self-driving car right to zero, which is where,
you know, most cars are all the way up to fully autonomous where you can, you know, you're,
there's someone in front driving and you're asleep in the back. So, CMM level five.
Yes. Exactly. The five levels. So what, what I believe in, and I think this is very true for,
for cars as well, is that we're actually making nice progress around level two or three.
That we can assist drivers under certain conditions in, for example, there are a nice,
nice weather and a freeway and it's well controlled, right? And then you can actually give up control
and it's getting very good at doing that. And that's got a lot of benefits, saving, safety,
and reducing tedium. So what's an example of in the remote surgery field? So it would be where,
for example, you are a surgeon is performing a complex procedure like a transplant and has to
perform lots and lots of sutures. So suturing is very tedious and it's just, you have to keep doing
this over and over and over again and closing up after a surgery and that, here's the thing is
that the uniformity consistency is very important in suturing, just as it is. And we've been doing
sewing machines for a really long time. Exactly. Thank you. That's exactly what I was going to say.
Great. Because sewing machines are very nice in uniform. They do a beautiful job. And the same
is true for surgery. If you have a uniform stitch, then what happens is the healing can be spread
out, the tension between the different stitches get, the sutures get spread out. The wound closes
and heals better, less scarring, all kinds of benefits. And my father-in-law, who was a surgeon,
told me that the difference between an average surgeon and a great surgeon is their suturing
skill. So if you could facilitate that, you could reduce the tedium and increase the recovery time,
healing, it would be a great thing. But it's a very hard problem. So we're trying to work on
that as well. Right now, we're looking into breedment, which is just to be able to pull out
dead or damaged tissue out of a particular body cavity. Okay. So if you had
necrology, there's a certain area where they have some dead tissue or under certain illumination,
you might see cancerous tissue, and you're just basically pulling it out with surgical tweezers.
That's very tedious also. But that's something that I do believe will be able to provide surgical
assistance for. Okay. So there's all kinds of interesting problems there. With uncertainty, I
mentioned earlier, it's even magnified with surgical robots because of the cables that drive them,
so there's a lot of uncertainty in the control, and there's uncertainty in the perception,
and there's uncertainty in the physics too. So, but that's another area that I'm excited about,
and I think we're going to make some progress in next few years. And agriculture? Agriculture. Okay.
So now we've been kind of talking about precision agriculture, and there have been some successes,
like Blue River, which was acquired by deer that was, I don't know, would you consider that
robotics? It's like the device mounted on a tractor that is using computer vision to check out
the weed that's is passing over and spraying them with fertilizer to burn them out or pesticide or
something. Right. No, I would definitely because one of the things that what you're trying to get
down to is controlling things at the plant level, per plant level. Normally in agriculture,
things are done on the field level. You just sort of have a setting of your combine that sort of
said, well, it's about right, and then it runs over the whole field roughshod. And what happens
is you waste a lot of the food. You also, it misses a lot of things. The setting is average,
so it's it's either too high or too low for for the actual crops, because they vary a lot.
And you try to make the crops as consistent as possible. That's really the name of the game
in agriculture. And you can do that genetically in other ways, right? So then your machine can be
more efficient. But it uses a lot of pesticides and other genetics and other other things to make
that to control that food. So the food is corresponding, the plants are corresponding to the machine.
Yeah. One thing that's really interesting is maybe if you don't have to do that,
but if you can have the machine correspond to the plant, adjust to the plant. So that's where you
want to be able to move through the field and detect what's going on. Notice where exactly where
the weed is showing up and then being able to zap that weed rather than just tapping the whole
field with a big dose of pesticide. So this is this is definitely robotics. It's being able to
precision get in and do that and do that fast and it cost effective. That's really exciting.
It's also another area of that is plant phenotyping where you're driving robots around in a field where
you're actually changing parameters and using it to look at the plants and see how they grow and
to be able to pick out the more successful combinations of parameters of pesticides and fertilizers,
et cetera. So there's a lot of interesting work going on. We are, Mike, are you describing like
I'm envisioning a dynamic AB testing or multivariate testing on your field that is all being
affected by some robot? Right. Yeah. There are researchers who are doing this. So they plant
the fields with all slightly varying levels of seed spacing and seed type and they're basically
running a huge large scale experiment, but they have to measure what each of the plants are doing
in terms of how they're growing. It's very tedious. So robot is great for doing that.
And so what's your lab doing in this space? So our lab is looking at, we started by looking at
precision irrigation and this is a project with colleagues at Merced and UC Davis and the idea
there was to use drones to essentially fly over a field and then identify where there was too
little water or too much and then adjust these small emitters all around the field, drip irrigation
emitters to compensate. One thing we found was that we couldn't, it was very hard to do experiments
because it's just that you have to fly out, usually you have to go into some remote area, you have
to fly, you have to measure the ground conditions and it was just taking forever. So we asked, could we,
what if we try a little miniature scale version of the farm and what if we build a farm in our lab?
So to make a long story short, we've recently built one and it's one and a half meters by three
meters. So it's fairly small. It's not in the lab, it's in the greenhouse, that's two blocks away
from the lab at Berkeley and it is, it has got over it a robot, this is a Gantry type robot, X,
Y and Z, and it's made by a company named Farmbot. Now so we are not innovating in the,
in the, in the hardware, they're doing great job with that. It's a commercial company, by the way,
you can, I recommend them very highly, it's about $3,000. They've been added for a while,
it's kind of set up like a 3D printer, but for gardening, right? Yeah, yeah, that's a good way
to put it. It's a really nice system, beautiful and they, they package it, it comes like a, it's like
an opening in Apple product, you know, everything is totally organized and it's going to set it up,
it's not trivial to put together, but it's very, very, it's very effective. So we have that,
and then what we've been doing though is adding cameras to be able to automatically monitor the
state of the garden, and what we're adding to it that, that, that Farmbot doesn't necessarily
recommend is we're doing polyculture, gardening. So we want to be able to handle the case where I
have lots of different plants growing in close proximity, and where there's a certain amount of
lack of structure in the garden. So I'm not trying to keep everything separate as, as, as monoculture,
and almost all like real farming is done, but I want to let things grow in wherever they happen
to grow. And there's benefits to that because you can reduce pesticides and increase resilience and
reduce water and all those nice, nice factors. So nutrients, so nutrients, but it takes more labor
to be able to do it. So can we, can we automate? Now I think it's an open question. It's a very hard
question because you have a very high dimensional state space again of all the plants that you're
looking at trying to make decisions about the action space. In this case, if you look at a particular
sector of the garden, you want to decide, do I just skip it, do nothing today, or do I water,
or do I prune, or plant, so it's a fairly small state space. But can we learn over time? Now the
other problem is this, this, where this differs from something like grasping is there's a very
long time constant. So if I go and try, if I grasp something, I fairly quickly to check that,
but in a garden, it takes weeks to see the effect of my actions. Right. And if you thought we were
bad at simulating grasping cylinders. Right. Right. And try simulating a bunch of different plants
growing your close proximity. It's very hard. Now there are people that have developed simulators
for individual plants. And that's actually a whole, you know, subfield. But when we talk to them
about, well, what about if I have two plants next to each other, they're like, oh, we don't do that.
They do it in graphics, of course, but graphics, of course, is a perfect example where like,
you know, Jurassic Park or something, you can make amazing things are very convincing visually,
but they're not realistic. So we've been developing a simulator that's very, very, very simple,
but allows us to test exactly this robustness aspect that you and I were talking about earlier,
which is I want to be able to have variation in everything and then see if a, if a particular
policy for planting and pruning and watering is robust to those variations. Okay. So that's
me we're running and we and just come full circle. Sam, it's a, we also considered an art project
because the, now we call it not the telegarden, but this is the alpha garden. Okay. And it's a,
it's basically it's an ongoing project. We launched it at a gallery in New York that's doing an
exhibition on called the question of intelligence. And so we, this opened about a month ago, it's a,
and it's online. If you go to alpha garden.org and you can see images and you can essentially
explore the garden. You can't plant yourself, but it's made to be able to observe what's happening.
And we're by the way on the first season and it will have multiple seasons. Every season is
about two or three months. Okay. Interesting. Interesting. One more question for you. We were talking
before we got started and this is maybe going back to health and medicine domain. It's the
middle of March. Kind of we're in the middle of the, the middle of the beginning of dealing with
COVID-19. And you mentioned some kind of thoughts on how robots might play in testing and assessments.
Yeah. So I'm working on, well, I'm not working on it, but I've been following some threads where
one of them was that because we expect there, there, I mean, it's likely that there could be
some real crisis in hospital population where we may have some spike in occurrences of the virus.
In the next, in the next four to six weeks or even sooner. And it will overwhelm the hospitals.
So we're trying to flatten that curve and avoid that. But if it happens, one of the things is that
people will want to suddenly report their, you know, how they're, they're feeling temperature
and they have problems breathing. So what happens is everybody wants to test all of a sudden.
We only have limited number of tests. So what do we do? Well, people are just self-reporting
as their, the White House just announced everyone should log in and, you know, sign up if you want
to test. Well, everybody's going to overstate their symptoms because they want to be tested.
So an alternative to that, which is really interesting is to use cameras.
Everybody has a camera built into their phone or their, their laptop. And you use the camera to
basically take a video of your face. And it, and this is where the research is, is can we
process that, that video to extract the pulse rate and the oxidation of your, oxymetry of your,
of your body? Is that enough to give us signal to detect a viral infection? Well, it's at 19.
No, but it would be helpful, especially over in temperature, by the way. It would also
might be able to be correlated with that. So you say you have 103, but I've seen video,
and that doesn't, it's not consistent, right? So one of the things, this is a very complex question.
You're right. It's not enough to just look at a video and say that person has it or doesn't.
Well, also, and I can get a pulse oxymeter on Amazon for a box, right?
And now you can, but it may be hard to get if this really gets, you know,
if it really starts gripping up. Plus, you might not have it with you, right? You're out somewhere
or something like that. So, but if you just, you always have your cell phone. So if you could point
that at yourself, you get like a nice read. And you could send that. This would be very helpful for,
for, for, for patient intake, essentially, it's the problem. So the problem is though, can we
analyze the noisy camera that is on your phone, which is not as good as a high quality video camera
that they used in past experiments? So people have been able to extract heartbeat fairly
reliably, but they're using a high quality camera and it's calibrated, et cetera. So I hope
a question is, can we do that with, with, with commodity cameras? So that's one interesting,
interesting question that's, that's coming up right now. And I just want to say, I think that
roboticists will have some, hopefully, will be able to contribute to helping address this kind
of problem by facilitating telemedicine robots that can be used, you know, to, to admit and screen
patients, et cetera. So you can imagine the, you know, the pictures that we've seen of drive through
testing facilities, right? At some point, in some future, you know, some of that could be automated
through robots. Right. So the whole idea being you should be able to walk through with those,
you buy it, you get it, maybe like the kit of a swab and a few things, but you kind of you,
you kind of walk through this, you know, and the camera helps me basically make sure it's consistent.
Last thing I know we're writing out of time, but can I just mention this idea that I have about,
about complementarity? Please. All right. So very quickly, I want to adjust this issue,
you talked about earlier Sam about the, the fears about robots taking over and becoming our overlords.
And I hear you, I mean, I think that's very, very prevalent and it's even in very major publications,
like the New York Times and New Yorker, but the reality is that we're, we're very far from that.
And what I think it's really, we're, we're important to keep in mind is that robots have great
potential, robots and AI systems have great potential to enhance us as human workers. And they can
reduce drudgery. So for example, that they can help do many things in almost every job. There's
some aspect of the job you don't like doing. And that's usually the most boring drudgery part.
So can we get robots to do that? That frees up humans to do what we do best, which is
interact with each other to grasp and manipulate complex objects, write all these things that we're
doing. And that, that's what I call complementarity. So it's an idea of looking at robots, not as some
threat that's going to take over, but it's actually something that won't work with us in, in new ways.
And I usually use the picture of, of, of, from Star Trek, of Captain Kirk and Spock,
because they exemplify complementarity. Spock is kind of like a robot.
It's all about logic and, and, and Kirk is all about, you know, intuition and, and, and humanity.
So the, but what's interesting if you look at the show is that it's very much always the two of
them working together. And that message, I think, is extremely relevant today. Oh, we've seen that
in industry, over the past few years, there's been a move in the robotics space to develop these
co-bots and to kind of optimize on the interactions between the human and the robot, both from kind of
a safety perspective, but also kind of training through imitation and some other things.
Right. And, and you're using the human is in the loop, both for the training side, but also after
that, that the, the human could be enhanced. So in a warehouse and setting, you would have, you're
not going to, people say, well, you know, we're going to wipe out all these jobs. Well, no, I really
don't think that's going to happen for, you know, any foreseeable future. We just can't get enough
humans to work on these jobs. So in e-commerce, for example, which is exploding right now, because of,
of, of, of the COVID virus. And, and we'll only continue to increase that, we, and also people
are not wanting to come into these big factories or warehouses to, to work. And so we need robots
to assist. But there's going to be still a need for humans, because it's all kinds of corner cases.
And we can facilitate human making, making intelligent decisions, but we're, we're, we're not fully
replacing the human. Do you think we iron out the social issues, um, you know, the relationship
between kind of income well-being and value creation and time on job and all these kinds of things
to create a path to this feature you're describing where, you know, the robots can get rid of the
drudgery, but people can still make a living or, you know, be alive. Right. Well, you know, I mean,
I think it's super important to be, to be very sensitive to that. And I think you're, you're right.
I mean, let's take, you know, the idea of, um, drivers. Right. Uh, it's been so much
said about the imminent, you know, self-driving car. And, and as many people who worry about that,
they make their living, um, driving, whether it's trucks or taxis or buses or, or, or, or ride, um,
services. But I, and there's a lot of fear out there, but I want to say that's, we're not going
to replace those drivers. They just, we're not going to put a, um, a truck driver into driving,
you're not going to be having a robot driving a truck through a city or a taxi for, for, for,
for 30, 50 years. It's just, it's, it's such a hard problem. So the drivers, you know, what the
driver is, you know, the driver's value is no longer staying awake for long stretches on a
pretty straight road. It's now getting this machine that got him from one end of this long road
to the next in and out of the city and, you know, offloading the goods and all that kind of stuff.
Right. Exactly. Exactly. So we can support the driver, assist the drivers. And we're already
seeing that with, uh, with something like Google Maps, right? Driving in a city is a lot less tedious
and stressful than it used to be because of Google Maps. Still stressful, but it's, you know,
it's better. So, you know, where can we start thinking about other ways that we can do that to
facilitate them? We don't have to do the boring things. And that's what I, I do think there's a,
there's a lot of positive potential. We're going to see a lot more of that. But at the same time,
let's be very conscious of who is put at disadvantage in all of these technologies.
Uh, well, Ken, thanks so much for taking the time to chat with us. Share a bit about what you're
up to. It's delightful speaking with you once again. Oh, it's a pleasure, Sam. Thank you so much for
having me on your show. All right, everyone. That's our show for today. For more information on
today's show, visit twomolai.com slash shows. As always, thanks so much for listening and catch you
next time.
