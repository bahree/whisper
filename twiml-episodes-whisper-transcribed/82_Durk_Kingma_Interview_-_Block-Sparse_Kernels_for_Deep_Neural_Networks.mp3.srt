1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:26,240
I'm your host Sam Charrington, we've got a special treat for you with this show.

4
00:00:26,240 --> 00:00:31,960
This episode is actually part of the OpenAI series of shows we ran last week, but because

5
00:00:31,960 --> 00:00:37,280
it features hot off the presses research, we weren't able to release it with those shows,

6
00:00:37,280 --> 00:00:39,720
but your weight is now over.

7
00:00:39,720 --> 00:00:45,280
This episode features an interview with Dirk Kingma, a research scientist at OpenAI.

8
00:00:45,280 --> 00:00:50,440
Although Dirk is probably best known for his pioneering work on variational auto encoders,

9
00:00:50,440 --> 00:00:55,280
he joined me this time to talk through his latest project on block sparse kernels, which

10
00:00:55,280 --> 00:00:58,480
OpenAI just published this week.

11
00:00:58,480 --> 00:01:03,640
Block sparsity is a property of certain neural network representations, and OpenAI's work

12
00:01:03,640 --> 00:01:08,600
on developing block sparse kernels helps make it more computationally efficient to take

13
00:01:08,600 --> 00:01:10,560
advantage of it.

14
00:01:10,560 --> 00:01:14,760
In addition to covering block sparse kernels themselves and the background required to understand

15
00:01:14,760 --> 00:01:18,560
them, we also discuss why they're important and walk through some examples of how they

16
00:01:18,560 --> 00:01:19,560
can be used.

17
00:01:19,560 --> 00:01:25,040
I'm happy to present another fine Nerd Alert show to close out this OpenAI series and

18
00:01:25,040 --> 00:01:27,560
I know you'll enjoy it.

19
00:01:27,560 --> 00:01:32,000
Support for our OpenAI series is brought to you by our friends at Nvidia, a company which

20
00:01:32,000 --> 00:01:35,840
is also a supporter of OpenAI itself.

21
00:01:35,840 --> 00:01:40,040
If you're listening to this podcast, you already know about Nvidia and all the great things

22
00:01:40,040 --> 00:01:43,680
they're doing to support advancements in AI research and practice.

23
00:01:43,680 --> 00:01:49,240
And as you'll hear in this show, Nvidia's DGX1 deep learning supercomputer was instrumental

24
00:01:49,240 --> 00:01:53,520
to Dirk and his team's work on the block sparse kernels project.

25
00:01:53,520 --> 00:01:58,320
If you happen to be at Nips this week, be sure to check out Nvidia's booth at the Expo,

26
00:01:58,320 --> 00:02:02,480
as well as the four accepted papers being presented by their team.

27
00:02:02,480 --> 00:02:09,040
To learn more about the Nvidia presence at Nips, head on over to twomlai.com slash Nvidia.

28
00:02:09,040 --> 00:02:19,240
And now on to the show.

29
00:02:19,240 --> 00:02:25,640
All right everyone, I am on the line with Dirk Kingma. Dirk is a research scientist with

30
00:02:25,640 --> 00:02:26,640
OpenAI.

31
00:02:26,640 --> 00:02:29,440
Dirk, welcome to this week in machine learning and AI.

32
00:02:29,440 --> 00:02:30,440
Thank you very much.

33
00:02:30,440 --> 00:02:31,440
It's my pleasure.

34
00:02:31,440 --> 00:02:32,440
It's great to have you on the show.

35
00:02:32,440 --> 00:02:36,600
I am looking forward to learning a bit about your background and what you're working

36
00:02:36,600 --> 00:02:37,600
on.

37
00:02:37,600 --> 00:02:41,280
Why don't we start with the first of those and have you tell us a little bit about how

38
00:02:41,280 --> 00:02:45,200
you got involved in machine learning and AI research?

39
00:02:45,200 --> 00:02:46,200
Sure.

40
00:02:46,200 --> 00:02:53,520
So, I started, I think, my first research position when I was doing my master's degree.

41
00:02:53,520 --> 00:02:57,600
I emailed Jan Lecun out of the blue with a couple of research ideas.

42
00:02:57,600 --> 00:02:59,400
I had no clue where he was.

43
00:02:59,400 --> 00:03:00,400
He responded.

44
00:03:00,400 --> 00:03:04,840
And, you know, it was welcoming me into his lab for six months.

45
00:03:04,840 --> 00:03:08,240
So I looked up where he was and turned out to be New York.

46
00:03:08,240 --> 00:03:10,240
So I was very happy with that, of course.

47
00:03:10,240 --> 00:03:11,240
I'm happy.

48
00:03:11,240 --> 00:03:12,240
Like 2009.

49
00:03:12,240 --> 00:03:16,840
Was that for, like, a postdoc or not a postdoc, but like a research position or?

50
00:03:16,840 --> 00:03:17,840
Yeah.

51
00:03:17,840 --> 00:03:21,040
So I was very interested in AI.

52
00:03:21,040 --> 00:03:27,160
I did a master's program in Utrecht in the Netherlands and a friend of mine was studying

53
00:03:27,160 --> 00:03:30,800
in Cornell and he got me interested in a couple of topics.

54
00:03:30,800 --> 00:03:37,480
So I spent about a year, you know, reading about the various ideas in the field and quickly

55
00:03:37,480 --> 00:03:42,200
discovered, you know, the Jan Lecun had a lot of interesting research going on.

56
00:03:42,200 --> 00:03:46,720
So I applied for a position in his lab and was accepted happily.

57
00:03:46,720 --> 00:03:53,160
So this was, you know, about three years before the learning resolution started.

58
00:03:53,160 --> 00:03:54,840
And I had a great time there.

59
00:03:54,840 --> 00:03:58,560
So that was, yeah, that was, you know, 2009.

60
00:03:58,560 --> 00:04:00,920
I published a paper with him.

61
00:04:00,920 --> 00:04:05,480
I decided to do a start-up afterwards with a friend of mine in the Netherlands.

62
00:04:05,480 --> 00:04:10,680
We did that for a couple of years and then I decided to go back to research and I spent

63
00:04:10,680 --> 00:04:19,720
again about half a year in Jan Lecun's lab and applied for a PhD position with Maxwelling.

64
00:04:19,720 --> 00:04:25,360
So he's a professor in, we just started and answered them and before he was a professor

65
00:04:25,360 --> 00:04:31,240
at Irvine and my girlfriend at the time was still studying in the Netherlands.

66
00:04:31,240 --> 00:04:34,600
So I, you know, preferred to have a position there.

67
00:04:34,600 --> 00:04:40,280
So I was very lucky to get a position with Maxwelling in 2013.

68
00:04:40,280 --> 00:04:42,360
And I was his first PhD student there.

69
00:04:42,360 --> 00:04:48,440
So basically, when you start as a professor, you get like a free student as your first,

70
00:04:48,440 --> 00:04:49,440
you know, student.

71
00:04:49,440 --> 00:04:51,840
So I was his free student.

72
00:04:51,840 --> 00:04:56,320
And because of that, I was also lucky to not be bound to a particular research direction

73
00:04:56,320 --> 00:04:57,320
or topic.

74
00:04:57,320 --> 00:05:02,440
So I had, you know, relative freedom to pursue my interests from the start.

75
00:05:02,440 --> 00:05:08,320
So what I was interested in was combining probabilistic models with deep learning.

76
00:05:08,320 --> 00:05:13,920
So I thought this was a field that was sort of underappreciated, you know, the combination.

77
00:05:13,920 --> 00:05:20,440
And the first paper I published with Maxwelling was on the variational autoencoder, which

78
00:05:20,440 --> 00:05:30,280
is a paper that sort of introduces a way to combine Bayesian inference in a natural way

79
00:05:30,280 --> 00:05:32,040
with deep learning.

80
00:05:32,040 --> 00:05:39,240
And this method allows you to train generative models in a principled way and a scalable

81
00:05:39,240 --> 00:05:40,240
way.

82
00:05:40,240 --> 00:05:42,760
So it skills you have to large data sets and to large models.

83
00:05:42,760 --> 00:05:45,800
So that's for the key advantages, okay?

84
00:05:45,800 --> 00:05:52,040
And we've talked about variational autoencoders on the podcast a few times, but I'm looking

85
00:05:52,040 --> 00:05:56,200
forward to digging into that topic with you.

86
00:05:56,200 --> 00:05:58,880
What else did you work on there?

87
00:05:58,880 --> 00:06:05,200
Right, so, you know, the variational autoencoder that sparked a wave of various application

88
00:06:05,200 --> 00:06:09,480
papers and extensions also in our lab.

89
00:06:09,480 --> 00:06:16,680
So we worked on trying to get uncertainty estimation work well in deep learning models.

90
00:06:16,680 --> 00:06:22,720
So as you know, most deep learning models all you do is basically learn a single value

91
00:06:22,720 --> 00:06:28,000
of the parameters that need to, you know, good predictions on your test set and your

92
00:06:28,000 --> 00:06:29,000
training set.

93
00:06:29,000 --> 00:06:30,000
Right.

94
00:06:30,000 --> 00:06:34,720
But it doesn't necessarily give you a good sort of estimate of how certain you should be

95
00:06:34,720 --> 00:06:38,600
about your predictions given your limited data.

96
00:06:38,600 --> 00:06:44,640
And the combination with variational inference, a potential way to achieve that.

97
00:06:44,640 --> 00:06:50,680
So deep behavioral variational autoencoders introduced a trick called reprimerization trick,

98
00:06:50,680 --> 00:06:55,760
which was also applicable to estimating a posterior distribution over the parameters,

99
00:06:55,760 --> 00:07:01,880
which is basically gives you a quantitative estimate of the uncertainty over your parameters

100
00:07:01,880 --> 00:07:03,720
given your training data.

101
00:07:03,720 --> 00:07:07,080
So we did some like a paper on that.

102
00:07:07,080 --> 00:07:10,320
And yeah, so that was one extension we did.

103
00:07:10,320 --> 00:07:18,280
And also I, because of the variational autoencoder paper, there was a similar research direction

104
00:07:18,280 --> 00:07:24,400
within the lab of deep mind that happened about at the same time.

105
00:07:24,400 --> 00:07:32,400
I happened to publish my paper a bit earlier, but so they developed independently a very

106
00:07:32,400 --> 00:07:33,400
similar trick.

107
00:07:33,400 --> 00:07:38,320
And because of this coincidence, we started also collaborating on this topic.

108
00:07:38,320 --> 00:07:47,200
So I went to deep mind in 2014 and we collaborated there on applying this method to the problem

109
00:07:47,200 --> 00:07:53,800
of semi-supervised learning, which means that if you want to train a classifier from

110
00:07:53,800 --> 00:08:00,240
training data, but not all your training images are labeled, then you would want to make

111
00:08:00,240 --> 00:08:08,080
use of the unlabeled data to get better predictions than what you would get if you would solely

112
00:08:08,080 --> 00:08:10,720
train on the labeled images, right?

113
00:08:10,720 --> 00:08:11,720
Okay.

114
00:08:11,720 --> 00:08:16,120
Yeah, so we basically proposed a method using variational autoencoder.

115
00:08:16,120 --> 00:08:21,440
And the results that we got at the time were, you know, state of the art, but of course

116
00:08:21,440 --> 00:08:28,440
we're, you know, quickly eclipsed by, you know, a wave of other papers and, you know,

117
00:08:28,440 --> 00:08:30,320
that's how it happens, right?

118
00:08:30,320 --> 00:08:32,160
Yes, that's how it happens.

119
00:08:32,160 --> 00:08:36,880
But I mean, the variational autoencoder effort that come up in so many different contexts,

120
00:08:36,880 --> 00:08:43,840
it seems like, I mean, you mentioned that even in your prior lab, you know, the initial

121
00:08:43,840 --> 00:08:48,800
paper was followed up by a bunch of applications, papers that you worked on, but a lot of folks

122
00:08:48,800 --> 00:08:54,000
have worked on papers using that papers and methods, I should say.

123
00:08:54,000 --> 00:09:00,600
I think, you know, one of the first places I saw it was in the context of, like, generative

124
00:09:00,600 --> 00:09:02,360
models for texts.

125
00:09:02,360 --> 00:09:04,000
Is that a popular, am I collecting that?

126
00:09:04,000 --> 00:09:08,320
Is that a popular place to use variational autoencoders, or maybe it was, there was another

127
00:09:08,320 --> 00:09:16,600
one where someone took a, I think they took a bunch of, like, movie videos or movie clips

128
00:09:16,600 --> 00:09:20,880
or something like that and ran them through a variational autoencoder and tried to, you

129
00:09:20,880 --> 00:09:25,000
know, generate a movie clip, which was kind of interesting as well.

130
00:09:25,000 --> 00:09:31,840
Before we dive into variational autoencoders, we haven't gotten yet to open AI, have we?

131
00:09:31,840 --> 00:09:32,840
Right.

132
00:09:32,840 --> 00:09:42,760
So, in the end of, or I think somewhere in mid 2015, I was approached by Rick Rubman, who

133
00:09:42,760 --> 00:09:48,520
was assembling sort of an initial team for open AI.

134
00:09:48,520 --> 00:09:54,800
And to me, it was very intriguing because, yeah, it matched very well with sort of my own

135
00:09:54,800 --> 00:10:03,080
philosophy that I think it's good to, you know, to publish and to, you know, to write open

136
00:10:03,080 --> 00:10:09,760
source code for your experiments, but also to, you know, put emphasis on, you know, maximizing

137
00:10:09,760 --> 00:10:13,080
the probability of a positive future with AI.

138
00:10:13,080 --> 00:10:14,080
Right.

139
00:10:14,080 --> 00:10:19,080
So, yeah, there is, of course, currently a lot of concentration of talent in the field

140
00:10:19,080 --> 00:10:21,640
at a small number of places.

141
00:10:21,640 --> 00:10:29,200
And I thought it would be good to have more of, you know, to perhaps, you know, safeguard

142
00:10:29,200 --> 00:10:32,800
the, you know, the openness of the field and the collaboration.

143
00:10:32,800 --> 00:10:33,800
Right.

144
00:10:33,800 --> 00:10:39,720
So, yeah, there is, because of the inflow of talent into, you know, commercial labs,

145
00:10:39,720 --> 00:10:46,720
there is a risk that eventually, you know, a lot of these labs will, you close up and

146
00:10:46,720 --> 00:10:52,040
that, you know, the benefits of the field might not be as evenly distributed as we would

147
00:10:52,040 --> 00:10:53,040
like.

148
00:10:53,040 --> 00:10:59,600
So, that is one of the goals of open AI, you know, to make sure that the benefits are distributed

149
00:10:59,600 --> 00:11:01,680
eventually in a fair way.

150
00:11:01,680 --> 00:11:02,680
Yeah.

151
00:11:02,680 --> 00:11:09,480
Also, it was, of course, located in California, which is not a lot of bad place to live.

152
00:11:09,480 --> 00:11:10,480
Not at all.

153
00:11:10,480 --> 00:11:11,480
Not at all.

154
00:11:11,480 --> 00:11:13,920
And you've been at Open AI for how long?

155
00:11:13,920 --> 00:11:19,040
Since the start, basically, so the first few months I was still working from Amsterdam.

156
00:11:19,040 --> 00:11:20,040
Okay.

157
00:11:20,040 --> 00:11:23,320
And then I moved here in 2016.

158
00:11:23,320 --> 00:11:24,320
Okay.

159
00:11:24,320 --> 00:11:25,320
Yep.

160
00:11:25,320 --> 00:11:26,320
Awesome.

161
00:11:26,320 --> 00:11:27,320
Awesome.

162
00:11:27,320 --> 00:11:30,960
Well, one of the things that I wanted to ask you, given that we're recording this the

163
00:11:30,960 --> 00:11:38,360
week before NEPs, and I am going to NEPs, actually for the first time, is I'm assuming

164
00:11:38,360 --> 00:11:42,480
NEPs is like your home conference, so you've been there many times.

165
00:11:42,480 --> 00:11:47,880
I wanted to, you know, get a sense from you both, you know, if there's anything in particular,

166
00:11:47,880 --> 00:11:53,760
you're looking forward to about, you know, this particular conference or, you know, any

167
00:11:53,760 --> 00:11:58,840
tips on approaching NEPs as it's become, you know, quite a large conference.

168
00:11:58,840 --> 00:11:59,840
Yeah.

169
00:11:59,840 --> 00:12:03,440
So, this has indeed become an enormous conference.

170
00:12:03,440 --> 00:12:08,360
As you probably know, the field is still more or less doubling every year.

171
00:12:08,360 --> 00:12:13,440
I think the, the attendance of NEPs, you know, follows a similar trend.

172
00:12:13,440 --> 00:12:17,640
So, you know, personally, for me, it's also a big, you know, social event.

173
00:12:17,640 --> 00:12:21,040
Of course, it's probably, you know, the biggest conference in our field now.

174
00:12:21,040 --> 00:12:25,360
So, all your friends and other colleagues come all over the world, you know, assemble

175
00:12:25,360 --> 00:12:32,920
at a single place, and this is a great, you know, moment to discuss collaborations

176
00:12:32,920 --> 00:12:37,280
or to discuss the newest results, you know, have a beer together, etc.

177
00:12:37,280 --> 00:12:43,600
In terms of preparation, I would recommend people to read just, you know, through the agenda

178
00:12:43,600 --> 00:12:48,480
and make sort of a list of, you know, papers they think are most interesting before they

179
00:12:48,480 --> 00:12:52,720
go to the conference, because, you know, like whenever you're there, you're, you know,

180
00:12:52,720 --> 00:12:53,720
it is overwhelming.

181
00:12:53,720 --> 00:12:59,240
So, there, like you will have, you know, no time to, you know, to orient.

182
00:12:59,240 --> 00:13:00,240
Yeah.

183
00:13:00,240 --> 00:13:03,040
And the agendas are overwhelming, to be honest.

184
00:13:03,040 --> 00:13:04,040
Yeah.

185
00:13:04,040 --> 00:13:05,840
There's a ton of stuff going on there.

186
00:13:05,840 --> 00:13:06,840
Yeah.

187
00:13:06,840 --> 00:13:07,840
Yeah, absolutely.

188
00:13:07,840 --> 00:13:08,840
Yeah.

189
00:13:08,840 --> 00:13:12,320
And of course, one, I think the interesting thing is that a lot of the work that you

190
00:13:12,320 --> 00:13:15,040
will see is already published at archive, right?

191
00:13:15,040 --> 00:13:16,040
Right.

192
00:13:16,040 --> 00:13:21,360
So, as soon as you see a paper, you know, you can actually already read it, you know,

193
00:13:21,360 --> 00:13:25,840
long before the paper has been published on the NEPs website.

194
00:13:25,840 --> 00:13:31,600
And often, you know, you follow up work has already, you know, followed the archive.

195
00:13:31,600 --> 00:13:37,080
So, some papers are already outdated, you know, at the moment you see them at NEPs, which

196
00:13:37,080 --> 00:13:38,320
is a bit tragic.

197
00:13:38,320 --> 00:13:39,320
That's funny.

198
00:13:39,320 --> 00:13:42,760
And you're publishing some work next week as well?

199
00:13:42,760 --> 00:13:43,760
That's right.

200
00:13:43,760 --> 00:13:44,760
Yeah.

201
00:13:44,760 --> 00:13:51,200
So, with two of my colleagues, we worked on, you know, publishing a set of your kernels

202
00:13:51,200 --> 00:13:59,040
to GPU kernels, which is a software for the GPU, which allow you to train and build

203
00:13:59,040 --> 00:14:02,400
models with block sparse layers.

204
00:14:02,400 --> 00:14:05,600
So, block sparse GPU kernels.

205
00:14:05,600 --> 00:14:09,560
Maybe you talked to me before we dig into the block sparse element of this.

206
00:14:09,560 --> 00:14:17,480
Tell me a little bit more about GPU kernels and where they fit in in the kind of the software

207
00:14:17,480 --> 00:14:18,480
stack.

208
00:14:18,480 --> 00:14:24,000
I think about, you know, at the lowest level, I think about, you know, frameworks like

209
00:14:24,000 --> 00:14:28,400
TensorFlow and things like that at a much higher level, where the kernels fit in.

210
00:14:28,400 --> 00:14:29,400
Right.

211
00:14:29,400 --> 00:14:33,440
So, GPU kernels are, you can view them as sort of middleware.

212
00:14:33,440 --> 00:14:34,440
Okay.

213
00:14:34,440 --> 00:14:43,680
So, they are libraries that require you to, you know, program on the fairly low level.

214
00:14:43,680 --> 00:14:51,120
And they allow you to sort of maximize, you know, usage of, you know, GPUs, which are,

215
00:14:51,120 --> 00:14:55,400
of course, you know, it is hardware that runs very parallel.

216
00:14:55,400 --> 00:14:59,160
So, it requires you to specialize software to make full use of that.

217
00:14:59,160 --> 00:15:07,320
Basically, they are a set of middleware that is, you know, typically already implemented

218
00:15:07,320 --> 00:15:08,840
in your framework.

219
00:15:08,840 --> 00:15:09,840
Okay.

220
00:15:09,840 --> 00:15:14,200
So, you basically call in various functions that make use of, you know, office kernels

221
00:15:14,200 --> 00:15:15,400
when you build your models.

222
00:15:15,400 --> 00:15:16,400
Right.

223
00:15:16,400 --> 00:15:21,760
So, for example, when you, you know, when you implement the convolutional network in

224
00:15:21,760 --> 00:15:29,280
TensorFlow, then you typically use, you know, various pre-built layers that themselves

225
00:15:29,280 --> 00:15:34,120
call, you know, GPU kernels to efficiently evaluate, you know, the forward path and the

226
00:15:34,120 --> 00:15:37,800
backward path and the gradient computation on the GPUs.

227
00:15:37,800 --> 00:15:42,360
Are you developing the kernels in CUDA or at a lower level than that?

228
00:15:42,360 --> 00:15:43,360
Right.

229
00:15:43,360 --> 00:15:45,760
So, I did not develop the kernels myself.

230
00:15:45,760 --> 00:15:46,760
Okay.

231
00:15:46,760 --> 00:15:52,200
So, this was all done by Scott Gray, who is a GPU kernel expert.

232
00:15:52,200 --> 00:15:53,200
Okay.

233
00:15:53,200 --> 00:15:54,200
Yeah.

234
00:15:54,200 --> 00:15:56,200
So, he basically wrote all the kernels.

235
00:15:56,200 --> 00:15:57,800
So, like, all the credits go to him.

236
00:15:57,800 --> 00:15:58,800
Right.

237
00:15:58,800 --> 00:16:04,880
I guess I was more asking like, well, I guess I should have asked, does one develop kernels

238
00:16:04,880 --> 00:16:10,880
in CUDA or is this at an even lower level than something like a CUDA?

239
00:16:10,880 --> 00:16:13,480
I'm just trying to get a sense for where they fit in.

240
00:16:13,480 --> 00:16:19,480
So, these kernels were developed both at the C level and at the assembly level.

241
00:16:19,480 --> 00:16:20,480
Okay.

242
00:16:20,480 --> 00:16:21,880
So, fairly low level.

243
00:16:21,880 --> 00:16:22,880
Yeah.

244
00:16:22,880 --> 00:16:23,880
Yeah.

245
00:16:23,880 --> 00:16:27,760
And then you mentioned, brought up convolutional neural nets as an example.

246
00:16:27,760 --> 00:16:32,120
Can you give some examples of the types of kernels that, you know, a framework might implement

247
00:16:32,120 --> 00:16:38,840
is this, like, for things like tensor operations, or they add a higher level or lower level

248
00:16:38,840 --> 00:16:39,840
than that?

249
00:16:39,840 --> 00:16:40,840
Okay.

250
00:16:40,840 --> 00:16:44,240
So, your question is, like, how you can use these kernels in your models?

251
00:16:44,240 --> 00:16:45,240
No.

252
00:16:45,240 --> 00:16:51,320
So, I guess I'm still talking about GPU kernels generally, and, you know, what operations

253
00:16:51,320 --> 00:16:57,000
they tend to represent in, for example, the case of a CNN in a framework that's, you

254
00:16:57,000 --> 00:16:59,840
know, it's calling down to a bunch of lower level kernels.

255
00:16:59,840 --> 00:17:03,120
I'm just looking for examples of what those kernels might be.

256
00:17:03,120 --> 00:17:04,120
Right.

257
00:17:04,120 --> 00:17:11,120
So, in a typical CNN, you know, you have a GPU kernel for, you know, the forward convolutional

258
00:17:11,120 --> 00:17:12,120
computation, right?

259
00:17:12,120 --> 00:17:17,600
So, you have a convolutional layer in a convolutional network, which is basically a linear layer

260
00:17:17,600 --> 00:17:20,640
that applies, like, a convolution or the inputs.

261
00:17:20,640 --> 00:17:21,640
Mm-hmm.

262
00:17:21,640 --> 00:17:23,840
And the result is the output of the layer.

263
00:17:23,840 --> 00:17:26,880
So, that's where you use, you know, GPU kernels.

264
00:17:26,880 --> 00:17:32,160
And then there are, for example, GPU kernels for elements-wise operations.

265
00:17:32,160 --> 00:17:33,160
Okay.

266
00:17:33,160 --> 00:17:34,160
Yeah.

267
00:17:34,160 --> 00:17:35,160
Got it.

268
00:17:35,160 --> 00:17:42,040
And so, the kernels that you are publishing are for block sparse operations.

269
00:17:42,040 --> 00:17:43,040
What are those?

270
00:17:43,040 --> 00:17:45,080
So, block sparse operations.

271
00:17:45,080 --> 00:17:55,360
So, what we release is it is a generalization of the usual matrix multiplication and convolutional

272
00:17:55,360 --> 00:17:57,760
kernels you typically use.

273
00:17:57,760 --> 00:17:58,760
Okay.

274
00:17:58,760 --> 00:18:06,040
So, typically, what you have is when you use a convolutional layer in your model is a

275
00:18:06,040 --> 00:18:14,080
weight matrix or, like, a weight tensor, which is dense, meaning that all the entries in

276
00:18:14,080 --> 00:18:18,840
the weight, in tensor, a weight matrix, like, have a non-zero value.

277
00:18:18,840 --> 00:18:19,840
Right.

278
00:18:19,840 --> 00:18:27,680
So, there is that in, you know, as you increase the width of the network, the number of weights

279
00:18:27,680 --> 00:18:28,680
increases quadratically.

280
00:18:28,680 --> 00:18:29,680
By width.

281
00:18:29,680 --> 00:18:32,680
You mean the number of features?

282
00:18:32,680 --> 00:18:34,200
The number of features.

283
00:18:34,200 --> 00:18:35,200
Right.

284
00:18:35,200 --> 00:18:36,200
Right.

285
00:18:36,200 --> 00:18:39,840
So, there is a quadratic relationship between the number of features and the number of weights.

286
00:18:39,840 --> 00:18:40,840
Mm-hmm.

287
00:18:40,840 --> 00:18:46,520
If you use a dense, your kernel, like, a dense weight matrix, an obvious solution to this

288
00:18:46,520 --> 00:18:53,680
is to, instead of using a dense weight matrix, you use a weight matrix where not necessarily

289
00:18:53,680 --> 00:19:01,520
all blocks, you know, or all your weights in the tensor, have a non-zero value.

290
00:19:01,520 --> 00:19:09,360
So, what the block sparse kernels allow you to do is to define which of, so you basically

291
00:19:09,360 --> 00:19:17,400
define your weight matrix or your weight tensor into blocks of either 8 by 8 or 16 by 16

292
00:19:17,400 --> 00:19:19,560
or 32 by 32.

293
00:19:19,560 --> 00:19:27,040
And then you can say beforehand for each block, whether that block has the value of zero

294
00:19:27,040 --> 00:19:32,280
for every entry in the block, or actually has an actual, you know, learns, you know,

295
00:19:32,280 --> 00:19:33,280
weights value.

296
00:19:33,280 --> 00:19:34,280
Right.

297
00:19:34,280 --> 00:19:39,880
Yeah. So, basically if the weight values are zero, which is equivalent to having a block

298
00:19:39,880 --> 00:19:43,880
is, you know, all zero, then you don't, you don't have to compute, you know, that block

299
00:19:43,880 --> 00:19:50,040
because the output of the layer of the operation is not a function of that block.

300
00:19:50,040 --> 00:19:52,360
So, you can, you know, skip that computation.

301
00:19:52,360 --> 00:19:57,680
And so, you would use this in a scenario where, or maybe I should just ask you to give

302
00:19:57,680 --> 00:19:58,680
us some examples.

303
00:19:58,680 --> 00:20:05,600
Imagine that, you know, what you're essentially saying by using a block sparse matrix is

304
00:20:05,600 --> 00:20:10,080
that you don't care about the relationship between some features and some layers.

305
00:20:10,080 --> 00:20:11,120
Is that the idea?

306
00:20:11,120 --> 00:20:19,440
It gives you more flexibility in choosing how your neurons are connected between layers.

307
00:20:19,440 --> 00:20:20,440
Mm-hmm.

308
00:20:20,440 --> 00:20:26,040
Basically in all the existing architectures, you've given a particular layer, all the input

309
00:20:26,040 --> 00:20:28,240
and output features are connected.

310
00:20:28,240 --> 00:20:29,240
Right.

311
00:20:29,240 --> 00:20:34,960
But this is not necessarily, you know, the optimal way to use your parameters, right?

312
00:20:34,960 --> 00:20:42,880
So, given a particular budget of parameters, you might want to use, for example, a wider

313
00:20:42,880 --> 00:20:49,800
network that where not all features are interconnected, but only like half of them are interconnected.

314
00:20:49,800 --> 00:20:50,800
Mm-hmm.

315
00:20:50,800 --> 00:20:52,560
Or, you know, 25% of them.

316
00:20:52,560 --> 00:20:57,560
Or you might want to say that, okay, I'm going to just use, you know, connecting a half

317
00:20:57,560 --> 00:21:01,480
of the neurons with each other and I'm going to increase, you know, the depth by a factor

318
00:21:01,480 --> 00:21:02,480
two.

319
00:21:02,480 --> 00:21:09,640
So basically by introducing sparsity, given a certain parameter budget, you can have either

320
00:21:09,640 --> 00:21:12,480
much wider or much deeper networks.

321
00:21:12,480 --> 00:21:16,120
So this is one particular application that we investigated.

322
00:21:16,120 --> 00:21:20,560
But there are, you know, much more applications possible that we haven't even touched upon.

323
00:21:20,560 --> 00:21:26,560
So there's a whole, like a whole series of papers that are coming out now that show that

324
00:21:26,560 --> 00:21:33,560
after training a neural network, it is typically possible to remove, you know, 99% of the weights

325
00:21:33,560 --> 00:21:36,480
without significantly affecting performance.

326
00:21:36,480 --> 00:21:37,480
Right.

327
00:21:37,480 --> 00:21:42,960
Which is, you know, it is a curious finding, like turns out that most of the weights are

328
00:21:42,960 --> 00:21:46,040
useless, you know, for prediction.

329
00:21:46,040 --> 00:21:51,360
These kernels, they will allow you to actually make use of this redundancy.

330
00:21:51,360 --> 00:21:56,680
So after training, you could potentially just move all the ways, you know, they're useless.

331
00:21:56,680 --> 00:22:00,520
That's how you have network that is much faster to evaluate.

332
00:22:00,520 --> 00:22:05,360
So it would give you a speed up and, you know, future work would also involve, you know,

333
00:22:05,360 --> 00:22:07,640
learning the connectivity, right?

334
00:22:07,640 --> 00:22:12,160
So this is something we have not done yet, but we hope that we or others will do a future

335
00:22:12,160 --> 00:22:13,160
work.

336
00:22:13,160 --> 00:22:19,520
It's basically a bit similar to the brain, you know, the neurons should be connected,

337
00:22:19,520 --> 00:22:21,760
which, you know, should not be connected.

338
00:22:21,760 --> 00:22:25,080
So basically this is a forum of learning the structure of the model, you know, beyond

339
00:22:25,080 --> 00:22:30,600
just the value of the weights, you also, you know, you also learn where, you know, where

340
00:22:30,600 --> 00:22:31,600
you have weights.

341
00:22:31,600 --> 00:22:32,600
Right.

342
00:22:32,600 --> 00:22:33,600
Right.

343
00:22:33,600 --> 00:22:38,600
And yeah, we also have some preliminary work that we did with Ristus, so we just, an

344
00:22:38,600 --> 00:22:40,000
internet was here this summer.

345
00:22:40,000 --> 00:22:41,000
Okay.

346
00:22:41,000 --> 00:22:48,640
So right now, you are saying before training, you're specifying like where the sparsity

347
00:22:48,640 --> 00:22:49,640
blocks are.

348
00:22:49,640 --> 00:22:50,640
Yeah.

349
00:22:50,640 --> 00:22:51,640
Is that correct?

350
00:22:51,640 --> 00:22:52,640
And then.

351
00:22:52,640 --> 00:22:53,640
Yeah.

352
00:22:53,640 --> 00:22:58,080
And then you're suggesting that there's, you know, work, you're hoping to see work where

353
00:22:58,080 --> 00:23:03,120
that's learned as part of the training process as opposed to being specified up front.

354
00:23:03,120 --> 00:23:04,120
Yeah.

355
00:23:04,120 --> 00:23:10,520
So this is something where we have some preliminary work, but I think this is a very exciting

356
00:23:10,520 --> 00:23:15,120
direction that we or others can pursue in, you know, future work.

357
00:23:15,120 --> 00:23:16,120
Yeah.

358
00:23:16,120 --> 00:23:24,680
So determine where the, where the spars blocks are currently or conversely where the connections

359
00:23:24,680 --> 00:23:25,680
are.

360
00:23:25,680 --> 00:23:31,280
Is it, you know, is it essentially another kind of hyperparameter or a meta hyperparameter

361
00:23:31,280 --> 00:23:38,000
or something that you are training and evaluating, you know, with regard to some optimization

362
00:23:38,000 --> 00:23:42,120
that you're trying to make or some, you know, or the performance of your, your network

363
00:23:42,120 --> 00:23:49,320
as a whole or is there, you know, some set of heuristics or intuition that tells you,

364
00:23:49,320 --> 00:23:52,120
you know, where you should have the connections.

365
00:23:52,120 --> 00:23:53,120
Right.

366
00:23:53,120 --> 00:23:59,680
So in choosing the sparsity pattern in our published work, we took inspiration from the field of

367
00:23:59,680 --> 00:24:02,120
the small world networks.

368
00:24:02,120 --> 00:24:03,120
Small world networks.

369
00:24:03,120 --> 00:24:04,120
What are those?

370
00:24:04,120 --> 00:24:05,120
Yeah.

371
00:24:05,120 --> 00:24:11,560
So small world networks, they are a type of graph basically that you find various systems,

372
00:24:11,560 --> 00:24:12,560
including the brain.

373
00:24:12,560 --> 00:24:13,560
Okay.

374
00:24:13,560 --> 00:24:14,560
Okay.

375
00:24:14,560 --> 00:24:16,600
So you also find it in social networks, right?

376
00:24:16,600 --> 00:24:23,560
So you or any person on earth is connected to any other person on earth in a small number

377
00:24:23,560 --> 00:24:24,560
of steps.

378
00:24:24,560 --> 00:24:25,560
Right.

379
00:24:25,560 --> 00:24:26,560
Six degrees of Kevin Bacon.

380
00:24:26,560 --> 00:24:27,560
Right.

381
00:24:27,560 --> 00:24:28,560
Right.

382
00:24:28,560 --> 00:24:29,560
Right.

383
00:24:29,560 --> 00:24:35,520
So this means that even though, you know, the number of connections you have and the number

384
00:24:35,520 --> 00:24:42,520
of connections that any person, you know, in the world has is relatively low, you are still

385
00:24:42,520 --> 00:24:45,560
connected to any other person in the small number of steps.

386
00:24:45,560 --> 00:24:51,560
And you find the same property in the human brain at a functional level at least.

387
00:24:51,560 --> 00:24:57,960
So, you know, the functional modules in the brain are often mostly locally connected,

388
00:24:57,960 --> 00:25:03,240
but there are some more or less random long range connections.

389
00:25:03,240 --> 00:25:07,720
And due to these more or less, you know, long range, long term connections, you know, the

390
00:25:07,720 --> 00:25:13,800
whole brain is connected in the small number of steps, which means that information is

391
00:25:13,800 --> 00:25:20,480
spread throughout the brain relatively quickly, even though you have a huge number of neurons.

392
00:25:20,480 --> 00:25:25,440
So this is something we took inspiration from when choosing, says, you know, a disparity

393
00:25:25,440 --> 00:25:27,440
mask in our networks.

394
00:25:27,440 --> 00:25:33,440
There are very simple algorithms for generating graphs that have this property.

395
00:25:33,440 --> 00:25:36,080
So these are called small world graphs.

396
00:25:36,080 --> 00:25:41,000
They basically, they just, you know, require you to have a certain percentage of your connections

397
00:25:41,000 --> 00:25:43,040
in your random.

398
00:25:43,040 --> 00:25:48,600
And so this is indeed a hyper parameter, but we do have the guarantee that information

399
00:25:48,600 --> 00:25:50,800
mixes in the relatively small number of steps.

400
00:25:50,800 --> 00:25:56,360
So even though you introduce varsity in your network, so for example, we train, we

401
00:25:56,360 --> 00:26:04,280
have trained a big LSTM that is, you know, 98% sparse, which means that, you know, 98%

402
00:26:04,280 --> 00:26:08,320
of the connections between type cents are not there, right?

403
00:26:08,320 --> 00:26:13,600
So any neuron is only connected to about, you know, 2% of the neurons in the previous

404
00:26:13,600 --> 00:26:15,400
time step.

405
00:26:15,400 --> 00:26:20,520
So even though it's only, you know, it's only 2% in a small number of steps every neuron

406
00:26:20,520 --> 00:26:26,240
is connected with every other neuron, which means that we basically do is we introduce

407
00:26:26,240 --> 00:26:32,040
a number of internal steps in the network between external time steps that allow information

408
00:26:32,040 --> 00:26:36,960
to spread through the whole network before you process a new input.

409
00:26:36,960 --> 00:26:38,760
Can you say that last part again?

410
00:26:38,760 --> 00:26:39,760
Right.

411
00:26:39,760 --> 00:26:43,160
So because of the small world property, right?

412
00:26:43,160 --> 00:26:50,800
We basically only need to introduce a small number of intermediate steps between inputs

413
00:26:50,800 --> 00:26:54,920
to basically have a network that is fully connected.

414
00:26:54,920 --> 00:27:03,000
So after we see an input at a certain time step, you want the, you know, your brain to

415
00:27:03,000 --> 00:27:09,320
basically, you know, integrate that information across, you know, all neurons.

416
00:27:09,320 --> 00:27:13,720
And because of the small world property, you only need about, you know, 5, you know,

417
00:27:13,720 --> 00:27:22,640
steps of the internal time steps in order to make sure that the information is spread

418
00:27:22,640 --> 00:27:25,160
through your, yeah, your whole network.

419
00:27:25,160 --> 00:27:30,280
The time steps are our property of LSTMs.

420
00:27:30,280 --> 00:27:35,120
Does this apply to other types of models as well, like CNNs?

421
00:27:35,120 --> 00:27:36,520
Yes, definitely.

422
00:27:36,520 --> 00:27:42,080
So we, we applied the same technique, you know, to convolutional kernels.

423
00:27:42,080 --> 00:27:48,640
So in this case, you, you know, we, it's your sparsity in the feature dimensions,

424
00:27:48,640 --> 00:27:54,520
convolutional kernels, and we also found there that indeed helps you get better performance.

425
00:27:54,520 --> 00:27:57,120
So yeah, we've done a couple of experiments.

426
00:27:57,120 --> 00:28:02,720
So one is on, so basically what we showed is that if you take an existing architecture

427
00:28:02,720 --> 00:28:10,120
with an existing convolutional kernel, so you take in a, like a res net or, you know,

428
00:28:10,120 --> 00:28:18,480
the pixels CNN and you replace the regular convolutional kernels with locks, spars kernels

429
00:28:18,480 --> 00:28:24,720
and then you eat or widen or deepen the network, you know, while keeping a number of parameters

430
00:28:24,720 --> 00:28:29,280
the same, you get better performance in many situations.

431
00:28:29,280 --> 00:28:36,320
What's the analog to introducing additional time steps in the LSTM and the convolutional

432
00:28:36,320 --> 00:28:37,320
network?

433
00:28:37,320 --> 00:28:38,320
Right.

434
00:28:38,320 --> 00:28:44,480
So in the convolutional network, you can, for example, in a res net, what worked well is

435
00:28:44,480 --> 00:28:50,480
that we, we simply doubled the, you know, the depth of the res net.

436
00:28:50,480 --> 00:28:58,040
So a res net, you know, consists of a couple of stages and between stages, you down sample.

437
00:28:58,040 --> 00:29:03,720
So in one experiment, we took a res net for, you know, C410 and we, we doubled the depth

438
00:29:03,720 --> 00:29:11,280
of the network and we introduced 50% sparsity, the 50% of the weights are not there anymore,

439
00:29:11,280 --> 00:29:16,000
which means that, you know, the total number of parameters is approximately the same as

440
00:29:16,000 --> 00:29:17,000
before.

441
00:29:17,000 --> 00:29:18,000
Right.

442
00:29:18,000 --> 00:29:23,040
And because of the additional depth, you still have a good mixing of information.

443
00:29:23,040 --> 00:29:28,000
And so the rest of, so we kept the rest of the architecture the same, so the same

444
00:29:28,000 --> 00:29:32,960
murder rates, the same, you know, the same hyper parameters, we kept, you know, everything

445
00:29:32,960 --> 00:29:38,600
fixed and we saw that it led to an improvement of the accuracy of the model.

446
00:29:38,600 --> 00:29:43,960
The smart world model that's giving you, that's giving you an algorithm that you can apply

447
00:29:43,960 --> 00:29:49,760
to determine which 50% of the data you're basically getting rid of and which you're keeping.

448
00:29:49,760 --> 00:29:56,160
Or is it giving you a, you know, some kind of bound or guarantee that if you get rid

449
00:29:56,160 --> 00:30:01,680
of, you know, X percent of information, you'll still have the number of, you know, a given

450
00:30:01,680 --> 00:30:04,920
degrees of connectivity or convergence or something.

451
00:30:04,920 --> 00:30:07,160
What exactly is that telling you?

452
00:30:07,160 --> 00:30:13,040
So we're, we're not actually getting rid of any data, right, because the, so the state

453
00:30:13,040 --> 00:30:16,840
is still, is still dense of the model.

454
00:30:16,840 --> 00:30:20,600
So all we're doing is introducing sparsity into weights.

455
00:30:20,600 --> 00:30:25,600
So we're not, we're not, you know, removing any, any information from, from the state.

456
00:30:25,600 --> 00:30:30,480
While you're not getting rid of data, strictly speaking, you're still kind of getting rid

457
00:30:30,480 --> 00:30:36,920
of data in motion in a sense, like you're making it harder for the network to, to learn

458
00:30:36,920 --> 00:30:40,720
or to get a piece of data in a given step, right?

459
00:30:40,720 --> 00:30:46,000
You mean that the number of weights per step is, is reduced by a factor two, so the number

460
00:30:46,000 --> 00:30:48,920
of revenues is reduced by a factor two?

461
00:30:48,920 --> 00:30:49,920
Right.

462
00:30:49,920 --> 00:30:55,240
I guess, I guess what I mean is that, you know, ultimately when you do this, the network

463
00:30:55,240 --> 00:31:02,040
is still operating on less information than it had in a fully connected sense.

464
00:31:02,040 --> 00:31:12,480
Maybe before you, you know, do things like add time steps and, you know, broaden, broaden

465
00:31:12,480 --> 00:31:19,120
or deepen your network, just if you were to have a fully connected network and then you

466
00:31:19,120 --> 00:31:23,360
use zero out some of the weights, like the network is then, is it fair to say that the

467
00:31:23,360 --> 00:31:30,200
network is operating on less, less information than in the fully connected sense?

468
00:31:30,200 --> 00:31:35,600
Yeah, so it is important to note that this is something we do before training, right?

469
00:31:35,600 --> 00:31:39,360
So we actually train with the sparsity, right?

470
00:31:39,360 --> 00:31:45,880
So yeah, so before you deepen or widen, you remove half weights, then indeed you do simply

471
00:31:45,880 --> 00:31:50,280
have half the capacity, you know, to store anything in your weights, obviously, right,

472
00:31:50,280 --> 00:31:51,280
right?

473
00:31:51,280 --> 00:31:56,280
So it is, it is important, I think, to do, to keep the number of, you know, parameters

474
00:31:56,280 --> 00:32:01,520
at least equal, it's just basically you are assigning your parameters in a different

475
00:32:01,520 --> 00:32:02,880
way to the model.

476
00:32:02,880 --> 00:32:03,880
Okay.

477
00:32:03,880 --> 00:32:08,720
So the model I still, in principle, the same capacity to store information, it's just

478
00:32:08,720 --> 00:32:13,480
that the, the way it uses the parameters is a little bit different.

479
00:32:13,480 --> 00:32:20,320
Just to summarize that, you, you are indeed reducing the model's capacity to store information

480
00:32:20,320 --> 00:32:27,280
when you remove half, let's say, of the weights, but you're compensating for that by either

481
00:32:27,280 --> 00:32:29,920
increasing your breadth or your depth.

482
00:32:29,920 --> 00:32:33,080
Yeah, that's what we are indeed doing.

483
00:32:33,080 --> 00:32:34,080
Okay.

484
00:32:34,080 --> 00:32:39,920
And then in the future work, I have, you know, believed that we can figure out how to,

485
00:32:39,920 --> 00:32:42,880
you know, learn the sparsity.

486
00:32:42,880 --> 00:32:48,960
And, you know, actually, you know, during training, be able to remove a large percentage

487
00:32:48,960 --> 00:32:53,680
of the weights, you know, without getting worse performance.

488
00:32:53,680 --> 00:32:54,920
So that's division.

489
00:32:54,920 --> 00:33:01,400
How sensitive are, well, in the case where I think with the LSTM, you said you got rid

490
00:33:01,400 --> 00:33:07,560
of 98% of the weights, I would imagine that which weights you get rid of is very important

491
00:33:07,560 --> 00:33:11,400
or, you know, conversely, which weights you keep are very important.

492
00:33:11,400 --> 00:33:18,480
But do you have some way of measuring the sensitivity of a given networks performance

493
00:33:18,480 --> 00:33:21,400
to which weights you remove?

494
00:33:21,400 --> 00:33:26,160
So we choose a particular sparsity, you know, pattern before training.

495
00:33:26,160 --> 00:33:27,160
Right.

496
00:33:27,160 --> 00:33:31,360
And then we use either, you know, a wider or a deeper network, right?

497
00:33:31,360 --> 00:33:32,360
Right.

498
00:33:32,360 --> 00:33:38,440
So then we train the weights, so the model has to sort of figure out what meaning to design,

499
00:33:38,440 --> 00:33:41,040
you know, to each neuron and to each weight.

500
00:33:41,040 --> 00:33:42,040
Right.

501
00:33:42,040 --> 00:33:47,200
So we leave it up to the model to use, you know, the given sparsity pattern.

502
00:33:47,200 --> 00:33:53,040
So what we found that worked well in case of LSTM is the so, you know, so called Borevelsi

503
00:33:53,040 --> 00:34:02,760
Albert Rav, which is a type of small world network where you have a small number of neurons

504
00:34:02,760 --> 00:34:06,480
that are connected to a very large number of auto neurons.

505
00:34:06,480 --> 00:34:07,480
Mm-hmm.

506
00:34:07,480 --> 00:34:13,680
And then you have like a long tail of neurons that are very sparsely connected.

507
00:34:13,680 --> 00:34:17,920
We found that it, you know, that this worked, you know, really well for text.

508
00:34:17,920 --> 00:34:24,760
So what we did in this case is that we increased, you know, the size of the state of the LSTM,

509
00:34:24,760 --> 00:34:25,760
right?

510
00:34:25,760 --> 00:34:30,840
So you have a much wider model, which also gives you a longer memory of the past, right?

511
00:34:30,840 --> 00:34:38,480
Because you get, you know, to fit more information into the state of the LSTM than otherwise.

512
00:34:38,480 --> 00:34:45,440
And what we found is that if we trained a sentiment classifier based on that state, so we first

513
00:34:45,440 --> 00:34:52,360
train an LSTM, completely unsupervised on text, and then we train a linear classifier

514
00:34:52,360 --> 00:34:58,840
on the state of the LSTM to predict the sentiment reviews.

515
00:34:58,840 --> 00:35:04,040
And then so what we found is that we get state of the art results in predicting sentiment

516
00:35:04,040 --> 00:35:05,960
based on that model.

517
00:35:05,960 --> 00:35:11,680
So the previous state of the art was published by Alec Redford, which is also co-author of

518
00:35:11,680 --> 00:35:12,680
this paper.

519
00:35:12,680 --> 00:35:13,680
Okay.

520
00:35:13,680 --> 00:35:14,680
A couple of months ago.

521
00:35:14,680 --> 00:35:15,680
Okay.

522
00:35:15,680 --> 00:35:19,320
But now we found that if you train a much, you know, wider network to this sparse, you

523
00:35:19,320 --> 00:35:21,120
get even better results.

524
00:35:21,120 --> 00:35:27,280
This basically, you gave us a state of the art results on like five benchmarks of just

525
00:35:27,280 --> 00:35:28,880
playing sentiment in text.

526
00:35:28,880 --> 00:35:29,880
Right.

527
00:35:29,880 --> 00:35:35,680
And so this is an example of how, you know, within the same parameter budget, reconfiguring

528
00:35:35,680 --> 00:35:40,480
the way you use that parameter budget can give you much better results.

529
00:35:40,480 --> 00:35:41,480
Exactly.

530
00:35:41,480 --> 00:35:42,480
Yeah.

531
00:35:42,480 --> 00:35:47,560
And is there any intuition as to where you're likely to see that effect or where you

532
00:35:47,560 --> 00:35:52,680
would likely want to apply block sparse kernels, or is it something that, you know, will

533
00:35:52,680 --> 00:35:54,760
just have come out of experimentation?

534
00:35:54,760 --> 00:35:55,760
Yeah.

535
00:35:55,760 --> 00:36:01,200
So, you know, the space of possible architectures that you can train with these kernels is so

536
00:36:01,200 --> 00:36:02,200
huge.

537
00:36:02,200 --> 00:36:06,000
And there's just no way that, you know, we can explore it all.

538
00:36:06,000 --> 00:36:07,000
So, yeah.

539
00:36:07,000 --> 00:36:12,680
Is it what we aim, you know, with this release is to basically, you know, give it to the world

540
00:36:12,680 --> 00:36:18,640
and let everyone experiment with it because, you know, you know, we're way too small to

541
00:36:18,640 --> 00:36:20,720
explore this useful space.

542
00:36:20,720 --> 00:36:24,680
So yeah, I have, you know, some limited intuition.

543
00:36:24,680 --> 00:36:25,680
Mm-hmm.

544
00:36:25,680 --> 00:36:26,680
And what is that?

545
00:36:26,680 --> 00:36:31,320
What's your intuition telling you that where my folks want to look first or where would

546
00:36:31,320 --> 00:36:34,000
you like to see folks looking to apply this?

547
00:36:34,000 --> 00:36:35,000
Right.

548
00:36:35,000 --> 00:36:42,840
So, Scott is building in the capability now of actually having like a dynamic sparsity.

549
00:36:42,840 --> 00:36:47,960
So hopefully this will be finished before the release, you can see, meaning that varies

550
00:36:47,960 --> 00:36:48,960
during training.

551
00:36:48,960 --> 00:36:49,960
Yes.

552
00:36:49,960 --> 00:36:53,160
So, you can actually learn the sparsity mouse during training.

553
00:36:53,160 --> 00:36:54,160
Okay.

554
00:36:54,160 --> 00:36:57,400
And you could then potentially optimize it, you know, to get more performance.

555
00:36:57,400 --> 00:36:58,400
Right.

556
00:36:58,400 --> 00:36:59,400
Right.

557
00:36:59,400 --> 00:37:04,760
So, in other humans, it's, you know, we know that the way our own neurons are connected

558
00:37:04,760 --> 00:37:05,760
is also learned.

559
00:37:05,760 --> 00:37:06,760
Right.

560
00:37:06,760 --> 00:37:07,760
It's based on data.

561
00:37:07,760 --> 00:37:08,760
Right.

562
00:37:08,760 --> 00:37:09,760
Yeah.

563
00:37:09,760 --> 00:37:12,240
So, so this is, this is a way of, you know, learning the architecture of your model.

564
00:37:12,240 --> 00:37:17,040
So, personally, I think this is, this will be a very interesting area of research.

565
00:37:17,040 --> 00:37:18,040
Yeah.

566
00:37:18,040 --> 00:37:21,240
So, you know, maybe I'm kind of beating a dead horse here.

567
00:37:21,240 --> 00:37:28,240
But it sounds like there's really two things that could be accomplished here and that they're

568
00:37:28,240 --> 00:37:29,240
kind of different.

569
00:37:29,240 --> 00:37:34,080
And I'm wondering if, you know, there's some way that you think about these one is, you

570
00:37:34,080 --> 00:37:43,680
know, given a parameter budget, you know, re-architection your network using block sparsity

571
00:37:43,680 --> 00:37:46,880
to improve your results.

572
00:37:46,880 --> 00:37:51,240
But separate from that, there's this, you know, issue of this whole, you know, finding

573
00:37:51,240 --> 00:37:56,880
the right 2% of parameters that actually matter and then using block sparsity as a way

574
00:37:56,880 --> 00:38:02,440
to implement the network that, you know, just has what matters and presumably the result

575
00:38:02,440 --> 00:38:05,880
is that you're able to compute those, you know, much more quickly.

576
00:38:05,880 --> 00:38:06,880
Right.

577
00:38:06,880 --> 00:38:10,400
And I'm thinking about that the right way and are those, you know, do you think of those

578
00:38:10,400 --> 00:38:16,480
as the same problem or are they, you know, kind of, you know, two different problems,

579
00:38:16,480 --> 00:38:21,320
you know, that are enabled by this block spars kernel approach?

580
00:38:21,320 --> 00:38:22,320
I think you have right.

581
00:38:22,320 --> 00:38:23,320
Yeah.

582
00:38:23,320 --> 00:38:28,600
And these are indeed two problems that you can, you know, now tackle.

583
00:38:28,600 --> 00:38:32,760
And indeed, yeah, so either, you know, static sparsity, which is what we have done in our

584
00:38:32,760 --> 00:38:38,160
experiments or dynamic sparsity, which is sort of where you allure the sparsity pattern.

585
00:38:38,160 --> 00:38:39,160
Yeah.

586
00:38:39,160 --> 00:38:42,280
That is also an interesting application of this.

587
00:38:42,280 --> 00:38:47,200
And so it strikes me that a big part of the reason why you care about any of this at

588
00:38:47,200 --> 00:38:50,800
all is because of computational limitations.

589
00:38:50,800 --> 00:38:53,120
Is that the main idea?

590
00:38:53,120 --> 00:38:56,400
That's the main idea, but that's, that's I think the main idea of the whole field of

591
00:38:56,400 --> 00:38:57,400
computer science.

592
00:38:57,400 --> 00:38:58,400
Right.

593
00:38:58,400 --> 00:39:00,920
Too sure.

594
00:39:00,920 --> 00:39:07,440
You know, that was a little bit of a segue into in spite of the fact that, you know, we're

595
00:39:07,440 --> 00:39:10,000
getting around computational limitations here.

596
00:39:10,000 --> 00:39:16,040
You actually had access to some, you know, pretty fancy hardware to kind of try this

597
00:39:16,040 --> 00:39:17,040
out on.

598
00:39:17,040 --> 00:39:21,600
Can you talk a little bit about, you know, kind of the specific problem that you, you

599
00:39:21,600 --> 00:39:26,880
know, the problems that you were looking at to kind of push the, you know, the, the

600
00:39:26,880 --> 00:39:33,560
limit and then, you know, how the experimental results you saw on the, you know, the hardware

601
00:39:33,560 --> 00:39:34,560
that you were using?

602
00:39:34,560 --> 00:39:35,560
Absolutely.

603
00:39:35,560 --> 00:39:36,560
Absolutely.

604
00:39:36,560 --> 00:39:45,400
So one of the problems that we wanted to solve is to train a huge LSTM on a very large,

605
00:39:45,400 --> 00:39:48,520
very large data type of text and then the Amazon reviews.

606
00:39:48,520 --> 00:39:49,520
Okay.

607
00:39:49,520 --> 00:39:57,520
And we were so lucky to have access to an NVIDIA DGX1, which is hardware from NVIDIA

608
00:39:57,520 --> 00:40:04,200
that allowed us to train much larger models than we could have trained otherwise.

609
00:40:04,200 --> 00:40:11,280
So yeah, this, this was something that enabled us to basically get state of your performance

610
00:40:11,280 --> 00:40:14,040
on the Amazon reviews data set.

611
00:40:14,040 --> 00:40:22,040
And this was also instrumental to get the results I talked about on access to flying sentiment.

612
00:40:22,040 --> 00:40:26,640
What was it about the LSTM that made it huge?

613
00:40:26,640 --> 00:40:33,640
So the Amazon reviews data set is just a very large data set of reviews on Amazon obviously.

614
00:40:33,640 --> 00:40:36,960
Do you remember how many reviews, how many reviews are in that data set?

615
00:40:36,960 --> 00:40:44,240
I don't know, like Adam has, but it's, it's like a large portion of all of them.

616
00:40:44,240 --> 00:40:49,880
So basically the model that you could fit on this data is like an order of magnitude

617
00:40:49,880 --> 00:40:52,520
larger than anything we tried earlier.

618
00:40:52,520 --> 00:40:56,440
So you also need, you know, hardware to be able to fit such a model.

619
00:40:56,440 --> 00:41:02,200
And if you weren't using the DGX1, what would you have used otherwise and how did the results

620
00:41:02,200 --> 00:41:04,560
that you saw compare?

621
00:41:04,560 --> 00:41:14,960
So if you wouldn't have had a DGX1, then we would have had to use a cluster of GPUs.

622
00:41:14,960 --> 00:41:21,640
So recently it has become clear that for some problems, it is possible to, to train with

623
00:41:21,640 --> 00:41:27,480
very large mini batches or, you know, to spread out and training a close, you know, cluster.

624
00:41:27,480 --> 00:41:35,200
But then you, you will still get into problems of your number of parameters so that is not

625
00:41:35,200 --> 00:41:36,200
ideal still.

626
00:41:36,200 --> 00:41:41,160
And typically, you know, still need to, you know, fit your parameters on the single GPU.

627
00:41:41,160 --> 00:41:48,160
So even if you can, you know, split your mini batch data across multiple machines, then

628
00:41:48,160 --> 00:41:51,760
you're, you know, still bottlenecked by the memory of all of the single machines.

629
00:41:51,760 --> 00:41:52,760
Okay.

630
00:41:52,760 --> 00:41:57,200
And do you have a sense for at the end of the day, you know, well, how long did it take

631
00:41:57,200 --> 00:42:01,040
you to train your models and like how, do you ever sense for how much faster it would,

632
00:42:01,040 --> 00:42:05,720
you know, it was relative to what you would have done otherwise?

633
00:42:05,720 --> 00:42:11,480
I think it allowed us to train the model about twice as fast as otherwise.

634
00:42:11,480 --> 00:42:15,720
So it's still true, I believe, about, you know, two weeks to train the model.

635
00:42:15,720 --> 00:42:16,720
Oh, wow.

636
00:42:16,720 --> 00:42:17,720
Wow.

637
00:42:17,720 --> 00:42:24,880
It sounds like a really interesting project with some potentially broad applications that,

638
00:42:24,880 --> 00:42:30,160
you know, I'll be keeping an eye out for clearly, I'll be publishing a paper around this.

639
00:42:30,160 --> 00:42:36,080
Are you also publishing code or is it more the, the research results that are the important

640
00:42:36,080 --> 00:42:38,400
takeaway for folks that want to build on it?

641
00:42:38,400 --> 00:42:39,400
Yeah.

642
00:42:39,400 --> 00:42:40,400
So it's a second.

643
00:42:40,400 --> 00:42:41,920
So we are actually publishing the code.

644
00:42:41,920 --> 00:42:42,920
Okay.

645
00:42:42,920 --> 00:42:44,640
So we are publishing the GPU kernels.

646
00:42:44,640 --> 00:42:50,800
I think this is by far the most interesting part of what we release because this actually

647
00:42:50,800 --> 00:42:57,160
allows practitioners and researchers to do, you know, completely new things very easily.

648
00:42:57,160 --> 00:43:05,360
So it's just basically a matter of importing a new library and, you know, replacing your

649
00:43:05,360 --> 00:43:11,720
existing conclusions or matrix applications with, you know, the block sparse ones and

650
00:43:11,720 --> 00:43:12,720
they're good to go.

651
00:43:12,720 --> 00:43:16,960
So yeah, it is actual software to use for others.

652
00:43:16,960 --> 00:43:17,960
Okay.

653
00:43:17,960 --> 00:43:23,120
Do you at this point have a place that you can point folks to or do you know where folks

654
00:43:23,120 --> 00:43:26,120
will be able to find this work once it's published?

655
00:43:26,120 --> 00:43:33,800
You can find a work on OpenEI.com and you go to the blog posts there.

656
00:43:33,800 --> 00:43:38,040
You will find a blog post on this topic and a link to get up.

657
00:43:38,040 --> 00:43:39,040
Okay.

658
00:43:39,040 --> 00:43:40,040
Fantastic.

659
00:43:40,040 --> 00:43:41,040
Great.

660
00:43:41,040 --> 00:43:42,040
Great.

661
00:43:42,040 --> 00:43:43,040
I really enjoy chatting with you.

662
00:43:43,040 --> 00:43:46,440
Is there anything else that you'd like to leave the audience with?

663
00:43:46,440 --> 00:43:48,680
Yeah, I think we had a very interesting conversation.

664
00:43:48,680 --> 00:43:51,120
Thank you for inviting me to the show.

665
00:43:51,120 --> 00:43:56,320
And yeah, I encourage everyone, you know, to keep sharing results, you know, to publishing

666
00:43:56,320 --> 00:44:03,360
source code, all of your experiments and keep an eye out on the research of OpenEI.

667
00:44:03,360 --> 00:44:04,360
So that's it.

668
00:44:04,360 --> 00:44:05,360
Great.

669
00:44:05,360 --> 00:44:06,360
All right.

670
00:44:06,360 --> 00:44:07,360
Well, thanks.

671
00:44:07,360 --> 00:44:08,360
Thanks very much.

672
00:44:08,360 --> 00:44:09,360
Okay.

673
00:44:09,360 --> 00:44:16,680
All right, everyone, that's our show for today.

674
00:44:16,680 --> 00:44:21,280
Thanks so much for listening and for your continued feedback and support.

675
00:44:21,280 --> 00:44:26,120
For more information on Dirk or any of the topics covered in this episode, head on over

676
00:44:26,120 --> 00:44:30,440
to twimlai.com slash talk slash 80.

677
00:44:30,440 --> 00:44:36,240
To catch up on our OpenEI series, visit twimlai.com slash open AI.

678
00:44:36,240 --> 00:44:41,440
Of course, you can send along your feedback or questions to me via Twitter to at Sam

679
00:44:41,440 --> 00:44:47,960
Charrington or at twimlai or leave a comment right on the show notes page.

680
00:44:47,960 --> 00:44:51,120
Thanks once again to Nvidia for their support of this series.

681
00:44:51,120 --> 00:44:56,480
To learn more about Nvidia and their presence at nips, remember to head on over to twimlai.com

682
00:44:56,480 --> 00:44:58,480
slash Nvidia.

683
00:44:58,480 --> 00:45:09,920
And thank you once again for listening and catch you next time.

