1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,640
I'm your host Sam Charrington.

4
00:00:31,640 --> 00:00:36,120
For those challenged with promoting the use of machine learning in an organization and

5
00:00:36,120 --> 00:00:41,680
making it more accessible, one key to success is to support data scientists and machine

6
00:00:41,680 --> 00:00:46,240
learning engineers with modern processes, tooling and platforms.

7
00:00:46,240 --> 00:00:50,480
This is a topic that we're excited to address here on the podcast with this AI Platforms

8
00:00:50,480 --> 00:00:55,640
Podcast series as well as a series of eBooks that we'll be publishing on this topic.

9
00:00:55,640 --> 00:00:59,560
The first of these eBooks takes a bottoms up look at AI Platforms and is focused on the

10
00:00:59,560 --> 00:01:04,880
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure

11
00:01:04,880 --> 00:01:09,920
at places like Airbnb, booking.com and open AI.

12
00:01:09,920 --> 00:01:14,040
The second book in the series looks at scaling data science and ML engineering from the top

13
00:01:14,040 --> 00:01:19,240
down, exploring the internal platforms that companies like Airbnb, Facebook and Uber

14
00:01:19,240 --> 00:01:23,040
have built and what enterprises can learn from them.

15
00:01:23,040 --> 00:01:27,120
If these are topics you're interested in and especially if part of your job involves

16
00:01:27,120 --> 00:01:32,720
making machine learning more accessible, I'd encourage you to visit Twimbleai.com slash

17
00:01:32,720 --> 00:01:38,480
AI Platforms and sign up to be notified as soon as these books are published.

18
00:01:38,480 --> 00:01:45,560
Alright, in this episode of our AI Platforms Podcast series, we're joined by Bicheng Chen,

19
00:01:45,560 --> 00:01:49,640
principal staff engineer and applied researcher at LinkedIn.

20
00:01:49,640 --> 00:01:56,160
Bicheng and I caught up to discuss LinkedIn's internal AI automation platform ProML, which

21
00:01:56,160 --> 00:02:01,080
was built with the hopes of providing a single platform for the entire life cycle of developing

22
00:02:01,080 --> 00:02:06,000
training, deploying and testing machine learning models at the company.

23
00:02:06,000 --> 00:02:12,080
In our conversation, Bicheng details ProML, breaking down some of its major components including

24
00:02:12,080 --> 00:02:17,800
its feature marketplace, model creation tooling and training management system to name just

25
00:02:17,800 --> 00:02:19,360
a few.

26
00:02:19,360 --> 00:02:24,160
We also discussed LinkedIn's experience bringing ProML to the company's developers and the

27
00:02:24,160 --> 00:02:28,880
role of the company's AI Academy has played getting them up to speed.

28
00:02:28,880 --> 00:02:32,960
We're excited to have LinkedIn as a sponsor and supporter of the show and to include their

29
00:02:32,960 --> 00:02:35,400
story in this series.

30
00:02:35,400 --> 00:02:43,560
And now on to the show.

31
00:02:43,560 --> 00:02:46,400
Alright everyone, I am on the line with Bicheng Chen.

32
00:02:46,400 --> 00:02:51,400
Bicheng is a principal staff engineer and applied researcher at LinkedIn.

33
00:02:51,400 --> 00:02:54,000
Bicheng, welcome to this week in machine learning and AI.

34
00:02:54,000 --> 00:02:57,000
Yeah, it's very nice to be here.

35
00:02:57,000 --> 00:03:02,240
I'm excited to have you on and to learn more about what LinkedIn is doing to support its

36
00:03:02,240 --> 00:03:03,240
AI efforts.

37
00:03:03,240 --> 00:03:08,560
But before we dive into that, you currently lead machine learning algorithms and tooling

38
00:03:08,560 --> 00:03:09,560
at LinkedIn.

39
00:03:09,560 --> 00:03:14,280
Can you tell us a little bit more about your role and your background, how you got to where

40
00:03:14,280 --> 00:03:15,760
you are?

41
00:03:15,760 --> 00:03:22,520
My current responsibility is to ensure LinkedIn has the right machine learning and AI technology

42
00:03:22,520 --> 00:03:27,680
and also ensure our developers are productive using this technology.

43
00:03:27,680 --> 00:03:30,840
I have been always interested in machine learning since college.

44
00:03:30,840 --> 00:03:37,960
I still remember that probably 20 years ago, I'm doing neural network for face detection.

45
00:03:37,960 --> 00:03:46,320
Then in my PhD study, I was in a database group, but my research is always on how to apply

46
00:03:46,320 --> 00:03:51,560
machine learning to database systems to make data systems more intelligent.

47
00:03:51,560 --> 00:04:00,240
And after I graduated, I joined Yahoo Research and started my career in recommender systems.

48
00:04:00,240 --> 00:04:06,400
So in addition to publishing papers and also a book on statistical methods for recommender

49
00:04:06,400 --> 00:04:13,440
system, we also designed the recommending algorithms for the Yahoo homepage, Yahoo News

50
00:04:13,440 --> 00:04:15,680
and some other applications.

51
00:04:15,680 --> 00:04:21,080
And something we find to be very interesting is that for recommendation system to work

52
00:04:21,080 --> 00:04:27,760
very well, human and algorithm combination is very, very important.

53
00:04:27,760 --> 00:04:34,080
So human being are very good at selecting good candidate articles for recommendation.

54
00:04:34,080 --> 00:04:42,400
And that algorithms are very powerful to identify the user's interest and do depersonization

55
00:04:42,400 --> 00:04:48,160
and quickly react to changes in the user behavior.

56
00:04:48,160 --> 00:04:53,720
And after that, I joined LinkedIn about six years ago.

57
00:04:53,720 --> 00:05:01,720
I started by working on news recommendation and also feed ranking algorithms.

58
00:05:01,720 --> 00:05:07,120
And I'm amazed that LinkedIn has a very unique and a very rich data sets.

59
00:05:07,120 --> 00:05:15,360
So we have a pretty large member base like more than 500 million members and more than

60
00:05:15,360 --> 00:05:23,520
30 million companies and also a lot of like open jobs that people can apply to you.

61
00:05:23,520 --> 00:05:31,040
And another unique aspect is for every member, LinkedIn has very good profiles of those

62
00:05:31,040 --> 00:05:37,400
members, which are difficult to find other places for web companies.

63
00:05:37,400 --> 00:05:43,960
And in addition to those profiles, we have people's connection, we have people's interaction,

64
00:05:43,960 --> 00:05:50,440
we have users activity, people are seeking jobs, people are consuming content on LinkedIn

65
00:05:50,440 --> 00:05:54,080
and also learning courses on LinkedIn.

66
00:05:54,080 --> 00:05:57,520
And each member also have different roles.

67
00:05:57,520 --> 00:06:03,120
Some people are seeking jobs, some people are hiring people, some people are consuming

68
00:06:03,120 --> 00:06:07,760
content, some people are using LinkedIn as a platform to publish content.

69
00:06:07,760 --> 00:06:13,120
So there's a lot of connections we want to make between users so that everyone has the

70
00:06:13,120 --> 00:06:16,000
best opportunities.

71
00:06:16,000 --> 00:06:24,520
After around probably two years ago, I started to lead a group which designs the algorithms

72
00:06:24,520 --> 00:06:31,200
and also tools to help LinkedIn to be able to use machine learning and AI technology in

73
00:06:31,200 --> 00:06:32,560
all of our products.

74
00:06:32,560 --> 00:06:39,520
So you've talked about some of the unique assets that LinkedIn has in terms of its members

75
00:06:39,520 --> 00:06:43,560
and the profiles and the connections between those members.

76
00:06:43,560 --> 00:06:50,800
What are some of the machine learning and AI applications that are in place at LinkedIn?

77
00:06:50,800 --> 00:06:51,800
Yeah.

78
00:06:51,800 --> 00:06:56,520
So machine learning is used almost in every product in LinkedIn.

79
00:06:56,520 --> 00:06:58,920
So just give a few examples.

80
00:06:58,920 --> 00:07:07,240
If you open LinkedIn homepage, you see a feed of updates from your connections.

81
00:07:07,240 --> 00:07:14,160
So those are articles shared by your connections and also their professional change.

82
00:07:14,160 --> 00:07:17,800
For example, they change a job or they have a job anniversary.

83
00:07:17,800 --> 00:07:24,160
So we use machine learning to rank these different types of items so that we can maximize

84
00:07:24,160 --> 00:07:26,120
users engagement.

85
00:07:26,120 --> 00:07:30,400
And also in the feed, we also have sponsored updates.

86
00:07:30,400 --> 00:07:39,760
So company, they are paying thing to put updates in the feed so that they can get attention.

87
00:07:39,760 --> 00:07:45,460
And we use machine learning to predict the click rates of these different more like

88
00:07:45,460 --> 00:07:57,400
device advertisements and so that we can maximize the revenue and also the optimizer's value.

89
00:07:57,400 --> 00:07:59,760
And also we recommend people to people.

90
00:07:59,760 --> 00:08:05,600
So for social network, growing a person's network is very important.

91
00:08:05,600 --> 00:08:12,640
So we look at people's connection structure and try to identify the connection strength

92
00:08:12,640 --> 00:08:17,280
between two users and then make recommendations that you may be interested in connecting

93
00:08:17,280 --> 00:08:19,080
to these people.

94
00:08:19,080 --> 00:08:23,680
And we also use machine learning in different search problems.

95
00:08:23,680 --> 00:08:28,160
So for example, recruiters come to LinkedIn to search for potential candidates.

96
00:08:28,160 --> 00:08:32,920
And we use machine learning to help recruiters to find the best candidate that can feed their

97
00:08:32,920 --> 00:08:33,920
need.

98
00:08:33,920 --> 00:08:40,120
And also in addition to recruiting product, we also have a sales solution product to help

99
00:08:40,120 --> 00:08:46,800
sales people to find leads that can lead to potential like sales opportunity.

100
00:08:46,800 --> 00:08:50,960
And this is also using machine learning to find the right connection between the people.

101
00:08:50,960 --> 00:08:57,200
Can you talk a little bit about the evolution of tooling for machine learning at LinkedIn?

102
00:08:57,200 --> 00:09:08,120
So we started by developing machine learning algorithms on Hadoop.

103
00:09:08,120 --> 00:09:15,120
And I think several years ago, we find that using Hadoop may not be the best or the most

104
00:09:15,120 --> 00:09:18,600
efficient way of doing machine learning computation.

105
00:09:18,600 --> 00:09:23,840
So we start to develop algorithms on Spark.

106
00:09:23,840 --> 00:09:30,400
And we find that the ML leap coming from the Spark package is not efficient enough.

107
00:09:30,400 --> 00:09:40,360
So we develop our own training algorithm called PhotonConnect PHOTON Photon ML.

108
00:09:40,360 --> 00:09:45,280
And we open source that so that like everyone can also use that.

109
00:09:45,280 --> 00:09:54,480
And in addition to algorithms, we also look at how people can deploy model into our production

110
00:09:54,480 --> 00:09:56,000
systems.

111
00:09:56,000 --> 00:10:03,200
And in the past, deploying model into product systems has been pinpoint.

112
00:10:03,200 --> 00:10:07,400
There's a lot of co-development that people need to do.

113
00:10:07,400 --> 00:10:12,120
And our current effort is to make this whole process as automatic as possible.

114
00:10:12,120 --> 00:10:18,520
OK, and is Photon ML is that a replacement for ML lib that also runs within Spark or

115
00:10:18,520 --> 00:10:19,520
does that?

116
00:10:19,520 --> 00:10:21,800
Is there another engine for that?

117
00:10:21,800 --> 00:10:24,880
That is another engine.

118
00:10:24,880 --> 00:10:29,960
And internally, we basically mainly use Photon ML for our model training purpose.

119
00:10:29,960 --> 00:10:36,880
And we are not using ML leap because ML leap, at least in our experience, doesn't really

120
00:10:36,880 --> 00:10:41,000
scale to the amount of data that we need to process.

121
00:10:41,000 --> 00:10:48,280
And so it's Photon ML replacement for Spark or just the ML lib piece.

122
00:10:48,280 --> 00:10:49,960
And do you still run within Spark?

123
00:10:49,960 --> 00:10:51,480
Yeah, that's correct, right?

124
00:10:51,480 --> 00:10:53,440
Just a replacement for ML leap.

125
00:10:53,440 --> 00:10:56,920
We are still running using Spark.

126
00:10:56,920 --> 00:11:02,640
Maybe before we dig deeper into kind of the tooling and platform side, can you talk a

127
00:11:02,640 --> 00:11:09,120
little bit about the developer audience for your tooling efforts at LinkedIn or you targeting

128
00:11:09,120 --> 00:11:15,880
primarily kind of applied researchers or how diverse is the audience that you're supporting

129
00:11:15,880 --> 00:11:19,640
with the various tools that you're building?

130
00:11:19,640 --> 00:11:26,000
The initial audience that we support are the machine learning developers, right?

131
00:11:26,000 --> 00:11:33,440
So those are the people who design algorithms for our application, use machine learning

132
00:11:33,440 --> 00:11:39,240
to make the best decision for a different LinkedIn product.

133
00:11:39,240 --> 00:11:41,320
That's our initial focus.

134
00:11:41,320 --> 00:11:47,680
But at the same time, we also recognize that we really need to scale machine learning development

135
00:11:47,680 --> 00:11:49,320
and LinkedIn.

136
00:11:49,320 --> 00:11:56,480
We made an effort to educate our self-engineers to be able to apply machine learning.

137
00:11:56,480 --> 00:12:05,240
We call that AI Academy LinkedIn, so that is a five week training program that our engineer

138
00:12:05,240 --> 00:12:10,440
can participate to learn about machine learning and later, they can also apply machine learning

139
00:12:10,440 --> 00:12:13,160
to their application area.

140
00:12:13,160 --> 00:12:21,480
So gradually, the tooling that we're building will also support those people who graduate

141
00:12:21,480 --> 00:12:23,720
from AI Academy.

142
00:12:23,720 --> 00:12:28,680
They don't have a lot of past machine learning background, but they are learning to use machine

143
00:12:28,680 --> 00:12:32,880
learning and apply machine learning to our applications.

144
00:12:32,880 --> 00:12:40,080
Before we kind of dug into the question about the developer audience, you walked us through

145
00:12:40,080 --> 00:12:47,960
this evolution from Hadoop to Spark ML Lib to Photon ML.

146
00:12:47,960 --> 00:12:52,920
LinkedIn also has a machine learning platform that you call ProML.

147
00:12:52,920 --> 00:12:53,920
That's correct.

148
00:12:53,920 --> 00:12:59,920
Tell us about ProML and the relationship between it and these other tools that you've

149
00:12:59,920 --> 00:13:00,920
mentioned.

150
00:13:00,920 --> 00:13:01,920
Okay.

151
00:13:01,920 --> 00:13:16,640
So ProML is an initiative to double productivity for ML developer, and this platform provides

152
00:13:16,640 --> 00:13:21,920
functionalities for the entire machine learning life cycle.

153
00:13:21,920 --> 00:13:30,480
So starting from a feature marquee place, in which people can share and find potentially

154
00:13:30,480 --> 00:13:35,680
useful features and manage features for their machine learning model.

155
00:13:35,680 --> 00:13:39,880
And the second is model creation tooling.

156
00:13:39,880 --> 00:13:47,920
So this involves all the tools we use to train machine learning model and also specify

157
00:13:47,920 --> 00:13:51,160
the computation logic of machine learning model.

158
00:13:51,160 --> 00:13:55,280
So Photon ML belongs to this category.

159
00:13:55,280 --> 00:14:02,640
So Photon ML is one of the tools in ProML ecosystem.

160
00:14:02,640 --> 00:14:11,800
And we also develop algorithms to automatically select features and also automatically selecting

161
00:14:11,800 --> 00:14:16,160
hyper parameters for your machine learning models.

162
00:14:16,160 --> 00:14:25,240
And a third area is model deployment in which we develop tools to manage all the previously

163
00:14:25,240 --> 00:14:34,440
trend models and allow users, like LinkedIn internal users, to click on a button to publish

164
00:14:34,440 --> 00:14:41,760
the model and then deploy the model into various places in our production systems.

165
00:14:41,760 --> 00:14:51,320
And a third, the fourth area is model inference engine in which we provide the runtime environment

166
00:14:51,320 --> 00:14:55,800
to run the model and to serve user traffic.

167
00:14:55,800 --> 00:15:01,400
And the fifth area is what we call health assurance.

168
00:15:01,400 --> 00:15:07,640
So in which we provide tools to automatically monitor the model performance and also data

169
00:15:07,640 --> 00:15:11,800
quality to ensure that our models are performing well.

170
00:15:11,800 --> 00:15:20,120
And we also provide anomaly detection capability so that when something goes wrong, we can quickly

171
00:15:20,120 --> 00:15:21,680
identify them.

172
00:15:21,680 --> 00:15:29,680
And after we identify issues, we also provide debug and explain tooling so that people can

173
00:15:29,680 --> 00:15:33,800
investigate and find the reason that caused the issue.

174
00:15:33,800 --> 00:15:39,920
OK, it sounds like there's a ton for us to dig into the air.

175
00:15:39,920 --> 00:15:45,880
Maybe let's start at the beginning and talk about the feature marketplace.

176
00:15:45,880 --> 00:15:50,760
What's the motivation for the feature marketplace?

177
00:15:50,760 --> 00:15:57,320
So I think feature marketplace is also, I think, one of the quite unique area that we are

178
00:15:57,320 --> 00:15:59,720
looking at.

179
00:15:59,720 --> 00:16:08,560
So the reason that we start this feature marketplace effort is that in the past, different teams,

180
00:16:08,560 --> 00:16:12,240
they have different pipelines to produce features.

181
00:16:12,240 --> 00:16:19,080
And what we observe is that there are different teams.

182
00:16:19,080 --> 00:16:21,040
They develop very similar features.

183
00:16:21,040 --> 00:16:24,680
There's a very little leverage across the teams.

184
00:16:24,680 --> 00:16:30,480
And also the pipeline that generate features over time get very complicated, right?

185
00:16:30,480 --> 00:16:37,000
So we get into a pipeline jungle that at some point, for some application, become very

186
00:16:37,000 --> 00:16:38,840
difficult to manage.

187
00:16:38,840 --> 00:16:46,640
So the effort of feature marketplace is to simplify all these so that we provide a

188
00:16:46,640 --> 00:16:55,520
abstraction layer. In this abstraction layer, the feature producers, they use simple configuration

189
00:16:55,520 --> 00:17:01,840
to set up features by defining the location of the feature and the extraction logic of

190
00:17:01,840 --> 00:17:03,600
the feature.

191
00:17:03,600 --> 00:17:12,240
After that, the feature users, they can just use a unique name to refer to the feature.

192
00:17:12,240 --> 00:17:16,480
And the system will automatically figure out where to get the feature and how to extract

193
00:17:16,480 --> 00:17:17,480
the feature.

194
00:17:17,480 --> 00:17:21,680
And when you say the feature location, what are you referring to there?

195
00:17:21,680 --> 00:17:28,560
Is that like a source system or is it, what are we referring to?

196
00:17:28,560 --> 00:17:37,440
Yep. So location refers to the source of the features of example.

197
00:17:37,440 --> 00:17:44,280
We need features for model training offline, for example, Hadoop.

198
00:17:44,280 --> 00:17:51,000
So then the location will be a path on HDFS.

199
00:17:51,000 --> 00:17:59,000
And when we do online serving, the features may come from, for example, a key value store.

200
00:17:59,000 --> 00:18:04,760
So in that case, the location will be the address of that particular service.

201
00:18:04,760 --> 00:18:09,440
So you've got kind of the source of the feature data.

202
00:18:09,440 --> 00:18:17,000
And then it sounds like you're also providing a way to declaratively specify some sequence

203
00:18:17,000 --> 00:18:19,360
of feature transformations.

204
00:18:19,360 --> 00:18:20,360
Is that right?

205
00:18:20,360 --> 00:18:21,360
Is that part of it?

206
00:18:21,360 --> 00:18:22,520
Yeah, that is correct.

207
00:18:22,520 --> 00:18:28,920
So there are a few common logic that people use to produce useful feature.

208
00:18:28,920 --> 00:18:38,520
One is sliding window aggregation. So basically, you look at a user's past activity and aggregate

209
00:18:38,520 --> 00:18:41,760
the activity to identify their interest.

210
00:18:41,760 --> 00:18:49,320
So for example, if we saw that a user's frequently clicked on an article that mentioned a particular

211
00:18:49,320 --> 00:18:50,320
company.

212
00:18:50,320 --> 00:18:54,240
So then we know that the user probably is interested in that company.

213
00:18:54,240 --> 00:19:01,000
So one of the concepts that comes up in some of my conversations around features and

214
00:19:01,000 --> 00:19:08,400
repeatable pipelines is the notion of a DAG or a graph for applying these different transformations

215
00:19:08,400 --> 00:19:11,680
are you using some kind of metaphor like that?

216
00:19:11,680 --> 00:19:14,480
Yes, that's correct.

217
00:19:14,480 --> 00:19:19,640
There are two kind of DAGs in our system.

218
00:19:19,640 --> 00:19:24,800
So one DAG is for this feature computation.

219
00:19:24,800 --> 00:19:29,480
Another DAG is for the model computation logic.

220
00:19:29,480 --> 00:19:39,720
So in the feature computation DAG, that can include, for example, aggregation and also join.

221
00:19:39,720 --> 00:19:42,440
So from a key, we can look up.

222
00:19:42,440 --> 00:19:53,760
So for example, if we look at a feature for a member, so from the member, we know the

223
00:19:53,760 --> 00:19:54,760
indenting, right?

224
00:19:54,760 --> 00:20:00,800
We know the job title of the member and from the job title, we can look up for more information

225
00:20:00,800 --> 00:20:06,680
about the job title that in term can become a feature for the member.

226
00:20:06,680 --> 00:20:11,640
So such a join look up are all in the DAG that process the feature.

227
00:20:11,640 --> 00:20:18,200
Do you run into the issue of kind of the leaky abstraction problem around features where

228
00:20:18,200 --> 00:20:22,400
a team will define a feature?

229
00:20:22,400 --> 00:20:28,320
And it has kind of very specific semantics to the problem that they're trying to solve

230
00:20:28,320 --> 00:20:33,600
and someone else might find it in a marketplace or catalog and want to use it, but it doesn't

231
00:20:33,600 --> 00:20:34,600
quite fit.

232
00:20:34,600 --> 00:20:38,720
And if you do run into that, how do you address that?

233
00:20:38,720 --> 00:20:47,160
Yes, I think different teams, always, right, may define some feature that is very specific

234
00:20:47,160 --> 00:20:49,200
to their application.

235
00:20:49,200 --> 00:20:55,400
And in that case, while those features probably won't be that shareable, however, there

236
00:20:55,400 --> 00:21:01,120
are a large number of features that can be shared across different applications, especially

237
00:21:01,120 --> 00:21:04,360
on the features that capture a user's interest, right?

238
00:21:04,360 --> 00:21:11,360
So for example, users' interest in different kinds of jobs can be used to, for example,

239
00:21:11,360 --> 00:21:16,880
rank the content like articles for the user also, right?

240
00:21:16,880 --> 00:21:22,880
So for example, you look at job activity that a particular user is interested in jobs

241
00:21:22,880 --> 00:21:28,400
from a particular company, then when we rank articles, then we can potentially rank

242
00:21:28,400 --> 00:21:34,480
articles about that company higher, right, so that user can get a value.

243
00:21:34,480 --> 00:21:40,600
And are the features that groups define and publish out to this marketplace, are they?

244
00:21:40,600 --> 00:21:48,040
I'm thinking of like different version control metaphors, like forkable and I guess mainly

245
00:21:48,040 --> 00:21:49,120
that's the one I'm thinking of.

246
00:21:49,120 --> 00:21:54,600
Like can you fork these things and extend them to make them more, to tailor them to specific

247
00:21:54,600 --> 00:21:58,920
use cases or are they kind of more static?

248
00:21:58,920 --> 00:22:07,120
So the way that we are managing the features is the following, we try to reduce the dependencies

249
00:22:07,120 --> 00:22:14,040
of features on other features, because I think when we have more and more dependency, then

250
00:22:14,040 --> 00:22:18,280
it becomes something that would be difficult to manage.

251
00:22:18,280 --> 00:22:27,920
So we tend to treat each feature as a independent unit, and if you need to modify the feature,

252
00:22:27,920 --> 00:22:31,720
to some extent you fork or you copy and then you make all the changes and become another

253
00:22:31,720 --> 00:22:33,600
like independent features.

254
00:22:33,600 --> 00:22:40,640
I'm curious, what are the, it's just kind of wrapping up the feature marketplace component.

255
00:22:40,640 --> 00:22:49,240
What are the key things that you've learned in kind of producing and supporting this

256
00:22:49,240 --> 00:22:56,320
particular component in terms of the way folks use features and the most important

257
00:22:56,320 --> 00:23:02,480
things to consider when you're thinking about creating some kind of reusability for

258
00:23:02,480 --> 00:23:04,480
features?

259
00:23:04,480 --> 00:23:13,120
I think something that we learn is that a abstraction layer for feature access is very

260
00:23:13,120 --> 00:23:15,480
important.

261
00:23:15,480 --> 00:23:23,360
This abstraction layer gives the user a very simple view of how you can use a feature,

262
00:23:23,360 --> 00:23:27,960
basically you just refer to a user by its name, right?

263
00:23:27,960 --> 00:23:33,760
And then the system basically just automatically figured out where to find the feature and

264
00:23:33,760 --> 00:23:35,200
how to extract feature.

265
00:23:35,200 --> 00:23:41,400
I think this greatly simplify the way that people use feature.

266
00:23:41,400 --> 00:23:46,920
And when you refer to features here, it sounds like you're referring to kind of a higher

267
00:23:46,920 --> 00:23:57,800
level concept that is analogous to a pipeline as opposed to a low level, you know, I've

268
00:23:57,800 --> 00:24:05,640
heard it, you know, sometimes referred to as like a snapshot in time of data, right?

269
00:24:05,640 --> 00:24:10,200
And so you've got these features, you've got features, then you can kind of fast forward

270
00:24:10,200 --> 00:24:16,600
or kind of go back in time and kind of access features at different time points and things

271
00:24:16,600 --> 00:24:17,600
like that.

272
00:24:17,600 --> 00:24:22,200
Are you addressing, does your model address that as well?

273
00:24:22,200 --> 00:24:26,080
I think that that is a great question.

274
00:24:26,080 --> 00:24:34,960
We are currently building that capability, I think yeah, so the ability of looking at

275
00:24:34,960 --> 00:24:40,600
the feature at different time point is a very important functionality.

276
00:24:40,600 --> 00:24:51,400
The next part of the promo that you mentioned was kind of the interface for kind of building

277
00:24:51,400 --> 00:24:52,640
new models.

278
00:24:52,640 --> 00:24:59,000
What does that look like from a user perspective?

279
00:24:59,000 --> 00:25:04,400
Just make sure I understand this correctly, when you say user, you mean the user of machine

280
00:25:04,400 --> 00:25:05,400
learning tooling.

281
00:25:05,400 --> 00:25:06,400
Right.

282
00:25:06,400 --> 00:25:07,400
Right.

283
00:25:07,400 --> 00:25:08,400
The developer.

284
00:25:08,400 --> 00:25:09,400
Yes.

285
00:25:09,400 --> 00:25:10,400
Over the year, right?

286
00:25:10,400 --> 00:25:17,920
We found that there are three types model that are very useful for LinkedIn application.

287
00:25:17,920 --> 00:25:28,440
One is the tree models, usually that is a set of trees like a gradient boosted trees.

288
00:25:28,440 --> 00:25:34,840
And a certain type of model we find to be very useful is like deep learning models, right?

289
00:25:34,840 --> 00:25:37,120
Those are the neural networks.

290
00:25:37,120 --> 00:25:44,240
And a third type of model that we find to be also very useful is a deep personalization

291
00:25:44,240 --> 00:25:45,240
model.

292
00:25:45,240 --> 00:25:52,280
Those models try to learn a set of model parameters for each individual user.

293
00:25:52,280 --> 00:26:01,000
And the tool we provide is a method to allow people to connect or combine different type

294
00:26:01,000 --> 00:26:04,000
of model together, right, into a deck structure.

295
00:26:04,000 --> 00:26:05,000
Right.

296
00:26:05,000 --> 00:26:10,560
So for example, you can first apply tree models to learn the interactions right between

297
00:26:10,560 --> 00:26:16,600
users and for example, jobs, right, if we are talking about like job recommendation.

298
00:26:16,600 --> 00:26:22,920
And then after that, you may have another model neural network, right, that try to learn

299
00:26:22,920 --> 00:26:29,840
the representation of the member and also learn the representation of jobs, right?

300
00:26:29,840 --> 00:26:36,440
Through a neural network, we generate a vector that represent the behavior of the user and

301
00:26:36,440 --> 00:26:41,640
that behavior of job, and we can combine all these together into features into our deep

302
00:26:41,640 --> 00:26:46,680
personalization model and the tooling will building basically give people the flexibility

303
00:26:46,680 --> 00:26:52,680
of peak and choose different type of model and then combine them together into a deck.

304
00:26:52,680 --> 00:26:58,760
And then we do model training to train all these models together is the implication there

305
00:26:58,760 --> 00:27:07,840
that the feature marketplace and these DAGs has some kind of first class notion of the

306
00:27:07,840 --> 00:27:15,200
type of a feature or the type of features inputs or outputs that can be used to validate

307
00:27:15,200 --> 00:27:16,200
this DAG.

308
00:27:16,200 --> 00:27:17,200
Yes.

309
00:27:17,200 --> 00:27:23,720
So at the type of features, what we are currently developing is, well, tensor, right, so

310
00:27:23,720 --> 00:27:27,400
which is pretty much the same as other tools.

311
00:27:27,400 --> 00:27:33,920
However, we want the tensors to also carry semantic meanings, right?

312
00:27:33,920 --> 00:27:44,040
So if you look at say tensor flow, each tensor basically just a numeric array of multiple

313
00:27:44,040 --> 00:27:51,040
dimensions, but to be able to like validate the features and also help the developer to

314
00:27:51,040 --> 00:27:56,320
understand the feature, we want to add semantic to the dimensions, right?

315
00:27:56,320 --> 00:28:02,800
So for example, one dimension of the tensor may represent, for example, skills of the

316
00:28:02,800 --> 00:28:03,800
user.

317
00:28:03,800 --> 00:28:12,360
But another dimension may represent, for example, companies of a job, for example, right?

318
00:28:12,360 --> 00:28:17,680
So we won't be able to capture all these semantic meaning and so that our tensor type, when

319
00:28:17,680 --> 00:28:20,840
user look at the tensor, they can understand the tensor better.

320
00:28:20,840 --> 00:28:24,000
And that can also be used in our later validation.

321
00:28:24,000 --> 00:28:29,120
And now that I'm envisioning this, you know, almost kind of a wizzy wig drag and drop kind

322
00:28:29,120 --> 00:28:34,600
of thing, is that the direction you're headed or using more of a Jupiter notebook or kind

323
00:28:34,600 --> 00:28:38,320
of a code based way of constructing these graphs?

324
00:28:38,320 --> 00:28:44,760
Yeah, we are using code based approach rather than imagine that would be your, yeah, I

325
00:28:44,760 --> 00:28:48,800
think yeah, drag and drop is easy to create some small DAG, right?

326
00:28:48,800 --> 00:28:54,240
But if you manage a large DAG, I think that code based is important.

327
00:28:54,240 --> 00:28:59,800
Are notebooks a paradigm of choice at LinkedIn?

328
00:28:59,800 --> 00:29:10,200
Yes, we use notebook mainly for data analysis and also the first stage of modeling.

329
00:29:10,200 --> 00:29:16,800
Usually people do like portal typing, but I want to try out different ways of specifying

330
00:29:16,800 --> 00:29:20,080
your model, we use notebook for that.

331
00:29:20,080 --> 00:29:26,840
But for our production model training workload, we are not using Jupiter notebook.

332
00:29:26,840 --> 00:29:27,840
Okay.

333
00:29:27,840 --> 00:29:35,280
And so it sounds like there's not particularly any effort or interest in integrating the

334
00:29:35,280 --> 00:29:39,800
notebooks, like automating the notebook to production pipeline.

335
00:29:39,800 --> 00:29:45,400
It's a kind of ad hoc analysis tool and then if someone's going to produce something

336
00:29:45,400 --> 00:29:50,120
that eventually is targeting production, they're starting from, you know, they're starting

337
00:29:50,120 --> 00:29:52,720
in an IDE more often than not.

338
00:29:52,720 --> 00:29:53,720
Yeah, that's correct.

339
00:29:53,720 --> 00:30:01,120
That's the current usage, however, we are also evaluating the potential of using notebook

340
00:30:01,120 --> 00:30:06,000
for manage production training, but that's still in the exploration phase.

341
00:30:06,000 --> 00:30:15,360
So you've got these, you've got models that are developed in notebooks and in code connected

342
00:30:15,360 --> 00:30:18,120
via these graphs.

343
00:30:18,120 --> 00:30:26,280
I think the next component of Fremel is a component that manages the training environments

344
00:30:26,280 --> 00:30:29,320
and training clusters, is that right?

345
00:30:29,320 --> 00:30:42,440
Yes, the way that we manage training is mainly through Hadoop and the Spark.

346
00:30:42,440 --> 00:30:52,440
Our data all stored on Hadoop cluster and we use Spark to coordinate training of different

347
00:30:52,440 --> 00:30:54,960
types of models.

348
00:30:54,960 --> 00:31:03,760
And after training, we need to deploy the models into our production environment.

349
00:31:03,760 --> 00:31:11,240
There's another workflow management to take the model we trained and the data available

350
00:31:11,240 --> 00:31:17,200
on Hadoop, the model available on Hadoop and deploy that into different places in our

351
00:31:17,200 --> 00:31:19,480
production environment.

352
00:31:19,480 --> 00:31:28,680
And I think that is actually quite challenging problem in our system.

353
00:31:28,680 --> 00:31:34,120
The reason is that our models are usually very large, right?

354
00:31:34,120 --> 00:31:41,920
In order to be able to model each individual user's behavior, in the model training process,

355
00:31:41,920 --> 00:31:46,400
we generate model parameters for each individual user, right?

356
00:31:46,400 --> 00:31:53,800
So if you look at the size of a machine learning model, usually that's in the order of tens

357
00:31:53,800 --> 00:31:57,760
of billions of parameters.

358
00:31:57,760 --> 00:32:03,600
That amount of model parameters usually cannot fit into a single container.

359
00:32:03,600 --> 00:32:11,680
So in reality, we need to deploy parts of the model into, for example, key value store,

360
00:32:11,680 --> 00:32:21,960
parts of the model into the scoring service and also part of the model into the index

361
00:32:21,960 --> 00:32:25,160
that provide all the items.

362
00:32:25,160 --> 00:32:31,560
So that's the complexity that we are dealing with for the model deployment tooling.

363
00:32:31,560 --> 00:32:33,560
Okay.

364
00:32:33,560 --> 00:32:42,320
And the model, is there kind of an automated pipeline to kind of overuse the term between

365
00:32:42,320 --> 00:32:45,120
the training tooling and the deployment tooling?

366
00:32:45,120 --> 00:32:46,800
Yeah, that's right.

367
00:32:46,800 --> 00:32:56,840
So we are building a web interface, we call that model explorer.

368
00:32:56,840 --> 00:33:04,520
People can go to this interface and look at all the previously trained model and you can

369
00:33:04,520 --> 00:33:07,560
publish a model from the interface.

370
00:33:07,560 --> 00:33:16,160
And after that, we start the deployment process and we have a centralized release tool which

371
00:33:16,160 --> 00:33:25,480
monitor the deployment process and this process usually start with validating the model in

372
00:33:25,480 --> 00:33:35,080
a test environment and then deploy that in one instance in the production service and

373
00:33:35,080 --> 00:33:42,720
to validate that it works well in that one instance and then deploy to all the instances

374
00:33:42,720 --> 00:33:46,800
right after we validate that it works well for a single instance.

375
00:33:46,800 --> 00:33:50,880
So yeah, so this is a tooling we are currently building.

376
00:33:50,880 --> 00:33:57,360
I remember seeing a while ago, some work that I think was done at LinkedIn around distributed

377
00:33:57,360 --> 00:34:00,360
TensorFlow training on Hadoop.

378
00:34:00,360 --> 00:34:01,360
That's correct.

379
00:34:01,360 --> 00:34:06,360
I think you open sourced a project, is that still in use there?

380
00:34:06,360 --> 00:34:07,360
Yes.

381
00:34:07,360 --> 00:34:08,360
Yeah.

382
00:34:08,360 --> 00:34:16,880
So in when we train deep learning models, we are mainly use TensorFlow and in the past,

383
00:34:16,880 --> 00:34:25,640
we evaluated using Spark to manage a cluster for TensorFlow model training.

384
00:34:25,640 --> 00:34:30,840
However, that framework didn't work very well.

385
00:34:30,840 --> 00:34:42,800
So we develop TensorFlow on Yang, which we call like Tony, T, Stanford TensorFlow, ON,

386
00:34:42,800 --> 00:34:49,400
Y, Yang, which is a Hadoop cluster management system.

387
00:34:49,400 --> 00:34:58,400
And that helped us to effectively manage a cluster of machines that we can run distributed

388
00:34:58,400 --> 00:34:59,400
TensorFlow.

389
00:34:59,400 --> 00:35:09,840
Yesterday, I talked to colleague in Spark, apparently with a new release for Spark, Spark 2.4,

390
00:35:09,840 --> 00:35:11,880
they provide better support for TensorFlow.

391
00:35:11,880 --> 00:35:14,840
I think that's also something interesting to look at.

392
00:35:14,840 --> 00:35:19,880
There are a lot of folks working on different ways to do distributed TensorFlow.

393
00:35:19,880 --> 00:35:24,640
There's the horror vod stuff, there's the distributed TensorFlow that's kind of baked

394
00:35:24,640 --> 00:35:25,640
in the TensorFlow.

395
00:35:25,640 --> 00:35:34,680
It seems like just based on the level of activity that there are either a lot of kind of decisions

396
00:35:34,680 --> 00:35:42,640
that don't work well for everyone or the current solutions aren't, folks aren't very

397
00:35:42,640 --> 00:35:44,600
happy with what's currently available.

398
00:35:44,600 --> 00:35:48,200
Do you have a perspective on that?

399
00:35:48,200 --> 00:35:54,360
I think this is still a quite active area.

400
00:35:54,360 --> 00:36:02,240
I think probably after half a year, we will probably start to see like clear winners.

401
00:36:02,240 --> 00:36:10,240
And in terms of the actual computation, TensorFlow itself provide distributed training.

402
00:36:10,240 --> 00:36:16,840
However, I think most of the effort is on how to set up a cluster for machines such that

403
00:36:16,840 --> 00:36:21,160
we can start TensorFlow distributed training.

404
00:36:21,160 --> 00:36:31,800
And the key is how to enable developer to very easily set up these set of machines.

405
00:36:31,800 --> 00:36:39,000
And the set up mechanism also interact with TensorFlow very well.

406
00:36:39,000 --> 00:36:43,280
We've talked about training and deployment.

407
00:36:43,280 --> 00:36:51,000
And you also have an aspect of ProML that's focused on what you call health assurance.

408
00:36:51,000 --> 00:36:55,000
And I'm taking to be model evaluation and performance assessment.

409
00:36:55,000 --> 00:36:57,760
Can you talk a little bit about that?

410
00:36:57,760 --> 00:36:58,760
Yes.

411
00:36:58,760 --> 00:37:06,760
I think after we deploy the model into production and we start to serve user traffic, it is

412
00:37:06,760 --> 00:37:12,480
very important to make sure that the model continues to run properly.

413
00:37:12,480 --> 00:37:16,560
There are many things that we want to monitor.

414
00:37:16,560 --> 00:37:22,640
One is that the data quality, especially the feature data quality, we want to make sure

415
00:37:22,640 --> 00:37:30,720
that in the online service, the feature data we receive is consistent with the feature

416
00:37:30,720 --> 00:37:34,800
data we observe in our offline training process.

417
00:37:34,800 --> 00:37:43,600
And we also want to continue to continuously monitor our feature data distribution.

418
00:37:43,600 --> 00:37:50,920
So whenever we see a distribution change, then we want to be able to react to that quickly.

419
00:37:50,920 --> 00:37:58,060
Usually, distribution change may indicate some problem in our feature generation or

420
00:37:58,060 --> 00:38:00,760
feature serving systems.

421
00:38:00,760 --> 00:38:09,040
And after people received alert, people need to be able to investigate the problems in

422
00:38:09,040 --> 00:38:11,560
our production system and also the model.

423
00:38:11,560 --> 00:38:18,760
So in that area, it's important that we provide a user interface.

424
00:38:18,760 --> 00:38:24,000
People can go and look at to be able to explain for a particular recommendation.

425
00:38:24,000 --> 00:38:29,240
Why we are making this recommendation, people should be able to see all the features and

426
00:38:29,240 --> 00:38:33,000
also the model decisions based on these features.

427
00:38:33,000 --> 00:38:34,840
How is the distribution even specified?

428
00:38:34,840 --> 00:38:41,320
Are you assuming kind of simple Gaussian types of distributions and just looking at means

429
00:38:41,320 --> 00:38:47,960
and variances, or is it agnostic to the distribution of the actual data?

430
00:38:47,960 --> 00:38:52,240
We start by looking at basic statistics, right?

431
00:38:52,240 --> 00:38:57,960
So like, meaning variance, those are like first, old and second, older moments.

432
00:38:57,960 --> 00:39:01,920
And we also look at some higher older moments.

433
00:39:01,920 --> 00:39:10,120
And we also look at the cumulative distribution and compare the difference between the two.

434
00:39:10,120 --> 00:39:18,520
And you find that there's been a shift in distribution and or a model has degrade and

435
00:39:18,520 --> 00:39:20,560
predictive performance.

436
00:39:20,560 --> 00:39:25,200
Are you doing anything where you're kind of automatically triggering retrains or things

437
00:39:25,200 --> 00:39:26,200
like that?

438
00:39:26,200 --> 00:39:32,760
Or is it more about notifying whoever owns that particular model to take the right action?

439
00:39:32,760 --> 00:39:41,920
So now we start with notification and we are building methods which can also trigger

440
00:39:41,920 --> 00:39:42,920
this retrain.

441
00:39:42,920 --> 00:39:45,280
But we have not yet gone there.

442
00:39:45,280 --> 00:39:48,000
But that's an important next direction.

443
00:39:48,000 --> 00:39:56,760
And are you also doing things like canary models or AB testing, where you're kind of varying

444
00:39:56,760 --> 00:40:02,040
the amount of traffic that you're sending to a model on the fly?

445
00:40:02,040 --> 00:40:07,160
Yeah, AB testing is a very, very important aspect.

446
00:40:07,160 --> 00:40:18,200
And for all of our machine learning development, we use AB testing to verify and quantify

447
00:40:18,200 --> 00:40:22,720
the improvement of a new model that we launched in production.

448
00:40:22,720 --> 00:40:30,000
And a LinkedIn, we have a AB testing platform that is widely used in all the products.

449
00:40:30,000 --> 00:40:36,080
It's not only for machine learning, but for any product feature change, we are also doing

450
00:40:36,080 --> 00:40:42,960
AB testing because we want to make decision based on measurable results.

451
00:40:42,960 --> 00:40:50,320
You're able to use the existing AB testing platform that I imagine was in place for switching

452
00:40:50,320 --> 00:40:56,960
out copy and images and things like that on web pages with models as well.

453
00:40:56,960 --> 00:41:03,920
Did it require a lot of retooling to be able to support the use with the machine learning

454
00:41:03,920 --> 00:41:05,600
models?

455
00:41:05,600 --> 00:41:09,800
The way it works is pretty much the same.

456
00:41:09,800 --> 00:41:18,200
So basically, we have different treatments, and we allocate user traffic to different

457
00:41:18,200 --> 00:41:19,400
treatment.

458
00:41:19,400 --> 00:41:27,600
And each treatment has a key, and that key basically depends on how we interpret that

459
00:41:27,600 --> 00:41:28,600
key.

460
00:41:28,600 --> 00:41:36,280
So for experimenting with product feature, then in our product, we use that key to design,

461
00:41:36,280 --> 00:41:41,080
which experience we want to show to a user.

462
00:41:41,080 --> 00:41:48,080
And for machine learning experiment, we use that key pretty much to decide which model

463
00:41:48,080 --> 00:41:51,480
ID we want to pick up to serve the user.

464
00:41:51,480 --> 00:41:54,960
So at the infrastructure level, it's pretty similar.

465
00:41:54,960 --> 00:42:03,280
One thing that we didn't talk about earlier on in the relating more to model developments

466
00:42:03,280 --> 00:42:08,760
and training to some extent is experiment management.

467
00:42:08,760 --> 00:42:17,280
Does ProML provide a feature set for data scientists to help them manage the various experiments

468
00:42:17,280 --> 00:42:20,200
that they're running?

469
00:42:20,200 --> 00:42:25,440
So this is an area that we will be looking into in the future.

470
00:42:25,440 --> 00:42:35,200
So currently, our AB test platform provides reasonably support for managing the experiment

471
00:42:35,200 --> 00:42:36,600
that we run.

472
00:42:36,600 --> 00:42:39,960
But these are mostly user-facing experiments, right?

473
00:42:39,960 --> 00:42:42,080
Those are running online.

474
00:42:42,080 --> 00:42:51,280
For offline experiments, that is an area that we are currently building to be able to

475
00:42:51,280 --> 00:42:58,080
allow a user to see all the past experiments and enable them to compare different models

476
00:42:58,080 --> 00:43:03,760
in terms of their performance and also the difference in their features and also the difference

477
00:43:03,760 --> 00:43:07,280
in their type of models are using.

478
00:43:07,280 --> 00:43:10,520
This is one of the next steps that we are looking into.

479
00:43:10,520 --> 00:43:16,360
And that ties to hyperparameter optimization, which is something that you mentioned earlier.

480
00:43:16,360 --> 00:43:21,520
It sounds like you do offer via the platform some capability there.

481
00:43:21,520 --> 00:43:23,320
That's correct.

482
00:43:23,320 --> 00:43:29,320
So to train a machine learning model, usually we need to set up different like tuning

483
00:43:29,320 --> 00:43:30,320
parameters.

484
00:43:30,320 --> 00:43:37,000
For example, if you do a regression model, then usually you have regularization terms,

485
00:43:37,000 --> 00:43:41,840
then you need to specify the regularization weights.

486
00:43:41,840 --> 00:43:49,040
And also when you do neural net models, there are also many hyperparameters you need to

487
00:43:49,040 --> 00:43:50,520
decide.

488
00:43:50,520 --> 00:44:00,240
What we do is that we develop a Bayesian optimization method that sequentially try out different

489
00:44:00,240 --> 00:44:07,960
hyperparameter settings and based on that, where we learn the distribution of a potential

490
00:44:07,960 --> 00:44:12,960
best parameter setting and then based on that, we pick the next step.

491
00:44:12,960 --> 00:44:19,960
And through this iterative process, we find that better and better parameter settings.

492
00:44:19,960 --> 00:44:27,320
Sounds like ProML as one would expect is kind of in constant evolution.

493
00:44:27,320 --> 00:44:37,040
That's been the experience in bringing these features to the users of the system, the

494
00:44:37,040 --> 00:44:38,040
developers.

495
00:44:38,040 --> 00:44:44,320
Do you have, I'm curious really about observations in terms of, you know, for folks that have

496
00:44:44,320 --> 00:44:50,280
some community of machine learning engineers or data scientists that they're supporting.

497
00:44:50,280 --> 00:44:57,880
And it wants to bring platform features to them to make them more efficient.

498
00:44:57,880 --> 00:45:04,720
Do you have any advice as to, you know, where to start or how to proceed or the best way

499
00:45:04,720 --> 00:45:10,200
to work with that community to give them the tools that they really need?

500
00:45:10,200 --> 00:45:18,680
I think when for people to start using machine learning, I would recommend to start from

501
00:45:18,680 --> 00:45:24,240
simple methods and then gradually build complexity.

502
00:45:24,240 --> 00:45:34,240
And I think currently many cloud platforms provide capability of building and managing simple

503
00:45:34,240 --> 00:45:35,760
models.

504
00:45:35,760 --> 00:45:43,120
By simple, I mean that the model size is not very large and also the size of data is

505
00:45:43,120 --> 00:45:45,640
not very large, right?

506
00:45:45,640 --> 00:45:53,520
And then gradually you can add more complexity and by and for those simple models, I think

507
00:45:53,520 --> 00:45:58,520
Jupiter notebook is a perfect place to get started.

508
00:45:58,520 --> 00:46:05,480
And then when you add more complexity, then you may need to train your model on a large

509
00:46:05,480 --> 00:46:07,200
amount of data.

510
00:46:07,200 --> 00:46:13,920
And for that, I would probably recommend the listeners to take a look at the photon ML,

511
00:46:13,920 --> 00:46:21,160
which is a LinkedIn open source project, which can enable you to train your models on a

512
00:46:21,160 --> 00:46:30,040
large amount of data using Spark and do very good and deep personalization.

513
00:46:30,040 --> 00:46:38,080
And then after that, when you have those complex and large models, you need to start to

514
00:46:38,080 --> 00:46:42,480
worry about how to deploy model into your production systems.

515
00:46:42,480 --> 00:46:46,040
I think currently that is still a challenge.

516
00:46:46,040 --> 00:46:53,760
I think after we solve the problem, and we can share more of the fact that things.

517
00:46:53,760 --> 00:46:57,560
Watch this space is definitely a quickly evolving area.

518
00:46:57,560 --> 00:46:58,560
That's right.

519
00:46:58,560 --> 00:47:08,720
So you started this and describing a goal of increasing developer productivity and in fact

520
00:47:08,720 --> 00:47:12,560
ProML stands for productive machine learning.

521
00:47:12,560 --> 00:47:18,920
Have you kind of benchmarked productivity gains of the developer community as they've

522
00:47:18,920 --> 00:47:21,680
used this platform?

523
00:47:21,680 --> 00:47:33,440
We start to measure productivity in our LinkedIn machine learning applications.

524
00:47:33,440 --> 00:47:43,040
We just get started and what we look at is the number of successful experiments per engineer.

525
00:47:43,040 --> 00:47:52,560
So we want to improve or increase the value that a single machine learning engineer can

526
00:47:52,560 --> 00:47:53,560
produce.

527
00:47:53,560 --> 00:47:59,200
And the way that we measure the value is by looking at how many successful experiments

528
00:47:59,200 --> 00:48:04,200
that a single engineer can do by using the tooling.

529
00:48:04,200 --> 00:48:10,720
I think over time, we will be able to quantify the gain of the tooling we are building.

530
00:48:10,720 --> 00:48:14,200
And this is still a process that we are going through.

531
00:48:14,200 --> 00:48:15,200
Awesome.

532
00:48:15,200 --> 00:48:19,720
Well, Bichong, thank you so much for taking the time to chat with us about what you're

533
00:48:19,720 --> 00:48:20,720
up to.

534
00:48:20,720 --> 00:48:24,200
It is really interesting stuff and I learned a ton.

535
00:48:24,200 --> 00:48:25,200
Cool.

536
00:48:25,200 --> 00:48:26,200
Thank you so much.

537
00:48:26,200 --> 00:48:31,320
All right, everyone, that's our show for today.

538
00:48:31,320 --> 00:48:36,360
For more information on Bichong or any of the topics covered in this episode, visit

539
00:48:36,360 --> 00:48:40,480
twimmalai.com slash talk slash 200.

540
00:48:40,480 --> 00:48:46,520
To learn more about our AI platform series or to download our eBooks, visit twimmalai.com

541
00:48:46,520 --> 00:48:48,760
slash AI platforms.

542
00:48:48,760 --> 00:48:58,760
As always, thanks so much for listening and catch you next time.

