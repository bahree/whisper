WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.600
I'm your host, Sam Charrington.

00:31.600 --> 00:36.680
In this episode of our deep learning endoba series, we're joined by Nila Murray, senior research

00:36.680 --> 00:41.240
scientist and group lead in the computer vision group at Naver Labs Europe.

00:41.240 --> 00:44.840
Nila presented at the endoba on computer vision.

00:44.840 --> 00:49.880
In this discussion, we explore her work on visual attention, including why visual attention

00:49.880 --> 00:54.320
is important and the trajectory of work in the field over time.

00:54.320 --> 00:59.520
We also discuss her paper Generalize Max Pooling and her recent research interests in learning

00:59.520 --> 01:02.880
representations with deep learning.

01:02.880 --> 01:07.000
Before we jump in, I'd like to send a big shout out to our friends at Google AI for their

01:07.000 --> 01:10.800
support of the podcast and their sponsorship of this series.

01:10.800 --> 01:16.920
Google AI recently opened up applications for their 2019 residency program.

01:16.920 --> 01:21.720
The Google AI residency is a one year machine learning research training program, with the

01:21.720 --> 01:26.760
goal of helping individuals become successful machine learning researchers.

01:26.760 --> 01:31.280
The program seeks residents from a very diverse set of educational and professional backgrounds

01:31.280 --> 01:35.640
from all over the world, so if you think that this is something that interests you, you

01:35.640 --> 01:37.880
should definitely apply.

01:37.880 --> 01:42.640
Find out more about the program at g.co slash AI residency.

01:42.640 --> 01:47.720
And now on to the show.

01:47.720 --> 01:48.720
All right, everyone.

01:48.720 --> 01:50.520
I am on the line with Nila Murray.

01:50.520 --> 01:55.960
Nila is a senior research scientist and group lead in the computer vision group at Naver

01:55.960 --> 01:56.960
Labs Europe.

01:56.960 --> 01:59.560
Nila, welcome to this week in machine learning and AI.

01:59.560 --> 02:00.560
Thanks so much.

02:00.560 --> 02:01.560
I'm happy to be here.

02:01.560 --> 02:04.080
I'm happy to have you on as well.

02:04.080 --> 02:09.960
And before we jump into the kind of heart of the conversation, I'd like to have the audience

02:09.960 --> 02:14.120
get to know you a little bit, you did your PhD in Barcelona, Spain.

02:14.120 --> 02:16.040
Tell us about your focus there.

02:16.040 --> 02:17.040
Sure.

02:17.040 --> 02:22.280
So I did my PhD at the University of Tata AutÃ³noma de Barcelona, so specifically in the

02:22.280 --> 02:26.120
computer vision center in Barcelona.

02:26.120 --> 02:30.960
And so the work I did there was very focused on subjective vision.

02:30.960 --> 02:36.560
So this is the problem of being able to model subjective properties of human vision.

02:36.560 --> 02:41.800
So that's things like, in particular, was focused on visual attention, in particular,

02:41.800 --> 02:44.680
bottom-up attention or what we might call saliency.

02:44.680 --> 02:48.920
So this is really understanding if you're given some kind of, if an individual is presented

02:48.920 --> 02:54.760
with a visual stimulus, let's see an image or a video, how can we model which regions

02:54.760 --> 02:57.880
of that visual stimulus would attract the user's attention.

02:57.880 --> 03:01.760
And then we can have various degrees of attention that we can predict.

03:01.760 --> 03:04.080
So that was one focus of my research through my PhD.

03:04.080 --> 03:06.440
Another focus was on visual aesthetics.

03:06.440 --> 03:11.000
This is another, let's say, subjective visual experience where you are trying to model

03:11.000 --> 03:14.520
or at least trying to understand or predict, you know, to what degree with a specific visual

03:14.520 --> 03:20.040
stimulus, let's say, an image or a video be considered visually appealing to someone.

03:20.040 --> 03:23.400
And so you can consider different ways of approaching that.

03:23.400 --> 03:27.960
There are some that are, let's say, some approaches that are a bit more in the computational

03:27.960 --> 03:33.120
neuroscience perspective, really taking a sort of biologically inspired approach.

03:33.120 --> 03:38.200
And I did investigate that to some degree, and they're also much more data-driven approaches.

03:38.200 --> 03:42.360
So things that would be, you know, using machine learning techniques.

03:42.360 --> 03:46.880
So I investigated both aspects through my PhD.

03:46.880 --> 03:52.840
And during your PhD, you did a stint with Xerox Research Lab in Europe.

03:52.840 --> 03:58.680
You went there after your PhD, and without leaving, you ended up at Naver Lab's Europe.

03:58.680 --> 04:00.560
What's tell us about that whole story?

04:00.560 --> 04:03.600
Sure, so it's very interesting time.

04:03.600 --> 04:09.760
So in fact, I was a visiting researcher at Xerox Research Center Europe, XRC, for those

04:09.760 --> 04:12.040
in the know, during my PhD.

04:12.040 --> 04:16.000
So actually, I was visiting them and collaborating with them as a PhD student.

04:16.000 --> 04:21.480
So after my PhD, you know, I had a great time at Xerox and so I decided to stay on as

04:21.480 --> 04:23.120
a research scientist.

04:23.120 --> 04:29.840
And eventually, I started leading the computer vision team at Xerox and at a certain

04:29.840 --> 04:38.200
point last year, Xerox decided to transition, and you know, we were looking for acquirers

04:38.200 --> 04:39.600
for the lab.

04:39.600 --> 04:45.040
And so, Naver Labs, and so I can get in a little bit into what, you know, who are we as

04:45.040 --> 04:46.040
Naver Labs?

04:46.040 --> 04:50.320
Naver Labs acquired us eventually, acquired XRC.

04:50.320 --> 04:55.480
And so now XRC is now known as NLE, or Naver Labs Europe.

04:55.480 --> 04:57.640
And so the institute is the same, the people are the same.

04:57.640 --> 05:02.840
But now we're under the umbrella of our new home, which is Naver Labs.

05:02.840 --> 05:04.400
And Naver Labs does what?

05:04.400 --> 05:09.800
Okay, so this is like an unintended, unintended story.

05:09.800 --> 05:12.160
So let me start off with Naver Corporation.

05:12.160 --> 05:16.200
So our parent company is Naver Corporation and this is a South Korean Internet technology

05:16.200 --> 05:17.200
company.

05:17.200 --> 05:21.320
We're a very dominant company in South Korea and then for different services in other parts

05:21.320 --> 05:24.800
of South and Southeast Asia and East Asia.

05:24.800 --> 05:28.320
So we're very well known, we're very dominant in search, for example, in web search and

05:28.320 --> 05:31.960
mobile search in South Korea and then we have many other services.

05:31.960 --> 05:38.120
We have a lot of internet and web services for things like for translation, for navigation,

05:38.120 --> 05:39.640
for maps, for blogging.

05:39.640 --> 05:44.600
We have more than 100 services that we provide over the web of the internet over mobile

05:44.600 --> 05:45.600
phones.

05:45.600 --> 05:50.280
We also, we own line corporation, for example, which is a very popular messaging app as

05:50.280 --> 05:51.280
well.

05:51.280 --> 05:52.800
So this is Naver Corporation.

05:52.800 --> 05:58.440
And so Naver Labs is actually a spin-off of Naver Corporation.

05:58.440 --> 06:05.000
And so Naver Labs, what we are, is we're a company with a vision of ambient intelligence.

06:05.000 --> 06:11.240
So what this means is we are interested in having services that are intelligent and that

06:11.240 --> 06:16.200
take context into account when it's deciding how to best provide some kind of service.

06:16.200 --> 06:20.280
So that may mean understanding where you are, understanding what you're doing and using

06:20.280 --> 06:24.360
that to provide you different services related to, for example, mobility or entertainment

06:24.360 --> 06:26.760
or communications, things like that.

06:26.760 --> 06:28.440
So that's the vision of Naver Labs.

06:28.440 --> 06:33.760
And as a result of that, we do a lot of research on things like of computer vision, natural

06:33.760 --> 06:38.040
language processing, you know, recommendation systems, a lot of machine learning tasks in

06:38.040 --> 06:44.320
general and also applications like robotics, autonomous driving and quite a few other things.

06:44.320 --> 06:49.920
You recently returned from the deep learning in Daba and in fact this interview will be

06:49.920 --> 06:56.240
published as part of our deep learning in Daba series where you presented an overview

06:56.240 --> 06:59.440
of tutorial on CNNs.

06:59.440 --> 07:02.840
I'm curious about your experience at the in Daba.

07:02.840 --> 07:09.600
Had you been to the in Daba event previously and tell me a little bit about your experience

07:09.600 --> 07:10.600
there?

07:10.600 --> 07:11.600
Sure.

07:11.600 --> 07:15.760
So I hadn't been previously as far as I understand the first, this is the second edition

07:15.760 --> 07:19.200
of the deep learning in Daba and I wasn't there last year.

07:19.200 --> 07:23.840
I was really happy to be invited to give to give this overview of convolutional models

07:23.840 --> 07:28.640
and I have to say it was really a great experience, you know, on all dimensions.

07:28.640 --> 07:30.800
In terms of the organization, it was great.

07:30.800 --> 07:36.640
In terms of the group of students and participants that the organizers put together, they were

07:36.640 --> 07:40.360
really a very engaged, motivated audience.

07:40.360 --> 07:43.600
I was a bit, you never know when you're giving a talk how engaged the audience is going

07:43.600 --> 07:50.320
to be, you know, we got so many questions during the talk, which is always great after

07:50.320 --> 07:51.320
the talk.

07:51.320 --> 07:55.600
A lot of questions as well, a lot of students coming up to me afterwards saying, oh, you

07:55.600 --> 08:00.040
know, I saw this, you spoke about this specific kind of flavor of a confnet.

08:00.040 --> 08:01.560
Could do you think it could be use in my case?

08:01.560 --> 08:02.880
Here's what I'm trying to do.

08:02.880 --> 08:06.360
So people are really, really willing to explain to describe their problems, to look for advice,

08:06.360 --> 08:08.520
to ask questions, to share knowledge.

08:08.520 --> 08:12.600
So I thought the general spirits of the DL and Daba was great.

08:12.600 --> 08:16.760
It was really, I think, in the best spirit of conferences, in the sense that people

08:16.760 --> 08:21.400
are really there to share knowledge and to find collaborations and things like that.

08:21.400 --> 08:24.120
So it was a very nice spirit.

08:24.120 --> 08:26.600
I think I also learned a lot listening to some of the other talks.

08:26.600 --> 08:32.600
They were great talks by people like Katja Hoffman and David Silver and others, many others,

08:32.600 --> 08:34.840
where I learned quite a bit of stuff myself too.

08:34.840 --> 08:37.280
So in all knowledge, it was really wonderful.

08:37.280 --> 08:39.080
Oh, fantastic.

08:39.080 --> 08:47.160
And so your recent work has been focused on areas like visual attention and learning different

08:47.160 --> 08:48.880
representations for visual search.

08:48.880 --> 08:54.280
Let's dive into some of your recent research.

08:54.280 --> 08:55.280
Visual attention.

08:55.280 --> 08:59.760
When we talk about visual attention, is it related at all to, you know, we talk about

08:59.760 --> 09:02.120
attention within models?

09:02.120 --> 09:03.440
What are the connections between those?

09:03.440 --> 09:04.960
Or is it just a naming collision?

09:04.960 --> 09:05.960
Sure.

09:05.960 --> 09:07.760
So that's a great question.

09:07.760 --> 09:11.880
I would say there's an intersection, but it's not a complete overlap.

09:11.880 --> 09:17.240
So visual attention has a very specific meaning in the neuroscience community and in other

09:17.240 --> 09:18.240
communities, right?

09:18.240 --> 09:24.440
It's not something that computer vision people came up with.

09:24.440 --> 09:25.920
But there is some overlap.

09:25.920 --> 09:30.480
Basically, when I say visual attention in the context of my research, what I really mean

09:30.480 --> 09:38.760
is human attention, let's say that's given, or that's a result of the human visual system.

09:38.760 --> 09:43.680
And you can think of even higher up visual processes and cognitive processes.

09:43.680 --> 09:48.360
The idea being that, so for example, to give you a typical setup, in terms of what we're

09:48.360 --> 09:52.120
trying to model, let's say, for example, if you sit a human, you sit somebody in front

09:52.120 --> 09:58.520
of like a computer screen, you show them an image, and then you have an eye tracker.

09:58.520 --> 10:05.200
And the eye tracker is tracking exactly where fixations are landing on that image, right?

10:05.200 --> 10:10.320
And the goal of a visual attention model will be to predict that, to be able to say, given

10:10.320 --> 10:14.240
this image, I think that, let's say the average person, because of course, there's a lot of

10:14.240 --> 10:19.080
subjectivity that goes into this, but the average person would pay more attention to this

10:19.080 --> 10:20.080
part or to that part.

10:20.080 --> 10:23.600
Or I think that part's going to gather a fair bit of attention.

10:23.600 --> 10:26.600
Or this is a degree to which I think it would have that amount of attention.

10:26.600 --> 10:30.360
So that's what I mean when I say visual attention, and that's the type of research I did.

10:30.360 --> 10:36.360
But in a lot of CNNs now, and let's say general deep learning models, it's used in a very

10:36.360 --> 10:40.160
similar sense, the idea being that, but it's not necessarily human driven, right?

10:40.160 --> 10:45.040
It's basically saying, what does the algorithm, what does a model basically think needs to

10:45.040 --> 10:49.360
be attended to in whatever stimulus you're talking about?

10:49.360 --> 10:53.960
And that can be a visual stimulus, it can be textual data, it can mean many other things.

10:53.960 --> 10:58.600
So it's not necessarily human driven, but the concept is similar in the sense that there's

10:58.600 --> 11:00.720
a lot of information.

11:00.720 --> 11:03.760
What information is relevant for me to make whatever decision I'm trying to make?

11:03.760 --> 11:06.040
Why do we want to study visual attention?

11:06.040 --> 11:12.320
Yeah, you could step back and think about it as more like a bug than a feature or limitation

11:12.320 --> 11:13.320
feature, right?

11:13.320 --> 11:20.320
It's like we, as humans, have this limited ability to focus due to a variety of kind of

11:20.320 --> 11:26.760
neurophysical limitations the way we're wired, but a computer could do more, right?

11:26.760 --> 11:30.880
Why do we need to worry about focusing on a specific part of an image?

11:30.880 --> 11:36.560
You could say that, but I actually think that visual attention is kind of a miracle.

11:36.560 --> 11:42.280
I wouldn't say it's a miracle, but it's actually a very wonderful feature of the human, let's

11:42.280 --> 11:43.640
say human cognition.

11:43.640 --> 11:47.680
In the sense that there is a vast amount of information that we're taking in every time

11:47.680 --> 11:51.080
we just look around us, right, every time we look around us, every time we see.

11:51.080 --> 11:55.680
If you think of the amount of data that you need to store it to just store like an hour

11:55.680 --> 12:03.240
of video and just imagine that we're just like having this be input into our eyes constantly

12:03.240 --> 12:07.160
as when we're awake and when we're looking around, there's a lot of information and it's

12:07.160 --> 12:11.160
not rather than think that, okay, it'd be great if we could use all of this, frankly,

12:11.160 --> 12:16.080
I mean, it's very smart for our visual system to say, a lot of this is not even necessary.

12:16.080 --> 12:17.080
Why would I waste my time?

12:17.080 --> 12:22.080
Why would I have waste my energy of my brain to process this stuff that's not even needed

12:22.080 --> 12:24.280
for whatever time of task I'm trying to do?

12:24.280 --> 12:29.240
The human brain is very computationally efficient and that's something that we struggle a lot

12:29.240 --> 12:33.200
with in computation, especially in deep learning right now.

12:33.200 --> 12:36.040
I think it's actually something very fascinating and something that we would love to be able

12:36.040 --> 12:43.480
to replicate in, let's say, machines and computer systems as well.

12:43.480 --> 12:44.480
That's just to start off with.

12:44.480 --> 12:48.040
I really think, I find it really fascinating the ability we have to be able to really

12:48.040 --> 12:50.640
attend to things that matter.

12:50.640 --> 12:53.960
So that's one reason why I think it's interesting, just because if we could replicate that, I

12:53.960 --> 12:57.520
think it would serve as a lot of computational power and many other things, right?

12:57.520 --> 13:03.520
We know that right now, for example, there's a lot of issues with environmental concerns,

13:03.520 --> 13:04.520
for example.

13:04.520 --> 13:07.360
How do you cool all these, like, GPUs that we're all using and things like this?

13:07.360 --> 13:11.560
So if we could manage to replicate that, that'd actually be pretty cool.

13:11.560 --> 13:15.760
And then, so another reason we're interested in it is just for purely, let's say, scientific

13:15.760 --> 13:18.280
purpose to really understand how the brain works.

13:18.280 --> 13:19.280
So that's one thing.

13:19.280 --> 13:22.080
And actually, a lot of this research came out of the neuroscience community, right?

13:22.080 --> 13:24.480
Like, people who really want to understand how does the brain work.

13:24.480 --> 13:26.080
That's just a fundamental question.

13:26.080 --> 13:27.720
And attention is a big part of that.

13:27.720 --> 13:31.480
Not just visual attention, but also attention to audio signals.

13:31.480 --> 13:34.840
And then also just from for computational reasons, I just gave you a few, right?

13:34.840 --> 13:42.720
It's really good for compression purposes and for just general computational efficiency.

13:42.720 --> 13:47.880
Can you scope out the landscape of visual attention?

13:47.880 --> 13:52.400
What's the research trajectory been in this field over time?

13:52.400 --> 13:57.720
It sounds like it's been something we've been looking at for a long time from the perspective

13:57.720 --> 13:59.640
of multiple disciplines.

13:59.640 --> 14:06.160
How is our understanding of how to replicate visual attention and machines evolved over

14:06.160 --> 14:07.160
time?

14:07.160 --> 14:08.160
Sure.

14:08.160 --> 14:11.160
So, yeah, I can try to take you through a very brief overview.

14:11.160 --> 14:16.880
So visual attention, let's say computational models have been around for many, many decades.

14:16.880 --> 14:20.160
There are some very seminal ones.

14:20.160 --> 14:25.200
So for example, there's one by Triesman et al, which is related to what we call feature

14:25.200 --> 14:26.640
integration theory.

14:26.640 --> 14:31.240
And then from there, there have been some works by professors such as Laurent Eti and a

14:31.240 --> 14:36.760
lot of his students who've worked on things like really taking an image and trying to decompose

14:36.760 --> 14:41.120
that image into some sort of compressed representation.

14:41.120 --> 14:44.560
The idea being that if you have an image, a lot of, there's a lot of redundancy in images,

14:44.560 --> 14:45.560
right?

14:45.560 --> 14:46.560
We all know that.

14:46.560 --> 14:50.120
So images can be very highly compressed and anybody who's used, you know, JPEP compression

14:50.120 --> 14:52.920
or any other types of compression techniques know that.

14:52.920 --> 14:59.080
And the idea is that there's been a long history of work thinking about the fact that to understand

14:59.080 --> 15:03.760
visual attention, we have to think about what can be compressed and what cannot be in order

15:03.760 --> 15:06.080
to retain the same information in the image.

15:06.080 --> 15:11.480
So that's been like a big theme throughout a lot of visual attention research.

15:11.480 --> 15:17.520
In early days, let's say a lot of things was very, very much focused on trying to come

15:17.520 --> 15:22.360
up with, let's say from first principles, first principles of neuroscience about, for

15:22.360 --> 15:26.520
example, how the brain works, how the visual cortex works, how the primary cortex works,

15:26.520 --> 15:30.440
et cetera, trying to, from that, come up with a model.

15:30.440 --> 15:34.040
More recently, people have gone into very much data-driven approaches.

15:34.040 --> 15:39.040
So by data-driven, I mean things like, you know, you have some sort of data set that

15:39.040 --> 15:40.040
you collect.

15:40.040 --> 15:46.080
So a typical data set that is used in this field is a data set of images and corresponding

15:46.080 --> 15:50.160
fixations from different, different individuals, different viewers.

15:50.160 --> 15:54.280
The data-driven approach would basically say, okay, I have some sort of model, and this

15:54.280 --> 15:58.280
model can be a deep learning model, it can be some other type of model, and I want to

15:58.280 --> 16:04.600
optimize this model such that it's able to predict as best as possible the these fixations.

16:04.600 --> 16:10.800
So basically to give some sort of, for example, a probability score for given pixels, what's

16:10.800 --> 16:14.120
a probability that a fixation would land on that pixel?

16:14.120 --> 16:20.520
So a lot of work on this field has used things like image decomposition methods, for example,

16:20.520 --> 16:23.880
using wavelet theory, using wavelet decomposition.

16:23.880 --> 16:27.800
More recently, it's gone, you know, a lot of the research has gone into deep learning.

16:27.800 --> 16:33.320
So for example, some of the recent research that I did used a very generic, let's say,

16:33.320 --> 16:36.240
visual backbone, you can say, visual front end.

16:36.240 --> 16:42.480
So for example, a ResNet model, and then for example, train the ResNet model architecture

16:42.480 --> 16:46.440
for the final layer that gives you a prediction on a pixel basis, saying, okay, given this

16:46.440 --> 16:49.920
image, what's the probability of this pixel being fixated upon?

16:49.920 --> 16:57.320
And so your specific work in this field, what were some of the papers that you've done

16:57.320 --> 16:58.640
in this area?

16:58.640 --> 17:03.000
So the first, the first work I did on this, so this was before the deep learning era,

17:03.000 --> 17:10.360
if we can put it that way, was really based on using a psychophysical model, a vision.

17:10.360 --> 17:15.720
So this model was really a model that was developed initially to predict color perception.

17:15.720 --> 17:17.880
So how do we perceive color in images?

17:17.880 --> 17:22.920
And then we adapted this model to predict visual attention.

17:22.920 --> 17:25.400
More specifically saliency.

17:25.400 --> 17:29.160
So maybe before I get into this, I can give a little bit of a distinction between the

17:29.160 --> 17:30.240
two things.

17:30.240 --> 17:35.680
So when we talk about visual attention, this is something that's extremely complex,

17:35.680 --> 17:40.840
let's say process in cognition, it involves many, let's say, what we might call low-level

17:40.840 --> 17:44.080
cues and also many top-down cues.

17:44.080 --> 17:46.760
By that, what I mean is that typically when you're looking around you and you're paying

17:46.760 --> 17:49.600
attention to things, you have some sort of purpose in mind.

17:49.600 --> 17:52.720
So for example, you might be reading or you might be looking at a movie or you might

17:52.720 --> 17:57.120
be searching for something on your desk, and that impacts a lot where you look.

17:57.120 --> 17:59.320
So this is why it's very subjective.

17:59.320 --> 18:05.080
So there's a component of attention, though, that's been traditionally called saliency,

18:05.080 --> 18:06.840
and this is something that people call more bottom-up.

18:06.840 --> 18:12.680
So what that means is it's something that's more related to things like textures, like

18:12.680 --> 18:17.640
low-level textures and colors and things that aren't really related to the task at hand.

18:17.640 --> 18:21.400
So what I may mean by that, for example, is even though you're looking at your desk and

18:21.400 --> 18:25.600
you're trying to find something, if there are certain things that are just, let's say,

18:25.600 --> 18:30.320
fundamentally salient for lack of a better word in your desk.

18:30.320 --> 18:34.320
So there are many patterns that have been found that have this property.

18:34.320 --> 18:35.680
So this was more what I was focused on.

18:35.680 --> 18:38.960
This aspect of really a bottom-up saliency or low-level saliency.

18:38.960 --> 18:46.640
Is it fair to draw this distinction along the lines of bottom-up or saliency is specific

18:46.640 --> 18:54.280
to the content of, let's say, an image, whereas top-down is more contextual or related to

18:54.280 --> 18:56.920
the goal of the observer, or is that too simple?

18:56.920 --> 19:01.240
Yeah, I think that's a fair distinction to make, exactly.

19:01.240 --> 19:06.280
So we would say that, and this is why I think if you're trying to model attention, many

19:06.280 --> 19:09.760
people have started off trying to model what we call bottom-up, bottom-up or low-level

19:09.760 --> 19:15.240
attention, because that's the sort of thing that's really, it's less task-dependent,

19:15.240 --> 19:16.240
right?

19:16.240 --> 19:19.480
It really sort of depends on just the fundamentals of the visual stimulus that you have.

19:19.480 --> 19:24.000
And so that weighs a bit more, there's a bit more continuity of that coherence in between

19:24.000 --> 19:25.000
different viewers.

19:25.000 --> 19:28.000
When you start to get into things like looking at different tasks, it can be very subjective

19:28.000 --> 19:29.320
and very different.

19:29.320 --> 19:34.160
So your specific research was taking a kind of a neuro-physical model, is that how you

19:34.160 --> 19:35.160
described it?

19:35.160 --> 19:43.280
So that was my initial work on this, which took a psychophysical model, and it attempted

19:43.280 --> 19:50.640
to modify it to better predict, let's say, visual saliency and in particular to predict

19:50.640 --> 19:53.560
visual fixations, so eye fixations.

19:53.560 --> 19:59.120
So what that meant is that you take a data set with these fixations that were recorded

19:59.120 --> 20:03.320
by different observers, and you try to really replicate that with your model.

20:03.320 --> 20:09.320
So later on, in more recent years, I've looked at a more data-driven approach with some

20:09.320 --> 20:10.840
learning involved.

20:10.840 --> 20:17.680
So in this case, what we did was, we really focused on, so this is work I did with students

20:17.680 --> 20:21.400
at MindSomniaJetly and also a colleague of mine, Noravig.

20:21.400 --> 20:27.400
And so for this work, we focused on really trying to understand what kind of objective functions

20:27.400 --> 20:31.760
would be appropriate to learn, to learn a model of saliency.

20:31.760 --> 20:37.200
And so we modeled saliency as a probability distribution.

20:37.200 --> 20:41.920
So we said, okay, we want to really predict the probability of fixations, we consider,

20:41.920 --> 20:47.440
we consider all our pixels in our image to be potential fixations, and we try to figure

20:47.440 --> 20:51.920
okay, what would be like the best objective function to learn to train this model.

20:51.920 --> 20:56.240
And so we consider different ones, we propose some new ones, basically, that we're trying

20:56.240 --> 20:58.480
to really capture this property of probability.

20:58.480 --> 21:03.360
And so what were the field of objective functions that you considered?

21:03.360 --> 21:08.880
So very much those related to capturing differences between, or let's say, to quantify differences

21:08.880 --> 21:10.680
between probability distributions.

21:10.680 --> 21:15.160
So they're, for example, we try things like, like, kale divergences, gents and shanen

21:15.160 --> 21:19.760
differences, kite divergences, and then many others.

21:19.760 --> 21:26.560
So then, basically, we, we, finally, we proposed like an objective function that's basically

21:26.560 --> 21:31.320
a combination of a soft smack function, which will allow you to have a valid probability

21:31.320 --> 21:37.240
distribution, then to apply different types of divergences, probability divergences on

21:37.240 --> 21:38.240
top of that.

21:38.240 --> 21:40.440
And we found that that worked that worked quite well.

21:40.440 --> 21:44.440
And in fact, we found that the bathe charia distance worked quite well.

21:44.440 --> 21:45.880
The what distance?

21:45.880 --> 21:50.160
So there's a, there's a, it's called, it's called a bathe charia distance.

21:50.160 --> 21:51.160
It's quite well known.

21:51.160 --> 21:52.760
It's been used in many, many fields.

21:52.760 --> 21:56.200
I think it could be used more, actually, than it is right now.

21:56.200 --> 21:59.960
But actually, the kale diverge is actually works also very well.

21:59.960 --> 22:02.920
We compared several ones, and then we found that these two were pretty good.

22:02.920 --> 22:10.200
So in the case of, I'm trying to map this to, like, simple Gaussian type distributions,

22:10.200 --> 22:17.200
where translating from one to the other is related to kind of the mean and the variance,

22:17.200 --> 22:24.240
do those correspond to correspond one to one, two terms, and these different, these different

22:24.240 --> 22:25.240
functions?

22:25.240 --> 22:26.240
No.

22:26.240 --> 22:27.240
So these are very generic.

22:27.240 --> 22:32.160
So basically, so, so these, they're not necessarily some sort of generative model.

22:32.160 --> 22:38.360
For example, it's really, you assume that you have for every, every value you have in

22:38.360 --> 22:42.640
your distribution, you're assuming that, you know, you know, what the value is, right?

22:42.640 --> 22:46.720
So you have, like, probability of X, one probability of X2, et cetera.

22:46.720 --> 22:50.960
So that can, that can be modeled, for example, with a GMM or with something else.

22:50.960 --> 22:53.800
But this is sort of agnostic to that, these types of distances.

22:53.800 --> 22:58.160
They're saying once you have some sort of probability distribution, this is, this distance

22:58.160 --> 23:02.160
is going to compute the difference between the two, but they can be modeled differently.

23:02.160 --> 23:03.160
All right.

23:03.160 --> 23:07.120
And so how do you use this ability to compute the distance between two distributions

23:07.120 --> 23:12.640
to help you figure out attention?

23:12.640 --> 23:18.320
So what we start off with, so our ground truth can be converted into a probability distribution.

23:18.320 --> 23:19.320
This is what we did.

23:19.320 --> 23:22.320
And when we're not the first people to do this, but this is something that's, that's, that's

23:22.320 --> 23:24.520
very, that has been done before.

23:24.520 --> 23:27.760
So for example, you start off with a set of fixations.

23:27.760 --> 23:34.360
So you can consider it, you can consider that you have an image and you know the, you

23:34.360 --> 23:39.040
know the X-Y coordinates of where somebody fixated on, on that image.

23:39.040 --> 23:47.440
Is your data set consist of one image and a large number of captured fixations from different

23:47.440 --> 23:52.920
observers based against that same image, or do you have a bunch of images each with their

23:52.920 --> 23:56.000
corresponding fixations?

23:56.000 --> 23:57.000
So both.

23:57.000 --> 24:01.680
We have multiple images, ideally, in the beginning we had very, we have very small data sets, but

24:01.680 --> 24:04.600
now we have pretty, pretty big ones.

24:04.600 --> 24:10.880
So we have multiple images and for each image you have multiple sets of fixations.

24:10.880 --> 24:14.600
So basically, for example, you might have like an image and you ask like 10 people to take

24:14.600 --> 24:17.920
a look at the image, you know, during, during a very small amount of time.

24:17.920 --> 24:20.040
So maybe like up to two seconds.

24:20.040 --> 24:22.520
And so then you capture these fixations.

24:22.520 --> 24:27.560
And normally what people do is they, they apply Gaussian blurring to that.

24:27.560 --> 24:31.240
So you can imagine you have like an image, which is full of zeros and let's say you have

24:31.240 --> 24:34.280
like ones at the locations of the fixations.

24:34.280 --> 24:37.400
And then you can apply Gaussian blurring to this and what you're going to get is you're

24:37.400 --> 24:40.200
going to get this resultant image, which is diffuse, right?

24:40.200 --> 24:44.280
So you have high values at the fixation points.

24:44.280 --> 24:48.800
And then these high values sort of diffuse in the immediate area of the fixation.

24:48.800 --> 24:54.400
So what you end up with is sort of like, it's not a, it's not sort of like a binary image,

24:54.400 --> 24:59.520
but you have some diffuse attention around the regions of fixation with the modes being

24:59.520 --> 25:01.400
at the fixation points.

25:01.400 --> 25:05.880
And so this can be what you have now is rather than have like an image full of zeros with

25:05.880 --> 25:10.520
a few, you know, with just a few points where there's some support of the distribution,

25:10.520 --> 25:14.600
you know, you've diffuse a distribution where you have basically some amount of non-zero

25:14.600 --> 25:16.440
support at all points.

25:16.440 --> 25:21.720
And if you normalize this resultant image appropriately, you can consider this image

25:21.720 --> 25:23.760
to be a probability distribution.

25:23.760 --> 25:28.520
And so then this becomes your ground truth and you just apply your typical machine learning

25:28.520 --> 25:31.080
framework and you say, okay, this is my ground truth.

25:31.080 --> 25:38.840
I have a model that takes us as inputs and image and produces some sort of predicted distribution.

25:38.840 --> 25:44.880
And then I compare the two using my probability distribution, my difference measure.

25:44.880 --> 25:47.000
And you backpropagate the loss, right?

25:47.000 --> 25:49.480
You backpropagate the difference that you see.

25:49.480 --> 25:57.840
And so why is that diffusion step key here as opposed to the more binary approach of looking

25:57.840 --> 25:59.600
at the fixations?

25:59.600 --> 26:06.600
As we have a somewhat subjective process, it's a little bit too strict of a task to ask

26:06.600 --> 26:10.960
the model to predict exactly the fixation point because you could imagine that the user

26:10.960 --> 26:14.560
could have very easily looked elsewhere and you can actually see this, right?

26:14.560 --> 26:18.480
Because if two observers are looking at the same image, you're not going to look at

26:18.480 --> 26:19.800
the exact same point.

26:19.800 --> 26:24.280
So therefore, if you have a location, so for example, you have an image and there's

26:24.280 --> 26:28.920
an image that contains a face, very likely there's going to be a ton of attention paid

26:28.920 --> 26:29.920
to the face.

26:29.920 --> 26:31.400
People like to look at faces.

26:31.400 --> 26:34.320
But they're not going to look at the same points, not everybody's going to focus exactly

26:34.320 --> 26:35.320
on the eye, right?

26:35.320 --> 26:40.720
So if you have ten people, you might see like a distribution of fixations very much on

26:40.720 --> 26:42.680
the face.

26:42.680 --> 26:46.360
But you can imagine very easily if you had another person, if you had an eleventh person

26:46.360 --> 26:48.480
that looked, they might look not exactly on that point, right?

26:48.480 --> 26:49.920
But they would look in the general area.

26:49.920 --> 26:54.240
So this is why it's nice to not use just the actual fixation that you have, but to sort

26:54.240 --> 27:01.320
of do this diffusion process where you basically apply gouchions to those regions and have

27:01.320 --> 27:03.800
this diffused distribution.

27:03.800 --> 27:12.160
And you're doing this diffusion process to each of your fixation images, if you will,

27:12.160 --> 27:19.640
as opposed to after aggregating or averaging across the different fixations for a particular

27:19.640 --> 27:20.640
image.

27:20.640 --> 27:21.640
Exactly.

27:21.640 --> 27:24.560
So there are many different ways to do this.

27:24.560 --> 27:27.120
You can see that there are different choices, right?

27:27.120 --> 27:29.640
And none of them are exactly correct, right?

27:29.640 --> 27:30.640
Right.

27:30.640 --> 27:35.080
So in this case, we're trying to solve like a pseudo problem, like an adjacent problem,

27:35.080 --> 27:36.840
because we're not actually solving the exact one, right?

27:36.840 --> 27:40.560
Because this is the way the subject of it comes into play.

27:40.560 --> 27:42.560
So yeah, there are different ways of doing it.

27:42.560 --> 27:51.200
And of course, when you're applying this gouchion blurring, the gouchion filter itself has

27:51.200 --> 27:52.200
its own parameters, right?

27:52.200 --> 27:55.800
So you have to decide how diffused do you want this to be?

27:55.800 --> 28:02.120
And that itself isn't a question that sort of left up to the specific researcher.

28:02.120 --> 28:07.120
And do you, in that sense, is it, it's not something you're learning.

28:07.120 --> 28:11.680
It's a hyperparameter that you're choosing the experimentation.

28:11.680 --> 28:17.080
Oh, it's, it's, it's not even a hyperparameter because this is really the ground truth you're

28:17.080 --> 28:18.920
setting at this point.

28:18.920 --> 28:23.360
So really, what many people have tried to do is to set this in a somewhat principled way

28:23.360 --> 28:29.560
by trying to look at things like, like peripheral vision and trying to understand, okay?

28:29.560 --> 28:34.640
If someone actually fixates what does that, how localized is that fixation really?

28:34.640 --> 28:39.920
And then you can kind of have a measure on what sort of local region is really being

28:39.920 --> 28:41.320
attended to.

28:41.320 --> 28:45.440
And there's a lot of psychophysical studies on that and that can, that can inform how

28:45.440 --> 28:47.400
diffused you want this to be.

28:47.400 --> 28:53.000
And so that's, are we just finishing up your, your first work in this, in this field?

28:53.000 --> 28:54.000
It sounds like it.

28:54.000 --> 28:57.400
Yeah, but we have a bit more to cover.

28:57.400 --> 28:59.200
So where'd you go next?

28:59.200 --> 29:03.280
You know, some of this work was done actually while I was at XRC now, NLE.

29:03.280 --> 29:09.720
Some other work I've done while here is related to, as I said, learning visual representations.

29:09.720 --> 29:13.480
So I also did a favourites of work on this once again before the deep learning era.

29:13.480 --> 29:19.480
So this was using handcrafted features and trying to understand how to, how to learn embeddings

29:19.480 --> 29:23.200
in an effective way for different tasks.

29:23.200 --> 29:26.680
So things like fine-grained recognition and also visual retrieval.

29:26.680 --> 29:31.120
So maybe I can start off with some of the work I did recently with colleagues from

29:31.120 --> 29:36.520
NREA and also colleagues here at NLE related to what we call aggregation.

29:36.520 --> 29:42.000
So this is also something that has become very well known in the deep learning context.

29:42.000 --> 29:46.880
What I mean by that in particular is, for example, people very often refer to the same

29:46.880 --> 29:49.280
sort of principle as pooling.

29:49.280 --> 29:52.920
So, you know, for example, average pooling or actually exactly.

29:52.920 --> 29:54.240
This is quite well known, right?

29:54.240 --> 29:59.240
So this is basically saying you have some amount of it local information or maybe not even

29:59.240 --> 30:03.720
not very localized information and you want to be able to summarise it somehow, right?

30:03.720 --> 30:07.680
And very often what you want to summarise might be different vectors.

30:07.680 --> 30:10.320
So let's say you have a set of vectors and you want to summarise them.

30:10.320 --> 30:14.280
So for example, in the deep learning context, if you think of like a feature like a hyper

30:14.280 --> 30:19.520
column and you want to to summarise that in some way or to, let's say, compress information,

30:19.520 --> 30:21.400
you might use max pooling or average pooling.

30:21.400 --> 30:26.400
You referred to a feature as a hyper column and I haven't heard that reference before.

30:26.400 --> 30:28.040
What does that mean?

30:28.040 --> 30:33.520
So when I say hyper column, this is sort of, it sounds a bit old school now.

30:33.520 --> 30:39.000
It's a term that comes from neuroscience once again and it refers basically to, it's

30:39.000 --> 30:44.160
what you might call this typical feature tensor that you find in many confnets where

30:44.160 --> 30:47.480
you have, for example, you have an input image and you're applying different convolutions

30:47.480 --> 30:48.480
to it.

30:48.480 --> 30:53.560
Very often you end up at some intermediate point with multiple feature maps, right?

30:53.560 --> 30:58.360
So you have like a feature map of size like h prime w prime and that's your feature

30:58.360 --> 30:59.960
map and you have multiple ones of them, right?

30:59.960 --> 31:01.520
Let's say you have d feature maps.

31:01.520 --> 31:05.720
So in the end, what you end up with is a tensor that's like d by h prime by w prime and

31:05.720 --> 31:08.560
this has often been referred to as a hyper column.

31:08.560 --> 31:09.560
Okay.

31:09.560 --> 31:12.720
So that's in a deep learning context, but you can think of many contexts where you have

31:12.720 --> 31:16.680
feature vectors that you want to summarize in some way.

31:16.680 --> 31:20.640
So for example, in my work, we worked a lot with what we call Fischer vectors and so Fischer

31:20.640 --> 31:27.480
vectors are, it's a type of representation of visual content, particularly used a lot

31:27.480 --> 31:29.600
with what we call local descriptors.

31:29.600 --> 31:32.840
And so it's a way of basically saying, okay, I have some local region of an image and

31:32.840 --> 31:38.120
I want to find some vector representation of that local region that's discriminative

31:38.120 --> 31:40.400
and compact hopefully, okay?

31:40.400 --> 31:45.040
And so imagine you have an image and you have a set of these descriptors that you extracted

31:45.040 --> 31:48.520
let's say for different regions of the image and you say, okay, I have this image, I have

31:48.520 --> 31:52.640
I don't know, like n of these descriptors and I don't want n.

31:52.640 --> 31:53.640
I want like fewer.

31:53.640 --> 31:55.640
I want maybe one or I want to.

31:55.640 --> 32:00.960
So it is, how do I go from that end to that much, much smaller number while not losing

32:00.960 --> 32:01.960
too much information?

32:01.960 --> 32:06.560
So this is like a fundamental problem that has been tackled in many ways and so I've

32:06.560 --> 32:11.080
done some work on how to construct these what we call aggregated representations from

32:11.080 --> 32:14.720
a set of like a larger number of them.

32:14.720 --> 32:20.200
It sounds like, and you mentioned the term embedding previously, it sounds like you're creating

32:20.200 --> 32:27.400
some embedding space and then doing something akin to dimensionality reduction on that embedding

32:27.400 --> 32:28.400
space.

32:28.400 --> 32:32.560
So it's not quite in the sense that in the sense that when people talk about dimensionality

32:32.560 --> 32:36.200
reduction, typically they mean that you have an embedding space, right?

32:36.200 --> 32:39.240
It can be in, let's say, the dimension.

32:39.240 --> 32:43.920
So for example, I don't know, 2000 dimensions and for for dimensionality reduction, what

32:43.920 --> 32:48.160
you want to try to do is find some smaller dimensional space in which you're embedding

32:48.160 --> 32:49.160
slip.

32:49.160 --> 32:50.160
So for example, I don't know.

32:50.160 --> 32:56.960
You might want to go down from 2000 dimensions to maybe 500 or 256 or something like this.

32:56.960 --> 33:00.200
In our case, what we're doing is we're not changing the embedding space, but we're

33:00.200 --> 33:05.520
just changing the amount of samples from that space.

33:05.520 --> 33:06.520
That's it.

33:06.520 --> 33:09.160
The amount of vectors that live in that space.

33:09.160 --> 33:16.200
So for example, rather than have, let's say, 1000 vectors each of size, each of dimension

33:16.200 --> 33:19.280
to K, you might want to have only one.

33:19.280 --> 33:21.360
So this is what this aggregation is about.

33:21.360 --> 33:25.840
Is that a typical example going from 2000 to one?

33:25.840 --> 33:27.080
You can go even higher than that.

33:27.080 --> 33:28.080
You can go even higher than that.

33:28.080 --> 33:34.360
So very often, you might find on the order of anywhere from like 1000 to 10,000 factors

33:34.360 --> 33:36.800
that you want to reduce to just one factor.

33:36.800 --> 33:39.880
So you can see there that you can see what is that doing for you.

33:39.880 --> 33:42.600
Yeah, what is that compression compression?

33:42.600 --> 33:47.280
It's, you know, you're reducing, you're reducing the size of your image representation

33:47.280 --> 33:51.800
by a factor of the number of descriptors that you have.

33:51.800 --> 33:57.120
If you go down to one, I guess if I'm thinking of it in the sense of compression, then certainly

33:57.120 --> 33:59.400
we would want to do that.

33:59.400 --> 34:05.960
But you know, I would imagine there's a tremendous amount of loss in a scenario like that.

34:05.960 --> 34:11.280
But when I think about it from the perspective of like an embedding space, I mean, I guess

34:11.280 --> 34:12.280
it's also loss.

34:12.280 --> 34:15.120
Like you just lose a ton of information.

34:15.120 --> 34:19.680
And so maybe I'm asking you to convince me that there's, you're left with something of

34:19.680 --> 34:24.760
utility after you do this 10,000 to one reduction.

34:24.760 --> 34:30.400
Sure, so you're definitely right in the sense that they both, they have like an effective

34:30.400 --> 34:31.720
compression, right?

34:31.720 --> 34:32.720
That's for sure.

34:32.720 --> 34:34.480
In one case, you're going to a smaller dimensional space.

34:34.480 --> 34:37.640
In other case, you're just reducing the amount of representations you have.

34:37.640 --> 34:39.920
In both case, you're losing information, that's for sure.

34:39.920 --> 34:40.960
And this was the point of my work.

34:40.960 --> 34:47.880
So my work was really on how to represent, how to, to perform this aggregation while maintaining

34:47.880 --> 34:53.240
as much as possible, you know, as much information you can in this aggregated representation.

34:53.240 --> 35:00.400
So what we proposed was a method called generalized max pooling, which aimed to, to sort of maintain

35:00.400 --> 35:01.400
this property.

35:01.400 --> 35:05.540
And of course, it's not perfect, but we found that it gave, you know, pretty, pretty interesting

35:05.540 --> 35:10.240
results over other techniques, for example, like average pooling or max pooling.

35:10.240 --> 35:16.080
And so the benefit there is, is once again, it's in terms of, of compression, which is

35:16.080 --> 35:17.240
which is important.

35:17.240 --> 35:23.560
So in many applications, you don't want, you can't afford to store, for example, 10,000

35:23.560 --> 35:25.800
descriptors to represent an image.

35:25.800 --> 35:30.600
To give an example, so we're very interested in my research center on the problem of image

35:30.600 --> 35:31.600
search, right?

35:31.600 --> 35:37.160
So let's, let's say, for example, you have a database of images, let's say, a billion

35:37.160 --> 35:38.840
images, right?

35:38.840 --> 35:42.400
And you have some image that you want to match to that.

35:42.400 --> 35:45.320
And you can think of many applications, you can think of, for example, shopping, let's

35:45.320 --> 35:48.640
say you want to, to find us, you know, you have an image of like some kind of clothing

35:48.640 --> 35:55.000
item, and you want to find in a huge, let's say, catalog by multiple retailers, anybody

35:55.000 --> 35:58.680
who has something similar, who has the exact same one.

35:58.680 --> 36:02.440
If you have to, if you have to compute the similarity between that image and all the

36:02.440 --> 36:07.400
images in those, in that database, and each image is represented by 10,000 vectors, each

36:07.400 --> 36:12.680
of which has a dimension of 2,000, it's going to take forever, it's not practical.

36:12.680 --> 36:18.360
So this is why very often people work on how do I get from 10,000 vectors to one, or

36:18.360 --> 36:20.680
some smaller number, some much smaller number.

36:20.680 --> 36:22.000
So this is really the utility there.

36:22.000 --> 36:24.120
It's a compression, it's a compression utility.

36:24.120 --> 36:28.360
You mentioned this, this algorithm is a generalized max pooling.

36:28.360 --> 36:35.880
Are you taking kind of an off the shelf, CNN architecture, ResNet, for example, and kind

36:35.880 --> 36:40.000
of swapping out, you know, wherever it says max pooling with this generalized max pooling

36:40.000 --> 36:45.720
and maybe chopping off the last layer, your classifier, and that's creating your vectors

36:45.720 --> 36:48.280
or is it more involved than that?

36:48.280 --> 36:53.800
So that would make sense, as a good guess.

36:53.800 --> 36:57.440
But actually, and this is something that could technically be done, but actually what

36:57.440 --> 36:59.600
we think of this is something that's very generic.

36:59.600 --> 37:04.320
So it's actually not related to deep, it's not specific to deep architectures.

37:04.320 --> 37:08.280
It's very generic in the sense that we are trying to solve the problem where we have

37:08.280 --> 37:12.640
a set of vectors and we want to reduce, we want to aggregate them in some way.

37:12.640 --> 37:17.040
And so that can involve features extracted by deep networks.

37:17.040 --> 37:21.840
So let's say, for example, you have a deep network and at some point you have this hyper

37:21.840 --> 37:25.920
column I talked about, you have this feature tensor and you want to pull it in some way,

37:25.920 --> 37:27.160
you want to aggregate it.

37:27.160 --> 37:32.320
So you can think of generalized max pooling in this scenario as an alternative to max pooling

37:32.320 --> 37:35.320
or average pooling, but you can also have other descriptors, right?

37:35.320 --> 37:40.480
So you can have descriptors that are handcrafted or come from many different, many different

37:40.480 --> 37:42.360
things we can think of.

37:42.360 --> 37:51.080
It's more presented as a fundamental operation, you can apply to a set of vectors to aggregate

37:51.080 --> 37:57.880
them as opposed to something that's specifically used in a context of a deep learning model.

37:57.880 --> 37:58.880
Exactly, yeah.

37:58.880 --> 38:02.400
And in fact, in the original work we didn't apply it on deep learning, although it's

38:02.400 --> 38:06.960
been subsequently used with deep models in combination.

38:06.960 --> 38:10.960
And so you've spent this time on the visual attention side.

38:10.960 --> 38:14.920
It sounds like that was a bit earlier in your research.

38:14.920 --> 38:19.800
More recently you're working on learning these representations.

38:19.800 --> 38:23.120
Where do you, where you headed?

38:23.120 --> 38:31.840
What's kind of interesting for you nowadays and where you investing your resources in terms

38:31.840 --> 38:32.840
of future research?

38:32.840 --> 38:33.840
Sure.

38:33.840 --> 38:38.080
So actually, you know, everything sort of all becomes new again.

38:38.080 --> 38:43.400
So actually I'm doing some work now on both things at the same time, because as you mentioned

38:43.400 --> 38:48.240
very patiently before, attention is it's used everywhere, right?

38:48.240 --> 38:50.760
In deep learning right now, in many things.

38:50.760 --> 38:55.520
So actually right now some of the work I'm really involved in at the moment involves

38:55.520 --> 39:03.000
how to learn representations using deep learning with attention involved as well.

39:03.000 --> 39:09.920
So meaning how do you use some sort of attention mechanisms in order to pay attention to regions

39:09.920 --> 39:14.360
of an image that you really need to focus on in order to solve your task, right?

39:14.360 --> 39:18.600
So this is where once again the, this is like the overlap between the two views, let's

39:18.600 --> 39:20.320
say, of attention.

39:20.320 --> 39:25.080
Because once again, there's limited processing power, there's a lot of information images.

39:25.080 --> 39:32.520
And so it pays to really only spend time computing over data that's valuable.

39:32.520 --> 39:38.040
And then not only that, but it also can be considered some sort of clean name mechanism,

39:38.040 --> 39:43.480
for example, because you can have a lot of things that you can just, that might be important,

39:43.480 --> 39:47.280
but it can also be just distracting and can be considered clutter basically.

39:47.280 --> 39:51.840
So let's say, for example, you're doing once again, let's look at the fashion example.

39:51.840 --> 39:56.360
Let's say you want to train to learn a representation for visual search, right?

39:56.360 --> 39:58.760
Let's use my previous example.

39:58.760 --> 40:05.240
And you want to search for, you know, you're looking for shirts.

40:05.240 --> 40:08.800
If you have an image, there are many parts of the image, which is not going to help you

40:08.800 --> 40:11.160
find, you know, whether these two images match, right?

40:11.160 --> 40:12.720
Whether they both have the same shirt.

40:12.720 --> 40:16.800
So you might want to only attend to the regions of the image that are relevant for that.

40:16.800 --> 40:19.800
So of course, like if there, if you might want to pay attention to the upper body of the

40:19.800 --> 40:23.400
person, you might want to be able to find, okay, where is the upper body of this person?

40:23.400 --> 40:27.480
Because this is where I'm going to really be able to tell if there's a shirt in the

40:27.480 --> 40:28.480
image.

40:28.480 --> 40:30.320
And if so, if it's the same shirt, if it's the shirt I'm looking for.

40:30.320 --> 40:34.640
Now, there might be other parts of the image that are also useful for that.

40:34.640 --> 40:37.480
So maybe, for example, if I find the head, I might have a better chance of finding the

40:37.480 --> 40:40.480
body, but there are many parts that may not be, right?

40:40.480 --> 40:46.200
So what I'm focused on right now is how to, how to incorporate different models of salience

40:46.200 --> 40:50.360
or let's do different ways of attending to regions so that, you know, you can simplify

40:50.360 --> 40:54.160
these tasks of learning representations for search.

40:54.160 --> 40:57.600
Naila, thanks so much for taking the time to chat with us.

40:57.600 --> 41:03.400
It's really interesting work and I'm excited to get to learn a bit about what you're up

41:03.400 --> 41:04.400
to.

41:04.400 --> 41:05.400
Thanks so much, Naila.

41:05.400 --> 41:07.400
It was really great to have this conversation.

41:07.400 --> 41:13.120
All right, everyone, that's our show for today.

41:13.120 --> 41:18.240
For more information on Naila or any of the topics covered in this episode, head on over

41:18.240 --> 41:22.640
to twimlai.com slash talk slash 190.

41:22.640 --> 41:28.760
For more information on the entire deep learning and daba podcast series, visit twimlai.com slash

41:28.760 --> 41:31.680
in daba 2018.

41:31.680 --> 41:34.920
Thanks again to Google for their sponsorship of this series.

41:34.920 --> 41:41.760
Be sure to check out the 2019 AI residency program at g.co slash AI residency.

41:41.760 --> 41:44.920
As always, thanks so much for listening and catch you next time.

