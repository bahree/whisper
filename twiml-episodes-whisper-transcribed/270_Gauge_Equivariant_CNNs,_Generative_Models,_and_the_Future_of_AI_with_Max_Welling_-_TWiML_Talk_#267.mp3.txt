Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, before we dive in, a quick community update.
The Twimble study group is added again for the next four weeks, starting May 25th, we'll
be diving headfirst into Peter Abiel's full stack deep learning course.
This course is a great complement to the fast.ai courses we've done so far and covers practical
topics, like problem formulation, data acquisition and preparation, establishing the right frameworks,
platforms and compute infrastructure, debugging and ensuring reproducibility, and deploying
and scaling your models.
For more information or to register for this study group, visit twimbleai.com slash full
stack.
I can't thank our volunteer study group hosts enough for all their hard work, a huge shout
out to Michael, Christian, Kai, Sanyam, Joseph, Dinesh and everyone else that's been involved
in making this group happen.
A hearty thanks to our friends at Qualcomm for sponsoring today's show.
As you'll hear in the conversation with Max, Qualcomm has been actively involved in
AI research for well over a decade, leading to advances in power efficient on device AI,
as well as an algorithm such as Bayesian deep learning, grass CNNs, gauge, echrovariant CNNs
and more.
Of course, we know Qualcomm powers some of the latest and greatest Android devices with
their Snapdragon chipset family.
From this strong foundation in the mobile chipset space, Qualcomm now has the goal of scaling
AI across devices and making AI ubiquitous.
In this vein, a product I'm particularly looking forward to is their Cloud AI 100 line
of data center inference chips, which I learned about at the recent press launch.
To learn more about what Qualcomm is up to, including their AI research platforms and developer
tools, visit twimmelai.com slash Qualcomm.
Alright everyone, I am on the line with Max Welling.
Max is a research chair in machine learning at the University of Amsterdam, as well as
vice president of technologies at Qualcomm and a fellow at the Canadian Institute for
advanced research or C-FAR.
Max, welcome to this week in machine learning and AI.
Thank you very much Sam.
It's great to get a chance to chat with you Max.
You're quite accomplished in the field of machine learning and I'd love to understand what
drew you to AI and how you kind of started your career in this area.
I actually started not in AI, I started in physics, so I did my PhD thesis in theoretical
physics and that was fun, but it was also somewhat abstract in the sense that I couldn't
see how I could have a major impact with that and so I decided I wanted to change to
some a field that was a bit more, you know, less static and more dynamic at that point
in time.
And so I wanted to do something like neuroscience, which I thought was a great choice.
And so I applied to Caltech to Pietro Perona's lab, but I basically ended up doing computer
vision there and then later when I went to London, I worked with Jeff Hinton, I was doing
machine learning.
So I sort of migrated my andered from physics through computer vision to machine learning.
And there, I really found a home because I thought, okay, this is going to grow big in
the future, which actually happened and there's so many applications with which this field
can impact the world in a positive way that I sort of decided to stay there.
That first jump into computer vision, was that computer vision applied to neuroscience
in some way?
No, it was just computer vision basically analyzing images on a computer screen, basically,
right?
So it didn't have anything to do with neuroscience, although I was, you know, it was fascinated
by that question and I, you know, I was going to some meetings on neuroscience and, you
know, integrated somewhat with neuroscience in Caltech.
In the end, I was doing computer vision, which was great too.
Okay, okay.
And I get a chance to talk to a lot of folks that started their career in physics.
A lot of folks more on the applied side, whether astronomy or, you know, dealing with things
like, you know, some folks that work at using colliders and things like that, but you are
more on the theoretical side.
That's right.
Yes, I was doing my thesis in 2 plus 1 dimensional quantum gravity, so it was a very abstract
and theoretical.
It gave me a good, you know, basis for mathematics, but it wasn't very applied, that's true.
And so at some point along the line, you started a company, Cypher.
When was that in your career and what was the inspiration for the company?
Yeah, so that was actually when I came back to the Netherlands.
So I had my career was in North America and a little bit of time in London until about
six years ago when I decided to come back to the Netherlands.
And fairly quickly after that, we founded this company.
It was an interesting story because we were working with a big Dutch bank and they wanted
to do a competition to predict, you know, what ads, you know, customers would want to
click on when you offered them.
And they were doing that with a bunch of big sort of consultancy companies.
And we were basically, I was asked to basically arbitrage, right, to make sure that, you know,
everything went right and the competition was fair.
And in order to do that, I just asked one of my master's student, Taco Cohen, who was
also working at Qualcomm at this point, to, you know, to also implement some models.
And he did that on his laptop, laptop with a single GPU in it.
And then we looked at the results, you know, we basically, we beat all the big companies
and the big banks that, you know, if that's the case, then you get the job.
And so that was the, that was the start of our company, right, I, I then, so then two
other people joined, I'm a block of work and you're a Sunday, you're a son of was an
old physics pal of mine.
And that's what started the company and, and from there, you know, five, five good years
running, we did lots of consultancy with companies and, you know, looked at their AI problems.
And then Qualcomm came around and said, you know, you have a good sized group here, all
very experienced people, nice practical attitude.
And they wanted to acquire us and, you know, there was a couple of other companies also
that were looking at us, but, you know, we went with Qualcomm.
And at the time Qualcomm came around, were you looking to be acquired or were you ready
to, to try something else or did it, you know, did the opportunity just present itself
and you evaluated it on its terms?
Yeah, we did the latter.
So we weren't optimizing, you know, for being acquired or anything.
It was just like, you know, we were having a lot of fun and some people actually thought,
you know, that we should keep going and grow.
Other people thought, you know, maybe it's, you know, maybe it's actually interesting
to join a big company and, you know, have a lot more, um, like a lot more power.
You can, you can make a bigger impact if you're part of a big company.
Um, and so it was basically an opportunity that we evaluated at that point and we thought,
oh, is this good?
We should try this.
Uh, but prior to the primary focus of the company was on, uh, consulting, there wasn't
a product being built or, or something along those lines.
Yeah, we, we did actually build a product as well.
Um, so there was an active learning tool, which was, uh, actually implemented the
Tata steel, um, and it works as follows.
So, um, you have an expert.
So there was like cameras looking at slabs of steel, which were being produced and there
was sometimes small little, uh, sort of defects on that steel.
Um, we had a battery of cameras above it and, um, we were detecting these defects.
Um, but, you know, there were a couple of classes and some of these defect classes were very
rare.
We didn't have a lot of data on them.
And so the algorithm didn't perform very well.
And so then the algorithm assessed itself and said, okay, so for the, for these types
of, you know, images, we need more labels.
And so that was then shipped to an expert.
The expert would label it.
It would go back into the system and the system would learn again and become better, gradually
over time.
So that's active learning where there's a human in the loop and the algorithm interacts
with the human, um, and that was basically our product.
Okay.
Uh, I don't, I don't, I can't speak necessarily to the approach, but it sounds, uh,
very similar, uh, at least from a problem domain and general direction to, uh, well, lots
of folks are going after this area, but landing AI, Andrew Ng's company, uh, comes to mind
is kind of tackling that similar kind of applying, uh, AI to industrial problems, including
like defect detection, things like that.
Um, was that, uh, was that product kind of one of a portfolio of, uh, uh, uh, uh, uh,
challenges that you were, uh, going after or was that a big focus?
It wasn't a big focus, actually.
So this company was more opportunistic in that sense.
So we, um, we basically talked to a lot of companies in a very diverse, uh, set of sectors.
So from finance to, you know, to, uh, manufacturing, you know, to retail.
And we basically, you know, we were very good at going in and talking to these people
and say, okay, what is your, you know, what is your, what is your problem look like?
What is your, where is your opportunity?
What data do you have?
And then we did a quick assessment and we made a recommendation and, uh, perhaps it's
very quick demo.
And then, um, and if they were happy, then we went for slightly longer sort of trial
where we would actually, you know, implement, you know, a system to just show them that,
you know, we can, you know, you can actually get value out of, out of the data that they
have.
And if they, they were still, you know, happy after that, and this was like maybe six
months later, then we would go into a, you know, an actual implementation of the whole
thing.
And so we repeated this many times as many companies which gave the team an enormous
amount of, uh, sort of experience with a very broad spectrum of problems.
And I think that's what made the team also very attractive.
Uh, so that acquisition was in 2017, you're a couple of years in now at Qualcomm.
You know, this thesis of kind of the umph of a larger company, uh, creating some opportunities
for you.
Did that thesis bear out?
The interesting part of working for Qualcomm for me, um, was that I, um, have always taken
compute for granted, um, basically if I needed to compute something, then, you know, there's
this computer, which has some chips in it and it will do its job.
Um, but with the advance of deep learning, um, we see that, you know, bigger models, um,
just perform better.
And so compute becomes a really important part of the equation.
Um, and so, you know, a good way to make progress, um, is to make sure that your algorithm
actually runs extremely efficiently.
And so of course, uh, you know, the, um, the GPUs came and, you know, they made compute,
deep learning compute a lot more efficient, which is part of why things are going so
well with deep learning, um, but this is a very fundamental problem of question, right?
It's like, okay, so, you know, a brain doesn't compute, you know, with, uh, 32 bits precision,
um, it's very noisy.
Um, so should we, you know, change our compute paradigm in, you know, in our computer as
well?
Should we, you know, should we maybe try to train with a lot less precision, maybe
a couple of bits precision?
Um, and, um, do these neural networks have to be so huge, right?
These neural networks typically have like even hundreds of millions of parameters, sometimes
billions of parameters, do they necessarily have to be that big because that can, can
choose a lot of, uh, memory and compute as well.
Um, and also, you know, if you go a little deeper, if you dig a little deeper in the problem,
then what you find is all these, all these parameters of these big models, they are living
in, uh, sort of off-shift memory called DDR, and you have to bring them to the registers
where you do all the Mac computes, and multiply and accumulate computes.
Um, and that movement of data costs a lot of energy.
And so, but in the brain, actually, it isn't so separated.
We call that separation as a normal architecture, where memory and computer are separated.
Um, but in, uh, in a brain, actually, what you find is that the memory and the computer
are very close, right, because, you know, things are stored in these, uh, synapses, um, between
neurons and, and the neurons compute, um, and so much more distributed.
And, um, so, so there is a lot of fundamental questions of, can we, can we just change the
compute paradigm, um, so that we can do things far more energy efficient, um, and that's,
that's basically what I find the biggest thrill of working in Qualcomm in actually trying
to make that happen, um, and then scaling up, uh, sort of AI computations, a lot, which
might actually be, you know, one of the big reasons why we're making a lot of progress.
And then, uh, yeah, basically shipping, whenever you have such a thing, and you build, you
know, a chip on a, on a new paradigm or a new principle, you can then put them in a
chip, and you can then ship them to, you know, a billion customers in a billion phones,
right?
And that's the scaling you were asking about, which is extremely exciting.
You've got joint appointments, both with, uh, in academia and Qualcomm, uh, that's becoming
increasingly common, but everyone kind of manages it differently.
Uh, what's the relationship for you?
How is your, your work and research distributed across those appointments?
Yeah.
So I have like about half an appointment in academia and half an appointment in Qualcomm
and at Qualcomm.
Okay.
Which I think is actually a very good distribution, um, so it typically, in the university, you
work on more of fundamental questions, which are very far out, um, and don't have a, you
know, sort of a, a horizon of being, you know, productized or, you know, finding applications
of four years, maybe.
So at, at Qualcomm, we work on, you know, on problems, which between, you know, uh, one year
and four years or five years, um, will find an application within the company, um, in
academia, you're completely free to do whatever you want and it could be, you know, 100 years
out if you wanted to.
Mm-hmm.
So it's interesting to have one lag in sort of both of these, uh, ecosystems in both
of these environments.
It does strike me that some of the things that we're talking about here, coming up with
fundamental new compute architectures, you know, perhaps that are more inspired by, uh,
the brain and, and synaptic and neural architectures, things like that.
That could be, uh, very far reaching, uh, research.
It is, are you working on, uh, that, uh, that kind of work in the academic setting as
well?
Um, so the, the compute, um, sort of thinking about how compute interacts with, um, you
know, uh, with AI and machine learning and intelligence, that's something that I exclusively
do at Qualcomm.
Although it did start, um, in academia, so I was doing, um, Bayesian deep learning.
So Bayesian statistics is a particular paradigm, um, as statistical paradigms, you have frequent
distant Bayesian sort of statistics and Bayesian statistics, you've put probability distributions
over your model.
You basically say, I don't know, you know, what my model really is.
Um, I say I have some probability distribution, um, over my possible parameters of my model
and then when I see data, I'm going to narrow down that distribution of parameters to the
ones that are actually describing the data that I see.
Um, so, so that's something that wasn't really applied at deep learning.
And so we started to apply that statistical paradigm to deep learning, which was a challenge
because there was, you know, millions and millions of parameters there.
You have to sort of handle that way.
Um, but what we found is, um, that we can use, we could use that paradigm to, uh, basically
compress a neural network, um, you know, with a factor of 100, uh, without losing any
of the accuracy, which was actually a big shock to me, which is like, okay, so we working
with these models, which are, we'd have a million parameters, but you could train the
same model.
You could just keep one out of 100 parameters and throw away all the rest, um, and you
would have a model of which would function exactly the same way, which would not be any
worst than the one that you started with.
Um, and so that's generally known as, as compression neural network compression.
So they are heavily over parameterized.
Mm-hmm.
So and we did that within the paradigm of Bayesian deep learning.
It was a very good fundamental tool, um, to, you know, to do that compression.
Did you apply this compression to these Bayesian deep learning models or is there, uh, something
fundamental about Bayesian deep learning that allows it to, um, you know, that synergistic
with this kind of compression?
Yeah.
So it's more the latter.
Okay.
Um, so it's basically, so you have a neural network.
Because we do this kind of compression with, uh, with traditional CNNs and, and the like
as well, right?
Yes, that's right.
So, so basically, um, it's one way, one technique to do this compression on a neural network.
It has some additional advantages, which is that you can also, um, sort of expression
on certainty over a prediction, right? So you can actually say, you know, I think it's
this class or, you know, uh, this is happening in an image, but, you know, I'm 80% certain
that that's actually correct, right?
And so the Bayesian paradigm also allows you to do that.
But, um, you can also use it to prune large parts of your neural network away.
So it's sort of a principle way of doing that.
Um, and so we started doing that and that's, you know, coming back to academia for this
talk, um, so we started doing that in academia because that was a very academic exercise at
that point.
But it became practical, um, because we could compress these neural nets.
And then we, you know, Qualcomm, we, we took it to Qualcomm.
And now there's a whole team, uh, led by Tamin, um, Blancavort, who is basically, um,
trying all, you know, sorts of algorithms to try to compress these neural networks, you
know, in the, in the most practical way.
And often the Bayesian way is not the most practical way.
You know, it could be perhaps like a very good way, but it's not yet a very hands-off,
you know, way to do things.
And so some of these other methods, which are much simpler to understand, um, can also
compress these neural networks to basically the same degree.
And those are the ones that actually make it into the toolkits that we use at Qualcomm.
It sounds like there's kind of a dynamic relationship.
I guess as one would expect between the kind of things you're working on from an academic
perspective and what you're doing at Qualcomm, although, um, you know, they differ in their
timeframes.
Yeah, they, you know, and it is a little bit like that.
So, um, so another maybe great example, which I'm very enthusiastic about, is, um, work
that I do with Taco Cohen and Maurice Weiler, um, on, um, so one is, uh, was a PhD student,
now, uh, full-term employee at Qualcomm, Taco Cohen, and then Maurice Weiler is now, uh,
PhD student at Qvalab, so Qvalab is actually a lab funded by Qualcomm at the University
of Amsterdam.
Um, and so that, so that work is, um, to include symmetries, um, in deep learning.
So in other words, if I, you know, turn my head, um, the objects that I see, you know, turn
around, um, in, in my brain, um, however, it's still the same objects, right?
And so that's what we call a symmetry.
If I, you know, if I move something, then it's still the same object, even though it,
it has moved, um, and so incorporating these types of symmetries into neural networks
is actually a very powerful way to make them better, turns out, um, and, um, and so, you
know, with Taco Cohen, we started that process, um, you know, almost, I don't know, six
years ago, or something like that, um, and it became quite successful, and now very
recently, um, we did something that actually is quite, you know, amazing, I would say.
So we started to use the mathematics of, um, general relativity, which is, uh, you
know, the fundamental theory of gravity, uh, made by Einstein, um, and the same mathematics
is actually in quantum field theory, which is, you know, behind the standard model, which
is the fundamental theory, but particles, um, and this, this, this, this, this theory is
called gauge theory, um, and it was actually the topic for my PhD thesis, which is interesting,
I would say, so that came back after 20 years, um, and we started to incorporate these mathematical
ideas into deep learning, um, and now we can do deep learning on arbitrary manifold.
So you can think of a sphere, like the, the earth, and you want to detect storms or other
weather patterns, um, on the earth, um, then you can use this particular tool, or, you
can deep learning tool to, to, to analyze, you know, these, these manifolds, um, you
can also think about, um, you know, in VR, right, in, in virtual reality, um, you generate,
you know, maybe a game or something like that, so you generate objects in your world,
these are synthetic, um, and you can sort of, uh, put texture, high resolution texture
on these objects using, you know, this kind of tool.
And so it's a very, very fundamental, you know, exercise, academic exercise to take, you
know, these mathematical theories or, or, that are used in theoretical physics and put
them into a deep learning algorithm, um, but then actually when you're done, once you're
done, and you, you look, you know, at it from a distance, you, you discover all these
beautiful applications, actually, um, and that's, I think the perfect, you know, I think
the perfect line of thinking, right, so you, you start with something very fundamentally,
you make big progress, um, you make an impact on, you know, a lot of, you know, the whole
field and then you find that there is all sorts of interesting applications popping, popping
up, um, that you hadn't thought, thought about before.
Uh, and so, uh, some questions on that, uh, you started out talking about, uh, the work
that you were doing around symmetry, uh, and then transitioned into the, the, the gauge,
uh, networks, are those, uh, I didn't catch the relationship between those or those, uh,
is that work specifically related to the one lead to the other?
Yeah.
Yeah, yeah, yeah, absolutely.
So, um, yeah, that's a good question because I went very fast over that, but in, in, in,
so, and it follows precisely the same, uh, steps as we're done in physics, right?
And I think in, uh, you know, 19, no, six or so, um, Einstein came with his special theory
of relativity, which basically described how different observers who move at a constant
speed relative to each other, see different phenomenon.
And, and he and Maxwell, uh, found that, um, basically magnetism will turn into electricity
if you, you change observer and, you know, and, and move it with a constant speed, um,
versus, so if a, if a, if a static observer sees an electric, you know, electric field
and, and moving observer will see a magnetic field.
Um, and so, um, and so that's the symmetry, right?
It's a constant symmetry, it's like, you rotate something or you, you know, you rotate
the whole, you know, earth around or something like that, um, now, uh, after that, um, came,
you know, Einstein generalized that into general relativity and he said, well, actually,
it's, it's not just, you know, these global symmetries, but actually, you know, we should
be able to have a larger set of symmetries, which is basically, I should be able to,
you know, to change speed, you know, and, and acceleration at every, any point in time,
and I can sort of change my frame of reference at, differently at every point in space
time.
Um, and that's a much more general theory, it's called a local, uh, symmetry.
So it's, it's a, it's a much more general symmetry.
It's called a local symmetry and a local symmetry is also referred to as a gate symmetry.
And it turns out when you think about that, that kind of large, a set of symmetries, then
he figured out, for instance, that acceleration and gravity are actually, you know, the same
phenomenon, but observed by, you know, one person who is standing still and another person
who is accelerating relative to the other person.
And so now again, you know, you'll, you'll have symmetries incorporated in your theory
and your theory becomes a lot richer.
So that's happened.
So exactly that progression has happened for us too.
So we started with these global symmetries, um, and then we turned to local symmetries
and, you know, we went from basically, you know, doing deep learning on flat images to
do deep learning on curved manifolds.
Interesting.
Yeah.
When you first started talking about symmetries, the, uh, the thing that came to mind was,
you know, that maybe you were going after the same type of problem as Jeffrey Hinton,
uh, who you've worked with and, uh, his capsule networks, uh, but it sounds like, uh,
you ended up in very different places.
But I would say that, um, you're actually quite right there.
So it turns out that, um, this is, you know, this is a little hard to explain, but out
of this theory of symmetries, um, capsules emerge quite naturally.
Um, and it's a bit hard to explain how that precisely works.
Um, but it's, it's true that what you'll have is you'll have these neurons.
Um, so when you, when you apply translational invariance, these neurons will now be
what we call feature maps, which is an entire sort of filter image.
Um, and when you apply additional symmetries like rotation, you get sort of stacks of filter
maps, um, that sort of, that sort of transform into each other if you rotate the underlying
image.
And that's, that stack of filter maps is what we call a capsule.
And that's actually what Jeff Hinton also talks about when he, when he talks about capsules.
Mm-hmm.
Now he has this sort of dynamic routing algorithm, which is, which is something on top
of these capsules, which, you know, we haven't implemented in our code yet.
Um, but there, these two things are actually remarkably related.
So you were actually quite right there.
So within the, the gauge CNNs, you have this concept of a capsule, uh, as well.
Is that correct?
It's, I would say, uh, yes, but it's also already in the sort of normal, uh, sort of, uh,
what we call group CNNs.
So it's, it, so before we did gauge CNNs with the local symmetry, um, capsules also naturally
appear in, you know, these, uh, these groups, uh, like a global rotation, uh, or a translation
or something like that.
And, uh, beyond the conceptual, uh, well, you've got this, this, uh, common conceptual
foundation between, uh, Hinton's capsule networks and your gauge, echrovariant, CNNs are,
are there other relationships between the two, um, that grot of these, or what are the
relationships between the two that kind of grot of these, uh, kind of the shared feature
maps and in variances within the networks?
Yeah, I think it, it would be a fairly technical discussion to try to explain that, but I
just, let me, let me limit myself to saying that, um, sort of in Jeff, in Jeff Hinton's
theory, um, he, he, he basically says, okay, so I, I want my feature maps to be divided
up in, in these capsules and then basically the particular configuration within a capsule,
you know, is the pose of something, um, and then, you know, the, maybe the, you know,
the strength of, you know, how much of that capsule is present, you know, is, is, is,
it, that indicates how strongly a particular object is seen in the image.
Um, so he, he goes in with, uh, sort of an intuition and he, he, he builds it in, um,
where in our case, we start with the symmetry from the principle of symmetry and actually
the math, you know, basically drives us to these capsules.
But in the end, these, they were, they're actually completely the same thing.
So, so I would say we laid a mathematical foundation for the capsules that, that, Hinton's
intuition sort of, uh, brought, um, yeah.
And so, you know, there's, so he, he has the Namooka routing, which is something that
is sort of on top of it, which, which we don't do necessarily.
We, you know, we, we went into the direction of, of course, Gage Equivareans, um, but it's,
you know, we, we are basically in parallel sort of developing, uh, these ideas, I would
say.
Okay.
And so is the idea with these Gage CNNs related to, uh, are they more compact or do
they open up new, uh, applications or do they perform better?
What, what does this approach buy us?
Right. So, um, so it's, it's mostly, if you want to do a deep learning on a, on something
that's not a plane, right?
So the manifolds, yeah.
So imagine, you know, you want to, you know, so of course, a simple example is, think
of the earth and think of a signal on the earth, like maybe temperature or, you know, wind
patterns or something like that, and you want to predict, you know, maybe the weather
in 10 days or something like that, or you want to find where is the storm or where is
the, you know, the weather pattern that you're interested in.
Yeah.
I also did a really interesting interview with a woman named Nina Mielan, who does, uh,
or Mielane, who does a Python package, Jams, that's that it's focused on doing statistics
on manifolds.
And the example that she gave was pretty interesting, it's like if you want to do statistics
or learning on something like a human heart in medicine, uh, it is, uh, you know, much
more naturally amenable to dealing with it as a set of manifolds or curves than in a,
you know, a typical rectilinear space.
Yeah.
So exactly.
So that's on our to-do list, and we are actually collaborating with people who know much
more about, you know, these medical applications than we do.
But that's certainly one of the applications area.
So application area.
So you can think of a, of a beating heart or something like that, then you would put
like a mesh on that beating heart.
Um, and you could sort of occur to mesh as opposed to a straight mesh.
Well, the mesh has to live on the manifold, right?
So there's a lot of nodes you distribute over this mesh and you connect them by little
lines, basically.
Um, and so, and so then you can try to maybe, you know, predict whether this heart is,
um, has, you know, strange behavior or something like that as abnormal behavior, um, or you
could try to figure out, you know, where's, you know, where certain pieces of the heart
are.
Are you want to segment them out or something like that?
Maybe you want to detect where the vessels are, the valves are or something like that,
right?
So certainly, you know, that's a, that's a prime example of where we have a, a manifold,
um, and where we want to do deep learning on that manifold.
Um, yeah.
And so that's certainly something that we are planning to do.
Okay.
And talking about this mesh, um, you also bring up the idea or brings up for me, the idea
of like graphs, um, is the gauge CNN kind of fundamentally a graph CNN or, um, is that
something that you're working on as well?
You're asking really good questions, I would say.
So in fact, um, one of the reasons why, um, you know, we are exploring this kind of mashed
version of gate CNNs, which is we put a mesh on the manifold and then we send messages
between the nodes over the edges, um, is that it looks a lot like a graph convolution,
which is, which is another object where you do deep learning on graphs.
But, um, a manifold, even a mashed manifold is not a graph because, um, this has something
to do with the fact that the neighbors are not sort of exchangeable.
The neighbors are, are not a set.
The neighbors are actually ordered, um, they, they, you know, something to their right
has a different meaning than something to the left.
And so people sometimes actually do graph convolutions on these meshes, um, but that's
actually slightly suboptimal.
And so the way we do it, so this, this gauge equity very neural networks, um, is precisely,
um, designed to do it optimally in, in a way, to do it the right way.
And we have a PhD student now at Qualcomm was Qualcomm and PhD student at University of Amsterdam,
Pim de Hanh, was precisely working on this topic.
So he's precisely trying to nail this topic of, you know, um, what is the difference between
a graph convolution and this mashed gauge convolution and, you know, is there a real practical
advantage above, you know, doing it correctly in a mathematical sense?
You know, is there also a practical advantage of doing it in this particular way?
Uh, so all of these, uh, we've talked about, uh, things from kind of very optimization-focused
ideas like compression to Bayesian deep learning and gauge, uh, CNNs within, within Qualcomm,
these are all more research oriented topics, as opposed to, um, kind of product oriented
topics.
Is that right?
Well, um, I would say that the, um, it's true to some degree, but I would say that compression,
neural network compression and quantization, um, has immediate, uh, sort of practical
applications.
And, in fact, you know, the team of time in blank, time in Blunkervort is already implementing
these methods into a toolbox, which is going to be commercialized.
So, and the reason why this is important is that, um, if I have my neural network that
I'm very fond of, and I trained it in the cloud, maybe using TensorFlow or PyTorch or
something like that, um, and now I want to run it on my phone, um, but it's too big to
run on my phone, um, because it takes too much energy and then my phone doesn't have that
much memory.
Right?
So, what I want to provide to the customer is a tool that automatically compiles this
big neural network into a much smaller neural network that basically has the same performance,
um, as the bigger one, um, but runs on our Snapdragon, uh, sort of AI chipsets, um, you
know, very, very efficient to be.
And, you know, doing that is actually, you know, you need to, you know, get the neural
network into a much more lean sort of, um, framework, um, and then you also need to compile
the operations that this neural network has to do, which is basically matrix multiplications,
matrix vector multiplications.
Um, you have to compile them in such a way that they run very quickly on the particular
piece of hardware, um, and, and you can also optimize that a lot.
Um, and so we are, for instance, also working on, um, on algorithms.
So this is Chang-Yong-Oh and Stratus-Govus, where, you know, people at, uh, this Q-Valab, um,
we are working on what we call Bayesian optimization algorithms, which is, um, algorithms which optimize
over a very large discrete space, right, of choices.
So the choices are, you know, should I first, you know, do this particular multiplication
and then this addition or should I first do it the other way around?
So there is an exponential number of possible choices that you have there in optimizing that
code.
Um, and so how can you quickly explore, um, all of these, all of these possibilities?
And so Chang-Yong developed a beautiful algorithm that, that, with a minimal number of trials,
um, quickly zooms in on the optimal sort of, uh, configuration of these discrete choices.
In order to apply a Bayesian optimization to this kind of problem, you first need to expose
the, I guess the features or the, uh, kind of the control levers, if you will, of the
problem, which operation goes first versus second, those aren't necessarily, necessarily
naturally, uh, expose from the models.
Is that right? Do, do you have to kind of do something or build something in order to
be able to apply Bayesian optimization to these kinds of problems?
Yeah.
So typically you can think of two levels.
So, um, let's, let's say on a phone, um, so if I have a particular proposal of doing
my computations, um, then I can, you know, compile that onto the phone, the actual physical
phone, and run it and measure how well I was doing.
Now, of course, that takes some time, right, we are talking about, you know, maybe seconds
or something like that, I don't know, um, to actually measure, you know, that, that
configuration.
And so if you do it that way, you go very slow.
Um, but you can also build a simulator, right, and the simulator would, um, basically
figure out in approximation how good that particular configuration would be.
But it's uncertain, right?
And so you can imagine that, um, you sort of, you know, you first do a couple, you know,
a couple of steps of optimization on the simulator, which is very fast.
And then when you get too uncertain, you then go to the physical device and you try a
few things, um, to see how, how well you're doing, you didn't update, update your simulator
or the sort of surrogate model that, that estimates how well you're doing.
Um, and then you sort of keep optimizing there again.
So there is this basically game that you play, um, you can do, you can measure things
very precisely, but quite expensively, or you can measure things very quickly, but sort
of approximately, right? And, you know, this is the choices that you have and you have
to navigate those choices as quickly as possible to get to the final best possible configuration.
One of the characteristics of dealing with, uh, devices that need to be produced in
silicon is relatively long lead times, as opposed to software products.
Can you talk a little bit about productizing these types of ideas in, in that kind of
environment?
Yeah, so I would say, um, we mostly work on software, right?
So in, so in, in, in many ways, what we do is, um, can, can be quite quickly productized,
I would say, because it's basically a software tool or, you know, enhancement of some kind.
Okay.
Um, but we do work with hardware folks, right?
So we do work on, you know, exciting new hardware developments, um, Princess Computing
Memory is something, you know, it's, it's well known, so a bunch of startup companies
are also working on that, um, that's basically where you would, instead of moving that data
that I talked to you about before, from, uh, the DDR memory to the chip, you would basically
do the computation directly in that memory cell.
So you would directly do your computation in, you know, in, in memory, which is actually,
you know, analog, so this is actually, you know, faults and currents and stuff like that.
Um, and so now the game is, and that's a very interesting game, you know, the game is,
you know, you try to develop a piece of hardware, um, that runs optimally for a particular
algorithm, like a deep learning algorithm in this case.
Um, and at the same time, you're trying to adapt the deep learning algorithm, uh, to work
as well as possible on that particular piece of hardware.
And so this is a trend, um, that you can see much more generally, which is that hardware
design and software design are starting to become more and more integrated and entangled
with each other.
It's not just here, but it's in many other places where you can see that, you know, a piece
of hardware being replaced by pieces of software run on, sort of, uh, a deep learning engine
on a chip, um, you know, you have, you know, an, in a heterogeneous compute environment
with many different types of compute, like DSP and CPU and GPU, et cetera.
Um, if you're faced with a certain computation, you will have to distribute that computation
across all of these, uh, you know, different compute engines, and you have to think smartly
about how to do that, right?
And again, there is a controller, sort of an intelligent agent that will have to learn
how to do this efficiently.
So you can see that software machine learning or, you know, learnable sort of software,
um, and hardware are going to get tight, more and more tightly integrated, which I think
is a very fascinating development.
Let's maybe shift gears and talk a little bit about, uh, kind of forward looking, uh,
ideas.
You recently wrote, uh, posts about the, uh, kind of responding to a rich Sutton blog.
I'll let you maybe talk a little bit about the background and then the posts, but it kind
of ponders this idea of, you know, what's most important models versus data versus compute.
Um, can you talk a little bit about that, that work and how you see that, um, kind of
playing out, uh, in the, the future of the space?
Yeah.
So I think this was more like my, sort of rainy Sunday afternoon, sort of write up about
something that, you know, I threw out there and it actually got a lot of attention.
So it was interesting.
Instead of hit the right, I guess the right nerve, people are very interested in that kind
of thing.
Um, and it, it's really about a super fundamental problem.
And I think, you know, researchers, uh, we should talk about this more because it, it
might determine from any young researchers in the field, you know, where they want to
head with their particular research.
Um, and so I was actually very grateful to rich that he posted that particular post.
Um, he basically said something and I made, I, I may, you know, charge you a little bit,
but it's like, um, you know, uh, we should really not, um, try to model all that much, um,
because in the end, uh, if we focus on scaling our architectures or sort of more general
purpose, um, machine learning architectures, um, if we wait long enough, then, you know,
using Moore's law, you'll basically get to a point where you always get beaten by these
kind of, uh, sort of scalable algorithms that are basically just, you know, eating data
and, and turning them into predictions.
Um, and, you know, there's been a lot of examples that actually, you know, where this was
actually the case, right?
So we had, um, you know, we had models of speech where, you know, people had built, you
know, models of the human, you know, uh, voice tract, um, and they were sort of, you
know, they had, they were, they were modeling how people produce speech and then they were,
you know, matching that with the actual observations and then trying to figure out, you know, what,
what the mouth, how the mouth actually moved and therefore what word was being spoken.
Um, and later people found, well, you know, if you just collect enough data, then, um,
you can just map, you know, basically take, take the, the audio signal that hits your
microphone, right, and, and map it, learn to map it to the words that produced it.
Um, and if you have enough of those pairs, like audio signal and word, um, then this,
this sort of, in, in some sense, stupid if you want, uh, sort of a statistical tool, uh,
vastly outperformed, um, these kind of more mechanical tools.
Um, and this happened in speech and it happened in, in, in, also in, um, in computer vision,
where, um, basically the best methods are now these deep learning methods where you basically
collect a huge amount of data on images, you segment them, you tell me what's the objects
in the image and then you train all that data and you get now with them that performs very,
very well, better than anything else.
I mean, this all goes back to the, I think the most, uh, quoted example of this, uh, and
there are many are the Peter Norvig, uh, unreasonable effectiveness of data paper, where he talks
about Google, you know, they're not better at this because they have better algorithms,
it's because they just have more data.
Absolutely, right?
So a lot of these internet companies are in the business of getting data.
Um, and by having all that data, they can do things that, you know, we didn't
thought were possible before, um, and this was a very valuable lesson, right? So, you know, collecting more data and
having the compute to do the computations, um, is basically, you know, one of the reasons why
we're seeing all this progress, um, but the, but it begs the question, how far can we take it?
Right? This particular idea.
And there's other people like Josh Tenenbaum, who has been, who have been saying the opposite.
So he's been saying, well, you know, um, the world actually operates in the other direction.
Which means that, you know, physics, um, basically the dead, the data generating process
is much more like we have objects in the world, these objects move under the laws of physics,
or maybe the laws of psychology or sociology when we interact with each other.
Um, there's causal relationships, right?
You know, things cause other things to happen.
And then finally, you know, signals hit our sensors, right?
And that's what's being recorded.
So that's the direction of, you know, the physics of the world into the sensors.
That's what we call the data generative model.
And then the, and then deep learning does the opposite.
It goes from these signals on the sensors and directly shortcuts a path to predict,
you know, what were the objects which were producing these signals.
Um, and actually the brain interestingly does something similar, right?
So we, we have in our brain the ability to, to simulate the world, right?
I can, I can close my eyes and I can imagine, you know,
what it means to ride on a horse or something like that or to fall off a building, right?
And I can just see it happen under the laws of physics.
At the same time, I can also instantaneously recognize objects in the world.
So there's these pathways in our brain which just take in sensory data and immediately produce
segment the world into objects and, and sort of tell me what's in the world without me thinking
about it. So in, in our brain, we have these two modalities as well.
And, and, and Kanamon calls this slow and fast thinking basically.
So it is two quite different pathways to, to think.
And so, um, so the question becomes, you know, how far can we take this good of data driven
approach? And, and my, my, my post was about, well, humans are a lot better in certain things
than current algorithms. Algorithms have trouble with generalizing away from the domain
in which they are trained, right? If I train an algorithm to play go, and then I tell it,
okay, now play chess, right? Or, or play go on a smaller board even or, you know,
with different color stones or something, it gets confused because it wasn't trained for that.
Right. For example, that, that always comes to mind for me was a video of, um,
and Peter Abil's lab training a robotic arm to think untangle a rope.
And it does great on a green, you know, when the rope is on a green table,
but when it's on a red table, and I'm making these colors up, but the idea is the same.
When it's on a red table, it totally fails. It's just, it's, the models are that dependent on the
specifics of, uh, the, with the environment in which they're trained. Yeah. And so, um, and,
and that's a very good example. And so, um, you know, maybe another example is, you know,
I was driving on a road a couple of times, and then, you know, there was road works. And what
they did was they kept all the white sort of lane dividers on the road, and they just put,
you know, yellow ones, you know, on completely different positions. And everybody knew what to do,
even though the, the white lane dividers were still there. Um, and I'm pretty sure an algorithm,
you know, when it's not trained on that, will that get totally confused?
Right. And what, so what's the underlying problem is that, um, so they don't generalize because
they don't understand the world in this generative data generating way. So, and, and, and,
you know, what Josh Tannenbaum has been saying about the others have been saying is that you
cannot collect nearly enough data to, you know, to capture all of your corner cases, right? I mean,
it's like, there's so many things that can happen in the world. It's like an exponential number
of things that can happen in the world. And many things are very rare. Um, and so we need ways to
generalize the lessons we learn in one context into sort of a completely different context. And,
you know, one school of thought is that the only way to do that is to understand the world in a
generative way. So, because, and the reason is that the generative direction is much more efficient
because, you know, it's, it's modular. Yeah, we have things that, you know,
relatively independently do things operate on these are the objects and the agents in the world,
right? And they follow causal laws. They follow the laws of physics. And there's very few parameters
in that, right? We know that, you know, the, the laws of physics eventually have very few parameters.
Um, and so, so that generative direction is a lot simpler than the opposite direction.
You said something really interesting in there, though, uh, the entire phrase was, was understand
the world in a generative way. When I think of most of the generative models that we're talking
about, there's not any understanding there. It's just spitting out probabilistically the, the best
next thing. Yeah. So, so understand, you know, thinking about, uh, what it really means to understand
something is a whole different area. But you could think about, you know, predicting the future,
right? So let's say, understand, if you understand the world at some level, I can predict the future
better, right? And now let's imagine, you know, um, I need to do some of a prediction. And I don't
know the causal laws. If I know the laws of physics and I know the laws of psychology and I know
what causes what it's going to be far easier for me to predict the future than if I don't have all
of that. Um, in fact, I can train something in one context and if I understand how physics work
and I get into a completely different context, I can still predict the future, right? Um,
because I understand the causal mechanisms. And so, so the claim is that the world is a lot
simpler in the causal direction or in the sort of physical sort of data generative direction.
And it's a lot more complicated than the opposite direction. Yet, if I define my application
area narrowly enough, I can collect a huge amount of data on that particular narrowly defined
problem. And then the inverse, you know, going directly from the sensors back to the predicting
the object is going to be more effective, right? Because that's the deep learning direction.
So with sufficient data, you know, that is the direction in which you actually also want to predict,
which is the inverse direction from the generative model. And, you know, that is the most effective
way of doing things if you have a lot of data and you have to find your domain narrowly enough.
And I think therefore, we need to find sort of a middle ground. We basically have to say if,
if, you know, if we don't know much about a domain, if we get thrown into a new situation,
we need to rely on our our generative models of the world. And we need to, you know,
try to, you know, invert those in order to make predictions. Yet, if we collect enough data
in a certain domain, then we can form this direct pathway from the sensors directly to making
predictions. And then that's going to be the most effective and accurate way of, of making predictions.
How do you take action on this idea or how do we as a community of practitioners and,
and researchers kind of take action on this idea? Well, it depends a little bit on what you're
interested in, right? So if you're interested in a narrow domain problem, like you want to predict
speech or you want to do speech translation or something like that, then you should go with the
deep learning approach, right? Because it just that, just the best thing because you can collect a lot of data
and works best. If you're interested in solving general AI, so developing, sort of agents that
are versatile and that can operate in many different circumstances, right? Then I think, you know,
you may have to start thinking about integrating these two models. And so what we have been doing
in the lab, in sort of my sort of M lab at the university, is we have been taking a sort of somewhat
older paradigm, which we call graphical models. So graphical models were models, which were very
popular, like maybe 10 years ago or so, where you would take sort of nodes in a graph. They seem
to be coming back a little bit, don't they? Maybe. I hear a ton about graphical models nowadays.
Like in recent NURPs, the past couple of years in NURPs, a lot of people seem to be doing work
around graphical models. Okay, well, it seems to stand to reason that, you know, something that is
good eventually will find its way back. The deep learning fire has burned out a little bit. And then,
of course, you know, you then you then get a phase where you can start to synthesize things,
where you can say, okay, we had this in the past. We have now, there's both are really good things.
Let's see if we can sort of combine them a little bit. So this is some of the work that we are
doing. So we have sort of a model where we will sort of generate, you know, we have a generated
model, let's say a Kalman filter, which is a dynamical model of, you know, somebody moving around
in the world and sort of observing sort of things about the world. And in these graphical models,
every node means something, right? That's the agent's position at every point in time is one node
in this graph. And if you want to figure out, let's say, okay, if I have these sort of,
these partial observations about the world, like a bunch of images maybe taken from the agent,
can I try, can I infer, you know, where the agent was? And that you can do with something that we
call inference. So that's a different inference than we these days, we call inference on the deep
neural net. This is a probabilistic inference where we basically say, what's the probability
distribution of this person being at this position at this point in time given all the observations
I have right now. And so that's actually a message passing scheme. So you send messages over the
edges of this graph to figure this out. That was belief propagation. And it was a very popular
research topic, you know, 10 years ago. Now we also have something. So the other option would
be to collect a huge amount of data. Basically, you know, person is here and observes this,
if person is here observes this, right? So if you collect a huge amount of data, you can also do it
the opposite way. You can say, okay, map directly from my observations, you know, back to the positions
of this particular person, because I happen to have that data available. And if I have enough of
that data, then I can actually do a better job, typically, than my Kelvin filter, because my,
you know, big, my Kelvin filter has like strong assumptions on linearity and Gaussianity,
um, basically build in it. And if the world isn't, you know, linear, um, then you're in trouble,
because, you know, you'll make, you'll make wrong predictions. Um, but if you just build this
neural network, which goes in the opposite direction, takes the observations and directly maps onto
the, onto the locations, then, you know, if you put enough parameters in it and you have enough
data, then you can train it very, very accurately, right? So now the trick becomes, okay, so if I put
into a new environment, I don't have data. So I will rely on this sort of, you know, clunky
Kelvin filter, but it will give me a fairly good estimate. And then as I go and collect more and
more data, as I mean, that is, as I live in that environment, I'm going to train up the model in
the opposite direction. Um, and, um, that at some point will become more accurate than my Kelvin
filter. Um, and then, you know, I just basically switch to the sort of, to the, to the deep learning
sort of solution. And so this is one example where, you know, we have a message passing scheme on a
graph that, you know, automatically switches between either the old-fashioned sort of, um, inference
and graphical model to the sort of more modern sort of graph convolutional neural networks.
Interesting. A lot of the folks that are pursuing deep learning as a path to AGI kind of
ultimately feel like the, you know, this thing that we call AGI or maybe the step before AGI will be
an ensemble of perhaps many, uh, deep models as opposed to, you know, one single Uber model.
If you kind of apply that same thinking to what you're describing, you can envision an ensemble
of many deep models and many generative models. And I'm almost thinking of like a hybrid car.
If you've ever, if you've been in a Prius recently, you might have seen that little picture where
they show you like whether it's the battery that's driving the motor or the, the motor that's kind
of charging the battery, kind of this back and forth flow, depending on what's happening and what
the model is being or the agent is being exposed to that's determining, you know, whether we're,
um, you know, relying more on the deep model in any particular point in time or the generative
model. Is that kind of the way you, you know, a way that you can see this evolving?
Yeah. So that's certainly, um, the way I see this evolving. So, um, let me stay with the example
of a car, maybe so you couldn't actually a modern car, even a self-driving car, would still have
a whole bunch of rule sets to cover all the corner cases. Um, because you can't collect enough data
on these corner cases, but maybe in the future, you know, there are enough, you know, for a particular
set of corner cases, let's say, uh, you know, you're driving on freeways, you can do fine, you've
collected a lot of data, but now you turn into Amsterdam, right? And there's so many exceptions
and difficult situations where bicycles will crush you and pedestrians will do weird things and
walk through red lights and all that, what's happening in Amsterdam. So you cannot rely on,
on just a learned model because it will fail, right? And so you have to basically go back to,
you know, if this, then that, if this, then that, um, and, um, and you have to know when to switch,
right? And so, and then maybe I can look back to Bayesian statistics. So you have to understand
when you don't understand, right? So it is deep learning model or this machine learning model.
Well, basically have to figure out, okay, so I'm running out of my, you know, domain where I'm
trained. Um, I'm starting to fail here. I'm going to switch over to rules or I'm going to switch
over to the actual driver, um, to make sure I don't get into an accident. Um, and it's this
interaction where, you know, and, and then you can imagine where a lot of cars are slowly,
you know, getting into new situations and they're all learning distributively, you know,
collectively, they're learning about some of these corner cases and they get embedded into the
model and then we slowly switch, you know, to using that model. And what's interesting to me about
that example is that the, the premise seems to be that we will get to a point where the complexity
of the environment is too much for the learned model and then we need to switch to rules.
Uh, but a lot of the, where learn models have proven themselves to be effective and powerful or,
you know, these situations where we can't come up with the rules because the rules are too complex.
Yeah. So, you know, clearly, um, in many situations where you have enough data, you should not
use rules, um, because, you know, it's just not good enough, right? Um, in fact, you should just
train that mapping directly. But the advantage of rules is that you can express them in human
language, right? I mean, we know in some sense when they're driving a car, right? You know, we know,
you know, when to stop for a red light, you know, when to, you know, when, when, when somebody is,
you know, passing the street or something like that or complicated combinations of situations,
we know how to, you know, express them in human language and turn them into a rule. I mean,
there's going to be a gigantic number of rules. I think it's, I don't know, genre,
million lines of codes in a car or something like that. So it's like huge. Um, but, you know,
it's, it is in some sense, you know, a backup system for situations that you can't cover with
your deep learning yet. And as I understand it, um, actually, I've, when I heard a talk about this,
uh, by Raquel Earthasoon actually, uh, working for Uber, uh, she mentioned that, um, uh, a large number
of, you know, the, the, you know, the large fraction of the intelligence of a car is actually
still rule based. I'm sure they want to move more to deep learning, but it's still a pretty large
fraction is still rule based. Awesome. Well, Max, this has been an amazing discussion. Uh, we could
continue. I'm sure for, uh, another hour, but, um, I really appreciate you taking the time to,
uh, uh, jump on with us and to share a bit about what you're working on. Thanks. It was a pleasure
talking to you. Fantastic. Thank you very much. All right, everyone. That's our show for today.
If you like what you've heard here, please do us a huge favor and tell your friends about the show.
And if you haven't already hit that subscribe button yourself, make sure you do so you don't miss
any of the great episodes we've gotten in store for you. As always, thanks so much for listening
and catch you next time.
