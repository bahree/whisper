WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.040
I'm your host Sam Charrington.

00:31.040 --> 00:35.380
If you're listening to this podcast, you probably have an opinion about AI and where it's

00:35.380 --> 00:36.380
headed.

00:36.380 --> 00:42.200
I personally think AI will make a huge difference in how we approach both our work and personal

00:42.200 --> 00:48.400
lives, but as of today, the biggest impacts for me are in the little things.

00:48.400 --> 00:52.800
But enough of what I think, I want to invite you to join the conversation.

00:52.800 --> 00:58.520
Jump on over to Twimbleai.com slash MyAI to let us know what you think about where personal

00:58.520 --> 01:01.000
and home AI is headed.

01:01.000 --> 01:07.560
Sharing your thoughts takes just two minutes and qualifies you to win some great prizes.

01:07.560 --> 01:13.080
In this episode, I'm joined by Charles Onu, PhD student at McGill University in Montreal

01:13.080 --> 01:19.640
and founder of Benoit, a startup tackling the problem of infant mortality due to asphyxia.

01:19.640 --> 01:24.040
Using SVMs and other techniques from the field of automatic speech recognition, Charles

01:24.040 --> 01:28.960
and his team have built a model that detects asphyxia based on the audible noises a child

01:28.960 --> 01:30.960
makes upon birth.

01:30.960 --> 01:35.600
We go into the process he uses to collect his training data, including the specific methods

01:35.600 --> 01:40.240
they use to record samples and how their samples will be used to maximize accuracy in

01:40.240 --> 01:41.920
the field.

01:41.920 --> 01:47.120
We also take a deep dive into some of the challenges of building and deploying who Ben was platform

01:47.120 --> 01:49.280
and mobile application.

01:49.280 --> 01:53.120
This is a really interesting use case, which I'm sure you'll enjoy.

01:53.120 --> 01:54.120
Let's go.

01:54.120 --> 01:59.960
Alright everyone, I am on the line with Charles Onu.

01:59.960 --> 02:03.400
Charles is a PhD student at McGill University.

02:03.400 --> 02:06.200
Charles, welcome to this weekend machine learning in AI.

02:06.200 --> 02:07.200
Thank you very much, Sam.

02:07.200 --> 02:08.560
I'm happy to be here.

02:08.560 --> 02:14.680
It's great to have you on the show. I had a chance to see your presentation on, I think

02:14.680 --> 02:20.920
the project was called the Benoit at the NIPS Black and AI workshop and it was very well

02:20.920 --> 02:21.920
done.

02:21.920 --> 02:23.920
Thank you.

02:23.920 --> 02:28.960
And I'm sure you'll tell us a lot more about that project, but before we do that, why

02:28.960 --> 02:34.040
don't you tell us how you got involved and interested in machine learning?

02:34.040 --> 02:39.400
Well, that story is very intertwined with the project, the Benoit that you just spoke

02:39.400 --> 02:40.400
about.

02:40.400 --> 02:41.400
All the better.

02:41.400 --> 02:48.440
Yeah, because indeed it was, it was, it might be to pursue that project that I got into

02:48.440 --> 02:54.520
machine learning and took a, I say, side roads from my original path.

02:54.520 --> 02:55.520
Okay.

02:55.520 --> 02:56.520
Yeah.

02:56.520 --> 03:02.960
So, so I had my undergrad education in electrical and computer engineering and when I finished

03:02.960 --> 03:05.880
my undergrad, I worked as a software engineer.

03:05.880 --> 03:13.640
I was doing that time that I was volunteering with an NGO called in Actus and then eventually

03:13.640 --> 03:18.280
co-founded one called Fisher Foundation with some colleagues in Nigeria.

03:18.280 --> 03:24.840
So I studied in Nigeria and that was when we, during our work in the local communities,

03:24.840 --> 03:30.520
we had projects across agriculture, health care and a number of other domains.

03:30.520 --> 03:36.840
And we came across the huge challenge of birth as fixed year, which is essentially when

03:36.840 --> 03:42.400
the baby does not breathe well right after birth and it was such a huge cause of infant

03:42.400 --> 03:49.280
mortality in our communities and but also in many parts of the world and particularly

03:49.280 --> 03:53.680
in developing regions or resource poor settings.

03:53.680 --> 03:59.440
So it was, you know, it was in a bid to see what causes this problem, how could we address

03:59.440 --> 04:00.440
it?

04:00.440 --> 04:06.200
I came across a research that, you know, first of all, detailed that one of the big reasons

04:06.200 --> 04:11.080
why as fixed year is causing such huge casualties in developing countries.

04:11.080 --> 04:16.000
So about one million babies die every year from as fixed year and another one million

04:16.000 --> 04:23.880
suffer severe lifelong disabilities like brain damage, intellectual disability, cerebral

04:23.880 --> 04:26.080
policy and so on.

04:26.080 --> 04:30.520
And one of the reasons why the causality was so high was because in most resource poor

04:30.520 --> 04:38.880
settings, the means for clinical diagnosis was too expensive, too resource intensive and

04:38.880 --> 04:40.640
was just not happening.

04:40.640 --> 04:41.640
Yeah.

04:41.640 --> 04:47.000
And so there was a need for something, a low cost way of being able to screen babies early

04:47.000 --> 04:51.280
enough for whether or not they are breathing well because when it's severe, it's easy

04:51.280 --> 04:55.960
to know, you know, the baby usually would not cry when it's not breathing at birth.

04:55.960 --> 05:01.240
But in many other cases, it's not as severe to cause baby not to cry, but it's there and

05:01.240 --> 05:05.320
it's causing damage to the brain of the oxygen starvation.

05:05.320 --> 05:10.960
And so the work we began to develop was based on clinical research done by doctors in the

05:10.960 --> 05:13.640
70s and 80s way back.

05:13.640 --> 05:18.880
And you know, they had looked at cries of infants suffering several pathologies and in particular

05:18.880 --> 05:23.160
as fixed year, they used their spectograms back in the days and we were able to notice

05:23.160 --> 05:28.760
that several perhaps characteristics of the cries, the fundamental frequencies and melodies

05:28.760 --> 05:34.720
and very important frequency components of the cries were different or in general in

05:34.720 --> 05:39.720
the population of babies that had some of these pathologies from those that, you know,

05:39.720 --> 05:42.520
were okay, but normal.

05:42.520 --> 05:48.120
And in particular, in the case of asphyxia, it's even more pronounced because breathing

05:48.120 --> 05:53.920
and speech are coordinated by the same region of the brain and within our vocal tract as

05:53.920 --> 05:59.920
well, the same set of organs are oscillating when a person is breathing and speaking.

05:59.920 --> 06:04.560
And so the connection is so intertwined that the baby is not breathing well, there is a

06:04.560 --> 06:07.280
manifestation in the frequency patterns in the scribe.

06:07.280 --> 06:11.600
And so it was upon, you know, I'm giving the long-winded story of how I came into a machine

06:11.600 --> 06:12.600
learning.

06:12.600 --> 06:19.440
And it was upon the story of this that I, I first of all, to head about pattern recognition

06:19.440 --> 06:20.960
as a discipline.

06:20.960 --> 06:21.960
Okay.

06:21.960 --> 06:27.000
And I began to study and take courses myself to learn more about it.

06:27.000 --> 06:33.640
And slowly began to, to, you know, to start what has become original as a project now.

06:33.640 --> 06:34.640
Yeah.

06:34.640 --> 06:35.640
Oh, wow.

06:35.640 --> 06:42.280
And you recently published a paper on this that won a Best Paper Award in the Machine

06:42.280 --> 06:45.000
Learning and Healthcare Workshop at Neps, is that right?

06:45.000 --> 06:46.000
Yes.

06:46.000 --> 06:47.000
That's right.

06:47.000 --> 06:48.000
Yes.

06:48.000 --> 06:52.360
Our paper won the Best Paper Award at Nips in 2015 Nips, actually.

06:52.360 --> 06:57.280
And at the last Nips, we published a new one, which was a follow-up to that paper in

06:57.280 --> 07:03.280
which we had moved from algorithm into an actual deployable device in the form of a smartphone

07:03.280 --> 07:04.280
application.

07:04.280 --> 07:05.280
Oh, okay.

07:05.280 --> 07:06.280
Okay.

07:06.280 --> 07:13.880
So maybe we can dive into that project, and you can tell us a little bit about, kind of

07:13.880 --> 07:16.840
a little, in a little bit more detail, how you approach it.

07:16.840 --> 07:24.480
You mentioned that the, the research came out of the 70s and 80s, is this the, the research

07:24.480 --> 07:28.680
that identified these fundamental relationships, or were you somehow able to borrow data from

07:28.680 --> 07:30.080
that research as well?

07:30.080 --> 07:31.080
Yeah.

07:31.080 --> 07:36.680
Well, the, this research, first of all, you know, told us that it was, there was something

07:36.680 --> 07:38.480
to be explored there.

07:38.480 --> 07:42.320
Because at the beginning, the last thing I would have thought of was to consider the infant

07:42.320 --> 07:46.360
cry as a useful signal for diagnostic too.

07:46.360 --> 07:51.960
And, and so it was really serendipitous, I'd say, to have stumbled upon one of these papers

07:51.960 --> 07:59.720
and to see that it was indeed a string of studies between the 70s and 80s, exploring the,

07:59.720 --> 08:03.560
the use of, you know, spectrograms to analyze the infant cries.

08:03.560 --> 08:08.000
And there was this winter afterwards in which nobody seemed to carry it about, you know,

08:08.000 --> 08:09.840
what infant cry could be used for.

08:09.840 --> 08:14.120
And my assumption was that, you know, at the time, there was no, there are no concrete

08:14.120 --> 08:20.040
methods for transferring this knowledge or this hypothesis into useful applications as

08:20.040 --> 08:25.440
we now have today with machine learning being so well developed as a field now.

08:25.440 --> 08:30.880
And several low cost technologies, like mobile phones, that was not conceivably at the time

08:30.880 --> 08:33.040
of the 70s.

08:33.040 --> 08:38.240
So my thoughts was, that was why some of the studies did not scale forward.

08:38.240 --> 08:43.640
You know, when you first came across this paper and got the idea that, you know, this might

08:43.640 --> 08:49.640
be applicable to this problem, you know, you mentioned that you started taking a bunch

08:49.640 --> 08:55.320
of courses, but what was some of the first, like, concrete things that you did to try

08:55.320 --> 08:59.440
to apply this knowledge that you came across?

08:59.440 --> 09:00.440
Yeah.

09:00.440 --> 09:06.480
Well, one of the, one of the first things I did was, you know, I had a doctor friend and

09:06.480 --> 09:08.680
we had long conversations about it.

09:08.680 --> 09:13.800
You know, I tried to understand physiology of the infant, of the infant breeding system

09:13.800 --> 09:14.800
from him.

09:14.800 --> 09:17.880
And, first of all, validated this was something worth pursuing.

09:17.880 --> 09:23.160
And we were together, you know, worked on the early part of the work that we eventually

09:23.160 --> 09:24.160
did.

09:24.160 --> 09:31.000
Well, the next challenge, too, was how do we find data to validate this hypothesis that

09:31.000 --> 09:35.400
infant cry holds information about the presence or not of asphyxia?

09:35.400 --> 09:36.400
Right.

09:36.400 --> 09:41.160
As you know, machine learning is based on the ability to use data and learn from it and

09:41.160 --> 09:44.320
be able to generalize from that going forward.

09:44.320 --> 09:50.520
But as you probably know as well, in medical space acquiring data is can be an extremely

09:50.520 --> 09:56.520
resource intensive process, what intensive time to get approval to interact with actual

09:56.520 --> 10:06.000
patients or time it takes to the amount of cost of funds it would require to conduct

10:06.000 --> 10:09.360
the whole study to acquire this data.

10:09.360 --> 10:13.960
And so one of the first things we thought was to find if someone had, by some chance, been

10:13.960 --> 10:19.880
collecting data of infants in the world, at the time, we only interested in say, we

10:19.880 --> 10:25.520
really thought we would find, you know, cries of babies that were normal.

10:25.520 --> 10:29.360
And if we were able to find that at least, we thought that would be a good point to begin

10:29.360 --> 10:33.720
to start modeling the inherent characteristics of cry.

10:33.720 --> 10:39.160
And to begin to phrase this as maybe an anomaly detection problem, where we have a good model

10:39.160 --> 10:48.480
of what a good cry should be, and we can say that any new example, any new sample,

10:48.480 --> 10:53.720
that doesn't fit this model, is likely an anomaly that has to be checked, you know, has

10:53.720 --> 10:59.360
to go for further verification and, you know, I'm referring to babies here.

10:59.360 --> 11:04.360
And that could be useful screening too that, you know, first responders could use to

11:04.360 --> 11:06.120
transfer babies for tertiary care.

11:06.120 --> 11:07.120
Right.

11:07.120 --> 11:11.120
So we saw we searched and eventually we came across the database that we used for our

11:11.120 --> 11:12.120
work.

11:12.120 --> 11:14.320
It's called the baby shilan to database.

11:14.320 --> 11:20.440
And it's collected across a set of specialist hospitals in Mexico.

11:20.440 --> 11:25.280
And this is a team of doctors who had been tracking several pathologies in babies.

11:25.280 --> 11:30.800
And they had collected the cries of babies in several states, from normal states like,

11:30.800 --> 11:38.160
from normal babies and in different states like hungry states, just a resting state, when

11:38.160 --> 11:44.280
receiving some measure of pain, through maybe blood sampling, and also they collected pathological

11:44.280 --> 11:49.240
states as well as fixier deafness and a number of other conditions.

11:49.240 --> 11:50.240
Okay.

11:50.240 --> 11:54.000
And so we reached out to them, you know, taught them about the work we were trying to

11:54.000 --> 11:55.000
do.

11:55.000 --> 11:58.840
And they gave us access to the subset of their data that had the normal baby's cries

11:58.840 --> 12:01.040
and the XPC database cries.

12:01.040 --> 12:02.040
Yeah.

12:02.040 --> 12:10.160
It was fairly small to set about 69 babies in total, but over a thousand or so recordings.

12:10.160 --> 12:15.600
And that was all we used to develop the, our first, our first work that I really showed

12:15.600 --> 12:21.960
that there was promising, was promising signal in the cry that we could use to develop it

12:21.960 --> 12:24.640
possibly it, it too for the agonies.

12:24.640 --> 12:25.640
Yeah.

12:25.640 --> 12:33.480
How did you use that data to develop a model, what kind of model did you end up pursuing

12:33.480 --> 12:35.640
and what was your general approach there?

12:35.640 --> 12:36.640
Yeah.

12:36.640 --> 12:37.640
Yeah.

12:37.640 --> 12:38.640
That's an interesting one.

12:38.640 --> 12:44.320
The primary subdomain of machine learning that we looked to was that of automatic speech

12:44.320 --> 12:45.320
recognition.

12:45.320 --> 12:53.200
As you can imagine, this is a very similar problem as well, whereas ASR is trying to take

12:53.200 --> 12:58.600
a speech and take a person's speech and understand what the person is saying.

12:58.600 --> 13:02.920
In this case, we're not trying to understand what the baby is saying, what the baby's body

13:02.920 --> 13:04.840
is communicating to us.

13:04.840 --> 13:09.080
And so you can think that the methods developed there will be useful.

13:09.080 --> 13:11.080
And so that was all we did.

13:11.080 --> 13:17.160
We adopted feature extraction methods like the use of male frequency, septal coefficients,

13:17.160 --> 13:18.160
MFCCs.

13:18.160 --> 13:26.560
So MFCCs are an extraction of key frequency components in the speech signal.

13:26.560 --> 13:31.720
And they've been used a lot in ASR because it's been, it's been easy, the methods of discrete

13:31.720 --> 13:37.360
Fourier transforms to extract these key components that really put society things that are not

13:37.360 --> 13:43.400
as relevant, you know, the artifacts of speech that are not as relevant and brings into

13:43.400 --> 13:46.200
the, for the most important component of it.

13:46.200 --> 13:47.200
Okay.

13:47.200 --> 13:53.120
And so MFCCs have used a lot in ASR and we used that for speech recognition and feature

13:53.120 --> 13:56.120
extraction path.

13:56.120 --> 13:59.840
And we use support vector machines at the classification phase and combine these with,

13:59.840 --> 14:04.440
you know, with non-linear kernels like the Gaussian kernels and so on.

14:04.440 --> 14:09.360
So that's made the core of our system in general.

14:09.360 --> 14:16.680
So you had the, you had these thousand recordings, you passed them through some pre-processing

14:16.680 --> 14:22.920
steps that basically broke them down into frequency components and those became your features

14:22.920 --> 14:24.520
for your SVM.

14:24.520 --> 14:28.320
Yes, exactly.

14:28.320 --> 14:34.400
And how, you know, beyond that initial kind of model, like, did you run into any challenges

14:34.400 --> 14:37.080
in, in doing that?

14:37.080 --> 14:41.160
So any challenges in building the classifier, you mean, from building the classifier,

14:41.160 --> 14:42.160
yes.

14:42.160 --> 14:43.160
Yeah.

14:43.160 --> 14:48.320
Well, the good thing was, you know, we've, we've had this data that had been collected.

14:48.320 --> 14:52.920
It had been, you know, someone had someone else had gone through all the work of getting

14:52.920 --> 14:58.600
a clinical approval for it to conduct this study, collected the data, you know, filtered

14:58.600 --> 15:06.120
the data, they had, you know, annotated it by labels, you know, which ones are the pathological

15:06.120 --> 15:08.840
samples and healthy samples and so on.

15:08.840 --> 15:13.920
So, so as you probably know, machine learning is a lot of work that goes into that early part

15:13.920 --> 15:18.800
which we were, fortunately, saved from the engagement.

15:18.800 --> 15:20.960
At least at that time.

15:20.960 --> 15:26.240
So, so we spend most of the time, you know, developing the classifiers and tuning and, you

15:26.240 --> 15:32.400
know, and searching the space of what's the best solution within this range of parameters

15:32.400 --> 15:36.520
between the MFCs extraction and the SVMs and hyperparameters.

15:36.520 --> 15:42.360
But, you know, ultimately, we are, we are now somewhat back to that first stage because

15:42.360 --> 15:48.000
we're able to use that data to show the promise of this approach, to show the, the feasibility

15:48.000 --> 15:49.880
of, of this method.

15:49.880 --> 15:55.920
So now we need to, one, validate it, you know, with data we collect ourselves, we need to,

15:55.920 --> 15:59.600
we need to get a larger data set to improve the performance.

15:59.600 --> 16:05.320
So, the performance of the system we have now is about 90% on specificity and sensitivity

16:05.320 --> 16:06.320
measures.

16:06.320 --> 16:12.640
That's in accuracy in detecting the babies, the expiated babies and that sensitivity

16:12.640 --> 16:16.760
and accuracy in detecting normal babies is our specificity.

16:16.760 --> 16:23.920
And so, you know, the goal is to try to, one, robustly validate this, this, this results

16:23.920 --> 16:29.880
we have using samples acquired from a different geographical location, different population.

16:29.880 --> 16:35.480
And hopefully acquired this more data to use it to improve the performance of the algorithm.

16:35.480 --> 16:41.560
And I'm beginning to develop into, into our software, very practical issues that would

16:41.560 --> 16:47.840
have to face in the real world, like making sure it's robust to noise, and because the samples

16:47.840 --> 16:55.160
were coded in very controlled environment without background noise, and just several optimizations

16:55.160 --> 16:59.880
by, you know, for instance, in the presence of overlapping signals, you know, several of

16:59.880 --> 17:05.920
these crying at one, can we separate them, the five one, can we optimize for the length

17:05.920 --> 17:13.400
of the audio signal that we require to make a useful, a valid diagnosis, and so on,

17:13.400 --> 17:15.840
and more of such things, we're looking at now.

17:15.840 --> 17:16.840
Okay.

17:16.840 --> 17:21.040
And now, one of the tools that will help you do all this is you were able to take the

17:21.040 --> 17:27.080
work that you did initially and then build a mobile app around this.

17:27.080 --> 17:31.440
Mel, tell us a little bit about that process and kind of what stage you are, you're in

17:31.440 --> 17:34.240
with deploying this mobile app.

17:34.240 --> 17:36.240
Yeah.

17:36.240 --> 17:39.720
So, you know, in terms of the development of mobile app, one of the challenges is that

17:39.720 --> 17:47.320
a lot of, a lot of machine learning has, you know, happens on Python and MATLAB, and

17:47.320 --> 17:52.600
what those are not the languages that are used in mobile development, right, in general.

17:52.600 --> 17:58.200
And so, you know, but they really help with experimentation and, and, you know, the fast

17:58.200 --> 18:03.000
tone of our design to take process, but thinking of deployment, you know, we had to start

18:03.000 --> 18:07.360
thinking about, first of all, we had to decide upon what was the best mobile platform

18:07.360 --> 18:12.240
for a place like Nigeria, where I lived when this, when we started this project.

18:12.240 --> 18:18.960
And Nigeria, Android phones are 90% or 95% of what people use.

18:18.960 --> 18:22.080
So that was our go-to platform to start with.

18:22.080 --> 18:27.840
Android at the time, at least, was 2014 and there about the main platform for development

18:27.840 --> 18:31.080
or the main language for Android development was Java.

18:31.080 --> 18:38.200
And now there's been C-shop portals to be done, I think Python, possibly now, yeah, but

18:38.200 --> 18:44.240
also the one of the big challenges too was transferring our code to Java and maintaining

18:44.240 --> 18:51.360
efficiency, maintaining performance as well, accuracy, and, you know, just all of that

18:51.360 --> 18:52.360
thing.

18:52.360 --> 18:53.360
So, it took quite a while.

18:53.360 --> 18:58.360
That was quite a bit of a few weeks or maybe months, we spent on, on just that part

18:58.360 --> 19:04.240
of the work, but we're able to do it ultimately and my colleague, Innocent, who is also a software

19:04.240 --> 19:05.240
engineer.

19:05.240 --> 19:10.560
We're able to completely move our code and maintain performance both in terms of time

19:10.560 --> 19:16.560
to diagnosis, we're able to maintain the original classification performance on the data

19:16.560 --> 19:19.960
set as well when we put it in the mobile application.

19:19.960 --> 19:23.560
So that was one challenge we had to face and it was good.

19:23.560 --> 19:28.920
And we also mentioned that we face this challenge too because we wanted to put the classification

19:28.920 --> 19:31.560
model on the device.

19:31.560 --> 19:36.240
We did not want it to require internet to do classification because if we are trying

19:36.240 --> 19:42.400
to deploy in some of the poorest parts of the world, in Nigeria that's also not so poor,

19:42.400 --> 19:48.280
the internet access is still very, very, very, very scanty in many places.

19:48.280 --> 19:54.000
And so if the device required internet to make it a diagnosis, then the purpose is halfway

19:54.000 --> 19:56.400
defeated already.

19:56.400 --> 20:02.800
And we could go there in the long run, but in the media term it was the most practical

20:02.800 --> 20:04.760
thing for our target groups.

20:04.760 --> 20:09.040
And so we went through that and we got it onto the mobile app.

20:09.040 --> 20:13.600
So where we are now is the questioner, so in terms of validating our mobile application

20:13.600 --> 20:20.240
and that's really love what's taking us, it's what takes out most of our time presently.

20:20.240 --> 20:26.520
So in Montreal, at the Maggi University Health Center, at the Children's Hospital here,

20:26.520 --> 20:32.560
we are doing the validation exercise in one year exercise with the doctors here.

20:32.560 --> 20:39.560
And the goal is to acquire more samples from babies who have experienced in different

20:39.560 --> 20:46.560
levels of species from mild to moderate to severe, acquire control samples of normal babies,

20:46.560 --> 20:53.680
and then to use a binwad to validate against, to classify the samples essentially.

20:53.680 --> 20:58.480
So these samples are going to be, the cry samples of these babies are going to be evaluated

20:58.480 --> 21:01.640
very clinical methods of diagnosis.

21:01.640 --> 21:07.040
So they have blood samples be taken to be analyzed with a blood gas analyzer and a doctor,

21:07.040 --> 21:12.720
especially doctor, would clinically determine what to label these babies as, and then

21:12.720 --> 21:18.840
validate these against the binwad and hopefully further develop the algorithm in that process.

21:18.840 --> 21:24.880
And the next stage after that would be to then take this to Nigeria and do field trials there.

21:24.880 --> 21:31.040
Our regional plan was to do this stage in Nigeria and it's still not a shutdown plan completely

21:31.040 --> 21:37.160
but we faced many challenges with that. Just because, whereas the whole clinical process

21:37.160 --> 21:42.280
I just explained of blood gas analysis and so to confirm the presence of our species,

21:42.280 --> 21:49.560
whereas it's a routine procedure in Montreal, in Canada, in Nigeria it doesn't happen,

21:49.560 --> 21:51.960
which is the root of this problem differently.

21:51.960 --> 21:52.960
Right.

21:52.960 --> 21:56.040
Equipments do not exist in any of the public hospitals there.

21:56.040 --> 22:00.040
There's no power, there's no electricity for the most part, so if we use it if it was

22:00.040 --> 22:08.520
there, there's a whole workflow and process requirements that should be in place or that

22:08.520 --> 22:11.720
would have to put in place if we were to try to do it there.

22:11.720 --> 22:18.160
And a lot of this would cause put time, money, effort and it would be a huge deviation

22:18.160 --> 22:22.720
from the ultimate or the precise focus of the project.

22:22.720 --> 22:26.600
And so after much thought from back, we decided that we'll do the first stage here and then

22:26.600 --> 22:31.680
move to do it, filter out in Nigeria with the doctors who work with there as well.

22:31.680 --> 22:39.160
And now it sounds like the samples you're collecting at McGill are, there's a high level

22:39.160 --> 22:44.280
of care clinically and evaluating the samples.

22:44.280 --> 22:54.760
Are you also collecting them with more professional equipment or are you trying to collect the

22:54.760 --> 22:59.080
samples via the mobile app so that you can kind of match the conditions that you'll be

22:59.080 --> 23:02.200
collecting them with in the field?

23:02.200 --> 23:07.960
Yeah, that was indeed a decision point for us, whether to use the mobile apps, the mobile

23:07.960 --> 23:13.080
phones themselves to apply it there or to use specialized or the recorders.

23:13.080 --> 23:16.240
And ultimately we went for special or the recorders just because of that.

23:16.240 --> 23:17.240
Oh really?

23:17.240 --> 23:18.240
Yeah, we did.

23:18.240 --> 23:24.440
We really want to maintain the one, the recording quality across the subjects, across the samples.

23:24.440 --> 23:33.320
We want to track things like sampling rates and just, you know, exclude some of the immediate

23:33.320 --> 23:39.440
let's say Mr. Lenners' functionalities are coming with a mobile phone.

23:39.440 --> 23:45.000
Also, you know, we've had some slight issues with the ethics before, I don't know if this

23:45.000 --> 23:50.560
probably lands on phone things I should not talk about as well, but yeah, there's also

23:50.560 --> 23:56.880
some ethics issues with using the phone right away for the acquisition and yeah, so we're

23:56.880 --> 24:02.000
going to be using digital audio recorders to pick these signals at this point.

24:02.000 --> 24:03.000
Okay.

24:03.000 --> 24:08.200
Yeah, I'll take your word on it if it's something you can't talk about, but you certainly

24:08.200 --> 24:09.200
pick my curiosity.

24:09.200 --> 24:16.080
I mean, I can imagine in the U.S. we have laws like HIPAA that require a certain level

24:16.080 --> 24:22.640
of standard of care with patient information, I imagine there's that kind of thing going

24:22.640 --> 24:23.640
on.

24:23.640 --> 24:27.280
Yeah, things on that line.

24:27.280 --> 24:28.280
Okay.

24:28.280 --> 24:32.800
Yeah, I mean, it's, it's, it can get complicated.

24:32.800 --> 24:38.680
You've got these samples that you're capturing with specialized audio equipment.

24:38.680 --> 24:44.920
How then do you address the issues that you mentioned previously, you know, kind of

24:44.920 --> 24:50.000
there, the transfer, transferability of those samples to the samples that are collected,

24:50.000 --> 24:53.920
you know, via your mobile device, so your background noises, you're overlapping, like

24:53.920 --> 25:01.880
are you, you recreating these situations after the fact or is that just reserved for a later

25:01.880 --> 25:04.000
stage of developing your model?

25:04.000 --> 25:09.760
Yeah, yeah, at the current stage, we are going to be working on those optimizations more

25:09.760 --> 25:15.200
in the research settings, so we're going to simulate what noise it looked like, okay.

25:15.200 --> 25:19.960
We could go out, you know, the sample, one way which we could do the case of the noises

25:19.960 --> 25:25.400
to record noise in record actual noise signals in the environment, we think the Apple

25:25.400 --> 25:31.000
be used and think that and use that noise to, I wanted to say noisy our samples further

25:31.000 --> 25:36.840
with a bit too much use of noise, but we use that to corrupt our samples essentially.

25:36.840 --> 25:40.080
And then, you know, work more from the research, so our research team is going to be working

25:40.080 --> 25:46.000
more on the research side of how can we improve the algorithm, making more robust noise.

25:46.000 --> 25:50.160
And I guess the more practical test of whatever we develop would happen when we are able

25:50.160 --> 25:56.200
to go through this phase of validation and begin to test on the fields in Nigerian, so

25:56.200 --> 26:01.600
we can put it down on the field and say, and test it in reality.

26:01.600 --> 26:07.040
How many samples are you expecting to collect in the current phase?

26:07.040 --> 26:14.320
Oh, you shouldn't have asked that, now I'm going to, now I'm going to disappoint you

26:14.320 --> 26:21.920
if you've heard about machine learning data, but we are collecting a total of 100 samples.

26:21.920 --> 26:24.640
Oh, wow, okay.

26:24.640 --> 26:30.560
You know, it's interesting because I was, I think perhaps the precursor to that question

26:30.560 --> 26:36.560
was, you know, thinking about, you know, whether this is something you would also evaluate

26:36.560 --> 26:41.520
like a deep learning type of approach, and I figured maybe one of the, you know, the

26:41.520 --> 26:47.000
limitations would be the number of available samples and needing a much more efficient

26:47.000 --> 26:48.000
model.

26:48.000 --> 26:50.000
Yes, yes, that's correct.

26:50.000 --> 26:55.000
I mean, first of all, getting medical data is really hard.

26:55.000 --> 27:02.200
I work on another study at McGill, in which we are also collecting cardio respiratory

27:02.200 --> 27:10.320
signals of newborns, and over a period of since 2014, it's taking us from then to 2017

27:10.320 --> 27:16.080
December to collect a total of 230 newborn data.

27:16.080 --> 27:22.080
So we're tracking, you know, respiratory difficulties in the intensive care unit.

27:22.080 --> 27:26.680
And so the bottom line is, yeah, it's really hard, it takes time to get, you know, quality

27:26.680 --> 27:27.680
medical data.

27:27.680 --> 27:35.520
And, but that's also part of what turns us more towards classical machine learning and

27:35.520 --> 27:42.960
less away from automatic representation methods like deep learning, so that we can really

27:42.960 --> 27:49.520
have control of what features we extract and really make direct connections between,

27:49.520 --> 27:53.960
the features and the cause they have on the system and, you know, and their projectability

27:53.960 --> 27:55.560
as well.

27:55.560 --> 28:02.960
On that note, are there, you know, you started with this understanding from the prior

28:02.960 --> 28:11.800
research that the frequency components would have a big play a big role in identifying

28:11.800 --> 28:19.200
the asphyxia samples in that kind of feature engineering process, feature identification

28:19.200 --> 28:20.200
process.

28:20.200 --> 28:25.320
Did you identify any new features beyond what you found in the existing research, and

28:25.320 --> 28:29.400
in particular, was there anything that you found that surprised you?

28:29.400 --> 28:38.400
I'll say, because there's such a wide range of audio characteristics that I was explored

28:38.400 --> 28:44.600
back then, but one of the features that I really stood out the most was the fundamental

28:44.600 --> 28:51.960
frequency of the cry, because the fundamental frequency is one of the most studied elements

28:51.960 --> 28:56.200
because it's like the very first thing that describes an audio signal even in music

28:56.200 --> 29:02.360
to the fundamental frequency determines the key of a song, and so it more or less determines

29:02.360 --> 29:08.800
what the, at what pace or at what pitch is this child crying at.

29:08.800 --> 29:14.160
And that would say it's been the most significant features consistently across our work.

29:14.160 --> 29:16.880
And across the previous work that was done as well.

29:16.880 --> 29:24.560
Okay, so it's not some, you know, there's not some kind of mysterious thing that's happening,

29:24.560 --> 29:31.080
like some, you know, it's like just the basic, you know, most fundamental thing that you

29:31.080 --> 29:33.760
would get out of this audio signal.

29:33.760 --> 29:40.200
Yeah, but the interesting question comes in in the, in how it varies in the time across

29:40.200 --> 29:41.200
time.

29:41.200 --> 29:42.600
So the time very in nature of this signal.

29:42.600 --> 29:47.960
And that's why it's not a simple problem that you could set it threshold or chord

29:47.960 --> 29:52.480
and say, okay, every value above this, right, right.

29:52.480 --> 29:56.800
So that's really why machine learning was necessitated because it's the complex time

29:56.800 --> 30:03.600
very nature of the signals and of these characteristics that is just impossible to put down a rule

30:03.600 --> 30:08.320
that says, you know, maybe the has this, this and this is going to be normal and otherwise

30:08.320 --> 30:10.880
for, for us fixated.

30:10.880 --> 30:15.200
Now, how do you capture that time very nature with the SVM?

30:15.200 --> 30:21.560
Yeah, so SVMs inherently are not time, time series classifiers.

30:21.560 --> 30:24.960
And so we've used the combination of several methods.

30:24.960 --> 30:29.720
So one of the things we did was use Gaussian kernels for the SVMs, which, which primarily

30:29.720 --> 30:37.240
do with scalar features, or we then segmented the samples into, into several time chunks

30:37.240 --> 30:43.560
and, and use the instances in time, as if they were independent for training.

30:43.560 --> 30:48.880
But at test, I, we test the, we test the instances, again, independently, but combine their

30:48.880 --> 30:53.840
results to determine whether or not we've, the performance has worked, the performance

30:53.840 --> 30:59.360
of the system is doing well at this, at the level of this subject now, at the level

30:59.360 --> 31:01.960
of the independent time instances.

31:01.960 --> 31:07.280
And so that way, by guiding the search, basically through our, our parameters, by guiding the search

31:07.280 --> 31:14.040
with the ultimate performance on the subject, we can go to a better, hyper parameter space

31:14.040 --> 31:18.720
that really optimizes performance at the level of the subject based on the nature of each

31:18.720 --> 31:20.720
of these instances in time.

31:20.720 --> 31:21.720
Hmm.

31:21.720 --> 31:25.960
Yeah, can you give me an example, maybe of how that, how that works?

31:25.960 --> 31:34.280
Like, what, how does the, the relationship between these samples, give you, you know, allow

31:34.280 --> 31:40.520
the algorithm to, to key in on this time-sensitive variation?

31:40.520 --> 31:48.040
Yeah, so, so you could look at it as, imagine we recorded, hypothetically, the, the length

31:48.040 --> 31:51.040
of the samples we have was ten samples.

31:51.040 --> 31:53.520
So that's it was one second at ten hertz.

31:53.520 --> 31:54.520
Okay.

31:54.520 --> 31:57.560
Ten seconds, ten, ten samples per second.

31:57.560 --> 31:58.560
Hmm.

31:58.560 --> 32:00.560
So this is for one subject, for one infant.

32:00.560 --> 32:07.520
So what we do is, you know, we take each of this, the, the feature, the feature vector

32:07.520 --> 32:11.920
at each of these time instances, so to be clear again, at each of these ten time instance,

32:11.920 --> 32:17.160
we have a feature vector of some length, the feature vector is determined by the number

32:17.160 --> 32:23.080
of characteristics we decide to extract from the, from the audio signals, you know, including

32:23.080 --> 32:29.200
the fundamental frequencies and many other characteristics from the MSC features.

32:29.200 --> 32:33.560
And so we take that feature vector of, say, some, some value, let's say it was three.

32:33.560 --> 32:38.400
And we take that feature, those feature vectors, we decore it as, first of all, for training.

32:38.400 --> 32:44.320
And the goal released to find in the, in the high dimensional space, is there a space

32:44.320 --> 32:51.120
on which we can separate the instances from success samples, from the time instances

32:51.120 --> 32:55.960
of failure samples. But when we do that training to try and separate them and find a good

32:55.960 --> 33:01.240
hyperplane, the high dimensional space, then on the evaluation point, we take, of course,

33:01.240 --> 33:05.360
a separate set of samples. So we don't use the same subjects we use in training as the

33:05.360 --> 33:10.560
evaluation phase. In the evaluation phase, we take each of the instances, the ten instances

33:10.560 --> 33:16.520
of a particular subject to evaluate it. We classify them based on the, the model that

33:16.520 --> 33:23.600
we are choosing to evaluate at this point. And, and ultimately, we, we then use another

33:23.600 --> 33:29.040
metric. So in our case, we, we, we explored both the use of just a simple majority count

33:29.040 --> 33:34.280
of the instances, but also we explored using another classifier, another support vector

33:34.280 --> 33:40.760
machine to be able to classify the predictions. So it's like a meta classifier to be able

33:40.760 --> 33:45.200
to classify the predictions of each of the ten instances. So each of the predictions

33:45.200 --> 33:50.600
become their own features to the next classifier. And then we use that to predict, to make

33:50.600 --> 33:59.760
one final prediction for that subject. And is that, yeah, is the meta classifier method

33:59.760 --> 34:05.400
more performant than a majority or some kind of quorum based system?

34:05.400 --> 34:10.640
Yeah, in our case, it performs slightly better than just doing a majority count. And that's

34:10.640 --> 34:17.640
partly because it's able to take into account temporal dynamics now of this, of the, the

34:17.640 --> 34:24.640
original instances in time. It's able to, it's able to find the relationship between,

34:24.640 --> 34:29.240
between these instances over time, pretty much, and connect the ultimate outcome.

34:29.240 --> 34:39.480
And the example that you use was ten data points a second is, is one second, is that the,

34:39.480 --> 34:46.480
was that just a, for illustration or is, it does one second, worth of sample, give you

34:46.480 --> 34:53.920
enough to identify this time varying indicator in the fundamental frequency of aspects.

34:53.920 --> 35:01.840
Yeah, so one of the, one of the things that we've done is, we use longer segments, but

35:01.840 --> 35:06.760
we, first of all, break them down into one second segments. It's also a way of, of dealing

35:06.760 --> 35:12.320
with the fact that we don't have so many subjects. And we want to make sure that whatever we build

35:12.320 --> 35:20.280
is, it leaves room for, for uncertainty. And so what we do is we break down the samples

35:20.280 --> 35:26.280
into one second segments, perform classification on each one second segment, and then average

35:26.280 --> 35:30.680
the results to give a prediction for that subject. So there's several levels of breaking

35:30.680 --> 35:37.520
down, including this. So we, we, we, we, we, we are the very lowest level deal with the

35:37.520 --> 35:42.280
one second segments. And then we, we, we, we, we, we, we, we, we, we average data on

35:42.280 --> 35:46.080
sample data at the higher level to make a prediction for each patient. So in practice

35:46.080 --> 35:49.480
to, in the test, we did with the mobile application. What we do is, we're called lens

35:49.480 --> 35:54.040
of 10 to 20 seconds. And we break it down that way for the classification. So we split

35:54.040 --> 36:01.120
it. That also helps us to do parallel computations if the device supports it. And then we eventually

36:01.120 --> 36:09.600
combine the predictions. And in terms of, you know, where you are currently, what kind

36:09.600 --> 36:16.960
of results are you saying with the, with the method? So, so the, the paper we have published,

36:16.960 --> 36:24.760
we obtained about precisely 86% of sensitivity. So that was the detection rate of infants who

36:24.760 --> 36:32.240
had a sphixia. And it's 9% of a specificity. That was detection rate of infants who did

36:32.240 --> 36:38.680
not have a sphixia who were healthy. And this is the one we published in, in 2015, that's

36:38.680 --> 36:46.080
one the basic part of what that nips. Nips machine learning for health care. Right, right. And

36:46.080 --> 36:53.560
practically speaking, your, your baseline is against, you know, in the field, it's against

36:53.560 --> 37:01.840
an unated doctor trying to identify asphyxia just based on being presented with an infant

37:01.840 --> 37:07.320
that isn't responding normally. Yeah. Yeah. I mean, that's what would, would like to,

37:07.320 --> 37:11.640
to change. And that's, you know, that's not good enough to have the doctor just eyeball

37:11.640 --> 37:16.600
it. Right, right. But it should be clear that there's a very clinical, there's a gold standard,

37:16.600 --> 37:20.040
you know, there's a gold standard, which is used in, in Canada and in many other parts

37:20.040 --> 37:24.240
of the world. That's the blood. And that's a general. Exactly. The blood class analysis.

37:24.240 --> 37:28.920
Okay. Right. So detect this example of the baby's blood through, usually through the

37:28.920 --> 37:34.400
cut on bleak record, the arterial blood sample. And they analyze it for several parameters,

37:34.400 --> 37:40.640
bilirubin, acidosis and electrolytes and so on. And usually they combine this with the

37:40.640 --> 37:46.400
abgas core of the infant. The abgas core is a, is a score assigned to literally, literally

37:46.400 --> 37:52.000
every baby who has been born for the last 40 years. And it's a physical assessment of how

37:52.000 --> 37:57.440
old baby is doing on five measures. So that's combined with this blood gas analysis to make

37:57.440 --> 38:05.760
a confirmatory diagnosis of a fiction. And that is, is the blood gas analysis also something

38:05.760 --> 38:10.720
that is routinely performed or is it performed when there's, you know, a certain level of the,

38:10.720 --> 38:17.840
of the score or some indication of a problem? Well, in Montreal and the rest of Canada,

38:17.840 --> 38:22.880
it's a routine process for every baby that's born. And really that's one of the things that makes

38:22.880 --> 38:28.320
it very convenient for us to do the study here. It's because we would not have to as part of our

38:28.320 --> 38:34.720
study design decide how we collect the gold standard. The gold standard is already a part of

38:34.720 --> 38:39.440
day-to-day clinical operations. We just have to read the charts to get that data. So yeah,

38:39.440 --> 38:47.520
it's routine here. Interesting. And you mentioned that you're working on another project

38:47.520 --> 38:57.280
there as well in addition to Benoit. What's that one? It's called the APEX project. APEX has in

38:57.280 --> 39:06.960
A4APEX, the stands for automated prediction of extubation readiness. It has, I guess, one technical

39:06.960 --> 39:14.000
term, the extubation. And basically that refers to the process of winning a child who is on the

39:14.000 --> 39:21.680
respiratory support. Yeah, so usually infants who are born pre-term, pre-matchell, which is

39:21.680 --> 39:30.000
usually around seven weeks or they're about, is I usually, the lungs are actually not well developed

39:30.000 --> 39:35.360
well-formed to support spontaneous breathing for them. And so they usually require respiratory

39:35.360 --> 39:41.040
support. And this happens in general through the insertion of a tube down the attractor.

39:41.040 --> 39:47.920
Yeah. And the other end of the tube is connected to a ventilator. And this ventilator provides oxygen

39:47.920 --> 39:55.520
to the infant at a certain interval, you know, tunable by the diffusitions. But these positions

39:55.520 --> 40:02.080
have to make a very critical choice under this setting. So they must decide when to remove the

40:02.080 --> 40:08.880
infant from this setup, from mechanical ventilation, as it's called, because the longer you leave the

40:08.880 --> 40:14.000
infant on that, that system, the increased chances of lung disease, because, you know, you've placed

40:14.000 --> 40:21.280
a plastic tube in the trachea. And that causes less interactions with the walls of the trachea

40:21.280 --> 40:27.280
and so on. And so to that degree, it causes something called BP, the bronchopulmonary dysplasia,

40:27.280 --> 40:32.160
which effectively is lung disease. And that's bad because it's going to be a live lung disability

40:32.160 --> 40:36.640
in most cases. So, but also you don't want to remove it too early. So that's the case when you

40:36.640 --> 40:41.760
leave it too much too long. If you remove it too early, the infant may not be ready to breathe on

40:41.760 --> 40:48.480
its own. And what could happen is that the infant would require re-intubation. So intubation is a

40:48.480 --> 40:53.680
process of putting the tube in and extubation is a process of removing the tube. The infant could

40:53.680 --> 41:00.320
require re-intubation. And re-intubation is a technically difficult challenge for infants that have

41:00.320 --> 41:05.360
been integrated already because they're swelling the trachea and so on. And in some cases they never

41:05.360 --> 41:11.440
succeed and the baby ends up having to die. And so there's this trade-off between how one is the

41:11.440 --> 41:19.440
optimal time to extubate an infant on the intubation in the ICU. Can I ask the stupid question,

41:19.440 --> 41:25.520
which is can't they remove the oxygen source without removing the tube and see if it works?

41:26.560 --> 41:29.360
Well, that turns out to not be a stupid question.

41:29.360 --> 41:38.800
Because for our study, that's indeed what we are tracking. Because one of the things the doctors

41:38.800 --> 41:45.840
do as partly a way of evaluating how ready the baby is is to disconnect the ventilator and let the

41:45.840 --> 41:52.000
child breathe through the tube first to just observe how well the infant does. The challenge with

41:52.000 --> 41:57.040
this is that while it's a good process better than nothing, it still has resulted to

41:57.040 --> 42:04.720
to currently North America the failure rate for re-intubation is about 25%. So the doctors get

42:04.720 --> 42:10.160
it wrong about 25% of the time. And really that's the core of the project we're trying to fix. I

42:10.160 --> 42:17.360
was trying to save those 25% of babies who end up either being re-intubated or who end up with

42:17.360 --> 42:25.600
long-term disability. So in a lot of ways similar to the asphyxia problem you have doctors that

42:25.600 --> 42:34.720
don't have an effective way of clinically assessing whether the baby is able to breathe on its own.

42:34.720 --> 42:41.120
And so they do in fact disconnect the ventilator without taking the tube out. But they're just eyeballing

42:41.120 --> 42:46.480
whether the baby looks to be ready. And in 25% of the cases they get it wrong, take the tube out

42:46.480 --> 42:49.360
and only then find out it has to go back in.

42:49.360 --> 42:56.080
Exactly. I say eyeballing, but the doctors have many medical devices connected to the baby,

42:57.600 --> 43:01.920
respiratory monitors and so on. And you know the look at these data, look at the patterns and

43:01.920 --> 43:06.080
make it an informed decision. And you know that explains why they do this too.

43:06.080 --> 43:11.760
Tell you well, 75% is not too bad. It could be better and that's really the goal of the project.

43:11.760 --> 43:18.080
To say how can we take machine learning methods, apply to an automated analysis of the heart rate

43:18.080 --> 43:22.880
signals, respiratory signals that we get from this infant during that spontaneous

43:22.880 --> 43:27.520
bidding trial period. So during that time when the ventilator is turned off, because that's

43:27.520 --> 43:33.360
usually the true measure of how well the infant might do when they don't have that ventilator on.

43:33.360 --> 43:38.080
So we take these signals at that point in time and we are building a primarily

43:38.880 --> 43:43.520
time series methods to analyze them and say how can we make a prediction from this.

43:43.520 --> 43:46.800
Okay. Yeah. And what stage are you in with this project?

43:46.800 --> 43:52.240
We are at the stage where, as I mentioned earlier, you know, in December 2015, we reached

43:53.360 --> 43:56.480
we reached our target of 200 and a set of the... Oh right.

43:58.240 --> 44:04.480
Right. Yeah. Yeah. But that was the target of the project anyways, to get to about,

44:05.040 --> 44:12.560
to get to 200 babies for our training and development and to get another 50 for validation.

44:12.560 --> 44:17.120
So those 50 will be left to the very end of the project, say, five year long project.

44:17.120 --> 44:23.200
We left to the very end before we to evaluate what about methods that we've developed on the other

44:23.200 --> 44:29.360
parts. So we've done quite a bunch of analysis on the data over the last year,

44:29.360 --> 44:34.480
but really much of it has been, much of the detailed work about, say, I've been starting this year

44:35.120 --> 44:41.440
since we now have the many multitasets that we require. Yeah. We've written about a few,

44:41.440 --> 44:47.840
a few works we've done on the data in terms of using Markov chain models to model the respiratory

44:47.840 --> 44:53.680
patterns and how they change over time. Yeah. So we have a paper that we have published about

44:53.680 --> 44:58.640
this work last year at the Engineering and Medicine and Biology Conference.

44:58.640 --> 45:01.920
Okay. And what was the conclusion of that paper?

45:03.120 --> 45:08.160
Yeah. The paper was pretty interesting. At least we find it a bit for people. It was pretty

45:08.160 --> 45:13.600
interesting. So in that case, what we did was we did not look at the raw signals of how the

45:13.600 --> 45:18.880
heart rate is changing, you know, the respiratory rate in particular. We looked more at states. So

45:18.880 --> 45:24.480
so breeding in general goes through a number of states. There's five of them roughly.

45:24.480 --> 45:30.480
There's synchronous breeding, which is when the breeding in the rib cage and abdomen,

45:30.480 --> 45:39.040
in synchrony, it's very creative. So they're happening at the compression and expansion is

45:39.040 --> 45:44.000
happening at the same rate pretty much in both sides. There is a synchronous breeding when they're

45:44.000 --> 45:49.680
out of phase. That's the second pattern. The third pattern is the pause states when the infant

45:49.680 --> 45:56.640
experiences a cessation of breeding at some point. And there's the movement artifact phase,

45:56.640 --> 46:03.120
which is somewhat a phase induced by external factors, either the infant is moving or it's been

46:03.120 --> 46:10.000
found by nurses and so on. And then there's the last phase is called an unknown state.

46:10.800 --> 46:16.080
And that is just the ones that don't fit into any of the four defined patterns. And so the

46:16.080 --> 46:20.160
goal of this project was to see how we could use switching models like the Markov chain,

46:20.160 --> 46:27.360
which models the transitions from one state to another. How we could take that and apply to this

46:27.360 --> 46:35.360
to learn more about what kind of transitions do infants who are ready for excavation go through

46:35.360 --> 46:40.960
and walk on of transitions to infants who are not ready for excavation go through and could this

46:40.960 --> 46:47.680
inform the classifiers will build in the future. And so it was a more of a modeling task using chain

46:47.680 --> 46:54.320
models. And do you think that the Markov chain model will play a big role in the approach you

46:54.320 --> 47:05.360
eventually take to create a diagnostic tool? Yeah. On one hand, it emphasized some of the things

47:05.360 --> 47:12.000
I was known somewhat within the clinical community that for instance pause states are bad.

47:12.000 --> 47:19.200
Session of breeding is not good because that means there's no power to drive the longer activity.

47:19.840 --> 47:23.840
So it's very important for some things like that. But also we observed some interesting trends

47:23.840 --> 47:30.000
in terms of the transitions from some of these phases to the others. That would eventually serve

47:30.000 --> 47:36.720
us features into whatever classifiers we build in the near future. Awesome. Awesome. Well,

47:36.720 --> 47:43.600
you've been very gracious with your time and we're running out of it. So I'd love to dig more

47:43.600 --> 47:47.760
into this. But I think in lieu of that we'll just make sure that we have a link to

47:49.200 --> 47:55.520
to this second paper here. But Charles, thank you so much for taking the time. You're doing

47:55.520 --> 48:01.280
some really interesting work. And I appreciate it having the opportunity to learn about it.

48:01.280 --> 48:06.400
Yeah, thank you very much. Some of us had a nice chat to meet you as well. Awesome. Thanks.

48:10.640 --> 48:16.720
All right, everyone. That's our show for today. For more information on Charles or any of the topics

48:16.720 --> 48:24.560
covered in this episode, head on over to twimlai.com slash talk slash 112. And remember to submit your

48:24.560 --> 48:32.720
thoughts on AI in your life at twimlai.com slash my AI. Thanks so much for listening and catch you

48:32.720 --> 49:02.640
next time.

