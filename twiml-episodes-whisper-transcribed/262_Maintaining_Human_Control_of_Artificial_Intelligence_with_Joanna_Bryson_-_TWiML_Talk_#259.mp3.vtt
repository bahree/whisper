WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:28.720
I'm your host Sam Charrington.

00:28.720 --> 00:33.040
Before we get going, I'd like to send a huge thanks to our friends at HPE for sponsoring

00:33.040 --> 00:38.080
this week's series of shows from the O'Reilly AI Conference in New York City.

00:38.080 --> 00:43.160
At the conference, HPE presented on InfoSight, which is the company's cloud-based AI ops

00:43.160 --> 00:49.240
solution for helping IT organizations better manage and ensure the health of their IT infrastructure

00:49.240 --> 00:50.240
using AI.

00:50.240 --> 00:55.440
I've previously written about AI ops and it's definitely an interesting use case for machine

00:55.440 --> 00:56.440
learning.

00:56.440 --> 01:05.760
To check out what HPE InfoSight is up to in this space, visit Twimbleai.com slash HPE.

01:05.760 --> 01:11.440
Alright everyone, I am here in New York City for the O'Reilly AI Conference and I'm with

01:11.440 --> 01:12.440
Joanna Bryson.

01:12.440 --> 01:15.240
Joanna is a reader at the University of Bath.

01:15.240 --> 01:19.920
Apparently, a reader is something like a super associate professor.

01:19.920 --> 01:25.240
And Joanna's here to speak on maintaining human control of artificial intelligence.

01:25.240 --> 01:30.480
We've been interacting on Twitter for quite some time and it is a pleasure to finally meet

01:30.480 --> 01:31.480
you in person.

01:31.480 --> 01:33.320
I am so bad with these things.

01:33.320 --> 01:35.480
I totally forgot you were that guy on Twitter too.

01:35.480 --> 01:36.480
That is awesome.

01:36.480 --> 01:39.120
Yeah, I'm super happy to be here.

01:39.120 --> 01:41.000
We've said super twice in the same 10 minutes.

01:41.000 --> 01:42.000
I hope that's okay.

01:42.000 --> 01:43.600
It is not a problem at all.

01:43.600 --> 01:46.160
It is not a problem at all, not a problem at all.

01:46.160 --> 01:49.520
Before we got started, we shared quite a bit about Chicago.

01:49.520 --> 01:51.760
We both spent about 10 years or so on Chicago.

01:51.760 --> 01:52.760
Yeah.

01:52.760 --> 01:55.040
And we went to the competing universities there, right?

01:55.040 --> 01:56.040
North Western for me.

01:56.040 --> 01:58.960
And University of Chicago for me as an undergraduate.

01:58.960 --> 01:59.960
Uh-huh.

01:59.960 --> 02:00.960
Uh-huh.

02:00.960 --> 02:05.960
So starting from your undergrad in Chicago, how did you make your way to working in AI?

02:05.960 --> 02:06.960
Oh, yeah.

02:06.960 --> 02:11.080
Well, that is absolutely critical to understand who I am that I did do that undergraduate

02:11.080 --> 02:13.400
degree, the liberal arts degree at Chicago.

02:13.400 --> 02:18.240
So my first degree was basically behavioral sciences, which at that time meant nonclinical

02:18.240 --> 02:19.240
psychology.

02:19.240 --> 02:22.080
They've annoyingly rebranded.

02:22.080 --> 02:25.160
So now it means a business degree, I did not do a business degree.

02:25.160 --> 02:29.200
Anyway, so I actually am interested in the fundamentally what makes, you know, what

02:29.200 --> 02:30.200
is intelligence?

02:30.200 --> 02:31.480
How does it work?

02:31.480 --> 02:33.320
Why does some species use it more than others?

02:33.320 --> 02:35.000
Why do some people use it more than others?

02:35.000 --> 02:38.000
Why do individuals use it more sometimes than other times?

02:38.000 --> 02:42.400
I mean, I am really interested in the blue sky, basic nuts and bolts of intelligence.

02:42.400 --> 02:46.160
And I got my start at Chicago with the liberal arts degree.

02:46.160 --> 02:50.640
And so then when I worked for five years in industry and I went into the master's degree

02:50.640 --> 02:54.840
at Edinburgh, which was at that time, one of the very few places that you could do just

02:54.840 --> 02:56.720
a degree in artificial intelligence.

02:56.720 --> 03:00.720
In fact, their AI department was older than their computer science department there.

03:00.720 --> 03:05.320
So it was no more computer science than it was linguistics or philosophy or music.

03:05.320 --> 03:07.320
They actually had a big music component.

03:07.320 --> 03:09.080
And of course, psychology and neuroscience.

03:09.080 --> 03:14.720
So that was a really great opportunity, too, to really get at what intelligence was.

03:14.720 --> 03:18.280
And I think Europe is still more inclined to be focused on that.

03:18.280 --> 03:24.680
Although there are some great programs in America like Indiana and Colorado, where AI sort

03:24.680 --> 03:28.360
of falls under psychology and not under computer science.

03:28.360 --> 03:35.960
Anyway, I already knew that I kind of wanted to go to MIT because that was the best thing.

03:35.960 --> 03:36.960
I thought.

03:36.960 --> 03:41.000
So I managed to get the letters of reference.

03:41.000 --> 03:46.920
I needed more than the grades and got into MIT for my PhD, which was super interesting

03:46.920 --> 03:51.600
and opened a lot of doors, but was also some kind of a nightmare in many ways.

03:51.600 --> 03:56.440
And especially for someone coming in with a psych degree into the computer science department.

03:56.440 --> 04:01.000
But I did learn some really, again, some really important things about computer science.

04:01.000 --> 04:05.320
Like for example, that there's something that is computational tractability.

04:05.320 --> 04:07.720
There are limits to what we can know.

04:07.720 --> 04:08.720
And there are physical limits.

04:08.720 --> 04:12.320
They take time, space, and energy to do computation.

04:12.320 --> 04:16.680
So then also while I was at MIT, I noticed the people projected.

04:16.680 --> 04:20.560
They over-identified with AI, and they had really weird beliefs about it.

04:20.560 --> 04:24.520
But only in some context, only if it was like, for example, I was working on a robot shaped

04:24.520 --> 04:25.520
like a human.

04:25.520 --> 04:26.520
Right?

04:26.520 --> 04:28.320
There were other robots everywhere that were actually better.

04:28.320 --> 04:32.480
This robot, it turned out, wasn't grounded properly.

04:32.480 --> 04:36.480
So it's like you're going to have to figure out that it wasn't grounded.

04:36.480 --> 04:39.760
We couldn't figure out why the different process, it was supposed to be like a parallel brain.

04:39.760 --> 04:42.360
We couldn't figure out why the different processors didn't talk to each other.

04:42.360 --> 04:47.320
So anyway, the person that figured that out was the one that they hired when they fired

04:47.320 --> 04:48.320
me.

04:48.320 --> 04:53.200
At some point, they cut all the sort of liberal arts people off the project because they weren't

04:53.200 --> 04:55.160
getting the funding they were hoping for.

04:55.160 --> 04:59.280
And so anyway, while I was on that, as I said, I noticed people, well, I couldn't

04:59.280 --> 05:00.680
help but notice they would interrupt me.

05:00.680 --> 05:04.120
I was sitting there and they'd be saying, hey, it'd be unethical to unplug that robot.

05:04.120 --> 05:06.640
And I'd be like, well, it's not plugged in.

05:06.640 --> 05:08.680
So that was how I got into AI ethics.

05:08.680 --> 05:12.760
So I wrote my first paper with a philosopher friend I had named Phil time.

05:12.760 --> 05:18.040
And so that was the way I've got my first AI ethics paper.

05:18.040 --> 05:22.920
And then I was already a mature student because as we also discussed that, that worked in

05:22.920 --> 05:27.280
Chicago in the financial industry for a few years and paid off my undergraduate debts.

05:27.280 --> 05:35.560
So I didn't get many calls when I was looking to work as a professor in America.

05:35.560 --> 05:39.840
So I applied to like two places in the UK and I got offered five interviews.

05:39.840 --> 05:43.160
So I was like, oh, okay, I guess these guys are still into this stuff.

05:43.160 --> 05:47.360
So I went back there and again, the British, like it's not the tenure system.

05:47.360 --> 05:49.960
So I could do whatever I wanted.

05:49.960 --> 05:54.280
They have a really high, or they did at the time have a very high toleration of eccentricity.

05:54.280 --> 05:59.320
And so I was able to pursue the blue sky stuff I was interested in as long as I was really

05:59.320 --> 06:03.880
good at teaching programming and I got the occasionally AI paper out.

06:03.880 --> 06:11.960
So then in things kind of checked the line and then in 2007, the last time we had a president

06:11.960 --> 06:18.160
we were terrified of, George Bush was talking, his administration gave like tens of millions

06:18.160 --> 06:23.880
to Ron Arkin to make ethical AI robots warriors.

06:23.880 --> 06:27.160
Now I thought, I didn't know that Ron Arkin had been trying to get that money for ages.

06:27.160 --> 06:30.680
I thought that this was Bush trying to do like, I would grade with robots because as I

06:30.680 --> 06:32.880
said, people over identified with them.

06:32.880 --> 06:37.480
So I started cranking up the publishing in the AI ethics area.

06:37.480 --> 06:42.200
And so then I started getting like called to the table by governments.

06:42.200 --> 06:43.920
The EU is really good at this stuff.

06:43.920 --> 06:45.960
The British are very good at it.

06:45.960 --> 06:50.600
And they would be asking questions that like nobody knew the answers to like, how is

06:50.600 --> 06:52.360
AI changing society?

06:52.360 --> 06:56.960
But because of the other work I've been doing, all that stuff about like, what is intelligence

06:56.960 --> 07:00.880
for and how does it change the species and what do you use it for?

07:00.880 --> 07:03.800
I was able to more answer those questions than a lot of other people.

07:03.800 --> 07:09.840
So I've found up doing a huge amount of sort of consultations into really interesting

07:09.840 --> 07:20.080
obviously companies, but also more NGOs like OECD and the Red Cross, Chatham House, you

07:20.080 --> 07:25.080
know, people that I was, I didn't think I would have anything to offer, but what's cool

07:25.080 --> 07:28.520
is they bring in like these round tables and then I would be one of the two or three

07:28.520 --> 07:32.000
AI experts and then I'd learn about the other stuff.

07:32.000 --> 07:36.080
So right now what I think is actually the most important stuff I'm working on in terms

07:36.080 --> 07:40.520
of research is that I started working on political economy trying to understand things

07:40.520 --> 07:46.880
like why polarization is correlated with wealth, wealth inequality, not wealth, I mean,

07:46.880 --> 07:48.520
wealth is fine, right?

07:48.520 --> 07:56.520
But when society gets to, when the amount of income is too different and we don't think

07:56.520 --> 07:59.480
it's probably the income that really is doing it, we think it's like how comfortable

07:59.480 --> 08:07.640
you feel, then that tends, although that always, to lead to a high political polarization.

08:07.640 --> 08:13.200
So that again goes back to the original stuff I was interested in about how do we cooperate,

08:13.200 --> 08:16.760
why do we have these conversations, why am I here, you're not paying me, right?

08:16.760 --> 08:20.040
But I think it's a worthwhile thing to do is if somebody wants my opinion, I think it's

08:20.040 --> 08:23.640
the obligation of academics, I mean, that's basically what we are, we're getting paid

08:23.640 --> 08:28.920
by taxpayer money to be receptacles of information and also to do some research, which people

08:28.920 --> 08:32.880
tend to focus on more, but really we were the ones who were good at answering questions.

08:32.880 --> 08:37.040
So then when people ask questions, we should answer, that was a long answer, I'm sorry,

08:37.040 --> 08:38.040
I like that.

08:38.040 --> 08:39.040
Do you want, is that okay?

08:39.040 --> 08:41.280
That is totally fine, that's totally fine.

08:41.280 --> 08:49.160
So one of the things that you touched on and it's a big focus of your, your research is

08:49.160 --> 08:52.120
understanding natural intelligence.

08:52.120 --> 08:56.400
And do you think of that as a prerequisite to understanding artificial intelligence or

08:56.400 --> 08:57.920
talking about artificial intelligence?

08:57.920 --> 09:02.640
No, I mean, obviously not because there's a lot of people who are leaders in AI that

09:02.640 --> 09:06.640
know nothing about natural intelligence and government, unfortunately.

09:06.640 --> 09:12.680
So I mean, one of the other things about Chicago is that politics is like a sport there.

09:12.680 --> 09:19.800
It's like, I watched the Cubs and I read about politics and it was, a lot of people just

09:19.800 --> 09:24.600
show a striking amount of ignorance about that, but there are great, great people in AI,

09:24.600 --> 09:27.160
they're making real contributions to society.

09:27.160 --> 09:31.320
But we need now at this point to realize that once you get to the point where you're really

09:31.320 --> 09:35.680
altering, even if you aren't making that much money like Twitter.

09:35.680 --> 09:39.400
But certainly if you're making a lot of money and also you're altering that landscape

09:39.400 --> 09:44.120
of what it is to be human, you've got to talk with people, you've got to go in and participate

09:44.120 --> 09:45.120
with government.

09:45.120 --> 09:48.440
I mean, really participate, not just try to block them from doing things to you.

09:48.440 --> 09:52.280
I mean, actually think about the problems you're causing in society, you know, the great

09:52.280 --> 09:56.920
power equals great responsibility thing, you know, that we need to get those guys talking.

09:56.920 --> 10:01.920
So no, no, unfortunately, it's not at all true that you have to understand this stuff.

10:01.920 --> 10:06.480
How can we best use our understanding of natural intelligence or even taking a step back?

10:06.480 --> 10:10.040
What do we understand about natural intelligence and how can we apply that to AI?

10:10.040 --> 10:12.960
Is that even a narrow enough question to start to answer?

10:12.960 --> 10:17.640
I mean, I think what academics are is we look at things in nature that are the world

10:17.640 --> 10:22.440
or anything that we can't understand and then we try to say, especially if we feel like

10:22.440 --> 10:26.120
we ought to be able to understand that, I almost understand it by don't quite.

10:26.120 --> 10:31.280
So one of the big questions that motivated me a lot as a PhD student was why are the different

10:31.280 --> 10:32.800
regions of the brain?

10:32.800 --> 10:35.040
Why do they have different architectures, right?

10:35.040 --> 10:38.200
Because why wouldn't nature just find one best architecture, right?

10:38.200 --> 10:40.200
Because it's had billions of years to do that, right?

10:40.200 --> 10:41.600
Why wouldn't it do that?

10:41.600 --> 10:45.080
And again, this was like, I wasn't taking to heart that part about computation I told

10:45.080 --> 10:46.360
you about before.

10:46.360 --> 10:49.920
Now I understand and that was something and it was interesting because I mean, I literally

10:49.920 --> 10:53.240
like really smart people like, you know, Sandy Pentland, some of these people when I was

10:53.240 --> 10:58.400
finishing my PhD, they were the ones who helped me see what I had found, which was that

10:58.400 --> 11:02.640
this explained why the modular approaches to AI were working well, right?

11:02.640 --> 11:08.680
It's that basically you have a lot of learning problems to solve and learning again, it's

11:08.680 --> 11:15.080
about tractability and it's about making it likely that you'll learn it in time, right?

11:15.080 --> 11:20.240
So you can prove sometimes, for example, we don't need in some deeply theoretical sense,

11:20.240 --> 11:25.200
we don't need deep learning, we don't need multiple layers to neurons in theory, like,

11:25.200 --> 11:28.960
you know, a single hidden layer is sufficient to solve any problem.

11:28.960 --> 11:33.920
But in practice, it's incredibly unlikely that you're going to get there with that single

11:33.920 --> 11:34.920
middle layer.

11:34.920 --> 11:39.640
And when you add lots of other layers, you're adding information in that can accelerate

11:39.640 --> 11:40.640
the learning.

11:40.640 --> 11:44.960
Well, of course, if you're accelerating some kinds of learning, you're decelerating other

11:44.960 --> 11:45.960
kinds.

11:45.960 --> 11:49.160
And that's the whole point is that you want to make something that's likely to learn

11:49.160 --> 11:50.840
the problem you're setting it.

11:50.840 --> 11:54.800
And so basically the different parts of the brain are sitting there solving different

11:54.800 --> 11:58.320
kinds of problems, like the problems that I have and the problems that ears have and

11:58.320 --> 12:04.640
the problems of planning and of choosing of all the options before you and of meeting

12:04.640 --> 12:05.640
the goals you have.

12:05.640 --> 12:07.880
I mean, these are just different kinds of problems.

12:07.880 --> 12:12.120
And so they have different architectures that are best able to facilitate you learning

12:12.120 --> 12:15.480
how and also, of course, controlling doing that.

12:15.480 --> 12:22.040
So yeah, I mean, I remember I was just laughing when, you know, the deep mind got sold

12:22.040 --> 12:24.160
for 400 million pounds.

12:24.160 --> 12:26.960
And then they're going, oh, we're providing artificial general intelligence.

12:26.960 --> 12:30.920
It's like, there's never going to be an algorithm that gives you a missing, you know,

12:30.920 --> 12:32.720
there's never going to be a single solution.

12:32.720 --> 12:34.440
And you guys, it's like 14 of them.

12:34.440 --> 12:37.840
And the reason they were getting 400 million pounds was they were really good at tweaking

12:37.840 --> 12:43.560
the parameters and nobody else was as good as they were at it.

12:43.560 --> 12:47.880
That was like, if there was a GI, they would have been worth anything, right?

12:47.880 --> 12:52.720
They were worth a lot of money because they had specialist capacity, some of which were

12:52.720 --> 12:56.760
intuitive, but some of which were obviously that they're smart people and they ran to good

12:56.760 --> 12:57.760
strategies.

12:57.760 --> 13:02.520
Well, maybe let's transition a little bit to the talk that you'll be giving tomorrow

13:02.520 --> 13:05.760
on the topic of maintaining human control of artificial intelligence.

13:05.760 --> 13:11.120
I think, you know, I think of that as a headline or a topic.

13:11.120 --> 13:15.960
In a lot of ways, it's obvious that we need to maintain control of artificial intelligence.

13:15.960 --> 13:19.560
But when you think about that, like, what does that mean to you and why is that important?

13:19.560 --> 13:21.040
Well, it's really interesting.

13:21.040 --> 13:23.160
There's two, there's two big differences.

13:23.160 --> 13:28.760
One is that some people really, really, really want to replace themselves.

13:28.760 --> 13:35.920
Like, it's part of their own drive for immortality is to have these AI offspring.

13:35.920 --> 13:36.920
Yeah.

13:36.920 --> 13:40.240
So there's a surprising number of people are still talking about.

13:40.240 --> 13:43.880
They think like AI is these aliens we've discovered and we need to make friends with

13:43.880 --> 13:44.880
them, you know?

13:44.880 --> 13:49.560
And I wish we would make friends with other people, you know, like the amount of devotion

13:49.560 --> 13:50.560
to that idea.

13:50.560 --> 13:55.520
I mean, strong, emotional, and also some, I mean, some of them are very well informed

13:55.520 --> 13:58.680
philosophers and things like that, you know, this is immoral necessity.

13:58.680 --> 14:04.920
And just like, no, it's like, so I tried for a long time to communicate that, look,

14:04.920 --> 14:06.520
look, we designed something.

14:06.520 --> 14:10.840
So if we build something that required our obligation, we would be being mean to it.

14:10.840 --> 14:14.280
But I think I, in a way, gave the whole idea of more credit than it deserved.

14:14.280 --> 14:17.640
And I should have focused more on the fact that you, I was just doing this logical thing

14:17.640 --> 14:20.400
and saying, look, you know, we shouldn't do that for logical reasons.

14:20.400 --> 14:24.480
But actually pragmatically, we're never going to build something that experiences the world

14:24.480 --> 14:26.280
as much like us as a fruit fly does.

14:26.280 --> 14:30.360
And people kill fruit flights left right in the center, you know, that just the, the,

14:30.360 --> 14:36.240
the phenomenological experience is dependent on all these sorts of, the sensors we have,

14:36.240 --> 14:41.560
the subset of all the computation we can do that we've evolved into, the space that we're

14:41.560 --> 14:42.560
in.

14:42.560 --> 14:44.960
So, but that's not the most important part.

14:44.960 --> 14:48.160
And I think I've just spent too much time on it probably again.

14:48.160 --> 14:54.040
But even that makes me, makes me wonder, granted, we can't create something that has the

14:54.040 --> 14:58.800
same sensory experience as we do because it won't have the same sensors.

14:58.800 --> 15:03.160
But does that also imply that you don't think that we can create something that has a degree

15:03.160 --> 15:04.160
of self-awareness?

15:04.160 --> 15:05.160
Oh.

15:05.160 --> 15:06.160
Yeah.

15:06.160 --> 15:07.160
See, that's again, there's a lot of points.

15:07.160 --> 15:09.400
Am I falling into some trap that everybody falls into?

15:09.400 --> 15:10.400
No, exactly.

15:10.400 --> 15:17.520
Everybody, like, so there's so many terms that, that we, we use to mean human, right?

15:17.520 --> 15:21.680
Actually, the best one, I think the one you're really interested in is moral agent and

15:21.680 --> 15:22.680
moral patient.

15:22.680 --> 15:23.680
That's two terms.

15:23.680 --> 15:27.260
moral agent is something that if it does something that's responsible, we are society

15:27.260 --> 15:28.880
considers it responsible.

15:28.880 --> 15:30.760
And that varies by society.

15:30.760 --> 15:35.400
No, there isn't, there isn't universal agreement about how old you have to be before you're

15:35.400 --> 15:36.400
an adult, right?

15:36.400 --> 15:39.800
I mean, some cultures, you're an adult when your dad dies, and obviously you're only an

15:39.800 --> 15:41.040
adult if you're male.

15:41.040 --> 15:42.040
Right.

15:42.040 --> 15:48.520
So, so this is, on the other hand, the moral patient is the thing you have to take care of.

15:48.520 --> 15:53.880
That's something that, that the society realizes it has, it has obligations towards.

15:53.880 --> 15:57.880
And so that can include things like the environment, you know, things that don't, or babies, right?

15:57.880 --> 16:00.240
Things that are not moral agents themselves.

16:00.240 --> 16:03.760
But, but that we, we think we realize we need to take care of.

16:03.760 --> 16:06.920
Although a lot of philosophers think the reason we have to take care of those things is

16:06.920 --> 16:08.280
because it's in our own interests.

16:08.280 --> 16:13.760
So that basically only moral agents have the, the, the, the foundation of moral patient

16:13.760 --> 16:18.800
agency, but that as we realize the, the sort of interconnected world we live in, then we

16:18.800 --> 16:23.840
extend our needs through these other organisms that, that we have identity with.

16:23.840 --> 16:29.520
And incidentally, that's the best explanation for why we would ever take care of AI is that

16:29.520 --> 16:33.960
if we don't take care of things that we feel empathy for, then we might, then we, we

16:33.960 --> 16:38.960
harden our hearts and we, and we learn to treat things we do.

16:38.960 --> 16:40.760
We should empathize with like other people badly.

16:40.760 --> 16:45.760
So that's like what the British have come up with, with response to that is say, okay,

16:45.760 --> 16:50.440
so don't make AI the reminds you of people, you know, there's two sides of that, right?

16:50.440 --> 16:55.920
So yes, of course we have, I would say we, you know, if, if you think that consciousness

16:55.920 --> 17:01.680
is self, self-awareness, then AI and robots have more of that than we do, right?

17:01.680 --> 17:02.680
They have RAM.

17:02.680 --> 17:04.240
I mean, any computer science has a RAM.

17:04.240 --> 17:07.080
It can know exactly what all the parameters are, right?

17:07.080 --> 17:10.520
Part of what it is to be human is not to have access to all that stuff, actually, because

17:10.520 --> 17:14.360
a lot of it will slow you down and clutter your way forward.

17:14.360 --> 17:18.920
So we, learning for us is about consolidating all that knowledge into the stuff that's

17:18.920 --> 17:22.200
actually going to be most likely to be useful in the future.

17:22.200 --> 17:24.200
So that's not, it's not quite the same thing.

17:24.200 --> 17:28.720
But anyway, let me skip on to the thing that's actually really important, which is I tend

17:28.720 --> 17:34.360
to talk about AI as necessarily extending from humans, like the A is artifact.

17:34.360 --> 17:36.880
So it's something you've made for some purpose.

17:36.880 --> 17:42.840
And so the other side of maintaining human control is actually basically not making something

17:42.840 --> 17:48.000
that's going to be a big dumpster fire, right?

17:48.000 --> 17:51.920
And I was really expecting a technical term there.

17:51.920 --> 17:58.560
So it's like, it almost got, it almost went south from that, but I was good at the last

17:58.560 --> 18:00.560
minute.

18:00.560 --> 18:08.720
But anyway, so the, I am particularly concerned about organizations that deliberately try

18:08.720 --> 18:11.160
to evade responsibility for what they do.

18:11.160 --> 18:17.000
So like when a government comes in and decides that they want to cut taxes for some of their

18:17.000 --> 18:23.040
friends and that the way they'll do that is by stopping services for people that didn't

18:23.040 --> 18:24.920
vote for them.

18:24.920 --> 18:29.840
And then they hide that within like some complicated software that they sort of outsource

18:29.840 --> 18:32.600
and try to obfuscate, right?

18:32.600 --> 18:34.720
So there's been cases of this.

18:34.720 --> 18:40.280
And if you don't have proper rules about accountability, well, if you have proper accountability,

18:40.280 --> 18:45.080
and now just to cut to the boring chase of this, all you need is decent DevOps, right?

18:45.080 --> 18:49.280
You just need to be able to go in and see, you know, what was the rationale to writing

18:49.280 --> 18:51.080
the software, right?

18:51.080 --> 18:57.120
And so you can see what was intended if people documented and logged that properly, right?

18:57.120 --> 19:01.960
Or you can even guess that I mean, they may not say, you know, we want to, you know, do

19:01.960 --> 19:06.680
bad things to the people on the wrong side of the town, but you can still see like, you

19:06.680 --> 19:10.720
know, save money in this neighborhood or something, you know?

19:10.720 --> 19:17.200
So the, the, but for quite a lot of what's happened is that people aren't doing the kind

19:17.200 --> 19:22.280
of standard standard stuff you do in software engineering in AI, again, because it's over

19:22.280 --> 19:25.560
identification, maybe because too many, you know, psychologists or whatever are dropping

19:25.560 --> 19:28.360
in and haven't had their computer science courses.

19:28.360 --> 19:33.240
But maybe, maybe because people just think, oh, you don't have to do that with AI, it's

19:33.240 --> 19:34.400
going to teach itself.

19:34.400 --> 19:37.480
Like it's the, the machine is supposed to responsible for itself.

19:37.480 --> 19:38.480
And we can't do that.

19:38.480 --> 19:40.040
We cannot hold machines responsible.

19:40.040 --> 19:44.160
There's no way, you know, the penalties of law that we've invented dissuade humans.

19:44.160 --> 19:47.520
In fact, they would dissuade sheep if the sheep could understand, right?

19:47.520 --> 19:51.000
So a lot of it is based on, you don't want to lose social status.

19:51.000 --> 19:55.320
You don't want to lose, you know, freedom, you don't want to lose time, right?

19:55.320 --> 19:59.880
Yeah, you're not going to build AI that is sincerely going to care about those kinds

19:59.880 --> 20:05.640
of things in the, in the pervasive way that we do, you know, it's just systemic for us.

20:05.640 --> 20:10.400
When you isolate someone, it is a form of torture, I mean, for any length of time, right?

20:10.400 --> 20:16.000
So the, and yeah, you're not going to make AI that, I mean, if you could, it would be unethical,

20:16.000 --> 20:20.440
but you're not going to make AI that, that, that, that not AI that you can maintain safely,

20:20.440 --> 20:21.440
right?

20:21.440 --> 20:25.360
Again, this is me being, um, probably overly generous.

20:25.360 --> 20:29.880
I don't think there's any way you can do a whole brain uploading, but if you could,

20:29.880 --> 20:34.200
then that would be more like a clone and then that would, and then all the stuff I'm

20:34.200 --> 20:37.440
saying wouldn't apply to that, but you wouldn't be able to maintain an extended, like,

20:37.440 --> 20:38.440
you can suffer.

20:38.440 --> 20:41.280
So it's, it wouldn't be a very good product, right?

20:41.280 --> 20:44.240
It would be, I mean, obviously some people would buy it because as I said, they want

20:44.240 --> 20:46.280
to have perpetual, I mean, so stupid.

20:46.280 --> 20:51.200
Why, why do people think that like AI, their AI child is going to live longer than they

20:51.200 --> 20:52.200
are, right?

20:52.200 --> 20:53.200
Because like, what is it?

20:53.200 --> 20:56.200
Like the meantime to failure of a software system, like five years and humans last like

20:56.200 --> 20:57.200
80 years, right?

20:57.200 --> 21:00.800
You know, that, that's just like, it reminds me of everyone being so excited about an Apple

21:00.800 --> 21:01.800
car.

21:01.800 --> 21:05.680
It's like, have you used a computer, an Apple phone, do you really want an Apple car?

21:05.680 --> 21:11.360
Well, yeah, I'm kind of excited about that, but yeah, but no, I assume that they would

21:11.360 --> 21:15.000
get together with some people who knew how to handle those things, you know, but they,

21:15.000 --> 21:18.520
like, yeah, no, that if you want to do something that's going to last longer than you're going

21:18.520 --> 21:22.040
to live, then you should get, you know, like carve some stuff in stone or something.

21:22.040 --> 21:27.960
It's not about building an eye, you know, or, you know, build a big cathedral thing.

21:27.960 --> 21:30.440
People seem to care about those.

21:30.440 --> 21:31.440
Right.

21:31.440 --> 21:32.440
I cared.

21:32.440 --> 21:33.440
I was really upset.

21:33.440 --> 21:34.440
I lost two hours.

21:34.440 --> 21:37.080
Really, really about that, Twitter.

21:37.080 --> 21:42.560
One of the things that struck me in your right up for your talk and just now, I don't

21:42.560 --> 21:45.600
know that I've ever heard an academic say the word DevOps.

21:45.600 --> 21:50.840
Well, that's, you know, okay, well, there's two parts of that.

21:50.840 --> 21:53.920
First of all, I did spend that five years doing software engineering.

21:53.920 --> 21:54.920
Uh-huh.

21:54.920 --> 21:58.400
Secondly, you know, part of the reasons I mentioned before that was when I was at MIT,

21:58.400 --> 22:01.200
I was a psychologist and I was having a little bit of a struggle.

22:01.200 --> 22:05.400
Actually, the very first course I took every other week was like doing a proof, every

22:05.400 --> 22:07.680
other week was doing like pseudo code.

22:07.680 --> 22:09.800
And I was getting like seas on the proofs and aids.

22:09.800 --> 22:13.960
I was like one of the leading in the class, my first, my, you know, like the, MIT doesn't

22:13.960 --> 22:17.920
program, they build computers from scratch or like out of free or transforms or something.

22:17.920 --> 22:18.920
Right.

22:18.920 --> 22:22.320
So, anyway, but, you know, so, but there were certain struggles I was having.

22:22.320 --> 22:26.680
But I just realized that the whole thing about systems engineering of AI was wide open.

22:26.680 --> 22:28.680
No one was paying attention to it.

22:28.680 --> 22:33.520
So I used to say I did, I did systems AI and, and unfortunately, I guess I wasn't influential

22:33.520 --> 22:34.520
enough then.

22:34.520 --> 22:36.680
Maybe I should try again now to make that a thing.

22:36.680 --> 22:37.680
Not good to talk.

22:37.680 --> 22:38.680
Yeah.

22:38.680 --> 22:39.680
Well, seriously.

22:39.680 --> 22:42.320
But anyway, it's the systems engineering of AI and I kind of want people to be able

22:42.320 --> 22:46.760
to Google that and find out the systems engineering predates even computers.

22:46.760 --> 22:49.840
You know, because there was like these people claiming there's only four people working

22:49.840 --> 22:50.840
on AI safety.

22:50.840 --> 22:51.840
It's such rubbish.

22:51.840 --> 22:56.040
I mean, everybody, there's AI is pervasive in software now.

22:56.040 --> 22:57.040
Everybody's using AI.

22:57.040 --> 23:00.000
Everybody's using machine learning, which is, you know, it's a huge change.

23:00.000 --> 23:02.960
But that's why there's all these guys from like, you know, Accenture and whatever here.

23:02.960 --> 23:03.960
Right.

23:03.960 --> 23:05.840
You know, everybody is using this stuff now.

23:05.840 --> 23:09.920
And everyone who's been developing a system and trying to make sure it stands up is obviously

23:09.920 --> 23:13.920
doing all kinds of AI safety, they're doing all kinds of security.

23:13.920 --> 23:15.960
And they're trying to figure this stuff out.

23:15.960 --> 23:19.560
And so there's thousands of people working in this area and there's probably hundreds

23:19.560 --> 23:20.560
who've published on it.

23:20.560 --> 23:25.360
But they just didn't go out and say, oh, yes, I'm saving the world, you know, I, I, there

23:25.360 --> 23:29.000
are some real problems because there are certain billionaires that only fund existential

23:29.000 --> 23:30.000
threats.

23:30.000 --> 23:33.800
And then there's certain academics that want to bring in millions of dollars or pounds

23:33.800 --> 23:34.800
or something.

23:34.800 --> 23:37.400
And, and so they say, oh, yeah, AI is an existential threat.

23:37.400 --> 23:39.280
It's like, you know, that's not helpful.

23:39.280 --> 23:42.680
But what is an existential life, it's an existential threat, but something that really

23:42.680 --> 23:47.040
screws up a lot of people's lives and can cause war and things like that is when we lose

23:47.040 --> 23:52.160
control of our governments or our corporations, you know, we, we need to figure out how to

23:52.160 --> 23:54.920
plug things together when we change the landscape so much.

23:54.920 --> 24:00.440
There are, you know, serious, serious problems, but it's not the AI itself.

24:00.440 --> 24:05.200
It's, it's, I actually think that the changes are more about the changes that are making

24:05.200 --> 24:10.200
it harder that we need to reinvent governance and that we need to really think about how

24:10.200 --> 24:17.280
to plug the software industry into regulation, have more to do with the digital, you know,

24:17.280 --> 24:24.360
the fact that we can move perfect replicas of things across the planet in an instant.

24:24.360 --> 24:25.680
That's what's really changing things.

24:25.680 --> 24:30.320
And it doesn't matter whether the intelligence is something that's coming from a computer

24:30.320 --> 24:34.160
or the intelligence is coming from eight billion connected humans, right?

24:34.160 --> 24:37.760
Either way, you've just got a totally new situation.

24:37.760 --> 24:41.640
And I think that's where a lot of the regulatory challenges are coming from.

24:41.640 --> 24:46.440
So then when you have a talk called maintaining human control of artificial intelligence,

24:46.440 --> 24:50.200
is it fair to say it's not really about human control of artificial intelligence at all,

24:50.200 --> 24:55.920
but rather about human control of human institutions in a world of artificial intelligence?

24:55.920 --> 24:59.640
I'm going to finish with that stuff, but I will focus on the DevOps.

24:59.640 --> 25:04.400
And what I'm going to, what I do when I'm talking to people that are mostly industry is I try

25:04.400 --> 25:08.080
to reach out with this idea that look, we can do this responsibly.

25:08.080 --> 25:10.880
We know how and we need to help people.

25:10.880 --> 25:16.040
I mean, the main thing is that we want to help the government enforce so that when we do

25:16.040 --> 25:20.800
good practice, other people doing bad practice don't suddenly get lots of money in all the

25:20.800 --> 25:22.680
VC or whatever, you know?

25:22.680 --> 25:27.720
So for the vast majority of actors, it's in all of our interests to do things that make

25:27.720 --> 25:29.960
the software stable, right?

25:29.960 --> 25:33.880
So yeah, so I've learned DevOps just by going to these kinds of meetings, right?

25:33.880 --> 25:38.360
That's not what we call it in the East, but that's okay, you know?

25:38.360 --> 25:44.040
But the focus on the problem and the solution, it's just unbelievable that people are sitting

25:44.040 --> 25:45.040
here.

25:45.040 --> 25:51.640
I mean, I've talked to the head of, I'm trying to say this, I will go ahead and say this.

25:51.640 --> 25:55.040
There's this thing that European Union, I have unbelievable respect for them.

25:55.040 --> 25:58.400
They are leading an AI in regulation, I'm about to say something bad, you can see it's

25:58.400 --> 26:01.880
because I'm going to say out the caveats first, they're about great, they are, right?

26:01.880 --> 26:05.320
So you know, it's 550 million people, I don't want to like run it down.

26:05.320 --> 26:08.560
I think it's like one of the, they're leading the free world right now and they're certainly

26:08.560 --> 26:14.840
leading an AI ethics and regulation, but they put together this high level experts group

26:14.840 --> 26:19.080
and there's like 52 people in it, which it was supposed to be small, but somehow, you

26:19.080 --> 26:20.480
know, things happened.

26:20.480 --> 26:21.480
Those are super smart people.

26:21.480 --> 26:26.160
There's some of my friends and colleagues that I work with, I have lots of respect for,

26:26.160 --> 26:27.160
you know?

26:27.160 --> 26:31.400
But I wound up at this European Union meeting with the guy that was the chair of that group

26:31.400 --> 26:37.000
and he had never heard about logging, you know, about revision control.

26:37.000 --> 26:43.520
He had no idea how easy it is to demonstrate that you follow good practice on AI and he

26:43.520 --> 26:47.880
had never thought about AI as a product being developed so that you have the same kinds

26:47.880 --> 26:51.800
of due diligence responsibilities as any other product.

26:51.800 --> 26:56.120
Now this stuff is already happening in the automotive industry, for example.

26:56.120 --> 27:00.800
That's why every time there's an accident or incident happening with a driverless car,

27:00.800 --> 27:01.800
it's on the front page.

27:01.800 --> 27:02.800
What went wrong?

27:02.800 --> 27:03.800
Why it went wrong?

27:03.800 --> 27:05.560
You know, what the car perceived?

27:05.560 --> 27:06.560
How it was developed?

27:06.560 --> 27:08.040
Why that thing happened?

27:08.040 --> 27:09.040
Why?

27:09.040 --> 27:12.760
Because that's a decently regulated space and AI doesn't change the fact that automotive

27:12.760 --> 27:14.320
is decently regulated.

27:14.320 --> 27:17.360
So what's not regulated because it's new and nobody's written the rules of stuff like

27:17.360 --> 27:19.320
social media and things like that.

27:19.320 --> 27:23.400
But there's nothing about the AI that's being used there that makes it any harder for

27:23.400 --> 27:28.040
us to know about what's going on in those companies and whether they were paying attention

27:28.040 --> 27:31.520
to the right things, then it does an automotive company.

27:31.520 --> 27:34.600
It's just that there hasn't been the history and so the people coming in, you know, the

27:34.600 --> 27:38.560
jot out of college when they were, you know, first years or whatever, just didn't realize

27:38.560 --> 27:40.600
what they needed to do.

27:40.600 --> 27:46.040
And the government hasn't, it's possible that governments need some kind of specialists

27:46.040 --> 27:51.280
like they have for environmental enforcement, right, you know, or medical that they need

27:51.280 --> 27:56.520
to bring together people that can go through and check, you know, the admin logs.

27:56.520 --> 28:01.920
But you know, we've just gotten incredibly sloppy and you can read one of my colleagues

28:01.920 --> 28:06.960
say to Gears has written some amazing thing about the agile turn that talking about how

28:06.960 --> 28:10.960
everybody's throwing their software together so fast they have no idea where their software

28:10.960 --> 28:12.440
libraries are coming from.

28:12.440 --> 28:15.680
And of course, famously, Frank Pasquale has done the same thing about data, just showing

28:15.680 --> 28:20.120
that data is coming from all over incredibly unethically the way it's passing between

28:20.120 --> 28:21.120
hands.

28:21.120 --> 28:26.360
We need to be able to demonstrate the provenance of our, the libraries we link to and the

28:26.360 --> 28:31.280
data we train our systems from because there are bad actors out there, you know, but it's

28:31.280 --> 28:33.520
just, it is basic DevOps.

28:33.520 --> 28:36.760
It's really not, it's not, it's not rocket science at all.

28:36.760 --> 28:42.040
It really is something that it's basic administration, it's basic bookkeeping and that's how we

28:42.040 --> 28:43.040
need, you know.

28:43.040 --> 28:47.200
And so they got super, I got to say these guys, it wasn't just, it was like several of

28:47.200 --> 28:50.640
the people that were looking at this, that there was, again, apparently a dumpster fire

28:50.640 --> 28:53.200
that was this, this high-level expert group.

28:53.200 --> 28:56.480
And they're just like, nobody told us this, I'm like, I don't know what's going on with

28:56.480 --> 29:02.320
the other 52 people, right, but, but maybe, I mean, I think this is an important thing.

29:02.320 --> 29:05.680
We were talking about our education and our bank and everybody's a little different.

29:05.680 --> 29:09.680
One of my colleagues at Princeton, our veterinarianin, has this wonderful paper showing, well, it's

29:09.680 --> 29:13.760
both terrifying and wonderful, that you can uniquely identify someone if you have like

29:13.760 --> 29:17.640
15 of their T.co links from their browser history.

29:17.640 --> 29:19.840
So T.co means it's Twitter.

29:19.840 --> 29:24.440
And so you can tell which tweets those links came from because they're compressed, you

29:24.440 --> 29:25.880
know, you are else, right?

29:25.880 --> 29:31.200
And then from seeing what links you've got it's about to click, you can, you can, for

29:31.200 --> 29:36.560
most, like for 95% of people, you can uniquely identify them because we all follow different

29:36.560 --> 29:38.600
people on Twitter, right?

29:38.600 --> 29:41.480
And so you can just, and that's the mother of public, right?

29:41.480 --> 29:44.040
So in the one hand, it's terrifying from a privacy perspective.

29:44.040 --> 29:46.680
But what's cool about it is it just shows we're all different.

29:46.680 --> 29:49.480
We all have different combinations and sets of knowledge.

29:49.480 --> 29:54.000
And yeah, so I'm the one person that happens to be in AI ethics because of the paper I wrote

29:54.000 --> 29:59.400
on as a PhD student about people over identifying with my robot and someone who knows what DevOps

29:59.400 --> 30:00.400
is, right?

30:00.400 --> 30:04.280
You know, like, and so, and so that's the, you know, that's something useful.

30:04.280 --> 30:11.560
It does strike me that the DevOps and specifically the disciplines that we're talking about, understanding

30:11.560 --> 30:17.280
logging and versioning and all of these things, they're necessary, but not sufficient for

30:17.280 --> 30:20.280
providing the kind of control that you're talking about.

30:20.280 --> 30:21.280
Right.

30:21.280 --> 30:24.880
But what that stuff is for is for providing accountability.

30:24.880 --> 30:30.600
If the companies want to defend themselves, if something goes horribly wrong, then you

30:30.600 --> 30:36.760
have to say who should pay for it, basically, you know, the taxpayers paying for it or,

30:36.760 --> 30:39.040
you know, are the companies paying for it?

30:39.040 --> 30:41.760
And in particular, when you're looking at things where you're saying that, you know, did

30:41.760 --> 30:45.840
some third country interfere with your election or whatever, you know, there, it really matters

30:45.840 --> 30:50.720
that you get to the nuts and bolts about why was things written the way they were written?

30:50.720 --> 30:54.600
And so if people don't follow DevOps, right?

30:54.600 --> 30:57.400
So first of all, you're right, you need somebody who would go through and read that stuff,

30:57.400 --> 30:58.400
right?

30:58.400 --> 31:00.920
And secondly, of course, they're still going to be unanticipated things.

31:00.920 --> 31:05.600
They will happen like the, you know, like flash crashes and stuff, like we just, we get

31:05.600 --> 31:07.960
better and better at handling those things, right?

31:07.960 --> 31:10.280
So I, you know, people freak out about flash crashes.

31:10.280 --> 31:13.880
I think the fact that they're only flash crashes, you know, like, okay, some people lost

31:13.880 --> 31:14.880
money.

31:14.880 --> 31:19.120
I understand that couldn't devastate some lives, but compared to like the 1929 when people

31:19.120 --> 31:23.920
were starving in the streets, which did not happen at least in America in 2008, incidentally,

31:23.920 --> 31:27.800
as bad as 2008 was, it wasn't like what 1929 was, right?

31:27.800 --> 31:31.080
So we are getting better and better at recovering from this stuff and we can dwell too much

31:31.080 --> 31:32.560
on the negative.

31:32.560 --> 31:37.200
But no, it is one part, but it's a core part and it's part, part of why it's important

31:37.200 --> 31:39.000
is about the demystification.

31:39.000 --> 31:43.720
Part of it is because then we can make sure that corporations absorb their fair share

31:43.720 --> 31:48.960
of the, of the costs when we're cleaning up from messes, right?

31:48.960 --> 31:54.320
And also, hopefully by making them do that, we make it less likely messes happen, right?

31:54.320 --> 31:58.680
But part of it is just to make it clear that you can do that stuff, right?

31:58.680 --> 32:03.600
And to help people understand, all right, oh, yeah, it's just, it's an ambient technology

32:03.600 --> 32:04.600
now.

32:04.600 --> 32:08.480
You know, just like we're going to look back and before there was newspapers, you know,

32:08.480 --> 32:10.560
or whatever, we can say what changed.

32:10.560 --> 32:16.040
You know, this, I had no idea about this, that polarization increased when people started

32:16.040 --> 32:17.640
getting their newspapers.

32:17.640 --> 32:20.240
Do you have an economic polarization?

32:20.240 --> 32:22.040
Yeah, no, political polarization.

32:22.040 --> 32:23.040
Political polarization.

32:23.040 --> 32:25.960
That would actually help people to get more information.

32:25.960 --> 32:30.120
But unfortunately, what it happened was they then started focusing at the national level

32:30.120 --> 32:35.600
and using and getting these national identity things and not, and so it actually, yeah,

32:35.600 --> 32:40.240
that was like, apparently that was part of what, getting mail delivered or something,

32:40.240 --> 32:44.800
that they, they, they, you know, it's like, I can't even keep all this stuff on my head.

32:44.800 --> 32:45.800
I get it from other people.

32:45.800 --> 32:46.800
But it was amazing.

32:46.800 --> 32:50.720
And that's only 100 years ago, the, the rate at which were changing society in terms

32:50.720 --> 32:52.720
of information is outstanding.

32:52.720 --> 32:57.320
And so again, projecting this onto AI and saying something's going to happen in 60 years

32:57.320 --> 32:59.840
or 30 years is not helpful.

32:59.840 --> 33:05.040
What we need to do is to see what we've already done and to recognize how much things are

33:05.040 --> 33:09.520
already changing and who's explaining that, you know, although one of the scary things,

33:09.520 --> 33:13.120
of course, is that every time you do discover something like this, you're also handing the

33:13.120 --> 33:18.800
levers to the bad actors, you know, as we saw with the, that PNAS article that came out,

33:18.800 --> 33:19.800
what was it?

33:19.800 --> 33:21.800
It was like 2012 or something.

33:21.800 --> 33:27.280
It showed that the one that you showed that you, you need like, I don't know, 80 Facebook

33:27.280 --> 33:30.400
likes and you could predict who someone was going to vote for better than their partner

33:30.400 --> 33:31.400
could.

33:31.400 --> 33:32.400
Right?

33:32.400 --> 33:36.840
You know, I remember seeing that and I thought, oh, but I actually thought a bigger deal

33:36.840 --> 33:42.000
was a paper that came out of Microsoft and like NYU about how you could tell how people

33:42.000 --> 33:45.840
are going to vote from their connect, their connect to sitting on top of their television

33:45.840 --> 33:46.840
set.

33:46.840 --> 33:50.800
And you can also tell when people are going to get divorced and you can tell, you know,

33:50.800 --> 33:53.480
as you could just see whether they're looking at each other, whether they're looking at

33:53.480 --> 33:58.080
the TV, when the commercials come on for the different presidential candidates, you

33:58.080 --> 34:03.120
know, you have all this information there and like no one is talking about this, right?

34:03.120 --> 34:07.040
And then in the end, that was, I thought I wrote a paper in 2014 that came out in 2015

34:07.040 --> 34:12.080
that predicted the 2016 might be altered by AI, but unfortunately I said by that Microsoft

34:12.080 --> 34:13.280
connect stuff, right?

34:13.280 --> 34:19.400
So I was near miss, but you know, obviously if you have this information, people are going

34:19.400 --> 34:20.880
to use it.

34:20.880 --> 34:27.800
And so apparently this is another crazy thing, autocrats usually really do their populists,

34:27.800 --> 34:30.720
but they usually really do make make the poor better off.

34:30.720 --> 34:32.560
They usually wind up giving their money.

34:32.560 --> 34:36.360
So it's possible, and again, we haven't seen this yet, but maybe the Trump is the first

34:36.360 --> 34:38.040
one who doesn't do that.

34:38.040 --> 34:43.120
And that might be because he has the social, the political science that says, oh, you know,

34:43.120 --> 34:47.080
decreasing polar, decreasing inequality actually decreases polarization.

34:47.080 --> 34:50.560
So you'd probably be less likely to get reelected if you actually helped the people that voted

34:50.560 --> 34:51.560
you for you.

34:51.560 --> 34:56.040
I'm wondering, do you have kind of three key takeaways for that you're planning to add

34:56.040 --> 34:57.040
it to us?

34:57.040 --> 35:02.280
I'm sorry.

35:02.280 --> 35:07.320
People usually come up with cool edits that at the end, but I realize it's a lot of work.

35:07.320 --> 35:11.000
So Joanna, you're giving this talk tomorrow, human control of artificial intelligence is

35:11.000 --> 35:12.000
a necessity.

35:12.000 --> 35:15.040
What are the three key takeaways that you're hoping folks will walk away with?

35:15.040 --> 35:17.920
Well, I think, I think there's actually only two.

35:17.920 --> 35:21.240
I mean, no, me, okay, I'll make it three.

35:21.240 --> 35:22.240
One is fun.

35:22.240 --> 35:23.240
That is possible.

35:23.240 --> 35:24.240
Right.

35:24.240 --> 35:27.040
So the most important thing is just realizing that it is an artifact.

35:27.040 --> 35:29.880
It's not only something we can do but it's something we should do.

35:29.880 --> 35:37.880
Secondly, that all maintaining responsibility involves is keeping track of what humans do.

35:37.880 --> 35:41.040
We don't need to know what every weight in a deep neural network does.

35:41.040 --> 35:45.160
We need to know who trained it when they thought they were done.

35:45.160 --> 35:49.840
What tests did they use, what libraries did they use?

35:49.840 --> 35:51.320
And that's about it, right?

35:51.320 --> 35:54.480
And maybe we also want to be able to go back and do some forensics.

35:54.480 --> 36:00.040
If it comes up with a result we don't like, and then we say, okay, let's test to make

36:00.040 --> 36:06.080
sure that it wasn't picking up on some defended characteristics, as we say in the UK.

36:06.080 --> 36:10.840
And try changing a few things in the resume and see if we can get the result we expected.

36:10.840 --> 36:11.840
Right.

36:11.840 --> 36:15.960
But those are things that we can do, even if we have a completely black box around the

36:15.960 --> 36:20.600
AI from the outside, just as long as we have government inspectors can go through and check

36:20.600 --> 36:23.560
the logs and make sure people file a good practice, right?

36:23.560 --> 36:25.440
So that's the main, the key thing.

36:25.440 --> 36:32.240
And then the third thing is that, or the original second thing, the first one, the third thing

36:32.240 --> 36:37.120
is that it isn't that AI is threatening us.

36:37.120 --> 36:42.400
That we are finding new ways to express power over each other.

36:42.400 --> 36:46.880
And it's really in the interest of everybody, all the big companies, all the little companies,

36:46.880 --> 36:53.560
all the medium sized companies, the ordinary people to get involved and the users, I fight

36:53.560 --> 36:54.560
for the users.

36:54.560 --> 36:55.560
I'm sorry, I digress.

36:55.560 --> 36:56.560
Right.

36:56.560 --> 37:05.000
It's in everybody's interest to live in a society where there is a stable and foreseeable

37:05.000 --> 37:10.960
economy and politics, so it makes sense to pay our taxes and to participate in governments.

37:10.960 --> 37:11.960
Awesome.

37:11.960 --> 37:13.960
Well, Joanna, thanks so much for taking the time.

37:13.960 --> 37:16.200
Again, I made something hard to add to that.

37:16.200 --> 37:17.200
Sorry.

37:17.200 --> 37:21.440
No, well, yeah, thanks for doing this.

37:21.440 --> 37:22.440
Absolutely.

37:22.440 --> 37:23.440
Absolutely.

37:23.440 --> 37:24.440
It's kind of fun.

37:24.440 --> 37:25.440
Thank you.

37:25.440 --> 37:29.440
And nice to meet you too.

37:29.440 --> 37:30.440
All right, everyone.

37:30.440 --> 37:32.160
That's our show for today.

37:32.160 --> 37:36.840
If you like what you've heard here, please do us a huge favor and tell your friends about

37:36.840 --> 37:38.000
the show.

37:38.000 --> 37:42.040
And if you haven't already hit that subscribe button yourself, make sure you do so you

37:42.040 --> 37:45.600
don't miss any of the great episodes we've gotten in store for you.

37:45.600 --> 37:50.840
For more information on any of the shows in our AI conference series, visit twimolei.com

37:50.840 --> 37:54.640
slash AINY19.

37:54.640 --> 37:57.440
Thanks again to HPE for sponsoring the series.

37:57.440 --> 38:01.800
Make sure to check them out at twimolei.com slash HPE.

38:01.800 --> 38:05.120
As always, thanks so much for listening and catch you next time.

