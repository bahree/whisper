1
00:00:00,000 --> 00:00:16,840
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:16,840 --> 00:00:21,840
people doing interesting things in machine learning and artificial intelligence.

3
00:00:21,840 --> 00:00:24,440
I'm your host Sam Charrington.

4
00:00:24,440 --> 00:00:30,180
This week we continue our series on industrial applications of machine learning and AI with

5
00:00:30,180 --> 00:00:35,960
a conversation with Chelsea Finn, a PhD student at UC Berkeley.

6
00:00:35,960 --> 00:00:41,200
Chelsea's research is focused on machine learning for robotic perception and control.

7
00:00:41,200 --> 00:00:45,680
Despite being early in her career, Chelsea is an accomplished researcher with more than

8
00:00:45,680 --> 00:00:51,400
14 published papers in the past two years on subjects like deep visual foresight, model

9
00:00:51,400 --> 00:00:56,560
diagnostic meta learning, visual motor learning, just to name a few.

10
00:00:56,560 --> 00:01:01,040
And we talk about all of these in the show, along with topics like zero shot, one shot

11
00:01:01,040 --> 00:01:02,360
and few shot learning.

12
00:01:02,360 --> 00:01:06,680
I'd also like to give a shout out to Shreyes, a listener who wrote into a request that

13
00:01:06,680 --> 00:01:11,000
we interview a current PhD student about their journey and experiences.

14
00:01:11,000 --> 00:01:14,080
Chelsea and I spend some time at the end of the interview talking about this and she

15
00:01:14,080 --> 00:01:19,560
has some great advice for current and prospective PhD students, but also for independent learners

16
00:01:19,560 --> 00:01:20,880
in the field.

17
00:01:20,880 --> 00:01:24,960
Also, during this part of the discussion, I wonder out loud if any listeners would be

18
00:01:24,960 --> 00:01:28,720
interested in forming a virtual paper reading club of some sort.

19
00:01:28,720 --> 00:01:32,720
I'm not sure yet exactly what this would look like, but please drop a comment in the

20
00:01:32,720 --> 00:01:34,640
show notes if you're interested.

21
00:01:34,640 --> 00:01:39,000
Okay, once again, I'm going to deploy the nerd alert for this episode.

22
00:01:39,000 --> 00:01:43,880
Chelsea and I really dig deep into her research and this conversation gets pretty technical

23
00:01:43,880 --> 00:01:48,120
at times, to the point that I had a hard time keeping up myself.

24
00:01:48,120 --> 00:01:53,760
Thanks again to our sponsor for this series and my industrial AI research, bonsai, bonsai

25
00:01:53,760 --> 00:01:59,360
offers an AI platform that empowers enterprises to build and deploy intelligent systems.

26
00:01:59,360 --> 00:02:04,440
If you're trying to build AI powered applications focused on optimizing and controlling the systems

27
00:02:04,440 --> 00:02:07,920
in your enterprise, you should take a look at what they're up to.

28
00:02:07,920 --> 00:02:12,720
They've got a unique approach to building AI models that let you use high level code

29
00:02:12,720 --> 00:02:16,520
to model the real world concepts in your application.

30
00:02:16,520 --> 00:02:21,560
Really generate, train, and evaluate low level models for your project, using technologies

31
00:02:21,560 --> 00:02:27,240
like reinforcement learning, and easily integrate those models into your applications and systems

32
00:02:27,240 --> 00:02:29,240
using APIs.

33
00:02:29,240 --> 00:02:35,800
You can check them out at bonds.ai, B-O-N-S.ai, and definitely let them know you appreciate

34
00:02:35,800 --> 00:02:47,640
their support of the podcast and now on to the show.

35
00:02:47,640 --> 00:02:51,280
Alright, hey everyone, I've got Chelsea Finn on the line with me.

36
00:02:51,280 --> 00:02:55,640
I'm super excited to have Chelsea here to speak with us.

37
00:02:55,640 --> 00:03:03,880
Chelsea is a PhD student at UC Berkeley and she is co-advised by both Peter Abiel and

38
00:03:03,880 --> 00:03:05,400
Sergei Levine.

39
00:03:05,400 --> 00:03:10,560
By the time this podcast is posted, you'll have heard my interview with Peter and we'll

40
00:03:10,560 --> 00:03:14,960
be digging in a little bit deeper into some of the things we spoke about with Peter with

41
00:03:14,960 --> 00:03:21,320
regards to reinforcement learning, but in particular, we'll focus on Chelsea's research

42
00:03:21,320 --> 00:03:27,680
into topics like deep sensory motor learning and few shot learning and some other things

43
00:03:27,680 --> 00:03:28,680
she's working on.

44
00:03:28,680 --> 00:03:31,320
Chelsea, thanks so much for joining us.

45
00:03:31,320 --> 00:03:32,320
Thanks for having me.

46
00:03:32,320 --> 00:03:33,800
It's great to have you.

47
00:03:33,800 --> 00:03:39,280
Why don't we get started by having you tell us a little bit about how you got interested

48
00:03:39,280 --> 00:03:42,280
in AI and how you got to where you are?

49
00:03:42,280 --> 00:03:48,600
Great, so I did my undergraduate at MIT as an undergrad and I pretty early on decided

50
00:03:48,600 --> 00:03:56,080
that I wanted to go, I wanted major in computer science and once I made that decision, there's

51
00:03:56,080 --> 00:04:03,080
a lot of different things that you can do with computer science, but machine learning and

52
00:04:03,080 --> 00:04:08,040
AI was the thing that I found myself most interested in given how much math it has, unlike

53
00:04:08,040 --> 00:04:13,640
some other areas of computer science, there's a lot of math involved, probability, statistics

54
00:04:13,640 --> 00:04:20,760
and I like that grounding in math and I also find that AI has a lot of very important

55
00:04:20,760 --> 00:04:26,720
applications and I think it's the potential to have a big impact on society.

56
00:04:26,720 --> 00:04:33,560
Absolutely, and I think the further we get with AI, the more of these potential applications

57
00:04:33,560 --> 00:04:42,200
we're seeing in particular some of these industrial use cases where we're using AI to control

58
00:04:42,200 --> 00:04:48,640
robotics and help automate things and that is a big focus of your research, is it not?

59
00:04:48,640 --> 00:04:54,280
Absolutely, I work a lot with real hardware and trying to get robots to learn how to do

60
00:04:54,280 --> 00:04:57,600
tasks and act intelligently, ultimately.

61
00:04:57,600 --> 00:05:02,560
So why don't we talk a little bit about some of the challenges that are involved in doing

62
00:05:02,560 --> 00:05:03,560
that?

63
00:05:03,560 --> 00:05:11,080
So unlike many problems in machine learning, in robotics you have a physical system

64
00:05:11,080 --> 00:05:17,040
that is in the real world and collecting data and the actions that you take affect

65
00:05:17,040 --> 00:05:22,160
the environment and affect the world and that affects what actions you want to take

66
00:05:22,160 --> 00:05:23,160
next.

67
00:05:23,160 --> 00:05:26,320
So you can't just download some data set and process it in a passive way.

68
00:05:26,320 --> 00:05:33,040
You need to actually be collecting data online and then learning from that data and then

69
00:05:33,040 --> 00:05:35,280
repeating essentially.

70
00:05:35,280 --> 00:05:43,280
And when you're collecting that data online, that poses a big challenge, particularly if

71
00:05:43,280 --> 00:05:49,080
you're working with systems that aren't in a lab environment, but in some production

72
00:05:49,080 --> 00:05:50,080
use.

73
00:05:50,080 --> 00:05:55,920
Can you talk a little bit about some of the specific challenges with regard to data collection

74
00:05:55,920 --> 00:06:02,360
and are there techniques or is there research being done that's focused on that particular

75
00:06:02,360 --> 00:06:07,120
slice, how to make that data collection more effective and efficient?

76
00:06:07,120 --> 00:06:12,520
So some of the big success stories in robotics in industrial applications has largely been

77
00:06:12,520 --> 00:06:16,920
in factories and in very controlled settings where you can essentially just pre-program

78
00:06:16,920 --> 00:06:21,280
exactly what motion the robot is going to be doing ahead of time and then just have the

79
00:06:21,280 --> 00:06:24,240
robot repeat that action again and again.

80
00:06:24,240 --> 00:06:28,760
And in these industrial applications, you really don't see robots even using any type of

81
00:06:28,760 --> 00:06:29,760
perception.

82
00:06:29,760 --> 00:06:33,080
They're simply just blindly executing motions.

83
00:06:33,080 --> 00:06:37,400
In machine learning research for robotics, we're trying to move beyond that and I think

84
00:06:37,400 --> 00:06:41,640
that what learning will bring to robotics is the ability to adapt to new environments

85
00:06:41,640 --> 00:06:47,280
and learn to do tasks in very unstructured environments where you don't know what the

86
00:06:47,280 --> 00:06:52,000
world looks like ahead of time and the environment might be dynamic.

87
00:06:52,000 --> 00:06:57,680
A lot of research right now in robotic learning is still in lab environments because that's

88
00:06:57,680 --> 00:07:00,240
where we can set up controlled experiments.

89
00:07:00,240 --> 00:07:04,320
It's more convenient to collect data in your lab than actually putting the robot out into

90
00:07:04,320 --> 00:07:05,320
the real world.

91
00:07:05,320 --> 00:07:09,160
But I think that in the very near future we're going to start seeing more and more research

92
00:07:09,160 --> 00:07:13,280
where robots are actually out in the real world and collecting data because that's where

93
00:07:13,280 --> 00:07:17,480
we'll be able to get the diversity of data that we need to be able to effectively generalize

94
00:07:17,480 --> 00:07:20,960
to new tasks, to new types of objects, etc.

95
00:07:20,960 --> 00:07:26,480
One of the things that I've seen that's been really interesting is been the use of clusters

96
00:07:26,480 --> 00:07:33,040
if you will of robots that are operating in parallel to try to accelerate both data acquisition

97
00:07:33,040 --> 00:07:34,040
and learning.

98
00:07:34,040 --> 00:07:36,840
Are you doing anything with that kind of environment?

99
00:07:36,840 --> 00:07:42,920
Yeah, so I did an internship at Google Brain about a year ago now where I worked on 10

100
00:07:42,920 --> 00:07:48,400
robot arms that were all the same and could collect data in parallel and share their experiences

101
00:07:48,400 --> 00:07:52,080
so that they could more efficiently collect a very large amount of data.

102
00:07:52,080 --> 00:07:56,560
And with that we were able to basically give a bunch of objects to each of the robots

103
00:07:56,560 --> 00:08:00,480
and let them play around with those objects and share their experience with one set of

104
00:08:00,480 --> 00:08:04,400
on one set of objects with another robot who had an experience with the different set

105
00:08:04,400 --> 00:08:05,840
of objects.

106
00:08:05,840 --> 00:08:12,600
We also now have four robot arms here at Berkeley and we might get more that we're just getting

107
00:08:12,600 --> 00:08:16,040
that set up right now and we're soon going to be able to have the capability to set something

108
00:08:16,040 --> 00:08:19,680
up at Berkeley in a similar fashion to what we did at Google.

109
00:08:19,680 --> 00:08:21,040
Oh, that's great.

110
00:08:21,040 --> 00:08:27,400
Is there a name for at use case of the robotics where you're training them in parallel and

111
00:08:27,400 --> 00:08:35,160
you're in the middle of the training transferring knowledge between the different robots?

112
00:08:35,160 --> 00:08:39,480
It sounds like, you know, some version of like active transfer learning or something like

113
00:08:39,480 --> 00:08:40,480
that.

114
00:08:40,480 --> 00:08:42,480
Is there a standard name for that yet?

115
00:08:42,480 --> 00:08:43,480
No, not yet.

116
00:08:43,480 --> 00:08:46,480
We typically just call it large scale robotic learning.

117
00:08:46,480 --> 00:08:47,480
Okay.

118
00:08:47,480 --> 00:08:48,480
Okay.

119
00:08:48,480 --> 00:08:55,520
Well, that's maybe you take a step back when I saw your talk at the rework deep learning

120
00:08:55,520 --> 00:08:56,920
summit.

121
00:08:56,920 --> 00:09:04,200
You started with a particular example that you used in your research and kind of built

122
00:09:04,200 --> 00:09:09,520
your discussion of the, you know, the various challenges and the research you were doing

123
00:09:09,520 --> 00:09:10,840
around this example.

124
00:09:10,840 --> 00:09:15,120
And if I remember correctly, it was, you know, something along the lines of taking, you

125
00:09:15,120 --> 00:09:19,080
know, triangular blocks and putting them in the right holes or something like that.

126
00:09:19,080 --> 00:09:24,360
Can you maybe, you know, walk us through that scenario and then talk us through, you

127
00:09:24,360 --> 00:09:27,240
know, some of what you discussed in your presentation?

128
00:09:27,240 --> 00:09:28,240
Yeah.

129
00:09:28,240 --> 00:09:34,400
So the first thing that I set out to do in my PhD was to try to see if it was possible

130
00:09:34,400 --> 00:09:41,120
to learn a deep neural network that maps from what the robot sees to the actions that

131
00:09:41,120 --> 00:09:46,520
the robot takes and see if we were able to learn, learn a policy that does this for manipulation

132
00:09:46,520 --> 00:09:52,080
skills and be able to do that successfully and one of the first tasks that we experimented

133
00:09:52,080 --> 00:09:59,440
with was inserting a block into a shapeshifting cube and the, I guess, usually when I begin

134
00:09:59,440 --> 00:10:04,320
with that example in my presentations, I talk about for a human, this is very intuitive

135
00:10:04,320 --> 00:10:09,880
to do because over the course of your life, you've learned how to guide your arm such that

136
00:10:09,880 --> 00:10:12,520
the, the block falls into the hole nicely.

137
00:10:12,520 --> 00:10:18,440
But for a computer for a robot, what the robot sees is just a bunch of numbers, a huge array

138
00:10:18,440 --> 00:10:24,040
of numbers in the picture and likewise, the actions that it's taking, maybe the torque

139
00:10:24,040 --> 00:10:28,520
supplied to the joints of the arm is also just a bunch of numbers.

140
00:10:28,520 --> 00:10:34,440
And to be able to map from one set of numbers to another set of numbers, to do that accurately

141
00:10:34,440 --> 00:10:40,000
and to do that in a way that will robustly handle a variety of environments, there's no way

142
00:10:40,000 --> 00:10:41,720
that a hand engineer approach will work.

143
00:10:41,720 --> 00:10:45,920
We need, we need to be able to learn that function and we need things like deep neural networks

144
00:10:45,920 --> 00:10:51,760
to provide a very flexible function in order to be able to effectively do learning in

145
00:10:51,760 --> 00:10:52,760
that scenario.

146
00:10:52,760 --> 00:10:57,560
Yeah, I think that's a, you know, a great way for folks to think about what deep learning

147
00:10:57,560 --> 00:11:04,680
is, you know, thinking about it as a function that, you know, maps from, you know, one set

148
00:11:04,680 --> 00:11:10,920
of, from a set of inputs to a set of outputs, it's interesting that, you know, that we're

149
00:11:10,920 --> 00:11:16,440
able to throw all this data at the problem and have the computer figure out these functions.

150
00:11:16,440 --> 00:11:17,840
Yeah, absolutely.

151
00:11:17,840 --> 00:11:23,200
And actually, one of the challenges in robotics is that typically in deep learning, you gather

152
00:11:23,200 --> 00:11:28,760
a very large data set and then train your neural network on that data set, whereas in robotics,

153
00:11:28,760 --> 00:11:33,760
it's impractical to collect a huge amount of data for a task that you might want to train

154
00:11:33,760 --> 00:11:38,560
because you're actually collecting that data on a real physical system.

155
00:11:38,560 --> 00:11:43,880
And so are there techniques that can be used to, or let's maybe talk a little bit about

156
00:11:43,880 --> 00:11:48,520
the techniques that can be used to address that problem.

157
00:11:48,520 --> 00:11:53,800
Deep learning historically requires lots and lots of data in these industrial environments.

158
00:11:53,800 --> 00:11:56,640
It's difficult to collect lots and lots of data.

159
00:11:56,640 --> 00:11:57,640
What can we do then?

160
00:11:57,640 --> 00:12:00,880
Yeah, so there's been a number of approaches.

161
00:12:00,880 --> 00:12:04,560
One is to, as we talked about before, just get a bunch of robots and have them collect

162
00:12:04,560 --> 00:12:05,880
data in parallel.

163
00:12:05,880 --> 00:12:09,480
Well, actually, one of the challenges with that approach is that if you're having a

164
00:12:09,480 --> 00:12:13,920
lot of robots collect data, you don't want to have a human for every single robot labeling

165
00:12:13,920 --> 00:12:17,720
the data or resetting the environment or providing other means of supervision because

166
00:12:17,720 --> 00:12:20,720
that defeats the point of having the robot there in the first place.

167
00:12:20,720 --> 00:12:25,200
So you can get a lot of data, but you need to algorithm that can learn from the raw data

168
00:12:25,200 --> 00:12:29,600
rather than from labeled data like we see in some of the most successful applications

169
00:12:29,600 --> 00:12:31,120
of deep learning.

170
00:12:31,120 --> 00:12:37,360
Another approach to this problem is to train in simulation where it is very practical

171
00:12:37,360 --> 00:12:44,600
to acquire a lot of data and then try to use what you learned in simulation to be able

172
00:12:44,600 --> 00:12:48,560
to effectively act in the real world, either with zero shot transfer where you don't get

173
00:12:48,560 --> 00:12:54,840
meet where you get zero data in the real world or with or a few shot transfer or just fine

174
00:12:54,840 --> 00:12:57,680
tuning in the real world where you just need less data in the real world than you would

175
00:12:57,680 --> 00:13:00,120
need otherwise if you didn't have that simulated data.

176
00:13:00,120 --> 00:13:03,920
Well, maybe let's talk a little bit about those three things.

177
00:13:03,920 --> 00:13:11,920
So with simulation, maybe walk through the process of using simulator to train a deep

178
00:13:11,920 --> 00:13:12,920
neural net.

179
00:13:12,920 --> 00:13:13,920
Yeah.

180
00:13:13,920 --> 00:13:20,720
So in simulation, we can use algorithms that require a lot of data, specifically reinforcement

181
00:13:20,720 --> 00:13:25,120
learning algorithms that reinforcement learning algorithms are typically very data inefficient

182
00:13:25,120 --> 00:13:31,480
because they don't have the exact input output labels that you have in supervised learning.

183
00:13:31,480 --> 00:13:34,760
You're not just trying to map from one thing to another where you know exactly what the

184
00:13:34,760 --> 00:13:36,240
output should be.

185
00:13:36,240 --> 00:13:40,160
Instead, you get experience in the environment, then you get feedback from the environment

186
00:13:40,160 --> 00:13:42,000
on how well you did.

187
00:13:42,000 --> 00:13:43,240
And that feedback might be delayed.

188
00:13:43,240 --> 00:13:44,240
It might be sparse.

189
00:13:44,240 --> 00:13:48,000
So you might not get it very often or it might not be very detailed.

190
00:13:48,000 --> 00:13:52,760
And so as a result, reinforcement learning algorithms are significantly slower than super

191
00:13:52,760 --> 00:13:56,440
and require significantly more data than supervised learning problems.

192
00:13:56,440 --> 00:13:59,840
It'd be already know that supervised learning problems require a lot of data.

193
00:13:59,840 --> 00:14:05,720
So typically what this kind of the, what an approach like this might look like is train

194
00:14:05,720 --> 00:14:10,920
a policy in simulation using your favorite reinforcement learning method and then take

195
00:14:10,920 --> 00:14:15,680
that policy and try to transfer it into the real world, either just by running it in

196
00:14:15,680 --> 00:14:20,480
the real world and hoping that it works potentially with some modularity to like a vision system

197
00:14:20,480 --> 00:14:25,000
on the robot, like a specific to the robot versus specific to the simulation or a controller

198
00:14:25,000 --> 00:14:28,840
that specific to the robot and specific to the simulation or trying to initialize with

199
00:14:28,840 --> 00:14:31,440
that policy and then fine tune in the real world.

200
00:14:31,440 --> 00:14:36,160
One of the reasons why that transfer doesn't just happen automatically is that one, similarly

201
00:14:36,160 --> 00:14:41,920
vision is usually lower fidelity and not as realistic as the vision that we get from

202
00:14:41,920 --> 00:14:44,120
cameras in the real world.

203
00:14:44,120 --> 00:14:50,440
And physics simulation, the physics in simulators is actually not at all accurate, especially

204
00:14:50,440 --> 00:14:55,960
when you encounter a contact between two different objects.

205
00:14:55,960 --> 00:15:00,960
Modeling contacts and exactly what goes on within that contact is quite complex, so that's

206
00:15:00,960 --> 00:15:05,920
not something that we can accurately model in simulators.

207
00:15:05,920 --> 00:15:07,480
So that's really interesting.

208
00:15:07,480 --> 00:15:16,960
So the specific challenge there is, for example, in simulation, you're modeling a robot manipulator

209
00:15:16,960 --> 00:15:24,960
like a hand that's picking up a block, I can imagine that the physics governing how that

210
00:15:24,960 --> 00:15:30,600
simulator is grasping that block and the coefficients of static friction and dynamic

211
00:15:30,600 --> 00:15:34,440
friction and all the things that determine the way the block will ultimately sit in the

212
00:15:34,440 --> 00:15:37,680
gripper can be quite difficult to model.

213
00:15:37,680 --> 00:15:41,440
Is that basically what you're describing in terms of when you're modeling two bodies?

214
00:15:41,440 --> 00:15:42,760
Yeah, exactly.

215
00:15:42,760 --> 00:15:48,360
And so with the difficulty in modeling that, how do you account for that?

216
00:15:48,360 --> 00:15:54,800
Do you just do a rough approximation and assume that the difference is noise in the system

217
00:15:54,800 --> 00:16:00,480
that your model just needs to account for or are there specific techniques for dealing

218
00:16:00,480 --> 00:16:01,480
with that?

219
00:16:01,480 --> 00:16:02,480
Yeah.

220
00:16:02,480 --> 00:16:07,360
Simulators have various ways to approximate them, then yeah, generally the learning algorithm

221
00:16:07,360 --> 00:16:10,920
doesn't look any different in simulation versus in the real world.

222
00:16:10,920 --> 00:16:15,520
Okay, one conversation that I had that I think I mentioned in the conversation with Peter

223
00:16:15,520 --> 00:16:20,920
as well was a conversation with Stefano Irman over at Stanford who was talking about like

224
00:16:20,920 --> 00:16:28,400
incorporating physics into models and Peter and I talked about that, I think fairly generally,

225
00:16:28,400 --> 00:16:32,600
is that something that comes into play specifically in your work?

226
00:16:32,600 --> 00:16:40,120
And in this issue of the grippers, for example, or the contact between the robot and other

227
00:16:40,120 --> 00:16:41,120
objects.

228
00:16:41,120 --> 00:16:44,720
So you're asking, like, do we try to learn models of the world?

229
00:16:44,720 --> 00:16:52,680
I guess I'm asking, how do you try to incorporate pre-existing knowledge about the way grippers

230
00:16:52,680 --> 00:16:58,480
grip and objects respond to being gripped into your deep learning models?

231
00:16:58,480 --> 00:17:04,960
So some of our algorithms, we do incorporate knowledge about the physical world.

232
00:17:04,960 --> 00:17:10,280
For example, we have a certain type of model usually in one of the algorithms that we use

233
00:17:10,280 --> 00:17:12,280
and that we can run on real robots.

234
00:17:12,280 --> 00:17:18,560
We use a Gaussian mixture model as a prior on a learned time-varing linear model, it

235
00:17:18,560 --> 00:17:21,720
is getting a bit complicated but a bit technical.

236
00:17:21,720 --> 00:17:25,960
But generally a Gaussian mixture model is or at least a prior from a Gaussian mixture

237
00:17:25,960 --> 00:17:34,160
model is a fairly reasonable way to model physics and that oftentimes there are different

238
00:17:34,160 --> 00:17:40,360
modes of physics, whether you're in different types of contact, whether you're in free space

239
00:17:40,360 --> 00:17:44,800
and that sort of model is well suited for that.

240
00:17:44,800 --> 00:17:48,880
Actually, I'd love to have you walk us through the details on that.

241
00:17:48,880 --> 00:17:51,600
Can we start with what is a Gaussian mixture model?

242
00:17:51,600 --> 00:17:58,240
Yeah, so this is one small part of a much larger algorithm for learning policies on the

243
00:17:58,240 --> 00:18:00,040
real robot.

244
00:18:00,040 --> 00:18:07,880
So a Gaussian mixture model is a distribution where there are multiple mixture components

245
00:18:07,880 --> 00:18:12,400
and each mixture component is a Gaussian distribution.

246
00:18:12,400 --> 00:18:18,280
So a normal distribution and then each component also has a weight.

247
00:18:18,280 --> 00:18:25,520
And then the Gaussian mixture model is simply the weighted sum of all of the Gaussians.

248
00:18:25,520 --> 00:18:34,280
And is the Gaussian mixture model are using a mixture to model one specific thing or using

249
00:18:34,280 --> 00:18:40,920
that mixture to model a several phenomena at once?

250
00:18:40,920 --> 00:18:48,120
We're using the mixture to model a mode of dynamics where by dynamics I mean a conditional

251
00:18:48,120 --> 00:18:53,800
distribution of the next state given the current state in action.

252
00:18:53,800 --> 00:18:58,320
So you're basically trying to predict what the next state is going to be given where

253
00:18:58,320 --> 00:19:04,280
you are right now and the action that you take and that conditional distribution will

254
00:19:04,280 --> 00:19:07,760
depend on whether or not you're in contact, whether or not your finger is sliding across

255
00:19:07,760 --> 00:19:12,880
the table versus in static contact or versus whether or not you're in free space.

256
00:19:12,880 --> 00:19:17,760
And so we're using each mixture component of the Gaussian mixture model to model those

257
00:19:17,760 --> 00:19:21,280
different modes of your dynamics.

258
00:19:21,280 --> 00:19:26,120
And so you mentioned a few things in terms of your finger sliding in those things.

259
00:19:26,120 --> 00:19:32,960
Do each of those map one to one to a component of the mixture or is all of that modeled

260
00:19:32,960 --> 00:19:35,240
by the whole of the mixture?

261
00:19:35,240 --> 00:19:40,520
Each of those will typically map to one component, although usually you don't know the number

262
00:19:40,520 --> 00:19:46,600
of components a priori, so you set it to a number that you think is slightly larger

263
00:19:46,600 --> 00:19:52,880
than the actual number of components, just like in k-means or in clustering algorithms.

264
00:19:52,880 --> 00:19:53,880
Okay.

265
00:19:53,880 --> 00:19:54,880
All right.

266
00:19:54,880 --> 00:20:00,920
So you've got this mixture that models some of the physics that rolls up into the broader

267
00:20:00,920 --> 00:20:03,160
model that you're trying to build.

268
00:20:03,160 --> 00:20:07,360
And you said you used that as a prior for fitting a model.

269
00:20:07,360 --> 00:20:08,360
Okay.

270
00:20:08,360 --> 00:20:14,640
And so when you say using that as a priori, you're basically using the output of that Gaussian

271
00:20:14,640 --> 00:20:19,920
mixture model as an input to your deep neural net, is that the right way to think about

272
00:20:19,920 --> 00:20:20,920
it?

273
00:20:20,920 --> 00:20:24,880
So in this case, actually, the dynamics model that we're learning is not a neural network.

274
00:20:24,880 --> 00:20:31,960
We're learning a local time varying linear model where by time varying linear, I mean

275
00:20:31,960 --> 00:20:37,120
that you basically sample a bunch of trajectories on your robot.

276
00:20:37,120 --> 00:20:43,480
And then at every time step in those set of trajectories, you fit a linear model using

277
00:20:43,480 --> 00:20:45,400
linear regression.

278
00:20:45,400 --> 00:20:50,720
And then the Gaussian mixture model is fit to all of the time steps of all of the samples

279
00:20:50,720 --> 00:20:54,280
and that's used as a prior for linear regression, done at every time step.

280
00:20:54,280 --> 00:20:55,280
Okay.

281
00:20:55,280 --> 00:20:56,280
Got it.

282
00:20:56,280 --> 00:20:57,280
Got it.

283
00:20:57,280 --> 00:21:01,680
And then ultimately we use this, well, there's more steps involved, but you use this

284
00:21:01,680 --> 00:21:09,600
this dynamics model that you fit to acquire an optimal policy for a certain version of

285
00:21:09,600 --> 00:21:10,600
your problem.

286
00:21:10,600 --> 00:21:11,600
Okay.

287
00:21:11,600 --> 00:21:18,400
And then when you have an optimal policy, you can at a high level, the top down explanation

288
00:21:18,400 --> 00:21:25,800
is that given a certain manipulation problem, you can decompose your problem into different

289
00:21:25,800 --> 00:21:31,080
instances of the problem, like for a single start position and a single end position.

290
00:21:31,080 --> 00:21:37,840
And then we are solving for the optimal policy for each individual condition using optimal

291
00:21:37,840 --> 00:21:42,120
control and using this linear model that we fit.

292
00:21:42,120 --> 00:21:49,080
And then once we solve each of the individual problems, then we use that for supervised

293
00:21:49,080 --> 00:21:53,360
learning of a deep neural network that can solve all of the instances of the problem.

294
00:21:53,360 --> 00:21:54,360
Okay.

295
00:21:54,360 --> 00:21:57,520
So let me try to paraphrase that to make sure I'm following.

296
00:21:57,520 --> 00:22:02,200
So you've got, it sounds like we are talking here.

297
00:22:02,200 --> 00:22:08,720
You mentioned point A and point B, are we talking about, you know, strictly the problem

298
00:22:08,720 --> 00:22:15,680
of you've got a robot arm, let's say, with, you know, N degrees of freedom, and you're

299
00:22:15,680 --> 00:22:20,040
trying to figure out a path, you know, to translate, you know, to get the gripper from

300
00:22:20,040 --> 00:22:23,560
point A to point B using those motors.

301
00:22:23,560 --> 00:22:28,040
Is that the scope of the problem that we're talking about or have I read into that to

302
00:22:28,040 --> 00:22:29,040
narrowly?

303
00:22:29,040 --> 00:22:32,720
That is a little bit too narrow, so it's not just moving the gripper of the robot, it

304
00:22:32,720 --> 00:22:34,920
could also involve moving objects.

305
00:22:34,920 --> 00:22:41,880
It could be, so this algorithm has also been applied to manipulating objects within a five

306
00:22:41,880 --> 00:22:42,880
fingered hand.

307
00:22:42,880 --> 00:22:48,280
A slightly different version of the algorithm has also been used for a locomotion, robot locomotion.

308
00:22:48,280 --> 00:22:49,280
Okay.

309
00:22:49,280 --> 00:22:53,880
So then maybe taking a step back, it sounds like you've got, you've got something that's,

310
00:22:53,880 --> 00:22:59,720
you're trying to figure out how to get it from point A to point B, and you've got some,

311
00:22:59,720 --> 00:23:03,560
you know, underlying dynamics that you need to model.

312
00:23:03,560 --> 00:23:10,200
And so you use, you train a linear model to tell you basically how to move your motors

313
00:23:10,200 --> 00:23:14,880
to get the thing from point A to point B or state A to state B.

314
00:23:14,880 --> 00:23:23,480
And then once you have those linear models, you're able to use them to generate more data

315
00:23:23,480 --> 00:23:26,440
that you can train deep networks with.

316
00:23:26,440 --> 00:23:30,800
It's basically you're, you're training a data generator.

317
00:23:30,800 --> 00:23:31,800
Essentially.

318
00:23:31,800 --> 00:23:32,800
Yeah.

319
00:23:32,800 --> 00:23:39,160
So the data generator is, is that in any given kind of robot manipulation problem, you

320
00:23:39,160 --> 00:23:42,880
can see your current observation, but you don't know what action you should take.

321
00:23:42,880 --> 00:23:47,160
And what you want to figure out is what action to take, and that's, and so the data generator

322
00:23:47,160 --> 00:23:51,520
is figuring out what action you should take for any given observation.

323
00:23:51,520 --> 00:23:55,560
And then once you have the actions that you should take, you can then just apply play

324
00:23:55,560 --> 00:23:58,280
and supervise learning with your neural network.

325
00:23:58,280 --> 00:23:59,280
Mm-hmm.

326
00:23:59,280 --> 00:24:00,280
Mm-hmm.

327
00:24:00,280 --> 00:24:06,240
Maybe let's take a step back to kind of this, the, you know, the broader problem, which

328
00:24:06,240 --> 00:24:14,000
is the putting the blocks in the right places, a big part of your research is the relationship

329
00:24:14,000 --> 00:24:19,600
between the robot is seeing that problem from a computer vision perspective.

330
00:24:19,600 --> 00:24:24,080
And you're essentially, the intelligence that the robot is acting on is kind of largely

331
00:24:24,080 --> 00:24:30,000
driven by, you know, the manipulation of, or an observation of, you know, the pixels

332
00:24:30,000 --> 00:24:33,160
coming from a camera or a set of cameras.

333
00:24:33,160 --> 00:24:38,680
Can you talk a little bit about the, you know, broadly speaking, the relationship between,

334
00:24:38,680 --> 00:24:43,400
you know, computer vision and the work that you're doing in robotics.

335
00:24:43,400 --> 00:24:47,080
And then, you know, maybe we can drill into some of the specifics.

336
00:24:47,080 --> 00:24:54,520
Yeah, so a lot of the algorithms that we develop for robotics, we aren't necessarily specific

337
00:24:54,520 --> 00:24:57,880
to certain sensory modalities, like tactile or vision.

338
00:24:57,880 --> 00:25:02,680
We want them to work with a fairly wide variety of, of modalities.

339
00:25:02,680 --> 00:25:08,440
Vision is perhaps one of the most interesting because it's, gives you a lot of information

340
00:25:08,440 --> 00:25:13,400
about the environment and it also is one of the most challenging because it is very

341
00:25:13,400 --> 00:25:20,400
high dimensional and not high dimensional in a way that's readily interpretable by a lot

342
00:25:20,400 --> 00:25:21,720
of these algorithms.

343
00:25:21,720 --> 00:25:25,360
One of the other reasons why we use vision a lot is because tactile sensing, while it's

344
00:25:25,360 --> 00:25:30,920
very, can give you a lot of information, good tactile sensors are hard to acquire.

345
00:25:30,920 --> 00:25:35,200
And they're typically either very expensive or very fragile and break a lot.

346
00:25:35,200 --> 00:25:42,200
So a lot of the algorithms that we develop, we try to incorporate, I mean, we use convolutional

347
00:25:42,200 --> 00:25:46,040
neural networks if we're going to use vision, convolutional neural networks are very efficient

348
00:25:46,040 --> 00:25:53,640
and very effective at doing their job, at these localizing objects or inferring things

349
00:25:53,640 --> 00:25:55,920
about the environment, et cetera.

350
00:25:55,920 --> 00:26:02,160
So a lot of my work will be developing algorithms based off of reinforcement learning, imitation

351
00:26:02,160 --> 00:26:07,520
learning, or inverse reinforcement learning, but focusing on algorithms which can scale

352
00:26:07,520 --> 00:26:09,680
to high dimensional inputs like vision.

353
00:26:09,680 --> 00:26:13,040
Okay, you mentioned inverse reinforcement learning.

354
00:26:13,040 --> 00:26:14,040
What's that?

355
00:26:14,040 --> 00:26:15,040
Yeah.

356
00:26:15,040 --> 00:26:21,200
So reinforcement learning first is the problem of given a reward function and the ability

357
00:26:21,200 --> 00:26:26,360
to sample experience from your environment, figure out what the optimal policy is for

358
00:26:26,360 --> 00:26:32,000
that reward function, figure out what actions you should take given a current observation.

359
00:26:32,000 --> 00:26:35,120
Inverse reinforcement learning is essentially the inverse of that.

360
00:26:35,120 --> 00:26:41,040
So in inverse reinforcement learning, you assume that you have rollouts or basically trajectories

361
00:26:41,040 --> 00:26:45,640
from the optimal policy, from an expert, like a human.

362
00:26:45,640 --> 00:26:48,640
And your goal is to figure out what the reward function is.

363
00:26:48,640 --> 00:26:53,240
So your goal is to figure out what the human was trying to accomplish.

364
00:26:53,240 --> 00:26:57,480
And then ultimately, once you figured out what the human was trying to accomplish, then

365
00:26:57,480 --> 00:27:01,840
you also want to learn a policy for yourself that also accomplishes what the human was trying

366
00:27:01,840 --> 00:27:03,760
to accomplish.

367
00:27:03,760 --> 00:27:09,800
So it was an application area that when you're doing imitation learning and you have a human

368
00:27:09,800 --> 00:27:16,880
explicitly move a robot from one position to another, you can then use inverse reinforcement

369
00:27:16,880 --> 00:27:21,800
learning to learn a policy that would produce that same motion.

370
00:27:21,800 --> 00:27:23,280
Yeah, exactly.

371
00:27:23,280 --> 00:27:28,480
So the typical application is given a set of demonstrations from a human, try to figure

372
00:27:28,480 --> 00:27:32,760
out what the human was doing and then figure out how to do it yourself.

373
00:27:32,760 --> 00:27:38,000
And then you said in your previous description, you said not just what the human was doing,

374
00:27:38,000 --> 00:27:41,920
but you kind of characterize it as why the human was doing that, or at least that's what

375
00:27:41,920 --> 00:27:46,640
I read into the way you said it, like the human's intent, is that do you make that distinction

376
00:27:46,640 --> 00:27:55,160
between what the human was doing and the model learning some degree of intent or higher level

377
00:27:55,160 --> 00:27:58,240
purpose, or am I reading too much into it?

378
00:27:58,240 --> 00:27:59,800
No, that's absolutely right.

379
00:27:59,800 --> 00:28:06,160
So the reason why this is interesting is that if a human is doing something, you don't

380
00:28:06,160 --> 00:28:10,400
want to necessarily mimic the exact actions that they do because you might have a different

381
00:28:10,400 --> 00:28:14,880
arm that looks slightly differently, or maybe the way that they're doing it isn't quite

382
00:28:14,880 --> 00:28:20,280
optimal, or you want to be able to generalize what they are doing to new scenarios.

383
00:28:20,280 --> 00:28:22,480
And in those new scenarios, you don't want to do exactly what they did.

384
00:28:22,480 --> 00:28:24,880
You want to achieve what they were trying to achieve.

385
00:28:24,880 --> 00:28:28,720
And so in reverse reinforcement learning, you're adding structure to the problem of imitation

386
00:28:28,720 --> 00:28:32,200
learning, where you're trying to, one, you're assuming that they were acting according

387
00:28:32,200 --> 00:28:36,920
to some reward function, and you're trying to infer what that was and what they were

388
00:28:36,920 --> 00:28:38,800
trying to achieve.

389
00:28:38,800 --> 00:28:39,800
Mm-hmm.

390
00:28:39,800 --> 00:28:47,160
And so bring this, bring all this back to the problem of placing the blocks for us.

391
00:28:47,160 --> 00:28:48,160
Okay.

392
00:28:48,160 --> 00:28:54,720
So in the block scenario, you'd see an example of a human putting the block in the shape

393
00:28:54,720 --> 00:29:00,000
sorting cube, and then you would try to infer the fact that their goal was to get it inside

394
00:29:00,000 --> 00:29:03,440
the cube, not just to take the actions that they were taking.

395
00:29:03,440 --> 00:29:07,560
And then once you infer that, figure out a policy for doing that.

396
00:29:07,560 --> 00:29:12,120
The block example, perhaps, isn't the best example for inverse reinforcement learning,

397
00:29:12,120 --> 00:29:16,400
because providing a reward function for that task is fairly straightforward.

398
00:29:16,400 --> 00:29:21,240
Your goal is to physically get this red block into the shape sorting cube.

399
00:29:21,240 --> 00:29:25,320
But in many other scenarios, it's actually hard to write down what their reward function

400
00:29:25,320 --> 00:29:26,480
should be.

401
00:29:26,480 --> 00:29:29,760
And that's actually one of the big challenges in applying reinforcement learning to real

402
00:29:29,760 --> 00:29:31,240
world scenarios.

403
00:29:31,240 --> 00:29:37,440
So for example, say you want your robot to pour a cup of water from one cup to another

404
00:29:37,440 --> 00:29:38,440
cup.

405
00:29:38,440 --> 00:29:42,800
In that task, you want all of the water to end up in the target cup.

406
00:29:42,800 --> 00:29:44,280
You don't want any water to spill.

407
00:29:44,280 --> 00:29:49,360
You probably want the robot to be somewhat gentle, and encoding those things in a reward

408
00:29:49,360 --> 00:29:54,760
function requires a lot of engineering, like you might need actually like a liquid detector

409
00:29:54,760 --> 00:29:59,200
to be able to detect where the liquid is and to detect if something got wet.

410
00:29:59,200 --> 00:30:03,000
And then also characterizing whether or not the robot was gentle.

411
00:30:03,000 --> 00:30:07,520
And so engineering that reward function may even be more work than engineering the behavior

412
00:30:07,520 --> 00:30:08,520
itself.

413
00:30:08,520 --> 00:30:13,160
And as much easier just to show the robot, this is the task that I want you to do.

414
00:30:13,160 --> 00:30:21,280
But you still need to do you still need to give the robot examples of a failure or of

415
00:30:21,280 --> 00:30:22,280
failures?

416
00:30:22,280 --> 00:30:28,160
Like, to what degree does that come into play as well with supervised learning?

417
00:30:28,160 --> 00:30:34,320
So you've got examples of, hey, I'm successfully getting the cup of water from point A to point

418
00:30:34,320 --> 00:30:39,000
B. And you can label those as successes, but what about labeling?

419
00:30:39,000 --> 00:30:44,480
You know, this is a failure explicitly, you know, even though 80% of the water may have

420
00:30:44,480 --> 00:30:47,840
gotten from point A to point B, does that make any sense?

421
00:30:47,840 --> 00:30:48,840
Yeah.

422
00:30:48,840 --> 00:30:54,320
So typically in inverse reinforcement learning, you only give successful demonstrations,

423
00:30:54,320 --> 00:31:00,600
although thinking about how you could incorporate failed examples would also be, is an interesting

424
00:31:00,600 --> 00:31:03,600
research direction, not a lot of research has been done there.

425
00:31:03,600 --> 00:31:08,760
And actually, perhaps an interesting analogy for people is that for people that are familiar

426
00:31:08,760 --> 00:31:14,480
with generative adversarial networks, you can actually show that the inverse reinforcement

427
00:31:14,480 --> 00:31:19,160
learning objective, that is one of the ones that's most widely used, is mathematically

428
00:31:19,160 --> 00:31:23,560
equivalent to the objective of a discriminator in a generative adversarial network.

429
00:31:23,560 --> 00:31:27,560
So in generative adversarial networks, you're given a data set of images and the goal is

430
00:31:27,560 --> 00:31:31,880
to be able to generate images from that data set or that look like images from that data

431
00:31:31,880 --> 00:31:32,880
set.

432
00:31:32,880 --> 00:31:37,240
And the goal of the discriminator in a generative adversarial network is to figure out

433
00:31:37,240 --> 00:31:42,240
if an image was generated in its fake or if the image came from that data set.

434
00:31:42,240 --> 00:31:46,840
And the reward function that you're trying to learn in inverse reinforcement learning plays

435
00:31:46,840 --> 00:31:48,320
the same role as the discriminator.

436
00:31:48,320 --> 00:31:52,560
So it's trying to, it's generally trying to say that the demonstrations, the example

437
00:31:52,560 --> 00:31:56,840
demonstrations that you got, which is the same as your data set, have high reward.

438
00:31:56,840 --> 00:32:01,240
And things that your policy is trying to generate is try, it has low reward.

439
00:32:01,240 --> 00:32:06,640
So just like in generative adversarial networks, you only get examples of positive data points.

440
00:32:06,640 --> 00:32:07,640
Okay.

441
00:32:07,640 --> 00:32:09,320
Of successful demonstrations.

442
00:32:09,320 --> 00:32:10,320
Okay.

443
00:32:10,320 --> 00:32:17,640
It strikes me that there's, you know, some signal and partial successes to some degree

444
00:32:17,640 --> 00:32:22,600
and it makes me wonder what research is being done out there, you know, in that direction

445
00:32:22,600 --> 00:32:23,600
if anything.

446
00:32:23,600 --> 00:32:24,600
Yeah.

447
00:32:24,600 --> 00:32:29,440
So actually one interesting thing is that humans, even if someone isn't successfully

448
00:32:29,440 --> 00:32:34,160
doing a task, you can typically infer humans can typically infer what they were trying

449
00:32:34,160 --> 00:32:35,160
to accomplish.

450
00:32:35,160 --> 00:32:36,160
Right.

451
00:32:36,160 --> 00:32:41,000
And that's actually one of the things that I'm working on right now is that is an algorithm

452
00:32:41,000 --> 00:32:46,040
which tries to learn from unsuccessful demonstrations that we're still going in the right direction

453
00:32:46,040 --> 00:32:49,920
and still have enough signal to indicate what goal they were trying to achieve.

454
00:32:49,920 --> 00:32:50,920
Okay.

455
00:32:50,920 --> 00:32:51,920
And so what's the approach there?

456
00:32:51,920 --> 00:32:57,000
So the approach that we're trying right now is an approach based on few shot learning

457
00:32:57,000 --> 00:32:58,400
or metal learning.

458
00:32:58,400 --> 00:33:02,120
And I guess another term for metal learning is learning how to learn.

459
00:33:02,120 --> 00:33:03,120
Okay.

460
00:33:03,120 --> 00:33:10,360
And just if I can interject, I think you mentioned also one shot learning previously.

461
00:33:10,360 --> 00:33:14,840
So we've got this, you know, spectrum of learning, if you will.

462
00:33:14,840 --> 00:33:20,600
One shot is, you know, learning on, you know, one example, few shots is learning on a few

463
00:33:20,600 --> 00:33:21,600
examples.

464
00:33:21,600 --> 00:33:25,280
There's also no shot learning, which is learning on no examples.

465
00:33:25,280 --> 00:33:32,840
And then metal learning, it's orthogonal to the end shot issue or is it not?

466
00:33:32,840 --> 00:33:38,000
So a few shot learning and one shot learning are typically achieved using metal learning

467
00:33:38,000 --> 00:33:39,000
algorithms.

468
00:33:39,000 --> 00:33:40,000
Okay.

469
00:33:40,000 --> 00:33:44,640
So metal learning is somewhat of a broader class of algorithms.

470
00:33:44,640 --> 00:33:45,640
Right.

471
00:33:45,640 --> 00:33:46,640
Right.

472
00:33:46,640 --> 00:33:47,640
Okay.

473
00:33:47,640 --> 00:33:48,640
So apologies.

474
00:33:48,640 --> 00:33:49,640
You were saying.

475
00:33:49,640 --> 00:33:54,720
I think you were saying that you were talking about how you're using metal learning or

476
00:33:54,720 --> 00:34:01,520
learning to learn to, you know, solve this problem of learning from failed examples.

477
00:34:01,520 --> 00:34:03,000
Yeah.

478
00:34:03,000 --> 00:34:09,080
So at a high level, what we're working on is being able to show collecting a data set

479
00:34:09,080 --> 00:34:16,760
of examples of demonstrations for different tasks and corrupting some of those demonstrations

480
00:34:16,760 --> 00:34:25,760
with noise and then trying to have a system that learns that the corrupted demonstrations

481
00:34:25,760 --> 00:34:30,400
that learns basically how to do the correct thing from the corrupted demonstrations.

482
00:34:30,400 --> 00:34:36,240
And so in this example, what is the, what is the demonstration?

483
00:34:36,240 --> 00:34:41,720
Are we still talking about the shape sorting cube or is this something else?

484
00:34:41,720 --> 00:34:43,720
And then what, what is the noise?

485
00:34:43,720 --> 00:34:47,720
Are we talking about noise added to sensory input?

486
00:34:47,720 --> 00:34:52,960
Are we talking about noise in some represent or perturbations in some representation of,

487
00:34:52,960 --> 00:34:54,360
you know, the underlying process?

488
00:34:54,360 --> 00:35:00,080
Are we talking about, you know, noise injected into some layer of the deep neural network

489
00:35:00,080 --> 00:35:03,840
or by noise, they just mean noise injected onto the actions.

490
00:35:03,840 --> 00:35:08,400
So the output of the neural network of the demonstrations, so essentially the labels

491
00:35:08,400 --> 00:35:12,640
of the demonstrations, okay, and that will make the demonstrations sub optimal.

492
00:35:12,640 --> 00:35:13,640
Okay.

493
00:35:13,640 --> 00:35:20,440
So basically, you've got some label data that you're training a model on and you just

494
00:35:20,440 --> 00:35:22,680
mess up some of the labels sometimes.

495
00:35:22,680 --> 00:35:26,520
Essentially, although the, we're not training a model in the typical way.

496
00:35:26,520 --> 00:35:32,080
So in this work, we're building on some of my recent work on few shot learning or one

497
00:35:32,080 --> 00:35:33,080
shot learning.

498
00:35:33,080 --> 00:35:38,400
Few shot learning is just kind of the general case where if you could be one to ten or maybe

499
00:35:38,400 --> 00:35:44,360
a little bit more, where we try to learn a representation that's very quickly adaptable

500
00:35:44,360 --> 00:35:46,800
to many different tasks.

501
00:35:46,800 --> 00:35:56,000
And maybe let's dig into the few shot learning issue and talk a little bit about just,

502
00:35:56,000 --> 00:36:00,400
you know, your approach, what you have done in your recent research, but also what others

503
00:36:00,400 --> 00:36:05,000
have done and, you know, a little bit of a background into the problem domain if we

504
00:36:05,000 --> 00:36:06,000
could.

505
00:36:06,000 --> 00:36:07,000
Yeah, absolutely.

506
00:36:07,000 --> 00:36:15,040
So in few shot learning, the goal is to be able to do a new task from only a very small

507
00:36:15,040 --> 00:36:17,480
number of data points from that task.

508
00:36:17,480 --> 00:36:23,800
So an example of this is in the, the visual recognition realm is say that you're given

509
00:36:23,800 --> 00:36:30,240
a picture of a segue and then your goal is given that single picture of a segue, be

510
00:36:30,240 --> 00:36:35,000
able to classify other examples of segues successfully.

511
00:36:35,000 --> 00:36:40,800
And the way that you learn how to do this is get a data set of lots of different types

512
00:36:40,800 --> 00:36:46,400
of objects with a few types of each different object and you learn about the, the variation

513
00:36:46,400 --> 00:36:47,400
across objects.

514
00:36:47,400 --> 00:36:52,120
So you actually learn how to identify objects just from one example.

515
00:36:52,120 --> 00:36:56,920
And so is the data set that you're referring to?

516
00:36:56,920 --> 00:37:01,880
Is that a label data set or that is that an unlabeled data set that you're just learning

517
00:37:01,880 --> 00:37:08,880
a bunch of things from right now methods, well, it's a label data set, okay.

518
00:37:08,880 --> 00:37:13,120
And is it a label data set?

519
00:37:13,120 --> 00:37:20,880
What are the, the nature of the labels, meaning is it labeled with, you know, objects or

520
00:37:20,880 --> 00:37:26,040
meaning this is an orange, this is a cat, or is it, you know, some kind of labels of the

521
00:37:26,040 --> 00:37:28,960
physical attributes of the things that are depicted?

522
00:37:28,960 --> 00:37:30,440
It depends on what you want to do.

523
00:37:30,440 --> 00:37:37,320
If your goal is to do what's called one shot image classification, then you take a standard

524
00:37:37,320 --> 00:37:42,000
image data set that has an image and it's corresponding label and a full data set of

525
00:37:42,000 --> 00:37:43,000
that.

526
00:37:43,000 --> 00:37:48,440
And just like image net, Mness, there's a data set called Omniglott that's very popular

527
00:37:48,440 --> 00:37:54,120
for one shot learning and yeah, that's the nature of the data set that you use for

528
00:37:54,120 --> 00:37:55,560
metal learning.

529
00:37:55,560 --> 00:38:01,560
And so you, you have, let's say image net, you have image net, huge database of labeled

530
00:38:01,560 --> 00:38:09,360
images and, you know, we'll assume that there's no segways in image net and then you're basically

531
00:38:09,360 --> 00:38:14,240
trying to show it a picture of a segway and then you're showing it a labeled picture

532
00:38:14,240 --> 00:38:18,920
of a segway, a single labeled picture of a segway and you want it to be able to identify

533
00:38:18,920 --> 00:38:21,080
subsequent segways.

534
00:38:21,080 --> 00:38:22,080
Yes.

535
00:38:22,080 --> 00:38:23,080
Correct.

536
00:38:23,080 --> 00:38:32,120
And I guess the question that what I'm wondering is why do the labels in image net

537
00:38:32,120 --> 00:38:33,120
matter?

538
00:38:33,120 --> 00:38:37,320
Like what doesn't matter if, you know, we have labels for oranges and these other things.

539
00:38:37,320 --> 00:38:43,240
I'm imagining that what's happening is you're throwing all that data, oh, you're training

540
00:38:43,240 --> 00:38:45,800
a deep neural network against all of that data.

541
00:38:45,800 --> 00:38:52,200
And then, you know, within, you know, the various layers of the neural net, it's kind of

542
00:38:52,200 --> 00:38:58,960
figuring out, you know, textures and colors and geometries and curves and edges and things

543
00:38:58,960 --> 00:38:59,960
like that.

544
00:38:59,960 --> 00:39:03,400
And it's using that to identify, you know, a segway.

545
00:39:03,400 --> 00:39:07,760
What is, what is the fact that the data set is labeled matter in that case?

546
00:39:07,760 --> 00:39:08,760
Yes.

547
00:39:08,760 --> 00:39:09,760
That's a very good question.

548
00:39:09,760 --> 00:39:16,000
It actually doesn't matter that it labeled an orange as an orange or a banana as a banana.

549
00:39:16,000 --> 00:39:21,160
What matters is that it is labeled in the sense that it knows like this set of images is

550
00:39:21,160 --> 00:39:22,160
one type of object.

551
00:39:22,160 --> 00:39:24,080
This set of images is another type of object.

552
00:39:24,080 --> 00:39:25,080
Yeah.

553
00:39:25,080 --> 00:39:26,080
That makes sense.

554
00:39:26,080 --> 00:39:27,080
Okay.

555
00:39:27,080 --> 00:39:31,680
So it's, you're basically, the label is is a way to communicate, you know, clustering or

556
00:39:31,680 --> 00:39:38,840
similarity of like images and it can use that to properly form the internal structure

557
00:39:38,840 --> 00:39:41,320
of your neural net to reflect all these things.

558
00:39:41,320 --> 00:39:42,320
Yes.

559
00:39:42,320 --> 00:39:43,320
Okay.

560
00:39:43,320 --> 00:39:49,920
Have you gotten yet the specifics of how folks are attacking fuchsot learning based

561
00:39:49,920 --> 00:39:53,000
on, you know, having, you know, with image and things like that?

562
00:39:53,000 --> 00:39:59,280
Like are there specific network architectures or specific training techniques or things like

563
00:39:59,280 --> 00:40:05,360
that that lend themselves to building these fuchsot models or meta learning models?

564
00:40:05,360 --> 00:40:12,360
Yeah. So there's actually a few fairly broad classes of techniques for solving this problem.

565
00:40:12,360 --> 00:40:19,480
One of the perhaps easier class of methods to explain is a class of methods that tries

566
00:40:19,480 --> 00:40:27,480
to learn an embedding of images such that when you run nearest neighbor or do like comparisons

567
00:40:27,480 --> 00:40:35,120
in that embedding space, you can very accurately generalize from just one or a few examples.

568
00:40:35,120 --> 00:40:44,600
And just for, for background, an embedding is basically taking a set of images and you

569
00:40:44,600 --> 00:40:49,880
mathematically kind of turning them into vectors that somehow relate one to the other.

570
00:40:49,880 --> 00:40:54,560
So say that again with that as background, you, you, you, you learn an embedding space

571
00:40:54,560 --> 00:41:00,440
such that when you make comparisons in that embedding space, you can generalize well from

572
00:41:00,440 --> 00:41:02,840
just a single example.

573
00:41:02,840 --> 00:41:09,280
So you're kind of learning, you're learning and embedding that kind of maximizes the,

574
00:41:09,280 --> 00:41:12,760
the distance between your examples basically.

575
00:41:12,760 --> 00:41:20,840
Yeah. And then a meta test time or a test time, you're given five examples of, or one

576
00:41:20,840 --> 00:41:26,840
example of five different objects. So five images total. And then your goal is, well,

577
00:41:26,840 --> 00:41:31,200
put those images into your embedding function to get the embedding of each of them.

578
00:41:31,200 --> 00:41:35,800
And then you can just do, when you then get a new image image, then you compare it to

579
00:41:35,800 --> 00:41:41,160
all of those, each of those five embeddings. And the closest one is the, then you assign

580
00:41:41,160 --> 00:41:44,920
the class of that example to the new image.

581
00:41:44,920 --> 00:41:51,160
Mm-hmm. Okay. Yeah, I need to do a show just on embeddings and word to vac and all these

582
00:41:51,160 --> 00:41:56,600
things that I've been meaning to learn more about and haven't really had a chance to dig

583
00:41:56,600 --> 00:41:57,880
into yet.

584
00:41:57,880 --> 00:42:02,800
So you said there are a number of techniques and that's one of them. Are there others that

585
00:42:02,800 --> 00:42:07,960
come to mind? Yeah, I'll talk a bit about the approach that we developed for my most

586
00:42:07,960 --> 00:42:14,840
recent paper. So the method that we developed was largely inspired by fine-tuning. So in

587
00:42:14,840 --> 00:42:19,800
computer vision, if you want to get good results on whatever tasks that you're doing, typically

588
00:42:19,800 --> 00:42:25,680
you'll take a network that was trained on ImageNet, start from that network, start from

589
00:42:25,680 --> 00:42:30,000
the weights of that network, and then fine-tune it on the tasks that you care about.

590
00:42:30,000 --> 00:42:35,400
Mm-hmm. Okay, transfer learning, right? Yeah, exactly. But if you try to do this directly

591
00:42:35,400 --> 00:42:41,800
for one shot learning, where you only fine-tune it on one example is going to overfit a lot.

592
00:42:41,800 --> 00:42:47,040
So it works well for transfer learning with a sufficiently large training set for your,

593
00:42:47,040 --> 00:42:51,040
for your, the tasks that you care about, where sufficiently large isn't as big as ImageNet,

594
00:42:51,040 --> 00:42:59,360
but it's a reasonable size. And so the approach that we take is to actually optimize for a set

595
00:42:59,360 --> 00:43:05,520
of features like ImageNet, such that when you fine-tune on a small number of examples,

596
00:43:05,520 --> 00:43:10,880
you get good performance, good generalization on that task. Okay, and how does that work?

597
00:43:10,880 --> 00:43:15,000
So you can write down this objective. It has a gradient in it that comes from the fine-tuning

598
00:43:15,000 --> 00:43:19,960
procedure. What the objective looks like is essentially you have your, you have your

599
00:43:19,960 --> 00:43:25,360
original weight vector, the features that you're trying to learn. And the updated feature

600
00:43:25,360 --> 00:43:30,640
vector, which is just the fine-tune version of that, and you're trying to minimize the

601
00:43:30,640 --> 00:43:37,120
loss of that updated feature vector with respect to your original parameter vector.

602
00:43:37,120 --> 00:43:42,840
Mm-hmm. I say on home, but I think the, the limits of not being able to have a white

603
00:43:42,840 --> 00:43:47,440
point in front of me is just, you know, we've reached that point. But what I kind of heard

604
00:43:47,440 --> 00:43:55,160
in there was, yeah, if you think to linear regression, right, you are using these gradients

605
00:43:55,160 --> 00:44:02,560
to try to get you to some kind of optimum and you are iterating over or descending, you

606
00:44:02,560 --> 00:44:08,760
know, these gradients, those gradient descent. And what I heard was, you are kind of tweaking

607
00:44:08,760 --> 00:44:16,440
the way you are descending the gradient so as to do something. Is there a way to finish

608
00:44:16,440 --> 00:44:21,560
that sentence? Yeah. So you're, you're tweaking your loss function such that you're optimizing

609
00:44:21,560 --> 00:44:28,000
for their performance after a gradient descent update on that task. And you do this for

610
00:44:28,000 --> 00:44:32,960
a wide variety of tasks. So I guess the, the just is to train for a parameter vector that

611
00:44:32,960 --> 00:44:39,320
can be very quickly adopted for a wide variety of tasks. And we can do this with, just

612
00:44:39,320 --> 00:44:46,240
with gradient based methods, just essentially just with SGD. Okay. So you're, you're basically

613
00:44:46,240 --> 00:44:51,880
changing your loss function so that I mean, it's kind of like a technique, like dropout

614
00:44:51,880 --> 00:44:56,200
and some of these other things where you're doing, you know, funky things to your loss

615
00:44:56,200 --> 00:45:05,200
function to be more impervious to overfitting. Kind of, we're actually optimizing. That, that

616
00:45:05,200 --> 00:45:12,200
kind of was like at the 300 million thousand foot, you know, level. Yeah. You can kind

617
00:45:12,200 --> 00:45:16,960
of see as optimizing for good generalization. Right. Right. And one of the nice things

618
00:45:16,960 --> 00:45:20,680
about this approach is that, well, it sounds kind of complicated, but we actually write

619
00:45:20,680 --> 00:45:27,280
it down. It's incredibly simple. I implemented it in less than a day. And you could apply it

620
00:45:27,280 --> 00:45:32,960
to a few shot classification, like I talked about before and get really good results. But

621
00:45:32,960 --> 00:45:38,040
you could also apply it to a wide range of other few shot learning problems, including

622
00:45:38,040 --> 00:45:44,280
few shot learning of behavior. Okay. So I think I'm just going to take this as a challenge

623
00:45:44,280 --> 00:45:50,640
to myself and anyone else who's listening that wants to dig into this to actually get

624
00:45:50,640 --> 00:45:57,120
your paper and go through it. And, you know, perhaps we'll reconvene after I've done

625
00:45:57,120 --> 00:46:02,000
that and see if I am able to have a coherent conversation about what we're discussing

626
00:46:02,000 --> 00:46:07,160
here. And so in order to facilitate that, what is the name of the paper that you're describing?

627
00:46:07,160 --> 00:46:13,400
Yeah. It's called model agnostic meta learning. Okay. Yeah. Those are the first four words

628
00:46:13,400 --> 00:46:19,720
of the title. Okay. Well, we'll find that paper and we'll make sure that the link is

629
00:46:19,720 --> 00:46:25,640
in the show notes. And anyone else who wants to, you know, dig into this with me, you

630
00:46:25,640 --> 00:46:29,760
know, can just drop a comment in the show notes and we'll kind of exchange notes as we

631
00:46:29,760 --> 00:46:34,720
learn us together. Yeah. I'm also thinking about writing a blog post on it at some point

632
00:46:34,720 --> 00:46:39,640
in the near future. Oh, really? Awesome. Well, if you do that, I would, you know, be happy

633
00:46:39,640 --> 00:46:45,560
to, you know, help in any way review it or ask you more dumb questions or whatever. Okay.

634
00:46:45,560 --> 00:46:51,120
Great. And so since we're talking about some of your, your research and your papers,

635
00:46:51,120 --> 00:46:58,120
any other pointers to papers that, you know, folks can dig into based on the things that

636
00:46:58,120 --> 00:47:02,760
we've talked about, you know, what are the top three, you know, papers that you'd want

637
00:47:02,760 --> 00:47:07,880
folks to take a look at to get a sense for your work? So the first one would be the one

638
00:47:07,880 --> 00:47:13,760
that I just mentioned. The second would be a paper on inverse reinforcement learning called

639
00:47:13,760 --> 00:47:21,400
guided cost learning. Okay. The third, let's see, it depends on how much, how much reading

640
00:47:21,400 --> 00:47:28,360
they want to do. I think that all refer them to the paper that starts with a deep visual

641
00:47:28,360 --> 00:47:34,120
foresight. Okay. That sounds compelling. So that one is, we didn't talk about that much,

642
00:47:34,120 --> 00:47:39,600
but essentially that one's trying to learn a predictive model of video, being able to

643
00:47:39,600 --> 00:47:45,600
predict the future video given the actions that the robot's going to take. Okay. Okay.

644
00:47:45,600 --> 00:47:53,480
Awesome. Awesome. Well, we'll have links to all of those in the show notes. Before we go,

645
00:47:53,480 --> 00:47:59,920
I got a request from one of our listeners a while ago, Shreys, who was about to embark

646
00:47:59,920 --> 00:48:08,680
on his own PhD pursuits. And he asked if we could get a PhD student on and talk about

647
00:48:08,680 --> 00:48:16,960
a little bit about their experiences as a PhD student and what are some things to keep

648
00:48:16,960 --> 00:48:25,040
in mind to be successful in pursuing research in this field. And I was wondering if you would

649
00:48:25,040 --> 00:48:31,080
maybe share some of your thoughts. You are obviously doing amazing research. You've

650
00:48:31,080 --> 00:48:38,400
got two great advisors. What are your secrets to success? Yeah. I think that it's important

651
00:48:38,400 --> 00:48:46,600
to continuously develop and learn throughout your PhD. So reading a lot of papers, especially

652
00:48:46,600 --> 00:48:52,320
these days with archive, having a tremendous number of relevant machine learning papers

653
00:48:52,320 --> 00:48:58,520
every day and working on research skills, learning from others around you. So at the

654
00:48:58,520 --> 00:49:04,000
beginning of my PhD, I started working with a postdoc very closely and I learned a lot

655
00:49:04,000 --> 00:49:10,840
from him on my first project. And the ability, like at the beginning, learn from people

656
00:49:10,840 --> 00:49:16,960
who are more seniored from you is very helpful because doing, like, there's no one perfect

657
00:49:16,960 --> 00:49:22,200
way to do research. I mean, if people, you're trying to solve problems that people haven't

658
00:49:22,200 --> 00:49:29,640
solved before. And that's hard. And the way to approach that is, is different for everyone,

659
00:49:29,640 --> 00:49:32,720
but I think that there's a lot to be learned from people who have been at it for a few

660
00:49:32,720 --> 00:49:39,160
years or more. So I think that learning from others and being open to that is very important.

661
00:49:39,160 --> 00:49:43,040
I've also developed my writing skills a lot in graduate school, depending on where you

662
00:49:43,040 --> 00:49:47,680
want to go. That can be very important, especially if you want to go into academia or if you

663
00:49:47,680 --> 00:49:54,000
want to continue publishing. But let's see. And then I think that work ethic is important,

664
00:49:54,000 --> 00:49:59,800
trying to keep up with the field these days and trying to actually get things to work takes

665
00:49:59,800 --> 00:50:05,040
a lot of work by nature. Research is, is trying to tackle on solve problems. And if those

666
00:50:05,040 --> 00:50:11,000
problems were easy to solve, then they wouldn't be on solve problems. Right. Right. So in other

667
00:50:11,000 --> 00:50:18,640
words, no shortcuts. Yeah. How do you keep up with archive and your paper reading list?

668
00:50:18,640 --> 00:50:25,040
I don't think I necessarily have a good solution, but typically what I do is check archive

669
00:50:25,040 --> 00:50:31,320
every day or every other day and see if there's any relevant papers. I don't, I will read

670
00:50:31,320 --> 00:50:37,440
papers to a varying degree based on how relevant it seems and how good the paper seems. And

671
00:50:37,440 --> 00:50:43,600
I also don't worry too much about missing papers because if it is a really good paper,

672
00:50:43,600 --> 00:50:49,680
then it will rise up through conferences, through publications. I'll see them at different

673
00:50:49,680 --> 00:50:58,800
publication venues or they'll become popular or common knowledge. Right. And so is it a

674
00:50:58,800 --> 00:51:03,920
worthwhile question to ask like how many papers do you read a, you know, day, week month

675
00:51:03,920 --> 00:51:11,640
or what have you or yeah, I read a paper end to end very infrequently. I guess the papers

676
00:51:11,640 --> 00:51:16,840
that I have to review for conferences, I will read end to end. And I will only read

677
00:51:16,840 --> 00:51:26,840
papers, other papers end to end if they are very relevant or if I volunteer to present

678
00:51:26,840 --> 00:51:32,680
it at a group meeting, we have, we have reading group meetings fairly regularly. So that's

679
00:51:32,680 --> 00:51:37,600
another good way to keep up with, keep up with a lot of new papers is find a group and

680
00:51:37,600 --> 00:51:41,640
have someone volunteer to present a paper or kind of present the key findings from a

681
00:51:41,640 --> 00:51:47,040
paper in that group. So then not everyone has to read everyone so we can just read it

682
00:51:47,040 --> 00:51:52,360
and then summarize the points that are relevant to that group. And is there for papers that

683
00:51:52,360 --> 00:51:59,240
are directly relevant to your research, is there a level beyond reading end to end where,

684
00:51:59,240 --> 00:52:05,040
you know, you're actually kind of digging into the math and trying to figure out, you know,

685
00:52:05,040 --> 00:52:09,160
what was some maybe missing steps that were glossed over in the paper or you're implementing

686
00:52:09,160 --> 00:52:14,480
them or things like that? Or do you do that, you know, fairly infrequently as well?

687
00:52:14,480 --> 00:52:20,520
Yeah, so if a paper is along the lines of what I'm working on, then typically I'll want

688
00:52:20,520 --> 00:52:25,880
to compare to that paper or compare to some version of that paper. And so that will

689
00:52:25,880 --> 00:52:31,160
involve grabbing an open source implementation or emailing the author or re-implementing

690
00:52:31,160 --> 00:52:36,240
it myself. And yeah, also thinking about the shortcomings of the paper is important.

691
00:52:36,240 --> 00:52:44,480
So one example is that I'm currently working on a certain application of metal learning

692
00:52:44,480 --> 00:52:49,400
and there was a new paper that came out on a very similar topic and one of the shortcomings

693
00:52:49,400 --> 00:52:55,000
was that the data set required for metal learning was huge. And so one of the benefits

694
00:52:55,000 --> 00:52:58,720
of metal learning is that at test time you can learn from a very small amount of data.

695
00:52:58,720 --> 00:53:05,520
But if you need a ton of data to learn that few shot learner, then it's not going to

696
00:53:05,520 --> 00:53:12,200
be feasible for applying to real robotic systems. And so that's, so that shortcoming is something

697
00:53:12,200 --> 00:53:17,520
that I'm one aware of when I try to, when I'm working on this problem and to something

698
00:53:17,520 --> 00:53:22,240
that I want to address concretely in the work that I do.

699
00:53:22,240 --> 00:53:31,080
Got it. Got it. I'm wondering out loud now, but I am intrigued by the thought of doing

700
00:53:31,080 --> 00:53:37,400
like a virtual paper reading group, like something along the lines of a podcast or an extension

701
00:53:37,400 --> 00:53:41,600
of the podcast. And I wonder if there are any readers or listeners rather that would,

702
00:53:41,600 --> 00:53:46,040
I guess, also be readers would be interested in something like that. So if you are, you

703
00:53:46,040 --> 00:53:50,600
know, shout out in the comments or Twitter or something like that and people are interested

704
00:53:50,600 --> 00:53:55,560
maybe we can find a way to do something. But I guess with that, Chelsea, you have been

705
00:53:55,560 --> 00:54:02,040
very gracious with your time. And I really appreciate you jumping on the, the skypline

706
00:54:02,040 --> 00:54:08,400
here. It's been a really interesting conversation. And I have definitely learned a ton. And, you

707
00:54:08,400 --> 00:54:14,080
know, my brain exploded a little bit, which is also a good sign like, and yeah, just,

708
00:54:14,080 --> 00:54:18,200
you know, thank you. Thanks so much. Yeah. Happy. Awesome. All right. Bye-bye.

709
00:54:18,200 --> 00:54:26,200
All right, everyone. That's our show for today. Thanks so much for listening and for your

710
00:54:26,200 --> 00:54:32,040
continued support, comments, and feedback. We really, really appreciate hearing from you

711
00:54:32,040 --> 00:54:38,040
and we love to incorporate your ideas into the show. I'd also like to thank our sponsor,

712
00:54:38,040 --> 00:54:45,520
Banzai once again. Be sure to check out what they're up to at Banz.ai. And one last reminder,

713
00:54:45,520 --> 00:54:51,160
next week, I'm at the O'Reilly AI Conference in New York City. You can still register using

714
00:54:51,160 --> 00:55:00,040
our discount code, PC Twimble, PC-TW-I-M-L, for 20% off. And if you live in New York or

715
00:55:00,040 --> 00:55:06,760
will be at the event, let's plan to meet up. I'm partnering with the NYAI Meetup to host

716
00:55:06,760 --> 00:55:12,960
a happy hour on Thursday evening after the event. If you'd like more details, please sign

717
00:55:12,960 --> 00:55:20,040
up using the form at twimbleai.com slash NY Meetup and we'll keep you posted. The notes

718
00:55:20,040 --> 00:55:27,080
for this episode can be found at twimbleai.com slash talk slash 29. For more information on

719
00:55:27,080 --> 00:55:34,560
industrial AI, my report on the topic or the industrial AI podcast series, visit twimbleai.com

720
00:55:34,560 --> 00:55:40,400
slash industrial AI. As always, remember to post your favorite quote or takeaway from

721
00:55:40,400 --> 00:55:45,160
this episode and we'll send you a laptop sticker. You can post them as comments to the

722
00:55:45,160 --> 00:55:51,840
show notes page, via Twitter, at twimbleai or via our Facebook page. Thanks again for

723
00:55:51,840 --> 00:56:19,080
listening and catch you next time.

