Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Today we're joined by JJ Espinoza, former director of data
science at 20th Century Fox. In this talk, we start out with the discussion of JJ's transition
from econometrician to data scientist and then dig into his and his team's experience
building and deploying a content recommendation system from the ground up.
In our conversation, we explore the design of a couple of key components of their system,
the first of which processes movie scripts to make recommendations about which movies
the studio should make and the second process is trailers to determine which should be
recommended to users. We discussed the challenges they've encountered fielding this systems,
some of the tools that were used along the way and a few of the upcoming projects that
could be layered on top of the platform that they've built and now on to the show.
Next week in machine learning and AI, thank you Sam for having me absolutely. So why don't we get started by having you share a little bit about your background, how did you get started working in data science and in particular, how did you get started working in data science and the context of a movie studio.
Yes, I mean, it's interesting. My background is in economics and mathematics. I got a bachelor's degrees from UCLA and then went on to earn a master's degree in econometrics, focusing on statistics,
a little over a decade ago, and originally I started working in economic consulting and in Los Angeles, the biggest industry that's here and some of the largest headquarters that are here are in entertainment.
So I started working in consumer research and building econometric models for the Walt Disney company for marketing ROI and science.
So I started working at the Walt Disney company then went on to NBC Universal, I was director of this analytics and now at 20th century Fox, I was brought on by one of my former bosses at Disney to work on data science.
So it's been interesting. I've worked in different aspects originally started out with the marketing piece. It was still focused on marketing mainly and it's evolved over time.
Now I talked to a lot of folks to come into data science from different fields, physics is often cited. I don't know, I don't think I've talked a ton of economists and econometracists if that's the right word.
How's the transition been from econometrics to data science?
Yeah, great question. So it's official. It's pronounced econometrician econometrician. So I would say and for any of the listeners out there, if you really want to see a very thorough comparison and and also accessible comparison, I would look at Google's chief economist's name is Halvarian.
He has a PowerPoint out there that basically describes the intersection between and the differences and what machine learning can learn from economics and vice versa. So economics is mainly focused on establishing causal inferences and highly explainable policy, relevant recommendations essentially.
So a lot of the techniques are regression analysis, linear regression analysis, logistic regression. There are some things that kind of look like neural networks, for example, instrumental variable regressions where you have some regressions connected to others, so some inputs feeding other inputs.
And in time series analysis, obviously, there's a lot of overlap with the econometric methods that are used. A lot of things like Arima look very similar to recurrent neural networks and things like that. So my interest in economics is more from the policy side.
And one of the original applications in terms of a business of economics and econometrics is actually marketing mix modeling, which is the, you know, how economists would allocate budgets, but now in the digital world that the data has become much more granular and much more detailed, the predictive power of machine learning artificial intelligence really is a good skill set. I would say to have for for economists to focus more on the predictive side of the analysis.
You mentioned how variant I haven't been keeping up with his work, but I came across some of his work quite a while ago. He's he kind of the, you know, the father of the whole idea of a network effect, if you will.
Yeah, he's definitely had he definitely had some some discussion around that and that that is that is a portion of of an economic analysis, and it's big in the internet now. So economists have been analyzing network effects since probably since the telephone, you know, the usefulness of networks with network product. So he's worked on that and he's he worked on search a lot at Google.
And so you are working on data science at 20th century Fox and tell us a little bit about the types of projects that you work on there.
Yes, so we are focusing mainly on recommendation systems. I was the second data scientist at the movie studio, my far bus at Disney was the first.
So initially the work involved collecting a lot of consumer data as you know, many people are not aware, but movie studios are not technically allowed to own movie theaters. That's from a Supreme Court case in 1948. So a lot of the consumer data that we get from ticket sales. We have to purchase.
So our two main projects involve recommendation systems that look at movie scripts and one that looks at movie trailers and the first one that started was the movie script movie trailer data.
And it's they're both coming at the same problem bit in different ways and there's a lot of objectives around these projects, but it starts from, you know, helping people decide, should we make this movie or not.
Down to who should see this trailer, you know, based on what they've seen before in the type of content and narratives and story that tend to appeal to them. Okay.
And so you you're you're building these recommendation systems and it turns out that you're also pretty active in publishing papers.
Several papers up on archive as well as some blog posts that you've published about these these types of systems, you know, typically think of a movie studio as a big publisher of research, you know, tell me a little bit about the motivations there.
Yeah, definitely. So one of the things because this is these two products are incredibly innovative. No other studio really has systems like these at the level that we have.
We figured it'd be good to share some of the basic scientific knowledge and outcomes from our models in these research papers.
In a way that could be accessible for the broader industry because as you may be aware, there's a lot of changes in our industry that are happening from technology.
And it's it's good just to have that out there, you know, at a fundamental level without giving away, you know, too much of the secret sauce for the studios to learn from because I think there's there's a shift happening and it's good if we all kind of had some some of this knowledge out there.
Well, maybe let's jump in a little bit deeper into what you're working on. You mentioned that the data that you're building these recommendation systems comes from third parties.
How do you kind of design a pipeline for, you know, getting all this data in and processing it.
So yes, our pipeline is is was designed on Google Cloud technology and what we we used several different tools. One of the ones we use is airflow to essentially orchestrate the data in a lot of the information that we have are related to mobile phone user IDs.
So there is some encryption that needs to happen and some of the third parties what they do is they encrypt the mobile phone IDs and help us match all of these different data sets and then it comes into our database system and then it goes through multiple layers of processes because we have information both about movie trailers.
We have information on the users. We also have to collect videos online videos and extract images from from the videos and create embeddings from the movie scripts.
So all these things are orchestrated on a daily and weekly basis through airflow.
We do have some things that are running in in cron jobs and things like that. Again, like I said, this is this was something that was done probably in nine months. So it's fairly automated given the time crunch and we're fairly proud of what it's able to do.
In the end, the report generation uses Google data studio.
So once the jobs are kicked off and the data is processed through the system, the reports are generated data studio, the cloud storage locations that we use to believe is we use Google cloud storage and we use big query for some of the reports and storing and all of that is housed automatically some of the data that we have is coming from AWS.
So that needs to be orchestrated 20th century Fox is a AWS shop. So some some of the data comes through there and you know we need to build some some jobs to ingest that in out of curiosity as an AWS shop what motivated you to build the data pipeline for these projects on the Google platform.
Yeah, so so the main reason was, you know, we had we had two people and we, you know, in the beginning, myself, my boss and you know, that's up to team of five.
And from a learning curve perspective, Google cloud doesn't have a steep of a learning curve as AWS does. It's much easier to set up some of these systems now having said that.
It isn't as configurable as AWS and there are some, you know, they don't have all the products and services, but because we wanted to get up, get something up running fairly quickly.
Google cloud just streamlined, they do a lot of the back engineering on their end. So it just felt like a quicker thing to do. So one reason was the efficiency of getting some products out.
And two was, since we're working in marketing data, a lot of the DCM logs, which is called basically double click. These are the YouTube ads and search ads and all the things that are, you know, that the major companies, you know, especially 20th century Fox used to analyze and collect their marketing data.
It's already connected to Google. So a lot of the pipelines to ingest that marketing data were right there. So we were able to, you know, build efficiency that way too.
Earlier you mentioned the scripts and the videos and I, when you first mentioned that I envision these as totally separate systems, but it sounds like you're bringing them into the same pipeline.
What's the relationship between the scripts and the videos and the goals that you're that you've ultimately have for this recommendation system?
Definitely. So you can imagine the final, the final stage or the final data set basically being a merge of customer data embeddings from a deep learning model of the video content, which is essentially a numerical representation of what the videos contain.
As well as embedding embeddings of the movie script and then we also have a lot of other metadata. So think of one customer saying, hey, this one customer saw this movie, this movie had this budget had these actors, by the way, it also has these and this is what the text and the video of the marketing that they saw looks like numerically.
And then just having millions and millions of rows of that customer data and understanding what movies they saw, what was advertised to them. So that's why we're really combining the computer vision, natural language processing because all of the trailers, one of the most important pieces of marketing for movie studio.
It doesn't have all the information needed, a script is pretty useful as well. It essentially outlines the entire movie line by line and gives us an additional layer of information.
In both of these cases, a script as well as a trailer, a script is kind of a representation of a couple of hours worth of movie, a trailer is a minute and a half, I suppose, of video content.
Are you creating a single static embedding space for an entire script and an entire trailer, or are you in the case of a trailer like going frame by frame or something like that, or creating a sequence of embedding spaces.
Yeah, we're using sequences, especially for the trailer analysis. So some of the corner wrote uses open CV to and essentially what we do is we download all the trailers, all the trailers that come out into into our database, and we extract one frame per second from that trailer.
And then we use Google's YouTube 8M model, which is a pre-trained model that you could find on GitHub that essentially is modeled off of YouTube, which is perfect because we're using YouTube data.
So then we feed it image by image into the pre-trained YouTube 8M model, and it creates an embedding layer back to us. So you can think of a trailer as, you know, it'll have a row every second, imagine a CSV file, imagine a row or JSON, whatever, you know, one row per second that contains an embedding.
So that's been really useful because the latest research paper that we've worked on really looked at the temporal aspects of the trailer because marketing is especially providing YouTube ads is really important to kind of get people's attention right in the beginning and seeing them through.
So understanding whether a car chase or romantic kiss in the first five or 10 seconds captured people, when do people drop off, it was really important to, you know, have that frame by frame analysis and in our computer vision model.
Now, for the natural language processing, that one is one embedding for the entire script. We found that we tried different things and we found that just at the level of the script, it's hard to really predict consumer behavior because you don't have actors, you don't have directors, you really don't have much.
But when you have the trailer, you have something a little bit more tangible and that that increased to predictive power. So we really focused on making the image processing much more granular.
I've heard anecdotes that with some of the streaming services, they will create, I don't know if they go as far as creating trailers on demand based on what they think you like, but certainly they have multiple versions of the given videos cover graphics, for example, and they'll kind of swap those out depending on what they think your preferences are.
Is that kind of personalization, what you're ultimately trying to drive for?
Yeah, we're doing some of that. Right now, the recommendations are being served to executives and creative people to help in, for example, modifying the trailer.
One of our, one of our, one of our one one good success and insights that we had actually impacted the commercial for Red Sparrow, they came out on the Super Bowl. We saw that certain content, certain aspects of the content.
We're resonating with certain people in our database and that went up to the president of the studio through a report and essentially that the actual content that was played on the Super Bowl was different.
So that was our first initial look into, you know, changing some of the content based on, on these analysis, but we also have the ability for executives and writers to modify what we call the synopsis of the script live.
So we have a very, you know, we have a Google, Google sheet up that has all of the synopsis.
It's our quick and dirty UI and essentially what people can do is when they're thinking about how to position a movie or advertise a movie, the synopsis is important because it's a summary.
So they're able to essentially emphasize different parts of the movie, even before it can be made and emphasize either the romantic aspects in these synopsis and we have jobs that run every week and essentially tell people, hey, we ran the analysis on your latest synopsis.
And here are, you know, the attendance probability for different people, different audiences based on how you change it.
Here are other comparable movies, here are geographic level insights based on the changes to the script or synopsis that you made.
And that's really interesting for them because then they get to see, they get to test different hypotheses.
So a lot of this is, right now there isn't like a product, for example, there isn't, there isn't a user interface. Our users are the executives who can play around with synopsis and trailers and then feed them into our system and see what the, how the audience changes based on their modifications.
You've got all these features from the images, from the trailers, you've got the embeddings from the scripts and you've got all this information about the user, as well as it sounds like metadata about the films.
Do you kind of concatenate all of this and build a model based on all of this concatenated information or is there some more sophisticated way that you're combining all of this information?
Yeah, definitely. So the combined, we've tried different ways of combining the information in the model at first.
We try to widen deep model because a lot of the audio and video features essentially benefited from deep learning, but a lot of the other film specific and user specific data would benefit from something a little bit more linear.
But we didn't see that much lift into the analysis from doing that, so we're essentially taking movie vectors and user vectors or characteristics and building a pretty deep neural network to extract the analysis.
But the script analysis, a lot of that is done through word to vac. So word to vac is used as part of the embedding process. And as I mentioned, the YouTube 8M project is taking the frame by frame features.
And that's how we're really able to get, you know, 75 80% accuracy on what people will watch.
And you've mentioned YouTube 8M a couple of times. What specifically is it doing? Great. So YouTube 8M is a project that you could find on GitHub and essentially what what YouTube did was it build a deep learning neural network that can classify its videos on YouTube.
So part of that project on GitHub essentially is it has parts of the trained model that you could download and essentially point to other videos and images to create an embedding of what basically a representation of the video.
And that worked perfect for us. Normally we build, we build the embedding from training the model, but because it's, you know, it's YouTube's classification system for the videos and we're using YouTube videos mainly trailers.
It worked well and it helped us get things to speed to market.
So let me make sure I understand this. They've got a data set of videos. How do you use that in conjunction with with your sequences to create an embedding or you creating the embedding just on their data set and then using that embedding in the context of your your images.
Yeah, no, we're essentially taking the last portion of their neural network. So the we're using their model essentially. So we download their model and then we take the last layer of the deep neural network before the classification portion, which is just basically a series of numbers from the neurons.
And then that becomes like the numerical inputs for that image or and frame or video into our inputs. They have, they have a folder called feature extractor and they essentially, you know, they called it the inception model.
You can basically download it's 25 megabytes. It's based off an array. It does things, you know, like PC a matrix analysis that uses tensor flow all of that essentially so that you can basically extract tensor flow records from videos and images.
And that's that was a simple way for us to convert the video data into numerical analysis that our deep learning models could could train off of.
Okay, got it. So they offer this YouTube 8M as a data set, but that sounds like they also offer a model that you can fine tune with your own data, which is what you did.
That's correct. They have a data set and then they have a model that they built on top of it and and the code is available on on GitHub.
Okay, great. And so you know, when you kind of think back to building this data pipeline and building the model, what were some of the biggest challenges that you had to overcome.
Right. So in this, some of the biggest challenges were I, this was my first time actually working at scale in the cloud.
I did work a little bit on AWS, but prior to this, most of my experience at previous entertainment companies and economic consulting involved just physical hardware servers.
So I play around a bit query a bit. I play around with some of these different tools, but just having to do it for the first time was fairly interesting learning airflow.
I didn't really know air flow at the time. We hired several data engineer software engineers and machine learning experts, some of them, which are our mentioned in the paper, you know, like Matt Nickens and Avitalion, as well as Andy and Miguel Campo.
And he was who was actually actually P who had these engineering know how so once they came on board onto the team, they were able to do things a little bit more, I would say a little bit smoother because they had experience with it.
So for the first couple of months, it was just kind of getting things up and running, understanding how things worked, getting the Python client applications and also security was a big one.
The good thing about Google Cloud is that the security is is fairly abstracted away, which is that another reason why it was good for someone with less cloud experience to to work on it. So that's in general, some of the challenges.
So, you know, some of the more specific challenges were, for example, you know, some legal challenges in terms of just like being able to download the trailers from YouTube, you know, installing things like open CV to, if you've never done it, I've spoken, we had training at Google in residents for a month.
And I spoke with several computer scientists there and people who were training with them, they said it's a pretty difficult package to install. If you don't know what you're doing, there's several different steps. So that was challenging. So even once it was installed, just being able to have it run on other computers required quite quite a bit of work.
We have some executable shell scripts now that can, you know, basically install all the software we need on any virtual machine or data block data proc cluster, which is like an EMR, you know, instance in AWS, but that part took a week.
And you know, you're talking about taking about a week to install import, you know, CV to, and that was at Google headquarters. That was with people that had master's degrees and computer science on the team, you know, so, and me being kind of a novice to the date engineering part of it.
It was, it was, it was a real challenge, but, you know, in about a week, we got it up and running. I actually had someone come up to me at the training at Google and basically say, wow, you got open CV running. We tried it for two weeks at my company and we quit.
So along the lines of that story, I'll actually tell you, once I had open CV up and running.
I did a fatal mistake, which was I, I pressed like remove everything in Linux. I think it's like Rm star.
Oh, gosh.
Yeah.
So the entire computer is like, so I had to basically under, you know, reverse, basically recollect the packages and the data and compilable C code.
So I remember spending one night just, you know, learning like some sort of forensic Linux methodologies to extract data. So the key is if Python normally doesn't compile, but if it compiles and something is deleted, you can, you can extract some of that in Linux.
It's a long story. It's a long story. So it took about a week to get it set up. It was then I deleted accidentally. I felt like hiding in a hole.
Then I learned some forensic Linux techniques to extract some of the parts of the code that were compiled and see when you open that open the Linux terminal.
Like it was getting eaten away, you know, as things get saved and saved on to disk and resaved on to disk, but they were still there.
So luckily, it took me just an extra day to set that up. So that was the challenge.
That sounds like a lesson you will never forget.
I never will forget that.
How do you measure the performance of these, this system?
Right. So we want, we have several dashboards that are up and what we want to do is we basically want to take.
First of all, we have during training, we have a holdout data set as any good data scientists would have.
We have dashboards that are published internally to our executives. Basically, it's a scorecard that tells you, hey, here's what the model predicted and here's what actually is happening.
That's really at the tail end of the modeling process. While the model is training, we have automated systems that keep track of precision, recall accuracy.
And save that inside of a dashboard inside of data studio. So after the model gets retrained, we can see, you know, if the accuracy has changed and we're using cloud ML engine and Google Cloud, which is great because it automatically creates versions of previous models.
So one thing we could do is if we see something's off, we can compare current model that has a decline or an increase in accuracy to previous models that we've had.
All of that is basically systematized and engineered into the process. And before model is actually published into what we call production, which is essentially our ability to push these dashboards up to the organization.
And that analysis is ran. We have automated emails as well, you know, generate through Python that look through the logs of a lot of these TensorFlow cloud ML jobs and parse through the accuracy, parse the things to basically alert us when something is up so that we don't always have to go check the dashboards all the time.
It's still good practice. We were trained once a week. So it's still good practice. We go in and check everything before before something something goes off. And luckily, it's been fairly steady.
You know, this being a model that you're not pushing in some kind of live production environment, but rather your mainly generating reports that these executives can use to make decisions.
How tight is the feedback loop between I would imagine it can be quite long if someone's making a decision that results in a new trailer being created.
And then you have to push that trailer out and then collect some data on how it's impacting users.
Does that create challenges for you and tying the, you know, and just being certain about causality?
No, yeah, it definitely does. And it's one of the reasons why we're looking to activate, meaning we are looking to take these insights and immediately start buying ad campaigns through Google that's a project we're working on now.
Because we've seen that the models people tend to trust, you know, some of the output of the models, other parts they have questions about. So we still have to talk through it.
But I would say the turnaround time for feedback and evaluation and pivoting it's about a week.
And week, it could be faster, but most of the time the way that movie trailers work is that come out on, you know, they'll come out during the week near the end of the week.
People watch them over the weekend. They get served these ads and people's viewing behavior on YouTube is fairly predictable.
But yeah, to your point, we'd love it if, you know, this project that we're working on, we call it activation because that's what they call it in ad tech, which is essentially taking these insights from the model and taking actual users IDs.
And pushing the YouTube ads directly to them as opposed to, hey, look, here's kind of what's working. Here's what isn't working.
You know, normally what we do is we contact our media agency, which is an agency that purchases ads for us and give them some of these insights as well.
But our goal is to move towards more of a programmatic ad buying strategy with these insights.
Reflection on kind of the modeling part of the process itself, are there ways that you're hoping to evolve the models and the way that you're building the models you mentioned, kind of doing more of this time series aspect of it.
Are there other other thoughts that you're pursuing?
Yeah, definitely. So images are fairly important. But one of the great things that that that we have access to is also audio and audio, especially in ads, some people have the audio turned off.
A lot of the auto play video features on, you know, social media sites may have the audio turned off.
Or, you know, a lot of times it's on. So in addition to the audio, we're looking into, sorry, in addition to the video, we're looking into doing these types of analysis for the audio of movies.
And I think that I'll add another dimension. We're essentially trying to, you know, get the written word and get what people see and help understand what people, what people hear when they're seeing a movie.
To kind of help that can kind of underscore things like music. And we had about a one week meeting with Google's magenta team and they specialize in artificial intelligence.
And they gave us some insights into how potentially we can leverage the sound that's coming into here. So that's one, that's one area of research that we're actively pursuing.
The second one that we looked at earlier in the year, but I found a bit of difficulty is in trying to combine, for example, two different scripts or two different movies.
Generating text is difficult. You know, you can find similarity metrics between two pieces of text. But, you know, what if you could combine predator and Deadpool and have it write a script for you? I mean, it's, I mean, it's something, I don't know what that would look like.
What does that even mean?
You know what I mean? So that's the thing like mathematically the space and dimensions where these embeddings live. Once you, it's hard to translate it back out to an actual readable text and format.
But it's something that that we've looked at. So we have, we have a lot of different ideas towards the end of the year, we've been just mainly systematizing and automating a lot of the processes that we have.
So as you, as some of you, some of you guys may be aware, we're being acquired by the Walt Disney company. I think they'll have some interest in this technology.
So we've mainly been doing is, you know, showing up the security, showing up the how automated this is documenting everything.
And, you know, keeping some, some of these ideas on hold for a bit until the Q one of this year.
And then that way we'll, we'll explore and pitch some of these other, some of these other features that could be added to the modeling data sets and different types of predictive algorithms that we can use to enhance our current work.
Fantastic. Well, I refer to this earlier, but your team is published some blog posts on this on the Google Cloud blog, as well as several papers detailing different aspects of this ongoing project.
And we'll be sure to link to those in the show notes. But I just wanted to thank you for taking the time to walk through some of this with us. It's really interesting work.
Thank you, Sam. It was, it was great chatting with you. Thanks, JJ.
All right, everyone. That's our show for today for more information on JJ or any of the topics covered in this episode.
Visit twimmelai.com slash talk slash 220. If this talk peaked your interest, you should also check out twimmeltalk201 where Lee Maynassri of Comcast breaks down how she and her team led the rebuild of the Comcast Xfinity X1 recommender platform.
As always, thanks so much for listening and catch you next time.
