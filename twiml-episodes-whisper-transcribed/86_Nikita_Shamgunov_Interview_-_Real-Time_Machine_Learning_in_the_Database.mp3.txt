Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're featuring a series of conversations from the AWS re-invent
conference in Las Vegas.
I had a great time at this event, getting caught up on the new machine learning and AI products
and services offered by AWS and its partners.
If you missed the news coming out of re-invent and want to know more about what one of the
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble
Talk number 83, which was a roundtable discussion I held with Dave McCrory and Lawrence
Chung.
We cover all of AWS's most important news, including the new SageMaker, DeepLens, Recognition
Video, Transcription Service, Alexa for Business, Greengrass ML inference and more.
In this episode, I'll be speaking with Nikita Shamganov, co-founder and CEO of MemSQL,
a company offering a distributed memory optimized data warehouse of the same name.
Nikita and I take a deep dive into some of the features of their recently released 6.0
version, which supports built-in vector operations to enable machine learning use cases like
real-time image recognition, visual search and predictive analytics for IoT.
We also discuss how to architect enterprise machine learning solutions around the data
warehouse by including components like data lakes and spark.
Finally, we touch on some of the performance advantages that MemSQL is seen by implementing
vector operations using Intel's latest AVX2 and AVX512 instruction sets.
And speaking of Intel, I'd like to thank our good friends over at Intel Nirvana for
their sponsorship of this podcast and our reinvent series.
One of the big announcements from reinvent this year was the release of Amazon DeepLens,
a fully programmable deep learning enabled wireless video camera designed to help developers
learn an experiment with AI in the cloud and at the edge.
DeepLens is powered by an Intel Atom X5 processor, which delivers up to 100 gigaflops of processing
power to onboard applications.
To learn more about DeepLens and the other interesting things Intel has been up to in
the AI space, make sure to check out intelnervana.com.
Okay, just a couple more quick announcements.
You may have heard me mention last time that over the weekend we hit a very exciting milestone
for the podcast.
One million listens.
What an amazing way to close out an amazing year for the show.
It occurred to us that we'd hate to miss an opportunity to show you some love.
So we're launching a listener appreciation contest to celebrate the occasion.
To enter, just tweet to us using the hashtag twimmel1mail.
Every entry gets a fly twimmel1mail sticker plus a chance to win one of 10 limited run
t-shirts commemorating the occasion.
We'll be giving away some other mystery prizes as well from the magic twimmel swag bag
so you should definitely enter.
If you're not on Twitter or want more ways to enter, visit twimmelai.com slash twimmel1mail
for the full rundown.
Last but not least, we are quickly approaching our final twimmel online meetup of the year,
which will be held on Wednesday, December 13th at 3pm Pacific time.
We'll start out by discussing the top ML and AI stories of 2017 and then for our main
presentation, Bruno Gonzalez will be discussing the paper understanding deep learning requires
rethinking generalization by Shi Yuan Zhang from MIT and Google Brain and others.
This will be a fun meetup and one you don't want to miss, so be sure to register at twimmelai.com
slash meetup if you haven't already done so.
And now on to the show.
Alright everyone, I am on the line with Nikita Shamganov.
Nikita is CEO and co-founder of MemSQL.
Nikita, welcome to this week in machine learning and AI.
Thank you.
Thank you for having me.
Absolutely.
Nikita, why don't we get started by having you tell us a little bit about your background.
You are the CEO of MemSQL, but you've got a pretty technical background, isn't that right?
That's right.
So I'm on my PhD in computer science from St. Petersburg, Russia and I moved to the stage to work
on Microsoft SQL Server, which I did for a number of years.
So I have a very strong database and query processing background.
After that, I moved over to Facebook where I was blown away by the magnitude of data
problem Facebook was solving back then.
And since they only increased in magnitude, seeing that and combining the visibility into
those workloads and kind of making that assumption that in five years, a lot more companies
are going to be facing those challenges just like Facebook.
I decided to start a company and combine that database background and expertise of building
systems and the early stages and the glimpse of the workloads that I saw at Facebook.
So I kind of knew where the world was going into.
So that triggered my desire and gave me some insights into starting MemSQL, the company.
So we've been in it for six plus years and certainly validated some of the assumptions
we had starting that journey.
So I've come across MemSQL and I think of the company as an in-memory database.
It's also a little bit about what the focus is there and in particular, what's the intersection
between what you're doing and machine learning in AI?
Are you seeing a lot of those types of workloads nowadays?
Definitely.
That's certainly where our customers are moving towards.
But let me step back for a second and talk about MemSQL and in-memory database
and technology.
We started as purely in-memory database and then since we've evolved to support a large
class of applications that I built on top of MemSQL and in-memory became an enabling
technology but it's not the technology at MemSQL.
As a matter of fact, the key advantage that MemSQL brings to the world is the fact that
it supports SQL which is a structured query language and the fact that it runs the database
in a distributed environment.
So you can run MemSQL on your laptop or you can run MemSQL on a thousand hosts that
gives you an immense compute power to enable the new class of applications.
So let me talk about what kinds of applications we support.
Maybe some of them have to do very little with AI and ML and MemSQL enables scale, latency.
You can build very low latency applications on top of MemSQL and that's where in-memory
technologies come handy.
And you also can build applications that require very high levels of concurrency.
And that concurrency is enabled on top of the system of record.
So MemSQL supports state and it supports full durable persistence.
SQL also allows you to build what we call real-time applications.
And kind of the idea of real-time applications is the opposite of batch.
Every time you need to do analytics and you do some sort of pre-calculations upfront using
head doops or data warehouses or any other offline and batch oriented technology, that's
basically our enemy.
So we're bringing the world to be completely real-time and perform all the computations
that you need to live.
And we deliver it on extremely low latency by leveraging an immense amount of compute.
And that we can do very, very well because MemSQL is a scalable technology that can run
on clusters of commodity hardware.
So now where AI and ML comes in, well, because we support this new class of applications
the fact that our customers are incredibly forward-looking, they want to do more with
the technology and they want to blend classical database workloads with the new types of
computations that are mostly stemmed from the AI and ML needs.
And one of the big ones that we see all the time is image recognition.
So we can talk a little more about it.
So let's say you have an application and you use that to power and use MemSQL to power
that application.
The application is large scale, there's a ton of data that's stored in MemSQL.
Like I said earlier in memory is an enabling technology, but it's not the technology.
You can actually put a lot more data into MemSQL than the resmemory on the cluster.
And what you want to do is you want to enable smart applications, the ones that make decisions
on either on behalf of a user or they provide recommendations or they provide some sort
of search capabilities on top of unstructured and semi-structured data.
Imagine an app that has a camera, it runs only on your cell phone, you snap a picture
with this app and just in a few tens of milliseconds this app finds similar images to the one
that you just took a picture of.
So why is it useful?
Well, it's useful because you just enabled visual search.
And the way we do it is we built some of the building blocks that allow you to run and
operationalize the machine learning models and we built them straight into the database.
And now the database allows you to scale them and deliver very low latencies for this type
of operations.
So that would be one example of an application.
I can give you another one, which we see a lot in the IoT space.
Before we go into the next example, can you drill down a little bit into what specifically
MemSQL is doing to enable the first example?
For example, are there pre-trained models for image recognition or image similarity in
this case built into the database, kind of like you might think of a stored procedure
or is it, you know, is there something else?
Is it a different type of functionality?
Yeah, it's even lower level than that.
MemSQL certainly supports stored procedures, but in this particular case, we implemented
a few building blocks, particularly dot product and Euclidean distance between vectors.
Oh, that is pretty low level.
And if you take a deep learning model and you look at the layers, you know, Matrix
multiplication, tensor multiplication, vector multiplication, is the fundamental building
block.
So what we do is, as we train that model, we take all the layers except for the very
last one and apply it on the database of images that we have and that allows us to extract
what we call, you know, or anybody else calls a feature vector, which is just a vector.
So once we have that and we have a model, applying that model to an incoming image, which
you just took a picture with your cell phone, will produce another feature vector.
And it just so happens that the multiplication of those two feature vectors normalized
gives you the similarity score, how close those images are together.
So the heavy lifting of building a model belongs to somewhere else.
And the data may might as well be still stored in them SQL and we enabling very fast data
transfer in and out of them SQL in a parallel way.
So we can send it into Spark or TensorFlow or any other training framework.
But once it gets to operationalizing, operationalizing that model and performing the last mile
computation by really powering your app, then them SQL gives you that scale.
And it allows you to perform those computations pretty much at the memory bandwidth speed
that the labels really low latencies for that last mile computation.
Okay.
Doesn't make sense?
Okay.
No, it does.
So you're computing these feature vectors.
Is that happening as is that happening on right of new images or is it happening?
I'm assuming that's the way you do it since your anti batch.
You're not doing some big batch job that's like updating, you know, some column in your
database with your feature vectors for all the images that are in there.
Correct.
And you can do, you can do either, but the typical workload is once you have that model,
you apply that model on right.
And we have technology, it's called pipelines that allows you to perform arbitrary computations
either in a store procedure or an external piece of code.
That's where you can invoke all third party libraries to apply that computation and then
the store, the feature vector in the database.
So you can do it in the ingest and if the, let's say you built something like you crawl
the web or you crawl your own product catalogs and once you identify those images, you immediately
stick them into the database as you do that, we trigger that computation.
So the feature vector arrives into the database at the same time instantly as the actual data.
And now it immediately participates in all sorts of computations.
So that allows you to never have kind of stop and go computations.
You never do, okay, step one, load all the data, step two, perform the batch computation
on top of all the data into the database and step three, do something else with it.
Rather than, it's all streamlined and it just flows in and out.
So how does this play out in the IoT case?
So in the IoT case, data is incumbent constantly.
So one of the use cases we have with a large energy company is to ingest IoT data from
drill bits.
Apparently in the world of fracking, you tend to drill a lot more and then there's a
non-trivial cost for a broken drill bit, those things are extremely expensive.
You have to stop your operation if the drill bit breaks.
So you're losing not only on the fact that you're fishing this thing out from the ground,
from hundreds and maybe even thousands of miles deep, but you also not producing oil,
which is an operational cost.
So what we do, we ingest that real time data, IoT data, and we're scoring that data,
applying a machine learning model in real time.
And then there's an application built that arrest the drill bit before it hits a problem.
Just by measuring temperature and all the very, you know, temperature, throughput, all
sorts of kind of vital signs of the drill bit, of the drill bit as it goes through the ground.
And so that was step one.
Step two is obviously you feed all this information back to direct the drilling.
So in the world of fracking, drilling is directional.
So it's not just vertical down into the ground, but it's more like you change in the direction
as you go and drilling.
So using all that input, you can direct the drill bit to make the whole operation a lot
more efficient.
And what are some of the algorithms that come into play in that use case?
So they started with, you know, like every typical data science, they started with some
sort of linear regressions that they moved it to decision trees very quickly.
And now they're experimenting with deep learning for that as well.
And the beauty of that, of the solution that we presented is it integrates natively
with all sorts of third party libraries.
So we made the experimentation for them very, very straightforward.
They started with SAS, where they produce models in SAS, as you know, SAS is a proprietary
technology, but since they've played with Spark and now experimenting with TensorFlow
as well.
Okay.
I was actually going to ask you about Spark and how that fits in with your model.
Do you?
I imagine you see it out and out working with customers, is it a competitive technology?
Spark plus the rest of the Hadoop ecosystem, or is it complementary?
Or how do you see that?
So all the big data technologies overlap a little bit.
In the case of Spark, I would say it's 9010.
So it's 10% competitive, 90% complementary.
And here is why Spark doesn't have state.
The state is usually stored somewhere else.
It's either HDFS, relational database, or S3, some sort of object store in the cloud.
So what we do in this case is we provide an extremely performant state.
And the combination of MemSQL and Spark, that's the 90% case.
They work really, really well together because we give you that transactional, scalable
state, and nothing else on the market can give you that state.
So not only you can store, you can retrieve, but you can also compute.
And we have a world class SQL query processing engine that allows you to produce reports,
but also allows you to modify that state in a transactional fashion.
You can say begin transaction, insert, update, delete.
You can run it at high concurrency, and you have full durability and all sorts of guarantees
for that data.
So that's where we're extremely complementary.
The typical deployment model is that there is a data lake, and the data lake stores hundreds
of terabytes of petabytes of data.
MemSQL is deployed alongside of the data lake to provide to power applications.
Because you cannot write applications on top of Hadoop because Hadoop is batch.
So now, and then Spark is the glue between those two, and it allows you to have rapid data
transfer, all the data that is born in MemSQL based on the applications, based on the interactions
with applications.
That data is captured.
You can pick it up and Spark very, very easily, we have a world class Spark connector, drop
it in the data lake for historical, archival, compliance type storage, and then provide
some, and then perform some overnight batch computations, take the results of those
computations, stick it into MemSQL, and Spark often times give you that unified API.
Because through that API, you can interact with MemSQL, you can interact with the data
lake, and it becomes kind of the go-to API for application developers.
And MemSQL, in this case, just gives you SQL, then you can attach a BI tool directly into
MemSQL, and they can scale the concurrency of data scientists that attach their BI tools
to MemSQL and look at the reports and visualizations.
And is it primarily data scientists and folks that are end user use of MemSQL?
Or you also, in this scenario, attaching your traditional applications to MemSQL or using
some other state technology for building applications that refer to this data?
Well, mostly it's actually application developers, because MemSQL powers applications.
And because the nature of applications is changing all the time, and we have higher scale requirements
for the applications, MemSQL is a perfect technology to power applications like this.
I'll give you a few more examples of such applications.
Now because modern applications have AI and ML requirements, that's where that intersection
comes in, where you need to have those models that you produced somewhere in your data
science lab, and you want to operationalize those models.
And that's where MemSQL plays very, very strongly.
So do you envision kind of a lot of lines of what Spark's done, kind of building higher
level abstractions beyond that product and Euclidean distance to enable folks to do machine
learning and AI more easily?
Absolutely.
Absolutely.
We have a number of ideas that are circulating inside the engineering team and certainly
influenced by our customers and what they want to do as a technology.
So the customers see in MemSQL is that very, very fast, scalable state that they can deploy
inside their data centers or in the cloud on the cheap, right?
Because MemSQL provides world-class compression, it has column store technology, and that
state is very fungible.
Because we support transactions, you can change that state, you can enrich that data with
attributes that you compute on the fly, et cetera, et cetera, et cetera.
Now what people want to do is they want to perform computations that are beyond SQL.
So we're world-class in SQL query processing, and that's great.
And our customers want to do that and more.
And when they want to do more right now, we resort to Spark, right?
We say, OK, well, deploy Spark alongside of MemSQL, pull the data out, and we'll give
you that data very quickly, perform the computation, put that data back, and then leverage some
of the building blocks that we have, you know, basic arithmetic operations, vectorized
operations, vector operations to do the last mile computation to power your applications.
Now, like once you do that, you just want to take that loop and you want to tighten
it up.
So you start bringing all the computations that you today, today you're taking data out
of MemSQL, put it somewhere else in a temporary store like Spark, data frames, and you want
to perform those computations in place.
And there are two interesting problems in that space.
One is what the API should be.
And today for machine learning and AI, the APIs tend to be either Spark driven or the Python
library ecosystem driven, you know, the likes of non-py, sci-py, pandas, TensorFlow.
So it seems like the Python world lives in one universe.
The Spark world lives in another universe.
And then there is the SQL universe that is pretty much ubiquitous because every database
exposes SQL as an API and also data warehouses expose SQL as an API.
Our view that the Spark universe and MemSQL universe should be enabled by rapid data transfer
between the two.
And then the Python universe should be enabled by pushing some of the computations that
you express through Python API into the database and putting them on steroids.
Now what exactly does that mean?
And can you give an example?
Yeah, totally.
So let's say you perform an entrepreneurial computation on your data.
Let's say you have hundreds of bytes of data, you know, certainly more data that can fit
on your laptop and your data scientists.
And as a data scientist, you stack, you live in the Python world.
And you're a big expert in the libraries like pandas, non-py and sci-py.
And let's say you're playing with TensorFlow as well.
So it's very natural for you to express computations on that data to perform training or
perform scoring on that data in that Python world.
The problem is all the computations are single threaded and all the computations are in
memory.
So you're stuck at this point in time because you cannot access hundreds of terabytes of
data because there's no way you can put hundreds of terabytes in memory.
And that's where we see the world is going to where you want those computations to become
instant and paralyze a bowl and scalable, not so you can just instead of bringing that
data into your high memory machine, you can perform those computations in place in something
that's very, very scalable, like MemSQL.
So you're going to see more and more of that happening over time.
You mentioned the three different modes of interacting with this for machine learning
or in general, Spark, Python, and SQL.
Are there any efforts to extend SQL to give it some kind of machine learning expressiveness
like SQLs got all kinds of aggregation operations like select average of x, where, whatever.
I can imagine something like select predict why, where, whatever, where you're kind of
telling expressing in SQL that you want a prediction based on something.
Does that exist or are folks playing with that today?
Yeah, so if you look at the history and you look at products like, you know, good old
products like Teradata or NETISA, or if you look at SQL server integration with R, that's
certainly something that is happening where trading is not happening in database, but scoring
is, that's a natural way to do things.
We achieve the same functionality through pipelines where we score data on ingest.
So I think this is valuable and you'll see database is exposing more and more primitives,
the likes of dot product and Euclidean distance, but you'll have instead of two, you will have
you know, a hundred of those built in into the database and then you will see packages
where it's just a package of storage procedures, which allows you to to run those predictions.
And those will work really well for simple use cases.
Now, if you look at the world of data scientists today, they actually don't like that.
They like to live in the world of data frames where there's a lot more control over the
types of computations that people are expressing.
People like to, people understand that world of data frames very, very well.
So I think the future is going to be actually in paralyzing and speeding up computations
inside data frames, so that's just my opinion.
Do you envision doing training directly in the database?
In the same way as I described the interaction with the data frames.
So if you want to enable inference in the database, then you don't want to have the database,
you don't want to have the database to be a crutch where you're constantly fighting and
trying to shoehorn that computation into SQL.
The natural way to expressing those computations is operations on top of data frames.
However, if you make the database, naturally support those data frame computations, then
you have tremendous value from the fact that the database actually owns the state.
And you never need to transfer that state from your storage to something else.
So you're bringing computations closer to data.
So that's where I see the industry is moving in the next five years.
You just mentioned bringing computations closer to data.
And that's obviously been one of the things that the Hadoop ecosystem has made more readily
accessible to folks, this idea of data locality.
Do you get to take advantage of that with machine learning types of models in general and
this pipelining approach that you have in particular, or are the pipelines run outside of separate
from any concept of locality of data?
Well, locality is a loaded concept, so you can start with, OK, well, I want to perform
computation exactly on the same machine where the data is stored.
So that's kind of the extreme case of locality.
Another way to think about is as you look at the computation plan, let's say in the database
terminology, there would be a query plan.
So let's extend the definition of a SQL query plan to a broader concept of you performing
some sort of arbitrary scalable computation.
And you know all the steps in the computation up front.
So you can optimize those computations and produce a query plan for those computations and
you will run that query in the distributed environment.
Now then you look at this and certain primitive computations, you really, really want to
perform closer to the data because you will save tremendously on the amount of IO that
is happening for in the data transfer.
Now from there, if you do that, you also deliver on the concurrency of such computations.
So now you can perform those computations at a highly concurrent environment where thousands
of data sciences are attaching to the same data that is collected and centralized and stored
in something like MCQL and then they are performing their computations concurrently.
So there are no opening up all the data to the organization and not having any data silos.
So data locality for computations is useful where it makes sense and not as useful in a
broader general purpose way.
And the perfect system would be engineered around pushing the computations closer to
the data where it makes a ton of sense. For example, you do a lot of filtering or you
performing a lot of what it's called group by operations.
And those operations make sense to be done locally.
Those operations make sense to be done in a vectorized fashion by leveraging the latest
and greatest CPU instructions in the Intel hardware and so on.
It makes sense to leverage indexes so you prune massive amounts of data so you don't
even touch them for your computations.
And then from there, it makes sense to bring all that data into a stateless distributed
environment and perform the rest of the computations.
So that approach allows you to build greater scalability for your system compared to a traditional
database compared to Hadoop or compared to Spark.
Okay. And now you just mentioned the taking advantage of the instruction sets of the underlying
hardware. And you guys are making an announcement with Intel at Amazon re-invent next week.
Is it related to that area?
Definitely. Intel has been supporting vectorized instructions for some time for over a decade.
And every new generation of CPUs increases the size of the vector that you can use and perform
vectorized operations on top of.
And where we stand right now, it's 512 bits.
So compared to matrices in a GPU, this is tiny.
However, when you perform a last mile computation like dot product, it allows you to perform that
computation much faster than the memory bandwidth that you have.
So you actually don't need more for that last mile computation than AVX 512 because your
limitation is not your compute.
Your limitation is memory bandwidth.
This is very, very different when you train models and perform computations for building
deep learning.
That's where you perform a lot of tensor multiplication.
And they mod of compute compared to the size of your data set is tremendously more when
you perform something like dot product.
That's where you want GPUs.
But once the model is built and the model is trained, you actually don't need GPUs to
score that model in many, many cases.
What that gives you is it gives you the ability to democratize that computation because
Intel CPU is everywhere.
If a machine has a GPU, it also has an Intel CPU.
And the point that I'm making that in many cases, all you need is a CPU and you're not going
to get the computation faster if you add a GPU to the system simply because it's the memory
bandwidth that's your bottleneck in the computation.
Now we use AVX 512 for vector dot product, for Euclidean distance, and for other vector
operations that were built into the database.
We also use AVX 512 for general purpose SQL computations.
And that allows you to just deliver on orders of magnitude faster SQL query processing
that our competitors have and certainly what Hadoop has or spark.
We're huge fans of that technology and we can't wait when Intel allows us to perform
computations on larger vectors, not just 512 bits.
Can you be more specific in terms of some of the actual results you've seen in production
systems in terms of performance differences?
Yeah, absolutely.
So with MemSQL 6.0 that we just released in October, we can do things like a group by
operation on 100 billion row table in a sub-second.
So 100 billion?
Yes, wow.
So it says a sub-second operation that runs something like give me an average stock price
over 100 billion data points and group it by security or by a stock value.
The computation itself is relatively straightforward but because we perform that computation, quote
unquote, on compressed data and we're using those vectorized operations, we achieve the
performance of a billion operations per second per core.
So if you throw 100 cores into the system and MemSQL is extremely scalable so you can
throw 100 cores or you can throw a thousand cores, you can achieve this type of performance
on your system.
And so that's using the AVX512 instructions, what were you seeing prior to that?
Well, there's a combination of two techniques that goes into this type of performance.
The first is how can you perform operations on compressed data?
So if, like I said earlier, oftentimes it's the memory bandwidth that's the bottleneck
of your computation.
So the better you compress but the better off you are in the final computation because at
the end of the day you scan fewer bytes.
What exactly do we mean by compressor?
Is it kind of your traditional binary compression or are we also talking some kind of deduplication
or something like that?
It's a combination.
So first of all, the compression is called column store compression and we also encode
similar values.
So let's say you, in that example that I said was stock trading, you have 10,000 different
stocks.
So if you have 10,000 different stocks, then each individual stock can be encoded with
just a few bits.
And so then you only use this many bits to represent each stock value.
And as you perform your computations, you never go back decoding those bits into, let's
say, an integer value or got for bit a string value of a stock, right?
Because those computation will become much more expensive.
So now that you perform computations on compressed data, the second step that you do is you optimize
your computation for as few branch mispredictions and as few cash misses as possible.
And those are a big deal because every time you have a branch misprediction, you flash
your CPU pipeline.
So that slows down your computation.
So if you express your computation, so there are no branch, no branches basically.
And there are some very, very cool papers out there and there are some innovation that
we've done here at MCQL as well.
So that will give you the second boost.
And then the third boost is now you take all those bits that you represent your values
and you perform vector operations on top of those encoded values.
And then that's where you use AVX512.
So now, and that gives you another, I would say, three to five X performance improvements
just by using AVX512.
But if you take that, plus you take operations on compressed data, plus you take care of
branch misprediction.
You multiply those all those together.
And then you can routinely see two orders of magnitude improvement in performance.
Wow.
That's pretty fantastic.
The kind of the point here is fancy hardware is cool.
And you want to use it where it's needed.
But also, there's a lot of potential in the hardware you have at hand.
And if you take full advantage of this of that hardware, you will have remarkable results.
Mm-hmm.
Oh, that's great.
I guess as we kind of wrap up, do you have any additional kind of advice to folks that
are looking to, you know, get better, you know, to just to take better advantage of, you
know, whether it's their, you know, their data storage systems, their, you know, hardware
systems, and to use these to make better, you know, predictions and better utilized machine
learning in AI.
Well, my advice here is, don't settle.
There are systems out there such as memtique will that allows you to take pretty much your
end-to-end machine learning AI and application development pipeline and make them completely
real-time.
So whatever you do in batch, whatever you have to wait for, what the right technology
can be squished down to zero.
Is that the technical term?
Squished to zero?
Yeah, let's say that.
And that's where the world is going.
So we'll live in the world in the future where compute and storage are going to be utilities.
And you know, if you're willing to pay for compute, you will have as much compute as you
need, at any concurrency as you need, and any latency as you need.
And the only thing that is going to be bounded by is the amount of money you pay for that
compute.
Great.
Well, Nikita, thank you so much for spending the time to chat with me, I really appreciate
it.
Yeah, absolutely.
Thanks for having me.
Alright everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Nikita or any of the topics covered in this episode, head on over
to twomolei.com slash talk slash 84.
To follow along with our AWS reinvent series, visit twomolei.com slash reinvent.
Of course, we would love to receive your feedback or questions, either via a comment on the
show notes page or via Twitter to at twomolei or at Sam Charrington.
Thanks once again to Intel Nirvana for their sponsorship of this series.
To learn more about deep lens and the other things they've been up to, visit intelnervana.com.
And thank you once again for listening and catch you next time.
