WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:37.840
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.

00:37.840 --> 00:42.840
And last week we kicked off the second volume of our listener favorite AI platform series,

00:42.840 --> 00:47.200
sharing more stories of teams working to scale and industrialize data science and machine

00:47.200 --> 00:49.800
learning at their companies.

00:49.800 --> 00:54.160
We've been teasing that there's more to come and today I am super excited to announce

00:54.160 --> 00:59.360
the launch of our inaugural conference, Twimblecon AI platforms.

00:59.360 --> 01:04.600
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices

01:04.600 --> 01:09.400
necessary to scale the delivery of machine learning and AI in the enterprise.

01:09.400 --> 01:14.840
Now you know Twimble for bringing you dynamic practical conversations via the podcast and

01:14.840 --> 01:18.640
we're creating our Twimblecon events to build on that tradition.

01:18.640 --> 01:24.240
The event will feature two full days of community oriented discussions, live podcast interviews

01:24.240 --> 01:30.440
and practical presentations by great presenters sharing concrete examples from their own experiences.

01:30.440 --> 01:34.640
By creating a space where data science, machine learning, platform engineering and ML ops

01:34.640 --> 01:39.640
practitioners and leaders can share, learn and connect, the event aspires to help see

01:39.640 --> 01:44.800
the development of an informed and sustainable community of technologists that is well equipped

01:44.800 --> 01:48.560
to meet the current and future needs of their organizations.

01:48.560 --> 01:52.880
Some of the topics that we plan to cover include overcoming the barriers to getting machine

01:52.880 --> 01:58.120
learning and deep learning models into production, how to apply ML ops and DevOps to your machine

01:58.120 --> 02:03.040
learning workflow, experiences and lessons learned in delivering platform and infrastructure

02:03.040 --> 02:08.440
support for data management, experiment management and model deployment, the latest approaches

02:08.440 --> 02:14.600
platforms and tools for accelerating and scaling the delivery of ML and DL and the enterprise.

02:14.600 --> 02:19.400
Some deployment stories from leading companies like Google, Facebook, Airbnb, as well as

02:19.400 --> 02:25.040
traditional enterprises like Comcast and Shell and organizational and cultural best practices

02:25.040 --> 02:27.000
for success.

02:27.000 --> 02:31.600
The two day event will be held on October 1st and 2nd in San Francisco and I would really

02:31.600 --> 02:33.600
love to meet you there.

02:33.600 --> 02:39.520
EarlyBurt Registration is open today at Twimblecon.com and we're offering the first ten listeners

02:39.520 --> 02:45.720
who register the amazing opportunity to get their ticket for 75% off using the discount

02:45.720 --> 02:47.320
code TwimbleFirst.

02:47.320 --> 02:55.320
Again, the conference site is Twimblecon.com and the code is TwimbleFirst and now on to

02:55.320 --> 02:56.320
the show.

02:56.320 --> 02:57.320
All right, everyone.

02:57.320 --> 02:59.320
I am on the line with Karen Levy.

02:59.320 --> 03:05.000
Karen is an assistant professor in the Department of Information Science at Cornell University.

03:05.000 --> 03:07.560
Karen, welcome to this week in machine learning and AI.

03:07.560 --> 03:08.560
Thanks.

03:08.560 --> 03:09.480
Nice to join you.

03:09.480 --> 03:18.120
So, Karen, you have a background in both law and sociology and you're currently teaching

03:18.120 --> 03:20.240
in an information science department.

03:20.240 --> 03:21.960
How did that come about?

03:21.960 --> 03:24.800
Yeah, so I started out.

03:24.800 --> 03:27.720
I went to law school and then I worked in a federal court for a couple of years after

03:27.720 --> 03:32.240
that and then kind of in the course of that, I got interested in sort of the social causes

03:32.240 --> 03:36.160
of the legal problems that came through the court on a fairly regular basis.

03:36.160 --> 03:41.280
So I went and got a PhD in sociology and when I was in graduate school, I got really

03:41.280 --> 03:46.240
interested in technology as kind of a route towards social control, right?

03:46.240 --> 03:52.760
Like a different way that people's life chances are impacted besides the law.

03:52.760 --> 03:58.040
So I started to look more at kind of intersections between the law and technology and how people's

03:58.040 --> 04:03.760
decisions are made and their social interactions are structured as a result of the technologies

04:03.760 --> 04:04.760
around them.

04:04.760 --> 04:09.720
It landed me in an information science department and yeah, that's where I am now.

04:09.720 --> 04:18.240
A lot of your focus now is on the social aspects of surveillance and monitoring.

04:18.240 --> 04:24.440
Maybe tell us a little bit about kind of your, the types of research that you do around

04:24.440 --> 04:25.440
that area.

04:25.440 --> 04:26.440
Sure.

04:26.440 --> 04:28.200
So yeah, I'm really interested in surveillance.

04:28.200 --> 04:32.560
So when I, my kind of framework for thinking about technology is that I'm really interested

04:32.560 --> 04:38.240
in the way we use technologies to enforce rules and sometimes those rules are like state

04:38.240 --> 04:43.760
or federal laws and sometimes they are organizational rules or even kind of like expectations and

04:43.760 --> 04:48.000
norms of behavior within intimate relationships like families or friendships.

04:48.000 --> 04:52.760
But I'm really interested in how we use technology to sort of enforce all of those expectations

04:52.760 --> 04:55.640
and one of the ways that we often do that is through surveillance, right, is through

04:55.640 --> 05:00.760
creating a record of the things that people do as it kind of means by which to tell whether

05:00.760 --> 05:02.880
or not they're following the rules or not.

05:02.880 --> 05:06.800
And so I look at that in a variety of different contexts, but the kind of the two big contexts

05:06.800 --> 05:11.360
where I've spent the most time and energy and my research are workplaces.

05:11.360 --> 05:17.040
So I've done a fair amount of work on surveillance in the workplace and intimate context.

05:17.040 --> 05:22.240
So I do a lot of work on families and sexual relationships, intimate relationships, trying

05:22.240 --> 05:26.960
to understand the role that technology plays in how people relate to one another there.

05:26.960 --> 05:31.920
You know, I think one of the things that I'm realizing as we're starting to talk about

05:31.920 --> 05:37.240
this topic is that I find it scary, like the thought of jumping into this conversation

05:37.240 --> 05:44.400
because while I tend to be very optimistic about technology, particularly AI, the whole

05:44.400 --> 05:49.080
surveillance thing kind of freaks me out sometimes.

05:49.080 --> 05:52.840
You're laughing, are you familiar with that kind of take on it?

05:52.840 --> 05:57.000
Well, I mean, I like the idea that within the first five minutes of us starting to talk

05:57.000 --> 05:59.000
you get scared.

05:59.000 --> 06:03.520
I don't know what that says about the research, but yeah, I mean, so there is kind of, certainly,

06:03.520 --> 06:07.400
you could characterize my interest in technology as being sort of about the dark side, right?

06:07.400 --> 06:12.640
About kind of maybe the nefarious ways people use technology or the unintended consequences

06:12.640 --> 06:20.120
that the use of these technologies might have on particularly vulnerable groups of people.

06:20.120 --> 06:24.520
I'm definitely interested in that, but I don't kind of approach it as like a technology

06:24.520 --> 06:26.480
naysayer.

06:26.480 --> 06:30.000
Like I think we ought to implement, I think, you know, I'm like a, I don't think we ought

06:30.000 --> 06:31.480
to be let-ites, right?

06:31.480 --> 06:34.960
Like I think there's a lot of positive roles that technologies can plan our lives, but

06:34.960 --> 06:40.160
I think doing that in a way that's attentive to, you know, ethical and privacy concerns,

06:40.160 --> 06:44.080
and particularly the role that technologies can have on marginalized groups that are marginalized

06:44.080 --> 06:45.600
in all different sorts of ways, right?

06:45.600 --> 06:50.760
Economically or socially, like it doesn't make any sense to deploy things without paying

06:50.760 --> 06:52.160
attention to that.

06:52.160 --> 06:53.160
Right, right.

06:53.160 --> 07:00.640
So we've got this relationship that's developing between artificial intelligence and data

07:00.640 --> 07:09.720
collection where AI has really been enabled by an array of data collection technologies

07:09.720 --> 07:15.720
and really the increased digitization both in our lives and our work.

07:15.720 --> 07:21.960
And it's also hungry for data and so it drives more data collection.

07:21.960 --> 07:31.960
What are, you know, when you think about just the increase in the amount of data that's

07:31.960 --> 07:37.600
being collected about us, like how do you, do you have like a taxonomy or a framework

07:37.600 --> 07:42.080
for thinking about the different impacts that this has?

07:42.080 --> 07:46.920
So my sense is that maybe, you know, to taxonomize different sorts of data, like I don't

07:46.920 --> 07:53.160
do that in a formal way, but I do think like one of the promises and perils of, you know,

07:53.160 --> 07:58.760
the scale and granularity of the data collection that we do now is how readily repurposed or combine

07:58.760 --> 08:00.320
different data sources are, right?

08:00.320 --> 08:05.040
Like that's part of the beauty of what we can do with data now.

08:05.040 --> 08:09.320
But it also creates risks for people that are perhaps unforeseeable, right?

08:09.320 --> 08:14.520
And so because of that, I think, you know, proceeding with caution makes a lot of sense.

08:14.520 --> 08:19.120
And, you know, having kind of a good feel for how, you know, again, particularly marginalized

08:19.120 --> 08:22.160
communities might be differentially impacted by data collection.

08:22.160 --> 08:27.480
So like an example that I like to use in my teaching is about like census population data,

08:27.480 --> 08:28.480
right?

08:28.480 --> 08:30.760
Which feels like kind of not that interesting maybe, right?

08:30.760 --> 08:36.000
Like censuses are just, they seem like pretty, you know, kind of general high level data collection.

08:36.000 --> 08:42.440
But there are all these examples in the past of how that data gets, you know, reused perhaps

08:42.440 --> 08:46.120
years later for all kinds of different purposes, including like human rights abuses, right?

08:46.120 --> 08:51.360
This is something that my colleague Alvaro Badoya at Georgetown has written a lot about.

08:51.360 --> 08:52.520
So things like that, right?

08:52.520 --> 08:56.920
Like things like sort of the lives that data can have later on, right?

08:56.920 --> 09:00.400
Or in the context of like, I've done a bunch of work with some other folks here at Cornell

09:00.400 --> 09:02.480
about intimate partner violence, right?

09:02.480 --> 09:08.000
Like thinking about how the data you generate on your phone or on your computer, you know,

09:08.000 --> 09:11.480
might be very interesting to a partner, to an abusive partner, right?

09:11.480 --> 09:15.840
Or me actually like reveal quite a bit more about your whereabouts than you might anticipate

09:15.840 --> 09:19.680
or know, you know, those are really important considerations for people in the way people

09:19.680 --> 09:23.840
experience, you know, technology and privacy and security.

09:23.840 --> 09:27.680
And so I think paying attention to those kind of like really like my almost mundane like

09:27.680 --> 09:34.160
day-to-day exchanges we have that involve our data, you know, it's not all about our relationship

09:34.160 --> 09:35.920
with kind of like the big tech companies, right?

09:35.920 --> 09:39.480
It's also about our relationships with one another and how those get mediated through

09:39.480 --> 09:42.560
the data trails that we generate.

09:42.560 --> 09:49.240
As we're thinking about this, do you, how do you structure that for, does your research

09:49.240 --> 09:54.720
aim to like structure for that for us like how we should be thinking about this or are

09:54.720 --> 10:02.160
you more kind of exploring different different stories and how folks are impacted?

10:02.160 --> 10:04.560
Yeah, I mean, I would say a lot.

10:04.560 --> 10:06.680
So certainly there are some design implications, right?

10:06.680 --> 10:09.720
So I think paying attention to examples of things that have happened in the past can

10:09.720 --> 10:14.520
help us to think more critically about how we design systems for the future.

10:14.520 --> 10:18.360
But a lot of it I think is actually what you said is like kind of telling these stories,

10:18.360 --> 10:23.200
right, of how people, how people's lives end up changing or end up, you know, impacted

10:23.200 --> 10:27.640
in these different ways based on their interactions with data intensive systems.

10:27.640 --> 10:32.000
And, and I think honestly a lot of the time like we tend to kind of use the most readily

10:32.000 --> 10:33.000
available tool.

10:33.000 --> 10:37.120
So one kind of theme that runs through a lot of my research is how, you know, oftentimes

10:37.120 --> 10:43.360
we might use a data driven system to address a problem that might actually be better addressed

10:43.360 --> 10:45.360
using some other tool, right?

10:45.360 --> 10:51.920
So like, for example, I've done a bunch of work about truck drivers and trying to understand

10:51.920 --> 10:56.720
how truck drivers are affected by the technologies that are used to track them.

10:56.720 --> 11:00.000
And the reason why we use those technologies is because truck drivers are really tired,

11:00.000 --> 11:01.000
right?

11:01.000 --> 11:02.000
They're overworked and they become unsafe.

11:02.000 --> 11:06.120
And so we use technologies to try and ensure that they're not breaking federal rules about

11:06.120 --> 11:07.960
how much they can drive.

11:07.960 --> 11:08.960
That's fine, right?

11:08.960 --> 11:09.960
That's one approach.

11:09.960 --> 11:13.600
Another approach that I argue would be actually much more effective is just to change like

11:13.600 --> 11:15.400
some of the labor laws around trucking, right?

11:15.400 --> 11:18.680
To change the way we incentivize different types of work, right?

11:18.680 --> 11:23.800
Like structural changes to the organization of the industry, you know, that's not necessarily

11:23.800 --> 11:29.160
a data intensive solution, but you know, it's not the one we've adopted, right?

11:29.160 --> 11:32.760
And so oftentimes I think we tend to use technology as kind of this like band-aid solution

11:32.760 --> 11:37.120
for problems that are inherently economic or social or cultural, but we tend to approach

11:37.120 --> 11:38.600
them as technology problems.

11:38.600 --> 11:42.840
And that often I think is the source of some of these unintended consequences for people.

11:42.840 --> 11:48.640
In the case of the truck drivers, it strikes me that, you know, one thing that that illustrates

11:48.640 --> 11:57.600
is that the technology can be more accessible to the individual groups, for example, trucking

11:57.600 --> 12:03.920
companies or truck drivers, you know, as opposed to, you know, structural changes, some

12:03.920 --> 12:09.440
of those structural regulatory changes that you're describing require, you know, broad

12:09.440 --> 12:15.480
consensus across a large influence group of people and organizations.

12:15.480 --> 12:18.840
Does that play a role in some of those choices?

12:18.840 --> 12:19.840
Yeah.

12:19.840 --> 12:23.680
I mean, I think the accessibility of technology, you know, as you mentioned, right, the

12:23.680 --> 12:28.000
inquiry, like the cheapness with which we can start to gather data and analyze data definitely

12:28.000 --> 12:31.440
lends itself to that being sort of an attractive solution for solving problems.

12:31.440 --> 12:34.800
And I think that can often be like a really positive thing.

12:34.800 --> 12:35.800
But as you say, right?

12:35.800 --> 12:39.600
Like that often might keep us from actually addressing some of the structural problems

12:39.600 --> 12:42.840
that might be more effective at kind of getting at the root cause of a problem.

12:42.840 --> 12:46.440
So in trucking, right, like you can monitor a driver to find out if he's super tired,

12:46.440 --> 12:47.440
right?

12:47.440 --> 12:48.440
Like that's pretty easy to do.

12:48.440 --> 12:51.920
But what's much harder is like making sure he doesn't, he isn't incentivized to get that

12:51.920 --> 12:52.920
tired in the first place.

12:52.920 --> 12:56.720
And you can see why that's politically more difficult, economically more difficult.

12:56.720 --> 12:59.520
But if you don't change that structure, then you're going to end up, you know, with

12:59.520 --> 13:03.360
almost this arms race, right, where people are still going to want to act the way they're

13:03.360 --> 13:04.880
incentivized to act.

13:04.880 --> 13:08.920
And you put technology in their place as sort of a roadblock, but, you know, you haven't

13:08.920 --> 13:11.720
actually addressed the root of the problem.

13:11.720 --> 13:17.200
Kind of that, you know, there's certainly merit in addressing the structural issues.

13:17.200 --> 13:22.960
Is there something wrong with using technology as just another tool to help drive the kind

13:22.960 --> 13:25.040
of behavior that we want?

13:25.040 --> 13:31.160
Or in other words, you know, what are some of the implications on these drivers of the

13:31.160 --> 13:35.800
surveillance technology that's been put in place to monitor their behavior?

13:35.800 --> 13:36.800
Yeah.

13:36.800 --> 13:41.240
So, I mean, technology can certainly be like a really useful tool in the, in the toolkit

13:41.240 --> 13:46.800
for enforcing behavior, you know, for incentivizing behaviors that we think are desirable or safer

13:46.800 --> 13:51.320
or better for society, like it absolutely can have a role.

13:51.320 --> 13:55.720
If we depend on it a lot, I think what we end up doing is making responsible the parties

13:55.720 --> 13:59.320
who often have like the least social and economic power, right?

13:59.320 --> 14:04.360
So essentially, like in trucking, for example, the way that these technologies end up functioning

14:04.360 --> 14:08.480
is to kind of punish drivers for doing like what they almost have no choice but to do, right?

14:08.480 --> 14:13.440
Like essentially, you know, in order to make ends meet, they've kind of violated the law

14:13.440 --> 14:16.880
for decades, like for generations and everybody kind of knows that, right?

14:16.880 --> 14:19.440
That's like a known fact within the industry.

14:19.440 --> 14:22.560
And now they get kind of hit from both sides because they're still incentivized to do all

14:22.560 --> 14:25.440
those things, but now they're also punished for doing them, right?

14:25.440 --> 14:28.240
And they kind of bear the brunt of that.

14:28.240 --> 14:32.640
And so I think the consequence of sometimes using technology as kind of like the first course

14:32.640 --> 14:37.440
of action is that oftentimes it can have those impacts on maybe the person with the least

14:37.440 --> 14:39.440
structural power.

14:39.440 --> 14:42.040
And that like merits a subnormative consideration, right?

14:42.040 --> 14:46.920
We have to ask ourselves, is that the way we want to address social problems is by kind

14:46.920 --> 14:51.880
of, you know, enforcing these rules kind of at the low end of the spectrum?

14:51.880 --> 14:53.400
And sometimes the answer might be yes, right?

14:53.400 --> 14:57.880
Or sometimes it may be that a problem is so pressing or, you know, so consequential

14:57.880 --> 15:01.760
that what that we do want to make sure that people like have to follow the rules.

15:01.760 --> 15:04.840
But we also want to ask like, well, why are they motivated not to follow the rules in

15:04.840 --> 15:05.840
the first place?

15:05.840 --> 15:09.680
And might we do something to actually improve their quality of life or, you know, change

15:09.680 --> 15:13.080
kind of the circumstances such that they're not putting out in that position?

15:13.080 --> 15:14.080
Right.

15:14.080 --> 15:18.720
It sounds like there may be an element of the situation that these drivers are in that

15:18.720 --> 15:21.120
we haven't fully explored.

15:21.120 --> 15:27.680
They're both required by someone not to work too much.

15:27.680 --> 15:32.240
There's some bounds placed on the number of hours, consecutive hours that they work

15:32.240 --> 15:39.720
without rest, but it sounds like there are also incentives on them working more, you

15:39.720 --> 15:47.800
know, beyond those limits, you've referred to incentives just to, you know, earn a living,

15:47.800 --> 15:49.840
put food on the table, all of that kind of thing.

15:49.840 --> 15:54.720
Are there also incentives on the part of their companies to do that?

15:54.720 --> 16:00.480
And maybe, you know, let's explore this relationship between the technology and what the companies

16:00.480 --> 16:02.600
are asking these drivers to do.

16:02.600 --> 16:06.200
Because otherwise, it leaves open questions as to, you know, why the driver is not more

16:06.200 --> 16:09.560
responsible for their individual behavior in this situation.

16:09.560 --> 16:10.560
Yeah.

16:10.560 --> 16:14.400
I mean, like so one parallel that you might, that maybe is, you know, an easy one to wrap

16:14.400 --> 16:18.160
your head around is, is, you know, thinking about like drug laws or something, right?

16:18.160 --> 16:22.200
We can say like, oh, you know, we really, like it's against the law to use these drugs,

16:22.200 --> 16:25.400
right? It's against a lot to do, you know, a variety of things.

16:25.400 --> 16:27.760
And we can enforce those rules, you know, and we often do, right?

16:27.760 --> 16:32.360
Like using technology, but it doesn't change the fact that people use and sell drugs,

16:32.360 --> 16:33.360
right?

16:33.360 --> 16:36.600
And so better enforcement, more consistent enforcement, like may have a role in the way

16:36.600 --> 16:41.200
we choose to fight that problem, but it doesn't actually change the underlying structure

16:41.200 --> 16:43.800
of neighborhoods or the opportunities people have available to them, right?

16:43.800 --> 16:48.560
Like it's a piece of the puzzle that it's easy to rely on, but it's not the whole puzzle.

16:48.560 --> 16:52.120
And I think, you know, what you brought up about kind of like the network of interests,

16:52.120 --> 16:53.120
right?

16:53.120 --> 16:54.960
Like the companies that truckers work for, things like that.

16:54.960 --> 17:01.240
Yeah, that does like definitely play a role in the way we, you know, think about technology

17:01.240 --> 17:02.240
and rules.

17:02.240 --> 17:06.320
So like, what ends up happening in tracking is that companies buy these systems that track

17:06.320 --> 17:07.320
drivers, right?

17:07.320 --> 17:11.600
Because the government actually now requires drivers to be, to have their time tracked

17:11.600 --> 17:12.800
it early.

17:12.800 --> 17:15.560
And then companies say like, well, we bought this system.

17:15.560 --> 17:19.560
But actually now is like pretty cheap or free for us to like also run a bunch of other

17:19.560 --> 17:24.760
analytics on how drivers are doing, meaning like how much fuel, what's their fuel economy,

17:24.760 --> 17:25.760
right?

17:25.760 --> 17:27.880
Like, you know, are they saving enough money for the company?

17:27.880 --> 17:29.160
Are they driving efficiently?

17:29.160 --> 17:30.480
Are they not breaking hard?

17:30.480 --> 17:31.480
Things like that.

17:31.480 --> 17:34.880
And so they end up actually tracking a much wider swath of driver's behavior.

17:34.880 --> 17:39.480
And so it definitely changes the relationship to the company in that it means that drivers

17:39.480 --> 17:44.520
end up actually getting managed in this more granular like real time way, in this way

17:44.520 --> 17:45.880
that they weren't before.

17:45.880 --> 17:49.560
And this is true across a lot of industries, like a lot of low wage workers end up being

17:49.560 --> 17:51.360
supervised really closely.

17:51.360 --> 17:55.680
It's interesting to me in trucking because this is a group of workers who have like sort

17:55.680 --> 17:59.240
of historically gone out of the reason why their truck drivers is because they didn't want

17:59.240 --> 18:00.920
this kind of oversight, right?

18:00.920 --> 18:03.640
Like they wanted to have a little bit of freedom and autonomy.

18:03.640 --> 18:06.600
And so it really becomes like a dignity issue for a lot of them, right?

18:06.600 --> 18:09.880
To feel as though, you know, their trucks are almost like their homes.

18:09.880 --> 18:13.360
They're in their trucks for weeks at a time, maybe.

18:13.360 --> 18:17.120
And now to kind of like have this in like this oversight, real time oversight from the

18:17.120 --> 18:20.800
government or from their employers, like it's quite, it's quite a thing for them.

18:20.800 --> 18:23.920
It's very different from like me being surveilled at work or you being surveilled at work.

18:23.920 --> 18:27.600
Like the nature of the workplace is just different.

18:27.600 --> 18:35.040
And so how does this manifest itself in terms of maybe relationship between the drivers

18:35.040 --> 18:36.040
and their companies?

18:36.040 --> 18:37.720
What have you seen in that regard?

18:37.720 --> 18:38.720
Yeah.

18:38.720 --> 18:39.720
So you know, it's interesting, right?

18:39.720 --> 18:45.640
Like a lot of the drivers that I've talked to have said things like, you know, like what

18:45.640 --> 18:49.560
it shows if you watch me really closely is that like you don't trust me, right?

18:49.560 --> 18:51.000
You don't trust me to do the right thing.

18:51.000 --> 18:52.560
You don't trust me to be safe.

18:52.560 --> 18:55.120
You don't trust me to know my body well enough, right?

18:55.120 --> 18:59.200
Like many, many, many drivers compare this to like feeling like a criminal or feeling

18:59.200 --> 19:00.200
like a child, right?

19:00.200 --> 19:07.560
They see it as like this really kind of demeaning experience to be tracked in real time.

19:07.560 --> 19:13.720
So that obviously, like it's pretty unpopular among most workers to feel like their work

19:13.720 --> 19:20.720
and livelihoods are being supervised so closely, kind of a sort of ironic like Coda to that

19:20.720 --> 19:23.120
is that, you know, they say like, well, you can't know my body.

19:23.120 --> 19:24.800
You can't know how tired I am.

19:24.800 --> 19:28.360
The next wave of technologies actually do sort of are able to infer those things more

19:28.360 --> 19:33.160
closely because a lot of them involve like wearable technologies or cameras that are trained

19:33.160 --> 19:35.480
on a driver's eyelids to monitor fatigue, things like that.

19:35.480 --> 19:39.440
So they actually are quite a bit more invasive and can infer quite a bit more about whether

19:39.440 --> 19:40.440
a driver is tired.

19:40.440 --> 19:42.760
You know, that doesn't necessarily make truckers feel better.

19:42.760 --> 19:47.360
They don't say like, oh, well, now you like are inside my body or have, have information

19:47.360 --> 19:52.400
about what's going on inside my body that doesn't affect their privacy concerns, but you

19:52.400 --> 19:55.000
know, they're kind of, they kind of can't win one way or the other.

19:55.000 --> 19:56.840
How are the drivers reacting to this?

19:56.840 --> 20:01.200
Have they started like organizing against the trucking companies or it, it sounds like

20:01.200 --> 20:06.080
there's multiple issues, well, there's clearly multiple issues here, but you know, there

20:06.080 --> 20:15.280
is the, you know, this technology being put in place to address a specific issue around,

20:15.280 --> 20:21.840
you know, driver fatigue, but then there are kind of the downstream effects of this surveillance

20:21.840 --> 20:27.280
and the data that's being collected about these drivers and how that's being used and

20:27.280 --> 20:32.520
how that is, how that's being used specifically to manage these drivers.

20:32.520 --> 20:37.720
Where is the industry at with regards to addressing this tension that's, that's starting to

20:37.720 --> 20:38.720
be created?

20:38.720 --> 20:43.760
Right, you're right that there's quite a bit of kind of resistance or, you know, workers

20:43.760 --> 20:47.960
kind of viewing these technologies as, as unpopular as invasive and this is true across

20:47.960 --> 20:51.480
lots of different workplace surveillance contexts, but I think you feel it really strongly

20:51.480 --> 20:55.920
in trucking just kind of based on the culture of the occupation.

20:55.920 --> 21:01.160
So, you know, as in most low wage workplaces, you know, workers don't often have the

21:01.160 --> 21:04.800
ultimate say they might not have, you know, as much power as management to make decisions

21:04.800 --> 21:09.400
about whether these things are in place, in trucking, you know, to some extent the stuff

21:09.400 --> 21:12.840
is now federally mandated like as of just a few months ago, so to some extent there's

21:12.840 --> 21:18.080
not much that employers can do, but there are some kind of movements where you see some

21:18.080 --> 21:21.480
companies trying to be responsive to this saying like, yeah, we'll treat you like an adult

21:21.480 --> 21:24.960
or there are certain types of data that we won't collect or that we won't give our

21:24.960 --> 21:27.000
dispatchers access to things like that.

21:27.000 --> 21:32.800
So, you see that a bit, but, you know, it's not a, it's not an industry with heavy unionization,

21:32.800 --> 21:36.680
which you might, you know, unions might be a party you would expect to push back on things

21:36.680 --> 21:37.680
like that.

21:37.680 --> 21:43.160
So, there aren't a lot of like really clear avenues for workers to, you know, resist the

21:43.160 --> 21:44.160
sort of data collection.

21:44.160 --> 21:47.840
It's just kind of becoming a new normal in the industry, and so that's a lot of what

21:47.840 --> 21:52.160
I'm interested in is kind of like, what does that transition end up looking like?

21:52.160 --> 21:55.200
You know, how do you see truckers resist? So like, one of the things that I've studied

21:55.200 --> 21:59.360
fairly extensively is trying to understand all the different ways that workers like try

21:59.360 --> 22:03.960
to thwart these technologies like block data collection or falsify data collection.

22:03.960 --> 22:07.520
You know, otherwise trying kind of maintain a little bit of independence where the technology

22:07.520 --> 22:10.000
is sort of seen as taking that away from them.

22:10.000 --> 22:12.360
And what kinds of things have you seen there?

22:12.360 --> 22:13.360
A whole bunch of stuff.

22:13.360 --> 22:15.640
I mean, actually like a whole bunch of really interesting stuff.

22:15.640 --> 22:21.160
So they, you know, kind of on one end of the spectrum, workers will just like outright

22:21.160 --> 22:24.920
break the thing, right, like we'll destroy it.

22:24.920 --> 22:30.800
Sometimes, you know, and it's kind of purposefully visible way as like sort of a form of protest.

22:30.800 --> 22:33.280
And on the other end of the spectrum, there are things that you can do that are much more

22:33.280 --> 22:34.280
subtle.

22:34.280 --> 22:37.920
Like you can, you know, kind of, I don't want to give all their secrets away, right?

22:37.920 --> 22:42.440
But you can kind of do things to, you know, get extra time out of the systems if you know

22:42.440 --> 22:44.680
what the limitations of the system are, right?

22:44.680 --> 22:48.360
Like you can eek out a little bit of extra driving time and actually kind of interestingly,

22:48.360 --> 22:50.920
you know, you mentioned their relationships with companies.

22:50.920 --> 22:54.000
These actually sometimes kind of want them to do those things or will instruct them about

22:54.000 --> 22:56.240
how to cheat because companies kind of want it both ways, right?

22:56.240 --> 23:00.400
They want to monitor and enforce the rules, but they also like want their stuff moved

23:00.400 --> 23:02.600
at the speed of business.

23:02.600 --> 23:06.680
So sometimes actually truckers actually get kind of told that, you know, you've got to

23:06.680 --> 23:08.440
violate the rules anyway.

23:08.440 --> 23:10.440
And here's how you do it.

23:10.440 --> 23:12.720
So there's a variety of different things they do, right?

23:12.720 --> 23:13.720
That are designed.

23:13.720 --> 23:16.960
And some of them, you know, like actually don't even get them any more driving time, but

23:16.960 --> 23:20.520
they're kind of just about like preserving kind of their identity or their feeling

23:20.520 --> 23:21.520
about autonomy.

23:21.520 --> 23:28.080
So like my favorite example of this is a trucker on YouTube, actually, who shows other

23:28.080 --> 23:35.200
truckers how to kind of get behind the system, the system runs on like a Windows XP backbone

23:35.200 --> 23:40.000
and he shows like here's how you can actually play solitaire on your monitor, right?

23:40.000 --> 23:41.000
Like, which is great.

23:41.000 --> 23:44.200
Like that's not a thing that you're supposed to be able to do, but he like manages to figure

23:44.200 --> 23:45.200
out how you do it.

23:45.200 --> 23:46.880
And I'm sure like that's no longer viable.

23:46.880 --> 23:51.040
Like I'm sure, you know, that's probably no longer a feasible thing to be able to do

23:51.040 --> 23:52.040
with these systems.

23:52.040 --> 23:55.160
But though he does it, you know, that's not making him any extra money, but that's like

23:55.160 --> 23:58.480
a way for him to kind of assert himself.

23:58.480 --> 24:01.400
And I actually find that really interesting, right, that there's a lot of resistance that

24:01.400 --> 24:06.400
takes place that's much more about like maintaining identity and autonomy than it is necessarily

24:06.400 --> 24:11.080
about kind some kind of instrumental like making more money, something like that.

24:11.080 --> 24:20.920
Have these stories taught you anything about how the broader society, you know, will

24:20.920 --> 24:23.920
react to increased surveillance?

24:23.920 --> 24:29.720
Like are there, you know, again, I guess I'm, you know, maybe I'm being overly analytical

24:29.720 --> 24:34.040
in this conversation, but I'm looking again for the taxonomy of, you know, resistance

24:34.040 --> 24:35.560
to surveillance.

24:35.560 --> 24:43.040
Yeah, I mean, a lot of the strategies that like truckers, truckers are the context that

24:43.040 --> 24:44.040
I know the best, right?

24:44.040 --> 24:47.200
Because I've spent the most time studying that industry, but you see some of these, you

24:47.200 --> 24:52.040
know, same techniques across other contexts of work or other just relational contexts,

24:52.040 --> 24:57.960
right, where people try and falsify data streams or, you know, find some way to make the technology

24:57.960 --> 25:03.040
appear to, you know, show one account when in fact, you know, another account might be,

25:03.040 --> 25:06.400
it might be different.

25:06.400 --> 25:09.880
So I think, you know, to some extent, like the, that you see that, I wouldn't say it's

25:09.880 --> 25:13.280
universal, but you see that across a lot of different contexts.

25:13.280 --> 25:18.720
I'm curious where you fall on kind of the surveillance spectrum personally, like do you, do you

25:18.720 --> 25:19.720
use ad blockers?

25:19.720 --> 25:22.520
Do you put tape over your computer camera?

25:22.520 --> 25:25.520
Like, yeah, it's, so it's a good question, right?

25:25.520 --> 25:28.160
So like, I do use ad blockers.

25:28.160 --> 25:31.640
Where this comes up, actually, the most for me is, you know, a lot of my work also looks

25:31.640 --> 25:37.480
at families, looks at how people and families surveil one another or an intimate relationships.

25:37.480 --> 25:41.000
And there, you know, one of the, one of the reasons I got like the most interested in

25:41.000 --> 25:44.480
that context is because I would go to all these privacy and security conferences and

25:44.480 --> 25:48.720
I like where people are, you know, working on, like, say national security issues or consumer

25:48.720 --> 25:49.720
privacy.

25:49.720 --> 25:54.160
Where, you know, these are like pretty staunch advocates of personal privacy.

25:54.160 --> 25:57.320
And then I would talk to them about like, oh, do you do something to like track your

25:57.320 --> 26:00.920
kids whereabouts or like, could you tell me where your spouse is right now?

26:00.920 --> 26:04.400
And a lot of them were like, oh, yeah, like I definitely like read all my kids texts,

26:04.400 --> 26:05.400
you know?

26:05.400 --> 26:09.240
And that struck me as like a really interesting situation, right, that like a lot of,

26:09.240 --> 26:13.560
a lot of the things that were really resistant to, you know, when they come, come at us from

26:13.560 --> 26:16.640
the government or from big multinational corporations, we're quite willing actually

26:16.640 --> 26:19.640
to kind of replicate in our own intimate lives.

26:19.640 --> 26:24.240
So that's something I've gotten really interested in lately is trying to understand kind of how

26:24.240 --> 26:29.560
people, how people become surveyors themselves, right, in their own homes.

26:29.560 --> 26:31.480
So I have a colleague Luke Stark.

26:31.480 --> 26:36.000
We just wrote a paper called the surveillance consumer that tries to look at how people

26:36.000 --> 26:40.320
actually become data collectors themselves in their own homes using a variety of consumer

26:40.320 --> 26:44.440
products, like, you know, nanny cams and things like that and trying to understand how

26:44.440 --> 26:50.560
that too ends up having this disproportionate effect on marginalized people.

26:50.560 --> 26:56.400
And when you say marginalized people, are you referring to specifically within the home

26:56.400 --> 26:57.400
relationship?

26:57.400 --> 27:01.080
Yeah, people, yeah, so I use marginalized fairly broadly, just to mean people who have,

27:01.080 --> 27:03.120
you know, relatively less power in a situation.

27:03.120 --> 27:07.360
So in families, certainly, like, I have a bunch of this work on domestic violence that,

27:07.360 --> 27:12.320
you know, we're, you know, it's often, often, but not always, women and children who suffer

27:12.320 --> 27:17.520
disproportionately from things like, you know, spyware, but in other contexts too.

27:17.520 --> 27:22.480
So like I mentioned, like the nanny cams and nannies in homes often tend to be women of

27:22.480 --> 27:27.400
color from, you know, with lower socioeconomic status, who end up being disproportionately

27:27.400 --> 27:33.120
surveilled in their work, you know, as a result of kind of this consumer, like consumer

27:33.120 --> 27:34.520
led surveillance.

27:34.520 --> 27:39.760
So it ends up being kind of like just another way in which communities of color, women,

27:39.760 --> 27:44.000
you know, people who historically have less power end up kind of suffering the brunt of

27:44.000 --> 27:45.000
data collection.

27:45.000 --> 27:46.000
It's interesting, actually, right?

27:46.000 --> 27:49.800
So my colleague Jonas Lerman has this really lovely piece about how when we think about

27:49.800 --> 27:55.000
surveillance, marginalized people are kind of simultaneously overrepresented and underrepresented.

27:55.000 --> 27:56.000
Right?

27:56.000 --> 27:59.800
So like we have all of these examples of how data sets are biased because, you know, they

27:59.800 --> 28:01.880
only include white men, right?

28:01.880 --> 28:06.720
Or they like don't include this kind of variety of people or images or of texts or whatever

28:06.720 --> 28:09.600
it is and that that can lead to these really biased outcomes.

28:09.600 --> 28:13.080
But then we also have these problems where like there's over representation in the data,

28:13.080 --> 28:14.080
right?

28:14.080 --> 28:18.840
Where like communities of color or poor people are historically like way over, over-surveiled,

28:18.840 --> 28:19.840
right?

28:19.840 --> 28:22.960
Much more data is collected about them for things like, you know, getting public benefits

28:22.960 --> 28:26.400
or in the course of, you know, the education system or criminal justice system.

28:26.400 --> 28:28.920
And so both of those things, I think are true at once, right?

28:28.920 --> 28:33.120
You have like harms both from inclusion and exclusion.

28:33.120 --> 28:37.320
So yeah, it's interesting to me to kind of try and untangle what that looks like in day

28:37.320 --> 28:39.280
to day life.

28:39.280 --> 28:52.000
And you've alluded to, you know, specific examples of unintended consequences in this,

28:52.000 --> 28:59.360
in the application of surveillance, particularly within these, or as it impacts marginalized

28:59.360 --> 29:04.360
communities, can you maybe talk through some specific examples there?

29:04.360 --> 29:09.080
There are a bunch of people who've done a lot of really excellent work in the Serena.

29:09.080 --> 29:13.840
So there's a bunch of work, for example, on risk assessment and criminal justice and

29:13.840 --> 29:20.040
in predictive policing that many of my colleagues have done in which, you know, they demonstrate

29:20.040 --> 29:26.040
that predictive policing systems which are trained on, you know, crime data that's already

29:26.040 --> 29:30.200
over-represents, you know, communities of color, marginalized communities, tends to,

29:30.200 --> 29:35.080
you know, just exacerbate the over-policing of those communities, right, for reasons

29:35.080 --> 29:38.160
that, you know, you just create these feedback loops.

29:38.160 --> 29:43.400
So that's, you know, one example that, you know, it's one example in this particular context.

29:43.400 --> 29:49.040
But we see it in lots of different contexts, right, Virginia U-Banks is a scholar at

29:49.040 --> 29:54.040
SUNY Albany who's written this really wonderful book called Automating Inequality that documents

29:54.040 --> 30:00.640
how local governments implement various, like, allocation algorithms to try and do things

30:00.640 --> 30:06.720
like apportion, housing to homeless people, or to direct public benefits programs, things

30:06.720 --> 30:12.160
like that, and how often, you know, because of various biases in the data or, you know,

30:12.160 --> 30:16.960
kind of the limitations that these systems are operating under, you know, it's not to

30:16.960 --> 30:20.720
say that they necessarily make the problems worse necessarily, but they may have impacts

30:20.720 --> 30:26.400
on communities that are not readily recognized, right, and, and, and, of course, they get

30:26.400 --> 30:31.840
a little bit more, the effects on communities can be a little bit more obscured, right?

30:31.840 --> 30:37.160
So, like, when, when communities adopt risk assessment algorithms or, you know, other

30:37.160 --> 30:41.560
systems like the people who are in positions of power to make decisions may not, you

30:41.560 --> 30:45.680
know, have a great sense for, for the right questions to be asking, or how these things

30:45.680 --> 30:50.320
will end up impacting their communities in the long run, which is not their fault, right,

30:50.320 --> 30:54.960
like, they're really difficult questions even for, you know, technical folks to untangle,

30:54.960 --> 30:58.920
but it definitely is a cause for concern as these, as these tools kind of gain traction

30:58.920 --> 31:00.400
across a bunch of different domains.

31:00.400 --> 31:06.520
Yeah, it strikes me that part of what you're, maybe it's not something that you're saying.

31:06.520 --> 31:14.240
It's a thread that I'm seeing in some of these stories is with regard to technology or

31:14.240 --> 31:21.720
surveillance in particular, you know, there's an element of power corruption here, meaning,

31:21.720 --> 31:31.560
you know, people will adopt surveillance to try to address specific point issues and surveillance

31:31.560 --> 31:39.000
tends to be, you know, almost this beast that consumes other, you know, adjacent freedoms

31:39.000 --> 31:40.000
or something like that.

31:40.000 --> 31:44.520
I'm trying to articulate this without being overly dramatic, but it was easy to get

31:44.520 --> 31:49.200
dystopian really fast, which I think is actually a problem, like, I think we should be

31:49.200 --> 31:54.160
really critical actually of our own, you know, like the critical community is, I think,

31:54.160 --> 31:57.400
a really important one to bring into these conversations, but we ought to also be critical

31:57.400 --> 32:02.080
of, you know, our own techniques and our own assertions, right, that there is potentially

32:02.080 --> 32:05.880
a lot of good to be, to come from some of these applications, but I think you're right,

32:05.880 --> 32:09.440
right, like so this sometimes gets called like surveillance creep, right, but like, once

32:09.440 --> 32:13.680
you started gathering data, you know, it gets easier to just add a little bit more data

32:13.680 --> 32:18.560
to the pile or to use the data for some other purpose, because you've got it now, right,

32:18.560 --> 32:23.520
this is, and there was a few years ago, a big debate in kind of the privacy community

32:23.520 --> 32:28.240
about collection restrictions versus use restrictions, right, so the idea that you tell companies,

32:28.240 --> 32:32.760
for example, like these, these are the types of data you can collect versus like, once

32:32.760 --> 32:37.440
you have data, here's how you can use the data, and the industry kind of was in favor

32:37.440 --> 32:41.640
generally of these use restrictions, right, because as opposed to collection restrictions,

32:41.640 --> 32:45.480
because they, you know, allowed them to amass more data, but, you know, then there was

32:45.480 --> 32:49.240
kind of some subsequent backlash where people say like, well, you know, like once you've

32:49.240 --> 32:53.960
collected it, like the game is kind of, you've given the game away at that point, right,

32:53.960 --> 32:58.040
like because things get used for all kinds of different purposes or rules change, right,

32:58.040 --> 33:01.240
like we have, you know, you can put all these safeguards in place that you want, but,

33:01.240 --> 33:04.520
you know, you don't always know who's going to be in power. I mean, this has come up actually

33:04.520 --> 33:09.880
even recently with, like New York City had this municipal ID program, okay, they rolled

33:09.880 --> 33:14.280
out a couple of years ago for undocumented people, right, where they said like, here's

33:14.280 --> 33:18.440
how you can get a municipal ID card that will allow you to do things like use the library,

33:18.440 --> 33:22.600
right, even though you don't have other documentation. And after doing, I mean, it was by,

33:22.600 --> 33:27.400
you know, meant to be like a positive social program, and it's a program that actually,

33:27.400 --> 33:31.880
you know, sort of accidentally created real risks for these groups of people, because

33:31.880 --> 33:35.880
suddenly now there were concerns that, you know, actually those, those records will be,

33:35.880 --> 33:40.440
would actually be used to target those people, right, for things like Homeland Security

33:40.440 --> 33:44.840
enforcement, right, for ICE enforcement. So, like those are the types of risks, right,

33:44.840 --> 33:49.000
like that's, that's an extreme one, but those are the types of things that, that we get concerned

33:49.000 --> 33:53.960
about, right, is that, you know, terms of service change, privacy policies change, you know,

33:53.960 --> 33:58.440
stuff gets hacked, like there are all kinds of different potential routes for data to be used

33:58.440 --> 34:06.200
in ways that we don't anticipate. You made an interesting point about being critical in our

34:06.200 --> 34:13.800
critiques has, how do we, how do we think about that? How do we, you know, how do we do that?

34:14.600 --> 34:18.040
Yeah, so I think about this a lot, right, because now that I'm in information science,

34:18.040 --> 34:21.720
so, and the information science program I'm in is really closely tied to computer science,

34:21.720 --> 34:24.360
so I sit in the same building with a bunch of computer scientists all the time,

34:25.240 --> 34:28.440
which I really enjoy, because I learn a lot from, you know, understanding their

34:28.440 --> 34:33.960
perspectives on these issues. I think the best thing that that we can do is, like, honestly,

34:33.960 --> 34:40.360
to try and amass more technical understanding. So I think, like, the worst examples of kind of

34:40.360 --> 34:44.280
critique of these systems come from people who don't necessarily, who have a thin understanding

34:44.280 --> 34:48.200
of how the systems actually work in practice, and that means both technically how they work and

34:48.200 --> 34:51.960
what their social effects are, right, so you can't really understand one without the other.

34:53.480 --> 34:57.320
And then the other thing is, is I think actually about engagement, it sounds kind of,

34:57.320 --> 35:01.480
you know, touchy-feely, but I think engagement with practitioners is the other, like, best way

35:01.480 --> 35:07.960
forward to kind of avoid painting like the other side as being the other side or as being like a

35:07.960 --> 35:13.240
straw man, you know, but to understand, like, the actual, you know, motivations and values

35:13.240 --> 35:16.440
behind these decisions, which really requires, like, actually talking to people.

35:17.560 --> 35:21.880
So maybe this is just like a plug for social science, but I really believe that, like,

35:21.880 --> 35:25.960
I really believe that, you know, having kind of more social science expertise at the table,

35:25.960 --> 35:29.320
and having people, having social scientists then take seriously technical expertise,

35:29.320 --> 35:32.840
and really try to understand it is, you know, the best we can do.

35:32.840 --> 35:39.000
Maybe this should be posed as a confirmation or question, but you made it sound like you find

35:39.000 --> 35:51.560
that people in computer science, in technology, are way less concerned about these issues than lay

35:51.560 --> 35:57.400
people. And that's almost my, I almost have the opposite instinct. Oh yeah, I definitely not

35:57.400 --> 36:02.600
what I tried to, that wasn't what I was hoping to portray. Okay, okay. Yeah, yeah, no, I definitely

36:02.600 --> 36:06.440
don't feel that way. I think, like, kind of the easy, so the easy critique that sometimes made

36:06.440 --> 36:12.520
of algorithmic systems from the outside, right, can be that algorithms have this kind of

36:12.520 --> 36:18.360
depersonalizing effect, right, or that, you know, the people designing them, like, don't really

36:18.360 --> 36:23.400
understand, you know, what biases might be embedded in them and don't really care, right? Like that,

36:23.400 --> 36:28.520
I think is like, I'm obviously, I'm creating a straw man by describing that as this process of

36:28.520 --> 36:32.520
straw manning that some other people do, but I think that's like, that's like the worst form of

36:32.520 --> 36:36.600
criticism, right? Because it doesn't acknowledge what you just said, which is that, like, oftentimes,

36:36.600 --> 36:41.640
I think people are like quite deeply concerned about the systems they create and the inequities

36:41.640 --> 36:47.000
that those might exacerbate. So I do, I think it's exactly what you said. I think recognizing those

36:47.000 --> 36:53.640
intentions and actually working with people to try and build the best systems we can, like, that

36:53.640 --> 36:59.080
ought to be our goal, I think, is critical scholars, not necessarily to try and tear down, you know,

36:59.080 --> 37:05.400
efforts to try and improve those things. So you're working on a book on surveillance,

37:05.400 --> 37:12.040
particularly as it applies to the truckers that you've been working with. It sounds like that

37:12.040 --> 37:18.200
book is in progress, but if you were to kind of step back and, you know, or maybe project forward

37:18.200 --> 37:26.280
to the the conclusion of this book, you know, what are the key messages or takeaways for folks that

37:26.280 --> 37:31.960
are, I guess in the case of listeners of this podcast, primarily technologists, primarily,

37:32.680 --> 37:39.480
you know, working to create, refine, perfect some of these technologies. You know, what are the

37:39.480 --> 37:48.920
takeaways for them in terms of, you know, understanding and dealing with the implications that they have

37:48.920 --> 37:56.600
on different communities and particularly marginalized communities? Yeah, so I think, I mean,

37:56.600 --> 38:00.680
I think they're probably two. Like, so the first is something that I kind of alluded to earlier,

38:00.680 --> 38:06.200
which is, you know, technology is a really, is a useful tool for solving a lot of problems,

38:06.200 --> 38:10.520
but not every problem is best solved through technology. And I can understand why particularly

38:10.520 --> 38:15.640
technologists, right, would want to put their tools to use in a particular way. But I think

38:15.640 --> 38:20.200
sometimes asking, like, well, you know, holistically, is this the best approach or is this an

38:20.200 --> 38:25.160
approach that could be combined with other approaches, you know, or different forms of expertise,

38:25.160 --> 38:29.640
you know, how will we evaluate what the real effects of these tools are on communities? Like,

38:29.640 --> 38:36.440
that seems super essential to me. And then the other piece that I think is, you know, like, like,

38:36.440 --> 38:41.320
in the case of these truckers, for example, right, like, there's concern recently about, like,

38:41.320 --> 38:47.080
the potential for massive unemployment among truck drivers based on the development of autonomous

38:47.080 --> 38:52.280
vehicles. Now, I think that that's an overstated concern. But, like, what, I mean, we would be telling

38:52.280 --> 38:56.440
sort of a simple and boring story if we focused only on the numbers, right? If we focused only on

38:56.440 --> 39:02.520
something like job loss, I think a more important and nuanced take on it would be to actually

39:03.320 --> 39:07.160
understand kind of like the dignity and quality of life that results when we put some of these

39:07.160 --> 39:11.960
systems in place, right? Which might not be something you can measure in numbers, right? Might

39:11.960 --> 39:15.560
not be something that you can look at in terms of dollars or, you know, jobs loss or something

39:15.560 --> 39:20.200
like that. But actually, like, that's, that ends up being like what it is to live with these

39:20.200 --> 39:25.240
systems, right? To live with data being collected about you or to live with automated decisions being

39:25.240 --> 39:29.720
rendered about you. And for that, I think it really takes like getting into the community,

39:29.720 --> 39:35.080
talking to the people who are affected on the front lines. Like, I just can't see that there's

39:35.080 --> 39:40.440
any replacement for trying to understand problems at that level. And just to be concrete about

39:41.960 --> 39:49.880
this, in the case of the truck drivers that will undoubtedly be impacted by self-driving

39:49.880 --> 39:58.360
trucks, how do you see that impacting them, their dignity, their, the way they work, and the way

39:58.360 --> 40:04.200
they feel about their work? Yeah, so, I mean, yeah, so because of kind of the nature of,

40:05.480 --> 40:09.640
so the nature of work, right, is that work is really complicated, right? And so to say like, oh,

40:09.640 --> 40:14.360
well, now we have self-driving trucks, so we don't need human truckers anymore, is to like really

40:14.360 --> 40:18.360
kind of oversimplify what really anybody's job looks like, right? So I think we will, and there are

40:18.360 --> 40:22.280
a lot of things that self-driving trucks are like very, very far from being able to accomplish

40:22.920 --> 40:27.960
technically, right? So what I think that means, and what a lot of people think that means is that,

40:27.960 --> 40:32.040
you know, it's not that we have like robots that replace people, and then we're done, it's that

40:32.040 --> 40:36.920
like very slowly robots change, you know, people end up working with automated systems, right? And

40:36.920 --> 40:42.200
so it's like the working with that becomes interesting and important. And there are ways that we can

40:42.200 --> 40:46.520
work with machines that, you know, augment both where everybody gets to kind of use their

40:46.520 --> 40:51.160
strengths, right? And it becomes like a really kind of collaborative, you know, supportive,

40:51.160 --> 40:55.320
synergistic relationship. And then there are ways where it doesn't appear that way, right? Where

40:55.320 --> 41:00.120
like the machine becomes kind of like the supervisor of you, right? Or the machine kind of,

41:00.920 --> 41:05.160
you know, in the case of truckers, right? Like actually like has a bunch of insight into your body,

41:05.160 --> 41:08.760
right? Or is seen as sort of intrusive? So like I mentioned, you know, truckers having these

41:08.760 --> 41:13.880
cameras trained on their eyelids, you know, or they're all these systems that will like jolt them

41:13.880 --> 41:19.400
or flash lights in their eyes or, you know, text their partners, like all these different things

41:19.400 --> 41:24.200
that they get tired. And then, you know, you can imagine why that like changes kind of the dignity

41:24.200 --> 41:29.880
in the nature of the job, right? Like being kind of forced to sort of hybridize with these systems

41:29.880 --> 41:33.880
is really different than kind of this kind of mutually supportive environment that we might otherwise

41:33.880 --> 41:37.320
create. And so it all becomes about the details, right? It all becomes about kind of like how we

41:37.320 --> 41:42.680
roll these things out, you know, who's incentives we acknowledge. And I think there's a lot of room

41:42.680 --> 41:47.800
to do it well. And in ways that actually like make work better for people. But it takes like really

41:47.800 --> 41:54.840
careful thought, I think when we when we try to do that. Again, maybe acknowledging my inclination to

41:55.560 --> 41:58.920
kind of framework eyes and analyze this. Because you love it. You love it.

42:00.120 --> 42:05.800
Like I'm thinking about like is there has anyone started working on like the, you know,

42:05.800 --> 42:12.600
principles of these this hybridization hybrid job roles? What does that mean? And how can we think

42:12.600 --> 42:17.240
about that consistently across different types of jobs? Or does that even do you think that even

42:17.240 --> 42:22.200
makes sense? Like are there things that apply equally to the to the truckers and people working

42:22.200 --> 42:26.840
in office environments and people working in industrial environments as these jobs are getting

42:26.840 --> 42:32.680
increasingly hybridized? Or do you think that they're they're all separate? Yeah, it's a good question.

42:32.680 --> 42:36.920
I don't think there are probably consistent answers, but I think there are probably consistent

42:36.920 --> 42:41.400
questions to ask across those different contexts, right? So understanding things like an occupational

42:41.400 --> 42:45.720
culture, the occupational culture of trucking is very different from the occupational culture of

42:45.720 --> 42:51.640
say like office work, right? But that but asking questions about the culture and the values

42:51.640 --> 42:57.000
and the traditions of an occupation seems like indispensable to me, right? Understanding like,

42:57.000 --> 43:01.720
you don't just drop technology in and run away, but like understanding, you know, who are these

43:01.720 --> 43:07.240
people? Why are they here? Like what is important to them? Which is both like, you know, monetary,

43:07.240 --> 43:11.720
right? Like what what, you know, how do they make their money and like how is this going to affect

43:11.720 --> 43:15.560
how they make their money? But also these kind of questions around identity and tradition and

43:15.560 --> 43:20.520
culture, things like that. You know, questions about autonomy, I think are probably important

43:20.520 --> 43:25.400
to ask across contexts. Like we pretty much know, right, that people like having autonomy in their

43:25.400 --> 43:29.800
work, like almost, you know, you know, across lots of different occupations, right? Like people

43:29.800 --> 43:34.760
don't like having things directed so much so that they feel like they're part of a machine.

43:34.760 --> 43:39.160
And so when we build machine human systems, like making sure that people have, you know,

43:39.720 --> 43:45.320
some of that decisional capability feels really important, right? And again, like that'll look

43:45.320 --> 43:50.360
different, like if you're looking at doctors working with robots to do surgery versus a truck driver,

43:50.360 --> 43:55.000
you know, with like kind of with an autonomous supervisor in the truck, but the values are the

43:55.000 --> 44:00.280
same, right? Well, Karen, thank you so much for taking the time to chat with us about this. It's

44:00.280 --> 44:04.520
been a really interesting conversation. Well, thanks. I enjoyed it. Thank you.

44:08.040 --> 44:13.160
All right, everyone. That's our show for today. For more information about today's guest,

44:13.160 --> 44:19.640
or to follow along with AI Platform Volume 2, visit twimmelaii.com slash AI Platforms 2.

44:19.640 --> 44:26.960
Make sure you visit twimmelcon.com for more information or to register for Twimmelcon AI

44:26.960 --> 44:56.800
Platforms. As always, thanks so much for listening and catch you next time.

