1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

4
00:00:35,920 --> 00:00:40,640
to present to you our first ever AI rewind series.

5
00:00:40,640 --> 00:00:45,040
In this series I interview friends of the show for their perspectives on the key developments

6
00:00:45,040 --> 00:00:49,720
of 2018 as well as a look ahead at the year to come.

7
00:00:49,720 --> 00:00:54,320
We'll cover a few key categories this year, namely computer vision, natural language

8
00:00:54,320 --> 00:00:59,200
processing, deep learning, machine learning and reinforcement learning.

9
00:00:59,200 --> 00:01:03,840
Of course we realize that there are many more possible categories than these, that there's

10
00:01:03,840 --> 00:01:08,800
a ton of overlap between these topics and that no single interview could hope to cover

11
00:01:08,800 --> 00:01:12,240
everything important in any of these areas.

12
00:01:12,240 --> 00:01:17,120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

13
00:01:17,120 --> 00:01:26,080
by commenting on the series page at twimbleai.com slash rewind 18.

14
00:01:26,080 --> 00:01:31,640
In this episode of our AI rewind series we introduce a new friend of the show Simon Ocindero,

15
00:01:31,640 --> 00:01:37,200
staff research scientist at DeepMind to discuss trends in deep reinforcement learning in 2018

16
00:01:37,200 --> 00:01:38,520
and beyond.

17
00:01:38,520 --> 00:01:42,600
We've packed a bunch into the show as Simon walks us through many of the important papers

18
00:01:42,600 --> 00:01:48,200
and developments seen this year in areas like imitation learning unsupervised RL, meta-learning

19
00:01:48,200 --> 00:01:49,680
and more.

20
00:01:49,680 --> 00:01:50,680
Enjoy.

21
00:01:50,680 --> 00:01:56,200
Alright everyone, I'm on the line with Simon Ocindero.

22
00:01:56,200 --> 00:01:59,560
Simon is staff research scientist at Google Monde.

23
00:01:59,560 --> 00:02:02,320
Simon, welcome to this week in machine learning and AI.

24
00:02:02,320 --> 00:02:03,320
Hi, sir.

25
00:02:03,320 --> 00:02:04,320
Good to see you.

26
00:02:04,320 --> 00:02:05,320
Awesome.

27
00:02:05,320 --> 00:02:06,320
It is great to have you on the show.

28
00:02:06,320 --> 00:02:15,880
You are joining us for our end of 2018 beginning of 2019 kind of reflections and predictions

29
00:02:15,880 --> 00:02:22,920
series and you're unique in this series and that the other guests that we've featured

30
00:02:22,920 --> 00:02:28,160
in this series are all kind of guests that are returning back to the podcast, but you

31
00:02:28,160 --> 00:02:33,920
are our new guests in this series.

32
00:02:33,920 --> 00:02:37,080
We wanted to catch up with you because we've been trying to get you on the show for a while.

33
00:02:37,080 --> 00:02:42,000
We just saw one another at Nourips and this seemed like a great opportunity.

34
00:02:42,000 --> 00:02:47,760
For our project here, we're going to talk through some of the interesting developments

35
00:02:47,760 --> 00:02:53,080
in your particular area of focus which is deep reinforcement learning.

36
00:02:53,080 --> 00:02:59,280
But you've identified a bunch of papers that caught your eye as being significant this

37
00:02:59,280 --> 00:03:00,280
year.

38
00:03:00,280 --> 00:03:05,600
Yeah, that's right, I guess when you asked me this, I was kind of trying to think of what

39
00:03:05,600 --> 00:03:11,880
was interesting and just the rate of publications at the moment is so high that given that and

40
00:03:11,880 --> 00:03:17,000
the number of subfields that I see developing in deeper all these days, it was kind of

41
00:03:17,000 --> 00:03:18,080
hard to come up with a short list.

42
00:03:18,080 --> 00:03:24,040
So I figured maybe what we do is touch on a couple of high level topic areas and I'll

43
00:03:24,040 --> 00:03:28,320
try and pull out from each of them a couple of the papers that I thought were interesting

44
00:03:28,320 --> 00:03:29,320
or that caught my eye.

45
00:03:29,320 --> 00:03:34,160
Actually, even doing that, there's probably going to be some focus areas that we could touch

46
00:03:34,160 --> 00:03:38,240
on that I don't know if we'll have time to but maybe we'll do that in another show.

47
00:03:38,240 --> 00:03:39,240
Awesome.

48
00:03:39,240 --> 00:03:41,440
So what's the first area you have for us?

49
00:03:41,440 --> 00:03:47,920
So the first step is what I'm going to call grouping together, sort of imitation learning,

50
00:03:47,920 --> 00:03:56,080
learning from demonstration, warm starts and curricula, so kind of this idea that we don't

51
00:03:56,080 --> 00:04:01,160
always need to learn from scratch without any information and in fact, doing that when

52
00:04:01,160 --> 00:04:07,840
rewards are very sparse is kind of difficult and so we can leverage other types of knowledge.

53
00:04:07,840 --> 00:04:13,040
So in particular, maybe demonstrations, a small number of demonstrations from other agents,

54
00:04:13,040 --> 00:04:17,320
some of those might be humans, another way of thinking about this problem is to think

55
00:04:17,320 --> 00:04:24,040
about curricula, so either over problems or model complexity and essentially starting

56
00:04:24,040 --> 00:04:28,880
off doing something simple and then bootstrapping from the solution to a similar problem to a

57
00:04:28,880 --> 00:04:30,760
more complicated one.

58
00:04:30,760 --> 00:04:36,640
With regards to imitation learning, I typically see this in the context of reinforcement

59
00:04:36,640 --> 00:04:41,920
learning, are they linked inseparably or can you do one without the other?

60
00:04:41,920 --> 00:04:48,400
Yeah, I mean, for sure you can do reinforcement learning without imitation and actually I guess

61
00:04:48,400 --> 00:04:52,600
depending on what you mean by imitation, you don't necessarily have to do it in a reinforcement

62
00:04:52,600 --> 00:04:53,600
context.

63
00:04:53,600 --> 00:04:58,240
Are there examples that come to mind of imitation learning outside of the reinforcement learning

64
00:04:58,240 --> 00:04:59,240
context?

65
00:04:59,240 --> 00:05:04,240
I mean, I guess depending on how you think about it, you can actually imagine even some of

66
00:05:04,240 --> 00:05:13,560
the way in which we train some language models is essentially being imitation learning,

67
00:05:13,560 --> 00:05:18,720
the kind of teacher forcing aspect of sequence model learning, you can kind of do that as

68
00:05:18,720 --> 00:05:20,280
a kind of imitation.

69
00:05:20,280 --> 00:05:26,800
All right, so the first paper on your list of important papers in this space comes out

70
00:05:26,800 --> 00:05:33,920
of deep mind is the playing exploration games by watching YouTube paper and actually by

71
00:05:33,920 --> 00:05:40,560
the time this interview is published, my interview with Nando to Freitas, one of the authors

72
00:05:40,560 --> 00:05:44,040
of that paper from Nureps, will have been published.

73
00:05:44,040 --> 00:05:47,760
So folks can take a listen to that one.

74
00:05:47,760 --> 00:05:48,760
Great.

75
00:05:48,760 --> 00:05:53,200
Yeah, what were the highlights of that paper for you?

76
00:05:53,200 --> 00:05:58,000
I guess a couple of things, and Nando's already said about this, I'll be relatively brief,

77
00:05:58,000 --> 00:06:03,600
but it's kind of nice that we have observational demonstration data rather than necessarily

78
00:06:03,600 --> 00:06:04,600
providing.

79
00:06:04,600 --> 00:06:08,400
So I guess there's a couple of ways that you could give an agent demonstrations, one is

80
00:06:08,400 --> 00:06:12,840
just letting it watch another agent and observing the world, the other is literally telling

81
00:06:12,840 --> 00:06:18,320
it action sequences that another agent have done that have been successful.

82
00:06:18,320 --> 00:06:23,400
So this is a kind of nice example in that it's purely observational, and so the hope is

83
00:06:23,400 --> 00:06:28,800
that by generalizing this technique, we really could take it from, in this case, it was

84
00:06:28,800 --> 00:06:32,880
looking at a video game that you can imagine wanting to scale this up and just having an

85
00:06:32,880 --> 00:06:38,160
agent watch, people doing stuff in YouTube and learn to do stuff in the real world.

86
00:06:38,160 --> 00:06:41,480
So it's kind of got a lot of potential in that direction.

87
00:06:41,480 --> 00:06:42,480
Okay.

88
00:06:42,480 --> 00:06:43,480
Cool.

89
00:06:43,480 --> 00:06:46,000
How about the next paper on your list?

90
00:06:46,000 --> 00:06:52,200
So the other one was another deep mind paper and similar kind of idea, so this is the

91
00:06:52,200 --> 00:06:56,480
observant look further, achieving consistent performance on Atari.

92
00:06:56,480 --> 00:07:05,040
And again, it's using demonstration, in this case, they do get to see the actions, but

93
00:07:05,040 --> 00:07:10,360
they basically learn to get good performance on some of these hard-expression gains with

94
00:07:10,360 --> 00:07:13,640
just a single demonstration trajectory.

95
00:07:13,640 --> 00:07:19,680
And the kind of idea there is that you basically, in your replay buffer, you have the experience

96
00:07:19,680 --> 00:07:25,720
of the agents generated, but you also get to populate that with experience that a demonstrators

97
00:07:25,720 --> 00:07:31,400
provided, and it turns out that if you do that and a couple of other tricks in terms of

98
00:07:31,400 --> 00:07:36,760
how you do the training, you can also get extremely good results from a very limited amount

99
00:07:36,760 --> 00:07:38,760
of demonstration data.

100
00:07:38,760 --> 00:07:44,960
So when you say a single demonstration trajectory?

101
00:07:44,960 --> 00:07:50,800
Yeah, so imagine a video game just kind of playing through it once, and that's all you

102
00:07:50,800 --> 00:07:51,800
get to see.

103
00:07:51,800 --> 00:07:57,080
So the reason that's kind of interesting is that, you know, a single, I mean, I guess it

104
00:07:57,080 --> 00:08:01,440
depends on the game, but a single playthrough wouldn't necessarily capture all the variability

105
00:08:01,440 --> 00:08:05,320
that you get when you're playing the game, you know, if you start to do the things a little

106
00:08:05,320 --> 00:08:10,960
differently at the beginning, then things can kind of diverge, but in this game, and I

107
00:08:10,960 --> 00:08:13,840
guess it's true of a lot of environments, you know, if you're starting from scratch and

108
00:08:13,840 --> 00:08:17,920
you don't know anything, then there's lots and lots of things that you could end up doing.

109
00:08:17,920 --> 00:08:22,920
And so even just seeing one demonstration of roughly the right thing to do can be extremely

110
00:08:22,920 --> 00:08:23,920
helpful.

111
00:08:23,920 --> 00:08:28,600
How is the imitation part of this paper approached?

112
00:08:28,600 --> 00:08:37,000
Yeah, so like I said, they're using the building known as Technica DQFD, and so DQ learning

113
00:08:37,000 --> 00:08:38,840
from demonstration.

114
00:08:38,840 --> 00:08:46,040
And so the idea there is you have this replay buffer, which is basically transitions that

115
00:08:46,040 --> 00:08:47,040
you've seen in the world.

116
00:08:47,040 --> 00:08:52,960
So you have your current state, the action you did, where you ended up next, and associated

117
00:08:52,960 --> 00:08:59,560
with rewards, and in regular Q learning, or you would essentially populate that buffer

118
00:08:59,560 --> 00:09:00,920
just with your own experience.

119
00:09:00,920 --> 00:09:04,160
And so when you're starting out, you maybe don't really know how the world works.

120
00:09:04,160 --> 00:09:09,960
And so it's put part of the reason that this is interesting and kind of why highlighting

121
00:09:09,960 --> 00:09:15,240
this as an area is, there's a lot of problems where just getting the agent off the ground

122
00:09:15,240 --> 00:09:19,880
and starting to do something simple, can actually end up being the hardest part, but once

123
00:09:19,880 --> 00:09:23,840
you're starting to see rewards and figuring out how the world works, then learning tends

124
00:09:23,840 --> 00:09:26,200
to progress reasonably.

125
00:09:26,200 --> 00:09:29,560
So in terms of learning curves, a lot of the time on some of these tough environments,

126
00:09:29,560 --> 00:09:34,360
you'll basically see flat lining for a very long time, because the agent is essentially

127
00:09:34,360 --> 00:09:39,600
just randomly exploring until it figures out something interesting, and then it starts

128
00:09:39,600 --> 00:09:41,040
to get a signal that it can learn from.

129
00:09:41,040 --> 00:09:47,800
And so I think that's another reason why just a very small amount of demonstration data

130
00:09:47,800 --> 00:09:54,640
or instruction or just something to kind of kick learning off the ground and end up accelerating

131
00:09:54,640 --> 00:09:59,400
learning and help us learn on problems that otherwise would be very hard to do from scratch.

132
00:09:59,400 --> 00:10:01,280
Were there other papers in the scatigory?

133
00:10:01,280 --> 00:10:05,240
Yeah, I guess there was another one that we had in Europe, so I was actually, so this

134
00:10:05,240 --> 00:10:08,960
was kickstarting deep reinforcement learning.

135
00:10:08,960 --> 00:10:14,480
And that's a slightly different approach, so it's not using demonstrations per se anymore.

136
00:10:14,480 --> 00:10:21,520
The idea is we have a free-training teacher agent, so you could imagine getting that in

137
00:10:21,520 --> 00:10:27,320
a couple of different ways, so maybe that would be a simple, easy-to-train agent that

138
00:10:27,320 --> 00:10:31,560
doesn't necessarily get the best performance, but it's easy to train in a reasonable time

139
00:10:31,560 --> 00:10:36,280
or maybe you have lots of different tasks in an environment you want to do, and you might

140
00:10:36,280 --> 00:10:41,640
be able to have a teacher that specializes on a single task, which is typically easier

141
00:10:41,640 --> 00:10:45,320
than learning a whole bunch of things, but you have a student who wants to be able to learn

142
00:10:45,320 --> 00:10:51,560
to do many different things, and the kind of, the approach we have there is we are able

143
00:10:51,560 --> 00:10:55,840
to have the student experience in the world, and we can also query the teacher in a sense

144
00:10:55,840 --> 00:10:58,240
he asks, what would you do if you were in the situation?

145
00:10:58,240 --> 00:11:05,320
So here we have access to the actions, and the idea is we'd like the student to match

146
00:11:05,320 --> 00:11:09,760
the teacher's behaviour, but we don't just want to end up completely copying the teacher

147
00:11:09,760 --> 00:11:14,880
because there can be no point in that, so we also have a mechanism that essentially

148
00:11:14,880 --> 00:11:19,480
over time allows the student to figure out how much attention it should pay to its teacher

149
00:11:19,480 --> 00:11:25,720
and how much attention it should pay to its own experience, so over time the student

150
00:11:25,720 --> 00:11:29,560
trusts its own experience more and more, listens to the teacher less and less, so you

151
00:11:29,560 --> 00:11:34,000
can end up with a student rather exceeding the performance of the teacher, and we kind

152
00:11:34,000 --> 00:11:40,400
of see this as being useful in a couple of different ways, so particularly right now

153
00:11:40,400 --> 00:11:44,400
in terms of the kind of nuts and bolts of experimental cycles, you often want to try out

154
00:11:44,400 --> 00:11:49,360
a lot of different ideas and architectures, and if for each one of those you are having

155
00:11:49,360 --> 00:11:55,400
to start from scratch each time that can be costly in terms of computation, but also can

156
00:11:55,400 --> 00:12:00,000
just be slow, so we kind of see this as a nice way to shorten experimental cycles if

157
00:12:00,000 --> 00:12:04,200
you're wanting to iterate through different aging architectures pretty quickly, another

158
00:12:04,200 --> 00:12:11,600
benefit that we see is, sometimes if you have a really big agent it's just a lot of

159
00:12:11,600 --> 00:12:16,240
parameters to train, so getting that off the ground initially can be kind of hard, and

160
00:12:16,240 --> 00:12:20,480
so we've actually seen really nice progress taking very simple agents that are quick

161
00:12:20,480 --> 00:12:25,000
to train, but they flatline it, not great performance, but by having that as a teacher

162
00:12:25,000 --> 00:12:30,360
we can then get one of these much, much bigger agents off the ground that then ultimately

163
00:12:30,360 --> 00:12:35,640
gives us much better performance, so it's, yes, as a technique it's something that we're

164
00:12:35,640 --> 00:12:37,440
using quite a lot internally.

165
00:12:37,440 --> 00:12:42,000
One of the contexts that you explained this in is in terms of like multi-task, or at least

166
00:12:42,000 --> 00:12:45,480
an agent trying to learn multi-task, I don't know if it's formerly multi-task learning,

167
00:12:45,480 --> 00:12:51,880
but what's the role of the teacher in that multi-multiple task scenario?

168
00:12:51,880 --> 00:12:58,160
Yes, so in that scenario it would be, often what we'd like is a single agent that can

169
00:12:58,160 --> 00:13:03,520
do lots and lots of things, and that training that transcript can be kind of challenging,

170
00:13:03,520 --> 00:13:07,440
what's often more manageable is you kind of subdivide the problem space into smaller

171
00:13:07,440 --> 00:13:13,800
and smaller chunks, so you are learning to do just one of those tasks, maybe even breaking

172
00:13:13,800 --> 00:13:19,120
a single task into smaller chunks, so learning to do a small part of the overall problem,

173
00:13:19,120 --> 00:13:23,880
you can get a teacher agent that specializes on that little bit of the overall problem

174
00:13:23,880 --> 00:13:29,080
space, and if you then have many different teachers that specialize in different parts between

175
00:13:29,080 --> 00:13:33,040
them that able to do most of the things you care about, but like I say what you'd really

176
00:13:33,040 --> 00:13:38,320
like is a single agent that has competency to do everything in the domain, and so in a

177
00:13:38,320 --> 00:13:42,280
situation like that you could build these simple agents that kind of specialize in parts

178
00:13:42,280 --> 00:13:48,120
of the problem space, and then use those different teachers to then transfer their knowledge

179
00:13:48,120 --> 00:13:53,920
to a student that is learning from all of them, and as I say over time it can figure out

180
00:13:53,920 --> 00:13:58,640
how much attention to pay to the different teachers and get good performance, so in the

181
00:13:58,640 --> 00:14:06,640
paper we have a set of different tasks that we want to do in the 3D DM lab environment,

182
00:14:06,640 --> 00:14:14,920
and so you know they can like navigation kind of tasks, avoidance kind of tasks, gathering

183
00:14:14,920 --> 00:14:19,720
tasks, all sorts of things like that, and we have teacher agents that are specialized

184
00:14:19,720 --> 00:14:24,480
for each one of those, and then we can build, but what we're really interested in, and

185
00:14:24,480 --> 00:14:30,000
this is kind of moving towards general intelligence, we really want one agent that can do lots and

186
00:14:30,000 --> 00:14:34,160
lots of things and can also learn to do new things, and so one way of getting that off

187
00:14:34,160 --> 00:14:40,200
the ground is to have these simpler, less general agents that specialize in particular parts

188
00:14:40,200 --> 00:14:44,640
of the problem space, and then distill their knowledge into the student to get the student

189
00:14:44,640 --> 00:14:46,320
off the ground.

190
00:14:46,320 --> 00:14:54,320
To be clear, so maybe an example, if you say you have, you're trying to train an agent,

191
00:14:54,320 --> 00:15:01,800
and you've got four kind of things that you want it to learn, and you create three of

192
00:15:01,800 --> 00:15:09,360
these teacher agents, are the teacher agents increasing performance in the three categories

193
00:15:09,360 --> 00:15:15,720
for which you have teacher agents, or is the lack of teacher agent allowing the agent

194
00:15:15,720 --> 00:15:20,680
to learn more about the one where you don't have a teacher agent?

195
00:15:20,680 --> 00:15:29,680
In this setting, I guess it's a little orthogonal to that, so in terms of, the teacher should

196
00:15:29,680 --> 00:15:37,360
never be too limiting in the sense that we have this mechanism, in the people are using

197
00:15:37,360 --> 00:15:41,960
population-based training, but you could imagine doing other things that essentially tries

198
00:15:41,960 --> 00:15:45,920
to meta-learn how much attention the page of the teacher.

199
00:15:45,920 --> 00:15:50,040
If the teacher is useful, if it's giving you learning progress on the thing that you

200
00:15:50,040 --> 00:15:55,160
care about, great, as that becomes less and less useful, you pay less and less attention

201
00:15:55,160 --> 00:15:56,160
to the teacher.

202
00:15:56,160 --> 00:16:01,600
So far, in these conversations, what's been interesting is that I could spend the entire

203
00:16:01,600 --> 00:16:07,720
time talking about one thing that kind of pulls me in that direction, but we'll try not

204
00:16:07,720 --> 00:16:12,400
to do that here, and so where there are other papers in this category, do you want to move

205
00:16:12,400 --> 00:16:13,680
on to the next one?

206
00:16:13,680 --> 00:16:19,240
So we had another paper mix and match agent curricula for reinforcement learning, and

207
00:16:19,240 --> 00:16:26,280
there, rather than having a teacher that's kind of pre-trained, it's kind of as if you

208
00:16:26,280 --> 00:16:35,360
have a companion agent that is trained jointly with the one you really care about, so in practice

209
00:16:35,360 --> 00:16:44,720
how we implement that, we have a mixed-your-policy, which is two different policies that we kind

210
00:16:44,720 --> 00:16:50,640
of blend together, and the idea there is maybe a much simpler policy would be easier to train

211
00:16:50,640 --> 00:16:57,160
to imagine something that has far fewer possible actions that it could take, and so often

212
00:16:57,160 --> 00:17:01,200
that's similar to learn, but what we'd really like is an agent that has access to many,

213
00:17:01,200 --> 00:17:02,360
many actions.

214
00:17:02,360 --> 00:17:08,400
One thing you could do is initially train an agent with few actions and then use the behavior

215
00:17:08,400 --> 00:17:12,600
of that agent to train the one that has many, many actions, and in this particular setup,

216
00:17:12,600 --> 00:17:17,760
we do that in a continuous way where the learning is shared between both different types

217
00:17:17,760 --> 00:17:24,280
of policy, and we have a term that essentially encourages the information to flow in a sense

218
00:17:24,280 --> 00:17:30,240
the simpler and faster learning agent to the complex, more solely learning agent, and again,

219
00:17:30,240 --> 00:17:36,040
we have that processes orchestrated, is done automatically with this method population-based

220
00:17:36,040 --> 00:17:42,040
training, which essentially tries a whole bunch of different values for how strongly we're

221
00:17:42,040 --> 00:17:48,200
coupling behavior and preferentially follows the ones that are giving the best empirical

222
00:17:48,200 --> 00:17:49,200
performance.

223
00:17:49,200 --> 00:17:54,600
So the next category that you identified was what?

224
00:17:54,600 --> 00:18:01,720
So this one is, I guess what I call unsupervised RRR, or you could think of it as self-supervision

225
00:18:01,720 --> 00:18:09,720
or different kinds of interesting auxiliary tasks, so I think this is sort of a general

226
00:18:09,720 --> 00:18:16,960
direction that lots of people are interested in, which is the information in a reward signal

227
00:18:16,960 --> 00:18:22,680
probably isn't enough to train the scale of agents that we're interested in in a reasonable

228
00:18:22,680 --> 00:18:23,680
amount of experience.

229
00:18:23,680 --> 00:18:28,920
It's not very information rich, but there actually is a ton of other stuff in the environment

230
00:18:28,920 --> 00:18:31,640
that we can learn from.

231
00:18:31,640 --> 00:18:39,040
And so leveraging that along with a reward signal is something that I think we're going

232
00:18:39,040 --> 00:18:40,040
to see a lot more.

233
00:18:40,040 --> 00:18:48,240
So you've probably heard Jan Lecun talk about the cherry analogy with a cake, so it's kind

234
00:18:48,240 --> 00:18:54,880
of getting at that, but the point is that there's all sorts of things that we can learn

235
00:18:54,880 --> 00:18:57,280
from in the reinforcement learning context.

236
00:18:57,280 --> 00:19:03,720
And so examples of this, not from this year, but previously, would be the unreal paper

237
00:19:03,720 --> 00:19:09,240
where you're essentially doing a whole bunch of auxiliary prediction tasks in addition to

238
00:19:09,240 --> 00:19:11,560
trying to learn the policy and value.

239
00:19:11,560 --> 00:19:14,600
One of the papers from this year that I thought was particularly interesting in that sense

240
00:19:14,600 --> 00:19:21,360
is this one unsupervised control through non-parametric discrimination rewards.

241
00:19:21,360 --> 00:19:30,080
And the basic idea is wanting to kind of learn to discover and achieve goals without

242
00:19:30,080 --> 00:19:31,080
any supervision.

243
00:19:31,080 --> 00:19:36,760
So it appears to be similar to what happens if you have a young kid, you're essentially

244
00:19:36,760 --> 00:19:40,920
just learning what you can control in the world, learning what you can do.

245
00:19:40,920 --> 00:19:46,080
But no one's, you do have some supervision, but a lot of learning is just figuring out,

246
00:19:46,080 --> 00:19:49,680
what can I affect, what can I control in the world, can I set myself arbitrary goals

247
00:19:49,680 --> 00:19:52,080
and then achieve them?

248
00:19:52,080 --> 00:19:54,760
And so it's kind of taking a shot at that problem.

249
00:19:54,760 --> 00:19:56,760
Does that kind of exist?

250
00:19:56,760 --> 00:20:04,680
Yeah, how did they kind of encode that knowledge that they're making not relative to the

251
00:20:04,680 --> 00:20:05,680
loss function?

252
00:20:05,680 --> 00:20:07,680
Yeah, that's a great question, actually.

253
00:20:07,680 --> 00:20:14,240
So they let's learn a loss function, so there's a couple of clever tricks that they do.

254
00:20:14,240 --> 00:20:23,000
So you start out behaving, let's say, randomly, and you know that states that you visited

255
00:20:23,000 --> 00:20:27,160
in that random paper are achievable, or you know, something closer that should be achievable

256
00:20:27,160 --> 00:20:29,040
because you've achieved that before.

257
00:20:29,040 --> 00:20:35,040
So this is drawing on some of the ideas from, there's a paper last year on hindsight experience

258
00:20:35,040 --> 00:20:39,360
replay, which essentially is, you know, maybe I was trying to do one thing.

259
00:20:39,360 --> 00:20:42,320
I didn't necessarily do that, but I did a whole bunch of things.

260
00:20:42,320 --> 00:20:46,520
And so you can learn about the count of factuals.

261
00:20:46,520 --> 00:20:48,560
When you're acting, you do something.

262
00:20:48,560 --> 00:20:56,640
And so learning about what you did is an alternative to learning about what you wanted to do.

263
00:20:56,640 --> 00:20:58,680
So that's kind of one element.

264
00:20:58,680 --> 00:21:06,480
The other element is how you figure out how well you achieve those goals, and they use

265
00:21:06,480 --> 00:21:11,840
an approach that's a little similar to the discriminator in again, so that the basic

266
00:21:11,840 --> 00:21:16,360
idea is, well, yeah, okay, the three components, you have a goal condition policy.

267
00:21:16,360 --> 00:21:22,640
So you have a policy that gets to see its current state, and it also gets given a goal state

268
00:21:22,640 --> 00:21:25,080
that it's trying to achieve.

269
00:21:25,080 --> 00:21:28,880
And under ideal conditions, you kind of plug both those in, and it will get you to the

270
00:21:28,880 --> 00:21:31,280
goal.

271
00:21:31,280 --> 00:21:34,600
But then there's the question that I think, well, you're saying initially, how do you figure

272
00:21:34,600 --> 00:21:36,800
out if you achieve your goal or not?

273
00:21:36,800 --> 00:21:40,640
And that's where this sort of bootstrap teacher comes in.

274
00:21:40,640 --> 00:21:47,000
So the idea is, imagine the goal that I'm trying to achieve, and imagine a whole bunch of

275
00:21:47,000 --> 00:21:48,520
other possible states that I could be in.

276
00:21:48,520 --> 00:21:52,760
So there's the kind of, there's the target and a whole bunch of distractors.

277
00:21:52,760 --> 00:21:59,440
What I'm going to do is try and learn a classifier that distinguishes between those distractors

278
00:21:59,440 --> 00:22:02,520
and the goal that I was trying to achieve.

279
00:22:02,520 --> 00:22:05,680
And the way that that's trained, it's a little subtle.

280
00:22:05,680 --> 00:22:11,280
So there's two things that we can plug in there, because if you've told what I was saying,

281
00:22:11,280 --> 00:22:13,400
you might worry there's a bit of a secularity.

282
00:22:13,400 --> 00:22:19,440
So one of them is to plug in some of those hindsight goals, so given some initial state

283
00:22:19,440 --> 00:22:24,120
and just a random trajectory, there'll be states that I go through.

284
00:22:24,120 --> 00:22:29,000
And so I could, after the fact, say, hey, that state that I ended up passing through,

285
00:22:29,000 --> 00:22:31,200
that wasn't my goal.

286
00:22:31,200 --> 00:22:35,280
And so the nice thing about that is you know that you achieved it because you were literally

287
00:22:35,280 --> 00:22:40,640
in that state, you can use that to kind of get this classifier a little off the ground.

288
00:22:40,640 --> 00:22:46,600
Now other data, the situation between that goal that you reached and these distractors,

289
00:22:46,600 --> 00:22:50,600
but then it's also bootstrapped a little bit, and there you essentially end up saying,

290
00:22:50,600 --> 00:22:53,720
wherever you've got to, when you were following this particular goal, I'm going to treat

291
00:22:53,720 --> 00:22:58,080
that from the point of view of the classifier as if that was the ground truth.

292
00:22:58,080 --> 00:23:00,720
Which sounds like it might be a little unstable, because initially you're not going to be

293
00:23:00,720 --> 00:23:06,080
achieving those goals, but what that ends up doing is it sort of gives you a scoring

294
00:23:06,080 --> 00:23:11,440
function that is kind of smooth in the sort of meaningful space of task achievement over

295
00:23:11,440 --> 00:23:12,440
time.

296
00:23:12,440 --> 00:23:17,440
So, you know, I don't necessarily want to need to be pixel perfect to say that I've achieved

297
00:23:17,440 --> 00:23:21,240
that goal and there might be different ways of achieving the same thing.

298
00:23:21,240 --> 00:23:29,720
And so by plugging the agent's behavior in it in that way, they're able to learn an

299
00:23:29,720 --> 00:23:34,400
intrinsic reward function that is effective at kind of critiquing how well they achieve

300
00:23:34,400 --> 00:23:36,240
these arbitrary self-derived goals.

301
00:23:36,240 --> 00:23:37,240
Yeah, yeah.

302
00:23:37,240 --> 00:23:43,040
So, I think that this kind of area of self-supervision or unsupervised RL is going to be something

303
00:23:43,040 --> 00:23:48,680
we're going to see a lot more of figuring out good ways of how we can set prediction

304
00:23:48,680 --> 00:23:53,560
or control tasks for ourselves in order to expand the knowledge and capabilities of an

305
00:23:53,560 --> 00:23:54,920
agent in a particular environment.

306
00:23:54,920 --> 00:24:02,840
And so, is this general category is it trying to help us overcome the difficulty of creating

307
00:24:02,840 --> 00:24:05,400
loss functions for RL?

308
00:24:05,400 --> 00:24:07,640
Yeah, you could do it.

309
00:24:07,640 --> 00:24:13,000
It's doing that in one way.

310
00:24:13,000 --> 00:24:17,160
So in terms of a lot of problems that we might be interested in, so imagine you know,

311
00:24:17,160 --> 00:24:22,720
a kind of robotic and control problem, we're sort of trying to move away from hand-engineering

312
00:24:22,720 --> 00:24:23,720
reward function.

313
00:24:23,720 --> 00:24:30,400
So, in that setting, you might, what you really care about is the task done or not.

314
00:24:30,400 --> 00:24:35,160
But just training on that is very sparse, so you basically get to know if you've done

315
00:24:35,160 --> 00:24:39,040
the task or not pretty much at the end.

316
00:24:39,040 --> 00:24:42,720
And there might be lots of different ways of achieving that and to the extent that you

317
00:24:42,720 --> 00:24:48,080
try and hand design or reward function, you're also in some sense hand designing a solution

318
00:24:48,080 --> 00:24:50,320
which were kind of trying to get away from it.

319
00:24:50,320 --> 00:24:53,000
If it was easy for us to design solution, then maybe we wouldn't need to learn it in

320
00:24:53,000 --> 00:24:54,000
the first place.

321
00:24:54,000 --> 00:24:55,000
Right.

322
00:24:55,000 --> 00:24:56,000
Right.

323
00:24:56,000 --> 00:25:00,920
So, yeah, this is kind of heading in this direction of how can I build a system that can

324
00:25:00,920 --> 00:25:03,920
learn to control the environment in lots of different interesting ways?

325
00:25:03,920 --> 00:25:09,320
Because if I can do that, then when you give me your problem, if I know how to do all

326
00:25:09,320 --> 00:25:12,680
sorts of things, then essentially when you give me your problem, I just need to figure

327
00:25:12,680 --> 00:25:17,280
out all the many things that I'm capable of, which one is the one that fits your particular

328
00:25:17,280 --> 00:25:18,280
problem?

329
00:25:18,280 --> 00:25:20,080
And so that then becomes a lot easier.

330
00:25:20,080 --> 00:25:21,560
So what's the next category?

331
00:25:21,560 --> 00:25:25,640
So, the next category, learning to learn or meta-learning.

332
00:25:25,640 --> 00:25:36,040
So, this idea of how do we use learning to improve the process of learning itself?

333
00:25:36,040 --> 00:25:43,200
And I'd sort of, this kind of blends in a little bit, especially in the RL setting with

334
00:25:43,200 --> 00:25:49,200
how do we learn to acquire experience that is useful for learning, or to us to be care

335
00:25:49,200 --> 00:25:51,920
about, which is, you know, more or less exploration.

336
00:25:51,920 --> 00:25:57,120
So, I had three papers from this topic that I thought would be good to highlight.

337
00:25:57,120 --> 00:26:06,960
So, the first one is called Meta-Reenforcement Learning Exploration Strategies.

338
00:26:06,960 --> 00:26:09,520
And there's a couple of elements to it.

339
00:26:09,520 --> 00:26:17,920
So, it builds on some meta-learning ideas from previous papers.

340
00:26:17,920 --> 00:26:24,240
So, there's a paper model-agnostic meta-learning, mammal that folks went ahead of where the

341
00:26:24,240 --> 00:26:29,520
idea there is, you're trying to find model parameters such that, given arbitrary new problems,

342
00:26:29,520 --> 00:26:31,520
you can rapidly adapt to solve them.

343
00:26:31,520 --> 00:26:39,000
This one is building on top of that, and you have a policy that has latent variables.

344
00:26:39,000 --> 00:26:46,280
And in this case, those latent variables are going to add, so, episode scale, noise

345
00:26:46,280 --> 00:26:48,200
or stochasticity to your behavior.

346
00:26:48,200 --> 00:26:52,840
But there's other ways you could do that conditioning, the kind of the point is, there's a latent

347
00:26:52,840 --> 00:27:00,480
variable that different settings of that cause your model to, cause your agent to behave differently.

348
00:27:00,480 --> 00:27:06,760
And so, by setting those parameters, you effectively can get different types of exploratory

349
00:27:06,760 --> 00:27:08,760
behavior.

350
00:27:08,760 --> 00:27:19,320
So, they essentially try to learn an initial distribution of these latent variables, such

351
00:27:19,320 --> 00:27:26,840
that for new problems from the domain of interest, if you sampled on that, you quickly produce

352
00:27:26,840 --> 00:27:31,560
behaviors that allow you to gain experience, that allow you to solve those new problems

353
00:27:31,560 --> 00:27:32,560
rapidly.

354
00:27:32,560 --> 00:27:41,280
So, rather than having to have stochasticity at each time step, you get to explore in

355
00:27:41,280 --> 00:27:44,640
a structured way across the scale of an episode.

356
00:27:44,640 --> 00:27:45,640
Okay.

357
00:27:45,640 --> 00:27:47,920
So, that was one of the ones.

358
00:27:47,920 --> 00:27:55,640
Another one, this paper, Evolved Policy Gradients, and there, it's a very similar metal learning

359
00:27:55,640 --> 00:28:03,200
flavor, there the idea is, can we evolve a reward functions or an intrinsic reward functions

360
00:28:03,200 --> 00:28:10,680
such that if I optimize, according to that reward function, I get good performance on the

361
00:28:10,680 --> 00:28:11,680
task that I care about.

362
00:28:11,680 --> 00:28:16,280
So, again, imagine one of these settings where you basically, all I care about is, did

363
00:28:16,280 --> 00:28:18,080
you do the thing that I wanted you to do or not?

364
00:28:18,080 --> 00:28:20,120
Did you complete the task successfully?

365
00:28:20,120 --> 00:28:25,720
And, you know, as I was saying before, hand designing, excuse me, a dancer reward function

366
00:28:25,720 --> 00:28:29,760
that tells you, you know, for each state action transition, how good was that?

367
00:28:29,760 --> 00:28:34,200
It's kind of hard, but maybe that's a function rather than specifying it, maybe we can learn

368
00:28:34,200 --> 00:28:35,200
that function.

369
00:28:35,200 --> 00:28:39,560
And that's what this paper is trying to do, and it's using evolutionary methods to do

370
00:28:39,560 --> 00:28:40,560
that.

371
00:28:40,560 --> 00:28:47,800
So, basically, you have an in-loop where you're using regular policy gradient learning,

372
00:28:47,800 --> 00:28:53,800
an absolute where you're using evolutionary search to figure out what a good function would

373
00:28:53,800 --> 00:28:54,800
be.

374
00:28:54,800 --> 00:29:00,200
So, I plug in a function, you get to train with that function, I see how well you did

375
00:29:00,200 --> 00:29:05,520
training with that reward function, and I'm slowly shifting my search space of functions

376
00:29:05,520 --> 00:29:09,240
towards ones that when I optimize them, give me good performance on the thing that I

377
00:29:09,240 --> 00:29:10,240
really care about.

378
00:29:10,240 --> 00:29:18,360
The evolved policy gradients paper is out of OpenAI and the Meta-reinforcement learning paper

379
00:29:18,360 --> 00:29:20,440
that I remember correctly is out of Berkeley.

380
00:29:20,440 --> 00:29:22,440
That's it, yeah.

381
00:29:22,440 --> 00:29:23,440
Okay.

382
00:29:23,440 --> 00:29:28,040
Yeah, I should think both of them, they might be both the OpenAI Berkeley collaborations,

383
00:29:28,040 --> 00:29:29,040
but.

384
00:29:29,040 --> 00:29:30,040
Oh, really?

385
00:29:30,040 --> 00:29:31,040
Okay.

386
00:29:31,040 --> 00:29:32,040
Yeah.

387
00:29:32,040 --> 00:29:33,040
Okay, interesting.

388
00:29:33,040 --> 00:29:34,040
Yeah.

389
00:29:34,040 --> 00:29:36,240
This, the meta-learning space is a really interesting one that I'm starting to hear a lot

390
00:29:36,240 --> 00:29:37,240
about.

391
00:29:37,240 --> 00:29:41,560
Yeah, yeah, I think there's a ton of interesting potential there.

392
00:29:41,560 --> 00:29:47,040
The other paper I was going to mention in that topic real quick was this one is another

393
00:29:47,040 --> 00:29:54,400
deep-mine paper, Meta-gradient reinforcement learning, and yes, similar themes again in

394
00:29:54,400 --> 00:29:57,760
terms of having two scales of optimization.

395
00:29:57,760 --> 00:30:03,960
Here we're asking ourselves that there are parameters that essentially influence the

396
00:30:03,960 --> 00:30:11,040
return that we're learning on that can affect the learning dynamics, so the discount, how

397
00:30:11,040 --> 00:30:17,400
much I discount rewards into the far future is a property of the algorithm that affects

398
00:30:17,400 --> 00:30:18,400
the learning dynamics.

399
00:30:18,400 --> 00:30:24,440
So even if there is a horizon that I truly care about, it might be the case that I get

400
00:30:24,440 --> 00:30:29,960
better empirical performance by using something that's different from that.

401
00:30:29,960 --> 00:30:37,640
And so this paper is combining regular policy gradient with essentially a second set of

402
00:30:37,640 --> 00:30:44,240
rollouts that try to, again, ask the question, if these meta-parameters of my learning were

403
00:30:44,240 --> 00:30:47,920
different, would I be making better learning progress?

404
00:30:47,920 --> 00:30:55,600
And so it's a way, for instance, of automatically and dynamically tuning things like the lambda

405
00:30:55,600 --> 00:31:04,000
or your discount, moving on, and I'll try and condense this section a little bit since

406
00:31:04,000 --> 00:31:07,960
we're probably going to get tied on time if we kind of go into a lot of detail.

407
00:31:07,960 --> 00:31:12,840
But we've been trying to hold back my many questions.

408
00:31:12,840 --> 00:31:20,640
So yeah, another kind of interesting area is exploration, learning to explore, and dealing

409
00:31:20,640 --> 00:31:30,080
with different types of uncertainty, so I guess this, you can think of two types of uncertainty.

410
00:31:30,080 --> 00:31:34,800
And so, folks like you've been talking a lot about, sort of, epistemic versus alliotoric

411
00:31:34,800 --> 00:31:35,800
uncertainty.

412
00:31:35,800 --> 00:31:39,400
So epistemic uncertainty is basically represents your lack of knowledge or data about

413
00:31:39,400 --> 00:31:40,400
the world.

414
00:31:40,400 --> 00:31:42,400
So it's like uncertainty in your model parameters.

415
00:31:42,400 --> 00:31:47,800
So whereas alliotoric, you can kind of think of that as like fundamental randomness in

416
00:31:47,800 --> 00:31:52,560
the world, so stuff that you just can't predict no matter how much data you see.

417
00:31:52,560 --> 00:31:57,520
And maybe that's because it's too many random, or I guess other people might argue that there's

418
00:31:57,520 --> 00:32:00,640
the things that are just kind of outside the capability of a model class, but it kind

419
00:32:00,640 --> 00:32:02,480
of boils down to the same thing.

420
00:32:02,480 --> 00:32:08,240
So yeah, there's a couple of papers that are closely related, actually, that touch

421
00:32:08,240 --> 00:32:09,240
on this.

422
00:32:09,240 --> 00:32:16,600
So there's a really nice paper at New York's in Osborne, John Aslanneeds and Albon Kissiro,

423
00:32:16,600 --> 00:32:21,080
customized prior functions for the DBRL.

424
00:32:21,080 --> 00:32:25,320
And there's a couple of really nice tricks in what they're doing.

425
00:32:25,320 --> 00:32:29,640
This is also kind of building a prior work that I've seen reused in a couple of different

426
00:32:29,640 --> 00:32:30,640
places.

427
00:32:30,640 --> 00:32:37,120
So this notion that we can use a sort of bootstrap ensemble of functions to try and represent

428
00:32:37,120 --> 00:32:38,120
uncertainty.

429
00:32:38,120 --> 00:32:45,640
So that's basically just rather than having a single function, if I train lots of copies

430
00:32:45,640 --> 00:32:52,720
of a function with different visualizations, and with, in this case, different prize,

431
00:32:52,720 --> 00:32:57,960
which I'll touch on briefly in a second, I can then look at to what degree the different

432
00:32:57,960 --> 00:33:01,200
models in the ensemble agree or disagree.

433
00:33:01,200 --> 00:33:04,840
So if there's a lot of variance in the models predictions, then in some sense that's a reflection

434
00:33:04,840 --> 00:33:05,840
of uncertainty.

435
00:33:05,840 --> 00:33:13,800
And it turns out you can kind of make that connection tight on the certain conditions

436
00:33:13,800 --> 00:33:16,680
and in general, it seems like a good heuristic.

437
00:33:16,680 --> 00:33:24,640
So the idea here is we're going to do that with our Q functions and then build on top

438
00:33:24,640 --> 00:33:31,080
of that for learning the notion of the randomised prior, essentially boils down to, I'm going

439
00:33:31,080 --> 00:33:36,040
to have a prior distribution over the parameters of our function class.

440
00:33:36,040 --> 00:33:39,440
And for each member of the ensemble, I'm going to sample from that prior and instead

441
00:33:39,440 --> 00:33:44,840
of effectively doing weight decay towards zero as a kind of regularization, I'm going

442
00:33:44,840 --> 00:33:51,560
to do weight decay towards those initially sampled parameters.

443
00:33:51,560 --> 00:33:57,120
You can kind of think about as starting off with some random function from your prior

444
00:33:57,120 --> 00:34:01,480
beliefs, and then learning a correction function top of that to model what you actually

445
00:34:01,480 --> 00:34:02,480
see in the world.

446
00:34:02,480 --> 00:34:08,320
And so it turns out, if you do that and you don't look the right way, you can basically

447
00:34:08,320 --> 00:34:13,200
sample when you're acting, you can sample a function from your ensemble, act according

448
00:34:13,200 --> 00:34:18,360
to that gather experience, participants in a buffer, bootstrap, update all the different

449
00:34:18,360 --> 00:34:20,160
members of the ensemble.

450
00:34:20,160 --> 00:34:28,440
And it deals with that uncertainty about the world in the right way that leads to good

451
00:34:28,440 --> 00:34:30,960
exploratory behaviour.

452
00:34:30,960 --> 00:34:32,840
So yeah, I thought that was a really cool paper.

453
00:34:32,840 --> 00:34:39,800
There was another paper, I think OpenAI and you'd most of Edinburgh folks, so Yoruba,

454
00:34:39,800 --> 00:34:46,200
Harrison Edward, David Storky, and all the claim of exploration by random network distillation

455
00:34:46,200 --> 00:34:53,280
that take a similar sort or the, you can actually draw a fairly tight connection between what

456
00:34:53,280 --> 00:35:00,520
they do and the paper I just described, although it's maybe not so obvious immediately.

457
00:35:00,520 --> 00:35:06,200
What they're doing is they're coming up with a function that, so you have a random

458
00:35:06,200 --> 00:35:10,760
function, and what you're going to try and do is predict the outputs of that random

459
00:35:10,760 --> 00:35:14,640
function, using a function that you're learning, and you're going to use your prediction error

460
00:35:14,640 --> 00:35:19,840
as a sign of unfamiliarity, and then you get to plug that in as an intrinsic award.

461
00:35:19,840 --> 00:35:26,160
So the idea being, you know, if you're in discrete states, then you can use some kind

462
00:35:26,160 --> 00:35:30,800
of account-based measure of how unfamiliar you are in a certain part of the world, and

463
00:35:30,800 --> 00:35:35,320
kind of give bonuses for exploring new places, that gets harder in much bigger statespaces

464
00:35:35,320 --> 00:35:38,000
where you can't enumerate everything.

465
00:35:38,000 --> 00:35:42,560
And so this is another way of approaching that.

466
00:35:42,560 --> 00:35:48,000
How do I measure how uncertain I am about this part of state space?

467
00:35:48,000 --> 00:35:54,200
And so yeah, essentially what you do, you have this random function, you're trying to

468
00:35:54,200 --> 00:35:58,560
learn something that predicts the outputs of that random function when applied to the

469
00:35:58,560 --> 00:36:02,360
state, and your prediction error, you're going to use as an intrinsic award.

470
00:36:02,360 --> 00:36:09,760
So places that you've visited lots and lots, you kind of know what the random function

471
00:36:09,760 --> 00:36:13,240
is going to behave there, and so you should have relatively low prediction error, places

472
00:36:13,240 --> 00:36:17,520
that you haven't visited that much, and you're going to have a higher prediction error,

473
00:36:17,520 --> 00:36:21,840
and so you'll be incentivized to go find those and explore those more.

474
00:36:21,840 --> 00:36:29,920
And so they use that approach on Montezuma, which is this hard expression game, and they

475
00:36:29,920 --> 00:36:32,080
get some really nice results using that.

476
00:36:32,080 --> 00:36:40,160
Is Montezuma still considered to be, you know, unplayable, you know, with high performance

477
00:36:40,160 --> 00:36:47,520
by RL agents, or have we kind of cracked that with this and other recent agents?

478
00:36:47,520 --> 00:36:50,520
Yeah, I mean, it's still pretty challenging.

479
00:36:50,520 --> 00:36:57,680
Yeah, with a person like this and other recent agents, I think in this paper, they get super

480
00:36:57,680 --> 00:36:59,480
human performance.

481
00:36:59,480 --> 00:37:00,480
Okay.

482
00:37:00,480 --> 00:37:01,480
Yeah.

483
00:37:01,480 --> 00:37:02,480
Okay, interesting.

484
00:37:02,480 --> 00:37:11,640
So these are always to kind of structure the structure, the exploration space, and

485
00:37:11,640 --> 00:37:18,040
kind of create different types of ensembles and train across those ensembles.

486
00:37:18,040 --> 00:37:19,040
That's right.

487
00:37:19,040 --> 00:37:21,960
Okay, very cool.

488
00:37:21,960 --> 00:37:26,640
And then the other one that I guess I'll just mention briefly that is sort of in the category,

489
00:37:26,640 --> 00:37:33,080
but as long as this is dealing with the other type of uncertainty, was a paper implicit

490
00:37:33,080 --> 00:37:41,000
quantile networks for distribution RL that was, will that be Geogos Grobski, David Silver

491
00:37:41,000 --> 00:37:47,480
and Ren Yunus, and it's sort of building on some of these previous ideas in distribution

492
00:37:47,480 --> 00:37:53,840
RL were the kind of the ideas, rather than just predicting the expected return, I'm going

493
00:37:53,840 --> 00:37:58,280
to try and predict the distribution of possible returns.

494
00:37:58,280 --> 00:38:00,760
And so that could happen.

495
00:38:00,760 --> 00:38:05,480
So if there is sort of intrinsic uncertainty in the environment, then even if I'm following

496
00:38:05,480 --> 00:38:10,920
the same policy, how the world turns out could end up having variability in it, and rather

497
00:38:10,920 --> 00:38:14,960
than just kind of model the mean of that, you can also try and capture properties of the

498
00:38:14,960 --> 00:38:15,960
distribution.

499
00:38:15,960 --> 00:38:23,080
That's potentially interesting for two different reasons, one, it's a richer prediction problem,

500
00:38:23,080 --> 00:38:26,800
so you can sort of think of predicting that distribution as helping in the same way

501
00:38:26,800 --> 00:38:29,080
that auxiliary tasks do.

502
00:38:29,080 --> 00:38:35,760
And then the other is having those sorts of estimates are themselves useful in some

503
00:38:35,760 --> 00:38:36,760
cases.

504
00:38:36,760 --> 00:38:41,200
So, you know, if you want to be risk sensitive for instance, then you might prefer something

505
00:38:41,200 --> 00:38:48,400
that even though the mean might be higher, you might prefer a policy that has a lower

506
00:38:48,400 --> 00:38:54,800
mean, but also a lower variance, so you kind of avoid worse but rare outcomes.

507
00:38:54,800 --> 00:38:58,560
So, that was the one for that one.

508
00:38:58,560 --> 00:39:04,560
The other area that I thought I'd mention briefly, and again, I think this is an area

509
00:39:04,560 --> 00:39:10,000
that we'll see more and more interesting developments over the coming years, is in model-based

510
00:39:10,000 --> 00:39:15,240
reinforcement learning, so this was another one from New York's this month, sample-efficient

511
00:39:15,240 --> 00:39:21,320
reinforcement learning with stochastic ensemble value expansion, and they have an algorithmic

512
00:39:21,320 --> 00:39:26,200
approach that they get the acronym Steve out of.

513
00:39:26,200 --> 00:39:32,880
And that was, I think, mostly Google brain folk, so Jake and Bookmoon don't know, I have

514
00:39:32,880 --> 00:39:39,080
to Joyce Chalker, Eugene Bervdo, and Holly Lakeley, and it's another one actually where

515
00:39:39,080 --> 00:39:47,480
they're using this trick of using a bootstrapped ensemble to help us capture model uncertainty.

516
00:39:47,480 --> 00:39:51,960
And here, they're actually using it in a model of the environment.

517
00:39:51,960 --> 00:39:56,840
So, in addition to learning our policy and value, we're also going to be trying to learn

518
00:39:56,840 --> 00:40:05,400
about how the world works, so given a particular state in an action, what is going to be my,

519
00:40:05,400 --> 00:40:09,360
what state will I transition into, and what will be the associated award?

520
00:40:09,360 --> 00:40:15,320
And if you have a really good model of the environment, then you can learn much more quickly

521
00:40:15,320 --> 00:40:22,080
because you can not only use the experience that you gather in the world, but you can also

522
00:40:22,080 --> 00:40:26,800
essentially simulate in your model what would happen if you did different things.

523
00:40:26,800 --> 00:40:32,440
But the problem, or one of the problems with that, is that if your model is inaccurate,

524
00:40:32,440 --> 00:40:37,640
then you can run into all sorts of trouble, and so what they do in this paper, they,

525
00:40:37,640 --> 00:40:40,720
because they're learning this model with an uncertainty estimate, they essentially

526
00:40:40,720 --> 00:40:48,720
are able to use the measure of model certainty to figure out how far to trust a model, their

527
00:40:48,720 --> 00:40:49,720
environment model.

528
00:40:49,720 --> 00:40:57,200
So, you essentially roll out your internal environment model to the point where you're essentially

529
00:40:57,200 --> 00:41:03,800
waiting how much you trust your model at different prediction depths into the future by how

530
00:41:03,800 --> 00:41:09,160
much uncertainty there is associated with that, and then at some point you end up bootstrapping.

531
00:41:09,160 --> 00:41:14,600
So, it's kind of a neat way of combining model based and model free RL.

532
00:41:14,600 --> 00:41:21,040
So, the other thing that, or one other area, and this is probably the last one that we'll

533
00:41:21,040 --> 00:41:26,000
touch on for today, I'll maybe just kind of be shout out to some of the other ones that

534
00:41:26,000 --> 00:41:29,400
we probably wouldn't have as much time to talk about.

535
00:41:29,400 --> 00:41:36,920
And that's this kind of trend of seeing sort of, ways in which DPRRL are being scaled

536
00:41:36,920 --> 00:41:45,200
up, so moving to kind of like these very large models, lots and lots of data distributed

537
00:41:45,200 --> 00:41:46,200
training systems.

538
00:41:46,200 --> 00:41:52,600
So, there's a couple of papers from this year that are doing that, so there was three,

539
00:41:52,600 --> 00:42:00,480
or there was a comment to mind as a three from demide and one from OpenAI, so we had this

540
00:42:00,480 --> 00:42:05,760
paper importance weighted at learner architectures in Paula, and so the set up there is, you

541
00:42:05,760 --> 00:42:12,760
have lots and lots of actors gathering experience, they send that experience to a smaller collection

542
00:42:12,760 --> 00:42:18,680
of learners, but even the learners can be distributed, and one of the kind of subtleties

543
00:42:18,680 --> 00:42:24,520
of that is, in that kind of set up, the experience that comes from the actors might be a little

544
00:42:24,520 --> 00:42:28,920
allowed of data, so there's a little bit of op-policy correction, but it essentially

545
00:42:28,920 --> 00:42:36,800
allows you to use extremely large batches and very big models, and still kind of get

546
00:42:36,800 --> 00:42:40,200
through a reasonable number of model updates in a short space of time, so that's kind

547
00:42:40,200 --> 00:42:42,480
of one direction for scaling.

548
00:42:42,480 --> 00:42:47,760
The others, I'll just mention these, I guess briefly, so there was a paper distributed

549
00:42:47,760 --> 00:42:55,560
prioritized experience replay, and that's essentially a kind of value learning analog of the

550
00:42:55,560 --> 00:43:00,040
impulsify I just mentioned, so again lots of active generating experience coming into

551
00:43:00,040 --> 00:43:06,320
a shared experience, a replay, and then learners pulling off on top of that.

552
00:43:06,320 --> 00:43:14,120
Another similar paper had an approach, D4PG, so there was all demide, and then the OpenAI

553
00:43:14,120 --> 00:43:19,480
data, I think there's not a paper on it, I'm not sure, but they kind of detail it in

554
00:43:19,480 --> 00:43:20,640
their blog post.

555
00:43:20,640 --> 00:43:26,800
They have a similar scheme, I think, to Impala where they have essentially lots of distributed

556
00:43:26,800 --> 00:43:35,800
workers generating experience, and then a small number of beefy large batch learners pulling

557
00:43:35,800 --> 00:43:40,040
from experience from the actors and generating parameter updates, and there's a couple of

558
00:43:40,040 --> 00:43:43,440
things there, as I said one, it sort of like allows you to kind of crunch through a lot

559
00:43:43,440 --> 00:43:48,280
more experience, and in some ways it's kind of just by scaling up in that way, it's able

560
00:43:48,280 --> 00:43:55,680
to address some of the inefficiencies of RL, so earlier we were talking a little bit

561
00:43:55,680 --> 00:44:00,600
about wanting to kind of pull as many different learning signals as we can out of the environment,

562
00:44:00,600 --> 00:44:06,160
particularly if reward is sparse or explosion is difficult, and there's sort of lots of

563
00:44:06,160 --> 00:44:10,160
different ways of tackling that, some are kind of more algorithmic, but also just by

564
00:44:10,160 --> 00:44:15,120
scaling things up and pushing a lot more data through, that's another way we're addressing

565
00:44:15,120 --> 00:44:20,240
that, and it also helps some of the kind of stability issues from the variants of the

566
00:44:20,240 --> 00:44:23,600
learning signals that you get a lot of these environments, so again I think that's another

567
00:44:23,600 --> 00:44:27,600
trend that we'll see just kind of being built on more and more, and you know, so it's

568
00:44:27,600 --> 00:44:32,200
somehow in computer vision kind of things have been scaling up and up over the past like

569
00:44:32,200 --> 00:44:33,200
five or six years.

570
00:44:33,200 --> 00:44:37,760
So yeah, maybe if I'll just mention some kind of other interesting areas that we work

571
00:44:37,760 --> 00:44:42,720
up time to dive into, but there's been some kind of interesting developments in hierarchy

572
00:44:42,720 --> 00:44:49,720
car, RL, multi-agent, there's been some neat work on sort of leveraging different types

573
00:44:49,720 --> 00:44:57,120
of memories in agents, and also just the kind of folding in of the sort of broader advances

574
00:44:57,120 --> 00:45:03,600
from machine learning in general in terms of better ways of doing representation learning,

575
00:45:03,600 --> 00:45:08,000
kind of some modeling, all those other kind of techniques, kind of feeding into DPRL.

576
00:45:08,000 --> 00:45:14,960
So just a quick review of the topics that we did cover here, imitation learning, unsupervised

577
00:45:14,960 --> 00:45:22,640
RL, meta-learning, exploration, model-based RL, and scaling up, and those are just a

578
00:45:22,640 --> 00:45:27,160
few of the ones that you were able to think of.

579
00:45:27,160 --> 00:45:34,240
So what would you say about the kind of the rate of growth of RL as a field?

580
00:45:34,240 --> 00:45:36,240
Yeah, super exciting.

581
00:45:36,240 --> 00:45:46,000
I mean, there's a lot of difficult problems that we don't yet have good ways to solve,

582
00:45:46,000 --> 00:45:50,480
but there's a ton of creative solutions coming out.

583
00:45:50,480 --> 00:46:00,720
Beyond papers, did you, did any other kind of broader developments in open source or on

584
00:46:00,720 --> 00:46:05,040
the commercial side, anything jump out at you?

585
00:46:05,040 --> 00:46:06,040
Yeah, couple.

586
00:46:06,040 --> 00:46:14,320
So I guess the commercial side, I've tracked a little less of late in terms of open source.

587
00:46:14,320 --> 00:46:21,320
There's a kind of background in terms of like the kind of continued development, both sort

588
00:46:21,320 --> 00:46:23,320
of like the TensorFlow and PyTorch frameworks in the sporting ecosystems.

589
00:46:23,320 --> 00:46:27,600
There've been a couple of things that there that I thought were pretty nice.

590
00:46:27,600 --> 00:46:35,760
So these like TF Hub, kind of having a kind of repository for models, and now they're

591
00:46:35,760 --> 00:46:37,760
kind of similar efforts.

592
00:46:37,760 --> 00:46:42,360
So there's a couple of frameworks that I saw at Neuros that I like.

593
00:46:42,360 --> 00:46:50,960
So there's dopamine, which is out of, I think, Google Brain Montreal, lucid, such a dopamine

594
00:46:50,960 --> 00:46:56,560
is a kind of a nice way of making some of these, yeah, with, with definitely lucid, and

595
00:46:56,560 --> 00:47:01,840
then there's also something from Uber Model 2, the kind of combination of those, it's kind

596
00:47:01,840 --> 00:47:08,200
of nice and that I think it'll maybe open up some opportunities to kind of play with

597
00:47:08,200 --> 00:47:10,920
deep RL with a lower entry bar.

598
00:47:10,920 --> 00:47:16,320
So with the model zoo they essentially have, I think, three or four different flavors

599
00:47:16,320 --> 00:47:22,200
of agent, pre-trained on all the games in the Atari Suite that you can just essentially

600
00:47:22,200 --> 00:47:28,480
download, and lucid is this kind of visualization framework.

601
00:47:28,480 --> 00:47:33,840
And so with the two of those, you can start to play with some of these pre-trained agents

602
00:47:33,840 --> 00:47:38,360
without necessarily having to have the resources to train them yourselves, which I think

603
00:47:38,360 --> 00:47:44,000
is probably something that will be helpful for new people wanting to get into the field,

604
00:47:44,000 --> 00:47:50,360
to kind of get a bit of a warm start, and then also, I think in general releasing pre-trained

605
00:47:50,360 --> 00:47:56,840
agents that people can build off of and poke is a nice trend, similar to some of the stuff

606
00:47:56,840 --> 00:48:03,480
we saw, again, sort of looking back at computer vision with making some of the pre-trained models

607
00:48:03,480 --> 00:48:07,760
on the ImageNet available that helped a lot of folks kind of get a foothold there, and

608
00:48:07,760 --> 00:48:11,520
then also then accelerated the progress in that field.

609
00:48:11,520 --> 00:48:19,920
With these tools and with RL models in general are the same kinds of things possible, like

610
00:48:19,920 --> 00:48:24,840
kind of taking off the last few layers of a network and fine-tuning, that same kind

611
00:48:24,840 --> 00:48:26,320
of transfer learning.

612
00:48:26,320 --> 00:48:34,240
I guess it would depend on the agent, not so much in what's available right now, the

613
00:48:34,240 --> 00:48:39,560
transfer between different Atari games, for instance, isn't huge, but just in terms of

614
00:48:39,560 --> 00:48:42,120
having something that can get you started.

615
00:48:42,120 --> 00:48:47,560
For instance, the kickstarting stuff that we were talking about earlier in the conversation,

616
00:48:47,560 --> 00:48:53,680
if you have pre-trained agents that have some decent behavior that can dramatically shorten

617
00:48:53,680 --> 00:48:58,720
your experience cycle time if you want to play with your own modifications of those architectures,

618
00:48:58,720 --> 00:49:04,760
eventually you might end up wanting to not use that pre-training to kind of get a clear

619
00:49:04,760 --> 00:49:09,320
look at innovations that you're looking at, but if you don't necessarily have access

620
00:49:09,320 --> 00:49:15,320
to a ton of compute resources, then leveraging things that are pre-trained is a really nice

621
00:49:15,320 --> 00:49:17,120
way of making it more accessible.

622
00:49:17,120 --> 00:49:18,120
Okay.

623
00:49:18,120 --> 00:49:23,640
So, no direct transfer in the way we might see with CNNs, but you can use these agents

624
00:49:23,640 --> 00:49:25,560
to kind of get things kicked off.

625
00:49:25,560 --> 00:49:32,560
Yeah, I think so, I guess, transfer in RL is an interesting conversation that we could

626
00:49:32,560 --> 00:49:38,840
have on the side, but yeah, it would depend on what task you want to transfer to, and

627
00:49:38,840 --> 00:49:43,480
there's a lot of subtleties there, so even things like training and simulation and deploying

628
00:49:43,480 --> 00:49:47,960
in the real world, you see some transfer there, so I guess that's one place where you could

629
00:49:47,960 --> 00:49:54,800
kind of think of that as being pre-training and fine-tuning, but yeah, I kind of think

630
00:49:54,800 --> 00:50:00,800
of that as a little differently than like I said, or at least in its current state, the

631
00:50:00,800 --> 00:50:04,920
model zero is just focused on Atari, so it's more kind of entry level, but I think that's

632
00:50:04,920 --> 00:50:09,120
probably still something that folks entering the field will find useful.

633
00:50:09,120 --> 00:50:10,120
Okay.

634
00:50:10,120 --> 00:50:11,280
Cool.

635
00:50:11,280 --> 00:50:19,600
And so, given all that we've seen happen in 2018, what are your kind of thoughts and

636
00:50:19,600 --> 00:50:25,440
predictions on what we'll see in 2019 and the near future beyond that?

637
00:50:25,440 --> 00:50:27,240
Yeah, prediction that was tough.

638
00:50:27,240 --> 00:50:37,120
I think a lot of the areas that I highlighted to me feel like they're particularly active

639
00:50:37,120 --> 00:50:43,520
frontiers, so yeah, I think we'll see more folks trying to come up with different ways

640
00:50:43,520 --> 00:50:51,000
of doing agent-based RL with more self-supervision, I think the kind of trend, sort of meta-learning,

641
00:50:51,000 --> 00:50:56,800
learning to learn, transfer learning, continual learning, so one agent learnings to do many,

642
00:50:56,800 --> 00:50:59,400
many different things through a lifetime.

643
00:50:59,400 --> 00:51:05,960
I trends will see, yeah, likewise, model-based RL, I feel like models have probably been a

644
00:51:05,960 --> 00:51:12,400
little less prominent in recent years, sorry, like in very models, definitely have been

645
00:51:12,400 --> 00:51:18,480
some papers that look to that, but I feel like that's an interesting area for growth,

646
00:51:18,480 --> 00:51:21,520
so yeah, those are a couple that come to mind.

647
00:51:21,520 --> 00:51:25,200
Any final thoughts before we wrap up?

648
00:51:25,200 --> 00:51:31,760
Yeah, I think a super exciting time in the field is kind of tons of groups, both from

649
00:51:31,760 --> 00:51:37,040
academia and industry that are doing really great work, and so yeah, it's a super exciting

650
00:51:37,040 --> 00:51:42,280
time to be doing research in the field, and then also, I guess, through the broader machine

651
00:51:42,280 --> 00:51:46,960
learning community, there's a lot of great efforts in terms of just working to expand

652
00:51:46,960 --> 00:51:51,200
that the pool of folks who can participate in AI research and who can kind of contribute

653
00:51:51,200 --> 00:51:54,760
to some of these problems that we're looking at, so yeah, I'm super optimistic for the

654
00:51:54,760 --> 00:51:55,760
future.

655
00:51:55,760 --> 00:52:00,440
Well Simon, thanks so much for taking the time to chat with us about RL, there's a ton

656
00:52:00,440 --> 00:52:06,720
of stuff here, and we'll try to get links to the various papers that you mentioned in

657
00:52:06,720 --> 00:52:08,760
the show notes, but thank you.

658
00:52:08,760 --> 00:52:17,320
All right, everyone, that's our show for today.

659
00:52:17,320 --> 00:52:22,920
For more information on Simon or any of the topics covered in this show, visit twimlai.com

660
00:52:22,920 --> 00:52:26,000
slash talk slash 217.

661
00:52:26,000 --> 00:52:33,960
You can also follow along with our AI rewind 2018 series at twimlai.com slash rewind 18.

662
00:52:33,960 --> 00:52:38,640
As always, thanks so much for listening and catch you next time.

663
00:52:38,640 --> 00:52:39,640
Happy Holidays.

