WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.560
I'm your host Sam Charrington.

00:23.560 --> 00:28.440
This week on the podcast we're featuring a series of conversations from the AWS re-invent

00:28.440 --> 00:30.280
conference in Las Vegas.

00:30.280 --> 00:34.680
I had a great time at this event, getting caught up on the new machine learning and AI products

00:34.680 --> 00:38.000
and services offered by AWS and its partners.

00:38.000 --> 00:42.160
If you missed the news coming out of re-invent and want to know more about what one of the

00:42.160 --> 00:47.360
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble

00:47.360 --> 00:52.540
Talk number 83, which was a roundtable discussion I held with Dave McCrory and Lawrence

00:52.540 --> 00:53.540
Chung.

00:53.540 --> 00:59.860
We cover all of AWS's most important news, including the new SageMaker, DeepLens, Recognition

00:59.860 --> 01:06.980
Video, Transcription Service, Alexa for Business, Greengrass ML inference and more.

01:06.980 --> 01:13.660
In this episode, I'll be speaking with Nikita Shamganov, co-founder and CEO of MemSQL,

01:13.660 --> 01:18.940
a company offering a distributed memory optimized data warehouse of the same name.

01:18.940 --> 01:23.180
Nikita and I take a deep dive into some of the features of their recently released 6.0

01:23.180 --> 01:28.540
version, which supports built-in vector operations to enable machine learning use cases like

01:28.540 --> 01:34.660
real-time image recognition, visual search and predictive analytics for IoT.

01:34.660 --> 01:38.500
We also discuss how to architect enterprise machine learning solutions around the data

01:38.500 --> 01:42.420
warehouse by including components like data lakes and spark.

01:42.420 --> 01:47.340
Finally, we touch on some of the performance advantages that MemSQL is seen by implementing

01:47.340 --> 01:54.660
vector operations using Intel's latest AVX2 and AVX512 instruction sets.

01:54.660 --> 01:58.900
And speaking of Intel, I'd like to thank our good friends over at Intel Nirvana for

01:58.900 --> 02:02.980
their sponsorship of this podcast and our reinvent series.

02:02.980 --> 02:07.980
One of the big announcements from reinvent this year was the release of Amazon DeepLens,

02:07.980 --> 02:13.380
a fully programmable deep learning enabled wireless video camera designed to help developers

02:13.380 --> 02:17.980
learn an experiment with AI in the cloud and at the edge.

02:17.980 --> 02:24.900
DeepLens is powered by an Intel Atom X5 processor, which delivers up to 100 gigaflops of processing

02:24.900 --> 02:27.820
power to onboard applications.

02:27.820 --> 02:31.580
To learn more about DeepLens and the other interesting things Intel has been up to in

02:31.580 --> 02:35.940
the AI space, make sure to check out intelnervana.com.

02:35.940 --> 02:38.740
Okay, just a couple more quick announcements.

02:38.740 --> 02:43.820
You may have heard me mention last time that over the weekend we hit a very exciting milestone

02:43.820 --> 02:45.500
for the podcast.

02:45.500 --> 02:48.340
One million listens.

02:48.340 --> 02:52.780
What an amazing way to close out an amazing year for the show.

02:52.780 --> 02:57.780
It occurred to us that we'd hate to miss an opportunity to show you some love.

02:57.780 --> 03:03.540
So we're launching a listener appreciation contest to celebrate the occasion.

03:03.540 --> 03:09.820
To enter, just tweet to us using the hashtag twimmel1mail.

03:09.820 --> 03:15.860
Every entry gets a fly twimmel1mail sticker plus a chance to win one of 10 limited run

03:15.860 --> 03:18.700
t-shirts commemorating the occasion.

03:18.700 --> 03:24.220
We'll be giving away some other mystery prizes as well from the magic twimmel swag bag

03:24.220 --> 03:26.780
so you should definitely enter.

03:26.780 --> 03:33.780
If you're not on Twitter or want more ways to enter, visit twimmelai.com slash twimmel1mail

03:33.780 --> 03:35.580
for the full rundown.

03:35.580 --> 03:41.300
Last but not least, we are quickly approaching our final twimmel online meetup of the year,

03:41.300 --> 03:46.740
which will be held on Wednesday, December 13th at 3pm Pacific time.

03:46.740 --> 03:52.580
We'll start out by discussing the top ML and AI stories of 2017 and then for our main

03:52.580 --> 03:58.180
presentation, Bruno Gonzalez will be discussing the paper understanding deep learning requires

03:58.180 --> 04:04.620
rethinking generalization by Shi Yuan Zhang from MIT and Google Brain and others.

04:04.620 --> 04:10.780
This will be a fun meetup and one you don't want to miss, so be sure to register at twimmelai.com

04:10.780 --> 04:14.900
slash meetup if you haven't already done so.

04:14.900 --> 04:24.340
And now on to the show.

04:24.340 --> 04:27.660
Alright everyone, I am on the line with Nikita Shamganov.

04:27.660 --> 04:32.260
Nikita is CEO and co-founder of MemSQL.

04:32.260 --> 04:35.020
Nikita, welcome to this week in machine learning and AI.

04:35.020 --> 04:36.020
Thank you.

04:36.020 --> 04:37.020
Thank you for having me.

04:37.020 --> 04:38.020
Absolutely.

04:38.020 --> 04:41.620
Nikita, why don't we get started by having you tell us a little bit about your background.

04:41.620 --> 04:46.420
You are the CEO of MemSQL, but you've got a pretty technical background, isn't that right?

04:46.420 --> 04:47.420
That's right.

04:47.420 --> 04:53.780
So I'm on my PhD in computer science from St. Petersburg, Russia and I moved to the stage to work

04:53.780 --> 04:58.780
on Microsoft SQL Server, which I did for a number of years.

04:58.780 --> 05:04.700
So I have a very strong database and query processing background.

05:04.700 --> 05:11.260
After that, I moved over to Facebook where I was blown away by the magnitude of data

05:11.260 --> 05:14.180
problem Facebook was solving back then.

05:14.180 --> 05:21.180
And since they only increased in magnitude, seeing that and combining the visibility into

05:21.180 --> 05:29.700
those workloads and kind of making that assumption that in five years, a lot more companies

05:29.700 --> 05:33.420
are going to be facing those challenges just like Facebook.

05:33.420 --> 05:40.100
I decided to start a company and combine that database background and expertise of building

05:40.100 --> 05:47.540
systems and the early stages and the glimpse of the workloads that I saw at Facebook.

05:47.540 --> 05:51.220
So I kind of knew where the world was going into.

05:51.220 --> 05:58.980
So that triggered my desire and gave me some insights into starting MemSQL, the company.

05:58.980 --> 06:05.660
So we've been in it for six plus years and certainly validated some of the assumptions

06:05.660 --> 06:09.460
we had starting that journey.

06:09.460 --> 06:16.300
So I've come across MemSQL and I think of the company as an in-memory database.

06:16.300 --> 06:22.620
It's also a little bit about what the focus is there and in particular, what's the intersection

06:22.620 --> 06:25.260
between what you're doing and machine learning in AI?

06:25.260 --> 06:27.820
Are you seeing a lot of those types of workloads nowadays?

06:27.820 --> 06:28.820
Definitely.

06:28.820 --> 06:33.420
That's certainly where our customers are moving towards.

06:33.420 --> 06:39.340
But let me step back for a second and talk about MemSQL and in-memory database

06:39.340 --> 06:41.220
and technology.

06:41.220 --> 06:48.620
We started as purely in-memory database and then since we've evolved to support a large

06:48.620 --> 06:55.580
class of applications that I built on top of MemSQL and in-memory became an enabling

06:55.580 --> 06:59.540
technology but it's not the technology at MemSQL.

06:59.540 --> 07:07.380
As a matter of fact, the key advantage that MemSQL brings to the world is the fact that

07:07.380 --> 07:14.180
it supports SQL which is a structured query language and the fact that it runs the database

07:14.180 --> 07:16.140
in a distributed environment.

07:16.140 --> 07:21.660
So you can run MemSQL on your laptop or you can run MemSQL on a thousand hosts that

07:21.660 --> 07:29.460
gives you an immense compute power to enable the new class of applications.

07:29.460 --> 07:34.180
So let me talk about what kinds of applications we support.

07:34.180 --> 07:42.140
Maybe some of them have to do very little with AI and ML and MemSQL enables scale, latency.

07:42.140 --> 07:46.980
You can build very low latency applications on top of MemSQL and that's where in-memory

07:46.980 --> 07:49.460
technologies come handy.

07:49.460 --> 07:54.980
And you also can build applications that require very high levels of concurrency.

07:54.980 --> 07:59.700
And that concurrency is enabled on top of the system of record.

07:59.700 --> 08:05.740
So MemSQL supports state and it supports full durable persistence.

08:05.740 --> 08:10.700
SQL also allows you to build what we call real-time applications.

08:10.700 --> 08:15.100
And kind of the idea of real-time applications is the opposite of batch.

08:15.100 --> 08:21.620
Every time you need to do analytics and you do some sort of pre-calculations upfront using

08:21.620 --> 08:28.980
head doops or data warehouses or any other offline and batch oriented technology, that's

08:28.980 --> 08:30.660
basically our enemy.

08:30.660 --> 08:36.300
So we're bringing the world to be completely real-time and perform all the computations

08:36.300 --> 08:38.180
that you need to live.

08:38.180 --> 08:45.260
And we deliver it on extremely low latency by leveraging an immense amount of compute.

08:45.260 --> 08:51.020
And that we can do very, very well because MemSQL is a scalable technology that can run

08:51.020 --> 08:53.700
on clusters of commodity hardware.

08:53.700 --> 08:58.900
So now where AI and ML comes in, well, because we support this new class of applications

08:58.900 --> 09:06.100
the fact that our customers are incredibly forward-looking, they want to do more with

09:06.100 --> 09:13.260
the technology and they want to blend classical database workloads with the new types of

09:13.260 --> 09:19.500
computations that are mostly stemmed from the AI and ML needs.

09:19.500 --> 09:24.580
And one of the big ones that we see all the time is image recognition.

09:24.580 --> 09:27.060
So we can talk a little more about it.

09:27.060 --> 09:31.500
So let's say you have an application and you use that to power and use MemSQL to power

09:31.500 --> 09:32.660
that application.

09:32.660 --> 09:37.900
The application is large scale, there's a ton of data that's stored in MemSQL.

09:37.900 --> 09:42.700
Like I said earlier in memory is an enabling technology, but it's not the technology.

09:42.700 --> 09:48.140
You can actually put a lot more data into MemSQL than the resmemory on the cluster.

09:48.140 --> 09:55.180
And what you want to do is you want to enable smart applications, the ones that make decisions

09:55.180 --> 10:01.820
on either on behalf of a user or they provide recommendations or they provide some sort

10:01.820 --> 10:07.380
of search capabilities on top of unstructured and semi-structured data.

10:07.380 --> 10:12.220
Imagine an app that has a camera, it runs only on your cell phone, you snap a picture

10:12.220 --> 10:20.700
with this app and just in a few tens of milliseconds this app finds similar images to the one

10:20.700 --> 10:22.900
that you just took a picture of.

10:22.900 --> 10:24.260
So why is it useful?

10:24.260 --> 10:28.420
Well, it's useful because you just enabled visual search.

10:28.420 --> 10:37.740
And the way we do it is we built some of the building blocks that allow you to run and

10:37.740 --> 10:42.580
operationalize the machine learning models and we built them straight into the database.

10:42.580 --> 10:47.460
And now the database allows you to scale them and deliver very low latencies for this type

10:47.460 --> 10:48.940
of operations.

10:48.940 --> 10:52.900
So that would be one example of an application.

10:52.900 --> 10:57.020
I can give you another one, which we see a lot in the IoT space.

10:57.020 --> 11:04.940
Before we go into the next example, can you drill down a little bit into what specifically

11:04.940 --> 11:07.980
MemSQL is doing to enable the first example?

11:07.980 --> 11:14.220
For example, are there pre-trained models for image recognition or image similarity in

11:14.220 --> 11:19.660
this case built into the database, kind of like you might think of a stored procedure

11:19.660 --> 11:22.780
or is it, you know, is there something else?

11:22.780 --> 11:24.860
Is it a different type of functionality?

11:24.860 --> 11:27.380
Yeah, it's even lower level than that.

11:27.380 --> 11:33.740
MemSQL certainly supports stored procedures, but in this particular case, we implemented

11:33.740 --> 11:39.260
a few building blocks, particularly dot product and Euclidean distance between vectors.

11:39.260 --> 11:42.020
Oh, that is pretty low level.

11:42.020 --> 11:48.100
And if you take a deep learning model and you look at the layers, you know, Matrix

11:48.100 --> 11:53.620
multiplication, tensor multiplication, vector multiplication, is the fundamental building

11:53.620 --> 11:54.620
block.

11:54.620 --> 12:00.660
So what we do is, as we train that model, we take all the layers except for the very

12:00.660 --> 12:07.540
last one and apply it on the database of images that we have and that allows us to extract

12:07.540 --> 12:13.380
what we call, you know, or anybody else calls a feature vector, which is just a vector.

12:13.380 --> 12:19.220
So once we have that and we have a model, applying that model to an incoming image, which

12:19.220 --> 12:24.620
you just took a picture with your cell phone, will produce another feature vector.

12:24.620 --> 12:30.620
And it just so happens that the multiplication of those two feature vectors normalized

12:30.620 --> 12:36.340
gives you the similarity score, how close those images are together.

12:36.340 --> 12:41.700
So the heavy lifting of building a model belongs to somewhere else.

12:41.700 --> 12:49.060
And the data may might as well be still stored in them SQL and we enabling very fast data

12:49.060 --> 12:52.420
transfer in and out of them SQL in a parallel way.

12:52.420 --> 12:58.980
So we can send it into Spark or TensorFlow or any other training framework.

12:58.980 --> 13:05.340
But once it gets to operationalizing, operationalizing that model and performing the last mile

13:05.340 --> 13:11.100
computation by really powering your app, then them SQL gives you that scale.

13:11.100 --> 13:16.340
And it allows you to perform those computations pretty much at the memory bandwidth speed

13:16.340 --> 13:20.020
that the labels really low latencies for that last mile computation.

13:20.020 --> 13:21.020
Okay.

13:21.020 --> 13:22.020
Doesn't make sense?

13:22.020 --> 13:23.020
Okay.

13:23.020 --> 13:24.020
No, it does.

13:24.020 --> 13:26.460
So you're computing these feature vectors.

13:26.460 --> 13:31.940
Is that happening as is that happening on right of new images or is it happening?

13:31.940 --> 13:34.660
I'm assuming that's the way you do it since your anti batch.

13:34.660 --> 13:38.620
You're not doing some big batch job that's like updating, you know, some column in your

13:38.620 --> 13:42.220
database with your feature vectors for all the images that are in there.

13:42.220 --> 13:43.220
Correct.

13:43.220 --> 13:47.940
And you can do, you can do either, but the typical workload is once you have that model,

13:47.940 --> 13:49.940
you apply that model on right.

13:49.940 --> 13:56.020
And we have technology, it's called pipelines that allows you to perform arbitrary computations

13:56.020 --> 13:59.300
either in a store procedure or an external piece of code.

13:59.300 --> 14:05.940
That's where you can invoke all third party libraries to apply that computation and then

14:05.940 --> 14:09.460
the store, the feature vector in the database.

14:09.460 --> 14:14.500
So you can do it in the ingest and if the, let's say you built something like you crawl

14:14.500 --> 14:22.380
the web or you crawl your own product catalogs and once you identify those images, you immediately

14:22.380 --> 14:27.380
stick them into the database as you do that, we trigger that computation.

14:27.380 --> 14:34.340
So the feature vector arrives into the database at the same time instantly as the actual data.

14:34.340 --> 14:38.380
And now it immediately participates in all sorts of computations.

14:38.380 --> 14:43.820
So that allows you to never have kind of stop and go computations.

14:43.820 --> 14:50.780
You never do, okay, step one, load all the data, step two, perform the batch computation

14:50.780 --> 14:55.700
on top of all the data into the database and step three, do something else with it.

14:55.700 --> 15:00.780
Rather than, it's all streamlined and it just flows in and out.

15:00.780 --> 15:04.580
So how does this play out in the IoT case?

15:04.580 --> 15:08.380
So in the IoT case, data is incumbent constantly.

15:08.380 --> 15:16.260
So one of the use cases we have with a large energy company is to ingest IoT data from

15:16.260 --> 15:17.740
drill bits.

15:17.740 --> 15:24.180
Apparently in the world of fracking, you tend to drill a lot more and then there's a

15:24.180 --> 15:31.220
non-trivial cost for a broken drill bit, those things are extremely expensive.

15:31.220 --> 15:35.340
You have to stop your operation if the drill bit breaks.

15:35.340 --> 15:41.620
So you're losing not only on the fact that you're fishing this thing out from the ground,

15:41.620 --> 15:47.220
from hundreds and maybe even thousands of miles deep, but you also not producing oil,

15:47.220 --> 15:49.660
which is an operational cost.

15:49.660 --> 15:56.460
So what we do, we ingest that real time data, IoT data, and we're scoring that data,

15:56.460 --> 15:59.660
applying a machine learning model in real time.

15:59.660 --> 16:07.100
And then there's an application built that arrest the drill bit before it hits a problem.

16:07.100 --> 16:12.500
Just by measuring temperature and all the very, you know, temperature, throughput, all

16:12.500 --> 16:18.900
sorts of kind of vital signs of the drill bit, of the drill bit as it goes through the ground.

16:18.900 --> 16:20.420
And so that was step one.

16:20.420 --> 16:27.700
Step two is obviously you feed all this information back to direct the drilling.

16:27.700 --> 16:30.740
So in the world of fracking, drilling is directional.

16:30.740 --> 16:37.460
So it's not just vertical down into the ground, but it's more like you change in the direction

16:37.460 --> 16:39.700
as you go and drilling.

16:39.700 --> 16:45.180
So using all that input, you can direct the drill bit to make the whole operation a lot

16:45.180 --> 16:46.180
more efficient.

16:46.180 --> 16:50.980
And what are some of the algorithms that come into play in that use case?

16:50.980 --> 16:55.260
So they started with, you know, like every typical data science, they started with some

16:55.260 --> 17:00.340
sort of linear regressions that they moved it to decision trees very quickly.

17:00.340 --> 17:04.300
And now they're experimenting with deep learning for that as well.

17:04.300 --> 17:09.740
And the beauty of that, of the solution that we presented is it integrates natively

17:09.740 --> 17:13.660
with all sorts of third party libraries.

17:13.660 --> 17:17.540
So we made the experimentation for them very, very straightforward.

17:17.540 --> 17:24.860
They started with SAS, where they produce models in SAS, as you know, SAS is a proprietary

17:24.860 --> 17:29.580
technology, but since they've played with Spark and now experimenting with TensorFlow

17:29.580 --> 17:30.580
as well.

17:30.580 --> 17:31.580
Okay.

17:31.580 --> 17:37.220
I was actually going to ask you about Spark and how that fits in with your model.

17:37.220 --> 17:38.220
Do you?

17:38.220 --> 17:43.900
I imagine you see it out and out working with customers, is it a competitive technology?

17:43.900 --> 17:48.820
Spark plus the rest of the Hadoop ecosystem, or is it complementary?

17:48.820 --> 17:50.900
Or how do you see that?

17:50.900 --> 17:54.260
So all the big data technologies overlap a little bit.

17:54.260 --> 17:57.420
In the case of Spark, I would say it's 9010.

17:57.420 --> 18:02.340
So it's 10% competitive, 90% complementary.

18:02.340 --> 18:05.100
And here is why Spark doesn't have state.

18:05.100 --> 18:08.500
The state is usually stored somewhere else.

18:08.500 --> 18:17.180
It's either HDFS, relational database, or S3, some sort of object store in the cloud.

18:17.180 --> 18:23.460
So what we do in this case is we provide an extremely performant state.

18:23.460 --> 18:27.820
And the combination of MemSQL and Spark, that's the 90% case.

18:27.820 --> 18:34.580
They work really, really well together because we give you that transactional, scalable

18:34.580 --> 18:39.260
state, and nothing else on the market can give you that state.

18:39.260 --> 18:45.620
So not only you can store, you can retrieve, but you can also compute.

18:45.620 --> 18:51.940
And we have a world class SQL query processing engine that allows you to produce reports,

18:51.940 --> 18:55.420
but also allows you to modify that state in a transactional fashion.

18:55.420 --> 18:59.420
You can say begin transaction, insert, update, delete.

18:59.420 --> 19:06.420
You can run it at high concurrency, and you have full durability and all sorts of guarantees

19:06.420 --> 19:07.660
for that data.

19:07.660 --> 19:10.860
So that's where we're extremely complementary.

19:10.860 --> 19:17.420
The typical deployment model is that there is a data lake, and the data lake stores hundreds

19:17.420 --> 19:20.100
of terabytes of petabytes of data.

19:20.100 --> 19:25.940
MemSQL is deployed alongside of the data lake to provide to power applications.

19:25.940 --> 19:31.420
Because you cannot write applications on top of Hadoop because Hadoop is batch.

19:31.420 --> 19:39.540
So now, and then Spark is the glue between those two, and it allows you to have rapid data

19:39.540 --> 19:45.860
transfer, all the data that is born in MemSQL based on the applications, based on the interactions

19:45.860 --> 19:47.540
with applications.

19:47.540 --> 19:48.740
That data is captured.

19:48.740 --> 19:54.620
You can pick it up and Spark very, very easily, we have a world class Spark connector, drop

19:54.620 --> 20:00.380
it in the data lake for historical, archival, compliance type storage, and then provide

20:00.380 --> 20:05.580
some, and then perform some overnight batch computations, take the results of those

20:05.580 --> 20:13.700
computations, stick it into MemSQL, and Spark often times give you that unified API.

20:13.700 --> 20:17.060
Because through that API, you can interact with MemSQL, you can interact with the data

20:17.060 --> 20:24.100
lake, and it becomes kind of the go-to API for application developers.

20:24.100 --> 20:29.020
And MemSQL, in this case, just gives you SQL, then you can attach a BI tool directly into

20:29.020 --> 20:36.660
MemSQL, and they can scale the concurrency of data scientists that attach their BI tools

20:36.660 --> 20:39.660
to MemSQL and look at the reports and visualizations.

20:39.660 --> 20:47.820
And is it primarily data scientists and folks that are end user use of MemSQL?

20:47.820 --> 20:55.580
Or you also, in this scenario, attaching your traditional applications to MemSQL or using

20:55.580 --> 21:02.140
some other state technology for building applications that refer to this data?

21:02.140 --> 21:07.980
Well, mostly it's actually application developers, because MemSQL powers applications.

21:07.980 --> 21:14.660
And because the nature of applications is changing all the time, and we have higher scale requirements

21:14.660 --> 21:20.340
for the applications, MemSQL is a perfect technology to power applications like this.

21:20.340 --> 21:24.220
I'll give you a few more examples of such applications.

21:24.220 --> 21:32.140
Now because modern applications have AI and ML requirements, that's where that intersection

21:32.140 --> 21:38.460
comes in, where you need to have those models that you produced somewhere in your data

21:38.460 --> 21:41.660
science lab, and you want to operationalize those models.

21:41.660 --> 21:44.340
And that's where MemSQL plays very, very strongly.

21:44.340 --> 21:49.700
So do you envision kind of a lot of lines of what Spark's done, kind of building higher

21:49.700 --> 21:56.580
level abstractions beyond that product and Euclidean distance to enable folks to do machine

21:56.580 --> 21:58.580
learning and AI more easily?

21:58.580 --> 21:59.580
Absolutely.

21:59.580 --> 22:00.580
Absolutely.

22:00.580 --> 22:06.820
We have a number of ideas that are circulating inside the engineering team and certainly

22:06.820 --> 22:11.820
influenced by our customers and what they want to do as a technology.

22:11.820 --> 22:20.820
So the customers see in MemSQL is that very, very fast, scalable state that they can deploy

22:20.820 --> 22:24.980
inside their data centers or in the cloud on the cheap, right?

22:24.980 --> 22:30.660
Because MemSQL provides world-class compression, it has column store technology, and that

22:30.660 --> 22:33.820
state is very fungible.

22:33.820 --> 22:39.780
Because we support transactions, you can change that state, you can enrich that data with

22:39.780 --> 22:44.220
attributes that you compute on the fly, et cetera, et cetera, et cetera.

22:44.220 --> 22:50.460
Now what people want to do is they want to perform computations that are beyond SQL.

22:50.460 --> 22:56.460
So we're world-class in SQL query processing, and that's great.

22:56.460 --> 22:59.740
And our customers want to do that and more.

22:59.740 --> 23:05.020
And when they want to do more right now, we resort to Spark, right?

23:05.020 --> 23:10.340
We say, OK, well, deploy Spark alongside of MemSQL, pull the data out, and we'll give

23:10.340 --> 23:16.660
you that data very quickly, perform the computation, put that data back, and then leverage some

23:16.660 --> 23:21.300
of the building blocks that we have, you know, basic arithmetic operations, vectorized

23:21.300 --> 23:28.540
operations, vector operations to do the last mile computation to power your applications.

23:28.540 --> 23:33.660
Now, like once you do that, you just want to take that loop and you want to tighten

23:33.660 --> 23:34.660
it up.

23:34.660 --> 23:40.620
So you start bringing all the computations that you today, today you're taking data out

23:40.620 --> 23:46.340
of MemSQL, put it somewhere else in a temporary store like Spark, data frames, and you want

23:46.340 --> 23:49.460
to perform those computations in place.

23:49.460 --> 23:53.700
And there are two interesting problems in that space.

23:53.700 --> 23:56.780
One is what the API should be.

23:56.780 --> 24:05.540
And today for machine learning and AI, the APIs tend to be either Spark driven or the Python

24:05.540 --> 24:11.260
library ecosystem driven, you know, the likes of non-py, sci-py, pandas, TensorFlow.

24:11.260 --> 24:15.260
So it seems like the Python world lives in one universe.

24:15.260 --> 24:18.540
The Spark world lives in another universe.

24:18.540 --> 24:23.620
And then there is the SQL universe that is pretty much ubiquitous because every database

24:23.620 --> 24:29.980
exposes SQL as an API and also data warehouses expose SQL as an API.

24:29.980 --> 24:38.880
Our view that the Spark universe and MemSQL universe should be enabled by rapid data transfer

24:38.880 --> 24:40.140
between the two.

24:40.140 --> 24:46.380
And then the Python universe should be enabled by pushing some of the computations that

24:46.380 --> 24:52.140
you express through Python API into the database and putting them on steroids.

24:52.140 --> 24:54.180
Now what exactly does that mean?

24:54.180 --> 24:55.460
And can you give an example?

24:55.460 --> 24:56.460
Yeah, totally.

24:56.460 --> 25:01.460
So let's say you perform an entrepreneurial computation on your data.

25:01.460 --> 25:05.300
Let's say you have hundreds of bytes of data, you know, certainly more data that can fit

25:05.300 --> 25:09.180
on your laptop and your data scientists.

25:09.180 --> 25:15.180
And as a data scientist, you stack, you live in the Python world.

25:15.180 --> 25:20.860
And you're a big expert in the libraries like pandas, non-py and sci-py.

25:20.860 --> 25:24.020
And let's say you're playing with TensorFlow as well.

25:24.020 --> 25:31.980
So it's very natural for you to express computations on that data to perform training or

25:31.980 --> 25:37.540
perform scoring on that data in that Python world.

25:37.540 --> 25:42.420
The problem is all the computations are single threaded and all the computations are in

25:42.420 --> 25:43.740
memory.

25:43.740 --> 25:49.700
So you're stuck at this point in time because you cannot access hundreds of terabytes of

25:49.700 --> 25:54.340
data because there's no way you can put hundreds of terabytes in memory.

25:54.340 --> 26:01.460
And that's where we see the world is going to where you want those computations to become

26:01.460 --> 26:08.100
instant and paralyze a bowl and scalable, not so you can just instead of bringing that

26:08.100 --> 26:15.340
data into your high memory machine, you can perform those computations in place in something

26:15.340 --> 26:18.900
that's very, very scalable, like MemSQL.

26:18.900 --> 26:21.500
So you're going to see more and more of that happening over time.

26:21.500 --> 26:29.660
You mentioned the three different modes of interacting with this for machine learning

26:29.660 --> 26:34.940
or in general, Spark, Python, and SQL.

26:34.940 --> 26:43.900
Are there any efforts to extend SQL to give it some kind of machine learning expressiveness

26:43.900 --> 26:54.020
like SQLs got all kinds of aggregation operations like select average of x, where, whatever.

26:54.020 --> 27:01.740
I can imagine something like select predict why, where, whatever, where you're kind of

27:01.740 --> 27:07.820
telling expressing in SQL that you want a prediction based on something.

27:07.820 --> 27:12.060
Does that exist or are folks playing with that today?

27:12.060 --> 27:16.660
Yeah, so if you look at the history and you look at products like, you know, good old

27:16.660 --> 27:23.100
products like Teradata or NETISA, or if you look at SQL server integration with R, that's

27:23.100 --> 27:28.860
certainly something that is happening where trading is not happening in database, but scoring

27:28.860 --> 27:31.980
is, that's a natural way to do things.

27:31.980 --> 27:37.660
We achieve the same functionality through pipelines where we score data on ingest.

27:37.660 --> 27:43.100
So I think this is valuable and you'll see database is exposing more and more primitives,

27:43.100 --> 27:48.380
the likes of dot product and Euclidean distance, but you'll have instead of two, you will have

27:48.380 --> 27:54.300
you know, a hundred of those built in into the database and then you will see packages

27:54.300 --> 28:02.940
where it's just a package of storage procedures, which allows you to to run those predictions.

28:02.940 --> 28:06.860
And those will work really well for simple use cases.

28:06.860 --> 28:12.580
Now, if you look at the world of data scientists today, they actually don't like that.

28:12.580 --> 28:19.060
They like to live in the world of data frames where there's a lot more control over the

28:19.060 --> 28:21.980
types of computations that people are expressing.

28:21.980 --> 28:27.300
People like to, people understand that world of data frames very, very well.

28:27.300 --> 28:34.540
So I think the future is going to be actually in paralyzing and speeding up computations

28:34.540 --> 28:39.420
inside data frames, so that's just my opinion.

28:39.420 --> 28:44.260
Do you envision doing training directly in the database?

28:44.260 --> 28:49.260
In the same way as I described the interaction with the data frames.

28:49.260 --> 28:55.900
So if you want to enable inference in the database, then you don't want to have the database,

28:55.900 --> 28:59.020
you don't want to have the database to be a crutch where you're constantly fighting and

28:59.020 --> 29:02.940
trying to shoehorn that computation into SQL.

29:02.940 --> 29:08.980
The natural way to expressing those computations is operations on top of data frames.

29:08.980 --> 29:16.740
However, if you make the database, naturally support those data frame computations, then

29:16.740 --> 29:21.700
you have tremendous value from the fact that the database actually owns the state.

29:21.700 --> 29:27.300
And you never need to transfer that state from your storage to something else.

29:27.300 --> 29:30.740
So you're bringing computations closer to data.

29:30.740 --> 29:36.820
So that's where I see the industry is moving in the next five years.

29:36.820 --> 29:39.340
You just mentioned bringing computations closer to data.

29:39.340 --> 29:45.020
And that's obviously been one of the things that the Hadoop ecosystem has made more readily

29:45.020 --> 29:50.180
accessible to folks, this idea of data locality.

29:50.180 --> 29:57.300
Do you get to take advantage of that with machine learning types of models in general and

29:57.300 --> 30:05.820
this pipelining approach that you have in particular, or are the pipelines run outside of separate

30:05.820 --> 30:08.460
from any concept of locality of data?

30:08.460 --> 30:15.980
Well, locality is a loaded concept, so you can start with, OK, well, I want to perform

30:15.980 --> 30:21.380
computation exactly on the same machine where the data is stored.

30:21.380 --> 30:23.980
So that's kind of the extreme case of locality.

30:23.980 --> 30:31.300
Another way to think about is as you look at the computation plan, let's say in the database

30:31.300 --> 30:34.420
terminology, there would be a query plan.

30:34.420 --> 30:41.100
So let's extend the definition of a SQL query plan to a broader concept of you performing

30:41.100 --> 30:44.220
some sort of arbitrary scalable computation.

30:44.220 --> 30:47.580
And you know all the steps in the computation up front.

30:47.580 --> 30:54.300
So you can optimize those computations and produce a query plan for those computations and

30:54.300 --> 30:59.460
you will run that query in the distributed environment.

30:59.460 --> 31:06.140
Now then you look at this and certain primitive computations, you really, really want to

31:06.140 --> 31:12.820
perform closer to the data because you will save tremendously on the amount of IO that

31:12.820 --> 31:15.580
is happening for in the data transfer.

31:15.580 --> 31:23.100
Now from there, if you do that, you also deliver on the concurrency of such computations.

31:23.100 --> 31:31.380
So now you can perform those computations at a highly concurrent environment where thousands

31:31.380 --> 31:39.100
of data sciences are attaching to the same data that is collected and centralized and stored

31:39.100 --> 31:44.540
in something like MCQL and then they are performing their computations concurrently.

31:44.540 --> 31:51.020
So there are no opening up all the data to the organization and not having any data silos.

31:51.020 --> 32:00.180
So data locality for computations is useful where it makes sense and not as useful in a

32:00.180 --> 32:03.940
broader general purpose way.

32:03.940 --> 32:10.140
And the perfect system would be engineered around pushing the computations closer to

32:10.140 --> 32:16.780
the data where it makes a ton of sense. For example, you do a lot of filtering or you

32:16.780 --> 32:20.100
performing a lot of what it's called group by operations.

32:20.100 --> 32:23.820
And those operations make sense to be done locally.

32:23.820 --> 32:29.980
Those operations make sense to be done in a vectorized fashion by leveraging the latest

32:29.980 --> 32:35.700
and greatest CPU instructions in the Intel hardware and so on.

32:35.700 --> 32:40.180
It makes sense to leverage indexes so you prune massive amounts of data so you don't

32:40.180 --> 32:42.900
even touch them for your computations.

32:42.900 --> 32:48.700
And then from there, it makes sense to bring all that data into a stateless distributed

32:48.700 --> 32:52.660
environment and perform the rest of the computations.

32:52.660 --> 33:00.420
So that approach allows you to build greater scalability for your system compared to a traditional

33:00.420 --> 33:03.380
database compared to Hadoop or compared to Spark.

33:03.380 --> 33:10.540
Okay. And now you just mentioned the taking advantage of the instruction sets of the underlying

33:10.540 --> 33:18.300
hardware. And you guys are making an announcement with Intel at Amazon re-invent next week.

33:18.300 --> 33:19.780
Is it related to that area?

33:19.780 --> 33:26.300
Definitely. Intel has been supporting vectorized instructions for some time for over a decade.

33:26.300 --> 33:33.860
And every new generation of CPUs increases the size of the vector that you can use and perform

33:33.860 --> 33:36.820
vectorized operations on top of.

33:36.820 --> 33:41.340
And where we stand right now, it's 512 bits.

33:41.340 --> 33:45.780
So compared to matrices in a GPU, this is tiny.

33:45.780 --> 33:51.860
However, when you perform a last mile computation like dot product, it allows you to perform that

33:51.860 --> 33:56.740
computation much faster than the memory bandwidth that you have.

33:56.740 --> 34:04.420
So you actually don't need more for that last mile computation than AVX 512 because your

34:04.420 --> 34:06.940
limitation is not your compute.

34:06.940 --> 34:09.620
Your limitation is memory bandwidth.

34:09.620 --> 34:16.540
This is very, very different when you train models and perform computations for building

34:16.540 --> 34:18.860
deep learning.

34:18.860 --> 34:23.180
That's where you perform a lot of tensor multiplication.

34:23.180 --> 34:29.260
And they mod of compute compared to the size of your data set is tremendously more when

34:29.260 --> 34:31.900
you perform something like dot product.

34:31.900 --> 34:34.420
That's where you want GPUs.

34:34.420 --> 34:39.220
But once the model is built and the model is trained, you actually don't need GPUs to

34:39.220 --> 34:42.260
score that model in many, many cases.

34:42.260 --> 34:48.580
What that gives you is it gives you the ability to democratize that computation because

34:48.580 --> 34:51.020
Intel CPU is everywhere.

34:51.020 --> 34:54.900
If a machine has a GPU, it also has an Intel CPU.

34:54.900 --> 34:59.260
And the point that I'm making that in many cases, all you need is a CPU and you're not going

34:59.260 --> 35:05.460
to get the computation faster if you add a GPU to the system simply because it's the memory

35:05.460 --> 35:09.020
bandwidth that's your bottleneck in the computation.

35:09.020 --> 35:16.060
Now we use AVX 512 for vector dot product, for Euclidean distance, and for other vector

35:16.060 --> 35:19.540
operations that were built into the database.

35:19.540 --> 35:25.860
We also use AVX 512 for general purpose SQL computations.

35:25.860 --> 35:34.460
And that allows you to just deliver on orders of magnitude faster SQL query processing

35:34.460 --> 35:38.780
that our competitors have and certainly what Hadoop has or spark.

35:38.780 --> 35:44.660
We're huge fans of that technology and we can't wait when Intel allows us to perform

35:44.660 --> 35:49.060
computations on larger vectors, not just 512 bits.

35:49.060 --> 35:55.500
Can you be more specific in terms of some of the actual results you've seen in production

35:55.500 --> 35:57.540
systems in terms of performance differences?

35:57.540 --> 35:58.860
Yeah, absolutely.

35:58.860 --> 36:06.060
So with MemSQL 6.0 that we just released in October, we can do things like a group by

36:06.060 --> 36:12.460
operation on 100 billion row table in a sub-second.

36:12.460 --> 36:15.060
So 100 billion?

36:15.060 --> 36:16.060
Yes, wow.

36:16.060 --> 36:22.860
So it says a sub-second operation that runs something like give me an average stock price

36:22.860 --> 36:30.020
over 100 billion data points and group it by security or by a stock value.

36:30.020 --> 36:36.060
The computation itself is relatively straightforward but because we perform that computation, quote

36:36.060 --> 36:45.140
unquote, on compressed data and we're using those vectorized operations, we achieve the

36:45.140 --> 36:49.820
performance of a billion operations per second per core.

36:49.820 --> 36:55.300
So if you throw 100 cores into the system and MemSQL is extremely scalable so you can

36:55.300 --> 37:01.500
throw 100 cores or you can throw a thousand cores, you can achieve this type of performance

37:01.500 --> 37:03.060
on your system.

37:03.060 --> 37:08.980
And so that's using the AVX512 instructions, what were you seeing prior to that?

37:08.980 --> 37:14.740
Well, there's a combination of two techniques that goes into this type of performance.

37:14.740 --> 37:19.580
The first is how can you perform operations on compressed data?

37:19.580 --> 37:24.740
So if, like I said earlier, oftentimes it's the memory bandwidth that's the bottleneck

37:24.740 --> 37:26.060
of your computation.

37:26.060 --> 37:32.420
So the better you compress but the better off you are in the final computation because at

37:32.420 --> 37:35.420
the end of the day you scan fewer bytes.

37:35.420 --> 37:37.220
What exactly do we mean by compressor?

37:37.220 --> 37:43.260
Is it kind of your traditional binary compression or are we also talking some kind of deduplication

37:43.260 --> 37:44.780
or something like that?

37:44.780 --> 37:45.780
It's a combination.

37:45.780 --> 37:52.780
So first of all, the compression is called column store compression and we also encode

37:52.780 --> 37:53.780
similar values.

37:53.780 --> 38:00.140
So let's say you, in that example that I said was stock trading, you have 10,000 different

38:00.140 --> 38:01.140
stocks.

38:01.140 --> 38:06.220
So if you have 10,000 different stocks, then each individual stock can be encoded with

38:06.220 --> 38:07.940
just a few bits.

38:07.940 --> 38:14.140
And so then you only use this many bits to represent each stock value.

38:14.140 --> 38:21.340
And as you perform your computations, you never go back decoding those bits into, let's

38:21.340 --> 38:26.620
say, an integer value or got for bit a string value of a stock, right?

38:26.620 --> 38:30.140
Because those computation will become much more expensive.

38:30.140 --> 38:36.420
So now that you perform computations on compressed data, the second step that you do is you optimize

38:36.420 --> 38:42.940
your computation for as few branch mispredictions and as few cash misses as possible.

38:42.940 --> 38:46.740
And those are a big deal because every time you have a branch misprediction, you flash

38:46.740 --> 38:48.420
your CPU pipeline.

38:48.420 --> 38:51.140
So that slows down your computation.

38:51.140 --> 38:55.900
So if you express your computation, so there are no branch, no branches basically.

38:55.900 --> 39:00.020
And there are some very, very cool papers out there and there are some innovation that

39:00.020 --> 39:03.100
we've done here at MCQL as well.

39:03.100 --> 39:06.300
So that will give you the second boost.

39:06.300 --> 39:12.340
And then the third boost is now you take all those bits that you represent your values

39:12.340 --> 39:18.580
and you perform vector operations on top of those encoded values.

39:18.580 --> 39:21.820
And then that's where you use AVX512.

39:21.820 --> 39:30.100
So now, and that gives you another, I would say, three to five X performance improvements

39:30.100 --> 39:32.380
just by using AVX512.

39:32.380 --> 39:37.900
But if you take that, plus you take operations on compressed data, plus you take care of

39:37.900 --> 39:39.660
branch misprediction.

39:39.660 --> 39:42.340
You multiply those all those together.

39:42.340 --> 39:47.500
And then you can routinely see two orders of magnitude improvement in performance.

39:47.500 --> 39:48.500
Wow.

39:48.500 --> 39:49.500
That's pretty fantastic.

39:49.500 --> 39:52.500
The kind of the point here is fancy hardware is cool.

39:52.500 --> 39:55.180
And you want to use it where it's needed.

39:55.180 --> 39:59.420
But also, there's a lot of potential in the hardware you have at hand.

39:59.420 --> 40:04.820
And if you take full advantage of this of that hardware, you will have remarkable results.

40:04.820 --> 40:05.820
Mm-hmm.

40:05.820 --> 40:06.820
Oh, that's great.

40:06.820 --> 40:15.500
I guess as we kind of wrap up, do you have any additional kind of advice to folks that

40:15.500 --> 40:21.900
are looking to, you know, get better, you know, to just to take better advantage of, you

40:21.900 --> 40:26.540
know, whether it's their, you know, their data storage systems, their, you know, hardware

40:26.540 --> 40:33.900
systems, and to use these to make better, you know, predictions and better utilized machine

40:33.900 --> 40:34.900
learning in AI.

40:34.900 --> 40:37.580
Well, my advice here is, don't settle.

40:37.580 --> 40:43.340
There are systems out there such as memtique will that allows you to take pretty much your

40:43.340 --> 40:50.580
end-to-end machine learning AI and application development pipeline and make them completely

40:50.580 --> 40:52.460
real-time.

40:52.460 --> 40:58.620
So whatever you do in batch, whatever you have to wait for, what the right technology

40:58.620 --> 41:02.700
can be squished down to zero.

41:02.700 --> 41:04.300
Is that the technical term?

41:04.300 --> 41:05.300
Squished to zero?

41:05.300 --> 41:08.340
Yeah, let's say that.

41:08.340 --> 41:09.820
And that's where the world is going.

41:09.820 --> 41:17.140
So we'll live in the world in the future where compute and storage are going to be utilities.

41:17.140 --> 41:21.860
And you know, if you're willing to pay for compute, you will have as much compute as you

41:21.860 --> 41:27.580
need, at any concurrency as you need, and any latency as you need.

41:27.580 --> 41:33.860
And the only thing that is going to be bounded by is the amount of money you pay for that

41:33.860 --> 41:34.860
compute.

41:34.860 --> 41:35.860
Great.

41:35.860 --> 41:41.140
Well, Nikita, thank you so much for spending the time to chat with me, I really appreciate

41:41.140 --> 41:42.140
it.

41:42.140 --> 41:43.140
Yeah, absolutely.

41:43.140 --> 41:47.380
Thanks for having me.

41:47.380 --> 41:50.340
Alright everyone, that's our show for today.

41:50.340 --> 41:55.780
Thanks so much for listening and for your continued feedback and support.

41:55.780 --> 42:00.860
For more information on Nikita or any of the topics covered in this episode, head on over

42:00.860 --> 42:05.140
to twomolei.com slash talk slash 84.

42:05.140 --> 42:11.980
To follow along with our AWS reinvent series, visit twomolei.com slash reinvent.

42:11.980 --> 42:16.820
Of course, we would love to receive your feedback or questions, either via a comment on the

42:16.820 --> 42:22.860
show notes page or via Twitter to at twomolei or at Sam Charrington.

42:22.860 --> 42:26.980
Thanks once again to Intel Nirvana for their sponsorship of this series.

42:26.980 --> 42:32.980
To learn more about deep lens and the other things they've been up to, visit intelnervana.com.

42:32.980 --> 42:36.740
And thank you once again for listening and catch you next time.

