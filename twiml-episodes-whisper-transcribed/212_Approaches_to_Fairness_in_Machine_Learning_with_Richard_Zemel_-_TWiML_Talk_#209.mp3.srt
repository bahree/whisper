1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,240
I'm your host Sam Charrington. Today we continue our exploration of trust in AI with this interview

4
00:00:35,240 --> 00:00:40,640
with Richard Zemel, professor in the Department of Computer Science at the University of Toronto

5
00:00:40,640 --> 00:00:45,840
and Research Director at the Vector Institute. In our conversation, Richard describes some

6
00:00:45,840 --> 00:00:50,320
of his work on fairness in machine learning algorithms, including how he defines both

7
00:00:50,320 --> 00:00:56,480
group and individual fairness, and his group's recent Norrop's poster predict responsibly,

8
00:00:56,480 --> 00:01:02,280
improving fairness and accuracy by learning to differ. Thanks once again to Georgian

9
00:01:02,280 --> 00:01:07,720
partners for their continued support of the podcast and for sponsoring this series. Georgian

10
00:01:07,720 --> 00:01:12,440
partners is a venture capital firm that invests in growth stage business software companies

11
00:01:12,440 --> 00:01:19,000
that use applied artificial intelligence, conversational AI and trust to differentiate

12
00:01:19,000 --> 00:01:24,720
and advance their business solutions. Post investment Georgian works closely with portfolio

13
00:01:24,720 --> 00:01:31,080
companies to accelerate the adoption of these key technologies for increased value.

14
00:01:31,080 --> 00:01:36,120
To help their portfolio companies hire the right technical talent Georgian recently published

15
00:01:36,120 --> 00:01:42,560
building conversational AI teams, a comprehensive guide to lead you through sourcing, acquiring

16
00:01:42,560 --> 00:01:51,680
and nurturing a successful conversational AI team. Check it out at twomlai.com slash Georgian.

17
00:01:51,680 --> 00:01:58,880
And now on to the show.

18
00:01:58,880 --> 00:02:04,200
Alright everyone, I am on the line with Rich Zemel. Rich is a professor in the Department

19
00:02:04,200 --> 00:02:09,000
of Computer Science at the University of Toronto as well as being Research Director at the

20
00:02:09,000 --> 00:02:14,280
Vector Institute in Toronto. Rich, welcome to this week in Machine Learning and AI.

21
00:02:14,280 --> 00:02:19,760
Thanks very much. It's great to have you on the show. I'd love to get started by hearing

22
00:02:19,760 --> 00:02:25,560
a bit about your background and your path to your work in machine learning.

23
00:02:25,560 --> 00:02:31,040
Sure, so I've been interested in machine learning or I've been interested in artificial

24
00:02:31,040 --> 00:02:36,560
intelligence for a long time. In fact, since high school, I got lucky and had a summer

25
00:02:36,560 --> 00:02:42,160
job and I was growing up in Pittsburgh and had a summer job at Carnegie Mellon working

26
00:02:42,160 --> 00:02:48,400
with my sister neighbor was named Hans Berliner, one of the original person who worked on

27
00:02:48,400 --> 00:02:56,680
road game playing programs for played games like Backam and then Checkers and had some of

28
00:02:56,680 --> 00:03:02,800
the original ideas that later led to some of the ideas in chess and go the famous game

29
00:03:02,800 --> 00:03:08,600
playing of these days. So yes, I've been interested in it since that time and worked for companies

30
00:03:08,600 --> 00:03:14,760
back in the 80s, artificial intelligence companies back in the 80s when the AI went

31
00:03:14,760 --> 00:03:22,440
to help first hit. So that was my first experience with AI. I went to graduate school after

32
00:03:22,440 --> 00:03:27,280
that and got my PhD in machine learning, finished in the early 90s and I've been working

33
00:03:27,280 --> 00:03:34,360
in it ever since. Oh, fantastic. And I mentioned your post with the Vector Institute in Toronto.

34
00:03:34,360 --> 00:03:38,400
Can you share a bit about what the Vector Institute is up to there?

35
00:03:38,400 --> 00:03:46,240
Happy to. So the Vector Institute is an independent non-profit institute that's a combination

36
00:03:46,240 --> 00:03:53,240
of academia and industry. So there's a lot of faculty members here who have their home

37
00:03:53,240 --> 00:04:00,520
in universities such as University of Toronto or Waterloo or Guelph and as well as more

38
00:04:00,520 --> 00:04:07,240
distant universities like Delhause and UBC in Canada. And those people are all doing machine

39
00:04:07,240 --> 00:04:12,120
learning research. We also have some research scientists who are with their primary employment

40
00:04:12,120 --> 00:04:18,440
is here at Vector. And there's a lot of graduate students and postdocs associated with Vector

41
00:04:18,440 --> 00:04:23,240
or affiliate with Vector all doing research in machine learning. And so that's my focus

42
00:04:23,240 --> 00:04:27,800
is the machine learning research side of it. There's also another very strong side of it

43
00:04:27,800 --> 00:04:34,160
which is industry and working with industry and trying to help industry grow in terms

44
00:04:34,160 --> 00:04:40,040
of their machine learning capabilities. So Vector has funding from the federal and provincial

45
00:04:40,040 --> 00:04:48,000
government in Canada as well as a lot of industrial sponsors to both to help the kind of industrial

46
00:04:48,000 --> 00:04:52,800
business side of machine learning as well as the research end of it. Now I've taken a look

47
00:04:52,800 --> 00:04:58,080
at some of your publications and you've got a pretty broad, what appears to be a pretty broad set

48
00:04:58,080 --> 00:05:04,160
of research interests. Can you talk about where you tend to focus and some of the things that you're

49
00:05:04,160 --> 00:05:11,840
working on? I've been interested in some of the kind of long-standing problems in machine learning

50
00:05:11,840 --> 00:05:17,600
and in AI in general. Seen understanding is an example. So how can the computer really understand

51
00:05:17,600 --> 00:05:22,960
what's in the scene? How can it pick out somebody saying something of interest to you across a

52
00:05:22,960 --> 00:05:30,720
crowded room or find your friend in a crowded bar? So it's like the kinds of things that were

53
00:05:30,720 --> 00:05:38,560
quite good at have been hard for computers for many years. So that involves good learnings,

54
00:05:38,560 --> 00:05:43,040
a lot's taking a lot of data and trying to find patterns in it. And one of the things we've

55
00:05:43,040 --> 00:05:47,360
learned over the years is that the important thing that machine learning can add to this is the idea

56
00:05:47,360 --> 00:05:54,480
of learning good representations for inputs based on the data. So any kind of learning of

57
00:05:54,480 --> 00:06:00,400
representations that involves what's known as unsupervised learning where you may not be told what

58
00:06:00,400 --> 00:06:04,320
to take out of the data. You just try to do it. The system tries to figure out itself what are

59
00:06:04,320 --> 00:06:10,160
good features for the learning. And so that's my research. And more recently I'm interested in

60
00:06:10,160 --> 00:06:15,200
things where the aim is to come up with what we would call a more structured representation.

61
00:06:15,200 --> 00:06:21,360
Something where the human has some input into this and can define what it wants. So it can say,

62
00:06:21,360 --> 00:06:26,560
for example, we know that there's going to be, if we want to recognize a face that there's

63
00:06:26,560 --> 00:06:31,120
going to be certain features of the face that we think are important. So the human is giving

64
00:06:31,120 --> 00:06:36,400
some hints that the computer is then going to use. So we've made a lot of good progress on these

65
00:06:36,400 --> 00:06:40,480
kinds of things, I think, much more so than I originally thought in some ways. I thought

66
00:06:40,480 --> 00:06:45,360
it was going to be a long way off, but we're making a lot of good progress in this area.

67
00:06:46,160 --> 00:06:55,440
That example you gave about the features in the face, it does resonate with this trend that I'm

68
00:06:55,440 --> 00:07:02,560
seeing in machine learning research where we've started with these models that, in many cases,

69
00:07:02,560 --> 00:07:09,920
at least, are very much grounded in the physical world, in the case of computer vision,

70
00:07:09,920 --> 00:07:15,360
kind of feature detectors and edge detectors and things like that. We kind of swung to the

71
00:07:15,360 --> 00:07:21,200
other end of the pendulum with deep learning where we don't explicitly do any of that stuff.

72
00:07:21,200 --> 00:07:25,520
And now, more and more, we're kind of trying to find a middle balance where we're incorporating

73
00:07:25,520 --> 00:07:30,880
in our knowledge of the physical world, but still trying to use the power of deep learning.

74
00:07:32,000 --> 00:07:34,240
Does this work fit into that mold?

75
00:07:34,240 --> 00:07:40,000
Exactly. So I think that, yeah, so for an example, something that we've recently made a lot of

76
00:07:40,000 --> 00:07:46,800
progress in my group and many other groups is where the input to the learning system isn't just

77
00:07:46,800 --> 00:07:54,240
like an image or something, or, you know, a sound bite. Instead, it's something that is represented

78
00:07:54,240 --> 00:07:58,800
as a graph. So you have nodes and relations between the nodes. And so then you know,

79
00:07:58,800 --> 00:08:04,560
so you have some sort of, that's what I meant by a structured representation. And the idea is that

80
00:08:04,560 --> 00:08:08,000
then you can learn from that. So you're kind of given something, some sort of information that's,

81
00:08:08,000 --> 00:08:13,200
you know, not starting from the basics, but given that, we're able to do a lot of interesting

82
00:08:13,200 --> 00:08:19,280
processing on top of it. So I think that's something where it's combining some kind of information

83
00:08:19,280 --> 00:08:23,280
that humans have in a lot of different forms, right? We have a lot know about objects and their

84
00:08:23,280 --> 00:08:30,800
relations could be in big databases or, you know, descriptions of the world. And then we can learn

85
00:08:30,800 --> 00:08:37,360
from those on top of it. And the biggest challenge is not to start with that kind of relational

86
00:08:37,360 --> 00:08:42,240
information, but try to learn it as you go along. So that's the kind of the frontier, I would say,

87
00:08:42,240 --> 00:08:50,240
in this area. And then you've also have an interest in fairness in machine learning and of

88
00:08:50,240 --> 00:08:55,760
public several papers in that area. Can you talk a little bit about your work there? For example,

89
00:08:55,760 --> 00:09:02,400
you've got, I noticed you've got a paper that has been accepted to the upcoming NURPS conference

90
00:09:02,400 --> 00:09:06,160
that will both be added in Montreal. Maybe we can start with that one.

91
00:09:11,520 --> 00:09:16,880
And I'll interrupt if you, if you would like to kind of broadly,

92
00:09:16,880 --> 00:09:22,960
you know, contextualize your interest in that space and your, the kinds of things you're working

93
00:09:22,960 --> 00:09:27,920
on, we can start. We can sit there. So I've done some other work with some colleagues on fairness

94
00:09:27,920 --> 00:09:34,000
and was invited to a workshop a few years ago in Washington, D.C., where the discussion was about

95
00:09:34,880 --> 00:09:39,840
computer algorithms and the courts. And so this was a workshop that had people studying

96
00:09:39,840 --> 00:09:50,160
civil rights and as well as people from the legal system, judges and lawyers as well as some more,

97
00:09:50,160 --> 00:09:54,480
people who are more on the activist side of things like the Million Hoodies group. And so I think

98
00:09:54,480 --> 00:09:58,800
it was quite an interest and then there were some computer science people. And so it was an

99
00:09:58,800 --> 00:10:03,360
interesting workshop. But one of the things I learned, and a lot of debates there, one of the things

100
00:10:03,360 --> 00:10:09,040
that about how much influence computers should have and those kinds of things. So I learned a lot

101
00:10:09,040 --> 00:10:14,640
of that meeting about how computers are being used to set bail and for sentencing or at least to give

102
00:10:14,640 --> 00:10:19,360
advice. It's the kind of thing like I learned about a system in Pennsylvania where there was

103
00:10:21,040 --> 00:10:26,000
the input to the system was a description of a defendant. And the system would then come up and say,

104
00:10:26,000 --> 00:10:30,720
what's the probability that that person is going to commit a violent crime in the next few years.

105
00:10:30,720 --> 00:10:35,040
Right. So these, these kinds of systems have become quite publicized in the popular press.

106
00:10:35,040 --> 00:10:41,520
But this one was one that was developed specifically for the state of Pennsylvania. And it was

107
00:10:41,520 --> 00:10:46,400
intriguing to me because it would came up with some probability, like I would say, probability of 0.7.

108
00:10:46,400 --> 00:10:50,560
This this defendants is going to commit a violent crime. And then the question is, what does that

109
00:10:50,560 --> 00:10:55,360
mean to the judge? How does the judge use it? And was that number really meaningful?

110
00:10:56,640 --> 00:11:02,480
So one question I asked the person who was what? A description of the defendant, something about

111
00:11:02,480 --> 00:11:08,640
the defendant's history. And it wasn't that clear. They said they weren't able to get access to

112
00:11:08,640 --> 00:11:13,280
too much information about the defendant for various either privacy reasons or just not having

113
00:11:13,280 --> 00:11:18,000
having enough information. So it'd be things like what kind of crimes the person has committed,

114
00:11:18,000 --> 00:11:22,480
what they were arrested for, what they've already been convicted for, that type of thing.

115
00:11:23,920 --> 00:11:30,880
And the shocking thing to me was just how little validation had gone into the system. So there

116
00:11:30,880 --> 00:11:36,640
wasn't a real sense that they had tested the system and said that 0.7 really meant what you'd

117
00:11:36,640 --> 00:11:44,080
hope it would, which would be that that in some unseen data, 70% of the people who were assigned

118
00:11:44,080 --> 00:11:49,840
of probably 0.7, 70% of those people actually went on committed to violent crime. So you'd like to

119
00:11:49,840 --> 00:11:56,800
have some, so we call it calibration, that's a well calibrated system. And in talking to the people

120
00:11:56,800 --> 00:12:01,840
who developed it, they said there really wasn't that much enough data. And the data was quite noisy

121
00:12:01,840 --> 00:12:08,080
that they couldn't do a very confident job of calibrating it. And so my, I was very surprised

122
00:12:08,080 --> 00:12:13,840
to find that this was actually in use in the court systems in Pennsylvania. So my reaction was,

123
00:12:13,840 --> 00:12:17,440
well, we got to do a better job before machine learning people. This is at its heart, a machine

124
00:12:17,440 --> 00:12:22,320
learning problem. So we should be working on this and trying to figure out how our systems can

125
00:12:22,320 --> 00:12:27,840
interact better and do a better job working in this. So that inspired the paper that we have in

126
00:12:27,840 --> 00:12:33,760
the upcoming NURPS conference. We call it predicting responsibly. Here, a machine learning system

127
00:12:33,760 --> 00:12:37,520
isn't making the ultimate decision. It's more of an assistive tool. It's something that's

128
00:12:37,520 --> 00:12:42,400
providing an input to someone like a judge or a doctor or whatever it might be. And so you want

129
00:12:42,400 --> 00:12:49,040
that assistive tool to produce information that's useful to the final decision maker. And so what

130
00:12:49,040 --> 00:12:54,160
we were doing in this case was saying, imagine that the tool comes out and says not yes or no,

131
00:12:54,160 --> 00:12:58,160
this person is going to commit a violent crime. But it could, you know, it's not going to even

132
00:12:58,160 --> 00:13:02,080
produce a probability because it's unclear what a judge or somebody might do with that probability.

133
00:13:02,080 --> 00:13:09,440
But a simpler problem is to say yes or no or kind of pass and just pass that information on and

134
00:13:09,440 --> 00:13:15,520
allow the judge in this case to make the decision more clearly so that on their own. So it's kind

135
00:13:15,520 --> 00:13:22,000
of like an initial system that's doing a call of kind of saying, you know, we're narrowing down

136
00:13:22,000 --> 00:13:27,040
with the next step that the decision maker would have to pay attention to. That could be useful

137
00:13:27,040 --> 00:13:31,520
in a lot of settings like, you know, and there's too many inputs and you want too many applicants

138
00:13:31,520 --> 00:13:36,400
to a job and you want to have some initial calling system. But the key is this calling system,

139
00:13:36,400 --> 00:13:42,560
the first system that we're building has to be fair in some sense. It has to not just try to

140
00:13:42,560 --> 00:13:51,440
be accurate and say yes or no or, you know, pass it on, but try to take into account information

141
00:13:52,000 --> 00:13:59,360
about the defendant in this case that would explicitly try to not discriminate against that

142
00:13:59,360 --> 00:14:04,480
defendant. And that could be based on society definition of what discrimination is, right? So

143
00:14:04,480 --> 00:14:10,960
discrimination could be based on, you know, race or ethnicity or gender or various things. So

144
00:14:10,960 --> 00:14:14,960
that's what, in general, that's what the area of fairness is about is saying that, you know,

145
00:14:14,960 --> 00:14:19,680
you're training up a machine learning system and you want it to make decisions that aren't biased

146
00:14:19,680 --> 00:14:25,680
against any particular, and most of these, most of the work in fairness has been done where that

147
00:14:25,680 --> 00:14:30,480
particular group is defined ahead of time, right? We like I said, it could be race or gender

148
00:14:30,480 --> 00:14:37,440
or, you know, or age or whatever the attribute may be that you want to be fair to prevent

149
00:14:37,440 --> 00:14:43,040
discrimination against. So that's what the aim is in this general and fairness. And that's what,

150
00:14:43,040 --> 00:14:48,080
that's what we hope in this particular system is that when it makes a decision to say yes or no

151
00:14:48,080 --> 00:14:55,200
or to pass it on, it's making those decisions in a way that is fair. That's taking into account

152
00:14:55,200 --> 00:15:01,440
and making decisions that are ensuring that there's not discrimination against the, what we call

153
00:15:01,440 --> 00:15:08,400
the sensitive attribute. We talk a lot in machine learning about how important problem definition

154
00:15:08,400 --> 00:15:15,920
is and just listening to the way you are describing this system and it's clear that a lot of

155
00:15:17,120 --> 00:15:24,240
the path that you take is set out by how you, you know, define the problem more so, you know,

156
00:15:24,240 --> 00:15:31,600
perhaps than even in some other areas. Have you learned to observe anything about that particular

157
00:15:32,880 --> 00:15:38,480
challenge as applied to this space in your work? Yeah, so you've, yeah, you've hit on the kind of

158
00:15:38,480 --> 00:15:44,640
key key question in this work, which is about how do we define it? And I agree that's a,

159
00:15:44,640 --> 00:15:50,560
it's in general a machine learning problem that's often not paid that much attention to in the

160
00:15:50,560 --> 00:15:55,840
sense that, you know, we were used to machine learning defining classification problems or,

161
00:15:55,840 --> 00:16:01,280
you know, something like an image classification is this, is this a hippopotamus in the image

162
00:16:01,280 --> 00:16:06,080
or is it a dog? And for that, you can say, well, there's a right answer and you can judge whether

163
00:16:06,080 --> 00:16:11,840
there's a right answer or not. And so that's just us, you know, not, but the answers you get depend,

164
00:16:12,720 --> 00:16:19,440
depend a lot on how exactly you score your answers, right? So is it important to, you know,

165
00:16:19,440 --> 00:16:25,280
is it better to say this is a hippopotamus if the right answer is a rhino than to say it's a dog,

166
00:16:25,280 --> 00:16:30,400
for example? So if you have a scoring function that's sensitive to how similar the classes are,

167
00:16:30,400 --> 00:16:35,840
then you'll get a very different learning system than if you use a definition of an error that isn't

168
00:16:35,840 --> 00:16:42,560
sensitive to that. And that applies in spades when we come to the problem, the area of fairness,

169
00:16:42,560 --> 00:16:50,480
because, you know, it's an area that, so I started working on it about six years ago. My wife,

170
00:16:50,480 --> 00:16:56,800
Tony Potassi and I spent the summer with Cynthia Duwork and her colleagues and Microsoft Research

171
00:16:56,800 --> 00:17:02,640
in Silicon Valley and we were working on exactly trying to come up with, we landed on this problem

172
00:17:02,640 --> 00:17:06,880
with fairness and we worked hard on trying to figure out what is a good definition of fairness.

173
00:17:06,880 --> 00:17:11,760
We spent the whole summer debating it. And we ultimately came, we wrote a paper where we came up

174
00:17:11,760 --> 00:17:17,280
with two different definitions. So this is with Omar Ryan Gold and Moritz Hart in addition to

175
00:17:17,280 --> 00:17:22,960
Cynthia and Tony myself. And we wrote a paper where we came up with a group definition of fairness

176
00:17:22,960 --> 00:17:27,280
or an individual fairness. So a group definition of fairness would be one that would say,

177
00:17:27,920 --> 00:17:36,880
you know, overall, for example, decision is fair if, let's say, the two groups have the same

178
00:17:36,880 --> 00:17:44,880
number of positive outcomes assigned to them. Okay, that's a very simple thing that's like a kind

179
00:17:44,880 --> 00:17:48,960
of thing like affirmative action gets at, right? So if you want to give admittance to a school,

180
00:17:49,520 --> 00:17:55,520
you know, we have to say that the same number of males and females should get in, right? So that's

181
00:17:55,520 --> 00:18:01,120
group fair. That's an example. And then there's kind of more, the individual level of fairness is

182
00:18:01,120 --> 00:18:05,680
saying, well, what you really want is for an individual that other individuals that are similar to

183
00:18:05,680 --> 00:18:10,640
that individual should get the same outcome, the same decision, right? So that's more in the individual

184
00:18:10,640 --> 00:18:16,640
level. And since, since, so that we wrote a paper where we had that and talked about various

185
00:18:17,360 --> 00:18:23,760
ways of carrying that out. And in the main idea was that when this decision made is made,

186
00:18:23,760 --> 00:18:27,840
it should be aware of whether the person belongs to that group. So we called this fair

187
00:18:27,840 --> 00:18:33,360
through awareness. And then in the years, since then, there's been a lot of debates about what

188
00:18:33,360 --> 00:18:37,520
the proper definitions are of fairness and like different kinds of group fairness and different

189
00:18:37,520 --> 00:18:43,520
types of, you know, individual fairness. Somebody wrote a paper about the, you know, 21 definitions

190
00:18:43,520 --> 00:18:49,040
of fairness. So exactly like you said, you know, defining it is the key problem and that's what,

191
00:18:49,600 --> 00:18:54,000
that's what makes it very interesting and challenging. It's a fun area to work in partly for this

192
00:18:54,000 --> 00:18:58,640
because, you know, you can come up with a definition to debate whether it's right or wrong. I would say

193
00:18:58,640 --> 00:19:03,120
in the end, none of them are actually right, but some of them are less wrong than the others.

194
00:19:03,120 --> 00:19:09,840
And so in the paper that you were describing, one of the things that I kind of zeroed in on

195
00:19:09,840 --> 00:19:19,120
quickly in your description was this system is designed to output, yes, no, it kind of almost

196
00:19:19,120 --> 00:19:26,080
sound like yes, no, maybe yes, no pass in particular where pass is deferring on making a decision.

197
00:19:26,080 --> 00:19:40,480
And the idea there is to with the pass presumably to kind of recognize the system's own uncertainty.

198
00:19:42,240 --> 00:19:47,360
Yes, so it's a combination of things. So, yes, so people have worked on the idea of what's

199
00:19:47,360 --> 00:19:51,920
called been rejection learning in the past. And that is, you know, saying, well, we can say,

200
00:19:51,920 --> 00:19:57,440
I don't know or pass when the system's not confident, right? So back to the idea of, you know,

201
00:19:57,440 --> 00:20:02,480
the defendant is defendant going to commit a violent crime. Well, we can say pretty confidently,

202
00:20:02,480 --> 00:20:06,880
you know, within some error bars, it's going to say yes, or pretty confidently, no,

203
00:20:07,600 --> 00:20:12,320
even though you can see in those cases that confidence is a little hard to assess, but

204
00:20:13,760 --> 00:20:18,880
or and there might be a big space in the middle of between those two where the system isn't very

205
00:20:18,880 --> 00:20:24,160
confident. And so the idea would be that if it's I don't know, it gets passed on to the next

206
00:20:24,160 --> 00:20:32,320
to the decision maker. But in our in our work, the idea was that we aren't going to just pass on

207
00:20:32,320 --> 00:20:38,160
when based on uncertainty, we're going to pass also taking into account what we know about the

208
00:20:38,160 --> 00:20:44,160
downstream decision maker. So imagine that, you know, the system is trained up on judges decisions,

209
00:20:44,160 --> 00:20:50,160
and it's also trained up on or in a doctor case rate, it knows a lot about the doctors. And so

210
00:20:50,160 --> 00:20:55,680
it's going to say taking into account the kinds of decisions that the judge or the doctor tends

211
00:20:55,680 --> 00:21:01,440
to make and how, where, what kinds of problems they're accurate on, what kinds of people they're

212
00:21:01,440 --> 00:21:08,400
fair or unfair to, that the that could influence our machine learning system when it wants to say

213
00:21:08,400 --> 00:21:14,960
pass versus yes or no. Okay, so that's the key idea. So it's going to defer when it it's it'll

214
00:21:14,960 --> 00:21:19,360
be smarter in terms of when it defers based on knowing something about what's going to happen

215
00:21:19,360 --> 00:21:24,640
downstream and not only smarter in terms of being more accurate, but also less discrimination.

216
00:21:25,520 --> 00:21:32,640
So specifically, the the system, if it knows that it doesn't have a high degree of certainty

217
00:21:32,640 --> 00:21:40,320
about a particular decision, but it knows that the the decision maker is worse in a particular

218
00:21:40,320 --> 00:21:46,640
category of decision making, it might make the decision anyway. That's right. And so that's on

219
00:21:46,640 --> 00:21:53,120
the accuracy side. And it could be that if it's not very confident, but this person happens to be,

220
00:21:53,120 --> 00:21:58,880
you know, in this protected set, right? So it could be in a particular gender or something that

221
00:21:58,880 --> 00:22:04,640
in it knows that this particular decision maker happens to be discriminatory against that group,

222
00:22:04,640 --> 00:22:10,400
it could decide to make a decision rather than sit that deferring to the downstream decision maker.

223
00:22:12,560 --> 00:22:18,800
Another thing that jumped out at me here is I think in the context of kind of these examples

224
00:22:18,800 --> 00:22:24,400
with the courts, you know, what, you know, if you think about this relative to thresholds,

225
00:22:24,400 --> 00:22:31,360
and you know, say that the yes represents some kind of, you know, guilty or something with

226
00:22:31,360 --> 00:22:36,480
negative implications, right? So guilty or higher bail or longer sentencing or what have you.

227
00:22:38,400 --> 00:22:45,440
You know, the threshold ban that I might want to attribute to a yes is going to be maybe different

228
00:22:45,440 --> 00:22:50,240
than, you know, what I would attribute to a no because it has a much greater human impact.

229
00:22:50,240 --> 00:22:56,080
Does the system come for that at all? Certainly. So that goes back to our definition in some sense

230
00:22:56,080 --> 00:23:01,760
or what we would call the loss function, right? So it could be that making certain kinds of

231
00:23:01,760 --> 00:23:09,200
errors are worse than others, right? So making a false positive, like, you know, some would say,

232
00:23:09,200 --> 00:23:14,720
like letting somebody out of jail when they are going to commit the violent crime, potentially is

233
00:23:14,720 --> 00:23:21,200
worse to society than, you know, then locking up an extra person who really shouldn't have been

234
00:23:21,200 --> 00:23:26,000
locked up. That's debatable, right? But I mean, there's, but again, so it's just a lot of these

235
00:23:26,000 --> 00:23:35,760
decisions are kind of these definitions have to come from society. And so, but I think, but certainly

236
00:23:35,760 --> 00:23:40,320
on the machine learning side, it has the flexibility to do that to say, you know, what certain kinds of

237
00:23:40,320 --> 00:23:46,240
errors are more costly than others and to set the thresholds appropriately. But I mean, but that

238
00:23:46,240 --> 00:23:52,160
does lead to these questions about, you know, how are we going to decide, you know, who does what

239
00:23:52,160 --> 00:23:57,520
kind of errors are more costly, right? And so that's a, I think something that society has to weigh

240
00:23:57,520 --> 00:24:02,560
on in on. Similarly, you know, what kinds of attributes do we want to not describe,

241
00:24:02,560 --> 00:24:06,880
what kinds of people do we do not want to be biased against? It's another kind of question.

242
00:24:06,880 --> 00:24:11,440
So part of the aim here goes back to what we were saying earlier is that we really want to

243
00:24:12,160 --> 00:24:17,440
add some, some, not my view is we're trying to add some knobs to a decision-making system.

244
00:24:17,440 --> 00:24:22,720
So a machine learning system, add some knobs to a black box so that society or other people can

245
00:24:22,720 --> 00:24:27,440
come in and add some control, has some insight into what's going on and add some control to it.

246
00:24:27,440 --> 00:24:31,840
And so it's control in this way by like you're saying, you know, it could be that certain kinds

247
00:24:31,840 --> 00:24:36,240
of errors are more important and it could be that we want to reduce the bias against this group or

248
00:24:36,240 --> 00:24:42,800
that group and what does that actually mean? I'm curious for the decision maker, the system has to

249
00:24:42,800 --> 00:24:49,040
have some, you've got to be able to represent the decision maker's decisions in some ways.

250
00:24:49,040 --> 00:24:53,360
Is it you providing it a distribution or a set of rules or something else?

251
00:24:56,000 --> 00:25:00,720
Well, the decision maker is going to make its decision, you know, based on whatever features

252
00:25:00,720 --> 00:25:06,960
it was originally going to make its decision. So the, at least the way we formulated the problem

253
00:25:06,960 --> 00:25:11,920
is that our system is either going to make a decision yes or no or pass it on. And when it gets

254
00:25:11,920 --> 00:25:16,560
passed on and that the downstream decision maker, you know, gets to look at the case and just

255
00:25:16,560 --> 00:25:21,440
review it on, on their own. So they get the same kind of input features. They would have

256
00:25:21,440 --> 00:25:27,120
anyways, you know, another work that we've done in the fairness approach is that we're actually

257
00:25:27,120 --> 00:25:32,960
taking in a little bit of a different attack on this. We're saying what we'd like to do is change

258
00:25:32,960 --> 00:25:39,760
the representations of come up with new features that could be used by a downstream decision maker.

259
00:25:39,760 --> 00:25:43,280
So that's not this particular paper you were talking about, but other work that we've done in

260
00:25:43,280 --> 00:25:48,480
this area is, is about that is about constructing a different representation of somebody so that

261
00:25:48,480 --> 00:25:55,200
the downstream decision has our own representation that we've constructed available to them rather

262
00:25:55,200 --> 00:26:02,240
than the original representation. But in this paper, the model is learning based on some observations

263
00:26:02,240 --> 00:26:08,640
from the decision maker. And so I guess my, my prior question was, are you, are you handing it

264
00:26:08,640 --> 00:26:14,240
some kind of representation of the decision maker in the form of a set of rules or a distribution

265
00:26:14,240 --> 00:26:19,760
or is it more like an active learning thing where it's actually observing, you know, independent,

266
00:26:19,760 --> 00:26:23,600
I don't know, coin flips or whatever. I guess it's still a distribution of some sort.

267
00:26:23,600 --> 00:26:27,360
I mean, yes, you're right. There's lots of different ways we can formulate what the downstream

268
00:26:27,360 --> 00:26:32,320
decision maker's doing. So the way we're thinking about it would be that the

269
00:26:33,040 --> 00:26:39,200
imagine that there's a database available of, you know, some dataset available of this downstream

270
00:26:39,200 --> 00:26:44,720
decision maker or decision makers and how they've responded to cases in the past. So that's kind

271
00:26:44,720 --> 00:26:50,480
of a separate training set that we're observing that. And we're taking that into account when we

272
00:26:50,480 --> 00:26:56,400
build a model of what's the decision maker likely to say for this new, new case. And then that's

273
00:26:57,120 --> 00:27:04,160
when our system is deciding yes or no or pass, it's, it's using it, the model we've developed

274
00:27:04,160 --> 00:27:12,720
separately of that downstream decision maker. And is this, is the system is it something that

275
00:27:12,720 --> 00:27:16,160
you've implemented? Is it something that you've modeled mathematically? Like how?

276
00:27:16,160 --> 00:27:21,840
Yeah, so we, you know, we don't have good data for this. It's very hard to get, you know,

277
00:27:21,840 --> 00:27:26,000
it's something that we've implemented and we, like it, like often happens, unfortunately,

278
00:27:26,000 --> 00:27:31,120
in this fairness area is that there aren't great datasets out there. There's very few. There's one

279
00:27:31,120 --> 00:27:36,480
that's well known as this compass dataset, which is a dataset like I was talking about that looks

280
00:27:36,480 --> 00:27:41,360
at this exact case of here's a defendant. And what's probably in the system's aim is to come up

281
00:27:41,360 --> 00:27:45,200
what's the probability that person's going to commit a violent crime in the next three years.

282
00:27:45,200 --> 00:27:51,520
So that's a very well well known dataset. Some of the other datasets that are in use are ones

283
00:27:51,520 --> 00:27:56,080
that we've actually constructed ourselves where we'll take a existing dataset and then we'll make

284
00:27:56,080 --> 00:28:02,000
it into one that's relevant for fairness by identifying some sensitive attribute. Like we'll say

285
00:28:02,000 --> 00:28:08,240
there's one known as the adult income dataset where you're trying to decide the decision to be made

286
00:28:08,240 --> 00:28:13,200
is this person going to, you know, is there income greater than 50,000 or not, right? So that

287
00:28:13,200 --> 00:28:19,680
could be useful for for loans or whatever, but the we made it into a fairness dataset by saying

288
00:28:19,680 --> 00:28:25,200
taking one of the attributes gender in the gender of the person and saying we want to make sure

289
00:28:25,200 --> 00:28:30,160
that the decisions are made are not only accurate about the income, but they're also fair with

290
00:28:30,160 --> 00:28:35,840
respect to gender. So that's so we take existing datasets and they can be relevant for fairness by

291
00:28:35,840 --> 00:28:42,960
identifying one or more attributes that are, you know, the relevant things for preventing discrimination.

292
00:28:43,840 --> 00:28:50,240
So that's what we did in this case is that we took some existing datasets and implemented it.

293
00:28:50,240 --> 00:28:54,160
And then what we had to do though, we had to simulate the downstream decision maker. We had to make

294
00:28:54,160 --> 00:28:58,400
up, you know, what is this downstream decision maker? And so we tried to think of three different

295
00:28:58,400 --> 00:29:05,600
cases. One case is a downstream decision maker that has more information available than the

296
00:29:05,600 --> 00:29:11,280
the system we're building, right? So I imagine it's a judge and the judge gets to not only get the

297
00:29:11,280 --> 00:29:16,640
same kind of information our system does about the defendant, but also gets to see the person face to

298
00:29:16,640 --> 00:29:21,200
face and ask the person questions and gets additional information about the defendant that way.

299
00:29:21,200 --> 00:29:27,280
So in a way, it's a more knowledgeable judge, but let's say that judge doesn't care about being

300
00:29:27,280 --> 00:29:32,720
fair, okay? It just wants to take make make his or her best decision. So that was our model of

301
00:29:32,720 --> 00:29:38,000
one version of a decision maker. We had other ones too, ones that were intentionally unfair,

302
00:29:38,000 --> 00:29:42,160
ones that were intentionally discriminating. And so what we did is we trained up our system based

303
00:29:42,160 --> 00:29:47,520
on these different simulated downstream decision makers and then observed what would happen.

304
00:29:48,160 --> 00:29:53,760
How do you characterize the results? So the results are character, I would characterize

305
00:29:53,760 --> 00:29:58,080
as saying that it depends what you want to compare to, right? So with it, so an interesting

306
00:29:58,080 --> 00:30:06,400
comparison is to say, how would we do if we A, we could force our system to make a decision.

307
00:30:06,400 --> 00:30:11,680
So there's that and not allow it to defer, right? So that's a kind of standard machine learning

308
00:30:11,680 --> 00:30:16,160
classification problem. We can take the decision maker out of the loop and just use our system.

309
00:30:16,160 --> 00:30:20,000
The other version of it is one where we take our system out of the loop and just use the ultimate

310
00:30:20,000 --> 00:30:24,160
decision maker, right? So those are the two extremes. And then we have different versions of our

311
00:30:24,160 --> 00:30:31,280
our system, one of which just says I don't know without paying attention to what it thinks about

312
00:30:31,280 --> 00:30:37,280
the downstream decision maker. And then the fourth is the kind of ultimate goal, which is what we

313
00:30:37,280 --> 00:30:41,440
wanted to really propose, which is this idea of learning about the downstream decision maker

314
00:30:41,440 --> 00:30:48,000
and taking that model into account in making our decisions. And in general, and we looked at

315
00:30:48,000 --> 00:30:54,640
these different scenarios, and in general, the effect was we wanted in the sense that we were able to

316
00:30:55,040 --> 00:31:02,560
achieve a pretty good tradeoff of fairness and accuracy using the system as we had formulated it.

317
00:31:02,560 --> 00:31:07,120
It's certainly better than either just relying on the downstream decision maker or relying on our

318
00:31:07,120 --> 00:31:12,960
system alone. And the interaction where we were able to take it, build the model of the downstream

319
00:31:12,960 --> 00:31:18,160
decision maker that was pretty good did improve overall both in terms of accuracy and fairness.

320
00:31:19,120 --> 00:31:26,320
I came across another paper that you worked on recently learning adversarially fair and transferable

321
00:31:26,320 --> 00:31:32,240
representations. Can you give me an overview of that one? So that one is along the lines of what I

322
00:31:32,240 --> 00:31:38,560
was mentioning earlier where the goal now of the system isn't necessarily to make a decision,

323
00:31:38,560 --> 00:31:43,200
but rather to come up with a representation. So this fits in with what we were talking about early

324
00:31:43,200 --> 00:31:49,280
on that machine learning systems. One of the big advances that we've had is in coming up with

325
00:31:49,280 --> 00:31:54,640
representations, right? So learning good features for visual recognition or for speech recognition.

326
00:31:56,160 --> 00:32:01,600
And so this is along those same lines. And so what we've worked on for several years is what we

327
00:32:01,600 --> 00:32:07,840
call fair representations. And that is coming up with representations for, let's say, individuals in

328
00:32:07,840 --> 00:32:16,160
this case rather than images or text bite sound bites that are fair in some sense. So and obviously

329
00:32:16,160 --> 00:32:21,120
again, it depends on what your definition of fair, but one intuition there would be what we'd like

330
00:32:21,120 --> 00:32:28,560
to do is have a representation of an individual that is clean of that doesn't have any information

331
00:32:28,560 --> 00:32:33,840
that kind of obfuscates any information about whatever the sensitive attribute is, right? So

332
00:32:33,840 --> 00:32:38,640
for instance, we'd like to come up with a representation of you where it's not clear if you're

333
00:32:38,640 --> 00:32:48,000
a male or female because if we want to ensure that that any classifier using that representation

334
00:32:48,000 --> 00:32:54,000
will not have be able to discriminate against you based on your gender, then we want to remove

335
00:32:54,000 --> 00:32:59,120
all information about gender in the representations. So then that will ensure that the downstream

336
00:32:59,120 --> 00:33:04,400
classifier won't be able to make a decision about you based on your gender. So that's the

337
00:33:04,400 --> 00:33:10,960
notion of fair representations. You kind of remove any information about gender before handing off

338
00:33:10,960 --> 00:33:16,880
that representation to a classifier. You mentioned an image previously. Is that typically the domain

339
00:33:16,880 --> 00:33:21,440
that you're working in for these fair representations like something that's characterizing, you know,

340
00:33:21,440 --> 00:33:29,360
that starts from an image of me and then generates a genderless or raceless image, or is it more abstract

341
00:33:29,360 --> 00:33:36,480
like you you're looking for some embedding space that isn't correlated with race or gender or

342
00:33:36,480 --> 00:33:41,360
things like that? Yeah, so it's more of the embedding space that isn't correlated with race or

343
00:33:41,360 --> 00:33:46,800
gender, and generally so far we aren't starting with though the original representation isn't an

344
00:33:46,800 --> 00:33:51,600
image. It's, you know, think of it as a database record of your demographics or something, right?

345
00:33:51,600 --> 00:33:56,800
It's about where you live and how old you are. This is something that you might be using

346
00:33:57,920 --> 00:34:01,600
you know, and another kind of setting that's not based on images or hearing your voice or anything.

347
00:34:01,600 --> 00:34:08,720
So it's, you know, a typical kind of let's say demographic record of an individual and that we

348
00:34:08,720 --> 00:34:13,760
want to do is take that demographic record record and construct a representation of it like an

349
00:34:13,760 --> 00:34:20,160
embedding of it as you describe where that embedding has lost information about your gender or your

350
00:34:20,160 --> 00:34:26,320
race or something like that. Okay, so that's where they identified attribute we've removed information

351
00:34:26,320 --> 00:34:31,360
from that. So that was, so that's the idea of the fair representation. So our paper now it's

352
00:34:32,240 --> 00:34:38,960
paper with David Madras and Elliot Krieger and as well as Tony and myself and that paper the idea

353
00:34:38,960 --> 00:34:44,080
was exactly, as you described, you want to cover it in embedding that doesn't have information about

354
00:34:44,080 --> 00:34:51,600
that attribute, let's say it's gender. And the way to do that is to construct an adversary where

355
00:34:51,600 --> 00:34:57,600
the adversary is going to take the representations or embeddings that we construct and try to

356
00:34:58,240 --> 00:35:05,280
pull out the information to figure out what is the gender of that individual. So we're trying to

357
00:35:05,280 --> 00:35:10,160
create a good embedding that will thwart this adversary, make it impossible for it to pull out that

358
00:35:10,160 --> 00:35:17,120
information. And the interesting thing about this is, so that idea is, you know, this adversarial

359
00:35:17,120 --> 00:35:23,600
approach, but we can then come up with a, there's been, as I said, there's been a bunch of

360
00:35:23,600 --> 00:35:28,640
definitions of fairness that have gone beyond the original kind of group fairness that we described.

361
00:35:28,640 --> 00:35:34,480
And so one of them is, it's called equalized odds. Another way of saying that is, you can think

362
00:35:34,480 --> 00:35:39,520
of it as a balanced errors. So rather than making decisions that are balanced between the two groups,

363
00:35:39,520 --> 00:35:45,360
you know, like 60% of the people of the males and females are going to get in. Instead, another

364
00:35:45,360 --> 00:35:52,000
criteria for fairness is that you want that when the system makes an error, those errors are balanced.

365
00:35:52,000 --> 00:35:56,720
So, you know, whatever the errors are, half of the errors are for males and half of the errors are

366
00:35:56,720 --> 00:36:01,840
for females. Or, you know, if they have the same kind of base rate or, you know, if they're 70%

367
00:36:01,840 --> 00:36:08,080
as many males in the databases as females and 30% females, then 70% of the errors are males

368
00:36:08,080 --> 00:36:13,920
and 30% of them are females. Okay, so that's a different fairness criteria. And we can,

369
00:36:13,920 --> 00:36:19,280
and we can adjust our adversary to reflect that fairness criteria. So it doesn't have to be that,

370
00:36:19,280 --> 00:36:25,840
so it could be that it's only going to try to extract the information about gender on the error

371
00:36:25,840 --> 00:36:30,320
cases. Okay, so that's the, and we want to thwart the adversary from doing that. So all I'm saying is

372
00:36:30,320 --> 00:36:36,160
that the, you know, we can kind of tailor our system, our adversary to different definitions

373
00:36:36,160 --> 00:36:42,480
of fairness and then train it up in that way. And so that's the main idea and what we call laughter

374
00:36:42,480 --> 00:36:48,160
learning adversarily, fair and transferable representations. And the key notion is that,

375
00:36:48,160 --> 00:36:54,720
so why do we want to come up with these representations that are fair is that this transfer idea.

376
00:36:54,720 --> 00:36:59,280
So it might be that we want to use that same representation in other settings. So we might want

377
00:36:59,280 --> 00:37:04,400
to say now I have this individual, I've taken your demographic information, I've taken out your

378
00:37:04,400 --> 00:37:12,320
gender information. And now I want to see if, and now maybe many different advertisers may want

379
00:37:12,320 --> 00:37:18,000
to decide whether they're going to advertise to you or not. And by making this new representation

380
00:37:18,000 --> 00:37:22,240
that doesn't have your gender information, we're ensuring that all of those different advertisers

381
00:37:22,240 --> 00:37:28,560
when they work on that on that new representation won't be able to discriminate against your,

382
00:37:28,560 --> 00:37:33,600
based on your gender. So that's the transfer ideas that we can take that same representation and

383
00:37:33,600 --> 00:37:39,200
use it in different settings for different kinds of classification problem. And if you come across

384
00:37:39,200 --> 00:37:47,440
anyone doing something like that in practice, you know, identifying some representation that

385
00:37:47,440 --> 00:37:53,920
is fair in this way and using that for downstream decisioning. I've talked to a number of people

386
00:37:53,920 --> 00:37:58,880
in companies where they're certainly concerned about fairness and trying to ensure that they have

387
00:37:58,880 --> 00:38:05,760
a classification system that is fair. And so they're interested in this idea of having a representation

388
00:38:05,760 --> 00:38:11,600
that would be kind of, you could give a stamp of, you know, a fairness to that representation that

389
00:38:11,600 --> 00:38:15,840
they could use multiple settings. I don't know anybody's actually using that idea in practice,

390
00:38:15,840 --> 00:38:20,800
but certainly there are more and more interest in companies where they want to be able to

391
00:38:20,800 --> 00:38:27,200
A, assess to what extent is their system fair that they're using and, you know, improve its

392
00:38:27,200 --> 00:38:31,520
fairness. And so they, I think this notion that you have a representation that could be used

393
00:38:31,520 --> 00:38:38,880
for multiple, multiple settings, multiple problems is of interest to people, but I don't know

394
00:38:38,880 --> 00:38:46,160
if anybody actually doing that yet. You mentioned the assessment piece. So this can be used,

395
00:38:46,160 --> 00:38:53,920
the, this method could be used for assessment independent of whether you're actually using

396
00:38:53,920 --> 00:39:01,040
these kind of representations downstream, right? That's right. I mean, you can evaluate the kind

397
00:39:01,040 --> 00:39:04,160
of, you know, based on whatever definition of fairness you might want to have, you could say,

398
00:39:04,160 --> 00:39:09,200
how well does this, you know, to what extent is this system violating that, right? So that's

399
00:39:09,200 --> 00:39:15,040
kind of like gives you an idea that says it, you know, are they errors? One definition I mentioned

400
00:39:15,040 --> 00:39:21,200
is balanced errors, definition, or equalized odds. You can say how much is our current system

401
00:39:21,200 --> 00:39:27,280
violating that? It's a little bit hard to quantify, you know, to the degree, but you can say

402
00:39:27,280 --> 00:39:33,440
on what percentage of the cases is it violating that or having out. So, you know, so there,

403
00:39:33,440 --> 00:39:38,480
so certainly people are interested in this notion that you can kind of audit an existing system

404
00:39:38,480 --> 00:39:44,080
and say how well is it doing based on various fairness metrics? So we're talking about a couple

405
00:39:44,080 --> 00:39:51,920
of your recent papers in this space. Can we take a couple of minutes and maybe get your perspective

406
00:39:51,920 --> 00:39:56,800
on the broader landscape around fairness and machine learning and what some of the big

407
00:39:56,800 --> 00:40:02,640
challenges and opportunity areas are? Yeah. So I think fairness is a kind of interesting

408
00:40:02,640 --> 00:40:07,600
microcosm of the machine learning in general, right? So some of the same ideas we've talked about

409
00:40:07,600 --> 00:40:13,360
about adversarial ideas is reflected in general and in machine learning, trying to come up with

410
00:40:13,360 --> 00:40:20,400
representations that are have identify either, you know, separate out or in our case, reduce the

411
00:40:21,840 --> 00:40:27,920
information in a particular case. And I think that's true of in general, but the rest of the

412
00:40:27,920 --> 00:40:33,520
fairness work these days. So one one interesting bit of work I would highlight is on fairness and

413
00:40:33,520 --> 00:40:40,160
causality. So in general, machine learning is very good at pulling out patterns and saying what

414
00:40:40,160 --> 00:40:45,360
which things are what aspects of data are correlated with some label movement want to give.

415
00:40:46,240 --> 00:40:50,560
But there's a big push now to try and come up with more causal reasoning. So it's not just

416
00:40:50,560 --> 00:40:54,640
that things are correlated, but they're actually causal and that would enable us to do

417
00:40:56,080 --> 00:40:59,760
counterfactual reasoning, right? So not only if you have something that's causal, you could say,

418
00:40:59,760 --> 00:41:06,880
well, what would hypothetically happen if we flip that that bit and one of our causes changed it.

419
00:41:06,880 --> 00:41:12,640
What do we expect to happen, right? It's a big challenge for the field because generally that

420
00:41:12,640 --> 00:41:17,200
means that kind of data isn't available in most cases. And so it's, you know, there aren't great

421
00:41:17,200 --> 00:41:23,520
data sets for it. And it's something that's, but it is an aim that's generally true in machine

422
00:41:23,520 --> 00:41:28,320
learning. And I think it's really important for the case of fairness because understanding the

423
00:41:28,320 --> 00:41:34,560
underlying causes for why some decision is made may be an important step to try to reduce

424
00:41:34,560 --> 00:41:39,680
discrimination. So that's a, I think a very, very interesting area of research. We actually have

425
00:41:39,680 --> 00:41:44,880
a paper in that direction coming out at the next fat star conference, but I think there's been

426
00:41:44,880 --> 00:41:50,320
great work in the field. Some number of papers on things called counterfactual fairness in other

427
00:41:50,320 --> 00:41:54,960
areas and other kinds of papers in that. That's one thing I would say is causality is another one

428
00:41:54,960 --> 00:41:59,920
which is there's a simplification I've mentioned, which is that you have a single sensitive attribute

429
00:41:59,920 --> 00:42:06,720
like its gender or race and in the real world, we know there's many sensitive attributes and

430
00:42:06,720 --> 00:42:13,200
we might want to be ensuring that the system is fair with respect to several at a time.

431
00:42:13,200 --> 00:42:19,840
And that in itself is not easy. So there's some nice work out of Stanford and some parallel

432
00:42:19,840 --> 00:42:25,440
work out of Penn research groups. Well, something they call fairness gerrymandering, which is,

433
00:42:25,440 --> 00:42:30,080
you know, just a cute name where it means like you can be fair with respect to one dimension.

434
00:42:30,080 --> 00:42:34,480
And that may make it less fair with respect to another. So how can you simultaneously be fair

435
00:42:34,480 --> 00:42:39,760
with respect to several attributes? So I think that's an important and interesting area in that.

436
00:42:39,760 --> 00:42:44,800
It also makes me wonder if some of the recent work happening around multitask learning could be

437
00:42:44,800 --> 00:42:51,120
applied in this space. Yeah. So I think the multitask learning was part of our inspiration for the

438
00:42:51,120 --> 00:42:55,520
the laughter thing, right? Which is your transfer ability, right? So you want to have a same representation

439
00:42:55,520 --> 00:43:02,160
that's useful for multiple tasks in that case. And I think the multitask, in this case, it's kind

440
00:43:02,160 --> 00:43:06,320
of harder than multitask. So I think when you have this multi-attributes, you want to be fair

441
00:43:06,320 --> 00:43:12,160
against all these different attributes. And the real challenge is that it's kind of like,

442
00:43:13,440 --> 00:43:16,480
it's also related to these other areas of machine learning. Like I said that, you know,

443
00:43:16,480 --> 00:43:20,480
I think it's a microcosm machine learning. It's a lot of work and few shot learning where you want

444
00:43:20,480 --> 00:43:26,400
to be able to learn with little data that's available, little label data. So it may be that you have

445
00:43:26,400 --> 00:43:31,120
some attribute that you want to learn, that you want to be fair to, but you have very little data

446
00:43:31,120 --> 00:43:35,680
available for that. So how can we, that really taxes a machine learning system, right? It's

447
00:43:35,680 --> 00:43:42,480
how it's very hard to do that. And then there's the kind of big open question. Maybe we don't really

448
00:43:42,480 --> 00:43:47,520
know what what attributes we want to be fair to. But you know, so rather than humans deciding

449
00:43:47,520 --> 00:43:51,040
that it's gender and race that are important, it could be some other group that's actually

450
00:43:51,040 --> 00:43:55,200
being discriminated against. And how do we, you know, so that's a bigger kind of open question of,

451
00:43:55,200 --> 00:44:01,680
you know, undefined attribute discrimination. And people are thinking about that these days as well.

452
00:44:01,680 --> 00:44:08,880
So I think those are some of the main areas I would say of in fairness that are currently

453
00:44:08,880 --> 00:44:13,760
of interest. And like I said, there's a lot of work that's paralleling what's going on and

454
00:44:13,760 --> 00:44:19,440
other parts of machine learning in the fairness literature. Any recommended starting places for

455
00:44:19,440 --> 00:44:26,560
folks that want to learn more or dive more deeply into this area? I know there's a book that's

456
00:44:26,560 --> 00:44:35,280
being put out by Solon Barakus and more its heart on fairness. They've developed it online.

457
00:44:35,280 --> 00:44:43,920
It's available. There's a number of very nice invited talks from people Kate Crawford gave

458
00:44:43,920 --> 00:44:50,320
one at NURPers last year. And in terms of papers, I think, you know, there's a fair number of

459
00:44:50,320 --> 00:44:56,800
papers that are looking at the definitions of fairness and interesting questions about

460
00:44:56,800 --> 00:45:02,880
incompatibility of fairness. So some some work there. John Kleinberg with some colleagues

461
00:45:02,880 --> 00:45:11,760
had some papers. So I would say that in these days, what's interesting is it used to be there

462
00:45:11,760 --> 00:45:15,840
were one or two papers on fairness a year and now if you go to machine learning conference

463
00:45:15,840 --> 00:45:20,880
like ICML or NURPers, you'll see there's five or six at least maybe eight or ten papers these

464
00:45:20,880 --> 00:45:25,520
days. So, you know, I think picking up some of the recent papers in any of these areas is a good

465
00:45:25,520 --> 00:45:30,400
good starting place. So that's my recommendation. Well, Rich, thanks so much for taking the time

466
00:45:30,400 --> 00:45:34,080
out to chat. I really enjoyed it. Sure, then fun. Thanks.

467
00:45:37,360 --> 00:45:43,120
All right, everyone, that's our show for today. For more information on Rich or any of the topics

468
00:45:43,120 --> 00:45:50,880
covered in this episode, visit twomlai.com slash talk slash 2009. Thanks once again to the great

469
00:45:50,880 --> 00:45:56,240
folks at Georgian Partners for their sponsorship of this series. Be sure to visit twomlai.com slash

470
00:45:56,240 --> 00:46:02,160
Georgian for more information on their building conversational AI teams guidebook. As always,

471
00:46:02,160 --> 00:46:32,000
thanks so much for listening and catch you next time.

