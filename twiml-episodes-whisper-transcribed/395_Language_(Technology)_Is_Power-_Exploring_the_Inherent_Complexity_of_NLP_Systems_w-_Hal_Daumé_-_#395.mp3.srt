1
00:00:00,000 --> 00:00:12,160
All right, everyone. I am here with Hal Daume. Hal is a professor at the University of Maryland

2
00:00:12,160 --> 00:00:18,640
and a senior principal researcher with Microsoft Research. Hal, welcome to the Twimal AI podcast.

3
00:00:18,640 --> 00:00:25,360
Yeah, thanks for having me. I'm excited. I am super excited as well. We typically start these

4
00:00:25,360 --> 00:00:33,040
conversations with a bit of background. What set you off on the path of exploring language

5
00:00:33,040 --> 00:00:37,920
and machine learning and fairness and bias and ethics and all these cool things that we're going

6
00:00:37,920 --> 00:00:46,240
to be talking about for the next few minutes? Yeah, I mean, I guess I've kind of always been

7
00:00:46,240 --> 00:00:50,960
interested both in language and then also in sort of like math computer science stuff.

8
00:00:50,960 --> 00:00:56,640
And it was really only in like my last year in college that I even discovered that these two

9
00:00:56,640 --> 00:01:02,720
things could go together. Like I was majoring in math. I had originally started minoring and

10
00:01:02,720 --> 00:01:08,080
creative writing, but it was really hard to get into the classes if you weren't actually a major.

11
00:01:10,000 --> 00:01:15,680
And yeah, so then like my last year of college I was talking to one of my friends and he's,

12
00:01:15,680 --> 00:01:19,600
he also was interested in math and computer science and language and he's like, hey, have you heard

13
00:01:19,600 --> 00:01:26,640
of natural language processing? What is that? And I was actually at Carnegie Mellon at the time.

14
00:01:26,640 --> 00:01:33,120
I mean, this was a long time ago, like way before, you know, NLP was kind of a known thing and

15
00:01:33,120 --> 00:01:38,240
they have a really big group and they did it at the time and so I got involved and I basically

16
00:01:38,240 --> 00:01:45,600
in like six months transitioned from like planning to apply to grad school in like pure math to

17
00:01:45,600 --> 00:01:50,240
applying to grad school in like linguistics and computer science programs. Oh, wow. I thought

18
00:01:50,240 --> 00:01:54,880
you were going to say planning to apply for, you know, fine arts programs. No, okay, not that.

19
00:01:54,880 --> 00:02:00,880
That would be quite a swing in six months. Yeah, so yeah, so then I went to grad school,

20
00:02:00,880 --> 00:02:07,120
started doing NLP and really didn't do anything in, you know, the bias car in a space until,

21
00:02:07,920 --> 00:02:15,200
you know, the past three or four years, but I think one of the things that I guess they're sort

22
00:02:15,200 --> 00:02:19,840
of two reasons. So one is, you know, sort of like obviously super important and the world and

23
00:02:19,840 --> 00:02:25,040
as NLP techniques and systems start having more impact, like I think it's important to consider

24
00:02:25,760 --> 00:02:30,800
like how that impact is actually affecting people. But then the other is that,

25
00:02:32,800 --> 00:02:37,440
I think that, you know, like I said, like I've always been interested in language and

26
00:02:37,440 --> 00:02:46,640
I think a lot of times, a lot of NLP work can end up a little bit like distant from the language

27
00:02:46,640 --> 00:02:52,880
itself. Like I really like the machine learning side too and like the map aside and things like

28
00:02:52,880 --> 00:02:58,720
that. But I think often that like the actual language gets lost. It's like, you know, okay,

29
00:02:58,720 --> 00:03:05,280
let X be a sentence and then you sort of proceed from there. And I think for me, like a lot of

30
00:03:05,280 --> 00:03:12,560
what makes, you know, sort of this like bias and NLP space really exciting is that it's like another

31
00:03:12,560 --> 00:03:17,440
way to like start thinking again really deeply about like what does language mean? How is it used,

32
00:03:17,440 --> 00:03:24,560
like how is it used in society and things like that? So I see it for me as like kind of a way of

33
00:03:24,560 --> 00:03:31,760
like re-steering back toward my language interests rather than sort of just focusing on the more

34
00:03:31,760 --> 00:03:37,600
mouthy side. At a great recent conversation with Emily Bender on just that topic, you know,

35
00:03:37,600 --> 00:03:44,880
have linguistics been, have linguists been, you know, unfortunately absent from, you know, NLP

36
00:03:44,880 --> 00:03:56,480
research and the like, my sense is that is your background kind of traditional linguistics or

37
00:03:56,480 --> 00:04:02,800
you came up much more on the computational machine learning side. Yeah, I came up much more on

38
00:04:02,800 --> 00:04:07,040
the computational machine learning side. Like I took a couple linguistics classes as an undergrad

39
00:04:07,040 --> 00:04:11,920
and a couple as a grad student. And then basically like four years, three or four years ago,

40
00:04:11,920 --> 00:04:16,560
when I first started getting into this, I basically bought like a giant stack of social

41
00:04:16,560 --> 00:04:22,720
linguistics books and just like read through all of them. Which was rough going at the start,

42
00:04:22,720 --> 00:04:29,040
because it's always hard to read stuff outside of your area. But yeah, so I definitely don't have

43
00:04:29,040 --> 00:04:37,680
anything like a formal linguistics training. And you're also a program co-chair for this year's

44
00:04:37,680 --> 00:04:44,960
ICML conference. I can only imagine that at the time you signed up for this, you had no idea what

45
00:04:44,960 --> 00:04:54,480
you were getting into on multiple levels. Yeah, that's an understatement. Yeah, I mean, I think,

46
00:04:54,480 --> 00:05:01,040
you know, so I'm doing this with Artie Singh as my program co-chair. And, you know, at the beginning

47
00:05:01,040 --> 00:05:05,520
of the process, when we both agreed, we had this like big brainstorming document. It's like,

48
00:05:05,520 --> 00:05:10,160
oh, here's like all the stuff we want to try like tweaking and experiments we want to do and stuff

49
00:05:10,160 --> 00:05:16,640
like that. And then, you know, and so there's a lot of stuff that we've done intentionally.

50
00:05:17,600 --> 00:05:24,720
There's also a lot of stuff that happened and that required adaptation. But, you know, I mean,

51
00:05:24,720 --> 00:05:30,160
there's been a lot of work, but it's it's also been like pretty rewarding and interesting. And,

52
00:05:30,160 --> 00:05:34,720
you know, working with all the committee members has been great. So when does that effort start?

53
00:05:34,720 --> 00:05:42,160
Does it start when the previous conference ends or before that or? It basically, I can't

54
00:05:42,160 --> 00:05:47,200
remember exactly when I signed on. I mean, it starts reasonably soon after the previous conference ends.

55
00:05:47,200 --> 00:05:53,680
I think the last one was over like end of July last year. And I think I got asked probably in August

56
00:05:53,680 --> 00:05:57,680
or September or something. So it's it's basically a year. Okay. Actually, no, that's,

57
00:05:57,680 --> 00:06:09,200
it's not true. I think that's true. And for those who, well, whenever you hear this, it'll be

58
00:06:09,200 --> 00:06:18,720
after today, we are speaking immediately before the conference starts. So congratulations on heroic

59
00:06:18,720 --> 00:06:24,320
organization that you're doing something like this, you know, a conversation like this before the

60
00:06:24,320 --> 00:06:36,800
the big show. Yeah. Sometimes you need breaks. Awesome. So it is a bunch of stuff that I'm curious

61
00:06:36,800 --> 00:06:42,640
about about the conference. Maybe we'll talk about that at some point, but I want to dig a little

62
00:06:42,640 --> 00:06:51,280
bit deeper into the work that you've been doing around language and machine learning and fairness

63
00:06:51,280 --> 00:06:59,840
and bias. And you kind of mentioned that a good part of it comes from, you know, this belief around

64
00:06:59,840 --> 00:07:06,320
the importance of language and kind of all that it, you know, means for us. Can you elaborate on that

65
00:07:06,320 --> 00:07:17,440
a little bit? Yeah. So, you know, I think that there's lots of stuff that makes language really

66
00:07:17,440 --> 00:07:24,480
interesting. I mean, I think it's generally considered that it kind of, you know, it's part of

67
00:07:24,480 --> 00:07:32,320
what sets humans apart. It's, it's arguably sort of a major way that, you know, we've been able

68
00:07:32,320 --> 00:07:37,760
to advance so far as a species because we don't have to like relearn everything immediately. Like

69
00:07:37,760 --> 00:07:43,120
when we're born, like we can actually, you know, read and be told things and like learn things that

70
00:07:43,120 --> 00:07:51,680
came before, you know, it's generally the primary way that we communicate, although there's

71
00:07:51,680 --> 00:08:02,560
certainly lots of non-linguistic communication that goes on too. And I think from, you know, there's

72
00:08:02,560 --> 00:08:08,320
sort of like two high level things I'm interested in. So one is like, you know, how do you use language to

73
00:08:08,320 --> 00:08:15,280
do, like as a way of interacting with the world. So like, do you, you know, if I have like a

74
00:08:17,680 --> 00:08:21,840
smartest assistant or a robot or something like that? And I want to communicate with it, like

75
00:08:21,840 --> 00:08:27,760
language is certainly a very natural way to do this for most of the world's population.

76
00:08:29,600 --> 00:08:36,240
So that's sort of like, you know, how do I use language to like operate devices? And then on

77
00:08:36,240 --> 00:08:43,200
the other side, it's sort of like the bias fairness side. I think it's generally considered like

78
00:08:43,200 --> 00:08:50,480
in social linguistics land that language serves kind of two roles in society. So one is it's a

79
00:08:50,480 --> 00:08:58,320
way by which we construct our own social identities. So, you know, there's sort of like contextualized

80
00:08:58,320 --> 00:09:04,160
things like, you know, if I'm sending an email to my department chair versus, you know, a grad

81
00:09:04,160 --> 00:09:09,200
student I work with, like the language that I use in those is going to be different because of like

82
00:09:09,200 --> 00:09:16,800
the respective roles that we have. But then, you know, more broadly, like, you know, I had this experience

83
00:09:18,640 --> 00:09:25,920
like 17 years ago. I was an intern at Microsoft actually over the summer. And I was giving a talk.

84
00:09:25,920 --> 00:09:33,280
I was like a second year grad student at the time. And one of the people in the audience,

85
00:09:33,280 --> 00:09:38,320
who was like a relatively senior guy raised his hand, like 10 minutes in, he's like, how did you

86
00:09:38,320 --> 00:09:46,160
grow up in Los Angeles? And I was like, yes. And then he proceeded to explain that he had heard

87
00:09:46,160 --> 00:09:52,880
this story on NPR that morning about like the LA dialect and LA accent. And he was like, he thought

88
00:09:52,880 --> 00:09:58,400
he was identifying it and he wanted to check that he had identified it properly. And so I guess

89
00:09:58,400 --> 00:10:04,240
he had, but that also made me incredibly self conscious, don't even talk about like how I was

90
00:10:04,240 --> 00:10:10,000
speaking. But like in general, like whether it's conscious or subconscious, like the way that we

91
00:10:10,000 --> 00:10:17,680
speak, this is often called like how we do language. So the way we do language is as much a part of

92
00:10:17,680 --> 00:10:24,640
what we're saying as like the actual content itself. So there's this whole sort of like constructing

93
00:10:24,640 --> 00:10:30,720
identity side of language. And then the other side is, you know, like I was saying, you know,

94
00:10:30,720 --> 00:10:37,440
we learn things from history. You know, not all of those things we might think of as like

95
00:10:37,440 --> 00:10:46,880
normatively good. So we also learn like stereotypes. And I don't know, like sort of like, I don't

96
00:10:46,880 --> 00:10:54,160
know, I can't think of another end. But yeah, so we learn like a lot of like group membership and

97
00:10:54,160 --> 00:10:59,040
stereotype and things like that through language. And so like a lot of the stuff that you see in

98
00:10:59,040 --> 00:11:07,440
sort of the bias and NLP space is kind of trying to tease apart like how much of those stereotypes

99
00:11:07,440 --> 00:11:12,160
are being like picked up by language systems, whether they're gender stereotypes or racial stereotypes,

100
00:11:13,040 --> 00:11:18,240
or whatever. And I think one of the things that makes language really interesting here is that like

101
00:11:18,240 --> 00:11:27,600
because it serves these dual roles, it means that I think it like significantly complicates

102
00:11:28,960 --> 00:11:36,960
like what you might want to do to mitigate harms that result from an NLP system. Because you

103
00:11:36,960 --> 00:11:42,240
know, maybe you want to say like, okay, I don't want my system to contain these like gender

104
00:11:42,240 --> 00:11:51,280
racial stereotypes. But then you run the risk in trying to like mitigate those, you run the risk

105
00:11:51,280 --> 00:11:56,560
of like minimizing the ability of users of your system to like express their own identity,

106
00:11:56,560 --> 00:12:01,680
because that's also embedded in language. And so like trying to figure out like, how do you

107
00:12:01,680 --> 00:12:09,600
build systems that like let people talk how they want to talk as part of how they do language. But

108
00:12:09,600 --> 00:12:19,440
then also minimize the harms that arise because of whatever societal stereotypes these systems

109
00:12:19,440 --> 00:12:26,960
have picked up. I think that's like a really interesting divide to walk. And it makes everything

110
00:12:26,960 --> 00:12:35,120
really complicated. And so that's why I like it. It does make a lot of things complicated. I'm

111
00:12:35,120 --> 00:12:42,640
thinking of, well, there's, you know, there's, as you know, there's been a lot of discussion in

112
00:12:42,640 --> 00:12:51,520
this area recently on Twitter and in other places. And I'm thinking of a tweet. I think Robert

113
00:12:51,520 --> 00:13:02,880
Ness was just commenting about, you know, how it was in the context of the Jan LaCoon and the

114
00:13:06,800 --> 00:13:12,640
front, I remember the name of the the upsampling algorithm conversation that

115
00:13:12,640 --> 00:13:19,520
right, a couple of weeks ago, Pulse, I think. In any case, he was talking about Robert was

116
00:13:19,520 --> 00:13:26,960
referencing the idea that, you know, it's hard enough for him as an individual to try to make,

117
00:13:26,960 --> 00:13:34,400
you know, assessments around, you know, race and identity and these things to let alone,

118
00:13:34,400 --> 00:13:40,160
you know, expect a computer system natural language processing. In this case, or computer vision,

119
00:13:40,160 --> 00:13:46,800
in that case, to, you know, try to be able to do that based on today's technology, you know,

120
00:13:46,800 --> 00:13:51,680
he was talking about it in the context of, you know, how many labels would you have to,

121
00:13:52,240 --> 00:13:58,560
how many, you know, images would you have to label and, you know, how would you, you know, train

122
00:13:58,560 --> 00:14:03,680
folks to label those images even? What is ground truth when we're talking about, you know,

123
00:14:03,680 --> 00:14:10,720
identity and, you know, race and things like that? Is that, is that problem easier or more

124
00:14:10,720 --> 00:14:19,440
difficult than an LP? I mean, I don't know that it's easier. I think it's like somewhat different.

125
00:14:20,400 --> 00:14:27,280
You know, I think that, you know, if we let's take like races, an example, right? So if we

126
00:14:27,280 --> 00:14:33,680
think about something like various Englishes that are spoken in the United States,

127
00:14:33,680 --> 00:14:42,320
there's several that would be categorized as, for instance, like, let's say like Chicano

128
00:14:42,320 --> 00:14:48,000
Englishes or African American English or, you know, what's sometimes called mainstream US English,

129
00:14:48,000 --> 00:14:59,520
I've also heard it called hegemonic English. But, you know, and these things are closely related

130
00:14:59,520 --> 00:15:08,320
with like how like US society constructs race. But they're not the same thing. You know,

131
00:15:08,320 --> 00:15:13,280
they're African Americans in the United States who don't speak African American English.

132
00:15:14,160 --> 00:15:21,200
They're white people in the United States who do. And so I think that like first we have to

133
00:15:21,200 --> 00:15:26,880
like be kind of careful about like what categories we're talking about and like what they mean.

134
00:15:26,880 --> 00:15:34,800
And, you know, certainly avoiding making assumptions about like how people do language as a result

135
00:15:34,800 --> 00:15:44,960
of like how we've like categorized them. But I think like getting back to the point, I think the

136
00:15:46,480 --> 00:15:56,080
the challenge that I see at least is like, I want systems that sort of work for whomever like

137
00:15:56,080 --> 00:16:02,320
kind of regardless of how they want to do language. So, you know, one of the, sorry, I don't know if

138
00:16:02,320 --> 00:16:09,040
you can hear the siren in the background. One of the things that like a handful of people

139
00:16:09,040 --> 00:16:15,360
have studied. So I'm thinking of like work by Brandeis Marshall. So she's looked at, for instance,

140
00:16:16,400 --> 00:16:24,080
the degree to which systems that don't work well on African American English on Twitter,

141
00:16:24,080 --> 00:16:31,440
they're essentially mean that like voices are not counted because they're not identified as,

142
00:16:31,440 --> 00:16:38,880
you know, even speaking English or other sorts of harms that come up. So, you know, if you have,

143
00:16:40,560 --> 00:16:44,880
if you're running some word processing system and so this, and you,

144
00:16:44,880 --> 00:16:56,560
I don't know what's a good example. Like even in like, I don't know if I can come up with a good

145
00:16:56,560 --> 00:17:05,200
example right now. So I guess an example that Courtney and Appolice at Graham really gave me was

146
00:17:05,200 --> 00:17:11,600
that, you know, if someone makes a typo and says something like IR space husband or something

147
00:17:11,600 --> 00:17:19,040
and it wants to spell correct IR, like do you spell correct to her, which would probably be the

148
00:17:19,040 --> 00:17:26,320
most common in data due to like heteronormativity? Or do you provide alternative spell corrections

149
00:17:26,320 --> 00:17:32,720
for that? And so like all of these things I think are about like, you know, when we're designing

150
00:17:32,720 --> 00:17:37,920
systems and, you know, they're being trained on say all the data you find on the internet or

151
00:17:37,920 --> 00:17:44,480
something like all that data comes with, you know, sort of a bunch of social baggage. And some of

152
00:17:44,480 --> 00:17:51,120
that is useful, but some of that serves to like either erase people and make them unseen,

153
00:17:52,480 --> 00:17:59,440
or serves to make the systems just not work well for them, or make suggestions, like, you know,

154
00:17:59,440 --> 00:18:08,880
sort of the classic example of like auto replies that, you know, if someone says, you know, I saw

155
00:18:08,880 --> 00:18:14,080
a doctor yesterday, the auto reply is something like, what did he say? And so all these things that

156
00:18:14,080 --> 00:18:20,960
sort of like reinforce social hierarchies that already exist in the world. I think that's sort

157
00:18:20,960 --> 00:18:26,160
of the interesting question. Now, how you go about it? I think this is super hard. And like,

158
00:18:26,160 --> 00:18:33,280
you know, this question of like, how much data do you need? I guess like my feeling on that is

159
00:18:34,320 --> 00:18:44,240
I don't know that like more data is always the right answer. And, you know, in some cases,

160
00:18:44,240 --> 00:18:49,600
I think we need to find other ways for domain experts to get their knowledge into machine learning

161
00:18:49,600 --> 00:18:54,480
systems. Like, I think a lot of this democratization of machine learning where it's like, okay,

162
00:18:54,480 --> 00:18:59,920
I have this black box. And, you know, there's like the machine learning experts on one side,

163
00:18:59,920 --> 00:19:03,360
and they're doing all those like developments. And then there's the domain experts on the other

164
00:19:03,360 --> 00:19:08,080
side. And like, all they're allowed to do is provide labeled data. Like, I think this is like

165
00:19:08,080 --> 00:19:14,880
an incredibly reductive view of like what machine learning can do. And I think it's something that's

166
00:19:14,880 --> 00:19:22,320
like much beyond fairness. Like, you know, when I was even in grad school forever ago at this point,

167
00:19:22,320 --> 00:19:28,960
you know, I was in a natural language processing group. And one of the things that we would kind

168
00:19:28,960 --> 00:19:33,600
of poke fun at the machine learning community for was that like, you know, to the machine learning

169
00:19:33,600 --> 00:19:39,280
community, like your API is someone gives you a matrix and you have to do something with that matrix.

170
00:19:40,080 --> 00:19:45,600
And this was just like a really weird way to conceptualize problems even as like an NLP person,

171
00:19:45,600 --> 00:19:49,760
which is like not that far from machine learning. Because we're always talking about like, okay,

172
00:19:49,760 --> 00:19:54,080
how do I develop new features? How do I collect new examples? How do I make better representations?

173
00:19:54,080 --> 00:19:59,040
Like all of these things that like even just in a small way break outside of this like matrix

174
00:19:59,040 --> 00:20:06,000
abstraction. And so I think that like in sort of like bias fairness space, this becomes like even

175
00:20:06,000 --> 00:20:11,600
more problematic because the sorts of like knowledge you need to integrate are really like the societal

176
00:20:11,600 --> 00:20:21,600
level. I don't know what the word is like societal level knowledge. And it's not clear to me that

177
00:20:22,400 --> 00:20:28,240
trying to do that by having people label data is like an efficient way to do that at all.

178
00:20:29,840 --> 00:20:38,160
Have you, yeah, what have you seen that's interesting that tries to get at this idea of incorporating

179
00:20:38,160 --> 00:20:45,440
the domain experts more deeply into the process? I mean, I think there's a lot of stuff.

180
00:20:46,720 --> 00:20:56,560
So, you know, back when I was in grad school, the technology du jour was Bayesian networks for

181
00:20:56,560 --> 00:21:03,040
everything rather than neural networks for everything. And, you know, I think a lot of the hope then

182
00:21:03,040 --> 00:21:09,120
was that, you know, domain experts could design these structured models that would then be useful.

183
00:21:09,120 --> 00:21:14,480
I think in some domains where things are sort of easy enough that you only have like 10 variables

184
00:21:14,480 --> 00:21:20,560
going on or 20 variables, you can do this. But I don't think it really panned out very much

185
00:21:20,560 --> 00:21:28,640
for a lot of language problems. You know, I think in neural nets land like sort of, you know,

186
00:21:28,640 --> 00:21:38,400
today's du jour, I think it's a lot harder because it's very difficult to understand these models.

187
00:21:38,400 --> 00:21:43,760
And so, I think like the rather significant amount of work that's been going on in like

188
00:21:43,760 --> 00:21:48,640
transparency and explainability is like super important here because like if we don't understand

189
00:21:48,640 --> 00:21:53,520
what these models are doing, it's like not even clear how a domain expert would intervene.

190
00:21:53,520 --> 00:22:01,600
But I think there's been also a bunch of work looking at like creative ways of

191
00:22:06,000 --> 00:22:14,240
trying to get knowledge in one form or another into systems. So one is, you know, there's been

192
00:22:14,240 --> 00:22:18,240
a handful of papers looking at, you know, okay, like let's say that I have some like logical rules

193
00:22:18,240 --> 00:22:25,760
and I want to make sure that my neural net model maybe not always obeys them but like strongly

194
00:22:25,760 --> 00:22:31,120
prefers things that are consistent with these rules I write down. So like how can I sort of compile

195
00:22:31,120 --> 00:22:36,960
these into neural net systems? I think there's been a bunch of work looking at, well not a bunch,

196
00:22:36,960 --> 00:22:44,480
but there's been a bit of work looking at like are there other forms of data that I can collect

197
00:22:44,480 --> 00:22:53,360
sort of alongside typical labels. So I think a lot of this goes back to like the mid-2000s with

198
00:22:54,560 --> 00:23:00,560
like annotator rationales so have annotators like mark the pieces of text that are most relevant

199
00:23:00,560 --> 00:23:08,240
to the decision that they made. And then a variant of this that's come up recently

200
00:23:08,240 --> 00:23:16,000
is instead of having them sort of highlight the relevant part have them change or relevant

201
00:23:16,000 --> 00:23:22,880
part so that the labeling decision would change. So we tried this a couple of years ago in like

202
00:23:22,880 --> 00:23:28,560
an evaluation context so this was I think we conceptualized it at the time as sort of like human

203
00:23:28,560 --> 00:23:35,840
adversarial examples so we ran the shared task where people could submit systems for two tasks

204
00:23:35,840 --> 00:23:41,760
sentiment analysis and semantic role labeling then so there this was this build it break at task

205
00:23:41,760 --> 00:23:47,120
so those were the builders and then the breakers would come in and they'd take the outputs from

206
00:23:47,120 --> 00:23:54,400
the systems on test data or on development data and then their job was to take a sentence from the

207
00:23:54,400 --> 00:24:01,280
development data and change it in a minimal way so that the label change the true label changes

208
00:24:01,280 --> 00:24:08,240
but perhaps the system prediction does not. And so then we score systems based on how hard it

209
00:24:08,240 --> 00:24:14,880
is to break them and we score breakers based on how many systems they break and one of the I

210
00:24:14,880 --> 00:24:20,880
think there was like an interesting outcome of this which was that we saw like a lot of the people

211
00:24:20,880 --> 00:24:26,480
who were breakers were linguists we kind of build it as like you know hate linguists come show

212
00:24:26,480 --> 00:24:32,480
these NLP people like that their systems are really fragile and don't learn like even basic sort

213
00:24:32,480 --> 00:24:39,520
of syntactic properties and so so that was I don't know three years ago or so and then in the

214
00:24:39,520 --> 00:24:45,360
past couple of years we've seen similar ideas applied to training data where people have elicited

215
00:24:45,360 --> 00:24:52,640
data that where they say okay label this and then change it minimally so that the label would flip

216
00:24:52,640 --> 00:24:58,640
and then it's sort of been observed that by using that type of training data you can learn something

217
00:24:58,640 --> 00:25:06,000
like just as good with far fewer examples and so I think that's still like largely in the space

218
00:25:06,000 --> 00:25:11,760
of having people label data but I think it's like making this interface between you know the machine

219
00:25:11,760 --> 00:25:17,040
learning developers and the domain experts like a little bit broader and I think like if we can

220
00:25:17,040 --> 00:25:22,400
continue pushing on that I think there's like a huge amount of space both for like progress and also

221
00:25:22,400 --> 00:25:26,960
a lot of creativity like I think there's a lot of really interesting things you can think about when

222
00:25:26,960 --> 00:25:33,840
you start like separating not separating but when you start trying to make this connection more

223
00:25:33,840 --> 00:25:43,360
porous I was curious about a paper I think it was a survey paper that you did a while back on

224
00:25:43,360 --> 00:25:48,720
language technology is power can you share a little bit about that one yeah I mean it wasn't a

225
00:25:48,720 --> 00:25:57,280
while back it was like just this week actually at ACL it wasn't so it is top of mind yes so this

226
00:25:57,280 --> 00:26:04,080
was with Suleyn Blodgett who is an intern at Microsoft last summer and then Suleyn Burkis and

227
00:26:04,080 --> 00:26:13,840
Hannah Wallach so yeah so what Suleyn did to start with was look at basically every paper we could

228
00:26:13,840 --> 00:26:22,960
find on quote unquote bias and NLP and that was basically 150 papers over the past couple of years

229
00:26:25,920 --> 00:26:31,920
and what we set out to do was try to understand

230
00:26:31,920 --> 00:26:44,960
and what are the like when people write papers on bias and NLP like sort of what is motivating them

231
00:26:44,960 --> 00:26:49,040
so like what does the paper say like this is the problem in the world that we're trying to solve

232
00:26:49,040 --> 00:26:57,520
what their sort of like normative commitments are so like they say it's a problem why do they say

233
00:26:57,520 --> 00:27:05,040
it's a problem and then because most of these papers present some sort of new method that's either

234
00:27:05,040 --> 00:27:11,200
a mitigation strategy or a measurement strategy you know what is that thing actually measuring and

235
00:27:11,200 --> 00:27:18,720
like how much does it align with for instance the harms that were in the motivation and so a lot

236
00:27:18,720 --> 00:27:26,560
of the initial work was sort of categorizing papers coming up with a taxonomy of different types

237
00:27:26,560 --> 00:27:33,760
of harms that we largely followed the allocational representational harms perspective that like

238
00:27:33,760 --> 00:27:40,640
Kate Crawford and Suleyn Burkis and colleagues have talked about before we had to create a couple

239
00:27:40,640 --> 00:27:47,200
new categories and there were some that didn't really apply so the first ever was kind of a

240
00:27:47,200 --> 00:27:54,160
categorization effort and then the second effort was really trying to take stock of like you know

241
00:27:54,160 --> 00:28:02,320
okay so we found that like motivations were often super vague like of the form stereotyping is bad

242
00:28:05,360 --> 00:28:12,160
and then we often found that papers sort of took normative stances for granted so

243
00:28:14,320 --> 00:28:19,200
something would be seen as bad but it wouldn't really be explained why and actually in the poster

244
00:28:19,200 --> 00:28:25,680
section what's an example of the previous yeah what's a good example of that

245
00:28:33,200 --> 00:28:39,280
so for instance I'm making this up so like ask me to point to something that's in particular

246
00:28:39,280 --> 00:28:45,040
but like you know something along the lines of you know word embedding and pick up

247
00:28:45,040 --> 00:28:54,400
associations between gender and stereotyped occupations associated with that gender so that may

248
00:28:54,400 --> 00:29:01,840
be true but the decision that this is bad is like fundamentally like a values question and so

249
00:29:01,840 --> 00:29:08,880
like sort of what is like what are the values that you're leaning on in order to like decide

250
00:29:08,880 --> 00:29:14,080
that this thing is bad and one reason why I think that's important is because like a lot of these

251
00:29:14,080 --> 00:29:19,440
things are really tricky and like we need to debate them as a community but like until we make

252
00:29:19,440 --> 00:29:24,080
them really Chris it's really hard to have a debate about like whether something's good or bad

253
00:29:24,080 --> 00:29:29,520
without saying why it's good or bad and actually in the poster session earlier this week for this

254
00:29:29,520 --> 00:29:38,720
paper Sharon Goldwater at Edinburgh brought up this this question about you know what does this

255
00:29:38,720 --> 00:29:46,960
paper say about how to teach sort of ethics in NLP in classes and one of the comments that

256
00:29:46,960 --> 00:29:54,000
Solon made that I thought was really nice was that and right and sorry the context in which Sharon

257
00:29:54,000 --> 00:29:59,200
was asking this question is that she teaches in Edinburgh she's American many students in Edinburgh

258
00:29:59,200 --> 00:30:06,400
are international we all come from like very different backgrounds and have different sets of

259
00:30:06,400 --> 00:30:13,360
values so like how do we teach in that context and one of the points that Solon made in response to

260
00:30:13,360 --> 00:30:18,320
this is that like if we made our normative commitments more explicit it would actually make

261
00:30:18,320 --> 00:30:24,640
teaching easier too because then a paper would explain like okay this is the problem this is why

262
00:30:24,640 --> 00:30:30,800
we say it's a problem and then go on and then even if you're not from the cultural context in which

263
00:30:30,800 --> 00:30:36,720
it might be taken for granted that this thing is a problem you can still at least understand why

264
00:30:36,720 --> 00:30:43,360
it's being couched as a problem so what you're looking for here is the explanation as opposed to

265
00:30:44,400 --> 00:30:51,680
you know reference to some pre-existing ethical code that you know may have already established

266
00:30:51,680 --> 00:31:00,320
that it's a problem yeah I mean I think both are good so you know there's a huge literature

267
00:31:00,320 --> 00:31:08,480
outside of NLP on like how language and society works this is like linguistics anthropology

268
00:31:08,480 --> 00:31:14,800
so it's the linguistics et cetera right and then like if you're looking at specific problems like

269
00:31:14,800 --> 00:31:20,160
like for instance gender you know there's like decades of gender studies literature out there

270
00:31:20,160 --> 00:31:28,160
I'm specifically thinking of the example you gave with the the you know predicting you know NLP

271
00:31:28,160 --> 00:31:34,240
predicting gender or we you know throw this example around with the you know the word to veck

272
00:31:34,240 --> 00:31:43,360
word embeddings like the professions and I think you're right that you know as was the case in

273
00:31:43,360 --> 00:31:48,400
the paper the I guess it was the hypothetical paper you were referring to we often throw this

274
00:31:48,400 --> 00:31:54,960
very example around as being you know so wrong that it doesn't need definition

275
00:31:57,200 --> 00:32:07,600
and or doesn't need explanation as to you know why it's problematic and I'm thinking if we want

276
00:32:07,600 --> 00:32:12,560
to be precise do we all need to keep recreating that definition of why that's a problem or is

277
00:32:12,560 --> 00:32:19,120
there some something that we can refer to yeah I mean I think yeah this is why references exist

278
00:32:19,120 --> 00:32:27,520
yeah yeah so I think we definitely don't need to like keep recreating it I think that

279
00:32:29,440 --> 00:32:34,560
like for instance in this example you know I think the question that I would ask is like

280
00:32:34,560 --> 00:32:43,680
you know okay so we've found that you know there's sort of these gender occupation stereotypes

281
00:32:43,680 --> 00:32:54,400
and word embeddings like you know what harm does that create so for instance a lot of

282
00:32:54,960 --> 00:33:01,600
I think even the original paper gives this example of resume filtering or something like that so

283
00:33:01,600 --> 00:33:10,240
this is a sort of commonly leaned upon example where you could imagine a scenario in which word

284
00:33:10,240 --> 00:33:15,600
embeddings are used in some sort of resume filter and then if you have you know I don't know if

285
00:33:15,600 --> 00:33:20,240
the word embeddings pick up correlations between names and gender and then also pick up correlations

286
00:33:20,240 --> 00:33:28,720
between occupations and gender then maybe it like sorts people according to whatever these

287
00:33:28,720 --> 00:33:35,760
stereotypes were in the language that it was trained on so I think right so this would generally

288
00:33:35,760 --> 00:33:39,280
be considered like an allocational harm in the sense that there's like a resource like a job or

289
00:33:39,280 --> 00:33:44,640
a job opportunity that's being denied someone on the basis of like what the system has picked up

290
00:33:46,560 --> 00:33:55,760
and so I think it's fine to lean on that but then I what would be I think really interesting to see

291
00:33:55,760 --> 00:34:01,360
and I think there's like a lot of work to be done here is like how can we actually go about measuring

292
00:34:01,360 --> 00:34:09,840
that and how can we build like mitigation strategies like for that like specific harm and so I think

293
00:34:09,840 --> 00:34:14,880
like this is sort of like the mismatch issue that I mentioned earlier so you know there's a lot of

294
00:34:14,880 --> 00:34:22,160
motivations along the lines of you know this resume filtering example but these are all sort of

295
00:34:22,160 --> 00:34:27,200
in the context of this hypothetical resume filtering system that like we don't even know exists we

296
00:34:27,200 --> 00:34:30,560
don't know if this actually happens and so we're thinking about like measuring whether we've been

297
00:34:30,560 --> 00:34:34,640
successful in mitigating these harms like we have to be able to sort of measure them in this first

298
00:34:34,640 --> 00:34:41,680
place but yeah I mean like on your sort of original question like definitely we don't need to

299
00:34:41,680 --> 00:34:49,440
all reinvent the wheel every time but I think as things have and I think this is the gender-based

300
00:34:49,440 --> 00:34:57,440
stereotypes is a more clear case I think there are less clear cases where it's I think much more

301
00:34:57,440 --> 00:35:06,880
up for debate whether these harms are like or whether these whether these biases that are proposed

302
00:35:08,160 --> 00:35:15,760
actually amount to some sort of harm or not in the world yeah I think the the question that

303
00:35:15,760 --> 00:35:29,200
curious about is to what degree are you know folks to what degree should folks that are thinking

304
00:35:29,200 --> 00:35:37,520
about these there's maybe too broad I'm curious if there's a cat to what degree there's a catalog

305
00:35:37,520 --> 00:35:50,400
or a survey of you know the kind of common biases and harms you know versus individual papers

306
00:35:50,400 --> 00:35:56,960
that have you know that are exploring specific harms so you know the word to veck word embeddings

307
00:35:56,960 --> 00:36:02,160
thing that's been explored in a bunch of individual papers you know but is there like something

308
00:36:02,160 --> 00:36:10,720
analogous to the the Wikipedia page of you know cognitive biases that just lists you know a

309
00:36:10,720 --> 00:36:17,040
thousand of them and the the various you know harm arguments does the work that you reference

310
00:36:18,320 --> 00:36:27,680
with Hannah Wollock and was that Hannah and Solon that yeah and Solon I don't know if something

311
00:36:27,680 --> 00:36:35,360
like that I would love it existed you know like you know I was saying like three or four years ago

312
00:36:35,360 --> 00:36:39,200
and I started doing all this stuff you know I read this giant pile or says you'll ingress

313
00:36:39,200 --> 00:36:47,200
six bucks right and it would certainly have saved me a lot of time if if I could you know if this

314
00:36:47,200 --> 00:36:53,360
was sort of like more distilled somewhere I think one of the challenges is that I think it's really

315
00:36:53,360 --> 00:36:58,880
hard to think about these things abstractly even in word embedding it's like you know no one goes to

316
00:36:58,880 --> 00:37:04,640
like word embedding.google.com and like asks for an embedding of a word like this is not

317
00:37:04,640 --> 00:37:13,200
like an end user technology it's like embedded in in other systems yeah and I think you know the

318
00:37:13,200 --> 00:37:20,160
further a technology is from like the use case though like it is much much harder to think about

319
00:37:20,160 --> 00:37:25,120
so if you look at the work that's been done in sort of like bias and machine translation systems

320
00:37:25,120 --> 00:37:29,920
or speech recognition or dialogue or something like that like these systems they're like much more

321
00:37:29,920 --> 00:37:36,000
user-facing I think it's a lot easier to think about you know you could for instance do you

322
00:37:36,000 --> 00:37:40,880
like a value-sensitive design type exercise and think about like who are the stakeholders involved

323
00:37:40,880 --> 00:37:47,040
in this system you know what happens to each of them when different types of errors are made

324
00:37:47,040 --> 00:37:52,640
and so on and it's just for these like these more component tasks whether like word embeddings

325
00:37:52,640 --> 00:37:58,560
or syntactic parsing or something like that it becomes much harder to think about like

326
00:38:00,400 --> 00:38:06,720
as errors are made like what sort of what sort of problems does this cause because they'll

327
00:38:06,720 --> 00:38:13,360
inevitably be like downstream causes don't start downstream effects so yeah so I think the answer

328
00:38:13,360 --> 00:38:18,480
is no I think I think that's something it would be great but I think it's also really tricky so

329
00:38:18,480 --> 00:38:24,480
for instance like we had this other ACL paper this year on trying to move toward like

330
00:38:24,480 --> 00:38:29,280
careference resolution that's more gender inclusive and particular for like binary and non-binary

331
00:38:29,280 --> 00:38:41,440
trans people and you know like I think like work in that area for instance I think needs to lean

332
00:38:41,440 --> 00:38:48,480
you know super heavily on queer studies and gender studies and topics like this and so I think

333
00:38:48,480 --> 00:38:56,240
you know as you pick these areas and like both tasks and like populations you're thinking about

334
00:38:56,240 --> 00:39:01,360
and things like that it's really hard to imagine anything other than like a full encyclopedia

335
00:39:02,080 --> 00:39:07,680
that's gonna like have the answer to everything I totally see where you know it's wishful thinking

336
00:39:07,680 --> 00:39:14,640
that another example that kind of comes to mind is like in you know writing you've got you know

337
00:39:14,640 --> 00:39:20,880
like Chicago style or AP style I'm envisioning something where you know someone is writing a paper

338
00:39:20,880 --> 00:39:26,160
can refer to you know it's you know that this paper is you know it should be evaluated in

339
00:39:26,160 --> 00:39:32,880
XYZ ethical frame and you know that's the lens in which I'm you know conveying judgments about

340
00:39:32,880 --> 00:39:38,080
what's right and what's wrong and etc etc I think you see a little bit of this so I think you see

341
00:39:38,080 --> 00:39:44,160
it kind of with connections like political philosophy so sometimes you'll see motivations and

342
00:39:44,160 --> 00:39:49,520
papers that say like you know we're gonna take like a Rawlsian view of justice and um

343
00:39:50,720 --> 00:39:58,000
uh you know sometimes this is a little bit more like cell then content but like it often is content

344
00:39:58,000 --> 00:40:03,920
and um you know and like I think a lot of people think like Rawls had interesting things to say I

345
00:40:03,920 --> 00:40:09,920
think you know certainly uh he's also been contested and so um but like at least by saying like okay

346
00:40:09,920 --> 00:40:17,440
this is like the framework that I'm using um it sort of lets other people say like well I either

347
00:40:17,440 --> 00:40:21,120
disagree with that or I agree with it and like I can evaluate like you were saying like I can

348
00:40:21,120 --> 00:40:28,960
evaluate the work in the context of like this framing right right right um so if you're if you're

349
00:40:28,960 --> 00:40:36,560
not researching this but you're actually trying to build systems yeah how do you kind of

350
00:40:36,560 --> 00:40:47,520
wade through all of these issues yeah that's really hard you know I think there's um I mean there's

351
00:40:47,520 --> 00:40:54,160
like been a lot of work recently trying to like okay so I'm going to like kind of put like a

352
00:40:54,160 --> 00:41:00,160
little bit I mean I think of this more in the context of like you know building systems not

353
00:41:00,160 --> 00:41:06,800
building academic systems but building like real systems in the world right um I mean you know

354
00:41:06,800 --> 00:41:17,200
so like there's been work trying to look at um you know what do what do the people who build

355
00:41:17,200 --> 00:41:24,080
these systems need so like Michael Veele had this nice paper in Kai 18 um looking at the public

356
00:41:24,080 --> 00:41:31,680
sector so systems that are developed um you know for things like uh well he was in the UK so like

357
00:41:31,680 --> 00:41:38,000
hospitals um or social services and things like that um and then we had basically a follow-up

358
00:41:38,000 --> 00:41:44,640
paper a year later looking at this in the context of private companies um and so you know I think

359
00:41:44,640 --> 00:41:50,560
one of the things that was consistent in both of those is that a lot of the work on like bias and

360
00:41:50,560 --> 00:41:58,000
fairness often comes down to sort of a single well motivated individual on a development team

361
00:41:59,120 --> 00:42:06,720
um so one of the people we interviewed referred to these people as like fairness vigilantes um and

362
00:42:06,720 --> 00:42:16,080
they're often working overtime uh to address issues like this um they're often not compensated

363
00:42:16,800 --> 00:42:22,080
for doing this extra work um I mean in many ways it's kind of reminiscent of like diversity

364
00:42:22,080 --> 00:42:28,400
and inclusion work in like companies and universities as well and so I think um you know I think

365
00:42:29,840 --> 00:42:36,320
as like a bare minimum you know this has to be part of like assessment of tools and

366
00:42:36,320 --> 00:42:40,400
assessment of success and like promotion criteria and raised criteria right so I think there's like

367
00:42:40,400 --> 00:42:46,400
all these like social things and like how companies are run that kind of has to change so that it's

368
00:42:46,400 --> 00:42:54,480
not this like you know one poor individual is like burdened with doing all this work um it's

369
00:42:54,480 --> 00:43:01,200
interesting though that you the you point to a cultural solution a company cultural solution as

370
00:43:01,200 --> 00:43:09,520
opposed to a process solution so I think it's both so I think there's the the cultural aspect um

371
00:43:09,520 --> 00:43:13,440
because you can certainly argue that maybe the process won't come about if the culture's not there

372
00:43:13,440 --> 00:43:19,920
to support it right but that's basically my concern yeah um you know so there's there's been more

373
00:43:19,920 --> 00:43:26,480
sort of like technical work so like um Michael Medio who was an intern last summer working with

374
00:43:26,480 --> 00:43:31,600
gentleman Vaughan and Hannah Wallach had this nice like checklist for fairness paper at CHI last

375
00:43:31,600 --> 00:43:40,400
year um which was really sort of exploring this question of you know how can we um like once

376
00:43:40,400 --> 00:43:45,440
people are bought into this either because they care themselves or because their boss tells them

377
00:43:45,440 --> 00:43:54,240
they care now um you know how can they like more easily navigate this space um I think there's

378
00:43:54,240 --> 00:43:59,920
also a lot of sharing of information that's needed I think this is like really hard but um

379
00:44:01,200 --> 00:44:06,240
you know like for instance machine translation and speech recognition are different tasks um

380
00:44:06,240 --> 00:44:11,280
they're often developed by different groups within if a company makes both they're often

381
00:44:11,280 --> 00:44:16,320
developed by different groups but they share a lot of similar issues in the sense that both are

382
00:44:16,320 --> 00:44:21,280
about writing down text as a function of some input um and it's just like is that input like

383
00:44:21,280 --> 00:44:27,440
something in another language or something um in speech and like finding ways to like share

384
00:44:27,440 --> 00:44:34,080
expertise about like what goes wrong like the thing everyone is like super concerned about is

385
00:44:34,080 --> 00:44:39,440
is like blind spots right so these things that you don't think to test for and then you release your

386
00:44:39,440 --> 00:44:44,800
system and then like 24 hours later there's a New York Times article about how your system is

387
00:44:44,800 --> 00:44:51,120
terrible and you know and how are you supposed to know this ahead of time um so I think there's

388
00:44:51,120 --> 00:44:55,600
a lot of process stuff so I think it's a lot of things so I think it's like cultural stuff I

389
00:44:55,600 --> 00:45:02,240
think it's process stuff and then I think it's like technological stuff so um you know the the

390
00:45:02,240 --> 00:45:08,000
sort of fairness in machine learning community has I think really gravitated and made a lot of

391
00:45:08,000 --> 00:45:14,960
progress on um these sort of parity constrained machine learning problems right it's like

392
00:45:14,960 --> 00:45:20,880
optimized classification accuracy subject to some maximum disparity between two predefined

393
00:45:20,880 --> 00:45:27,360
um social subgroups um so that's I think like by far the dominant paradigm that people have been

394
00:45:27,360 --> 00:45:35,840
thinking about um and we have pretty good tools for that for problems that fit in that category

395
00:45:35,840 --> 00:45:41,760
at this point um I think the thing that we found in this kind of paper last year with Ken Holstein was

396
00:45:41,760 --> 00:45:48,800
that like that is definitely not all that people need like people definitely need that but um you

397
00:45:48,800 --> 00:45:55,360
know like we were talking about before a lot of this is about data so you know how do you know

398
00:45:55,360 --> 00:46:03,600
say that I've discovered that my system has this gap like I can try to address it algorithmically

399
00:46:04,320 --> 00:46:10,800
but like if I just don't have any data or I have like very little data from some uh from some

400
00:46:10,800 --> 00:46:16,560
sub-population like no amount of algorithmic finessing is going to help me and so you know this

401
00:46:16,560 --> 00:46:20,720
is where we need to start thinking about like how do I grow that matrix right so like how do I

402
00:46:21,440 --> 00:46:28,160
like select new rows like how do I elicit more data from like um populations where I'm observing

403
00:46:28,160 --> 00:46:34,400
like large disparities um so I think there's yeah so I think it's really these three things like

404
00:46:34,400 --> 00:46:40,080
cultural process and then technical and I think um you know I'm a technical person so

405
00:46:40,080 --> 00:46:45,040
it's easiest for me to think about the technical problems but you know I think it's also the case

406
00:46:45,040 --> 00:46:52,640
that you can have the best technical thing in the world but if no one uses it and no one's encouraged

407
00:46:52,640 --> 00:47:04,640
to use it then yeah um yeah ultimately if the the change is happening on the part of these

408
00:47:05,280 --> 00:47:10,640
you know fairness and the vigilantes is the right word but you know advocates or whatever

409
00:47:10,640 --> 00:47:17,760
yeah advocates is probably that's it then we need tools to enable the the advocates and

410
00:47:17,760 --> 00:47:25,440
and it is it easy now you know is that tools you know here a bunch of links to papers go read them

411
00:47:25,440 --> 00:47:30,960
and you know that's what you have or I think there's in conferences what what are the best

412
00:47:30,960 --> 00:47:35,760
resources you think for folks that you know hear this and say oh I want to be that in my company

413
00:47:35,760 --> 00:47:43,200
yeah so I mean on these on these problems of you know sort of like optimize classification

414
00:47:43,200 --> 00:47:48,800
performance with respect to some parity metric I think you know sort of at least IBM Google and

415
00:47:48,800 --> 00:47:53,840
Microsoft all have toolkits at this point that will do these I think you know they all have pros

416
00:47:53,840 --> 00:48:01,200
and cons you know with my Microsoft hat on I'll be like the Microsoft ones but but you know and

417
00:48:01,200 --> 00:48:08,960
but I think they they certainly all have strengths and weaknesses um the but they're kind of

418
00:48:09,680 --> 00:48:16,160
I won't say all but they're like largely focused on sort of this like one very specific sub problem

419
00:48:17,040 --> 00:48:30,800
um I think for other things like the technology hasn't matured that much yet so um you know like

420
00:48:30,800 --> 00:48:37,760
I've been thinking a lot about data collection um and uh and in some ways this is a really natural

421
00:48:37,760 --> 00:48:41,520
machine learning problem like machine learning people have been studying active learning which

422
00:48:41,520 --> 00:48:49,520
is basically automated data collection for like 50 years probably at least um and uh so there's a

423
00:48:49,520 --> 00:48:54,400
paper I apologize I can't remember the author's names but there's a there's a paper already on

424
00:48:54,400 --> 00:48:59,840
archive on this topic um we've been doing some work on this topic basically figuring out you know

425
00:48:59,840 --> 00:49:05,520
okay I have some disparate like I built a system I observed some disparity in its performance

426
00:49:06,240 --> 00:49:12,640
um I have a giant pool of data that I could have labeled which data points should I label

427
00:49:13,440 --> 00:49:20,000
um in order to close this parity gap as much as possible um and one of the things that I actually

428
00:49:20,000 --> 00:49:27,360
really like about this conceptualization is that you know we very frequently hear about this um

429
00:49:27,360 --> 00:49:36,400
um like accuracy fairness tradeoff um and I think to some degree like there's there's some truth

430
00:49:36,400 --> 00:49:40,480
to that in the sense that like if I'm solving an unconstrained optimization problem versus a

431
00:49:40,480 --> 00:49:44,480
constrained optimization problem like the constrained optimization problem is not going to do

432
00:49:44,480 --> 00:49:50,880
it it's going to do it at best as well as the unconstrained one um but you know sort of like

433
00:49:50,880 --> 00:49:55,680
caveat one is that you know accuracy with respect to like what test set so if your test set is

434
00:49:55,680 --> 00:50:01,680
like supervised then like who cares if I trade off accuracy on a supervised test set um in order to

435
00:50:01,680 --> 00:50:09,360
get parity but I think like I actually really prefer to think about this as a like accuracy versus

436
00:50:09,360 --> 00:50:17,520
effort tradeoff so um like if we take for granted that some notion of fairness measured by parity

437
00:50:17,520 --> 00:50:25,440
or measured some other way is just a constraint in how we build systems then the question is how

438
00:50:25,440 --> 00:50:30,560
much effort do I have to put in in order to get my accuracy up to a sufficient level like under

439
00:50:30,560 --> 00:50:35,360
that constraint and so I I I much prefer to think of this as like how much work am I willing to do

440
00:50:35,360 --> 00:50:42,240
in order to get this accuracy when I demand that you know this thing is you know quote-unquote fair

441
00:50:42,240 --> 00:50:48,240
and in however I've technically defined that which is great because it puts a responsibility on you

442
00:50:48,240 --> 00:50:54,640
for deciding that you're not willing to put an effort to right yeah well not on me I hope

443
00:50:54,640 --> 00:51:03,600
someone else yeah I mean data collection is you know it's it's tough it's it's important right like

444
00:51:03,600 --> 00:51:09,920
I mean you know you can go back to the like the gender shades paper by Joy and Tim Nade and um

445
00:51:11,120 --> 00:51:17,120
you know like that paper only was possible because they went in collective data um and you know

446
00:51:17,120 --> 00:51:24,000
I think we've seen a lot of papers basically following up on that work where it's basically like oh

447
00:51:24,000 --> 00:51:27,840
I collect another data set and this thing is terrible and I collect another data set and this

448
00:51:27,840 --> 00:51:34,560
thing is terrible and you know we had a paper like that at ACL right so um you know I think that we

449
00:51:35,760 --> 00:51:40,720
um I think that that's been like a really productive way of thinking about problems um

450
00:51:43,680 --> 00:51:49,120
I would like to think that there's a way to do this without having to go out and like collect a

451
00:51:49,120 --> 00:51:54,800
full test right this is an expensive process and so like what can we do to sort of like streamline

452
00:51:54,800 --> 00:52:02,640
this so that we can find issues um like quickly and I don't think I mean going back to your original

453
00:52:02,640 --> 00:52:09,520
tools question like um you know this like blind spots thing that comes up over and over again

454
00:52:09,520 --> 00:52:19,520
I mean there's very few even papers on this um not to mention um like tools that exist so you know

455
00:52:20,480 --> 00:52:27,760
people should work on that. What was the paper that you were referring to uh from ACL?

456
00:52:30,080 --> 00:52:35,200
This is like with uh my PhD student Tristan Sal so this is on this is the gender

457
00:52:35,200 --> 00:52:41,920
inclusive co-reference paper so um yeah the observation there is you know most so in co-reference

458
00:52:41,920 --> 00:52:48,240
resolution I'm trying to determine like you know if I say like um you know I had a meeting with

459
00:52:48,240 --> 00:52:57,440
Sam yesterday he blah blah blah I want to know that he refers to Sam um and um you know the vast

460
00:52:57,440 --> 00:53:04,240
majority of data sets that have been collected for this are from newswire um you know like New

461
00:53:04,240 --> 00:53:08,960
New York Times and Wall Street Journal it was only like last year I think that they their style

462
00:53:08,960 --> 00:53:16,480
guidelines started allowing the use of third person singular they um to refer to non-binary people

463
00:53:17,600 --> 00:53:22,160
I think otherwise they would just avoid I'm not actually sure what they did um but they must have

464
00:53:22,160 --> 00:53:28,320
had some way of talking around this um and so like if your data doesn't even contain like third

465
00:53:28,320 --> 00:53:36,080
person singular they not to mention like neo-pronouns like see here um you know how is your system

466
00:53:36,080 --> 00:53:42,400
gonna learn to do anything like this so um you know our data collection exercise was basically like okay

467
00:53:42,400 --> 00:53:49,760
what are like naturally occurring sources where we will see um uh like gender neutral pronouns

468
00:53:49,760 --> 00:53:56,080
where we will you know unfortunately see things like dead naming and and misgendering um and then

469
00:53:56,080 --> 00:54:02,000
I mean even how do we annotate those things is not entirely clear um and so we had like a handful

470
00:54:02,000 --> 00:54:06,720
of data sources that we annotated and then you know not surprisingly systems don't do very well

471
00:54:06,720 --> 00:54:14,560
because they've never seen um language used this way in their training time so did you after collecting

472
00:54:14,560 --> 00:54:23,920
the data set did you benchmark existing uh systems yeah we ran like five yeah five systems um they

473
00:54:23,920 --> 00:54:29,440
all performed like in terms of the main measure that we collected they all performed essentially

474
00:54:29,440 --> 00:54:38,480
the same uh say systems are these systems that are used out in the wild by companies or algorithms

475
00:54:38,480 --> 00:54:45,280
that were presented in a paper around some uh academic task yeah so certainly the latter

476
00:54:45,280 --> 00:54:51,920
whether they're using companies I honestly don't know there's been work for instance looking at

477
00:54:51,920 --> 00:54:57,360
you know how much does using careerference resolution help with like information retrieval systems

478
00:54:57,360 --> 00:55:04,720
like web search and the idea there is like you know if I search for um like if I search for someone's

479
00:55:04,720 --> 00:55:10,880
name I want to highly rank documents that mention that person a lot it shouldn't really matter

480
00:55:10,880 --> 00:55:14,800
whether the document mentions them by name or whether it mentions them phenomenally or they

481
00:55:14,800 --> 00:55:25,360
it mentions them as you know like uh you know the president or the professor or or whatever um and

482
00:55:25,360 --> 00:55:29,840
I think there's kind of been mixed results like at the end of the day you know do major search engines

483
00:55:29,840 --> 00:55:38,320
use this I have no idea um yeah so yeah so whether they're used in real systems or not I just don't

484
00:55:38,320 --> 00:55:46,080
know um so it's if you know we were looking mostly at like academic systems yeah they don't do

485
00:55:46,080 --> 00:55:52,080
super well so like on on more like uh I don't want to use the word standard but on

486
00:55:52,880 --> 00:56:01,920
previous data sets um they get uh sort of these f scores in like the 60s on our data set it's like

487
00:56:01,920 --> 00:56:14,720
in the 30s um and you know certainly not um the the errors that right this this gap between 30 and 60

488
00:56:15,440 --> 00:56:21,280
the errors are not uniformly distributed over people like they're um much more peaky

489
00:56:21,920 --> 00:56:27,200
uh for for non-binary people or uh people who are gender fluid and so on

490
00:56:27,200 --> 00:56:36,080
um so there's you know clearly a ton of work to be done in this area are we generally heading

491
00:56:36,080 --> 00:56:41,760
in the right direction I've had some conversations recently um and there's been some papers published

492
00:56:41,760 --> 00:56:50,160
recently that try to save variations of kind of the you know the fairness community you know

493
00:56:50,160 --> 00:56:56,560
isn't going in the right direction or is you know has a huge blind spot in area xyz kind of what's

494
00:56:56,560 --> 00:57:08,720
your take on all that um it's hard to say like I think that you know I mean you know okay so like my

495
00:57:08,720 --> 00:57:18,080
own bias is that um I think that like breaking outside of this sort of like parity constrained

496
00:57:18,880 --> 00:57:25,440
classification um abstraction would be really good for the community and I think we're seeing that

497
00:57:25,440 --> 00:57:32,400
um like I think it's happening um uh not because that problem's unimportant but like I actually

498
00:57:32,400 --> 00:57:37,040
think we have pretty good solutions to that at this point um and I think that like the space of

499
00:57:37,040 --> 00:57:46,720
problems is just so much bigger than that um I think that you know I know that people possibly

500
00:57:46,720 --> 00:57:52,080
not possibly I know that you will disagree with me on this but like I really think it's hard to

501
00:57:52,080 --> 00:58:03,680
do work in this space without engaging in um and like thinking closely about like what role does

502
00:58:03,680 --> 00:58:12,480
this technology have in the world um so you know I think there's like especially in computer science

503
00:58:12,480 --> 00:58:20,080
I think we're often trained to think like very abstractly and you know like um you know I build

504
00:58:20,080 --> 00:58:26,800
this thing and you know people can use it for good people can use it for bad blah blah blah but like

505
00:58:26,800 --> 00:58:31,040
I kind of feel like it's years to collect the right data to use this thing that I built

506
00:58:31,040 --> 00:58:37,760
yeah exactly and I think at the end of the day it's like we live in a world and like that world

507
00:58:37,760 --> 00:58:44,480
has properties and like some of those properties are good so these properties are bad but I think that

508
00:58:44,480 --> 00:58:52,400
like it's you know if I build a technology that's like only useful in a world that doesn't exist

509
00:58:52,400 --> 00:58:56,240
like it's like socially construct like if I build something that's only useful in a world that's

510
00:58:56,240 --> 00:59:05,040
socially constructed like fundamentally different from how ours is um like at best that's neutral

511
00:59:05,040 --> 00:59:10,400
right but like at worst that that's going to be bad and so you know this is something that you

512
00:59:10,400 --> 00:59:16,640
know certainly I haven't thought about for my whole career right like um you know I really like

513
00:59:16,640 --> 00:59:21,840
I've only been thinking about these things for like three or four years um but I do see a little

514
00:59:21,840 --> 00:59:28,240
bit of a push in like some parts of the fairness community to try to maintain this like divide

515
00:59:28,240 --> 00:59:33,680
between like the technical work that happens and the world in which that work is happening

516
00:59:33,680 --> 00:59:50,160
and and I understand like that um that desire but I think that like for systems to or for like the

517
00:59:50,160 --> 00:59:56,960
technology that we build to be like um to actually like support things like justice and equity

518
00:59:56,960 --> 01:00:04,000
and stuff like that I I think it's really hard to do this without like engaging in like house

519
01:00:04,000 --> 01:00:16,240
society is structured and do you how well do you think we're doing on that or what's

520
01:00:16,240 --> 01:00:24,880
I actually I mean I'm in through like I'm I'm generally positive I mean I'm generally a

521
01:00:24,880 --> 01:00:32,640
positive person but like yeah yeah and I really mean it to to to make it so reductive but I

522
01:00:32,640 --> 01:00:42,480
guess more you know practically um again that you know if I'm kind of working in this space and

523
01:00:42,480 --> 01:00:53,840
and I'm just I'm always curious with with I'm always curious around um you know how folks kind of

524
01:00:53,840 --> 01:01:02,960
wrap their arms around the entire scope of um the technology that um you know this technology

525
01:01:02,960 --> 01:01:07,840
that we're all working in like AI you know as huge potentials for good huge potentials for bad

526
01:01:07,840 --> 01:01:18,960
and um you know how folks kind of parse that and um it is always interesting to me particularly

527
01:01:18,960 --> 01:01:23,600
for folks that take on the challenge of parsing it as opposed to putting that in a box that

528
01:01:23,600 --> 01:01:29,760
you know I'm just doing what I do and that's somebody else's issue how have you kind of grapple

529
01:01:29,760 --> 01:01:35,120
with that or come to terms with that or assess that or you know what do you see other folks doing

530
01:01:35,120 --> 01:01:41,680
I mean I think you see a broad range I mean I think the thing that makes me enthusiastic is you

531
01:01:41,680 --> 01:01:47,520
know you see things like the fact conference and um AI ethics and society and like these have been

532
01:01:47,520 --> 01:01:54,000
growing there's now you know sort of like fairness related workshops that essentially every AI

533
01:01:54,000 --> 01:02:06,560
conference that happens um so I think there's clearly like momentum here um I think that it's

534
01:02:08,560 --> 01:02:20,640
I don't know how to say this like I kind of so I remember like a bunch of years ago when like

535
01:02:20,640 --> 01:02:25,760
this is gonna seem like a tangent but it's actually relevant like I remember a bunch of years ago

536
01:02:25,760 --> 01:02:31,280
and like people were like kind of starting to think more seriously about like image captioning

537
01:02:31,920 --> 01:02:37,680
um and so you know I'm thinking of like the work by like tomorrow burg um back in like the

538
01:02:39,200 --> 01:02:47,120
early 2010 something like that maybe late 2000s um and I you know I think

539
01:02:47,120 --> 01:02:54,480
we as a community and like I did a little bit of work in this space like we were kind of an

540
01:02:54,480 --> 01:03:02,960
exploration mode like we didn't really know what was possible and um you know we hadn't quite

541
01:03:02,960 --> 01:03:08,480
figured out like how do I evaluate captions like you know what makes a caption good what makes a

542
01:03:08,480 --> 01:03:16,000
caption bad and you know over time as the field matured it became more and more important to like

543
01:03:16,000 --> 01:03:21,360
tease these things out um and so I think you see a bit of this happening and sort of at least in

544
01:03:21,360 --> 01:03:27,440
the fairness and NLP space these days where like I think for a while we kind of weren't really

545
01:03:27,440 --> 01:03:32,400
sure what we were doing and what was possible and like what wasn't possible and like as we're

546
01:03:32,400 --> 01:03:39,760
getting a better sense of the space um like I think things are starting to get like more concrete um

547
01:03:39,760 --> 01:03:48,240
and uh like more grounded in like the real world I think the challenge with this analogy is that

548
01:03:48,240 --> 01:03:56,880
like getting a caption wrong and like getting fairness wrong are like very different things um

549
01:03:56,880 --> 01:04:01,680
and so I think that's like one of the reasons why I think it's like I mean I want to sound like a

550
01:04:01,680 --> 01:04:08,080
broken record but like why it's like super important to like lean on all the work from uh you know

551
01:04:08,080 --> 01:04:12,480
social linguistics and linguistic anthropology and particular like black feminist scholarship in

552
01:04:12,480 --> 01:04:20,880
the space because like there's been so much written about this and like if we like I have this kind

553
01:04:20,880 --> 01:04:26,960
of metaphor like when the NLP community started doing like syntactic parsing it's not like a bunch of

554
01:04:26,960 --> 01:04:34,000
people like huddled in a room who didn't know anything about syntax and like tried to invent syntax

555
01:04:34,000 --> 01:04:39,600
like we went and we read what linguists had to say about syntax and like you know we built I

556
01:04:39,600 --> 01:04:43,600
mean not we are then involved in this of course but like you know the people who did this like

557
01:04:43,600 --> 01:04:50,240
built you know like a 120-page annotation guideline for how to annotate syntactic trees on sentences

558
01:04:50,240 --> 01:04:56,480
and we spent you know a million dollars annotating the pantry bank and you know an annotation has

559
01:04:56,480 --> 01:05:01,760
continued like this and so I think in various parts of NLP there's been like really tight connections

560
01:05:01,760 --> 01:05:08,320
between um you know I don't know maybe not quite what like Emily would like but um there has been

561
01:05:08,320 --> 01:05:13,520
tight connections between how linguists conceptualize problems and how we go about annotating them

562
01:05:13,520 --> 01:05:19,680
and building systems and I think like what we really need to see more of I mean not to say that

563
01:05:19,680 --> 01:05:25,120
it doesn't exist but I think we could do a lot more in uh sort of the fairness for NLP space

564
01:05:25,120 --> 01:05:32,960
is do a similar thing like um you know it would be kind of absurd to the community to think about

565
01:05:32,960 --> 01:05:38,320
doing parsing without looking at what syntacticians have thought about like why should we be thinking

566
01:05:38,320 --> 01:05:44,240
about doing like gender in NLP without looking at what gender studies people or sociolinguistic

567
01:05:44,240 --> 01:05:51,120
the sociolinguists have have talked about so um you know I think people are digging more into this

568
01:05:51,120 --> 01:05:59,680
literature it's kind of dense sometimes so it can be time consuming but like I think it's like

569
01:05:59,680 --> 01:06:09,760
super worth it awesome well how thanks so much for uh sharing all that with us taking a few minutes

570
01:06:09,760 --> 01:06:16,640
out of your clearly busy schedule about to launch a major research conference uh this weekend

571
01:06:16,640 --> 01:06:23,200
a couple days one day yeah this is sunday or monday for this one um you know you'd think I

572
01:06:23,200 --> 01:06:29,920
would know sunday is the expo um monday is tutorials and then that's also when we give out awards

573
01:06:29,920 --> 01:06:34,560
and stuff like that and then the main conference program starts Tuesday uh awesome well thanks for

574
01:06:34,560 --> 01:06:41,520
taking a few minutes out of your busy pre-conference schedule to share a bit with us um it was

575
01:06:41,520 --> 01:06:52,880
wonderful thank you yeah thanks for having me

