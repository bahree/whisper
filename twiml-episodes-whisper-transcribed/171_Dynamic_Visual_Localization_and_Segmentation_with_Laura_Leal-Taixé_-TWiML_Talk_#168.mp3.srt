1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,760
I'm your host, Sam Charrington. Let me start today's show with a huge thanks to everyone

4
00:00:34,760 --> 00:00:40,840
who's taken the time out to vote for us in the People's Choice Podcast Awards. Voting ends

5
00:00:40,840 --> 00:00:48,280
tomorrow, July 31st, and we are extremely appreciative of everyone who's voted, commented,

6
00:00:48,280 --> 00:00:54,760
shared, or otherwise. We'll be sure to share the results of the voting as soon as they become

7
00:00:54,760 --> 00:01:01,160
available. If you haven't already cast your vote, please, please, please visit twimbleai.com

8
00:01:01,160 --> 00:01:11,240
slash nominate to do so. In this episode, I'm joined by Laura Lail Tai Sheh, 2017 recipient

9
00:01:11,240 --> 00:01:17,080
of the prestigious Sophia Kovalev Skaya Award and professor at the Technical University of Munich,

10
00:01:17,080 --> 00:01:22,520
where she leads the dynamic vision and learning group. In our conversation, Laura and I discuss

11
00:01:22,520 --> 00:01:27,720
several of her recent projects, including work on image-based localization techniques that

12
00:01:27,720 --> 00:01:32,680
fuse traditional model-based computer vision approaches with a data-driven approach based

13
00:01:32,680 --> 00:01:38,680
on deep learning. We also discuss her paper on one-shot video object segmentation and the broader

14
00:01:38,680 --> 00:01:44,040
vision for her research, which aims to create tools for allowing individuals to better navigate

15
00:01:44,040 --> 00:01:49,720
cities using systems constructed from visual data. And now on to the show.

16
00:01:53,080 --> 00:01:58,920
All right, everyone. I am on the line with Laura Lail Tai Sheh. Laura is a professor at the

17
00:01:58,920 --> 00:02:03,640
Technical University of Munich, where she leads the dynamic vision and learning group.

18
00:02:03,640 --> 00:02:06,120
Laura, welcome to this week in Machine Learning and AI.

19
00:02:06,920 --> 00:02:11,960
Hello, Tim. Nice to be here. Why don't we get started by having you tell us a little bit about your

20
00:02:11,960 --> 00:02:19,800
background and how you made your way into machine learning research? Yes, so I studied actually

21
00:02:20,520 --> 00:02:25,880
not a role machine learning or computer vision. So I studied telecommunications engineering

22
00:02:25,880 --> 00:02:32,360
back in Barcelona. And then I went to do my master thesis at Northeastern University in Boston.

23
00:02:32,360 --> 00:02:37,960
And that's the first time that I took a course in computer vision. And so that's where,

24
00:02:37,960 --> 00:02:44,600
let's say, my passion for this field started. And then I decided to pursue a PhD. So I went back

25
00:02:44,600 --> 00:02:52,120
to Northern Germany to do a PhD in Hanover. And during the course of my PhD, machine learning

26
00:02:52,120 --> 00:02:57,800
and especially deep learning started to become really, really popular within the field of computer

27
00:02:57,800 --> 00:03:04,440
vision. And towards the end of my PhDs, when I started to focus a little bit more or try to

28
00:03:04,440 --> 00:03:09,880
get into machine learning and understanding how could this be useful for the tasks that I was

29
00:03:09,880 --> 00:03:17,880
dealing with here in my PhD. And after that, I went to ETH in Zurich for a couple of years,

30
00:03:17,880 --> 00:03:27,400
for a post-op. And after that, I moved to Munich, also for another post-op, until well,

31
00:03:27,400 --> 00:03:33,000
until recently, where I was giving the wonderful chance of becoming a tenure-track professor

32
00:03:33,000 --> 00:03:38,200
also here in Munich. Great. What was the focus of your PhD research?

33
00:03:38,840 --> 00:03:45,320
So I was working on multiple object tracking. I started with a project where we had to track

34
00:03:45,320 --> 00:03:55,080
algis. And this was for to study essentially how algis get stuck or get attached into the

35
00:03:55,080 --> 00:04:01,880
surfaces of ships. And so basically, they ruined the surfaces. And so the whole idea, this was

36
00:04:01,880 --> 00:04:08,200
together with the department of chemistry and physics. So they were really studying the

37
00:04:08,200 --> 00:04:15,880
possible materials that could repel these algis. And within this project, my goal was essentially

38
00:04:15,880 --> 00:04:21,560
to analyze the way these algis moved. And of course, they had tons and tons of video data.

39
00:04:21,560 --> 00:04:28,760
And they could not just manually follow each of these algis manually. So they wanted to have

40
00:04:28,760 --> 00:04:35,480
an automatic tool for analyzing this large amount of data. And so this is how I started

41
00:04:35,480 --> 00:04:41,800
with the topic of multiple object tracking. And then I moved a bit more towards

42
00:04:42,440 --> 00:04:49,240
people tracking because the motion of people were more interesting. There was the whole

43
00:04:50,200 --> 00:04:56,440
goal of analyzing the pedestrian motion, for example, for autonomous driving applications.

44
00:04:56,440 --> 00:05:05,160
And so this is where basically I shifted more towards crowded scenes, analyzing people interaction

45
00:05:05,160 --> 00:05:09,640
and using these interactions for better prediction of people motion.

46
00:05:10,600 --> 00:05:15,640
And more recently, you've been working on visual localization. Can you tell us about that

47
00:05:15,640 --> 00:05:24,040
problem and your approach to it? Yes. So visual localization is an entirely different problem.

48
00:05:24,040 --> 00:05:33,560
So here, what we're interested in is estimating the position of a camera in 3D within a given scene.

49
00:05:34,360 --> 00:05:40,040
And you can imagine that this given scene can be either a building for indoor localization

50
00:05:40,040 --> 00:05:48,280
or an entire city for outdoor localization. And this scene, this model is essentially

51
00:05:48,280 --> 00:05:54,440
reconstructed from images using standard computer vision techniques, such as structural

52
00:05:54,440 --> 00:06:00,680
promotion. And from this, you can just crawl a bunch of images from flicker and reconstruct,

53
00:06:00,680 --> 00:06:06,760
for example, entire cities in 3D. And so now the idea is that you're walking around that city

54
00:06:06,760 --> 00:06:11,960
and you take a picture of some building and you want to know exactly where your camera was located

55
00:06:11,960 --> 00:06:19,400
when you took that picture. And this has applications in, for example, augmented reality or robotic

56
00:06:19,400 --> 00:06:26,440
navigation where you really need a very precise localization of your camera. And so this is not a

57
00:06:26,440 --> 00:06:33,160
new field at all. So it has been tackled in computer vision for a really long time. But recently

58
00:06:33,160 --> 00:06:39,000
people started tackling this problem, of course, with deep learning. And so we were looking into

59
00:06:39,000 --> 00:06:45,320
this with a master thesis here at TUM. And we actually realized that people were kind of

60
00:06:46,200 --> 00:06:51,720
forgetting all the classic research in these fields. So they were just blindly taking neural

61
00:06:51,720 --> 00:06:56,520
networks and applying it to this problem where the input was an image and the output was directly

62
00:06:56,520 --> 00:07:03,880
the camera poles. So what we're, what we're working on now is to kind of fusing the classic knowledge

63
00:07:03,880 --> 00:07:11,880
of geometry, multiple view geometry, feature extraction and all the knowledge that was gathered

64
00:07:11,880 --> 00:07:18,200
in something like 10 plus years of research in visual localization and trying to fuse this with

65
00:07:18,200 --> 00:07:26,520
the data driven or deep learning methods. And within this project in particular, what we're doing

66
00:07:26,520 --> 00:07:32,600
is not actually trying to extract an absolute pose of the camera, which is what methods like

67
00:07:32,600 --> 00:07:39,240
PostNet are doing, but actually estimating a relative pose between a camera that you know

68
00:07:39,240 --> 00:07:46,600
where dislocated because you used it to create your model and this new image that you're capturing.

69
00:07:46,600 --> 00:07:52,440
And so this method is actually much more flexible because you can go anywhere, take a picture,

70
00:07:52,440 --> 00:07:59,240
and as long as you have some other pictures around you that you know were used to create your model,

71
00:07:59,240 --> 00:08:03,720
you can actually localize yourself, which is not true for PostNet where you actually need to

72
00:08:03,720 --> 00:08:09,000
retrain a different neural network for every specific scene where you want to localize yourself.

73
00:08:09,640 --> 00:08:14,920
And so we have, we have been working towards this goal of relative pose estimation with neural

74
00:08:14,920 --> 00:08:20,200
networks and including some geometry, multiple view geometry information. And this was,

75
00:08:21,640 --> 00:08:28,600
this is the work of one of my PhD students who also she recently started and it's also

76
00:08:28,600 --> 00:08:35,800
together in collaboration with Thorsten Sadler from ETH Zurich. And the ultimate goal is,

77
00:08:35,800 --> 00:08:41,480
of course, to handle more and more complex scenes that could not be handled before with classic

78
00:08:41,480 --> 00:08:47,560
methods. And by more complex scenes, what I mean is, for example, dynamic scenes. And this is

79
00:08:47,560 --> 00:08:54,920
where my expertise in multiple object tracking comes in. So if you are actually observing a city,

80
00:08:54,920 --> 00:09:01,640
it's very easy to localize your camera if your city is empty. And so you only have the buildings

81
00:09:01,640 --> 00:09:07,480
and the static parts in there. But as soon as you have crowds of people and all these dynamic objects,

82
00:09:07,480 --> 00:09:13,560
they just, they are not good for your localization. So they're basically noise for your localization.

83
00:09:13,560 --> 00:09:18,760
So we want eventually to build a pipeline that is robust enough to handle these dynamic scenes.

84
00:09:18,760 --> 00:09:25,320
One of the things that you mentioned is a bit of a recurring theme here on the podcast. And that

85
00:09:25,320 --> 00:09:35,000
is this idea of fusing deep learning based methods, CNNs, I imagine, in this case, and more

86
00:09:35,000 --> 00:09:43,240
traditional methods to kind of inform those CNNs. And there's, there seems to be a tension in the

87
00:09:43,240 --> 00:09:50,200
in the research about whether an end-to-end deep learning approach is, you know, better given

88
00:09:50,200 --> 00:09:57,640
sufficient data and sufficient compute or whether we should try to incorporate the, you know,

89
00:09:57,640 --> 00:10:01,400
the various things we've learned about these various problems we're trying to solve or the

90
00:10:01,400 --> 00:10:08,440
physics of given situations. Maybe talk a little bit about the model that you're building to

91
00:10:08,440 --> 00:10:16,040
fuse those two types of approaches and how you've gone after that specific part of it.

92
00:10:17,160 --> 00:10:22,360
Yes, so that's actually a very, a very good question because it's really a central point that

93
00:10:22,360 --> 00:10:27,640
is, that is coming up more and more in discussions within the community. So I think there was a

94
00:10:27,640 --> 00:10:34,200
really big shift from these model-based methods to data-driven methods, fully data-driven.

95
00:10:34,200 --> 00:10:40,600
And in theory, if one has enough computational resources enough data, this is the perfect setup

96
00:10:40,600 --> 00:10:47,000
because you have a universal approximator that can just express any function that you want to find.

97
00:10:47,000 --> 00:10:52,840
So this in theory is perfect. The problem is that data is very costly to obtain and so no people

98
00:10:52,840 --> 00:10:58,600
are talking about constraining your deep architecture with some previous knowledge as you said,

99
00:10:58,600 --> 00:11:04,520
like, like, physics or in our case, multiple view geometry. Now, the interesting question is,

100
00:11:04,520 --> 00:11:10,680
where do you put this information? So do you put it in the loss function? Do you constrain your

101
00:11:10,680 --> 00:11:17,080
activation maps to have certain shapes? Or you can also, for example, actually make your

102
00:11:17,080 --> 00:11:22,920
architecture more modular so that each of the different parts is interpretable and has,

103
00:11:22,920 --> 00:11:28,920
for example, physical meaning. And this is still a question that there is not entirely solved,

104
00:11:28,920 --> 00:11:33,560
right? So this is something that people are actively researching and we don't have the ultimate

105
00:11:33,560 --> 00:11:38,760
answer. So how we did it for the Visual Localization project is essentially,

106
00:11:40,120 --> 00:11:46,760
we propose to mimic the pipeline of the feature matching that is also present in classical

107
00:11:46,760 --> 00:11:52,840
Visual Localization methods where you have two images. You want to find the relative pose between

108
00:11:52,840 --> 00:11:58,440
them. And essentially what you do is you compute the series of features, then you try to match

109
00:11:58,440 --> 00:12:05,080
these features and find a pose that explains these matches in a coherent way so that the geometry

110
00:12:05,080 --> 00:12:11,240
is okay for most of the matches. And this pipeline is actually very robust. And one of the problems

111
00:12:11,240 --> 00:12:17,880
that it has is that this feature extraction step was completely handcrafted before. So people

112
00:12:17,880 --> 00:12:23,320
were using sift features or surf features. And this worked really well for many scenes, but they

113
00:12:23,320 --> 00:12:31,560
didn't work, for example, indoor. What are sift and surf features? Oh, these are so sifter scaling

114
00:12:31,560 --> 00:12:40,600
variant features. They're essentially feature descriptors like a bit like corner detectors. And they

115
00:12:40,600 --> 00:12:49,240
just, they tell you where an interesting or let's say a defining feature of your scene is.

116
00:12:49,800 --> 00:12:55,880
And what is it's descriptor? So you attach a vector to this particular feature. So when you look

117
00:12:55,880 --> 00:13:02,360
at the same scene, maybe from a slightly different angle, you get for the same point a very similar

118
00:13:02,360 --> 00:13:07,480
descriptor. And then you can say, oh, these two points are actually the same 3D point. They are

119
00:13:07,480 --> 00:13:15,080
just seen by two different from two different positions. But the point is that this descriptor

120
00:13:15,080 --> 00:13:21,880
should be very similar for the same pointy 3D. So that the sift features are playing a role

121
00:13:21,880 --> 00:13:28,440
similar to like an edge detector and classical object detection. Yeah, for example, yeah.

122
00:13:29,240 --> 00:13:35,240
You mentioned a few different ways to incorporate this geometry into a pipeline.

123
00:13:36,200 --> 00:13:41,640
How does the way that you've chosen to do it map up to some of the general ways that you

124
00:13:41,640 --> 00:13:46,760
mentioned incorporating into the loss function, incorporating it into the shape of activations?

125
00:13:46,760 --> 00:13:56,360
So it kind of approaches two sides. So one is definitely the loss function. The loss function is

126
00:13:56,920 --> 00:14:02,440
acting directly on the essential matrix prediction, which is the matrix that gives you the relative

127
00:14:02,440 --> 00:14:09,800
pose between the cameras. And the other thing is the the actual architecture. So we're making the

128
00:14:09,800 --> 00:14:17,320
architecture modular in in the sense that since we're mimicking or trying to mimic this this

129
00:14:17,320 --> 00:14:25,480
classic feature matching pipeline, we try also to place the architecture to design the architecture

130
00:14:25,480 --> 00:14:32,120
so that each of the parts mimics this this visualization pipeline. So we know that in the first

131
00:14:32,120 --> 00:14:37,160
CNN, you're going to extract some features that are going to be representative of your scene.

132
00:14:37,160 --> 00:14:41,720
Then you're going to have a matching step and then you're going to have an essential matrix

133
00:14:41,720 --> 00:14:48,680
or relative pose regression. And and this kind of dividing your neural network into different

134
00:14:48,680 --> 00:14:54,360
interpretable parts is is also one way to first of all understand what's happening with your

135
00:14:54,360 --> 00:14:59,800
neural network and second of all includes some information, some hard coded information in the middle

136
00:14:59,800 --> 00:15:06,200
if you need to. You mentioned the essential matrix. What is what is that and what's the role that it

137
00:15:06,200 --> 00:15:12,840
plays in this pipeline? So the essential matrix gives you essentially the relative pose between

138
00:15:12,840 --> 00:15:19,560
two cameras. So it tells you how can you go from a point in one camera to the exact same point

139
00:15:19,560 --> 00:15:24,440
but a scene from the other camera. So it basically transforms your coordinates from one camera

140
00:15:24,440 --> 00:15:29,400
to the other. And this is this is all that we want to find. So once you have relative poses,

141
00:15:29,400 --> 00:15:35,640
then you can localize your camera. You can create all these maps, these 3D maps from images that

142
00:15:35,640 --> 00:15:41,640
I was talking about. And this is this is essentially what we want as a result from our algorithm.

143
00:15:41,640 --> 00:15:52,680
Okay. And so how how do you characterize the results of this approach relative to the alternatives,

144
00:15:52,680 --> 00:15:57,720
you know, both traditional as well as entirely data driven approaches?

145
00:15:58,520 --> 00:16:05,000
Well, compared to other data driven approaches, one big difference is that most of the

146
00:16:05,000 --> 00:16:11,320
approaches are still tackling the absolute pose estimation problem, which means that you have

147
00:16:11,320 --> 00:16:17,800
a very well defined scene with an origin, which is the world coordinate of that origin,

148
00:16:17,800 --> 00:16:24,280
of that scene, sorry. And then you you localize yourself within that world coordinate,

149
00:16:24,280 --> 00:16:29,080
which means that suddenly now if you have another scene, this other scene will have a completely

150
00:16:29,080 --> 00:16:34,200
different world coordinate. And so what you're going to do is you're going to have to train one

151
00:16:34,200 --> 00:16:39,400
neural network per each of the scenes where you want to localize yourself. And this is not

152
00:16:39,400 --> 00:16:44,040
something that we want to do. Of course, we want to build a system that is able with a single

153
00:16:44,040 --> 00:16:50,840
network to localize yourself anywhere. And this is one of the key differences of our proposed method,

154
00:16:50,840 --> 00:16:56,600
which actually tackles relative pose. So you can localize yourself everywhere with a single network.

155
00:16:56,600 --> 00:17:05,000
This is one huge difference. And compared to classic visualization, we also showed in an

156
00:17:05,000 --> 00:17:13,000
in an ICCV paper last year, 2017, we showed for the first time a comparison of classic methods versus

157
00:17:13,640 --> 00:17:19,640
deep learning methods. And we showed essentially that that classic methods are much much more accurate

158
00:17:19,640 --> 00:17:25,960
by by even an order of magnitude sometimes in some scenes, which means that well deep learning is

159
00:17:25,960 --> 00:17:32,920
still not there yet. But an advantage that deep learning has is that it can handle, for example,

160
00:17:32,920 --> 00:17:41,880
large indoor scenes, which have a few texture. So for example, large textureless surfaces like

161
00:17:41,880 --> 00:17:47,880
white walls or repetitive structures. For example, in a building, you have all the stairs that look

162
00:17:47,880 --> 00:17:53,880
the same, all the doors that look the same. And sometimes even as a person, if you go, for example,

163
00:17:53,880 --> 00:17:58,520
to a hospital, it's hard to to know where you are in the hospital. It's very easy to get lost.

164
00:17:59,080 --> 00:18:05,080
And this is the same for for classic methods where they focus on on these basic features,

165
00:18:05,080 --> 00:18:11,000
and then they compare to staircases and they don't know which is which. And deep learning methods

166
00:18:11,000 --> 00:18:17,560
are very, very good at capturing other subtle features that help them to localize better in this

167
00:18:17,560 --> 00:18:23,960
repetitive indoor environments. So this is one real strength and where we see really the the

168
00:18:23,960 --> 00:18:30,840
application for for deep learning methods. And this is why also we proposed a new data set called

169
00:18:30,840 --> 00:18:38,280
TUM LSI large scale indoor localization data set. And this we also published last year in our

170
00:18:38,280 --> 00:18:44,280
ICC paper. And this is a really, really challenging task to localize yourself in those indoor

171
00:18:44,280 --> 00:18:52,680
environments. What's your intuition for why deep learning is so much better at that particular

172
00:18:52,680 --> 00:19:00,920
type of environment relative to classical approaches and yet worse overall in the traditional

173
00:19:00,920 --> 00:19:08,200
types of environments? Well, the key is that classic methods are based on these features that I

174
00:19:08,200 --> 00:19:17,080
was talking about before. So sift or serve. And these features can only be present when you have

175
00:19:18,760 --> 00:19:26,360
let's say characteristics parts in your image. So if you have a white wall with no corners,

176
00:19:26,360 --> 00:19:33,080
no edges, then there's not going to be any feature on that white wall. Well, if you go for example

177
00:19:33,080 --> 00:19:38,920
to marine plots and you have this this beautiful building, which is the city hall, it's full of

178
00:19:38,920 --> 00:19:45,240
tiny details, tiny corners. And so you're going to have really many, many features firing on

179
00:19:45,240 --> 00:19:50,600
that building. So when you have a localization pipeline that is based on those features,

180
00:19:51,560 --> 00:19:55,960
it's really easy to localize yourself in marine plots, for example, because you have many, many

181
00:19:55,960 --> 00:20:01,480
features to base your localization on. But if you just see a white wall and you have no features,

182
00:20:01,480 --> 00:20:08,520
then you have nothing to compute your camera position, because there's no features, so there's

183
00:20:08,520 --> 00:20:15,080
no matching happening. And this matching between 2D features and the features in your 3D model is

184
00:20:15,080 --> 00:20:24,120
the key to the visual localization pipeline. And so deep learning looks more at the overall picture,

185
00:20:24,120 --> 00:20:28,840
so looks at the white wall, but also maybe at the column on the right, maybe there's some chair

186
00:20:28,840 --> 00:20:34,520
also in this scene, and try to look at the whole picture and give you a descriptor for the whole

187
00:20:34,520 --> 00:20:40,760
scene. So even if you don't have many corners, or if you have like a white wall, you can also use

188
00:20:40,760 --> 00:20:45,720
the fact that there's a white wall without features in there to localize yourself. And this is

189
00:20:45,720 --> 00:20:52,120
something that classic methods don't do, or at least classic methods based on sift and serve

190
00:20:52,120 --> 00:20:59,800
type of features. Now you've got in this architecture, you've got kind of this, I guess what I'm

191
00:20:59,800 --> 00:21:07,160
envisioning as a feature identification or extraction stage or module or something along those lines.

192
00:21:07,160 --> 00:21:16,760
And then once you've done that, are you reusing existing neural network architectures to perform

193
00:21:16,760 --> 00:21:24,120
some of the rest of the pipeline, or is it all kind of new architectures?

194
00:21:24,920 --> 00:21:32,280
No, so actually the feature extraction part is based on well-known architectures. So we tried

195
00:21:32,280 --> 00:21:39,240
a Google net, we tried ResNet, currently ResNet is what works best. And so this essentially

196
00:21:39,240 --> 00:21:44,440
is pre-trained on ImageNet, so it gives us a really, really good initialization for our ways.

197
00:21:44,440 --> 00:21:54,360
And how we use it is essentially we're training to go from the image to a descriptor, a vector

198
00:21:54,360 --> 00:22:02,200
of a certain number of elements. And for this, we do reuse these classic architectures. And then

199
00:22:02,200 --> 00:22:08,120
the rest is trained from scratch. And how do you determine what the dimensionality of this

200
00:22:08,120 --> 00:22:15,560
feature vector is? Well, this is a bit of a trial and error, right? So right now we're using,

201
00:22:15,560 --> 00:22:25,320
we tried using something like 2000, I think it was 2048. We also try using 4000 or even smaller.

202
00:22:25,320 --> 00:22:29,880
And in the end, you kind of do trial and error and see what works best for your problem.

203
00:22:29,880 --> 00:22:39,800
And then taking a step back to the broader problem you've got, these kind of many images of these

204
00:22:39,800 --> 00:22:47,480
worlds or environments from different angles. And you've talked about the relative performance

205
00:22:47,480 --> 00:22:54,440
of these different types of approaches for kind of interior scenes versus exterior scenes.

206
00:22:54,440 --> 00:23:02,760
Are you training models that are good at one of these environments or are you somehow

207
00:23:02,760 --> 00:23:11,400
integrating these and starting to look at models that can handle different scales or different

208
00:23:12,360 --> 00:23:18,440
transition from one type of environment to the next? That's actually an excellent question.

209
00:23:18,440 --> 00:23:25,080
Ideally, so theoretically our network can handle any type of environment, indoor and outdoor.

210
00:23:25,640 --> 00:23:32,600
But in practice, the best thing is to train an network for indoor and an network for outdoor.

211
00:23:32,600 --> 00:23:37,080
So this is, in my opinion, still OK, because you only have two types of networks.

212
00:23:37,720 --> 00:23:44,840
And currently, the bottleneck is actually the step before the one where you actually train

213
00:23:44,840 --> 00:23:52,680
your network, which is to find pairs between your image, so the image that you are taking

214
00:23:52,680 --> 00:23:58,440
a test time and the training images that you have around you. So of course, you can imagine that

215
00:23:58,440 --> 00:24:05,080
you are in this city and you cannot evaluate all the possible training images that you have

216
00:24:05,080 --> 00:24:10,680
from the city and that you use to reconstruct your model. And so you have to prune all these

217
00:24:10,680 --> 00:24:15,880
images. And so a test time you arrive there, you take a picture and you want to find, let's say,

218
00:24:15,880 --> 00:24:22,760
the 30 pictures that are most similar to your picture. And so for this, we use yet another CNN

219
00:24:22,760 --> 00:24:31,160
architecture. And it's basically solved the problem that is called image retrieval. So it retweets

220
00:24:31,160 --> 00:24:36,920
images that are similar to your own image. So for example, you want to localize yourself in

221
00:24:36,920 --> 00:24:42,120
Munich. It doesn't make any sense to compute relative poses within your images and the images

222
00:24:42,120 --> 00:24:49,080
from Paris or London. So this network is able to tell you, look, these are 30 images that are

223
00:24:49,080 --> 00:24:55,400
most likely located in the same place where you are roughly speaking. And then you can compute

224
00:24:55,400 --> 00:25:01,640
the relative pose between these 30 images and your own query or test image. And the problem is

225
00:25:01,640 --> 00:25:07,720
that the CNN for image retrieval that works for outdoors doesn't work so well for indoors.

226
00:25:07,720 --> 00:25:13,080
So now we're trying to figure out whether we need to retrain this network completely different

227
00:25:13,080 --> 00:25:18,760
and just use two networks, one for indoor one for outdoor. Or if we can actually reuse some,

228
00:25:19,640 --> 00:25:25,720
some of this network that actually works so well outdoors. So this is kind of what we're researching

229
00:25:25,720 --> 00:25:32,600
right now. There's also a part of this that is related to scale and maybe it's kind of the same

230
00:25:32,600 --> 00:25:38,520
answer and you've addressed it. But when I think of, you know, these outdoor images, you mentioned

231
00:25:38,520 --> 00:25:47,080
at a square, you know, the camera tends to be a lot further from the buildings. Whereas if I'm

232
00:25:47,080 --> 00:25:54,040
navigating a hospital, you know, corridor, the camera tends to be very, very close, does just

233
00:25:54,040 --> 00:26:02,040
this, the idea of having two models address the scale issues or do you run into scale issues?

234
00:26:02,040 --> 00:26:09,000
You know, for example, depending on the within, you know, either of these two types of environments.

235
00:26:10,040 --> 00:26:17,160
So actually, we have to fix the scale of our essential matrices. So essentially, what we do is,

236
00:26:17,160 --> 00:26:23,160
is we take the translation and we give it, we force it to have norm of one. And with this,

237
00:26:23,160 --> 00:26:27,800
what we're essentially saying is we're fixing the scale of our essential matrix. Then we train

238
00:26:27,800 --> 00:26:34,840
the network to predict only essential matrices with this particular scale. And then at a later

239
00:26:34,840 --> 00:26:41,080
stage, we will triangulate from all the training images and the relative poses. We will triangulate

240
00:26:41,080 --> 00:26:48,600
the real scale and the real pose of the image at test time. And so with this, actually, we can

241
00:26:48,600 --> 00:26:56,360
handle varying scales. So it's not to say that the network is very robust to all the range of

242
00:26:56,360 --> 00:27:02,040
scales. So this is also something that we need to dig into. But theoretically, you can actually

243
00:27:02,040 --> 00:27:09,320
handle any type of scales. And how about generalization? How do you explore the generalizability of

244
00:27:09,320 --> 00:27:18,680
this approach? That's an excellent question. Because what we have recently observed is that the, so the

245
00:27:18,680 --> 00:27:25,000
more one of the advantages of our method is that since we're predicting relative poses and therefore

246
00:27:25,000 --> 00:27:32,680
we can use any image from any scene. Now we're not bounded to one scene. We can use much, much more

247
00:27:32,680 --> 00:27:37,560
training data. And we have observed that the more training that they use, of course, the better

248
00:27:37,560 --> 00:27:44,600
your localization is. So first it gets worse, but then it gets better. But still, you do need to see

249
00:27:44,600 --> 00:27:52,040
some examples from your test scene to be able to localize yourself properly there. So this is

250
00:27:52,040 --> 00:27:58,840
actually something that we're trying to figure out. Why is that? Because technically, the generalization

251
00:27:58,840 --> 00:28:05,160
part of the network is not perfect yet. Because ideally, you would want to train on a set of scenes

252
00:28:05,160 --> 00:28:09,800
and then have a completely new scene that the network has never seen before and be able to

253
00:28:09,800 --> 00:28:16,360
localize yourself there also. Well, this is not entirely true yet. So it does do a fairly good

254
00:28:16,360 --> 00:28:23,080
job, but it does a much, much better job when you see at least some images from that test scene.

255
00:28:23,080 --> 00:28:27,320
And so the generalization is still something that we're looking into.

256
00:28:27,320 --> 00:28:32,680
Really interesting, really interesting. So you're also doing some work on object segmentation.

257
00:28:32,680 --> 00:28:36,120
Is that related to this project or is it a totally separate effort?

258
00:28:36,840 --> 00:28:44,040
Well, it's all part of the interest of my group of the dynamic vision and learning group,

259
00:28:44,040 --> 00:28:51,800
which is to actually be able to analyze the dynamic scenes around you. And this, of course,

260
00:28:51,800 --> 00:28:57,320
englobs a bunch of problems. So ranging from the multiple object tracking problem where I start

261
00:28:57,320 --> 00:29:03,800
my PhD to the visual localization, which, of course, we're working also on video localization.

262
00:29:05,240 --> 00:29:12,520
And to be able to fully analyze what's happening around you and also analyze it in time.

263
00:29:12,520 --> 00:29:17,400
So not only image-based, but video-based, you also need to perform, for example,

264
00:29:17,400 --> 00:29:23,880
video object segmentation. And this is one project that we are also doing in collaboration with

265
00:29:23,880 --> 00:29:31,000
people from ETH Zurich. And in this particular project, what we wanted to do was we wanted to

266
00:29:31,000 --> 00:29:37,480
tackle the problem of supervised video object segmentation. And by supervised, what I mean is that

267
00:29:38,200 --> 00:29:46,520
you're given in the first frame of your video, a perfect mask that expresses which object you

268
00:29:46,520 --> 00:29:52,520
want to follow, which object you want to segment in your video. And so your goal now is to segment

269
00:29:52,520 --> 00:29:58,520
this object to find out which pixels in the following frames belong to this particular object.

270
00:29:59,080 --> 00:30:04,920
And so you can imagine that this is kind of easy if the object doesn't move much, if the object

271
00:30:04,920 --> 00:30:10,280
doesn't change the appearance much. But as soon as the object, for example, turns around and now

272
00:30:10,280 --> 00:30:15,640
you're seeing a completely different side of the object or the illumination changes or the position

273
00:30:15,640 --> 00:30:22,760
of the camera changes, these are all big challenges for this problem. And so what we proposed in a

274
00:30:22,760 --> 00:30:31,880
work that we presented at CEPR 2017 already is to actually do this fully data driven.

275
00:30:31,880 --> 00:30:39,320
So there is this wonderful data set created by Disney Research and ETH people that is called

276
00:30:39,320 --> 00:30:45,960
the Davis data set. And this contains really accurate segmentation masks. And by accurate, I mean

277
00:30:45,960 --> 00:30:51,720
sometimes crazy accurate where you see, I don't know, the hair of horses segmented out one by one.

278
00:30:51,720 --> 00:30:59,080
Oh wow. Yeah, it's really crazy. And so with this now, you have like this wonderful source

279
00:30:59,080 --> 00:31:08,840
to train your networks. And so what we propose to do is to actually, so the key idea in that paper

280
00:31:08,840 --> 00:31:14,040
was actually the training scheme. So it's not a classical training where you just input your data

281
00:31:14,040 --> 00:31:19,720
and you get your output. But first you take this network which is a classic convolutional neural

282
00:31:19,720 --> 00:31:25,800
network. You kind of pre-training for the task of video object segmentation. So you give it a

283
00:31:25,800 --> 00:31:32,680
bunch of of masks of object and you train your network. But what is happening now is that your

284
00:31:32,680 --> 00:31:39,960
network is trained to do some sort of foreground background separation. But it doesn't know which object

285
00:31:39,960 --> 00:31:45,160
you actually want to follow, right? You just have trained it with a bunch of objects. You can segment

286
00:31:45,160 --> 00:31:53,960
a bus, a car, a chair. So there's all these types of objects in your data set. And so now you still

287
00:31:53,960 --> 00:32:00,280
have to tell your network which object you actually want to segment. And so how we do it is in a

288
00:32:00,280 --> 00:32:06,440
second training stage, we use this first mask of the object that we do have available. And we

289
00:32:06,440 --> 00:32:12,360
kind of overfeed the network to that mask. So we say, really learn the appearance of that particular

290
00:32:12,360 --> 00:32:17,720
object. So overfeed your network to that particular object. And then the only thing that you need

291
00:32:17,720 --> 00:32:24,120
to do is you run your network for the rest of the frames. And it will be able to segment your

292
00:32:24,120 --> 00:32:30,840
object for the rest of the video. And this was actually a really, really novel way of doing the

293
00:32:30,840 --> 00:32:37,000
object segmentation. And there has been a bunch of works that have followed later this paradigm.

294
00:32:37,000 --> 00:32:45,560
And this was presented at CBR 17 and it's called Osvos for one shot video object segmentation.

295
00:32:46,200 --> 00:32:51,560
And the idea is that, so why we call it one shot is because you only see the object once in

296
00:32:51,560 --> 00:32:56,920
this first frame. And then you're able to segment it and tracking over the whole video.

297
00:32:57,640 --> 00:33:05,400
I'm trying to wrap my head around how you would go about the overfitting part of the training.

298
00:33:05,400 --> 00:33:13,240
And maybe the question that I'm coming to is, is this process something that is

299
00:33:14,040 --> 00:33:19,320
automatable or something that you can build into an automated process? Or is it a very manual

300
00:33:19,320 --> 00:33:27,560
process that needs to be supervised by a researcher or practitioner in order to work?

301
00:33:28,280 --> 00:33:32,440
No, essentially, this is completely automatic. So the only thing that you need is the

302
00:33:32,440 --> 00:33:39,960
mask of your object for one of the frames. And then you pass this mask as a training example

303
00:33:39,960 --> 00:33:46,520
to your network. You do all sorts of data segmentation, for example. And you train your network for

304
00:33:46,520 --> 00:33:52,600
really quite some iterations. And during this process, essentially what your network learns is

305
00:33:52,600 --> 00:33:56,920
what is the appearance of this object and what is the appearance of the background. And this is

306
00:33:56,920 --> 00:34:01,160
completely automatic. So as long as you have the mask of your object, you're good to go.

307
00:34:02,200 --> 00:34:09,320
Of course, the bad thing or the small drawback is that if your background or your object change

308
00:34:09,320 --> 00:34:16,040
the appearance too much. For example, a new object appears in the scene or a second object that

309
00:34:16,040 --> 00:34:21,240
looks exactly like your first object appears on the scene that is going to be a problem.

310
00:34:21,240 --> 00:34:29,000
So we had this sequence, the famous two camel sequence, where you're segmenting this camel that

311
00:34:29,000 --> 00:34:34,600
is moving along. And then a second camel appears on the scene. And of course, the second camel looks

312
00:34:34,600 --> 00:34:39,720
exactly the same as the first camel. And so our network suddenly says, well, this is also my

313
00:34:39,720 --> 00:34:45,080
object. I also want to follow this. And so it segments the two camels. But now the problem is that

314
00:34:45,080 --> 00:34:50,920
you only want to segment one camel. So what you can do is is provide another training image to your

315
00:34:50,920 --> 00:34:56,920
network where you have the two camels. And one is segmented as your true camel that you want to

316
00:34:56,920 --> 00:35:02,280
follow. And the other is segmented as background. So you don't really care about the other camel.

317
00:35:02,280 --> 00:35:06,680
So now you're giving your network the further information that there's a second camel in the

318
00:35:06,680 --> 00:35:11,800
scene, but you don't really want to segment it. And with this, you can actually correct your

319
00:35:11,800 --> 00:35:17,480
network and find you need a little bit more towards only the first camel. And so now you will be able

320
00:35:17,480 --> 00:35:25,560
to segment only one camel in this scene. Does this apply to specific types of objects? How

321
00:35:25,560 --> 00:35:32,840
broadly does it apply? For example, what is the types of objects that are in this Davis data set?

322
00:35:32,840 --> 00:35:39,960
And does that, to what extent does that constrain the model or is the model focused on those types

323
00:35:39,960 --> 00:35:46,200
of images? So actually, you can tackle any type of object. This is completely independent from

324
00:35:46,200 --> 00:35:52,360
the object types. Davis has quite a wide range of objects, I would say. But the interesting thing

325
00:35:52,360 --> 00:36:00,360
is that we're not basing ourselves on object proposals or on object classifiers. But it's really

326
00:36:00,360 --> 00:36:05,640
this foreground background separation. So if you have a picture with whatever strange object

327
00:36:05,640 --> 00:36:10,440
and this object is segmented out, the network is going to be able to learn the appearance of this

328
00:36:10,440 --> 00:36:14,760
object. And it doesn't matter if it's an elephant or if it's a chair, you're going to be able to

329
00:36:14,760 --> 00:36:21,320
follow it. Now, of course, if you are constrained by certain object types, you can do a better work.

330
00:36:21,320 --> 00:36:29,000
And indeed, we also worked on that in the upcoming journal extension for this paper, which we

331
00:36:29,000 --> 00:36:35,880
also published last year. And this was the idea that you can use all these object proposals,

332
00:36:35,880 --> 00:36:42,120
for example, coming from Moscow CNN, to kind of help you and guide you through the segmentation.

333
00:36:42,760 --> 00:36:47,560
So even the first mask, you find that you have a lot of overlap between your segmentation.

334
00:36:47,560 --> 00:36:53,720
And for example, the proposal of a motorbike, then you're kind of sure that the object that

335
00:36:53,720 --> 00:36:58,600
you're trying to follow is a motorbike. And then you can constrain your segmentation also

336
00:36:58,600 --> 00:37:04,520
with the series of proposals that the maskar CNN gives you. But this is something that is,

337
00:37:04,520 --> 00:37:10,040
it's kind of an extra to improve segmentation, but you don't need to do it. So if you don't know

338
00:37:10,040 --> 00:37:14,680
what object you want to follow, you can still use the method as it is, and it will still be able to

339
00:37:14,680 --> 00:37:20,920
follow it. There was another project that came across that you were working on called social maps.

340
00:37:20,920 --> 00:37:29,400
What's that one about? Yeah, so social maps is actually a bit of my vision of what computer vision

341
00:37:29,400 --> 00:37:37,000
and AI can do for society. So in this project, I actually proposed this whole research project

342
00:37:37,000 --> 00:37:45,640
for an award. So it was this research grant proposal. And I was lucky enough that it was accepted

343
00:37:45,640 --> 00:37:54,360
for the Sofia Kovalovskaya Award by the Humboldt Foundation last year. So very few really talented

344
00:37:54,360 --> 00:38:01,960
researchers get this award. And I was lucky to be in this pool of researchers. And congratulations.

345
00:38:01,960 --> 00:38:07,720
Thank you. Thank you very much. Well, the main advantage of course is that it comes with quite

346
00:38:07,720 --> 00:38:16,280
some money, 1.65 million euros. And with that, of course, I can pay for PhD students for equipment.

347
00:38:16,280 --> 00:38:23,320
So this is really the big advantage of such an award that I was able to start my research group

348
00:38:23,320 --> 00:38:28,280
right away. And so I have right now three wonderful PhD students that are working really hard.

349
00:38:28,280 --> 00:38:34,440
And so all these projects that I mentioned are actually with all of them. And essentially the

350
00:38:34,440 --> 00:38:40,360
project social maps is very much related to the idea that I've said before that we want to

351
00:38:40,360 --> 00:38:47,720
understand these dynamic scenes around us. But it has a very specific application. So if you think

352
00:38:47,720 --> 00:38:54,280
about Google maps, for example, you see that they are really an excellent source of static information.

353
00:38:54,280 --> 00:39:02,040
So they have very precise maps of where roads are, where shops and restaurants are. So wherever

354
00:39:02,040 --> 00:39:07,800
you want to go, Google maps is going to give you a really optimized route that you can follow and

355
00:39:07,800 --> 00:39:15,320
you will get to your place, wherever you are. And so the idea though is that this is still a

356
00:39:15,320 --> 00:39:21,720
relatively limited information because it's only static information. And what I would like to

357
00:39:21,720 --> 00:39:27,560
input into maps is the social information. And so what I mean by social information is essentially

358
00:39:27,560 --> 00:39:34,280
how pedestrians, how people use public spaces. So I want to automatically analyze the motion of

359
00:39:34,280 --> 00:39:42,280
pedestrians, what is happening in the streets in real time. And I want to input this information

360
00:39:42,280 --> 00:39:48,360
into maps. So for example, let's assume that I have to go from my home, which is in the south of

361
00:39:48,360 --> 00:39:53,400
Munich. And I have to go to my workplace, which is actually in Garching, so quite far away.

362
00:39:53,400 --> 00:39:59,720
And I'm taking my car and I'm putting into Google maps, my destination. And then Google maps

363
00:39:59,720 --> 00:40:06,360
just tells me a way to follow. But let's imagine that now I have some cameras in the middle of

364
00:40:06,360 --> 00:40:11,960
the city of Munich. And I can detect automatically that there's some demonstration starting. So there's

365
00:40:11,960 --> 00:40:16,840
a bunch of people gathering, they are cutting the streets. So I'm not going to be able to pass

366
00:40:16,840 --> 00:40:23,960
through those streets. Now if you take Google maps as it is now, you will only be notified about

367
00:40:23,960 --> 00:40:30,280
this once there's already a traffic jam. So traffic reports are included into Google maps,

368
00:40:30,280 --> 00:40:35,640
but you don't want to be the first one that is stuck into traffic. So I want to be able to detect

369
00:40:35,640 --> 00:40:40,840
how people use public spaces and whether there's a demonstration or whether kids are coming out

370
00:40:40,840 --> 00:40:47,080
of school. I want to be able to detect it fast by using this dynamic scene understanding and

371
00:40:47,080 --> 00:40:53,480
using computer vision and AI. And I want to input this information into maps so that now I don't get

372
00:40:54,280 --> 00:40:59,640
the normal optimized route, but somehow a social route. And by social route, what I mean is

373
00:40:59,640 --> 00:41:06,040
I want to decouple pedestrian traffic from vehicle traffic. So if cars are going always in certain

374
00:41:06,040 --> 00:41:12,120
roads, and now these roads are being used by pedestrians, I want to tell cars to use

375
00:41:12,120 --> 00:41:17,880
other routes or other roads. And this is kind of the, for example, one of the short-term

376
00:41:17,880 --> 00:41:24,120
applications, but of course in the long term, having access to all these data and all these

377
00:41:24,120 --> 00:41:29,720
analyzed data. So trajectories of pedestrians, how pedestrians use public spaces, where do they

378
00:41:29,720 --> 00:41:36,920
tend to cross? How do which exits, for example, from a railway station do they use most?

379
00:41:37,640 --> 00:41:41,800
If you provide this information to people that are actually planning the cities,

380
00:41:41,800 --> 00:41:47,560
so the urban planners, then they can hopefully design better cities also for pedestrians.

381
00:41:47,560 --> 00:41:51,720
So this is kind of the long-term goal of this project social maps.

382
00:41:51,720 --> 00:42:00,680
Ah, interesting. It makes me think of a kind of a map system like Waze, but curated using,

383
00:42:00,680 --> 00:42:08,440
or updated using video information as opposed to this telemetry from other drivers using the app.

384
00:42:09,160 --> 00:42:13,320
Exactly, exactly. And the advantage is that you don't have to share your information,

385
00:42:13,320 --> 00:42:18,440
so you are just there and things are just detected. But of course, you're not identified,

386
00:42:18,440 --> 00:42:24,200
you're not followed. So it's just about following the trends of people, not the actual persons.

387
00:42:24,200 --> 00:42:26,760
I think this is an important difference to make here.

388
00:42:27,400 --> 00:42:33,640
So it sounds like the projects that we've talked about are you kind of think of as individual

389
00:42:33,640 --> 00:42:42,200
pieces within this overarching vision of social maps. Are there specific projects that you are

390
00:42:42,200 --> 00:42:51,640
working on that get you to the map piece, or does that come later? So that comes a bit later.

391
00:42:51,640 --> 00:42:58,600
So I'm more interested in the dynamic understanding part. So I've also one PhD student that just

392
00:42:58,600 --> 00:43:03,960
started and it's going to work more on what we talked about before, some called training networks

393
00:43:03,960 --> 00:43:10,360
with physical models. And in particular, apply them to multiple object tracking. So this is still

394
00:43:10,360 --> 00:43:19,000
a problem that is not solved. So we in 2014 opened a benchmark for multiple object tracking,

395
00:43:19,000 --> 00:43:24,440
called Mod Challenge. And you can see there that multiple object tracking methods have been

396
00:43:24,440 --> 00:43:29,400
really pushing the state of the art up. And so they've become better and better. But there's still

397
00:43:29,400 --> 00:43:35,400
tons of problems there. And in crowded scenes where you can barely identify one pedestrian from

398
00:43:35,400 --> 00:43:40,920
the other, this is still a big problem. So we want to still work on that particular problem.

399
00:43:41,720 --> 00:43:48,040
We want to work also on semantic segmentation, so identifying where in the city you are and

400
00:43:48,040 --> 00:43:53,080
which part of the city you're observing. Of course, for moving cameras, this is when the visual

401
00:43:53,080 --> 00:43:59,560
localization project comes in. And then I have also one other PhD student working more

402
00:43:59,560 --> 00:44:07,080
on general training or improving the training, making it easier to train big networks. So working

403
00:44:07,080 --> 00:44:13,480
more on a bit theoretical machine learning. So it's all these little projects that are coming

404
00:44:13,480 --> 00:44:21,480
together. But I'm still not working on the part of the actual map. So I think Google Maps already

405
00:44:22,680 --> 00:44:27,560
has a good architecture and a good construction for these maps. So it would just be about

406
00:44:27,560 --> 00:44:33,000
providing these information. So performing this this motion analysis for pedestrians. And that's

407
00:44:33,000 --> 00:44:39,320
the main work of the that I want to tackle. Well, it sounds like really exciting work. Laura,

408
00:44:39,320 --> 00:44:44,360
I really appreciate you taking the time to chat with me about it. Thanks so much. Yeah, thank you

409
00:44:44,360 --> 00:44:52,200
very much for inviting me. It was great. All right, everyone. That's our show for today.

410
00:44:52,200 --> 00:44:58,920
For more information on Laura or any of the topics covered in this episode, head over to Twomlai.com

411
00:44:58,920 --> 00:45:06,600
slash talk slash 168. Don't forget July 31st is your last chance to nominate us for this year's

412
00:45:06,600 --> 00:45:14,520
People's Choice Podcast Awards. Head over to Twomlai.com slash nominate and cast your vote right now.

413
00:45:14,520 --> 00:45:24,520
As always, thanks so much for listening and catch you next time.

