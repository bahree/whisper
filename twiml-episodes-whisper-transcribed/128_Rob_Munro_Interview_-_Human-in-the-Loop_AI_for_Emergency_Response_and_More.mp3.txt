Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
In this episode, I chat with Rob Monroe, CTO of the newly rebranded Figure 8, formerly
known as Crowdflower.
Figure 8's human and aloof AI platform supports data science and machine learning teams, working
on autonomous vehicles, consumer product identification, natural language processing, search relevance,
intelligent chat bots, and more.
Rob and I had a really interesting discussion, covering some of the work he's previously
done applying machine learning to disaster response and epidemiology, including a use case
involving text translation in the wake of the catastrophic 2010 Haiti earthquake.
We also dig into some of the technical challenges that he's encountered in trying to scale
the human and aloof side of machine learning, since joining Figure 8, including identifying
more efficient approaches to image annotation, as well as the use of zero-shot machine learning
to minimize training data requirements.
Finally, we briefly discuss Figure 8's upcoming train AI conference, which takes place
on May 9th and 10th in San Francisco.
At train AI, you can join me and Rob, along with a host of amazing speakers like Gary
Casparov, Andre Carpathi, Marty Hurst, and many more, and receive hands-on AI, machine
learning, and deep learning training through real world case studies on practical machine
learning applications.
For more information on train AI, head over to figure-8.com slash train-AI, and be sure
to use the code TwimmelAI, that's TWIMLAI, for 30% off of your registration.
For those of you listening to this on or before Friday, April 6th, Figure 8 is offering an
even better deal on event registration.
Use the code Figure-8 to register for only $88.
A huge thanks to Figure 8 for sponsoring this episode of the podcast.
And now on to the show.
All right, everyone.
I am on the line with Rob and Rob.
Rob is CTO of Figure 8, the company that you may know as Crowdflower.
They recently changed their name.
Rob, welcome to this weekend machine learning and AI.
Thank you.
It's great to be here.
It's great to have you on the show.
We usually start by having our guests give us a little bit of an introduction to their
backgrounds and how they got interested and involved in machine learning and AI.
Why don't you introduce yourself to the audience?
I'd be delighted to.
My introduction to AI was a little bit circular.
So I was working as a software developer for a number of years, not really focused on
artificial intelligence.
And that had taken me to work for the United Nations Eye Commission for Refugees.
I was working in refugee camps in Liberia while I was living in Sierra Leone.
And there was one particular moment when I was in rural Liberia in a refugee camp there.
And we were installing solar power systems at a clinic supporting the camp.
And we heard all these rumors about refugees coming over the border from Cottovoire.
But we didn't know whether there were 10 refugees or 10,000.
They're in just the next valley, but we couldn't reach them.
And ultimately, after a day of doing work at this clinic, we had to move on without establishing
work condition.
These refugees were in and how many they were.
And what really stood out for me was that I had five bars of cell phone reception at
that time.
And I'm lucky to get a full reception here in Silicon Valley.
And so there's no doubt that this refugee community also had cell phones with them.
Those cell phones were probably bouncing signals off the same tower as mine, but I'd
know where to connect with them.
And even if we did, we probably didn't share any languages.
And the languages for which they did speak, even basic things that we took for granted
and AI 10 years ago, like spam filtering or search engines, wouldn't have worked well
or at all for their languages.
So there really just wasn't anything out there.
And in terms of the supporting technology, which would allow us to understand or translate
between the languages.
And so it was clear to me at that time that we really solved the problem of connect in
the world.
Now, but we had an imbalance in how we're bringing services.
And so I thought, well, anyone can be out drilling solar panels into the roof of a clinic
that doesn't really require specialized skills.
I studied artificial intelligence as an undergraduate or something that I'd remain interested
in.
And so it was at that point that I decided that I really wanted to pursue a career in
artificial intelligence and make sure that we could bring it to everybody in the world.
And so that's what ultimately took me back to graduate school.
So I completed my PhD at Stanford focused on how we can apply natural language processing
to low resource languages, both in health and disaster response context.
And since then, I've worked in a combination of social impact companies and also, you know,
right through to very large, you know, 4 to 500 tech companies like Amazon.
And that brought me to working at Figure 8 just six months ago where I'm really excited
to be the new CTO here.
Awesome.
What did you do your PhD on?
Sure.
So I was looking at ways where we could go from very little initial training data to adapted
models in health and disaster response contexts, especially looking at text messages.
So when people were sending text messages to a hospital clinic in Malawi in a language
district with Chichoa, and then also looking at two disaster response communication data
sets in Haiti and Pakistan from an earthquake in 2010 and floods later that year in Pakistan.
These are actual disasters that as a disaster response professional I worked in.
And so I was very much answering that question for myself, okay, if we don't have any initial
training data in these languages, what's the minimum amount of human interaction and labeling
and categorization and mapping of these messages that is required until we can start building
out automated services in these languages in a way that's language independent.
Oh, interesting.
And how far did you get?
Did you clearly not have completely solved problem, but what did you, what conclusion did
you arrive at?
The broad conclusion was that you could do a lot of so word modeling, which would mean
that you could take one technique and get very similar accuracies across these different
languages.
But what an important observation was that the techniques that worked in English would
not necessarily carry over to other languages.
So linguistically, English tends to be an outlier language in that the spelling is extremely
standardized, even with like the US and the UK variants.
People are highly literate and we have very few affixes in English, very few prefixes
and suffixes compared to most languages.
And what that means is that a lot of approaches to data language processing that views a single
word as a fundamental unit won't apply, won't be very accurate in other languages.
So for example, in the text messages between health workers in Chichoa, in 600 messages,
there were more than 50 different spellings for the word patient due to variations in literacy,
but also just due to the large number of different suffixes and combinations of suffixes and
prefixes that you get in that language.
And so it was positive in terms of being able to find techniques that we've now deployed
in disaster response situations globally, but it also raised some questions about a lot
of research in natural language process, and there's just focused on English as to how
much it really could make an impact if you were taking it to the 7000 other languages
in the world.
Right, right.
Because your sense that the techniques broadly apply but the tooling and the dictionaries
and all of that, you know, haven't, you know, need to be kind of ported over to these
other languages or that there's, you know, a whole set of different research that has
to happen to support languages with different structures.
And part of the question is prompted by at least on the, you know, with deep learning
and neural networks, a lot of what I hear in the conversations that I'm thinking back
to a specific interview where the, I think this was Shubus and Gupta at Baidu at the time
was talking about how they did their, you know, English to Mandarin translation without
any Mandarin speakers, you know, on staff at the time because the deep neural networks
were able to figure out the meaning and the ability to translate without having specific
knowledge, structural knowledge of really either of the languages.
So how do you think that that kind of translates to, you know, this issue that you're pointing
out the disparity in kind of tools and research across languages?
I think a lot of that research is being really exciting in terms of being able to get up
to speed a lot faster without resources like dictionaries or the sources or other resources
in those languages.
So for most languages in the world, we don't have a dictionary, so we're not an online
dictionary that is easily available for, for natural language tools, for the majority
of the world's languages, the only real scientific resource we have is probably a PhD written
by a hippy linguist 40 years ago, and so that doesn't, doesn't give you much of a, much
of a starting point.
And so a lot of the techniques that I was looking at in my PhD and then I've seen taken
much further in exciting ways in more recent deep learning approaches has really focused
on making no assumptions about the structure going in and still being able to get these
really accurate results, and I think a lot of work in machine translation has shown
that.
The only caveat is that a lot of the neural-based techniques typically require a lot more
in the way of labeled data, and so there are particular use cases where the first time
you work in a language is following a sudden onset disaster, then you're coming up against
other problems of, you need some kind of model to start working with a very small amount
of data, and certainly you can't be waiting days, which is typical for training some
of these large machine-limited models.
So it's certainly been net positive, moved towards language independence, but it has been
bringing up some new problems associated with it.
Yeah, so this experience on the research side and running into the need to have these
labeled data sets in place in order to really make progress and create the kind of tools
that you're looking to create really set you up for, I guess, a deep and personal experience
for the need for organizations to be able to amass these labeled training data sets, and
you spent quite a bit of time working on that from the time you spent at the UNHCR and
then in grad school.
That's right.
Yeah, I was a client of figure eights, maybe five or six different times before joining
the company, and fully appreciated the value in being able to build up training data
sets for a number of different use cases.
The first time I used figure eight was during grad school in 2010, so at that point I was
tasked with running their first step in a 911 service, following earthquake in Haiti in
January 2010.
And at the time, more than 100,000 people killed immediately, and while a lot of local
services collapsed, as in buildings, physically collapsed, housing, and to respond services,
most of the cell phone towers remained active.
And so working with a number of people, and including the US State Department, we set
up a free phone number that anyone in Haiti gets in a text message to.
So the lines were overloaded for phone calls, but text messages were still getting through.
But it showed this problem where everyone in Haiti, almost everyone in Haiti, only spoke
Haitian Creole, a language that is not widely spoken.
But everyone come in into Haiti, only had English as a common language.
So I was tasked with finding people who could translate a text message sent in Haitian
Creole, categorize that, plot the location on a map based on the written location and
the message.
And so then you would have a structured English report with a longitude and latitude streamed
back to the men's seat responders.
So people like the US Coast Guard, who could go in and respond to those actual messages.
And that was my first experience with figure eight is something we launched in just 48 hours
and then ultimately found about 2,000 members of the Haitian diaspora worldwide from
across 49 different countries.
And they were the workforce.
So they were the ones who were reading the messages, translating them, categorizing.
Using their local knowledge to know where villages were, which villages would not appear
on any map, but they could at least know from the satellite view where they were.
And then they were able to do this for 80,000 messages.
So an incredibly large amount of data, like several novels with a data in more or less
real time in about four and a half minutes.
So that was a really important moment for me in realizing with distributed human computing
how quickly you can structure data and how you can do this across a really large number
of people.
And so it was certainly a positive experience for as much as it could be for the members
of the Haitian diaspora to be able to help their country from so far away.
Obviously, I really appreciated being able to get that scale of information to the disaster
responders on the ground as well.
And that's ended up also being something that I, like I mentioned, I then studied from
my PhD, looking at ways that we could extend that process with natural language processing
so that we could scale it to even large data sets, larger than even very large numbers
of humans I could process.
And that's where a lot of my work has been used inside the time.
It sounds like you have been involved in a number of, or were involved in a number of
different uses of figure eight and as you, as you described it, distributed human computing,
prior to actually joining the company, what were some of the others?
Yeah, so I use case that extends from from that one very closely is in epidemic tracking.
So just these outbreaks are still the largest large cause of mortality in the world.
And no one is really tracking them all.
So you see a lot of movies where there's a wall room and a big map of the world and
like a heat map it flares up every time there's an outbreak that only exists in the movies
unfortunately.
And I think that the budget for those movies might be bigger than the actual budget for tracking
epidemic globally.
Maybe the closest thing is the Google trends or Google predict.
I forget the specific thing but Google and blue trends, yeah, flu trends, right?
Is it still only flu or is it, do they, are they able to predict a broader set of things
now?
They were able to predict a broader set of things that, that group which was in Google
or at the time were actually one of the funders of the company that I was working in doing
epidemic tracking.
So we're within closely with them.
And this was also a problem which was very linguistic.
So like I mentioned, English only makes up about 5% of the world's conversations daily.
And most of the world's diseases come in the thin band of the tropics where you just
correlates with ecological diversity.
So most of the world's ecological diversity is in the tropics therefore most of the pathogens
come from there too.
That also happens to correlate with more than 90% of the world's linguistic diversity.
So the first time that a language is mentioned, it's really, really unlikely to be in English
or even any other dominant language.
And so we can go back in time and find examples of disease outbreaks well before they were
finally identified by virologists and epidemiologists as being new strains of a potential disease
outbreak.
Okay.
So for swine flu, there are open reports in a local spanish language newspaper written
in Mexico about months before it was identified as new virus.
In the case of bird flu coming out of areas just outside of Hong Kong, there were reports
weeks before it was identified as being a stranger of the flu.
And so this is another example using crowd flower to collect information about potential
disease outbreaks worldwide, you know, make sure they're real feverers, not Justin Bieber
feverers, and then filter from millions of reports across 15 different languages, use
a machine learning and a small number of human experts as well.
So that the turnout breaks each day that mattered, whether only reports seen by the virologists
out of the millies of potential reports out there.
I'm wondering if you're aware of any efforts to kind of bring together a network of, you
know, data scientists and machine learning AI experts to be able to help respond to emergencies
as they happen.
Like the, I'm not sure the specifics of how you got pulled into the, well, you got pulled
into the Haiti situation based on the company you were at after, yeah, I forget how you
say you got pulled into the, it was a connection at the U.S. State Department.
So at that time, I was doing work on a text messages between health workers in Africa
in the general language of Malawi.
And when it became clear that that text messages were the only formal communication, those
widely available in Haiti, I got dragged into it as really just as the only person working
in this field.
So unfortunately, no one has, as far as I know, tried to complete a PhD in text messages
and low resource languages and health and disaster response context since.
So I was, I remain the expert on that area, unfortunately, by being the only person who
has looked into that kind of research.
What's been very encouraging is that since that time, more than 10 years later, a lot of
large companies have been moving in the direction of providing better tooling and support.
So immediately before I joined, figure eight, I was running product for natural language
processing and translation at AWS, so emeralds, web services, Amazon AI.
And so as the product lead there, I was able to have a lot of influence and found a lot
of internal support for making sure that Amazon's first suite of native NLP technology on
AWS is language independent where they a clear roadmap to supporting as wide a variety
of languages as possible.
And I think that this is where we can see a lot of the biggest impact while I appreciated
my time working for the UN and with charity organizations.
I found that it's in many ways more important to make sure that you get this diversity in
people's everyday tools.
So the analogy I like to use is that if you go into an area after a disaster, the disaster
response community are all driving turtle land rivers, right, because a turtle is a well
known car.
It's been tested for millions of miles before you took it into a critical situation.
And there's plenty of people who know how to operate one or to repair one.
And I take a similar view with software.
And so software, which is specifically made for low resource health and disaster response
environments, I can tend to be buggy, not well supported.
And so while it's not as easy to immediately measure the impact, I think some of the greatest
ways to improve the world have been in making sure that the most popular cloud platform
AWS as a view to be language independent so that people can build out these tools in
a well known environment, along with other work that I've done both here at Figure 8 and
previously.
So for example, working with a lot of manufacturers of cell phones to ensure that they give
as much linguistic diversity as possible in their speech recognition systems.
I think a lot of people associate Figure 8 slash crowd flower with this idea of human and
loop.
And I think one of the themes that has been part of our conversation around machine learning
and AI is this idea that often people will put it as AI versus humans.
And there has been, I think a consistent and growing set of voices that it's not AI versus
humans or AI or humans is AI and humans.
How is your perspective on that evolve since doing this early work?
I imagine you get involved in a lot of customer activity at Figure 8.
What's your take on the importance of humans in delivering machine learning and AI products
and solutions and how do you see that evolving over time?
Yeah, I think one of the most important problems we're solving in technology right now is
working out what it looks like for humans and AI to work together on problems.
This is something that we're seeing across our client base, whether it's a self-driving
car company looking to get the right human feedback on the hours and hours of videos taken
from their cars collecting data right through to medical imaging, what's the right way for
a predictive service to identify, for example, breast cancer cells to either replace or advise
the doctor right through to the use cases that I've been looking at in text where your
letting humans make a decision based on large volumes of reports and the AI component
is prioritizing that for a person.
And so I think we're really just scratching the surface right now in the ways in which
AI and humans can work together, and there's a lot more excited work.
When you say more exciting work is that work to be identified or work that is being done
in different places that you're specifically aware of?
Yeah, I mean, we're doing a lot of exciting work right now with companies.
So for example, I imagine you're a self-driving car company and you're looking to identify
pedestrians on the street.
If you don't have any AI systems in an incredible training data, there's going to be a pretty
tedious experience.
So a person might have to draw a manually draw a bounding box around every single pedestrian
and every frame in hours and hours of video.
And that's incredibly tedious.
And most of that video is going to be empty, it's going to be people driving down freeways
and then highways.
And so there's at least four different ways where we're commonly introducing AI into
that human annotation process at the moment.
So the first is selecting what's interesting, so what's the car at the intersection and
having some highway driving, but not having that be 90% of what the car is learning.
And then with those bounding boxes around the pedestrians, to what extent can we pre-populate
using the machine learning model and have the humans' edits accept or reject those boxes.
And then automatically track those objects between frames against allowing humans to edit
except then reject.
And then finally, taking advantage of a lot of craft class quality control methods, how
can we give the same task to multiple people to make sure that they agree with each other
and errors don't propagate and then combine those different, maybe slightly overlapping
boxes in the most optimal way.
And so every piece of that step, selecting the right data, using predictive bounding
boxes rather than having somebody manually draw them, semi-automating the tracking in
videos and then combining in different human judgments into one final judgment for the
training data or four of those steps using machine learning in different ways.
So I think that's already incredibly exciting in terms of the ways that the humans and
machines are collaborating, but at the end of the day, that's just putting a box around
objects.
I think the kinds of interfaces that we can develop, not even at R&D stage yet.
I think this is going to be some of the most exciting advances at the intersection of human
computer interaction in AI in the coming decades.
And do you have any ideas around what those might look like or do you know places where
folks are working on that kind of thing that you can point us to?
You know what, there's no one really working on this anymore than we are right now.
We're going to this out with a lot of our customers and just starting to see some research
papers coming out of AI labs and human computer interaction labs, but it's really new.
There's not even conferences in academia dedicated to the intersection of human computer
interaction and artificial intelligence at the moment.
So this is really nice and I think incredibly important.
Yeah, I agree.
It's something that I've been kind of mentioning and asking about and for on the podcast
probably for over a year now.
I think there's a book that's kind of classic in the design, the design field, the design
of everyday things.
I forget the name of the author.
But we've built up a huge body of expertise and approaches, methodologies, senses for
what works and what doesn't around design minus intelligence, just kind of design of
stuff.
And it strikes me that designing with intelligence, designing for devices and systems that have
intelligence is its own field and we need to have people that are thinking about this
stuff and researching it and kind of pushing the frontiers of our knowledge about it.
I think so too and when you add in the complexity of people approaching technology on very different
devices in very different cultures, speaking very different languages, there is so much
out there that we are yet to even look at from an academic viewpoint that I can't wait
to see more people get involved with.
Yes, when you look at these, the four ways that you're looking to evolve the kind of
the human AI interface from a training perspective, can you talk a little bit about what some
of the most interesting technical challenges have been and how you've approached them?
Yeah, yeah, absolutely.
Some of them are things which we haven't seen at all before, which is somewhat surprising.
So for example, combining the different people's judgments, so taking those say three different
bounding boxes around a pedestrian which don't quite overlap with each other from three
different workers and making sure that that final bounding box which would then become
the training data is the correct one.
So it turns out there's no simple heuristic.
You can't take the average of those boxes.
You can't take the weighted average given the past accuracy of each of those workers on
the task, but you can make this a machine learning task in itself, where you have the
past accuracy of those people, you have their three boxes and you have the image itself
and you can give all of that to a machine learning algorithm.
And so a fun way that one of our scientists set up here was to make this its own layer.
So in a typical image, you have RGB, red, green and blue layers, because that's how the
image is encoded.
And then we treat the human, the various human identifications of boxes as additional layers.
And so this was I think a really neat solution to think about new channels of information.
Some human generated, some computer generated, and then allow in the machine learning algorithm
to find the right combination of information to produce the most accurate result.
Does it work at all to not think of these boxes as as bounding boxes per se, but more
like probability densities?
That's exactly what we do.
That's a good suggestion.
Yeah, we treat them as probability densities where those probabilities are taken from
their their past accuracy.
So with the figure eight platform, we have a lot of quality control measures such as
making sure that people pass quizzes before they can take a task, plus embed in the known
answers into tasks so that we can track someone's accuracy over time and remove them from
a task if they're not accurate.
And what this means is that we have fairly accurate probabilities given every single
person's past work, so someone might be 70 percent accurate, someone might be 95.
And so then each of those probably distributions can be represented.
And then the machine learning can figure out exactly what the right densities or thresholds
are to combine with the image data to produce that final result.
Interesting, interesting.
Anything else come to mind in terms of interesting challenges in this area?
Well, like I said, I think we're just scratching the surface.
And one of the things we lean on really heavily is transfer learning.
So when we're helping someone do predictive bounding boxes or create their final model,
we were able to do this by extending a model that's built on millions of previous images
that that figure it has access to.
And that gets a very high degree of accuracy, even when someone has relatively little training
data to begin with.
I think what's really interesting at the moment is that transfer learning has been incredibly
effective with images, but not so much with language.
So you can take a model learnt on everyday objects.
And then if you start applying that with transfer learning to medical imaging, you get a really
large head start in terms of accuracy from small amounts of training data.
And if you take things which seem almost identical, so you take sentiment analysis on Twitter
content in English, and then you try to apply that to sentiment analysis on your reviews
in English, then that transfer learning is giving you a couple of percent increase
in accuracy only, certainly not enough to make a big difference from a business standpoint.
And so what do you think that is?
Well, I think linguistically, it's just because language is so complex and exciting, the
ways in which we will talk about food for your review, which is really different to the
way we will talk about whatever it is people complaining about on Twitter.
And those stylistics are enough that it's not really the same signal at all.
Whereas with whether it's medical imaging or everyday objects all about satellite imagery,
a 2D object with just three layers of colours has more similarities.
There are 3D objects represented in two dimensions, there are colours, there are clear boundaries
between objects.
And so as a linguist, I find it fascinating, as a machine learning practitioner, I find
it frustrating.
I know a lot of smart people are working on this problem.
I think some of the early results in what's been called zero-shot machine learning, machine
translation, good early examples of the supplemental language.
So I'm hopeful that we can get to a point where we have these models of different language
tasks, which can make adapted new tasks much, much more efficient.
Yeah, this keeps bringing me back to the, I guess the earlier comment or a related comment
to the earlier one I made that it's, I think it's easy to look at some of the results we
see around, you know, machine translation and deep neural net, language modeling.
And I forget the person who made the quote, but there's a famous quote about, you know,
for every linguist I fire, every linguist I get rid of, my accuracy of my models increases,
talking about, you know, the advances we've made in statistical language modeling.
It's interesting, in this conversation here, you, you know, reinforce the importance of
the underlying, you know, linguistics and the language modeling and even within English,
you know, to the extent that even within English, you know, examples from, you know, one
type of communication, Twitter and another type of communication, Yelp reviews, you
know, produce such different results that you can't really transfer easily between the
two.
Yeah, and I think we're at Westside and increasing and the importance of linguistics.
I remember that adage, I think it was every time a linguist quits.
I'm not sure if it was them being fired, the original quote, or maybe it was.
And that's been an amazing change in terms of we really don't as much need someone with
that linguistic knowledge to manually say what the features are, like this is a word
boundary, you know, this is a proper noun, etc.
The deep learning models can take care of a lot of that for us.
And that's certainly also been a research focus for a while.
So in my own research, I was trying to abstract away from specifically linguistic knowledge
and resources so we could scale.
But I'm thinking about some use cases here at the moment where while we don't need the
linguists to extract the features, we get deep learning to that for us.
Linguistic intuitions about how to set up a task can really, really help.
So for one current use case, we're working with a large online retailer that also has
a in-home device.
And they have this problem where on their online store, there are titles, or often like
40 or 50 words long, because the people putting products up there assume that a 50 word
title is going to have better search engine optimization.
However, that's a terrible experience if you're speaking to a device in your home and
you have to listen to all 40 words and then that title to do audio shopping.
And so they need to need to shorten those titles.
And so making this purely a summarization task with sequence sequence models and deep
learning really isn't producing very good results.
The shortened titles and very natural that it look very good at all.
But a bunch is a little bit of linguistic knowledge.
We can see which are the important entities within the title.
We have the name of the brand, the size, the color, the function.
And treat this as a task where we identify those entities and then we can pile that short
title from those entities.
And so we were going from something about 50% of human level accuracy to now something
at 90% of human level accuracy, just because we had a smart linguist analyze the problem
and not have to hard code any features, but they just knew how to cast this problem as
a sequence sequence problem for identifying entities rather than a sequence sequence problem
for summarizing text.
So I think this is one example of where domain expertise is coming back into machine learning.
So a quick note, because I'm sure at least someone out there listening is going to be
wondering who about this quote.
The quote is from Frederick Jalenic, it's every time I fire a linguist, the performance
of the speech recognizer goes up.
So you are much more gracious than Frederick Jalenic in your treatment of the linguists.
But I think what's interesting here is the kind of going back to this broader question
of humans and AI, humans or AI, and human and the loop, there are multiple layers where
AI and humans interface, there's the user, and the AI is getting, in many cases, training
data from the user, the user has to use the AI in some cases, the user is configuring
things that have AI in them and those have peculiarities, and so there's kind of work
that needs to happen there about that user experience element.
There's the folks that are creating training data, and you've talked about some of the
work that you're doing in applying machine learning to that interface between the organization
or entity that's collecting training data and humans that are helping to produce that
training data, helping to label.
And then we've talked about just now, and certainly plenty of times on this podcast, the
role of domain expertise in, you know, broadly speaking data science teams that are building
out machine learning.
So those are kind of three places that humans are kind of in this loop.
Are there others that don't fall into those three?
So this, I'm numerating those three, so that would be the exit.
The end uses the experts and pretzels work is doing labeling of those of three.
Yeah, yeah.
I'm just wondering if you guys have a model for thinking about, or if you have a model
for thinking about kind of the roles, the varying roles that humans play in working with
AI, and does that model, you know, tell you anything interesting?
Yeah, I think we're seeing more and more tiered model of humans creating training data.
So historically, when Figuert was called Crowdflower, it was because initially most of the work
down the platform was CrowdSource.
And Crowdflower still runs at the largest marketplace for CrowdSource workers for training
data.
So we have hundreds of thousands of people and more than 150 countries working on the
platform.
However, that's becoming a smaller and smaller part.
So about a third of people using the Figuert software today are using the running
tunnel experts.
So they have domain expertise, whether that's in medical or financial domain, or simply
sensitive data that I don't want to farm out.
And while they're recognizing the importance of creating training data, they don't see
this as a problem that they want to outsource, at least not for the human component, certainly
they'll listen to our software still.
And then we also have a very large number of, but maybe another third of workforce who
are what we call an NDA crowd.
So these are people who work in a center.
They're often guaranteed an hourly salary, so they have job security.
And they're everywhere.
So that could be in New Orleans, or the Philippines, or in India, we just started to work with
the UN org, hoping to provide employment for refugees in Europe as well.
And the advantage of having groups of people in a room means that one, you know their identity,
they can sign an NDA and see more sensitive data, and also they can be trained up a lot
more.
So they have the ability to understand more complicated data concepts.
And so I find this to be really, really interesting.
So for example, there is a center employing only women in areas of India where people normally
don't get to take part in the information economy.
And they're doing a lot of the work for us for self-driving car companies, which means
that people whose other job opportunities would really only be in agriculture or in roadwork
have learned what every single street sign means across North America, Japan, Europe,
and can very quickly annotate all of those.
So I find it to be fascinating because I don't know how to do these tasks, but they're
able to take on this work, become domain experts, and then contribute to the training data
worldwide.
I think this is often very overlooked.
So I read recently that someone made an estimate that there were 10,000 AI professionals
worldwide.
And so last year we had 60,000 people from Venezuela were alone creating training data
on our platform and became very, really, really proficient at it.
And so I think elevating people who are creating training data, whether that's a crowdsourced
worker or an expert analyst within a company, is going to become something that we'll see
more in the future as people recognize the human input component.
And do you have a perspective on the kind of jobs and broader economic implications of
that?
Yeah, I think ultimately it'll find its way to the end users.
And a lot of the software will be making sure that they can get that implicit feedback
for the training data from the end user.
And so we've seen this already in search engines.
We do power a lot of search engines for various companies of crowdflower, but there are some
like very large commercial search engines that really don't need a lot of extra training
data because they get explicitly when you type something in a Google, whichever link
you click on gives them that feedback as to how they should optimize their search engine
for future similar search terms.
And so that's the piece that's more missing at the moment.
And this is where I see especially for areas like the medical domain.
So there's huge debates at the moment about whether AI are going to replace physicians.
And I can understand people worried about their job, but whenever I hear these arguments,
I'm like, well, before I moved here to the US, the village I lived in in Sierra Leone
had one doctor for every 100,000 people.
And a little more than a decade later, it's still as one doctor for every 100,000 people.
And that's going to be true in 10 years time.
That's not really going to change.
So it's economics of that nation I'm going to change that quickly.
So I would love to be in a world where physicians were a seeded in idle because AI had solved
healthcare.
But I really don't think that's going to happen.
What will happen is that that one physician will be able to scale what they are capable
of working on as they are assisted by AI.
And because they are going to encounter diseases or combinations of infections, which are
very specific to their environment, we want to make sure that we're capturing the data
in a safe but meaningful way at that point of care.
So that AI that they're using can become optimized to the local population that they're
serving.
Awesome.
I think we are about out of time.
Is there anything that you'd like to add to close us out?
Yeah.
So I would love to invite anyone who is in the Bay Area to our train AI conference, which
is in about one month.
We'll be willing to it.
And we have a fun keynote speaker there.
So Gary Casparov, probably the most famous person to fight AI in lose, we'll be talking
about his experience.
And I love how positive he's become.
So he is now one of the biggest advocates for chess that combines AI in humans, showing
that the best chess players are combinations of humans and machines, which will consistently
be both the best machines and the best humans even today.
So we're looking forward to hearing about his experience and then having a large number
of industry practices of machine learning, also sharing their experiences in combining
human and machine intelligence.
Awesome.
And I will be there as well.
And any listeners that are interested in attending can use the code TWIMAL to receive
30% off of registration.
All right.
Well, Rob, it was great chatting with you.
Thanks so much for taking the time.
Same was my pleasure.
Thanks for speaking to me today.
All right, everyone.
That's our show for today.
For more information on Rob or any of the topics covered in this episode, you'll find
the show notes at twimalai.com slash talk slash 125.
If you're new to the podcast and like what you hear or you're a veteran listener and haven't
already done so, please head on over to your podcast app of choice and leave us your most
gracious rating and review.
It helps new listeners find us, which helps us grow.
Thanks in advance.
Thanks again to Rob and to figure eight for sponsoring this show.
And of course, make sure you head on over to figure-8 dot com slash train dash AI to learn
more about the train AI conference and be sure to use code twimalai for 30% off of your
registration.
Thanks for listening and catch you next time.
