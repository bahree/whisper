1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,340
I'm your host Sam Charrington.

4
00:00:31,340 --> 00:00:36,960
In this episode, University of Edinburgh PhD student George Papa Mercarios and I discuss

5
00:00:36,960 --> 00:00:41,760
his paper, Masked Autoregressive Flow for Density Estimation.

6
00:00:41,760 --> 00:00:46,040
George walks us through the idea of Masked Autoregressive Flow, which uses neural networks

7
00:00:46,040 --> 00:00:51,000
to produce estimates of probability densities from a set of input examples.

8
00:00:51,000 --> 00:00:55,080
We discuss some of the related work that's laid the groundwork for his research, including

9
00:00:55,080 --> 00:01:00,440
inverse autoregressive flow, real MVP and masked auto encoders.

10
00:01:00,440 --> 00:01:04,440
We also look at some of the properties of probability density networks and discuss some

11
00:01:04,440 --> 00:01:08,440
of the challenges associated with this effort.

12
00:01:08,440 --> 00:01:13,640
This Friday, June 1st, we'll begin working through the fast AI practical deep learning

13
00:01:13,640 --> 00:01:15,280
for coders course.

14
00:01:15,280 --> 00:01:19,360
A bunch of you have already signed up and the Slack channel has been hosting some very

15
00:01:19,360 --> 00:01:21,440
lively discussion.

16
00:01:21,440 --> 00:01:25,920
If you'd like to learn deep learning with us, there are just three simple steps to join.

17
00:01:25,920 --> 00:01:31,960
First, sign up for the meetup at tumma.ai.com slash meetup, noting fast.ai in the what

18
00:01:31,960 --> 00:01:33,840
you hope to learn box.

19
00:01:33,840 --> 00:01:38,400
Second, use the email invitation you'll receive to join our Slack group.

20
00:01:38,400 --> 00:01:42,800
And third, once you're there, join the fast AI channel.

21
00:01:42,800 --> 00:01:46,600
I'll be publishing some additional information about how we'll work through the course and

22
00:01:46,600 --> 00:01:49,160
the next day or so, so stay tuned.

23
00:01:49,160 --> 00:01:54,120
If you have questions, feel free to shoot me a message on Twitter at Sam Charrington or

24
00:01:54,120 --> 00:01:57,280
via the contact form at tumma.ai.com.

25
00:01:57,280 --> 00:01:58,280
See you soon.

26
00:01:58,280 --> 00:02:03,360
Finally, we continue to celebrate the second anniversary of the podcast and we want to

27
00:02:03,360 --> 00:02:04,520
hear from you.

28
00:02:04,520 --> 00:02:06,200
Have you found this podcast useful?

29
00:02:06,200 --> 00:02:08,800
If so, we want to hear how.

30
00:02:08,800 --> 00:02:13,840
Post a comment over at tumma.ai.com slash 2av to let us know.

31
00:02:13,840 --> 00:02:20,200
And now on to the show.

32
00:02:20,200 --> 00:02:21,200
All right, everyone.

33
00:02:21,200 --> 00:02:24,920
I am here at Nips and I'm with George Papamacarios.

34
00:02:24,920 --> 00:02:27,440
Good to be here.

35
00:02:27,440 --> 00:02:33,080
George is a PhD student at the School of Informatics at the University of Edinburgh.

36
00:02:33,080 --> 00:02:39,960
And he's here at Nips, having just presented a paper on mast auto regressive flow for

37
00:02:39,960 --> 00:02:40,960
a density estimation.

38
00:02:40,960 --> 00:02:43,880
And I'm looking forward to learning more about what that's all about.

39
00:02:43,880 --> 00:02:45,400
George, welcome.

40
00:02:45,400 --> 00:02:46,400
Great to be here.

41
00:02:46,400 --> 00:02:47,400
Thanks for inviting.

42
00:02:47,400 --> 00:02:48,400
Absolutely.

43
00:02:48,400 --> 00:02:50,320
So, you're in the School of Informatics.

44
00:02:50,320 --> 00:02:55,680
How did you get involved in, you know, what's the intersection between Informatics and,

45
00:02:55,680 --> 00:02:59,880
you know, your research and how did you come to get interested in machine learning?

46
00:02:59,880 --> 00:03:03,280
Informatics is quite a general time.

47
00:03:03,280 --> 00:03:11,200
In Edinburgh, where I'm doing my PhD, the School of Informatics, essentially came to be

48
00:03:11,200 --> 00:03:17,760
by combining computer science, artificial intelligence, and cognitive science, and an

49
00:03:17,760 --> 00:03:23,040
umbrella term for everything that has to with information, either artificial information

50
00:03:23,040 --> 00:03:28,160
or, you know, natural information, came to be known as Informatics.

51
00:03:28,160 --> 00:03:31,520
So my background, though, was in, actually, was in electrical engineering.

52
00:03:31,520 --> 00:03:35,480
So I did my undergraduate studies in electrical engineering.

53
00:03:35,480 --> 00:03:41,720
And then I, after I graduated, I was working as a research assistant doing computer vision.

54
00:03:41,720 --> 00:03:45,560
And I was actually working in a project where it was a fun project.

55
00:03:45,560 --> 00:03:51,280
We were trying to detect activities and classify activities.

56
00:03:51,280 --> 00:03:53,440
So we had, in smart homes.

57
00:03:53,440 --> 00:03:59,440
So we had, we put sensors and cameras in, you know, people's flats.

58
00:03:59,440 --> 00:04:05,280
And we could, you know, take measurements such as temperature and humidity, and we could

59
00:04:05,280 --> 00:04:07,560
track the person's location in the flat.

60
00:04:07,560 --> 00:04:11,360
And we wanted to infer what they were doing.

61
00:04:11,360 --> 00:04:16,440
Were they sleeping, watching television, doing the dishes, eating, et cetera.

62
00:04:16,440 --> 00:04:18,080
And that was a fun project.

63
00:04:18,080 --> 00:04:24,320
But it became obvious to me that it was very difficult to, you know, to figure out what

64
00:04:24,320 --> 00:04:26,760
the person was doing and being certain about it.

65
00:04:26,760 --> 00:04:27,760
Right.

66
00:04:27,760 --> 00:04:29,920
Because they could be doing anything, right?

67
00:04:29,920 --> 00:04:33,240
Does it write in temperature mean, for instance?

68
00:04:33,240 --> 00:04:40,720
And so that's how I got into machine learning and tried to infer information from data in

69
00:04:40,720 --> 00:04:46,160
a more principled way, and trying to quantify the uncertainty of that information.

70
00:04:46,160 --> 00:04:50,640
For instance, a person might be doing, you know, might be doing the dishes, but I was

71
00:04:50,640 --> 00:04:52,680
sure that they're doing the dishes.

72
00:04:52,680 --> 00:04:53,680
Or maybe I'll be cooking.

73
00:04:53,680 --> 00:04:54,680
Or maybe they're cooking.

74
00:04:54,680 --> 00:04:58,400
Or have to be, we have to be able to quantify uncertainty.

75
00:04:58,400 --> 00:05:02,800
That's how I got into machine learning and in particular, based in inference.

76
00:05:02,800 --> 00:05:06,560
And that's what brought me to Edinburgh to a PhD.

77
00:05:06,560 --> 00:05:10,680
How is that interest kind of flowed into your current line of research?

78
00:05:10,680 --> 00:05:19,880
So my, so I got interested in based in inference, which is a principled way of making predictions

79
00:05:19,880 --> 00:05:23,000
at the same time, quantifying uncertainty.

80
00:05:23,000 --> 00:05:28,160
And this has all to do with, you know, probability distributions and probability densities.

81
00:05:28,160 --> 00:05:32,680
And, you know, a lot of probabilistic modeling.

82
00:05:32,680 --> 00:05:36,400
And it's difficult to get these probabilities.

83
00:05:36,400 --> 00:05:42,760
We have to, how do we, how do we obtain the probability of data in the first place?

84
00:05:42,760 --> 00:05:50,160
And that's what brought me into doing research in density estimation, which is what my,

85
00:05:50,160 --> 00:05:56,080
which is what the research I'm presenting here at NIPS is about.

86
00:05:56,080 --> 00:05:59,960
And here we're talking about probability densities as opposed to material.

87
00:05:59,960 --> 00:06:00,960
That's correct.

88
00:06:00,960 --> 00:06:02,040
That's correct.

89
00:06:02,040 --> 00:06:09,240
And so the technique you're using to estimate probability densities is mass, autoregressive

90
00:06:09,240 --> 00:06:10,240
flow.

91
00:06:10,240 --> 00:06:11,240
What does that mean?

92
00:06:11,240 --> 00:06:12,240
That's correct.

93
00:06:12,240 --> 00:06:15,520
So, mass autoregressive flow is, it's a deep neural network.

94
00:06:15,520 --> 00:06:16,520
That's why it is.

95
00:06:16,520 --> 00:06:23,200
So, it's a deep neural network, which takes data point, could be an image, could be a

96
00:06:23,200 --> 00:06:30,160
time series, could be a piece of information, and spits out a number.

97
00:06:30,160 --> 00:06:35,440
And that number is the probability density associated with this data point.

98
00:06:35,440 --> 00:06:40,640
Essentially, it's a score that says how likely is this data point.

99
00:06:40,640 --> 00:06:46,120
For instance, if I'm trying to learn the probability density of our images, an actual

100
00:06:46,120 --> 00:06:51,360
image, like say an image of a person, I would score high so that the network would return

101
00:06:51,360 --> 00:06:53,240
a high score.

102
00:06:53,240 --> 00:06:59,320
And a random image, an image of random noise would return almost zero score.

103
00:06:59,320 --> 00:07:05,600
That tells you how plausible, how likely the image is, so that's what probability density

104
00:07:05,600 --> 00:07:06,600
is.

105
00:07:06,600 --> 00:07:14,240
And you can learn, you can train such a network directly on data and get an estimate

106
00:07:14,240 --> 00:07:18,240
of what their probability density should be.

107
00:07:18,240 --> 00:07:30,320
And so, the probability density that the network is producing is a vector of, what does it

108
00:07:30,320 --> 00:07:31,320
look like?

109
00:07:31,320 --> 00:07:36,960
I guess, when I think of probability densities, I think of them in terms of, from an analytical

110
00:07:36,960 --> 00:07:42,600
sense, like, is it, you know, it could be a Gaussian with a certain set of parameters

111
00:07:42,600 --> 00:07:46,720
or some other kind of probability function.

112
00:07:46,720 --> 00:07:54,840
Is this telling you the distribution as well as its parameters or is it telling you something

113
00:07:54,840 --> 00:07:55,840
else?

114
00:07:55,840 --> 00:08:01,640
Yeah, there are many ways to go about doing this.

115
00:08:01,640 --> 00:08:07,480
So it could be that the neural network spits out the parameters of a distribution.

116
00:08:07,480 --> 00:08:12,760
For example, if that distribution were a Gaussian, it would spit out the mean and the variance

117
00:08:12,760 --> 00:08:15,800
that Gaussian, which defines that Gaussian.

118
00:08:15,800 --> 00:08:22,800
There are many ways of doing this, the challenge here, so if you chose, let's say, a particular

119
00:08:22,800 --> 00:08:29,080
type of distribution, say, a Gaussian, that network would only be able to spit out

120
00:08:29,080 --> 00:08:30,080
Gaussian's.

121
00:08:30,080 --> 00:08:31,080
Right.

122
00:08:31,080 --> 00:08:38,080
The challenge here is to make it more flexible, because the data might follow a different

123
00:08:38,080 --> 00:08:39,280
distribution, right?

124
00:08:39,280 --> 00:08:45,560
So how can we make it flexible enough so that it can spit out whatever distribution, the

125
00:08:45,560 --> 00:08:48,000
data happens to follow?

126
00:08:48,000 --> 00:08:52,480
And that's the challenge here, and that's what this paper was about, essentially, was

127
00:08:52,480 --> 00:09:00,000
proposing a very flexible mechanism, so a particular architecture that would spit out

128
00:09:00,000 --> 00:09:05,440
parameters that would describe a distribution that was very flexible.

129
00:09:05,440 --> 00:09:11,720
Is it picking from some catalog of distributions that it knows about, or is there a type of

130
00:09:11,720 --> 00:09:19,800
distribution that is essentially high-dimensional, very flexible, lots of parameters that every

131
00:09:19,800 --> 00:09:25,920
other distribution is kind of a degenerate version of, if you will?

132
00:09:25,920 --> 00:09:28,560
It's a bit like, well, it's more like the second one.

133
00:09:28,560 --> 00:09:37,280
So it is parametric, and it is described by a set of parameters, but these parameters

134
00:09:37,280 --> 00:09:39,640
can adapt themselves in what ways.

135
00:09:39,640 --> 00:09:42,400
So they can change, so you can imagine.

136
00:09:42,400 --> 00:09:46,080
They can change in value, or they can change in meaning in a patient.

137
00:09:46,080 --> 00:09:47,080
In value.

138
00:09:47,080 --> 00:09:53,240
So you can imagine a distribution being sort of like a rubber sheet that you put over data,

139
00:09:53,240 --> 00:09:58,520
and you can stretch that and squash it in many ways, and that's what the parameters

140
00:09:58,520 --> 00:09:59,520
describe.

141
00:09:59,520 --> 00:10:06,360
The stretching is the squashing of this flexible object, and by transforming this object,

142
00:10:06,360 --> 00:10:11,040
you can form it into a distribution that fits the data well.

143
00:10:11,040 --> 00:10:12,040
So that's the idea.

144
00:10:12,040 --> 00:10:20,360
This parameters describe how I would need to stretch and bend a simple distribution like

145
00:10:20,360 --> 00:10:23,760
a Gaussian in order to get a complicated distribution.

146
00:10:23,760 --> 00:10:25,520
That's essentially the idea.

147
00:10:25,520 --> 00:10:28,400
What's this distribution called?

148
00:10:28,400 --> 00:10:33,640
This distribution is the model, is mass-totary-gressive flow.

149
00:10:33,640 --> 00:10:42,000
So as part of this research, you identified or created this distribution, but also demonstrated

150
00:10:42,000 --> 00:10:44,480
how to calculate it from a data set.

151
00:10:44,480 --> 00:10:50,240
So mass-totary-gressive flow is essentially this neural network that squashes and stretches

152
00:10:50,240 --> 00:10:55,800
a simple distribution to form it into a more complicated distribution.

153
00:10:55,800 --> 00:10:57,880
It's free to learn what transformation to do.

154
00:10:57,880 --> 00:11:01,880
So you don't know what the end distribution is going to look like.

155
00:11:01,880 --> 00:11:10,200
But you know that the more simple distribution, the starting distribution, you know that.

156
00:11:10,200 --> 00:11:15,880
Just so I'm clear on this, is mass-totary-gressive flow your creation or is that an existing thing

157
00:11:15,880 --> 00:11:17,560
that you then applied to density estimate?

158
00:11:17,560 --> 00:11:24,840
This is my creation, which was inspired by very similar models in the literature.

159
00:11:24,840 --> 00:11:29,480
So there was a model before that, which was very similar, which is called inverse autoregressive

160
00:11:29,480 --> 00:11:36,040
flow, another model, which is called real MVP, and another model, which is called massed

161
00:11:36,040 --> 00:11:38,360
autoencoder for distribution estimation.

162
00:11:38,360 --> 00:11:44,280
So all these are, you know, in the same family of models.

163
00:11:44,280 --> 00:11:52,120
And this one is essentially a continuation of the same idea and an application of previous

164
00:11:52,120 --> 00:11:54,840
ideas in the problem of density estimation.

165
00:11:54,840 --> 00:12:00,440
And the paper, did you, did you base your results on a particular input distribution,

166
00:12:00,440 --> 00:12:04,680
such as a Gaussian, or did you do something else?

167
00:12:04,680 --> 00:12:12,280
The paper did use Gaussian's as, you know, as base distribution, but that's fine because

168
00:12:12,280 --> 00:12:17,080
the idea of these models is that they're not limited by what a distribution you start

169
00:12:17,080 --> 00:12:23,880
with, because if you, you know, form it, if you transform it in many ways, you can in the

170
00:12:23,880 --> 00:12:27,160
end get almost any distribution you like.

171
00:12:27,160 --> 00:12:33,640
So in the paper, what, what we did is we took various data sets, some of which were images,

172
00:12:33,640 --> 00:12:40,360
like MNIST or CIFAR, others were time series, and there were two more data sets that

173
00:12:40,360 --> 00:12:47,800
described collisions in particle physics. So these data sets are all publicly available.

174
00:12:48,440 --> 00:12:56,280
And we trained the same model, so same, same neural networks, same deep model to each one of them,

175
00:12:56,280 --> 00:13:03,880
and saw that it works well across a range of, of domains. So it's not particularly engineered

176
00:13:03,880 --> 00:13:09,720
to work well with images or anything. So one, one of the motivations in the paper was to

177
00:13:09,720 --> 00:13:11,480
demonstrate generality as well.

178
00:13:11,480 --> 00:13:14,120
And what was your metric of working well?

179
00:13:14,120 --> 00:13:22,680
Test log likelihood. So having trained this deep neural network on training data and having,

180
00:13:23,560 --> 00:13:28,840
you know, tried to estimate the probability density in training data,

181
00:13:29,640 --> 00:13:38,920
does that probability density assign high scores to held out data? So this is the metric.

182
00:13:38,920 --> 00:13:44,760
So if we, let's say, have a data set of images, and we know they should all have high scores

183
00:13:44,760 --> 00:13:51,560
because they're all plausible images. And I, I don't use like 10% of them for training,

184
00:13:52,760 --> 00:13:58,920
then the model I learn should assign high scores to these images that I held out as well.

185
00:13:58,920 --> 00:14:01,080
And let me take a step back.

186
00:14:01,080 --> 00:14:05,560
In this case, the, what does it mean to have a score for a given?

187
00:14:05,560 --> 00:14:10,520
It's the, the score is the, the value of the probability density.

188
00:14:10,520 --> 00:14:14,280
The value of the problem, the value of the probability density where?

189
00:14:15,160 --> 00:14:15,880
Or for what?

190
00:14:15,880 --> 00:14:23,640
At the, for that particular data point. So the probability density assigns a value to each data point.

191
00:14:23,640 --> 00:14:29,480
And these values should be high for, for, for data points that are typical,

192
00:14:29,480 --> 00:14:36,680
natural looking, if, if we're talking about images, and very small, unlikely objects.

193
00:14:36,680 --> 00:14:42,760
So in the case of your test data, you've got an image, a complete image. And when you talk about

194
00:14:42,760 --> 00:14:49,160
the score, are we looking at like an arbitrary pixel and the, the probability value for,

195
00:14:49,160 --> 00:14:54,520
we're looking at the whole image, the whole image. And so kind of an, an average score across

196
00:14:54,520 --> 00:15:00,360
all of the pixels or kind of, yeah, kind of, kind of each pixel would get an individual score.

197
00:15:01,240 --> 00:15:07,240
And the, the, the product of the scores would be the score of an image.

198
00:15:08,120 --> 00:15:11,880
And so how's this, how would you envision this being applied?

199
00:15:11,880 --> 00:15:16,680
That's a very good question. So, so what, what I described is one

200
00:15:17,480 --> 00:15:21,240
way to do unsupervised learning, right? So in unsupervised learning,

201
00:15:21,240 --> 00:15:27,160
what you have is, what you have data, like unlabeled data, like a bunch of images,

202
00:15:27,800 --> 00:15:32,120
that you downloaded from, from the web. Yeah. And you want to know something about the data.

203
00:15:32,120 --> 00:15:38,440
You don't have a specific, you know, goal in mind. You, you want to learn something about

204
00:15:38,440 --> 00:15:45,480
the structure of the data. So one way of doing this is by density estimation, because the density,

205
00:15:45,480 --> 00:15:51,960
which the probability density that describes the data probabilistically tells you that structure.

206
00:15:51,960 --> 00:15:56,040
So it's one way of doing generative modeling and unsupervised learning.

207
00:15:57,240 --> 00:16:03,400
For example, GANs and VAEs would be similar ways, but slightly different approaches.

208
00:16:05,000 --> 00:16:10,520
But what I'm particularly interested in and the reason I got involved with density estimation

209
00:16:10,520 --> 00:16:17,960
to begin with is for doing inference, as I said at the beginning. The probabilistic way of doing

210
00:16:17,960 --> 00:16:25,800
inference is, you know, recovering distributions of unknown information. So if we want to,

211
00:16:25,800 --> 00:16:30,680
for example, in the, in the examples I was, I was talking about in the beginning,

212
00:16:32,440 --> 00:16:37,960
where you want to infer what activity a person is doing. Mathematically, what you want to recover

213
00:16:37,960 --> 00:16:45,960
is a distribution over possible activities. And you could use these type of models. So these density

214
00:16:45,960 --> 00:16:52,920
estimators, these neural networks that learn densities in order to learn densities over things

215
00:16:52,920 --> 00:16:58,440
that you, that you don't know that you want to recover. So this is, this is my motivation of

216
00:16:58,440 --> 00:17:05,800
using neural density estimation to do probabilistic inference. So I'm thinking of applications that,

217
00:17:05,800 --> 00:17:13,400
you know, we see a lot of, for example, GANs apply to, you've got incomplete images and you're

218
00:17:13,400 --> 00:17:18,440
trying to reconstruct those images in some kind of way and you want to infer some of the missing data.

219
00:17:18,440 --> 00:17:24,200
That's an example of how you might apply this. That would exactly be, yeah. So for example,

220
00:17:24,200 --> 00:17:29,560
you've, you've seen half of the image and you want to know, you want to infer what the other half

221
00:17:29,560 --> 00:17:34,920
would be. So what you want is a distribution or a density. Right. Over the pixels that you haven't

222
00:17:34,920 --> 00:17:44,040
seen, for instance, and that could be done potentially with this kind of models. So the, the density

223
00:17:44,040 --> 00:17:49,960
that you, this, the mass-auto-regressive flow distribution, how many, does that have a fixed

224
00:17:49,960 --> 00:17:54,680
number of parameters or a variable number? It has a fixed number of parameters. This is a

225
00:17:54,680 --> 00:18:00,760
design choice. So it's, it's, it's a neural network, it's a deep neural network, which

226
00:18:00,760 --> 00:18:09,880
essentially implements this transformation of the base density. And the parameters of that neural

227
00:18:09,880 --> 00:18:16,280
network is the parameters of density. So then maybe not parameters in the way we're used to

228
00:18:16,280 --> 00:18:22,040
thinking about them for probability densities, but you know, they are hyperparameters for a neural

229
00:18:22,040 --> 00:18:28,440
network. They are, they are the, the weights and the biases of that neural network. It's, it's

230
00:18:28,440 --> 00:18:33,880
difficult to, to interpret them. Yeah, that was my question. Like, is there like an intuitive,

231
00:18:33,880 --> 00:18:40,440
you know, transformation between those and what we might think of as the, you know, the parameters

232
00:18:40,440 --> 00:18:46,120
of a probability distribution. Yeah. Sounds like no, not really. It's, yeah, yeah. So if we,

233
00:18:46,120 --> 00:18:50,600
let's say if the probability distribution is a Gaussian, then we can interpret the parameters.

234
00:18:50,600 --> 00:18:55,800
So the mean is, you know, the descent multiplication. Right. Yeah, standard deviation is the width.

235
00:18:55,800 --> 00:19:00,920
Right. But now it's difficult to interpret. What, what, what are the parameters of the neural

236
00:19:00,920 --> 00:19:08,360
network that, you know, describes this density do? Yeah, you're right. And so is, is it even correct

237
00:19:08,360 --> 00:19:15,800
to think of this, these parameters as describing a density as opposed to describing a machine that

238
00:19:15,800 --> 00:19:21,080
spits out densities? It's, it's, it's, it's how you think it's, it's, it's, it's, it's, it's mostly

239
00:19:21,080 --> 00:19:29,080
semantics. Yeah. And you can think of them either way. Okay. I think. And so what's the

240
00:19:30,200 --> 00:19:36,920
dimensionality of this parameter space? It's, it's huge. It's, it's, it's, however big you design

241
00:19:36,920 --> 00:19:43,640
your neural network to be. So it's, think of it as, it's, it's, it's a modification over a standard

242
00:19:43,640 --> 00:19:51,000
fit forward neural network. And the parameters are the weights and the biases of this neural network.

243
00:19:51,000 --> 00:19:58,200
So you could have potentially thousands or millions of parameters. And so what, what characterizes

244
00:19:58,200 --> 00:20:09,480
the architecture of this network relative to others? So, um, if I, let's say I take an arbitrary

245
00:20:09,480 --> 00:20:16,280
fit forward neural network, like standard, you know, standard stuff that we use, for example,

246
00:20:16,280 --> 00:20:23,400
for regression. And what we want, if, if we want this network to represent, if we want the output

247
00:20:23,400 --> 00:20:28,360
of this network to represent a probability density, then it has to have some properties.

248
00:20:29,080 --> 00:20:34,120
The first property is that it has to be non negative. And the second property, which,

249
00:20:34,120 --> 00:20:40,920
which is a difficult one, is that it has to, the, or the set of all outputs, the set of all

250
00:20:41,640 --> 00:20:48,920
possible probabilities have to sum to one. And this, these two properties are not, not trivial

251
00:20:50,280 --> 00:20:57,800
to ensure. And that's exactly what the research is about. How, how can we design the architecture

252
00:20:57,800 --> 00:21:02,040
of this neural network? So that's kind of your starting point. Yeah. And you design from there.

253
00:21:02,040 --> 00:21:05,800
Yeah. There are other requirements like, uh, that the outputs have to be continuous. Like,

254
00:21:05,800 --> 00:21:11,480
is that of interest or is that trivial? They will be continuous. And they will be

255
00:21:11,480 --> 00:21:16,600
differentiable because otherwise we wouldn't be able to learn the neural network. So given that,

256
00:21:17,480 --> 00:21:23,160
the, the functions represented by neural networks are differentiable. Yeah. Then, yeah,

257
00:21:23,160 --> 00:21:27,640
this is a requirement for being able to learn it. Right. Well, by learning, I mean, you know,

258
00:21:27,640 --> 00:21:32,840
by training. Yeah. Yeah. So like, what was your, what was your process in

259
00:21:32,840 --> 00:21:39,320
then architecting a network to meet these criteria? There is a, uh, there is a line of research.

260
00:21:40,280 --> 00:21:48,760
For instance, the, uh, the methods I, I mentioned before. So essentially, we, we continued from,

261
00:21:49,400 --> 00:21:55,320
you know, rely on the shoulders of giants. Sure. People have, they've been there before you.

262
00:21:55,320 --> 00:22:03,960
Um, so there were, in the literature, there were two types, um, of models. The first one

263
00:22:05,160 --> 00:22:10,760
is called autoregressive model. And the second one, the second one is called a normalizing flow.

264
00:22:11,880 --> 00:22:19,320
Um, and these were, these are ideas of how to design the neural networks to satisfy these

265
00:22:19,320 --> 00:22:25,480
requirements. So what are these individual ideas mean or represent? Yeah. So the, the, the

266
00:22:25,480 --> 00:22:34,520
autoregressive, um, model is essentially a, it's a neural net that has as many outputs as inputs.

267
00:22:35,640 --> 00:22:41,880
And each output is, uh, the parameters of a simple density of the corresponding input.

268
00:22:41,880 --> 00:22:50,200
So for instance, the, the first input would be a mean and a variance of a Gaussian for the,

269
00:22:50,200 --> 00:22:54,040
uh, so the first output would be a mean and a variance of a Gaussian for the first input.

270
00:22:54,040 --> 00:22:59,000
And the second output would be a mean and a variance of a Gaussian for the second input.

271
00:22:59,560 --> 00:23:06,680
So they essentially break down the problem in estimating simple, simple densities for each

272
00:23:06,680 --> 00:23:13,480
variable for, for each input. If they, if the input were an image, then that would be individual

273
00:23:13,480 --> 00:23:20,440
densities over each pixel. So they break down the problem, the problem of estimating,

274
00:23:20,440 --> 00:23:26,760
enjoying density of all inputs into the simpler problem of estimating a density of one,

275
00:23:26,760 --> 00:23:32,600
of only one input at a time. So that's, that's the idea. Do you keep the parameters at each

276
00:23:32,600 --> 00:23:37,480
level, or are you only keeping ones at the end? Or you keep, uh, keep all of them, right? Yeah,

277
00:23:37,480 --> 00:23:42,200
that's your, yeah, so your information comes from. Exactly. Okay. Got it. Um, yeah. So the collection

278
00:23:42,200 --> 00:23:47,880
of all of them, right, describes your density. Okay. So that's auto-aggressive models. And the,

279
00:23:47,880 --> 00:23:56,520
and, uh, there are many examples. So wave units is, is a model like this. Okay. Pixel RNN is a

280
00:23:56,520 --> 00:24:02,680
model like this. Pixel RNN. So these, these, these, these models are great learning complex

281
00:24:02,680 --> 00:24:09,960
distributions over images or sound or, uh, other types of data as well. Um, so that's one, one

282
00:24:09,960 --> 00:24:16,920
approach. The other approach is the normalizing flow approach. This is essentially what I was

283
00:24:16,920 --> 00:24:23,880
describing before. These, these, these take, these, these neural networks take a, a random

284
00:24:23,880 --> 00:24:30,040
input coming from a simple distribution like a Gaussian, and they transform it and produce a

285
00:24:30,040 --> 00:24:37,560
data point. So essentially, they implement what I was describing before they take a simple density,

286
00:24:37,560 --> 00:24:44,760
for instance, a Gaussian, and transforming it, stretching it, like a rubber band, you know,

287
00:24:44,760 --> 00:24:54,920
in order to form a different density. Um, so real and vaping was this type of model. So our work

288
00:24:54,920 --> 00:25:01,560
is, was based on the idea that we can actually, before you, before you go to your, the, this

289
00:25:01,560 --> 00:25:11,560
latter model normalizing flow is a normalizing flow. Are you, is your, your input data just sampling

290
00:25:11,560 --> 00:25:17,000
from a input distribution or is there some kind of training data set or something? It's the random

291
00:25:17,000 --> 00:25:22,520
noise from an input distribution. It's like the generator of a gun. So it's just random noise.

292
00:25:23,560 --> 00:25:36,760
Um, and the output is data. Okay. Um, and so now work, what, what we, the idea, um, that we use

293
00:25:36,760 --> 00:25:43,160
is that you can actually, it turns out that some autoregressive models are actually normalizing

294
00:25:43,160 --> 00:25:48,520
flows. So there's a deep connection between this two type of model, which was really cool.

295
00:25:49,480 --> 00:25:56,920
And that gave us an, an idea of how to make normalizing flows better. And the idea was to use

296
00:25:57,480 --> 00:26:02,680
autoregressive models as normalizing flows. So that was, and that, that's exactly what we

297
00:26:02,680 --> 00:26:07,320
called this thing, an autoregressive flow. And so when you, you started with this idea of making

298
00:26:07,320 --> 00:26:12,520
them better, what was, what were the particular things that you wanted to improve on? So, um,

299
00:26:13,640 --> 00:26:20,360
the, the challenge with designing, uh, neural density estimators, neural networks that,

300
00:26:20,360 --> 00:26:28,520
estimate densities is exactly that it's difficult to ensure that the outputs are actual densities,

301
00:26:28,520 --> 00:26:34,920
that they satisfy this, this, this properties of summing to one and being non-negative. Um,

302
00:26:36,280 --> 00:26:42,040
and if you try to impose restrictions on the architecture in order to achieve that,

303
00:26:42,040 --> 00:26:48,920
you end up hurting expressivity. So you end up making the networks very rigid, um,

304
00:26:49,720 --> 00:26:54,680
very difficult to express complicated distributions. That's why we're trying to improve, we're

305
00:26:54,680 --> 00:27:02,760
trying to improve, uh, the expressivity of these type of models without hurting, um, without,

306
00:27:03,320 --> 00:27:08,040
violating, violating, well, exactly with, uh, violation of the requirements. And when you hurt

307
00:27:08,040 --> 00:27:15,240
expressivity, um, does that lead to a model that tends to underfit, a model that doesn't generalize?

308
00:27:15,240 --> 00:27:21,480
Exactly. What are, what are all of the above? Yeah. Uh, uh, underfitting could happen because

309
00:27:21,480 --> 00:27:26,440
the model is not expressive enough. Right. To represent a complicated density. So, for instance,

310
00:27:26,440 --> 00:27:33,640
a complicated density can have a, you know, a very strange shape in space, right? If, if the,

311
00:27:34,680 --> 00:27:40,360
if the model can only represent gousians, then it will underfit, it will,

312
00:27:40,840 --> 00:27:46,840
right, you know, uh, force fit everything. Exactly. Yeah. So you will try to sort of cover

313
00:27:46,840 --> 00:27:53,560
everything, in the space, and it will end up, um, assigning lots of probability to

314
00:27:54,360 --> 00:28:00,360
unlikely, uh, objects, like unlikely images, for instance. Yeah, that's what underfitting means

315
00:28:00,360 --> 00:28:06,040
in this, in this context. Are there other areas of your paper or presentation that we haven't

316
00:28:06,040 --> 00:28:12,520
covered thus far? That's well, that was pretty much, uh, everything. And so this work is, uh,

317
00:28:12,520 --> 00:28:17,800
uh, kind of a stepping stone and a sequence of stepping stones, you know, where do you see it going?

318
00:28:18,520 --> 00:28:23,960
Um, I'm hoping that it will, it will prove useful in, uh, doing inference.

319
00:28:25,160 --> 00:28:31,240
Uh, and they're, there are many ways you can use it for doing inference. So last year we,

320
00:28:31,880 --> 00:28:37,560
say, last year nips, we had another paper where we used a neural density estimator

321
00:28:37,560 --> 00:28:45,640
to learn, uh, posterior distributions. So that would be one way we can use it to, to do inference.

322
00:28:46,600 --> 00:28:52,600
What's the difference between learning a density estimator and learning in the posterior?

323
00:28:52,600 --> 00:28:58,920
The posterior is a density. Uh, it's a particular, uh, particular density that describes

324
00:28:59,640 --> 00:29:05,240
your beliefs about something you haven't seen. For instance, if you have an image and you've

325
00:29:05,240 --> 00:29:14,280
observed, you have half of it and the other half is corrupted. Then the density of the corrupted

326
00:29:14,920 --> 00:29:23,000
bit of the image would be the posterior of that, uh, part of the image given the, the part you've

327
00:29:23,000 --> 00:29:29,400
seen. Uh, so that, that would be a posterior. Um, but we use that same example to describe or to

328
00:29:29,400 --> 00:29:35,960
illustrate the work that you've done. Uh, so, uh, but it sounded like you were drawing a distinction

329
00:29:35,960 --> 00:29:42,440
between the work that you done and the work that you did, uh, with this paper and the previous one

330
00:29:42,440 --> 00:29:47,240
that was focused on, uh, uh, past, past areas. What's the distinction between the two?

331
00:29:48,200 --> 00:29:54,360
So the distinction is that the previous paper was, was mainly on how to train it for inference,

332
00:29:54,360 --> 00:30:00,440
how to train, it's a given and you're a density estimator. How can you train it, uh, to learn

333
00:30:00,440 --> 00:30:06,680
posterior? Uh, and this one was, how can we design a good density estimator? Got it, got it, got it,

334
00:30:06,680 --> 00:30:12,920
got it. So if we have a good one, we have a good density estimator and we also have a good way

335
00:30:12,920 --> 00:30:18,600
of training, training it to learn posterior. Right. Maybe we can, then we can actually use the

336
00:30:18,600 --> 00:30:25,320
reference. Yeah, we can use the reference. Exactly. So that's the idea. Okay. Um, how did you benchmark

337
00:30:25,320 --> 00:30:33,880
the performance of the, uh, the, uh, the mast auto regressive flow model? Uh, the, the benchmarking

338
00:30:33,880 --> 00:30:41,000
was, was done on general purpose density estimation tasks. So it was, what I was describing before,

339
00:30:41,000 --> 00:30:48,280
we took with these three different tasks. Yeah. We took a number of data sets. And so how well,

340
00:30:48,280 --> 00:30:54,280
how well mast auto regressive flow can fit the density of, of the data set. The density of, of

341
00:30:54,280 --> 00:31:01,000
data is something we don't know, right? It's, we know that, you know, we, we stipulate that, uh,

342
00:31:01,720 --> 00:31:09,400
that data comes from, from a density, um, but we don't know which, what, what, what it is,

343
00:31:09,960 --> 00:31:16,360
and we try to learn it by, but by these type of models by mast auto regressive flow. And if it has

344
00:31:16,360 --> 00:31:22,920
done a good job at learning the underlying density of the data, then it should assign high density

345
00:31:22,920 --> 00:31:30,760
to unseen data. And that, that's the, uh, that's, that's a, uh, benchmark, uh, we can use to test these

346
00:31:30,760 --> 00:31:37,320
models. Uh, and so presumably you compare the results you saw with the, the previous generations of,

347
00:31:37,320 --> 00:31:44,040
yeah, exactly, perform favorably. Yeah. Um, competitive. Okay. It's, it's not clear what's the best

348
00:31:44,040 --> 00:31:50,360
model for, so given a data set, right? It's not clear what the best model is because depending on

349
00:31:50,360 --> 00:31:57,240
the structure, some, some density models are better at, um, estimating cluster structure, structure,

350
00:31:57,960 --> 00:32:06,760
uh, other, um, models are better at estimating manifold structure. And some models work better in

351
00:32:06,760 --> 00:32:13,640
high dimensions, some models work better in low dimensions. So it's not easy. It's, it's, it's difficult to

352
00:32:13,640 --> 00:32:21,320
design a, a model that rules them all, right? That fits everything. Um, so there is, there's always, um,

353
00:32:22,280 --> 00:32:29,160
the question, what do I use for my data set? Uh, we, we saw that mast auto regressive load does well

354
00:32:29,800 --> 00:32:35,080
across a range of data sets, but it's not necessarily the best for each particular case of steel.

355
00:32:35,080 --> 00:32:41,640
If, if I'm interested in a particular data set, then I, I need to think, um, what I should use.

356
00:32:41,640 --> 00:32:46,600
If I'm interested in a range of data sets, then something like mascot or aggressive flow,

357
00:32:46,600 --> 00:32:52,440
as a starting point is something I would probably use. And did we explicitly cover what the

358
00:32:52,440 --> 00:32:59,880
mast in mast auto regressive load, uh, where that comes from? Um, so this comes from a previous

359
00:32:59,880 --> 00:33:08,040
model that we used ideas from. So that previous model was called a mast auto encoder for distribution

360
00:33:08,040 --> 00:33:16,600
estimation. So we took the mast bit from that. And so that model was an auto regressive model.

361
00:33:16,600 --> 00:33:24,840
So it predicted one density at a time. So one, one density. So it had one output for every input.

362
00:33:24,840 --> 00:33:33,080
And that output was a density over the corresponding input. So the, the, the restriction that these

363
00:33:33,080 --> 00:33:43,240
models have is that the, so the, the fifth output is only allowed to be connected to the first four.

364
00:33:43,240 --> 00:33:55,160
It's not allowed to receive, um, input from the fifth, the sixth, the seventh, uh, input. And the,

365
00:33:55,160 --> 00:34:03,400
in, in that, uh, in that particular paper, they, they enforced this restriction by masking out

366
00:34:04,200 --> 00:34:11,000
weights, connections in the network by removing, uh, connections. And that forces kind of like a

367
00:34:11,000 --> 00:34:16,840
summarization at different levels or, uh, it forces, it forces the concentration or something.

368
00:34:17,720 --> 00:34:27,640
It forces, uh, that the fifth output doesn't change by looking, so by looking at the, at the

369
00:34:27,640 --> 00:34:35,560
actual value of the fifth input. Um, that, that's, that's what, that's what it does. And it's,

370
00:34:35,560 --> 00:34:40,760
in, in technical terms, what it does, it, it, it decomposes a, a joint density into a product of

371
00:34:40,760 --> 00:34:49,400
conditionals, um, each, uh, output density is one of those conditionals. And conditionals are,

372
00:34:50,120 --> 00:34:59,720
um, conditioned on only previously seen inputs and not on future ones. Um, so masking was the,

373
00:34:59,720 --> 00:35:05,880
the technique they used for, for doing the sense, it's a very clever idea. And we, we took this idea

374
00:35:05,880 --> 00:35:12,040
from there. Awesome. Well, George, thanks so much for, uh, taking some time, uh, to chat with me

375
00:35:12,040 --> 00:35:16,360
about this. I really enjoyed learning about what you're up to. It was a pleasure for, uh, for me.

376
00:35:16,360 --> 00:35:23,560
Great. Thank you. Thanks very much. All right, everyone. That's our show for today.

377
00:35:23,560 --> 00:35:28,280
For more information on George or any of the topics covered in this episode,

378
00:35:28,280 --> 00:35:36,120
head on over to twimmolai.com slash talk slash 145. Also, take a moment to show us some love for

379
00:35:36,120 --> 00:35:42,920
the podcast's second anniversary and share how it's been helpful to you over at twimmolai.com slash

380
00:35:42,920 --> 00:35:58,840
2av. And finally, thanks so much for listening and catch you next time.

