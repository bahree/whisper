Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.
All right, everyone. I am here with Ruman Childry. Ruman is a managing director and global
lead of responsible AI at Accenture. Ruman, welcome to the Twimal AI Podcast. Thank you for having me,
Sam. This has been a few years in the making and I'm glad we were able to do this. You know what?
It only takes a global pandemic to make this conversation happen. These are the best conversations,
I think, when I'm finally able to connect with friends and folks I know from the industry and
you and I in particular, I think I've been trying to make this conversation happen for as you said
a few years and it's always, oh, well, I'm going to be in Asia. I'm not in the Bay Area.
We haven't quite been able to make it happen. So I'm super, super, super excited.
We get this one going. Let's start out as we usually do here on the show and have you share
a little bit about your background? You work in ethical and responsible AI, how did you come into
that field? I do. And the answer to that is a lot of meandering. So by background, I'm a data
scientist and a social scientist. I would officially say I make quantitative social scientists.
I have degrees in political science, management, economics, masters and quantitative methods.
Is that one degree or like five or six? Oh, I know folks like you.
But I moved to Silicon Valley in 2013 to pursue a job in this like weird little field called
data science, which I had heard about anecdotally while at my PhD program at UCSD.
Everyone thought it was crazy. Nobody understood why I was leaving a political science PhD program
to do some weird tech job. But here we are seven years later with data science and anything related
to data science and AI being the only thing people talk about. So after my stint as a data scientist,
I was actually teaching data science at a bootcamp called Metis. And that's one of such a family.
I was doing talks on polling and the elections and in the sense of how numbers and statistics
can be misleading because I have a background in things like survey design, polling and
quantity of human behavior analysis. And you know, Accenture was this is about three years ago
looking for someone to lead this weird thing called responsibility. And that's how I got this job.
Oh, wow. And is the Metis the thing you're doing with Laura?
No, that was something else. Okay. Okay. Cool.
Interesting. Interesting. And so you've been at Accenture how long now?
Three years actually hit the three year mark in early February.
Wow. Nice. Nice. Which in the responsible air world makes me ancient?
Absolutely ancient. And you're based in San Francisco. How have things been going for you
with shelter in place and COVID and all that kind of stuff?
Yeah. I mean, fortunately San Francisco had a really good response and people, you know,
stayed at home and they more or less have been listening. I think everyone's just getting
a little bit antsy. So I see more and more people out. Although people are still being careful.
Fortunately, it's been pretty quiet. I live in Mission Bay, which is near the UCSF
hospitals. And, you know, it hasn't been that bad.
Unfortunately, I live in a really walkable neighborhood. There's parks nearby, etc. So it hasn't
been overly unpleasant. I just think this is also the shortest, as far as the longest
amount of time I've ever spent not flying somewhere in the last few years. So it's kind of been nice.
Yeah, I've commented to, in fact, just earlier today, like by this time on a normal year, I'd
have been probably to half a dozen at least conferences, you know, if not, if not a dozen.
And you probably would have been around the world a couple of times, but I mean, it's funny
because I have all these placeholders on my calendar and one by one, they all got dropped. But,
you know, by now, I would have been in London twice, India and the Nordic. Actually, this week,
I was supposed to be doing it toward different Nordic countries to visit different
Accenture offices and client partners. And then in a month, I was supposed to be in Atlanta.
I still have this thing in Singapore that apparently is still on the calendar for August,
but I think they're being ambitious at this point. But, you know, it's nice to be home.
You know, it's nice to be around my pets and organize my apartment and do all those things
that help me talk it to do.
Nice. So what are you up to at their Accenture, you know, in Responsible AI? What does that mean?
What does Accenture's role in Responsible AI? And how do you help fulfill that?
Yeah, so I have a really interesting and still unique job, although I hope that there will be
more jobs that are like mine. My job is to actually provide and create practical client solutions
around responsible and ethical use of artificial intelligence. And this can mean anything from
data ethics to the unpacking of the black box. There's a lot of people call it,
to just understandability in models, even to like the strategic organizational structure companies.
And how do you create a government's infrastructure? And it's been really fascinating,
because as I said, like I got this job through meandering, but what's fascinating is in this job,
I use every degree I've ever had, every part of my brain. So, you know, while I do have to tap
into my data science skills and think about model explainability, interpretability,
different ways of doing data assessments, etc. I tap into my social science brain all the time.
And I think about human behavior and human responses and how to construct something so that,
you know, we're getting accurate data or we're creating policies that are inclusive or understandable.
But then also I go to companies and we help them redesign their organizational infrastructure
to create the right kind of scaffolding to enable responsible AI. And actually,
I've been working with some folks, and we have a paper coming up quite soon. We did a workshop
at FACT, online in this FACT store. I've been working with a researcher of mine, Bob Donna
Marcova, Penny Wayne at Paymark, who's Spotify Labs and Jeanine Gang.
Probably a huge fan of her yet. I haven't seen her forever.
Because she's super busy and even pandemic.
She's another conversation just like this. So, maybe I should reach out to her and say,
you, you know, you're not pandemic-busier or something.
We're sort of not pandemic-busier, but for sure, similar to me, she's on planes less. So,
that's good. But yeah, so part of my job is the technical deployment. And there's a part of it
that's also about the organizational structure and the strategic deployment. Because a big part
of the public simply understanding what you're doing with AI is to improve how we communicate
what this technology can and cannot do. Because there's a lot of hype, there's a lot of noise,
and frankly, there's a lot of backlash to the hype. There's even ethics hype now, frankly.
And so, I apologize. I got excited when you said, Henry, what was the paper about?
You're like, who paper? What? Henry yet? No, it's fine. So, the paper was really interesting,
because we're at this phase in responsible AI or ethics where there are principles that
there are so many principles were drowning in principles, right? Algorithm Watch keeps
this database. I think right now they have 150 plus principles of AI and ethics and organizations
at the OUCD down to companies like Telefonica or even Accenture. And when you say 150 principles,
are these 150 kind of published frameworks by some group or company or 150? If you want to be
ethical, you need to do things one through 100. I got it. No, so 150 different sets of principles,
so different organizations that have come up. But to point, interestingly, there have been a few
papers trying to understand what are common themes across principles. So, like a meta analysis.
So, Anna Jobin has won the channel Floridian Josh Calls of another. And there are some themes
that are pretty common. I can't remember them all. I'll talk my head with stuff like Malmalfezan,
Non-Malfezans, and you know, there's like five or six common themes that Floridian Calls find.
So, it's interesting. So, to your point, everyone's talking about it, but there are common themes
to it in general. And the big thing now is how do we drive principles into action? How do we do
stuff with this? And our paper was about how do you enable the right sort of organizational
structure? So, drawing from, like I said, I use every word of my brain, drawing from like
management literature, thinking through organizational analysis, organizational structure,
how do we draw from those principles to understand the kinds of shifts companies need to make,
to enable the people they've hired to institute responsible AI. So, we did
our long surveys with 24 different people across 18 different companies. And we specifically
focused on people like myself who are there for the application, not just the research. And,
you know, I found a lot of really, really interesting things.
Interesting. Yeah, in fact, the last time I reached out to Henriette was an advance of
our conference, two moles conference in the fall. And we did a panel on operationalizing AI ethics.
So, you know, for organizations that are, you know, trying to implement AI and do it responsibly,
you know, what are some of the things that they could do. And, you know, we got into this long
back and forth email conversation about how it's all evolving. Yeah, it's, and that's actually what
the paper is about. It's about how we're in this critical inflection point where there's this,
so there's this whole organizational organizational literature about external and internal pressures
and organizations that enable ethical change. And right now we have a significant amount of
external pressure. And it's all just like, so it just takes time, right, and just have to do the
things. And it took a while for the community to evolve for people to make principles, people
to start talking about them, you know, the media engagement, public awareness. And now it's
reached this point where companies are getting this pressure. And we also have internal champions
and companies trying to drive this change. So, you know, how can we utilize this internal and
external, this external pressure to enable the internal champions to do their job and what can
companies do to help them? But yeah, absolutely, it's all, you know, so we look at the current state
which is kind of where things are today. The prevalent state, which is, you know,
an immediate future where things like need to go. And then this ideal future state, like where
would people love things to be in the future? What is this ideal state of a responsible company?
Interesting, interesting. For a while, I would hear about organizations that we're taking this
position that, you know, we're just not going to pursue AI because it's too ethically fraught
and we don't know what to do. Do you see that at all? Or had you seen that? Is it still something
that you come across? Well, I have definitely seen that sentiment when it comes to individual projects.
I think every company is really excited about the promise of AI. And I don't think anybody is
turning down, you know, the whole concept in general. I mean, frankly, you just will be a market leader.
Yeah, you have a chance. However, yes, I have seen hesitation when it came to implementing,
when it comes to implementing certain projects. And it's definitely a blocker in companies
scaling AI. One thing we find in general as a company, as Accenture, you know, there are many
blockers to scaling artificial intelligence. Companies are drowning in proofing concepts.
Anybody can spin up a proofing concept based on a nice database. They find some sort of online
data, do some fancy neural net on it. You can have data science spin something up in three weeks.
But productionizing it, scaling it is a whole other endeavor.
Yeah. And so, part of, you've kind of talked about all these different parts of your brain that
come into play when trying to help organizations think through this. The most organizations
already have the kind of pieces in place to build, you know, to build ethics into the way they
operationalize machine learning. Like, if you're in, you know, financial services, for example,
you know, you've had to deal with, you know, making loan decisions ethically and, you know,
things like that. So, you probably have some kind of organizational, some governance structure
in place. To what extent did most, you know, or how prevalent is that? And what are the things,
even if you have that, that you need to do to make it make sense in the context of ML and AI?
Yeah, that's a really great question. And the answer is it just varies a lot. And some of it is
the culture of the organization or the company and some of it is just the nature of the industry
that it's in. I would say by my rule of thumb for, you know, thinking through which companies are,
you know, would be the most successful at enabling responsibility, at least at this point.
One would be understanding AI output as probabilistic and not deterministic. So,
understanding the math and statistics behind it, rather than thinking of it as a magical
computational outcome, just helps someone in my position a lot because a lot of the bias and
unfairness from artificial intelligence or biases from the fact that, you know, if you understand
that the that the output is a likelihood and on a certainty, then you can appreciate bias in,
you know, in a almost like a technical sense, the way data scientists think about bias.
And then you can appreciate how bias can enter in a system and think of, you know, how to remove
that bias. Second would be some sort of adoption of, you know, use of AI in your organization
already. It's difficult if you're, again, like you think this is like some sort of magical
technology. The third, interestingly, is either develop legal functions or regulated
industry and not just because of external pressures like regulators, but companies that are in
industry that are highly regulated have legal functions that are already data delivery, sort of
building that infrastructure. So, if I were to talk to a company, you know, in a less regulated
industry, their lawyers are usually more like contract lawyers or, you know, certain types of
maybe liquidigation lawyers, but they haven't had to actually work with ethics and compliance
necessarily and not necessarily in that developed with the sense. Although, I really do think
this notion of risk functions and the value of something like a chief risk officer is going to
change quite a bit and that role will be increasingly valuable, you know, as we adopt more
official intelligence. So, to your point about financial services, they're my favorite customers.
In the sense that like, I mean, they're ready to, they get it also. So, marginal amount of bias
if because I am a statistician also by background, like, you know, I think a lot of anecdotally,
I'll say a lot of statisticians who work at these financial services organizations got a little
bit marginalized by the rise of data science because people did not, because again, it was sort of
sold as this magical computer thing, not a bunch of math. And the statisticians are like,
no, we've been doing this for a long time and I wholeheartedly agree they had been doing this for a
long time. So, they understand models, they understand how to assess them, they understand that
accuracy isn't the only thing to look at when you look at models, you know, in this notion of
testing, it's all like built, they get all of it. But also, the industry itself has this culture
of ethics that comes about close to 2008 financial crisis and this really interesting document
called SR-117 and it was written by the Federal Reserve and on that birthday in 2011.
And if you were to read that document, you would be like, it reads as if like someone like
you or me wrote this today about issues with models and need for ethical use, irresponsible use
of data and appropriate use of models. It's fascinating because they were thinking about this 10
years ago and there's a lot you can learn and certainly artificial intelligence introduces
new challenges, but the building blocks are there. And it's probably the most robust in financial
services and any roles. Yeah, I think when I think about ethics and one of
the challenges of, you know, just trying to address it, you know, organizationally, it's,
I guess I can characterize it as like you've got this one side, it's like idealistic and
it's another side that's kind of very practical and I'm wondering, you know, do we lose something
when we kind of hand over this concept of ethics to lawyers and risk management people?
Um, I think that there is an evolution of the risk function that's probably going to happen.
And you're right. When I think about this work, I try to broaden the phrase to be about risk and
impact. And I do agree there's a cynical take on risk, which is more about, you know, how do we,
and it gets close to the line as possible. Yeah, or also like yeah, risk is not liability.
Yeah, but I will say working with a lot of folks who are in this field, I think there is this
desire. So, you know, going back to financial services, a lot of this, a lot of what exists
today as model risk management started from this conference, this gathering called Basil on
his Basil 1, 2, 3, like these different documents. And it's not just about like here's how you
audit a model. It's actually about how do you make an ethical company? They thought about things like
compensation for employees being linked to performance, which would then create incentives for
them to either misrepresented lie about what's going on, right? And not to say it's been perfect.
There have been some notable disasters since then. But the intent is there. Maybe this is,
you know, a good motivator for the people who are truly dedicated. But you're right. I like to
expand the language to move beyond risk to impact. I think impact is more proactive, I think,
impact things more broadly. And impact also isn't focused on like how do we shift the liability
for me to someone else? Because theoretically in a pure risk function, I can just say like well,
my organization's risk is X. And if that person's absorbing the risk, then my risk is lowered.
It doesn't mean that people didn't get harmed, it just means I'm not being sued, right? That's
a very, that's a more cynical thing. But then bringing the language of impact broadens it to me
and like, no, you are actually socially responsible to, you know, the environment at large, whether
it's the market, whether it's society or your customers. So if you're an organization that is
kind of down the path of exploring machine learning, maybe you have, you know, one or
any of these prototypes that you've mentioned and you're starting to operationalize ML technically.
And you're listening to this interview and you're like, oh, ethics, I need to, I need to get
me some of that. You know, maybe, you know, first of all, kind of what do you do there? But also,
you know, I think you mentioned in like thinking about these problems, you kind of, you're using
all these different kind of parts of your brain, your management part of your brain, your social
sciences part of your brain, your technical part of your brain. You know, when I think about,
you know, kind of me, I've got a little bit of management part of my brain from kind of, you know,
industry, I've got a technical part of my brain. But like when I was in school, I went to an
engineering school, RPI, go engineers. I've took like maybe two social science classes in the
whole time. So like, I don't have that, you know, that kind of formal training to draw on. And
there are a lot of people in industry that don't. So how do you, you know, what is the path
look like to start to understand, you know, maybe kind of encapsulate the room on social sciences,
you know, brain or what have you? You can really kind of, you know, if not rigorously,
thoughtfully, think through these kinds of issues and put a structure in place so that you can
do it repeatedly and start to scale it. So two thoughts. One is the first talk there for
at Accenture. I actually still give it today and it's called, what do we talk about when we talk
about bias? And what I realized is that when data scientists were communicating with non-data
scientists, there was this like lost and translation moment about some of the most basic terms,
like bias is one. And to your point about, you know, going to a purely technical school where you
didn't really take social science classes, when I was teaching at Metas, I realized that my
students that came from pure STEM backgrounds had no concept that data could be biased because,
you know, for them, data represented an objective truth. And I had to, and it's something that
you sort of rationalized. You never see that argument on Twitter today. No, never happens.
No, never. But it's interesting because it's sort of anecdotally people get it, but then,
you know, and I understand like if you're a computer scientist or a mathematician, you've always
optimized two A dataset, right? And explaining to them that the collection of data can be flawed
was just like this interesting aha moment. And that that's where some of my early work on this
stuff comes in. And it's still like, you know, I told you, both at top three, over three years
ago at this point, I still give it today. And still people are like, oh, oh yeah, actually,
that's true. So how, also, hat size, my dog.
Okay. Pets are unavoidable in the pen. Pets and pets and children are unavoidable in the pandemic.
I've had too many calls with some of this child that really ran it. It's very cute. It's always very
cute. Just earlier today, I was watching the Microsoft Bill technical keynote and Scott Hanselman,
do you know, Scott? He was giving a, you know, long time kind of blogger inspiration, kind of champion
of diversity and tech, all this stuff, Scott out to shout out to Scott. But he was doing his keynote
in front of who knows how many, you know, virtual tens of thousands of people. And his kid sneaks
into the back of his office. I love it. I love all of it.
He's still some toy. I love it. It's so humanizing. It's like, look, life happens. It's fine.
We all work. We're not, like, we're not automatons. Like, you know, we have pets. We have children.
We have lives for humans. I think it's great. But back to your question, you know, I think that there,
so if I were, if I could reconstruct the world, I think data science, much like quantitative
finance, frankly, should actually have an arm called critical data science that there should be
people trained in data science fields in the art of critiquing data science models. I think it
is way too complex, frankly, for an individual to just do as an add-on to their everyday project.
And also, it's hard to have that level of objectivity when you're auditing your own work, right?
Because everybody likes to think they did a good job and maybe didn't, maybe didn't. And, you know,
there are things you would miss. So a large part of like what people are saying when they say,
how do we institute this? A lot of folks call for things like red teams,
drawing from the way security works. Well, in order to do that, you kind of need these,
this is a third party of people, whether it's another organization or whether it's people with
any organization, but they need to have this like specialized skill set of being able to like
assess models for real business and all sorts of ways, right? Whether it's how the data was collected
to like literally the parameters of the model and even to like where you will be implementing
and who it will be used on, whether it is well suited for those individuals, right? So that
would be my like if I could change the world, I would add something called critical data science
as an actual field of study to then do the science. But for today, you know, I think that people
sometimes forget that quantitative social sciences, scientists exist and that's literally what we've
done, like our whole lives. And it wasn't just saying when I first moved here, like I got a lot of
slack from not being a programmer. And I just did not, like I always like to say I'm not born of tech,
like I was not built and created here. I was 33 when I moved here, so I wasn't like this young,
green kid like learning about how you know places should be. And I'm like, I haven't been so
obsessed with programming. And it's obviously a great skill to have, but for me, it was one of
many skills. And I so I think it was more valuable than other skills and the sort of weird
tearing of what's more or less valuable is very odd to me. And then, you know, like it was interesting
because it's, you know, you certainly get a lot of flak with being a social science scientist
in this world. But you know, now it's interesting because all of my quantum social science
skills come into play. So, you know, I think one is to bring in more social scientists on your
team, you would be amazed and surprised that our level of statistical and quantitative skill
and abilities and our programming skills, we can do all of it. But you know, to be fair,
there's plenty of things engineers do that I can't do, right? And, but the whole goal is to make
this an interdisciplinary group. I actually don't think anyone here for body can do this. I certainly
personally cannot do all of it, it's supposed to be my job kind of at a high level, right?
The more you dig into it, the more you realize how there's a role for everyone to play.
So, thinking through, you know, the study that I did with like a Henrietta and Dreaming and Bobby,
that's actually the answer. It's like the entire organization is responsible and everybody has
their job to do. And I can appreciate how it's extremely daunting task if you are a data scientist
on a project, you know, there's always this literature with data scientists as if they're gods,
as if like, you know, and I remember my first job as a data scientist, like, you're not a god,
you're responding to someone else's demands. You got to meet deadlines, right? I mean,
it actually takes a pretty brave person to say, you know, we need to put this project on hold
because this data is wrong or incomplete or to go to your project manager and say, like, you know,
this product is not built ethically. And we need to create the right sort of incentives and
community structures to do so. It's very, you know, like, I suppose high-level answer to your question,
sorry, I don't have an easy answer to it. I think it's fine that, you know, people are, I think
it's totally fine that people are offering ethics, curricula, etc. I think it's certainly needed,
but is it in software problem? No, it's not. And I think that's a great point. And, you know,
one of the biggest things that's changed in data science over the past, you know, three, five or so
years is kind of this move away from thinking of the data scientists as this kind of, you know,
lone ninja that kind of roms the night. It was never that short. It was never that. It's
all like a weird 10% engineer thing. Like, who is that? I don't know, like, honestly, I don't know
data scientists who are like that. I don't know who these people are. I've certainly never worked
with one and, you know, and if they existed, nobody ever liked them and they were actually never
very good. I remember these pictures that you that we used to see with like the data scientists
and like all of the skills in their backpacks. We remember what I'm talking about.
My favorite used to be these job descriptions. And you can still find them. And it'll be like,
you know, computer science degree PhD preferred 10 years plus engineering knowledge of like
like all of it. And I'm like, this person doesn't exist. And like up on them, they'll also say
something like, you know, back in 2015, I would say like 10, like six to 10 years of experience
in data science. I'm like, that term didn't exist 10 years ago. Okay. Go back to like Yahoo. And
go find the like OG data scientists. Maybe one of them is qualified. Yeah. Yeah. But yeah,
but to your point, actually, at my first job, I one of the their first data science hire was
actually somebody from Yahoo, who was one of their original data scientists. And like he certainly
wasn't that way. He was a really, he was a great teacher, a wonderful person to work with.
He taught me a lot about how to production wise code. That was one of the things I didn't know how
to do. I mean, I knew how to like assess models and build it in like this closed environment,
but how do you take it and, you know, make it ready for an engineering team, though these
were skills I didn't have. And and such it was really helpful and like a wonderful person to work
with. I have no idea what that weird comes from. Interesting, but I think
by and large organizations with few exceptions have kind of moved away from that towards
more of a team approach. StitchFix comes to mind as a counter example where they are very much,
you know, they still look for a full stack data scientist they call them. But in general,
you know, I tend to see more of kind of a team approach that has specialists. And I think to
your point, you know, ethics or, you know, computational social science thinking or everyone
I call it is a, you know, it's a complimentary skill that belongs on that team as opposed to,
you know, we have to, you know, make everyone social science ninjas in addition to being
technical ninjas and deployment. It's actually, it's like, so I guess the, is it one of those
things where the field just had to mature, right? And the analogy that I always think of is like
remember the title live master from the 90s. Like, okay, like try to tell anybody born after the
year 2000 that once upon a time, companies would hire one person to manage their website. The whole
thing. This included graphics design. And now it's not only is it a team, it is a team of people
who are like graphics designers, like, you know, and pure creative folks, two engineers and programmers,
two somebody who's just developing content and really good at writing copy and messaging,
you know, and it's a, and I think that over time, I mean, it took a while to get there and that,
that's where it went because it was just logical. You know, our web presence became just as important,
if not sometimes more so than a physical presence for any given company. If you wanted to do it
right, you had to invest in people. Guess what? With interdisciplinary skill sets and you couldn't
just write, you know, this, this job description of someone who knew HTML, CSS, and, you know,
you know, do you know a color palette? You know, just don't make this website and windings.
I'm perfect. Comic Sans and Winding. Oh, man.
Nice, nice. So there's kind of beyond kind of the ethics, you know, should we shouldn't we,
you know, not to oversimplify ethics. I guess I'm, I want to transition to kind of explainability
and the extent to which that comes up in your work and as a concern for the people that you
are working with and assuming so, you know, what are you seeing as kind of the ways that folks
are addressing it? Yeah, it's interesting. So the notions of both explainability and transparency
come about in part because of a lot of the legislation from the EU, so General Data Protection
Regulation. And it's been interesting to see how people interpret it. So like,
other talk that I give is about both a notion of explainability and transparency and explain
the idea of explaining the concept of what it means to respond is really interesting, right?
So again, like, talking into this other part of my brain. This is where I draw on like political
philosophy and, you know, governance and democracy, right? So, you know, we want to create all
this governments around AI. And we want to like explain things, right? But often the explainability
is this notion that I'm just going to tell you things and somehow you're supposed to understand
what I'm saying. So the idea would be like this panoptic, like, teacher and electoral kind of thing.
Like, I'm standing in front of a room. I'm putting up this lecture. If you don't get it, it's your fault
and you got to figure it out. And the, so sometimes like our notion of explainability because we
translate from data science to, you know, other fields, whether it's a customer service representative
or a loan officer or a judge, right? These are other people who have no interest or skills or
abilities in our field, same way we don't have any interest colorability in their fields, right?
So what are we at? How are we explaining things? And, you know, as it was a more concrete
technical example, I use is that the end user license agreement. So, you know, we've all gone
through the update our OS and then we get this like long legalized document, which none of us read.
And even if we sat down and read it, we would not understand it because it's written by lawyers
or lawyers. Right. So fully explained, completely explained. That is all understood. And often
I think about explainability, I think about the notion of understanding and how, you know, if you're
a good teacher, you're really focused on whether or not you students have understood what you've
said, right? And even if it means explaining it again or spending it differently or, you know,
taking more effort to understand your audience rather than kind of just saying it the way you would
say it. So that would be sort of the government's component of it. And even this notion of transparency,
right? So similarly, transparency assumes that if I have a pro-transparent process or a
transparent model, that it's great. That's it. I'm done. But then there's this assumption there
that I can do something about it. And we're not, we're, you know, all systems are inherently
have a power dynamic. So if a massive tech corporation just says, by the way, we've changed our
models to be like X, what are you or I or the average person's vehicle? What would you be able to do
nothing? Right? Absolutely nothing. And this came up quite a bit thinking about like image, facial
recognition, image detection. One pushback I would get from people, you know, on the use of facial
recognition in stores, et cetera, to maybe that identify a shop lifter. Then that weird behavioral,
you know, behave, what does it quite be? A effective computing stuff. I just mean like straight up,
like identifying someone from a video. And they would say, well, what's the difference between
that and having a security guard? And I'm like, well, the difference is if there's a security guard
and they unfairly targeted me, I can ask who's your, like, I want to talk to your manager,
I can call the company, I can actually take action. But if it is a image recognition system and
it incorrectly maps my face to somebody else who is dealing, I actually have no agency,
I have no form of redress. So I can have full transparency. I know there was facial recognition,
I know it was used, but I actually don't have any agency. So this critical missing part of
transparency as an agency. But from like a technical perspective, I think that's not a really
cool stuff going on. Like I really love, you know, a lot of the, the way adversarial models are
being used to understand model explainability. There are a lot of the, the mimic models, right,
where you have this like student teacher model. I think there's a lot of really good stuff going
on there. And I'm super excited to see more of it being used in production. And like again,
going back to this notion of cooking concept versus production, it's actually really hard to move
some of the more advanced models into a way what big corporation or even like a medium size company
could use it. Like they're conceptually really interesting, but you know, just like how long it
takes to run one of those assessments is sometimes a deal breaker. To run the model or an assessment
of the model because of its like of transparency. The complexity of an explainability model.
So some, so for example, like some of the models on kind of factual fairness would have to
iterate across every single potential scenario that could happen with the data in order to compute
what would happen if someone's gender was which are male to female, right? Those things take
a while. And maybe it works if you have like 10 variables, 15 variables, it's really hard if you
have like 300 variables. What, you know, what other interesting things are you seeing happening in
the field that will kind of, you know, are most likely to impact your customers, your clients.
You know, I think there's a lot of conversations happening in the field that kind of range in their
practicality and pragmatism. And I'm curious about the more pragmatic side of that. Yeah, me too.
What, like, so like I said, I started the shop three years ago. I used to have a slide on
every single one of my decks, like every single one I swear because I just got so tired of it.
I would start every talk by saying there are three things I don't talk about. And my three
things were Terminator Hal and Silicon Valley entrepreneurs saving the world. Because at that time,
we had a closer view of, oh, you know, but I said every, because I swear to God, if someone sucked me
into the Hal Conversation, it was quite into lose my mind. All right.
And so is that, is that to say that, you know, just kind of putting, putting pause on the previous
question, is that to say that you don't get involved in like, you know, do your customers even
care about like AI safety kinds of questions and like paperclip maximizing kinds of questions?
Yeah, I mean, like maybe for like intellectual curiosity, sure. But, you know,
and paperclip reference for those that are not following is a reference to Peter Bostrom. And I'll
we'll drop an analogy that he gives and we'll drop a link to my podcast with Peter in the show notes.
Oh, cool. Is it a podcast? And that's really awesome. Yeah. I'm pretty sure we talked about the paperclip
thing too. I would be surprised if you didn't. But yeah, I mean, I mean, I think yes,
our intellectual curiosity sure. But I don't think, I think we may actually slowly be moving
into a scarier world than we were before, particularly with some of the uses of AI and human resources,
which is interesting because it's not just like super sexy fields and oh, yeah, you know,
autonomous vehicles, sort of that the first field where we're actually adjusting some of these
like more existential ethical concerns has been in HR because of the rise of effective computing
and this concept that you can somehow measure someone's potential by their face or by their
expressions or by their mannerisms. And also, you know, and it's you know, in retrospect,
it's actually not surprising because in when we hire somebody, it is such a
nebulous and difficult to quantify factor that we hire people on, right? Like, sometimes it's
like ability, frankly, right? We like to think it's based on and even if we're being really
vigorous about it, it's often based on potential and you may have a different interpretation
of someone's potential than I do and then and it's it's like almost an inherently a biased process.
So once we, but yet it is a world in which there's clearly a need for some sort of automation,
whether or just in sheer amount of volume that companies have to deal with or, you know,
mitigating the bias that already exists. So it's interesting and kind of a difficult problem.
And some of the answers have brought in new, weird ethical existential problems. But in general,
thank goodness. All right, well, and I say that because not that I'm not interested in the philosophical
conversations, but sometimes it tracks from the it is a way of avoiding the actual problems that
exist, right? Which are, by the way, none of these are new problems. These are problems that people
have, you know, the first thing you realize and any of these ethical conversations about AI is that
these are the same problems I have existed in society. They're just maybe more shumped in our
face because they can happen faster in its scale. Right, right. I think there are,
you know, there are, I think a range of reactions to ideas like AI and machine learning
assisted hiring and, you know, computer vision as a kind of broad class of applications. And
you see reactions ranging from, you know, the, you know, the technology is just a hammer. The hammer
didn't kill anyone. It was the person that used the hammer that killed someone to, you know,
the technology is, you know, the root of the evil and we should not use, you know, technology
X for problem Y. Right. And I'm curious how you, you know, both how you personally kind of parse
those kinds of arguments and also how you lead folks through a process to figure out, you know,
what makes sense for them. This is why I really like talking to my lawyer friends. Seriously,
I know, I've actually learned a lot from legal people, whether it's like, I thought you were about
to run into a disclosure. No, no, no, as in like, you know, these are, these are actually questions
that, you know, they've thought of. So the big question is always who has the liability. And in
some sense, when someone says, oh, technology is just the hammer. They're kind of staying like
technology is liable. Right. And I'm not liable. Right. Or yes, or the, you know, or I'm just the engineer,
like I'm not liable, et cetera. And it is an interesting conundrum, even from like a legal
liability perspective. Like, I think we can more or less agree that we can't hold an AI or model
liable. So fine. Is it the data scientist? Is it the company? And if so, who at the company?
And that's not really a solved problem. I think there was a lot of discussion around the Uber
self driving car incident, right? Whether you're a self driving car, hit this woman on the road.
And there was a cop potato of who's liable. So it turned out that the instruments had been tuned
in a particular way that it didn't actually see her or didn't pick her up as like a, like a,
like an object moving that it should avoid. If it were tuned a different way, it could have,
by the way, so I think that's a really interesting point. I know at one point, they were talking about
how well the woman driving, who was like the test driver should have been paying better attention.
But you know, one can make a pretty easy argument that if you're in an autonomous vehicle,
it's really hard to be constantly paying attention. Yeah. And just to have the level of like quick
response and reaction that you need to, if you're, you know, you can't be on alert when you're not
doing anything for hours at a time. So, you know, and I actually don't know where that, where that
netted. So I'd be curious. But on, you know, on, on the other end, like just, just thinking about,
like, you know, like this, the notion of technology being neutral, like, I, it's just, it's such a,
again, the social science, such a silly concept. Because everything we build is in view
with our values, just literally just by creating something, because we built it in a way just to
solve a problem. And sometimes I like to use, you know, maybe non-value slate and examples,
I think people get very emotionally polarized one way or the other. I'll give you a really
good example and like how I, I thought about this myself. So I was in the Nordics in January,
and I had to go to this event and I was using like whatever maps on my phone, right? And I go
outside and it's like negative 30 or some insane temperature, right? And like in two minutes,
my phone completely breaks like it dies. Like I'm watching the battery go to zero and it dies,
no, it's going on. And then I go to the event and I'm going to and I'm like, hey, it's my
job with my phone, I'm really sorry, I'm late, I don't know, like, oh yeah, yeah, that happens.
Apparently, at certain, at a particular temperature and below that temperature, a smart phone,
batteries die. And all you have to do, so they all, everybody has their own solutions for it,
like they have little, like, they stick them in their mittens or whatever. Or, and then also,
you, you just need to recharge it for a few minutes and it brings with battery back to full power.
And I thought that was really interesting because I'm like, you know, that, that to me doesn't sound
like an intractable problem. But if I were developing this technology in Cupertino, California,
where it is never below 30 degrees, I probably wouldn't see that. I probably would never have
had that problem as, you know, something I would adjust. And I thought it was really interesting
that like I have a watch that knows what kind of slim stroke I'm doing, whether it's a backstroke
or butterfly or, you know, freestyle or whatever. But my phone, the phone for a significant
population of the world completely freezes in the temperature that's actually not very unusual
for what they are. And, you know, all that is to say, like, everything we build has values,
we choose to prioritize certain things over others. So it's on to say that, like, it's,
it's fundamentally, like, not values driven. And, you know, it's, and, and yes, there are panels
to be made with like, you know, the, the creation of the nuclear energy, et cetera. A lot of people
use the Manhattan Project when thinking about things like liability or responsibility.
But ultimately, you know, it is a responsibility because we do make these things, right? And the
things that we make are taken and used by people. And yes, we can't control how everyone's
going to use what we've built. But I do think creating a culture of responsibility is absolutely
critical and necessary. I'm trying to think through whether that answered my question at all.
It did not. Right. Well, you know, and I think there's an argument that, you know, you live in a
domain to which there aren't answers to, you know, answer the way, you know, that, you know,
there may be answers to questions that, uh, you know, engineers might. Yeah. And I think sometimes
it's fine. Like, I think the, the act of interrogation and the act of understanding is, like,
is actually sometimes which the thing that brings you to where you want to be. Because a lot of
these things are like personal choices, right? Personal values. And sometimes it's hard to have
these conversations because they do end up being values-laden. So we did this survey about a
year ago at Accenture called the gray area survey. And rather than ask these, like, really obvious
questions, like, should the AI kill person A or B? They were kind of something that was a lot
more difficult to answer. So one, you know, one that I think was interesting was, you know,
our company is responsible for not having surge pricing if there is a potential threat of a disaster.
And that actually happened to me because I was in London and uh, it was crazy. I was like walking
on bomb street and then all of a sudden this flood of people come running at me and it turns out
that there, like, they, there was a scare where they thought there was somebody with a gun.
It turned out to not be. But then none of us could leave because everyone was trying to get a car
and uh, prices for, like, Uber's et cetera, like, absolutely insane. Or 300 pounds, right?
Um, but then that leads to the question, like, is, is it a company's responsibility to, you know,
make their models such that if there's a threat of a, an attack that, you know, they don't have
search, is that unsafe? Is it unfair? I don't know if the answer is to that, right?
Another one would be, you know, uh, an assert engine, if you search for CEO, you're largely going
to get point men, uh, who are over the age of 40, uh, is that fair or unfair? Is it ethical or
unethical? You could say, well, it's the ground truth. That's what it is. You know, most CEOs in
the world are old white men named John, literally. Um, but, or, and then there's certainly an argument
to be made that, you know, just because you're searching CEO doesn't necessarily mean you have to
be told of bias truth. And maybe it can be more as, maybe we can show images of people who are
CEOs who don't fit that singular mold. And again, that's a values solution that that's not,
I think there is no right answer to some of these things. And I think part of it's actually being
comfortable with the fact that there are no single answers to a lot of these questions.
Yeah. Yeah. Yeah. Yeah. I think the example that was maybe floating around in the back of my
mind was the, I forget the name of the individual, but one of the developers of yellow, which is
a object detection library announced, uh, on Twitter. Hey, this computer is being a scary. I'm out of here.
And it was, uh, right. It was, it was really, it was that's powerful. Yeah. I mean, the, the thing
isn't that's the thing about like somebody creating a technology absolving themselves in
responsibility. It sends a message of indifference going back to this notion of like power structure.
You know, it doesn't matter if I personally mean Ramon say, I don't want to use computer vision
technologies like, okay, you know, whatever. There's the person literally created it says,
you know what? Like, this is not something I can, like, be behind anymore and then horrified by
how this has been used. But it's such a powerful message to send, you know, and it's very clear
that something needs to be done. And so, you know, with something like that as kind of background or
context, say, how do you walk through, you know, personally, where you draw the line or, you know,
say you're an engineer at place X, you know, is there an answer for, you know, where I'm going? You've
had this conversation. It's just like, you know, go off on a mountain and like, you know, sit in the
position. It's tough. But then like, actually, I had like a benefit like an existential crisis
last year and thinking through exactly these decisions, right? You know, one can arrive at the
conclusion that inherently all capitalism is evil, right? This notion that there's no such thing
as an ethical company. And then I had this really great book recommended to me and it's called
against purity and has nothing to do with ethics and AI. It actually has to do with, you know,
movements that are about, you know, sort of like a moral good and how this sort of this idea of
the most pure or the most good is actually detrimental to the cause. And the author actually uses
climate change and the environmental movements to talk about how it can be harmful if we're just
trying to like our ethics each other. And then you do see it sometimes. And I think, you know,
this idea of like who's better than someone else because person acts works at evil company. Why,
you know, I mean, you know, I can get fucked because I work at Accenture, right? Does that mean
that everything I do is, you know, nullified or painted? And, you know, I don't, I would like to
think that's not the best way to approach it because frankly, you do end up in a race to the
bottom, right? Where everybody has to be more ethical or more good to the other person or more pure
than the other person. And it's just not a helpful, it's not not a helpful, especially how ethical is
it to be the ethics police that, you know, thinks they're superior at everybody? Exactly, which
actually interestingly, like when I think about AI governance, I worry about this a lot. So again,
like, thank you about everything going about states markets and democracy. Uh, often the way we do
AI governance, like, and everyone's doing the same thing. You get a group of quote experts together.
These are all like folks like me, right? And we all sit in the room and we just decide
what ethics is. It's very strange and problematic. And if someone would have created a problem
in that way, we would call it an authoritarian regime. We would actually not call it a democracy.
So it's interesting that I have seen very few democratic processes being built around
governments. And interestingly, I think corporations would be the first ones to actually do that
if they do it right. Interesting, interesting. Well, I appreciate the subtitle of this book,
you're recommending against purity, living ethically and compromised times. I think we can all
relate to, yeah, yeah, or another. Um, cool. What else, what else is going on? Anything else that we
should, uh, that we should be sure to cover or any pearls of wisdom for us, other books that we
should be. Oh, man. I was looking at my book. Like, what's going on over here? Um, I don't know.
What am I eating? Actually, my laptop is like propped up on this book called The Essentials of
Risk Management, 600 pages on a financial risk management. Our pass. I thought it was a really
interesting book like making those parallels. I tried to read and go to Esher Bach, right? I just,
it made me go to sleep. I supposed to be one of those classics of, you know, thinking through
computing, etc. And like, I guess I'm not smart enough for it. So, I mean, I thought it was fine.
Like the anecdotes are kind of cool, but like, I can't read like a thousand pages of like
disjointed anecdotes. I was kind of rough. Um, I suppose like for the end time STEM people are,
like, we are, are collapsing social sciences. I don't know if you, uh, uh, I was like, I found
something today where it just sort of boggles my mind where, you know, entire feet like
the soul field can just decide like, oh, wow. Behavioral sciences, we are the ones who discovered it.
And I'm like, yeah, social sciences. Well, as an example, there was something, I don't know,
maybe six, six months ago or something where someone wrote this article that got a lot of
publicity. It was like, oh, we should have a field of study that does X, Y, Z, and
and like, oh, yeah, that exists. We've been STS. We've been doing that for.
Yeah. I mean, without naming names, because it, you know, this person is a very nice individual,
but I was like, was talking to somebody pretty high up at a major tech company,
leading AI. And he had never heard of the field of HCI or STS. Never heard of it, but he was
incredibly proud of the fact that he had just hired an ethicist to advise it. And I'm like,
so you hired a philosopher to advise you, which you've never heard of STS or HCI.
Cool. STS being science and technology studies in HCI human computer interaction.
Yes. And I mean, I was like, good luck with your platform.
And the thing is like, it's not to belittle the people who are trying, right? I think that
there is this inherent ego about tech that I just, I found like, actually like mind boggling
when I moved here. I just never been in a field where I don't just thought they were gods.
I thought it was very strange. But hey, I come from the lowly social sciences. I worked in
public policy and nonprofits before, you know, I was an economist for a minute, you know, like,
I just never worked in a field where literally people thought that they just did everything better
than everybody. It's amazing. As a data scientist, you know, I just, like I said, I didn't understand
and I still don't. This idea that programming is better than everything or that technology is,
you know, this notion of technological solutionism. And I think, you know, with the,
it's kind of being brought to the forefront, the more we build artificial intelligence
technologies, right? That we can't automate away human behavior and human preferences. And
actually, I'd mention Bobby. Bobby and I have a paper on something I was working on called
technological determinism and specifically thinking through recommendation systems and how they
might nudge you to kind of be the same person forever. And they don't really encourage you to explore
and expand your horizons simply because of the way they're constructed, right? A recommendation
system takes your prior behavior, maps that, you know, who you are to an assessment of people who
are quote, like you and gives you recommendations based on things you might like given other people
who are profiled the same as you. That's actually kind of scary. A lot of conversation around this
in the context of, you know, it actually, this conversation gets more prevalent kind of every
kind of political, you know, election cycle, right? As we start talking about filter bubbles and
things like that and monocultures and... Yeah, I mean, filter bubbles are kind of a more extreme
example. But I'm even thinking of like, I don't know, I think a lot. I fortunately went to college
and did all my stupid things before these ways existed of tracking and chasing God forbid,
they're really a Facebook when I was in college for a year. But I wonder about these, you know,
but in college, you know, back in the olden times, was an interesting way to sort of reinvent yourself,
right? To think of yourself as just get like a fresh start. And I feel like sometimes for younger
people, like you can't ever escape who you are and where you're from. And you can't ever be someone
else or think of the world in a different way, right? Because like you have this weird baggage,
just technological baggage with you forever. And I find that kind of sad. And I think about like,
how much I have changed and how much it's part of, you know, human nature to change over time.
Like we're supposed to evolve. We're supposed to be weird and different, you know? I worry a lot
about the homogeneity that comes with tech. And I wonder whether, you know, social scientists
or political scientists, middle of a PhD program today could even possibly enter the field of
data science, right? Because, you know, whether or not it's because these job ad platforms,
or these, you know, these NLP based engines that match, uh, resumes with jobs, which is not
sometimes qualified or didn't, you know, didn't know what I was doing. I just didn't fit someone
paradigm. And I felt my life and not fitting other people's paradigms. And I worry very much about
like how people can still be individuals and be curious and just learn other things. Even though
the internet has given us basically unlimited access to things, I wonder if, you know, in a sense,
we may actually stifle all that possibility by thinking that like everything is this repeat cycle
of behavior that we can predict patterns and these patterns are deterministic. And I would say that
like, you know, rule number one of quantitative social science is patterns exist in human behavior,
right? Rule number two is just because a pattern exists doesn't mean individual follows it.
And I think rule number two hasn't really been arrived yet in some of the fields that we're talking
about. That seems like a good place to leave things. Holy you with the big questions.
I mean, it is interesting stuff to think about right on a, what day is today? I'm like on a Wednesday
on a Friday. Clearly a Friday. Clearly it's a Friday. Friday called Tuesday.
I have no idea what day it is anymore. It's really hard. It's, I don't recall if I've talked
about this in an interview, but I've independently kind of validated this experience for several
people like that March was this super, super, super long month and then April kind of flew by
really quickly. But time is just really weird right now. I think for many of us. Yeah, it's like
an absolute reminder that time is absolutely relative. Although it must be great, kind of great
to be a kid right now because you're like forever summer vacation. You know, like you're just like
the longest summer ever. I don't know. You've got to be old enough to know how what you didn't have before, though, I think, right?
Otherwise, that's true. That's true. It must be kind of cool to be like young enough that you don't
really get how scary the situation is. Yeah. But like you're like, cool enough that you're like, oh wow,
I just get to be home for months on end and like playing with my toys. Yeah. And all of that must be,
it must be really interesting time. I do feel bad for all the kids graduating. And he's sort of
for a band right now. This is worse than two thousand. Yeah. Well, Ramon, it was wonderful catching
up with you as always. We'll have to make sure to do it more frequently. Yes, yes, absolutely.
Thanks so much. Thank you very much. Happy Mianzen.
All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview,
visit twimmelai.com. Of course, if you like what you hear on the podcast, please subscribe,
rate and review the show on your favorite pod catcher. Thanks so much for listening and catch you next time.
