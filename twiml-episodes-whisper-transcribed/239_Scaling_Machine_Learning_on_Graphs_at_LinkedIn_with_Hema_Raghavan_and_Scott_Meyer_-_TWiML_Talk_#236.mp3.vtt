WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:36.000
Today we're joined by Hema Rogovon and Scott Meyer of LinkedIn.

00:36.000 --> 00:41.440
Hema is an engineering director responsible for AI for growth and notifications and Scott

00:41.440 --> 00:45.080
serves as a principal software engineer.

00:45.080 --> 00:49.320
In this conversation, Hema Scott and I dig into the graph database and machine learning

00:49.320 --> 00:55.480
systems that power LinkedIn features such as people you may know and second degree connections.

00:55.480 --> 00:59.720
Hema shares her insight into the motivations for LinkedIn's use of graph-based models

00:59.720 --> 01:04.040
and some of the challenges surrounding using graphical models at LinkedIn scale, while

01:04.040 --> 01:08.520
Scott details his work on the software used by the company to support its biggest graph

01:08.520 --> 01:11.360
databases.

01:11.360 --> 01:15.840
Before we get going, I'd like to send a huge thanks to LinkedIn for sponsoring today's

01:15.840 --> 01:16.840
show.

01:16.840 --> 01:22.080
LinkedIn Engineering solves complex problems at scale to create economic opportunity for

01:22.080 --> 01:24.320
every member of the global workforce.

01:24.320 --> 01:29.120
AI and machine learning are integral aspects of almost every product the company builds

01:29.120 --> 01:32.080
for its members and customers.

01:32.080 --> 01:36.520
LinkedIn's highly structured dataset gives their data scientists and researchers the ability

01:36.520 --> 01:41.280
to conduct applied research to improve member experiences.

01:41.280 --> 01:48.720
To learn more about the work of LinkedIn Engineering, visit engineering.linkton.com slash blog.

01:48.720 --> 01:53.360
And now on to the show.

01:53.360 --> 01:58.280
Alright everyone, I am here with Hema Rogovon and Scott Meyer of LinkedIn.

01:58.280 --> 02:04.640
Hema is an engineering director and responsible for AI for growth and notifications and Scott

02:04.640 --> 02:06.480
is a principal software engineer.

02:06.480 --> 02:09.440
Hema and Scott, welcome to this week in machine learning and AI.

02:09.440 --> 02:10.440
Thank you.

02:10.440 --> 02:11.440
Glad to be here.

02:11.440 --> 02:17.520
I'm very glad to be here at LinkedIn and looking forward to our conversation and digging

02:17.520 --> 02:22.200
in to some of the things you're doing with graphs for machine learning and powering

02:22.200 --> 02:24.200
various LinkedIn features.

02:24.200 --> 02:29.400
But before we dive into that, I'd love to introduce our audience to you.

02:29.400 --> 02:33.080
So would you take a minute to share a little bit about your background and how you got

02:33.080 --> 02:36.880
started working on graphs and machine learning, Hema?

02:36.880 --> 02:42.640
Okay, so I've been in the field of machine learning for several years now.

02:42.640 --> 02:48.880
I got my PhD back in 2006, largely focused on search and information retrieval, I've worked

02:48.880 --> 02:57.680
on advertising, I've worked on question answering systems at IBM, IBM research in particular.

02:57.680 --> 03:03.960
And then when I came to LinkedIn, I came in to actually work on content, on content

03:03.960 --> 03:05.440
recommendations.

03:05.440 --> 03:11.920
I spent a year doing that and I realized that one of the fundamental assets that LinkedIn

03:11.920 --> 03:19.560
has is the underlying social network and that's where content propagates.

03:19.560 --> 03:28.320
And I got really interested in the problem of building the graph in particular, thinking

03:28.320 --> 03:33.560
about algorithms for people you may know like problems or follow recommendations and so

03:33.560 --> 03:40.040
on. And that's what brought me from the general area of search and information retrieval

03:40.040 --> 03:41.040
to graphs.

03:41.040 --> 03:44.240
And it's been four years and I'm still having fun.

03:44.240 --> 03:45.240
Fantastic.

03:45.240 --> 03:46.240
Fantastic.

03:46.240 --> 03:47.240
How about you Scott?

03:47.240 --> 03:48.240
Well, let's see here.

03:48.240 --> 03:49.240
I guess the story starts.

03:49.240 --> 03:54.720
I did relatively well in the.com boom and actually quit for three years to go sailing.

03:54.720 --> 03:59.720
And at the end of that, I was semi-relectorately looking for a job and I bumped into this company

03:59.720 --> 04:01.040
called MetaWeb.

04:01.040 --> 04:05.960
And they were like, hey, we want to build a world-rideable graph of all common knowledge.

04:05.960 --> 04:08.040
And by the way, we're going to call this thing freebase.

04:08.040 --> 04:12.280
And I was like, wow, I would really regret not taking that job.

04:12.280 --> 04:15.680
So I wound up leading the storage team at MetaWeb.

04:15.680 --> 04:18.280
MetaWeb was acquired by Google.

04:18.280 --> 04:22.320
Freebase became Google's knowledge graph.

04:22.320 --> 04:24.960
Spent four years at Google working on knowledge graph infrastructure.

04:24.960 --> 04:30.160
I think the best summary of that is Google and I disagreed about graph databases.

04:30.160 --> 04:36.280
LinkedIn found me actually through LinkedIn, which was a huge endorsement, and recruited

04:36.280 --> 04:40.440
me to come here and lead a next-generation graph database project.

04:40.440 --> 04:43.520
So that's what I've been doing for the last four and a half years.

04:43.520 --> 04:46.440
So it's like 15 years in graph databases now.

04:46.440 --> 04:48.400
Can't shake the habit.

04:48.400 --> 04:53.360
So we're going to go deep into graphs in the way LinkedIn uses them.

04:53.360 --> 04:59.720
We spoke a little bit about this with a previous interview with Romer, who's one of your colleagues.

04:59.720 --> 05:06.720
Heima, but we'll dive even deeper here, but to kind of contextualize that for us, talk

05:06.720 --> 05:13.480
a little bit about the motivation for the emphasis on graphs at LinkedIn.

05:13.480 --> 05:14.480
Yeah.

05:14.480 --> 05:20.240
So the social network for LinkedIn, we are an online professional social network.

05:20.240 --> 05:28.360
The fundamental asset that a user has on LinkedIn, perhaps even comparison to our competitors

05:28.360 --> 05:34.640
but indeed, or any of the other job sites, is the social network.

05:34.640 --> 05:44.160
So in that context, a graph is the prime driver of all of the other parts of our business

05:44.160 --> 05:45.760
at LinkedIn.

05:45.760 --> 05:50.360
In particular, if you can just think about it intuitively, it makes sense that when you're

05:50.360 --> 05:56.480
looking for a job, the most common thing for you to do is actually reach out to your

05:56.480 --> 05:59.120
first degree connections.

05:59.120 --> 06:07.280
If you are in a job that you like, you might want to stay informed through what your first

06:07.280 --> 06:11.440
degree connections are talking about, and that's pretty much what your feed does.

06:11.440 --> 06:18.240
And if you are likely missing out on a conversation, then your notifications are what are sent

06:18.240 --> 06:19.320
to you.

06:19.320 --> 06:25.400
And notifications from your first degree connections are more valuable than otherwise.

06:25.400 --> 06:30.720
So at all stages of your professional network, your connections are your core asset.

06:30.720 --> 06:35.720
And that's the core of what LinkedIn offers.

06:35.720 --> 06:45.520
So in terms of the set of AI problems that surround graphs is first building the graph itself.

06:45.520 --> 06:50.200
So when a member comes in, the most primitive way that a member would build their network

06:50.200 --> 06:56.920
is Google buy memory and think about all of their colleagues that actually exist on LinkedIn

06:56.920 --> 06:58.720
and go into a search.

06:58.720 --> 07:02.240
Now you would not get a lot of coverage that way.

07:02.240 --> 07:07.160
I would probably not remember colleagues from 10 years ago when I'm building my network.

07:07.160 --> 07:11.560
So that's when a recommendation engine like people you may know comes in.

07:11.560 --> 07:19.600
Likewise for the feed, then how do you know which of your first degree connections are

07:19.600 --> 07:24.240
actually relevant to your current context?

07:24.240 --> 07:31.120
So I may have colleagues from school, from three other jobs before, but maybe the conversation

07:31.120 --> 07:37.560
that I really care about in today's context is deep learning and AI.

07:37.560 --> 07:41.560
So which of those colleagues are contextually relevant to my feed?

07:41.560 --> 07:43.160
So that's also a graph problem.

07:43.160 --> 07:48.680
We can think about it as affinity modeling amongst your first degree connections.

07:48.680 --> 07:54.280
Likewise, if you're on the jobs page, then which of your connections are relevant to you

07:54.280 --> 07:57.840
and who might potentially give you a referral, right?

07:57.840 --> 08:00.160
So all of these are graph problems.

08:00.160 --> 08:03.120
They can be modeled as prediction.

08:03.120 --> 08:07.280
They can come down to prediction problems given a certain context.

08:07.280 --> 08:12.440
The people you may know problem is what edge should I build and that is most relevant

08:12.440 --> 08:13.760
for the member.

08:13.760 --> 08:20.360
And then the other problems like feed notifications or jobs say the member has a set of edges,

08:20.360 --> 08:24.000
which of these edges are most relevant in that context.

08:24.000 --> 08:28.200
So that's graphs everywhere at LinkedIn.

08:28.200 --> 08:32.680
So LinkedIn is clearly already doing this.

08:32.680 --> 08:38.360
When you think about the big challenges or opportunities in this space and among the

08:38.360 --> 08:42.880
various features that you've talked about, people you may know in the feed, what are

08:42.880 --> 08:46.160
those big challenges from a machine learning perspective?

08:46.160 --> 08:47.160
Correct.

08:47.160 --> 08:52.600
When I think about graph problems, I actually think the challenge, just compared to other

08:52.600 --> 08:59.600
recommender systems, is often scale because in the limit, a lot of these problems are

08:59.600 --> 09:00.600
n squared.

09:00.600 --> 09:06.440
So if you have a graph that's 500 million members, then you're looking at n squared of

09:06.440 --> 09:07.440
that.

09:07.440 --> 09:11.960
So there's no way that you can talk about some of these problems without having a distributed

09:11.960 --> 09:17.320
systems expert co-seated at the table with you, which is part of why we're at in this

09:17.320 --> 09:20.560
conversation today together with Scott, right?

09:20.560 --> 09:25.640
So in fact, that's the one piece we realize that even when we're training the models,

09:25.640 --> 09:31.440
like just thinking about network propagation algorithms or about inference on these large

09:31.440 --> 09:39.160
graphs, there is a community, a research community out there, but it's not as prolific

09:39.160 --> 09:47.280
as perhaps, you know, other fields of AI, for example, Tex and LP have broader communities.

09:47.280 --> 09:52.920
And it's perhaps also because the problem space is rather niche.

09:52.920 --> 10:01.600
So those are the biggest challenges, the lack of large data sets for academia to publish

10:01.600 --> 10:06.320
research that necessarily we can directly leverage from.

10:06.320 --> 10:11.640
So oftentimes we are pushing the boundaries internally ourselves.

10:11.640 --> 10:17.600
And there are a few problem spaces which sort of intersect so closely with distributed

10:17.600 --> 10:19.760
systems and AI.

10:19.760 --> 10:23.600
Scott, how about from your perspective, what are the big challenges in addressing this

10:23.600 --> 10:24.600
scale?

10:24.600 --> 10:25.600
Yeah.

10:25.600 --> 10:30.120
Well, interestingly, I had a conversation with a recruiter when I came in and he said,

10:30.120 --> 10:34.080
look, the magic in LinkedIn is in the second degree.

10:34.080 --> 10:38.120
That is in, it's in connections that you could have, but probably don't.

10:38.120 --> 10:43.200
So for example, any problem that you're experiencing, probably there's somebody in your second

10:43.200 --> 10:47.480
degree who has solved it or who knows more about it than you.

10:47.480 --> 10:49.920
And you may not know this person.

10:49.920 --> 10:55.120
Conversely, with the first degree, like mostly you already know it, it's, you know, top

10:55.120 --> 10:57.480
of mind.

10:57.480 --> 11:01.240
From the standpoint of data, a second degree is very difficult to work with, right?

11:01.240 --> 11:06.800
Because it's big, the way it got big is because it changes rapidly.

11:06.800 --> 11:11.520
So if I talk about storing first degrees, this is very easy to do with a key value store.

11:11.520 --> 11:17.560
However, if I were to try and materialize second degrees, the data footprint would be huge

11:17.560 --> 11:20.720
and the right way it would be enormous.

11:20.720 --> 11:24.720
And so there isn't a simple brute force solution to working with a second degree.

11:24.720 --> 11:31.200
So one of the key things that a graph database does, probably the defining thing is joining.

11:31.200 --> 11:35.840
And in particular, we join two first degrees to put together a second degree.

11:35.840 --> 11:39.680
And we have to do this interactively in real time.

11:39.680 --> 11:46.400
And talking about graph databases, where does the graph database sit in the process of

11:46.400 --> 11:49.320
kind of serving up these graph based features?

11:49.320 --> 11:56.960
Is it kind of at the end of the chain, you know, close to where a page is being rendered

11:56.960 --> 12:02.040
or is it kind of early in the chain, serving up the models?

12:02.040 --> 12:06.880
Certainly, our online, we are an online service.

12:06.880 --> 12:08.720
So it's fundamental to LinkedIn.

12:08.720 --> 12:13.680
If the graph is down, LinkedIn does not display web pages.

12:13.680 --> 12:17.160
And you'll see it all over the site, for example, graph distances, how far apart you are

12:17.160 --> 12:19.280
from someone you're looking at.

12:19.280 --> 12:22.200
That's computed out of the graph.

12:22.200 --> 12:28.600
So the graph itself is definitely online.

12:28.600 --> 12:33.720
The graph, the contents of the graph, some of it is raw data that is curated directly

12:33.720 --> 12:35.000
by users.

12:35.000 --> 12:41.240
And some of it is the result of a relevance or AI process where we take data that may be

12:41.240 --> 12:46.000
unreliable and figure out what's good, for example, filter out spam.

12:46.000 --> 12:47.000
Okay.

12:47.000 --> 12:54.320
So is it, is the graph a single monolithic system, an instance of a graph database, or

12:54.320 --> 12:59.280
are there multiple systems that contribute to making the graph real for?

12:59.280 --> 13:02.040
The graph is an instance of a graph database.

13:02.040 --> 13:09.360
We actually have seven instances in four different collos to actually provide a continuous

13:09.360 --> 13:10.600
service.

13:10.600 --> 13:16.760
And the instance of the graph database itself is a cluster of machines, roughly 20 terabytes

13:16.760 --> 13:19.040
of RAM per cluster.

13:19.040 --> 13:24.960
And is this a LinkedIn kind of internally developed graph database, or is it an open-source

13:24.960 --> 13:25.960
project?

13:25.960 --> 13:32.000
This is a complete internal project, it's built, it's an in-memory database, it's built

13:32.000 --> 13:34.520
on top of basically bare Linux.

13:34.520 --> 13:35.520
Okay.

13:35.520 --> 13:37.520
Oh wow, wow.

13:37.520 --> 13:45.760
And so you talked about, Hema, you talked about across these different capabilities,

13:45.760 --> 13:52.960
the common thing that you're trying to drive for, across the graph is relevant, some

13:52.960 --> 13:54.960
aspect of relevance.

13:54.960 --> 14:02.280
You mentioned affinity modeling as one of the types of technical approaches that you're

14:02.280 --> 14:03.280
applying.

14:03.280 --> 14:04.280
Can you talk a little bit more about that?

14:04.280 --> 14:05.280
Sure.

14:05.280 --> 14:11.440
So let's take affinity modeling for the people you may know problem, and then we'll talk

14:11.440 --> 14:15.520
about it in the context of some other problems.

14:15.520 --> 14:22.800
So the people you may know problem says, is A likely to know B, and, but we also want

14:22.800 --> 14:25.000
to consider long-term value.

14:25.000 --> 14:32.480
So if A may know B, but if they're not going to derive value from on LinkedIn in any

14:32.480 --> 14:35.640
potential future, right?

14:35.640 --> 14:40.800
Is it worth connecting to your neighbor down the road, like should we be showing?

14:40.800 --> 14:48.240
It's someone you know, but using a top slot in a recommendation system for that versus

14:48.240 --> 14:55.840
perhaps somebody who might be a good referral for a job for you down in the future.

14:55.840 --> 15:02.200
Or someone who may share really good content, like someone that you know, but they just

15:02.200 --> 15:09.720
write really well and you would benefit from their writing.

15:09.720 --> 15:14.960
So you can think about predicting future edges or predicting.

15:14.960 --> 15:20.080
So what we're doing is actually predicting future edges, right?

15:20.080 --> 15:23.680
Is Hema likely to connect to Sam?

15:23.680 --> 15:28.760
And so is Hema likely to send an invitation to Sam, and is Sam likely to accept that invitation?

15:28.760 --> 15:33.360
So you have to know that, you know, Hema knows Sam, and Sam also knows Hema.

15:33.360 --> 15:36.160
So it's a two-way problem.

15:36.160 --> 15:42.480
But you want more than just that edge in building that edge in itself.

15:42.480 --> 15:50.400
You want to be able to predict that once Hema and Sam connect, what's the likelihood

15:50.400 --> 15:52.280
that they're going to interact?

15:52.280 --> 15:57.000
An interact could be in the context of a job, interaction could be in the context of the

15:57.000 --> 16:00.840
feed or a notification or some other context.

16:00.840 --> 16:03.760
So affinity turns into a predictive problem.

16:03.760 --> 16:10.520
You can generalize it to are you likely to interact in some context in the future?

16:10.520 --> 16:18.680
And how likely is a person likely to accept an invitation from a person and derive value

16:18.680 --> 16:19.680
from that?

16:19.680 --> 16:25.160
So those are all the broader problems that we're trying to solve when we're building edges.

16:25.160 --> 16:30.360
So one thing that comes to mind for me and think about that is you've got this graph

16:30.360 --> 16:38.120
database that is presumably you're capturing the nodes which are the people in some aspect

16:38.120 --> 16:41.800
of the relationships, the edges.

16:41.800 --> 16:48.440
But you're also, when you're making these predictions, are you then taking the high

16:48.440 --> 16:52.040
likelihood predictions and then putting them into the graph database?

16:52.040 --> 16:59.080
I guess I'm trying to reconcile the graph as this kind of existent thing that's captured

16:59.080 --> 17:05.520
concretely in a database and this predicted thing that doesn't exist.

17:05.520 --> 17:09.800
And when does it get kind of materialized into the database?

17:09.800 --> 17:10.800
God.

17:10.800 --> 17:16.400
So the existing graph is the existing set of members and their connections.

17:16.400 --> 17:18.440
And it can actually be richer than that.

17:18.440 --> 17:21.920
You can have nodes in the economic graph of LinkedIn.

17:21.920 --> 17:25.560
You can have nodes that are schools, companies.

17:25.560 --> 17:27.680
You can have edges that people follow.

17:27.680 --> 17:30.840
So that rich database in its current state.

17:30.840 --> 17:37.680
So if you take a snapshot in time, that's what Scott Steen built serves for all of LinkedIn.

17:37.680 --> 17:43.160
And then what we do is, given that snapshot, how do we query it to get the potential second

17:43.160 --> 17:44.160
degree?

17:44.160 --> 17:47.400
These are people who are not connected to each other.

17:47.400 --> 17:53.440
And then potentially even third degrees for a low degree members because if your second

17:53.440 --> 17:57.960
degree is too sparse, we might want to walk the graph out a little more.

17:57.960 --> 18:03.560
So we look at the second degree, the third degree, and that itself is huge.

18:03.560 --> 18:08.760
Even if a few of your second degree nodes have 30 odd connections each that can blow up

18:08.760 --> 18:10.200
in computation.

18:10.200 --> 18:12.840
So we take that large list.

18:12.840 --> 18:16.960
And then we say all of these people are not connected to each other, which ones are

18:16.960 --> 18:18.840
likely to connect to each other.

18:18.840 --> 18:21.400
And that's the predictive problem.

18:21.400 --> 18:27.240
And that's what you see on the people you may know or even follow recommendation pages

18:27.240 --> 18:29.800
at LinkedIn.

18:29.800 --> 18:37.120
And once people start sending invitations, so the bidirectional edge is a connection edge.

18:37.120 --> 18:42.720
So when they send in an invitation and it gets accepted, it materializes back into the

18:42.720 --> 18:43.720
database.

18:43.720 --> 18:48.280
A follow recommendation will materialize right away.

18:48.280 --> 18:59.280
And so Scott maintains the state of truth or of what connections have been accepted as

18:59.280 --> 19:01.960
well as what follow links exist.

19:01.960 --> 19:02.960
Okay.

19:02.960 --> 19:06.800
I see the people you may know quite page all the time.

19:06.800 --> 19:11.120
And you certainly wouldn't want to recompute that every time I'm going to see it.

19:11.120 --> 19:19.240
So it needs to be cashed in some kind of way, is it cashed outside of the graph database?

19:19.240 --> 19:20.400
That's also a graph.

19:20.400 --> 19:27.040
Why not put the potential kind of secondary connections into the graph as well?

19:27.040 --> 19:30.080
Like it is kind of a turtles all the way down kind of problem.

19:30.080 --> 19:37.800
That's a very good question because the assumption around, you know, could we take a state

19:37.800 --> 19:45.040
of the graph a time and point maybe once a week or once in every end days and then take

19:45.040 --> 19:50.160
the second degree network and then actually build recommendations and cash it in some kind

19:50.160 --> 19:57.760
of key value store, go as the premise with which LinkedIn built the people you may know

19:57.760 --> 20:00.480
system for several years.

20:00.480 --> 20:08.120
But something we discovered in more recent years was that network building is contextual.

20:08.120 --> 20:15.600
So when people say people don't necessarily engage with people you may know application

20:15.600 --> 20:21.560
or as a daily use case, but when they do it's long and deep sessions and say if you would

20:21.560 --> 20:27.400
join the company and you connected with your manager, if we could in real time then explore

20:27.400 --> 20:33.360
the second degree network from that new connection itself, right?

20:33.360 --> 20:39.720
Then you start exploring nodes around that company, around perhaps, you know, the team.

20:39.720 --> 20:44.680
You will start discovering some of these patterns and people go down PYM, can they go click,

20:44.680 --> 20:50.760
click, click and then if we can actually refresh those results in each subsequent scroll based

20:50.760 --> 20:57.040
on how we're walking that graph, we'd actually discovered that we can show significant

20:57.040 --> 20:59.640
improvements in memory experience.

20:59.640 --> 21:06.560
So actually walking the graph in your real time has been one of our internal discoveries.

21:06.560 --> 21:12.800
So to speak the fact that we can actually do it and the fact that it gives huge improvements

21:12.800 --> 21:19.760
in experience and performance has been one of the key insights in recent times.

21:19.760 --> 21:25.600
And it strikes me that there are significant kind of systems level challenges there.

21:25.600 --> 21:28.040
Is that in your realm or is that?

21:28.040 --> 21:29.920
Yeah, I can speak to that.

21:29.920 --> 21:37.240
So I think traditional graph processing, something like Pregel or GraphX, the notion is

21:37.240 --> 21:40.640
I'm going to process the entire graph.

21:40.640 --> 21:46.120
And if you're going to process the entire graph, the way that you serialize the graph is

21:46.120 --> 21:50.560
really important because serial IO, you know, when you can read disc blocks in order is

21:50.560 --> 21:53.400
much, much faster than random access.

21:53.400 --> 21:58.880
And so a lot of graph processing focuses on clever ways to serialize the data and the

21:58.880 --> 22:02.960
processing so that you do the most efficient processing.

22:02.960 --> 22:07.120
The tack we've taken is what if we used random access, right?

22:07.120 --> 22:11.560
So for random access, you've got to be in RAM.

22:11.560 --> 22:17.160
Your basic problem is how fast can I get something from memory into the CPU?

22:17.160 --> 22:19.840
So you're counting L3 cache misses.

22:19.840 --> 22:24.840
Every link that you follow is an L3 cache miss, at least, that would be if you followed

22:24.840 --> 22:27.080
it with a pointer.

22:27.080 --> 22:32.680
In practice, we would like to follow links in constant time.

22:32.680 --> 22:34.440
It's a little bit more expensive.

22:34.440 --> 22:38.920
We typically average out between two and three L3 cache misses.

22:38.920 --> 22:44.480
So by building a system which can do this, you can now use edges pretty much the way

22:44.480 --> 22:47.040
you use pointers pretty casually.

22:47.040 --> 22:53.320
And you can do, say, online materialization of a second degree in real time.

22:53.320 --> 22:58.280
I'm not sure that this is a direction that I really want to go down, but you know, you're

22:58.280 --> 23:03.440
talking about kind of optimizing around cache hits and avoiding cache misses.

23:03.440 --> 23:09.080
You know, there's a whole emerging field of like baking machine learning into the low

23:09.080 --> 23:14.320
level infrastructure here to make some of these predictions for you.

23:14.320 --> 23:22.280
Is that, is that something that you're, are you thinking about it, what I do is a lot

23:22.280 --> 23:23.280
more boring.

23:23.280 --> 23:28.640
It's really nuts and bolts, retrieve data, figure out if it's the data that you want, you

23:28.640 --> 23:30.760
know, pretty, pretty boring database stuff.

23:30.760 --> 23:31.760
Okay.

23:31.760 --> 23:32.760
Okay.

23:32.760 --> 23:39.000
A lot of the challenges for building machine learning systems at scale have to do with kind

23:39.000 --> 23:44.480
of the data pipeline and getting that in place before you even get to modeling.

23:44.480 --> 23:48.920
Does having access to this graph database alleviate all of that for you?

23:48.920 --> 23:54.320
Are you still wrangling with data pipeline types of issues?

23:54.320 --> 24:01.480
So data pipeline issues exist definitely for modeling because oftentimes models are still

24:01.480 --> 24:04.760
built offline prototyping is done offline.

24:04.760 --> 24:09.960
I mean, a graph database comes in when you're actually serving.

24:09.960 --> 24:18.240
So one of the pieces around making data pipelining easier has been what LinkedIn calls productive

24:18.240 --> 24:29.240
ML or ProML and we have actually facilitated tooling to become much more easier for data

24:29.240 --> 24:32.360
scientists to actually build prototypes.

24:32.360 --> 24:40.920
Now that said, we have also started thinking about how do we make computing graph features

24:40.920 --> 24:50.920
because if you have a data scientist who wants to try a slightly different variant of

24:50.920 --> 24:55.600
a second degree algorithm because we talked about a very simple version which is get

24:55.600 --> 24:58.000
or get me all the second degree notes.

24:58.000 --> 25:05.440
And if you want to think about it as a probabilistic random walk and you want to weigh the edges

25:05.440 --> 25:12.160
in terms of by a model, right, like how close people are based on some of these affinity

25:12.160 --> 25:15.400
models that we talked about.

25:15.400 --> 25:20.360
How would it be easy for a data scientist to not have to worry about scale?

25:20.360 --> 25:25.920
So can we just provide an interface, a simple, maybe Jupyter notebook like interface

25:25.920 --> 25:28.280
to actually query these kind of databases?

25:28.280 --> 25:33.160
So we've actually, we've, we're working on these and that makes the life for our data

25:33.160 --> 25:35.120
scientists easier.

25:35.120 --> 25:41.560
So and that definitely takes away the data pipeline issues or the scale issues.

25:41.560 --> 25:47.280
And when you get to modeling, can you talk a little bit about the modeling, the types

25:47.280 --> 25:52.640
of models that folks are building to solve these types of problems are there.

25:52.640 --> 25:58.840
You know, if you kind of go to models that you apply broadly or do you customize the

25:58.840 --> 26:03.400
models very deeply for the specific types of problems you're attacking here?

26:03.400 --> 26:04.400
Yeah.

26:04.400 --> 26:12.440
So at the very basic, oftentimes for any kind of affinity modeling, we would start with

26:12.440 --> 26:20.520
the simplest class of models, decision trees, logistic regression and so on and so forth.

26:20.520 --> 26:26.400
That said, we do find benefits in deep learning like models.

26:26.400 --> 26:32.600
So there are some, there are, there's published literature.

26:32.600 --> 26:39.360
In particular, there is a graph sage algorithm that came out of work from graph sage.

26:39.360 --> 26:40.360
Graph sage?

26:40.360 --> 26:41.360
Yes.

26:41.360 --> 26:45.000
And that has come out of Stanford and Pinterest.

26:45.000 --> 26:52.680
So we've done variants of those, we see certain advantages to algorithms of that nature.

26:52.680 --> 26:58.120
So deep learning definitely is one direction we're pursuing.

26:58.120 --> 27:01.560
Another direction is just personalization.

27:01.560 --> 27:07.480
So how do we think about different users who are in different stages of career building?

27:07.480 --> 27:14.120
You may be a new user and you may be building your graph and what LinkedIn calls a novice

27:14.120 --> 27:15.120
member.

27:15.120 --> 27:21.680
So you're more tentative to send out invitations, you're showing you somebody that you're likely

27:21.680 --> 27:25.720
to connect in that context makes a lot more sense.

27:25.720 --> 27:31.760
So knowing the stage of the user, the degree of the of connectivity of the user and the

27:31.760 --> 27:37.200
lifecycle of the users, particularly important and likewise for someone who's very well connected

27:37.200 --> 27:41.000
already, the additional value of a connection may be much lower.

27:41.000 --> 27:44.760
So how do we find that one connection that that person's missing?

27:44.760 --> 27:52.640
So personalization is something that the simple models like logistic regression would

27:52.640 --> 27:53.640
not achieve.

27:53.640 --> 27:59.640
So we get to more advanced models like mixed effect models.

27:59.640 --> 28:00.640
Okay.

28:00.640 --> 28:03.480
I'd like to explore some of these.

28:03.480 --> 28:06.280
So graph sage, what's that trying to do?

28:06.280 --> 28:16.520
So graph sage looks at, so most deep learning models have, if you think about the body

28:16.520 --> 28:25.000
of literature in text processing or NLP, typically looks at sequences of words.

28:25.000 --> 28:31.760
There are a few models that actually look at general graphs for deep learning.

28:31.760 --> 28:37.920
But essentially what graph sage is trying to do is similar to what to work or might be

28:37.920 --> 28:43.000
trying to achieve, which is trying to learn a representation or an embedding, but it

28:43.000 --> 28:48.320
takes into account properties of the node and it takes into account the graph structure.

28:48.320 --> 28:56.000
So for example, you may be a sparse, leconnected node, but connected to one dense, leconnected

28:56.000 --> 28:59.480
node with very rich profile features.

28:59.480 --> 29:05.040
And I actually, in for some properties about the sparse, leconnected node, which with

29:05.040 --> 29:09.640
an incomplete profile from the densely connected node.

29:09.640 --> 29:13.960
So actually looking at network propagation, looking at the structure of the underlying graph

29:13.960 --> 29:17.840
is the class of algorithms that we're going after.

29:17.840 --> 29:18.840
Okay.

29:18.840 --> 29:27.120
So thinking about it as a graph embedding kind of approach your, your building a model

29:27.120 --> 29:35.120
that is finding relationships, ultimately between the nodes, that such that in this embedding

29:35.120 --> 29:38.360
space, nodes that are close together have similar properties.

29:38.360 --> 29:39.880
Yes, exactly.

29:39.880 --> 29:44.320
And then you also mentioned mixed effect models, what are those?

29:44.320 --> 29:45.320
Yeah.

29:45.320 --> 29:55.640
So the way I want to sort of generalize that class of models is how do I learn models

29:55.640 --> 30:04.880
that are specific to certain groups and groups that are learned automatically, right?

30:04.880 --> 30:14.720
So we can think about it as a logistic regression is a simple, like simple formula, which does

30:14.720 --> 30:21.960
not necessarily consider effects of the viewer.

30:21.960 --> 30:28.960
So the member that's being, for whom the recommendations are being rendered, the, or the

30:28.960 --> 30:36.640
in the, or doesn't take into account personalized features of the items being recommended, right?

30:36.640 --> 30:42.480
Or you would actually have to hand code these deeply into a logistic regression model.

30:42.480 --> 30:50.440
But classes of models that can actually learn subgroupings of people and learn those interactions

30:50.440 --> 30:56.400
between the viewer, that is, for whom it is being recommended and the items that are being

30:56.400 --> 31:00.400
recommended and learn interactions between them.

31:00.400 --> 31:07.600
So is it fair to think of it as almost conditional clustering, like your clustering condition

31:07.600 --> 31:10.800
on to a specific node or something?

31:10.800 --> 31:16.400
The other way people can think about it, these are also hierarchical models, like in more

31:16.400 --> 31:21.840
classical statistical statistics literature, you can think of taking all of your feature

31:21.840 --> 31:24.600
data and actually building clusters.

31:24.600 --> 31:27.920
So you know, you think about, you're called it conditional clustering, but just building

31:27.920 --> 31:34.360
these groups and then almost learning a separate model for each group, but not having to

31:34.360 --> 31:39.480
hand code these groups by themselves, letting the model info what these groups may be.

31:39.480 --> 31:45.840
And then we jumped into some of the more advanced models that I wasn't familiar with, but in

31:45.840 --> 31:53.080
thinking about the application of kind of your linear regression and other basic models

31:53.080 --> 32:01.720
to graph types of data, is that are there unique kind of challenges or approaches to doing

32:01.720 --> 32:09.960
that or is it kind of pretty standard to the way you do a linear regression on more simple

32:09.960 --> 32:10.960
types of data?

32:10.960 --> 32:20.200
Yes, so the challenges for a simpler model would often be just the volume of data we have

32:20.200 --> 32:21.200
for training.

32:21.200 --> 32:26.840
So just processing all of that data can often be a challenge.

32:26.840 --> 32:32.600
The other part is feature computation, so if you actually work computing something like

32:32.600 --> 32:37.760
just a number of common connections between two nodes, that can be a pretty heavy computation

32:37.760 --> 32:45.760
and you have in an offline system like Spark or MapReduce.

32:45.760 --> 32:51.720
And the other piece is the online serving, so actually inference is can be a challenge.

32:51.720 --> 32:57.360
So if you're actually thinking about how the PYMK is rendered, we talked about neural

32:57.360 --> 32:59.720
real time candidate generation.

32:59.720 --> 33:05.560
So what happens is if Sam comes to his PYMK page, we have, we need to look at features

33:05.560 --> 33:11.520
for Sam, but then we may have a few thousand candidates to rank for Sam.

33:11.520 --> 33:16.840
So then we have to look up the, all of the features for those few thousand candidates.

33:16.840 --> 33:23.160
So then we're pounding a key value store at our PQPS for that many candidates.

33:23.160 --> 33:28.720
Then you start hitting against systems challenges to actually retrieve all of those features

33:28.720 --> 33:31.040
at that rate.

33:31.040 --> 33:38.400
So both at the data munging side for just offline analysis or feature engineering, some of

33:38.400 --> 33:46.560
these graph features can be computationally complex and we, you're doing a lot of large

33:46.560 --> 33:47.640
joints.

33:47.640 --> 33:54.960
So we actually become smarter at the way we do joints, just simple, sequil it like pick

33:54.960 --> 34:00.520
joints don't necessarily work that well for us at the scale that we're at.

34:00.520 --> 34:04.640
And then again, online inference is the other piece in terms of those joints got what

34:04.640 --> 34:05.640
does work.

34:05.640 --> 34:07.360
Oh, what does work?

34:07.360 --> 34:14.360
I think the two problems with, with joins are, are query planning.

34:14.360 --> 34:18.440
So typically if you talk about a graph, it's represented as a table of edges.

34:18.440 --> 34:22.480
This is a very, very easy to do in any SQL database.

34:22.480 --> 34:27.680
However, when you start to query it, you're doing self joints.

34:27.680 --> 34:32.520
And the problem with self joints is that the optimizer is based on statistics.

34:32.520 --> 34:39.520
And after your first round of projections, you no longer have any statistics.

34:39.520 --> 34:44.560
This gets exacerbated by the fact that graphs tend to have a lot of skew in them.

34:44.560 --> 34:49.680
That is some nodes are very, very dense and others are very sparse.

34:49.680 --> 34:55.360
And so basic techniques like, oh, I'll just take an average, don't work very well.

34:55.360 --> 35:03.480
You can be looking at things, oh, this guy has 200, 150, 113, 25 million.

35:03.480 --> 35:09.320
So if you have to look at 25 million things, you have to pay that price.

35:09.320 --> 35:12.440
But if you're in a complex query, maybe you don't have to.

35:12.440 --> 35:17.960
Maybe that highly skewed node will be knocked out by some other condition.

35:17.960 --> 35:26.000
So you need an optimization framework that can respond to that problem of skew.

35:26.000 --> 35:30.680
And then, so this typically shows up in offline jobs where you have a hot, you have a hot

35:30.680 --> 35:31.840
shard.

35:31.840 --> 35:37.400
All the other shards are done and one shard is just spinning at 100% CPU or it runs out

35:37.400 --> 35:39.080
of RAM.

35:39.080 --> 35:42.520
That's a very, very typical problem.

35:42.520 --> 35:50.320
And the query planning we solve with a dynamic planner rather than a static planner.

35:50.320 --> 35:53.440
So typical SQL database, you give it the query.

35:53.440 --> 35:56.640
It looks at the statistics of the tables that are referred to.

35:56.640 --> 36:02.480
It comes up with a plan and then it just executes the plan without change.

36:02.480 --> 36:09.440
What we do is we look at the query, look at the sizes of sets, which we can determine

36:09.440 --> 36:13.760
in constant time and we do the cheapest thing.

36:13.760 --> 36:18.960
And then we see what that's done to the sizes of sets and we do the next cheapest thing.

36:18.960 --> 36:23.680
So if we're pursuing a key brand and it turns out to be expensive, we'll go look at other

36:23.680 --> 36:25.080
keeper things.

36:25.080 --> 36:30.800
So we'll change the query plan in midstream as it were.

36:30.800 --> 36:36.280
The hot shard problem that you were mentioning is that something that you deal with kind

36:36.280 --> 36:45.760
of operationally or is that something that does this kind of dynamic element of the underlying

36:45.760 --> 36:47.920
platform know how to deal with that?

36:47.920 --> 36:53.480
It's something that would, I mean, so in liquid the graph database, it's something that

36:53.480 --> 36:55.760
our planner handles.

36:55.760 --> 37:00.240
So it's a problem that we don't have in the online world.

37:00.240 --> 37:05.080
In the offline world, it's just a thing that can happen, right?

37:05.080 --> 37:09.880
So very often people are grumbling about their offline job not finishing and oh, well,

37:09.880 --> 37:16.720
we need to give everybody more memory or we need to fiddle with the query plan for this

37:16.720 --> 37:20.840
particular thing that we're computing.

37:20.840 --> 37:23.440
It's just a bad thing that can happen.

37:23.440 --> 37:28.320
And when you say fiddle with the query plan, how much of that query planning is kind of

37:28.320 --> 37:35.360
hand-tuned like that versus the system is creating the query plan?

37:35.360 --> 37:37.960
It varies a lot.

37:37.960 --> 37:44.640
The tendency is for things to be more SQL-like, where somebody else figures out the plan.

37:44.640 --> 37:52.240
But a lot of people are still writing handwritten map-produced jobs, where you are actually,

37:52.240 --> 37:58.200
the human programmer is figuring out a plan that the computer program will exit.

37:58.200 --> 38:03.000
And the programmer has to realize, hey, if I want to talk about people who are following

38:03.000 --> 38:09.040
Richard Branson, I probably don't ask about the shard that has Richard Branson.

38:09.040 --> 38:13.720
I probably ask all the other shards like, hey, if you're connected, if anybody that you

38:13.720 --> 38:18.280
have is connected to Richard Branson, I want to do some stuff with that, right?

38:18.280 --> 38:23.680
So that's a typical way to work around that problem.

38:23.680 --> 38:27.880
Is that type of thinking natural for the kinds of engineers that are working on these

38:27.880 --> 38:28.880
problems?

38:28.880 --> 38:30.880
Or is there a lot of education?

38:30.880 --> 38:35.760
Yeah, I don't think it's natural for anyone.

38:35.760 --> 38:40.520
Graph databases, it takes a while to, there's no top or bottom.

38:40.520 --> 38:46.480
So I've always seen, it takes at least six months for people to come up to speed on the

38:46.480 --> 38:48.800
domain and really start being productive.

38:48.800 --> 38:51.200
That's like the new concurrency, right?

38:51.200 --> 38:53.760
It's like something you just have to wrap your head around.

38:53.760 --> 38:56.920
There's this fog and eventually you come out on the other side of the fog and it kind

38:56.920 --> 38:57.920
of makes sense.

38:57.920 --> 38:58.920
Yeah, yeah.

38:58.920 --> 38:59.920
I think so.

38:59.920 --> 39:08.440
I'm curious about if you're seeing anything happening on the hardware side of supporting

39:08.440 --> 39:09.440
graph databases.

39:09.440 --> 39:15.760
It strikes me that there's maybe an analogy in, there are a number of companies going

39:15.760 --> 39:20.960
at doing machine learning in Silicon and one of the big innovations there is the way

39:20.960 --> 39:26.400
where we do a lot of machine learning via TensorFlow and PyTorch and the like is

39:26.400 --> 39:32.280
by constructing computational graphs, so let's build Silicon that is better suited towards

39:32.280 --> 39:35.720
computational graphs as opposed to linear instruction sets.

39:35.720 --> 39:38.400
Now we're talking about graph databases.

39:38.400 --> 39:43.600
I don't think I've heard of like a graph server or graph hardware that's kind of tuned

39:43.600 --> 39:45.440
for supporting graph databases.

39:45.440 --> 39:49.880
Do you, have you heard of that or, and do you think that maybe I think at some point?

39:49.880 --> 39:56.520
I mean, honestly, for my standpoint, hardware has steadily gotten worse, instruction pipelines

39:56.520 --> 39:58.280
are longer.

39:58.280 --> 40:06.640
People tend to focus on, well, if you can do serial IO, then it's much, much faster.

40:06.640 --> 40:09.320
They want things to fit in the L1 cache.

40:09.320 --> 40:15.520
So the sweet spot for modern hardware is I have figured out how to chop things up into

40:15.520 --> 40:21.720
small enough pieces that one piece fits in the L1 cache and then I can use vectorized

40:21.720 --> 40:24.840
operations really fast.

40:24.840 --> 40:26.240
That's absolutely wonderful.

40:26.240 --> 40:29.160
Unfortunately, that's not what a graph is.

40:29.160 --> 40:32.160
A graph is really random access, right?

40:32.160 --> 40:34.880
There's no easy way to sort a graph.

40:34.880 --> 40:41.600
So for example, if I'm storing edges, well, the edge has, say, three fields, a subject

40:41.600 --> 40:45.440
predicate an object, I have to sort in one way.

40:45.440 --> 40:50.920
So if I sort it by subject, I can find all of your edges very easily.

40:50.920 --> 40:56.000
But when I start looking at objects at the other end, it's unsorted, right?

40:56.000 --> 41:03.960
So you really cannot escape the need for random access and unfortunately modern processors

41:03.960 --> 41:08.120
are very heavily pipelined and don't handle random access very well.

41:08.120 --> 41:13.920
So do you envision a hardware that's better, have you come across that?

41:13.920 --> 41:16.680
I think people are doing the best they can.

41:16.680 --> 41:19.320
It's very hard and expensive.

41:19.320 --> 41:21.240
So we have L1 memory.

41:21.240 --> 41:25.440
Why don't we just make L1 be like all of the RAM?

41:25.440 --> 41:27.240
It's very expensive to do that.

41:27.240 --> 41:32.040
Hey, man, when you think about the, you know, a lot of the challenges that we've been

41:32.040 --> 41:37.880
talking about are challenges that arise from kind of the scale at which LinkedIn is manipulating

41:37.880 --> 41:41.320
graphs and applying machine learning to graphs.

41:41.320 --> 41:44.320
Not a lot of people are operating at that scale.

41:44.320 --> 41:50.080
Do you think about kind of scaling down this kind of problem and like, you know, getting

41:50.080 --> 41:55.680
started thinking about applying machine learning to graph types of problems, what are some

41:55.680 --> 42:01.040
of the things that people can do to explore this area?

42:01.040 --> 42:08.560
There are definitely lots of graph datasets out in the public domain.

42:08.560 --> 42:11.840
The clue web corpus is a web graph in itself.

42:11.840 --> 42:12.840
Clue web.

42:12.840 --> 42:13.840
Okay.

42:13.840 --> 42:20.040
There are academic publications that reference protein interaction networks.

42:20.040 --> 42:22.520
You can think of those as graphs as well.

42:22.520 --> 42:26.920
So there's lots of embodiments of graphs in different subdomains.

42:26.920 --> 42:36.560
The set of problems that come to mind often is, it often boils down to link prediction,

42:36.560 --> 42:40.760
like, does a link between A and B exist.

42:40.760 --> 42:47.080
Another problem that actually many companies in particular financial institutions are generally

42:47.080 --> 42:53.680
interested in is interaction graphs, but thinking in terms of the problem of abuse.

42:53.680 --> 43:00.600
So the general problem of link prediction has many applications both in, you know, the

43:00.600 --> 43:07.080
public domain as well as, you know, which the niche, you know, corporate domains.

43:07.080 --> 43:13.040
So I think thinking about a newer classes of link prediction problems that go beyond

43:13.040 --> 43:21.360
random walks, bringing embedding like algorithms, basically deep learning algorithms, but that

43:21.360 --> 43:24.120
can operate at scale.

43:24.120 --> 43:27.200
So basically, how can we distribute the processing?

43:27.200 --> 43:32.520
Thinking about distributed graphs is itself a hard problem.

43:32.520 --> 43:40.120
The other piece that we didn't cover too much about, but I think for folks who are academically

43:40.120 --> 43:47.560
oriented, I would actually think about how do you do a B testing on a graph?

43:47.560 --> 43:57.320
Because in some sense, nodes are not IID when you expose a node to a particular treatment,

43:57.320 --> 43:59.560
its neighborhood is impacted too.

43:59.560 --> 44:05.000
So a very nice theoretical problem may be just, you know, taking a look at how we might

44:05.000 --> 44:08.280
actually do experiments on these graph structures.

44:08.280 --> 44:11.960
So IID independent, identically distributed.

44:11.960 --> 44:17.120
So where's the, what's the missing part of the academic literature in this area?

44:17.120 --> 44:26.720
So think about this today when most companies do a B testing or they definitely, they partition

44:26.720 --> 44:32.040
their user base into random buckets, right?

44:32.040 --> 44:37.640
But when your users are connected by links, right?

44:37.640 --> 44:42.360
So for example, let's take the example that you and I are connected and you share an

44:42.360 --> 44:46.320
article and I view the article.

44:46.320 --> 44:53.640
If we're in two different buckets for an AB test, I'm not really independent of you.

44:53.640 --> 44:56.320
So that assumption completely breaks apart.

44:56.320 --> 45:00.440
So how do we actually tackle this problem?

45:00.440 --> 45:09.000
I think that's something you can study even in a smaller graph in a simulation like framework.

45:09.000 --> 45:13.200
So I would, you know, sort of love to toss that problem out at the academics.

45:13.200 --> 45:19.120
It's got any requests from your part for the academic community.

45:19.120 --> 45:22.720
I think, I don't really have requests.

45:22.720 --> 45:25.560
You were talking about, you know, how do you process this at small scale?

45:25.560 --> 45:26.560
Yeah.

45:26.560 --> 45:32.160
I think one of the, one of the insights here is it's actually pretty economic to have a

45:32.160 --> 45:35.000
lot of RAM, right?

45:35.000 --> 45:40.960
And so if I was, if I was wanting to do a graph work at a smaller scale, where a small

45:40.960 --> 45:47.960
probably means, you know, up to three terabytes, I would just focus on a single machine and

45:47.960 --> 45:50.360
use, use RAM.

45:50.360 --> 45:57.160
It's, I think you can buy a one and a half terabyte machine from Dell at the rack price for

45:57.160 --> 45:59.960
20,000 bucks, 25,000 bucks.

45:59.960 --> 46:01.760
It's really quite economical.

46:01.760 --> 46:06.720
And if you didn't have access to liquid, what graph database would you be exploring?

46:06.720 --> 46:07.720
That's a harder one.

46:07.720 --> 46:12.400
I know the guys at Franz, who, who build a leg rat graph.

46:12.400 --> 46:18.240
And I think that's, to me, probably the most interesting of the extent graph databases.

46:18.240 --> 46:22.560
Well, Hema Scott, thank you so much for chatting with me.

46:22.560 --> 46:24.040
Thank you so much for having us here.

46:24.040 --> 46:26.560
Yeah, thank you very much.

46:26.560 --> 46:32.480
All right, everyone, that's our show for today.

46:32.480 --> 46:37.800
For more information on Hema, Scott, or any of the topics covered in today's show, visit

46:37.800 --> 46:42.480
twimmalei.com slash talk slash 236.

46:42.480 --> 46:45.200
Thanks once again to LinkedIn for their support.

46:45.200 --> 46:50.320
Be sure to check out what their engineering team is up to at engineering.linkedin.com slash

46:50.320 --> 46:51.840
blog.

46:51.840 --> 47:02.800
As always, thanks so much for listening and catch you next time.

