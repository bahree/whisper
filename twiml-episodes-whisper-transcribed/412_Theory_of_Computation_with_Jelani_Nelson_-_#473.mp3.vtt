WEBVTT

00:00.000 --> 00:28.000
All right, everyone. I am here with Jalani Nelson. Jalani is a professor in the theory group at UC Berkeley. Jalani, welcome to the Twoma AI podcast. Thanks for having me. Hey, I'm super excited about our conversation and looking forward to learning more about what you're up to.

00:28.000 --> 00:43.000
You focus on theory, computational theory in particular, and we'll be digging into that topic. But before we do, I'd love to hear a bit about your story and how you came to work in computational theory and machine learning.

00:43.000 --> 00:46.000
Tell us a little bit about your background.

00:46.000 --> 01:00.000
Sure. So I'm from St. Thomas and the US Virgin Islands. I'd say I first got into computer science as a kid because of interesting video games. I got an Nintendo entertainment system for my fourth birthday.

01:00.000 --> 01:11.000
And sometime in middle school, I learned HTML. I taught myself read a book on it and later in high school, I taught myself C++. And I learned some basic from my CS teacher in 12th grade.

01:11.000 --> 01:22.000
I really liked CS. I knew I liked CS from what I had read in those books. I knew I liked math. You know, in middle school, we had math counts in the Virgin Islands and I did math counts.

01:22.000 --> 01:29.000
And I like just reading my math textbook. I mean, even elementary school, I remember I would just go ahead and read my math textbook.

01:29.000 --> 01:47.000
By the second month of the school year, I had already read the entire math textbook for the year and done all the workbook exercises. So, but when I was entering college, I decided, even in high school, I decided when I go to college, I'm going to double major in math and CS.

01:47.000 --> 02:03.000
And I went to MIT. I had actually never heard of MIT until senior year. I found out about it through a college ranking magazine. I think US news. And I only applied there because I didn't even realize that college admissions was competitive.

02:03.000 --> 02:10.000
I was very oblivious, but it worked out. I got in. I went there. I did double major in math and CS just as I said I would.

02:10.000 --> 02:25.000
And sophomore year was my first 3CS course. It was an algorithms course. And I enjoyed the course, but I would say that's not really where that's not the time that I decided I'm going to do 3CS.

02:25.000 --> 02:40.000
I was still exploring other things. I was taking computer math courses as well, et cetera. But junior year, I started doing online competitive programming. So MIT attracts a lot of kids who did competitive math and competitive programming in high school.

02:40.000 --> 02:45.000
Things like the IMO and the IY. These are the Olympia. It's math Olympia and Informatics Olympia.

02:45.000 --> 02:59.000
And some of my friends did those things and a couple of my friends who were into competitive programming told me about top coder. Nowadays it's not as popular nowadays code forces is the big thing, but back then it was top coder.

02:59.000 --> 03:16.000
It's a website that still hosts and used to host more frequently these 75 minute competitions where they would give you three problems, algorithmic problems, and you would have to code them up as fast as you can correctly pass all the test cases.

03:16.000 --> 03:22.000
And depending on how long it took you to solve the problem, you would get, you know, I would decide to make points you get. So the faster you solve it, more points.

03:22.000 --> 03:36.000
So that really got me hooked. You know, I was, as I mentioned, I was really into gaming as a kid playing video games since age four. So to me, top coder was just another game and there was a summer when I probably played top coder eight to 10 hours a day.

03:36.000 --> 03:44.000
I mean, even outside of the contests, I would just do practice problems on their website on other websites that had a lot of practice problems.

03:44.000 --> 03:54.000
And that's I think when I really thought, yeah, I want to do, I want to do theory CS. I did a masters of one year masters at MIT in algorithms and data structures.

03:54.000 --> 04:01.000
And then my PhD as stated MIT and I was in the theory group then. And I did algorithms.

04:01.000 --> 04:06.000
Awesome, awesome. And how did you work in theory intersect with machine learning?

04:06.000 --> 04:16.000
Yeah, so I work on, I guess we'll get into it a little bit, I think, after, but I work a lot on sketching and streaming and dimensionality reduction.

04:16.000 --> 04:31.000
And it turns out that some of these tools or some of these things are people in the ML community uses tools. So like, for example, random projections, the idea is you have a high dimensional data set. Maybe you took some database and featureized it.

04:31.000 --> 04:36.000
You know, you featureize your emails and turn them into a bag of words. So now they're high dimensional vectors.

04:36.000 --> 04:46.000
And you want to run some algorithm on them. Maybe you want to train some some classifier, for example, like a spam classifier.

04:46.000 --> 04:53.000
If you can map these high dimensional vectors to a lower dimensional representation while still preserving some kind of geometric structure.

04:53.000 --> 05:01.000
And then maybe the thing you learn will still perform well. And people have done that, for example, there's a paper and I think ICML09 by Weinberger at all.

05:01.000 --> 05:10.000
They were looking at training a collaborative spam filter for Yahoo Mail. So they use, I mean, they called it the hashing trick, but it's dimensionality reduction.

05:10.000 --> 05:22.000
So they use these kinds of tools to speed up, I guess, their training algorithm, because it only had to run on the lower dimensional version of the factorized the featureized emails.

05:22.000 --> 05:46.000
That's one example, I mean, there are others. Yeah, awesome. And so as a, as a theorist, are you primarily thinking about new algorithms and creating new approaches to solve problems or a lot of that work is identifying the limitations and the kind of performance envelope, if you will, of existing algorithms.

05:46.000 --> 05:48.000
How do you think about that balance?

05:48.000 --> 05:59.000
Yeah, so for me, I would say that the problem is king. So first, I identify what I think is an important problem. And once I do that, I try to understand.

05:59.000 --> 06:13.000
In my area of sketching and streaming algorithms, both upper bounds and lower bound. So upper bound means kind of getting an algorithm that's as good as possible for this problem, you know, minimize the memory, minimize the running time, minimize the failure probability, etc.

06:13.000 --> 06:21.000
It may be the case that there's already an algorithm that I think is optimal. That's great. But sometimes that's not the case. And oftentimes I have to design a new algorithm.

06:21.000 --> 06:30.000
And then the lower bound side is, as you mentioned, understanding limitations, proving that there is no algorithm that is better than something.

06:30.000 --> 06:47.000
And then hopefully, hopefully, you're done with your job when you've kind of squeezed the problem between the two, the upper and the lower bounds match. So you have an algorithm, you have a lower bound in the match, meaning you have found, maybe the asymptotically at least best algorithm for this problem.

06:47.000 --> 07:03.000
So let's dig into one of the areas that you spend a lot of time on, streaming and sketching. I think we're generally familiar with the idea of streaming and streaming systems.

07:03.000 --> 07:20.000
Sketching is a bit of a new terminology, but why don't you kind of walk us through the problem space as a whole and the way you approach it?

07:20.000 --> 07:41.000
It still allows you to answer some family of queries that you're interested in answering. So it's a data structure, really. But when you use that word sketch, the connotation is that this data structure should use some linear memory, meaning the memory footprint of the data structure should be much less than storing the database.

07:41.000 --> 07:56.000
You're not remembering most of the data. And streaming, you know, when I talk about streaming, I mean the data is streaming at you, and you want to maintain a sketch of that data on the fly as the data is streaming in.

07:56.000 --> 07:59.000
So kind of an updatable sketch.

07:59.000 --> 08:17.000
And I imagine that as you're approaching these kind of problems, you know, there are lots of different directions you could go algorithmically, but there's, you know, one set of approaches that you look at that's more probabilistic in nature.

08:17.000 --> 08:33.000
A lot of the algorithms I develop in this space are randomized. There are some problems for which you can prove that unless you use randomness, it's impossible to have a low memory footprint algorithm. You basically have to remember almost everything.

08:33.000 --> 08:45.000
And there are other problems for which you can get something non trivial deterministically without randomness, but you can get much more efficiency, if you allow yourself to be randomized.

08:45.000 --> 08:56.000
Can you give us some some concrete ideas of the types of problems or applications that streaming and sketching and your work in particular apply to.

08:56.000 --> 09:06.000
I'll give a couple of example problems. So one of them is counting. Okay, so I just want to maintain a counter, think of a counter as a data structure.

09:06.000 --> 09:15.000
So you're maintaining this number. What are the operations that you allow? One is you initialize it, meaning set to zero. So that's the beginning of the algorithm.

09:15.000 --> 09:33.000
Yes, as you increment the counter, and then the third is that you query the value of the counter. Okay, so if if the number you're counting gets to be large, you know, K digits large, the naive approach of just remembering the number would use, would consume K units of your memory.

09:33.000 --> 09:56.000
And this is an example where you can prove that any deterministic algorithm, if you don't use randomness, you need to, you need to have, you know, K units of memory. But if you allow, if you allow randomness and you allow approximation, meaning when you ask me to report the value of the counter, I'm allowed to slightly air, I'm allowed to give you an answer that say 1% off the true counter value.

09:56.000 --> 10:07.000
And it's allowed to be incorrect with some low probability with probability 99% I'll give you a value that's within 1% of the true value, but with 1% probability, I'm allowed to say total junk.

10:07.000 --> 10:21.000
Okay, yeah, if you allow for that kind of slack, then it's been known since the late 70s that there is a non trivial algorithm, instead of having to consume K units of memory for a K digit number, you only have to consume log K roughly log K units of memory.

10:21.000 --> 10:42.000
So this is a problem I've worked on, I mean, we knew about the log K from the late 70s, but what we didn't know was, what's the type relationship between the size that the number gets versus your memory and the approximation ratio you want, I said 1% error, but what if you want 10%, but if you want 0.1%, you know, how does the memory trade off with that, as well as the failure probability.

10:42.000 --> 10:53.000
You want to succeed even 99%, but again, you can parameterize that. How does your memory consumption relate to the optimal memory consumption relate to the failure probability success probability one.

10:53.000 --> 11:05.000
So now we have a very tight understanding of how all these parameters trade off with each other. That's an example of something I worked on and we just, we have a pre print on this from the fall, so less than a year ago.

11:05.000 --> 11:20.000
The idea, I'm imagining is that the resulting algorithm with the randomness approach applied is going to perform better than like a more naive quantization type of approach.

11:20.000 --> 11:35.000
Yeah, so I mean, the problem with, if you said, yeah, I'm not sure exactly what you have in mind when you say quantization, but let's say dropping digits.

11:35.000 --> 11:50.000
Because someone says increment your counter. So let's say that you don't remember the last three digits. So someone says increment the counter. So that fourth digit, did it stay the same or does it get incremented now, right?

11:50.000 --> 11:55.000
You don't know because you don't remember, you know, do you end in 999 or not? You don't remember, right?

11:55.000 --> 12:05.000
So yeah, you have to do something, you know, a little more. But yeah, so I mean the basic idea of the algorithm, I mean, I'll say it maybe the sentence not get to into it, but.

12:05.000 --> 12:13.000
Yeah, this is an algorithm to do more is from the late 70s. The idea is you try to remember something that's like the logarithm of the number.

12:13.000 --> 12:25.000
And when, when someone says increment, with some probability, you increment that logarithm. And with, with the remaining probability, you just keep it the same.

12:25.000 --> 12:31.000
So if there have been a lot of increments, then with high likelihood, you have incremented the logarithm, right?

12:31.000 --> 12:45.000
But if there were only a few, then probably didn't change. I mean, so that's the basic idea. Got it. And in terms of applications of this, where do you see that this kind of approach being applied?

12:45.000 --> 12:53.000
Or is it a theoretical exercise that you doesn't have any application specifically just yet?

12:53.000 --> 13:07.000
I know it had applications from the very beginning. Actually, when Morris developed this, he was at Bell Labs. That was in the 70s. There was a, there's a survey by Jeremy Lombroso that talks about the history of this algorithm or the history of this problem.

13:07.000 --> 13:11.000
So I'm quoting Jeremy or paraphrasing it.

13:11.000 --> 13:26.000
So my understanding is that in the 70s, Rob Morris was working on a spell checker for early eunuch systems. And, you know, as part of a spell checker, he, he wanted to keep Trigram counts.

13:26.000 --> 13:36.000
So like how many times have I seen A, A and this text versus ABC versus A, B, A, et cetera, right? And I mean, I don't know exactly how is, how he was using those Trigram counts as part of a spell check.

13:36.000 --> 13:48.000
I guess the idea is, if there's a Trigram that's infrequent, and now someone type something that has an infrequent Trigram in it, maybe you should flag that as a possible type of.

13:48.000 --> 13:54.000
So, right. Well, how many Trigrams are there or something like 26 cubed?

13:54.000 --> 14:06.000
You know, back in the late 70s, computers didn't have as much memory to do today. So he couldn't, you know, if he had to keep track of 26 cubed counters, he just didn't have enough memory to keep accurate counts.

14:06.000 --> 14:11.000
So he needed to compress the counts somehow. And that's why he developed this approximate counter.

14:11.000 --> 14:28.000
And that's the more algorithm because that let him fit approximate counts inside the memory constraints at the time. So I mean, that's the late 70s. Why, why should anyone care about this today? The idea is still, let me look, like I can, my computer can store a number.

14:28.000 --> 14:32.000
Okay, and it's not a big deal or even a long, right? Yeah.

14:32.000 --> 14:42.000
The places where this gets used today are where you want to store a lot of numbers. Okay. And so there's that multiplicative overhead. Yes, this is a 64, this is only 64 bits for one number.

14:42.000 --> 14:52.000
But if I'm storing a billion numbers, then it's 64 times a billion. So if I can, if I can get rid of that multiplying constant 64 and make it, you know, eight or something that I've saved.

14:52.000 --> 15:07.000
So one place where this gets used is I found this YouTube talk by someone who contributes to the redis open source database. It's an in memory database. And there are many people who use redis apparently as a cache as an in memory cache.

15:07.000 --> 15:20.000
And then when you have a cache, you need a cache of fiction policy, right? When it gets fully to kick someone out. So one of the eviction policies they've implemented in redis is to evict the key that's least frequently used.

15:20.000 --> 15:30.000
So they need to keep track of the frequency of access of each key. And frequency involves you, you know, understanding the count. How many times is this key been an access in some time window, right?

15:30.000 --> 15:39.000
So they implement those counters using using the Morris algorithm, actually, because you've got a lot of keys, typically a lot of objects stored in one of these.

15:39.000 --> 15:43.000
Right. Yeah, for someone who has a big database. Yes.

15:43.000 --> 15:46.000
Interesting. Interesting.

15:46.000 --> 16:04.000
Awesome. And so tell us a little bit about the dimensionality reduction work that you do. Yeah, sure. So one of the flagship results in this area is by not computer scientists, but by mathematicians Johnson and London Strauss.

16:04.000 --> 16:16.000
They're analysts. So it was in 1984, they had a math application in mind. And for them, this wasn't even their main results, but they needed it to prove something else.

16:16.000 --> 16:19.000
It's the so called Johnson, London Strauss lemma.

16:19.000 --> 16:40.000
So what it says is, if you have N vectors in some arbitrarily high dimensional space, you can map those N vectors down to smaller dimension log N O of log N dimensions, such that all the pair wise Euclidean distances after you do the mapping are roughly what they used to be up to a constant factor.

16:40.000 --> 16:51.000
So that can be parameterized and you can trade off against it. So let's say I want to preserve all the distances up to 1% error, you know, the jail, I can give you that.

16:51.000 --> 16:58.000
And the target dimension depends on that 1% if you want to say you want 0.1% error, then, you know, the dimension has to increase.

16:58.000 --> 17:01.000
So that's a problem I've worked on.

17:01.000 --> 17:18.000
You know, how does that relate to, for example, ML, again, you know, I mentioned this with the emails, right, you, you, you feature eyes or data, now you have high dimensional vectors, you map down to low dimension and run whatever you want to run on those things are faster, you slow memory, et cetera.

17:18.000 --> 17:34.000
There are other applications of this approach of dimensionality reduction, for example, clustering of data on a cluster, I can map down to lower dimension cluster with lower dimensional vectors, and I can prove that the clustering I find is almost as good as the optimal clustering had I not done the dimensionality reduction.

17:34.000 --> 17:48.000
There are also applications to PCA, approximate PCA, as well as approximate regression, constrained regression problems. So, you know, this thing gets used in applications that people in ML care about.

17:48.000 --> 18:01.000
I guess some of the things I've worked on related to this, trying to understand exactly the trade off between the number of vectors, that approximation that you want to preserve, let's say 1% error, the distances are 0.1%.

18:01.000 --> 18:18.000
And the target dimension, how do these three quantities trade off against each other. So, now we have an optimal asymptotically optimal understanding of that, that some work I did in late 2017 with Casper Green Larson, and I've also worked on efficiency of the dimensionality reduction.

18:18.000 --> 18:31.000
So, you have these high dimensional vectors, you want to map to lower dimension, there's some computation that happens there and that computation takes time. How can we make that happen as fast as possible, do the embedding into load dimension as fast as possible.

18:31.000 --> 18:55.000
So, I've worked on that, getting faster embeddings, you know, a lot of these embeddings, I guess all the embeddings that people know are linear embeddings, meaning, you have your high dimensional vector, and you multiply it by some matrix that has few rows, and that gives you your lower dimensional vector. So, this matrix, you know, making it sparser, for example, that's one thing I worked on that way, since it's sparser, you can multiply it more quickly.

18:55.000 --> 19:14.000
Often it's randomized, you select this matrix randomly from some distribution, how many random bits do you need to sample this random matrix? If it's a few number of random bits, then you can actually call that the seed that generates the matrix, rather than store this large matrix in memory, you can also store the seed.

19:14.000 --> 19:20.000
So, you have some compressed representation of the embedding itself, so that's another thing I've worked on.

19:20.000 --> 19:42.000
When you think about the relationships between these various quantities, characteristics, they tend to be very complex relationships, or once you kind of complete your analysis, do you get to relationships that are fairly straightforward to apply?

19:42.000 --> 19:56.000
You mean like the trade-offs between these parameters? The trade-off between the parameters in particular, yeah? Yeah, so, I mean, for something like John Sullivan and Strauss, the jail limit itself from the 80s turns out to be optimal.

19:56.000 --> 20:05.000
So, that's a situation where we didn't have to come up with a new method. We just proved that the method that people had been using for 30 plus years was already optimal.

20:05.000 --> 20:24.000
So, what's the trade-off there? If the number of vectors that you want to embed, that you want to map down to learn dimension is n, and you want to preserve the distances up to 1 plus epsilon, multiplicative distortion, so if you say 1% error, that means epsilon is 0.01.

20:24.000 --> 20:38.000
Then what Jail showed, John Sullivan and Strauss showed is that the target dimension can be some constant times 1 of reps long squared times log n, and that's what we showed is optimal.

20:38.000 --> 20:44.000
There are data sets where you cannot do better than that.

20:44.000 --> 20:57.000
When you say there are data sets where you can't do better than that, how complex or limiting are the constraints that you find you have to put on the data sets?

20:57.000 --> 21:13.000
Yeah, so in our proof that there are data sets that you can't do better, we actually, I mean, it's kind of a funny proof in that we never actually can pinpoint which data set is the hard one.

21:13.000 --> 21:24.000
We just prove that there is one that exists that is hard, and there are things we can say about it. We know that the data set basically looks as follows.

21:24.000 --> 21:32.000
It contains the 0 vector, then a bunch of standard basis vectors, meaning it has a 1 in a single entry and the rest of the vector is 0.

21:32.000 --> 21:53.000
Then a bunch of very sparse vectors where each one of the vectors has exactly K, where K is something like 1 of reps long squared, it has exactly K values that are non-zero, and all those values have the same, they're like 1 over root K, and the rest of the vector is 0.

21:53.000 --> 22:02.000
We have some understanding of what the hard case looks like, but if you just give me an arbitrary data set, and you ask me, can you beat JL for this data set?

22:02.000 --> 22:19.000
The answer is maybe, and there is some work on that, starting again in the 80s, there was another mathematician named Gordon, who showed that if you do random projections, the kind of thing that JL was doing, then there are cases where you can beat 1 of reps long squared times log in.

22:19.000 --> 22:26.000
In particular, the log in term can be replaced by something that is a function of the database itself.

22:26.000 --> 22:37.000
So if your database is nice for some formal definition of nice, then you can get away with something much less there in terms of the target dimension.

22:37.000 --> 22:52.000
I am curious, as machine learning gets more and more quote unquote democratized, more accessible, the tools are more readily available.

22:52.000 --> 23:21.000
If you have thoughts on the core elements of theory that any practitioner should know, whether they studied it formally or not, are there things that jump out to you as elements of the theoreticians toolbox that enable you to, that would enable a practitioner to be more effective, productive, think more deeply about the problems they're working on, whatever your criteria might be.

23:21.000 --> 23:49.000
Yeah, so, I mean, there are many faculty who have been at universities asking this question of ourselves, because there is a standard toolbox we teach in our algorithms courses at the undergraduate level, you know, intro to algorithms, we teach dynamic programming, we teach, you know, flow problems, max flow, minimum cuts, and, you know, maybe the fast Fourier transform, et cetera.

23:49.000 --> 24:02.000
I mean, I think something like dynamic programming is something that people should really understand well and, you know, is widely, is a widely applicable applicable technique to optimization problems.

24:02.000 --> 24:09.000
Some of the other stuff, you know, it's, it's there more, you know, the lot of this stuff was designed.

24:09.000 --> 24:23.000
You know, these courses were designed a while ago, decades ago, and it's not, it's not totally clear that they're the right tools, if you're just thinking in terms of tools to be used and practice later, it's not clear that these are the right tools.

24:23.000 --> 24:29.000
I mean, they do, they do train you to think in a certain way, which I think is a useful way of learning to think.

24:29.000 --> 24:47.000
But in terms of tools, I mean, I do think that people should know about, like from the things I, you know, know about, the things that I work on, like sketching and dimensionality reduction, I think people should be aware that, you know, these tools exist, things like hyper log log, you know, for approximate distinct counting.

24:47.000 --> 25:01.000
And I think people do know, like I remember once I was actually in a bus in Orvis in Denmark, and I was trying to pay my bus fare, and I realized I didn't have enough Danish coins in my pocket to pay.

25:01.000 --> 25:10.000
And this guy is like, oh, I can pay for you, he pays for me, and then he mentions, I forgot exactly, maybe I can go right by the CS building, I was going to the university.

25:10.000 --> 25:21.000
And he mentioned that he was a computer scientist, and I said, oh, yeah, I, you know, I am as well, and I work on sketching algorithms, and he's like, oh, yeah, like hyper log log.

25:21.000 --> 25:29.000
So, you know, he's not, he's not a researcher, he doesn't work on sketching algorithms. So I think, I think some of this stuff is becoming more known, which I think is good.

25:29.000 --> 25:41.000
You know, there are, there are blog posts by big company, big tech companies like Facebook and Google talking about how they engineer and hyper log log to be used, you know, in house. So people are, people are using this stuff.

25:41.000 --> 25:51.000
There's a data sketches library that's put together by the main contributors are people who are at Verizon, formerly Yahoo, and people download, you know, that library and use it.

25:51.000 --> 26:00.000
So, yeah, I think distinct counties, basically, you see a sequence of elements in a stream, how many distinct elements did you see?

26:00.000 --> 26:09.000
Another one is like frequent items, again, I see a sequence of items in a stream, like words that are being searched, you know, search queries to Google or something like that.

26:09.000 --> 26:15.000
And now I want to know, what are the frequent items I saw, the frequent things that people have been querying. So there are algorithms for that as well.

26:15.000 --> 26:21.000
The problems I've worked on, by the way, I've worked on distinct counties, I've worked on frequent items.

26:21.000 --> 26:26.000
But yeah, I think these are, they're not just a theoretical interest, they're things that I think people, people use.

26:26.000 --> 26:30.000
Actually, I remember one more story.

26:30.000 --> 26:39.000
You know, I know a lot of people who were very early at Dropbox, because the person who the CEO right now was my year at MIT.

26:39.000 --> 26:48.000
I don't know him very well, you know, I met him like once or twice maybe, but some of us early hires though are people who I know quite well.

26:48.000 --> 26:59.000
And back when I was in grad school, maybe this is almost 15 years ago, whenever I went to San Francisco, I would stop by Dropbox back then it was a small company, right.

26:59.000 --> 27:08.000
So I just like walk in and meet one of my friends, Dan, one of my friends, Dan used to work there. And you know, grabbed dinner with him or lunch with him.

27:08.000 --> 27:22.000
And I remember one time I was talking to one of their software engineers and, you know, he was talking about exploring the usage of frequent items algorithms to detect popular files that were being stored on Dropbox.

27:22.000 --> 27:33.000
So they're actually doing that now. But yeah, I mean, people are people are aware of these tools and are and use them. So I think that's good.

27:33.000 --> 27:46.000
Maybe switching gears a bit. You also run a nonprofit. Tell us a little bit about the nonprofit and your effort there and some of the story of Addis Coder.

27:46.000 --> 27:55.000
Yeah, sure. So, as you mentioned, it's called Addis Coder. It's a summer camp, four weeks each summer, 40 hours a week, Monday through Friday, nine to five.

27:55.000 --> 28:06.000
We bring in kids from all over Ethiopia. I mean, I should say we also work with a partner organization in Ethiopia. It's called the Melisinawe Foundation. So we kind of coordinate it together.

28:06.000 --> 28:15.000
We bring kids from all over Ethiopia. They come to the capital city. They're provided free room and board. The program is free as well.

28:15.000 --> 28:26.000
A lot of these kids have no CS exposure. Computer science is not part of the standard curriculum for high schoolers in Ethiopia. So we first have to teach them some basic programming and Python.

28:26.000 --> 28:35.000
And then we get into algorithms. Really, you know, really the program is trying to teach them about algorithms. But it's hard to do that when they don't even know what an if statement is.

28:35.000 --> 28:47.000
So we do a little basic programming in the beginning just so that they know enough that we can teach them the algorithms. It's very hands on, you know, classes eight hours a day, but only maybe two hours of that time is lecture.

28:47.000 --> 29:03.000
And there's a lunch break for an hour. And then the rest of the time, like, five hours, they're in computer lab, solving exercises. It's flipped classroom style. We get a lot of TAs, like 40 TAs per summer, kind of walking around the computer labs and helping the kids as they get stuck.

29:03.000 --> 29:14.000
So, yeah, I mean, I think it's had impact, you know, getting exposure to the kids on top of just teaching them the material. We also, we have a career date.

29:14.000 --> 29:25.000
So many of the kids say, OK, this, this seems cool, but like, what am I going to do with this? So we want to show them that there are interesting career paths with a CS background.

29:25.000 --> 29:36.000
And I want to show them like that these career paths exist locally too. I mean, it's one thing to bring in some Facebook software engineer to talk about Facebook, but there, you know, there is no Facebook office in Ethiopia, right?

29:36.000 --> 29:49.000
So we try to bring in people who are using CS locally. So for example, Ethiopian Airlines has been one of our sponsors and the head of their, you know, software engineering division came and gave a presentation to our students.

29:49.000 --> 29:58.000
We've had startup founders come in and talk to our students. There's an IBM research Africa. They have an office in Kenya and they also have an office in South Africa.

29:58.000 --> 30:04.000
We've had a researcher flying from Kenya to talk to our students about, you know, what it's like to do research in CS.

30:04.000 --> 30:19.000
So we've had a lot of people from industry, as well as government, as well as academia, talk to the kids. And I think it's really shown them that there's a lot of opportunity in CS. You know, we do survey, we do surveys amongst our kids like exit surveys.

30:19.000 --> 30:25.000
We ask them, we ask them things like, what did you want to major in before you started the program? What do you want to major in now?

30:25.000 --> 30:35.000
The number of kids who say CS, the number of kids who want to do CS at the beginning versus the number of kids who want to do CS at the end of the program is like three acts, you know.

30:35.000 --> 30:44.000
So I think it's an impact. A lot of our kids now, the majority stay in Ethiopia and study there. We've had kids come to the US to study.

30:44.000 --> 30:58.000
The first iteration of the program was 10 years ago, 2011. So there have been a lot of batches over the years. We've had, I think four kids, four kids go to MIT so far. A fifth one just got accepted two days ago.

30:58.000 --> 31:11.000
We have three kids, just this cycle, we're going to start at Harvard in the fall. They got accepted to Harvard and they're all going to go. And we've had kids go there in the past and to Princeton and to Stanford. We have one kid at Stanford right now.

31:11.000 --> 31:19.000
So we've had, I don't know, like a Columbia, et cetera, et cetera. And the older kids, if you look at the kids who did the 2011 version of the program, some of them are doing PhDs.

31:19.000 --> 31:29.000
So we've had two kids go to Columbia for PhD, a kid at USC right now doing computer security, he's a PhD student. We have another one in North Carolina.

31:29.000 --> 31:36.000
We have one at RPI doing a math PhD, Stony Brook doing an applied math PhD, and I'm probably forgetting people.

31:36.000 --> 31:50.000
Yeah, I think it's been going well. And also in industry, we've had our alums go to Google, Facebook, Microsoft, AMD, YouTube, which I guess is part of Google, et cetera. And people do, you know, also going into industry and Ethiopia as well.

31:50.000 --> 31:52.000
Yeah, very cool.

31:52.000 --> 32:09.000
So what have you learned about CS education in running this program over 10 years, what, you know, in terms of observations on how to actually teach CS and get folks excited about it. It gets excited about it.

32:09.000 --> 32:25.000
So one thing for one thing, let me preface by saying that these kids are not like the way our curriculum works might not work for the general public because these kids are, you know, they're not the average kid, right.

32:25.000 --> 32:39.000
So Ethiopia has a population of 110 million plus and we work with the government and we have our own application website to really select like the top top kids in the whole country.

32:39.000 --> 32:44.000
So giving an example, Ethiopia has a national exam at the end of 12th grade.

32:44.000 --> 32:59.000
And it's like think like the SAT, right. To get into Ethiopia University, but unlike the SAT where many kids each year get a perfect score, these exams are designed to be hard enough that they really do distinguish people at the very top.

32:59.000 --> 33:04.000
So there's usually a unique person in the whole country who has the first high score.

33:04.000 --> 33:19.000
There's a unique person with the second highest score, etc. No one ever gets a perfect score in the exams to arm. So we've had at least three kids, maybe more, be in the top three in the whole country who came through our program, right.

33:19.000 --> 33:37.000
But really, really, really strong kids. So we don't do as much, maybe hand holding or, you know, I mean, we really throw everything at them. Like we're teaching them stuff that I was, when I was at Harvard, I used to be in the faculty at Harvard for six years before I moved to Berkeley.

33:37.000 --> 33:51.000
So the kind of stuff I was teaching the kids at Harvard software year, I mean, I wasn't giving them the same intensity, but I was giving them pretty similar stuff to these to these Ethiopian high school students.

33:51.000 --> 34:11.000
So in terms of getting them excited though, that part of the reason I chose algorithms instead of just like teaching them how to code is, you know, I'm excited about algorithms. So maybe I hope that like my excitement is contagious and infects them and makes them excited.

34:11.000 --> 34:23.000
And I tell them about things that I think a high schooler wouldn't even realize, well, to realize to think about like, for example, you know, one of my lecturers is on multiply integers.

34:23.000 --> 34:29.000
Okay. So we all learned how to multiply integers when we were little kids, right, in elementary school or before.

34:29.000 --> 34:41.000
And we learned the so-called grade school algorithm, you write one number on top, the other number on the bottom, and then you start with the last digit, you multiply through, you write it down, you go to the next digit, you multiply through, you write it down, etc.

34:41.000 --> 34:49.000
And then you add everything up at the end. So if the two numbers are multiplying each have end digits, then the total runtime of this is n squared.

34:49.000 --> 35:01.000
So I teach them that actually this is not the best algorithm, you know, something like 50 years ago, there was this Russian math circle run by this guy, Kolmogorov.

35:01.000 --> 35:09.000
And he believed that the n squared algorithm was the best possible and he didn't know how to prove it. So he asked his colleagues to try to prove it.

35:09.000 --> 35:19.000
And I came back, this guy named Karatsuba, also a Russian mathematician, and showed Kolmogorov that actually n squared is not a limitation for multiply integers.

35:19.000 --> 35:27.000
There's an algorithm whose runtime is something like n to the 1.585, specifically it's n to the log base 2 of 3.

35:27.000 --> 35:35.000
Okay. And I teach them that algorithm. I teach them Karatsuba's algorithm to these high school kids. And, you know, and there, there have been improvements since then, right.

35:35.000 --> 35:42.000
So nowadays, you can multiply integers in almost linear time, almost and log in time.

35:42.000 --> 35:53.000
But yeah, I mean, I think it kind of maybe blows their minds that, oh, like this thing that I've known how to do since I was a little kid is actually not the best way to do something.

35:53.000 --> 36:05.000
Awesome. Awesome. So it sounds like you still, you're still active and like during the kids each year. Yes, I mean, last year we had a cancel. Unfortunately, you do the pandemic. But yes, it's an annual thing now.

36:05.000 --> 36:25.000
Awesome. Well, Zalani, thanks so much for joining us and sharing a bit about what you're up to. Thanks for having me.

