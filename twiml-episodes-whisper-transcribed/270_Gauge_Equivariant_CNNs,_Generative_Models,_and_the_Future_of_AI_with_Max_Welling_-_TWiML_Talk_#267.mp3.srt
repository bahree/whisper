1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,040
I'm your host, Sam Charrington, before we dive in, a quick community update.

4
00:00:35,040 --> 00:00:40,240
The Twimble study group is added again for the next four weeks, starting May 25th, we'll

5
00:00:40,240 --> 00:00:45,240
be diving headfirst into Peter Abiel's full stack deep learning course.

6
00:00:45,240 --> 00:00:50,480
This course is a great complement to the fast.ai courses we've done so far and covers practical

7
00:00:50,480 --> 00:00:56,600
topics, like problem formulation, data acquisition and preparation, establishing the right frameworks,

8
00:00:56,600 --> 00:01:01,480
platforms and compute infrastructure, debugging and ensuring reproducibility, and deploying

9
00:01:01,480 --> 00:01:03,680
and scaling your models.

10
00:01:03,680 --> 00:01:08,720
For more information or to register for this study group, visit twimbleai.com slash full

11
00:01:08,720 --> 00:01:09,880
stack.

12
00:01:09,880 --> 00:01:14,800
I can't thank our volunteer study group hosts enough for all their hard work, a huge shout

13
00:01:14,800 --> 00:01:20,400
out to Michael, Christian, Kai, Sanyam, Joseph, Dinesh and everyone else that's been involved

14
00:01:20,400 --> 00:01:24,760
in making this group happen.

15
00:01:24,760 --> 00:01:29,000
A hearty thanks to our friends at Qualcomm for sponsoring today's show.

16
00:01:29,000 --> 00:01:33,000
As you'll hear in the conversation with Max, Qualcomm has been actively involved in

17
00:01:33,000 --> 00:01:38,880
AI research for well over a decade, leading to advances in power efficient on device AI,

18
00:01:38,880 --> 00:01:44,800
as well as an algorithm such as Bayesian deep learning, grass CNNs, gauge, echrovariant CNNs

19
00:01:44,800 --> 00:01:46,200
and more.

20
00:01:46,200 --> 00:01:50,680
Of course, we know Qualcomm powers some of the latest and greatest Android devices with

21
00:01:50,680 --> 00:01:53,400
their Snapdragon chipset family.

22
00:01:53,400 --> 00:01:57,640
From this strong foundation in the mobile chipset space, Qualcomm now has the goal of scaling

23
00:01:57,640 --> 00:02:01,600
AI across devices and making AI ubiquitous.

24
00:02:01,600 --> 00:02:06,520
In this vein, a product I'm particularly looking forward to is their Cloud AI 100 line

25
00:02:06,520 --> 00:02:11,480
of data center inference chips, which I learned about at the recent press launch.

26
00:02:11,480 --> 00:02:16,680
To learn more about what Qualcomm is up to, including their AI research platforms and developer

27
00:02:16,680 --> 00:02:24,920
tools, visit twimmelai.com slash Qualcomm.

28
00:02:24,920 --> 00:02:27,880
Alright everyone, I am on the line with Max Welling.

29
00:02:27,880 --> 00:02:33,360
Max is a research chair in machine learning at the University of Amsterdam, as well as

30
00:02:33,360 --> 00:02:39,280
vice president of technologies at Qualcomm and a fellow at the Canadian Institute for

31
00:02:39,280 --> 00:02:41,680
advanced research or C-FAR.

32
00:02:41,680 --> 00:02:44,520
Max, welcome to this week in machine learning and AI.

33
00:02:44,520 --> 00:02:46,680
Thank you very much Sam.

34
00:02:46,680 --> 00:02:50,000
It's great to get a chance to chat with you Max.

35
00:02:50,000 --> 00:02:56,560
You're quite accomplished in the field of machine learning and I'd love to understand what

36
00:02:56,560 --> 00:03:02,400
drew you to AI and how you kind of started your career in this area.

37
00:03:02,400 --> 00:03:08,280
I actually started not in AI, I started in physics, so I did my PhD thesis in theoretical

38
00:03:08,280 --> 00:03:15,240
physics and that was fun, but it was also somewhat abstract in the sense that I couldn't

39
00:03:15,240 --> 00:03:20,640
see how I could have a major impact with that and so I decided I wanted to change to

40
00:03:20,640 --> 00:03:26,040
some a field that was a bit more, you know, less static and more dynamic at that point

41
00:03:26,040 --> 00:03:27,040
in time.

42
00:03:27,040 --> 00:03:33,080
And so I wanted to do something like neuroscience, which I thought was a great choice.

43
00:03:33,080 --> 00:03:41,680
And so I applied to Caltech to Pietro Perona's lab, but I basically ended up doing computer

44
00:03:41,680 --> 00:03:47,840
vision there and then later when I went to London, I worked with Jeff Hinton, I was doing

45
00:03:47,840 --> 00:03:48,840
machine learning.

46
00:03:48,840 --> 00:03:56,280
So I sort of migrated my andered from physics through computer vision to machine learning.

47
00:03:56,280 --> 00:04:02,480
And there, I really found a home because I thought, okay, this is going to grow big in

48
00:04:02,480 --> 00:04:09,880
the future, which actually happened and there's so many applications with which this field

49
00:04:09,880 --> 00:04:14,640
can impact the world in a positive way that I sort of decided to stay there.

50
00:04:14,640 --> 00:04:20,400
That first jump into computer vision, was that computer vision applied to neuroscience

51
00:04:20,400 --> 00:04:21,400
in some way?

52
00:04:21,400 --> 00:04:28,800
No, it was just computer vision basically analyzing images on a computer screen, basically,

53
00:04:28,800 --> 00:04:29,800
right?

54
00:04:29,800 --> 00:04:35,120
So it didn't have anything to do with neuroscience, although I was, you know, it was fascinated

55
00:04:35,120 --> 00:04:41,840
by that question and I, you know, I was going to some meetings on neuroscience and, you

56
00:04:41,840 --> 00:04:44,720
know, integrated somewhat with neuroscience in Caltech.

57
00:04:44,720 --> 00:04:49,040
In the end, I was doing computer vision, which was great too.

58
00:04:49,040 --> 00:04:51,080
Okay, okay.

59
00:04:51,080 --> 00:04:58,680
And I get a chance to talk to a lot of folks that started their career in physics.

60
00:04:58,680 --> 00:05:05,360
A lot of folks more on the applied side, whether astronomy or, you know, dealing with things

61
00:05:05,360 --> 00:05:11,320
like, you know, some folks that work at using colliders and things like that, but you are

62
00:05:11,320 --> 00:05:14,720
more on the theoretical side.

63
00:05:14,720 --> 00:05:15,720
That's right.

64
00:05:15,720 --> 00:05:21,760
Yes, I was doing my thesis in 2 plus 1 dimensional quantum gravity, so it was a very abstract

65
00:05:21,760 --> 00:05:22,760
and theoretical.

66
00:05:22,760 --> 00:05:28,880
It gave me a good, you know, basis for mathematics, but it wasn't very applied, that's true.

67
00:05:28,880 --> 00:05:33,440
And so at some point along the line, you started a company, Cypher.

68
00:05:33,440 --> 00:05:37,560
When was that in your career and what was the inspiration for the company?

69
00:05:37,560 --> 00:05:40,840
Yeah, so that was actually when I came back to the Netherlands.

70
00:05:40,840 --> 00:05:47,520
So I had my career was in North America and a little bit of time in London until about

71
00:05:47,520 --> 00:05:52,400
six years ago when I decided to come back to the Netherlands.

72
00:05:52,400 --> 00:05:56,480
And fairly quickly after that, we founded this company.

73
00:05:56,480 --> 00:06:02,880
It was an interesting story because we were working with a big Dutch bank and they wanted

74
00:06:02,880 --> 00:06:09,080
to do a competition to predict, you know, what ads, you know, customers would want to

75
00:06:09,080 --> 00:06:11,760
click on when you offered them.

76
00:06:11,760 --> 00:06:16,400
And they were doing that with a bunch of big sort of consultancy companies.

77
00:06:16,400 --> 00:06:20,120
And we were basically, I was asked to basically arbitrage, right, to make sure that, you know,

78
00:06:20,120 --> 00:06:22,760
everything went right and the competition was fair.

79
00:06:22,760 --> 00:06:27,640
And in order to do that, I just asked one of my master's student, Taco Cohen, who was

80
00:06:27,640 --> 00:06:33,200
also working at Qualcomm at this point, to, you know, to also implement some models.

81
00:06:33,200 --> 00:06:38,480
And he did that on his laptop, laptop with a single GPU in it.

82
00:06:38,480 --> 00:06:43,120
And then we looked at the results, you know, we basically, we beat all the big companies

83
00:06:43,120 --> 00:06:47,800
and the big banks that, you know, if that's the case, then you get the job.

84
00:06:47,800 --> 00:06:53,520
And so that was the, that was the start of our company, right, I, I then, so then two

85
00:06:53,520 --> 00:06:58,040
other people joined, I'm a block of work and you're a Sunday, you're a son of was an

86
00:06:58,040 --> 00:07:01,120
old physics pal of mine.

87
00:07:01,120 --> 00:07:06,040
And that's what started the company and, and from there, you know, five, five good years

88
00:07:06,040 --> 00:07:12,040
running, we did lots of consultancy with companies and, you know, looked at their AI problems.

89
00:07:12,040 --> 00:07:15,640
And then Qualcomm came around and said, you know, you have a good sized group here, all

90
00:07:15,640 --> 00:07:19,360
very experienced people, nice practical attitude.

91
00:07:19,360 --> 00:07:24,080
And they wanted to acquire us and, you know, there was a couple of other companies also

92
00:07:24,080 --> 00:07:28,280
that were looking at us, but, you know, we went with Qualcomm.

93
00:07:28,280 --> 00:07:33,440
And at the time Qualcomm came around, were you looking to be acquired or were you ready

94
00:07:33,440 --> 00:07:39,400
to, to try something else or did it, you know, did the opportunity just present itself

95
00:07:39,400 --> 00:07:42,600
and you evaluated it on its terms?

96
00:07:42,600 --> 00:07:44,400
Yeah, we did the latter.

97
00:07:44,400 --> 00:07:48,120
So we weren't optimizing, you know, for being acquired or anything.

98
00:07:48,120 --> 00:07:51,720
It was just like, you know, we were having a lot of fun and some people actually thought,

99
00:07:51,720 --> 00:07:54,120
you know, that we should keep going and grow.

100
00:07:54,120 --> 00:07:57,320
Other people thought, you know, maybe it's, you know, maybe it's actually interesting

101
00:07:57,320 --> 00:08:02,200
to join a big company and, you know, have a lot more, um, like a lot more power.

102
00:08:02,200 --> 00:08:05,080
You can, you can make a bigger impact if you're part of a big company.

103
00:08:05,080 --> 00:08:10,240
Um, and so it was basically an opportunity that we evaluated at that point and we thought,

104
00:08:10,240 --> 00:08:11,240
oh, is this good?

105
00:08:11,240 --> 00:08:12,240
We should try this.

106
00:08:12,240 --> 00:08:20,600
Uh, but prior to the primary focus of the company was on, uh, consulting, there wasn't

107
00:08:20,600 --> 00:08:23,120
a product being built or, or something along those lines.

108
00:08:23,120 --> 00:08:25,600
Yeah, we, we did actually build a product as well.

109
00:08:25,600 --> 00:08:29,900
Um, so there was an active learning tool, which was, uh, actually implemented the

110
00:08:29,900 --> 00:08:32,840
Tata steel, um, and it works as follows.

111
00:08:32,840 --> 00:08:34,880
So, um, you have an expert.

112
00:08:34,880 --> 00:08:39,360
So there was like cameras looking at slabs of steel, which were being produced and there

113
00:08:39,360 --> 00:08:42,920
was sometimes small little, uh, sort of defects on that steel.

114
00:08:42,920 --> 00:08:47,520
Um, we had a battery of cameras above it and, um, we were detecting these defects.

115
00:08:47,520 --> 00:08:52,360
Um, but, you know, there were a couple of classes and some of these defect classes were very

116
00:08:52,360 --> 00:08:53,360
rare.

117
00:08:53,360 --> 00:08:55,120
We didn't have a lot of data on them.

118
00:08:55,120 --> 00:08:58,000
And so the algorithm didn't perform very well.

119
00:08:58,000 --> 00:09:01,480
And so then the algorithm assessed itself and said, okay, so for the, for these types

120
00:09:01,480 --> 00:09:04,440
of, you know, images, we need more labels.

121
00:09:04,440 --> 00:09:06,960
And so that was then shipped to an expert.

122
00:09:06,960 --> 00:09:08,240
The expert would label it.

123
00:09:08,240 --> 00:09:12,880
It would go back into the system and the system would learn again and become better, gradually

124
00:09:12,880 --> 00:09:14,120
over time.

125
00:09:14,120 --> 00:09:17,520
So that's active learning where there's a human in the loop and the algorithm interacts

126
00:09:17,520 --> 00:09:21,320
with the human, um, and that was basically our product.

127
00:09:21,320 --> 00:09:22,320
Okay.

128
00:09:22,320 --> 00:09:27,600
Uh, I don't, I don't, I can't speak necessarily to the approach, but it sounds, uh,

129
00:09:27,600 --> 00:09:35,800
very similar, uh, at least from a problem domain and general direction to, uh, well, lots

130
00:09:35,800 --> 00:09:41,680
of folks are going after this area, but landing AI, Andrew Ng's company, uh, comes to mind

131
00:09:41,680 --> 00:09:49,480
is kind of tackling that similar kind of applying, uh, AI to industrial problems, including

132
00:09:49,480 --> 00:09:52,360
like defect detection, things like that.

133
00:09:52,360 --> 00:09:57,560
Um, was that, uh, was that product kind of one of a portfolio of, uh, uh, uh, uh, uh,

134
00:09:57,560 --> 00:10:02,960
challenges that you were, uh, going after or was that a big focus?

135
00:10:02,960 --> 00:10:04,600
It wasn't a big focus, actually.

136
00:10:04,600 --> 00:10:07,520
So this company was more opportunistic in that sense.

137
00:10:07,520 --> 00:10:14,520
So we, um, we basically talked to a lot of companies in a very diverse, uh, set of sectors.

138
00:10:14,520 --> 00:10:21,160
So from finance to, you know, to, uh, manufacturing, you know, to retail.

139
00:10:21,160 --> 00:10:24,720
And we basically, you know, we were very good at going in and talking to these people

140
00:10:24,720 --> 00:10:28,160
and say, okay, what is your, you know, what is your, what is your problem look like?

141
00:10:28,160 --> 00:10:29,920
What is your, where is your opportunity?

142
00:10:29,920 --> 00:10:31,160
What data do you have?

143
00:10:31,160 --> 00:10:35,520
And then we did a quick assessment and we made a recommendation and, uh, perhaps it's

144
00:10:35,520 --> 00:10:37,120
very quick demo.

145
00:10:37,120 --> 00:10:41,920
And then, um, and if they were happy, then we went for slightly longer sort of trial

146
00:10:41,920 --> 00:10:46,680
where we would actually, you know, implement, you know, a system to just show them that,

147
00:10:46,680 --> 00:10:51,000
you know, we can, you know, you can actually get value out of, out of the data that they

148
00:10:51,000 --> 00:10:52,080
have.

149
00:10:52,080 --> 00:10:55,560
And if they, they were still, you know, happy after that, and this was like maybe six

150
00:10:55,560 --> 00:10:59,120
months later, then we would go into a, you know, an actual implementation of the whole

151
00:10:59,120 --> 00:11:00,120
thing.

152
00:11:00,120 --> 00:11:04,840
And so we repeated this many times as many companies which gave the team an enormous

153
00:11:04,840 --> 00:11:09,000
amount of, uh, sort of experience with a very broad spectrum of problems.

154
00:11:09,000 --> 00:11:12,160
And I think that's what made the team also very attractive.

155
00:11:12,160 --> 00:11:20,480
Uh, so that acquisition was in 2017, you're a couple of years in now at Qualcomm.

156
00:11:20,480 --> 00:11:26,040
You know, this thesis of kind of the umph of a larger company, uh, creating some opportunities

157
00:11:26,040 --> 00:11:27,040
for you.

158
00:11:27,040 --> 00:11:28,440
Did that thesis bear out?

159
00:11:28,440 --> 00:11:36,320
The interesting part of working for Qualcomm for me, um, was that I, um, have always taken

160
00:11:36,320 --> 00:11:41,200
compute for granted, um, basically if I needed to compute something, then, you know, there's

161
00:11:41,200 --> 00:11:45,000
this computer, which has some chips in it and it will do its job.

162
00:11:45,000 --> 00:11:52,040
Um, but with the advance of deep learning, um, we see that, you know, bigger models, um,

163
00:11:52,040 --> 00:11:53,040
just perform better.

164
00:11:53,040 --> 00:11:56,320
And so compute becomes a really important part of the equation.

165
00:11:56,320 --> 00:12:02,480
Um, and so, you know, a good way to make progress, um, is to make sure that your algorithm

166
00:12:02,480 --> 00:12:04,240
actually runs extremely efficiently.

167
00:12:04,240 --> 00:12:09,760
And so of course, uh, you know, the, um, the GPUs came and, you know, they made compute,

168
00:12:09,760 --> 00:12:12,960
deep learning compute a lot more efficient, which is part of why things are going so

169
00:12:12,960 --> 00:12:18,440
well with deep learning, um, but this is a very fundamental problem of question, right?

170
00:12:18,440 --> 00:12:24,760
It's like, okay, so, you know, a brain doesn't compute, you know, with, uh, 32 bits precision,

171
00:12:24,760 --> 00:12:26,480
um, it's very noisy.

172
00:12:26,480 --> 00:12:32,320
Um, so should we, you know, change our compute paradigm in, you know, in our computer as

173
00:12:32,320 --> 00:12:33,320
well?

174
00:12:33,320 --> 00:12:37,920
Should we, you know, should we maybe try to train with a lot less precision, maybe

175
00:12:37,920 --> 00:12:40,120
a couple of bits precision?

176
00:12:40,120 --> 00:12:44,400
Um, and, um, do these neural networks have to be so huge, right?

177
00:12:44,400 --> 00:12:48,680
These neural networks typically have like even hundreds of millions of parameters, sometimes

178
00:12:48,680 --> 00:12:52,680
billions of parameters, do they necessarily have to be that big because that can, can

179
00:12:52,680 --> 00:12:56,480
choose a lot of, uh, memory and compute as well.

180
00:12:56,480 --> 00:13:01,440
Um, and also, you know, if you go a little deeper, if you dig a little deeper in the problem,

181
00:13:01,440 --> 00:13:06,120
then what you find is all these, all these parameters of these big models, they are living

182
00:13:06,120 --> 00:13:11,360
in, uh, sort of off-shift memory called DDR, and you have to bring them to the registers

183
00:13:11,360 --> 00:13:15,440
where you do all the Mac computes, and multiply and accumulate computes.

184
00:13:15,440 --> 00:13:20,040
Um, and that movement of data costs a lot of energy.

185
00:13:20,040 --> 00:13:23,160
And so, but in the brain, actually, it isn't so separated.

186
00:13:23,160 --> 00:13:28,720
We call that separation as a normal architecture, where memory and computer are separated.

187
00:13:28,720 --> 00:13:33,880
Um, but in, uh, in a brain, actually, what you find is that the memory and the computer

188
00:13:33,880 --> 00:13:39,800
are very close, right, because, you know, things are stored in these, uh, synapses, um, between

189
00:13:39,800 --> 00:13:44,200
neurons and, and the neurons compute, um, and so much more distributed.

190
00:13:44,200 --> 00:13:49,080
And, um, so, so there is a lot of fundamental questions of, can we, can we just change the

191
00:13:49,080 --> 00:13:55,600
compute paradigm, um, so that we can do things far more energy efficient, um, and that's,

192
00:13:55,600 --> 00:14:01,880
that's basically what I find the biggest thrill of working in Qualcomm in actually trying

193
00:14:01,880 --> 00:14:09,440
to make that happen, um, and then scaling up, uh, sort of AI computations, a lot, which

194
00:14:09,440 --> 00:14:13,640
might actually be, you know, one of the big reasons why we're making a lot of progress.

195
00:14:13,640 --> 00:14:17,600
And then, uh, yeah, basically shipping, whenever you have such a thing, and you build, you

196
00:14:17,600 --> 00:14:22,880
know, a chip on a, on a new paradigm or a new principle, you can then put them in a

197
00:14:22,880 --> 00:14:26,880
chip, and you can then ship them to, you know, a billion customers in a billion phones,

198
00:14:26,880 --> 00:14:27,880
right?

199
00:14:27,880 --> 00:14:31,160
And that's the scaling you were asking about, which is extremely exciting.

200
00:14:31,160 --> 00:14:38,080
You've got joint appointments, both with, uh, in academia and Qualcomm, uh, that's becoming

201
00:14:38,080 --> 00:14:41,600
increasingly common, but everyone kind of manages it differently.

202
00:14:41,600 --> 00:14:44,880
Uh, what's the relationship for you?

203
00:14:44,880 --> 00:14:49,200
How is your, your work and research distributed across those appointments?

204
00:14:49,200 --> 00:14:50,200
Yeah.

205
00:14:50,200 --> 00:14:54,480
So I have like about half an appointment in academia and half an appointment in Qualcomm

206
00:14:54,480 --> 00:14:55,480
and at Qualcomm.

207
00:14:55,480 --> 00:14:56,480
Okay.

208
00:14:56,480 --> 00:15:03,000
Which I think is actually a very good distribution, um, so it typically, in the university, you

209
00:15:03,000 --> 00:15:09,600
work on more of fundamental questions, which are very far out, um, and don't have a, you

210
00:15:09,600 --> 00:15:15,280
know, sort of a, a horizon of being, you know, productized or, you know, finding applications

211
00:15:15,280 --> 00:15:16,600
of four years, maybe.

212
00:15:16,600 --> 00:15:23,280
So at, at Qualcomm, we work on, you know, on problems, which between, you know, uh, one year

213
00:15:23,280 --> 00:15:29,800
and four years or five years, um, will find an application within the company, um, in

214
00:15:29,800 --> 00:15:34,560
academia, you're completely free to do whatever you want and it could be, you know, 100 years

215
00:15:34,560 --> 00:15:35,800
out if you wanted to.

216
00:15:35,800 --> 00:15:36,800
Mm-hmm.

217
00:15:36,800 --> 00:15:41,200
So it's interesting to have one lag in sort of both of these, uh, ecosystems in both

218
00:15:41,200 --> 00:15:42,600
of these environments.

219
00:15:42,600 --> 00:15:45,520
It does strike me that some of the things that we're talking about here, coming up with

220
00:15:45,520 --> 00:15:52,320
fundamental new compute architectures, you know, perhaps that are more inspired by, uh,

221
00:15:52,320 --> 00:15:56,680
the brain and, and synaptic and neural architectures, things like that.

222
00:15:56,680 --> 00:16:01,160
That could be, uh, very far reaching, uh, research.

223
00:16:01,160 --> 00:16:07,760
It is, are you working on, uh, that, uh, that kind of work in the academic setting as

224
00:16:07,760 --> 00:16:08,760
well?

225
00:16:08,760 --> 00:16:16,120
Um, so the, the compute, um, sort of thinking about how compute interacts with, um, you

226
00:16:16,120 --> 00:16:20,880
know, uh, with AI and machine learning and intelligence, that's something that I exclusively

227
00:16:20,880 --> 00:16:22,320
do at Qualcomm.

228
00:16:22,320 --> 00:16:28,760
Although it did start, um, in academia, so I was doing, um, Bayesian deep learning.

229
00:16:28,760 --> 00:16:34,960
So Bayesian statistics is a particular paradigm, um, as statistical paradigms, you have frequent

230
00:16:34,960 --> 00:16:40,400
distant Bayesian sort of statistics and Bayesian statistics, you've put probability distributions

231
00:16:40,400 --> 00:16:41,400
over your model.

232
00:16:41,400 --> 00:16:44,680
You basically say, I don't know, you know, what my model really is.

233
00:16:44,680 --> 00:16:50,640
Um, I say I have some probability distribution, um, over my possible parameters of my model

234
00:16:50,640 --> 00:16:55,200
and then when I see data, I'm going to narrow down that distribution of parameters to the

235
00:16:55,200 --> 00:16:59,080
ones that are actually describing the data that I see.

236
00:16:59,080 --> 00:17:02,640
Um, so, so that's something that wasn't really applied at deep learning.

237
00:17:02,640 --> 00:17:07,240
And so we started to apply that statistical paradigm to deep learning, which was a challenge

238
00:17:07,240 --> 00:17:10,000
because there was, you know, millions and millions of parameters there.

239
00:17:10,000 --> 00:17:11,800
You have to sort of handle that way.

240
00:17:11,800 --> 00:17:18,840
Um, but what we found is, um, that we can use, we could use that paradigm to, uh, basically

241
00:17:18,840 --> 00:17:24,880
compress a neural network, um, you know, with a factor of 100, uh, without losing any

242
00:17:24,880 --> 00:17:28,600
of the accuracy, which was actually a big shock to me, which is like, okay, so we working

243
00:17:28,600 --> 00:17:32,920
with these models, which are, we'd have a million parameters, but you could train the

244
00:17:32,920 --> 00:17:33,920
same model.

245
00:17:33,920 --> 00:17:38,280
You could just keep one out of 100 parameters and throw away all the rest, um, and you

246
00:17:38,280 --> 00:17:41,960
would have a model of which would function exactly the same way, which would not be any

247
00:17:41,960 --> 00:17:44,160
worst than the one that you started with.

248
00:17:44,160 --> 00:17:48,200
Um, and so that's generally known as, as compression neural network compression.

249
00:17:48,200 --> 00:17:50,560
So they are heavily over parameterized.

250
00:17:50,560 --> 00:17:51,560
Mm-hmm.

251
00:17:51,560 --> 00:17:54,040
So and we did that within the paradigm of Bayesian deep learning.

252
00:17:54,040 --> 00:17:59,520
It was a very good fundamental tool, um, to, you know, to do that compression.

253
00:17:59,520 --> 00:18:08,960
Did you apply this compression to these Bayesian deep learning models or is there, uh, something

254
00:18:08,960 --> 00:18:16,560
fundamental about Bayesian deep learning that allows it to, um, you know, that synergistic

255
00:18:16,560 --> 00:18:18,280
with this kind of compression?

256
00:18:18,280 --> 00:18:19,280
Yeah.

257
00:18:19,280 --> 00:18:20,280
So it's more the latter.

258
00:18:20,280 --> 00:18:21,280
Okay.

259
00:18:21,280 --> 00:18:22,880
Um, so it's basically, so you have a neural network.

260
00:18:22,880 --> 00:18:27,200
Because we do this kind of compression with, uh, with traditional CNNs and, and the like

261
00:18:27,200 --> 00:18:28,200
as well, right?

262
00:18:28,200 --> 00:18:29,520
Yes, that's right.

263
00:18:29,520 --> 00:18:36,800
So, so basically, um, it's one way, one technique to do this compression on a neural network.

264
00:18:36,800 --> 00:18:42,560
It has some additional advantages, which is that you can also, um, sort of expression

265
00:18:42,560 --> 00:18:46,880
on certainty over a prediction, right? So you can actually say, you know, I think it's

266
00:18:46,880 --> 00:18:52,320
this class or, you know, uh, this is happening in an image, but, you know, I'm 80% certain

267
00:18:52,320 --> 00:18:54,800
that that's actually correct, right?

268
00:18:54,800 --> 00:18:58,280
And so the Bayesian paradigm also allows you to do that.

269
00:18:58,280 --> 00:19:03,640
But, um, you can also use it to prune large parts of your neural network away.

270
00:19:03,640 --> 00:19:06,400
So it's sort of a principle way of doing that.

271
00:19:06,400 --> 00:19:10,480
Um, and so we started doing that and that's, you know, coming back to academia for this

272
00:19:10,480 --> 00:19:15,080
talk, um, so we started doing that in academia because that was a very academic exercise at

273
00:19:15,080 --> 00:19:16,080
that point.

274
00:19:16,080 --> 00:19:19,160
But it became practical, um, because we could compress these neural nets.

275
00:19:19,160 --> 00:19:22,160
And then we, you know, Qualcomm, we, we took it to Qualcomm.

276
00:19:22,160 --> 00:19:28,760
And now there's a whole team, uh, led by Tamin, um, Blancavort, who is basically, um,

277
00:19:28,760 --> 00:19:33,560
trying all, you know, sorts of algorithms to try to compress these neural networks, you

278
00:19:33,560 --> 00:19:35,720
know, in the, in the most practical way.

279
00:19:35,720 --> 00:19:38,040
And often the Bayesian way is not the most practical way.

280
00:19:38,040 --> 00:19:43,480
You know, it could be perhaps like a very good way, but it's not yet a very hands-off,

281
00:19:43,480 --> 00:19:44,680
you know, way to do things.

282
00:19:44,680 --> 00:19:50,640
And so some of these other methods, which are much simpler to understand, um, can also

283
00:19:50,640 --> 00:19:53,320
compress these neural networks to basically the same degree.

284
00:19:53,320 --> 00:19:57,520
And those are the ones that actually make it into the toolkits that we use at Qualcomm.

285
00:19:57,520 --> 00:20:01,800
It sounds like there's kind of a dynamic relationship.

286
00:20:01,800 --> 00:20:05,720
I guess as one would expect between the kind of things you're working on from an academic

287
00:20:05,720 --> 00:20:12,680
perspective and what you're doing at Qualcomm, although, um, you know, they differ in their

288
00:20:12,680 --> 00:20:13,680
timeframes.

289
00:20:13,680 --> 00:20:18,360
Yeah, they, you know, and it is a little bit like that.

290
00:20:18,360 --> 00:20:23,800
So, um, so another maybe great example, which I'm very enthusiastic about, is, um, work

291
00:20:23,800 --> 00:20:32,160
that I do with Taco Cohen and Maurice Weiler, um, on, um, so one is, uh, was a PhD student,

292
00:20:32,160 --> 00:20:37,520
now, uh, full-term employee at Qualcomm, Taco Cohen, and then Maurice Weiler is now, uh,

293
00:20:37,520 --> 00:20:42,280
PhD student at Qvalab, so Qvalab is actually a lab funded by Qualcomm at the University

294
00:20:42,280 --> 00:20:43,280
of Amsterdam.

295
00:20:43,280 --> 00:20:49,280
Um, and so that, so that work is, um, to include symmetries, um, in deep learning.

296
00:20:49,280 --> 00:20:56,440
So in other words, if I, you know, turn my head, um, the objects that I see, you know, turn

297
00:20:56,440 --> 00:21:02,120
around, um, in, in my brain, um, however, it's still the same objects, right?

298
00:21:02,120 --> 00:21:03,840
And so that's what we call a symmetry.

299
00:21:03,840 --> 00:21:07,880
If I, you know, if I move something, then it's still the same object, even though it,

300
00:21:07,880 --> 00:21:13,840
it has moved, um, and so incorporating these types of symmetries into neural networks

301
00:21:13,840 --> 00:21:20,120
is actually a very powerful way to make them better, turns out, um, and, um, and so, you

302
00:21:20,120 --> 00:21:23,920
know, with Taco Cohen, we started that process, um, you know, almost, I don't know, six

303
00:21:23,920 --> 00:21:28,120
years ago, or something like that, um, and it became quite successful, and now very

304
00:21:28,120 --> 00:21:33,080
recently, um, we did something that actually is quite, you know, amazing, I would say.

305
00:21:33,080 --> 00:21:38,640
So we started to use the mathematics of, um, general relativity, which is, uh, you

306
00:21:38,640 --> 00:21:45,000
know, the fundamental theory of gravity, uh, made by Einstein, um, and the same mathematics

307
00:21:45,000 --> 00:21:48,680
is actually in quantum field theory, which is, you know, behind the standard model, which

308
00:21:48,680 --> 00:21:53,200
is the fundamental theory, but particles, um, and this, this, this, this, this theory is

309
00:21:53,200 --> 00:21:59,120
called gauge theory, um, and it was actually the topic for my PhD thesis, which is interesting,

310
00:21:59,120 --> 00:22:05,120
I would say, so that came back after 20 years, um, and we started to incorporate these mathematical

311
00:22:05,120 --> 00:22:11,400
ideas into deep learning, um, and now we can do deep learning on arbitrary manifold.

312
00:22:11,400 --> 00:22:15,960
So you can think of a sphere, like the, the earth, and you want to detect storms or other

313
00:22:15,960 --> 00:22:21,640
weather patterns, um, on the earth, um, then you can use this particular tool, or, you

314
00:22:21,640 --> 00:22:26,960
can deep learning tool to, to, to analyze, you know, these, these manifolds, um, you

315
00:22:26,960 --> 00:22:33,440
can also think about, um, you know, in VR, right, in, in virtual reality, um, you generate,

316
00:22:33,440 --> 00:22:36,760
you know, maybe a game or something like that, so you generate objects in your world,

317
00:22:36,760 --> 00:22:42,360
these are synthetic, um, and you can sort of, uh, put texture, high resolution texture

318
00:22:42,360 --> 00:22:45,040
on these objects using, you know, this kind of tool.

319
00:22:45,040 --> 00:22:51,040
And so it's a very, very fundamental, you know, exercise, academic exercise to take, you

320
00:22:51,040 --> 00:22:56,400
know, these mathematical theories or, or, that are used in theoretical physics and put

321
00:22:56,400 --> 00:23:00,600
them into a deep learning algorithm, um, but then actually when you're done, once you're

322
00:23:00,600 --> 00:23:04,440
done, and you, you look, you know, at it from a distance, you, you discover all these

323
00:23:04,440 --> 00:23:09,120
beautiful applications, actually, um, and that's, I think the perfect, you know, I think

324
00:23:09,120 --> 00:23:13,120
the perfect line of thinking, right, so you, you start with something very fundamentally,

325
00:23:13,120 --> 00:23:18,680
you make big progress, um, you make an impact on, you know, a lot of, you know, the whole

326
00:23:18,680 --> 00:23:23,040
field and then you find that there is all sorts of interesting applications popping, popping

327
00:23:23,040 --> 00:23:26,400
up, um, that you hadn't thought, thought about before.

328
00:23:26,400 --> 00:23:33,280
Uh, and so, uh, some questions on that, uh, you started out talking about, uh, the work

329
00:23:33,280 --> 00:23:39,600
that you were doing around symmetry, uh, and then transitioned into the, the, the gauge,

330
00:23:39,600 --> 00:23:46,040
uh, networks, are those, uh, I didn't catch the relationship between those or those, uh,

331
00:23:46,040 --> 00:23:48,920
is that work specifically related to the one lead to the other?

332
00:23:48,920 --> 00:23:49,920
Yeah.

333
00:23:49,920 --> 00:23:50,920
Yeah, yeah, yeah, absolutely.

334
00:23:50,920 --> 00:23:55,120
So, um, yeah, that's a good question because I went very fast over that, but in, in, in,

335
00:23:55,120 --> 00:24:00,800
so, and it follows precisely the same, uh, steps as we're done in physics, right?

336
00:24:00,800 --> 00:24:07,240
And I think in, uh, you know, 19, no, six or so, um, Einstein came with his special theory

337
00:24:07,240 --> 00:24:13,720
of relativity, which basically described how different observers who move at a constant

338
00:24:13,720 --> 00:24:17,160
speed relative to each other, see different phenomenon.

339
00:24:17,160 --> 00:24:24,600
And, and he and Maxwell, uh, found that, um, basically magnetism will turn into electricity

340
00:24:24,600 --> 00:24:29,920
if you, you change observer and, you know, and, and move it with a constant speed, um,

341
00:24:29,920 --> 00:24:35,080
versus, so if a, if a, if a static observer sees an electric, you know, electric field

342
00:24:35,080 --> 00:24:37,920
and, and moving observer will see a magnetic field.

343
00:24:37,920 --> 00:24:41,120
Um, and so, um, and so that's the symmetry, right?

344
00:24:41,120 --> 00:24:44,720
It's a constant symmetry, it's like, you rotate something or you, you know, you rotate

345
00:24:44,720 --> 00:24:51,760
the whole, you know, earth around or something like that, um, now, uh, after that, um, came,

346
00:24:51,760 --> 00:24:57,360
you know, Einstein generalized that into general relativity and he said, well, actually,

347
00:24:57,360 --> 00:25:02,680
it's, it's not just, you know, these global symmetries, but actually, you know, we should

348
00:25:02,680 --> 00:25:07,000
be able to have a larger set of symmetries, which is basically, I should be able to,

349
00:25:07,000 --> 00:25:13,600
you know, to change speed, you know, and, and acceleration at every, any point in time,

350
00:25:13,600 --> 00:25:19,080
and I can sort of change my frame of reference at, differently at every point in space

351
00:25:19,080 --> 00:25:20,080
time.

352
00:25:20,080 --> 00:25:24,040
Um, and that's a much more general theory, it's called a local, uh, symmetry.

353
00:25:24,040 --> 00:25:26,520
So it's, it's a, it's a much more general symmetry.

354
00:25:26,520 --> 00:25:30,640
It's called a local symmetry and a local symmetry is also referred to as a gate symmetry.

355
00:25:30,640 --> 00:25:35,720
And it turns out when you think about that, that kind of large, a set of symmetries, then

356
00:25:35,720 --> 00:25:41,080
he figured out, for instance, that acceleration and gravity are actually, you know, the same

357
00:25:41,080 --> 00:25:45,440
phenomenon, but observed by, you know, one person who is standing still and another person

358
00:25:45,440 --> 00:25:48,760
who is accelerating relative to the other person.

359
00:25:48,760 --> 00:25:52,640
And so now again, you know, you'll, you'll have symmetries incorporated in your theory

360
00:25:52,640 --> 00:25:54,960
and your theory becomes a lot richer.

361
00:25:54,960 --> 00:25:55,960
So that's happened.

362
00:25:55,960 --> 00:25:58,360
So exactly that progression has happened for us too.

363
00:25:58,360 --> 00:26:03,760
So we started with these global symmetries, um, and then we turned to local symmetries

364
00:26:03,760 --> 00:26:10,240
and, you know, we went from basically, you know, doing deep learning on flat images to

365
00:26:10,240 --> 00:26:12,600
do deep learning on curved manifolds.

366
00:26:12,600 --> 00:26:13,600
Interesting.

367
00:26:13,600 --> 00:26:14,600
Yeah.

368
00:26:14,600 --> 00:26:20,400
When you first started talking about symmetries, the, uh, the thing that came to mind was,

369
00:26:20,400 --> 00:26:25,240
you know, that maybe you were going after the same type of problem as Jeffrey Hinton,

370
00:26:25,240 --> 00:26:31,960
uh, who you've worked with and, uh, his capsule networks, uh, but it sounds like, uh,

371
00:26:31,960 --> 00:26:34,200
you ended up in very different places.

372
00:26:34,200 --> 00:26:37,040
But I would say that, um, you're actually quite right there.

373
00:26:37,040 --> 00:26:42,680
So it turns out that, um, this is, you know, this is a little hard to explain, but out

374
00:26:42,680 --> 00:26:48,520
of this theory of symmetries, um, capsules emerge quite naturally.

375
00:26:48,520 --> 00:26:52,080
Um, and it's a bit hard to explain how that precisely works.

376
00:26:52,080 --> 00:26:56,960
Um, but it's, it's true that what you'll have is you'll have these neurons.

377
00:26:56,960 --> 00:27:01,920
Um, so when you, when you apply translational invariance, these neurons will now be

378
00:27:01,920 --> 00:27:05,760
what we call feature maps, which is an entire sort of filter image.

379
00:27:05,760 --> 00:27:11,560
Um, and when you apply additional symmetries like rotation, you get sort of stacks of filter

380
00:27:11,560 --> 00:27:17,440
maps, um, that sort of, that sort of transform into each other if you rotate the underlying

381
00:27:17,440 --> 00:27:18,440
image.

382
00:27:18,440 --> 00:27:21,320
And that's, that stack of filter maps is what we call a capsule.

383
00:27:21,320 --> 00:27:26,120
And that's actually what Jeff Hinton also talks about when he, when he talks about capsules.

384
00:27:26,120 --> 00:27:27,120
Mm-hmm.

385
00:27:27,120 --> 00:27:30,360
Now he has this sort of dynamic routing algorithm, which is, which is something on top

386
00:27:30,360 --> 00:27:34,960
of these capsules, which, you know, we haven't implemented in our code yet.

387
00:27:34,960 --> 00:27:38,320
Um, but there, these two things are actually remarkably related.

388
00:27:38,320 --> 00:27:40,360
So you were actually quite right there.

389
00:27:40,360 --> 00:27:48,680
So within the, the gauge CNNs, you have this concept of a capsule, uh, as well.

390
00:27:48,680 --> 00:27:50,000
Is that correct?

391
00:27:50,000 --> 00:27:55,760
It's, I would say, uh, yes, but it's also already in the sort of normal, uh, sort of, uh,

392
00:27:55,760 --> 00:27:57,560
what we call group CNNs.

393
00:27:57,560 --> 00:28:04,760
So it's, it, so before we did gauge CNNs with the local symmetry, um, capsules also naturally

394
00:28:04,760 --> 00:28:12,480
appear in, you know, these, uh, these groups, uh, like a global rotation, uh, or a translation

395
00:28:12,480 --> 00:28:14,240
or something like that.

396
00:28:14,240 --> 00:28:24,360
And, uh, beyond the conceptual, uh, well, you've got this, this, uh, common conceptual

397
00:28:24,360 --> 00:28:33,560
foundation between, uh, Hinton's capsule networks and your gauge, echrovariant, CNNs are,

398
00:28:33,560 --> 00:28:39,200
are there other relationships between the two, um, that grot of these, or what are the

399
00:28:39,200 --> 00:28:45,120
relationships between the two that kind of grot of these, uh, kind of the shared feature

400
00:28:45,120 --> 00:28:48,120
maps and in variances within the networks?

401
00:28:48,120 --> 00:28:54,240
Yeah, I think it, it would be a fairly technical discussion to try to explain that, but I

402
00:28:54,240 --> 00:29:00,640
just, let me, let me limit myself to saying that, um, sort of in Jeff, in Jeff Hinton's

403
00:29:00,640 --> 00:29:07,600
theory, um, he, he, he basically says, okay, so I, I want my feature maps to be divided

404
00:29:07,600 --> 00:29:16,280
up in, in these capsules and then basically the particular configuration within a capsule,

405
00:29:16,280 --> 00:29:22,520
you know, is the pose of something, um, and then, you know, the, maybe the, you know,

406
00:29:22,520 --> 00:29:27,880
the strength of, you know, how much of that capsule is present, you know, is, is, is,

407
00:29:27,880 --> 00:29:33,520
it, that indicates how strongly a particular object is seen in the image.

408
00:29:33,520 --> 00:29:39,920
Um, so he, he goes in with, uh, sort of an intuition and he, he, he builds it in, um,

409
00:29:39,920 --> 00:29:44,200
where in our case, we start with the symmetry from the principle of symmetry and actually

410
00:29:44,200 --> 00:29:48,520
the math, you know, basically drives us to these capsules.

411
00:29:48,520 --> 00:29:52,760
But in the end, these, they were, they're actually completely the same thing.

412
00:29:52,760 --> 00:29:58,120
So, so I would say we laid a mathematical foundation for the capsules that, that, Hinton's

413
00:29:58,120 --> 00:30:01,720
intuition sort of, uh, brought, um, yeah.

414
00:30:01,720 --> 00:30:05,200
And so, you know, there's, so he, he has the Namooka routing, which is something that

415
00:30:05,200 --> 00:30:08,640
is sort of on top of it, which, which we don't do necessarily.

416
00:30:08,640 --> 00:30:13,480
We, you know, we, we went into the direction of, of course, Gage Equivareans, um, but it's,

417
00:30:13,480 --> 00:30:18,640
you know, we, we are basically in parallel sort of developing, uh, these ideas, I would

418
00:30:18,640 --> 00:30:19,640
say.

419
00:30:19,640 --> 00:30:20,640
Okay.

420
00:30:20,640 --> 00:30:30,040
And so is the idea with these Gage CNNs related to, uh, are they more compact or do

421
00:30:30,040 --> 00:30:35,840
they open up new, uh, applications or do they perform better?

422
00:30:35,840 --> 00:30:38,560
What, what does this approach buy us?

423
00:30:38,560 --> 00:30:45,120
Right. So, um, so it's, it's mostly, if you want to do a deep learning on a, on something

424
00:30:45,120 --> 00:30:46,720
that's not a plane, right?

425
00:30:46,720 --> 00:30:49,480
So the manifolds, yeah.

426
00:30:49,480 --> 00:30:54,640
So imagine, you know, you want to, you know, so of course, a simple example is, think

427
00:30:54,640 --> 00:31:00,080
of the earth and think of a signal on the earth, like maybe temperature or, you know, wind

428
00:31:00,080 --> 00:31:03,520
patterns or something like that, and you want to predict, you know, maybe the weather

429
00:31:03,520 --> 00:31:07,600
in 10 days or something like that, or you want to find where is the storm or where is

430
00:31:07,600 --> 00:31:11,920
the, you know, the weather pattern that you're interested in.

431
00:31:11,920 --> 00:31:12,920
Yeah.

432
00:31:12,920 --> 00:31:17,480
I also did a really interesting interview with a woman named Nina Mielan, who does, uh,

433
00:31:17,480 --> 00:31:24,080
or Mielane, who does a Python package, Jams, that's that it's focused on doing statistics

434
00:31:24,080 --> 00:31:25,080
on manifolds.

435
00:31:25,080 --> 00:31:30,200
And the example that she gave was pretty interesting, it's like if you want to do statistics

436
00:31:30,200 --> 00:31:37,000
or learning on something like a human heart in medicine, uh, it is, uh, you know, much

437
00:31:37,000 --> 00:31:44,000
more naturally amenable to dealing with it as a set of manifolds or curves than in a,

438
00:31:44,000 --> 00:31:47,000
you know, a typical rectilinear space.

439
00:31:47,000 --> 00:31:48,000
Yeah.

440
00:31:48,000 --> 00:31:49,000
So exactly.

441
00:31:49,000 --> 00:31:53,320
So that's on our to-do list, and we are actually collaborating with people who know much

442
00:31:53,320 --> 00:31:57,760
more about, you know, these medical applications than we do.

443
00:31:57,760 --> 00:32:00,320
But that's certainly one of the applications area.

444
00:32:00,320 --> 00:32:01,320
So application area.

445
00:32:01,320 --> 00:32:05,800
So you can think of a, of a beating heart or something like that, then you would put

446
00:32:05,800 --> 00:32:08,760
like a mesh on that beating heart.

447
00:32:08,760 --> 00:32:13,760
Um, and you could sort of occur to mesh as opposed to a straight mesh.

448
00:32:13,760 --> 00:32:17,120
Well, the mesh has to live on the manifold, right?

449
00:32:17,120 --> 00:32:21,320
So there's a lot of nodes you distribute over this mesh and you connect them by little

450
00:32:21,320 --> 00:32:23,040
lines, basically.

451
00:32:23,040 --> 00:32:29,280
Um, and so, and so then you can try to maybe, you know, predict whether this heart is,

452
00:32:29,280 --> 00:32:35,280
um, has, you know, strange behavior or something like that as abnormal behavior, um, or you

453
00:32:35,280 --> 00:32:39,960
could try to figure out, you know, where's, you know, where certain pieces of the heart

454
00:32:39,960 --> 00:32:40,960
are.

455
00:32:40,960 --> 00:32:42,200
Are you want to segment them out or something like that?

456
00:32:42,200 --> 00:32:45,200
Maybe you want to detect where the vessels are, the valves are or something like that,

457
00:32:45,200 --> 00:32:46,200
right?

458
00:32:46,200 --> 00:32:51,200
So certainly, you know, that's a, that's a prime example of where we have a, a manifold,

459
00:32:51,200 --> 00:32:54,600
um, and where we want to do deep learning on that manifold.

460
00:32:54,600 --> 00:32:55,600
Um, yeah.

461
00:32:55,600 --> 00:32:58,440
And so that's certainly something that we are planning to do.

462
00:32:58,440 --> 00:32:59,440
Okay.

463
00:32:59,440 --> 00:33:06,160
And talking about this mesh, um, you also bring up the idea or brings up for me, the idea

464
00:33:06,160 --> 00:33:19,080
of like graphs, um, is the gauge CNN kind of fundamentally a graph CNN or, um, is that

465
00:33:19,080 --> 00:33:21,280
something that you're working on as well?

466
00:33:21,280 --> 00:33:23,080
You're asking really good questions, I would say.

467
00:33:23,080 --> 00:33:32,040
So in fact, um, one of the reasons why, um, you know, we are exploring this kind of mashed

468
00:33:32,040 --> 00:33:39,160
version of gate CNNs, which is we put a mesh on the manifold and then we send messages

469
00:33:39,160 --> 00:33:45,760
between the nodes over the edges, um, is that it looks a lot like a graph convolution,

470
00:33:45,760 --> 00:33:51,320
which is, which is another object where you do deep learning on graphs.

471
00:33:51,320 --> 00:33:57,440
But, um, a manifold, even a mashed manifold is not a graph because, um, this has something

472
00:33:57,440 --> 00:34:00,640
to do with the fact that the neighbors are not sort of exchangeable.

473
00:34:00,640 --> 00:34:02,800
The neighbors are, are not a set.

474
00:34:02,800 --> 00:34:07,080
The neighbors are actually ordered, um, they, they, you know, something to their right

475
00:34:07,080 --> 00:34:09,680
has a different meaning than something to the left.

476
00:34:09,680 --> 00:34:15,840
And so people sometimes actually do graph convolutions on these meshes, um, but that's

477
00:34:15,840 --> 00:34:17,880
actually slightly suboptimal.

478
00:34:17,880 --> 00:34:24,600
And so the way we do it, so this, this gauge equity very neural networks, um, is precisely,

479
00:34:24,600 --> 00:34:28,720
um, designed to do it optimally in, in a way, to do it the right way.

480
00:34:28,720 --> 00:34:34,200
And we have a PhD student now at Qualcomm was Qualcomm and PhD student at University of Amsterdam,

481
00:34:34,200 --> 00:34:37,600
Pim de Hanh, was precisely working on this topic.

482
00:34:37,600 --> 00:34:42,720
So he's precisely trying to nail this topic of, you know, um, what is the difference between

483
00:34:42,720 --> 00:34:49,280
a graph convolution and this mashed gauge convolution and, you know, is there a real practical

484
00:34:49,280 --> 00:34:53,760
advantage above, you know, doing it correctly in a mathematical sense?

485
00:34:53,760 --> 00:34:57,760
You know, is there also a practical advantage of doing it in this particular way?

486
00:34:57,760 --> 00:35:04,920
Uh, so all of these, uh, we've talked about, uh, things from kind of very optimization-focused

487
00:35:04,920 --> 00:35:16,040
ideas like compression to Bayesian deep learning and gauge, uh, CNNs within, within Qualcomm,

488
00:35:16,040 --> 00:35:24,880
these are all more research oriented topics, as opposed to, um, kind of product oriented

489
00:35:24,880 --> 00:35:25,880
topics.

490
00:35:25,880 --> 00:35:27,080
Is that right?

491
00:35:27,080 --> 00:35:33,280
Well, um, I would say that the, um, it's true to some degree, but I would say that compression,

492
00:35:33,280 --> 00:35:39,440
neural network compression and quantization, um, has immediate, uh, sort of practical

493
00:35:39,440 --> 00:35:40,440
applications.

494
00:35:40,440 --> 00:35:45,800
And, in fact, you know, the team of time in blank, time in Blunkervort is already implementing

495
00:35:45,800 --> 00:35:49,600
these methods into a toolbox, which is going to be commercialized.

496
00:35:49,600 --> 00:35:54,280
So, and the reason why this is important is that, um, if I have my neural network that

497
00:35:54,280 --> 00:35:58,400
I'm very fond of, and I trained it in the cloud, maybe using TensorFlow or PyTorch or

498
00:35:58,400 --> 00:36:03,240
something like that, um, and now I want to run it on my phone, um, but it's too big to

499
00:36:03,240 --> 00:36:07,320
run on my phone, um, because it takes too much energy and then my phone doesn't have that

500
00:36:07,320 --> 00:36:08,320
much memory.

501
00:36:08,320 --> 00:36:09,320
Right?

502
00:36:09,320 --> 00:36:14,520
So, what I want to provide to the customer is a tool that automatically compiles this

503
00:36:14,520 --> 00:36:20,280
big neural network into a much smaller neural network that basically has the same performance,

504
00:36:20,280 --> 00:36:26,880
um, as the bigger one, um, but runs on our Snapdragon, uh, sort of AI chipsets, um, you

505
00:36:26,880 --> 00:36:28,960
know, very, very efficient to be.

506
00:36:28,960 --> 00:36:34,760
And, you know, doing that is actually, you know, you need to, you know, get the neural

507
00:36:34,760 --> 00:36:41,600
network into a much more lean sort of, um, framework, um, and then you also need to compile

508
00:36:41,600 --> 00:36:46,480
the operations that this neural network has to do, which is basically matrix multiplications,

509
00:36:46,480 --> 00:36:48,120
matrix vector multiplications.

510
00:36:48,120 --> 00:36:52,600
Um, you have to compile them in such a way that they run very quickly on the particular

511
00:36:52,600 --> 00:36:57,280
piece of hardware, um, and, and you can also optimize that a lot.

512
00:36:57,280 --> 00:37:01,480
Um, and so we are, for instance, also working on, um, on algorithms.

513
00:37:01,480 --> 00:37:07,280
So this is Chang-Yong-Oh and Stratus-Govus, where, you know, people at, uh, this Q-Valab, um,

514
00:37:07,280 --> 00:37:13,840
we are working on what we call Bayesian optimization algorithms, which is, um, algorithms which optimize

515
00:37:13,840 --> 00:37:17,240
over a very large discrete space, right, of choices.

516
00:37:17,240 --> 00:37:22,520
So the choices are, you know, should I first, you know, do this particular multiplication

517
00:37:22,520 --> 00:37:25,640
and then this addition or should I first do it the other way around?

518
00:37:25,640 --> 00:37:29,920
So there is an exponential number of possible choices that you have there in optimizing that

519
00:37:29,920 --> 00:37:30,920
code.

520
00:37:30,920 --> 00:37:37,920
Um, and so how can you quickly explore, um, all of these, all of these possibilities?

521
00:37:37,920 --> 00:37:44,400
And so Chang-Yong developed a beautiful algorithm that, that, with a minimal number of trials,

522
00:37:44,400 --> 00:37:51,040
um, quickly zooms in on the optimal sort of, uh, configuration of these discrete choices.

523
00:37:51,040 --> 00:38:03,120
In order to apply a Bayesian optimization to this kind of problem, you first need to expose

524
00:38:03,120 --> 00:38:09,280
the, I guess the features or the, uh, kind of the control levers, if you will, of the

525
00:38:09,280 --> 00:38:14,720
problem, which operation goes first versus second, those aren't necessarily, necessarily

526
00:38:14,720 --> 00:38:18,760
naturally, uh, expose from the models.

527
00:38:18,760 --> 00:38:22,280
Is that right? Do, do you have to kind of do something or build something in order to

528
00:38:22,280 --> 00:38:25,800
be able to apply Bayesian optimization to these kinds of problems?

529
00:38:25,800 --> 00:38:26,800
Yeah.

530
00:38:26,800 --> 00:38:29,240
So typically you can think of two levels.

531
00:38:29,240 --> 00:38:34,880
So, um, let's, let's say on a phone, um, so if I have a particular proposal of doing

532
00:38:34,880 --> 00:38:40,560
my computations, um, then I can, you know, compile that onto the phone, the actual physical

533
00:38:40,560 --> 00:38:43,240
phone, and run it and measure how well I was doing.

534
00:38:43,240 --> 00:38:47,080
Now, of course, that takes some time, right, we are talking about, you know, maybe seconds

535
00:38:47,080 --> 00:38:51,640
or something like that, I don't know, um, to actually measure, you know, that, that

536
00:38:51,640 --> 00:38:52,640
configuration.

537
00:38:52,640 --> 00:38:55,200
And so if you do it that way, you go very slow.

538
00:38:55,200 --> 00:39:00,320
Um, but you can also build a simulator, right, and the simulator would, um, basically

539
00:39:00,320 --> 00:39:05,360
figure out in approximation how good that particular configuration would be.

540
00:39:05,360 --> 00:39:07,240
But it's uncertain, right?

541
00:39:07,240 --> 00:39:11,880
And so you can imagine that, um, you sort of, you know, you first do a couple, you know,

542
00:39:11,880 --> 00:39:15,440
a couple of steps of optimization on the simulator, which is very fast.

543
00:39:15,440 --> 00:39:19,600
And then when you get too uncertain, you then go to the physical device and you try a

544
00:39:19,600 --> 00:39:24,920
few things, um, to see how, how well you're doing, you didn't update, update your simulator

545
00:39:24,920 --> 00:39:29,640
or the sort of surrogate model that, that estimates how well you're doing.

546
00:39:29,640 --> 00:39:33,080
Um, and then you sort of keep optimizing there again.

547
00:39:33,080 --> 00:39:38,360
So there is this basically game that you play, um, you can do, you can measure things

548
00:39:38,360 --> 00:39:42,920
very precisely, but quite expensively, or you can measure things very quickly, but sort

549
00:39:42,920 --> 00:39:47,120
of approximately, right? And, you know, this is the choices that you have and you have

550
00:39:47,120 --> 00:39:52,600
to navigate those choices as quickly as possible to get to the final best possible configuration.

551
00:39:52,600 --> 00:39:59,360
One of the characteristics of dealing with, uh, devices that need to be produced in

552
00:39:59,360 --> 00:40:04,800
silicon is relatively long lead times, as opposed to software products.

553
00:40:04,800 --> 00:40:10,160
Can you talk a little bit about productizing these types of ideas in, in that kind of

554
00:40:10,160 --> 00:40:11,160
environment?

555
00:40:11,160 --> 00:40:16,000
Yeah, so I would say, um, we mostly work on software, right?

556
00:40:16,000 --> 00:40:23,160
So in, so in, in, in many ways, what we do is, um, can, can be quite quickly productized,

557
00:40:23,160 --> 00:40:27,680
I would say, because it's basically a software tool or, you know, enhancement of some kind.

558
00:40:27,680 --> 00:40:28,680
Okay.

559
00:40:28,680 --> 00:40:31,280
Um, but we do work with hardware folks, right?

560
00:40:31,280 --> 00:40:37,000
So we do work on, you know, exciting new hardware developments, um, Princess Computing

561
00:40:37,000 --> 00:40:41,280
Memory is something, you know, it's, it's well known, so a bunch of startup companies

562
00:40:41,280 --> 00:40:46,840
are also working on that, um, that's basically where you would, instead of moving that data

563
00:40:46,840 --> 00:40:53,680
that I talked to you about before, from, uh, the DDR memory to the chip, you would basically

564
00:40:53,680 --> 00:40:56,800
do the computation directly in that memory cell.

565
00:40:56,800 --> 00:41:02,800
So you would directly do your computation in, you know, in, in memory, which is actually,

566
00:41:02,800 --> 00:41:08,200
you know, analog, so this is actually, you know, faults and currents and stuff like that.

567
00:41:08,200 --> 00:41:13,200
Um, and so now the game is, and that's a very interesting game, you know, the game is,

568
00:41:13,200 --> 00:41:19,480
you know, you try to develop a piece of hardware, um, that runs optimally for a particular

569
00:41:19,480 --> 00:41:22,800
algorithm, like a deep learning algorithm in this case.

570
00:41:22,800 --> 00:41:27,480
Um, and at the same time, you're trying to adapt the deep learning algorithm, uh, to work

571
00:41:27,480 --> 00:41:30,800
as well as possible on that particular piece of hardware.

572
00:41:30,800 --> 00:41:35,600
And so this is a trend, um, that you can see much more generally, which is that hardware

573
00:41:35,600 --> 00:41:40,720
design and software design are starting to become more and more integrated and entangled

574
00:41:40,720 --> 00:41:41,880
with each other.

575
00:41:41,880 --> 00:41:46,040
It's not just here, but it's in many other places where you can see that, you know, a piece

576
00:41:46,040 --> 00:41:51,760
of hardware being replaced by pieces of software run on, sort of, uh, a deep learning engine

577
00:41:51,760 --> 00:41:57,760
on a chip, um, you know, you have, you know, an, in a heterogeneous compute environment

578
00:41:57,760 --> 00:42:02,440
with many different types of compute, like DSP and CPU and GPU, et cetera.

579
00:42:02,440 --> 00:42:07,440
Um, if you're faced with a certain computation, you will have to distribute that computation

580
00:42:07,440 --> 00:42:12,320
across all of these, uh, you know, different compute engines, and you have to think smartly

581
00:42:12,320 --> 00:42:13,640
about how to do that, right?

582
00:42:13,640 --> 00:42:17,840
And again, there is a controller, sort of an intelligent agent that will have to learn

583
00:42:17,840 --> 00:42:19,080
how to do this efficiently.

584
00:42:19,080 --> 00:42:25,080
So you can see that software machine learning or, you know, learnable sort of software,

585
00:42:25,080 --> 00:42:29,160
um, and hardware are going to get tight, more and more tightly integrated, which I think

586
00:42:29,160 --> 00:42:31,000
is a very fascinating development.

587
00:42:31,000 --> 00:42:37,360
Let's maybe shift gears and talk a little bit about, uh, kind of forward looking, uh,

588
00:42:37,360 --> 00:42:38,360
ideas.

589
00:42:38,360 --> 00:42:46,160
You recently wrote, uh, posts about the, uh, kind of responding to a rich Sutton blog.

590
00:42:46,160 --> 00:42:49,400
I'll let you maybe talk a little bit about the background and then the posts, but it kind

591
00:42:49,400 --> 00:42:56,200
of ponders this idea of, you know, what's most important models versus data versus compute.

592
00:42:56,200 --> 00:43:02,240
Um, can you talk a little bit about that, that work and how you see that, um, kind of

593
00:43:02,240 --> 00:43:05,720
playing out, uh, in the, the future of the space?

594
00:43:05,720 --> 00:43:06,720
Yeah.

595
00:43:06,720 --> 00:43:11,480
So I think this was more like my, sort of rainy Sunday afternoon, sort of write up about

596
00:43:11,480 --> 00:43:16,440
something that, you know, I threw out there and it actually got a lot of attention.

597
00:43:16,440 --> 00:43:17,760
So it was interesting.

598
00:43:17,760 --> 00:43:22,000
Instead of hit the right, I guess the right nerve, people are very interested in that kind

599
00:43:22,000 --> 00:43:23,000
of thing.

600
00:43:23,000 --> 00:43:25,960
Um, and it, it's really about a super fundamental problem.

601
00:43:25,960 --> 00:43:30,400
And I think, you know, researchers, uh, we should talk about this more because it, it

602
00:43:30,400 --> 00:43:34,960
might determine from any young researchers in the field, you know, where they want to

603
00:43:34,960 --> 00:43:36,840
head with their particular research.

604
00:43:36,840 --> 00:43:42,120
Um, and so I was actually very grateful to rich that he posted that particular post.

605
00:43:42,120 --> 00:43:47,320
Um, he basically said something and I made, I, I may, you know, charge you a little bit,

606
00:43:47,320 --> 00:43:54,920
but it's like, um, you know, uh, we should really not, um, try to model all that much, um,

607
00:43:54,920 --> 00:44:01,240
because in the end, uh, if we focus on scaling our architectures or sort of more general

608
00:44:01,240 --> 00:44:07,600
purpose, um, machine learning architectures, um, if we wait long enough, then, you know,

609
00:44:07,600 --> 00:44:12,680
using Moore's law, you'll basically get to a point where you always get beaten by these

610
00:44:12,680 --> 00:44:18,520
kind of, uh, sort of scalable algorithms that are basically just, you know, eating data

611
00:44:18,520 --> 00:44:20,520
and, and turning them into predictions.

612
00:44:20,520 --> 00:44:24,800
Um, and, you know, there's been a lot of examples that actually, you know, where this was

613
00:44:24,800 --> 00:44:25,800
actually the case, right?

614
00:44:25,800 --> 00:44:31,640
So we had, um, you know, we had models of speech where, you know, people had built, you

615
00:44:31,640 --> 00:44:39,480
know, models of the human, you know, uh, voice tract, um, and they were sort of, you

616
00:44:39,480 --> 00:44:43,920
know, they had, they were, they were modeling how people produce speech and then they were,

617
00:44:43,920 --> 00:44:48,360
you know, matching that with the actual observations and then trying to figure out, you know, what,

618
00:44:48,360 --> 00:44:52,840
what the mouth, how the mouth actually moved and therefore what word was being spoken.

619
00:44:52,840 --> 00:44:58,360
Um, and later people found, well, you know, if you just collect enough data, then, um,

620
00:44:58,360 --> 00:45:03,400
you can just map, you know, basically take, take the, the audio signal that hits your

621
00:45:03,400 --> 00:45:09,480
microphone, right, and, and map it, learn to map it to the words that produced it.

622
00:45:09,480 --> 00:45:14,520
Um, and if you have enough of those pairs, like audio signal and word, um, then this,

623
00:45:14,520 --> 00:45:19,960
this sort of, in, in some sense, stupid if you want, uh, sort of a statistical tool, uh,

624
00:45:19,960 --> 00:45:23,640
vastly outperformed, um, these kind of more mechanical tools.

625
00:45:23,640 --> 00:45:31,000
Um, and this happened in speech and it happened in, in, in, also in, um, in computer vision,

626
00:45:31,000 --> 00:45:35,880
where, um, basically the best methods are now these deep learning methods where you basically

627
00:45:35,880 --> 00:45:40,600
collect a huge amount of data on images, you segment them, you tell me what's the objects

628
00:45:40,600 --> 00:45:45,720
in the image and then you train all that data and you get now with them that performs very,

629
00:45:45,720 --> 00:45:47,240
very well, better than anything else.

630
00:45:47,240 --> 00:45:55,640
I mean, this all goes back to the, I think the most, uh, quoted example of this, uh, and

631
00:45:55,640 --> 00:46:01,480
there are many are the Peter Norvig, uh, unreasonable effectiveness of data paper, where he talks

632
00:46:01,480 --> 00:46:06,040
about Google, you know, they're not better at this because they have better algorithms,

633
00:46:06,040 --> 00:46:08,280
it's because they just have more data.

634
00:46:08,280 --> 00:46:09,800
Absolutely, right?

635
00:46:09,800 --> 00:46:13,800
So a lot of these internet companies are in the business of getting data.

636
00:46:14,520 --> 00:46:18,840
Um, and by having all that data, they can do things that, you know, we didn't

637
00:46:18,840 --> 00:46:25,720
thought were possible before, um, and this was a very valuable lesson, right? So, you know, collecting more data and

638
00:46:25,720 --> 00:46:31,560
having the compute to do the computations, um, is basically, you know, one of the reasons why

639
00:46:31,560 --> 00:46:37,080
we're seeing all this progress, um, but the, but it begs the question, how far can we take it?

640
00:46:37,080 --> 00:46:38,680
Right? This particular idea.

641
00:46:38,680 --> 00:46:42,840
And there's other people like Josh Tenenbaum, who has been, who have been saying the opposite.

642
00:46:42,840 --> 00:46:48,760
So he's been saying, well, you know, um, the world actually operates in the other direction.

643
00:46:48,760 --> 00:46:55,480
Which means that, you know, physics, um, basically the dead, the data generating process

644
00:46:55,480 --> 00:47:00,920
is much more like we have objects in the world, these objects move under the laws of physics,

645
00:47:00,920 --> 00:47:05,000
or maybe the laws of psychology or sociology when we interact with each other.

646
00:47:05,880 --> 00:47:08,520
Um, there's causal relationships, right?

647
00:47:08,520 --> 00:47:10,760
You know, things cause other things to happen.

648
00:47:10,760 --> 00:47:15,160
And then finally, you know, signals hit our sensors, right?

649
00:47:15,160 --> 00:47:16,840
And that's what's being recorded.

650
00:47:16,840 --> 00:47:21,400
So that's the direction of, you know, the physics of the world into the sensors.

651
00:47:21,400 --> 00:47:23,480
That's what we call the data generative model.

652
00:47:24,040 --> 00:47:26,280
And then the, and then deep learning does the opposite.

653
00:47:26,280 --> 00:47:33,240
It goes from these signals on the sensors and directly shortcuts a path to predict,

654
00:47:33,240 --> 00:47:35,880
you know, what were the objects which were producing these signals.

655
00:47:36,840 --> 00:47:40,040
Um, and actually the brain interestingly does something similar, right?

656
00:47:40,040 --> 00:47:45,160
So we, we have in our brain the ability to, to simulate the world, right?

657
00:47:45,160 --> 00:47:48,920
I can, I can close my eyes and I can imagine, you know,

658
00:47:48,920 --> 00:47:54,440
what it means to ride on a horse or something like that or to fall off a building, right?

659
00:47:54,440 --> 00:47:57,160
And I can just see it happen under the laws of physics.

660
00:47:58,120 --> 00:48:02,760
At the same time, I can also instantaneously recognize objects in the world.

661
00:48:02,760 --> 00:48:09,400
So there's these pathways in our brain which just take in sensory data and immediately produce

662
00:48:09,400 --> 00:48:14,040
segment the world into objects and, and sort of tell me what's in the world without me thinking

663
00:48:14,040 --> 00:48:17,560
about it. So in, in our brain, we have these two modalities as well.

664
00:48:17,560 --> 00:48:21,160
And, and, and Kanamon calls this slow and fast thinking basically.

665
00:48:21,160 --> 00:48:23,800
So it is two quite different pathways to, to think.

666
00:48:24,680 --> 00:48:29,160
And so, um, so the question becomes, you know, how far can we take this good of data driven

667
00:48:29,160 --> 00:48:34,840
approach? And, and my, my, my post was about, well, humans are a lot better in certain things

668
00:48:35,400 --> 00:48:41,880
than current algorithms. Algorithms have trouble with generalizing away from the domain

669
00:48:41,880 --> 00:48:46,680
in which they are trained, right? If I train an algorithm to play go, and then I tell it,

670
00:48:46,680 --> 00:48:51,560
okay, now play chess, right? Or, or play go on a smaller board even or, you know,

671
00:48:51,560 --> 00:48:55,240
with different color stones or something, it gets confused because it wasn't trained for that.

672
00:48:55,240 --> 00:49:01,480
Right. For example, that, that always comes to mind for me was a video of, um,

673
00:49:02,360 --> 00:49:07,480
and Peter Abil's lab training a robotic arm to think untangle a rope.

674
00:49:07,480 --> 00:49:12,040
And it does great on a green, you know, when the rope is on a green table,

675
00:49:12,040 --> 00:49:15,640
but when it's on a red table, and I'm making these colors up, but the idea is the same.

676
00:49:15,640 --> 00:49:20,680
When it's on a red table, it totally fails. It's just, it's, the models are that dependent on the

677
00:49:20,680 --> 00:49:27,640
specifics of, uh, the, with the environment in which they're trained. Yeah. And so, um, and,

678
00:49:27,640 --> 00:49:32,040
and that's a very good example. And so, um, you know, maybe another example is, you know,

679
00:49:32,040 --> 00:49:35,800
I was driving on a road a couple of times, and then, you know, there was road works. And what

680
00:49:35,800 --> 00:49:41,240
they did was they kept all the white sort of lane dividers on the road, and they just put,

681
00:49:41,240 --> 00:49:46,600
you know, yellow ones, you know, on completely different positions. And everybody knew what to do,

682
00:49:46,600 --> 00:49:53,480
even though the, the white lane dividers were still there. Um, and I'm pretty sure an algorithm,

683
00:49:53,480 --> 00:49:57,240
you know, when it's not trained on that, will that get totally confused?

684
00:49:57,880 --> 00:50:03,640
Right. And what, so what's the underlying problem is that, um, so they don't generalize because

685
00:50:03,640 --> 00:50:10,440
they don't understand the world in this generative data generating way. So, and, and, and,

686
00:50:10,440 --> 00:50:14,600
you know, what Josh Tannenbaum has been saying about the others have been saying is that you

687
00:50:14,600 --> 00:50:21,240
cannot collect nearly enough data to, you know, to capture all of your corner cases, right? I mean,

688
00:50:21,240 --> 00:50:25,400
it's like, there's so many things that can happen in the world. It's like an exponential number

689
00:50:25,400 --> 00:50:31,240
of things that can happen in the world. And many things are very rare. Um, and so we need ways to

690
00:50:31,240 --> 00:50:38,280
generalize the lessons we learn in one context into sort of a completely different context. And,

691
00:50:38,280 --> 00:50:43,000
you know, one school of thought is that the only way to do that is to understand the world in a

692
00:50:43,000 --> 00:50:49,880
generative way. So, because, and the reason is that the generative direction is much more efficient

693
00:50:49,880 --> 00:50:55,240
because, you know, it's, it's modular. Yeah, we have things that, you know,

694
00:50:55,240 --> 00:51:01,880
relatively independently do things operate on these are the objects and the agents in the world,

695
00:51:02,600 --> 00:51:09,960
right? And they follow causal laws. They follow the laws of physics. And there's very few parameters

696
00:51:09,960 --> 00:51:14,280
in that, right? We know that, you know, the, the laws of physics eventually have very few parameters.

697
00:51:15,080 --> 00:51:21,480
Um, and so, so that generative direction is a lot simpler than the opposite direction.

698
00:51:21,480 --> 00:51:26,440
You said something really interesting in there, though, uh, the entire phrase was, was understand

699
00:51:26,440 --> 00:51:32,440
the world in a generative way. When I think of most of the generative models that we're talking

700
00:51:32,440 --> 00:51:38,200
about, there's not any understanding there. It's just spitting out probabilistically the, the best

701
00:51:38,200 --> 00:51:45,080
next thing. Yeah. So, so understand, you know, thinking about, uh, what it really means to understand

702
00:51:45,080 --> 00:51:50,040
something is a whole different area. But you could think about, you know, predicting the future,

703
00:51:50,040 --> 00:51:54,440
right? So let's say, understand, if you understand the world at some level, I can predict the future

704
00:51:54,440 --> 00:52:01,400
better, right? And now let's imagine, you know, um, I need to do some of a prediction. And I don't

705
00:52:01,400 --> 00:52:06,920
know the causal laws. If I know the laws of physics and I know the laws of psychology and I know

706
00:52:06,920 --> 00:52:12,120
what causes what it's going to be far easier for me to predict the future than if I don't have all

707
00:52:12,120 --> 00:52:18,120
of that. Um, in fact, I can train something in one context and if I understand how physics work

708
00:52:18,120 --> 00:52:23,000
and I get into a completely different context, I can still predict the future, right? Um,

709
00:52:23,000 --> 00:52:29,400
because I understand the causal mechanisms. And so, so the claim is that the world is a lot

710
00:52:29,400 --> 00:52:35,320
simpler in the causal direction or in the sort of physical sort of data generative direction.

711
00:52:35,320 --> 00:52:41,240
And it's a lot more complicated than the opposite direction. Yet, if I define my application

712
00:52:41,240 --> 00:52:47,720
area narrowly enough, I can collect a huge amount of data on that particular narrowly defined

713
00:52:47,720 --> 00:52:53,400
problem. And then the inverse, you know, going directly from the sensors back to the predicting

714
00:52:53,400 --> 00:52:58,600
the object is going to be more effective, right? Because that's the deep learning direction.

715
00:52:59,240 --> 00:53:04,360
So with sufficient data, you know, that is the direction in which you actually also want to predict,

716
00:53:04,360 --> 00:53:10,200
which is the inverse direction from the generative model. And, you know, that is the most effective

717
00:53:10,200 --> 00:53:14,760
way of doing things if you have a lot of data and you have to find your domain narrowly enough.

718
00:53:14,760 --> 00:53:19,240
And I think therefore, we need to find sort of a middle ground. We basically have to say if,

719
00:53:19,240 --> 00:53:23,640
if, you know, if we don't know much about a domain, if we get thrown into a new situation,

720
00:53:23,640 --> 00:53:28,600
we need to rely on our our generative models of the world. And we need to, you know,

721
00:53:28,600 --> 00:53:33,720
try to, you know, invert those in order to make predictions. Yet, if we collect enough data

722
00:53:33,720 --> 00:53:39,560
in a certain domain, then we can form this direct pathway from the sensors directly to making

723
00:53:39,560 --> 00:53:44,840
predictions. And then that's going to be the most effective and accurate way of, of making predictions.

724
00:53:44,840 --> 00:53:50,440
How do you take action on this idea or how do we as a community of practitioners and,

725
00:53:50,440 --> 00:53:55,240
and researchers kind of take action on this idea? Well, it depends a little bit on what you're

726
00:53:55,240 --> 00:54:00,440
interested in, right? So if you're interested in a narrow domain problem, like you want to predict

727
00:54:00,440 --> 00:54:05,160
speech or you want to do speech translation or something like that, then you should go with the

728
00:54:05,160 --> 00:54:09,560
deep learning approach, right? Because it just that, just the best thing because you can collect a lot of data

729
00:54:09,560 --> 00:54:15,960
and works best. If you're interested in solving general AI, so developing, sort of agents that

730
00:54:15,960 --> 00:54:22,280
are versatile and that can operate in many different circumstances, right? Then I think, you know,

731
00:54:22,280 --> 00:54:27,800
you may have to start thinking about integrating these two models. And so what we have been doing

732
00:54:27,800 --> 00:54:40,600
in the lab, in sort of my sort of M lab at the university, is we have been taking a sort of somewhat

733
00:54:40,600 --> 00:54:45,960
older paradigm, which we call graphical models. So graphical models were models, which were very

734
00:54:45,960 --> 00:54:52,040
popular, like maybe 10 years ago or so, where you would take sort of nodes in a graph. They seem

735
00:54:52,040 --> 00:54:57,960
to be coming back a little bit, don't they? Maybe. I hear a ton about graphical models nowadays.

736
00:54:57,960 --> 00:55:02,600
Like in recent NURPs, the past couple of years in NURPs, a lot of people seem to be doing work

737
00:55:02,600 --> 00:55:08,840
around graphical models. Okay, well, it seems to stand to reason that, you know, something that is

738
00:55:08,840 --> 00:55:17,080
good eventually will find its way back. The deep learning fire has burned out a little bit. And then,

739
00:55:17,080 --> 00:55:21,800
of course, you know, you then you then get a phase where you can start to synthesize things,

740
00:55:21,800 --> 00:55:25,240
where you can say, okay, we had this in the past. We have now, there's both are really good things.

741
00:55:25,880 --> 00:55:29,640
Let's see if we can sort of combine them a little bit. So this is some of the work that we are

742
00:55:29,640 --> 00:55:36,120
doing. So we have sort of a model where we will sort of generate, you know, we have a generated

743
00:55:36,120 --> 00:55:42,440
model, let's say a Kalman filter, which is a dynamical model of, you know, somebody moving around

744
00:55:42,440 --> 00:55:49,000
in the world and sort of observing sort of things about the world. And in these graphical models,

745
00:55:49,000 --> 00:55:54,920
every node means something, right? That's the agent's position at every point in time is one node

746
00:55:54,920 --> 00:56:00,600
in this graph. And if you want to figure out, let's say, okay, if I have these sort of,

747
00:56:00,600 --> 00:56:07,320
these partial observations about the world, like a bunch of images maybe taken from the agent,

748
00:56:07,320 --> 00:56:13,960
can I try, can I infer, you know, where the agent was? And that you can do with something that we

749
00:56:13,960 --> 00:56:18,920
call inference. So that's a different inference than we these days, we call inference on the deep

750
00:56:18,920 --> 00:56:24,440
neural net. This is a probabilistic inference where we basically say, what's the probability

751
00:56:24,440 --> 00:56:30,760
distribution of this person being at this position at this point in time given all the observations

752
00:56:30,760 --> 00:56:38,680
I have right now. And so that's actually a message passing scheme. So you send messages over the

753
00:56:38,680 --> 00:56:43,960
edges of this graph to figure this out. That was belief propagation. And it was a very popular

754
00:56:43,960 --> 00:56:49,560
research topic, you know, 10 years ago. Now we also have something. So the other option would

755
00:56:49,560 --> 00:56:56,440
be to collect a huge amount of data. Basically, you know, person is here and observes this,

756
00:56:56,440 --> 00:57:00,520
if person is here observes this, right? So if you collect a huge amount of data, you can also do it

757
00:57:00,520 --> 00:57:07,640
the opposite way. You can say, okay, map directly from my observations, you know, back to the positions

758
00:57:07,640 --> 00:57:12,440
of this particular person, because I happen to have that data available. And if I have enough of

759
00:57:12,440 --> 00:57:17,880
that data, then I can actually do a better job, typically, than my Kelvin filter, because my,

760
00:57:17,880 --> 00:57:24,760
you know, big, my Kelvin filter has like strong assumptions on linearity and Gaussianity,

761
00:57:24,760 --> 00:57:31,720
um, basically build in it. And if the world isn't, you know, linear, um, then you're in trouble,

762
00:57:31,720 --> 00:57:37,640
because, you know, you'll make, you'll make wrong predictions. Um, but if you just build this

763
00:57:37,640 --> 00:57:42,120
neural network, which goes in the opposite direction, takes the observations and directly maps onto

764
00:57:42,120 --> 00:57:46,600
the, onto the locations, then, you know, if you put enough parameters in it and you have enough

765
00:57:46,600 --> 00:57:52,600
data, then you can train it very, very accurately, right? So now the trick becomes, okay, so if I put

766
00:57:52,600 --> 00:57:58,920
into a new environment, I don't have data. So I will rely on this sort of, you know, clunky

767
00:57:58,920 --> 00:58:03,400
Kelvin filter, but it will give me a fairly good estimate. And then as I go and collect more and

768
00:58:03,400 --> 00:58:08,120
more data, as I mean, that is, as I live in that environment, I'm going to train up the model in

769
00:58:08,120 --> 00:58:14,280
the opposite direction. Um, and, um, that at some point will become more accurate than my Kelvin

770
00:58:14,280 --> 00:58:20,280
filter. Um, and then, you know, I just basically switch to the sort of, to the, to the deep learning

771
00:58:20,280 --> 00:58:26,200
sort of solution. And so this is one example where, you know, we have a message passing scheme on a

772
00:58:26,200 --> 00:58:32,360
graph that, you know, automatically switches between either the old-fashioned sort of, um, inference

773
00:58:32,360 --> 00:58:37,480
and graphical model to the sort of more modern sort of graph convolutional neural networks.

774
00:58:38,200 --> 00:58:47,960
Interesting. A lot of the folks that are pursuing deep learning as a path to AGI kind of

775
00:58:47,960 --> 00:58:53,720
ultimately feel like the, you know, this thing that we call AGI or maybe the step before AGI will be

776
00:58:55,160 --> 00:59:02,920
an ensemble of perhaps many, uh, deep models as opposed to, you know, one single Uber model.

777
00:59:03,720 --> 00:59:11,720
If you kind of apply that same thinking to what you're describing, you can envision an ensemble

778
00:59:11,720 --> 00:59:21,560
of many deep models and many generative models. And I'm almost thinking of like a hybrid car.

779
00:59:21,560 --> 00:59:25,400
If you've ever, if you've been in a Prius recently, you might have seen that little picture where

780
00:59:25,400 --> 00:59:31,720
they show you like whether it's the battery that's driving the motor or the, the motor that's kind

781
00:59:31,720 --> 00:59:36,680
of charging the battery, kind of this back and forth flow, depending on what's happening and what

782
00:59:36,680 --> 00:59:43,320
the model is being or the agent is being exposed to that's determining, you know, whether we're,

783
00:59:43,960 --> 00:59:49,400
um, you know, relying more on the deep model in any particular point in time or the generative

784
00:59:49,400 --> 00:59:54,200
model. Is that kind of the way you, you know, a way that you can see this evolving?

785
00:59:54,920 --> 01:00:01,160
Yeah. So that's certainly, um, the way I see this evolving. So, um, let me stay with the example

786
01:00:01,160 --> 01:00:07,240
of a car, maybe so you couldn't actually a modern car, even a self-driving car, would still have

787
01:00:07,240 --> 01:00:13,160
a whole bunch of rule sets to cover all the corner cases. Um, because you can't collect enough data

788
01:00:13,160 --> 01:00:19,320
on these corner cases, but maybe in the future, you know, there are enough, you know, for a particular

789
01:00:19,320 --> 01:00:24,360
set of corner cases, let's say, uh, you know, you're driving on freeways, you can do fine, you've

790
01:00:24,360 --> 01:00:30,120
collected a lot of data, but now you turn into Amsterdam, right? And there's so many exceptions

791
01:00:30,120 --> 01:00:35,320
and difficult situations where bicycles will crush you and pedestrians will do weird things and

792
01:00:35,320 --> 01:00:40,280
walk through red lights and all that, what's happening in Amsterdam. So you cannot rely on,

793
01:00:40,280 --> 01:00:45,160
on just a learned model because it will fail, right? And so you have to basically go back to,

794
01:00:45,160 --> 01:00:51,160
you know, if this, then that, if this, then that, um, and, um, and you have to know when to switch,

795
01:00:51,160 --> 01:00:57,480
right? And so, and then maybe I can look back to Bayesian statistics. So you have to understand

796
01:00:57,480 --> 01:01:02,440
when you don't understand, right? So it is deep learning model or this machine learning model.

797
01:01:02,440 --> 01:01:08,680
Well, basically have to figure out, okay, so I'm running out of my, you know, domain where I'm

798
01:01:08,680 --> 01:01:14,680
trained. Um, I'm starting to fail here. I'm going to switch over to rules or I'm going to switch

799
01:01:14,680 --> 01:01:20,680
over to the actual driver, um, to make sure I don't get into an accident. Um, and it's this

800
01:01:20,680 --> 01:01:26,520
interaction where, you know, and, and then you can imagine where a lot of cars are slowly,

801
01:01:26,520 --> 01:01:31,080
you know, getting into new situations and they're all learning distributively, you know,

802
01:01:31,080 --> 01:01:35,080
collectively, they're learning about some of these corner cases and they get embedded into the

803
01:01:35,080 --> 01:01:39,640
model and then we slowly switch, you know, to using that model. And what's interesting to me about

804
01:01:39,640 --> 01:01:48,280
that example is that the, the premise seems to be that we will get to a point where the complexity

805
01:01:48,280 --> 01:01:54,360
of the environment is too much for the learned model and then we need to switch to rules.

806
01:01:54,360 --> 01:02:01,800
Uh, but a lot of the, where learn models have proven themselves to be effective and powerful or,

807
01:02:01,800 --> 01:02:07,320
you know, these situations where we can't come up with the rules because the rules are too complex.

808
01:02:07,960 --> 01:02:14,360
Yeah. So, you know, clearly, um, in many situations where you have enough data, you should not

809
01:02:14,360 --> 01:02:20,520
use rules, um, because, you know, it's just not good enough, right? Um, in fact, you should just

810
01:02:20,520 --> 01:02:26,120
train that mapping directly. But the advantage of rules is that you can express them in human

811
01:02:26,120 --> 01:02:31,640
language, right? I mean, we know in some sense when they're driving a car, right? You know, we know,

812
01:02:31,640 --> 01:02:36,760
you know, when to stop for a red light, you know, when to, you know, when, when, when somebody is,

813
01:02:36,760 --> 01:02:41,480
you know, passing the street or something like that or complicated combinations of situations,

814
01:02:42,520 --> 01:02:49,240
we know how to, you know, express them in human language and turn them into a rule. I mean,

815
01:02:49,240 --> 01:02:52,920
there's going to be a gigantic number of rules. I think it's, I don't know, genre,

816
01:02:52,920 --> 01:02:58,440
million lines of codes in a car or something like that. So it's like huge. Um, but, you know,

817
01:02:58,440 --> 01:03:03,960
it's, it is in some sense, you know, a backup system for situations that you can't cover with

818
01:03:03,960 --> 01:03:09,800
your deep learning yet. And as I understand it, um, actually, I've, when I heard a talk about this,

819
01:03:09,800 --> 01:03:17,480
uh, by Raquel Earthasoon actually, uh, working for Uber, uh, she mentioned that, um, uh, a large number

820
01:03:17,480 --> 01:03:22,760
of, you know, the, the, you know, the large fraction of the intelligence of a car is actually

821
01:03:22,760 --> 01:03:28,600
still rule based. I'm sure they want to move more to deep learning, but it's still a pretty large

822
01:03:28,600 --> 01:03:35,400
fraction is still rule based. Awesome. Well, Max, this has been an amazing discussion. Uh, we could

823
01:03:35,400 --> 01:03:42,520
continue. I'm sure for, uh, another hour, but, um, I really appreciate you taking the time to,

824
01:03:42,520 --> 01:03:49,160
uh, uh, jump on with us and to share a bit about what you're working on. Thanks. It was a pleasure

825
01:03:49,160 --> 01:03:58,280
talking to you. Fantastic. Thank you very much. All right, everyone. That's our show for today.

826
01:03:58,280 --> 01:04:04,040
If you like what you've heard here, please do us a huge favor and tell your friends about the show.

827
01:04:04,040 --> 01:04:08,840
And if you haven't already hit that subscribe button yourself, make sure you do so you don't miss

828
01:04:08,840 --> 01:04:14,680
any of the great episodes we've gotten in store for you. As always, thanks so much for listening

829
01:04:14,680 --> 01:04:44,520
and catch you next time.

