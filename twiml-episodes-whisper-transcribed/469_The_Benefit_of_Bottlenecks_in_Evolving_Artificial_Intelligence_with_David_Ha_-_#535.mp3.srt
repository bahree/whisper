1
00:00:00,000 --> 00:00:16,240
All right, everyone. I am here with David Ha. David is a research scientist at Google Brain.

2
00:00:16,240 --> 00:00:20,080
David, welcome to the Twomal AI podcast. Thanks for having me, Sam.

3
00:00:21,280 --> 00:00:28,160
Hey, I'm really looking forward to diving into our conversation. I've been a long time follower

4
00:00:28,160 --> 00:00:34,400
viewers on Twitter, and I definitely recommend folks to check you out there at Hard Maroo.

5
00:00:34,400 --> 00:00:39,120
Why don't we get started by having you share a little bit about your background and how you came

6
00:00:39,120 --> 00:00:48,560
to work in AI? Yeah, sure. It's kind of a weird background. I was originally studying

7
00:00:49,680 --> 00:00:56,160
control systems back in the day in university. Eventually, for some one reason or another,

8
00:00:56,160 --> 00:01:06,400
I entered the finance industry. I started off as a quants on Wall Street, actually.

9
00:01:07,120 --> 00:01:13,200
I started working at banks, and then eventually I became worked on a trading desk as a trader,

10
00:01:13,200 --> 00:01:19,920
and I spent around 10 years or so of my life in the derivatives trading, various different investment

11
00:01:19,920 --> 00:01:28,480
banks, but things got a bit old and tried to learn different things. I was always interested in

12
00:01:28,480 --> 00:01:35,200
neural networks because they're always fascinating, especially the biological inspired component,

13
00:01:35,200 --> 00:01:43,280
and I started to do some reading and learning by myself. One thing that to the other, and

14
00:01:43,280 --> 00:01:52,800
around the five years ago, I was able to join Google in one of their research residency programs,

15
00:01:52,800 --> 00:02:00,720
and as a researcher, so that I was able to change careers and became a four-time AI researcher.

16
00:02:01,680 --> 00:02:10,400
And so this is where I am now. That's awesome. That's awesome. As the idea or the attraction,

17
00:02:10,400 --> 00:02:15,120
the initial attraction to the biological inspiration, has that held up for you? Do you

18
00:02:16,640 --> 00:02:26,320
feel like the biological inspiration continues to inspire you, or was it a let down to find out

19
00:02:26,320 --> 00:02:33,040
that the neural networks and computers are not all that similar to the biological ones?

20
00:02:33,040 --> 00:02:40,160
I think to this day is still continues to inspire me and drives some of my work,

21
00:02:41,040 --> 00:02:48,000
but we do have to recognize that modern deep learning or machine learning systems are very

22
00:02:48,000 --> 00:02:55,040
different than biological processes. For one thing, we can scale them up. We have lots of

23
00:02:55,040 --> 00:03:03,760
electricity and compute power, and the trend is actually having more compute resources,

24
00:03:03,760 --> 00:03:10,080
and for machine learning and training to increasingly scale to larger models and larger datasets

25
00:03:10,080 --> 00:03:16,800
and larger environments. And it's a bit different than biology, because in biological systems,

26
00:03:16,800 --> 00:03:24,800
it's more like a biological intelligent life is more like coming from and evolving because we

27
00:03:24,800 --> 00:03:30,320
have not because we have an abundance of resources, but more like we have a lack of it.

28
00:03:30,960 --> 00:03:39,200
And I was fascinated at how evolution seems to select systems that are able to always do more

29
00:03:39,200 --> 00:03:47,840
with less. But in a way, it's not just biological systems, but also the creativity process as well.

30
00:03:47,840 --> 00:03:55,200
Sometimes we see some creative works. It's always like you're able to express more with less.

31
00:03:55,200 --> 00:04:04,560
And I think the good, the interesting thing about being a researcher, especially at Google is

32
00:04:04,560 --> 00:04:10,880
you do have a lot of resources. So you get to see both ends of the spectrum. On one hand,

33
00:04:10,880 --> 00:04:20,320
you do get to see people who are really excited at scaling up the research and making very large

34
00:04:20,320 --> 00:04:26,000
systems to work on large datasets. And on the other end of the spectrum, you have people working

35
00:04:26,000 --> 00:04:34,240
on theory or on coming from theoretical physics backgrounds. And actually, they may not even

36
00:04:34,240 --> 00:04:43,200
do a lot of extensive computational modeling. So it's good to see a balance of the spectrum of

37
00:04:43,200 --> 00:04:53,600
a resource-heavy stuff, and also the things that concentrate on having very low resources.

38
00:04:54,480 --> 00:05:01,520
And ultimately, you need both. You can have large models and you have a smart chips that run them

39
00:05:01,520 --> 00:05:07,920
with less power. You alluded to this idea of constraints as playing a role in the way you think

40
00:05:07,920 --> 00:05:16,960
about machine learning systems. Can you elaborate a bit on that? Yeah. Getting back to the idea of

41
00:05:16,960 --> 00:05:25,760
constraints we see in nature, it certainly plays a role in shaping some of the research work

42
00:05:25,760 --> 00:05:34,320
that we've been doing. In nature, for example, there are lots of examples of these so-called

43
00:05:34,320 --> 00:05:41,120
bottlenecks that shape their development as a species. Just looking at the fascinating way

44
00:05:41,120 --> 00:05:49,040
of how our brain is wired, how our consciousness is able to process abstract thought.

45
00:05:49,600 --> 00:05:53,040
We have a language I'm talking to you in language, even though we have a video feed.

46
00:05:53,040 --> 00:06:00,000
And also how we're able to convey concepts to each other, not just using languages,

47
00:06:00,000 --> 00:06:07,200
but using drawings or gestures like this. And that's developed into languages, stories and

48
00:06:07,200 --> 00:06:16,800
cultures. I guess to me, it's kind of debatable whether these bottlenecks or constraints

49
00:06:16,800 --> 00:06:23,120
from our development is a requirement for intelligence to emerge. But it's also not

50
00:06:23,120 --> 00:06:30,320
deniable that our own intelligence is a result of these constraints. On one hand,

51
00:06:30,320 --> 00:06:38,400
maybe the argument is just because we have constraints that led to us, it doesn't mean we cannot,

52
00:06:38,400 --> 00:06:46,080
I have the development of a general, a strong AI needs to have such constraints.

53
00:06:46,080 --> 00:06:51,680
So there are opposite views of this spectrum. But for my research work, it's more like a

54
00:06:51,680 --> 00:07:00,800
led by the idea of constraints. And you can see this from some of the work I've done even very

55
00:07:00,800 --> 00:07:06,400
earlier, a few years ago, when I started to get into things like generative models.

56
00:07:08,000 --> 00:07:14,960
Back then, GANs were really taking off in 2017 or 2016. They started to take off

57
00:07:14,960 --> 00:07:23,680
and at the beginning, people were really excited at generating C-Fart 10 images, 32 by 32 pixels.

58
00:07:23,680 --> 00:07:32,720
But then they got bigger, 64 by 64, 128 by 128, pictures of a datasets of CD albums or something

59
00:07:32,720 --> 00:07:38,720
really cool. And so there's all this exciting work going on. And I had my share of playing around

60
00:07:38,720 --> 00:07:50,320
with these generative models as well. So a few early works I've done is to build a generative model

61
00:07:50,320 --> 00:07:58,640
for MNIST, the simplest dataset ever. But rather than taking the approach of generating pixels

62
00:07:58,640 --> 00:08:07,440
directly, I try to generate a parameterization of MNIST, which can be very abstract in nature.

63
00:08:07,440 --> 00:08:15,360
And with that led to like an early work, I combine some of the models from another researcher,

64
00:08:15,360 --> 00:08:23,120
Ken Stanley at the time, who designed this network called CPPNs. So if you're familiar with that,

65
00:08:23,120 --> 00:08:27,760
it's more like you take in the pixels. And you take in the coordinates and it outputs a pixel.

66
00:08:28,480 --> 00:08:35,440
So if you have a simple rule that can take in the coordinate and output a pixel,

67
00:08:35,440 --> 00:08:39,840
then you actually don't need to train the network to output the entire pixel. You just train

68
00:08:39,840 --> 00:08:45,680
the network to simply give you the coordinate, I'll give you the pixel value. So I train such a

69
00:08:45,680 --> 00:08:52,960
network to generate MNIST, so it's very elegant. And the end process is you can actually train it

70
00:08:52,960 --> 00:09:00,480
on an MNIST dataset, 28 by 28, into this half-stract generative model, which I call a CPPN,

71
00:09:00,480 --> 00:09:08,960
EAE or GANs. And then you can actually blow it up and generate MNIST digits that are like

72
00:09:08,960 --> 00:09:20,480
1,000 by 1,000 resolution back in 2016. So before people can do, now we can actually model GANs on

73
00:09:20,480 --> 00:09:29,680
1,000 by 1,000 resolution datasets with our exponentially increasing hardware. But I thought at the

74
00:09:29,680 --> 00:09:38,960
time, it was kind of cool to be able to train again in 2016. Right after the EN Goodfellow is

75
00:09:38,960 --> 00:09:44,960
again paid for K-vogue for a few months, and you're able to also produce 1,000 by 1,000 resolution

76
00:09:44,960 --> 00:09:52,400
images by skipping entirely the need to produce such big images. And the key is to abstract the

77
00:09:52,400 --> 00:10:03,520
principles of that image into an abstract representation using the CPPNs. And later work, I kind of

78
00:10:03,520 --> 00:10:10,080
follow the same trend. I looked at creating a generative model of doodles. There's a model,

79
00:10:10,080 --> 00:10:16,480
I don't know, you probably played around with it called a sketch RNN, that you can interactively

80
00:10:16,480 --> 00:10:22,560
draw something on the web browser. And the model is like an anguish model. It'll continue to

81
00:10:22,560 --> 00:10:28,640
predict what you're going to draw, like a stroke by stroke in a vector format. It's also like

82
00:10:28,640 --> 00:10:35,600
an auto encoder model as well. So you can draw a full pig and then you can compress it into a

83
00:10:35,600 --> 00:10:41,840
lean space and then redraw the pig out. So now is a very trivial in retrospect. But a few years ago,

84
00:10:41,840 --> 00:10:53,440
it was one of the different models, because most people working on GANs and generative models

85
00:10:53,440 --> 00:10:59,840
on pixels. And we're trying to do it on doodles. And at the time, the challenging thing was finding

86
00:10:59,840 --> 00:11:10,320
the datasets. And luckily, one of my colleague, Jonas, at Creative Labs, they created a viral

87
00:11:10,320 --> 00:11:16,640
game called a quick draw that collected some of this doodle data that we can use. So I thought

88
00:11:16,640 --> 00:11:26,080
that was kind of fun, man. For that project, it was more inspired by, rather than trying to create

89
00:11:26,080 --> 00:11:32,560
a representation, a minimalist representation, maybe we can use machine learning to study how

90
00:11:32,560 --> 00:11:42,240
we humans ourselves do like a representation learning. Because of our own inductive biases,

91
00:11:42,240 --> 00:11:49,440
you know, we're forced to draw doodles, maybe from the time we're cave people, because we have

92
00:11:49,440 --> 00:11:55,200
a we have hands and we have sticks. So we develop this type of drawing. So maybe it's, you know,

93
00:11:55,200 --> 00:12:01,200
a good idea, a good idea to get machine learning models to analyze how we develop this

94
00:12:01,200 --> 00:12:09,040
representation. And that could lead to other ideas. And one of those, the ideas after

95
00:12:09,040 --> 00:12:15,920
this sketch RNN paper was when I started to get into doing some work on reinforcement learning.

96
00:12:17,680 --> 00:12:23,680
And, you know, like, what I thought was we have all of these cool algorithms that can train agents

97
00:12:23,680 --> 00:12:31,120
to perform tasks when the agents are fed pixels like the entire screen, which was really amazing

98
00:12:31,120 --> 00:12:38,960
at the time. At the time, the DQM model from the mine came out and agents started playing

99
00:12:39,520 --> 00:12:46,240
pong or like Atari games, the entirety of pixels. I thought that was cool. But in a way, I kind of

100
00:12:47,440 --> 00:12:52,640
think that could be information overload as well as most of the pixels are not useful when you're

101
00:12:52,640 --> 00:13:00,880
playing like pong games or Atari games. So what I try to do is like, you know,

102
00:13:02,400 --> 00:13:11,120
have enforced a type of constraints onto the policy or the controller so that it's not allowed

103
00:13:11,120 --> 00:13:17,520
to see the full pixel information or the stream of pixels. And it's only allowed to see a

104
00:13:17,520 --> 00:13:25,840
representation of its environments. So that, that work led to a model, a paper called the

105
00:13:25,840 --> 00:13:33,360
World Models. It's kind of an exploratory paper that I published a few years ago with

106
00:13:33,360 --> 00:13:40,800
JÃ¼rgen Schmittuber. And it was a really fun project. So the idea is we have a really simple

107
00:13:40,800 --> 00:13:48,320
generative model. We use a variational auto-encoder to simply compress all of the screens into a

108
00:13:48,320 --> 00:13:54,640
load-dimensional latent space. And we have another recurring neural network that simply predicts

109
00:13:54,640 --> 00:14:00,320
the future latent space of the environments. So if you have a VAE that's trained on,

110
00:14:00,880 --> 00:14:06,800
trained on your your game that produces a load-dimensional latent vector, your R&M will predict

111
00:14:06,800 --> 00:14:11,200
your future latent vectors. That, that, like, depending on its current.

112
00:14:11,200 --> 00:14:12,960
Are you projecting the future state of the game?

113
00:14:13,600 --> 00:14:23,040
Yes, exactly, exactly. So that was a fun project because we're able to use these two simple concepts

114
00:14:23,040 --> 00:14:33,120
to build a neural simulator of, like, games if we're able to connect enough data on it. And

115
00:14:33,120 --> 00:14:38,560
what, what was fun about the project is we can, we show that we can just feed in

116
00:14:40,080 --> 00:14:44,880
the representations learned from such a model, like the VAE's latent vector and also the

117
00:14:44,880 --> 00:14:51,200
R&M's hidden state and feed it to an agent. And, like, at the time, this, we show that,

118
00:14:51,760 --> 00:14:59,120
this hidden vector, like, this small, this bottleneck allowed the agents to, to discover policies

119
00:14:59,120 --> 00:15:06,640
much, much more easier than compared to, you know, like, having to see the entire pixel information

120
00:15:06,640 --> 00:15:11,040
is from an optimization standpoint. It's easier to figure out what you have to do if you're only

121
00:15:11,040 --> 00:15:19,120
given, like, maybe 200 numbers and give me an action compared to if you're given, like, you know,

122
00:15:19,120 --> 00:15:23,760
a million numbers every time set, give me an action, right? So because of that,

123
00:15:23,760 --> 00:15:30,400
it was able to, like, solve tasks like the car racing game and open the agent.

124
00:15:31,760 --> 00:15:37,360
Back then, it was, like, considered a hard task. Now it's trivial. But no one was able to get

125
00:15:37,360 --> 00:15:43,520
the required score. I'm sure people tried hard enough, they could. But at the time, this was

126
00:15:43,520 --> 00:15:49,600
the first approach that was able to get the required score for that game, you know, which was,

127
00:15:49,600 --> 00:15:56,800
I guess, from, from the machine learning research point of view, it was considered state of the art.

128
00:15:59,200 --> 00:16:04,880
I'm sure it'd be tweaked. Any model of it enough, you can also beat it. But, but apart from that,

129
00:16:06,080 --> 00:16:15,840
with those kind of results, have we seen that idea of, you know, constraining your latent space

130
00:16:15,840 --> 00:16:24,080
who come generally used as part of state of the art approaches in RL and similar areas?

131
00:16:24,960 --> 00:16:31,120
Yeah, yeah, definitely. So from that, from that paper, I think, you know, whether we got state

132
00:16:31,120 --> 00:16:35,200
of the art or not, it doesn't really matter for, in general, in machine learning papers, because

133
00:16:35,200 --> 00:16:43,200
it'll always be beaten by later on. But the idea on that paper of that, you can, you can train

134
00:16:43,200 --> 00:16:50,080
and you can learn a janitor model and train the agents entirely inside of that model to produce

135
00:16:50,080 --> 00:16:58,800
a policy. That, that was the main idea that, that seems to have taken off in subsequent works.

136
00:17:00,240 --> 00:17:07,360
Like, for example, after the world model's paper, there was another paper about, about

137
00:17:07,360 --> 00:17:16,080
the model base learning for Atari. So, and where they literally caught their algorithm simple.

138
00:17:17,040 --> 00:17:21,440
And the idea is basically, okay, you would, you would, you would, you have your agents collect data,

139
00:17:22,160 --> 00:17:30,480
train a generative model of the environment to predict the future. And then, and then, and then train

140
00:17:30,480 --> 00:17:36,000
your policy inside of that environment only. Right? Of course, at the beginning, you're not going to

141
00:17:36,000 --> 00:17:40,240
get a good policy, but then it doesn't matter. You deploy that policy out and you collect

142
00:17:40,240 --> 00:17:46,560
more data. And then you would refine your model and then you would redeploy it. So, so at,

143
00:17:46,560 --> 00:17:53,920
when that work came out in 2019, at the time, that was then the state of the art for, for,

144
00:17:54,960 --> 00:18:01,680
for sample efficiency for various Atari games, because simply because the learning took place in

145
00:18:01,680 --> 00:18:09,600
the model. Because a lot of the sample efficiency, we've been, if we noticed, when we run an

146
00:18:09,600 --> 00:18:15,840
RL algorithm, is, you know, you have the data collection process, but you're also learning

147
00:18:15,840 --> 00:18:22,080
Indian environments. And that, that could, there could be some slippage in the efficiency.

148
00:18:22,080 --> 00:18:29,360
So, if you're able to isolate the data collection from, from policy learning and your interactions

149
00:18:29,360 --> 00:18:37,280
through the environment is strictly for data collection and for evaluation policy of your policy.

150
00:18:37,280 --> 00:18:40,960
And all of your learning is done in the model. Then, then intuitively, that would help,

151
00:18:42,560 --> 00:18:48,320
that would also help the data efficiency. And another line of work done by my colleague, Danny

152
00:18:48,320 --> 00:18:56,160
J.R. Halfner, who is also working at a Google, but based in Toronto is, is a new, he started using

153
00:18:56,160 --> 00:19:03,040
these latent based role models and combined them with planning algorithms. So, like traditionally,

154
00:19:03,040 --> 00:19:09,280
planning algorithms are really useful for robotics. But at the same time, they're kind of flaky as

155
00:19:09,280 --> 00:19:16,160
well, especially when it comes to, when you're getting the, like, for example, four video feeds

156
00:19:17,200 --> 00:19:25,840
of sensory data, maybe traditionally, a lot of the planning algorithms were used on the state

157
00:19:25,840 --> 00:19:32,320
observations. So, you're feeding, like, you know, like a really well engineered measurements

158
00:19:32,320 --> 00:19:37,280
of your robot controller, right? But, like, how, then, how the key question is, how do you get

159
00:19:37,280 --> 00:19:45,920
your robot controller or your control system to work on video feeds? So, then, I guess,

160
00:19:45,920 --> 00:19:52,640
something like a world model or a latent based world model with, with this latent bottleneck

161
00:19:52,640 --> 00:19:58,800
could be useful for planning algorithms, because then you, if they're really good at working with

162
00:19:58,800 --> 00:20:06,640
load-dimensional data. So, then you give it load-dimensional data. So, the key idea behind that line of

163
00:20:06,640 --> 00:20:13,760
work initially started by a model called a planet. So, it's a great kind of name is to, you have

164
00:20:13,760 --> 00:20:20,880
you have this kind of latent, latent spatial temporal world model that is constantly updated as you

165
00:20:20,880 --> 00:20:28,000
get more data, and you have a planner that would, that would get, that would figure out the optimal

166
00:20:28,000 --> 00:20:33,200
action within the model. So, then, you don't actually go into doing any learning.

167
00:20:34,480 --> 00:20:41,040
This is also helped with generalization. You'd think that a lower latent space model

168
00:20:42,080 --> 00:20:49,280
has gotten rid of some of the noise that the full, the full world or environment

169
00:20:49,280 --> 00:20:57,360
contains. And so, the agent might be able to perform, the agent performance might transfer

170
00:20:57,360 --> 00:21:01,760
from one specific environment to another better. Is that actually the case?

171
00:21:02,880 --> 00:21:12,160
That is still like an open question. For instance, if I naively train that world model based

172
00:21:12,160 --> 00:21:19,600
on based on the data that we collect, then like, no, it's not going to generalize to variations.

173
00:21:21,040 --> 00:21:30,400
An example is if we change the background color of your environments, then yeah, your VAE

174
00:21:30,400 --> 00:21:36,160
or your latent model has never seen that before. And that generalization can only be done

175
00:21:36,160 --> 00:21:45,280
in VAE learning. So, that algorithm would need to collect the new data and relearn its world again.

176
00:21:47,280 --> 00:21:56,800
And whether it can generalize or not will be a question of how many shots, how many time

177
00:21:56,800 --> 00:22:04,000
steps it has to generalize rather than a zero shot. But that being said, there is like a line of

178
00:22:04,000 --> 00:22:12,640
work on looking at generalization problems within latent space models. There's actually a few

179
00:22:12,640 --> 00:22:20,000
challenges. There's a deep mind robotic control, have a variation with explicitly introduced

180
00:22:20,000 --> 00:22:29,120
lots of distractions and changing the backgrounds. And then you can employ lots of all sorts of ways,

181
00:22:29,120 --> 00:22:34,320
like rather than training like an image-based latent space, you can do contrasts of learning.

182
00:22:34,320 --> 00:22:43,520
There's a line of work doing that and so on. But for me, around that time, I also step back a bit.

183
00:22:44,240 --> 00:22:48,960
There's all these, I have the same question as you, you know, we can this generalization.

184
00:22:48,960 --> 00:22:54,480
Yeah, of course, there's lots of different ways of doing these latent space models.

185
00:22:54,480 --> 00:23:02,320
But I looked at, along with my colleagues and my team, we started to explore them.

186
00:23:02,320 --> 00:23:08,800
It's maybe latent space bottlenecks is one solution, but it might not be the best solution

187
00:23:08,800 --> 00:23:16,160
for these generalization tasks. So we looked at maybe another bottleneck we can use is

188
00:23:16,160 --> 00:23:24,160
something like attention. Or in our case, we try to use a hard attention. So like in a paper,

189
00:23:24,160 --> 00:23:30,160
we published two years ago called a neural evolution of self-interpretable agents,

190
00:23:30,160 --> 00:23:37,360
which is let by my colleague, Eugene Tang, rather than using a latent space to do this bottleneck.

191
00:23:38,560 --> 00:23:46,480
The idea is we will only allow an agent to see 10 patches, for instance, of the screen.

192
00:23:46,480 --> 00:23:55,840
And its decision is solely based on those 10 patches. Or however, however, number of patches we want.

193
00:23:55,840 --> 00:24:05,920
So it's kind of like, you know, biological vision for our fovea type system where we have to really,

194
00:24:05,920 --> 00:24:11,200
you know, like a one when when we study how humans see things, it's always like, you know,

195
00:24:11,200 --> 00:24:20,640
attending to a bunch of points in front of us. But somehow we have a mental understanding of what

196
00:24:20,640 --> 00:24:26,000
we're seeing. But it's not like we're getting like, you know, full on HD resolution directly in my

197
00:24:26,000 --> 00:24:31,600
eyes. I'm actually seeing a bunch of things. So it's kind of inspired by that.

198
00:24:32,080 --> 00:24:39,920
And in this example or paper is the where the patches, were you trying to emulate like a visual

199
00:24:39,920 --> 00:24:44,800
field and the patches were kind of contiguous in a particular arrangement or were they,

200
00:24:45,920 --> 00:24:51,520
you know, randomly distributed across the image. Oh yeah, so for this work,

201
00:24:52,960 --> 00:25:01,200
it's part of the policy actually. So rather than having a randomized frame, the agent actually

202
00:25:01,200 --> 00:25:12,080
has to learn to decide first which 10 patches to choose. Right. So like it's like it's like how

203
00:25:12,080 --> 00:25:18,960
when you're looking at me, somehow you're deciding where to position your your eyeball on the

204
00:25:18,960 --> 00:25:26,160
screen. So in the same way, the agent has to decide which 10 or it doesn't have to be 10. It could

205
00:25:26,160 --> 00:25:32,080
be one or two that don't still work. But we do a sweep and we can be a bit more general and then

206
00:25:32,080 --> 00:25:39,680
choose five or 10. So it's less like I was originally thinking it was along the lines of masking

207
00:25:39,680 --> 00:25:46,800
for kind of generalization or regularization. This is more learning where to attend to within the

208
00:25:46,800 --> 00:25:55,920
image as a as a constraint. Yes, exactly. And it was it's also inspired by a line of work in

209
00:25:55,920 --> 00:26:03,680
psychology a few decades ago. The whole concept of a script you heard about is this in

210
00:26:03,680 --> 00:26:16,640
in a tentative selective attention. So this is an intentional blindness. So sometimes our brains

211
00:26:16,640 --> 00:26:22,560
just don't see part of the screen or part of what we see. So there was a psychology experiment done

212
00:26:22,560 --> 00:26:32,160
back then where the subjects were asked to to look at a scene and the scene had two two teams of

213
00:26:32,160 --> 00:26:38,960
basketball players. One wearing white shirts and the second one wearing black shirts. And I think

214
00:26:38,960 --> 00:26:45,600
the subjects have to count the number of times that's the ball was passed between the white shirt

215
00:26:46,400 --> 00:26:50,400
the players to the black shirt players and something like that. And then there's a gorilla working

216
00:26:50,400 --> 00:26:56,400
walking up from the background. And most of the time the subjects were not able to to see the gorilla

217
00:26:56,400 --> 00:27:03,920
because they're so focused on the ball and the colors of what people are wearing. So that kind of

218
00:27:03,920 --> 00:27:14,080
helped create some some analogies between okay we have this thing called whether we like it or not

219
00:27:14,080 --> 00:27:20,320
called intentional blindness. What if we try to do something like that with an RIO agents? What are

220
00:27:20,320 --> 00:27:25,920
the pros and cons? Does it give it more abilities or does it actually deduct some of the abilities?

221
00:27:25,920 --> 00:27:31,440
And that's that's what we're trying to explore. And it turns out that's using this simple scheme

222
00:27:32,000 --> 00:27:37,760
we were also able to train some simple agents to do the same task as the world models paper

223
00:27:37,760 --> 00:27:42,960
like getting a pretty good score on a car racing game from pixels and playing the dune game.

224
00:27:42,960 --> 00:27:50,240
But unlike the previous latent space models, this model we found can easily adapt

225
00:27:50,240 --> 00:27:58,960
to to augmentations to the environments. Like for instance, if we in doom in that doom

226
00:27:58,960 --> 00:28:06,720
this in environments if we change the color of the ground it will still work. If we add like a

227
00:28:06,720 --> 00:28:12,320
little blob on the side of the tracks on the car racing game, it will still work. And the reason

228
00:28:12,320 --> 00:28:20,960
is those patches are likely not to be selected by the pre-trained agents. So so it's just simply

229
00:28:21,600 --> 00:28:26,960
it works because it's just not attending to the things that deemed not to be that relevant

230
00:28:28,400 --> 00:28:35,760
to some extent. Of course this is very like a naive way of like approaching the problem

231
00:28:35,760 --> 00:28:41,520
because like in reality it's very nuanced like we do see a bit of it but it's kind of a simple

232
00:28:41,520 --> 00:28:49,040
model that that clearly demonstrates that in attentional blindness if we strictly enforce it

233
00:28:49,040 --> 00:28:55,200
in the context of NRL agents it will have these properties that because it's simply not allowed

234
00:28:55,200 --> 00:29:01,360
to see certain parts of the screen it may lack if you throw away information but you also gain

235
00:29:01,360 --> 00:29:10,880
ability to to generalize to to changes in the environments. Yeah yeah and does it how does it compare

236
00:29:11,760 --> 00:29:18,480
from a sample efficiency to the constrained latent space does it retain that advantage in some way?

237
00:29:19,440 --> 00:29:26,720
For for this one no no because it actually takes a bit more time to train or to evolve the policy

238
00:29:26,720 --> 00:29:33,680
for to be able to perform the task but but the way I think about these issues is that there's

239
00:29:33,680 --> 00:29:41,920
a few dimensions you know you can you can work on optimizing the sample efficiency like maybe reducing

240
00:29:41,920 --> 00:29:49,840
an RL algorithm from you know 200 million time steps to 100 million time steps to achieve some

241
00:29:49,840 --> 00:29:57,440
score or you can think of a sample efficiency in terms of of a zero shot transfer so one can

242
00:29:57,440 --> 00:30:04,080
argue that okay I spent all of this time figuring out the policy using a hard attention in this

243
00:30:04,080 --> 00:30:11,600
paper but if you give it a new environment which is not the same as the original environment but

244
00:30:11,600 --> 00:30:18,640
one that has has some augmentations to it we can argue that that's a new environment and and how

245
00:30:18,640 --> 00:30:24,000
many time steps would it take your agents to adapt to that new environment and here the case is

246
00:30:24,000 --> 00:30:30,800
zero because it's a zero shot so that's also like another way of looking at the sample efficiency

247
00:30:30,800 --> 00:30:37,360
as well not not on the training task but on the task that it has never seen in life kind of arguing

248
00:30:37,360 --> 00:30:43,600
for a global sample efficiency in a sense across multiple problems or versions of a problem

249
00:30:43,600 --> 00:30:50,640
is exactly or in in our case I think I'm really interested in in in sample efficiency across

250
00:30:50,640 --> 00:30:59,120
on the same versions of the problem and that that's basically what what one of the the goals

251
00:30:59,120 --> 00:31:07,520
of of AI is it's like of course given enough compute we're we're gonna solve every known problem

252
00:31:07,520 --> 00:31:12,880
that is well defined but one of the thing is that this thing which is us from machines so far is

253
00:31:12,880 --> 00:31:20,080
like our ability to to solve problems quickly that that we have not seen before with variations

254
00:31:21,120 --> 00:31:29,200
you mentioned earlier your work with Ken Stanley and you just mentioned this concept of evolution

255
00:31:29,200 --> 00:31:34,720
I spoke to him probably several years ago talking about his work in neuro evolution

256
00:31:34,720 --> 00:31:43,440
and this we're using evolution loosely or have you also studied the these ideas of neuro evolution

257
00:31:43,440 --> 00:31:50,000
and kind of evolutionary neural nets and machine learning oh yeah yeah for sure for example the work

258
00:31:50,000 --> 00:31:56,640
that I just talked about the neuro evolution of itself interpretive agents like we actually

259
00:31:56,640 --> 00:32:04,400
used evolution or computational evolution algorithms to train the agents so rather than using

260
00:32:04,400 --> 00:32:13,280
like like reinforcement learning to train them so like in in general I kind of like some of

261
00:32:13,280 --> 00:32:19,840
the evolution algorithms because they're kind of like we can use them as a black box optimizer as

262
00:32:19,840 --> 00:32:26,560
well we we don't necessarily need everything to be nice and differentiable which is one of the

263
00:32:26,560 --> 00:32:32,800
key properties of many many domains right now so one one of the things are differentiable

264
00:32:32,800 --> 00:32:39,280
than you can put it into the machine and you know you're you're you're you're gradient based

265
00:32:39,280 --> 00:32:45,280
optimizer would get the solution but because hard attention is it's kind of difficult to make

266
00:32:46,160 --> 00:32:52,240
it very differentiable or there are methods but there is challenging it's just easier sometimes to

267
00:32:52,240 --> 00:33:00,640
use evolution to solve these problems so one hand we we do like to use evolution and specifically

268
00:33:00,640 --> 00:33:11,040
evolution strategies and genetic algorithms as a tool to to help us find solutions but we I did work

269
00:33:11,040 --> 00:33:18,800
on some research projects where we're also developing these evolution algorithms as well

270
00:33:18,800 --> 00:33:31,360
so there there was another paper with with with these themes of constraints was done with with me

271
00:33:31,360 --> 00:33:40,080
and I was led to my former colleague and intern Adam Geier in a paper called weight agnostic neural

272
00:33:40,080 --> 00:33:48,240
networks so the the key concept in that paper is you know we want to find the neural

273
00:33:48,240 --> 00:33:55,920
network architectures that have a really strong inductive bias for certain like reinforcement

274
00:33:55,920 --> 00:34:04,080
learning or machine learning task and can we go to the extreme and find architectures that can work

275
00:34:04,720 --> 00:34:10,960
even without training weights so so usually when we think of neural networks you think of

276
00:34:10,960 --> 00:34:16,240
having a neural network architecture okay and then let's run the optimization algorithm using

277
00:34:16,240 --> 00:34:23,120
SGD to find the weights but here we okay what can we still find the architecture that can still work

278
00:34:23,120 --> 00:34:27,360
when we don't train the weights like when the weights are chosen from a random distribution

279
00:34:28,560 --> 00:34:37,280
so that that one is when we when we looked at we essentially looked at doing architecture search

280
00:34:37,280 --> 00:34:47,680
where we want to optimize the performance of the architecture with a given weight distribution

281
00:34:49,840 --> 00:34:55,600
right so of course your architecture is not going to perform as well as when all the ways we

282
00:34:55,600 --> 00:35:02,480
refine tune but this is still very useful because as you know a neural architecture search is

283
00:35:02,480 --> 00:35:08,640
extremely computational intensive so you you will have a batch of architectures and then you'll

284
00:35:08,640 --> 00:35:14,880
have to find the weights of all of these architectures and and then you would go on to find the

285
00:35:14,880 --> 00:35:22,880
next set of architectures you use your results but here we can simply find the architectures and

286
00:35:22,880 --> 00:35:30,560
evaluate their performance on on random weights and and then we can find architectures that are

287
00:35:30,560 --> 00:35:37,760
have have a very strong inductive or even like an innate bias for certain tasks so then

288
00:35:38,720 --> 00:35:47,120
the intuition is like is kind of inspired by by the biology sometimes the organisms have some

289
00:35:47,120 --> 00:35:54,640
ability the moment they're born to escape predators right you can imagine like maybe you can

290
00:35:54,640 --> 00:36:01,440
you can have a bipedal walker controller that can already still walk forward when the

291
00:36:01,440 --> 00:36:07,280
weights are not trained but if that's the starting point then then training the weights will be

292
00:36:07,280 --> 00:36:13,840
a lot more efficient if you want to fine tune the network later on so that that's like to answer

293
00:36:13,840 --> 00:36:18,560
your your question earlier is like this is like one example of the work that I was involved in

294
00:36:18,560 --> 00:36:27,760
where we actually try to extend and and improve upon architecture or neuro evolution methods

295
00:36:28,480 --> 00:36:36,080
to find neural networks that that are where we're not just an user of the evolution algorithm

296
00:36:36,080 --> 00:36:44,720
as a black box optimizer and this paper was was apparently like a talk about in the neuro

297
00:36:44,720 --> 00:36:49,440
science community a bit more than the machine learning community it was not so useful to them

298
00:36:49,440 --> 00:36:58,480
like it would be like that we got our best score on M this was like 92% or I forgot

299
00:36:58,480 --> 00:37:07,040
I'm there so it was like the the M this performance is is horrible so it's not going to be so

300
00:37:07,040 --> 00:37:13,440
useful for for the M this committee but but the the papers do still got accepted at the

301
00:37:13,440 --> 00:37:17,600
new groups conference you know and it got like a spotlight but it's probably one of those papers

302
00:37:17,600 --> 00:37:24,000
that where we we massively underperformed the state of the art with like 90% on M this but

303
00:37:24,000 --> 00:37:36,880
somehow still got in kind of luck done on that one nice nice I tend to hear neuro evolution coming

304
00:37:36,880 --> 00:37:45,120
up most in the context of architecture search are there other areas where you see it being used

305
00:37:46,400 --> 00:37:53,120
yeah well like as I mentioned earlier we we use it a lot just for policy search

306
00:37:56,800 --> 00:38:03,040
we also see it used quite often in in robotics as well

307
00:38:03,040 --> 00:38:11,680
huh like for example some of my colleagues in in the robotics team they they like to use

308
00:38:12,800 --> 00:38:21,760
simple evolution algorithms to to quickly find policies like the the one that is

309
00:38:22,800 --> 00:38:31,920
is used most often there's two that is really used often one one is a CMAES so that that is

310
00:38:31,920 --> 00:38:36,720
that kind of like the the defaults evolution strategies algorithm that people like to use as a

311
00:38:36,720 --> 00:38:43,920
black box optimizer the other one is caught a I think it's caught a augmented random search

312
00:38:43,920 --> 00:38:50,080
is basically evolution is a form of random search and then this one is a it's a it's a very simple

313
00:38:50,080 --> 00:38:58,720
random search algorithm that's directed in in a very simple way so so the robotics folks likes

314
00:38:58,720 --> 00:39:05,280
these simple approaches because they're they're they're explainable and they're intuitive so I see

315
00:39:05,280 --> 00:39:14,480
some people are using them to find the policies on on like on like like robots and then using them to

316
00:39:15,120 --> 00:39:22,880
to like a control control these like mini-tower robots that they have in the lab

317
00:39:22,880 --> 00:39:31,840
uh we I but I use them a lot for in general like especially if when I have a I have a neural net

318
00:39:31,840 --> 00:39:38,880
without so many parameters like which is very common in RL like unlike deep learning where you

319
00:39:38,880 --> 00:39:45,600
do have like you know 20 million parameter solutions in RL a lot of the the controllers like my

320
00:39:45,600 --> 00:39:53,520
might work uh when when we only have have like you know the the 10,000 parameters or even 1,000

321
00:39:53,520 --> 00:39:59,600
parameters so so it's like yeah and it could nice to say the RL is like a cherry on a cake

322
00:39:59,600 --> 00:40:05,680
and so so the trend is uh you have all these self supervised models that are trained with

323
00:40:05,680 --> 00:40:11,680
gradient descent with hundreds of millions of parameters but your actual policy network that

324
00:40:11,680 --> 00:40:17,920
could be using all of these things maybe perhaps via a world model and and those networks could

325
00:40:17,920 --> 00:40:24,560
might even just be a few thousand parameters and that can you know wipe out or using gradient descent

326
00:40:24,560 --> 00:40:30,480
to train them when uh like uh if we're able to use evolution to train them we can get away with

327
00:40:30,480 --> 00:40:37,840
doing things like non-differentiable environments and and whatnot so so we we tend to like to use

328
00:40:37,840 --> 00:40:47,520
those as as a baseline in that case in conjunction with with other RL methods and and approach

329
00:40:47,520 --> 00:40:55,200
this to policy or kind of um in isolation we usually like if we're able to get the solution we

330
00:40:55,200 --> 00:41:02,800
want then we can use them like in isolation okay I mean there's some of my colleagues have been

331
00:41:02,800 --> 00:41:11,120
working on ways to to combine the reinforcement learning with evolution so yeah then evolution

332
00:41:11,120 --> 00:41:18,160
can kind of be the outer loop and the RL can be the inner loop but in a lot of the work where I'm

333
00:41:18,160 --> 00:41:27,040
simply using like an end user of an optimizer then I simply use it to to get me get me a set of

334
00:41:27,040 --> 00:41:36,480
weights or get me a set of parameters and go on in a day. Did we talk about the sensory neuron

335
00:41:36,480 --> 00:41:48,240
paper? Oh no no no it's it sounds like I mean it kind of fits right into this idea of uh constraints

336
00:41:48,240 --> 00:41:56,480
and um applying constraints to to make problem solving easier can you talk a little bit about that

337
00:41:56,480 --> 00:42:03,520
paper? Yeah sure so like uh some of the previous work I I discuss it's it's more like you know

338
00:42:03,520 --> 00:42:08,800
the constraint is more like a bottleneck like an information bottleneck and maybe you're doing more

339
00:42:08,800 --> 00:42:16,480
with less but it doesn't always have to be like that so in in this paper we it was a really fun

340
00:42:16,480 --> 00:42:23,920
project also with with my teammates Eugene Tang we we looked at the problem of uh what what if we

341
00:42:23,920 --> 00:42:35,040
we gave an agent's uh an observation space that is shuffled around so like usually in a in these

342
00:42:35,040 --> 00:42:40,400
reinforcement learning environments or in machine learning in general you have to give a model

343
00:42:41,680 --> 00:42:50,320
very well-specified input data like like if if you give it like the the observation space of

344
00:42:50,320 --> 00:42:58,000
of a humanoid or an ant's robots like every single input means something like maybe a tour good

345
00:42:58,000 --> 00:43:04,640
of velocity or the positions or maybe the pixels on the screen this pixel corresponds to

346
00:43:04,640 --> 00:43:12,000
to this has to be this position so we we toyed around with the idea of what if we we can randomly

347
00:43:12,000 --> 00:43:20,480
shuffle the observations and the agent actually has to like figure out what each input like each

348
00:43:20,480 --> 00:43:29,600
sensory input means before you know deciding an action and if an agent is able to to solve

349
00:43:30,720 --> 00:43:36,880
a particular task or environments or or a machine learning problem from from uh shuffled

350
00:43:36,880 --> 00:43:45,280
observations we can also examine the properties whether it has extra benefits that it has compared

351
00:43:45,280 --> 00:43:51,600
to agents that are otherwise trained the normal way of just getting getting the inputs so this

352
00:43:51,600 --> 00:43:57,360
is another type of constraint that I don't consider to be a bottleneck or information bottleneck

353
00:43:57,360 --> 00:44:03,120
because I you're actually giving the agent the same information I guess like the dimensionality

354
00:44:03,120 --> 00:44:11,360
of the information is the same but but here we we try to just shuffle the order and surprisingly

355
00:44:11,360 --> 00:44:19,120
we're able to we're able to get it to work so like uh the the information of this work originally

356
00:44:19,120 --> 00:44:25,520
came from some ideas on in a meta learning space because we we're essentially trying to

357
00:44:25,520 --> 00:44:31,440
to get an agent to adapt to changing environments when it's like the agent will get a shuffled

358
00:44:31,440 --> 00:44:37,680
and re-shuffled screen and it has to re-adapt but also in the in the neuroscience there's the

359
00:44:37,680 --> 00:44:45,280
the area called a sensory substitution and uh it's a psychologist have to measure the human's

360
00:44:45,280 --> 00:44:53,440
ability to adapt to when when what our senses give us suddenly change like like there's this

361
00:44:53,440 --> 00:44:59,440
popular experiment done even a hundred years ago where you you're wearing this uh you're wearing

362
00:44:59,440 --> 00:45:05,200
an upside down goggle I'm not sure you saw that so there's a there's a simple mirror glass

363
00:45:05,200 --> 00:45:10,560
in front of your eyes so what you're seeing is is completely flipped and what people notice is

364
00:45:10,560 --> 00:45:17,040
it requires maybe maybe like ten minutes or half an hour of readjusting and you're able to walk

365
00:45:17,040 --> 00:45:22,800
perfectly fine with with this flip sensory but once you take off the glass then you're messed up

366
00:45:22,800 --> 00:45:27,760
again for another half an hour or so so that that's kind of I guess one of the easier tasks

367
00:45:27,760 --> 00:45:36,240
uh there there's a TED talk a few years ago where someone had a video of an inverted bicycle

368
00:45:36,240 --> 00:45:44,080
so this one is harder when when you turn left you actually go right and when you turn right you

369
00:45:44,080 --> 00:45:49,840
actually go left and they found this one really messed people up so you know like I guess because

370
00:45:49,840 --> 00:45:58,080
a riding a riding a bicycle is more like a it's like a human invention I guess so it takes it takes

371
00:45:58,080 --> 00:46:03,360
a long time for people to to read that because you actually have to balance as well you're like

372
00:46:03,360 --> 00:46:08,880
a complicated control system constantly balancing you so that one really messed people up

373
00:46:08,880 --> 00:46:15,280
I don't know if you've ever had the experience where your your screen controls get flipped

374
00:46:15,280 --> 00:46:22,480
I don't remember what caused it um but you know your you know trackpad right becomes left and

375
00:46:22,480 --> 00:46:29,040
up becomes down and and vice versa and that can be you know infuriated it's very difficult to

376
00:46:29,040 --> 00:46:36,240
to adjust to yeah exactly yeah especially like you know apple like whenever you use like

377
00:46:36,240 --> 00:46:41,600
apple products that okay but sometimes your trackpad goes the other way around when you're on

378
00:46:41,600 --> 00:46:48,400
another person's trackpad or when they have a new model so I booked or MacBook Pros you have

379
00:46:48,400 --> 00:46:54,880
you have a touch bar and you suddenly don't have a touch bar yeah no no we're back to the other day

380
00:46:55,760 --> 00:47:03,600
but hey there's another uh there's another uh neuroscientist uh Paul Paul back Rita who's kind

381
00:47:03,600 --> 00:47:11,920
of a pioneer of sensory substitution and and his claim to fame was uh he he had uh sub he had uh

382
00:47:11,920 --> 00:47:20,080
people who were unfortunately blind like cannot like uh see uh this lack vision and back in the

383
00:47:20,080 --> 00:47:27,440
end of the 60s he had an experiment where uh he put a low-dimensional uh video camera

384
00:47:27,440 --> 00:47:38,000
uh there's analog back in the day and he he uh he fed some of those uh signals from the analog

385
00:47:38,000 --> 00:47:44,960
video camera into a low-dimensional 2D grid of pokes into the person's back I've heard about this

386
00:47:44,960 --> 00:47:53,200
yeah so yeah one of the cool things is uh our our skin or our our touch senses is under

387
00:47:53,200 --> 00:47:59,440
utilized everywhere outside of our hands like maybe from evolution when we're hunter-gatherers our

388
00:47:59,440 --> 00:48:04,720
skin was really important but in kind of modern times that we we wear you know like a clothing and

389
00:48:04,720 --> 00:48:09,440
we we don't really use our touch senses but that does another interesting topic but it's kind of

390
00:48:09,440 --> 00:48:15,120
getting signed like but back for for this particular idea is that he poked a low-dimensional

391
00:48:15,120 --> 00:48:23,520
resolution of uh commissioned to the subjects back within a few weeks or months uh people gain vision

392
00:48:23,520 --> 00:48:30,560
they were able to to see and understand things uh by sitting on his chair uh so so he showed that

393
00:48:31,680 --> 00:48:39,280
through through uh touches or through pokes on a set person's back that person can can learn

394
00:48:39,280 --> 00:48:47,840
to to interpret those signals as if that person was was seeing uh what's in front of the camera

395
00:48:47,840 --> 00:48:56,880
and in the in the late 90s in turn of the century there was a variation of this uh from this team

396
00:48:56,880 --> 00:49:05,200
where they they fed in a higher resolution video feed into a 2D grid of uh electrodes that was

397
00:49:05,200 --> 00:49:12,240
placed on a person's tongue so so from from the the stimulation on the tongue the person's is

398
00:49:12,240 --> 00:49:19,200
able to the the subject is able to interpret uh what the video uh camera mounted on the subjects

399
00:49:19,200 --> 00:49:26,320
head is seeing and this was actually you know gain popularity uh so like uh people were able to

400
00:49:26,320 --> 00:49:33,760
to live their lives then like having a low-dimensional uh vision system simply by learning to

401
00:49:33,760 --> 00:49:40,800
predicting um how um the these sensory uh signals from the from their I guess from from their

402
00:49:40,800 --> 00:49:47,680
tongue but however um these these are incredible it shows how how great we are but they require

403
00:49:47,680 --> 00:49:54,160
like months if not years of training uh to gain mastery so it's it's kind of like okay sure

404
00:49:54,160 --> 00:50:00,320
you can you can change you can switch around your inputs and retrain your machine learning

405
00:50:00,320 --> 00:50:06,160
model from scratch even uh and using the new inputs and of course yeah then then you can deal

406
00:50:06,160 --> 00:50:12,880
with these sensory substitutions so what the what we're trying to ask ourselves is can we actually

407
00:50:12,880 --> 00:50:18,320
get an algorithm to do this without training in the in the traditional sense like with without

408
00:50:18,320 --> 00:50:26,320
like a retraining your model where so uh where the the the the agent is able to explicitly adapt

409
00:50:26,320 --> 00:50:35,840
to to these uh inputs uh so in the end uh even though this work is biologically inspired from

410
00:50:35,840 --> 00:50:42,800
from on the problem side the solution we use has nothing to do with biology uh we're lucky enough

411
00:50:42,800 --> 00:50:49,360
to to build on to previous work that gave us to do the tools to work with uh permutation

412
00:50:49,360 --> 00:50:56,080
invariant networks and some of these uh works have been pioneered by by people working on the

413
00:50:56,080 --> 00:51:02,160
transformer paper the original transformer paper uses the linear attention which was predate

414
00:51:02,160 --> 00:51:09,600
transformer by lots but those those were shown to be uh permutation acrovariants so if if you

415
00:51:09,600 --> 00:51:14,960
change the input order the output order changes the same way but uh there's another paper that came

416
00:51:14,960 --> 00:51:20,720
out later called a set transformer which had a really cool trick on making one of the the

417
00:51:20,720 --> 00:51:28,320
query matrix uh constant and that converted this attention mechanism to be permutation invariant

418
00:51:28,320 --> 00:51:38,160
so suddenly you're able to feed in a signal of like uh of any order and the output will be the

419
00:51:38,160 --> 00:51:46,640
same thing okay so it'll be like um it's it's a method to to take on a sets of an unordered

420
00:51:46,640 --> 00:51:53,120
variable length sets and and you're able to to get uh get a permutation invariant representation

421
00:51:53,120 --> 00:51:58,560
of it so we played around with this idea and applied it to reinforcement learning problems

422
00:51:58,560 --> 00:52:04,720
so it's a weather you can uh you can feed in all of your signals whether those signals could be

423
00:52:04,720 --> 00:52:11,120
the states of a pie bullets like locomotion environments uh or it could be like all of the

424
00:52:11,120 --> 00:52:19,200
the tiles of of uh of an Atari game uh and you can feed them in any order you want uh and you know

425
00:52:19,200 --> 00:52:26,960
you get the same representation coming up so we we tried to uh to to feed these representations

426
00:52:26,960 --> 00:52:33,680
into into a policy network and train the entire system to to perform the task

427
00:52:33,680 --> 00:52:41,280
and uh what we notice is that it is uh after after uh development we have to iterate on this method

428
00:52:41,280 --> 00:52:48,160
it doesn't work at the beginning so uh some some of the the improvements that my my colleague

429
00:52:48,160 --> 00:52:54,160
Eugene discovered is we actually have to feed in things like the previous action and have

430
00:52:54,160 --> 00:53:02,480
have each sensory neuron uh for certain tasks have its own internal state so so for for for example

431
00:53:02,480 --> 00:53:09,600
like your locomotion robot actually you know every sensory uh input goes into its own LSTM

432
00:53:09,600 --> 00:53:14,960
and that LSTM will output a broadcast signal to the attention mechanism that will generate

433
00:53:15,680 --> 00:53:19,520
this permutation invariance representation that can produce the action

434
00:53:20,400 --> 00:53:28,880
so it's fairly expensive yeah yeah but yeah so in a way abstracting like usually uh

435
00:53:28,880 --> 00:53:33,280
you know our in traditionally our networks are we just get the input right away

436
00:53:33,280 --> 00:53:40,720
into into our particular uh input node of a neural network but here we treat every input node

437
00:53:41,280 --> 00:53:48,000
as uh as a neural network itself so that's why uh yes the papers type of is the sensory neuron

438
00:53:48,000 --> 00:53:53,680
as a transformer because these neural networks is has been inspired by the transformer architecture

439
00:53:53,680 --> 00:54:04,880
so to to uh to I guess pay some give some credit to that and what we notice is like uh of course

440
00:54:04,880 --> 00:54:12,080
this it's going to work for permutation invariance observations but we were actually without

441
00:54:12,800 --> 00:54:19,440
additional training these agents tend to work even when we shuffle them during the observations

442
00:54:19,440 --> 00:54:25,920
during an episode like like like for example if you have a locomotion robot plot forward uh

443
00:54:25,920 --> 00:54:32,080
and of course it's going to work when you shuffle to input once at the beginning uh and keep

444
00:54:32,080 --> 00:54:38,160
keep that shuffle order the same for the rest of the one thousand time steps because like by the

445
00:54:38,160 --> 00:54:44,720
condition the the the representations don't change but what we notice is we can shuffle them

446
00:54:44,720 --> 00:54:50,240
many times during the environments like like if you if your episode is one thousand time steps

447
00:54:50,240 --> 00:54:55,280
you can shuffle them you know every one hundred time steps and the performance without additional

448
00:54:55,280 --> 00:55:01,520
training remains roughly the same so so there there's something to be said about the the power of

449
00:55:01,520 --> 00:55:08,240
uh of uh the agent's ability to to quickly re-adapt uh without explicitly learning to re-adapt

450
00:55:08,240 --> 00:55:17,120
uh to the environments um that you look at um or would you expect to see that if you then

451
00:55:18,160 --> 00:55:23,440
uh you train an agent with this capability or this constraint you know as you might say

452
00:55:23,440 --> 00:55:30,160
and then you give it unsuffled data does it perform better because it you know has learned to

453
00:55:31,040 --> 00:55:37,760
attend to important relationships in the scene as opposed to um an agent that you know

454
00:55:37,760 --> 00:55:43,840
hasn't been trained in this way for for this one um if if we give a shuffle or on shuffle data

455
00:55:43,840 --> 00:55:51,280
it'll perform exactly the same way um because the representations are consistent uh but but

456
00:55:51,280 --> 00:56:00,480
that being said uh we we could do things like um like take away information uh from the input

457
00:56:00,480 --> 00:56:07,360
or give it additional redundance information uh into input and have it still kind of work

458
00:56:07,360 --> 00:56:15,040
like like for instance if the agent expects like five input signals to do a task uh you can give it

459
00:56:15,040 --> 00:56:22,960
like you know 20 signals but five of them are the actual important signals and the other 15 are

460
00:56:22,960 --> 00:56:28,720
pure noise uh a little like a small amount of noise and the whole thing can be shuffled

461
00:56:28,720 --> 00:56:34,880
and without actually extra further training like uh it's only trained originally on the five inputs

462
00:56:34,880 --> 00:56:42,080
it's it's still able to identify like it work i guess uh it's able to to somehow learn

463
00:56:42,080 --> 00:56:49,600
that it should identify which signals are important without explicitly training to identify those

464
00:56:49,600 --> 00:56:57,280
so i feel it's it's kind of like um this is somewhere between learning and metal learning

465
00:56:57,840 --> 00:57:02,320
for something like a metal learning you're explicitly training the algorithm to learn an algorithm

466
00:57:02,320 --> 00:57:09,040
that learns uh and if we're for learning you're just getting the algorithm policy for the task

467
00:57:09,040 --> 00:57:16,320
here i think the the method is like indirectly learning uh self identification method

468
00:57:16,320 --> 00:57:21,920
to to identify um like which patches or or which uh inputs are important and

469
00:57:22,720 --> 00:57:30,640
the other uh interesting result is uh is from the robustness standpoint so if we apply this uh

470
00:57:30,640 --> 00:57:37,200
methodology to visual tasks like the car racing game or pong like Atari games uh we know

471
00:57:37,200 --> 00:57:44,160
this that we can do we can change the backgrounds uh of the game and the policy can still continue to

472
00:57:44,160 --> 00:57:50,800
work uh to some extent uh this was not possible in the earlier work on on the on the hard attention

473
00:57:50,800 --> 00:57:56,640
so when we change the background it still fails but here uh when we change the background for

474
00:57:56,640 --> 00:58:03,280
for the car racing task uh without explicitly training on these new backgrounds the policy can

475
00:58:03,280 --> 00:58:10,560
can still work to some extent and the the performance is is almost as well that is good for

476
00:58:10,560 --> 00:58:17,600
these generalization tasks compared to existing works in the literature that are explicitly designed

477
00:58:17,600 --> 00:58:24,320
to do such generalization uh but but here it's like a byproduct of of okay let's train our agent

478
00:58:24,320 --> 00:58:30,000
to work with shuffle inputs and oh by the way you know the generalization abilities are

479
00:58:30,000 --> 00:58:38,640
are like just a byproduct of of this this constraint um so when we duck into further like uh the

480
00:58:38,640 --> 00:58:46,560
hypothesis is if we if we shuffle up all of the the patches or the tiles of the screen uh we we

481
00:58:46,560 --> 00:58:54,080
could force the agents to to learn like the to essence the essential important things for the

482
00:58:54,080 --> 00:59:00,960
task and because it's it's forced to learn the essential properties that may help it generalize

483
00:59:00,960 --> 00:59:07,600
to to variations of the environment with different backgrounds and when we did further analysis

484
00:59:07,600 --> 00:59:13,920
we actually looked at the patches that they learned they learned to attend to like in in the

485
00:59:13,920 --> 00:59:18,880
car racing game and it turns out that even though the patches are all shuffled around

486
00:59:18,880 --> 00:59:26,320
it still learns to to attend mostly to the patches that correspond to the edge of the road

487
00:59:27,600 --> 00:59:33,760
even even when the screen is all shuffled around and so then this can help us explain why

488
00:59:33,760 --> 00:59:39,360
the generalization still works to to environments with with when we change the background

489
00:59:40,000 --> 00:59:46,160
because like it's not seeing it's not really attending to to the positions with with different

490
00:59:46,160 --> 00:59:52,720
backgrounds is still looking at at the road so some of the analysis is done in the paper to to

491
00:59:52,720 --> 01:00:01,920
explain why the the transfer works got it so given this uh you know body of research that

492
01:00:01,920 --> 01:00:08,400
that you've pursued focused on the ideas of constraints incorporating ideas like noravolution

493
01:00:09,120 --> 01:00:13,440
you know what are you excited about looking forward where where do you see your research headed

494
01:00:13,440 --> 01:00:21,920
yeah so like I'm really fascinated with the whole concept of of the self-organization

495
01:00:23,200 --> 01:00:30,560
so especially like I was I'm really inspired by this body of work that that my my colleague

496
01:00:31,280 --> 01:00:39,200
Alexander Mordensev did on a neurosurder automata and also self-organizing

497
01:00:39,200 --> 01:00:46,400
uh class and this classifiers which was uh recent articles on the disto pub platform and

498
01:00:46,400 --> 01:00:52,960
like one of one of the things that excited me about this self uh the the sensory you know neuron

499
01:00:52,960 --> 01:01:00,400
paper is uh the it is sort of like a self-organized system every input goes into uh an identical

500
01:01:00,400 --> 01:01:06,640
neural network with its own hidden recurrent state and somehow these neural networks learn to

501
01:01:06,640 --> 01:01:12,880
communicate via attention mechanism to have this emergent property which is the policy and I'm

502
01:01:12,880 --> 01:01:22,000
really excited about like going forward with exploring more of these uh collective intelligence

503
01:01:22,880 --> 01:01:30,480
themes where you where you have a you have an emergent property from thousands or even like

504
01:01:30,480 --> 01:01:39,280
hundreds of thousands of different unique agents or units that have their own local processing

505
01:01:39,280 --> 01:01:48,560
rules but somehow as a whole you have some global emergent property that that is a that is a result

506
01:01:48,560 --> 01:01:57,120
of maybe some evolutionary optimization and I want to explore like properties of these emergent

507
01:01:57,120 --> 01:02:04,640
prop uh behavior because maybe that will help us address some of the shortcomings we see in

508
01:02:04,640 --> 01:02:11,840
in reinforcement learning like like a like a lot of like some some of the issues in RL has to do

509
01:02:11,840 --> 01:02:22,960
with like robustness generalization um maybe like uh out like a sample efficiency and so on

510
01:02:22,960 --> 01:02:29,600
but uh we can get inspiration from other areas like like like for example swarm

511
01:02:29,600 --> 01:02:37,280
swarm computing swarm optimization uh like a multi-agent systems and and maybe if we look at

512
01:02:37,280 --> 01:02:44,960
if we try to break down a problem into into a large like a complex systems problem where you

513
01:02:44,960 --> 01:02:50,240
have lots of local computation perhaps that might give us some insight or or different types of

514
01:02:50,240 --> 01:03:01,200
solutions to to how we've been able to approach them so far so I'm excited about the general idea

515
01:03:01,200 --> 01:03:08,320
of like a collective intelligence complex systems and going forward we want to see you know how we

516
01:03:08,320 --> 01:03:17,040
can like a bridge between the complex systems uh research and incorporate some some of the good

517
01:03:17,040 --> 01:03:22,560
ideas into machine learning and also maybe look at the other way around maybe we can use machine

518
01:03:22,560 --> 01:03:30,320
learning to also help uh advanced state of complex systems research awesome awesome i'm looking

519
01:03:30,320 --> 01:03:37,280
forward to following along as you uh push forward in that direction david has been wonderful chatting

520
01:03:37,280 --> 01:03:48,720
with you thanks so much for joining us well thanks for having me Sam always thank you

