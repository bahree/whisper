WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.200
I'm your host Sam Charrington.

00:32.200 --> 00:36.600
Next Tuesday, May 15, the Twimble online meetup is back.

00:36.600 --> 00:43.040
Our main event will be a presentation by Santosh GSK on the paper YOLO9000 Better Faster

00:43.040 --> 00:46.760
Stronger by Joseph Redman and Ali Farhaddi.

00:46.760 --> 00:51.640
We'll also discuss the landscape of object detection, the current state of algorithms

00:51.640 --> 00:54.720
in that space, and the challenges ahead.

00:54.720 --> 01:01.480
If you aren't already signed up, head over to twimbleai.com slash meetup to register.

01:01.480 --> 01:03.640
See you there.

01:03.640 --> 01:05.520
And event season continues.

01:05.520 --> 01:10.480
Today in tomorrow I'll be at Figure 8 Train AI Conference in San Francisco.

01:10.480 --> 01:15.240
I'll be podcasting all day from the event, so if you're attending the conference, stop

01:15.240 --> 01:17.360
by and pick up a sticker.

01:17.360 --> 01:23.560
It's not too late to use our Twimbleai discount code if you'd like to attend.

01:23.560 --> 01:28.360
In this episode, I'm joined by Jose Hernandez Arayo, professor and the Department of Information

01:28.360 --> 01:34.640
Systems in Computing at the Polytechnic University of Valencia, and fellow at the Leverhume Center

01:34.640 --> 01:40.160
for the Future of Intelligence, working on the Kinds of Intelligence Project.

01:40.160 --> 01:44.440
Jose and I caught up at Nips last year after the Kinds of Intelligence Symposium that he

01:44.440 --> 01:46.440
helped organize there.

01:46.440 --> 01:51.000
In our conversation, we discussed the three main themes of the symposium, which were

01:51.000 --> 01:56.960
understanding and identifying the main types of intelligence, including non-human intelligence,

01:56.960 --> 02:02.000
developing better ways to test and measure these intelligence, and understanding how and

02:02.000 --> 02:06.240
where research efforts should focus to best benefit society.

02:06.240 --> 02:10.240
Alright, let's do it.

02:10.240 --> 02:17.240
Alright, everyone, I am here at Nips in Long Beach, California, and I have the pleasure of

02:17.240 --> 02:25.040
being seated with Jose Hernandez Arayo, who is a professor at the University of Polytechnica

02:25.040 --> 02:31.800
of Valencia, and is currently on sabbatical and visiting at the Center for the Future

02:31.800 --> 02:34.720
of Intelligence at Cambridge, UK.

02:34.720 --> 02:38.240
Jose, welcome to this week in Machine Learning and AI.

02:38.240 --> 02:39.720
Thank you for having me here.

02:39.720 --> 02:40.720
Absolutely.

02:40.720 --> 02:41.720
Absolutely.

02:41.720 --> 02:44.280
So, why don't we get started by having you tell us a little bit about your background

02:44.280 --> 02:47.400
and how you got involved in the Machine Learning world?

02:47.400 --> 02:52.920
Well, it's a long story, probably, that many people in the area would share.

02:52.920 --> 02:59.600
As a child, I had my first computers, and at some times I thought, how can I make this

02:59.600 --> 03:01.960
computer think?

03:01.960 --> 03:07.200
And at the time I had, well, probably it's not very common here in the States, but in

03:07.200 --> 03:13.080
Europe there was this ZX, which is kind of a Commodore, the early computer, the early

03:13.080 --> 03:15.080
computer, like the Sinclair.

03:15.080 --> 03:16.080
Yeah, Sinclair, yeah.

03:16.080 --> 03:20.800
I had that one with 48 Ks, memory.

03:20.800 --> 03:26.640
And at some point I started to play with some kind of basic ideas of logic, because you

03:26.640 --> 03:29.920
have that at high school and, okay, this is about thought.

03:29.920 --> 03:34.800
And then, well, even I just played a little bit with some kind of probabilistic versions

03:34.800 --> 03:40.040
of that, even now it's completely naive from the point of view of, well, you know, the

03:40.040 --> 03:41.920
things are much more complicated than that.

03:41.920 --> 03:44.320
They will try as a teenager and all of that.

03:44.320 --> 03:49.280
And then, well, because of all of that, I started the computer science, but at some point

03:49.280 --> 03:55.320
when I was, what I completed that, I felt that what I really liked was AI, that of course

03:55.320 --> 04:01.040
I was from the beginning, but I didn't see that much AI, just one subject during a five

04:01.040 --> 04:05.240
year degree, and I was like, can it be that you do computer science, no machine learning

04:05.240 --> 04:06.240
at the time.

04:06.240 --> 04:09.160
It was a wind, it was what was this, this the 90s, right?

04:09.160 --> 04:17.160
So, and I decided I have to do a PhD one, and at some time it was an MSC and a PhD on

04:17.160 --> 04:18.840
something different, more AI.

04:18.840 --> 04:21.840
It wasn't that easy to find something.

04:21.840 --> 04:29.640
It was all about these expert systems, and I said, this is not exactly AI, for me, so

04:29.640 --> 04:37.200
this is, and at some point, I did a PhD on something related to, I started with deduction

04:37.200 --> 04:42.320
I still, I thought the reason it was a lot about logic and all of that, and at some point

04:42.320 --> 04:47.560
to learn, this is more about inductive inference about learning.

04:47.560 --> 04:54.360
And then I moved a little bit to machine learning, at some point, then when I started as

04:54.360 --> 05:01.400
an academic, at some point, I was really concerned, and that was from the beginning, about

05:01.400 --> 05:09.560
how to certify at some point that the system has some capabilities, or how you would say

05:09.560 --> 05:14.920
that the system is able to do something, so that's an kind of evaluation of these systems.

05:14.920 --> 05:20.520
And of course, at that time, people already knew about the during tests, and I was also

05:20.520 --> 05:22.200
interested in psychometric tests.

05:22.200 --> 05:23.200
Okay.

05:23.200 --> 05:26.120
About 20 years ago, so it's a long time.

05:26.120 --> 05:31.320
And then I, well, because you, at that time, machine learning started to flourishing with

05:31.320 --> 05:39.440
applications, the terms data mining became fashionable at that time, and I also worked on building

05:39.440 --> 05:47.480
different kinds of classifiers, I was involved in ICML for some periods, in terms of trying

05:47.480 --> 05:56.800
to submit papers, so that's basically a traditional, let's say, a career in trying to get into

05:56.800 --> 06:04.040
the field of machine learnings in the 2000, and then at some point, about a few years

06:04.040 --> 06:09.800
ago, I recorded some of these ideas out, because I was working on evaluation of classifiers,

06:09.800 --> 06:17.960
okay, so things like ROC curves, and how to evaluate a system for a range of context rather

06:17.960 --> 06:23.040
so related to cost-sensitive learning, and things like that, and I said, okay, but some

06:23.040 --> 06:28.880
of these ideas could be linked to the evaluation of AI as well, and how to evaluate some other

06:28.880 --> 06:30.840
more general systems.

06:30.840 --> 06:38.720
And at some point, and I started to work on this area that I call AI evaluation, and

06:38.720 --> 06:43.480
I saw that there was a lot of things to do there, it's perhaps not an area where people

06:43.480 --> 06:49.840
prefer talking about building things, but I like to evaluate what you do is, well, you

06:49.840 --> 06:57.680
are on the other side, like you are not an engineer, but I thought that this was really

06:57.680 --> 07:03.200
important, and whenever I talk to people, well, we need to know where we are, where we

07:03.200 --> 07:07.680
are going, so it's important we have very good measurement instruments, and at the moment,

07:07.680 --> 07:11.960
we don't have very good instruments in AI, because it's evolving so far, and we are also

07:11.960 --> 07:19.760
focused on particular challenges and tasks that we are not really sure whether our systems

07:19.760 --> 07:21.200
are really progressing.

07:21.200 --> 07:24.440
We have that feeling, and of course, there's a kind of objectivity there that there

07:24.440 --> 07:29.160
were systems that are much better now, because we are able to solve more, but at some point

07:29.160 --> 07:34.040
I said that there's a gap here, and I tried to recover some of the old ideas with new

07:34.040 --> 07:39.160
ideas, tried to see what people were doing in terms of evaluating, not only machine learning,

07:39.160 --> 07:46.600
but AI in general, also robotics, and I started to work on this area, and then about

07:46.600 --> 07:52.040
one year ago I wrote a book about this, I've been involved in the organization of several

07:52.040 --> 07:59.000
events around AI evaluation, all of these things, and now I'm here at the last year, at

07:59.000 --> 08:06.600
this center for the future intelligence in Cambridge, where they are doing this idea of evaluation

08:06.600 --> 08:14.320
goes well with some of the general objectives of the center, and basically I'm still really

08:14.320 --> 08:17.800
enthusiastic about machine learning and AI, and see what's going on.

08:17.800 --> 08:19.560
Awesome, what's the title of your book?

08:19.560 --> 08:24.800
Well, the title is the measure of all minds, evaluating natural and artificial intelligence,

08:24.800 --> 08:33.320
so it covers AI evaluation, but also it puts that in the context of how natural intelligence

08:33.320 --> 08:38.960
is evaluated, animals, non-human animals, and humans, a little bit of animal commission,

08:38.960 --> 08:43.880
how tests that you can use, for instance, for development in children, or tests that

08:43.880 --> 08:49.400
you would use to test, or maybe IQ tests, what's the relation between IQ tests, and what

08:49.400 --> 08:56.200
we are doing in AI, and then the first answer is perhaps nothing, but there's something

08:56.200 --> 09:00.440
that you scratch a little bit on the surface, you find connections, and these interesting

09:00.440 --> 09:03.960
questions, some of these connections are developed in the book, for instance.

09:03.960 --> 09:07.400
I'd love to dig into that a little bit, in a little bit more detail, but first I'd like

09:07.400 --> 09:12.760
you to describe the symposium that you helped organize here at Nips, called the Kinds

09:12.760 --> 09:14.760
of Intelligence, right?

09:14.760 --> 09:21.760
Yes, so it was around a project that we have at the center of the future of intelligence

09:21.760 --> 09:26.800
in Cambridge, UK, this project is called Kinds of Intelligence, or at some point there

09:26.800 --> 09:33.520
was a suggestion to propose a symposium on some of the ideas of how to characterize the

09:33.520 --> 09:38.880
different kinds of intelligence that we know at the moment, not only human intelligence,

09:38.880 --> 09:47.320
but also non-human animal intelligence, natural intelligence, and how to locate AI in this

09:47.320 --> 09:49.160
landscape of intelligence.

09:49.160 --> 09:54.880
So that was the original idea of the symposium, and then at some point we developed that idea

09:54.880 --> 10:05.520
into three strands, the first strand was to understand this space, and to see where AI is

10:05.520 --> 10:10.640
just represented as a subset of human intelligence, or something completely different that is

10:10.640 --> 10:15.160
taking us to different places, so trying to analyze this landscape a little bit from different

10:15.160 --> 10:22.800
perspectives, and we wanted to have different perspectives from animal commission and from

10:22.800 --> 10:27.320
human commission as well, for human intelligence and development.

10:27.320 --> 10:35.320
And the second strand was about how to test all of this, and of course I had some influence

10:35.320 --> 10:42.160
in having this strand in the symposium, I was okay, now we have this landscape, but

10:42.160 --> 10:48.660
how can we locate where we are, so we are moving in some direction, but can we say okay,

10:48.660 --> 10:54.240
what are the dimensions of this landscape, and how can we certify that the system is

10:54.240 --> 10:56.360
moving in that direction.

10:56.360 --> 11:02.040
And the third one was okay, we are able to answer all these questions, which of course

11:02.040 --> 11:09.400
sounds very abstract and challenging, the third question is now that we are able to understand

11:09.400 --> 11:14.600
this landscape where we are, the question is where we want to go, and there was a third

11:14.600 --> 11:22.320
strand of this symposium about what are the priorities for society, and whether these

11:22.320 --> 11:29.680
priorities, I wouldn't say the low hanging fruits because some of the recent challenges

11:29.680 --> 11:35.520
are really, really challenging, but sometimes we are motivated by things that are doable

11:35.520 --> 11:43.360
in with current technology, rather than perhaps aiming at the really important problems,

11:43.360 --> 11:47.000
because you are not going to have a success in a matter of one, two years, so it's more

11:47.000 --> 11:51.600
like a long-term project that perhaps academia, or even government, can be interested,

11:51.600 --> 11:56.920
they have to ask companies, but this is changing because the tech giants are also interested

11:56.920 --> 12:05.520
now in long-term goals, and also some areas that we might reach at some point in the future,

12:05.520 --> 12:12.320
even in the near future, that are perhaps dangerous or unethical, but if we don't know where

12:12.320 --> 12:16.040
we are, it is very difficult also to assess the progress in the direction that we want

12:16.040 --> 12:17.040
to take.

12:17.040 --> 12:23.200
So we have that, and in the end, yesterday we had a fantastic lineup of speakers.

12:23.200 --> 12:24.200
I saw that.

12:24.200 --> 12:25.480
Yeah, from different areas.

12:25.480 --> 12:31.720
So it seemed very interdisciplinary, very diverse in terms of the viewpoints that folks came

12:31.720 --> 12:32.720
from.

12:32.720 --> 12:37.880
Yeah, we have people from, for animal cognition, from human intelligence, we have people

12:37.880 --> 12:46.200
from AI, for different perspectives from AI, people perhaps more in favor about more orthodox

12:46.200 --> 12:52.080
approach to AI, where you would like to learn from a lot of example of more in alignment

12:52.080 --> 12:58.240
with the, with nips, people are more contrarian to this view, where you're okay, what about

12:58.240 --> 13:05.280
the learning more human-like, or more with a few examples, or more hypothesis-driven rather

13:05.280 --> 13:07.400
than data-driven?

13:07.400 --> 13:12.640
And we have a very interesting discussion in the second session about this, the different

13:12.640 --> 13:13.640
perspectives.

13:13.640 --> 13:16.880
Also, we had people from the first session come in and ask questions to the people

13:16.880 --> 13:17.880
of the second session.

13:17.880 --> 13:19.040
It was very active.

13:19.040 --> 13:23.440
The third session was a little bit about this society, some of the risks, and some of

13:23.440 --> 13:28.360
the things that people are talking that we are going to see in the future, like at some

13:28.360 --> 13:32.960
point the discussion, it was, also indeed, it was at the right moment also to talk about

13:32.960 --> 13:38.760
at some point things like corporations having a lot of power, and there was a moment where

13:38.760 --> 13:46.120
there was this link about AI in the future, could resemble some of the corporations that

13:46.120 --> 13:50.840
we have at the moment, or that we have had in the past two centuries, and they have a

13:50.840 --> 13:52.440
lot of power.

13:52.440 --> 13:56.840
So that perhaps we can link some of the risks to some things that we have already seen,

13:56.840 --> 14:02.240
so not everything that is coming is new in terms of the effects on society.

14:02.240 --> 14:03.240
Interesting.

14:03.240 --> 14:08.160
Maybe we can take these three strands in turn and spend a little bit of time kind of

14:08.160 --> 14:13.360
having you characterize the landscape before diving a little bit deeper into the measurement

14:13.360 --> 14:18.720
in your research in that area, and then we'll finish up with some of the directional stuff

14:18.720 --> 14:20.760
that you discussed.

14:20.760 --> 14:22.600
So what does that landscape look like?

14:22.600 --> 14:26.280
How do you characterize the way folks are thinking about the kinds of intelligence?

14:26.280 --> 14:34.040
Yeah, that's a really challenging problem, because the first thing is we disagree on our

14:34.040 --> 14:35.920
notion of intelligence.

14:35.920 --> 14:41.520
Some people even say that the term intelligence should be eliminated from our discourse,

14:41.520 --> 14:49.360
and we should only focus on, or even just use the word learning and forget about intelligence.

14:49.360 --> 14:56.480
Some people, because we have these two different views of intelligence, I would call one extreme

14:56.480 --> 15:03.640
is we negate that there's such a thing as intelligence, and some of these people, you

15:03.640 --> 15:08.280
can also have some of these people from, even from machine learning, saying we have the

15:08.280 --> 15:14.720
no freelance theorem, so even if you design a system to solve this problem, there's some

15:14.720 --> 15:18.600
other problem for which there are other systems that will be optimal for these problems,

15:18.600 --> 15:20.480
but not for the first problem and the other way around.

15:20.480 --> 15:27.600
So this idea of just having more general systems or more intelligent systems is nonsense.

15:27.600 --> 15:31.880
Well, I would argue against that, but then you have the other extreme.

15:31.880 --> 15:37.320
It seems like being intelligent is very different from being optimal at everything, which is

15:37.320 --> 15:41.880
kind of what this no freelance theorem seems to argue against.

15:41.880 --> 15:46.040
Yeah, well, I think that that's assuming that our world is random in a way.

15:46.040 --> 15:50.720
Do you have this block uniformity as an assumption, which is, well, the special case is that

15:50.720 --> 15:55.760
the data you receive is random, so it just says, if that's a case, well, the theorem

15:55.760 --> 16:00.120
is a theorem, that's a proof, and you say, okay, so any machine learning algorithm will

16:00.120 --> 16:04.840
be equally good or equally bad at, but the thing is the assumption that there's assuming

16:04.840 --> 16:10.200
something very, very strong, and then basically that our university is completely chaotic,

16:10.200 --> 16:14.880
and it's not like, wow, there are patterns, and there are patterns not because they're

16:14.880 --> 16:23.320
laws of physics, patterns because what we receive goes through humans, animals, devices,

16:23.320 --> 16:28.920
so not just to explain this a little bit more technical, when you have a theory machine

16:28.920 --> 16:33.160
or any machine, it doesn't have to be a theory machine, but just any machine and you put

16:33.160 --> 16:37.960
random inputs, what you get as an output is not random at all, right, right.

16:37.960 --> 16:41.880
So what we see in our world, because it's just going through all of these filters and

16:41.880 --> 16:50.600
these filters are machines in a way that it's an animal or it's a thermostat or something

16:50.600 --> 16:56.680
like that, what you get as a result has patterns, and then you can explore these patterns,

16:56.680 --> 17:03.560
and if that's the case, then the assumptions for the North Korean land students don't hold

17:03.560 --> 17:07.640
in that scenario, but well, that will be like sometimes it gets a little bit philosophical

17:07.640 --> 17:14.280
about that question, but that's one extreme, so you have like infinitely many intelligences,

17:14.280 --> 17:19.560
and this links a little bit with to psychology where you say, okay, there's not such a thing

17:19.560 --> 17:25.640
as intelligence, and you will have just many of the multiple intelligences theory where

17:25.640 --> 17:30.440
you will be good at this, but you're not good at that. The other extreme is there's only one

17:30.440 --> 17:36.440
single intelligence, and we hear that occasionally, especially for some of these discussions

17:36.440 --> 17:40.840
about superintelligence, and people think of intelligence, something monolithic,

17:41.800 --> 17:48.200
and even some people try to assimilate intelligence in humans with IQ values, which is of course

17:48.200 --> 17:52.280
a simplification, and in psychomedic, people will say, no, no, no, no, this is just an indicator

17:52.280 --> 17:57.240
that each useful predicts this and that, but you can't assimilate that to intelligence,

17:57.240 --> 18:02.840
not people in. So the idea you have to infinitely many intelligences is just one,

18:03.480 --> 18:06.920
perhaps the virtue here is to think that intelligence is something with structure,

18:07.880 --> 18:12.840
so you can think of perhaps something like a journal intelligence, but is related to some other

18:12.840 --> 18:18.200
abilities, until you go down, you can see this as in a hierarchical well from the top where you

18:18.200 --> 18:24.440
would have something like a general ability like that or more, and you go to some kind of skills,

18:24.440 --> 18:31.640
until you reach the bottom when you have a very specific ability, sorry, a very specific task,

18:31.640 --> 18:35.640
so you go from very specific, as you aggregate tasks into, you put the, say,

18:35.640 --> 18:40.840
task skills generally, you have these, you can have many levels as you want,

18:40.840 --> 18:48.840
and that gives you a structure, so when we were going back to your question about the

18:48.840 --> 18:52.600
landscape of intelligence, there's one way of looking at this landscape of intelligence,

18:52.600 --> 18:57.960
so we can just look at the, at this landscape at the bottom, and we will have no structure,

18:57.960 --> 19:03.080
so there's no interest in this landscape, because we will have, okay, I'm able to solve this task,

19:03.080 --> 19:07.320
this task, this task, and I'm also able to solve these tasks, and these tasks are important

19:07.320 --> 19:13.000
commercially or whatever, that's interesting, but that's not, this is not going to give us a hint

19:13.000 --> 19:18.440
about what if I give you this new task, can you tell us something about your system solving these tasks?

19:18.440 --> 19:24.680
I mean, a lot of the research that we see today, it's kind of predicated on an assumption that if we,

19:24.680 --> 19:30.760
you know, build towards very task-based intelligence that will help us understand and get towards

19:30.760 --> 19:36.520
a general intelligence, I'm thinking of things like AlphaGo, right? That's, yeah, maybe task is too

19:36.520 --> 19:42.040
limited for what it's doing, but it's solving a very specific problem, but the only reason why

19:42.040 --> 19:46.840
they're doing that is because of this hope that it'll get us towards a higher level, more general

19:46.840 --> 19:53.400
intelligence. Yeah, I think that the program, the program is, they're there having research in

19:53.400 --> 20:03.240
deep mind with AlphaGo, AlphaGo 0, and Alpha0, we had that talk yesterday, Damage gave an excellent

20:03.240 --> 20:08.600
talk yesterday, and people were talking about AlphaStar at the end of the end of the session.

20:09.480 --> 20:15.160
He made a very good point in one of the questions about, okay, what we're interested in, perhaps

20:15.160 --> 20:20.600
we have to put a lot of bias to solve Alpha, or well, there was a discussion about

20:20.600 --> 20:28.440
quality bias or knowledge, or to solve AlphaGo, then we, and human knowledge, we can remove that

20:28.440 --> 20:35.080
and call it AlphaGo 0, still there's some knowledge or something, you have to put the rules of the

20:35.080 --> 20:40.840
game, for instance, but then it's a first stage that you can even make this general enough to cover

20:40.840 --> 20:46.920
some other games, and then you have this Alpha0, which is, so this line of progress that we see

20:47.640 --> 20:52.200
is really interesting because you're going for a very specific task, right? Using a lot of knowledge,

20:52.200 --> 20:59.560
which is very difficult to add up to just a small change of that, and you need a lot of effort

20:59.560 --> 21:05.960
to succeed for that task, to a system where you can have, even if you have to put still some

21:05.960 --> 21:11.000
knowledge, the rules and that, the system is able to do much more independently, much more

21:11.000 --> 21:16.280
autonomous ways, and then you have a third step in this progress where you have a system that

21:16.280 --> 21:21.480
does more general, still the system is only able to solve some kind of ball games if you give the

21:21.480 --> 21:27.640
rules, but that sounds amazing in terms of just 10 years ago, that you would have just one single

21:27.640 --> 21:33.000
system, just give the rules of any ball game, and perhaps we don't know because you need to do all

21:33.000 --> 21:39.240
of the experiments, but perhaps for a wide range of programs, you have a system that is much better

21:39.240 --> 21:45.160
than humans for all of them. This goes in that direction of bottom up, which is a very interesting,

21:45.160 --> 21:50.920
so you're getting more general. How general you can get, how interesting is to have something

21:50.920 --> 21:58.600
like a very general system without, by as I told it, was one of the topics for discussion

21:58.600 --> 22:05.240
yesterday. I think that in some case we have this discussion about the kinds of tasks we are

22:05.240 --> 22:10.600
interested in, and some people say we are interested in those tasks and humans are well at,

22:10.600 --> 22:16.920
a good at, but perhaps there are some other tasks that people are very bad at, and they are really,

22:16.920 --> 22:23.720
really interesting as complementary to what we are doing, so it's not easy to map this space,

22:23.720 --> 22:31.560
but I think that's perhaps a very important challenge, and it's, I wouldn't say that we are

22:31.560 --> 22:36.760
playing with fire, but we are doing a lot of great things with a really understanding where we

22:36.760 --> 22:43.320
are, and perhaps just train to analyze the connection between tasks. What the tasks have in common,

22:43.320 --> 22:49.880
because we say, okay, test and go have something in common, but can we extrapolate what alpha

22:50.600 --> 22:57.960
star is able to do with robotic navigation? It was a completely different system. You can,

22:57.960 --> 23:02.760
you can be used some of the ideas, but of course, this system is not meant to solve this kind of

23:02.760 --> 23:07.720
task, it's not generally not, but the human, even if it is not that good, is not optimal

23:08.440 --> 23:14.920
for any of the stacks, and so we go to this idea of intelligence being second best at everything,

23:14.920 --> 23:21.640
you just have to be the best, and we move away as well from this idea that your child is a genius

23:21.640 --> 23:26.120
at something, but very bad at all the rest, so intelligence is something in the middle, you are

23:26.120 --> 23:31.000
not the best for everything, but also it's not this idea, you are very good at just one thing,

23:31.000 --> 23:36.360
because you wouldn't call that intelligence, that's the narrow approach to AI, so I think this

23:36.360 --> 23:42.680
is basically the interesting part to categorize what it is, what we have in the middle,

23:42.680 --> 23:47.800
this kind of structure of intelligence and relation between tasks, and how can we,

23:47.800 --> 23:56.040
how we can extrapolate from what the system is able to do to other tasks, and it's a question of

23:56.040 --> 24:01.400
applicability in machine learning and AI, sometimes what about, I have this task, can I use your

24:01.400 --> 24:05.960
technique, and you have all these discussions you will need to train, it's not only that you have

24:05.960 --> 24:11.320
to train all the hyper parameters, sometimes you have to change the architecture completely,

24:11.320 --> 24:17.720
and we have all these discussions about general IT in AI, machine learning, and so this landscape

24:17.720 --> 24:25.000
tries to give some kind of conceptualization about this, of course, I don't think there's going

24:25.000 --> 24:30.840
to be one way of looking at this landscape, but if there are different proposals in some areas,

24:30.840 --> 24:35.960
at least there's some, you can say, okay, what about a number of examples that you need,

24:35.960 --> 24:40.840
a training course, you can put some dimensions that you can plot some systems according to

24:40.840 --> 24:46.280
these dimensions, okay, I'm here, and I want to go there, and humans are present at that corner,

24:47.160 --> 24:52.840
and we know where we are, but we need to do more of this, and that was one of the motivations of

24:52.840 --> 24:58.040
the symposium, and also some of the projects that we have in the CFI as well.

24:59.240 --> 25:05.960
And so where does measurement fit in, and how does that led you to a specific set of research

25:05.960 --> 25:12.200
that you work on? Yeah, I think measurement is everywhere in science and technology.

25:12.200 --> 25:20.600
Sure. So for me, it was a kind of a surprise to say, okay, how much we progress in AI,

25:20.600 --> 25:26.040
especially in machine learning, and we don't have very good measurement tools. Of course,

25:26.040 --> 25:29.720
we have tasks, and we can say, okay, accuracy, or some other metrics.

25:29.720 --> 25:33.560
Task performance, for example. Yeah, with different, but where is that lacking?

25:34.440 --> 25:40.120
Yeah, the thing is that the question about measurement here is measurement for which tasks,

25:40.120 --> 25:47.000
and that's the main question. Just for one single task, you can put some kind of a metric of

25:47.000 --> 25:52.600
performance, and this is going to work well. And there's no objection if even you have a utility

25:52.600 --> 25:58.760
function, or even just some cost function money associated with a task. That's perfectly okay,

25:58.760 --> 26:05.880
for instance, self-driving car. I think that's complex to find a good metric, but it's clear.

26:05.880 --> 26:13.000
You can say, okay, these routes are more frequent than others, accidents, spinny things like that.

26:13.000 --> 26:16.840
You can put all of these in a formula and say, yeah, I want to maximize this. Yeah.

26:16.840 --> 26:23.880
But for some other systems we want them to be more general, we don't have a, we want to say,

26:23.880 --> 26:29.080
we want a system that has very good variable abilities. This is too abstract.

26:30.120 --> 26:34.440
We don't really know how to do that. Well, so we can have a good translator.

26:34.440 --> 26:45.240
Even for machine translation, there's a strong debate about the evaluate metrics,

26:45.240 --> 26:51.720
whether what you get is readable, or you get the idea, or what kind of mistakes are worth.

26:51.720 --> 26:56.440
Because we are entering an area where we have to talk about meaning and interpretations,

26:56.440 --> 27:00.520
human interpretations, about whether I think that this is a good translation. Of course,

27:00.520 --> 27:06.680
you can always say, okay, I can just perform a lot of translations and have a human access

27:06.680 --> 27:10.280
these translations from zero to ten, or something like that. And you can have a metric.

27:10.280 --> 27:14.440
But in that case, you are not sure what you are really evaluating.

27:15.880 --> 27:21.240
In contracts where the, for instance, the self-driving car, which is kind of objective in a way.

27:21.240 --> 27:26.600
So we find that even in robotics, there were some discussions, for instance, in the,

27:26.600 --> 27:33.560
there were, there have been several European Union projects on robotics. And one of the big meetings

27:33.560 --> 27:40.120
that was someone who said, okay, we are progressing. And can we really measure that?

27:41.400 --> 27:45.720
You say you saw this task, but we still have this task without being solved. And the system

27:45.720 --> 27:49.720
solved this task, but not the other task. So we have more, because we solve more tasks,

27:49.720 --> 27:56.120
because we have more robots. So a more specialization, but not really because we have one robot

27:56.120 --> 28:03.240
that is able to solve. And in terms of efficiency and economy, what we want in machine learning is

28:03.240 --> 28:11.800
to have systems that are able to solve a range of tasks without a lot of tweaking and tuning

28:11.800 --> 28:18.120
and changing architectures, because that takes a lot of effort from teams. And we want to have

28:18.120 --> 28:24.600
systems that are easier to develop. They require, for instance, less data, all of these things.

28:24.600 --> 28:30.200
So having metrics around all of this is going to give us a better assessment of whether we are

28:30.200 --> 28:37.800
really progressing. And especially at conferences, or when we have one of these breakthroughs in AI,

28:37.800 --> 28:47.480
really understand whether this is a great breakthrough. For instance, when I saw Alpha, Alpha go,

28:47.480 --> 28:53.400
I was impressed like everybody had one. Come on, this is, wow, this is so good. And

28:53.400 --> 28:59.960
but that was, for instance, the other day, I was more impressed about this system just learning

28:59.960 --> 29:06.600
go test, and this Japanese go, this Japanese test as well. And I was really, I think for me,

29:06.600 --> 29:13.080
this is even more important than what we saw two years ago. Right. And well, this is my view.

29:13.080 --> 29:17.720
And at some point, we would like to have some kind of metrics where we can say, okay,

29:17.720 --> 29:21.960
can we say something about what these systems are able to do and put that some kind of

29:21.960 --> 29:30.280
even quantitative assessment. So talking more in terms of skills and abilities rather than

29:30.280 --> 29:39.320
specific tasks. So that's basically the interest in measurement. And now all of these new trends

29:39.320 --> 29:46.120
about evaluation using video games. I think this is really, really interesting. But again,

29:46.120 --> 29:51.880
grab that work. Is that like related to the reinforcement learning type of work? Yeah, we are having many

29:51.880 --> 30:02.760
platforms. For instance, we have Microsoft research, launch, my graph, which is my more platform.

30:02.760 --> 30:09.960
We have good AI release these, well, now it's integrated in the AI universe. Open AI universe.

30:09.960 --> 30:13.720
Open AI universe. Open AI universe. And then you have deep mind where there are also

30:13.720 --> 30:18.520
the evaluation platforms and also some competitions using video games. And we also have these

30:18.520 --> 30:26.760
the Atari games. And so we in a way, we see many evaluation platforms where we have a range of

30:26.760 --> 30:33.400
tasks as different video games. And we see that our systems have been evaluated according to how

30:33.400 --> 30:39.000
well they behave for this range of games. It seems like relative to some of the,

30:39.000 --> 30:45.560
you know, relative to more kind of higher level or conceptual definitions of intelligence,

30:45.560 --> 30:51.960
performance on a video game is more akin to evaluating a self-driving car. What's my score?

30:51.960 --> 30:58.040
How fast did I do it? How many times did I die? Things like that. What makes these video game

30:58.040 --> 31:03.240
platforms interesting for the types of measurement that you want to get to? Yeah, that's a very good

31:03.240 --> 31:10.040
question. But I think that in a way, a self-driving car is, even if it, perhaps, it requires

31:10.040 --> 31:14.440
more technology, more different technologies than, for instance, a video game. A video game is a

31:14.440 --> 31:20.680
mini-world in a way. It's an environment. Of course, the self-driving car is an environment,

31:20.680 --> 31:25.640
but the thing is that for video games, if you just define one video games, I see no difference,

31:25.640 --> 31:29.480
even, of course, that the self-driving car is much more complex, much more into it,

31:29.480 --> 31:34.760
in terms of the technology that you need to solve it. But the good thing of video games,

31:34.760 --> 31:41.400
you usually include 20, 50, 100 video games, and then you have a range of tasks. And then we have

31:41.400 --> 31:46.840
a range of tasks, different tasks, even if they're still, they are 2D or 3D, and there are some kind

31:46.840 --> 31:55.800
of a shared input output for these platforms. There's the kind of a... So you're speaking to

31:55.800 --> 32:04.040
being able to kind of demonstrate generalizability. Yeah, that's the idea. So you tried these games

32:04.040 --> 32:09.000
to be as diverse as possible, in order to show that you're able to learn these tasks.

32:10.520 --> 32:17.720
Then if your system is good at all the tasks, or just kind of, or even if it's not optimal at any of

32:17.720 --> 32:22.200
them, I think you have a kind of a general performance for these range of tasks. And the more general,

32:22.200 --> 32:30.200
you make these pool of tasks, you go up in this skill from bottom task specific performance to

32:30.200 --> 32:35.800
kind of skills, or maybe to towards you go up in this direction of more general intelligence.

32:35.800 --> 32:41.400
And that's what you can claim that even if you take 100 games, Atari Games, that's very specific

32:41.400 --> 32:47.160
subset of all the possible tasks that you might have. And that's true. I think you can explore these

32:47.160 --> 32:52.920
first steps from bottom up from very specific tasks to more general. And you can also analyze all

32:52.920 --> 32:58.200
of these ideas about why is it that this system is good at these tasks, but not at those set?

32:58.200 --> 33:05.000
Because then we'll know these tasks are more about abstract thinking or planning. So we see

33:05.000 --> 33:09.800
that for us how we're reinforcement learning techniques, even if we are using deep learning with

33:09.800 --> 33:15.400
them. They are not very good at generalizing for these, so they are not solving these tasks.

33:15.400 --> 33:21.160
Well, so we can learn much more than we've just focused on one single problem that in the

33:21.160 --> 33:26.920
end, we put a lot of effort, we'll ace at that problem, that perhaps how much of that effort

33:26.920 --> 33:32.200
is extrapolable to other problems, that's basically what we want. We want this generalization

33:32.200 --> 33:39.160
ability in machine learning. Have you published any research that specifically looks at

33:39.160 --> 33:46.920
applying measurement to these video game scenarios? I've taken two different approaches. There's

33:46.920 --> 33:52.440
the idealistic approach, which is basically going from first principles, which is basically

33:52.440 --> 33:59.880
in my book and some related publications. How can we define a set of tasks that, by definition,

34:00.680 --> 34:07.320
are necessary to show that the system has this ability? That will be great, because of course

34:07.320 --> 34:15.240
you can say, okay, why 100 games? Why don't you use 1000? Why are these 100 games sufficient

34:15.240 --> 34:21.720
for this? You can decompose the games to these set of map them to these set of tasks and

34:21.720 --> 34:27.560
determine this game framework's ability to even assess intelligence at all. Is that one of the

34:27.560 --> 34:36.120
things? Yes, there is. We can generate these tasks, rather than we use some of these platforms

34:36.120 --> 34:42.760
generating games. If you generate a game just randomly, you get something that is completely

34:42.760 --> 34:48.360
meaningless. Usually, and it's not fun to play. You have a lot of random noise. You have to

34:48.360 --> 34:53.320
pull some structure into an environment to make it appealing for humans and meaningful,

34:53.320 --> 34:59.080
or even for machine learning. The thing is, how can we generate these tasks in a principle way,

34:59.080 --> 35:05.400
so that we ensure that a key concept here is that you can define the notion of difficulty of this

35:05.400 --> 35:10.440
task, so you can have a scale of difficulty as well. You can relate these tasks in terms of

35:10.440 --> 35:17.240
that difficulty and what they have in common. The problem about this approach is that you can only

35:17.240 --> 35:24.040
generate, at least in principle or easily, you can generate some kind of very abstract environments,

35:24.040 --> 35:32.280
a very simple environment. That's a very good way, but perhaps it might take more efforts and more

35:32.280 --> 35:39.080
ideas just to make it work. In practice, and the other one, which is summarizing some papers,

35:39.080 --> 35:45.400
for instance, last year at the European conference on AI, we had a paper on that,

35:45.400 --> 35:51.480
and it's in some other machine learning conferences. It's how to use, for instance,

35:51.480 --> 35:56.440
one technique from psychology, from psychology, with this item response theory,

35:56.440 --> 36:00.840
where you can characterize from a range of tasks, for instance, you take a competition.

36:00.840 --> 36:05.880
This is the video game playing competition. You take that, you take the results from last year,

36:07.160 --> 36:13.560
and you analyze the, you have, for instance, a 40 task, and you have a 30 competitors,

36:13.560 --> 36:19.560
and you analyze, you just try to understand not only the performance of each of them,

36:19.560 --> 36:24.280
but you try to extract latent variables by analyzing these result matrix,

36:24.280 --> 36:28.040
and you try to analyze, okay, can I add to the difficulty of this task,

36:28.040 --> 36:33.560
or the relations between these tasks? Can I analyze the relation between the participants as well?

36:33.560 --> 36:37.480
And at the end, you can get, for some of these, and you can have something like the difficulty

36:37.480 --> 36:43.000
of the task, and then you can realize these tasks, according to this population of AI systems,

36:43.000 --> 36:47.640
or machine learning systems, this is the most difficult task, but not because a score is 101,

36:47.640 --> 36:55.080
the other is 80, but just how challenging it is for the systems, and you can also determine

36:55.080 --> 37:00.280
the ability of the agents. Of course, this is simplification because you can extract two latent

37:00.280 --> 37:05.880
variables that you could do 10 latent variables to be, but these latent variables explain the

37:05.880 --> 37:10.520
behavior of these systems for this set of tasks, and it gives you information, for instance,

37:10.520 --> 37:16.360
the next competition, which tasks are completely, you get some other preferences, you can extract

37:16.360 --> 37:20.520
a third parameter, which is discrimination, you say, okay, these tasks are completely useless,

37:20.520 --> 37:25.800
because, or sometimes you have some kind of a negative discrimination, this task is performed

37:25.800 --> 37:31.720
well by the, by the bad methods, and the other way around, they say, okay, so what's the point

37:31.720 --> 37:36.840
of having this task here? And that's, that's something inherited from psychomedic, we have a question

37:36.840 --> 37:41.960
that you say, okay, good students are typically bad at that question, you remove the question,

37:41.960 --> 37:46.760
if that doesn't make sense, there's something, there's a catch there or something, because people

37:46.760 --> 37:51.400
get confused, because otherwise you wouldn't get an explanation. So all of the things help us

37:52.200 --> 37:55.800
to understand a little bit better, what is going on, especially when we have competitions,

37:55.800 --> 38:00.520
when we have benchmarks, and we've also applied that to the Atari Games results,

38:01.320 --> 38:07.000
and see what's happening with some of the games and the difficulty of the games, and we can have

38:07.000 --> 38:12.280
a better understanding of the abilities on the problems of current technology. Of course,

38:12.280 --> 38:17.880
this is, this is not for first principle, this is experimental, but at least gives us more

38:17.880 --> 38:23.000
understanding of what's going on that just looking at the results and the winner of a competition.

38:24.200 --> 38:31.080
So it's a set of methods for essentially decomposing these results into kind of generating the tasks

38:31.080 --> 38:37.000
themselves, or inferring the underlying task and their difficulty from the, the results,

38:37.000 --> 38:42.920
is that the right way to think about it? It's a question of trying to describe the tasks,

38:42.920 --> 38:47.800
and also the participants, which is kind of a latent variable that you put into the models and

38:47.800 --> 38:52.520
that you estimate through the results, and you say, okay, and these latent variables, you can call

38:52.520 --> 38:58.600
them difficulty, discrimination, and on the side of the techniques, you can call that ability,

38:59.240 --> 39:04.280
these factors of variables help you understand what's going on. Is there something that you do in

39:04.280 --> 39:10.120
some other, in some other scenarios? It's nothing really new for it's, but it gives you more

39:10.120 --> 39:16.200
understanding just the, just looking at the matrix, or just a scale that you get from the performance.

39:17.320 --> 39:21.640
For many of these games, it doesn't make sense to make it an average of the results.

39:22.280 --> 39:27.720
Some other, in many papers, you have this comparison with humans, which is okay.

39:27.720 --> 39:34.440
For instance, you have 50 Atari games, and you have one technique, one machine learning technique,

39:34.440 --> 39:38.600
and you have the results for each of them, and then you have the humans, the average human.

39:39.240 --> 39:45.400
Basically, we have use, or we are on Amazon Turk, or whatever they have, and they have

39:45.400 --> 39:51.960
this average score, and you say, we are superhuman on task one, we are, we are subhuman on task two,

39:51.960 --> 40:01.640
well, that doesn't say too much to me in a way, because well, we are not able to solve this

40:01.640 --> 40:07.640
task in terms of the human level, but we are not comparing also the number of games that we

40:07.640 --> 40:12.520
are given to humans and the number of games that we agree with. So it's not a fair comparison

40:12.520 --> 40:19.160
in the first place. It doesn't allow us, either to say, we are, there are 50 games, we are

40:19.160 --> 40:26.680
above human on 40 of them, and we are below human on 10 of them. Perhaps as an assistant is 39

40:26.680 --> 40:31.080
11, and it's much better because the difference are much better. So we have a lot of discussion

40:31.080 --> 40:37.640
about how to integrate things that are not commensurate in a way, when we put many, many tasks

40:37.640 --> 40:43.000
together. Okay. And all of these questions appear again and again, especially benchmarks,

40:43.000 --> 40:48.040
competitions about people typically would like to have one kind of a single score or something like

40:48.040 --> 40:55.160
that. And looking at all these specific scores is you don't get a lot of meaning, but you just

40:55.160 --> 41:00.360
simplify over, simplify to a single average score, something like that, and you probably, you miss

41:00.360 --> 41:06.360
a lot of information as well. So something in the middle that you can summarize the behavioral

41:06.360 --> 41:12.040
of these systems for this set of tasks that help us to understand whether we are going in the right

41:12.040 --> 41:17.640
direction, we are still very far from solving the problem that we want to solve. Okay.

41:17.640 --> 41:22.920
And all of these things. Interesting. So how about quickly kind of a quick overview of the

41:23.560 --> 41:27.000
you know, the future directions and priorities? Where did that conversation end up?

41:28.040 --> 41:37.720
Yeah, it was a very open discussion yesterday. When you set up a session with what society needs,

41:37.720 --> 41:48.360
it's like, well, everyone has a say here and we have many questions as well. And we had a panel

41:48.360 --> 41:55.080
for 40, 50 minutes. We was a very long panel. A lot of things we touched upon many, many topics.

41:55.080 --> 42:04.760
But there was this idea of having more human-like AI rather than complementary or not to what

42:04.760 --> 42:13.720
humans are able to do. I think that there's no agreement there. Because computer science and AI,

42:13.720 --> 42:18.600
one of the main goals have been automation of tasks. So of course it's good that the machine's

42:18.600 --> 42:24.520
able to do things that we can't do. That's great. But we are also interested in some tasks that

42:24.520 --> 42:29.320
can be automated and perhaps we can do some other things. But of course you have a lot of

42:29.320 --> 42:35.880
implications about that. And that these idealists view that we want humans and machines to be

42:35.880 --> 42:42.520
complementary ethics. It's very idealistic. And there's a kind of that. But perhaps we can focus

42:42.520 --> 42:47.560
on these things rather than just so human-like intelligence is important. Another way finding

42:48.200 --> 42:54.920
things that machines can do that we can't that is not really human-like. This is also a very

42:54.920 --> 43:01.320
good direction. There was also some more political questions about domination, power,

43:02.360 --> 43:08.120
for instance, corporations and organizations having a lot of power. There was a discussion

43:08.120 --> 43:13.560
not really in terms of AI companies and all that, which is a different way that we didn't touch

43:13.560 --> 43:21.960
them. But about this idea that in the future intelligence has changed our planet.

43:21.960 --> 43:30.680
And having more of that intelligence or different kinds of ways of extending the intelligence

43:30.680 --> 43:35.960
that we know of at the moment is going to transform everything. So we have a debate on all of

43:35.960 --> 43:41.560
these things that might be transformed. And also ethical things about there was this discussion

43:41.560 --> 43:46.760
about buyers in machine learning. So we had a little bit of everything with about 40 minutes

43:46.760 --> 43:52.440
of discussion. Oh, wow. Do you know if the symposium was recorded? Will people be able to find

43:52.440 --> 43:56.200
it afterwards? Yeah, we've been told that it has been recorded. It was recorded. There was a

43:56.200 --> 44:03.640
camera there. So well, probably it will take a while to have it online. But we hope that everything

44:03.640 --> 44:10.120
will be online soon. Okay, great. It sounds like a really wide-ranging and interesting set of

44:10.120 --> 44:15.880
questions that it raised. And I hope to get a chance to take a look at it. Thank you Jose

44:15.880 --> 44:24.360
for spending some time to chat with us. Any final thoughts or ways that folks can catch up with you

44:24.360 --> 44:30.600
or find out more about what you're doing? Well, we are having some workshops next year. But

44:30.600 --> 44:36.040
just if you Google my name or you will find some of the things that we are working on and some of

44:36.040 --> 44:41.160
the projects in the past years and now with the center of the future intelligence. There are many

44:41.160 --> 44:46.200
things, many interesting things going on there. Okay, awesome. Well, thanks so much. That's for

44:46.200 --> 44:54.680
having me here and it's been a pleasure. All right, everyone. That's our show for today.

44:54.680 --> 45:00.120
For more information on Jose or any of the topics covered in this episode, head on over to

45:00.120 --> 45:14.840
twimmaleye.com slash talk slash 137. Of course, thanks so much for listening and catch you next time.

