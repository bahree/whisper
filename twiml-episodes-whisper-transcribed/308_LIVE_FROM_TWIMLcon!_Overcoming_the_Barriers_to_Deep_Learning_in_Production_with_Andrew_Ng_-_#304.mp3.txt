We're excited to kick off our coverage of TwilmoCon AI platforms with an interview
recorded today on the TwilmoCon stage featuring none other than Andrew Ng, founder of Landing
AI among so many other ventures.
Before we get to the show though, I'd like to acknowledge TwilmoCon sponsors.
We couldn't have done this without our founding sponsors who supported us in a big way as
we pulled this event together. A huge thanks to all of our friends at Sigopt, IBM and
DotScience.
We're also grateful to CloudDera who joined us as a gold sponsor. CloudDera was our first
ever sponsor for this podcast and we thank them for their continued support.
Our silver sponsors include Fiddler Labs and Virtua, two innovators you'll undoubtedly
be hearing more about in the near future. And last but not least, we are overwhelmed by
the outpouring of support by our community sponsors including Apple, Etsy, Figurate,
Georgian Partners, Imerit, LinkedIn, Logical Clocks, Neuromation, NYU Future Labs,
Valahai and Waits and Biasis. Thanks so much to all of these great companies. Please be
sure to check them out at twilmocon.com slash sponsors and thank them for supporting us.
And now on to the show.
My first guest really needs no introduction but I will try anyway. Through the podcast,
I get to have a lot of conversations with a lot of incredible people and learn about
how they got their start in machine learning. And it probably wouldn't surprise you that
many of the folks that I talk to got their start with my first guest courses, his Stanford
University courses, Coursera and more recently deep learning AI. In fact, I got my start
in the space taking one of his courses and I can't think of an individual who's helped
more people enter this space than our first guest, Andrew Ang. Andrew?
So it's great to see all of you here. It is great to have you here. So my first question
is, I don't know if you saw it on the document that had your autograph but I lost 4.85 points
on homework assignments in your course. What was that all about?
Yeah, I'm sorry, I think you are perfect. It must have been a bug and it's a great
question. Awesome, awesome. So you also launched your most important learning machine,
Nova, back in February. How's fatherhood treating you?
Yes, so Nova is now seven months old. But you know, there's actually a story about how
we chose her name. We wanted her to have the initials, you know, neural networks taking
off. So we wanted her initials to be NN, so Nova. But one layer deeper than that, you know,
thinking, all right, have a new baby. I think every person on the planet is a unique person,
is a unique human being. So people on, no one is a number. So we gave her the middle name,
Athena. So the full name is Nova Athena. Does the initials are NAN?
That doesn't burden, she'll have to carry it for her whole life. Nice, nice.
You are prolific. You are up to so many things. Can you give us an overview of what you're
working on nowadays? Yeah, the teams, the rise of AI, the rise of machine learning means there
are a lot of pieces needed for it to reach this full potential. So right now, the three teams,
swimming most time with them, leading our landing AI, which is helping companies jumpstart AI
adoption, deep learning.ai, which is our educational producers content, a lot of it is on Coursera,
also a weekly newsletter called The Batch, which is subscribed to get weekly news about machine
learning, and then also AI funds, which is a startup studio that builds AI power startups from
scratch. The rise of machine learning creates a lot of new opportunities. So building these three
teams which work together in the ecosystem, trying to build many pieces that, you know, let's build
AI power future. Let's dig into landing AI a bit. I associate that with doing work in the
manufacturing space. Give us an overview of the company and what it's up to. Is that the case?
So I think I saw with my own eyes how an injection of modern AI can make a company much more
effective and valuable. I think, you know, building up Google Brain and then leading AI at by-do,
I saw, right, with my own eyes, a couple of great companies become modern AI companies,
have become much more effective and valuable along the way. But if you look at what we've done
in machine learning world, I think we've transformed the software internet sector. So many of the
companies represented, you know, at this conference and many companies in Silicon Valley and Beijing,
even outside the top small handful, you kind of have a giving a lot of traction in AI.
I think the next step for AI is first to transform all of the other industries as well,
outside software internet. And so landing AI works with many companies from manufacturing to
agriculture, to healthcare to others. And we can act as a partners outsource chief AI officer
to help a partner build a AI function, channel team, develop IP. And all of those we help partners,
we will cook for you, but also teach you how to cook so that after a couple of years, you can
insource the function and be an AI-enabled business in your vertical, which we think can
help a lot of people and help a lot of companies become more effective and more valuable.
And frankly, we are go, there's more to life than your financials, but I think our impact,
we hope actually has a mature impact on the market cap of companies we work with.
Can you give us an example of some of the types of problems that you're helping customers solve?
Let's see, one of our engagements has been with a large agriculture machine in your company.
And I think if you can help a company, you know, reposition from being a traditional agriculture
company to being an AI-enabled agriculture machinery company, then you can build smart agriculture
machinery. We're going to have the same farmer, same farm, but with automation suggestions for how
to control the machine better, you get more crop from the same farm, the same farm. And so this
is direct impact on the farmer as well as not surprisingly on the company building these types of
things. I think, you know, I was just in Latin America actually last week in Colombia visiting
companies in different industries as well, from logistics to manufacturing. And what I'm seeing
is that there's very strong CEO level interest to help companies in all sorts of industry sectors
become AI-enabled. And it's not that, it's not that if you're a, you know,
manufacturing company, you want to become an AI company. It's just that, you know, let someone
else do that. But I think in the future, an AI-enabled manufacturing company can be much more effective
and valuable than one that doesn't. Maybe one lost disruptive ways to technology was to rise
to the internet. And we saw that if you have a shopping mall plus a website, you know, yes,
everyone has to build a website, but that doesn't turn you into Amazon. Or if you're a taxi company
and you build a website, you know, you're not an internet company, instead Uber, Lyft, Grab,
you know, DD are true internet companies. AI arguably is as disruptive as the internet. And so
there will, and then it changes the core of how different companies will compete. What are the
things that help you build a defensive business? What are the things that generate value?
Where do you play? Where do you not play? What is the new strategy? And I think companies able
to figure that out will become, you know, what will survive and thrive. One of the myths we
tell Silicon Valley, which is not true, is that whenever there's a disruptive technology, it's always
a style that's that wouldn't. That's just not true. Where they rise the internet, some startups
that did well include Google, Facebook, and Amazon, but some incumbents that did well include
Microsoft and Apple, which were not internet companies, but became great internet companies.
So what the rise of modern machine learning, and the exciting work that many of you in this
community are doing to, you know, land these technologies, to bring them to fruition, I think
this is very, that the races again on, where their great opportunities for startups,
but incumbents also have a lot of advantages. And if they play their cards right, they can become
very valuable, very effective AI-enabled businesses in their verticals. So what's involved in
playing those cards right? One of the hardest things for companies to embrace AI is to scope
the right set of projects. And so we spend a lot of time, I think we've actually become very
good at working with companies to figure out what you should and should not use machine learning
to do. I think, you know, some people advise a good company is one that's not small.
Maybe actually here's story. When I was, some companies tried to do the biggest, most glamorous
project as probably number one, and that's usually a mistake. At least the failure that then causes
you lose faith, actually says to company back, because you need to regain the faith.
One story, early days of Google Brain, people in Google, where Sophie didn't know how to use deep
learning, or even skeptical about it. So my first internal customer was the Google speech team.
It wasn't the most, you know, it's not web search advertising, right? Speech recognition is a
nice project, but it's not web search advertising. But by making Google speech more accurate,
it helped other teams within Google gain faith in our ability to deliver results. It also taught
the company how to use, you know, deep learning. I remember when I first GPU server, it was just
a server sitting under some guy's desk with a nest of wires. But that told us important lessons
about how to train models on GPUs. After the first successes, second internal customer was Google maps,
where we use a OCR, photo OCR to read house numbers to more accurately place houses on Google maps,
the improved quality map data. So only after those two successes that I then started the most
serious conversation with the advertising team. So, so one lesson from this is I think start small.
It's more important that your first project comes something like speech recognition, you know,
back in the day to help the company learn what it feels like and then to use that to build momentum.
And then I think it's important to form cross functional teams with machine learning experts
and business application experts to brainstorm projects together. One tip I offer a lot of
the companies kind of approach it. Often the number one project that the CEO gets excited about
does actually not the project you should work on. So I recommend to companies to brainstorm at
least half a dozen projects and spend a few weeks deeply evaluating, is it technically feasible,
is it actually valuable and do that before investing, you know, several few months worth of resources
to do that. And I think it's also a quick answer. Several months ago published online
an AI transformation playbook which talks about the sequence of steps from scoping pilot projects
to building a team, to providing training, to thinking through your strategy, to even
communications, including some of the pre IPO companies you work with, you know, value,
mature, communicating their AI value thesis creation clearly. But that type of AI transformation
playbook which can find online tell the company to become AI Naval.
Yeah, one of the things that I find fascinating in speaking to folks that are leading these
efforts is the challenge of managing the portfolio of projects. They need to start small and kind of
pick off the easy wins, but they also need to maintain that vision, the overall promise of AI
and what it can lead to so that they can continue to drive enthusiasm. Do you see that as well?
That's interesting. I think the panel of the company, some of the things that is important
is like go-de-locks principle for AI, right, to not be over the optimistic. So HGI is not around
the corner, at least unless Elon Musk has a secret lab somewhere. But also not to. But also not be too
pessimistic because there's a lot it can do if you under-ame then your competitors or someone else,
you know, you're missing out on opportunities. So I think it's important that not just engineers
know how to do this, but that company management and executives also learn how to do this.
Actually, we're running an event in Columbia. We run these events around the world called
Pine AI, but it was running an event in Columbia and I met this engineer who came up to me and
she said, Hey Andrew, I love your AI for everyone course. And this is a very technical person,
right, AI for everyone was scope for a non-technical audience and she said, love your AI
for everyone course, not for myself, but I'm getting tons of non-engineers to take it and just
making them much easier for me to work with. And I find that in a company, if the management
structure, the product managers, the executives, even the C suite executives have a basic on the
survey AI, then the machine learning team or the data science team is actually much better
set up for success. When organizations do make that commitment and initial investment, spin up
teams, how well do you think enterprises are doing and getting value out of the other end of
their ML investments? I think it's really hard. We've seen the launch companies, you know,
the mature, relatively sophisticated technology companies are getting pretty good at building
and deploying machine learning systems, but I think a couple of challenges. One, AI grew up
in consumer internet companies, which just have a lot of data, 100 million users or billion users,
you have a lot of data. In other industries, you often don't have that much data. So if you're
manufacturing plant, doing visual inspection, using computer vision to tell if smartphone is
scratch or not, you don't have a million pictures of scratch smartphones because you just
fortunately did not manufacture a million scratch smartphones. So I think how do you get
machine learning to work with small data instead of big data is an important technical challenge.
And then it turns out, some of the processes we use in big companies, I mean, so when I was deploying,
actually, actually, so, you know, deploying a launch speech recognition system, right? Well,
what happens? You deploy the model and then the world gives you data that's different than what you
had in your test set, stored in your hot disk, right? So again, in the early days, the conversations
we go like this, the machine learning is to say, wow, look, I do so well on the test set,
and then the business people will say, no, look, the customers are doing these crazy things,
they're talking a car, there's background noise, your speech recognition engine doesn't work.
And then the machine learning says, look, I did so well on the test set. What are you talking about?
And I think, I think, you know, the machine learning world, I think now, more machine learning
people realize our job is not to do well on the test set you have on the hot disk. The job is to
build a product, move the product or the business forward. But if you look at how we use to solve
these problems, what we used to do was, if we ship a speech system and for whatever reason,
the performance degrades, then we would monitor it, alarm it, you know, and then page at UT,
whatever, and then someone like me would go, hey, you know, you 20 engineers, there's a problem,
please go and fix it. Now, the launch companies could do that. Every time there's a, you know,
stick them a problem for a major product, we could find 20 engineers to then go and, you know,
page at UT people and fix it. But for other applications where you just cannot afford to do that,
we need better, more systematic ways to monitor, alarm, mitigate. So I think the whole
machine learning world, including I think many of you, you know, coming up with better, I guess
in this community, MLO seems to be a growing term to figure the processes to manage that.
So many organizations, including many of the folks here, are in this transition point that I
described earlier. They've launched successful proof of concepts, they've generated excitement
within their organizations, and now they need to scale up so they can get more models out into
production. What have you seen working in those organizations that are similarly situated,
that are scaling up their ability to, you know, drive real value out of these models?
I think that, I know that this conference talks about platforms, and I think platforms are
important, but the hopes of the technical aspects of scaling these up, I think what makes
scaling up hard is even more the people, business aspects of it, although the technical aspects
of it are really important to, you know, actually after Sam and I were chatting a couple days ago,
and after our conversation, it was very interesting. I went back and reread the 30-year-old paper
by Fred Brooks titled No Silver Bullet, and it made the point that even as we improve
programming languages, software engine is still hot, right? Now I'm really glad that I could
code in Python and not, you know, Fortran or Assembly or something. I think we're all happy about that.
Even though we now have Python and these fancy tools, TensorFlow PyTorch, software engine
is still really hot, and that's because the tools we have did not remove the essential complexity,
this is Fred Brooks' term, the essential complexity of software engineering, which is a
thing through clearly what you want to do to write the specification, to then express it, and then
to test it. And the hot part of software engineering is not, you know, do you use a go-to statement,
or do you use a fancy wild loop or whatever. It is thinking through clearly what is the problem,
and what are the steps needed to solve it. Now, one of the beautiful things about
modern machine learning is that it removed essential complexity from a task. As in, you know,
10 years ago, if you're building a computer vision system, it was really complicated. You would
download these, now, maybe, arguably, obsolete algorithms, you know, sift, surf, hog,
have these features, then you do something crazy about colonization, where you feed it through,
you know, some software library, say OpenCV, you find it doesn't work, the walk the image,
well, it's just crazy complicated set of steps. And you did it on an octave.
Oh, yeah. Actually, OpenCV, which you'll see at the time, I think.
But what deep learning allowed us to do is to say, forget all these steps, let's get a lot of data,
trade in your network on it, and then, and so the workflow changed dramatically with the rise
of deep learning, which is why we're so many more things now. Now, I think just as I'm happy to
use Python instead of, you know, Fortrad, we've been seeing a lot of improvement in developer tools
for building a deploy machine learning. Some of this removes, you know, accidental complexity,
Fredbroxist term. And I think we need more thought on how to remove the essential complexity
at the work we still have, which is, and a lot of my work is meeting with a company, meeting with,
you know, CEOs, leaders, brainstorming, what are the things actually valuable for the business,
thinking about where do you get data, and do they have data, and how do you organize the data,
and then those things are essential complexity things that are still difficult to, to relieve with
today's tools. Which is not to say tooling is not important. The game Python is great, but I think
the heart of what makes a machine learning problem difficult is still thinking through clearly,
what does the problem want to solve, and where to get the data, even though I'm grateful for the
much better ways we have today for expressing the neural network architecture I want to train.
Talking about that, what you refer to as accidental complexity versus kind of essential complexity,
very similar theme is the core mission and motivation behind Airbnb's platform team. They
refer to it as, as their mission is being eliminating the incidental complexity of machine learning,
so that their engineers can focus on the unavoidable complexity that essential complexity that you
refer to, and that has kind of power their development of a whole set of, you know, platforms and
tools to allow them to move more quickly. And when we spoke about this a couple of days ago,
you made an interesting point about one of the success factors being an organization's ability
to iterate quickly. Are there particular things that you've seen organizations do that allow
them to iterate very quickly and do more experiments, understand what's working, what's not, you know,
with a shorter time like? Yeah, so here's one example of a slightly unusual process that some of
my teams use. We're all used to agile, you know, two-week sprints. Some of my teams use a one-day
sprint, so there were up those like this. We wake up in the morning and look at the experiment results
that we had run overnight, and then we do our analysis, gap analysis, our analysis to figure out,
you know, what are the shortcomings of the algorithm? So we do that in the morning. And then,
based on that, we brainstorm what to do, do collect more data, chase their architecture,
our regularization, whatever, brainstorm a bunch of stuff, and then different tasks,
different team members take on different tasks. I get more data. You try that regularization
hyperparameter. You try that, you know, data augmentation, whatever. We write code in the afternoon,
or write code in the morning through, you know, afternoon evening. And then, we run the experiment,
next experiment overnight. And then the next morning, we wake up and look at experiments from
last night, and then iterate the next day, our analysis, plan what to do, run code overnight.
So now, this workflow isn't for everyone. It tends to work best for when the machine learning
job takes, you know, like four or five hours to train, because that workflow fits in well,
with this type of cadence where you code during the day and run experiments overnight.
But I found that if you could, in agile development, we were used to this idea that you should
replan every two weeks, or whatever is your sprint cycle. But in machine learning, it turns
out a lot of workflow of building a machine learning system. It feels more like debugging than
development. And as you know, it's a key insight. And with that, and so this daily sprint cycle,
we found to be good practice for quickly driving down errors of certain types of workflows.
I think a lot of us are still making up, you know, are still indenting these new processes.
I think of how long did it take our community? We went through a lot of versions of version control,
right? From emailing each other, code, email, to get, you know, I guess CVS version get, right?
So I think we're still in the early phases of making up the processes that are suitable for
machine learning workflows. Yeah, I think the kind of contemporary version of emailing files for
version control is probably putting hyper parameters in file names to track experiments.
Yeah, and actually, let me describe to you another, another strange process that we use,
which is talking about version control. So we're pretty good tools for editing and
versioning code. I think we're still, you know, indicating some companies are working on this,
but I think there's still nascent tools for editing and versioning data. And, you know,
and this is one thing you, I think there's a gap between what is done in academia versus what,
you know, we need to do it to build production machine learning systems. Here's one example.
Say we have a test set, train a system, it does poorly on the test set. What do you do? Well,
sometimes we go in and edit the test set, right? Because we, now you can't do that. If you go
to the publisher paper, it's not legit to edit the test and look, I edited the test set and got
great results. You don't want to publish that paper. But from a, from a, from a business production
point of view, sometimes you realize the test set labels are wrong. They don't, they're not
longer business objective. And so you go and edit the test set. And I think that needs to become
part of our accepted workflow. And, and then we need better tools for doing the editing and
versioning it and having multiple people collaborate on editing data. I think these are things that,
I think I feel like, you know, some of my teams, land AI, deep learning AI, I find we're making
about our own processes, but hopefully as we as a committee learn from each other and come up
much better ways to do this. One of the challenges I see is that academia has a much
easy time working on certain problems where progress is, you know, measurable and repeatable.
And if every team edits their test set and changes the labels in a different way, it's,
you know, it's much harder to benchmark, right, who's better at editing test set labels.
So, so there are certain category problems that I think are difficult to study systematically.
And so for good or bad reason, they get less attention in academia. Maybe one, one of the example,
I was at ICML International Conference of Machine Learning, there was a lot of discussion
on robustness, but maybe what happens is a lot of times is let's say, let's say we train the
deep learning system to diagnose from X-rays. And we train on high quality, high-res, X-ray images
taken off of a Stanford University X-ray machine. And we post your paper saying this does really well.
But if you ship your model to, you know, all the hospital down the street with a blurrier X-ray
machine, whether radiologists has a different protocol for how the organ, the patient, it doesn't
generalize, right, doesn't generalize well. So I think we know that this robustness or
generalization problem, it is a problem. But how do you systematically study how well your
learning algorithm can do on a brand new distribution of data, you know, that the algorithm's never seen,
right, so that's just been a relatively more difficult problem to study systematically. So even
though this is a problem that we've seen production all the time, and we have, you know, practical
solutions solving it, I think the amount of attention in academia to this problem is underweighted.
And there are some academic papers, but they tend to be framed in, you know, like domain adaptation
or framed in certain ways that makes it easier to study systematically, whereas sometimes the world
fills you some random data, and so it's been more difficult to formulate benchmarks to drive for
systematic study of some of these issues. Along the lines of robustness when we were chatting
a couple of days ago, you mentioned some interesting things that we're happening at landing around
managing the risk of machine learning deployments once they get in the production. Can you elaborate
on that for us? Yeah, so we've been doing, I feel like my team's doing a lot of strange things
one of the processes I've used before is called the FME analysis, it actually does a set of
processes that grew up in the hardware world where, you know, if you're building an airplane and you
want to make sure the airplane is saved, then you do an analysis on the risk of what could go wrong,
and typically you categorize, what's the chance of this thing going wrong, what's the chance of,
you know, this error on not moving or something, what is the severity, how bad is it if it
does happen, and how detectable is it, right? So those are the three things you try to categorize
each risk again. So one of the things we've been doing is when we look through our machine learning
projects, when you think about how to deploy these projects, you know, in a factory or elsewhere,
my teams tend to, we like going through a very rigorous exercise where we, you know, pre-mortem
or think through all the risks and have a team discuss in debate, but each of the things we could
go wrong, what's the probability, what's the severity, and what's the detectability,
so that we could do a better job seeing around corners and plan for things rather than be surprised
by them later. I think we're seeing a lot of startups, a lot of, there are a lot of machine learning
projects that end up at the proof of concept stage, but unable to go beyond, and I think, you know,
landing AI, putting a lot of thought into how to see around corners so you can
take all these things all the way to deployment and production. So I think we've become pretty good
at that due to processes like these. Are there specific examples of issues that this, the
figure mode and effects analysis process has allowed you to avoid? I would say the one number one
most common bucket is planning for the ways that the real world test sets may be different than
the test sets stored on your heart disc, and whether the consequences, what's your mitigation,
what's your escalation process, and what's your response time, so you can give reasonable SLAs
for what happens to the shipping machine and the system, the world does something crazy and then what?
How broadly applicable do you think this, this way of thinking about deploying
models and AI products is, do you think it's something that everybody should be, you know,
pulling up the Wikipedia page and figuring out how to do, or is it very specific to the kinds of
scenarios you're looking at at landing? I think, I think, my analysis, and then you can look
along with Wikipedia, is a relatively heavyweight process, but maybe for most of my life, most of parts
that I work on, I feel like, for most parts that work on, I tend to like to identify the risks
up front, you know, actually when those ads on Polsera, one of my direct reports gave me,
you know, gave me direct feedback, and she said, hey Andrew, I find my one-on-ones would
you very depressing, because every time we meet, you want to talk about all the things that could
go wrong, but look at all the things going right, why are you always, you know, so depressing,
and then I actually told her, yes, I understand, vision for the bright future, but I tend to spend
most of my time planning for the scenarios that could go wrong so that we don't hit those,
then she was okay after, I gave her that context, but most of my project planning, you know,
is the discipline of having a vision for a bright future, but then also being very explicit in
listing out all the things that could go wrong, so that, so that hopefully we're not, you know,
as surprised as something, so to avoid those outcomes. Having just planned an event,
I can definitely relate to trying to anticipate all the things that could possibly go wrong.
Yeah, and just say, I think, you know, this community, I find the work being done in this
community, really exciting. I think over the past few years, you know, machine learning research
has really raised ahead, our ability to build machine learning models has really raised ahead,
but if this is a set of things, if it's a small set of things needed to do a good machine learning
model, the set of software you need to write to a super product is so much bigger than the work
of building the machine learning model, so the advances in our ability to build high accuracy
machine learning models has raised ahead, and this whole technology improvement is driving a
lot of value, but there are also a lot of other things that I think our whole community is still
figuring out, and I think, you know, the, the ability of organizations like, like you guys
told us now, as well as all of us in this community sharing our learnings with each other,
I think we have important work to do to move the machine learning world forward.
And I think this is especially important if you want machine learning to have an impact
outside the software into that industry. I feel like, you know, the rise of open source tools,
things like TensorFlow, PyTorch, many others, well, the rise of archive papers, right,
knowledge can download for free. That's done a lot to hold machine learning break outside
the software into the industry, which it's going to be a big part of the next wave of
where machine learning needs to go. Well, Andrew, thank you so much for joining us,
and helping me kick off our very first Twimmelcon AI platforms.
That's good. Thank you, Zaz. Thank you.
All right, everyone. That's our show for today. For more information about this and every show,
visit twimmelai.com. Thanks again to all the great sponsors of Twimmelcon AI platforms.
Head over to twimmelcon.com slash sponsors to learn more about each and every one of them.
While you're there, be sure to click on over to the Twimmelcon news page to follow along with
all of the updates coming out of Twimmelcon. Thanks so much for listening and catch you next time.
