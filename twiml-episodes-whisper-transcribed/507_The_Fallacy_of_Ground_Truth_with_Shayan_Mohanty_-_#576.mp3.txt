All right, everyone. Welcome to another episode of the Twomel AI podcast. I am, of course,
your host, Sam Charington. And today I'm joined by Shayan Mahanti, CEO of Watchful. Before
we get going, be sure to take a moment to hit that subscribe button wherever you're
listening to today's show. Shayan, we are going to spend some time talking about a topic
that we both enjoy chatting about. So this should be a fun one. Data-centric AI in particular.
But before we do, I'd love to have you share a little bit about your background.
Awesome. Thanks so much for having me, Sam. My name is Shayan. I'm the CEO and co-founder
of a company called Watchful. And we're building the platform for machine teaching, which
is very much like a data-centric AI approach. And I'll talk about that a little bit later,
I assume. But prior to this, I built systems at Facebook. So I was a tech lead for a stream
processing team that processed all the ads-metrics data for all Facebook products. And I led
some machine learning teams there as well. I'm also a guest scientist at Los Alamos National
Labs, where I've given talks ranging from Atomada Theory to machine learning. So really, really
pleased to be here. Yeah, before we get too far into the conversation, give us a quick
overview of Watchful. Yeah. So as I mentioned before, we're building what we call a machine
teaching platform. Really, our goal is to build tools to help people largely automate the
process of labeling data for machine learning. We kind of see that as the biggest bottleneck
to getting machine learning into most organizations, most companies. And we're really trying
to sort of close that gap and make it so that we can increase the number of machine teachers
in the world, so to speak, or allow companies and organizations to solve their hardest problems
using machine learning. When you mentioned that you've seen labeling
be the biggest gap, but I want to ask you to elaborate on that. But at the same time,
it's like we've been talking about that for a long time. It shouldn't be that much of
a surprise. And yet, you know, here we are with Andrew and others having to name this
challenge and talk about data centric AI. You know, share a little bit about your experience
with, you know, how labeling is actually done in organizations and the kind of challenge
that it represents. So I would say that they're kind of like two main ways that labeling
is done today. The first is by crowd. So that's where you get mostly managed vendors. That's
where you used to have mechanical Turk. And now it's, you know, largely upended by the
scales and appens of the world and so on. You can think of that shape of process as you
have some data, you hand your data to an army of humans, and that army of humans labels
it for you and hands it back. That works really well for tasks that require almost no context.
So you want to do things like box stop signs or box pedestrians or you want to say, you
know, get some sentiment on some tweets or something like that. That works really well.
The second primary way that labeling is done today is by bringing it in house. So that's
where you have vendors that provide interfaces for managing your own army of humans or that's
kind of where we play, which is kind of like in the auto labeling space where the goal
is instead of having an army of humans to begin with, you could have the one or two experts
who are really like the de facto domain sort of like knowledge owners of a particular
task or particular set of problems and have them label the data. But instead of requiring
30 of them to spend a month in a room just labeling data, instead ideally they should
only have to spend maybe four hours across two people and output the same quantity and
the same quality of data, but several orders of magnitude faster. So that's kind of the
space that we play. Now we're seeing more of a shift to kind of like this ladder way
of labeling. What we found is that a lot of the time the things that you'd otherwise want
to use crowd labeling for actually for the most part has been solved by way of these
like foundation models. So now you've got open AI building things like GPT-3 and so
on where you can largely just reuse those models for a lot of like the common tasks that
you'd otherwise be using an army of humans for. Now obviously like in computer vision
there's just like a longer tail on a lot of those tasks so that might not be the case.
But especially in language, you can for the most part these days use a foundation model
for a lot of these common tasks and not have to worry too much about army of human labeling.
What gets really interesting is when you want to fine tune that foundation model on your
task and that then requires bringing that labeling task in house and really trying to formulate
what exactly the data set needs to look like in order to get good performance out of your
system. And that's where data centric AI really can help.
It strikes me that you're making a bit of a leap there that I'd love for you to elaborate
on. Why is it that fine-tuning one of these foundation models requires you bringing
the labeling in house. I suspect it has something to do with this idea of context that you
refer to.
Yeah, exactly. And again like what I'm not going to say is that every use case requires
in house labeling. Like there are a fair number of them that you know you could you could
just farm out to as many people as you'd like because it requires very little context.
But the moment it requires context that is specific to your organization that's when
you must bring it in house or you have to find some way to produce that context externally
for your organization which frankly is like a philosophically hard problem. So that's
where sort of like having as I mentioned you know your one or two experts who really
are like the the de facto standard for how a particular problem is solved inside of
your organization.
If you could sit them down and have them label a very small amount of data and use that
in the fine-tuning process that should yield a better system than if you were to try and
capture that context or knowledge by way of like a set of instructions that you then
hand to a bunch of people who have never done this task before.
That's sort of the idea but then you get into all sorts of sort of interesting pitfalls
where how much data is the right amount of data to get a good system you know like what
parts of my data should I be focused on how much time should I be spending and in what's
more is that these experts generally speaking have other things to do then sit there and
label data all day right you know doctors have doctor things to do lawyers have lawyers
things to do so like what you want to do is make sure that you're using their time as
efficiently as possible and data centric AI is really just like a framework for how to
think about all these things.
And when you think about data centric AI in terms of a framework like how do you parse
out the pieces of it?
So it's kind of like a hard question I can tell you the way I think about it right which
is they're kind of like two primary like there's one big decision you have to make which
is do I want big data or do I want small data that's kind of like again a philosophically
hard question answer but let me let me try and like unpack this.
Not all data is made equal when we're talking about machine learning training a lot of
the time when you're training on several millions of rows or several several million
data points the vast majority of them are actually not moving the needle that much for
a model.
So that begs the question okay if I could just filter out the vast majority of my data
especially so if I can interrupt especially so if you're starting with a pre-trained
model totally that's already been trained on the general exactly right exactly so you
want to like narrow in on on on the specific right but now the question is okay if I have
a small data set like the reason why you might want to train on a larger data set is because
it includes more diversity in that data set or there's more of a chance that it includes
more diversity there's obviously not that guarantee it really depends on the distribution
of your data but there's a higher likelihood that a much larger sample will yield the right
diverse sort of like properties that you'd want in your data set for your model to generalize
well on the other hand if you artificially constrain the size of the data set you have
to be very careful about not introducing unintended bias or lack of diversity in the data because
again you're artificially limiting it so that's sort of like the first question you have
to ask do you have the right make up of data the right make up of techniques to be able
to select just a small portion of the data set such that you still get the same properties
of diverse who you're looking for for generalization of your model and you're not risking
very much on the other hand if your data set or your task requires large amounts of diversity
and you're not confident that you can sub select you know a tiny portion that is both
needle moving and highly diverse you might actually be better off training on a larger data
set but then you need ways to label that larger data set so depending on which direction
you go with that fork you might end up with different techniques you know I'll throw
two examples out there like active learning is a very good technique for the sub selection
and sort of like iteration on very small data sets while something like weak supervision
may be better at larger scale data labeling sort of tasks so depending on which side of
that equation you err on you start going down a different path that then takes you through
several different techniques that you can kind of like pull together like Lego blocks the
yield a very good labeled data set but the properties of that will actually depend on both
your underlying data set type of model you're trying to train and you know kind of the task
at hand now for completeness I'd love to have you explain active learning and weak supervision
but I think before we do that it's interesting and it strikes me as interesting and important
to really emphasize that data centric AI isn't a technique it's like a theme or an idea
and and that is an umbrella for a number of techniques that could help an organization
be more data centric which I guess we haven't defined but yeah yeah and your end calls it
a movement yeah which is pretty much what it is I mean like frankly a lot of these techniques
a lot of these like um processes are not new you know like they've been done for decades
already at several companies the reason why it's an interesting movement is because while
a select few have been able to sort of intuit a lot of this motion just by nature of working
with their data and working with their models many haven't yet and it's a valuable process
to sort of like capture that knowledge and bring it to bear on the rest of the world by
wave like a movement or a framework or set of frameworks so that's I think what makes
data centric AI so interesting it's the fact that it's not a technique it's not a selection
of techniques in fact it's it's really just like a shift in mentality which then means
that like data scientists don't necessarily have to learn brand new types of machine learning
you know you don't have to learn a brand new skill set in order to apply this you really
just have like shift your mentality a little bit and that's what makes it so interesting
so explain active learning for us so active learning takes a lot of different shapes and forms
I think one of the most common shapes is sort of this idea of uncertainty sampling so mechanically
what you do is you have let's say a large data set and maybe some of it is really well
labeled other parts or not what you do is you train a model on the parts that are already labeled
and then you inference against the parts that are not and you measure how uncertain the model is
on certain data points there and the ideas that the parts that are most uncertain are the parts
where a human should probably look at it and evaluate and adjust the label as necessary so
those are theoretically the data points that will move the needle the most for the model when
it's retrained on them and you kind of keep doing this until eventually the model is actually doing
a pretty good job of predicting across the entire data set so that's sort of active learning and
again it's very very good for these like you know small data problems so to speak and again I'm
painting with a very broad brush here because active learning can also be applied to very large data
problems but it's specifically very very good at these small data problems where you have a very
very large set of data and you want to know what part of it is likely the most interesting for you
to sort of spend your time looking at as a human that's where active learning can be like uniquely
valuable on the other hand weak supervision is a technique that's been gaining popularity in the
last couple of years where the idea is that perhaps you don't have any like highly trustworthy labels
so as an example your data might be so huge or so varied or just require an immense amount of
subject matter expertise that it's very difficult to get any amount of like direct supervision
in any meaningful way so what you might do is create labeling functions or these like weak
learners or or you know sources of weak supervision and it's weak supervision because you can kind of
think of these as like functions you know a very simple function could be like a regax like if I
see the word credit card somewhere in this text that I know that this customer support ticket is
very likely payment related right and then you rinse and repeat with several other patterns like
if I see bank or invoice or if I see interest rate or whatever then then you know it has something
to do with money so in a way these are kind of like heuristics you know they're rules of thumb
they're they're no better than just saying like you know this thing is going to be noisy some
percentage the time but note that you're not going row by row anymore you're not saying okay I
have to sit down and say yes or no whether this is you know money related or not or payment related
or not or whatever you're saying generally speaking if I see these words and maybe
approximately this order then it is very likely payment related so by doing this you create a
collection of these functions that you can then pass into a model that learns a label based on
the presence or lack thereof certain labeling functions and kind of like the combinations and
the contradictions between them and then it doesn't really matter how much data you're passing
into the system you know you could be passing 2000 rows which would be easy or 20,000 rows or
200,000 rows or two million rows it doesn't really matter because the idea is that as long as
you've sampled the data in a way where it's representative of the larger whole then the functions
you've created are still representative of the larger whole you know you can you can apply them
to a larger sample as long as it's sampled in the same way and get the same or predictable results
um but you can imagine that like weak supervision as a side effect is very very good at labeling
for like the head of a distribution where there might be a lot of commonality you know you can create
labeling functions that cover large quantities of data but as you get further and further into
long tail it becomes harder and harder you know all of a sudden you're creating labeling functions
that cover maybe one row at a time and that's like strictly no better than having hand labeled them
you know it might even be worse because you have to like sit there and articulate a rule for it
active learning on the other hand can can help direct you to where that long tail might be or at
least the parts of the long tail that are most needle moving for your model yeah well it strikes me
is super interesting about the about weak supervision uh and you know tell me if this this resonates
in a lot of ways is kind of what you might do absent machine learning like you create a bunch of
heuristics and you you know we probably wouldn't have called it label something but like you assign a
tag or you categorize a transaction and you know so we know how to do that you know but the problem
is always that you know using a rejects isn't going to be exact or if you need to make it exact or
account for kind of noise then your system gets a whole lot more complex if you're trying to do it
declaratively or you know imperatively but machine learning helps with that because now all of
these you know imputed labels are kind of probabilistic and we're assuming noise and so the two
the heuristic and the machine learning kind of marry at well in this idea of of weak supervision
yeah that that's exactly right I think like the theme of this conversation I think is things that
have been done for decades but haven't really had like labels put on them uh like we we're talking
about expert systems and rule systems and and so on like you know there's a very classic motion
in just like software engineering in general where it's like there's a very real business problem
the way you solve it first is you throw humans at it right you like have a bunch of people
just like go and manually do the thing eventually you build simple software to solve that the very
first iteration of simple software will be things like rules you know but then very quickly you discover
that your rules are brittle and like as as business logic changes and as your problem space changes
and so on it becomes very difficult to manage just large quantities of rules that they overlap or
conflict in various ways um so there's this like non-linear jump they end up having to make
from an expert system or rule system to machine learning and that to me is one of the most
interesting jumps that an organization can can make and that's sort of why weak supervision is so
interesting it's because as you rightfully pointed out you can repurpose a lot of those rules
as heuristics you know it's it's a simple mentality shift which then yields an implementation change
but still it's like instead of talking about them as rules treat them as rules of thumb right just
assume that they're heuristic in nature assume that they're going to be noisy in some unknowable
capacity and build robustness around your system that way and then output some label data that
you can then train a model on that will hopefully then learn an even more robust and generalized
form of the relationship between the input and the output and that's where you get some like really
meaningful gains and and I think that gap is one of the most interesting to traverse because
prior to the formalization like weak supervision there was no linearization of the process going
from like a rule system to machine learning like you would have to stop what you're doing and then
like go hand your data to a crowd and then like you know you have to kind of go all in on machine
learning at the moment you do that here there's like all of a sudden a nice incremental process
that you can you can adhere to which makes weak supervision a very interesting solution
for lots of organizations that find themselves in that sort of like chasm between I know I need to
get to AI but I still have a bunch of like legacy rule systems or expert systems that I don't
want to like lose because you're providing value how do I sort of bridge that gap yeah I think the
other part of my intuition and tell me if this is your experience is that we think of machine learning
as you know more complex than kind of the traditional way of doing things in some ways right in
other ways know but I guess I'm trying to get it like building a really robust rule based system
that can accommodate all of the corner cases and uncertainty in a noisy system it is hard and it's
like bespoke and I you know I don't have any data to this but my sense is that a lot of organizations
have plot a lot of time and money trying to handcraft this over and over again and this idea of
you know weekly system for weak supervision provides a you know a kind of an elegant you know
framework for a side stepping needing to make the rules piece bullet proof and still getting to
the ultimate outcome that you're trying to to get to your intuition is absolutely correct I mean
we've seen this time and time again where an organization will attempt to build the world's best
set of rules and it always turns out that it's not the world's best set of rules like it's never good
you know it's it's maybe good enough but like we as humans are like notoriously bad at being able
to articulate edge cases like it's it's if we can articulate it very easily it's no longer an
edge case you know it's something that we can write a rule for but that's where highly generalizable
like deep learning models come into play that's the whole reason why you would train a model out
of a weak supervision like process as opposed to just using the weak supervision process itself
as the model it's because of that added level of generalization generalizability you would train
a larger like you know deep learning model that has no concept of the rules of the heuristics that
went into creating the labels all it sees are the label and the input data and ideally then it learns
a very rich relationship between the input and and the projected output such that it you know
it's not going to reverse engineer the rules because it has no concept of how many rules it
took to create those labels it's going to learn something else and we hope that the thing it learns
is actually more generalizable than the human input rules that went into that to begin with.
Is there a an in-between kind of traditional rules and you know something more sophisticated
that's like rule functions you know functions or that you were thinking of when you made that
comment like how how far can you you know have you seen folks go before introducing the machine
learning model part beyond just kind of rules engine maybe that's a way to get at it.
Interestingly we actually have customers that are using watchful as their model in production
despite us telling them hey like you know you should probably consider training a model
and then using that model in production here here kind of like the properties to think about right
it really depends on your use case and this is sort of you know my my co-founder hates it when
I say this but it holds true it's like every data science problem the answer is it depends like
it doesn't matter what the problem is the answer is always it depends so the answer here is it
depends so if your data changes frequently you know from from time to time or if you find that
the variability that you captured in the samples not truly representative the variability that you
see in the total population of your data or if you need you know sort of like an additional layer
of nuance that you find very difficult to capture by way of heuristics you know where perhaps you
actually do want something that's pre-trained that you're then fine tuning and so on that's
we're having a deep learning model trained after the fact is so helpful you know it it provides
that additional level of granularity that additional level of generalizability that would be
notoriously difficult for a human to articulate in heuristics again these are these are functions
right these are functions that a human is writing in some capacity now there's obviously a lot of
like cleverness that you can add on top of this to make sure that these functions are expressive
and and very powerful and so on but at some point you start almost converging on like model
activations you know a sufficiently complex function is just like you know almost no different
from just like modeling a single activation in a model or something like that you know so
there's also a trade-off between explainability and power as we've seen in in AI in general but
you know obviously the same goes for weekly supervised systems it's as your functions become more
and more powerful as they become more and more general they also lose some degree of explainability
so again the answer is always it that it depends but here especially it depends because
in some use cases you can get away with just creating a couple of heuristics that are really
really good at labeling your data and all you really needed was that like additional layer of
fuzziness almost that that embrace of noise in these heuristics as opposed to rules right and
and that that might be good enough in other cases you might definitely rely on having some like
you know pre-trained quote-unquote common sense in your model that is then used to sort of like
boost overall like system performance so it kind of depends on your use case so we've had
customers go the entire you know 10 miles that they had to go with just pure watchful we've
had other customers trained very sophisticated models after the fact and get you know really really
good results so yeah again it depends you mentioned previously this idea machine teaching
can you elaborate on that is that kind of a you know an umbrella terminology that you're
using for all of the things that we've discussed from active learning to week supervision or
is it is it more or something different so machine teaching was actually a term coined by
Microsoft Microsoft research specifically and so the best way to frame it relative to data center
AI is that data center AI is very much a movement machine teaching is sort of a mentality coupled
with like a set of strategies or proposed strategies so it becomes slightly more concrete machine
teaching is a part of the data center AI movement it's just one layer of granularity deeper so the
idea behind machine teaching is kind of simple it's that classical machine learning research
has largely focused on building bigger and better machine learning models right it's like
focusing on the student if you want to call it that and the idea is that you want to create a
student that requires very little data to learn how to do the thing the problem is that is in
large part very difficult to predict you know machine learning invention true like machine learning
architecture sort of like innovation happens in almost like step wise motions so it's very
difficult to predict when the next innovation is going to happen on the other hand
the whole idea of machine teaching is if you could flip the coin and instead of focusing so much on
the student focus more on the teacher and make it so that the person training this model or teaching
it how to do the thing is several orders of magnitude more effective at doing that what does that
world look like you know where it doesn't matter what student you're training you know it could be
a very sophisticated deep learning model or it could be you know something more classical it shouldn't
really matter because the same tool sets should be applicable in both cases where we're focusing a
lot on making the teacher much more effective so techniques like weak supervision and active learning
go into this as well as things like you know transfer learning and and even like
like simulated data approaches and you know synthetic data rather like all of these things kind of
belong under that umbrella where it's a very strong focus on taking what's in the head of an
expert and applying that programmatically as much as possible to data which is then used to train
models and not requiring you know 40 doctors to sit in a room for a month labeling data
that's that's sort of the idea yeah I really appreciate the what I think I was the broadening
of the term machine teaching I know very well the folks said bonsai which was a company that was
acquired by Microsoft and they were kind of the spark of this machine teaching idea applied to
reinforcement learning and industrial AI we collaborated on an industrial AI ebook and
they were a long time client of mine and it's great to see that term resurface here in the context
of data and data center AI yeah we think it's criminally underused we like we we stumbled upon
the terminology in one of the research papers out of the Microsoft research lab after they got
acquired and it just really resonated with us like that is exactly what we're building that's
exactly what needs to be built for AI to make its way into most organizations so yeah absolutely
I mean we're we're really excited about all the work that's happening in this space and it's
it's becoming more like the concept of machine teaching is becoming more and more popular so
I'm glad to see it's it's you know slow vital resurgence how do you see this fitting into the overall
model development workflow when an organization you know has a problem you know they think machine
learning is the solution to this you know do they start with rules do they start with
do they need to think totally differently about the the way that they approach building models today
yeah as I mentioned before it's it's more of like a mentality shift so it's less about like what
technique they start about they start with it's more about matching the data to the architecture
and and here's here's what I mean by that especially these days like machine learning coding
for the most part is like kind of a solve problem you know I'm going to hand wave like a lot here
so bear with me uh it's like you know with with with the very frameworks that exist there's Keras
there's you know there's torch and and so I'm like uh there's several just like very nice
commoditized ways to build almost any neural architecture you can imagine and even more than that
they're like very nice architectures that you can just get off the shelf that will work for most
problems so a lot of the time organizations don't really have to innovate on machine learning code
itself uh so for all intents and purposes like a data centric AI perspective you can kind of hold
the code static and you can iterate on the data to get better yield and that's where the shift
and mentality comes you know it's yes I could have like an army of humans on standby and just give
them data and like if if the data I get back is not good I tweak the rules or tweak the description
just slightly to get you know perhaps better yield and so on but it's very much like again stepwise
motion and each time you do that you might have a lag of like two weeks before you get your data back
it's not a very efficient process so data centric AI is about not only focusing very carefully
on what data and how you're getting it labeled and that sort of thing but it's also about
the user experience or at least the experience around acquiring that labeled data how do I shorten
the period of time between hey I need data to okay I've got good data and that process could
you know utilize things like weak supervision depending on what your data looks like and that sort
of thing it could use things like active learning it could use things like synthetic data it really
depends on what you have at your disposal both from like an infrastructure and like machine learning
ops perspective as well as from like a data distribution perspective like does your data lend itself
to active learning particularly well does it lend itself to weak supervision particularly well does
it lend itself perhaps better to synthetic data so really this is just about like talking about the
data specifically as opposed to the model because good data can be used to train almost any model
and again lots of hand waving here depending on the model architecture you might need more data
you might need less data you know it depends on so many variables but the idea is that for the
most part the hard thing about machine learning and the enterprise today is no longer the actual
modeling component and it's actually on the data and it's just about thinking holistically about
that now and do you find that that's the case uh you know hand waving notwithstanding kind of
across the the most valuable of an organization's problems you know I've you know talked about this
idea of model driven enterprise and like it's the anti commoditization of machine learning like yeah
you can you know use machine learning in your organization and get some advantage if it's
you know just built into all the stuff that you use but it's not really your models you know
solving your problems for your data that's where the real values going to be uh is is that in line
with what you're saying or yeah it's it's absolutely in line I don't know that this is a hot
take anymore but I I firmly believe that in the next 10 years every company will have some sort
of AI functionality internally um it's just an inevitability and the question ask is like
why hasn't that happened yet you know like modeling techniques have again become largely
commoditized already like why haven't we been able to bring AI into the fold in a meaningful way
across all enterprises so that's sort of where our conviction is around the data piece it's that
the hard part is not actually on the model architecture or the code aspect of this if you think
about AI is just like code plus data it's exactly as you said it's like my hardest problems are around
like is one part of the business where I got like one subject matter expert who knows how to perform
this particular task no one else knows how to perform it except for this person I want to build a
model to augment that person but very quickly I end up in a catch 22 where I would need that person
to be in the process of labeling that data for that model but they're already a critical bottleneck
bottleneck to my business so I can't have them sit in a room for like a month labeling data I
really do need them to be productive on top of labeling data and that it just becomes very very
hard to build a system that will actually help there so that's why a lot of the time if you do
see enterprises kind of like trying AI they're usually trying it for use cases that are not the most
needle moving you know that's why like the typical hello world for machine learning is like a
Twitter sentiment analysis bot it's because it's like the most accessible data and it's not that
the architecture is particularly simple a lot of the time the sentiment analysis bots like
they use off-the-shelf architectures that are fine you know a lot of it is just around like the
mechanics of acquiring the data cleaning it like going through process of training the model
iterating on it and so on it's just availability of data so our conviction is that if we can make it
dead simple for your expert to label your data in your environment as quickly as humanly
possible without requiring them to like take time away from the things that they're already a bottleneck
day-to-day for then we can price organizations in to actually using AI for solving their hardest
problems that's that's kind of like the laser focus we have you know maybe an insight you know
out of this question in your responses that model is kind of an overloaded term and
you can use an off-the-shelf model architecture off-the-shelf code but the thing that makes you know
after you apply your kind of proprietary data to those things you have a proprietary high-value
model at the you know coming out of the process that may be based on a commodity architecture
yeah that that's exactly right generally speaking again like I'm I'm very excited about like
these massive foundation models that are being built and I still think that we need more of these
foundation models across other data modalities and other tasks so I am certainly not pushing against
the idea of these foundation models that use lots of generic label data to produce you know
generally available good insight the hard part is marrying that to specific use cases and a lot
of the time you can use a GPT-3 for like very interesting use cases but in order to match that
with enterprise value you have to marry it to data or a use case that is unique to that enterprise
most of the time and that's where like that whole data centric movement becomes very interesting
because then you can marry the foundation model with very specific very curated data that you
can then combine to fine tune a model that again becomes proprietary it could be based on this
like massive architecture that you kind of like repurpose from somewhere else but the piece that
makes it yours is the fact that it was trained on your data for your task by your people what what
do organizations need to be successful in making this mindset shift I think like the big thing is
just like this is kind of again going to be hand wave but just like being open to it that's like
like the biggest hurdle it's like a lot of the time folks get entrenched in the way things have
been done for a very long time and it's very difficult to break out of it one of the things that we
find like very common is the idea that hand labeled data is ground truth um and and this is sort of
like an interesting idea where like it's it's just not the case in the real world you know a lot
of the time people have uh subjectivity that comes to play or or they have biases that manifest in
their labeled data and and try as hard as you might it's impossible to get a 100% accurate and
fair data set labeled by an army of humans uh so that's like one of the biggest hurdles that
we've had to face in like communicating what we do to the rest of the market it's getting people
to acknowledge that the way they've gone about doing things so far has been leveraging
words that they don't actually mean do you go as far as you know trying to convince people to stop
using the term ground truth when yes yeah it's not absolutely ground truth it's just their labeled
data yeah that's exactly right I mean like we we we try and educate them on life hey you're like
hey this actually means something and it's not what you're what you're thinking yeah words matter
you know like like you can't go around saying that your data is ground truth when it's like
clearly riddled with bias and like incorrect labels and assumptions like we've seen this time and
time again so first we educate them that like look hand labels themselves are noisy in some
unknowable capacity they are by nature weak you know perhaps not as weak as functions but they
are still weak in some way and now the question is how weak right how much can we trust the hand
labels that you already have like how do we leverage them in in a more robust system how do we
combine them with other sources of signal to then boost or reduce the signal introduced overall
those become very very interesting questions and part of our like overall conviction is that
you know as we move towards a more mature nl ops workflow overall you know data centric AI
notwithstanding right if we're talking about just like the maturity of machine learning as a whole
from the enterprise explainable AI has been a very very important thing for several years now
but the interesting thing to us is that a lot of the techniques there are all around explaining
your model and like why your model is making certain decisions and how they could play out
but the reason why your model is making a bias decision is because it was trained on bias data
right like right especially if we're holding yeah potential exceptions to that naturally you know
I saw a good one where someone was using distilbert to predict sentiment on certain phrases so it
was like if a movie was like the term was like this movie was produced in India that would have
a very high sentiment then there's another where it was like this movie was produced in Germany
and that had a very low sentiment and the reason why that happened there's several reasons
obviously distilbert was trained on like internet data there's obviously imbalance and in that
data because it's probably overfitting on like this pre-trained notion of like World War 2 and
you know several mentions of Germany that are like not very favorable but even more interestingly
the model architecture is such that like on the sentiment analysis side there's no notion of
neutrality it's either positive or negative so the model architecture actually ended up biasing
the entire outcome because there's like those are inherently neutral statements but it was forced
to either make a positive or negative assumption that's such a clear example of this thing that
we've been fighting about on Twitter for years like that bias is only in the data and not in the
models no it's everywhere it's everywhere you can't escape it it's it's it's literally everywhere
and but you just have to be smart about like where you're limiting the possible sources of bias
you know there's human-generated bias both in the way of data as well as in the way of model
architectures there's bias in the way labels or predictions are used you know downstream there's
so many different ways that you could sort of like capture bias unintentionally in a system but
as part of that like we should be building systems to find where this bias might be being introduced
you know we should we should be building more robust systems exactly all the way up and all the way
down the stack so it's part of this like because we touch the data a lot our notion is that you
should be able to explain that data you should be able to explain the data that goes into your
models otherwise how are you going to reason about it so like again the point I'm trying to make
is that you just have to be like open to a lot of these things you have to be willing to shift
your mentality and willing to shift again as a side effect your techniques in producing some of
these models and then naturally it also helps if you've taken the first couple of steps in your
machine learning journey if you know what status quo looks like if you know why this is different
if you know how this plays into the rest of your anal ops stack that's helpful but not required
I think the biggest requirement is very much just like a willingness to have your mind changed
you talked about the noisiness of hand label data and you know to some degree we've
recognized that and sophisticated organizations have you know employed a number of strategies
to try to mitigate that you know quorum among laborers and managing labor labor labor
quality and all this kind of stuff is it data centric AI or some of the things that we've
been talking about under that banner week supervision etc is that a substitute for all that do you
need kind of these you know advanced labeling strategies less if you're employing some of the
things that we've been talking about our argument is yes however that's not necessarily the argument
shared across the board yeah it depends exactly so so here's a deal like you've got you've got
things like cap of values right so like measuring how good of a label or someone is and
and that sort of thing you've got inter annotator disagreement as a side effect of that you've got
as you mentioned quorum between hand labelers you've got all these different things that play in
I think they're ways to do that correctly I just haven't seen them yet so for instance quorum
amongst hand labelers like let's say you have like seven out of 10 people say that this thing is
toxic and three people say that it's not toxic like it's a tweet or something like that like
my conviction is that every single one of those people was correct in some capacity right there's
some amount of subjectivity right like what what exactly is toxicity toxicity is a thing that
is very difficult to actually capture the nuance of especially if you're trying to write up a
description so you have to acknowledge that there's like cascading systems that go into these labels
like you have to have the world's most perfect rules that describe how to label a thing in order to
get inherent consistency in the labels that come out the other end and we're human you know we're
not going to write the world's most perfect descriptions it's just not going to be possible we're
not going to write the world's most perfect criteria for labeling it's just not possible
so this kind of speaks to you know we talked about bias and data and how there's also bias and
models you know here we're pointing specific to there's also kind of inherent bias in
labeling systems and labeling tools and the way you're you know labeling instructions
exactly I mean it's it's all over the place right like bias can be introduced in a multitude of
places so you have to be like very cognizant about that and I think that like trying to articulate
whether someone is a good or bad labeler is fundamentally the wrong approach the reason for
this is because you might be trying to categorize them overall as a bad labeler but they might be
very very good at labeling certain parts of the data because of their context because of their
expertise but very bad at labeling other parts of the data and I think it's more accurate to
indicate whether the expertise they're bringing to the table in particular segments of the data
is actually valuable or not so that's sort of like point number one point number two is I believe
that these labels should capture the rich context behind what you know sort of the one or zero
output is so for instance in the earlier case if seven out of ten people said this thing is
toxic I'd want to know that I'd want my model to acknowledge the nuance that went into this so that
it can actually learn a richer relationship between the input space and the output value so
that is actually very very important and again that's why I'm saying that I think that there
there are ways that you could build a system that does a lot of this stuff but I haven't seen it yet
and so like we take kind of like just a different approach to it our system is not a hand labeling
system you know we we have hand labeling as part of our process but a we acknowledge that there's
no such thing as ground truth and b all of our labels are inherently probabilistic by nature
so you can have probabilities that are 100% or 0% so you can have extreme confidence on either side
but you can also have confidence in the middle you know like you can have that 70% likelihood
probability or or or something else right but that allows for 101 degrees of freedom in your
in your label space which then hopefully yields a model that will learn a richer relationship
between the input and the output so it's a long way of saying yeah I think you could you know
build better hand labeling systems I just haven't seen it yet you referred to
data centric AI and and the the kind of stuff we're talking about is part of the ml ops stack
historically and in a lot of ways there's been kind of a separation between all of the data stuff
the data stack and even you know data ops and data prep and all the stuff and kind of ml ops
you kind of it does data centric AI kind of bring those together yeah it does again the whole
concept here is that in today's world as it relates to AI you can broadly consider the code
to be like held static and again AI is sort of like code plus data and if you're holding the
code static then the thing you're iterating on is data which means that by nature if you want
that to go back through an ml ops system it has to be connected in some way so the argument here
is data should be a part of the stack in some meaningful way perhaps not the entire like process
of procuring the data and so on like you know maybe some of that is a little bit more
bespoke or you know a little less framework but really the idea is that AI is code plus data
and we have very very robust ways of managing that code due to decades of innovation on the
software engineering side and we haven't seen quite the same thing in an ml concept or context
on the data side and that's I think the part that data centric AI really aims to help
sort of like push it's the idea that data really should be a part of the ml ops stack
and do you find that these ideas apply equally well across media type modality text audio
video yeah yeah absolutely um the obviously the only real place where it starts breaking down
is the moment you start leaving like supervised machine learning because then it you might not
need data right which is totally reasonable but but broadly speaking when we're talking about
machine learning the enterprise we are generally talking about some sort of supervised system
and some capacity and yeah we have seen that this applies like roughly the same across every
modality across every task now devil's always in the details right like managing things like
like 3d point cloud data is very different to managing things like you know just single
dimensional text or something like that like um the way you go about interfacing with this data
and the way you go about applying your subject matter expertise to it will differ like quite a lot
from from modality modality um now like a whole purpose to be for for my company is finding the
common threads across these modalities and across these these tasks such that you can reuse the
same skill set the same workflow no matter what data you're bringing to the table and no matter
what task you're bringing to the table not all things will be similar not all things will be the
same but a lot of them will at least the core will and that's kind of like where we focus almost
all of our effort it's it's finding that common ground well cyan has been wonderful having you on
the show and chatting about your take on data centric AI lots of great stuff in here um thank you
thank you so much Sam this is a lot of fun I really appreciate it
