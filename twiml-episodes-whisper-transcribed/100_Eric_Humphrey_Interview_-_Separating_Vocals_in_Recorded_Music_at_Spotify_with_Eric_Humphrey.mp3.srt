1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:24,040
I'm your host Sam Charrington.

4
00:00:24,040 --> 00:00:29,760
Last week I spent some time at CES, the consumer electronics show in Las Vegas, exploring

5
00:00:29,760 --> 00:00:36,360
the vast sea of drones, cameras, paper thin TVs, robots, laundry folding closets and other

6
00:00:36,360 --> 00:00:37,760
smart devices.

7
00:00:37,760 --> 00:00:38,840
You name it?

8
00:00:38,840 --> 00:00:40,160
It was there.

9
00:00:40,160 --> 00:00:44,920
Of course, I was also able to sit down with some really interesting folks working on some

10
00:00:44,920 --> 00:00:48,360
pretty cool AI-enabled products.

11
00:00:48,360 --> 00:00:52,400
Head on over to our YouTube channel to check out some behind-the-scenes footage from my

12
00:00:52,400 --> 00:00:55,680
interviews and other quick takes from the show.

13
00:00:55,680 --> 00:01:01,960
And beyond the look up for our AI and consumer electronics series right here on the podcast,

14
00:01:01,960 --> 00:01:03,200
coming soon.

15
00:01:03,200 --> 00:01:08,280
The show you're about to hear is part of a series of shows recorded at the rework Deep Learning

16
00:01:08,280 --> 00:01:11,760
Summit in Montreal back in October.

17
00:01:11,760 --> 00:01:17,760
This was a great event and in fact their next event, the Deep Learning Summit San Francisco,

18
00:01:17,760 --> 00:01:23,200
is right around the corner on January 25th and 26th and will feature more leading researchers

19
00:01:23,200 --> 00:01:28,320
and technologists like the ones you'll hear on the show this week, including Ian Goodfellow

20
00:01:28,320 --> 00:01:33,920
of Google Brain and Daphne Kohler of Calico Labs and more.

21
00:01:33,920 --> 00:01:40,440
Definitely check out the event and use the code TwimmelAI for 20% off of registration.

22
00:01:40,440 --> 00:01:45,920
In today's show, I sit down with Eric Humphrey, research scientist in the Music Understanding

23
00:01:45,920 --> 00:01:48,000
Group at Spotify.

24
00:01:48,000 --> 00:01:52,400
Eric was at the Deep Learning Summit to give a talk on advances in deep architectures

25
00:01:52,400 --> 00:01:56,520
and methods for separating vocals and recorded music.

26
00:01:56,520 --> 00:02:02,320
We discuss this talk, including how Spotify's large music catalog enables such an experiment

27
00:02:02,320 --> 00:02:07,720
to even take place, the methods they use to train algorithms to isolate and remove vocals

28
00:02:07,720 --> 00:02:13,400
from music, and how neural network architectures like UNET and Pix2Pix come in the play.

29
00:02:13,400 --> 00:02:18,280
We also hit on the idea of creative AI in general, Spotify's attempts at understanding

30
00:02:18,280 --> 00:02:23,840
music content at scale, optical music recognition, and much more.

31
00:02:23,840 --> 00:02:26,400
And now on to the show.

32
00:02:26,400 --> 00:02:36,600
All right, Eric, so why don't you tell us a little bit about your background and how

33
00:02:36,600 --> 00:02:38,840
you got interested in machine learning and AI?

34
00:02:38,840 --> 00:02:39,840
Sure.

35
00:02:39,840 --> 00:02:42,120
I guess it all started in high school.

36
00:02:42,120 --> 00:02:44,720
I always played music.

37
00:02:44,720 --> 00:02:46,680
My dad was an engineer.

38
00:02:46,680 --> 00:02:50,280
And the compromise of that when I was going into college was electroengineering.

39
00:02:50,280 --> 00:02:52,000
I really wanted to make guitar pedals.

40
00:02:52,000 --> 00:02:56,600
I wanted to learn about how amplifiers worked, and then that kind of got into more signal

41
00:02:56,600 --> 00:03:02,040
processing and algorithms, which led to a lot of parameter tuning by hand.

42
00:03:02,040 --> 00:03:06,000
If you want a delay pedal, you know, you have all these different knobs and whatnot.

43
00:03:06,000 --> 00:03:10,040
And I started doing some more stuff around beat tracking and tempo tracking for running

44
00:03:10,040 --> 00:03:11,040
to music.

45
00:03:11,040 --> 00:03:14,880
And it was like, I spent so much time designing these algorithms by hand.

46
00:03:14,880 --> 00:03:19,040
It's like a lot of the tradition for a lot of digital signal processing things.

47
00:03:19,040 --> 00:03:21,720
And that was right around the time that machine learning really started to pick up.

48
00:03:21,720 --> 00:03:25,600
And it's like collecting data for these things and then training an algorithm.

49
00:03:25,600 --> 00:03:27,920
It just simplified the process so much.

50
00:03:27,920 --> 00:03:30,200
And like, well, this is a no-brainer.

51
00:03:30,200 --> 00:03:34,240
I can have the system that I want to do all these cool things, but leveraging these kind

52
00:03:34,240 --> 00:03:39,120
of like elegant data-driven solutions that freed me up to go running and play music and

53
00:03:39,120 --> 00:03:42,160
do all these other things and just, you know, have these systems work.

54
00:03:42,160 --> 00:03:43,720
And this is all why you're in school?

55
00:03:43,720 --> 00:03:49,120
Yeah, so I did a master's down at University Miami in the really great music tech program

56
00:03:49,120 --> 00:03:52,800
down there and got to do a little bit more with running music and worked with the music

57
00:03:52,800 --> 00:03:58,640
therapy department and then parlayed that into a PhD at New York University, where I got

58
00:03:58,640 --> 00:04:04,440
to work with Juan Pablo Beo and Jan Lecun a little bit, which really kind of pivoted it

59
00:04:04,440 --> 00:04:05,440
even further.

60
00:04:05,440 --> 00:04:06,440
Okay.

61
00:04:06,440 --> 00:04:10,080
So that was really when it was, I took a pretty formational machine learning class with

62
00:04:10,080 --> 00:04:14,360
Jan, my first semester at NYU, and kind of put me off on that.

63
00:04:14,360 --> 00:04:15,360
That's the way to get started.

64
00:04:15,360 --> 00:04:16,360
Yeah.

65
00:04:16,360 --> 00:04:17,360
Yeah.

66
00:04:17,360 --> 00:04:18,360
It was pretty, pretty certain dipitus.

67
00:04:18,360 --> 00:04:21,760
I didn't fully appreciate what I was getting myself into at the time.

68
00:04:21,760 --> 00:04:24,120
It was right around 2009-2010.

69
00:04:24,120 --> 00:04:28,800
But for me, it was always much more about computer visions, doing a lot of things with images

70
00:04:28,800 --> 00:04:29,800
and audio.

71
00:04:29,800 --> 00:04:35,400
It had some stuff around speech recognition, but music was always kind of, you know, lagging

72
00:04:35,400 --> 00:04:36,400
behind.

73
00:04:36,400 --> 00:04:42,760
So for me, I always wanted to say, you know, to be similar to computer vision or ASR, recognize

74
00:04:42,760 --> 00:04:43,760
chords and music.

75
00:04:43,760 --> 00:04:44,760
I'm like a tourist.

76
00:04:44,760 --> 00:04:46,520
Can you show me how to play this song automatically?

77
00:04:46,520 --> 00:04:49,720
Can you show me where the beats, the bars, the chords are?

78
00:04:49,720 --> 00:04:54,160
Can I have a playlist of just choruses, these kinds of things?

79
00:04:54,160 --> 00:04:59,440
So when you really start to think about the opportunities around music, leveraging things

80
00:04:59,440 --> 00:05:05,000
like deep learning and machine learning, you can, you know, the imagination can run wild,

81
00:05:05,000 --> 00:05:06,840
the kinds of things you can do with that.

82
00:05:06,840 --> 00:05:07,840
Awesome.

83
00:05:07,840 --> 00:05:08,840
Yeah.

84
00:05:08,840 --> 00:05:11,120
Tell us a little bit about what you're up to nowadays at Spotify.

85
00:05:11,120 --> 00:05:12,840
Did you go to Spotify right after your grad school?

86
00:05:12,840 --> 00:05:19,160
No, I spent some time at a small music ed tech startup that was trying to do things around

87
00:05:19,160 --> 00:05:20,160
optical music recognition.

88
00:05:20,160 --> 00:05:24,880
So in the same way that you could do document scanning, one of the core pieces of technology

89
00:05:24,880 --> 00:05:29,680
that we were working on was if you took a picture of sheet music, could you turn it into

90
00:05:29,680 --> 00:05:35,800
MIDI so that kids could learn how to play any piece of music at their disposal and then

91
00:05:35,800 --> 00:05:39,720
be able to, you know, on the longer arc gives them feedback about how they're doing and

92
00:05:39,720 --> 00:05:40,840
kind of taking it from there.

93
00:05:40,840 --> 00:05:43,360
It seems like that should be fairly straightforward.

94
00:05:43,360 --> 00:05:48,960
So you'd think that one of the really, really interesting things about music in all of its

95
00:05:48,960 --> 00:05:53,800
forms, and I'll mention this tomorrow is that it's so fundamentally intelligent that,

96
00:05:53,800 --> 00:05:59,600
you know, when you have even just a piece of music for a single voice, monophonic instrument,

97
00:05:59,600 --> 00:06:01,480
you don't have polyphony or these other things.

98
00:06:01,480 --> 00:06:06,440
You actually have, instead of OCR for being generally one-dimensional, it's linear and

99
00:06:06,440 --> 00:06:08,800
you'll span a vertical axis.

100
00:06:08,800 --> 00:06:16,280
But music, you actually have a two-dimensional grid that moves linearly with these really

101
00:06:16,280 --> 00:06:19,640
complicated links backward and forward.

102
00:06:19,640 --> 00:06:24,440
So if you have DSL signals or quotas or repetitions or multiple endings, it turns into death

103
00:06:24,440 --> 00:06:29,000
by a million paper cuts because one of the things you'll find is triplets will be

104
00:06:29,000 --> 00:06:33,480
notated in the first measure and then known after because it's cheaper that way to not

105
00:06:33,480 --> 00:06:36,680
print these additional threes on all these things.

106
00:06:36,680 --> 00:06:39,120
But there, any musician would kind of know that it's there.

107
00:06:39,120 --> 00:06:41,880
Any intelligent musician would know that I was there, got it.

108
00:06:41,880 --> 00:06:46,560
Or you'll run into these other really interesting common cases, there aren't even educations

109
00:06:46,560 --> 00:06:51,040
where for children's music, they won't notate rests because they're trying to simplify

110
00:06:51,040 --> 00:06:52,200
the musical surface.

111
00:06:52,200 --> 00:06:53,200
Okay.

112
00:06:53,200 --> 00:06:57,320
So it's in four, but you'll have these two notes here and then those two notes and the

113
00:06:57,320 --> 00:07:01,080
upper staff and a machine is like, I guess they're the same.

114
00:07:01,080 --> 00:07:07,920
So having all of these and then it doesn't have a ton of data for supervised training and

115
00:07:07,920 --> 00:07:12,480
it creates this really, really interesting, challenging, but fundamentally intelligent

116
00:07:12,480 --> 00:07:13,480
problem.

117
00:07:13,480 --> 00:07:14,480
Interesting.

118
00:07:14,480 --> 00:07:15,480
Yeah.

119
00:07:15,480 --> 00:07:20,120
It's one of those things that, you know, to crack from a very like human interest level,

120
00:07:20,120 --> 00:07:25,000
there's a ton of music that just hasn't been brought into the 21st century yet, you know.

121
00:07:25,000 --> 00:07:30,360
Something that was notated from Gregorian Chants all the way up to kind of now where we've

122
00:07:30,360 --> 00:07:36,560
started to shift away from notated music to more recorded music or digital audio workstations,

123
00:07:36,560 --> 00:07:41,800
like Ableton style project files where there isn't really an artifact of the music except

124
00:07:41,800 --> 00:07:42,800
the recording.

125
00:07:42,800 --> 00:07:48,280
There's all this older stuff that could be brought into the future for creative purposes,

126
00:07:48,280 --> 00:07:52,480
musicology and kind of more anthropological consideration.

127
00:07:52,480 --> 00:07:53,480
Okay.

128
00:07:53,480 --> 00:07:54,480
Interesting.

129
00:07:54,480 --> 00:07:58,840
So I did an interview with the dog at Google Brain Project Magenta.

130
00:07:58,840 --> 00:08:04,360
He had an interesting presentation at another conference a few months ago that talked about

131
00:08:04,360 --> 00:08:09,160
even the step beyond what you're just describing, like once you have this sheet music or you

132
00:08:09,160 --> 00:08:15,200
can describe the music to the computer accurately, how do you then get it to play expressively?

133
00:08:15,200 --> 00:08:16,800
And they've been doing some interesting things there.

134
00:08:16,800 --> 00:08:19,760
I don't know if you're familiar with it, but it's interesting stuff.

135
00:08:19,760 --> 00:08:27,560
Yeah, I mean, expressivity and music creation and composition, it's so interesting to really

136
00:08:27,560 --> 00:08:31,000
dig into because I think it cuts to the core of humanity.

137
00:08:31,000 --> 00:08:37,520
There's been so much amazing work around game playing and AI recently, AlphaGo, Atari,

138
00:08:37,520 --> 00:08:40,400
but you have these well-defined objective functions, right?

139
00:08:40,400 --> 00:08:44,720
Make the score high, win the game, that so you have these extrinsic motivators that fit

140
00:08:44,720 --> 00:08:47,800
pretty well into a reinforcement learning formulation.

141
00:08:47,800 --> 00:08:51,800
And music, the most interesting things are internal, they're intrinsic.

142
00:08:51,800 --> 00:08:55,280
It's the novelty and the surprise of, I didn't see that chord coming.

143
00:08:55,280 --> 00:08:59,480
You know, when you're sitting in your room when Hendrix was really just digging in and just

144
00:08:59,480 --> 00:09:03,920
sinking his teeth in deep to a solo is for him, this sort of is hard.

145
00:09:03,920 --> 00:09:08,640
And you could think about having these feedback loops for listens on Spotify or revenue

146
00:09:08,640 --> 00:09:14,520
generated, but these aren't really what cuts to the heart of creativity and expression.

147
00:09:14,520 --> 00:09:16,440
Like, what are you, what are you getting after?

148
00:09:16,440 --> 00:09:20,640
And I think that's going to be one of the really interesting challenges as we start to move

149
00:09:20,640 --> 00:09:28,320
into that next stage of kind of like really autonomous or, you know, things that are self-directed

150
00:09:28,320 --> 00:09:29,880
in the AI space.

151
00:09:29,880 --> 00:09:30,880
Why is it doing it?

152
00:09:30,880 --> 00:09:31,880
What motivates it?

153
00:09:31,880 --> 00:09:33,160
Does it have this notion of self?

154
00:09:33,160 --> 00:09:39,200
So when you think about elements of, I think music, humor, sarcasm, these kinds of things

155
00:09:39,200 --> 00:09:44,760
kind of start to encroach on that in a way that a lot of the recent history of machine

156
00:09:44,760 --> 00:09:46,600
learning hasn't gotten to yet.

157
00:09:46,600 --> 00:09:47,600
Interesting.

158
00:09:47,600 --> 00:09:51,800
Interesting that you put sarcasm in that bucket as a New Yorker living in the Midwest.

159
00:09:51,800 --> 00:09:56,920
I find that sarcasm is underappreciated in a lot of places and I would love to dig deep

160
00:09:56,920 --> 00:09:59,440
into AI and sarcasm.

161
00:09:59,440 --> 00:10:05,520
I think it's an, I think it's a East Coast Northeast kind of thing and I joke off and back,

162
00:10:05,520 --> 00:10:11,360
nothing to back this up that it's probably related to just, you know, local climates and

163
00:10:11,360 --> 00:10:15,680
it's a way to kind of deal with it, a great deal, great weather we're having, right?

164
00:10:15,680 --> 00:10:20,360
And it's just gray snow and there's sludge and all the street corners are backed up and

165
00:10:20,360 --> 00:10:23,440
whatnot, but you don't have that same thing on the West Coast where it's beautiful every

166
00:10:23,440 --> 00:10:24,440
day.

167
00:10:24,440 --> 00:10:29,320
sarcasm doesn't land in the same way with native Californians.

168
00:10:29,320 --> 00:10:30,320
So true.

169
00:10:30,320 --> 00:10:34,040
So at Spotify, you worked, what aspect of this problem are you working on there?

170
00:10:34,040 --> 00:10:35,040
Sure.

171
00:10:35,040 --> 00:10:39,640
So I think at a higher level to the extent that I can kind of delve into, I work on a team

172
00:10:39,640 --> 00:10:47,040
of researchers where we are building algorithms that can understand music content at scale.

173
00:10:47,040 --> 00:10:52,280
So some of the obvious applications would be to fit into, can we better understand users?

174
00:10:52,280 --> 00:10:54,480
Can we help provide better recommendations?

175
00:10:54,480 --> 00:11:01,280
A lot of recommender systems right now have gotten very far by looking at how users,

176
00:11:01,280 --> 00:11:07,360
consumers, listeners interact with content, whether it's purchased at the same time or

177
00:11:07,360 --> 00:11:11,880
in the same catalog, or they've been grouped into the same basket or playlist or things

178
00:11:11,880 --> 00:11:12,880
like that.

179
00:11:12,880 --> 00:11:15,680
You can get really far without having to look inside the box.

180
00:11:15,680 --> 00:11:22,000
So the metaphor I like to make is algorithms for content work as well as they do for say

181
00:11:22,000 --> 00:11:26,440
Amazon when you can kind of just, you don't have to look inside the box, but when you really

182
00:11:26,440 --> 00:11:31,280
want to say more deeply understand a user and what they're after, it's like what color

183
00:11:31,280 --> 00:11:32,280
is the shoe?

184
00:11:32,280 --> 00:11:33,280
Is it felt?

185
00:11:33,280 --> 00:11:34,280
Is it vinyl?

186
00:11:34,280 --> 00:11:40,720
Is it interact with music, do they really gravitate toward an artist or is it lyrical content?

187
00:11:40,720 --> 00:11:42,760
Is it about the harmonic content?

188
00:11:42,760 --> 00:11:47,880
Because one of the things that's really interesting about music in particular is your ability

189
00:11:47,880 --> 00:11:53,520
to enjoy it is tightly coupled to how well you can understand it and to what extent you're

190
00:11:53,520 --> 00:11:54,960
surprised by it.

191
00:11:54,960 --> 00:12:02,080
So for example, there's a one artist called Marsbo and for the untrained listener, it's

192
00:12:02,080 --> 00:12:06,400
going to sound a little bit like noise art, but if you go to a Marsbo concert, there will

193
00:12:06,400 --> 00:12:10,000
be people that are just totally rocking out and they're in it and they get it because

194
00:12:10,000 --> 00:12:13,840
they have a model and an understanding for what's going on.

195
00:12:13,840 --> 00:12:18,880
So you can use how people interact with content as a proxy for what they understand and what

196
00:12:18,880 --> 00:12:20,320
they can relate to.

197
00:12:20,320 --> 00:12:22,680
How explicit is that understanding for them though?

198
00:12:22,680 --> 00:12:27,480
A lot of people can't, well, I can't speak for Marsbo fans.

199
00:12:27,480 --> 00:12:32,480
But one of the really interesting things about music is that a lot of ways that we describe

200
00:12:32,480 --> 00:12:37,400
it are so personal and occasionally they're cultural or they're niche.

201
00:12:37,400 --> 00:12:42,560
So you'll find that in certain subgenres, certain words are used certain ways and you'll

202
00:12:42,560 --> 00:12:43,560
see things pop up.

203
00:12:43,560 --> 00:12:49,120
Oh, this playlist is on fleek and it's like you'll find that some of those language and

204
00:12:49,120 --> 00:12:53,560
the semantics, there are course intermediaries to describe the thing you actually mean.

205
00:12:53,560 --> 00:12:57,960
You're like, I love the part of this song where it really just makes my heart pump.

206
00:12:57,960 --> 00:12:59,280
You're like, well, what is it?

207
00:12:59,280 --> 00:13:00,280
I have no idea.

208
00:13:00,280 --> 00:13:01,840
I have no language to describe these things.

209
00:13:01,840 --> 00:13:06,680
Which makes education and visualization and interaction with the content, a really

210
00:13:06,680 --> 00:13:11,080
interesting area for some of this content understanding at scale.

211
00:13:11,080 --> 00:13:12,080
Botify.

212
00:13:12,080 --> 00:13:13,080
Why do I like this song?

213
00:13:13,080 --> 00:13:16,840
Are there other songs out there that you don't know about that would make you feel

214
00:13:16,840 --> 00:13:19,960
have such a strong physiological reaction in a similar way?

215
00:13:19,960 --> 00:13:21,920
Am I going to be able to ask botify that?

216
00:13:21,920 --> 00:13:22,920
Why do I like this song?

217
00:13:22,920 --> 00:13:23,920
Wouldn't that be a great question?

218
00:13:23,920 --> 00:13:24,920
That would be an answer.

219
00:13:24,920 --> 00:13:25,920
That would be awesome.

220
00:13:25,920 --> 00:13:31,360
I mean, especially because as I hear you describe this, I am not a music kind of sore by

221
00:13:31,360 --> 00:13:38,160
any stretch of the imagination, but it resonates for me that a lot of that is not really having.

222
00:13:38,160 --> 00:13:42,680
I guess when we talked about understanding right there's, you know, the impact that music

223
00:13:42,680 --> 00:13:46,320
has on you and kind of its ability to move you.

224
00:13:46,320 --> 00:13:53,400
And then your ability to understand it moving you and how and why and when.

225
00:13:53,400 --> 00:13:56,800
And then the next level is like your ability to articulate all that.

226
00:13:56,800 --> 00:14:01,600
And I feel like out of for me, if I was able to maybe come at it from the back and be

227
00:14:01,600 --> 00:14:05,480
able to articulate and understand kind of these things at a conceptual level, that might

228
00:14:05,480 --> 00:14:08,760
help me to connect to other types of music.

229
00:14:08,760 --> 00:14:13,040
And it would be really cool if I could ask if Spotify could basically teach me this.

230
00:14:13,040 --> 00:14:17,920
And I would actually take to the logical conclusion because beyond that, then you could say all

231
00:14:17,920 --> 00:14:20,640
this music was composed by another person.

232
00:14:20,640 --> 00:14:25,880
So generally when you're composing as a composer, you're pulling upon all of your experience

233
00:14:25,880 --> 00:14:29,760
and your own surprise and novelty, but that's going to relate and land with an audience

234
00:14:29,760 --> 00:14:31,120
in a very particular way.

235
00:14:31,120 --> 00:14:36,360
So when you think about an artist composing for their fan base or in a genre or a style

236
00:14:36,360 --> 00:14:41,240
or trying to achieve a certain outcome, you know, you might say, you know, I really want

237
00:14:41,240 --> 00:14:46,120
to drop, I want to a verse that's minor so we can step into a major chorus because people

238
00:14:46,120 --> 00:14:49,560
will feel that as this release intention.

239
00:14:49,560 --> 00:14:53,880
But the only way that that can be conveyed appropriately is if everyone has that similar

240
00:14:53,880 --> 00:14:55,280
expectation.

241
00:14:55,280 --> 00:14:59,760
So you're playing off of certain kind of musical behaviors that are in culture in certain

242
00:14:59,760 --> 00:15:06,160
ways, which also makes music at the level of like a global culture really interesting.

243
00:15:06,160 --> 00:15:10,280
Or micro culture as you start to be able to connect the dots across really neat genres

244
00:15:10,280 --> 00:15:15,000
that, you know, didn't have any bandwidth in more of a mainstream music era.

245
00:15:15,000 --> 00:15:17,200
And are you a musician personally?

246
00:15:17,200 --> 00:15:18,720
Do you play what do you play?

247
00:15:18,720 --> 00:15:20,640
I play everything I can get my hands on.

248
00:15:20,640 --> 00:15:26,080
I grew up playing saxophone for about a decade, switched over to guitar and then guitar

249
00:15:26,080 --> 00:15:27,080
and voice.

250
00:15:27,080 --> 00:15:29,320
And I've been learning drums for the last couple of years.

251
00:15:29,320 --> 00:15:33,440
And anyway, that I can kind of express myself with sound is a good time.

252
00:15:33,440 --> 00:15:38,200
And when you're expressing yourself with sound, do you think about it in the way that you

253
00:15:38,200 --> 00:15:43,000
previously described, like, I'm going to try and hit this chorus and I don't even have

254
00:15:43,000 --> 00:15:46,440
my, I can't even, my ability with the words is so poor.

255
00:15:46,440 --> 00:15:49,160
I can't even repeat what you just said.

256
00:15:49,160 --> 00:15:53,400
But I get at least the impression that I have from, you know, pop to immediate TV, whatever

257
00:15:53,400 --> 00:15:57,720
is it, you know, they just go into a room and then music comes out and not like, oh,

258
00:15:57,720 --> 00:16:00,600
I'm going to nail them with this crescendo right here.

259
00:16:00,600 --> 00:16:02,840
I think everyone's process is different.

260
00:16:02,840 --> 00:16:07,800
And I mean, as my PhD was in a music program, so I had to take some graduate level theory

261
00:16:07,800 --> 00:16:13,840
courses, which actually gave me not a perfect vocabulary for it, but a better understanding

262
00:16:13,840 --> 00:16:16,240
of the things that I had developed an intuitive feel for.

263
00:16:16,240 --> 00:16:20,000
And I certainly, I don't think about it that way in the moment.

264
00:16:20,000 --> 00:16:22,200
When I'm playing music, I'm very much in it.

265
00:16:22,200 --> 00:16:26,200
You may have heard of like the idea of like creative flow, where it's like time flies

266
00:16:26,200 --> 00:16:27,200
and whatnot.

267
00:16:27,200 --> 00:16:32,800
So I think for myself kind of coming back at it as an editor, I can think about it like,

268
00:16:32,800 --> 00:16:35,240
oh, you know, like, this is a really good raw idea.

269
00:16:35,240 --> 00:16:38,560
And I can massage this in a way, you know, if I piece these things together, here's a

270
00:16:38,560 --> 00:16:40,320
really interesting musical pun.

271
00:16:40,320 --> 00:16:42,480
I don't think I'm more in the moment.

272
00:16:42,480 --> 00:16:44,560
It's a little bit more like I surprised myself.

273
00:16:44,560 --> 00:16:45,560
You know, that was neat.

274
00:16:45,560 --> 00:16:49,440
I have to record everything and then go back through with a little bit more of a higher

275
00:16:49,440 --> 00:16:50,440
planning process.

276
00:16:50,440 --> 00:16:54,720
So I may be getting away here and turning this into this week in music and, uh, they're

277
00:16:54,720 --> 00:16:55,720
related.

278
00:16:55,720 --> 00:16:56,720
And the arts.

279
00:16:56,720 --> 00:16:57,720
I contend they're related.

280
00:16:57,720 --> 00:16:59,600
But you're speaking here at the conference.

281
00:16:59,600 --> 00:17:00,600
What's your talk on?

282
00:17:00,600 --> 00:17:06,280
I'm talking about one project that we've recently published out of our newly minted music

283
00:17:06,280 --> 00:17:13,840
understanding group at Spotify around primarily singer vocal separation from recorded music.

284
00:17:13,840 --> 00:17:19,120
I lovingly refer to it, hat tip to a colleague from Miami, but it's unbaking the cake in

285
00:17:19,120 --> 00:17:20,120
a way.

286
00:17:20,120 --> 00:17:25,080
So in recorded music, you have these, this is basically acapella from any song, any track

287
00:17:25,080 --> 00:17:27,200
or instrumental from any song.

288
00:17:27,200 --> 00:17:30,560
So you can isolate the vocalist, you can remove the vocalist.

289
00:17:30,560 --> 00:17:34,880
It's a little bit of like the audio processing wizardry that, you know, if you look to computer

290
00:17:34,880 --> 00:17:40,400
vision, there have been some amazing, really interesting things with style transfer or texture

291
00:17:40,400 --> 00:17:41,560
mapping.

292
00:17:41,560 --> 00:17:49,200
So we took some recent advances with the picks to picks and the unit architecture and have

293
00:17:49,200 --> 00:17:51,880
adapted that to music processing.

294
00:17:51,880 --> 00:17:55,640
But the thing that really kind of made it work for us is that, you know, we have this really

295
00:17:55,640 --> 00:18:01,800
the large music catalog and one of the big bottlenecks for source separation for a long

296
00:18:01,800 --> 00:18:03,040
time has been data.

297
00:18:03,040 --> 00:18:07,280
And we were brainstorming one day, it's like, you know, deep warning is great when you

298
00:18:07,280 --> 00:18:10,760
have data for training and you have two options when it comes to data.

299
00:18:10,760 --> 00:18:14,000
You can curate it or you can get clever and try to harvest it.

300
00:18:14,000 --> 00:18:19,000
And a lot of computer vision stuff has gotten really far by using text around images and

301
00:18:19,000 --> 00:18:22,720
leveraging these other kind of serendipitous signals that occurs a byproduct of other

302
00:18:22,720 --> 00:18:23,720
kinds of things.

303
00:18:23,720 --> 00:18:26,960
It's bought a similar thing with play listing.

304
00:18:26,960 --> 00:18:31,000
So one of those signals we were able to harvest is that instrumental versions actually occur

305
00:18:31,000 --> 00:18:33,600
with a non negligible frequency.

306
00:18:33,600 --> 00:18:38,480
So if you have, say, a web scale music collection, you can actually end up with about a month

307
00:18:38,480 --> 00:18:42,240
or two's worth of straight audio for training these kinds of algorithms.

308
00:18:42,240 --> 00:18:47,520
So we were able to do some work out to like a percentage basis, like what percentage of

309
00:18:47,520 --> 00:18:52,600
your catalog has instrumental versions, a very small portion that I probably couldn't

310
00:18:52,600 --> 00:18:56,440
give you a number, either from memory or other reasons.

311
00:18:56,440 --> 00:19:00,920
But it's small, but when you have a large enough catalog, it becomes sufficient.

312
00:19:00,920 --> 00:19:04,200
So we end up with a couple of months of audio and we're able to train these algorithms

313
00:19:04,200 --> 00:19:08,520
to both isolate and remove vocals from the mix.

314
00:19:08,520 --> 00:19:12,320
So we nudge the state of the art a little bit versus some other models that have been

315
00:19:12,320 --> 00:19:13,320
published.

316
00:19:13,320 --> 00:19:14,320
Well, let's jump into that.

317
00:19:14,320 --> 00:19:17,520
So you mentioned, you mentioned picks to picks and another one unit.

318
00:19:17,520 --> 00:19:18,520
Yeah.

319
00:19:18,520 --> 00:19:20,920
So the unit architecture preceded the picks to picks work.

320
00:19:20,920 --> 00:19:21,920
Okay.

321
00:19:21,920 --> 00:19:22,920
What about those two?

322
00:19:22,920 --> 00:19:23,920
Sure.

323
00:19:23,920 --> 00:19:29,160
So the unit architecture, actually taking a step back, one of the ways that a lot of, you

324
00:19:29,160 --> 00:19:33,240
can generally call it signal processing in machine learning has worked for a while as

325
00:19:33,240 --> 00:19:37,120
the idea that if you could have some kind of compressing auto encoder, reducing the

326
00:19:37,120 --> 00:19:40,480
dimensionality, you'll preserve the attributes that are most important.

327
00:19:40,480 --> 00:19:45,240
You can back it out and then by minimizing some loss over the reconstruction, then you

328
00:19:45,240 --> 00:19:48,760
can start doing some interesting things, fiddling with your intermediary representations.

329
00:19:48,760 --> 00:19:55,280
But what ends up happening is especially for audio, a lot of the high frequency detail

330
00:19:55,280 --> 00:19:57,600
in the outputs is just the loss.

331
00:19:57,600 --> 00:20:01,840
So it works pretty well for general shapes, but sharp edges, these kind of things fall

332
00:20:01,840 --> 00:20:06,480
away in these auto encoder and butterfly style architectures.

333
00:20:06,480 --> 00:20:12,040
The unit takes this butterfly style architecture, folds it over into a V or U, however you want.

334
00:20:12,040 --> 00:20:15,640
And butterfly, like as the visual, you got this wide input, you're kind of compressing

335
00:20:15,640 --> 00:20:18,320
down the dimensionality and you're fanning it back out.

336
00:20:18,320 --> 00:20:19,320
Exactly.

337
00:20:19,320 --> 00:20:20,320
They're like a bow tie.

338
00:20:20,320 --> 00:20:23,560
But if you're going to take the butterfly, the bow tie, and fold the wings on itself,

339
00:20:23,560 --> 00:20:27,040
what you can do is you can take the, this is a convolutional architecture, you can take

340
00:20:27,040 --> 00:20:31,520
the feature maps from the forward path and concatenate them with the feature maps from

341
00:20:31,520 --> 00:20:33,320
the reverse path or the inverse path.

342
00:20:33,320 --> 00:20:39,000
And what that ends up doing is providing a lot more detail and just like more fine-grained

343
00:20:39,000 --> 00:20:43,840
granularity so that when you, in source separation, what you generally try to do is produce

344
00:20:43,840 --> 00:20:48,480
a mask and then you apply that mask over say an input, time frequency representation,

345
00:20:48,480 --> 00:20:53,600
like a spectrogram, spectrograms are kind of like an equalizer curve drawn out over time.

346
00:20:53,600 --> 00:20:59,080
So you can wait the relative contribution of each frequency been in time, just as a zero

347
00:20:59,080 --> 00:21:00,080
to one.

348
00:21:00,080 --> 00:21:05,600
So we use this unit architecture to produce a mask over the input representation.

349
00:21:05,600 --> 00:21:06,600
Let's back up a second.

350
00:21:06,600 --> 00:21:07,600
Sure.

351
00:21:07,600 --> 00:21:13,160
Because I'm losing, what does it mean to map the, what does it mean to take the feature

352
00:21:13,160 --> 00:21:15,000
map and circle it back on itself?

353
00:21:15,000 --> 00:21:16,360
I think that's the way you said it.

354
00:21:16,360 --> 00:21:20,400
So generally in a convolutional architecture, you'll have a bank of kernels, you'll take

355
00:21:20,400 --> 00:21:25,480
each kernel and you'll confolve it with an input representation and you'll get out generally

356
00:21:25,480 --> 00:21:29,040
a three-dimensional tensor of feature maps.

357
00:21:29,040 --> 00:21:34,560
If your input is 2D, you'll have k kernels by x by y.

358
00:21:34,560 --> 00:21:39,240
And on the inverse path, you can, you're also producing these feature map tensors.

359
00:21:39,240 --> 00:21:44,400
So with the corresponding layer in the inverse path, you can concatenate the input feature

360
00:21:44,400 --> 00:21:51,160
maps with the reverse feature maps along that case dimension, which becomes that in input,

361
00:21:51,160 --> 00:21:55,680
you can convolve another kernel matrix or kernel tensor, I guess.

362
00:21:55,680 --> 00:22:00,080
So it's kind of like you have your forward path features and your inverse path features

363
00:22:00,080 --> 00:22:02,120
being processed at the same time.

364
00:22:02,120 --> 00:22:08,400
So it allows the kind of like the composition of parts intuition for a deep architecture

365
00:22:08,400 --> 00:22:14,160
that information's propagating all the way to what is effectively this higher level representation

366
00:22:14,160 --> 00:22:19,120
of the network while preserving some of that fine grain detail on the way back.

367
00:22:19,120 --> 00:22:23,960
And what we find is that the masks that are produced by this architecture, do you have

368
00:22:23,960 --> 00:22:27,840
a lot more detail in what they're able to pinpoint in terms of the frequencies?

369
00:22:27,840 --> 00:22:32,560
So it's able to really dial in the components that are contributing to say singing voice.

370
00:22:32,560 --> 00:22:38,000
So the way this works is you can train one of these unit architectures for pinpointing

371
00:22:38,000 --> 00:22:39,000
the voice.

372
00:22:39,000 --> 00:22:41,960
So you can train in a separate one for isolating the voice.

373
00:22:41,960 --> 00:22:46,600
If you had different data, you could imagine doing something similar for say drums or

374
00:22:46,600 --> 00:22:49,440
other source specific architectures.

375
00:22:49,440 --> 00:22:54,800
When you're training to pinpoint the voice, are you using like an acapella version as

376
00:22:54,800 --> 00:23:00,080
you're training data, or are you still using your instrumental and somehow like inverting

377
00:23:00,080 --> 00:23:01,640
it or something like that?

378
00:23:01,640 --> 00:23:02,640
That's a great question.

379
00:23:02,640 --> 00:23:08,080
And for the work that we published and we'll be presenting at the Izmir conference in Suzhou,

380
00:23:08,080 --> 00:23:12,760
China and the near future, what we actually did was there were far more instrumental versions

381
00:23:12,760 --> 00:23:13,960
than acapella versions.

382
00:23:13,960 --> 00:23:19,520
So we kind of estimate what the vocals would be by looking at the positive difference

383
00:23:19,520 --> 00:23:22,560
between a full mix and then the corresponding instrumental.

384
00:23:22,560 --> 00:23:26,240
So it's kind of like a proxy for what the voice would be.

385
00:23:26,240 --> 00:23:30,240
Which is not surprising why we get much better vocal removal results because we're training

386
00:23:30,240 --> 00:23:35,240
on the actual instrumental spectra than the vocal which is estimated.

387
00:23:35,240 --> 00:23:39,400
But it's a really interesting point for future work to say what other kinds of content

388
00:23:39,400 --> 00:23:43,800
do we have at our disposal that we can kind of fold in to this kind of work.

389
00:23:43,800 --> 00:23:46,680
So we've got some really encouraging results, we've got some demos that I'll be sharing

390
00:23:46,680 --> 00:23:47,680
a little bit later.

391
00:23:47,680 --> 00:23:51,160
I imagine there'll be out on the internet in not too long.

392
00:23:51,160 --> 00:23:54,720
And so what else are there any other things that you talked about during your talk or

393
00:23:54,720 --> 00:23:57,560
that you're planning to talk about during your talk you haven't talked yet?

394
00:23:57,560 --> 00:24:01,240
I guess the only other thing I would mention is being that we're in a really pivotal

395
00:24:01,240 --> 00:24:05,760
time for a lot of machine learning and artificial intelligence research as we really start to

396
00:24:05,760 --> 00:24:07,760
frame this conversation.

397
00:24:07,760 --> 00:24:11,520
It's been an interest of mine for a while now that like what we discussed earlier, this

398
00:24:11,520 --> 00:24:14,120
idea of creative AI.

399
00:24:14,120 --> 00:24:19,280
And I do want to take the time tomorrow to just have that shameless plug, music is a really,

400
00:24:19,280 --> 00:24:20,680
really interesting domain.

401
00:24:20,680 --> 00:24:26,000
It doesn't necessarily have the same societal impacts that autonomous vehicles could have

402
00:24:26,000 --> 00:24:31,840
in terms of saving lines, but in a lot of ways music does have that human power in much

403
00:24:31,840 --> 00:24:36,040
more of a emotional and kind of cultural way.

404
00:24:36,040 --> 00:24:41,440
So as we start to think about like what other ways do we want to tackle artificial intelligence?

405
00:24:41,440 --> 00:24:47,720
Like can we really study music without inherently studying humanity and kind of the intelligence

406
00:24:47,720 --> 00:24:50,480
that we've been in doubt with?

407
00:24:50,480 --> 00:24:56,000
And one of the questions that comes up for me in this conversation is a little bit out

408
00:24:56,000 --> 00:25:02,600
of left field, but what in your view is the kind of open source as a concept has like swept

409
00:25:02,600 --> 00:25:08,240
over a bunch of different industries and areas of human activity, right?

410
00:25:08,240 --> 00:25:09,360
Is there an open source?

411
00:25:09,360 --> 00:25:10,960
What is the open source for music?

412
00:25:10,960 --> 00:25:16,440
And the thought that preceded that was it'd be kind of interesting if in some period of

413
00:25:16,440 --> 00:25:22,040
time a musician delivered not this like one final thing, but you know there was like almost

414
00:25:22,040 --> 00:25:25,760
like a project file for your digital workstation it had.

415
00:25:25,760 --> 00:25:28,960
You can pull out the drums if you wanted to, you can pull out, you can isolate all different

416
00:25:28,960 --> 00:25:34,680
kinds of things because it followed some kind of open source, you know, ideology or something

417
00:25:34,680 --> 00:25:35,680
like that.

418
00:25:35,680 --> 00:25:38,440
Is there an analog like that for music or what's the closest we get?

419
00:25:38,440 --> 00:25:41,200
I think that's a really, really fascinating idea.

420
00:25:41,200 --> 00:25:44,280
And you know, I have to smile a little bit when you say what's the analog to that?

421
00:25:44,280 --> 00:25:48,920
In a lot of ways playing music together is that analog, you know, for a while ever since

422
00:25:48,920 --> 00:25:54,040
it was music publishing or sound recording, it's been really interesting what sampling,

423
00:25:54,040 --> 00:26:00,560
remixing, reuse means for music because everything that you compose, create, share is an

424
00:26:00,560 --> 00:26:03,320
amalgamation of all your prior experiences.

425
00:26:03,320 --> 00:26:04,320
So Larry Lessig.

426
00:26:04,320 --> 00:26:05,320
So Larry Lessig.

427
00:26:05,320 --> 00:26:08,320
A little bit of an inherent open source to music from that perspective.

428
00:26:08,320 --> 00:26:09,320
Exactly.

429
00:26:09,320 --> 00:26:12,720
Larry Lessig wrote a really great book called Remix and that touches on a lot of these

430
00:26:12,720 --> 00:26:17,040
interesting things, both from a legal but also a philosophical and kind of cultural perspective.

431
00:26:17,040 --> 00:26:22,920
A focus particularly on music or more generally on touches on, touches on things.

432
00:26:22,920 --> 00:26:27,320
So there have been some famous mashup artists like Greg Gillis and Girl Talk.

433
00:26:27,320 --> 00:26:32,520
I'm testing my memory at this point, but there's a famous composer who decried the rise

434
00:26:32,520 --> 00:26:37,680
of sound recordings and that it was going to like change what reuse and remixing really

435
00:26:37,680 --> 00:26:38,680
meant.

436
00:26:38,680 --> 00:26:44,000
We've seen that get a little bit murky as copyright laws change or whatnot.

437
00:26:44,000 --> 00:26:47,240
But yeah, it's music wants to be open source by nature.

438
00:26:47,240 --> 00:26:51,680
So I think the analog is analog music, it's the acoustic signal.

439
00:26:51,680 --> 00:26:52,680
Interesting.

440
00:26:52,680 --> 00:26:56,560
It'll be interesting to see how music evolves along with machine learning and AI.

441
00:26:56,560 --> 00:26:57,560
Yeah.

442
00:26:57,560 --> 00:26:58,560
I think they have a bright future together.

443
00:26:58,560 --> 00:26:59,560
Absolutely.

444
00:26:59,560 --> 00:27:00,560
Well, thanks so much Eric.

445
00:27:00,560 --> 00:27:02,560
Thank you so much Sam.

446
00:27:02,560 --> 00:27:08,320
All right everyone, that's our show for today.

447
00:27:08,320 --> 00:27:13,320
Thanks so much for listening and for your continued feedback and support.

448
00:27:13,320 --> 00:27:19,280
Thanks to you, this podcast finished the year as a top 40 technology podcast on Apple

449
00:27:19,280 --> 00:27:20,800
podcasts.

450
00:27:20,800 --> 00:27:26,880
My producer says that one of his goals this year is to crack the top 10 and to do that,

451
00:27:26,880 --> 00:27:29,800
we will need your help.

452
00:27:29,800 --> 00:27:33,240
Please head on over to the podcast app, rate the show.

453
00:27:33,240 --> 00:27:35,400
Hopefully we've earned your five stars.

454
00:27:35,400 --> 00:27:40,400
Give us a glowing review and share it with your friends, family, co-workers, Starbucks

455
00:27:40,400 --> 00:27:43,880
baristas, Uber drivers, everyone.

456
00:27:43,880 --> 00:27:46,360
Every review and rating goes a long way.

457
00:27:46,360 --> 00:27:48,560
So thanks in advance.

458
00:27:48,560 --> 00:27:53,600
For more information on Eric or any of the topics covered in this episode, head on over

459
00:27:53,600 --> 00:27:58,000
to twomlai.com slash talk slash 98.

460
00:27:58,000 --> 00:28:03,520
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

461
00:28:03,520 --> 00:28:06,680
or via Twitter at at twomlai.

462
00:28:06,680 --> 00:28:34,320
Thanks once again for listening and catch you next time.

