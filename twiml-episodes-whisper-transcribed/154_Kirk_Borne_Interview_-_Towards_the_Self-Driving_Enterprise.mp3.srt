1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,720
I'm your host Sam Charrington.

4
00:00:31,720 --> 00:00:36,480
A couple of weeks ago I spent some time at the Pegaworld conference in Las Vegas.

5
00:00:36,480 --> 00:00:41,920
The theme of the conference was automation, particularly in service of the customer experience.

6
00:00:41,920 --> 00:00:47,200
And I had a great time seeing all the advancements coming into this field by way of machine learning

7
00:00:47,200 --> 00:00:49,080
and AI.

8
00:00:49,080 --> 00:00:55,160
In this show, the first of our Pegaworld 2018 series, I'm joined by Kirk Born, principal

9
00:00:55,160 --> 00:01:01,080
data scientist at management consulting firm Booz Allen Hamilton.

10
00:01:01,080 --> 00:01:06,280
In our conversation, Kirk shares his views on automation as it applies to enterprises

11
00:01:06,280 --> 00:01:08,080
and their customers.

12
00:01:08,080 --> 00:01:14,240
We discuss his experiences evangelizing data science within the context of a large organization

13
00:01:14,240 --> 00:01:19,600
and the role of AI in helping organizations achieve automation.

14
00:01:19,600 --> 00:01:24,760
Along the way, Kirk shares a great analogy for intelligent automation comparing it to

15
00:01:24,760 --> 00:01:27,120
an autonomous vehicle.

16
00:01:27,120 --> 00:01:31,560
We cover a ton of ground in this chat, which I think you'll get a kick out of.

17
00:01:31,560 --> 00:01:36,640
Before we jump into the interview, I'd like to send a huge thanks to our friends at Pegasystems

18
00:01:36,640 --> 00:01:40,400
for hosting me at Pegaworld and sponsoring this series.

19
00:01:40,400 --> 00:01:45,440
One of the great announcements coming out of the conference was Pegasystems' new self-optimizing

20
00:01:45,440 --> 00:01:48,720
AI-powered marketing capabilities.

21
00:01:48,720 --> 00:01:53,800
This is a really interesting offering designed to reduce marketers' dependence on traditional

22
00:01:53,800 --> 00:02:01,200
segment-based campaigns and transition them towards real-time one-to-one customer engagement.

23
00:02:01,200 --> 00:02:05,920
These new capabilities will be available as part of their new Pegga Infinity platform,

24
00:02:05,920 --> 00:02:08,680
which was also announced at the event.

25
00:02:08,680 --> 00:02:13,840
For more info on Pegga Infinity, head to pegga.com slash infinity.

26
00:02:13,840 --> 00:02:17,440
All right, let's do it.

27
00:02:17,440 --> 00:02:20,200
All right, everyone.

28
00:02:20,200 --> 00:02:26,520
I am here at Pegaworld in Las Vegas, and I have the distinct pleasure of being seated across

29
00:02:26,520 --> 00:02:28,280
from Kirk Bourne.

30
00:02:28,280 --> 00:02:32,680
Kirk is a principal data scientist at Booz Allen Hamilton.

31
00:02:32,680 --> 00:02:35,480
Kirk, welcome to this week in Machine Learning and AI.

32
00:02:35,480 --> 00:02:36,480
Fantastic.

33
00:02:36,480 --> 00:02:37,480
Thank you, Sam.

34
00:02:37,480 --> 00:02:42,120
We're here with you in Vegas, or I should say hot Vegas.

35
00:02:42,120 --> 00:02:43,120
Hot Vegas.

36
00:02:43,120 --> 00:02:44,720
Not hot, Lanna hot Vegas.

37
00:02:44,720 --> 00:02:45,720
Exactly.

38
00:02:45,720 --> 00:02:49,600
The great time to talk about AI, machine learning, and other cool things in a hot environment.

39
00:02:49,600 --> 00:02:50,600
Absolutely.

40
00:02:50,600 --> 00:02:51,600
Absolutely.

41
00:02:51,600 --> 00:02:55,520
Why don't we jump right in and have you tell us a little bit about your background?

42
00:02:55,520 --> 00:02:56,520
Wow.

43
00:02:56,520 --> 00:03:01,480
So, I've been around the block a few times, so my background is astrophysics.

44
00:03:01,480 --> 00:03:05,960
I fell in love with trying to understand the universe years ago, which meant I fell

45
00:03:05,960 --> 00:03:10,920
in love with data years ago, because data tells us about things, how things work, and

46
00:03:10,920 --> 00:03:15,160
the universe is my system that I studied for decades.

47
00:03:15,160 --> 00:03:21,640
I worked at NASA for nearly 20 years on data systems to help other astronomers access data,

48
00:03:21,640 --> 00:03:25,080
as well as myself accessing data to study our universe.

49
00:03:25,080 --> 00:03:30,720
And during that period, I fell in love with concepts like machine learning, data mining,

50
00:03:30,720 --> 00:03:34,880
and things that we call data science nowadays, and even AI.

51
00:03:34,880 --> 00:03:39,200
And how can we do greater discovery from data?

52
00:03:39,200 --> 00:03:46,160
And so during those years, I eventually decided that I wanted to move out of the NASA world

53
00:03:46,160 --> 00:03:52,320
into education, to teach the next generation students, next generation workforce, about

54
00:03:52,320 --> 00:03:53,640
data science and data.

55
00:03:53,640 --> 00:04:01,440
So I left NASA that was 15 years ago, and spent 12 years at George Mason University in Virginia,

56
00:04:01,440 --> 00:04:03,640
teaching data science to students.

57
00:04:03,640 --> 00:04:08,680
And that's what I love doing, teaching, educating, informing people about this.

58
00:04:08,680 --> 00:04:13,680
And then this company, Booz Allen Hamilton, called me up a few years ago, and said, hey, how

59
00:04:13,680 --> 00:04:19,080
do you like to do that for our clients and for a bigger world than just the science world?

60
00:04:19,080 --> 00:04:21,160
And I said, yes.

61
00:04:21,160 --> 00:04:25,000
So I've been doing this principle data science thing for Booz Allen Hamilton for three

62
00:04:25,000 --> 00:04:26,000
years now.

63
00:04:26,000 --> 00:04:27,000
Awesome.

64
00:04:27,000 --> 00:04:28,000
Awesome.

65
00:04:28,000 --> 00:04:32,920
And you're also quite prolific on Twitter, and speaking at conferences, and kind of

66
00:04:32,920 --> 00:04:37,760
the whole gamut, I don't know that there's a point to that other than, I'm one of your

67
00:04:37,760 --> 00:04:41,840
Twitter admirers, and we go back and forth quite a bit on there.

68
00:04:41,840 --> 00:04:42,840
Yeah.

69
00:04:42,840 --> 00:04:43,840
Well, thank you.

70
00:04:43,840 --> 00:04:48,680
Well, one thing I don't do, I admire that you're doing is the podcast series.

71
00:04:48,680 --> 00:04:54,760
My son-in-law gave me a bunch of equipment a few months ago for my birthday and said, hey,

72
00:04:54,760 --> 00:04:55,760
here's your chance.

73
00:04:55,760 --> 00:04:56,760
Nice.

74
00:04:56,760 --> 00:05:00,800
I haven't quite put it to use yet, but no, it's, I really love sharing knowledge about this

75
00:05:00,800 --> 00:05:03,720
field. I mean, it's exciting what's happening.

76
00:05:03,720 --> 00:05:08,680
I don't, and I think we're part of a sharing economy in the world in general, but I think

77
00:05:08,680 --> 00:05:13,440
in the data science community, especially data scientists love to share their knowledge.

78
00:05:13,440 --> 00:05:17,480
If you look at all the hackathons that take place, people love to use their knowledge for

79
00:05:17,480 --> 00:05:18,480
social good.

80
00:05:18,480 --> 00:05:23,280
If you see all the data for good hackathons and competitions and activities that people

81
00:05:23,280 --> 00:05:25,400
participate in.

82
00:05:25,400 --> 00:05:29,800
And so it's just part of our community to share knowledge, to share what we know, to

83
00:05:29,800 --> 00:05:30,800
help things.

84
00:05:30,800 --> 00:05:33,360
And if we can earn a living doing it also, that's fantastic.

85
00:05:33,360 --> 00:05:34,360
Nice.

86
00:05:34,360 --> 00:05:35,360
Nice.

87
00:05:35,360 --> 00:05:38,440
Yeah, that is one of the great things about the community, both from a research perspective

88
00:05:38,440 --> 00:05:42,680
as well as the commercial side of it, the willingness to publish, for example, you don't

89
00:05:42,680 --> 00:05:44,520
see that in a lot of other areas.

90
00:05:44,520 --> 00:05:52,360
Yeah, I think the open source community, which includes Python and R, is sort of ingrained

91
00:05:52,360 --> 00:05:54,360
in this community.

92
00:05:54,360 --> 00:05:57,120
And as such, people are willing to share lots of things.

93
00:05:57,120 --> 00:06:00,960
You know, not just their code, but knowledge that they've learned and ideas that they've

94
00:06:00,960 --> 00:06:01,960
had.

95
00:06:01,960 --> 00:06:06,040
And I mean, I grew up in an era where people were very protective, you know, peer-reviewed

96
00:06:06,040 --> 00:06:11,480
research was published journals that you had to pay subscriptions to receive, and otherwise

97
00:06:11,480 --> 00:06:14,360
you wouldn't have access to the knowledge.

98
00:06:14,360 --> 00:06:18,040
And you know, I think the world has changed a lot that we believe more in sort of open

99
00:06:18,040 --> 00:06:23,600
science, open data, open knowledge, because it's really the benefit to society that this

100
00:06:23,600 --> 00:06:29,960
stuff brings that should take precedence over me getting some kind of special accolades

101
00:06:29,960 --> 00:06:33,320
or attention because I came up with a thought.

102
00:06:33,320 --> 00:06:34,320
Right.

103
00:06:34,320 --> 00:06:38,000
I've learned, if I'm old enough to know that if I came up with a thought, someone else

104
00:06:38,000 --> 00:06:43,960
probably has also thought, so I don't think it's worth arguing over who came up with

105
00:06:43,960 --> 00:06:44,960
the idea first.

106
00:06:44,960 --> 00:06:47,240
The fact is we have great ideas in this community.

107
00:06:47,240 --> 00:06:52,040
We love to share them with each other and to benefit our clients and ourselves and our

108
00:06:52,040 --> 00:06:54,120
society from it.

109
00:06:54,120 --> 00:06:57,000
So what is your day job at Booz Allen Hamilton?

110
00:06:57,000 --> 00:07:01,240
Are you primarily evangelizing and educating or are you doing?

111
00:07:01,240 --> 00:07:03,720
Are you involved in projects as well?

112
00:07:03,720 --> 00:07:09,000
It's mostly the former, but some of the latter, so primarily what they call the horizontal

113
00:07:09,000 --> 00:07:11,080
matrix guy, okay?

114
00:07:11,080 --> 00:07:15,040
So I like to tell people we have a thousand data scientists, the world's best kept secret

115
00:07:15,040 --> 00:07:20,280
and data science, because our data scientists are working primarily on client data, on client

116
00:07:20,280 --> 00:07:25,480
projects, on client site, and we're not allowed to talk about it.

117
00:07:25,480 --> 00:07:28,240
And so people pretty much don't know what we're doing.

118
00:07:28,240 --> 00:07:32,760
So all these different projects are in different vertical markets, mostly in the federal government

119
00:07:32,760 --> 00:07:37,960
space, but things like healthcare or military or intelligence or transportation or energy

120
00:07:37,960 --> 00:07:42,920
or treasury or Homeland Security, we got something going on.

121
00:07:42,920 --> 00:07:48,200
And there's chief data scientists who basically manage, not so much manage, but they basically

122
00:07:48,200 --> 00:07:54,520
oversee the talent acquisition and the work acquisition in those markets.

123
00:07:54,520 --> 00:07:58,800
So we have many chief data scientists, but the firm has one principal data scientist and

124
00:07:58,800 --> 00:07:59,800
that's me.

125
00:07:59,800 --> 00:08:03,600
So one of the ways they were able to extract me at the university made me an offer I couldn't

126
00:08:03,600 --> 00:08:07,960
refuse and he actually created a job title just for me.

127
00:08:07,960 --> 00:08:11,920
So my day job doesn't include interacting with some of these projects and occasionally

128
00:08:11,920 --> 00:08:16,920
doing some advising and mentoring of people who are working on those projects, but more

129
00:08:16,920 --> 00:08:22,520
often than not, it's the evangelization thought leadership is a phrase they like to throw

130
00:08:22,520 --> 00:08:23,520
around.

131
00:08:23,520 --> 00:08:28,320
It means lots of writing and public speaking, even executive advising.

132
00:08:28,320 --> 00:08:34,480
So I say I mentioned the newest members on the block and doing this stuff and the more

133
00:08:34,480 --> 00:08:39,800
senior executives who maybe have heard of this, but don't know quite what it is.

134
00:08:39,800 --> 00:08:46,800
And so I have this desire to share knowledge, and I sort of fulfilled that desire by

135
00:08:46,800 --> 00:08:50,400
being a university professor for 12 years.

136
00:08:50,400 --> 00:08:55,200
And I haven't surrendered that passion of teaching and mentoring and training by going

137
00:08:55,200 --> 00:08:56,200
to Booz Allen.

138
00:08:56,200 --> 00:09:01,000
I'm just doing it in more interesting and diverse environments.

139
00:09:01,000 --> 00:09:04,800
Much more interesting use cases than maybe the galaxies I used to work on, even though

140
00:09:04,800 --> 00:09:09,400
I used to love those galaxies, there's probably a handful of people in the world who cared

141
00:09:09,400 --> 00:09:11,400
about those galaxies.

142
00:09:11,400 --> 00:09:16,600
But now when I tweet something on Twitter, there's 200,000 people who are reading

143
00:09:16,600 --> 00:09:17,600
it.

144
00:09:17,600 --> 00:09:18,600
So that's interesting.

145
00:09:18,600 --> 00:09:20,600
That impact it now has.

146
00:09:20,600 --> 00:09:21,600
Right.

147
00:09:21,600 --> 00:09:22,600
Right.

148
00:09:22,600 --> 00:09:28,480
So we've been here at Peggo World for the past couple of days, and a lot of the conversation

149
00:09:28,480 --> 00:09:34,400
has been around automation, automation and service of digital transformation and service

150
00:09:34,400 --> 00:09:44,160
of customer experiences, and increasingly adding elements of intelligence to that process

151
00:09:44,160 --> 00:09:45,160
of automation.

152
00:09:45,160 --> 00:09:47,880
And what's your take on that?

153
00:09:47,880 --> 00:09:51,720
Well, first of all, I just love this.

154
00:09:51,720 --> 00:09:58,760
And one of the more trivial reasons why I love this is it gives me the opportunity to

155
00:09:58,760 --> 00:10:05,240
use one of my most favorite words in a sentence, and that word is confluence.

156
00:10:05,240 --> 00:10:08,840
So if you understand what confluence means and the confluence of rivers and if you ever

157
00:10:08,840 --> 00:10:11,760
visit such things, that's really interesting.

158
00:10:11,760 --> 00:10:13,080
But that's another story.

159
00:10:13,080 --> 00:10:16,160
But we're living in an age of the confluence of many technologies.

160
00:10:16,160 --> 00:10:23,160
Many technologies converging, merging, working together to a greater outcome in the same

161
00:10:23,160 --> 00:10:30,160
way that many rivers, tributaries, can merge together and create a mighty force.

162
00:10:30,160 --> 00:10:36,840
And so we have automation, we have AI, and when you think about customer experience, things

163
00:10:36,840 --> 00:10:39,960
like that, for me, it's like everyone has a customer.

164
00:10:39,960 --> 00:10:44,800
I get a little bit of friction with some of my astrophysics colleagues from days of

165
00:10:44,800 --> 00:10:48,120
long ago saying, Kirk, what are you doing now?

166
00:10:48,120 --> 00:10:49,920
What is the stuff you're doing now?

167
00:10:49,920 --> 00:10:53,800
And I said, well, that's not really all that different.

168
00:10:53,800 --> 00:10:59,240
Because back then, the people who we needed to impress in order to get grant money might

169
00:10:59,240 --> 00:11:05,960
have been the proposal reviewers or the agencies who fund our research or the paper reviewers

170
00:11:05,960 --> 00:11:09,280
to get our papers published in journals.

171
00:11:09,280 --> 00:11:13,200
And so we're selling our ideas, we're selling our thoughts, we're selling our work in some

172
00:11:13,200 --> 00:11:14,200
sense.

173
00:11:14,200 --> 00:11:18,800
Okay, maybe not in the strict sense of selling, but we're making a good case for what we've

174
00:11:18,800 --> 00:11:20,800
done as a value to someone.

175
00:11:20,800 --> 00:11:27,160
And so customers need to have those value propositions from companies, and they just don't

176
00:11:27,160 --> 00:11:28,160
want to hear the words.

177
00:11:28,160 --> 00:11:32,760
They want to sort of, I say, sort of the three questions have to be answered for that

178
00:11:32,760 --> 00:11:33,760
customer.

179
00:11:33,760 --> 00:11:36,160
What, so what, and now what?

180
00:11:36,160 --> 00:11:37,160
What is it you did for me?

181
00:11:37,160 --> 00:11:38,160
Why should I care?

182
00:11:38,160 --> 00:11:40,680
And what does that mean for me now?

183
00:11:40,680 --> 00:11:45,760
And so being able to answer those questions improves both customer relationship and customer

184
00:11:45,760 --> 00:11:49,080
experience, customer journey, whatever you want to call it.

185
00:11:49,080 --> 00:11:51,000
And every, like I said, everyone has a customer, right?

186
00:11:51,000 --> 00:11:55,040
So whether you're in a government sector and you have stakeholders or publicly traded

187
00:11:55,040 --> 00:11:56,960
company, you have shareholders.

188
00:11:56,960 --> 00:12:02,440
If you're actually customer facing, obviously you have customers, whatever it is, someone

189
00:12:02,440 --> 00:12:07,760
is, will buy your product, will buy your ideas, will listen to you, will pay attention

190
00:12:07,760 --> 00:12:08,760
to you.

191
00:12:08,760 --> 00:12:13,240
You know, if all you're selling are your words and your thoughts and your ideas to someone,

192
00:12:13,240 --> 00:12:18,840
you have to be able to say it in ways that are empathetic, that is puts the words in ways

193
00:12:18,840 --> 00:12:22,600
that they can understand it, that they see the value for them.

194
00:12:22,600 --> 00:12:27,560
And so it's putting yourself in their shoes and allowing them to sort of see it from their

195
00:12:27,560 --> 00:12:28,560
perspective.

196
00:12:28,560 --> 00:12:35,480
And as a data scientist, I love this way of describing what I do, one of the ways I describe

197
00:12:35,480 --> 00:12:38,600
my job to people is I talk the walk, all right?

198
00:12:38,600 --> 00:12:40,960
So there's a lot of companies out there that can talk the talk, right?

199
00:12:40,960 --> 00:12:46,560
There's AI, lots of hype, data science, machine learning, blockchain, I mean, you name it,

200
00:12:46,560 --> 00:12:48,840
there's hype cycles all over the place.

201
00:12:48,840 --> 00:12:51,200
People love to talk the talk, right?

202
00:12:51,200 --> 00:12:55,960
And there's a famous quote from years ago, when big data was at the peak of its hype cycle,

203
00:12:55,960 --> 00:12:59,760
a person said that, you know, big data is like teenage sex.

204
00:12:59,760 --> 00:13:00,960
Everyone is talking about it.

205
00:13:00,960 --> 00:13:04,960
No one really knows how to do it, but everyone thinks everyone else is doing it.

206
00:13:04,960 --> 00:13:07,640
So you claim you're doing it even though you don't know what you're doing, okay?

207
00:13:07,640 --> 00:13:12,680
So I think a lot of hype cycles are like that, people do a lot of talking the talk.

208
00:13:12,680 --> 00:13:16,600
And so we talked about moving beyond that to being able to walk the talk, being able

209
00:13:16,600 --> 00:13:20,360
to actually do the thing you're talking about, and a lot of companies are now reaching

210
00:13:20,360 --> 00:13:21,360
that pace.

211
00:13:21,360 --> 00:13:25,800
So when we see data analytics, I think we're past the hype cycle, AI is sort of peaking

212
00:13:25,800 --> 00:13:26,800
out.

213
00:13:26,800 --> 00:13:31,720
And I think we see tons of implementations, amazing processes being automated and all

214
00:13:31,720 --> 00:13:36,960
kinds of things being implemented, AI showing up and I'll crawl across enterprises everywhere.

215
00:13:36,960 --> 00:13:40,400
And so people are really, you know, walking the talk.

216
00:13:40,400 --> 00:13:44,640
And so I say, I see my role now as to being able to now to describe that to someone.

217
00:13:44,640 --> 00:13:47,800
You know, I don't want to sit there and describe something in very mathematical language

218
00:13:47,800 --> 00:13:51,800
and all about deep learning and neural networks and words that people have no idea what I'm

219
00:13:51,800 --> 00:13:52,800
talking about.

220
00:13:52,800 --> 00:13:57,200
I had to be able to explain it in a way that's empathetic, that is they can see why, you

221
00:13:57,200 --> 00:14:01,360
know, the what, the so what, and the know what, and this thing that we're doing.

222
00:14:01,360 --> 00:14:07,000
And so I take that as my personal role at Booz Allen Hamilton and in my life in general

223
00:14:07,000 --> 00:14:12,400
to be able to explain to people in those terms, what is this stuff we're doing?

224
00:14:12,400 --> 00:14:17,040
And I think this is a very interesting time to do that because of this confluence of

225
00:14:17,040 --> 00:14:18,040
so many technologies.

226
00:14:18,040 --> 00:14:21,160
And to be able to explain what we're doing to people, we have to be able to explain what

227
00:14:21,160 --> 00:14:26,800
machine learning is, what AI is, what big data is, what data, you know, what data privacy

228
00:14:26,800 --> 00:14:28,040
has to do with it.

229
00:14:28,040 --> 00:14:30,160
And we're not trying to steal people's identity.

230
00:14:30,160 --> 00:14:32,120
We're not trying to destroy the world with robots.

231
00:14:32,120 --> 00:14:35,520
I mean, so you got to have all kinds of different kinds of conversations with people, but you

232
00:14:35,520 --> 00:14:40,560
got to put yourself in their shoes at the same time not being untrue to yourself and

233
00:14:40,560 --> 00:14:42,800
untrue to the technology.

234
00:14:42,800 --> 00:14:47,840
And that's, you know, that's a fine line and I sort of enjoy walking that tight rope.

235
00:14:47,840 --> 00:14:51,000
What do you mean by untrue to the technology?

236
00:14:51,000 --> 00:14:53,800
That is slipping into talking to talk.

237
00:14:53,800 --> 00:14:54,800
All right.

238
00:14:54,800 --> 00:14:58,520
It's really easy when you talk to people about the AI machine learning and stuff like this

239
00:14:58,520 --> 00:15:02,360
and say, oh, it's going to cure cancer, it's going to change the world, it's going to

240
00:15:02,360 --> 00:15:08,640
reduce poverty, it's going to remove gender inequalities, it's going to fix our environment.

241
00:15:08,640 --> 00:15:12,440
And all of a sudden, you're just sort of like mouthing all these platitudes.

242
00:15:12,440 --> 00:15:15,840
And it's not being true to the technology because some of this stuff is just plain hard.

243
00:15:15,840 --> 00:15:21,800
And some of the AI's we see in businesses are pretty small and minor and that's okay.

244
00:15:21,800 --> 00:15:23,040
That's really okay.

245
00:15:23,040 --> 00:15:26,920
Like if you use a cell phone, right, use a smartphone and you're using text messaging.

246
00:15:26,920 --> 00:15:32,040
There's an autocomplete, right, a spell check, an autocorrect on your cell phone, right,

247
00:15:32,040 --> 00:15:35,800
as you're typing the word, it sort of completes the word for you, all right, so that's a type

248
00:15:35,800 --> 00:15:36,800
of head feature.

249
00:15:36,800 --> 00:15:39,640
It sees what you're typing and it sort of guesses what the word is going to be, even the

250
00:15:39,640 --> 00:15:40,960
next word.

251
00:15:40,960 --> 00:15:47,320
That's an AI, all right, that I don't think any autocomplete or autocorrect has ever come

252
00:15:47,320 --> 00:15:51,720
out of your phone and taken over the world.

253
00:15:51,720 --> 00:15:55,480
So we, so we can be true to the technology, say this, this is an AI, what you have in

254
00:15:55,480 --> 00:15:57,880
your hand is an AI.

255
00:15:57,880 --> 00:16:04,200
But it's not the kind of AI you see in the movies, right, all right, so I can, so I don't

256
00:16:04,200 --> 00:16:10,880
want to say that AI is this pure force that's going to cure all of the world's illnesses

257
00:16:10,880 --> 00:16:14,000
and problems and will live happily ever after.

258
00:16:14,000 --> 00:16:16,640
I mean, that's not being true to the technology.

259
00:16:16,640 --> 00:16:20,840
It's hard work and a lot of that hard work, it has to deal with the ethical questions

260
00:16:20,840 --> 00:16:24,400
and all the bias questions and data privacy questions.

261
00:16:24,400 --> 00:16:29,240
There's lots of really hard problems to solve there and it's not being true to the technology

262
00:16:29,240 --> 00:16:32,360
or to yourself to ignore those.

263
00:16:32,360 --> 00:16:40,800
I imagine that you have had encounters with executives at Booz Allen where, you know,

264
00:16:40,800 --> 00:16:43,800
you hear, well, why can't we just?

265
00:16:43,800 --> 00:16:48,320
And it's, you know, some, you know, then a miracle occurs type of statement or, you

266
00:16:48,320 --> 00:16:52,600
know, something that is more like, you know, maybe something out of the movies or something,

267
00:16:52,600 --> 00:16:53,600
you know.

268
00:16:53,600 --> 00:16:57,640
How do you deal with that beyond just saying it's hard?

269
00:16:57,640 --> 00:17:02,600
Well, to tell you the truth, it's usually the other way around, really, at least the

270
00:17:02,600 --> 00:17:03,600
bike experience.

271
00:17:03,600 --> 00:17:08,280
I find a lot of the, of course, we're a management consulting firm, right, so we go

272
00:17:08,280 --> 00:17:14,280
to clients and we're pitching our consulting services, whether it's analytics or digital

273
00:17:14,280 --> 00:17:16,720
or cyber security or something.

274
00:17:16,720 --> 00:17:20,520
And so we're selling to a customer, right, and this customer could be a federal director

275
00:17:20,520 --> 00:17:22,520
of some agency or something like this.

276
00:17:22,520 --> 00:17:25,760
And so they're a very critical person, right, they're not going to spend their money any

277
00:17:25,760 --> 00:17:28,640
more than when you would go buy a product, you're going to spend your money just because

278
00:17:28,640 --> 00:17:31,920
some salesperson says so, right, you're going to be critical.

279
00:17:31,920 --> 00:17:36,840
So I see a lot of good critical thinking and critical question asking from that side of

280
00:17:36,840 --> 00:17:42,080
the table, but occasionally what happens is on the data science side of the table, which

281
00:17:42,080 --> 00:17:43,080
is where I sat.

282
00:17:43,080 --> 00:17:49,360
And I see myself as a younger person doing this years ago when I was really getting excited

283
00:17:49,360 --> 00:17:53,320
about this automation and AI and machine learning stuff, and I first just sort of discovered

284
00:17:53,320 --> 00:17:58,040
it, if you will, 20 years ago, I was, I was that, I was that sort of, you know, pying

285
00:17:58,040 --> 00:18:02,120
this guy guy and the other side of the table, talking to my NASA clients, oh, we can be

286
00:18:02,120 --> 00:18:04,000
able to do all this space exploration.

287
00:18:04,000 --> 00:18:08,920
We can autonomously drive spacecraft around Mars and fight all kinds of interesting discoveries

288
00:18:08,920 --> 00:18:14,880
and new, you know, new water deposits and new titanium deposits, and we can build colonies

289
00:18:14,880 --> 00:18:19,320
on Mars and we're going to have an Amazon type service, automatic supply chain of delivery

290
00:18:19,320 --> 00:18:24,480
of the supplies that astronauts need just in time from a supply ship or in orbit that's

291
00:18:24,480 --> 00:18:29,080
delivering packages to their door by satellite deployment.

292
00:18:29,080 --> 00:18:31,720
And so I was going on and on with all this pie in this guy stuff.

293
00:18:31,720 --> 00:18:36,240
So I was, I was the guy who was sort of needed to be pulled back, you know, and of course,

294
00:18:36,240 --> 00:18:43,200
I learned a bit in my old age to rein in some of that sort of unrealism in the stories.

295
00:18:43,200 --> 00:18:48,520
And I think it works better because again, the person who's, who's buying, if you will,

296
00:18:48,520 --> 00:18:52,600
the client, who you're trying to sell a product or a service to, you know, they're not

297
00:18:52,600 --> 00:18:56,360
going to buy that bloney and it's, it's snake oil, right?

298
00:18:56,360 --> 00:18:59,800
And so I remember a friend of mine years ago in astronomy wrote a book called, with the

299
00:18:59,800 --> 00:19:00,800
internet first started.

300
00:19:00,800 --> 00:19:03,720
I mean, not the internet per se, but the web first started.

301
00:19:03,720 --> 00:19:07,560
He wrote this book about the, the internet being silicon snake oil, right?

302
00:19:07,560 --> 00:19:11,440
And he, he predicted the demise of the internet.

303
00:19:11,440 --> 00:19:13,000
He said, this will last a couple of years.

304
00:19:13,000 --> 00:19:14,000
It's just a fad.

305
00:19:14,000 --> 00:19:15,000
This is stupid.

306
00:19:15,000 --> 00:19:16,000
It's just snake oil.

307
00:19:16,000 --> 00:19:21,080
Smoking mirrors, and yeah, completely wrong, right?

308
00:19:21,080 --> 00:19:25,640
And so I sort of forgot about that, but a few years ago, I sort of remembered that he wrote

309
00:19:25,640 --> 00:19:26,640
this book, right?

310
00:19:26,640 --> 00:19:32,520
And so I went back to Wikipedia where, and looked up in his page there, and he actually

311
00:19:32,520 --> 00:19:38,720
has a pretty deep and long apology about how stupid and arrogant and naive he was, even

312
00:19:38,720 --> 00:19:44,720
though he was a PhD astrophysicist, he was, I'm like, a lot of PhD folks who don't normally

313
00:19:44,720 --> 00:19:50,200
admit when they're wrong, he was very humble about the fact that he really screwed up

314
00:19:50,200 --> 00:19:52,960
on that prediction.

315
00:19:52,960 --> 00:20:00,760
It's a tough line to walk, seeing, you know, as we do the promise of technologies like

316
00:20:00,760 --> 00:20:05,600
AI and others, but knowing the limitations or at least as you put it, the hard work that

317
00:20:05,600 --> 00:20:11,720
has to go to get there and knowing, you know, being able to kind of project, you know,

318
00:20:11,720 --> 00:20:17,400
how long it's going to take to get to some vision state X, and it's really hard.

319
00:20:17,400 --> 00:20:18,400
Yeah, exactly.

320
00:20:18,400 --> 00:20:22,480
I mean, if you think about like self-driving cars, there's an example where so many things

321
00:20:22,480 --> 00:20:27,160
have to work together, and autonomous vehicle in order for it to not only do what it does,

322
00:20:27,160 --> 00:20:33,000
but do it safely, and not only safely one time, but safely all the time in very different

323
00:20:33,000 --> 00:20:40,960
environments, and so step one, I mean, just get a car to steer straight, okay, get a

324
00:20:40,960 --> 00:20:45,840
car to recognize a stop sign, get a car to turn a corner, get a car to recognize the

325
00:20:45,840 --> 00:20:46,840
speed limit.

326
00:20:46,840 --> 00:20:52,560
I mean, just one little step at a time, and there's like so many steps.

327
00:20:52,560 --> 00:20:57,520
And so I think a lot of implementations and enterprises using AI, machine learning,

328
00:20:57,520 --> 00:21:02,320
automation, whatever you want to call that stuff, intelligent automation, again, requires

329
00:21:02,320 --> 00:21:08,720
a lot of moving parts to get right, and so we need to be more humble, so to speak, in

330
00:21:08,720 --> 00:21:15,760
our way of believing our own story, believing our own promises, that how far can we get

331
00:21:15,760 --> 00:21:21,960
an X amount of time, and I frequently say to people, and I still believe this, that one

332
00:21:21,960 --> 00:21:27,800
should think big, but start small, that is don't think small and start small, because that's

333
00:21:27,800 --> 00:21:35,640
not very useful, but think big, but no way of saying it is think strategically, but

334
00:21:35,640 --> 00:21:40,480
tactically, and my father was Air Force, so I learned some of that language when I was

335
00:21:40,480 --> 00:21:46,600
younger, that strategy is about winning the war, and tactics have to do about theater or

336
00:21:46,600 --> 00:21:50,640
our battle specific, so sometimes you have to lose the battle to win the war, sometimes

337
00:21:50,640 --> 00:21:56,280
you have to give up the hill in order to win the battle, okay, so it's not always about

338
00:21:56,280 --> 00:22:04,040
win, win, win, win, win, it's about the long-term goal, and that long-term goal is what keeps

339
00:22:04,040 --> 00:22:08,680
sure, certainly your North Star keeps your focus going, so you need to have that, but

340
00:22:08,680 --> 00:22:14,040
recognize that there's a lot of steps on the way, those sort of tactical steps, and tactical

341
00:22:14,040 --> 00:22:19,480
failure is, people say failure is not an option, I say strategic failure is not an option,

342
00:22:19,480 --> 00:22:25,800
but tactical failure is how you learn, so sometimes that's called fast fail, I get a lot

343
00:22:25,800 --> 00:22:30,800
of sort of knee jerk negative reactions from my clients when I talk about fast fail,

344
00:22:30,800 --> 00:22:35,440
they say we don't want to fail here, so now I call it just fast learn, okay, so we

345
00:22:35,440 --> 00:22:38,880
want to have a fast learn environment, and the implication is how do you learn, but you

346
00:22:38,880 --> 00:22:44,480
learn from mistakes, from failures, you learn from edge cases that didn't work out, and

347
00:22:44,480 --> 00:22:50,480
so you want to be in a fast learn environment, so that you can do these smaller incremental

348
00:22:50,480 --> 00:22:54,960
steps, and I learned a new expression this week, we used to call those sort of incremental

349
00:22:54,960 --> 00:22:59,640
steps minimal viable products, the MVP minimal viable product, now you're going to talk

350
00:22:59,640 --> 00:23:04,280
about the MLP, yeah, so this week, that was awesome, so this week I learned about the

351
00:23:04,280 --> 00:23:09,440
minimal lovable product, and I said I like that minimal lovable product, and one of the

352
00:23:09,440 --> 00:23:18,400
keynotes here in the world, so I think that's going to be my new thing, MLPs, I didn't

353
00:23:18,400 --> 00:23:22,200
say NLP for people who are listening out there, that's natural language processing,

354
00:23:22,200 --> 00:23:28,560
I said MLP, minimal lovable product, and remember, I mean data like products are like

355
00:23:28,560 --> 00:23:34,240
your children, you love all of them, because even when something goes wrong, you learn

356
00:23:34,240 --> 00:23:38,520
from that, and that's what this is all about, it's about learning.

357
00:23:38,520 --> 00:23:42,760
You mentioned just a moment ago self-driving cars, and I've heard you use a really interesting

358
00:23:42,760 --> 00:23:49,720
analogy between applying the idea of self-driving cars to the enterprise to explain intelligent

359
00:23:49,720 --> 00:23:55,560
automation, the theme of this event, what does the self-driving enterprise mean to you?

360
00:23:55,560 --> 00:24:00,360
To me, it means basically doing what the self-driving car does.

361
00:24:00,360 --> 00:24:06,400
It senses its environment, it sort of sees what's coming, takes an action that's going

362
00:24:06,400 --> 00:24:12,320
to optimize the outcome, and it uses all the contextual data, so what I just described

363
00:24:12,320 --> 00:24:18,360
is descriptive analytics, predictive analytics, prescriptive analytics, and cognitive analytics.

364
00:24:18,360 --> 00:24:22,680
So a self-driving car, it's collecting data, what's going on now, so it's diagnosing

365
00:24:22,680 --> 00:24:25,760
its environment from sensors in the car.

366
00:24:25,760 --> 00:24:29,240
An enterprise, no matter what it is, whether you're customer engagement, your sales, your

367
00:24:29,240 --> 00:24:35,640
marketing campaign, your employee activities, your human resources, anything in your business,

368
00:24:35,640 --> 00:24:40,520
and across your enterprise, whether it's an enterprise specific or a customer-facing

369
00:24:40,520 --> 00:24:44,400
thing, you're collecting data.

370
00:24:44,400 --> 00:24:46,520
Just like a car, you're collecting data.

371
00:24:46,520 --> 00:24:51,680
You got to do more than just collect data from your sensors, you got to do something.

372
00:24:51,680 --> 00:24:53,840
You want to take some action.

373
00:24:53,840 --> 00:24:58,120
So part of your action is to sort of look ahead and see, well, where is the road going?

374
00:24:58,120 --> 00:25:03,080
So let me make sure I stay on the road as I move forward.

375
00:25:03,080 --> 00:25:07,760
So that's the predictive model, so you see what's ahead, so you can move forward in that

376
00:25:07,760 --> 00:25:08,760
direction.

377
00:25:08,760 --> 00:25:14,240
So if you're trying to increase sales and prove customer interaction or whatever, you

378
00:25:14,240 --> 00:25:18,240
sort of see what kind of steps you can take that will move you in that direction.

379
00:25:18,240 --> 00:25:23,200
That's pretty predictive model, but more so, what's prescriptive that is, you can say,

380
00:25:23,200 --> 00:25:25,520
what can I do to optimize the outcome?

381
00:25:25,520 --> 00:25:32,520
So just like a car driving, it says, if I go down this street, my app tells me less traffic,

382
00:25:32,520 --> 00:25:37,000
even though there's more stoplights, normally I wouldn't go that way because it's a longer

383
00:25:37,000 --> 00:25:41,800
drive and a normal low traffic day, but a high traffic day is going to be faster to go

384
00:25:41,800 --> 00:25:43,320
this other way.

385
00:25:43,320 --> 00:25:48,200
So in the same way with customer engagement, you might have a more optimal outcome.

386
00:25:48,200 --> 00:25:53,720
If you take a particular path or a particular road, so to speak, but it's, again, it's

387
00:25:53,720 --> 00:25:58,800
even more than that, and that's the cognitive analytics, the cognitive phase of driving,

388
00:25:58,800 --> 00:26:01,200
which is now you take in all contextual data.

389
00:26:01,200 --> 00:26:05,840
So using the car analogy again, you look at all the data, what's the weather condition?

390
00:26:05,840 --> 00:26:06,840
What's the road condition?

391
00:26:06,840 --> 00:26:09,480
Are there pedestrians on the road?

392
00:26:09,480 --> 00:26:11,480
Are there children playing down the street?

393
00:26:11,480 --> 00:26:13,280
You know, am I in a school zone?

394
00:26:13,280 --> 00:26:14,280
Is it whatever?

395
00:26:14,280 --> 00:26:19,720
And so all this contextual information now informs you how to take next best action.

396
00:26:19,720 --> 00:26:25,320
So cognitive analytics is about next best action, or I like to say next best question.

397
00:26:25,320 --> 00:26:28,200
What is the thing I should be asking of my data?

398
00:26:28,200 --> 00:26:32,360
What kind of things should I be informed about from my data?

399
00:26:32,360 --> 00:26:37,600
And so that's the cognitive phase of the self-driving enterprise is not just doing the

400
00:26:37,600 --> 00:26:43,400
thing that you always do, collecting data, selling products, serving customers.

401
00:26:43,400 --> 00:26:45,000
What is the thing you ought to be doing?

402
00:26:45,000 --> 00:26:49,920
What is the more contextually based thing you should be doing?

403
00:26:49,920 --> 00:26:52,320
That context could be time of day, right?

404
00:26:52,320 --> 00:26:57,160
So let's just say you're building something as simple as a recommender engine to a customer.

405
00:26:57,160 --> 00:27:01,520
What you recommend a particular customer is not always the same, even for the same customer

406
00:27:01,520 --> 00:27:06,080
could depend upon time of day, or day of week, for example, I might be looking at completely

407
00:27:06,080 --> 00:27:10,080
different products online if I'm at home on a weekend, then if I'm at work, or if I'm

408
00:27:10,080 --> 00:27:14,160
on vacation in Vegas, or if I'm at work.

409
00:27:14,160 --> 00:27:21,760
And so context, location, time, those kind of contextual data points sort of change your

410
00:27:21,760 --> 00:27:25,720
action even for the exact same customer.

411
00:27:25,720 --> 00:27:30,520
And so being able to bring in the contextual data makes you more cognitively aware and

412
00:27:30,520 --> 00:27:35,520
able to take those best, next best actions and ask the next best questions.

413
00:27:35,520 --> 00:27:40,280
And so that enables your enterprise, your business to be that self-driving enterprise in

414
00:27:40,280 --> 00:27:46,360
the sense you can start automating more of the processes, automating more of the activities,

415
00:27:46,360 --> 00:27:52,960
not taking human out of the loop, but augmenting the human with the right information and knowledge

416
00:27:52,960 --> 00:27:56,480
and inputs and insights that they need to do their work.

417
00:27:56,480 --> 00:27:59,840
So I like to say AI is no longer about artificial intelligence.

418
00:27:59,840 --> 00:28:03,120
In fact, there's nothing artificial at all about it in my mind.

419
00:28:03,120 --> 00:28:10,240
It's about other types of AI, amplified intelligence, assisted intelligence, accelerated, augmented,

420
00:28:10,240 --> 00:28:15,000
adaptable, and just go on and on and on.

421
00:28:15,000 --> 00:28:20,640
I mean, there's all these interesting ways of thinking about AI, besides artificial.

422
00:28:20,640 --> 00:28:25,880
And so your self-driving enterprise is that one that, in a sense, it's doing for your

423
00:28:25,880 --> 00:28:29,600
enterprise what an autonomous car is doing for a driver.

424
00:28:29,600 --> 00:28:34,520
You don't want to have the driver completely disengaged from the driving experience, I

425
00:28:34,520 --> 00:28:35,520
think.

426
00:28:35,520 --> 00:28:41,960
I think we've learned that from recent incidents that you still need to have a person there.

427
00:28:41,960 --> 00:28:46,640
And so you still need to have a person there in the enterprise, obviously.

428
00:28:46,640 --> 00:28:51,840
So future of work is another completely different dialogue we could be having in this area.

429
00:28:51,840 --> 00:28:54,920
But you did mention a phrase earlier called digital transformation, which is one of the

430
00:28:54,920 --> 00:29:00,040
big themes here at Peggo World this week, and digital transformation includes two words,

431
00:29:00,040 --> 00:29:05,640
right, digital, which means we're looking at digital information, digital data, digital

432
00:29:05,640 --> 00:29:12,200
signals to inform us and to do our self-driving car thing, self-driving enterprise thing.

433
00:29:12,200 --> 00:29:17,440
But there's the other word, their transformation, and transformation means change, and change

434
00:29:17,440 --> 00:29:18,440
means change.

435
00:29:18,440 --> 00:29:19,440
Love.

436
00:29:19,440 --> 00:29:24,840
So jobs will change, career paths will change, what people do will change in the same

437
00:29:24,840 --> 00:29:27,680
way with every industrial revolution.

438
00:29:27,680 --> 00:29:35,760
And so it is happening, and you can't stop it from happening, it would be ridiculous

439
00:29:35,760 --> 00:29:40,720
to stop it from happening, any more than stopping the invention of the printing press or something

440
00:29:40,720 --> 00:29:41,720
like that.

441
00:29:41,720 --> 00:29:42,720
Oh, my gosh.

442
00:29:42,720 --> 00:29:47,120
What are we going to do with all those monks and monasteries whose job it is to transcribe

443
00:29:47,120 --> 00:29:56,880
our copy to write copies, endless copies, beautifully artistic copies of the Bible or

444
00:29:56,880 --> 00:29:57,880
whatever.

445
00:29:57,880 --> 00:30:04,960
So they found some other things to do, obviously, so work changes, the tasks we do change.

446
00:30:04,960 --> 00:30:08,200
And it means transformation.

447
00:30:08,200 --> 00:30:13,960
So it's okay that we are going through a change now because change is good, and it's

448
00:30:13,960 --> 00:30:19,040
growing pains, you might say we're going through the adolescence phase of digital transformation,

449
00:30:19,040 --> 00:30:22,640
so there's a lot of growing pain right there.

450
00:30:22,640 --> 00:30:31,120
Earlier you mentioned you were discussing tactics versus strategy, tactical, the Hill versus

451
00:30:31,120 --> 00:30:33,600
the battle, the battle versus the war.

452
00:30:33,600 --> 00:30:43,480
A lot of what we talk about in kind of applied AI, automated decision making is very

453
00:30:43,480 --> 00:30:49,760
tactical decisions, what's the next offer, what's the next step.

454
00:30:49,760 --> 00:30:56,560
Are you seeing AI, machine learning applied to helping businesses get a handle on the

455
00:30:56,560 --> 00:30:59,320
strategy, the bigger picture?

456
00:30:59,320 --> 00:31:00,520
That's a good question.

457
00:31:00,520 --> 00:31:04,800
I'm not sure where we are in that state.

458
00:31:04,800 --> 00:31:10,480
I see a lot of a lot more discussion on topics that you might label data strategy or analytic

459
00:31:10,480 --> 00:31:11,480
strategy.

460
00:31:11,480 --> 00:31:16,400
And I imagine also AI strategy.

461
00:31:16,400 --> 00:31:24,080
So I think that the idea would be, let's stop and think what are our business goals,

462
00:31:24,080 --> 00:31:26,880
what are our outcomes we're trying to achieve.

463
00:31:26,880 --> 00:31:29,280
So be outcomes driven and not technology driven.

464
00:31:29,280 --> 00:31:34,520
I mean, I sort of cringe when I see people say that their business is data driven, and

465
00:31:34,520 --> 00:31:39,520
I've used that phrase myself, so I've all pointed myself to be guilty there.

466
00:31:39,520 --> 00:31:43,960
They say they're data driven or technology driven, and I'm starting to catch myself

467
00:31:43,960 --> 00:31:44,960
before I say that.

468
00:31:44,960 --> 00:31:50,840
We're data empowered, we're data fueled, data informed, and we're technology powered.

469
00:31:50,840 --> 00:31:52,640
I mean, that's a better way to say.

470
00:31:52,640 --> 00:31:58,120
We're data informed and technology powered, but we need to be product or outcome or

471
00:31:58,120 --> 00:32:00,440
customer driven, driven, driven.

472
00:32:00,440 --> 00:32:01,440
Right.

473
00:32:01,440 --> 00:32:06,720
Well, outcome driven, because a customer's success might be your desired outcome, customer,

474
00:32:06,720 --> 00:32:08,920
and you might say customer sales might be an outcome.

475
00:32:08,920 --> 00:32:11,560
I mean, that's a metric, right?

476
00:32:11,560 --> 00:32:15,160
So when you think about outcomes, you need to think about the metrics to measure whether

477
00:32:15,160 --> 00:32:16,640
you've achieved your outcomes, right?

478
00:32:16,640 --> 00:32:20,360
So that's traditional KPI, that's traditional six sigma, right?

479
00:32:20,360 --> 00:32:23,240
You say what you're going to do, you do it, and then you prove it, right?

480
00:32:23,240 --> 00:32:25,040
That's how I learned six sigma.

481
00:32:25,040 --> 00:32:26,040
And so how do you prove it?

482
00:32:26,040 --> 00:32:30,240
Well, you have some kind of measurement that you've agreed to, that this is the thing we're

483
00:32:30,240 --> 00:32:35,640
going to capture and measure to demonstrate whether or not we've achieved the outcome.

484
00:32:35,640 --> 00:32:43,320
And so the outcome is customer sale is a metric, I would say customer success, customer

485
00:32:43,320 --> 00:32:48,360
loyalty might be outcomes that you're, that's the big goal, right?

486
00:32:48,360 --> 00:32:50,920
And so you sell a product to a customer, that's a tactic, right?

487
00:32:50,920 --> 00:32:56,200
And so I make a recommendation, the person bought the product, yay, hooray for us, but

488
00:32:56,200 --> 00:32:57,200
is that a loyal customer?

489
00:32:57,200 --> 00:32:59,840
Are they going to come back and buy more from us?

490
00:32:59,840 --> 00:33:03,440
And so sometimes, like I said, you've got to sort of lose the battle to win the war.

491
00:33:03,440 --> 00:33:09,360
So this is why companies offer things like discount coupons and premium type products where

492
00:33:09,360 --> 00:33:10,960
they give you something for free.

493
00:33:10,960 --> 00:33:16,120
Hopefully later you'll buy the more premium plan when you have the sort of the low, the

494
00:33:16,120 --> 00:33:21,480
zero cost plan, which has a fewer bells and whistles, fewer services than the full premium

495
00:33:21,480 --> 00:33:22,800
plan.

496
00:33:22,800 --> 00:33:26,440
But if you really like what you get to see there, you're willing to pay more.

497
00:33:26,440 --> 00:33:31,880
And so the company is willing to take that loss in order to win the bigger, the bigger

498
00:33:31,880 --> 00:33:35,640
war that is going to gain a loyal customer in the end.

499
00:33:35,640 --> 00:33:39,840
So yes, so if you looked at the bottom line, you say, well, we lost money today because

500
00:33:39,840 --> 00:33:44,840
we gave away all this stuff at 20% off and, you know, we have a 15% margin in our company,

501
00:33:44,840 --> 00:33:46,600
so we're doing some money today.

502
00:33:46,600 --> 00:33:50,200
But in the long run, you've gained lifetime customers, loyal customers.

503
00:33:50,200 --> 00:33:54,680
And that, that's really the bigger picture, the bigger, the bigger goal.

504
00:33:54,680 --> 00:33:55,680
And so strategy intact.

505
00:33:55,680 --> 00:34:00,040
I mean, some people interchange those words and not here to argue semantics, but I'm really

506
00:34:00,040 --> 00:34:06,960
arguing about, think about sort of long-term goals versus short-term metrics and accomplishments.

507
00:34:06,960 --> 00:34:13,040
And sometimes that, the negative step backward leads to a bigger step forward later, and

508
00:34:13,040 --> 00:34:14,560
that's okay.

509
00:34:14,560 --> 00:34:20,200
That brings to mind for me, and in fact, this came up in an interview earlier today,

510
00:34:20,200 --> 00:34:26,840
the notion of architecting the, you know, our optimization functions, our reward functions

511
00:34:26,840 --> 00:34:33,320
to, as you put it here, better reflect the outcomes as opposed to the individual transactions.

512
00:34:33,320 --> 00:34:39,000
What kind of progress are you seeing towards data scientists being able to capture a more

513
00:34:39,000 --> 00:34:43,280
holistic view of the, the business strategy and the business outcome in the way that they

514
00:34:43,280 --> 00:34:48,200
optimize, in the way that they build, you know, machine learning algorithms and AI systems

515
00:34:48,200 --> 00:34:55,640
to optimize towards those, and where do you, what's your sense for where we are in the

516
00:34:55,640 --> 00:34:59,960
maturity curve and how we get to, you know, the next best thing?

517
00:34:59,960 --> 00:35:06,080
Well, I think we're in a much better place than just a few years ago, in what sense?

518
00:35:06,080 --> 00:35:14,440
And I mean that primarily in the access to more data, but there's really sort of three

519
00:35:14,440 --> 00:35:18,240
sort of, again, using the word confluence in a sentence, there's sort of three things

520
00:35:18,240 --> 00:35:24,560
that are, technologies that are merging, large amounts of data, that is, sensor technologies

521
00:35:24,560 --> 00:35:30,440
for collecting data on just about every process, person, thing, and enterprises and homes

522
00:35:30,440 --> 00:35:33,200
and cars in the universe.

523
00:35:33,200 --> 00:35:38,600
There's also faster, better, more powerful algorithms, so lots of development and algorithms

524
00:35:38,600 --> 00:35:42,080
for detecting patterns and trends and behaviors and data.

525
00:35:42,080 --> 00:35:46,040
And then the third one is access to high performance computing.

526
00:35:46,040 --> 00:35:50,160
So yes, we've had HPC high performance computing for many years, but you have to buy a super

527
00:35:50,160 --> 00:35:53,040
computer to have access to a super computer.

528
00:35:53,040 --> 00:35:55,400
Now you can rent one on the cloud, right?

529
00:35:55,400 --> 00:35:59,960
So you basically can rent as many CPUs as you need from a cloud service provider for

530
00:35:59,960 --> 00:36:02,880
the two minutes or five minutes that you need it and then give it back.

531
00:36:02,880 --> 00:36:05,800
So all you've paid for is those couple of minutes.

532
00:36:05,800 --> 00:36:10,600
And it's the cloud services provider's job to buy the hardware, to maintain the hardware,

533
00:36:10,600 --> 00:36:16,880
to upgrade the hardware and all these things, which most, and past days, I remember working

534
00:36:16,880 --> 00:36:20,760
in institutions where they didn't want to make that expense because they knew that within

535
00:36:20,760 --> 00:36:24,560
five years it would be obsolete and what a huge capital investment that would be and wouldn't

536
00:36:24,560 --> 00:36:29,480
it be better if we invested our of money and XYZ, other direction, like hiring more staff

537
00:36:29,480 --> 00:36:35,120
or more funding more students or doing whatever, yeah, we all agree, ultimately, no, don't

538
00:36:35,120 --> 00:36:40,680
go buy the big super computer because it'll be buddy down the drain five years or no.

539
00:36:40,680 --> 00:36:45,240
And so we got powerful computing, we got powerful algorithms, we got lots of data.

540
00:36:45,240 --> 00:36:46,240
So what does that buy us?

541
00:36:46,240 --> 00:36:51,560
It buys us insights and the ability to derive insights from data.

542
00:36:51,560 --> 00:36:55,520
And so when you're talking about optimizing a function, if a function is multi-dimensional,

543
00:36:55,520 --> 00:37:00,120
which in this case, I would say it is, there are many, many factors that determine the

544
00:37:00,120 --> 00:37:05,760
optimal, something, right, optimal customer experience, optimal sales, optimal, whatever.

545
00:37:05,760 --> 00:37:10,000
I mean, no matter what it is, optimal performance in a manufacturing plant, optimal supply chain,

546
00:37:10,000 --> 00:37:11,760
think about the traveling salesman problem.

547
00:37:11,760 --> 00:37:17,040
I mean, it's like in factorial, which is a very large number for a traveling salesman

548
00:37:17,040 --> 00:37:19,440
who's going to end different stops, right?

549
00:37:19,440 --> 00:37:24,880
And so this is a challenge problem that's quantum machine learning is addressing.

550
00:37:24,880 --> 00:37:31,120
How can you do a faster, much faster solution to the traveling salesman problem, which

551
00:37:31,120 --> 00:37:37,480
is basically optimal routing, whether it's for any kind of routing, whether it's shipping,

552
00:37:37,480 --> 00:37:44,040
industry, logistics in the military, or whatever, a network traffic, always looking for optimal

553
00:37:44,040 --> 00:37:45,040
routing.

554
00:37:45,040 --> 00:37:51,120
Okay, so how do you solve an in-factoid problem, one that basically is grow so fast, there's

555
00:37:51,120 --> 00:37:55,280
enough computing power on the planet and the universe to solve the problem.

556
00:37:55,280 --> 00:37:57,240
And that is you have more data.

557
00:37:57,240 --> 00:38:03,320
So the data from all these sensors gives us essentially a map of our, if you will, an

558
00:38:03,320 --> 00:38:10,400
indimensional map of the output variable, and then I'll just pick a number, let's just

559
00:38:10,400 --> 00:38:11,400
say revenue.

560
00:38:11,400 --> 00:38:15,840
Okay, let's just say that's our, if revenue is our thing we're trying to maximize, there's

561
00:38:15,840 --> 00:38:16,840
all kinds of factors.

562
00:38:16,840 --> 00:38:22,920
So we can look at all these different conditions and factors and activities and see which

563
00:38:22,920 --> 00:38:26,200
ones lead to the maximum of that function.

564
00:38:26,200 --> 00:38:33,040
And so you no longer have to do complex modeling per se, you can, but the data becomes

565
00:38:33,040 --> 00:38:36,320
the model, because you know, have enough data that the data now tells you, this is how

566
00:38:36,320 --> 00:38:41,760
the system responds, and in the case of say marketing campaigns, right, I heard a story

567
00:38:41,760 --> 00:38:46,880
once years ago that eBay had, they did, like, A.B. testing on their website, you know,

568
00:38:46,880 --> 00:38:50,320
like changing the fonts and changing the colors and changing the locations of things on

569
00:38:50,320 --> 00:38:56,560
the page, they did 10 million A.B. tests every single day.

570
00:38:56,560 --> 00:38:59,760
Every single day, they've moved things around and changed things, changed colors, changed

571
00:38:59,760 --> 00:39:02,960
fonts, changed locations, changed the sizes of the pictures, et cetera.

572
00:39:02,960 --> 00:39:08,400
And at that point, you don't need any kind of model of customer behavior.

573
00:39:08,400 --> 00:39:12,320
You just look at the data, say, this is what works, let's go.

574
00:39:12,320 --> 00:39:19,920
And so I think the, the ability to just use all the different data sources we now have,

575
00:39:19,920 --> 00:39:26,840
and again, if we're going back to the customer story, we got sales data, we got customer

576
00:39:26,840 --> 00:39:31,760
call center data from that customer, we have return data, you know, we have all kinds

577
00:39:31,760 --> 00:39:38,320
of data, customer care data, customer interest data, you know, product history purchased

578
00:39:38,320 --> 00:39:41,000
data, all kinds of information about that customer.

579
00:39:41,000 --> 00:39:45,760
So we can now figure out how to optimize our interaction with that customer based upon

580
00:39:45,760 --> 00:39:50,360
the data as opposed to what we would do in the past with some kind of modeling, right?

581
00:39:50,360 --> 00:39:54,120
So okay, I'm a white male or 50 and might live in a certain zip code, therefore every

582
00:39:54,120 --> 00:39:58,160
white male who lives in my zip code over the 50 must like the same things.

583
00:39:58,160 --> 00:40:01,240
And every morning I walk up my front door on my house, I know that isn't true because

584
00:40:01,240 --> 00:40:05,160
the guy next door to me has a Pittsburgh dealer banner hanging out front of his house,

585
00:40:05,160 --> 00:40:06,840
and I'm a Baltimore Raven thing.

586
00:40:06,840 --> 00:40:12,480
Okay, and so I mean, I'm immediately informed that no, it is not true that every single

587
00:40:12,480 --> 00:40:16,920
person in my demographic likes the same thing, I mean, and of course we know that now,

588
00:40:16,920 --> 00:40:17,920
right?

589
00:40:17,920 --> 00:40:21,480
So I say the big data era represents the end of demographics, we're no longer like

590
00:40:21,480 --> 00:40:29,440
using these limited biased variables to determine outcomes or marketing campaigns or offers

591
00:40:29,440 --> 00:40:35,120
or whatever, we just look at the data and say, what does this individual prefer like desire

592
00:40:35,120 --> 00:40:39,160
and serve them for who they are and what they like and desire?

593
00:40:39,160 --> 00:40:44,240
And again, that's our optimal optimization of customer experience, which leads to optimization

594
00:40:44,240 --> 00:40:48,040
hopefully a revenues is driven by data.

595
00:40:48,040 --> 00:40:53,480
And at the end of the day, that's what I say, it's all about data, therefore digital transformation

596
00:40:53,480 --> 00:40:54,480
is happening.

597
00:40:54,480 --> 00:40:59,720
It's not the gutted instinct anymore that drives your decisions as a business, it's the

598
00:40:59,720 --> 00:41:00,720
data.

599
00:41:00,720 --> 00:41:01,720
Awesome.

600
00:41:01,720 --> 00:41:07,240
Well, that sounds like a great note to close on any parting thoughts before we push the

601
00:41:07,240 --> 00:41:08,240
button.

602
00:41:08,240 --> 00:41:09,240
No, I think this is great.

603
00:41:09,240 --> 00:41:10,240
I thank you so much.

604
00:41:10,240 --> 00:41:11,760
Sam really enjoyed the conversation today.

605
00:41:11,760 --> 00:41:12,760
Same here.

606
00:41:12,760 --> 00:41:13,760
Thanks for.

607
00:41:13,760 --> 00:41:20,080
All right, everyone, that's our show for today.

608
00:41:20,080 --> 00:41:25,520
For more information on Kirk or any of the topics covered in this episode, head on over

609
00:41:25,520 --> 00:41:30,520
to twimmelai.com slash talk slash 151.

610
00:41:30,520 --> 00:41:38,520
To follow along with the Pegaworld series, visit twimmelai.com slash Pegaworld 2018.

611
00:41:38,520 --> 00:41:44,600
For more information on Pegasystems or their new Pegat Infinity offering, visit pegat.com

612
00:41:44,600 --> 00:41:46,840
slash infinity.

613
00:41:46,840 --> 00:41:50,360
As always, thanks so much for listening and catch you next time.

