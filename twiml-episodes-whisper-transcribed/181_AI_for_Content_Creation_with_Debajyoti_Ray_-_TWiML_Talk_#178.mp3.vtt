WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.760
I'm your host Sam Charrington.

00:32.760 --> 00:38.440
In today's episode we're joined by Deba Jotire, founder and CEO of Rivet AI, a startup

00:38.440 --> 00:42.440
producing AI-powered tools for storytellers and filmmakers.

00:42.440 --> 00:46.400
Rivet's tools are inspired and part by the founder's collaboration with the team that created

00:46.400 --> 00:51.800
SunSpring, a short AI-written film starring Silicon Valley's Thomas Middleditch, which

00:51.800 --> 00:57.600
you may have seen when it was making the rounds a while back.

00:57.600 --> 01:03.040
Deba and I discussed some of what he's learned in the journey to apply AI to content creation,

01:03.040 --> 01:07.720
including how Rivet approaches the use of machine learning to automate creative processes,

01:07.720 --> 01:13.160
the company's use of hierarchical LSTM models and auto encoders, and the tech stack that

01:13.160 --> 01:16.840
they've put in place to support the business.

01:16.840 --> 01:20.640
Before we dive in, a couple of meetup related updates.

01:20.640 --> 01:25.680
First off, great news for those of you who missed the first round of our fast AI deep learning

01:25.680 --> 01:27.760
for coder's study group.

01:27.760 --> 01:32.720
Sebastian Ween, who participated in the first study group, has stepped up to lead a second

01:32.720 --> 01:35.320
group through the Part 1 course.

01:35.320 --> 01:40.600
Next, if you missed the announcements on email and Twitter, we recently launched a new

01:40.600 --> 01:46.040
online meetup group that's much more conveniently timed for folks in Europe, the Middle East,

01:46.040 --> 01:47.360
and Africa.

01:47.360 --> 01:52.160
This group is being led by Kai Lichtenberg, who delivered a great presentation on capsule

01:52.160 --> 01:56.920
networks earlier this week for that meetups inaugural session.

01:56.920 --> 02:01.480
To learn more or join either of these groups, please sign up for the TwimmelOnline meetup

02:01.480 --> 02:04.880
at twimmelai.com slash meetup.

02:04.880 --> 02:10.400
All right, on to the show.

02:10.400 --> 02:11.400
All right, everyone.

02:11.400 --> 02:14.160
I am on the line with David Jotie Ray.

02:14.160 --> 02:17.480
Dave is founder and CEO of Rivet AI.

02:17.480 --> 02:20.640
Dave, welcome to this week in machine learning and AI.

02:20.640 --> 02:23.520
Thank you very much for having me, Sam.

02:23.520 --> 02:30.120
So Dave, you studied math as an undergrad at the University of Toronto and actually got

02:30.120 --> 02:32.920
a chance to work in Jeff Hinton's lab.

02:32.920 --> 02:37.920
Tell us a little bit about that and how your career evolved from that point.

02:37.920 --> 02:46.200
Yeah, so I was a math major, math specialist at the University of Toronto, and at the same

02:46.200 --> 02:51.200
time, I was really interested in programming and coding.

02:51.200 --> 02:55.480
So I wanted to combine both of my passions.

02:55.480 --> 03:03.560
And University of Toronto, Jeff Hinton was one of the luminaries in the field, so really

03:03.560 --> 03:08.480
what drew me to the University and to math in particular is I wanted to explore the field

03:08.480 --> 03:09.880
of AI.

03:09.880 --> 03:15.640
And in my first year, I actually convinced Jeff Hinton to allow me to sit in one of his

03:15.640 --> 03:16.640
classes.

03:16.640 --> 03:25.720
And then at the end of the summer, I offered to, I offered my programming services as an

03:25.720 --> 03:28.160
undergrad researcher.

03:28.160 --> 03:32.880
And I was really fortunate when they allowed me to do some of the projects.

03:32.880 --> 03:39.240
But I got a taste of machine learning since my undergrad days itself, working with Jeff

03:39.240 --> 03:47.800
Hinton and other researchers in this lab, especially in the field of Bayesian neural networks.

03:47.800 --> 03:53.320
And I had several publications coming out of it during my undergrad.

03:53.320 --> 03:59.080
After that, I wanted to explore the field of computational neuroscience and dive into

03:59.080 --> 04:03.200
neuroscience a little bit more, so I moved to London.

04:03.200 --> 04:07.560
But at that time, there was kind of a boom in the financial sector.

04:07.560 --> 04:14.760
So I made a digression into more hedge funds and quantitative finance.

04:14.760 --> 04:24.800
But in 2008, I also discovered that I wanted to go back to academia and explore my interests

04:24.800 --> 04:32.200
in deep learning and machine learning, again, armed with a new set of applications.

04:32.200 --> 04:40.360
So I went back to grad school, went to Caltech in the Computation and Neural Systems Group.

04:40.360 --> 04:48.880
And there, my focus was more around generative models, because one of the advantages of deep

04:48.880 --> 04:58.600
learning models is it can capture very nuanced statistical behaviors, very complex statistical

04:58.600 --> 04:59.600
behaviors.

04:59.600 --> 05:05.640
So I wanted to see if that could be used to, as a generative model, to study different

05:05.640 --> 05:09.640
types of behaviors, especially behavioral economics.

05:09.640 --> 05:15.200
So that's something that I pursued during my PhD at Caltech.

05:15.200 --> 05:20.960
But more generally looking at generative models for deep learning.

05:20.960 --> 05:29.640
So from there, while you did start a couple of companies between your grad school experience

05:29.640 --> 05:37.960
and starting rivet, there is a connection in that you were working on generative models

05:37.960 --> 05:38.960
there.

05:38.960 --> 05:45.120
And now you're currently working on applying AI to the idea of content creation.

05:45.120 --> 05:50.240
And maybe tell us a little bit about the inspiration for starting rivet, what drove you to look

05:50.240 --> 05:52.240
at that problem.

05:52.240 --> 05:59.440
So from the business point of view, previously, my previous startup was a video amp, which

05:59.440 --> 06:02.560
was in the video advertising space.

06:02.560 --> 06:08.400
It was looking at different behaviors, consumer behavior, and finding the best kind of video

06:08.400 --> 06:10.800
ads to target to people.

06:10.800 --> 06:16.760
But what I saw there is advertising kind of sits on top of content.

06:16.760 --> 06:23.400
If you don't have good content, then you don't get the kinds of engagements.

06:23.400 --> 06:28.920
That's one of the reasons why companies don't get as much ROI out of their video advertising

06:28.920 --> 06:30.400
spent.

06:30.400 --> 06:36.840
And the other thing that I saw is the shift that more and more companies wanted to put their

06:36.840 --> 06:42.800
money instead of putting into video advertising, they wanted to put it into sponsored content.

06:42.800 --> 06:50.040
Because at the end of the day, consumers, humans in general, we resonate much more to stories.

06:50.040 --> 06:53.720
So an ad is basically grabbing attention.

06:53.720 --> 07:00.880
It's 30 seconds to grab somebody's attention and maybe create Pavlovian responses of the

07:00.880 --> 07:03.320
consumer will go and buy something.

07:03.320 --> 07:12.000
But what really resonates with us forms culture, forms behavior is story creation, story-making.

07:12.000 --> 07:18.760
So there are a couple of, in terms of my own interests, I have been very interested in

07:18.760 --> 07:20.920
natural language generation.

07:20.920 --> 07:25.000
So when we look at generative models, especially generative models for language, I wanted to

07:25.000 --> 07:29.520
see how that could be applied to generate language.

07:29.520 --> 07:34.640
We start with a training set, but you also feed a bunch of different parameters.

07:34.640 --> 07:39.760
So can you actually produce narrative structures?

07:39.760 --> 07:46.040
Maybe even stories someday, right, could a model generate a story.

07:46.040 --> 07:49.320
So that was my general kind of research interest.

07:49.320 --> 07:56.560
And also at the same time, noticing this kind of shift in spend by companies from videos

07:56.560 --> 08:03.120
to content, the technology, especially the deep learning technology, was pretty ripe

08:03.120 --> 08:08.800
to be able to analyze content because the tech has matured to the point where you can

08:08.800 --> 08:12.720
analyze videos, you can analyze audios.

08:12.720 --> 08:19.280
These things are not really very possible a few years back, because it's really the perfect

08:19.280 --> 08:24.880
like confluence of tech maturity and the business moving in that direction.

08:24.880 --> 08:31.320
And also it's my personal passion, which is what led me to start Rivet AI.

08:31.320 --> 08:40.880
Maybe as a touchstone for folks, we can reference a project that folks may have come across.

08:40.880 --> 08:42.200
This is SunSpring.

08:42.200 --> 08:51.200
It was a short film that star Thomas Middleditch, who's the star of Silicon Valley.

08:51.200 --> 08:55.800
And it was written by an AI, basically an LSTM.

08:55.800 --> 09:03.880
Tell us about that project and some of the ways you've worked with the team behind that

09:03.880 --> 09:04.880
sense.

09:04.880 --> 09:10.560
Yeah, so the company EndQ produced SunSpring.

09:10.560 --> 09:17.760
And when I was making my transition from video advertising to content, I teamed up with

09:17.760 --> 09:24.720
EndQ because EndQ is a production company that makes feature films, short films, animation

09:24.720 --> 09:26.440
films, and so on.

09:26.440 --> 09:30.280
And really, who creates the best content?

09:30.280 --> 09:31.680
It's Hollywood.

09:31.680 --> 09:36.800
So I wanted to partner up with the folks who really know how to tell good stories, who

09:36.800 --> 09:40.080
know how to create great content.

09:40.080 --> 09:46.560
And EndQ was really the perfect partner because one of the principles of EndQ is a Caltech

09:46.560 --> 09:49.320
trustee, Walter Corkcheck.

09:49.320 --> 09:55.920
And he's also funded projects in AI and at Caltech.

09:55.920 --> 10:03.360
So that's how we had a meeting of minds and wanted to explore the ideas even further.

10:03.360 --> 10:12.920
SunSpring was a project where the tech behind it was essentially a very simple LSTM.

10:12.920 --> 10:17.680
So it's kind of like the same predictive text that you have from your phone.

10:17.680 --> 10:24.920
So imagine you created a story by taking every predictive response from your phone.

10:24.920 --> 10:29.480
So SunSpring was the seed word.

10:29.480 --> 10:33.120
And then you start with SunSpring and imagine putting that into your phone, but something

10:33.120 --> 10:34.640
more complex, obviously.

10:34.640 --> 10:41.640
Imagine your phone's predictive response is now trained on movie scripts and has a little

10:41.640 --> 10:45.600
bit more of a complex LSTM, long short-term memory model.

10:45.600 --> 10:53.600
And you started using very simple, maximum likelihood estimation and picked every word that

10:53.600 --> 10:57.880
came next, every next word that your phone suggested.

10:57.880 --> 10:59.720
So that's how that script was made.

10:59.720 --> 11:07.760
It was very simple, LSTM trained on sci-fi movie scripts and then just maximum likelihood

11:07.760 --> 11:09.160
estimation.

11:09.160 --> 11:17.520
So from there, the real kind of genius in that project was the director Oscar Sharp

11:17.520 --> 11:23.720
was able to take a script like that and turn it into a very interesting watchable movie.

11:23.720 --> 11:27.320
Interesting and watchable, but not necessarily intelligible.

11:27.320 --> 11:30.320
Exactly.

11:30.320 --> 11:37.680
And really kudos to the actors and the director for making something out of it.

11:37.680 --> 11:45.800
But it was a very interesting experiment that very few people would be willing to do.

11:45.800 --> 11:50.800
And so that experiment was like, what if we could actually take some model like that and

11:50.800 --> 11:51.800
turn it into a movie?

11:51.800 --> 11:53.360
What would that look like?

11:53.360 --> 12:00.480
And the response has been very kind of a bi-model in the sense that some people love

12:00.480 --> 12:02.880
it and love the idea of what it represents.

12:02.880 --> 12:07.120
On the other hand, there's hate.

12:07.120 --> 12:11.760
So whenever you get a bi-model response, but lots of responses, that's when you know

12:11.760 --> 12:15.080
you're onto something and that you have to pursue that.

12:15.080 --> 12:22.080
So we made several iterations, especially with RiverDI, we made several iterations on

12:22.080 --> 12:29.880
top of the models before and we came up with the next generation of AI written content

12:29.880 --> 12:30.880
after that.

12:30.880 --> 12:38.920
But also our philosophy changed quite a lot, which was SunSpring where it was completely

12:38.920 --> 12:40.440
unsupervised.

12:40.440 --> 12:47.160
You know, it's completely unsupervised, maximum likelihood estimation, let's create a script.

12:47.160 --> 12:50.440
And there's absolutely no direction involved.

12:50.440 --> 12:55.920
Whereas really, if you're telling a story and the story has to make sense to an audience,

12:55.920 --> 12:59.560
then two ingredients are super important.

12:59.560 --> 13:06.680
One is knowing the preference functions, right, of who your list or who your audiences.

13:06.680 --> 13:10.640
It could be in the form of general audience data or it could be in the form of something

13:10.640 --> 13:16.400
that interacts with the director and tries to learn their preferences.

13:16.400 --> 13:24.080
And the second part is some sort of physical or even a cultural embodiment that AI program

13:24.080 --> 13:31.080
completely lacks, so imagine just an alien showing up or worse and trying to just generate

13:31.080 --> 13:36.640
something with no idea of what the context is at all.

13:36.640 --> 13:43.400
So what we focused on in our next set of models was to really put a sense of context in that

13:43.400 --> 13:48.800
and also learn preference functions, whether it's a human interaction to a human interaction

13:48.800 --> 13:53.000
with the director or writer or audience responses.

13:53.000 --> 13:57.120
And so that kind of evolved into your next set of models.

13:57.120 --> 14:04.640
And our philosophy now is not so much AI-written everything or AI-autonomously generating content

14:04.640 --> 14:13.160
or AI-autonomously writing stories, but AI-augmenting creativity in a couple of different ways.

14:13.160 --> 14:19.280
One is AI-augmenting creativity because a lot of the creative process for those who are

14:19.280 --> 14:25.600
actually engaged in making content, a lot of the process is very tedious.

14:25.600 --> 14:36.120
95% of the time goes into the tedious stuff and only 5% on inspiring creative works.

14:36.120 --> 14:42.600
So our tech is more focused on augmenting the creative aspects and trying to where possible

14:42.600 --> 14:46.240
take away a lot of the or automating a lot of the tedious aspects.

14:46.240 --> 14:53.720
Can you give us some examples of where the TDM comes into play in this particular use case

14:53.720 --> 14:56.920
with this kind of content creation?

14:56.920 --> 14:57.920
Right.

14:57.920 --> 15:04.760
So, you know, for listeners who are not in the content creation process or filmmaking,

15:04.760 --> 15:05.760
what happened?

15:05.760 --> 15:11.720
The typical flow is, you know, you have a story, a script, everything kind of starts there.

15:11.720 --> 15:15.000
And of course, there's a whole lot that goes into making a script.

15:15.000 --> 15:17.760
But let's, as you may start with the script.

15:17.760 --> 15:24.280
Now the first decision is development, like whether you actually decide to work on the

15:24.280 --> 15:25.280
script or not.

15:25.280 --> 15:29.440
So, usually a studio or a production company will get lots and lots of scripts and they

15:29.440 --> 15:34.840
have to evaluate whether they want to produce it or not.

15:34.840 --> 15:42.800
So tip, usually in the industry, it's kind of surprising how, you know, even when you

15:42.800 --> 15:48.640
have these amazingly huge budgets, there's not a whole lot of data that goes into that

15:48.640 --> 15:49.640
decision making.

15:49.640 --> 15:53.440
So, there are different ways you can incorporate data into that decision whether you want

15:53.440 --> 15:58.800
to go with the script or not, whether you want to analyze that script, make some predictions

15:58.800 --> 16:02.160
of how well it would perform and so on.

16:02.160 --> 16:06.120
So let's say you decide to go with the script.

16:06.120 --> 16:13.640
Now the first point is, you know, first step involves a script breakdown.

16:13.640 --> 16:19.640
So a script is essentially just words on a piece of paper, but they need to be depicted

16:19.640 --> 16:23.280
physically or, you know, through an animation.

16:23.280 --> 16:27.600
So it has to go from words to actual physical representation.

16:27.600 --> 16:30.280
So that is a very tedious process.

16:30.280 --> 16:36.960
You know, you have a 200 page script, what happens now is someone goes in with a marker and

16:36.960 --> 16:43.960
starts going through the script word by word, saying, okay, this is, this here is a character,

16:43.960 --> 16:51.400
the character says this, so we do need a person here in a speaking role or it could be,

16:51.400 --> 16:55.160
and they're sitting at a coffee shop talking about this.

16:55.160 --> 17:02.440
So you need to have a set with a coffee shop and the lead actor, he takes a sit from a cup.

17:02.440 --> 17:04.760
So you need to represent that cup as a prop.

17:04.760 --> 17:09.480
So there's so many elements that goes from telling the story to actually putting it in

17:09.480 --> 17:10.800
production.

17:10.800 --> 17:18.160
So that is something that piece itself is hugely time consuming several weeks often, but

17:18.160 --> 17:23.800
that's something that we can solve with natural language understanding because we can,

17:23.800 --> 17:31.480
with a training corpus of previously annotated scripts, we can identify where characters need

17:31.480 --> 17:39.240
to be represented, where props are required, what kind of sets are involved, set dressing.

17:39.240 --> 17:44.000
You know, if you need a vehicle, you need an animal in that screen, an animal handler.

17:44.000 --> 17:48.880
So all of those, like that script breakdown process can be automated, but there needs

17:48.880 --> 17:56.560
to be a better set of training data for that because films always involve analogies,

17:56.560 --> 18:01.120
so you can have a sentence like buzzing like bees, so you don't actually have to have a bee

18:01.120 --> 18:02.720
in the scene, just an analogy.

18:02.720 --> 18:09.360
So those are things that kind of the AI needs to understand.

18:09.360 --> 18:15.320
It sounds like a very specialized, named entity resolution type of problem.

18:15.320 --> 18:21.880
Yes, we've named entity resolution that understands analogies, or where analogies are coming.

18:21.880 --> 18:26.320
So you have to have a context understanding there.

18:26.320 --> 18:33.120
So that's a huge piece that can be automated, like going from like few weeks to just a

18:33.120 --> 18:36.960
few clicks of a button and a few minutes of a script breakdown.

18:36.960 --> 18:42.000
And then from there, you know, the next phase is once you've got, okay, I need for like

18:42.000 --> 18:49.880
over 200 scenes, every scene, I need these, these parts and this actor and this location,

18:49.880 --> 18:54.000
then it's kind of a scheduling problem, a budgeting problem.

18:54.000 --> 18:59.000
So even the first phase, which is a budget approximation, that again is a hugely time consuming

18:59.000 --> 19:07.040
bees, but since we have all these entities extracted and we have training data from budgets

19:07.040 --> 19:11.960
and how these entities map to different budget estimates, we can construct budget estimates

19:11.960 --> 19:14.640
very quickly.

19:14.640 --> 19:19.840
Same thing with schedules, you know, you have so many parameters, you have certain location

19:19.840 --> 19:27.280
where a film needs to be shot and this actor who's extremely, who's time is extremely expensive.

19:27.280 --> 19:34.560
So how do you combine these to get the best schedules in terms of both cost and time effectiveness?

19:34.560 --> 19:38.320
There are billions of parameters to deal with.

19:38.320 --> 19:41.960
That is often done manually in that industry surprisingly.

19:41.960 --> 19:47.800
So just automating a lot of that, just bringing in AI scheduling tools just, you know, gives

19:47.800 --> 19:56.160
you 10-50 percent efficiencies right off the bat, which for these $100 million budget

19:56.160 --> 20:00.120
productions, you know, are substantial savings.

20:00.120 --> 20:08.920
So those are the tedious parts of this content creation where AI can come in and automate

20:08.920 --> 20:12.800
and provide, you know, budget and time savings.

20:12.800 --> 20:23.320
Do you remain involved in the creative content generation side as well or have you mostly,

20:23.320 --> 20:30.800
I think of the various things that you've described as content support, but more on the analysis

20:30.800 --> 20:34.400
side, if you will, do you tend to focus on that as a company now?

20:34.400 --> 20:41.640
Yeah, so we have also been bringing in different kind of tools, but slowly.

20:41.640 --> 20:49.000
So first, it's kind of, whenever we try to talk about AI in the creative industry or content

20:49.000 --> 20:52.840
creation industry, there's a bit of resistance initially.

20:52.840 --> 20:57.760
So it's also a different strategy for kind of entering the market.

20:57.760 --> 21:04.200
So starting with analysis, starting with things that save time and effort, you know, builds

21:04.200 --> 21:12.400
more comfort in allowing people to understand that, yes, AI tools can actually help my work.

21:12.400 --> 21:16.760
You know, the analogy that I draw is AutoCAD, right?

21:16.760 --> 21:22.920
You had architects who, you know, where it was always considered to be a very creative

21:22.920 --> 21:23.920
endeavor.

21:23.920 --> 21:26.360
Nobody touched my clay models.

21:26.360 --> 21:32.280
But then AutoCAD came in and took away a lot of the tedious aspects of architecture.

21:32.280 --> 21:40.520
And now it's hard to imagine, you know, creating these complex architectural models without

21:40.520 --> 21:43.840
using computers and AI tools.

21:43.840 --> 21:50.960
So the same thing, you know, can happen in the field of content creation, but incrementally.

21:50.960 --> 21:58.880
So one of the, you know, features we've added in is things that make the writing better,

21:58.880 --> 22:01.360
but in the sense of, hey, can we make it more readable?

22:01.360 --> 22:03.800
Can we make, you know, improve readability?

22:03.800 --> 22:05.960
Can we improve audience appeals?

22:05.960 --> 22:12.520
So for example, I'm thinking like, I'm thinking like Grammarly for script writers.

22:12.520 --> 22:20.960
Yeah, I mean, Grammarly, you know, improves, it's targeted to a very broad audience.

22:20.960 --> 22:27.520
So imagine for something that where people are creating content where you can take something

22:27.520 --> 22:31.520
that was, let's say, readability levels 11.

22:31.520 --> 22:37.760
And if you turn that into readability level 5 or 6, then you can show that it'll appeal

22:37.760 --> 22:43.240
to a much broader audience while keeping the fundamental idea, the story, everything

22:43.240 --> 22:44.240
the same.

22:44.240 --> 22:47.760
You're just kind of changing some of the words, right?

22:47.760 --> 22:51.960
And simplifying the sentence structures, but all of a sudden you've reached a much wider

22:51.960 --> 22:52.960
audience.

22:52.960 --> 22:59.760
So that's an example of how you can augment the writing, augment the creation.

22:59.760 --> 23:06.680
The other pieces are, you know, with the short films that we have created where the dialogues

23:06.680 --> 23:13.240
are actually generated or some of the AI characters, dialogues are generated with natural

23:13.240 --> 23:14.920
language generation.

23:14.920 --> 23:23.920
So what is different with that from the previous LSTM models is, and I described the script

23:23.920 --> 23:24.920
breakdown.

23:24.920 --> 23:30.880
Now, the next thing you can do is you can also look at character interactions.

23:30.880 --> 23:36.920
So in Scene 1, let's say you have Peter and Amanda interacting and Scene 5, they interact

23:36.920 --> 23:38.400
again.

23:38.400 --> 23:40.560
You can look at the words that they've spoken.

23:40.560 --> 23:45.000
You can look at, you know, the sentiment of their interactions.

23:45.000 --> 23:50.320
You can even look for continuity by analyzing the sentences that they've spoken.

23:50.320 --> 23:54.040
So imagine doing that for a movie or even a TV series.

23:54.040 --> 24:02.040
You can turn a script into a knowledge graph basically, which is characters and their interactions

24:02.040 --> 24:11.880
over a period of time and use that to identify any continuity issues or things that trail

24:11.880 --> 24:12.880
off.

24:12.880 --> 24:19.520
Like it could be just a story sideline that trails off and doesn't really add much to the central

24:19.520 --> 24:20.720
theme.

24:20.720 --> 24:28.000
Now, for writers, for, you know, imagine like Westworld, there's so many little subplots

24:28.000 --> 24:29.000
going on.

24:29.000 --> 24:33.120
And what you're going to a writer's room like that, it was kind of eye opening for me,

24:33.120 --> 24:38.240
going into a writer's room where the wall is literally covered with sticky notes and

24:38.240 --> 24:41.400
strings tying things together.

24:41.400 --> 24:44.280
And the scripts are ginormous.

24:44.280 --> 24:49.040
That's something that's really hard to do where, you know, again, bringing in tools,

24:49.040 --> 24:56.040
like visualization tools and even a step further, like continuity analysis, looking into, you

24:56.040 --> 25:03.320
know, things that trail off or that improves the content generation a lot.

25:03.320 --> 25:08.520
But you can get a step further, which is let's say now I want to make certain changes

25:08.520 --> 25:09.800
to the story itself.

25:09.800 --> 25:15.640
So I have a knowledge graph and I go to a certain node, which is characters interacting.

25:15.640 --> 25:18.320
And I, you know, I want to move around and play with it.

25:18.320 --> 25:21.280
Let's say let's move the story around and let's see what happens.

25:21.280 --> 25:27.440
So there you can actually use a Bayesian inference to figure out what would happen to the rest

25:27.440 --> 25:31.880
of the nodes if you make a particular change in one node.

25:31.880 --> 25:37.840
So those are the different ways that we can give tools in the hands of writers and producers.

25:37.840 --> 25:42.880
So they feel free to change around different parameters of the story.

25:42.880 --> 25:47.720
Now, something we can't do right now, but we're working towards is also mapping that audience

25:47.720 --> 25:49.440
response data.

25:49.440 --> 25:55.160
So perhaps you can make tweaks to a story, you know, in a way that will be more interesting

25:55.160 --> 26:00.840
to the audience, or maybe you want to try something completely new, just to experiment and

26:00.840 --> 26:04.400
see how the audience would react to something.

26:04.400 --> 26:12.760
Can you talk through the knowledge graph and the use of Bayesian inference at the next

26:12.760 --> 26:19.440
level of detail, how do you translate that, for example, translate that, you know, knowledge

26:19.440 --> 26:26.720
graph and Bayesian inference to something that's to an output that a script writer can use.

26:26.720 --> 26:27.720
Right.

26:27.720 --> 26:34.200
So as an analogy, you know, you can use knowledge graphs have been used in a lot of different

26:34.200 --> 26:35.200
domains.

26:35.200 --> 26:43.280
Let's say in the case of, you know, genetics, right, so you look at pairwise interaction

26:43.280 --> 26:48.040
in one place and then you can use Bayesian inference to say, okay, what if I, you know, made

26:48.040 --> 26:54.320
a change here, like the genetic code here, how that's that influence things in other parts,

26:54.320 --> 27:00.160
right, and connected notes in a, in a, in a call in more of a causal model, where you

27:00.160 --> 27:05.320
know the underlying causality, you can make tweaks to one node and you have a causal model

27:05.320 --> 27:09.960
that will tell you what happens as you move one thing and change, and you know, the effects

27:09.960 --> 27:15.160
that it has for their downstream or on the rest of the graph.

27:15.160 --> 27:22.600
So we have certain models of sentiment and models of, you know, emotional interactions

27:22.600 --> 27:25.400
and character interactions.

27:25.400 --> 27:31.960
These are not like models that you can necessarily write down, but more coral, that we've picked

27:31.960 --> 27:35.240
up by breaking down lots and lots of scripts out there.

27:35.240 --> 27:42.560
So there's a universe of like, you know, 50,000 scripts that may have been produced, an

27:42.560 --> 27:46.560
order of magnitude more if you look at scripts that have never been produced.

27:46.560 --> 27:52.440
So you can do this exercise with, you know, produce knowledge graphs for every script

27:52.440 --> 27:56.520
out there and learn some of these interactions.

27:56.520 --> 28:04.280
Now the way we learned our interactions is we had people go and code these interactions

28:04.280 --> 28:11.840
or annotate from one node to another what the sentiment interactions were.

28:11.840 --> 28:18.200
So we were able to use that data to train kind of like a sentiment causality graph.

28:18.200 --> 28:23.960
If you change one, one, the words or the sentiment of one node, how does that impact the

28:23.960 --> 28:25.880
others downstream?

28:25.880 --> 28:31.680
So that's kind of like a high level version of how we can use a knowledge graph to see,

28:31.680 --> 28:33.960
you know, predict what would happen down the road.

28:33.960 --> 28:35.840
How are we representing sentiment?

28:35.840 --> 28:41.480
Is it like trying to apply human emotions to these things, love, hate, whatever?

28:41.480 --> 28:48.160
Now I'm really just trying to wrap my head around maybe a concrete example of how

28:48.160 --> 28:56.560
a script might apply or translate into this model and then how manipulating this model

28:56.560 --> 29:00.320
helps us with the script process.

29:00.320 --> 29:01.320
Yeah.

29:01.320 --> 29:07.600
So, you know, the simplest example is let's say Paul and Andy are in a fight and Paul kills

29:07.600 --> 29:08.600
Andy.

29:08.600 --> 29:14.200
So now all of a sudden that character Andy can no longer exist in the rest of the scenes,

29:14.200 --> 29:15.200
right?

29:15.200 --> 29:16.200
So that's a very simple inference.

29:16.200 --> 29:19.200
That you fly to the rest of the notes.

29:19.200 --> 29:21.080
Let's get more complicated.

29:21.080 --> 29:28.160
Paul and Andy fight, but then they realize they're working towards the same common goal.

29:28.160 --> 29:31.160
So therefore their interactions now are positive.

29:31.160 --> 29:37.040
So now the rest of the nodes, if you have the next node where Paul and Andy interact

29:37.040 --> 29:40.400
were, Nick was a negative interaction.

29:40.400 --> 29:45.440
So for you to change Paul and Andy's interaction to a positive one, you know, that creates

29:45.440 --> 29:48.200
a big difference.

29:48.200 --> 29:54.680
It's basically leave an explanation gap between your node now with Paul and Andy's interaction

29:54.680 --> 29:56.600
and the next one.

29:56.600 --> 30:02.000
So those are basically gaps that you can look at in their interactions, the character's

30:02.000 --> 30:06.200
interactions that are that become fairly easy to identify.

30:06.200 --> 30:11.200
Now you can get a lot more nuanced, but what we see in scripts generally is that's not

30:11.200 --> 30:11.960
necessary.

30:11.960 --> 30:14.600
That's not really necessary.

30:14.600 --> 30:21.600
If you look at successful scripts and look at their knowledge graphs, they tend to cluster

30:21.600 --> 30:28.480
into very fairly predictable storylines.

30:28.480 --> 30:33.920
So if you just map the sentiments, there are certain things like the hero's journey or

30:33.920 --> 30:36.360
the fall from grace.

30:36.360 --> 30:44.520
These are things that even Aristotle wrote about in the poetics identified a certain

30:44.520 --> 30:47.920
set of story archetypes.

30:47.920 --> 30:51.200
And things haven't really changed even after we analyze all these scripts.

30:51.200 --> 30:53.480
These common themes reappear.

30:53.480 --> 30:59.160
It could be because humans tend to respond to those type of storylines.

30:59.160 --> 31:03.760
So at a high level, that's kind of a mapping to understand what is the general storyline

31:03.760 --> 31:06.720
from the knowledge graph.

31:06.720 --> 31:12.800
The second phase after that is, again, in terms of good storytelling, there is always

31:12.800 --> 31:17.040
a tension and resolution.

31:17.040 --> 31:18.040
Happens throughout.

31:18.040 --> 31:23.480
So if you look at good storytelling, it's never a flat affect all the way through because

31:23.480 --> 31:26.040
that doesn't make for an interesting story.

31:26.040 --> 31:28.120
But something always happens, right?

31:28.120 --> 31:32.120
Even if you're writing a story about somebody doing research, it's not like they kept

31:32.120 --> 31:34.880
working night and day and then found a solution.

31:34.880 --> 31:38.320
It's like, no, they worked really hard.

31:38.320 --> 31:43.960
They were close to a breakthrough and something happened and the world came crashing down the

31:43.960 --> 31:50.400
next day like they're, you know, some of the relatives entered into a car crash and they

31:50.400 --> 31:51.400
got depressed.

31:51.400 --> 31:54.000
So there's always has to be something.

31:54.000 --> 32:00.160
You need that volatility in the story just to attract the audience.

32:00.160 --> 32:05.400
So those are things that can be extracted, you know, without necessarily going too deep

32:05.400 --> 32:12.880
into kind of an emotional analysis or sentiment analysis of the script, even like more higher

32:12.880 --> 32:19.400
level things that we can extract, like positive negative interactions, friendly, unfriendly

32:19.400 --> 32:20.400
interactions.

32:20.400 --> 32:24.960
Those are things that we can extract from the script and that can give us a very good

32:24.960 --> 32:28.520
read into how well it would resonate with the audience.

32:28.520 --> 32:33.400
You've helped us understand the problem that you're taking on, providing, since you

32:33.400 --> 32:37.120
create a support for script writers.

32:37.120 --> 32:43.200
In many cases centered around this knowledge graph and you've mentioned, you know, using

32:43.200 --> 32:50.440
techniques like Bayesian inference, but in trying to apply kind of the full breadth of

32:50.440 --> 32:56.960
machine learning, NLP, NLG, NLU, you know, all of this stuff to these types of problems.

32:56.960 --> 33:03.240
I'm curious if you can talk through the, I just, what are, what are some of the

33:03.240 --> 33:08.000
roadblocks that you've run into applying, you know, things that may have been created

33:08.000 --> 33:12.400
in some research lab to, you know, practical tools that you're trying to put in the hands

33:12.400 --> 33:17.080
of script writers and how have you overcome those challenges?

33:17.080 --> 33:23.760
Yeah, I mean, you know, our, we have had to develop a lot of new methods along the way.

33:23.760 --> 33:31.760
So the, one of the big challenges whenever we deal with generative models is a lot of

33:31.760 --> 33:38.480
the research papers. They look really promising because you read it and it looks like this

33:38.480 --> 33:42.120
natural language generation technique really works, but a lot of the times it could just

33:42.120 --> 33:49.240
be really hand-picked answers, hand-picked solutions, and, you know, working with that very

33:49.240 --> 33:51.320
specific kind of data set.

33:51.320 --> 33:57.320
So, you know, LSTM, right, with that was an experiment we did, if we could just generate

33:57.320 --> 34:01.720
a script using LSTM, what it would look like and we ended up with sunscreen.

34:01.720 --> 34:09.080
And in trying to make that useful, we realized a lot of, you know, a lot of gaps, right,

34:09.080 --> 34:15.880
in where academic research is right now versus what we need to create this full solution.

34:15.880 --> 34:21.400
So with LSTMs, we needed to augment that those models in a lot of different ways.

34:21.400 --> 34:28.280
So first, context becomes super important. Also, we modified those models involved active

34:28.280 --> 34:33.280
learning because we wanted to learn preference functions, we wanted the model to kind of

34:33.280 --> 34:43.280
be, wanted to train it more iteratively. Also, we started looking at conditioning on like

34:43.280 --> 34:50.680
the knowledge graph to generate dialogue or generate next responses instead of just

34:50.680 --> 34:56.680
an unsupervised solution, which a lot of these LSTM-like models papers they talk about.

34:56.680 --> 35:03.520
So just from model architecture and coming up with new solutions, we had to take many

35:03.520 --> 35:10.160
steps beyond what was currently shown. And so, a couple of things, one is, you know,

35:10.160 --> 35:18.840
it may not be a great academic paper if you just combine context plus memory models,

35:18.840 --> 35:23.480
plus hierarchical models into one solution and show how great the results are. A lot

35:23.480 --> 35:28.720
of the machine learning papers kind of are publication center around coming up with

35:28.720 --> 35:35.920
a whole new model and showing the mathematics behind it. And that's what ends up, you

35:35.920 --> 35:42.400
know, getting accepted to nips and ICML. So a lot of that research is not necessarily

35:42.400 --> 35:48.720
directly applicable to what we're doing, whereas it's kind of like more on engineering.

35:48.720 --> 35:57.400
So there's a lot of engineering work that goes in, which may not be the most interesting

35:57.400 --> 36:03.040
machine learning advance, like a new model advance, but it's still pushing the results

36:03.040 --> 36:09.520
and the applications a lot along the way. So that's more on the model side. And, you

36:09.520 --> 36:17.720
know, I think something that there's been a lot of work in on the generative side, there's

36:17.720 --> 36:24.720
been a lot of work in in images. So we look at sharp looking images that have been produced

36:24.720 --> 36:31.960
by GANs. And that's why GANs have become such popular models. But that GANs don't work

36:31.960 --> 36:37.280
very effectively when you're looking at natural language generation. There we have found

36:37.280 --> 36:43.440
auto encoder models to be much more effective, partly because the GAN objective function,

36:43.440 --> 36:52.880
it starts pushing it towards defining the posterior distribution very sharply. So that's how

36:52.880 --> 36:57.200
you end up getting very sharp realistic looking images. But when it comes to natural language

36:57.200 --> 37:04.480
generation, it kind of produces the same very predictable sentences over and over. So

37:04.480 --> 37:09.280
that's where something like an auto encoder model works very well. So these are certain

37:09.280 --> 37:16.880
kind of findings that would be interesting for researchers to explore, but that doesn't

37:16.880 --> 37:22.880
really fit into a lot of the current academic models or current models of, you know, what

37:22.880 --> 37:26.560
gets what's a more interesting publication over another.

37:26.560 --> 37:34.440
So you mentioned a few things in there. One is hierarchical LSTMs. What are those and

37:34.440 --> 37:40.720
how do they come into play? So SunSpring, you know, it all came about because we want

37:40.720 --> 37:46.720
to improve models, the model beyond SunSpring. So an LSTM long short to memory model, you

37:46.720 --> 37:55.320
know, it does a better job than an RNN in terms of long term dependencies. So an RNN tends

37:55.320 --> 38:05.520
to kind of forget, you know, what was the word that was like five, six sequences ago,

38:05.520 --> 38:11.160
whereas an LSTM tends to remember. So like within a sentence, you get more coherence,

38:11.160 --> 38:16.480
but there's still limitations. So if you keep producing coherent sentences, that doesn't

38:16.480 --> 38:22.480
make for a story. So let's add more structure to that. So now if you're with a hierarchical

38:22.480 --> 38:30.240
LSTM, you can learn dependencies across sentences, for example. So now your paragraph that you're

38:30.240 --> 38:37.360
produced becomes more coherent, more understandable. Now you can take it a step further. Now you

38:37.360 --> 38:44.600
can define hierarchies, right? It could be paragraphs to sentences to words. It could

38:44.600 --> 38:49.240
be even beyond that. It could be multiple paragraphs to sentences to words. So these hierarchical

38:49.240 --> 38:59.000
LSTMs provide more consistency in the sentences or the words that have been generated.

38:59.000 --> 39:06.000
Are you training them hierarchically as well, meaning independently, or are they, is it

39:06.000 --> 39:13.360
a model that you're training end to end? Yeah, that's a very good question. So when we

39:13.360 --> 39:20.000
tried to train them purely unsupervised, the hierarchical models, you do get consistency

39:20.000 --> 39:25.400
across paragraphs. That's not necessarily the most useful when you're trying to generate

39:25.400 --> 39:33.080
like stories or do something interactive with a story writer or director. So we had to

39:33.080 --> 39:40.360
use a lot of like annotations. So annotations where basically we had human annotators saying

39:40.360 --> 39:49.040
these paragraphs kind of depict the same idea. Or we had encoding saying which we're using

39:49.040 --> 39:55.960
purely hierarchical models, then every paragraph would be sequentially. We'll make the assumption

39:55.960 --> 40:01.400
that every paragraph is sequentially follows a sequential format. That's not necessarily

40:01.400 --> 40:07.480
the case with stories. So we had to tag these stories by paragraph or by even sentences

40:07.480 --> 40:17.400
to say this relates to that paragraph from page 3 section 2. So those are the longer

40:17.400 --> 40:24.800
range dependencies that we could train our model on because we had the annotated data.

40:24.800 --> 40:33.680
It does strike me that a lot of the challenge in what you're doing has to do with how do

40:33.680 --> 40:39.240
you represent these different contexts or these different concepts and contexts for

40:39.240 --> 40:46.880
that matter and the relationships between them and what are the properties of, I guess

40:46.880 --> 40:51.240
you can get arbitrarily detailed with this. Like what are the properties of a given prop

40:51.240 --> 40:57.320
and is there some inconsistency in the way this LSTM is trying to generate someone's

40:57.320 --> 41:03.760
use of a prop or you're probably not trying to go that far. But have you learned any secrets

41:03.760 --> 41:08.080
or tips for you know just dealing with this representation problem generally?

41:08.080 --> 41:14.440
Yeah. So in terms of representation, that's why we found that the knowledge graph is

41:14.440 --> 41:21.320
the more of most efficient structure for learning or encoding these dependencies. It's

41:21.320 --> 41:28.040
the most succinct representation of these dependencies. So now we augmented our model

41:28.040 --> 41:36.440
so that even for the LSTM it's conditioning on all the output is conditioned on the state

41:36.440 --> 41:41.480
within a knowledge graph. So let's say you have dialogues that you're trying to generate

41:41.480 --> 41:47.640
for a character. Now node number 5 is where let's say we need in node in scene 5 is where

41:47.640 --> 41:53.680
we need to generate new dialogue. So we could either go with everything, all the words

41:53.680 --> 42:01.400
that were written before or we represent the state of node 5 and then the LSTM generates

42:01.400 --> 42:09.520
output conditioned of how you've encoded that state in node number 5. So that's a much

42:09.520 --> 42:17.840
much better representation and I think you know the more research needs to be done in looking

42:17.840 --> 42:21.960
at these knowledge graphs and how you can condition on these knowledge graphs to do

42:21.960 --> 42:28.880
a generation. I'm curious if the concept of embeddings comes into play when you're building

42:28.880 --> 42:36.680
these models? Definitely. So you can't just rely on just straight straight up words because

42:36.680 --> 42:42.680
again when we're looking at words embeddings becomes very important. You have analogies,

42:42.680 --> 42:48.440
you have similarities. So we have to learn we base everything on learning a word and embedding

42:48.440 --> 42:54.040
first. And generally I mean that's you know it's the whole natural language generation and

42:54.040 --> 43:01.560
understanding is a very interesting problem in its own right because everything is so context

43:01.560 --> 43:08.080
dependent compared to let's say image recognition. And you know that's why you know you have

43:08.080 --> 43:16.640
a company like Google and Facebook because they have the largest data set of images you

43:16.640 --> 43:23.440
can see the most variation in that data. But that doesn't necessarily work so much when

43:23.440 --> 43:30.840
you're dealing with natural language where you really have to focus on a particular focusing

43:30.840 --> 43:36.680
on a particular vertical gives you much better results on trying to go after a broad problem.

43:36.680 --> 43:42.600
You know there's some of the natural language processing APIs out there cannot even get

43:42.600 --> 43:49.600
anywhere close to what we want to do because they're going after a lot of breath in their

43:49.600 --> 43:55.640
responses. So that's why in terms of like understanding sentences in their context to

43:55.640 --> 44:02.600
given depth is very difficult because context changes kind of so much based on the domain

44:02.600 --> 44:08.000
and the vertical. You also mentioned auto encoders can you share a little bit more about

44:08.000 --> 44:15.480
how you've used those? Yes so auto encoders we've found work very well with natural

44:15.480 --> 44:23.360
language generation. So what an auto encoder does is you know the input layers have some

44:23.360 --> 44:31.080
number of nodes. The hidden layers actually have fewer nodes than or fewer units than the

44:31.080 --> 44:37.720
input layer and the output layer. So what an auto encoder does is it starts with a large

44:37.720 --> 44:44.240
number of input input nodes. So let's say 100 and the output nodes could be another 100.

44:44.240 --> 44:51.640
But the hidden layers would be fewer like 20 nodes. So what that forces the auto encoder

44:51.640 --> 45:00.840
to do because it's using fewer nodes fewer units in the middle. It has to compress the

45:00.840 --> 45:07.480
input and try to generate the output of the same dimensionality. But it has to compress

45:07.480 --> 45:15.360
the data that's coming in. So what that does in terms of language is you know it forces

45:15.360 --> 45:21.760
one way of understanding it. It takes words and forces that into concepts. So let's say

45:21.760 --> 45:29.400
there's you know the input you want to represent the idea that I take walk stale. There are

45:29.400 --> 45:36.840
different ways of saying that like I'm a regular pedestrian. I occasionally walk around

45:36.840 --> 45:44.240
a lot. So the same idea can be expressed in multiple formats. So an auto encoder because

45:44.240 --> 45:50.480
it's compressing the data that's coming in kind of maps things or is forced to map words

45:50.480 --> 45:58.720
into concepts in non supervised way. Are you doing anything where you're taking those

45:58.720 --> 46:05.480
concept vectors from the middle part of the auto encoder and then using those as representations

46:05.480 --> 46:10.160
in your knowledge graph. Is that kind of what you're getting at? Yes. So those kind of

46:10.160 --> 46:16.000
form you know they're basically they can form the states of the knowledge graph. So the

46:16.000 --> 46:21.560
state doesn't necessarily of a knowledge graph doesn't necessarily need to be something

46:21.560 --> 46:28.800
that is human interpretable. It could be you know the middle layers of an auto encoder.

46:28.800 --> 46:37.200
That could inform the state. Oh interesting. It's an interesting case study perhaps on

46:37.200 --> 46:45.480
the you know the practical applications of of AI like all of the different pieces that

46:45.480 --> 46:53.240
you've had to put together to build the solution. Very very interesting. We started off in

46:53.240 --> 46:59.280
something that can be considered fairly niche because these are scripts written for you

46:59.280 --> 47:06.200
know Hollywood. But from business point of view a Hollywood itself is a very big industry.

47:06.200 --> 47:15.640
But even if you look at beyond Hollywood it's the whole content creation industry is massive

47:15.640 --> 47:22.680
in hundreds of billions of dollars. So although it's a very niche problem the economic impact

47:22.680 --> 47:30.360
of it is very substantial you know it's going it's going after one of the biggest markets.

47:30.360 --> 47:38.840
So at this point still there's a lot of you know trying different experiments or taking

47:38.840 --> 47:43.000
different ideas or taking different models and putting these things together. I think

47:43.000 --> 47:50.640
there's fundamentally more work that can be done you know to to build a whole new class

47:50.640 --> 47:58.360
of models that are able to take context you know in different settings able to encode

47:58.360 --> 48:05.240
relationships between entities between people and use that in generation. Now that's that

48:05.240 --> 48:13.160
is a very interesting research problem to go after. And so as a startup trying to bring

48:13.160 --> 48:23.440
an AI product to market do you see part of your contribution is kind of advancing that

48:23.440 --> 48:29.040
research effort or is that outside of scope of what you're trying to do and if someone

48:29.040 --> 48:36.120
does it great you'll use it but you're not in a position to push these research types

48:36.120 --> 48:42.040
of questions. How do you manage or balance that? As a startup we discovered whole new

48:42.040 --> 48:48.440
market instead of applications we started in you know in Hollywood and analyzing scripts

48:48.440 --> 48:54.760
because that data is plentiful like there's a lot of public data available but we found

48:54.760 --> 49:01.200
that a lot of companies fortune 500 fortune 1000 companies have the same need they want

49:01.200 --> 49:06.600
to produce content you know they all are striving to produce better and better content.

49:06.600 --> 49:15.640
So you know you we trained our AI trained our models on the best data out there or we we

49:15.640 --> 49:21.680
cut our teeth on the harder problem which is producing really high quality storytelling

49:21.680 --> 49:26.880
high quality content that's you know what Hollywood is all about and then applying that

49:26.880 --> 49:34.480
to a broader set of content creation that is very specific like a particular company

49:34.480 --> 49:40.200
you know they're they have a certain context in which they want to produce their content.

49:40.200 --> 49:47.720
So that you know our model is basically translate over. So generally I think you know as a startup

49:47.720 --> 49:54.400
we can't go after you know the broad class of problems and spend years dedicated to one

49:54.400 --> 50:05.240
or two core areas. So our goal is to produce products that our end users want at the end

50:05.240 --> 50:11.680
of the day are end users are producers production companies corporations producing content.

50:11.680 --> 50:17.280
So that's who we are catering to but along the way we've discovered a lot of very good

50:17.280 --> 50:22.640
problems and that's one of the reasons we have collaborated with a lot of universities.

50:22.640 --> 50:29.720
So we have strong collaborations going on with with Caltech and you know we've like hired

50:29.720 --> 50:37.960
interns to work on research problems very specifically because you know I think eight like discovering

50:37.960 --> 50:43.360
these solutions kind of help the community overall and I wish more and more people would

50:43.360 --> 50:50.480
look at these problems because you know that's benefiting everybody but also we can kind

50:50.480 --> 50:56.360
of discover or we're always looking at the at the research findings so we'll notice them

50:56.360 --> 51:01.160
before everybody else and we'll you know implement them towards problems where we we have

51:01.160 --> 51:07.080
a path to market. So that's our approach which is you know trying to build the best product

51:07.080 --> 51:14.040
but also keeping a close eye on research out there and also funding and collaborating

51:14.040 --> 51:19.960
with research labs and universities. It's been really interesting kind of chatting about

51:19.960 --> 51:26.760
how your path bringing this to bringing this to market. I think they've been a bunch of

51:26.760 --> 51:32.680
interesting tidbits for me but probably the most so you want is this just all of the different

51:32.680 --> 51:38.040
pieces that you've pulled together to build a solution and when I think about you know what

51:38.040 --> 51:47.240
it means to build kind of a knowledge graph for these scripts it strikes me as a really you

51:47.240 --> 51:53.160
know potentially you know challenging problem like in a lot of ways you know it's not although

51:53.160 --> 52:01.880
the scale is very different in terms of the number of documents that you're trying to incorporate

52:01.880 --> 52:08.600
into this graph the the challenge in a lot of ways doesn't seem all that dissimilar from you

52:08.600 --> 52:13.400
know what a Google's knowledge graph you know in terms of the diversity of concepts that you have

52:13.400 --> 52:18.920
to represent in it and the way you're pulling together all these technologies to support that

52:18.920 --> 52:27.640
is very interesting. Yeah thank you and and you know yes the volume is less but we do have a lot

52:27.640 --> 52:36.200
of complexity in the relationships and the data that we're trying to analyze. So as a result of

52:36.200 --> 52:41.720
that we've had to you know build some really solid proprietary data sets we have to had a we

52:41.720 --> 52:48.040
needed a lot of annotations so fortunately we're you know we offer the product that

52:48.040 --> 52:54.040
products a lot of those annotations and we could train our system so that goes hand in hand

52:54.040 --> 53:00.280
you know landing on you know our products are used by production company and producers that

53:01.000 --> 53:07.000
so we have the best data for this type of problems and then we are looking for different ways to

53:07.000 --> 53:13.720
like use that and enriching the knowledge graph in producing better generative output.

53:13.720 --> 53:17.560
Just out of curiosity what does your technology stack look like?

53:18.280 --> 53:25.480
Most of our models are code and in TensorFlow we use Python there's different for on the product

53:25.480 --> 53:35.320
side you know we use more Node.js and web interfaces and for our knowledge graph and quoting a lot

53:35.320 --> 53:43.640
of that internally I'm a champion of OCaml. A Lisp guy? Well OCaml has related to that right?

53:43.640 --> 53:51.160
Yeah OCaml is a functional programming language like Lisp but OCaml does a few other nice things

53:51.160 --> 53:57.720
like it has formal verification that actually helps I don't think the research community is really

53:58.600 --> 54:04.920
figured out how to make best use of that but if you incorporate formal verification

54:04.920 --> 54:10.920
you can do knowledge graphs pretty well there's just some properties that you can exploit from

54:10.920 --> 54:19.080
formal verification to construct knowledge graphs and also OCaml has object you know you can

54:19.080 --> 54:25.960
encode objects and classes that means you can have a richer you encode richer data sets that

54:25.960 --> 54:34.600
you can't do with you know Haskell and Lisp for sure. So functional programming languages generally

54:34.600 --> 54:43.400
I think are the way to go for programming in AI because you know functions are primitives

54:43.400 --> 54:48.520
and at the end of the day you're you're doing lots and lots of functional operations on pieces

54:48.520 --> 54:54.920
of data. So I think like the whole AI community could do well to switch to you know functional

54:54.920 --> 55:02.600
programming languages from you know Python and all the imperative languages that people use now.

55:03.560 --> 55:08.840
Awesome well Dave thanks so much for taking the time to chat today it's been really interesting.

55:09.640 --> 55:11.240
Thank you very much for having me Sam.

55:15.240 --> 55:21.480
All right everyone that's our show for today for more information on Dave or any of the topics

55:21.480 --> 55:27.400
covered in this episode head over to twimmalai.com slash talk slash 178.

55:28.840 --> 55:33.320
If you're a fan of the podcast we'd like to encourage you to visit your Apple or Google

55:33.320 --> 55:39.720
podcast app and leave us a five star rating and review. Your reviews help inspire us to create

55:39.720 --> 55:45.320
more and better content and they help new listeners find the show. As always thanks so much

55:45.320 --> 55:54.440
for listening and catch you next time.

