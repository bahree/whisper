Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, we're featuring a series of conversations from the Nips conference
in Long Beach, California.
This was my first time at Nips and I had a great time there.
I attended a bunch of talks and of course learned a ton.
I organized an impromptu round table on building AI products and I met a bunch of wonderful
people including some former Twimble Talk guests.
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should
take a second right now to subscribe to at twimblei.com slash newsletter.
This week through the end of the year, we're running a special listener appreciation contest
to celebrate hitting one million listens on the podcast and to thank you all for being
so awesome.
Tweet to us using the hashtag Twimble1Mill to enter.
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and
other mystery prizes.
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill
for the full rundown.
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship
of this podcast and our Nips series.
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster
sessions, their big news this time was the first public viewing of the Intel Nirvana
neural network processor or NNP.
The goal of the NNP architecture is to provide the flexibility needed to support deep learning
primitives while making the core hardware components as efficient as possible, giving
neural network designers powerful tools for solving larger and more difficult problems
while minimizing data movement and maximizing data reuse.
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel
Nirvana.com.
In this episode, I sit down with Timnit Gebrew, postdoctoral researcher at Microsoft Research
in the Fairness Accountability Transparency and Ethics in AI or Fate group.
I've been following Timnit's work for a while now and was really excited to get a chance
to sit down with her at the conference.
We packed a ton into this conversation, especially keying in on her recently released paper,
including deep learning and Google Street View to estimate the demographic makeup of the
US.
Timnit describes the pipeline she developed for this research and some of the challenges
she faced building an end-to-end model based on Google Street View images, census data,
and commercial car vendor data.
We also discussed the role of social awareness in her work, including an explanation of how
domain adaptation and fairness are related and her view on the major research directions
in the domain of fairness.
Timnit is also one of the organizers behind the Black and AI group, which held a very interesting
symposium and poster session at Nips.
I'll link to the group's page in the show notes.
This was a really interesting conversation and one that I'm sure you'll enjoy.
Timnit, welcome to the podcast.
Thank you.
Thanks for having me.
Absolutely.
What is the fate group at Microsoft Research?
Fates stands for Fairness Accountability Transparency and Ethics and AI and it's a very new group
started by Kate Crawford and Hannah Wallach and there's some other people there like Jen
Warman, Vaughan, and some economists and some computational social scientists, so it's
a combination of machine learning people and social science and economics people.
Try to study the societal implications of AI and just make sure that we create algorithms
that are fair, so our research is focused towards that.
Oh wow.
And how did you get interested in fairness and AI in particular and artificial intelligence
in general?
My background is in computer vision and as I was working on, there's a number of things
I've always been interested in social justice and towards the end of my PhD, I saw this
purplica article about a software that was being used by judges to figure out a person's
likelihood of committing a crime again and judges where this was one of the inputs that
they used to figure out how many years they should sentence you to prison.
This being some machine learning algorithm.
That's being sold by North Point and I think it's the name of this startup and that was
very terrifying for me knowing because I had the background to know what kind of biases
we have in the criminal justice system already and what kind of biases we have and how much
discrimination there is in the data, that would be trained for it.
So that was one and then while I was working on my PhD, I kind of figured that my work,
my own work, could be susceptible to this kind of bias as well because my whole work was
trying to show that we can do data mining using images, so large scale computer vision
plus data, you know, like most people use text and social networks and other kinds of
textual data to do data mining and the whole point of my PhD was to show that we could
gain useful societal information using images and so if the ground truth for that that you
use to train is biased, you're going to have, you know, biased conclusions.
So I thought that I should be very cognizant of what kinds of issues could exist with
that type of work, given that my work lies in that type of work.
Okay.
And now you're relatively new at Microsoft, is that right?
I am, yeah, I started in July, yeah, I'm very new.
Awesome.
And you just published your first paper with the group?
Is that right?
No, no, no.
So the my paper that just came out in PNAS is actually from my PhD.
Oh, really?
So this is a project that took four years.
Wow.
So I think, you know, it's more, when I give talks about it, I think people understand
the level, the amount of work it went into it, but it's harder to see a thing from just
from that one paper, but yeah, like that paper took a very long time and it just got
published.
Wow.
So tell us about that paper.
The paper was using Google Street View images to predict demographic characteristics.
So what we did was we detected and classified cars, all the cars and 15 million Google
Street View images across 200 American cities.
And then we were able to use the characteristics of the cars that we detected and classified,
you know, in a particular zip code or precinct and associated that with certain demographic
characteristics like income or political affiliation, like an earlier paper we had even we
did, like we looked at income segregation levels and even like CO2 emission rates.
So we then, you know, once we detected and classified the cars, we represented each
geographic region, like basically for us it would be like a zip code or a precinct,
like a, you know, a voting precinct.
We represented by like the type, the features that of the cars that are in that zip code.
So for example, the percentage of Hondas or the percentage of each make that you have,
that's one feature, right?
Like percentage of triodas or Hondas or like Nisans or whatever percentage of sedans,
you know, other metadata like the average miles per gallon, like efficiency of that particular
zip code, et cetera.
So once we had that information, then we used round truth census data or other data depending
on what we're trying to predict to train another model to go from the car features to
like predicting demographic characteristics.
Okay.
That's the project.
Interesting.
Interesting.
So you mentioned census data as part of your training data set.
When I think of the kinds of things that you're trying to predict for, like the, you know,
the wealth or income of a geographic area, that's already in the census data.
So how would someone use this technique?
Oh, so what we did is, so we had 200 cities in our data set, right?
Okay.
So we used a subset of our cities for training.
Okay.
So we assume we have census data for like, let's say, I think is it 13 percent of, you
know, or we sweep it to see, like, how much training data we need, but like, we assume
we have census data for a small, very small subset of the cities that we have.
And then we train a model using, and then for the rest of the cities, we don't have census
data.
We only have images and cars.
So I guess the way we, someone would use it is if you have census data for some cities
and you want to try to see, like, you know, for other cities, what the data might be, you
could train our model using the census data that you have, and then the cars that are
detected in the other cities.
Okay.
Right?
So that's how we used it.
And, you know, we also did some experiments and trying to do this across time.
So say you have data for past census data for New York.
Like we had it using Google Street View time lapse data.
So for New York, and you have a bunch of images as well.
And you then try to predict what's going to happen in the future, or, you know, like before
you have the census data.
So we did some experiments like that as well.
So, you know, for me, this is kind of like, I don't want people to, like, read too much
into the cars, you know, think for me, it's a proof of concept, basically, like, a new
tool that you can use to do this kind of analysis, like, demography or social science applications
or work, you know, it's like a new tool that is available to researchers, and we want
to show, if one were, you know, say you wanted to study the, I don't know, relationship
between trees or tree species and people's health or something, you know, how would you
go about using images to do that?
Well, now that you saw our paper, you could, like, apply a very similar pipeline to it,
you know?
Got it.
So that's more what I want to take away to be.
It's not specifically about cars, you know, projecting income based on cars.
It's more about, we've got all of this visual data from cameras and sensors and things
like that.
How can we use that as proxy for any other thing that we might want to do?
And for us, like cars, you know, there's, you know, like, there are other things you
could study where the only data you could probably have is probably only visual data,
right?
Like, so for cars, you can argue that that's not necessarily the case you can use DMV
data, right?
But I guess our street view gives you a different perspective, which is you're not looking
at the cars of the people who necessarily live there.
You're just saying, if I were to just walk around the street, you know, what does that street
look like, right?
Like, what kind of cars are driving?
What kind of cars are parked?
That's kind of, and then what does that tell me about the people who live there?
Right.
For me, I'm most excited about the tool, you know, and this pipeline, I'm not, you know,
and I was very, very surprised that our thing actually worked, you know, because there's
a lot of stuff.
I was very surprised.
There's a lot of stuff that could go wrong because the pipeline, there's many, many different
components of it.
Well, can you walk us through the pipeline?
Yeah.
So the biggest thing is a lot of people in AI don't talk about this, but it's data collection
is huge.
So if you want to do a sort of supervised machine learning, and you want to do it in the
real world, right, we're not like talking about a toy data set here.
So we were saying, what is our end goal?
Our end goal is to detect and classify all the cars and 50 million Google's review images
and then to predict, demograph, is using that, right?
So to get to that end goal, we first have to figure out, oh, okay, like how are we going
to, okay, how are we going to get data?
How are we going to get label data?
Like how are we going to label cars and Google's review images?
This is very hard.
What we've been getting the data, is that easily accessible via an API, or did you have
to scrape them?
Google images?
Yeah, they have an API.
Okay.
And these are publicly available images.
Okay.
But so then, you know, we had to be like, okay, so what are all the different types of
cars around, and how we might see Google's review images?
Where do we get that list?
Right.
So we found Edmunds.com and they have all the cars since 1990, so it's about 15,000 types
of cars.
But guess what?
A computer vision algorithm can only, you know, we can only really kind of classify cars
based on what they look on the outside.
And these, you know, a subset of these 15,000 cars look the same, because they don't, they
don't change them, you know, from year to year or from trim to trim or whatever.
So we had to figure out how to cluster the cars that look the same.
So we had a paper on this, it was a CHI paper, it was more of an HCI work.
How do we get our initial subset of classes, where we bucket like cars that look the same
into one class?
And then that, that, that process in and of itself took a few months, by the way, because
you know, you try something, it doesn't work, try something else, you know, so we did
that.
And so this is unsupervised.
You're just clustering the cars that you're seeing in the images.
Yeah.
So we show.
And you could say it's, yeah, so, so what we do is we show, we use Amazon Mechanical Turk
and we show people, it's a graph-based algorithm.
Like we show people images, two images of cars.
So we have example images of cars from Edmonds.com.
Okay.
And so we show people, we say, are these two cars the same or different?
And like we have, we have one of them is from Street View and the others from Edmonds.
No, no, this is all from Edmonds.
So right now, we're not even, okay, we haven't even gotten to getting labeled data right
now.
Right.
We're trying to define what our classes are.
Okay.
What does class one mean?
Class one means 2,500, chord 2,600, chord 2,700, you know, all of them look the same.
So we haven't even started getting data, we're just starting to define what our classes
are.
Okay.
That already takes a lot of time.
That's the first thing you got to do.
Right.
The second one, once you define what your classes are, then for each of those classes,
you have to have labeled data to train your car detector, right?
So that's where we need the experts for Google Street View images.
We also scrape data from like e-commerce sites, like cars.com and Craigslist.com.
But you know, this is why dopamine adaptation exists.
If you just, you know, train a plane like CNN or some sort of supervised machine learning
algorithm on things that look like, you know, cars from Craigslist and try to test it
on cars, you know, try to like detect cars in Google Street View or classify cars in
Google Street View, it's not going to work.
The distribution looks very, very different.
So then there is a whole like, then I had an ICCV paper where we did like, we had, it's
a domain adaptation based paper.
Okay.
So I think actually this data set is a very good domain adaptation data set.
And why don't we do like an in set here on domain adaptation?
What's the 32nd overview of domain adaptation?
So domain adaptation is a subset of what people call transfer learning.
So domain adaptation is like, you have one task, one, one exact task.
And we have something that we call a source domain and a target domain.
So in our example, let's say the source domain is cars from e-commerce sites, Craigslist.com.
And let's say the target domain is cars from like Google Street View images.
Okay.
And so what you try to do in domain adaptation is you assume that you have labeled data in
the source domain.
But in the target domain, you know, in unsupervised adaptation, you assume you have no labeled
data.
So in unsupervised, we would assume that we don't have any images in Google Street View
that are labeled with the types of cars they contain.
That's unsupervised.
In fully supervised adaptation, we would assume that in Google Street View, we have labeled
images for all classes.
And then in semi-supervised, we would assume that in Google Street View, which is our target
domain, we have labeled, you know, data for a subset of our classes.
Okay.
So the idea of domain adaptation is when you're training set and your tests that have different
distributions, how can you best use them, you know, within these different domains,
assuming, you know, making these different assumptions that we just talked about, how can
you best use data in your social domain, I guess, in conjunction with data in your target
domain to maximize your accuracy on the target domain.
Okay.
Right?
So this is a very real, in the real world, this is usually the case because you're never
going to have like camera statistics are different, you know, occlusion or whatever.
So like if you have a training set from like Google images or, you know, you want search
images or something.
And then you're like, you know, you want to apply it, you know, your model to like some
other thing.
Yeah.
That has a different statistic.
You need to know about adaptation techniques, you know.
And when I think about Google Street View images, all those images have, you know, from
the perspective, like they have a very specific kind of look that's different from anything
you'd ever see on Edmonds or any other part side.
Oh, yeah.
Because Edmonds, you know, I'm trying to sell you my car.
So I want to give you the best perspective.
Like you have a really nice resolution.
It's in one car in the middle.
There's no other car, like, occluding this car.
There's no trees or like, you know, I don't know, like there's, it's very, very different.
You don't, you know, there's cars in Google Street View where you only see like, it's
a stage of damage.
Yeah.
Yeah.
Yeah.
Yeah.
There's cars in Google Street View where you only see like a couple of lights or something
like that.
You have this side, you know, view.
So, so that's where, you know, I'm very interested in the domain adaptation problem because
of that.
This project helped me decide what core machine learning and computer vision areas I'm very
interested in.
Because of this, it's domain adaptation.
The second one is data collection, efficient data, like basically what some people call efficient
machine learning, a data efficient machine learning.
So I would say domain adaptation is part of it.
How to efficiently collect data sets.
So when I think of data efficient machine learning, I think of like one shot, few shot
machine learning.
I kind of have anything.
I haven't done too much on a few shot, one shot learning, whatever.
I don't know.
The semantic sometimes kind of like confused me, but you know, but just any, it basically,
so because data collection does not sound fun, I don't think AI people are, you know,
in computer vision, we a lot of us work on it and there's even people doing like a
hybrid computer vision at HCI kind of work, especially in our labs from the very beginning,
like a lot of people were always working on data collection because we don't want to
work on toy problems.
Like, you can't, you can only do so much with MNIST, you know, you can only, you know,
really gain, you can only gain so much insight, you know, to mean into like what kind of problems
we should be solving.
If you're always just using like, you know, readily available data so that you can get
to the next like conference deadline or something like that, you know, I'm saying you should
always spend so much time collecting data, but I'm saying you should do it at least a couple
of times in your life, just to see, you know, where AI is right now if we were wanted
to apply it to the real world, you know.
So this project really, really like, I would say that I mean, I was complaining about it
the whole time I was working on it and that it's over.
It really cemented like what I think is really important to do, to work on.
And actually the issue of bias also came up in this project because in an earlier paper,
you know, we also, we were just trying to see like, okay, we can predict this, we can
predict that.
And one of the things we looked into was crime, crime rates, right?
So, so, and then with crime rates, as you know, the ground truth, whatever ground truth
we were going to have is bias, because all we know is who got arrested for the crime,
who's crime got reported.
So if I say, hey, look, with images, we can, you know, with cars, we can do this, then
that's already bias, right?
So, so basically like, if I'm going to do this type of work, if I'm going to continue
to do this type of work, I have to, I have to also be working on the bit bias and fairness
and other types of issues.
And what's really interesting is that domain adaptation and this whole fairness thing
are very, very related, actually, I just noted.
So like, so some of the techniques that you can even use, that people, some people have
even already used are very related.
So one of the ways in which people do domain adaptation is to say, you know, say, you
know, I want a classifier, let's see your classifier has a primary task, which is to classify
something, maybe the type of card my image, right?
And so I want that classifier to do well, regardless of what domain my image comes from,
whether or not it's Craigslist or, you know, whether it's Craigslist or Google
Street View, right?
So the way some people do this and there's many variations of this is they have another
classifier and the other classifier all it does is it uses the features that its input
is the features learned by the first classifier.
And so the input is those features and the output is, you know, which domain the image
came from Craigslist or Google Street View, right?
So then the first classifier in its job is in addition to accurately classifying the
car, its job is to also confuse the second classifier because if the second classifier
can't tell based on the features learned by the first classifier, which domain the image
came from, then it means you've learned features that are sort of domain invariant,
correct?
Now can you see how you might apply this to fairness?
So say that you want to classify something like, you know, your risk score or something
like this.
You want to say, if my other classifier can identify some class that I don't want to
identify it.
Yeah.
So say you want this to be like, you know, invariant to like your race or something.
So then you can have another classifier that classifies the race of the person.
And then you, this first one confuses that.
You know, now, now, like, there's been work already that does this and, you know, it's
not like a done deal, it's not solved because then there's different fairness criteria
and then like, which criteria do you use, et cetera, et cetera, but but I do think that
these two things, like, you know, these two fields are kind of very related.
Yeah.
And it's so weird to me because I didn't think about that when I got interested in both
of them.
I didn't think about that at all.
And now I'm like, oh, wait a minute, you know, interesting.
Yeah.
Now this, this last thing you were describing, it sounds like it, like the kind of thing
we see in adversarial networks is it?
Yeah.
Yeah, exactly.
I mean, you people have done it with adversarial networks.
So you can implement it with adversarial networks.
You don't have to implement with adversarial networks.
There's other works that use like a different loss function, like, there's work called,
there's the gradient reversal work from a white way back.
I was like 2014ers and like that.
There's also domain confusion loss.
So Judy, who is, well, was opposed to that in our lab, and I had a paper at ICCV where
I used her loss from like a different, a prior paper of hers, which was not adversarial.
But it's very, again, like, it's funny that people independently were working on this
idea by the time Ian came up with his adversarial networks thing, with his GAN thing.
And then once he came up with a GAN thing, they're like, oh, wait a minute.
We could just use GANs for this.
But that's another thing I find interesting is that like this, this idea was kind of
concurrently being thought of by other people.
Interesting.
And so how do you take this forward in your new role at Microsoft?
Oh, so at Microsoft, I'm working a whole bunch of stuff right now.
So Joy Bulwamini from MIT is here, by the way, and she and I have a paper that is hopefully
going to come out at this new fairness conference for this accountability, transparency, and ethics
in AI conference.
It's a whole conference in February, in February, it's in New York, yeah.
So I'm part of this engineering community there.
So we have this paper that's basically doing algorithmic audits.
It's going to come out soon so you can read about it when it comes out.
And then I'm also working on this idea of standardizing.
So you basically want to standardize what kind of information you should put out with
your data sets or pre-trained models or whatever.
So I've been telling everybody about this.
So I used to be a hardware engineer and hardware, we have a data sheet that comes with every
component.
And when you're a circuit designer, you would be very intimately familiar with the data
sheet.
And the designers and all this stuff, before you design something into your model, right?
So that process.
We'll pull down the data set.
We're not doing that.
We're putting in our trainer model on it without thinking about it.
You have an API that someone releases an API that you have to pay for.
Really, you have no information about how you really have zero information about how
are you supposed to, is it supposed to work on this new data set that you're using?
Are there recommended applications?
What's going to happen if you use it the wrong way?
And it's very dangerous.
Because in hardware, at least, I think the reason things were very, it's a mature field,
but things that were standardized because the failure mode is so, it's visible to most
people to everyone, right?
Like, your battery catches fire or something like that, whereas here could be visible to
some people, you know, and might not be like, if face recognition doesn't work for you
because you're black or something like that, it'll be visible, very visible to you.
It won't be visible for other people because they didn't test it out or something like
this.
Right.
Okay, so it's, you know, because probability is involved, it's, you know, might not even
be, you know, formally visible in the class that's affected.
Yeah, exactly.
And so, and, you know, like, we, a lot of people aren't, we're even doing these tests.
So like, the first thing we have to start doing is like doing these audits, audits, you
know?
So stuff like this that I'm working on, I'm also working on, you know, just like, I was
telling you, you know, models that, that are fairer, you know, what is the, the fairness
criterion.
And very, you know, I've learned a lot about this field.
So I, I'm putting you to this field and now I've like gotten embedded in the community
which is nice.
What I mean is to the fairness community.
So yeah, like, so kind of working from the, from the purely machine learning perspective,
you know, like, how can we have account for fairness criterion in our models?
It's very broad, but like, that's one of the things I'm working on, in addition to just
like my regular computer vision kind of work, like domain adaptation blah, blah.
So yeah, so that's how I'm, I'm taking this to my new role at MSR.
Okay.
Interesting.
I'm really interested in this conference.
I'll need to get some info from you about it and check it out.
What are some of the other kind of major research directions in the domain of fairness?
So I think that uncovering bias is one.
Okay.
So Joy and I just did this paper that's coming out where like we were auditing, you
know, commercial gender classification APIs that are sold by people that you have to
pay for and looking at disparities among certain groups by skin color, by gender.
And then, you know, Meg Mitchell just came out with this paper.
So I'm just talking about computer vision, because actually in computer vision, it's
very new.
People have been doing this stuff for like, you know, there is this, I don't know, have
you heard of this?
Man is to programmer as woman is to a filmmaker, that's the kind of like, I've seen a few
various.
Yeah.
So, you know, and so I think in computer vision, that's one of the reasons I wanted to get
into it.
I felt like computer vision people weren't thinking about this.
Where's NLP?
It's been a little bit worse.
And I think the first time Meg is an NLP, even Hannah does some NLP and like, I mean, does
NLP.
And like, even, and also theory people, I feel like in terms of technical people,
theory people were starting to think about it, privacy people, like, send you to work
and other, you know, like, and also from the ethics side, but I felt like, okay, now
deep learning people are talking about it too.
And there's like papers, things like this, but like last year, I felt like, yeah, people
were starting to talk about it, but it wasn't a real, you know, concern.
So now, like, in computer vision, even, we're starting to see some of these work.
So one is uncovering bias, like, you know, what kind of bias exists.
And the second one is how do you mitigate it if you uncover it?
So, you know, there's a lot of work.
So with, I guess the work I just talked about, the word-to-vec work, you know, the first
uncover that bias, and then they tell you some strategies of mitigating that particular
bias.
And then the third one, which I'm very interested in is, I'm interested in all of them,
right?
But the third one is also just like understanding how these things are being used.
So if you have, and how to standardize them, how to have transparency, like, if you have
law enforcement that's using inaccurate face recognition algorithms, where are they
using it?
How are they using it?
Well, you know, we have no idea.
And also just like, you know, there's people who are using your social network data to,
like, then selling it to other people or trying to figure out, like, your credit ratings
and things like their startups, right?
Kathy Neal talks about it in her book.
Have you read this book?
Weapons of Math Destruction.
Okay.
Yeah.
That's hard to get.
And it's on my list.
I mean, and so that's a very important part of the issue actually, because as AI researchers,
a lot of times, I mean, me included, okay, like, I just want to sit in a corner, read my
papers, you know, and I honestly write some code.
That's what I love doing most, still, you know, even though I do this whole, like, social
activism stuff, like, what I enjoy doing is just, just, like, reading papers, thinking
about ideas, writing code, right?
But we have to understand, like, what are the implications of our work are, you know?
And so, like, keeping track of, I just signed this extreme vetting letter against this extreme
vetting initiative.
I don't know if you've heard about it by the DHS.
Yeah.
That was trying to, I didn't even know about it until I was, yeah, until I was, I had no
idea this was going on and it's terrifying.
Yeah.
You know, like, so this kind of stuff, we have to keep track of and we have to make our
voices heard in addition to, you know, working on uncovering bias and mitigating bias.
Right.
Awesome.
Well, I know you've got to run off to a talk.
Yes.
So, let's wrap it up here, but if you have any final words or thoughts or places that
folks should be, you know, looking to keep up with all this information or finding you,
yeah.
Now's the time.
Well, I recently got on Twitter, I was told it's a good thing.
Wow.
Well, yeah, I'm Tim Nick, who's my, my Twitter, and I'm going to have a website.
So, I will be releasing data soon for this monstrous work that I hate that we just discussed
with PNAS paper and to look out for my new, yeah, paper with joy as well.
Okay.
And we're just going to be releasing a couple weeks.
All right.
Well, we'll link you on Twitter and the show notes as well as to your homepage and looking
forward to following this line of research and the stuff you do at Microsoft.
Thank you.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Timnett or any of the topics covered in this episode, head on
over to twimlai.com slash talk slash 88.
To follow along with the nip series, visit twimlai.com slash nips 2017.
To enter our Twimlai one mill contest, visit twimlai.com slash twimlai one mill.
Of course, we'd be delighted to hear from you either via a comment on the show notes page
or via a tweet to at twimlai or at Sam Charrington.
Thanks once again to Intel Nirvana for their sponsorship of this series to learn more about
the Intel Nirvana NNP and the other things Intel's been up to in the AI arena, visit intel
Nirvana.com.
As I mentioned a few weeks back, this will be our final series of shows for the year.
So take your time and take it all in and get caught up on any of the old pods you've been
saving up.
Happy holidays and happy new year.
See you in 2018.
And of course, thanks once again for listening and catch you next time.
