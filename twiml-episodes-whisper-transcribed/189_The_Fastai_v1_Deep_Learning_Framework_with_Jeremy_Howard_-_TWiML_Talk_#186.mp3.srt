1
00:00:00,000 --> 00:00:15,240
Hello and welcome to another episode of Twomble Talk, the podcast where I interview interesting

2
00:00:15,240 --> 00:00:19,960
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:19,960 --> 00:00:30,800
I'm your host, Sam Charrington.

4
00:00:30,800 --> 00:00:34,720
For today's show, we'll be taking a break from our strategy data conference series coverage

5
00:00:34,720 --> 00:00:40,520
and presenting a special conversation recorded yesterday with Jeremy Howard, founder and researcher

6
00:00:40,520 --> 00:00:46,000
at fast.ai, accompanying many of our listeners are quite familiar with due to their popular

7
00:00:46,000 --> 00:00:48,240
deep learning course.

8
00:00:48,240 --> 00:00:51,900
The podcast is being released today in conjunction with the company's announcement of version

9
00:00:51,900 --> 00:00:59,040
1 of their fast AI library at the inaugural PyTorch DevCon in San Francisco.

10
00:00:59,040 --> 00:01:02,080
Jeremy and I cover a ton of ground in this conversation.

11
00:01:02,080 --> 00:01:05,920
Of course, we dive into the new version of the library and explore why it's important

12
00:01:05,920 --> 00:01:07,480
and what's changed.

13
00:01:07,480 --> 00:01:12,160
We also explore the unique way in which it was developed and what it means for the future

14
00:01:12,160 --> 00:01:14,520
of the fast.ai courses.

15
00:01:14,520 --> 00:01:19,480
What's more, Jeremy shares a ton of great insights and lessons learned in this conversation,

16
00:01:19,480 --> 00:01:22,960
not to mention a bunch of really interesting sounding papers.

17
00:01:22,960 --> 00:01:24,880
I know you'll enjoy this one.

18
00:01:24,880 --> 00:01:29,360
Alright, let's do it.

19
00:01:29,360 --> 00:01:34,360
Alright everyone, I am on the line with Jeremy Howard.

20
00:01:34,360 --> 00:01:38,600
Jeremy is founder and researcher with fast AI.

21
00:01:38,600 --> 00:01:41,840
Jeremy, welcome to this week in machine learning and AI.

22
00:01:41,840 --> 00:01:43,560
Thank you very much.

23
00:01:43,560 --> 00:01:46,400
It is great to finally get you on the show.

24
00:01:46,400 --> 00:01:55,880
As you know, the tool more community in particular, our meet up has really been enjoying the

25
00:01:55,880 --> 00:01:57,600
deep learning for coders course.

26
00:01:57,600 --> 00:02:05,520
We brought a group of folks through that course earlier this summer and spun up a second

27
00:02:05,520 --> 00:02:08,640
group to work on that course.

28
00:02:08,640 --> 00:02:12,240
And then you've recently announced the machine learning course and we've got a group starting

29
00:02:12,240 --> 00:02:16,320
soon that will be working on that course together.

30
00:02:16,320 --> 00:02:20,160
So we are big fans of fast AI.

31
00:02:20,160 --> 00:02:25,280
The interview I did with Rachel, that was Twomble Talk 138 back in May, remains one of

32
00:02:25,280 --> 00:02:28,720
the most popular ones shows to date.

33
00:02:28,720 --> 00:02:30,400
So I am super excited.

34
00:02:30,400 --> 00:02:35,400
So I'm the pleasure of all mine and let me just say thank you so much for everything

35
00:02:35,400 --> 00:02:42,200
that you're doing for the machine learning and AI community and thanks so much for spending

36
00:02:42,200 --> 00:02:44,080
time looking at our little course.

37
00:02:44,080 --> 00:02:47,000
I'm really grateful.

38
00:02:47,000 --> 00:02:52,640
It is a great course and we'll jump into all the reasons why I am sure.

39
00:02:52,640 --> 00:02:57,480
But before we do that, I want to talk a little bit about your background.

40
00:02:57,480 --> 00:03:02,520
So you've been former president and chief scientist at Kaggle.

41
00:03:02,520 --> 00:03:08,000
You founded several startups in this space, your published researcher as well as on the

42
00:03:08,000 --> 00:03:12,000
faculty at U.S.F. and you're unabashedly Ph.D.

43
00:03:12,000 --> 00:03:13,000
Less.

44
00:03:13,000 --> 00:03:14,000
That's true.

45
00:03:14,000 --> 00:03:19,960
Unlike Rachel, who is facetly Ph.D. or something.

46
00:03:19,960 --> 00:03:26,280
So I want to start out by talking about, I want to give you a chance to more fully walk

47
00:03:26,280 --> 00:03:35,040
folks through your background, but also maybe end up at the Ph.D. I think has come to,

48
00:03:35,040 --> 00:03:43,840
this is a space where the Ph.D. carries a lot of weight and I'm curious your thoughts

49
00:03:43,840 --> 00:03:48,400
on kind of navigating the space and doing all that you've done without one and what it

50
00:03:48,400 --> 00:03:52,000
says to other folks that want to participate in the space.

51
00:03:52,000 --> 00:03:57,000
Yeah, I mean, I understand where the question is coming from Sam because it's definitely

52
00:03:57,000 --> 00:04:03,400
always been intimidating and terrifying for me doing what I'm doing without an academic

53
00:04:03,400 --> 00:04:04,920
background.

54
00:04:04,920 --> 00:04:11,040
And I can't begin to tell you how deeply surprised I was when I discovered I was actually pretty

55
00:04:11,040 --> 00:04:15,640
good at machine learning because I had, no, literally I had no idea, like you have to

56
00:04:15,640 --> 00:04:17,640
realize I, I mean, sort of answer your question.

57
00:04:17,640 --> 00:04:25,640
I started my career when I was 18 at a, at a strategy corporate strategy company called

58
00:04:25,640 --> 00:04:34,160
McKinsey and Company, I got a degree in a Bachelor of Arts in Philosophy, but I didn't

59
00:04:34,160 --> 00:04:38,840
turn up to any lectures because I was working full time, so I just turned up to exams.

60
00:04:38,840 --> 00:04:43,920
So I really, even though in theory I have a degree, I haven't really studied anything

61
00:04:43,920 --> 00:04:47,240
much in a formal way.

62
00:04:47,240 --> 00:04:53,240
So I'm kind of very unfamiliar with the university system and the academic system overall.

63
00:04:53,240 --> 00:04:59,200
And so I spent, you know, eight years in corporate strategy and then 10 years running a couple

64
00:04:59,200 --> 00:05:06,160
of companies, which were somewhat related to machine learning, but one of them was an

65
00:05:06,160 --> 00:05:09,440
email provider called FastMail, which is still pretty popular.

66
00:05:09,440 --> 00:05:15,520
And you know, the machine learning I did there was like, you know, try to improve the spam

67
00:05:15,520 --> 00:05:17,440
detector mainly.

68
00:05:17,440 --> 00:05:22,840
And then the other one was an insurance pricing company called Optimal Sisions, which

69
00:05:22,840 --> 00:05:28,960
was more about operations research and optimization and simulation than it was about predictive

70
00:05:28,960 --> 00:05:29,960
modeling.

71
00:05:29,960 --> 00:05:36,880
So at the end of all that, when I entered my first Kaggle competition, because I wanted

72
00:05:36,880 --> 00:05:43,880
to like finally learn to do machine learning properly, and I won it, I was just like, that

73
00:05:43,880 --> 00:05:48,640
can't be right, because I knew that it was an econometrics competition and there was

74
00:05:48,640 --> 00:05:54,120
a lot of PhDs and professors and econometrics in the competition and I just thought, that's

75
00:05:54,120 --> 00:06:02,760
really weird that some kind of self-taught hack, you know, business guy could possibly

76
00:06:02,760 --> 00:06:06,280
beat these guys at predicting time series.

77
00:06:06,280 --> 00:06:11,920
I just thought that was, yeah, deeply weird and surprising and very pleasing.

78
00:06:11,920 --> 00:06:15,720
I mean, it was the start of a new career for me because when you find out you're good

79
00:06:15,720 --> 00:06:20,680
at something, you kind of, you know, at least in my case, I went all in on trying to be as

80
00:06:20,680 --> 00:06:22,680
good as I could be.

81
00:06:22,680 --> 00:06:28,760
I didn't realize that your career at Kaggle began with competing in a competition.

82
00:06:28,760 --> 00:06:29,760
Yeah.

83
00:06:29,760 --> 00:06:37,440
So at that point, it was just Anthony Goldblum and he had, you know, hired a contractor

84
00:06:37,440 --> 00:06:46,000
to kind of hack together a site based on his idea of creating a competition platform.

85
00:06:46,000 --> 00:06:54,640
And yeah, I came across it because I went to the R meetup in Melbourne thinking that would

86
00:06:54,640 --> 00:07:00,480
be a good way to learn to do machine learning properly and somebody there told me, oh, there's

87
00:07:00,480 --> 00:07:05,560
a good way to learn machine learning would be to try one of these Kaggle competitions.

88
00:07:05,560 --> 00:07:11,320
And so, yeah, so I did much to my surprise, you know, I won my first one and I think I also

89
00:07:11,320 --> 00:07:15,600
won my second one and I got to the top of the leaderboard and so by the time the next

90
00:07:15,600 --> 00:07:20,960
meetup came along, Anthony was actually at the meetup and somebody introduced me and

91
00:07:20,960 --> 00:07:27,360
by that stage, he knew who I was because I was the highest ranked Kaggleer and I ended

92
00:07:27,360 --> 00:07:30,400
up becoming the first investor in the company.

93
00:07:30,400 --> 00:07:38,120
And then I rewrote the whole platform from scratch as a volunteer because it turned out

94
00:07:38,120 --> 00:07:41,160
it wasn't really going to be scalable enough to do what was needed to be done.

95
00:07:41,160 --> 00:07:46,000
And yeah, I ended up becoming an equal partner in the company.

96
00:07:46,000 --> 00:07:48,920
So that was, yeah, that was a cool way to get involved.

97
00:07:48,920 --> 00:07:51,120
Yeah, that's that's pretty amazing.

98
00:07:51,120 --> 00:07:57,520
And people often kind of poo poo Kaggle competitions and they do with comments like, oh, it's

99
00:07:57,520 --> 00:08:01,520
not really the real world, all the hard stuff is done for you.

100
00:08:01,520 --> 00:08:04,640
I imagine you have a slightly different to the Kaggle competition.

101
00:08:04,640 --> 00:08:09,840
So these are people who, these are people who have heard third hand and it's, yeah, what's

102
00:08:09,840 --> 00:08:10,840
your take on that?

103
00:08:10,840 --> 00:08:17,440
A lot of the exercises and the deep learning for coders course are, hey, just go find

104
00:08:17,440 --> 00:08:19,960
a data set on Kaggle and do some stuff with it.

105
00:08:19,960 --> 00:08:27,880
So you obviously believe pretty strongly in that as a way to learn.

106
00:08:27,880 --> 00:08:28,960
Yeah, I do.

107
00:08:28,960 --> 00:08:34,560
You know, that's a great question, actually, because you know, like with many of these

108
00:08:34,560 --> 00:08:39,320
troubling myths, there's just enough of an element of truth in it to make it sound

109
00:08:39,320 --> 00:08:45,800
believable, you know, which is like, obviously, there is a lot to building a data product

110
00:08:45,800 --> 00:08:52,880
or solving a data-oriented problem that is not about creating a more predictive, predictive

111
00:08:52,880 --> 00:08:53,880
model.

112
00:08:53,880 --> 00:08:55,760
So yes, that's true.

113
00:08:55,760 --> 00:09:01,040
But then, you know, taking that premise that the, therefore, competing in Kaggle competitions

114
00:09:01,040 --> 00:09:06,400
as a waste of time is a totally ridiculous leap.

115
00:09:06,400 --> 00:09:13,240
It's actually, if you want to do a good job of your data product, having a predictive

116
00:09:13,240 --> 00:09:17,880
model that's good at predicting things is actually a pretty good, pretty important part

117
00:09:17,880 --> 00:09:19,200
of that.

118
00:09:19,200 --> 00:09:26,880
Furthermore, Bryman, who developed the Random Forest amongst other things, had this

119
00:09:26,880 --> 00:09:33,640
fantastic two cultures of statistics paper in which he talked about how incredibly

120
00:09:33,640 --> 00:09:40,480
powerful a good predictive model is is providing a platform for data interpretation for understanding

121
00:09:40,480 --> 00:09:41,480
your data.

122
00:09:41,480 --> 00:09:45,280
And that's one of the things that I spend a lot of time on on the new machine learning

123
00:09:45,280 --> 00:09:46,480
course.

124
00:09:46,480 --> 00:09:51,720
So if you're going to kick us on a Kaggle competition, you need to understand the data

125
00:09:51,720 --> 00:09:54,480
really, really well.

126
00:09:54,480 --> 00:09:59,040
And so if you're good at understanding data really, really well and building models that

127
00:09:59,040 --> 00:10:03,600
are very predictive, and you also have to be really good at software engineering,

128
00:10:03,600 --> 00:10:06,960
in a very practical way, because every idea you come up with, you have to be able to

129
00:10:06,960 --> 00:10:12,160
code it in a way that actually works correctly and that's tested.

130
00:10:12,160 --> 00:10:15,680
And then you need to make sure it's maintainable because you've got to patch lots and things

131
00:10:15,680 --> 00:10:19,000
on top of each other over the three months of the competition.

132
00:10:19,000 --> 00:10:26,280
So realistically, if you can get a good result on a Kaggle competition, you have exercised

133
00:10:26,280 --> 00:10:31,840
many of the pieces necessary for machine learning in the real world.

134
00:10:31,840 --> 00:10:37,800
The other pieces like productionising that model, that's a whole different skill, which

135
00:10:37,800 --> 00:10:43,480
you can practice elsewhere, or skills like figuring out what problem to solve and what

136
00:10:43,480 --> 00:10:45,680
constraints there are to solve that problem.

137
00:10:45,680 --> 00:10:49,680
I mean, that's kind of more of a management consulting corporate strategy kind of issue,

138
00:10:49,680 --> 00:10:52,000
which again, there are resources for that.

139
00:10:52,000 --> 00:10:56,800
But yeah, for the part that is really about machine learning, Kaggle competitions are

140
00:10:56,800 --> 00:10:58,720
a great exercise.

141
00:10:58,720 --> 00:11:05,180
You made an interesting comment in there that winning a Kaggle competition and building

142
00:11:05,180 --> 00:11:13,400
models in general is very much about really understanding the data.

143
00:11:13,400 --> 00:11:20,600
And in the deep learning for coders, part one course, you kind of, my personal experience,

144
00:11:20,600 --> 00:11:28,600
we kind of sail through the first three lessons that were focused on building object detectors

145
00:11:28,600 --> 00:11:30,080
and things like that.

146
00:11:30,080 --> 00:11:39,120
And then we got to this really interesting set of lessons around building, using machine

147
00:11:39,120 --> 00:11:43,480
learning for more tabular data.

148
00:11:43,480 --> 00:11:49,000
And the promise of that is that actually kind of the opposite of what you said, that

149
00:11:49,000 --> 00:11:59,120
previously, in order to really work with traditional enterprise data, a data scientist really

150
00:11:59,120 --> 00:12:05,360
had to be a domain expert and understand those domains and the data sources very deeply.

151
00:12:05,360 --> 00:12:11,440
The impression a lot of us took from what we learned in applying deep learning to tabular

152
00:12:11,440 --> 00:12:18,680
data is that we didn't have to be as deeply ingrained in that particular field because the

153
00:12:18,680 --> 00:12:22,800
network would learn a lot of the patterns for us.

154
00:12:22,800 --> 00:12:26,280
How do you reconcile those two perspectives?

155
00:12:26,280 --> 00:12:32,080
Well, the first thing to note is there's a difference between getting a pretty good model

156
00:12:32,080 --> 00:12:34,280
and winning a Kaggle competition.

157
00:12:34,280 --> 00:12:38,760
So winning a Kaggle competition, you actually have to do everything better than everybody

158
00:12:38,760 --> 00:12:42,600
else because if you don't, somebody else will do that thing better than you and will

159
00:12:42,600 --> 00:12:43,600
beat you.

160
00:12:43,600 --> 00:12:48,880
So you know, Kaggle, it's definitely true that Kaggle solutions, like the top 10 Kaggle

161
00:12:48,880 --> 00:12:54,280
solutions are going to be very over engineered for practical purposes.

162
00:12:54,280 --> 00:13:01,800
But what they show you is kind of the full menu of things which can improve your model

163
00:13:01,800 --> 00:13:05,880
and you can kind of pull each one out one at a time to figure out how important each

164
00:13:05,880 --> 00:13:07,080
piece is.

165
00:13:07,080 --> 00:13:14,680
So doing some amount of feature engineering is almost always helpful and doing a lot is

166
00:13:14,680 --> 00:13:17,600
going to be necessary to actually winning a competition.

167
00:13:17,600 --> 00:13:26,880
Having said that, it's definitely true that deep learning allows us to do less feature

168
00:13:26,880 --> 00:13:33,560
engineering and still get as good results as we might of with a GBM or a random forest

169
00:13:33,560 --> 00:13:34,560
or something.

170
00:13:34,560 --> 00:13:41,560
And there's been some interesting papers and talks from folks like Instacart and Pinterest

171
00:13:41,560 --> 00:13:45,800
who have switched from GBM based methods in their companies.

172
00:13:45,800 --> 00:13:52,920
So Pinterest switched from GBM to deep learning, for example, for their home page, you know,

173
00:13:52,920 --> 00:13:55,760
kind of main home page recommendation feed.

174
00:13:55,760 --> 00:14:01,720
And they've talked about how doing that made their engineering process less complex because

175
00:14:01,720 --> 00:14:07,520
they didn't need to do as much feature engineering as they did before, the architecture kind

176
00:14:07,520 --> 00:14:10,080
of did more for them.

177
00:14:10,080 --> 00:14:14,960
So you generally, you know, with deep learning, you don't have to bucketize your continuous

178
00:14:14,960 --> 00:14:18,080
variables, which some kinds of model require.

179
00:14:18,080 --> 00:14:22,800
You don't have to create interactions, which some kinds of model require.

180
00:14:22,800 --> 00:14:28,240
You don't have to do special tricks to allow it to extrapolate further, which you certainly

181
00:14:28,240 --> 00:14:31,200
need for any tree based method.

182
00:14:31,200 --> 00:14:34,440
So yeah, I think both of those things are true at the same time.

183
00:14:34,440 --> 00:14:43,480
Yeah, certainly the course in our group created a number of fans of the whole technique of

184
00:14:43,480 --> 00:14:44,480
entity embeddings.

185
00:14:44,480 --> 00:14:45,480
Oh, yeah.

186
00:14:45,480 --> 00:14:46,480
Yeah.

187
00:14:46,480 --> 00:14:47,480
Yeah.

188
00:14:47,480 --> 00:14:48,480
Very underappreciated.

189
00:14:48,480 --> 00:14:55,120
I mean, the idea that you can use a mixture of categorical and continuous variables kind

190
00:14:55,120 --> 00:14:58,800
of without thinking about it is great.

191
00:14:58,800 --> 00:15:02,880
And you can use them for time series, you can use them for tabular, you can use them

192
00:15:02,880 --> 00:15:06,280
for collaborative filtering, you can use them for text.

193
00:15:06,280 --> 00:15:08,520
I mean, they pop up everywhere.

194
00:15:08,520 --> 00:15:14,640
And actually, I'll be interested to see your feedback when you try it.

195
00:15:14,640 --> 00:15:22,280
But the new Fast AI version one library makes that ridiculously easy.

196
00:15:22,280 --> 00:15:28,240
Like now you can basically create a model with a mixture of categorical and continuous variables

197
00:15:28,240 --> 00:15:34,240
in, you know, and train it in three or four lines of code, because we're really trying

198
00:15:34,240 --> 00:15:40,400
to, yeah, make that that idea of entity embeddings be as natural to use as possible.

199
00:15:40,400 --> 00:15:47,680
Well, I appreciate you helping us get to talking about the new library, which you're going

200
00:15:47,680 --> 00:15:53,600
to be announcing tomorrow today for those who listen to the podcast the day it's released

201
00:15:53,600 --> 00:15:56,400
because we're going to try and get this out tomorrow.

202
00:15:56,400 --> 00:15:58,440
Let's talk about the library.

203
00:15:58,440 --> 00:16:04,600
So Fast AI as a company is focused on research, it's focused on software, it's focused

204
00:16:04,600 --> 00:16:05,600
on courses.

205
00:16:05,600 --> 00:16:09,960
We talked a little bit about all of that with Rachel.

206
00:16:09,960 --> 00:16:13,440
This is really about the software.

207
00:16:13,440 --> 00:16:14,440
Yeah.

208
00:16:14,440 --> 00:16:19,720
And this is the long term vision of what Fast.ai is doing.

209
00:16:19,720 --> 00:16:25,880
And I'm a bit sloppy about this myself, but our company is Fast.ai and the software is

210
00:16:25,880 --> 00:16:28,720
just Fast.ai.

211
00:16:28,720 --> 00:16:35,600
So Fast.ai, you know, it's all about getting to a point where people can use deep lining

212
00:16:35,600 --> 00:16:40,360
to help them do whatever it is they're doing really easily.

213
00:16:40,360 --> 00:16:43,560
And to allow anybody to do that, we actually have to get to a point where you don't have

214
00:16:43,560 --> 00:16:49,360
to use code at all because only something like 0.1% of the global population knows how

215
00:16:49,360 --> 00:16:50,680
to code.

216
00:16:50,680 --> 00:16:56,240
So kind of the Fast.ai library is step one along that path to kind of require less and less

217
00:16:56,240 --> 00:17:01,720
code to be able to do more and more things, more and more reliably, more and more quickly.

218
00:17:01,720 --> 00:17:08,480
Since you grounded us out on terminology, another area that's somewhat confusing is that

219
00:17:08,480 --> 00:17:19,440
you're announcing what you call the V1 version one of the Fast.ai library, but it supersedes

220
00:17:19,440 --> 00:17:26,880
a previous version of the library that was also written in PyTorch, which itself supersedes

221
00:17:26,880 --> 00:17:33,240
a previous version of the library that was written in TensorFlow.

222
00:17:33,240 --> 00:17:38,680
So when we talk about the V1 library, we're talking about the new thing.

223
00:17:38,680 --> 00:17:39,680
Yeah.

224
00:17:39,680 --> 00:17:45,480
So just to go back through that, so the previous thing was version 0.7.

225
00:17:45,480 --> 00:17:52,000
And that never got a version one tag because I wrote it knowing that it was not going

226
00:17:52,000 --> 00:17:56,160
to be that particularly close to the final form of what we wanted.

227
00:17:56,160 --> 00:18:01,000
It was kind of something that I hacked together that was good enough for the needs of the

228
00:18:01,000 --> 00:18:08,920
initial PyTorch-based courses, because as you won't know, PyTorch is really not suitable

229
00:18:08,920 --> 00:18:13,200
on its own as a first library for deep learning because you have to write your own training

230
00:18:13,200 --> 00:18:18,440
loop and you have to do a lot of things yourself, and it's an amazingly great library,

231
00:18:18,440 --> 00:18:22,720
but it's kind of missing that carous style layer on top.

232
00:18:22,720 --> 00:18:31,720
So we kind of built the amount we needed to get people going, but it was certainly not

233
00:18:31,720 --> 00:18:38,920
a really carefully integrated, fully thought through library.

234
00:18:38,920 --> 00:18:47,080
So here we are, 18 months later, we've rewritten the entire thing from scratch in a way which

235
00:18:47,080 --> 00:18:51,280
is explicitly designed to provide a long-term foundation for all the software we built

236
00:18:51,280 --> 00:18:52,440
for now on.

237
00:18:52,440 --> 00:18:57,960
And then before that was something that was, yeah, a sad on top of carous and TensorFlow,

238
00:18:57,960 --> 00:19:00,960
and I don't think we ever even called that a library or gave it a name.

239
00:19:00,960 --> 00:19:07,920
It was only a kind of bunch of little utilities that's sad on top of carous, to smooth over

240
00:19:07,920 --> 00:19:10,520
some of the slightly rough edges.

241
00:19:10,520 --> 00:19:13,600
So yeah, that's been kind of the progression.

242
00:19:13,600 --> 00:19:18,920
And so it's really the first time where we're saying, okay, you know what, this software

243
00:19:18,920 --> 00:19:24,880
is actually now, we think pretty damn good, we're pretty damn proud of it, we think people

244
00:19:24,880 --> 00:19:31,200
should start using it for, you know, basically anybody who's trying to train neural nets, we

245
00:19:31,200 --> 00:19:35,360
think this is the best software out there for doing that.

246
00:19:35,360 --> 00:19:43,120
And specifically for kind of production use as opposed to education or rapid prototyping?

247
00:19:43,120 --> 00:19:50,720
Yeah, and I know there are certainly Fortune 500 companies using FastAI 0.7, but you know,

248
00:19:50,720 --> 00:19:55,880
they're nearly entirely Fortune 500 companies where they have a lot of their technical

249
00:19:55,880 --> 00:20:00,480
staff who have done the courses and got introduced to it that way and are kind of happy to dig

250
00:20:00,480 --> 00:20:04,440
into the source code as necessary to make it work the way they wanted to.

251
00:20:04,440 --> 00:20:12,080
So yeah, now this is, you know, we think this is totally ready for everybody to use.

252
00:20:12,080 --> 00:20:18,760
It's also like a good time for it because it's aligned with the PyTorch version 1 release.

253
00:20:18,760 --> 00:20:22,720
I mean, we're a little bit ahead of them, so we're actually using the PyTorch version

254
00:20:22,720 --> 00:20:32,040
1 pre-release now and when the final release comes out, we'll be obviously supporting that.

255
00:20:32,040 --> 00:20:38,240
But PyTorch version 1 has done a lot of work on the productionization story with the

256
00:20:38,240 --> 00:20:43,360
integration of Cafe 2 and, you know, the kind of full support of Owen and X and stuff

257
00:20:43,360 --> 00:20:44,360
like that.

258
00:20:44,360 --> 00:20:50,760
So because any FastAI model is also a PyTorch model, you can use all of that new functionality

259
00:20:50,760 --> 00:20:54,960
to serve your FastAI models directly.

260
00:20:54,960 --> 00:21:01,920
You made an interesting post about the methodology behind developing the new library that was

261
00:21:01,920 --> 00:21:07,920
actually somewhat controversial on the twimmel slack.

262
00:21:07,920 --> 00:21:14,760
If I remember the gist of it, it was that the library was built, just kind of built from

263
00:21:14,760 --> 00:21:23,360
the ground up relative to 0.7 and specifically built kind of using or around the Jupyter

264
00:21:23,360 --> 00:21:28,560
notebooks that will eventually become part of the course or maybe the part 2 version

265
00:21:28,560 --> 00:21:29,560
of the course.

266
00:21:29,560 --> 00:21:32,000
Yeah, talk a little bit about that process.

267
00:21:32,000 --> 00:21:34,960
Yeah, okay, so this is actually pretty fascinating.

268
00:21:34,960 --> 00:21:39,880
I'm glad you boarded up because it's not something I've had a chance to write about yet.

269
00:21:39,880 --> 00:21:43,120
So this will be the first kind of proper description of this.

270
00:21:43,120 --> 00:21:45,160
Here's what happened.

271
00:21:45,160 --> 00:21:49,280
The first thing is I just love working in Jupyter notebooks.

272
00:21:49,280 --> 00:21:55,800
I am, you know, I've been coding for gosh, you know, well over 30 years.

273
00:21:55,800 --> 00:22:02,160
I just, in many different languages, but I just write better code faster when I'm in

274
00:22:02,160 --> 00:22:04,560
a notebook.

275
00:22:04,560 --> 00:22:09,320
And so I like that, but it's, you know, writing stuff in a notebook is not something that's

276
00:22:09,320 --> 00:22:14,920
really been well suited to creating, you know, reusable modules in the past.

277
00:22:14,920 --> 00:22:17,360
So kind of that was issue number one.

278
00:22:17,360 --> 00:22:23,280
Issue number two is that we've got this kind of unique thing we do when there's an in-person

279
00:22:23,280 --> 00:22:28,320
course on where I make a big room available every day during the course.

280
00:22:28,320 --> 00:22:31,920
And I tell everybody all the students, I'm going to be working in this room.

281
00:22:31,920 --> 00:22:37,000
If anybody else wants to work with me, you are most welcome to do so.

282
00:22:37,000 --> 00:22:41,760
And so during the course, we have a whole bunch of fast AI students, you know, hanging out

283
00:22:41,760 --> 00:22:43,160
working together.

284
00:22:43,160 --> 00:22:49,000
And generally I've got my work being projected onto a big screen so people can kind of watch.

285
00:22:49,000 --> 00:22:53,840
And one of the most common things I hear from the students is, gosh, I learned so much

286
00:22:53,840 --> 00:23:00,640
watching you test and refactor and build, which I can't, I don't get out of the course,

287
00:23:00,640 --> 00:23:04,040
you know, like in the course, everything kind of just appears all done.

288
00:23:04,040 --> 00:23:08,960
And I think one of the things people surprised me is how much I screw things up, you know,

289
00:23:08,960 --> 00:23:11,880
I'm just constantly making mistakes and fixing them.

290
00:23:11,880 --> 00:23:17,440
And, you know, everything is a lot harder than people perhaps realize based on what they

291
00:23:17,440 --> 00:23:18,680
see in the course.

292
00:23:18,680 --> 00:23:24,800
So the question I often get is like, how can we learn to kind of develop software, you

293
00:23:24,800 --> 00:23:28,440
know, machine learning software, the way you're developing it?

294
00:23:28,440 --> 00:23:34,800
So what I did was I decided the next version of the software version, one of fast AI,

295
00:23:34,800 --> 00:23:40,040
which we're releasing today, I am going to build in notebooks where each stage, I'm

296
00:23:40,040 --> 00:23:42,320
going to like, I'm going to leave all the cells in there.

297
00:23:42,320 --> 00:23:45,360
So you can see every stage of that progression.

298
00:23:45,360 --> 00:23:49,400
So you can see what I built first and then how I refactored it and then what I checked

299
00:23:49,400 --> 00:23:51,480
it against and so forth.

300
00:23:51,480 --> 00:23:57,840
So as a result, there's this series of, I don't know, something like 14 or 15 notebooks,

301
00:23:57,840 --> 00:24:02,360
which if you go through the whole thing, you end up writing the entire fast AI library

302
00:24:02,360 --> 00:24:03,360
yourself.

303
00:24:03,360 --> 00:24:04,520
Wow.

304
00:24:04,520 --> 00:24:13,600
And so that 14 notebooks, and that'll eventually be the part two course, which the current

305
00:24:13,600 --> 00:24:17,680
part two course is kind of taking you into the internals of the old library.

306
00:24:17,680 --> 00:24:21,160
So this is a new approach to doing that for the new library.

307
00:24:21,160 --> 00:24:22,160
Yeah.

308
00:24:22,160 --> 00:24:27,200
And in the process, you're going to have to learn about a lot of recent research results

309
00:24:27,200 --> 00:24:33,600
because as you're aware, the, you know, even the 0.7 and particularly 1.0 integrates

310
00:24:33,600 --> 00:24:38,520
a lot of recent research results to kind of make them directly available.

311
00:24:38,520 --> 00:24:44,200
So as you go through part two, you're going to be like learning about, you know, what

312
00:24:44,200 --> 00:24:49,400
paper is this particular line of code based on and why has it done that way and so forth.

313
00:24:49,400 --> 00:24:55,360
So yeah, you're, you're both learned a lot about modern kind of cutting edge deep learning

314
00:24:55,360 --> 00:25:00,920
research, but also about the process of building machine learning software.

315
00:25:00,920 --> 00:25:05,440
And so what we did is like really simple, but it worked really well is from time to time

316
00:25:05,440 --> 00:25:09,840
as I was kind of building a notebook and I could create a function that I kind of think,

317
00:25:09,840 --> 00:25:13,320
okay, that's probably going to be useful to use again.

318
00:25:13,320 --> 00:25:17,680
So I just put a little comment at the top and my comment was always a hash export.

319
00:25:17,680 --> 00:25:21,520
And anytime I had a little thing running in the background that anytime it saw a cell that

320
00:25:21,520 --> 00:25:26,880
said hash export, it would chuck it into a Python module.

321
00:25:26,880 --> 00:25:31,400
And so I could then build the next notebook on top of the previous notebook by simply importing

322
00:25:31,400 --> 00:25:34,760
the previous notebooks, auto generated module.

323
00:25:34,760 --> 00:25:38,960
And so then at the end of all that, we then went through a manual process of kind of

324
00:25:38,960 --> 00:25:42,520
figuring out like, okay, well, what have we ended up with here and we kind of structured

325
00:25:42,520 --> 00:25:49,520
things into a carefully decoupled set of independent modules, which ended up being

326
00:25:49,520 --> 00:25:53,880
fast AI and wrote all the documentation and so on and so forth.

327
00:25:53,880 --> 00:26:00,600
But you know, it really, you can really see the final result is close to identical to

328
00:26:00,600 --> 00:26:03,000
what you'll see in those notebooks.

329
00:26:03,000 --> 00:26:08,600
One of the issues that was raised, and I think I raised questions along this line, well,

330
00:26:08,600 --> 00:26:12,880
I'm sure I raised questions along this line as well and our slack is, you know, that sounds

331
00:26:12,880 --> 00:26:20,440
like a very kind of organic way of coming up with the library that may not lend itself

332
00:26:20,440 --> 00:26:28,320
well to kind of the door ability and, you know, well architected APIs that you would want

333
00:26:28,320 --> 00:26:30,840
if you're going to be using this for a general purpose.

334
00:26:30,840 --> 00:26:31,840
Like how do you?

335
00:26:31,840 --> 00:26:33,160
That's a great question.

336
00:26:33,160 --> 00:26:36,920
So that's how I used to think too.

337
00:26:36,920 --> 00:26:47,080
And about 20 years ago, I got maybe a bit less, I got super interested in test driven development.

338
00:26:47,080 --> 00:26:51,040
And for those that aren't familiar with it, the kind of basic idea of test driven development

339
00:26:51,040 --> 00:26:55,720
is you kind of write a bunch of tests, you make each one pass one at a time.

340
00:26:55,720 --> 00:27:01,680
Every time you find you've got some duplicate code, you refactor it into a class or a function

341
00:27:01,680 --> 00:27:06,000
and you try to figure out what it is that class or functions doing based on that duplicate

342
00:27:06,000 --> 00:27:08,240
code and give it a name.

343
00:27:08,240 --> 00:27:11,320
And you keep on repeating this again and again and again.

344
00:27:11,320 --> 00:27:15,120
And I kind of just played around with it because a few people I respected thought it was

345
00:27:15,120 --> 00:27:16,120
great.

346
00:27:16,120 --> 00:27:18,040
I thought it was really weird.

347
00:27:18,040 --> 00:27:21,640
And one of my key issues was what you just said, which is, well, what point do you actually

348
00:27:21,640 --> 00:27:26,480
design a durable thought for API?

349
00:27:26,480 --> 00:27:33,080
Much to my surprise, I discovered that this process of testing and refactoring naturally

350
00:27:33,080 --> 00:27:37,960
ended up just through having to refactor out these abstractions and then abstractions

351
00:27:37,960 --> 00:27:42,400
on top of those and then abstractions on top of those and carefully naming things.

352
00:27:42,400 --> 00:27:49,160
Yeah, like I ended up with better APIs than I'd ever written when I'd carefully designed

353
00:27:49,160 --> 00:27:50,160
them.

354
00:27:50,160 --> 00:27:53,480
And you know, I should mention I've spent a lot of time designing APIs.

355
00:27:53,480 --> 00:27:56,440
I wrote a number of MVC modules.

356
00:27:56,440 --> 00:28:00,680
I was the Pell 6 chair for all the data functionality in Pell 6.

357
00:28:00,680 --> 00:28:07,560
So I like explicitly was writing RFCs for that API, you know, I've written a lot of

358
00:28:07,560 --> 00:28:14,040
APIs in my time and I've discovered that this organic approach, I end up with better APIs.

359
00:28:14,040 --> 00:28:19,120
I end up with, I end up with no stuff in them which actually doesn't need to be there.

360
00:28:19,120 --> 00:28:23,040
So unnecessary complexity is entirely avoided because you only build what you need.

361
00:28:23,040 --> 00:28:30,880
So I end up with less kind of unnecessary and over complex abstractions.

362
00:28:30,880 --> 00:28:36,400
So I kind of end up with something that's concise and neat and I don't know, like when

363
00:28:36,400 --> 00:28:37,640
you try it out, see what you think.

364
00:28:37,640 --> 00:28:44,160
I'm really, really proud of this API, like I found that we have almost identical four

365
00:28:44,160 --> 00:28:50,080
lines of code to build a NLP model versus a computer vision model versus a tabular model

366
00:28:50,080 --> 00:28:53,400
versus a collaborative filtering model.

367
00:28:53,400 --> 00:28:59,200
You know, it's, it's been a, I found it a real pleasure to work in frankly.

368
00:28:59,200 --> 00:29:01,400
Definitely looking, looking forward to that.

369
00:29:01,400 --> 00:29:07,960
Yes, since you mentioned your work with Pearl, I will admit that in working with the 0.7

370
00:29:07,960 --> 00:29:13,760
version of the API, at some point I grumbled in our slide channel that, you know, some

371
00:29:13,760 --> 00:29:19,400
of the, you know, the attribute and function names were seemed unnecessarily terraced.

372
00:29:19,400 --> 00:29:23,000
And, you know, Jeremy must have been a Pearl developer in a former life.

373
00:29:23,000 --> 00:29:26,400
And someone said, oh, yeah, he was actually a Pearl committer.

374
00:29:26,400 --> 00:29:27,400
Yeah.

375
00:29:27,400 --> 00:29:34,640
And I mean, I think it's fair to say also that some of them were unnecessarily terraced.

376
00:29:34,640 --> 00:29:41,080
So we've actually changed from a rule of thumb of symbol parts should try to be three

377
00:29:41,080 --> 00:29:44,880
letters or less to kind of symbol parts should try to be five letters or less.

378
00:29:44,880 --> 00:29:48,320
And that's actually made a big difference to be able to say train rather than TRN and

379
00:29:48,320 --> 00:29:50,360
valid rather than VAL.

380
00:29:50,360 --> 00:29:56,000
Um, it's actually worse than that though, I'm not just a, a keen Pearl programmer, I'm also

381
00:29:56,000 --> 00:30:01,560
a keen kind of J and APL programmer where everything is one symbol.

382
00:30:01,560 --> 00:30:07,400
So we should expect like, uh, deltas and upside down deltas and Greek letters, creeping

383
00:30:07,400 --> 00:30:08,400
it.

384
00:30:08,400 --> 00:30:09,400
Not quite, right?

385
00:30:09,400 --> 00:30:14,240
Because, um, I mean, I could talk about this for hours, but I'm, I'm just fascinated

386
00:30:14,240 --> 00:30:21,120
in notation and my kind of hero here is Kenneth Iverson, the cheering award winner who wrote

387
00:30:21,120 --> 00:30:26,880
the classic cheering award lecture notation as a tool for thought.

388
00:30:26,880 --> 00:30:33,960
And in it, he describes how good notation helps you, you know, do good work, do good

389
00:30:33,960 --> 00:30:38,960
research, come up with new ideas, implement things more quickly and easily.

390
00:30:38,960 --> 00:30:46,640
And good notation as he defines it is very, very different to what kind of pairpate Python

391
00:30:46,640 --> 00:30:47,640
looks like.

392
00:30:47,640 --> 00:30:53,880
Good notation is something where you, you know, you're kind of I, um, can quickly look

393
00:30:53,880 --> 00:30:59,960
at one part of the screen and capture the gist of gist of what's going on.

394
00:30:59,960 --> 00:31:03,760
So it's about using vertical space really carefully.

395
00:31:03,760 --> 00:31:10,480
It's about kind of having common idioms be easily recognizable.

396
00:31:10,480 --> 00:31:16,160
And so I would say to get to that point where you've got notation as a tool for thought,

397
00:31:16,160 --> 00:31:24,440
you kind of have to slightly give up on Python's goal of being immensely friendly, even to

398
00:31:24,440 --> 00:31:30,120
the most new programmers and say, okay, now I actually want people to invest a little

399
00:31:30,120 --> 00:31:36,800
bit of time learning this, but the promise will be that if you do so, you will be immensely

400
00:31:36,800 --> 00:31:39,440
more productive as a result.

401
00:31:39,440 --> 00:31:45,600
So you know, we definitely don't go anywhere near to the APL and J level of that kind of

402
00:31:45,600 --> 00:31:46,600
premise.

403
00:31:46,600 --> 00:31:49,200
I don't know less it is, it is the premise.

404
00:31:49,200 --> 00:31:55,520
I don't know of J, but APL was a language that I studied for a few weeks and my first

405
00:31:55,520 --> 00:32:02,080
like computer languages survey classes at school, and I remember that those weird keyboards

406
00:32:02,080 --> 00:32:03,080
very well.

407
00:32:03,080 --> 00:32:09,840
So J is kind of an ASCII version of that largely written by Kenneth Iverson's son, actually,

408
00:32:09,840 --> 00:32:13,520
because Kenneth Iverson wrote the original APL, but you've got to realize like APL goes

409
00:32:13,520 --> 00:32:19,400
back to, I mean, it was originally written as a mathematical notation, not as a programming

410
00:32:19,400 --> 00:32:20,920
language.

411
00:32:20,920 --> 00:32:25,520
So, you know, it came out, that came out in the late 50s, the implementations came out

412
00:32:25,520 --> 00:32:33,720
in the early 60s, still today many big and successful companies choose to use APL for their

413
00:32:33,720 --> 00:32:38,200
most important stuff, particularly, you know, big hedge funds.

414
00:32:38,200 --> 00:32:43,280
So the fact that this is a notation that has survived many, many decades longer than any

415
00:32:43,280 --> 00:32:48,040
other widely in use language today, I think it tells you a lot about how extraordinarily

416
00:32:48,040 --> 00:32:49,360
it was designed.

417
00:32:49,360 --> 00:32:53,840
And it's something everybody should study it at some point, because it's, you know,

418
00:32:53,840 --> 00:33:01,080
you just learned so much from seeing this totally independent language evolution path.

419
00:33:01,080 --> 00:33:05,480
I did not realize it was still in wide use.

420
00:33:05,480 --> 00:33:07,880
Oh, yeah, absolutely.

421
00:33:07,880 --> 00:33:16,280
So you know, maybe to digress a little bit from the library, you know, you've already

422
00:33:16,280 --> 00:33:23,280
mentioned a few papers, and one of the hallmarks of the library is, you know, that you have

423
00:33:23,280 --> 00:33:30,320
identified these, in some cases, relatively obscure papers that have outsized results in

424
00:33:30,320 --> 00:33:32,400
terms of training efficiency.

425
00:33:32,400 --> 00:33:33,920
I love doing that.

426
00:33:33,920 --> 00:33:36,760
And clearly, clearly you do.

427
00:33:36,760 --> 00:33:43,000
The question, and maybe why it's a digression is, you know, how do you keep up with all of

428
00:33:43,000 --> 00:33:47,160
the stuff that's happening in the space and, you know, to the extent that you're able

429
00:33:47,160 --> 00:33:52,200
to drop in these obscure references in both code and conversation?

430
00:33:52,200 --> 00:33:53,200
Hmm.

431
00:33:53,200 --> 00:34:01,960
Twitter, basically, you know, the Twitter machine learning community is really terrific.

432
00:34:01,960 --> 00:34:06,120
And you can quickly get, you know, become a part of it.

433
00:34:06,120 --> 00:34:12,280
If you go to my Twitter page, so I am Jeremy P Howard, if you go to my profile, you can

434
00:34:12,280 --> 00:34:18,000
click on my likes, and you can immediately see, you know, what, who am I liking things

435
00:34:18,000 --> 00:34:19,000
from?

436
00:34:19,000 --> 00:34:22,720
And you'll see that there's just a long list of people posting about, basically, mainly

437
00:34:22,720 --> 00:34:24,680
about deep learning.

438
00:34:24,680 --> 00:34:27,960
So you can start following the people you find interesting.

439
00:34:27,960 --> 00:34:32,240
And it's just a really rich, highly technical community.

440
00:34:32,240 --> 00:34:39,280
And so the other thing I do is I explicitly look out for the stuff that other people are

441
00:34:39,280 --> 00:34:40,280
missing.

442
00:34:40,280 --> 00:34:45,000
Or rather than kind of trying to say, oh, here's a really popular paper everybody's talking

443
00:34:45,000 --> 00:34:46,000
about.

444
00:34:46,000 --> 00:34:52,520
Instead, I kind of look out for like, oh, here's a extremely good model somebody's built,

445
00:34:52,520 --> 00:34:56,480
but nobody's talking about this paper, because that's where I get to do something that other

446
00:34:56,480 --> 00:34:57,480
people aren't doing.

447
00:34:57,480 --> 00:34:59,560
So I get to kind of contribute more.

448
00:34:59,560 --> 00:35:04,760
And so I particularly interested in looking at, you know, I always read the winning blog posts

449
00:35:04,760 --> 00:35:11,400
from Kaggle competitions, I look at the, if you go to any machine learning or deep learning

450
00:35:11,400 --> 00:35:18,200
workshop website, you'll generally find posted papers from the people that won that competition.

451
00:35:18,200 --> 00:35:24,640
They are at a astonishing source of tricks, but those things never generally get published

452
00:35:24,640 --> 00:35:26,240
anywhere else.

453
00:35:26,240 --> 00:35:30,000
So and they're never kind of, they don't appear in search results.

454
00:35:30,000 --> 00:35:31,600
You have to go and find them.

455
00:35:31,600 --> 00:35:35,280
But in like these workshops where people say, here's how I won, you know, this academic

456
00:35:35,280 --> 00:35:38,320
competition's object detection path.

457
00:35:38,320 --> 00:35:41,360
They'll generally say, you know, these are all the other papers we looked at.

458
00:35:41,360 --> 00:35:42,560
These are the things we tried.

459
00:35:42,560 --> 00:35:43,560
These are the things we worked.

460
00:35:43,560 --> 00:35:46,000
These are the things that didn't work.

461
00:35:46,000 --> 00:35:47,600
So that's, that's a really great trick.

462
00:35:47,600 --> 00:35:53,560
Oh, that's an interesting hack to, to focus your efforts on papers where there's a competition

463
00:35:53,560 --> 00:35:54,560
involved somewhere.

464
00:35:54,560 --> 00:35:56,720
Yeah, focus on stuff that works.

465
00:35:56,720 --> 00:36:00,920
And you know, even that's much more controversial than it should be, because this is kind of

466
00:36:00,920 --> 00:36:08,520
a view in the academic community that researchers don't really need to worry about, you know,

467
00:36:08,520 --> 00:36:13,760
the latest and greatest tweaks, you know, and techniques.

468
00:36:13,760 --> 00:36:16,880
But the truth is, you know, people building deep learning models are doing it because

469
00:36:16,880 --> 00:36:20,840
they're trying to make something that's more accurate than somebody else's model or trains

470
00:36:20,840 --> 00:36:25,600
faster than somebody else's model, you know, we're not just doing it because of the mathematical

471
00:36:25,600 --> 00:36:27,000
purity of it or something.

472
00:36:27,000 --> 00:36:33,600
So, you know, furthermore, if you're claiming that your model, you know, your architecture,

473
00:36:33,600 --> 00:36:37,440
your training method, your optimizer is better than some other thing, you know, you have

474
00:36:37,440 --> 00:36:39,600
to compare it to that other thing.

475
00:36:39,600 --> 00:36:45,120
And if you don't know how to actually train a good modern model, then your experiments

476
00:36:45,120 --> 00:36:46,800
are pretty much meaningless anyway.

477
00:36:46,800 --> 00:36:52,920
So, you know, I think it's really important for practitioners to be familiar with, you

478
00:36:52,920 --> 00:36:58,040
know, in practice, how are people actually training these most accurate and fastest

479
00:36:58,040 --> 00:37:00,040
models?

480
00:37:00,040 --> 00:37:06,000
At the recent deep learning in Daba event in South Africa, Jeff Dean made a comment

481
00:37:06,000 --> 00:37:11,920
about how, you know, one of his secrets to success, if you will, is as opposed to going

482
00:37:11,920 --> 00:37:21,240
deep on fewer papers, going shallower on, you know, many more papers, he thinks is

483
00:37:21,240 --> 00:37:26,840
a preferred approach, you know, do you ascribe to something similar or do you have other

484
00:37:26,840 --> 00:37:29,760
kind of, you know, hacks for learning and keeping up?

485
00:37:29,760 --> 00:37:30,760
Yeah.

486
00:37:30,760 --> 00:37:31,760
I think that's basically right.

487
00:37:31,760 --> 00:37:37,080
I mean, like when you think about it, if you don't use that approach, then, you know,

488
00:37:37,080 --> 00:37:41,120
you know, think of like your knowledge is a weighted average of the kind of the inputs

489
00:37:41,120 --> 00:37:42,480
you're putting into it, right?

490
00:37:42,480 --> 00:37:46,720
And so, anything that you don't read or look at gets away to zero.

491
00:37:46,720 --> 00:37:50,000
So you can't like, you know, it's kind of easy to pretend like, oh, I didn't look at

492
00:37:50,000 --> 00:37:55,520
it so it doesn't count, but no, you know, you're supposed to put 98% there and 2% there

493
00:37:55,520 --> 00:37:58,240
and 0% on these other thousand things.

494
00:37:58,240 --> 00:38:03,160
So yeah, I think it's definitely worth going broad, but I'd also say like, I look out

495
00:38:03,160 --> 00:38:08,240
for the themes, you know, and so for me, one of the most important theme at the moment

496
00:38:08,240 --> 00:38:14,360
is transfer learning beats everything all the time.

497
00:38:14,360 --> 00:38:18,640
And almost nobody's really doing, you know, not totally nobody, but almost nobody's

498
00:38:18,640 --> 00:38:21,960
actually doing research about transfer learning.

499
00:38:21,960 --> 00:38:26,360
Most papers don't actually apply their techniques to transfer learning.

500
00:38:26,360 --> 00:38:28,240
So you know, here for me, here's a theme, right?

501
00:38:28,240 --> 00:38:33,960
So every time I see anything that I think, oh, that, you know, what if you added transfer

502
00:38:33,960 --> 00:38:38,400
learning to that or, you know, what if I use that transfer learning technique in this

503
00:38:38,400 --> 00:38:41,200
different field that doesn't currently use transfer learning?

504
00:38:41,200 --> 00:38:47,960
So like our ULM fit model, which is the state of the art for text classification, pretty

505
00:38:47,960 --> 00:38:53,200
much everywhere it's been looked at now in multiple languages.

506
00:38:53,200 --> 00:38:57,760
That was just me saying, how come people never use transfer learning properly in NLP?

507
00:38:57,760 --> 00:38:59,160
You know, I should try it.

508
00:38:59,160 --> 00:39:03,560
And yeah, it's kind of like following that, that, that theme.

509
00:39:03,560 --> 00:39:08,680
And I won't go deeper on that because I do have a conversation with Sebastian scheduled

510
00:39:08,680 --> 00:39:11,160
to dig into that sometime soon.

511
00:39:11,160 --> 00:39:12,160
Oh, good.

512
00:39:12,160 --> 00:39:18,880
Maybe back to the, the library and the course, I've heard some, I don't know if they're

513
00:39:18,880 --> 00:39:19,880
rumors.

514
00:39:19,880 --> 00:39:24,520
You know, there's something that you wrote gave folks the impression that the new part

515
00:39:24,520 --> 00:39:30,960
one course is kind of turning away from this top down approach and taking more of a bottom

516
00:39:30,960 --> 00:39:32,800
up approach.

517
00:39:32,800 --> 00:39:33,800
Is that true?

518
00:39:33,800 --> 00:39:38,240
And before you answer that, I will say that, you know, so we did these weekly study

519
00:39:38,240 --> 00:39:45,080
group sessions, kind of like virtual sessions where we all got online and talked about, you

520
00:39:45,080 --> 00:39:46,840
know, what we were learning in the course.

521
00:39:46,840 --> 00:39:52,240
And we spent a ton of time almost every week talking about top down and just getting

522
00:39:52,240 --> 00:39:53,240
used to it.

523
00:39:53,240 --> 00:39:59,920
It's very different from the way folks learn, but very effective at kind of getting you

524
00:39:59,920 --> 00:40:00,920
pulled in quickly.

525
00:40:00,920 --> 00:40:01,920
Yeah.

526
00:40:01,920 --> 00:40:03,880
I mean, it depends a lot on your background, right?

527
00:40:03,880 --> 00:40:11,400
I mean, you know, in a lot of kind of more MBA style stuff, it's often, it is often a

528
00:40:11,400 --> 00:40:16,920
lot more top down, a lot of kind of executive education is a lot more top down.

529
00:40:16,920 --> 00:40:21,360
You know, but most people who are kind of coding have come from more of a, yeah, maybe

530
00:40:21,360 --> 00:40:27,000
come from more of a computer science, academic background, which is very bottom up.

531
00:40:27,000 --> 00:40:34,680
So no, we're not stepping away from the top down approach at all because it's, you know,

532
00:40:34,680 --> 00:40:42,200
all of the educational research I've studied, which is a lot, shows that most, by far the

533
00:40:42,200 --> 00:40:49,240
majority of people learn better with the top down approach, even though it requires to

534
00:40:49,240 --> 00:40:54,160
some extent unlearning how to learn based on the bottom up approach that we kind of get

535
00:40:54,160 --> 00:41:00,160
used to from school and university, I will say the machine learning course, the introduction

536
00:41:00,160 --> 00:41:05,960
to machine learning course is it's still pretty top down like lesson one, you know, the

537
00:41:05,960 --> 00:41:10,600
first cell is here's a random forest, we've trained it and here's the result.

538
00:41:10,600 --> 00:41:14,720
And then we kind of gradually dig into like how do we interpret that result and how to

539
00:41:14,720 --> 00:41:18,520
be actually build that tree and by kind of lesson seven, we write our own random forest

540
00:41:18,520 --> 00:41:25,800
from scratch in pure Python, but I don't know, yeah, I think because the people I was originally

541
00:41:25,800 --> 00:41:36,120
doing that course for were master's students, I possibly was a little more, I don't know,

542
00:41:36,120 --> 00:41:40,560
go a little bit deeper a little bit earlier so that they didn't get too uncomfortable

543
00:41:40,560 --> 00:41:43,280
with the kind of change of style.

544
00:41:43,280 --> 00:41:49,200
I'm looking for the details here since you mentioned that machine learning course, of course

545
00:41:49,200 --> 00:41:55,480
has been kind of, you know, it's been around in a pre-release state, I guess for quite

546
00:41:55,480 --> 00:42:02,840
some time, but since it's been formally released, we've got a study group that some folks

547
00:42:02,840 --> 00:42:07,040
in the community are organizing that will be starting very soon.

548
00:42:07,040 --> 00:42:14,640
In fact, lesson one is going to be on Sunday, October 7th at three o'clock GMT and they're

549
00:42:14,640 --> 00:42:18,880
going to continue that for, you know, 12 weeks on Sundays at that time.

550
00:42:18,880 --> 00:42:24,760
Well, let me encourage people to get into that even if, even for folks who haven't done

551
00:42:24,760 --> 00:42:30,120
the deep learning course yet, like you can do the two in either order, they both are designed

552
00:42:30,120 --> 00:42:36,960
to work well together, but it's definitely true that people who study with a group on

553
00:42:36,960 --> 00:42:41,200
average have more effective learning outcomes and more importantly tend to stick with it

554
00:42:41,200 --> 00:42:44,840
for longer than people who study independently.

555
00:42:44,840 --> 00:42:50,440
So yeah, I think that sounds like a fantastic initiative and I hope people listening join

556
00:42:50,440 --> 00:42:51,440
in.

557
00:42:51,440 --> 00:42:58,640
Well, we had a great group that did the deep learning part one course together this summer

558
00:42:58,640 --> 00:43:08,320
and in fact, most of the, you know, this course and we've got another session of the deep

559
00:43:08,320 --> 00:43:13,440
learning course that's going now, I'll elaborate on that in a second, but these are all kind

560
00:43:13,440 --> 00:43:17,280
of being run by folks that came together to do this deep learning course together.

561
00:43:17,280 --> 00:43:22,320
So it's been awesome for us on that deep learning course.

562
00:43:22,320 --> 00:43:29,720
So we had a group that started three, three weeks ago, I think they've had three sessions,

563
00:43:29,720 --> 00:43:36,160
a second group on the deep learning for coders part one course.

564
00:43:36,160 --> 00:43:38,160
We made some tweaks to how we did it.

565
00:43:38,160 --> 00:43:43,160
The first time we did it, we did, we were planning for a weekly pace.

566
00:43:43,160 --> 00:43:47,320
We held that through about the fourth lesson and then we went to buy weekly because it

567
00:43:47,320 --> 00:43:49,400
gets a lot harder.

568
00:43:49,400 --> 00:43:50,400
Yeah.

569
00:43:50,400 --> 00:43:51,400
Yeah.

570
00:43:51,400 --> 00:43:56,040
This time they started doing a bi-weekly pace all the way through, but one of the things

571
00:43:56,040 --> 00:44:02,120
that through a little bit of a wrench in our plan, a good wrench, is that you announced

572
00:44:02,120 --> 00:44:09,200
that for the new course, which is going to be starting towards the end of October, although

573
00:44:09,200 --> 00:44:17,320
we noted that the notebooks haven't been written yet, yeah, I should probably do that.

574
00:44:17,320 --> 00:44:24,440
For this new course, previously the options for taking this course were you could take

575
00:44:24,440 --> 00:44:30,360
it in person and you offered a bunch of different types of scholarships or apply to take

576
00:44:30,360 --> 00:44:40,640
it remotely during the live course time or wait a few months and catch it via video.

577
00:44:40,640 --> 00:44:47,560
But you just announced with some pretty interesting kind of background commentary that you've

578
00:44:47,560 --> 00:44:53,560
decided to make the remote course available to anyone who wants to sign up for it.

579
00:44:53,560 --> 00:44:54,560
Yeah.

580
00:44:54,560 --> 00:44:58,480
As long as, you know, the only thing I ask is that you follow a long live, which means

581
00:44:58,480 --> 00:45:05,840
if you're in a annoying time zone for our evening US time classes that you get up and

582
00:45:05,840 --> 00:45:06,840
do it anyway.

583
00:45:06,840 --> 00:45:13,760
I want people involved to be contributing to the real-time chat during the course.

584
00:45:13,760 --> 00:45:20,520
But other than that request, yeah, it's open to anybody who has at least a year of coding

585
00:45:20,520 --> 00:45:26,560
experience and definitely looking forward to seeing how this little experiment works out.

586
00:45:26,560 --> 00:45:27,760
I think it's going to be really cool.

587
00:45:27,760 --> 00:45:32,680
Yeah, that was one of the questions that folks had was whether the folks that are in kind

588
00:45:32,680 --> 00:45:37,520
of off-time zones, whether the videos would be made available any sooner so that they

589
00:45:37,520 --> 00:45:43,320
could, you know, participate via forums and groups like ours, but watch the videos at

590
00:45:43,320 --> 00:45:46,120
a slightly more convenient time.

591
00:45:46,120 --> 00:45:51,400
I mean, it's, I mean, I can't technically stop anybody from doing that, I guess, because

592
00:45:51,400 --> 00:45:56,640
I mean, it's on YouTube live and the videos appear on YouTube live, but I would certainly

593
00:45:56,640 --> 00:46:02,560
much prefer people to actually be there during the class because that's where we're having

594
00:46:02,560 --> 00:46:04,440
the discussion.

595
00:46:04,440 --> 00:46:10,240
You can ask me questions, you know, while the lesson's going on, you know, I think that's

596
00:46:10,240 --> 00:46:14,000
what makes it a really rich and interesting experience.

597
00:46:14,000 --> 00:46:23,760
We started this course, a second session of the part one course through this really

598
00:46:23,760 --> 00:46:25,800
a great wrench in our plan.

599
00:46:25,800 --> 00:46:31,360
And so what we've decided to do is we'll kind of continue for the part one course, the

600
00:46:31,360 --> 00:46:39,680
first couple of lessons, but as soon as that, the new course starts, which I think you

601
00:46:39,680 --> 00:46:46,400
said was the 22nd of October, if that date holds, we will be kind of switching gears and

602
00:46:46,400 --> 00:46:50,920
working on the new course, the new library, et cetera, and everyone's really excited

603
00:46:50,920 --> 00:46:51,920
about that.

604
00:46:51,920 --> 00:46:56,600
And people do need to sign up to participate in fast AI live, so I'm sure you can provide

605
00:46:56,600 --> 00:46:58,880
the link there that they can do that with.

606
00:46:58,880 --> 00:47:04,400
All right, so we'll include that in the show notes and then the sign up for the meetup

607
00:47:04,400 --> 00:47:09,800
and for our study groups, that'll be at twimmalei.com slash meetup.

608
00:47:09,800 --> 00:47:18,240
So maybe kind of jumping back to the library, one of the things that you are kind of

609
00:47:18,240 --> 00:47:23,320
including in your announcement about the new library is some benchmarking that you did

610
00:47:23,320 --> 00:47:26,560
relative to the Kieres library.

611
00:47:26,560 --> 00:47:29,920
Can you talk a little bit about that and what you showed?

612
00:47:29,920 --> 00:47:36,440
Yeah, so I mean, I'd love for people to help do more benchmarking, I only had time to

613
00:47:36,440 --> 00:47:38,040
quickly throw something together.

614
00:47:38,040 --> 00:47:43,160
And I will say it's been a while since I really used Kieres, so I tried to find documentation

615
00:47:43,160 --> 00:47:49,000
on best practices, but there actually is, I couldn't find anything post 2016 in terms

616
00:47:49,000 --> 00:47:54,200
of official, here's how to do transfer learning in Kieres, so it's possible that my Kieres

617
00:47:54,200 --> 00:47:59,760
approach is not as optimal as it could be, but basically, yeah, I tried to use the classic

618
00:47:59,760 --> 00:48:10,040
Kaggle Dog's versus Cats data set to transfer and learn a ResNet 34 and just to attract how

619
00:48:10,040 --> 00:48:13,880
many lines of code it took, and I tried to optimize my Kieres lines of code as best as

620
00:48:13,880 --> 00:48:19,960
I could, how long it took to train and what accuracy I could get.

621
00:48:19,960 --> 00:48:25,600
And yeah, I was very pleased to find that the first AI library code was something like

622
00:48:25,600 --> 00:48:33,040
a fifth as many lines and I was quite a bit faster and quite a bit more accurate, which

623
00:48:33,040 --> 00:48:38,400
is, yeah, it's really all about the little, you know, all the stuff we curate both in terms

624
00:48:38,400 --> 00:48:44,440
of papers that we incorporate into the defaults and also some unpublished research, which

625
00:48:44,440 --> 00:48:51,000
we hope to publish later this year around ways to do this kind of training better.

626
00:48:51,000 --> 00:48:59,080
One of the things that I did notice about the new library is the code to do fine-tuning

627
00:48:59,080 --> 00:49:07,200
got very compact to the 0.7, can you maybe, you know, talk a little bit about that and

628
00:49:07,200 --> 00:49:12,320
more broadly, like the big, the differences that folks, you know, should expect to see

629
00:49:12,320 --> 00:49:14,560
when they start working with the new library.

630
00:49:14,560 --> 00:49:19,400
So, you know, Perl, since we've talked about Perl has Larry Wall who wrote it has this

631
00:49:19,400 --> 00:49:24,720
motto, which is, make the easy things easy and make the hard things possible.

632
00:49:24,720 --> 00:49:30,000
And so, to me, it's not just about making the easy things easy, but make the important

633
00:49:30,000 --> 00:49:34,520
things easy and there's nothing more important than transfer learning.

634
00:49:34,520 --> 00:49:41,200
So yeah, we tried very, very hard to make it, you know, basically transfer learning is

635
00:49:41,200 --> 00:49:44,960
kind of the thing you get automatically and for free.

636
00:49:44,960 --> 00:49:49,840
So if you create a model by default, you'll get some pre-trained weights and by default,

637
00:49:49,840 --> 00:49:57,280
you'll get something that is set up for discriminative fine-tuning and set up for, you know, layer

638
00:49:57,280 --> 00:50:01,080
freezing and, you know, all the stuff you need that you'll know from the course if you've

639
00:50:01,080 --> 00:50:02,080
done it.

640
00:50:02,080 --> 00:50:08,440
Yeah, but then, you know, the nice thing is that all of the power of PyTorch, we very

641
00:50:08,440 --> 00:50:12,440
intentionally make as close to the surface as possible.

642
00:50:12,440 --> 00:50:17,400
So for example, you know, we integrate very closely with PyTorch and we use a lot of

643
00:50:17,400 --> 00:50:19,680
PyTorch APIs directly.

644
00:50:19,680 --> 00:50:24,880
So for example, a fast AI data set is a PyTorch data set.

645
00:50:24,880 --> 00:50:30,880
The fast AI data loader is a wrapper for the PyTorch data loader and in our docs, which

646
00:50:30,880 --> 00:50:33,760
actually we should talk about our docs a bit because it's one of the things I'm most

647
00:50:33,760 --> 00:50:39,560
proud of, but in our docs, everywhere that we are using a PyTorch object, you'll see

648
00:50:39,560 --> 00:50:44,680
that there is actually a hyperlink directly to the PyTorch documentation for that class

649
00:50:44,680 --> 00:50:45,680
or function.

650
00:50:45,680 --> 00:50:50,040
So, you know, the two libraries are really nicely integrated.

651
00:50:50,040 --> 00:50:51,280
Oh, that's awesome.

652
00:50:51,280 --> 00:50:58,320
I'd love to hear you talk a little bit about the docs with the 0.7 library for the most

653
00:50:58,320 --> 00:51:00,200
part, the code was the docs.

654
00:51:00,200 --> 00:51:01,720
Yeah, there was no docs.

655
00:51:01,720 --> 00:51:09,000
And in fact, someone from our, from the tumble community, Kai contributed some docs and

656
00:51:09,000 --> 00:51:12,200
was told, well, you know, hold off on this because we're going to do better.

657
00:51:12,200 --> 00:51:14,800
So what have you done with the version 1 library?

658
00:51:14,800 --> 00:51:18,480
Well, you won't be shocked to hear this, but the entire set of documentation is written

659
00:51:18,480 --> 00:51:19,760
as Jupyter notebooks.

660
00:51:19,760 --> 00:51:21,520
Oh, nice.

661
00:51:21,520 --> 00:51:29,000
And so that means that every page in the documentation, you can actually run it in your own Jupyter

662
00:51:29,000 --> 00:51:32,000
notebook and see the experiments.

663
00:51:32,000 --> 00:51:37,700
So that all of the documentation is designed to be a very, you know, every, every module

664
00:51:37,700 --> 00:51:43,760
starts with an overview of like, hey, here's some code you can run right now to, you know,

665
00:51:43,760 --> 00:51:48,240
build this kind of model or to do this kind of data orientation.

666
00:51:48,240 --> 00:51:54,200
We wrote our entire, a kind of computer vision library from scratch in PyTorch, full

667
00:51:54,200 --> 00:51:58,680
new set of transformations, every transformation, therefore, is documented as, you know, a line

668
00:51:58,680 --> 00:52:04,080
of code in the, in the docs that actually, you know, prints out examples of that data

669
00:52:04,080 --> 00:52:06,520
orientation shows you the pictures.

670
00:52:06,520 --> 00:52:12,680
So then what we've written is we've written a whole new documentation framework, which

671
00:52:12,680 --> 00:52:18,520
takes those docs and turns them into a, into a website.

672
00:52:18,520 --> 00:52:22,520
And that website, for example, it does things like any, any, any place you've used back

673
00:52:22,520 --> 00:52:26,080
to text to kind of, you know, say, hey, this is, this is code.

674
00:52:26,080 --> 00:52:31,920
This is a symbol, it will automatically try and find anything in that code that represents

675
00:52:31,920 --> 00:52:39,720
a PyTorch or, first AI class or function or object automatically generate a hyperlink

676
00:52:39,720 --> 00:52:40,720
to that.

677
00:52:40,720 --> 00:52:46,360
So you can basically write, you know, standard mark down cells, you can standard code

678
00:52:46,360 --> 00:52:54,560
cells, inputs, outputs, pictures, everything will then automatically generate this fully

679
00:52:54,560 --> 00:53:02,920
hyperlinked table of contents, searchable documentation with, you know, embedded pictures

680
00:53:02,920 --> 00:53:04,680
and all that kind of thing.

681
00:53:04,680 --> 00:53:10,000
And then, yeah, you can go and try it out yourself by actually loading up that, that notebook

682
00:53:10,000 --> 00:53:11,760
locally and trying it out.

683
00:53:11,760 --> 00:53:15,440
So I think it's, as far as I know, it's the first time anything like this has been done

684
00:53:15,440 --> 00:53:20,240
and I think it's going to be super helpful for this kind of thing we say to people, which

685
00:53:20,240 --> 00:53:23,480
is how you should be experimenting, you should be running code, you should be trying things

686
00:53:23,480 --> 00:53:24,480
out.

687
00:53:24,480 --> 00:53:28,920
So the documentation, you know, is stuff that you can try out as experiments.

688
00:53:28,920 --> 00:53:32,320
That sounds, that sounds tremendous.

689
00:53:32,320 --> 00:53:39,520
The, I'm not aware of any, I've never seen code distributed as Jupyter notebooks.

690
00:53:39,520 --> 00:53:42,240
That does sound very cool.

691
00:53:42,240 --> 00:53:49,200
Someone mentioned that you tweeted something about some extensions to Jupyter that you

692
00:53:49,200 --> 00:53:52,800
were creating, is that what that's referring to?

693
00:53:52,800 --> 00:53:57,640
Yeah, they're not actually extensions, it's a, it's a, it's a framework of, it's a bit

694
00:53:57,640 --> 00:54:02,600
like a, if you've ever used things, it's kind of syncs on steroids, you know, about

695
00:54:02,600 --> 00:54:03,600
for notebooks.

696
00:54:03,600 --> 00:54:08,640
So basically something where you feed in Jupyter notebooks and it spits out a nice document

697
00:54:08,640 --> 00:54:10,640
documentation website for you.

698
00:54:10,640 --> 00:54:11,640
Hmm.

699
00:54:11,640 --> 00:54:16,800
And one of the things that comes up occasionally is, yeah, folks that want to develop in Jupyter

700
00:54:16,800 --> 00:54:25,200
notebooks and then productize, productionalize that code are, are you using techniques in, in

701
00:54:25,200 --> 00:54:32,720
this process that could be, also used to take a Jupyter notebook and, you know, maybe

702
00:54:32,720 --> 00:54:38,080
identify parts that are annotated as being, you know, that kind of the export thing that

703
00:54:38,080 --> 00:54:39,280
you're referring to.

704
00:54:39,280 --> 00:54:41,480
And then put those into a production module.

705
00:54:41,480 --> 00:54:43,200
Yeah, absolutely.

706
00:54:43,200 --> 00:54:47,600
So, you know, as we go through part two, which I guess will be kind of first and second

707
00:54:47,600 --> 00:54:51,760
quarters of 2019, that's kind of what we'll be doing is we'll be seeing how to, how to

708
00:54:51,760 --> 00:54:53,640
go through that process.

709
00:54:53,640 --> 00:54:59,120
You know, for me, I like automating things, the right amount, which is like not too much

710
00:54:59,120 --> 00:55:00,920
and not too little.

711
00:55:00,920 --> 00:55:07,920
So for me, this kind of exporting cells is a great way to kind of gradually build up much

712
00:55:07,920 --> 00:55:11,240
of the API that you want, but then the piece where it's like, okay, let's create a really

713
00:55:11,240 --> 00:55:17,080
nicely designed decoupled set of modules with clear dependency paths and all that kind

714
00:55:17,080 --> 00:55:18,080
of stuff.

715
00:55:18,080 --> 00:55:25,200
That's something I did, actually Sylvan and I did together, you know, manually and carefully.

716
00:55:25,200 --> 00:55:30,360
So I'm not really into kind of necessarily directly turning that Jupyter notebook into

717
00:55:30,360 --> 00:55:36,520
a module, but we certainly got some, yeah, use some simple tools to help us semi automate

718
00:55:36,520 --> 00:55:37,520
that process.

719
00:55:37,520 --> 00:55:38,720
Okay.

720
00:55:38,720 --> 00:55:45,600
What are some other highlights that folks that are familiar with the.7 library should

721
00:55:45,600 --> 00:55:49,800
expect to see or should be on a lookout for with version one?

722
00:55:49,800 --> 00:55:50,800
Yeah.

723
00:55:50,800 --> 00:55:56,800
So this new data augmentation slash computer vision library is something which you should

724
00:55:56,800 --> 00:56:05,600
definitely check out because it's, it's actually something which the, the PyTorch team helped

725
00:56:05,600 --> 00:56:11,800
make sure that PyTorch was explicitly performance accelerated for exactly the things we needed

726
00:56:11,800 --> 00:56:12,800
for it.

727
00:56:12,800 --> 00:56:21,840
So it's very fast and it actually ends up with much higher quality outputs than any existing

728
00:56:21,840 --> 00:56:27,480
data augmentation library because normally if you do like a rotate and then a zoom, for

729
00:56:27,480 --> 00:56:31,560
example, it basically interpolates on top of interpolation and you end up with these kind

730
00:56:31,560 --> 00:56:37,520
of fuzzier and fuzzier images where else we use an approach which actually keeps all of

731
00:56:37,520 --> 00:56:41,720
the sharpness of the image throughout the process.

732
00:56:41,720 --> 00:56:49,040
We also have some, this allows us to actually incorporate as default some kinds of transformation

733
00:56:49,040 --> 00:56:55,880
which pretty much nobody else is using, particularly perspective warping, which is a really important

734
00:56:55,880 --> 00:57:01,960
transformation in practice but it really requires this special kind of library to make it work

735
00:57:01,960 --> 00:57:02,960
effectively.

736
00:57:02,960 --> 00:57:08,640
So I'd say definitely check out the computer vision transformation library we've built.

737
00:57:08,640 --> 00:57:17,540
I'd also say have a look at the really good support for NLP transfer learning.

738
00:57:17,540 --> 00:57:25,720
Quite a few of the community on the fuzzier forums have been trained out the ULM fit techniques

739
00:57:25,720 --> 00:57:29,960
of basically transfer learning for NLP in non-English languages.

740
00:57:29,960 --> 00:57:37,600
In the last week or two, I've heard from two Polish students that they just won the

741
00:57:37,600 --> 00:57:42,000
Polish main Polish academic competition.

742
00:57:42,000 --> 00:57:46,680
Somebody else has just got the state of the art of a German and then a few months ago,

743
00:57:46,680 --> 00:57:48,840
somebody got the state of the art of a tie.

744
00:57:48,840 --> 00:57:55,280
Like in the German example, the chap on the forum basically said, hey, I tried it.

745
00:57:55,280 --> 00:57:56,480
I trained it for 20 minutes.

746
00:57:56,480 --> 00:57:59,880
The first thing that popped out was the immediate state of the art of a German.

747
00:57:59,880 --> 00:58:06,560
So for NLP, you can basically do stuff with this library that you will get better results

748
00:58:06,560 --> 00:58:10,520
than anybody's published before, kind of trivially.

749
00:58:10,520 --> 00:58:15,000
And we actually include, if you look in the examples directory of the repo, just click

750
00:58:15,000 --> 00:58:20,280
on the text example that'll actually show you how to do it end to end.

751
00:58:20,280 --> 00:58:27,360
And certainly, very much easier support for tabular data sets that's there is definitely

752
00:58:27,360 --> 00:58:28,960
worth checking out.

753
00:58:28,960 --> 00:58:33,560
It's not that it's doing anything that you couldn't do with 0.7, but it's now super, super

754
00:58:33,560 --> 00:58:34,560
easy.

755
00:58:34,560 --> 00:58:35,560
Yeah.

756
00:58:35,560 --> 00:58:39,720
And I think also, just check out the documentation framework because we do plan to extract

757
00:58:39,720 --> 00:58:46,640
that out into a separate independent project at some time soon-ish and I'd love to see

758
00:58:46,640 --> 00:58:53,600
more people trying to build Jupyter-based documentation systems because I think it's

759
00:58:53,600 --> 00:58:58,560
really helpful to your users to be able to say, hey, all of the documentation is code

760
00:58:58,560 --> 00:59:01,440
that you can run yourself right now and try it out.

761
00:59:01,440 --> 00:59:09,560
So the current library is very much focused on supervised learning and as we've discussed

762
00:59:09,560 --> 00:59:12,120
transfer learning.

763
00:59:12,120 --> 00:59:20,200
Do you see the library moving into models like reinforcement learning or unsupervised

764
00:59:20,200 --> 00:59:22,240
learning?

765
00:59:22,240 --> 00:59:27,440
Reinforcement learning, no, at least not in the short to medium term.

766
00:59:27,440 --> 00:59:32,880
I'm still unconvinced that we really know what we're doing as a community when it comes

767
00:59:32,880 --> 00:59:34,680
to reinforcement learning.

768
00:59:34,680 --> 00:59:40,520
The good results we're seeing are largely because of hacks that basically involve throwing

769
00:59:40,520 --> 00:59:44,120
ridiculous amounts of hardware at a problem.

770
00:59:44,120 --> 00:59:50,360
So I still want to spend some time doing some deep research into reinforcement learning

771
00:59:50,360 --> 00:59:55,280
to try and find ways to make it work better on smaller amounts of resources and on kind

772
00:59:55,280 --> 01:00:02,640
of more practical problems, you know, not many people need to win Dota or go.

773
01:00:02,640 --> 01:00:08,520
Unsupervised learning definitely kind of, I don't really believe that there's such a thing

774
01:00:08,520 --> 01:00:14,640
as unsupervised learning, but I do believe very much in what Jan LaCouden calls self-supervised

775
01:00:14,640 --> 01:00:21,480
learning, which is coming up with a supervised learning model which doesn't require explicit

776
01:00:21,480 --> 01:00:22,760
labels.

777
01:00:22,760 --> 01:00:29,720
And so for example, ULM fit is entirely based on something called a language model.

778
01:00:29,720 --> 01:00:34,560
A language model is just a model which predicts the next word in a piece of text.

779
01:00:34,560 --> 01:00:38,160
So you don't need any labels, you just need some text and you can build a model that

780
01:00:38,160 --> 01:00:42,240
tries to predict the next word after every sequence.

781
01:00:42,240 --> 01:00:47,760
So that's a self-supervised model because it uses the input data itself to generate labels.

782
01:00:47,760 --> 01:00:55,840
So I'm very keen to continue to provide richer and richer and more and more varied self-supervised

783
01:00:55,840 --> 01:01:01,480
learning support because that allows you to do transfer learning even in situations where

784
01:01:01,480 --> 01:01:04,720
you may not have explicit labels.

785
01:01:04,720 --> 01:01:10,200
The idea of self-supervised learning as you described it is related to another recurring

786
01:01:10,200 --> 01:01:16,680
theme that comes up from time to time on the podcast and that is incorporating model-based

787
01:01:16,680 --> 01:01:22,600
approaches into deep learning models, like physics-based approaches or other depending on

788
01:01:22,600 --> 01:01:23,600
the domain.

789
01:01:23,600 --> 01:01:28,160
Is that something that you are interested in, you're seeing and do you see it?

790
01:01:28,160 --> 01:01:31,040
How do you see it working with the library?

791
01:01:31,040 --> 01:01:33,480
Oh yeah, so very much so.

792
01:01:33,480 --> 01:01:35,320
It's something I really enthusiastic about.

793
01:01:35,320 --> 01:01:40,920
I mean, a great example of a model-based approach is the convolutional neural network.

794
01:01:40,920 --> 01:01:47,320
So if you start out with the observation that your data is auto-currelated along some

795
01:01:47,320 --> 01:01:52,320
dimensions, so for example, in an image, one pixel tends to be similar to the pixel above

796
01:01:52,320 --> 01:01:55,400
it and to the left of it and to the right of it and below it.

797
01:01:55,400 --> 01:02:01,800
So you've got two-dimensional autocorrelation and so a convolution is an operation explicitly

798
01:02:01,800 --> 01:02:08,800
architected to allow it to be easier to identify those kind of autocorrelated patterns.

799
01:02:08,800 --> 01:02:14,640
So like I see, it is a great example of a model-based or model inspired architecture.

800
01:02:14,640 --> 01:02:19,480
Then there's more recently, there's been a lot of, maybe not a lot, quite a few good

801
01:02:19,480 --> 01:02:26,480
examples of places where domain experts have come up with tweaks to an architecture which

802
01:02:26,480 --> 01:02:32,040
allow it to, you know, more easily represent the kinds of things that are most common

803
01:02:32,040 --> 01:02:33,280
in that domain.

804
01:02:33,280 --> 01:02:38,480
So for example, there was a couple of papers, one of which was by Saunderdeeleman talking

805
01:02:38,480 --> 01:02:44,680
about group convolutions, which is basically saying, hey, if you're doing something like

806
01:02:44,680 --> 01:02:52,040
pathology images or satellite images, which are rotation invariant, here is an architecture

807
01:02:52,040 --> 01:02:57,720
that is explicitly designed so that rotation invariance is built into the variant architecture.

808
01:02:57,720 --> 01:03:03,960
I've seen similar things in models for physics, where the kind of basic invariances or constraints

809
01:03:03,960 --> 01:03:09,000
of what would be required in those physics modules are, models are built into the architecture.

810
01:03:09,000 --> 01:03:14,600
So with fast AI, we've tried to make that kind of thing pretty easy to add because there's

811
01:03:14,600 --> 01:03:20,800
this idea of a custom head, you can very easily create a custom head for your architecture

812
01:03:20,800 --> 01:03:27,800
which might contain some of those ideas, or you can create a custom module, a custom

813
01:03:27,800 --> 01:03:34,680
NN module, you know, a PyTorch model, and pass that directly to a fast AI learner, and

814
01:03:34,680 --> 01:03:38,680
we've kind of provided lots of hooks for you to make it easy to add all the additional

815
01:03:38,680 --> 01:03:42,520
fast AI features into your model.

816
01:03:42,520 --> 01:03:47,960
So yeah, I think we've, you know, tried to support what we can there and maybe people who

817
01:03:47,960 --> 01:03:54,240
are interested in specific domains that already have some kind of model inspired architectures,

818
01:03:54,240 --> 01:03:58,440
hopefully we'll be able to start contributing some of their approaches.

819
01:03:58,440 --> 01:04:03,240
Maybe to start to wrap things up, I hear you're writing a book on machine learning, what's

820
01:04:03,240 --> 01:04:04,240
that going to cover?

821
01:04:04,240 --> 01:04:06,880
Okay, so we've got two books coming up.

822
01:04:06,880 --> 01:04:10,800
Why does a book with Professor Terence Pa about machine learning?

823
01:04:10,800 --> 01:04:17,040
The first few chapters are available in an early draft form online, and it's basically

824
01:04:17,040 --> 01:04:18,920
inspired by the course.

825
01:04:18,920 --> 01:04:24,120
So Terence Pa, if your listeners may recognize the name, he's a pretty famous computer scientist

826
01:04:24,120 --> 01:04:28,840
who built the antler parser generator, which I guess is kind of the most widely used parser

827
01:04:28,840 --> 01:04:35,120
generator around, and he spent decades becoming, say, the world's leading expert, or certainly

828
01:04:35,120 --> 01:04:40,600
one of them in parser generators, and he's now turning his attention to machine learning,

829
01:04:40,600 --> 01:04:44,200
and he's a colleague here at the University of San Francisco.

830
01:04:44,200 --> 01:04:49,440
He actually sat in on my machine learning course when I taught it, and said, oh, I like

831
01:04:49,440 --> 01:04:50,440
that so much.

832
01:04:50,440 --> 01:04:52,760
I think we should write a book based on it.

833
01:04:52,760 --> 01:04:59,520
So yeah, so we're doing that, and everything Terence does, he does exceptionally deeply and

834
01:04:59,520 --> 01:05:00,920
exceptionally well.

835
01:05:00,920 --> 01:05:04,880
So if you check out some of the early material, you will see there's a lot of beautiful

836
01:05:04,880 --> 01:05:08,960
visualizations and really nice descriptions.

837
01:05:08,960 --> 01:05:16,720
The second book is with Sylvia Guga, Sylvia, some of your listeners may recognize as being

838
01:05:16,720 --> 01:05:22,840
an exceptional student from an earlier course who repeatedly wrote brilliant material that

839
01:05:22,840 --> 01:05:28,400
I featured in the course, and since then he's gone on to do some really cool research

840
01:05:28,400 --> 01:05:35,080
around transfer learning, and AWS were kind enough to actually sponsor him as the first

841
01:05:35,080 --> 01:05:42,360
FastAI scholar in residence, so he's been working full-time with us for most of this year,

842
01:05:42,360 --> 01:05:50,480
and he's going to help me write a book about the FastAI Deep Learning Library, and learning

843
01:05:50,480 --> 01:05:55,520
Deep Learning using the Fast.AI top-down approach.

844
01:05:55,520 --> 01:05:59,760
So that one's going to be coming out published by O'Reilly sometime in 2019.

845
01:05:59,760 --> 01:06:00,760
Awesome.

846
01:06:00,760 --> 01:06:07,920
Related to your own evolution in the space and what you've seen from Sylvia, I'm sure

847
01:06:07,920 --> 01:06:13,760
you've been asked for folks that are interested in this field, don't want to get a PhD but

848
01:06:13,760 --> 01:06:16,560
want to contribute to research.

849
01:06:16,560 --> 01:06:22,000
Do you have kind of a path that you point folks down?

850
01:06:22,000 --> 01:06:23,000
Yeah.

851
01:06:23,000 --> 01:06:28,680
I mean, Sylvia would be a great example of that.

852
01:06:28,680 --> 01:06:33,800
His background is pure math, so minimal computer science.

853
01:06:33,800 --> 01:06:37,400
He's actually visiting us in San Francisco this week, so I've just been chatting to him

854
01:06:37,400 --> 01:06:42,000
about this, and he was telling me, yeah, he's been doing Deep Learning for less than a

855
01:06:42,000 --> 01:06:48,520
year, and he thinks his story of kind of going from a pure math, minimal computer science

856
01:06:48,520 --> 01:06:57,640
background to contributing to the FastAI library and doing well-regarded research is kind

857
01:06:57,640 --> 01:07:03,040
of a good example of how people, yeah, it doesn't have to take a long time.

858
01:07:03,040 --> 01:07:07,200
You don't have to have some kind of formal background.

859
01:07:07,200 --> 01:07:12,160
I think all it needs is tenacity.

860
01:07:12,160 --> 01:07:17,080
I think it requires strong coding shops, and so Sylvia has been working very hard to

861
01:07:17,080 --> 01:07:23,800
become a better coder, because then you can run lots of experiments every time you come

862
01:07:23,800 --> 01:07:26,800
up with an idea, you can try it out.

863
01:07:26,800 --> 01:07:28,560
And then just start writing.

864
01:07:28,560 --> 01:07:33,440
Start writing simple little blog posts, like just pick something that you've seen come

865
01:07:33,440 --> 01:07:42,040
out where somebody maybe, give me a simple example, ULMFIC came out showing good results

866
01:07:42,040 --> 01:07:44,320
on English classification.

867
01:07:44,320 --> 01:07:49,160
So if you're a big French, why not try it on French classification, and that's a simple

868
01:07:49,160 --> 01:07:53,040
piece of research, and you can write a blog post, or you can write a PDF, or put it on

869
01:07:53,040 --> 01:07:55,920
archive, showing your results on French.

870
01:07:55,920 --> 01:08:00,080
And then maybe you could say, oh, well, now that I've done French classification, let's

871
01:08:00,080 --> 01:08:04,720
also try sequence to sequence, or let's try sequence labeling, you know, just little

872
01:08:04,720 --> 01:08:10,520
extensions, and you'll be pretty surprised that you kind of keep publishing little extensions

873
01:08:10,520 --> 01:08:18,600
to something that you like pretty quickly, you'll find that you've somehow become more

874
01:08:18,600 --> 01:08:23,280
of an expert on that particular area than anybody else, and people start coming to you

875
01:08:23,280 --> 01:08:29,000
for advice, and you can start suddenly realizing, oh, you know, when Jeremy and Sebastian did

876
01:08:29,000 --> 01:08:32,240
that paper, they never tried this other technique.

877
01:08:32,240 --> 01:08:35,800
So maybe they'd be like, oh, they didn't try using a transformer model, maybe I should

878
01:08:35,800 --> 01:08:39,240
try a transformer model instead of an LSTM, let's try that.

879
01:08:39,240 --> 01:08:45,800
And so, you know, just keep digging, keep trying things, I'd say try to be as practical and

880
01:08:45,800 --> 01:08:50,600
useful as possible, don't get too lost in the math.

881
01:08:50,600 --> 01:08:56,200
And then, yeah, if you start coming up with some things that start showing good results,

882
01:08:56,200 --> 01:09:00,640
you can reach out to other people who have been doing work in that field and say, you

883
01:09:00,640 --> 01:09:04,880
know, here's my results, here's my code, what do you think?

884
01:09:04,880 --> 01:09:09,960
And if they think it's good, you know, maybe you can start doing some collaborative research

885
01:09:09,960 --> 01:09:16,320
with other people, which is exactly how Sebastian and I ended up doing some work together,

886
01:09:16,320 --> 01:09:21,960
and now it is Leslie Smith and I, you know, talking about collaborating, you know, yeah,

887
01:09:21,960 --> 01:09:27,440
it's all about doing useful work in a field and getting to know some of the other folks

888
01:09:27,440 --> 01:09:29,480
that are doing that kind of work too.

889
01:09:29,480 --> 01:09:35,480
Leslie, we know from the work on cyclical learning rates that was featured in the library,

890
01:09:35,480 --> 01:09:36,480
right?

891
01:09:36,480 --> 01:09:40,200
Yeah, and the learning rate finder, and more recently, the one cycle schedule that's going

892
01:09:40,200 --> 01:09:47,000
to be kind of the main featured training method in fast AI version one, and perhaps most importantly,

893
01:09:47,000 --> 01:09:50,960
the super convergence phenomenon that we're finding we can train things five to ten times

894
01:09:50,960 --> 01:09:55,040
faster than we were before he discovered that.

895
01:09:55,040 --> 01:10:01,040
Well, that sounds like another conversation, maybe one that I'll need to have with him.

896
01:10:01,040 --> 01:10:02,040
Yeah.

897
01:10:02,040 --> 01:10:04,120
But for now, Jeremy, thank you so much.

898
01:10:04,120 --> 01:10:08,320
Once again, for taking the time to chat with me, it was really great having you on the

899
01:10:08,320 --> 01:10:09,320
show.

900
01:10:09,320 --> 01:10:13,400
That's my pleasure, and I hope folks, you know, if people interested in the fast AI library,

901
01:10:13,400 --> 01:10:17,800
if you just go to docs.fast.ai, you'll find all the information to get started there

902
01:10:17,800 --> 01:10:22,480
and come to our forums and tell us how you go.

903
01:10:22,480 --> 01:10:23,480
Fantastic.

904
01:10:23,480 --> 01:10:24,480
Thanks so much.

905
01:10:24,480 --> 01:10:25,480
Thanks, Sam.

906
01:10:25,480 --> 01:10:26,480
Bye-bye.

907
01:10:26,480 --> 01:10:31,680
All right, everyone, that's our show for today.

908
01:10:31,680 --> 01:10:37,360
For more information on Jeremy or any of the topics covered in this show, visit twimmelai.com

909
01:10:37,360 --> 01:10:40,480
slash talk slash 186.

910
01:10:40,480 --> 01:10:46,320
You'll find there a link to the fast AI library and the new fast.ai courses as well.

911
01:10:46,320 --> 01:10:52,360
To join our community of machine learning enthusiasts, including our study groups for the fast.ai courses,

912
01:10:52,360 --> 01:10:56,040
visit twimmelai.com slash meetup.

913
01:10:56,040 --> 01:11:00,120
If you're a fan of the podcast and you haven't already done so or you're a new listener

914
01:11:00,120 --> 01:11:05,280
and you like what you hear, head to your Apple or Google podcast app and leave us a five-star

915
01:11:05,280 --> 01:11:06,800
rating and review.

916
01:11:06,800 --> 01:11:10,840
The reviews help inspire us to create more and better content and they help new listeners

917
01:11:10,840 --> 01:11:12,440
find the show.

918
01:11:12,440 --> 01:11:39,560
As always, thanks so much for listening and catch you next time.

