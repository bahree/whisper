WEBVTT

00:00.000 --> 00:16.000
All right, everyone. I am here with Kavita Bala. Kavita is chair of the Department of Computer Science at Cornell University, as well as a research advisor at Facebook.

00:16.000 --> 00:18.000
Kavita, welcome to the Tunnel AI podcast.

00:18.000 --> 00:20.000
Thank you so much for having me.

00:20.000 --> 00:34.000
I'm really looking forward to this talk. You recently gave a keynote at the woman in computer vision workshop about some of your work, and we will get deep into that in the course of our discussion.

00:34.000 --> 00:42.000
But let's please start out by having you share a little bit about your background and how you came to work in computer vision.

00:42.000 --> 00:54.000
Yeah, absolutely. So I started off, so I'll just describe my academic background and more recent stuff that I've been doing. I got my bachelor's in the Institute of Technology in Bombay.

00:54.000 --> 01:06.000
And then I came to MIT for my PhD, my master's in my PhD. And in fact, I started off in systems in computer science, and then I switched into computer graphics.

01:06.000 --> 01:16.000
And I got interested in computer graphics because there's very interested in sort of understanding images, and I like the visual aspect and the perception, the perceptual aspects of understanding images.

01:16.000 --> 01:24.000
So I started off in computer graphics, and then because of my interest in human perception, I actually started doing work more on the boundary of computer graphics and computer vision.

01:24.000 --> 01:37.000
And now my works, it's sort of formally in that in on the vision and the graphic side of boundary. So I do on the graphic side and very interested. I daily do see them as sort of a in and a young on the graphic side.

01:37.000 --> 01:45.000
We're very interested in building up models of the world, you know, whether it's a shape of the materials of the lighting in the world and producing images that look real.

01:45.000 --> 01:54.000
And the vision is the exact converse problem. You have these great real world images and you're trying to figure out what is the underlying model that created that image.

01:54.000 --> 02:01.000
So I've been very intrigued by both those sides of the question. And so that's where I sit with my research.

02:01.000 --> 02:18.000
And to those sides, are they kind of independent streams of research, or do they inform one another and influence one another? That's a great question. It's actually for me because of the human perception side of it that there's a very strong tie between the two.

02:18.000 --> 02:32.000
And there's lots of researches who sit squarely only on one side or the other side because they're looking at parts of the problem that don't overlap. But if you have this interest in perception, then I think that there's such a strong tie that you do see end up seeing both sides of them.

02:32.000 --> 02:37.000
And when you say perception, what is it about perception that ties them together strongly?

02:37.000 --> 02:50.000
So on the graphic side, you're producing images, but for whom there's a person who is going to be the consumer of that image who's trying to then understand what you try to sort of stuff into the image so that they understand the world.

02:50.000 --> 02:56.000
For example, you know, on the graphics research, one of the questions I ask is, what makes silk look like silk?

02:56.000 --> 03:08.000
And if you understand why silk look like silk or velvet looks like velvet, you can then produce realistic images of silk and velvet, which is very important for textile modeling and for, of course, for the entertainment industry, etc.

03:08.000 --> 03:20.000
And that perception question, you flip it around on the vision side, when you have an image of cloth, recognizing whether silk or velvet is something a human being can do automatically, but a vision algorithm right now still struggles.

03:20.000 --> 03:28.000
So in fact, your recognition on both sides was one of the ways I got into into research on both of these directions.

03:28.000 --> 03:37.000
Awesome, awesome. So you, your research led you to found a startup that,

03:37.000 --> 03:53.000
that's called Groc style that played in this area. Tell us a little bit about that experience and what you were doing there.

03:53.000 --> 04:04.000
Thanks. Yeah, that was, that's, that's been a very, very fun and eye opening experience. So I talked a bit about, you know, what I did, I, as I said, I did my bachelor's, my PhD, and then I came to Cornell, and I've been an academic at Cornell, my whole, sort of my whole life, so to speak from 2002.

04:04.000 --> 04:15.000
And then I had a while we were doing this material recognition research that I talked to you about where we're trying to understand, take a picture and try to recognize all the materials in it.

04:15.000 --> 04:29.000
My student was working with me at that time, Sean Bell, and I got very intrigued. We were looking at all of these images of great interiors of homes, because in our houses, we typically have a rich variety of materials, and we wanted to recognize them.

04:29.000 --> 04:40.000
And if you want to design, for example, a robot, a house cleaning robot, it would need to recognize what materials that's seeing and play with it. So that's sort of what motivated our research when we're doing material recognition.

04:40.000 --> 04:52.000
And while they were doing that, we actually were browsing all of these great sites, these interior design sites of places where people would post great images of homes that they were building or that they had seen.

04:52.000 --> 04:59.000
And they would ask questions like, I wonder what that, you know, that countertop looks like it's marble, is it really marble? What is it actually?

04:59.000 --> 05:20.000
And this led us to sort of realizing that there was a real hunger for the problem, which is called fine-grained visual recognition, which is not only recognizing that a table is a table or a chair is a chair, but going that extra strap beyond and saying, this chair is the Eames chair, or that table is the IKEA, you know, Mama table.

05:20.000 --> 05:29.000
And if you know that, and this is something that a random consumer or user who's looking at a picture, they know they like it, but they don't know what it is.

05:29.000 --> 05:35.000
And so they'd give them an expert level knowledge that is available at their fingertips, if you could solve that problem.

05:35.000 --> 05:42.000
So that got us, it's Sean and he got really excited about that, and we wrote a paper at Cigraf, this isn't 2015, so it's been a while now.

05:42.000 --> 05:56.000
But in our first paper, we wrote about this visual recognition problem, how you could use deep learning for it, with the right combination of data, the right learning architecture, and the right sort of all the magic pieces that go into making something that works end to end.

05:56.000 --> 06:03.000
We demonstrated that you can get really high quality visual, fine-grained visual recognition, and that launched off our company.

06:03.000 --> 06:28.000
Let's dig into those magic pieces a little bit. What are some of the key differences between the techniques that we would use to solve the course grain problem, which we do very well, and are kind of, you know, we can get standard off the shelf components that are fairly sophisticated, and this fine-grained problem, which I think is still a bit more challenging, correct?

06:28.000 --> 06:38.000
Yeah, it is quite a bit more challenging, and I can tell you so, you know, there are different pieces of it. One is we use the Siamese Network, so you actually are training and embedding.

06:38.000 --> 06:49.000
So instead of actually just a classifier, you're training an entire embedding where, you know, you can think of it as sort of, you need to know what your place in the world is.

06:49.000 --> 06:58.000
So it's like a global map, things that look similar, cluster in the common part of an embedding, things that are dissimilar, go to different parts of the embedding.

06:58.000 --> 07:12.000
So first is, you know, this idea of using Siamese Networks at that time was, I mean, lots of deep learning ideas had been explored, you know, decades ago, but people weren't using it quite in this context of using it for fine-grained recognition, and that was a first insight.

07:12.000 --> 07:25.000
Let's take that, but then the question was, how do you actually train it? You have some amount of data, so you get, you know, these designer photographs, let's say, let's say, here's a picture of this particular item.

07:25.000 --> 07:43.000
So it's this a pendulum or, you know, say a pendulum, let's say, or a pendulum product light, and you have the picture that you get from the catalog, which is this perfect image that comes from the manufacturer against a white background, it's all beautiful.

07:43.000 --> 08:02.000
There's some real world photographs, I'm going to get in their living room, and they showed how they're using it in their space. How correlate these two is the hard problem, and you put this through the Siamese Network, which says they're the same thing, even though they look completely different pixel for pixel, if you were to do comparisons between those two images, they look completely different, but they're the same thing.

08:02.000 --> 08:17.000
So you then train your embedding to pull similar things together, and then push dissimilar things apart, and after two weeks of training, you arrive at an embedding that actually represents sort of the visual, the visual reality of that domain.

08:17.000 --> 08:37.000
So that was, you know, so that was a first realization is that you have to do that, but then again, some of the magic is that you have to set up the right sort of normalizations, et cetera, so that we published this all in our paper, and we put it out there, but that helps in really creating robust learning process rather than something that's pretty fragile.

08:37.000 --> 08:53.000
So is the ability to go from the kind of in the wild picture taken on the cell phone camera to, you know, something that's taken in a more sterile environment.

08:53.000 --> 09:03.000
Is that a unique property of the Siamese Network, or is it part of the training process domain adaptation and data augmentation techniques and the like?

09:03.000 --> 09:17.000
Siamese Network is basically saying that these utterly dissimilar looking things pixel for people are the same thing, so that's where it set up with these laws functions that pull together the similar and push apart the dissimilar things.

09:17.000 --> 09:37.000
And in fact, so when we had first started this, we, you know, published this paper in 2015, we, I can tell you the whole story of the startup, but fast forward, Grocknet, which was, and should I talk about Grocknet, which was just released by Facebook, which is based on the Grock style word.

09:37.000 --> 09:52.000
Essentially is the same idea at its core, except many, many more day domains, a lot more data and a few more tweaks to the error functions, so that, but at its core, that's sort of the high level idea, once you get that, you can go actually very far with it.

09:52.000 --> 10:14.000
And is this idea of the Siamese Network architecture, is that, you know, are all networks where you're training embedding and embedding space end to end, are they all, you know, the same, is that, is there something, you know, something unique about the Siamese Network that is different from other approaches to training and embedding?

10:14.000 --> 10:26.000
Yeah, I mean, different people will use different types of losses, et cetera, but the core architecture there that was used as actually pretty robust and ends up being underlying a lot of lot of solutions of people.

10:26.000 --> 10:27.000
Okay.

10:27.000 --> 10:44.000
But there are different kind of losses, et cetera, that people said there's, you know, hundreds of papers in different different alternatives, but the, you know, the Grocknet paper actually was, so we created Rockstyle, the team went over to Facebook, so Facebook acquired, so I'll tell you a little bit about the story of Rockstyle.

10:44.000 --> 10:57.000
We launched it for fine-grained recognition, then be partnered with IKEA, actually, and IKEA was very interested in adding visual recognition in their augmented reality app, so in augmented reality.

10:57.000 --> 11:12.000
Well, in furniture, you really want to visualize a piece of furniture in your house before you buy it, so AR makes a lot of sense in that context, but once you have say you have an existing piece of furniture finding out what it is, seeing what compliments it, et cetera, all good use cases.

11:12.000 --> 11:20.000
So they added our visual recognition into it, we were on the radar of Facebook, and then last year Facebook acquired Grockstown.

11:20.000 --> 11:28.000
And then this was in 2019, and then in 2020, in May, they announced Grocknet, which is their core AI to make every image shockable.

11:28.000 --> 11:37.000
It's based on Grockstown, and actually Sean Bell, who's my PhD student, was my PhD student, and is now at Facebook, is heading up the project.

11:37.000 --> 11:45.000
They have a KDD paper on what Grocknet does, and as I said, it has much more data, you know, it has every form of data.

11:45.000 --> 11:54.000
We had started off with furniture, but now it has furniture, fashion, automotive, automobiles, and other forms.

11:54.000 --> 12:06.000
It has 83 different loss functions, but sort of three of them are sort of these core, you know, cyme-type losses that are there, and there are some variants on them.

12:06.000 --> 12:12.000
And once you add that in, you have something that actually is now powering commerce in a sort of a Facebook scale.

12:12.000 --> 12:27.000
A library on this idea of 83 loss functions, and how they come in, are they all trained simultaneously, or are they trained in stages or phases or layers, or something like that?

12:27.000 --> 12:48.000
So they are trained. There's a lot of magic to that, and the KDD paper. Actually, it's just primarily any of them approximately are to get those various data sets in, and then about three of them are actually that the losses that pull things together and push them apart.

12:48.000 --> 12:57.000
So those are the ones that you use, and there was sort of this double margin we had originally had a very simple function that pulls together, pushes apart to learn the embedding.

12:57.000 --> 13:05.000
Here, there's a slightly more sophisticated version that gives you a little more robustness to noise, et cetera, that gets added.

13:05.000 --> 13:23.000
Got it. And so is it, is it, you know, can you think of it as kind of like an ensemble type of thing where particular one of these loss functions will be, you know, coming into play when we're talking about sweaters versus tables versus something else.

13:23.000 --> 13:38.000
So it adds a lot of that domain, various domains and various data sets that are coming in and it helps you incorporate them all into one thing.

13:38.000 --> 13:49.000
Got it. Got it. And we'll be sharing in the show notes video. We'll be linking to the video of your CVPR keynote, and there's a very cool segment of that about 40 minutes in where you go through the demonstration of the system.

13:49.000 --> 14:04.000
It looks pretty cool in terms of, you know, take a picture of a our point, the camera at a piece of furniture, it tells you in fine detail, like even down to the pillows, you know, what those are.

14:04.000 --> 14:08.000
I guess I'm presuming that those are.

14:08.000 --> 14:22.000
Well, maybe let me ask this as a question. Are you in the case of that example where you're identifying the a specific branded pillow.

14:22.000 --> 14:38.000
Are you with the way that this be used is you've got a base model and then you fine tune on a catalog or something like that, or do you have to train deeply on the catalog?

14:38.000 --> 14:41.000
Is there a transfer learning type of elements of this?

14:41.000 --> 14:52.000
So the details of the KDD stuff are actually I was not involved in some parts of that work, but the catalog. Yeah, you keep adding as new parts come in.

14:52.000 --> 15:02.000
You incorporate them in with the existing embedding. So you're not going to go and just completely throw everything off, but there's a pretty stable and then you end them in and you incorporate them.

15:02.000 --> 15:20.000
And there was an interesting AR angle to that as well, where is there any ML AI component to that or is that, you know, traditional, you have this digital model of the piece of furniture and you're just placing it in space with a our kid or whatever you're using.

15:20.000 --> 15:38.000
So in the original one that was that it was pretty separate. So the AR piece was completely separate, but it was doing its own thing longer term, right, adding those digital models in for the training and robustness actually is an interesting thing too, which is something that we, you know, we looked at in various forms too.

15:38.000 --> 15:52.000
And that's obviously one of the parts that actually better than keep as these models improve in quality and they will help you with sort of the long tail of distribution of products that you might not actually otherwise be able to recognize.

15:52.000 --> 16:08.000
But are you saying there that are you alluding to training on a 3D digital model as opposed to a 2D image and using that to inform the model itself.

16:08.000 --> 16:28.000
Yeah, we found that actually having both of those helps with robustness. Now, that's not what the Facebook thing is doing. The solution is to, in fact, that's what we did in rocks where we actually have played with having both things together and it was it does result in much more robustness to various angle variations, etc.

16:28.000 --> 16:42.000
And, you know, a lot more variety that you can add in. So that was certainly something we had explored in rocks. And did you, did you flatten the 3D images to a 2D projection so that they can be.

16:42.000 --> 16:44.000
Yeah, we found to see an end or something.

16:44.000 --> 16:51.000
We fed them in a pretty traditional way, but it was more that they explored a richer variety by the using the 3D model. Yeah.

16:51.000 --> 16:59.000
Interesting. So yes, there's been interesting work also looking at sort of the 3D 3D CNNs itself, right, which is a whole different game.

16:59.000 --> 17:07.000
And actually, as we get, you know, one of the applications of this technology that is definitely within an AR mixed reality kind of a context.

17:07.000 --> 17:14.000
So going forward, I think all of these are going to get quite a bit more sophisticated in terms of incorporating virtual 3D data.

17:14.000 --> 17:20.000
Right now, the material models, etc. tend to be a little limited, but they could go, they could go far.

17:20.000 --> 17:26.000
And when you talked about the, the Grocknet being released recently.

17:26.000 --> 17:33.000
What is that release? Is that a data set? Is that a paper? Is that a service? Is that a model that you can download?

17:33.000 --> 17:35.000
Do you have any details there?

17:35.000 --> 17:45.000
Yeah. So they have, so Grocknet has been deployed, I believe, within the commerce side of Facebook so that it is used for captioning automatic captioning.

17:45.000 --> 17:52.000
If you have a person uploads an image, then it automatically provides captions, it does recognition, etc.

17:52.000 --> 17:58.000
So they see this as sort of the core engine that they're going to use to continue to train up to do.

17:58.000 --> 18:09.000
As they say, make every image shoppable, which means right now it's for some set of domains, but the hope is to make every aspect in every domain to be completely recognized.

18:09.000 --> 18:19.000
And I imagine that you are a biased believer in this vision, but lots of folks have been going after this for a long time.

18:19.000 --> 18:38.000
You know, not just, you know, Pinterest and the like, but also, you know, we've talked about this in the context of that, you know, that old technology TV, right where the TV providers would be able to, you know, on demand or pull out products and offer them.

18:38.000 --> 18:45.000
And it's never quite materialized. Do you think it would take on the space in general?

18:45.000 --> 18:57.000
I'm definitely a believer, right? I believe that. Yes. So I do believe. And in fact, I'll talk about sort of, you know, I've actually moved on to the next sort of how you assuming this stuff works, what you do with it.

18:57.000 --> 19:14.000
But let me, I am a true believer in this for sure. Video and recognition at scale, as you're walking around having every part of your image being fully annotated with a fully recognized understanding of not only at the highest scale, at the most detail scale.

19:14.000 --> 19:28.000
What am I seeing? Is this this particular detail, but also matter information, the history of the item, all of these I see as being something that will be available to you as you walk around with your AR sort of mixed reality glasses or something else.

19:28.000 --> 19:37.000
And there's a very interesting movie. I don't know if you've seen it call anon, which was available on Netflix. It's actually quite a dystopian world.

19:37.000 --> 19:56.000
But it's talked about something called the mind's eye, which is sort of this retinal implant. And as you walk around, there's a full overlay of everything you see has an overlay of information with it so that you can explore you basically are plugged into this full knowledge, comprehensive knowledge of the world you're in.

19:56.000 --> 20:09.000
I think that's actually a very exciting interface that I hope we will be able to realize and that means that every image, every video, everything, everything in person you see will be recognizable and tagged with information.

20:09.000 --> 20:18.000
Now there's all kinds of security implications of such a thing of course and privacy implication and of course society has to look into those they're very important to get right.

20:18.000 --> 20:27.000
And I see that as also going hand in hand with the kind of development of this kind of technology, but what's interesting is how much it could potentially enrich your life.

20:27.000 --> 20:41.000
If you have all of that information available on your fingertips as you're walking around in the world right if you walk across a tree or a flower immediately recognizes it for you just enriches your life to know all of the things that you're potentially interacting with.

20:41.000 --> 20:59.000
What's your personal take or read on the societal implications and how do you balance balance those or kind of answer for yourself the question of on the balance should this be something that exists I was just reading an article about.

20:59.000 --> 21:17.000
I guess this slate publish an article about customs and border patrol has access to some database of scan license plates and apparently they're crowdsourcing them they give people some company gives people these license plate scanners and it just you know random people driving around with these scanners and.

21:17.000 --> 21:38.000
You know we as you know we're and as you've alluded to we're moving to this this world where everything's recorded on some camera and uploaded to to some cloud how do you parse whether you know that is what creates the dystopia that was alluded to in your movie or you know whether there are other factors.

21:38.000 --> 21:48.000
I think it's a fundamental question and it is critical for us as a society to grapple with it so I think it is one of the most important questions we need to deal with and actually different.

21:48.000 --> 22:06.000
Society's and different cultures are arriving at different places as to whether they believe they should respect that privacy of an individual or not right the U has a very different set of laws your the US has a different set of laws and different parts you know different countries China for example a very different set of laws.

22:06.000 --> 22:24.000
We so from a technological point of view I think we should develop the technologies and also keep privacy preserving aspects of it as fundamental hobby develop these technologies so we should think about privacy preserving X privacy preserving machine learning you know when you can learn large scale.

22:24.000 --> 22:37.000
You know useful because the recognition that you get through you know the out of the learn are actually incredibly useful right they are made society more productive there's lots of positive benefits but there is clearly the negative side to it to.

22:37.000 --> 22:53.000
So you just you while you design your technologies you keep an eye on this aspect that you might want to preserve privacy in it and separately I think you actually have to have that societal and policy engagement where there are clear policies about what and clear teeth with the policies that come with it.

22:53.000 --> 23:09.000
As to what the society wants to sustain what aspects of privacy wants to maintain and how we should go make sure that you know that's executed on in all of the technology companies that releases so I think this is a critical question and we should definitely do it.

23:09.000 --> 23:29.000
I'm not of the position that though because that there is a danger of that there is it can be used badly that you should not invest in this technology at all and pull back that's not reasonable what the right thing to do is to design the mechanisms to protect the values that you hold here and privacy is one of them.

23:29.000 --> 23:46.000
And you know and make sure that they can be executed. And this innovation not only in the albums also sometimes hardware like your trusted computer base and what you know have you preserved privacy there so all the way is what you need to maintain and I think that's how you should approach this investment.

23:46.000 --> 24:09.000
Are there is the current how far have we gotten in terms of integrating the work that's happening around privacy preserving machine learning and embeddings in general and or the specific work or those are they kind of orthogonal concerns or does your does the you know,

24:09.000 --> 24:18.000
is net, for example, I have to be aware of you know differential privacy or some kind of privacy preserving technique in order for it all to work.

24:18.000 --> 24:29.000
Yeah, so I think you know, so the kinds of work that we've done in the case of recognition where you're applying it for you know products and stuff there's not so much of a problem, you know, it's actually okay.

24:29.000 --> 24:47.000
Yes, when you get to face mission, for example, that's the kind of dangerous technology where it's you have to be very careful in how you and there's also health data and things like that right when you when you have that embedded in you have to be very careful in designing it from the from the get go to actually have that kind of

24:47.000 --> 25:01.000
kind of graph, you know, privacy preservation in it, so I think that's how far has it gotten I think it's a very, very open ongoing area of research that people are continuing to explore and push on and hopefully get to useful basis, but I think it's wide open.

25:01.000 --> 25:15.000
Okay, so your work on rock style kind of set the stage for you to start to explore some kind of broader cultural implications of this kind of approach tell us a little bit about about those.

25:15.000 --> 25:36.000
Yeah, so I got very interested, you know, when you're looking at this problem of how do you make an image, you know, fully understandable so fully shoppable was the first one I said, but let's assume we have that technology so fast forward 10 years we do all the innovations and we have a base so that every image that we have available to us is fully

25:36.000 --> 25:51.000
understandable what can I do with that knowledge and think of it as you know, let's say there's sort of a hundred years from now somebody's going through our historical images trying to understand who we are so there's an anthropologist was looking back in time and trying to understand who we are.

25:51.000 --> 26:05.000
It turns out that we are recording ourselves at an unprecedented rate right we're storing whether it's on Instagram or on Facebook or you know any of your technologies we are recording ourselves and posting information about ourselves online.

26:05.000 --> 26:22.000
Which it turns out is this giant camera of the entire planet and so when you have a planet size camera, what can you understand about the planet so I'm working with cultural anthropologists and this a joint work with Noah Snavley Kevin Matson,

26:22.000 --> 26:46.000
with Krishmal and Bharatari Haran these are all my Cornell collaborators who are looking at this questions of take all of the images from social media and can be understand people can we understand what do people where what do they eat who do they who do they hang out with or you know what kind of social groups do they hang out with you know do they hang out in pairs and big groups what do they do what kinds

26:46.000 --> 26:54.000
of activities they do they do they do and how do it does do people do these things differently in different parts of the world.

26:54.000 --> 27:05.000
So this is where the cultural anthropologists are very interested so normally how a cultural anthropologist would work is they would go and they would visit a site so they would visit a different country.

27:05.000 --> 27:15.000
They would go and embed themselves there for a few months they do interviews you know with a handful of people and they try to say yeah you know this in this place they are very you know these

27:15.000 --> 27:26.000
clothes they wear this is kind of what they eat these are kind of them the social norms and instead we have this world size camera that can give you all that information the visual information that you can find.

27:26.000 --> 27:37.000
So that's what motivated this sort of work that I've been doing one we have two projects one is called street style and the other is called geostyle and we started looking at the first problem what do people wear around the world.

27:37.000 --> 27:46.000
So how do they dress similarly or differently around the world and this has ended up being an incredibly rich sort of exploration of human of the human race.

27:46.000 --> 27:51.000
And by the way you know when we're talking about you have to be very careful talking about the privacy issue.

27:51.000 --> 28:01.000
As I said before you can actually do this in a privacy preserving we can block out the faces it's not about the faces you're actually just trying to understand what people wear in different parts of the world.

28:01.000 --> 28:16.000
But you do want to be careful when you're asking these questions what is everybody doing it every it's not big brother it's more understanding actually population distribution so it's understanding how we work as a group not about violating any individual's privacy.

28:16.000 --> 28:33.000
So okay so what do we do so first thing we did is just look at how people dress across the world and so we recognize you know we apply a standard sort of detect the body recognize what a person is wearing you know what's the top what's the bottom are they wearing glasses or not etc so there are some very simple set of attributes.

28:33.000 --> 28:52.000
But then what we did is you know just like that embedding it really we learned this embedding where the clustered parts of the picture parts of the dresses that look similar into the same you know same region of the embedding and in the different part different clothing got clustered you know separated out in different parts of the embedding if they look different.

28:52.000 --> 29:16.000
Once you have this embedding now you get this really interesting thing so you can go and do unsupervised learning when you can find these style clusters and each style cluster stands for some very unique thing like people wearing a plaid shirt and say you know and glasses that's one particular style let's say another style is a dark jacket jacket and say a white shirt.

29:16.000 --> 29:27.000
And when you look at these style clusters because we've collected this information from the whole world we had actually started with sort of three years of Instagram data from 2013 to 2016.

29:27.000 --> 29:43.000
And because we've collected around the whole world you can then look at a particular style cluster and then go and look at well which cities did those images come from and which times of the of the year did they did they wear this clothes and you find really fascinating stuff.

29:43.000 --> 30:01.000
So there are some very unique clusters for example we had one cluster that had and I'll share the slides with you but this very unique headgear and it turns out when you go and you look at which cities it comes from there's one city that lights up its logos and this is the GLA headgear that women were there and it just pops right out.

30:01.000 --> 30:28.000
So you know we found very distinctive things that people wear in different parts of the world by looking at these style clusters. We also found one very distinctive cluster that was not actually spatially unique so it was when you look at the cluster it's pretty obvious as people wearing sort of these heavy dark jackets with sort of shirts underneath the layered shirt underneath and it turns out you when you look at you know which cities they come from and which time of the year kind of X axis as a city and the Y axis as the time.

30:28.000 --> 30:57.000
You can actually see all cities have this but it's winter that it pops up so and you also see the northern hemisphere pops up in December January and conversely in the southern hemisphere cities it pops up exactly six months later so essentially just by looking at these clusters you then go and look at where the data is coming from and you understand how those people live at that time or if you were to go and visit that place you should go and look at this cluster and say oh I'm going that in January I think this is the kind of clothing I need to take so that I can blend with everybody who's there.

30:57.000 --> 31:13.000
Those are sort of the distinctive clusters on the flip side there are these very standard clusters it turns out there are some things that the whole planet wears like there's no one city that's very unique no time of the year and that is plaats plaats shirts are popular in the whole planet.

31:13.000 --> 31:23.000
Also blue shirts button down blue shirts like people wear that just it's their standard go to kind of slightly formal wear that they wear when they're going out and taking selfies of themselves.

31:23.000 --> 31:48.000
So that was very interesting so we did that was our original street style work and so that actually led us to sort of this geostyle work where we said this is interesting can you automate sort of discovering very unique clothing across the world and so we created a whole mechanism where you can find spikes in these clusters when people really dress in something and it was fascinating so we now found out things like sporting events.

31:48.000 --> 32:16.000
Protests so if everybody dresses a particular way in a protest then there's a big spike in those clothes so for example in Spain there was these protests in 2013 or 2014 for Catalan and so they all dressed in these yellow shirts on that one day and when we were doing the recognition there was a big spike in the in the yellow shirt wearing and we could go and say whether something interesting going on there it's a political rally and you can just find that so we learned about.

32:16.000 --> 32:22.000
A social event that we didn't know about just by mining them sort of visually.

32:22.000 --> 32:39.000
And so in order to do this were you able to use the embeddings that you created for a rock style rock net kind of out of the box or do you have to customize the loss function or the training data or some aspect of the system.

32:39.000 --> 32:48.000
Training data yes not the last function this one is actually we use it pretty the this is a rock style so this work also started in 2016 so it was more than that era.

32:48.000 --> 33:08.000
Rock net is very recent it just use the standard recognition box to learn this embedding with a very simple amount of training data we had I think 27,000 labels to recognize different attributes of clothing now this was very focused on clothing if we had to recognize food or recognize activity we have to apply a slightly different.

33:08.000 --> 33:28.000
Training but we did a simple thing of recognizing some attributes and then what was interesting was this unsupervised sort of recognition of the style clusters that's where it became interesting after you learn the embedding then discovering the style clusters is where you they sort of get this rich rich discovery process which is fun.

33:28.000 --> 33:39.000
You talk about this as being you know part of an era where every image is understandable and this kind of the word understanding has a lot of.

33:39.000 --> 33:54.000
Connotations I think in this case we're talking about humans using this as a tool to increase their understanding of the cultures does this play into computers being able to better understand the images as well.

33:54.000 --> 34:20.000
Yeah so that would be the so computers so I was taking as granted that the computers can do the recognition piece right if they do a job and they recognize it that's great but the deeper question of actually understanding cultural trends and understanding so social economic the social events or events of great political import understanding that just by looking at the data is something that you would hope.

34:20.000 --> 34:35.000
An algorithm could do and we are a long way off from that but this takes the first steps as can be actually recognize these events that are very unique and then actually and in fact there's a question so you recognize a visual event everybody's wearing a yellow shirt.

34:35.000 --> 34:45.000
But it's just a yellow shirt by yeah one of the things we found also one of our style clusters was there was a big spike somewhere in around.

34:45.000 --> 34:59.000
In the middle of this time period and we're looking at it it was of a yellow shirt with v neck and graphics on it and it turns out we had discovered work of soccer so everybody was wearing the work of soccer you know the insignia and all of that and we just recognize that in the style cluster.

34:59.000 --> 35:11.000
So you can find these clusters but then if you correlate them with captions and other stuff that's going on you start to see culture you say you can recognize something as a cultural event versus a sports sporting event.

35:11.000 --> 35:18.000
So that's where it starts becoming interesting where you can have the algorithm hopefully recognize new cultural events.

35:18.000 --> 35:26.000
Great and so where do you see this going what's next and you know the next step or two in this line of research for you.

35:26.000 --> 35:37.000
I'm super excited about this kind of you know temporal recognition and I think this is just it's a huge opportunity because of the amount of visual data we have.

35:37.000 --> 35:42.000
So one of the big next steps we're looking at is taking this and applying it in the context of satellite imagery.

35:42.000 --> 35:56.000
So again you know can we look at large scale trends around the planet bear something interesting spikes up that's local something that cyclical right that that changes season by season versus something that's a one off and that's sort of what we discover in geostyle.

35:56.000 --> 36:09.000
If we discover the one off events are they problematic or are they a good outcome so those talking about examples like forest fires or landslides or crop growth or things like that.

36:09.000 --> 36:35.000
Yeah so those are they look to you and there was a you know all gel bloom all of those things are things that you can actually study there's incredible satellite image data can you then you know there are things that are done cyclically as I said just seasonal and then there are things that are the the man made and that's exactly what we're trying to analyze and we're working in this case with crop scientists and they are actually they're very interested in this makes a huge difference.

36:35.000 --> 36:50.000
If you can make simple predictions of how just seeing sort of some early signals you can predict how they change over time for example which are event you know mechanism can do because it basically tries to fit sort of the temporal signal of the of the pattern it's saying.

36:50.000 --> 37:01.000
Then you can make a huge difference in you know predicting whether or not you'll have a productive crop whether you should take some steps to mitigate problems that you're seeing early on so those are the kinds of problems you're looking at.

37:01.000 --> 37:06.000
Awesome so satellite images any other future directions for this.

37:06.000 --> 37:30.000
So that's one of the big ones and I'm very excited about but we continue even with the context of you know as I said you know Instagram is one example but all of the social in the rich variety of social media looking at what people where is one piece but there's so much more you know our activities and as I said how do we congregate how do you know do we are we just loners who are taking selfies against monuments or are there actually group.

37:30.000 --> 37:44.000
So there's a lot more to do on understanding sort of social interaction in this context to which is so it's two different you know satellite images versus human data sort of but there are two very interesting and distinct parts to take which I'm excited.

37:44.000 --> 38:00.000
Awesome awesome well Kavita thanks so much for taking the time to share with us what you've been up to thank you so much.

38:00.000 --> 38:15.000
Thank you so much.

