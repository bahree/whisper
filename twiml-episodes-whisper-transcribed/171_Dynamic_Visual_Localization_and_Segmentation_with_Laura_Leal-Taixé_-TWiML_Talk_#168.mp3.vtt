WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.760
I'm your host, Sam Charrington. Let me start today's show with a huge thanks to everyone

00:34.760 --> 00:40.840
who's taken the time out to vote for us in the People's Choice Podcast Awards. Voting ends

00:40.840 --> 00:48.280
tomorrow, July 31st, and we are extremely appreciative of everyone who's voted, commented,

00:48.280 --> 00:54.760
shared, or otherwise. We'll be sure to share the results of the voting as soon as they become

00:54.760 --> 01:01.160
available. If you haven't already cast your vote, please, please, please visit twimbleai.com

01:01.160 --> 01:11.240
slash nominate to do so. In this episode, I'm joined by Laura Lail Tai Sheh, 2017 recipient

01:11.240 --> 01:17.080
of the prestigious Sophia Kovalev Skaya Award and professor at the Technical University of Munich,

01:17.080 --> 01:22.520
where she leads the dynamic vision and learning group. In our conversation, Laura and I discuss

01:22.520 --> 01:27.720
several of her recent projects, including work on image-based localization techniques that

01:27.720 --> 01:32.680
fuse traditional model-based computer vision approaches with a data-driven approach based

01:32.680 --> 01:38.680
on deep learning. We also discuss her paper on one-shot video object segmentation and the broader

01:38.680 --> 01:44.040
vision for her research, which aims to create tools for allowing individuals to better navigate

01:44.040 --> 01:49.720
cities using systems constructed from visual data. And now on to the show.

01:53.080 --> 01:58.920
All right, everyone. I am on the line with Laura Lail Tai Sheh. Laura is a professor at the

01:58.920 --> 02:03.640
Technical University of Munich, where she leads the dynamic vision and learning group.

02:03.640 --> 02:06.120
Laura, welcome to this week in Machine Learning and AI.

02:06.920 --> 02:11.960
Hello, Tim. Nice to be here. Why don't we get started by having you tell us a little bit about your

02:11.960 --> 02:19.800
background and how you made your way into machine learning research? Yes, so I studied actually

02:20.520 --> 02:25.880
not a role machine learning or computer vision. So I studied telecommunications engineering

02:25.880 --> 02:32.360
back in Barcelona. And then I went to do my master thesis at Northeastern University in Boston.

02:32.360 --> 02:37.960
And that's the first time that I took a course in computer vision. And so that's where,

02:37.960 --> 02:44.600
let's say, my passion for this field started. And then I decided to pursue a PhD. So I went back

02:44.600 --> 02:52.120
to Northern Germany to do a PhD in Hanover. And during the course of my PhD, machine learning

02:52.120 --> 02:57.800
and especially deep learning started to become really, really popular within the field of computer

02:57.800 --> 03:04.440
vision. And towards the end of my PhDs, when I started to focus a little bit more or try to

03:04.440 --> 03:09.880
get into machine learning and understanding how could this be useful for the tasks that I was

03:09.880 --> 03:17.880
dealing with here in my PhD. And after that, I went to ETH in Zurich for a couple of years,

03:17.880 --> 03:27.400
for a post-op. And after that, I moved to Munich, also for another post-op, until well,

03:27.400 --> 03:33.000
until recently, where I was giving the wonderful chance of becoming a tenure-track professor

03:33.000 --> 03:38.200
also here in Munich. Great. What was the focus of your PhD research?

03:38.840 --> 03:45.320
So I was working on multiple object tracking. I started with a project where we had to track

03:45.320 --> 03:55.080
algis. And this was for to study essentially how algis get stuck or get attached into the

03:55.080 --> 04:01.880
surfaces of ships. And so basically, they ruined the surfaces. And so the whole idea, this was

04:01.880 --> 04:08.200
together with the department of chemistry and physics. So they were really studying the

04:08.200 --> 04:15.880
possible materials that could repel these algis. And within this project, my goal was essentially

04:15.880 --> 04:21.560
to analyze the way these algis moved. And of course, they had tons and tons of video data.

04:21.560 --> 04:28.760
And they could not just manually follow each of these algis manually. So they wanted to have

04:28.760 --> 04:35.480
an automatic tool for analyzing this large amount of data. And so this is how I started

04:35.480 --> 04:41.800
with the topic of multiple object tracking. And then I moved a bit more towards

04:42.440 --> 04:49.240
people tracking because the motion of people were more interesting. There was the whole

04:50.200 --> 04:56.440
goal of analyzing the pedestrian motion, for example, for autonomous driving applications.

04:56.440 --> 05:05.160
And so this is where basically I shifted more towards crowded scenes, analyzing people interaction

05:05.160 --> 05:09.640
and using these interactions for better prediction of people motion.

05:10.600 --> 05:15.640
And more recently, you've been working on visual localization. Can you tell us about that

05:15.640 --> 05:24.040
problem and your approach to it? Yes. So visual localization is an entirely different problem.

05:24.040 --> 05:33.560
So here, what we're interested in is estimating the position of a camera in 3D within a given scene.

05:34.360 --> 05:40.040
And you can imagine that this given scene can be either a building for indoor localization

05:40.040 --> 05:48.280
or an entire city for outdoor localization. And this scene, this model is essentially

05:48.280 --> 05:54.440
reconstructed from images using standard computer vision techniques, such as structural

05:54.440 --> 06:00.680
promotion. And from this, you can just crawl a bunch of images from flicker and reconstruct,

06:00.680 --> 06:06.760
for example, entire cities in 3D. And so now the idea is that you're walking around that city

06:06.760 --> 06:11.960
and you take a picture of some building and you want to know exactly where your camera was located

06:11.960 --> 06:19.400
when you took that picture. And this has applications in, for example, augmented reality or robotic

06:19.400 --> 06:26.440
navigation where you really need a very precise localization of your camera. And so this is not a

06:26.440 --> 06:33.160
new field at all. So it has been tackled in computer vision for a really long time. But recently

06:33.160 --> 06:39.000
people started tackling this problem, of course, with deep learning. And so we were looking into

06:39.000 --> 06:45.320
this with a master thesis here at TUM. And we actually realized that people were kind of

06:46.200 --> 06:51.720
forgetting all the classic research in these fields. So they were just blindly taking neural

06:51.720 --> 06:56.520
networks and applying it to this problem where the input was an image and the output was directly

06:56.520 --> 07:03.880
the camera poles. So what we're, what we're working on now is to kind of fusing the classic knowledge

07:03.880 --> 07:11.880
of geometry, multiple view geometry, feature extraction and all the knowledge that was gathered

07:11.880 --> 07:18.200
in something like 10 plus years of research in visual localization and trying to fuse this with

07:18.200 --> 07:26.520
the data driven or deep learning methods. And within this project in particular, what we're doing

07:26.520 --> 07:32.600
is not actually trying to extract an absolute pose of the camera, which is what methods like

07:32.600 --> 07:39.240
PostNet are doing, but actually estimating a relative pose between a camera that you know

07:39.240 --> 07:46.600
where dislocated because you used it to create your model and this new image that you're capturing.

07:46.600 --> 07:52.440
And so this method is actually much more flexible because you can go anywhere, take a picture,

07:52.440 --> 07:59.240
and as long as you have some other pictures around you that you know were used to create your model,

07:59.240 --> 08:03.720
you can actually localize yourself, which is not true for PostNet where you actually need to

08:03.720 --> 08:09.000
retrain a different neural network for every specific scene where you want to localize yourself.

08:09.640 --> 08:14.920
And so we have, we have been working towards this goal of relative pose estimation with neural

08:14.920 --> 08:20.200
networks and including some geometry, multiple view geometry information. And this was,

08:21.640 --> 08:28.600
this is the work of one of my PhD students who also she recently started and it's also

08:28.600 --> 08:35.800
together in collaboration with Thorsten Sadler from ETH Zurich. And the ultimate goal is,

08:35.800 --> 08:41.480
of course, to handle more and more complex scenes that could not be handled before with classic

08:41.480 --> 08:47.560
methods. And by more complex scenes, what I mean is, for example, dynamic scenes. And this is

08:47.560 --> 08:54.920
where my expertise in multiple object tracking comes in. So if you are actually observing a city,

08:54.920 --> 09:01.640
it's very easy to localize your camera if your city is empty. And so you only have the buildings

09:01.640 --> 09:07.480
and the static parts in there. But as soon as you have crowds of people and all these dynamic objects,

09:07.480 --> 09:13.560
they just, they are not good for your localization. So they're basically noise for your localization.

09:13.560 --> 09:18.760
So we want eventually to build a pipeline that is robust enough to handle these dynamic scenes.

09:18.760 --> 09:25.320
One of the things that you mentioned is a bit of a recurring theme here on the podcast. And that

09:25.320 --> 09:35.000
is this idea of fusing deep learning based methods, CNNs, I imagine, in this case, and more

09:35.000 --> 09:43.240
traditional methods to kind of inform those CNNs. And there's, there seems to be a tension in the

09:43.240 --> 09:50.200
in the research about whether an end-to-end deep learning approach is, you know, better given

09:50.200 --> 09:57.640
sufficient data and sufficient compute or whether we should try to incorporate the, you know,

09:57.640 --> 10:01.400
the various things we've learned about these various problems we're trying to solve or the

10:01.400 --> 10:08.440
physics of given situations. Maybe talk a little bit about the model that you're building to

10:08.440 --> 10:16.040
fuse those two types of approaches and how you've gone after that specific part of it.

10:17.160 --> 10:22.360
Yes, so that's actually a very, a very good question because it's really a central point that

10:22.360 --> 10:27.640
is, that is coming up more and more in discussions within the community. So I think there was a

10:27.640 --> 10:34.200
really big shift from these model-based methods to data-driven methods, fully data-driven.

10:34.200 --> 10:40.600
And in theory, if one has enough computational resources enough data, this is the perfect setup

10:40.600 --> 10:47.000
because you have a universal approximator that can just express any function that you want to find.

10:47.000 --> 10:52.840
So this in theory is perfect. The problem is that data is very costly to obtain and so no people

10:52.840 --> 10:58.600
are talking about constraining your deep architecture with some previous knowledge as you said,

10:58.600 --> 11:04.520
like, like, physics or in our case, multiple view geometry. Now, the interesting question is,

11:04.520 --> 11:10.680
where do you put this information? So do you put it in the loss function? Do you constrain your

11:10.680 --> 11:17.080
activation maps to have certain shapes? Or you can also, for example, actually make your

11:17.080 --> 11:22.920
architecture more modular so that each of the different parts is interpretable and has,

11:22.920 --> 11:28.920
for example, physical meaning. And this is still a question that there is not entirely solved,

11:28.920 --> 11:33.560
right? So this is something that people are actively researching and we don't have the ultimate

11:33.560 --> 11:38.760
answer. So how we did it for the Visual Localization project is essentially,

11:40.120 --> 11:46.760
we propose to mimic the pipeline of the feature matching that is also present in classical

11:46.760 --> 11:52.840
Visual Localization methods where you have two images. You want to find the relative pose between

11:52.840 --> 11:58.440
them. And essentially what you do is you compute the series of features, then you try to match

11:58.440 --> 12:05.080
these features and find a pose that explains these matches in a coherent way so that the geometry

12:05.080 --> 12:11.240
is okay for most of the matches. And this pipeline is actually very robust. And one of the problems

12:11.240 --> 12:17.880
that it has is that this feature extraction step was completely handcrafted before. So people

12:17.880 --> 12:23.320
were using sift features or surf features. And this worked really well for many scenes, but they

12:23.320 --> 12:31.560
didn't work, for example, indoor. What are sift and surf features? Oh, these are so sifter scaling

12:31.560 --> 12:40.600
variant features. They're essentially feature descriptors like a bit like corner detectors. And they

12:40.600 --> 12:49.240
just, they tell you where an interesting or let's say a defining feature of your scene is.

12:49.800 --> 12:55.880
And what is it's descriptor? So you attach a vector to this particular feature. So when you look

12:55.880 --> 13:02.360
at the same scene, maybe from a slightly different angle, you get for the same point a very similar

13:02.360 --> 13:07.480
descriptor. And then you can say, oh, these two points are actually the same 3D point. They are

13:07.480 --> 13:15.080
just seen by two different from two different positions. But the point is that this descriptor

13:15.080 --> 13:21.880
should be very similar for the same pointy 3D. So that the sift features are playing a role

13:21.880 --> 13:28.440
similar to like an edge detector and classical object detection. Yeah, for example, yeah.

13:29.240 --> 13:35.240
You mentioned a few different ways to incorporate this geometry into a pipeline.

13:36.200 --> 13:41.640
How does the way that you've chosen to do it map up to some of the general ways that you

13:41.640 --> 13:46.760
mentioned incorporating into the loss function, incorporating it into the shape of activations?

13:46.760 --> 13:56.360
So it kind of approaches two sides. So one is definitely the loss function. The loss function is

13:56.920 --> 14:02.440
acting directly on the essential matrix prediction, which is the matrix that gives you the relative

14:02.440 --> 14:09.800
pose between the cameras. And the other thing is the the actual architecture. So we're making the

14:09.800 --> 14:17.320
architecture modular in in the sense that since we're mimicking or trying to mimic this this

14:17.320 --> 14:25.480
classic feature matching pipeline, we try also to place the architecture to design the architecture

14:25.480 --> 14:32.120
so that each of the parts mimics this this visualization pipeline. So we know that in the first

14:32.120 --> 14:37.160
CNN, you're going to extract some features that are going to be representative of your scene.

14:37.160 --> 14:41.720
Then you're going to have a matching step and then you're going to have an essential matrix

14:41.720 --> 14:48.680
or relative pose regression. And and this kind of dividing your neural network into different

14:48.680 --> 14:54.360
interpretable parts is is also one way to first of all understand what's happening with your

14:54.360 --> 14:59.800
neural network and second of all includes some information, some hard coded information in the middle

14:59.800 --> 15:06.200
if you need to. You mentioned the essential matrix. What is what is that and what's the role that it

15:06.200 --> 15:12.840
plays in this pipeline? So the essential matrix gives you essentially the relative pose between

15:12.840 --> 15:19.560
two cameras. So it tells you how can you go from a point in one camera to the exact same point

15:19.560 --> 15:24.440
but a scene from the other camera. So it basically transforms your coordinates from one camera

15:24.440 --> 15:29.400
to the other. And this is this is all that we want to find. So once you have relative poses,

15:29.400 --> 15:35.640
then you can localize your camera. You can create all these maps, these 3D maps from images that

15:35.640 --> 15:41.640
I was talking about. And this is this is essentially what we want as a result from our algorithm.

15:41.640 --> 15:52.680
Okay. And so how how do you characterize the results of this approach relative to the alternatives,

15:52.680 --> 15:57.720
you know, both traditional as well as entirely data driven approaches?

15:58.520 --> 16:05.000
Well, compared to other data driven approaches, one big difference is that most of the

16:05.000 --> 16:11.320
approaches are still tackling the absolute pose estimation problem, which means that you have

16:11.320 --> 16:17.800
a very well defined scene with an origin, which is the world coordinate of that origin,

16:17.800 --> 16:24.280
of that scene, sorry. And then you you localize yourself within that world coordinate,

16:24.280 --> 16:29.080
which means that suddenly now if you have another scene, this other scene will have a completely

16:29.080 --> 16:34.200
different world coordinate. And so what you're going to do is you're going to have to train one

16:34.200 --> 16:39.400
neural network per each of the scenes where you want to localize yourself. And this is not

16:39.400 --> 16:44.040
something that we want to do. Of course, we want to build a system that is able with a single

16:44.040 --> 16:50.840
network to localize yourself anywhere. And this is one of the key differences of our proposed method,

16:50.840 --> 16:56.600
which actually tackles relative pose. So you can localize yourself everywhere with a single network.

16:56.600 --> 17:05.000
This is one huge difference. And compared to classic visualization, we also showed in an

17:05.000 --> 17:13.000
in an ICCV paper last year, 2017, we showed for the first time a comparison of classic methods versus

17:13.640 --> 17:19.640
deep learning methods. And we showed essentially that that classic methods are much much more accurate

17:19.640 --> 17:25.960
by by even an order of magnitude sometimes in some scenes, which means that well deep learning is

17:25.960 --> 17:32.920
still not there yet. But an advantage that deep learning has is that it can handle, for example,

17:32.920 --> 17:41.880
large indoor scenes, which have a few texture. So for example, large textureless surfaces like

17:41.880 --> 17:47.880
white walls or repetitive structures. For example, in a building, you have all the stairs that look

17:47.880 --> 17:53.880
the same, all the doors that look the same. And sometimes even as a person, if you go, for example,

17:53.880 --> 17:58.520
to a hospital, it's hard to to know where you are in the hospital. It's very easy to get lost.

17:59.080 --> 18:05.080
And this is the same for for classic methods where they focus on on these basic features,

18:05.080 --> 18:11.000
and then they compare to staircases and they don't know which is which. And deep learning methods

18:11.000 --> 18:17.560
are very, very good at capturing other subtle features that help them to localize better in this

18:17.560 --> 18:23.960
repetitive indoor environments. So this is one real strength and where we see really the the

18:23.960 --> 18:30.840
application for for deep learning methods. And this is why also we proposed a new data set called

18:30.840 --> 18:38.280
TUM LSI large scale indoor localization data set. And this we also published last year in our

18:38.280 --> 18:44.280
ICC paper. And this is a really, really challenging task to localize yourself in those indoor

18:44.280 --> 18:52.680
environments. What's your intuition for why deep learning is so much better at that particular

18:52.680 --> 19:00.920
type of environment relative to classical approaches and yet worse overall in the traditional

19:00.920 --> 19:08.200
types of environments? Well, the key is that classic methods are based on these features that I

19:08.200 --> 19:17.080
was talking about before. So sift or serve. And these features can only be present when you have

19:18.760 --> 19:26.360
let's say characteristics parts in your image. So if you have a white wall with no corners,

19:26.360 --> 19:33.080
no edges, then there's not going to be any feature on that white wall. Well, if you go for example

19:33.080 --> 19:38.920
to marine plots and you have this this beautiful building, which is the city hall, it's full of

19:38.920 --> 19:45.240
tiny details, tiny corners. And so you're going to have really many, many features firing on

19:45.240 --> 19:50.600
that building. So when you have a localization pipeline that is based on those features,

19:51.560 --> 19:55.960
it's really easy to localize yourself in marine plots, for example, because you have many, many

19:55.960 --> 20:01.480
features to base your localization on. But if you just see a white wall and you have no features,

20:01.480 --> 20:08.520
then you have nothing to compute your camera position, because there's no features, so there's

20:08.520 --> 20:15.080
no matching happening. And this matching between 2D features and the features in your 3D model is

20:15.080 --> 20:24.120
the key to the visual localization pipeline. And so deep learning looks more at the overall picture,

20:24.120 --> 20:28.840
so looks at the white wall, but also maybe at the column on the right, maybe there's some chair

20:28.840 --> 20:34.520
also in this scene, and try to look at the whole picture and give you a descriptor for the whole

20:34.520 --> 20:40.760
scene. So even if you don't have many corners, or if you have like a white wall, you can also use

20:40.760 --> 20:45.720
the fact that there's a white wall without features in there to localize yourself. And this is

20:45.720 --> 20:52.120
something that classic methods don't do, or at least classic methods based on sift and serve

20:52.120 --> 20:59.800
type of features. Now you've got in this architecture, you've got kind of this, I guess what I'm

20:59.800 --> 21:07.160
envisioning as a feature identification or extraction stage or module or something along those lines.

21:07.160 --> 21:16.760
And then once you've done that, are you reusing existing neural network architectures to perform

21:16.760 --> 21:24.120
some of the rest of the pipeline, or is it all kind of new architectures?

21:24.920 --> 21:32.280
No, so actually the feature extraction part is based on well-known architectures. So we tried

21:32.280 --> 21:39.240
a Google net, we tried ResNet, currently ResNet is what works best. And so this essentially

21:39.240 --> 21:44.440
is pre-trained on ImageNet, so it gives us a really, really good initialization for our ways.

21:44.440 --> 21:54.360
And how we use it is essentially we're training to go from the image to a descriptor, a vector

21:54.360 --> 22:02.200
of a certain number of elements. And for this, we do reuse these classic architectures. And then

22:02.200 --> 22:08.120
the rest is trained from scratch. And how do you determine what the dimensionality of this

22:08.120 --> 22:15.560
feature vector is? Well, this is a bit of a trial and error, right? So right now we're using,

22:15.560 --> 22:25.320
we tried using something like 2000, I think it was 2048. We also try using 4000 or even smaller.

22:25.320 --> 22:29.880
And in the end, you kind of do trial and error and see what works best for your problem.

22:29.880 --> 22:39.800
And then taking a step back to the broader problem you've got, these kind of many images of these

22:39.800 --> 22:47.480
worlds or environments from different angles. And you've talked about the relative performance

22:47.480 --> 22:54.440
of these different types of approaches for kind of interior scenes versus exterior scenes.

22:54.440 --> 23:02.760
Are you training models that are good at one of these environments or are you somehow

23:02.760 --> 23:11.400
integrating these and starting to look at models that can handle different scales or different

23:12.360 --> 23:18.440
transition from one type of environment to the next? That's actually an excellent question.

23:18.440 --> 23:25.080
Ideally, so theoretically our network can handle any type of environment, indoor and outdoor.

23:25.640 --> 23:32.600
But in practice, the best thing is to train an network for indoor and an network for outdoor.

23:32.600 --> 23:37.080
So this is, in my opinion, still OK, because you only have two types of networks.

23:37.720 --> 23:44.840
And currently, the bottleneck is actually the step before the one where you actually train

23:44.840 --> 23:52.680
your network, which is to find pairs between your image, so the image that you are taking

23:52.680 --> 23:58.440
a test time and the training images that you have around you. So of course, you can imagine that

23:58.440 --> 24:05.080
you are in this city and you cannot evaluate all the possible training images that you have

24:05.080 --> 24:10.680
from the city and that you use to reconstruct your model. And so you have to prune all these

24:10.680 --> 24:15.880
images. And so a test time you arrive there, you take a picture and you want to find, let's say,

24:15.880 --> 24:22.760
the 30 pictures that are most similar to your picture. And so for this, we use yet another CNN

24:22.760 --> 24:31.160
architecture. And it's basically solved the problem that is called image retrieval. So it retweets

24:31.160 --> 24:36.920
images that are similar to your own image. So for example, you want to localize yourself in

24:36.920 --> 24:42.120
Munich. It doesn't make any sense to compute relative poses within your images and the images

24:42.120 --> 24:49.080
from Paris or London. So this network is able to tell you, look, these are 30 images that are

24:49.080 --> 24:55.400
most likely located in the same place where you are roughly speaking. And then you can compute

24:55.400 --> 25:01.640
the relative pose between these 30 images and your own query or test image. And the problem is

25:01.640 --> 25:07.720
that the CNN for image retrieval that works for outdoors doesn't work so well for indoors.

25:07.720 --> 25:13.080
So now we're trying to figure out whether we need to retrain this network completely different

25:13.080 --> 25:18.760
and just use two networks, one for indoor one for outdoor. Or if we can actually reuse some,

25:19.640 --> 25:25.720
some of this network that actually works so well outdoors. So this is kind of what we're researching

25:25.720 --> 25:32.600
right now. There's also a part of this that is related to scale and maybe it's kind of the same

25:32.600 --> 25:38.520
answer and you've addressed it. But when I think of, you know, these outdoor images, you mentioned

25:38.520 --> 25:47.080
at a square, you know, the camera tends to be a lot further from the buildings. Whereas if I'm

25:47.080 --> 25:54.040
navigating a hospital, you know, corridor, the camera tends to be very, very close, does just

25:54.040 --> 26:02.040
this, the idea of having two models address the scale issues or do you run into scale issues?

26:02.040 --> 26:09.000
You know, for example, depending on the within, you know, either of these two types of environments.

26:10.040 --> 26:17.160
So actually, we have to fix the scale of our essential matrices. So essentially, what we do is,

26:17.160 --> 26:23.160
is we take the translation and we give it, we force it to have norm of one. And with this,

26:23.160 --> 26:27.800
what we're essentially saying is we're fixing the scale of our essential matrix. Then we train

26:27.800 --> 26:34.840
the network to predict only essential matrices with this particular scale. And then at a later

26:34.840 --> 26:41.080
stage, we will triangulate from all the training images and the relative poses. We will triangulate

26:41.080 --> 26:48.600
the real scale and the real pose of the image at test time. And so with this, actually, we can

26:48.600 --> 26:56.360
handle varying scales. So it's not to say that the network is very robust to all the range of

26:56.360 --> 27:02.040
scales. So this is also something that we need to dig into. But theoretically, you can actually

27:02.040 --> 27:09.320
handle any type of scales. And how about generalization? How do you explore the generalizability of

27:09.320 --> 27:18.680
this approach? That's an excellent question. Because what we have recently observed is that the, so the

27:18.680 --> 27:25.000
more one of the advantages of our method is that since we're predicting relative poses and therefore

27:25.000 --> 27:32.680
we can use any image from any scene. Now we're not bounded to one scene. We can use much, much more

27:32.680 --> 27:37.560
training data. And we have observed that the more training that they use, of course, the better

27:37.560 --> 27:44.600
your localization is. So first it gets worse, but then it gets better. But still, you do need to see

27:44.600 --> 27:52.040
some examples from your test scene to be able to localize yourself properly there. So this is

27:52.040 --> 27:58.840
actually something that we're trying to figure out. Why is that? Because technically, the generalization

27:58.840 --> 28:05.160
part of the network is not perfect yet. Because ideally, you would want to train on a set of scenes

28:05.160 --> 28:09.800
and then have a completely new scene that the network has never seen before and be able to

28:09.800 --> 28:16.360
localize yourself there also. Well, this is not entirely true yet. So it does do a fairly good

28:16.360 --> 28:23.080
job, but it does a much, much better job when you see at least some images from that test scene.

28:23.080 --> 28:27.320
And so the generalization is still something that we're looking into.

28:27.320 --> 28:32.680
Really interesting, really interesting. So you're also doing some work on object segmentation.

28:32.680 --> 28:36.120
Is that related to this project or is it a totally separate effort?

28:36.840 --> 28:44.040
Well, it's all part of the interest of my group of the dynamic vision and learning group,

28:44.040 --> 28:51.800
which is to actually be able to analyze the dynamic scenes around you. And this, of course,

28:51.800 --> 28:57.320
englobs a bunch of problems. So ranging from the multiple object tracking problem where I start

28:57.320 --> 29:03.800
my PhD to the visual localization, which, of course, we're working also on video localization.

29:05.240 --> 29:12.520
And to be able to fully analyze what's happening around you and also analyze it in time.

29:12.520 --> 29:17.400
So not only image-based, but video-based, you also need to perform, for example,

29:17.400 --> 29:23.880
video object segmentation. And this is one project that we are also doing in collaboration with

29:23.880 --> 29:31.000
people from ETH Zurich. And in this particular project, what we wanted to do was we wanted to

29:31.000 --> 29:37.480
tackle the problem of supervised video object segmentation. And by supervised, what I mean is that

29:38.200 --> 29:46.520
you're given in the first frame of your video, a perfect mask that expresses which object you

29:46.520 --> 29:52.520
want to follow, which object you want to segment in your video. And so your goal now is to segment

29:52.520 --> 29:58.520
this object to find out which pixels in the following frames belong to this particular object.

29:59.080 --> 30:04.920
And so you can imagine that this is kind of easy if the object doesn't move much, if the object

30:04.920 --> 30:10.280
doesn't change the appearance much. But as soon as the object, for example, turns around and now

30:10.280 --> 30:15.640
you're seeing a completely different side of the object or the illumination changes or the position

30:15.640 --> 30:22.760
of the camera changes, these are all big challenges for this problem. And so what we proposed in a

30:22.760 --> 30:31.880
work that we presented at CEPR 2017 already is to actually do this fully data driven.

30:31.880 --> 30:39.320
So there is this wonderful data set created by Disney Research and ETH people that is called

30:39.320 --> 30:45.960
the Davis data set. And this contains really accurate segmentation masks. And by accurate, I mean

30:45.960 --> 30:51.720
sometimes crazy accurate where you see, I don't know, the hair of horses segmented out one by one.

30:51.720 --> 30:59.080
Oh wow. Yeah, it's really crazy. And so with this now, you have like this wonderful source

30:59.080 --> 31:08.840
to train your networks. And so what we propose to do is to actually, so the key idea in that paper

31:08.840 --> 31:14.040
was actually the training scheme. So it's not a classical training where you just input your data

31:14.040 --> 31:19.720
and you get your output. But first you take this network which is a classic convolutional neural

31:19.720 --> 31:25.800
network. You kind of pre-training for the task of video object segmentation. So you give it a

31:25.800 --> 31:32.680
bunch of of masks of object and you train your network. But what is happening now is that your

31:32.680 --> 31:39.960
network is trained to do some sort of foreground background separation. But it doesn't know which object

31:39.960 --> 31:45.160
you actually want to follow, right? You just have trained it with a bunch of objects. You can segment

31:45.160 --> 31:53.960
a bus, a car, a chair. So there's all these types of objects in your data set. And so now you still

31:53.960 --> 32:00.280
have to tell your network which object you actually want to segment. And so how we do it is in a

32:00.280 --> 32:06.440
second training stage, we use this first mask of the object that we do have available. And we

32:06.440 --> 32:12.360
kind of overfeed the network to that mask. So we say, really learn the appearance of that particular

32:12.360 --> 32:17.720
object. So overfeed your network to that particular object. And then the only thing that you need

32:17.720 --> 32:24.120
to do is you run your network for the rest of the frames. And it will be able to segment your

32:24.120 --> 32:30.840
object for the rest of the video. And this was actually a really, really novel way of doing the

32:30.840 --> 32:37.000
object segmentation. And there has been a bunch of works that have followed later this paradigm.

32:37.000 --> 32:45.560
And this was presented at CBR 17 and it's called Osvos for one shot video object segmentation.

32:46.200 --> 32:51.560
And the idea is that, so why we call it one shot is because you only see the object once in

32:51.560 --> 32:56.920
this first frame. And then you're able to segment it and tracking over the whole video.

32:57.640 --> 33:05.400
I'm trying to wrap my head around how you would go about the overfitting part of the training.

33:05.400 --> 33:13.240
And maybe the question that I'm coming to is, is this process something that is

33:14.040 --> 33:19.320
automatable or something that you can build into an automated process? Or is it a very manual

33:19.320 --> 33:27.560
process that needs to be supervised by a researcher or practitioner in order to work?

33:28.280 --> 33:32.440
No, essentially, this is completely automatic. So the only thing that you need is the

33:32.440 --> 33:39.960
mask of your object for one of the frames. And then you pass this mask as a training example

33:39.960 --> 33:46.520
to your network. You do all sorts of data segmentation, for example. And you train your network for

33:46.520 --> 33:52.600
really quite some iterations. And during this process, essentially what your network learns is

33:52.600 --> 33:56.920
what is the appearance of this object and what is the appearance of the background. And this is

33:56.920 --> 34:01.160
completely automatic. So as long as you have the mask of your object, you're good to go.

34:02.200 --> 34:09.320
Of course, the bad thing or the small drawback is that if your background or your object change

34:09.320 --> 34:16.040
the appearance too much. For example, a new object appears in the scene or a second object that

34:16.040 --> 34:21.240
looks exactly like your first object appears on the scene that is going to be a problem.

34:21.240 --> 34:29.000
So we had this sequence, the famous two camel sequence, where you're segmenting this camel that

34:29.000 --> 34:34.600
is moving along. And then a second camel appears on the scene. And of course, the second camel looks

34:34.600 --> 34:39.720
exactly the same as the first camel. And so our network suddenly says, well, this is also my

34:39.720 --> 34:45.080
object. I also want to follow this. And so it segments the two camels. But now the problem is that

34:45.080 --> 34:50.920
you only want to segment one camel. So what you can do is is provide another training image to your

34:50.920 --> 34:56.920
network where you have the two camels. And one is segmented as your true camel that you want to

34:56.920 --> 35:02.280
follow. And the other is segmented as background. So you don't really care about the other camel.

35:02.280 --> 35:06.680
So now you're giving your network the further information that there's a second camel in the

35:06.680 --> 35:11.800
scene, but you don't really want to segment it. And with this, you can actually correct your

35:11.800 --> 35:17.480
network and find you need a little bit more towards only the first camel. And so now you will be able

35:17.480 --> 35:25.560
to segment only one camel in this scene. Does this apply to specific types of objects? How

35:25.560 --> 35:32.840
broadly does it apply? For example, what is the types of objects that are in this Davis data set?

35:32.840 --> 35:39.960
And does that, to what extent does that constrain the model or is the model focused on those types

35:39.960 --> 35:46.200
of images? So actually, you can tackle any type of object. This is completely independent from

35:46.200 --> 35:52.360
the object types. Davis has quite a wide range of objects, I would say. But the interesting thing

35:52.360 --> 36:00.360
is that we're not basing ourselves on object proposals or on object classifiers. But it's really

36:00.360 --> 36:05.640
this foreground background separation. So if you have a picture with whatever strange object

36:05.640 --> 36:10.440
and this object is segmented out, the network is going to be able to learn the appearance of this

36:10.440 --> 36:14.760
object. And it doesn't matter if it's an elephant or if it's a chair, you're going to be able to

36:14.760 --> 36:21.320
follow it. Now, of course, if you are constrained by certain object types, you can do a better work.

36:21.320 --> 36:29.000
And indeed, we also worked on that in the upcoming journal extension for this paper, which we

36:29.000 --> 36:35.880
also published last year. And this was the idea that you can use all these object proposals,

36:35.880 --> 36:42.120
for example, coming from Moscow CNN, to kind of help you and guide you through the segmentation.

36:42.760 --> 36:47.560
So even the first mask, you find that you have a lot of overlap between your segmentation.

36:47.560 --> 36:53.720
And for example, the proposal of a motorbike, then you're kind of sure that the object that

36:53.720 --> 36:58.600
you're trying to follow is a motorbike. And then you can constrain your segmentation also

36:58.600 --> 37:04.520
with the series of proposals that the maskar CNN gives you. But this is something that is,

37:04.520 --> 37:10.040
it's kind of an extra to improve segmentation, but you don't need to do it. So if you don't know

37:10.040 --> 37:14.680
what object you want to follow, you can still use the method as it is, and it will still be able to

37:14.680 --> 37:20.920
follow it. There was another project that came across that you were working on called social maps.

37:20.920 --> 37:29.400
What's that one about? Yeah, so social maps is actually a bit of my vision of what computer vision

37:29.400 --> 37:37.000
and AI can do for society. So in this project, I actually proposed this whole research project

37:37.000 --> 37:45.640
for an award. So it was this research grant proposal. And I was lucky enough that it was accepted

37:45.640 --> 37:54.360
for the Sofia Kovalovskaya Award by the Humboldt Foundation last year. So very few really talented

37:54.360 --> 38:01.960
researchers get this award. And I was lucky to be in this pool of researchers. And congratulations.

38:01.960 --> 38:07.720
Thank you. Thank you very much. Well, the main advantage of course is that it comes with quite

38:07.720 --> 38:16.280
some money, 1.65 million euros. And with that, of course, I can pay for PhD students for equipment.

38:16.280 --> 38:23.320
So this is really the big advantage of such an award that I was able to start my research group

38:23.320 --> 38:28.280
right away. And so I have right now three wonderful PhD students that are working really hard.

38:28.280 --> 38:34.440
And so all these projects that I mentioned are actually with all of them. And essentially the

38:34.440 --> 38:40.360
project social maps is very much related to the idea that I've said before that we want to

38:40.360 --> 38:47.720
understand these dynamic scenes around us. But it has a very specific application. So if you think

38:47.720 --> 38:54.280
about Google maps, for example, you see that they are really an excellent source of static information.

38:54.280 --> 39:02.040
So they have very precise maps of where roads are, where shops and restaurants are. So wherever

39:02.040 --> 39:07.800
you want to go, Google maps is going to give you a really optimized route that you can follow and

39:07.800 --> 39:15.320
you will get to your place, wherever you are. And so the idea though is that this is still a

39:15.320 --> 39:21.720
relatively limited information because it's only static information. And what I would like to

39:21.720 --> 39:27.560
input into maps is the social information. And so what I mean by social information is essentially

39:27.560 --> 39:34.280
how pedestrians, how people use public spaces. So I want to automatically analyze the motion of

39:34.280 --> 39:42.280
pedestrians, what is happening in the streets in real time. And I want to input this information

39:42.280 --> 39:48.360
into maps. So for example, let's assume that I have to go from my home, which is in the south of

39:48.360 --> 39:53.400
Munich. And I have to go to my workplace, which is actually in Garching, so quite far away.

39:53.400 --> 39:59.720
And I'm taking my car and I'm putting into Google maps, my destination. And then Google maps

39:59.720 --> 40:06.360
just tells me a way to follow. But let's imagine that now I have some cameras in the middle of

40:06.360 --> 40:11.960
the city of Munich. And I can detect automatically that there's some demonstration starting. So there's

40:11.960 --> 40:16.840
a bunch of people gathering, they are cutting the streets. So I'm not going to be able to pass

40:16.840 --> 40:23.960
through those streets. Now if you take Google maps as it is now, you will only be notified about

40:23.960 --> 40:30.280
this once there's already a traffic jam. So traffic reports are included into Google maps,

40:30.280 --> 40:35.640
but you don't want to be the first one that is stuck into traffic. So I want to be able to detect

40:35.640 --> 40:40.840
how people use public spaces and whether there's a demonstration or whether kids are coming out

40:40.840 --> 40:47.080
of school. I want to be able to detect it fast by using this dynamic scene understanding and

40:47.080 --> 40:53.480
using computer vision and AI. And I want to input this information into maps so that now I don't get

40:54.280 --> 40:59.640
the normal optimized route, but somehow a social route. And by social route, what I mean is

40:59.640 --> 41:06.040
I want to decouple pedestrian traffic from vehicle traffic. So if cars are going always in certain

41:06.040 --> 41:12.120
roads, and now these roads are being used by pedestrians, I want to tell cars to use

41:12.120 --> 41:17.880
other routes or other roads. And this is kind of the, for example, one of the short-term

41:17.880 --> 41:24.120
applications, but of course in the long term, having access to all these data and all these

41:24.120 --> 41:29.720
analyzed data. So trajectories of pedestrians, how pedestrians use public spaces, where do they

41:29.720 --> 41:36.920
tend to cross? How do which exits, for example, from a railway station do they use most?

41:37.640 --> 41:41.800
If you provide this information to people that are actually planning the cities,

41:41.800 --> 41:47.560
so the urban planners, then they can hopefully design better cities also for pedestrians.

41:47.560 --> 41:51.720
So this is kind of the long-term goal of this project social maps.

41:51.720 --> 42:00.680
Ah, interesting. It makes me think of a kind of a map system like Waze, but curated using,

42:00.680 --> 42:08.440
or updated using video information as opposed to this telemetry from other drivers using the app.

42:09.160 --> 42:13.320
Exactly, exactly. And the advantage is that you don't have to share your information,

42:13.320 --> 42:18.440
so you are just there and things are just detected. But of course, you're not identified,

42:18.440 --> 42:24.200
you're not followed. So it's just about following the trends of people, not the actual persons.

42:24.200 --> 42:26.760
I think this is an important difference to make here.

42:27.400 --> 42:33.640
So it sounds like the projects that we've talked about are you kind of think of as individual

42:33.640 --> 42:42.200
pieces within this overarching vision of social maps. Are there specific projects that you are

42:42.200 --> 42:51.640
working on that get you to the map piece, or does that come later? So that comes a bit later.

42:51.640 --> 42:58.600
So I'm more interested in the dynamic understanding part. So I've also one PhD student that just

42:58.600 --> 43:03.960
started and it's going to work more on what we talked about before, some called training networks

43:03.960 --> 43:10.360
with physical models. And in particular, apply them to multiple object tracking. So this is still

43:10.360 --> 43:19.000
a problem that is not solved. So we in 2014 opened a benchmark for multiple object tracking,

43:19.000 --> 43:24.440
called Mod Challenge. And you can see there that multiple object tracking methods have been

43:24.440 --> 43:29.400
really pushing the state of the art up. And so they've become better and better. But there's still

43:29.400 --> 43:35.400
tons of problems there. And in crowded scenes where you can barely identify one pedestrian from

43:35.400 --> 43:40.920
the other, this is still a big problem. So we want to still work on that particular problem.

43:41.720 --> 43:48.040
We want to work also on semantic segmentation, so identifying where in the city you are and

43:48.040 --> 43:53.080
which part of the city you're observing. Of course, for moving cameras, this is when the visual

43:53.080 --> 43:59.560
localization project comes in. And then I have also one other PhD student working more

43:59.560 --> 44:07.080
on general training or improving the training, making it easier to train big networks. So working

44:07.080 --> 44:13.480
more on a bit theoretical machine learning. So it's all these little projects that are coming

44:13.480 --> 44:21.480
together. But I'm still not working on the part of the actual map. So I think Google Maps already

44:22.680 --> 44:27.560
has a good architecture and a good construction for these maps. So it would just be about

44:27.560 --> 44:33.000
providing these information. So performing this this motion analysis for pedestrians. And that's

44:33.000 --> 44:39.320
the main work of the that I want to tackle. Well, it sounds like really exciting work. Laura,

44:39.320 --> 44:44.360
I really appreciate you taking the time to chat with me about it. Thanks so much. Yeah, thank you

44:44.360 --> 44:52.200
very much for inviting me. It was great. All right, everyone. That's our show for today.

44:52.200 --> 44:58.920
For more information on Laura or any of the topics covered in this episode, head over to Twomlai.com

44:58.920 --> 45:06.600
slash talk slash 168. Don't forget July 31st is your last chance to nominate us for this year's

45:06.600 --> 45:14.520
People's Choice Podcast Awards. Head over to Twomlai.com slash nominate and cast your vote right now.

45:14.520 --> 45:24.520
As always, thanks so much for listening and catch you next time.

