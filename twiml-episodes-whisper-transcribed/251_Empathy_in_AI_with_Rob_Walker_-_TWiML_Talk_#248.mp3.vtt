WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.040
I'm your host Sam Charrington.

00:32.040 --> 00:37.880
Today we're joined by Rob Walker, Vice President of Decision Management at Pegasystems.

00:37.880 --> 00:43.760
Rob joined us back in episode 127 to discuss hyper-personalizing the customer experience

00:43.760 --> 00:49.400
and today he's back for a discussion about the role of empathy in AI systems.

00:49.400 --> 00:53.560
In our conversation we dig into the role that empathy plays in consumer facing human

00:53.560 --> 00:59.800
AI interactions, the differences between empathy and ethics and a few examples of the ways

00:59.800 --> 01:05.240
that empathy should be considered when building enterprise AI systems.

01:05.240 --> 01:06.240
So what do you think?

01:06.240 --> 01:09.320
Should empathy be a consideration in AI systems?

01:09.320 --> 01:14.200
And if so, do any examples jump out for you of where and how it should be applied?

01:14.200 --> 01:16.280
We'd love to hear your thoughts on the topic.

01:16.280 --> 01:20.820
Feel free to reach out with a tweet at Sam Charrington or leave a comment on the show notes

01:20.820 --> 01:22.960
paid with your thoughts.

01:22.960 --> 01:26.800
Before we jump in, I'd like to thank Rob and the folks at Pegas for sponsoring this

01:26.800 --> 01:27.800
episode.

01:27.800 --> 01:33.240
If the topic of AI empathy peaks your interest, I'd encourage you to join Rob and I at Pegasworld

01:33.240 --> 01:39.840
June 2nd through 5th in Las Vegas, where he'll be delivering a keynote on this very topic.

01:39.840 --> 01:44.160
At the event you'll also hear great stories of AI applied to the customer experience at

01:44.160 --> 01:49.240
real Pegasystems customers, and of course, I'll be there and speaking as well.

01:49.240 --> 01:54.980
To register, visit pegaworld.com and use the promo code Twimal19 when you sign up for

01:54.980 --> 01:56.680
$200 off.

01:56.680 --> 02:00.320
Again, that's Twimal19, it's as easy as that.

02:00.320 --> 02:02.040
Hope to see you there.

02:02.040 --> 02:04.880
And now on to the show.

02:04.880 --> 02:10.760
Alright everyone, I am on the line with Rob Walker.

02:10.760 --> 02:15.760
Rob is the Vice President of Decision Management and Analytics with Pegasystems.

02:15.760 --> 02:23.240
You may remember Rob's name from TwimalTalk number 127 on hyper-personalizing the customer

02:23.240 --> 02:25.520
experience with AI.

02:25.520 --> 02:28.280
Rob, welcome back to this week in Machine Learning and AI.

02:28.280 --> 02:31.280
Yeah, thanks Sam, glad to be back.

02:31.280 --> 02:33.520
Absolutely, glad to have you back.

02:33.520 --> 02:39.200
Looking forward to what I believe will be a really interesting conversation on the role

02:39.200 --> 02:42.400
of empathy in AI.

02:42.400 --> 02:47.920
For folks that want to learn more about you, I'll refer them back to the previous episode,

02:47.920 --> 02:51.880
but give us a quick overview of your focus at Pegasystems.

02:51.880 --> 02:53.800
Yes, happy to do that.

02:53.800 --> 03:02.400
So with the impact, I'm responsible for our AI space, but we really try, I mean, there's

03:02.400 --> 03:07.400
so much hype around AI and we don't do AI just for AI sake.

03:07.400 --> 03:15.040
We really try to focus on making AI work for typically pretty large enterprises and

03:15.040 --> 03:18.120
typically in the area of customer engagement.

03:18.120 --> 03:19.120
Right?

03:19.120 --> 03:24.000
So in the previous episode, we talked about hyper-personalization and hyper-personalization

03:24.000 --> 03:31.920
is really trying to be one-to-one conversations with customers for companies to do that.

03:31.920 --> 03:35.960
And that requires a lot of AI.

03:35.960 --> 03:41.080
It also requires lots of other things, but AI is an important aspect of that.

03:41.080 --> 03:46.960
And that's what I mostly worry about, and also sort of areas around AI.

03:46.960 --> 03:50.720
It's not just, hey, AI is cool, let's use that in customer engagement to make customer

03:50.720 --> 03:55.920
engagement better, but it's stuff like, can we trust it?

03:55.920 --> 04:01.360
Who in your organization should be responsible for it if it makes weird decisions, if there

04:01.360 --> 04:03.960
is a bias, those kind of things?

04:03.960 --> 04:07.240
So that's typically what I worry about.

04:07.240 --> 04:12.840
The concept that kept coming up in our last conversation, I think this is pretty central

04:12.840 --> 04:21.720
to the way you think about applying AI to optimizing customer experiences, this idea of helping

04:21.720 --> 04:28.440
your customers figure out the next best action to take with their customers, is that right?

04:28.440 --> 04:29.440
That is correct.

04:29.440 --> 04:30.440
Yes.

04:30.440 --> 04:37.680
So the companies we work with typically implement like this customer decision hub, right?

04:37.680 --> 04:44.080
And it's a centralized decision authority that across all the different channels that

04:44.080 --> 04:52.320
companies may have, figures out the next best action during conversations, right?

04:52.320 --> 04:53.840
So what should we say?

04:53.840 --> 04:56.000
What should we not say?

04:56.000 --> 05:03.760
What price should we mention if it's commercial, basically trying to have very reasonable

05:03.760 --> 05:09.880
conversations, but at the same time, because most of the companies we work with are not

05:09.880 --> 05:11.840
just charities, right?

05:11.840 --> 05:16.640
So at the same time, you need to sort of improve customer value, right?

05:16.640 --> 05:22.400
So the next best action, the best in next best action, is typically some metric about customer

05:22.400 --> 05:28.960
value and trying to improve that over time by doing the best thing possible to optimize

05:28.960 --> 05:29.960
that.

05:29.960 --> 05:37.760
So this concept of empathy in AI is something that you'll be speaking about at the next

05:37.760 --> 05:43.120
Pegaworld, an event that your company hosts annually.

05:43.120 --> 05:47.040
I attended the last one and I'll be attending the next one.

05:47.040 --> 05:54.600
How did this idea of empathy, introducing empathy into these kinds of transactions or customer

05:54.600 --> 05:56.400
experiences?

05:56.400 --> 05:58.120
Where did that come from?

05:58.120 --> 06:00.600
Well, so I've always been interested.

06:00.600 --> 06:06.880
So before I joined Pegaw at some point before that I was a scientist, right, in AI, I did

06:06.880 --> 06:15.160
my PhD in that area and I've always been interested in not just all the cool things AI can do

06:15.160 --> 06:21.040
around predicting customer behavior and things like that, but also potentially the not so

06:21.040 --> 06:22.200
cool things, right?

06:22.200 --> 06:24.600
So can you trust it?

06:24.600 --> 06:25.800
Is it transparent?

06:25.800 --> 06:27.560
Is it opaque?

06:27.560 --> 06:29.400
Is there a bias?

06:29.400 --> 06:31.000
Can it go rogue?

06:31.000 --> 06:32.840
Those kind of things.

06:32.840 --> 06:38.960
So over the last years, we've really tried to sort of guard the moral high ground, if you

06:38.960 --> 06:44.920
will, around AI and now just look at what it can, the value it can bring, but also

06:44.920 --> 06:50.480
mitigate the risk that you can have with AI.

06:50.480 --> 06:54.720
And following sort of that path, empathy was a very natural thing.

06:54.720 --> 07:01.480
I mean, you know, the bigger thing is morality, you know, the morality of AI and AI decisions,

07:01.480 --> 07:04.960
which is, you know, that's a big beast.

07:04.960 --> 07:10.800
And sort of more ethical behavior and empathy seemed to be something that was just about

07:10.800 --> 07:17.480
tangible enough to try to really put it into the, into the product and into the vision

07:17.480 --> 07:20.600
of best practice around using AI.

07:20.600 --> 07:26.600
So we've been spending quite some time thinking about that and how you can operationalize that

07:26.600 --> 07:27.600
kind of thing.

07:27.600 --> 07:35.880
Now, when you start talking about the morality of AI, certainly, and even to large degree

07:35.880 --> 07:41.400
empathy, start to, you know, the picture in my head starts to form around, you know, what

07:41.400 --> 07:47.040
some of us will call AGI, artificial general intelligence, you know, what we talk about

07:47.040 --> 07:50.360
more commonly is like sci-fi AI, right?

07:50.360 --> 07:52.000
Is that what we're talking about here?

07:52.000 --> 07:57.360
Or are we talking about something that, you know, how can you make this more concrete

07:57.360 --> 07:58.360
for us?

07:58.360 --> 08:02.200
Yeah, because, because I'm definitely not talking about that kind of thing.

08:02.200 --> 08:05.840
I mean, I mean, that's really, well, it's, it's, it's interesting.

08:05.840 --> 08:06.840
Right?

08:06.840 --> 08:12.880
I mean, I think everybody in AI is thinking about how that, how that, how that works.

08:12.880 --> 08:17.600
But I think just as a human species, if I just, you know, just even reading the news, you

08:17.600 --> 08:23.560
know, today, I think I'm, I'm not sure we have morality, you know, very, very, very,

08:23.560 --> 08:27.520
very clear for, for, for everyone, at least there is a lot of discussion about what would

08:27.520 --> 08:32.080
be a moral judgment and not a moral judgment to even expect that of AI.

08:32.080 --> 08:36.640
I think it's already a tough act.

08:36.640 --> 08:41.520
But I'm certainly not talking about it in that kind of realm quite yet, although it's a

08:41.520 --> 08:42.840
very interesting topic, right?

08:42.840 --> 08:45.840
I was just thinking about, you know, self-driving cars, right?

08:45.840 --> 08:51.840
And, and, and sort of the moral judgments, they may, one day, have to make, right?

08:51.840 --> 08:54.040
In extreme circumstances.

08:54.040 --> 09:01.080
But this is very much sort of a smaller subset of those challenges where we're talking about

09:01.080 --> 09:04.360
customer engagement and, and those kinds of things.

09:04.360 --> 09:12.160
And I think in that area, empathy really shows in, in, in, in, in, in, in, in pretty clear

09:12.160 --> 09:17.320
dimensions, like stuff like, is this, if we are talking to me as a company, and it's an

09:17.320 --> 09:21.280
AI driven conversation, is that, is that a relevant, right?

09:21.280 --> 09:25.960
Or are you wasting my time stuff like, is it, is it appropriate?

09:25.960 --> 09:26.960
Right?

09:26.960 --> 09:27.960
So you're talking to me about something.

09:27.960 --> 09:30.200
And it might be interesting, it may be interesting to me,

09:30.200 --> 09:33.480
but it may not be still, it may not be appropriate, right?

09:33.480 --> 09:39.840
Maybe you shouldn't be selling me a gun or a car or a credit

09:39.840 --> 09:44.600
card that I actually can't pay back when I'm in debt, right?

09:44.600 --> 09:46.800
So it's those kind of things.

09:46.800 --> 09:48.760
And is there mutual value?

09:48.760 --> 09:51.040
Are you talking to me for something that, you know,

09:51.040 --> 09:53.760
can we have a transaction that has a mutual value?

09:53.760 --> 09:56.600
Or is it just about the company?

09:56.600 --> 09:59.600
And I think if companies implement those kind of

09:59.600 --> 10:06.040
considerations well, I think that will do pretty well

10:06.040 --> 10:09.240
on an empathy scorecard for starters.

10:09.240 --> 10:10.680
Now that's interesting.

10:10.680 --> 10:14.280
And actually somewhat different from the,

10:14.280 --> 10:15.880
I don't know if it's different from the direction

10:15.880 --> 10:20.400
that I thought we were going to go here or that, you know,

10:20.400 --> 10:23.960
the picture that formed in my mind when we were talking

10:23.960 --> 10:30.200
earlier about empathy or if one is part of the other.

10:30.200 --> 10:32.960
But I'll tell you, I'll kind of recount the picture

10:32.960 --> 10:34.080
that I have in my money.

10:34.080 --> 10:37.960
Let me know where it fits into this world.

10:37.960 --> 10:42.560
You know, I was envisioning primarily the kinds of interactions

10:42.560 --> 10:46.680
that you might have via a chat bot or, you know,

10:46.680 --> 10:48.120
chat kind of interface.

10:48.120 --> 10:51.760
And you often have, or even, you know,

10:51.760 --> 10:56.560
the extent to which AI is driving a call,

10:56.560 --> 11:00.160
a call center agent and their responses

11:00.160 --> 11:04.280
because that's some of that, or an IVR.

11:04.280 --> 11:07.120
You know, some of that is starting to happen.

11:07.120 --> 11:12.720
But I was envisioning kind of this set of capability

11:12.720 --> 11:14.880
where, you know, maybe the, you know,

11:14.880 --> 11:17.080
whether the chat bot or the IVR, you know,

11:17.080 --> 11:18.480
IVR is a great example, right?

11:18.480 --> 11:23.280
It's like, IVR should be able to tell from my voice,

11:23.280 --> 11:26.240
or could tell from my voice that I'm getting frustrated

11:26.240 --> 11:29.120
navigating the 50 million, you know, menus

11:29.120 --> 11:31.200
and maybe escalate me, you know,

11:31.200 --> 11:32.680
a little bit more quickly to someone.

11:32.680 --> 11:35.840
Or, you know, you can imagine the same kind of thing

11:35.840 --> 11:39.920
happening in a chat interaction where, you know,

11:39.920 --> 11:43.720
I'm interacting with this virtual agent.

11:43.720 --> 11:45.840
It's not getting what I need to do

11:45.840 --> 11:49.920
or it's, you know, needing asking me to repeat myself,

11:49.920 --> 11:52.040
you know, multiple times.

11:52.040 --> 11:55.400
You know, there's a degree of empathy and all that

11:55.400 --> 11:58.240
where it's understanding my,

11:58.240 --> 12:00.080
I guess I'm kind of simplifying that

12:00.080 --> 12:01.880
as understanding my emotional state

12:01.880 --> 12:05.400
and using that as part of the decisioning

12:05.400 --> 12:08.920
around what the next best action to take is.

12:08.920 --> 12:09.760
Yeah.

12:09.760 --> 12:12.400
But it sounds like maybe that's a piece of what you're saying,

12:12.400 --> 12:14.880
but you're also talking about, you know,

12:14.880 --> 12:17.840
maybe about the broader AI ethics conversation,

12:17.840 --> 12:20.800
your example around, you know,

12:20.800 --> 12:25.120
should we offer the credit card to the person

12:25.120 --> 12:28.640
who can't afford it is one that kind of,

12:28.640 --> 12:31.240
you know, resonates and kind of drives me in that direction.

12:32.240 --> 12:35.400
Yeah. Now, I think, so I think the example that you gave,

12:35.400 --> 12:38.600
right, like is somebody getting frustrated

12:38.600 --> 12:42.360
and those kind of things I think are a very important part

12:42.360 --> 12:44.200
of empathy, but I think it's part

12:44.200 --> 12:46.640
of the delivery mechanism.

12:46.640 --> 12:49.640
So I think what we're trying to do is sort of take that

12:49.640 --> 12:52.680
in two different layers, if you will, right?

12:52.680 --> 12:54.560
One is when you're talking to someone

12:54.560 --> 12:56.160
and somebody is getting frustrated

12:56.160 --> 12:59.760
or if you do voice detection and or inflection

12:59.760 --> 13:03.760
and you sort of notices that somebody is getting upset,

13:05.200 --> 13:09.480
you may want to change that may influence the next best action

13:09.480 --> 13:11.160
as part of the context.

13:11.160 --> 13:14.040
So I would call that, although it's cool,

13:14.040 --> 13:17.240
more of the sort of the superficial way of empathy, right?

13:17.240 --> 13:19.520
It's trying to feel somebody's mood

13:19.520 --> 13:21.640
and use that as a context,

13:21.640 --> 13:24.400
but that can become from a lot of different senses.

13:24.400 --> 13:26.480
It could be, you know, as I said,

13:26.480 --> 13:28.160
the inflection of a voice,

13:28.160 --> 13:29.960
it could be when this is face to face

13:29.960 --> 13:31.440
or you're in front of a camera,

13:31.440 --> 13:34.720
it can be that, you know, people can sort of read their face

13:34.720 --> 13:37.320
or the system can read the face of the customer

13:37.320 --> 13:39.160
and see that's not going well.

13:39.160 --> 13:44.080
But that's not the same as the underlying level

13:44.080 --> 13:46.840
of making sure that the next best action

13:46.840 --> 13:50.840
that you are contemplating is one that is empathetic

13:50.840 --> 13:52.760
or even moral, right?

13:52.760 --> 13:56.040
So I see that as two different, different thing.

13:56.040 --> 13:58.040
I think people think about empathy a lot

13:58.040 --> 13:59.680
like you were just describing it like,

13:59.680 --> 14:02.400
hey, I see this is making you upset.

14:02.400 --> 14:03.400
So I need to, you know,

14:03.400 --> 14:06.200
hurry this along or ask you what's wrong

14:06.200 --> 14:08.680
and that's all cool, very human stuff.

14:08.680 --> 14:12.320
But it's on the delivery of a particular action.

14:12.320 --> 14:15.200
But determining what you're going to do,

14:15.200 --> 14:17.440
that also requires empathy

14:17.440 --> 14:19.680
and that's more along the line of is this,

14:19.680 --> 14:21.640
is this relevant, is this appropriate,

14:21.640 --> 14:23.520
is this suitable, does kind of things.

14:24.600 --> 14:28.760
And do you specifically use the word empathy

14:28.760 --> 14:31.080
to distinguish it from ethics

14:31.080 --> 14:35.560
or are those ideas different in your mind

14:35.560 --> 14:40.040
or are they, you know, just a different word in this case?

14:40.040 --> 14:42.280
Yeah, now I think, I think the,

14:42.280 --> 14:44.920
if I think about ethics, that sort of, you know,

14:44.920 --> 14:49.120
ethical behavior, I think empathy is basically

14:49.120 --> 14:53.080
the step where humans, for now, maybe AI at some point,

14:53.080 --> 14:55.240
will basically be able to, you know,

14:55.240 --> 14:58.560
please themself in someone else's place

14:58.560 --> 15:02.280
and say, hey, if this is happening to me, you know,

15:02.280 --> 15:05.320
is this going to make me, you know, happier?

15:05.320 --> 15:06.440
And things like that.

15:06.440 --> 15:10.880
So I think they are, in that sense, very, very related.

15:10.880 --> 15:13.720
So in that sense, ethics is kind of relevant

15:13.720 --> 15:18.360
to some broader set of, you know, societal norms,

15:18.360 --> 15:21.720
whereas empathy, we don't have to figure all that out.

15:21.720 --> 15:24.520
It's just about, you know, this particular customer,

15:24.520 --> 15:26.880
am I doing the right thing by this customer?

15:27.880 --> 15:29.600
Yes, am I doing the right thing?

15:29.600 --> 15:32.680
And I think there are a few dimensions to that, right?

15:32.680 --> 15:35.000
Am I doing the right thing in terms of,

15:35.000 --> 15:37.960
am I forcing you to take like a risk

15:37.960 --> 15:40.440
that I actually think, you know, is too high?

15:40.440 --> 15:43.040
I already know that you won't be able to pay back

15:43.040 --> 15:45.080
that mortgage or that credit card or that thing,

15:45.080 --> 15:47.720
or you don't really need this kind of thing, right?

15:47.720 --> 15:51.080
So that's part of that decision,

15:51.080 --> 15:53.520
but also is it relevant in the first place, right?

15:53.520 --> 15:57.800
It's an empathetic thing to not try and waste somebody's time,

15:57.800 --> 15:59.840
right? I mean, if you don't, I don't know about you, Sam,

15:59.840 --> 16:02.760
but if you like all these ads, all the stuff that you get,

16:02.760 --> 16:06.000
if you, you know, browse the internet

16:06.000 --> 16:09.720
and look at all of these pages, it's, we're used to it.

16:09.720 --> 16:11.960
It doesn't show a lot of empathy, right?

16:11.960 --> 16:14.400
Everybody's trying to get your attention to do things

16:14.400 --> 16:18.000
that, you know, maybe 1% of the time,

16:18.000 --> 16:19.960
you're vaguely interested in, right?

16:19.960 --> 16:22.080
So that's part of it.

16:23.120 --> 16:23.960
Is it relevant?

16:23.960 --> 16:25.840
Is it not wasting my time?

16:25.840 --> 16:27.920
Is it, do you remember the context?

16:27.920 --> 16:31.560
Do you remember that I just spoke to you in another channel, right?

16:31.560 --> 16:34.920
I just walked into the branch, I just visited your website

16:34.920 --> 16:36.960
and now I'm going into the IPR.

16:36.960 --> 16:40.680
Do you even remember that or are you forcing me

16:40.680 --> 16:42.960
to basically repeat everything I did?

16:42.960 --> 16:45.760
I did, you know, in the previous channel.

16:45.760 --> 16:47.640
That's empathy, right?

16:47.640 --> 16:51.680
Empathy with customers that are trying to solve a problem

16:51.680 --> 16:55.560
or that want to get value out of their interaction

16:55.560 --> 16:56.600
with the company.

16:56.600 --> 17:01.600
This is clearly an issue that is much bigger than AI, right?

17:02.680 --> 17:06.960
We don't have to, you know, look very far to recognize

17:06.960 --> 17:11.680
that in many ways, the previous financial crisis

17:11.680 --> 17:15.760
with the mortgage bubble grew out of giving loans

17:15.760 --> 17:19.320
to people that, you know, weren't qualified for them.

17:19.320 --> 17:21.400
And there are many, many more examples

17:21.400 --> 17:25.320
where organizations, you know, fail to exhibit

17:25.320 --> 17:27.720
the kind of empathy that you are describing

17:27.720 --> 17:30.160
that have nothing to do with artificial intelligence

17:30.160 --> 17:34.240
or machine learning, you know, why take this on

17:34.240 --> 17:37.160
from an AI perspective?

17:37.160 --> 17:38.800
Well, I think that's a good question.

17:38.800 --> 17:42.040
And I think the answer to that is

17:42.040 --> 17:45.400
that the way we look at customer interaction in general

17:45.400 --> 17:48.720
is to always do this next best action kind of thing.

17:48.720 --> 17:51.880
And the next best action is actually collaboration

17:51.880 --> 17:55.480
between humans, you know, inside the company,

17:55.480 --> 18:00.480
deciding on, you know, rules or thresholds or policies

18:01.400 --> 18:05.840
working together with AI, where AI is maybe determining

18:05.840 --> 18:10.840
the risk, it's determining the level of likely interest

18:11.040 --> 18:12.480
from the customer.

18:12.480 --> 18:16.200
And it's that combination that creates the metric

18:16.200 --> 18:19.480
for this is the best thing to do right now, right?

18:19.480 --> 18:21.280
So you're quite right.

18:21.280 --> 18:23.680
It's actually that mortgage example or the bubble

18:23.680 --> 18:25.920
that you just described is a great combination, right?

18:25.920 --> 18:29.280
There are analytic models that should have said,

18:29.280 --> 18:32.600
listen, for this group of people,

18:32.600 --> 18:35.680
the risk is not really acceptable.

18:35.680 --> 18:38.360
And you shouldn't be pushing them

18:38.360 --> 18:42.560
on this level of mortgage, right?

18:42.560 --> 18:46.480
Suitability, for instance, is not taking into account, right?

18:46.480 --> 18:51.480
So it's that combination of AI and rules that I, you know,

18:53.440 --> 18:56.520
we call that decisioning or decision management

18:57.640 --> 19:02.640
that basically needs to represent empathetic behavior, right?

19:03.400 --> 19:08.400
So it's not just the AI, it's also the rules.

19:08.400 --> 19:11.960
But one of the reasons I think the AI aspect is so important

19:11.960 --> 19:14.600
is because the AI is learning, right?

19:14.600 --> 19:19.600
So it can, you know, have evolved a particular bias, right?

19:20.960 --> 19:23.680
It may be a very opaque algorithm

19:23.680 --> 19:27.240
that may have evolved at bias and you don't even know, right?

19:27.240 --> 19:29.680
So there are a lot of aspects of AI

19:29.680 --> 19:33.760
that I think really touch on ethics and empathy as well.

19:33.760 --> 19:35.880
When you're talking about this at Pegaworld,

19:35.880 --> 19:39.240
are you, you know, you raising this as an issue

19:39.240 --> 19:42.960
that customers should start thinking about this?

19:42.960 --> 19:46.360
Are you talking about new capabilities

19:46.360 --> 19:50.440
that you're unveiling at Pegas with, you know,

19:50.440 --> 19:54.520
with the product that will help them address these issues?

19:54.520 --> 19:58.800
Is it, you know, or is it something else?

19:58.800 --> 20:00.920
Well, I don't want to feel all my own thunder,

20:00.920 --> 20:04.800
but we'll definitely, so two years ago,

20:04.800 --> 20:07.560
one of the things that I was talking about in the keynote

20:07.560 --> 20:10.120
was about this thing called the T switch

20:10.120 --> 20:12.600
and the T was, you know, stands for transparency,

20:12.600 --> 20:14.080
but also for trust.

20:14.080 --> 20:16.840
And it's basically the ability that once you have

20:16.840 --> 20:20.160
this centralized next best action capability

20:20.160 --> 20:24.560
inside of your company, that you can have full control

20:24.560 --> 20:29.280
over where you allow sort of opaque algorithms

20:29.280 --> 20:31.800
like deep learning or genetic algorithms

20:31.800 --> 20:33.640
or that's kind of fancy stuff

20:33.640 --> 20:36.480
or where you insist on more transparent algorithms

20:36.480 --> 20:39.040
that you can actually explain to a customer

20:39.040 --> 20:42.920
and that you really explain or that you really understand yourself.

20:42.920 --> 20:44.280
So that was one aspect.

20:45.360 --> 20:49.160
Next up is the thing around empathy.

20:49.160 --> 20:52.880
I want, I think it's a really good thing if companies

20:52.880 --> 20:57.880
are aware at all times how empathetic their behavior is.

20:58.880 --> 21:03.560
So think about sort of a dashboard

21:03.560 --> 21:07.480
where you would see of all the actions my company is taking

21:07.480 --> 21:10.280
and these are, you know, the side of companies we work with,

21:10.280 --> 21:12.760
these are hundreds of millions a day, right?

21:12.760 --> 21:15.240
Hundreds of millions of interactions a day

21:15.240 --> 21:19.680
and then being able to see, okay, these are actions

21:19.680 --> 21:21.440
that we are taken automatically

21:21.440 --> 21:24.400
is a combination of AI and rules

21:24.400 --> 21:27.080
that are not empathetic

21:27.080 --> 21:30.240
and that means that they are going against the relevance

21:30.240 --> 21:33.280
or they are not appropriate, like not suitable.

21:33.280 --> 21:35.680
So we're talking about this credit card

21:35.680 --> 21:40.480
but it's not really suitable or it doesn't really create value

21:40.480 --> 21:41.680
for the customer, right?

21:41.680 --> 21:44.920
So imagine that while all of this is running,

21:44.920 --> 21:47.120
this combination of AI and rules

21:47.120 --> 21:50.000
and it's making hundreds of millions of decisions

21:50.000 --> 21:52.800
and having all of these conversations with customers

21:52.800 --> 21:55.680
that you can just see that as a real-time thing

21:55.680 --> 21:59.880
and say, hey, really, we're getting less empathetic.

22:00.840 --> 22:04.760
Let's see in our strategies, in our customers' strategies

22:04.760 --> 22:07.920
that we have in the algorithms that we use,

22:07.920 --> 22:09.720
where we're losing that, right?

22:09.720 --> 22:12.320
Are we pushing products that we shouldn't be pushing?

22:12.320 --> 22:16.360
Don't we have the rules that are determining suitability?

22:16.360 --> 22:21.360
It's those kind of things and then in addition to that,

22:21.360 --> 22:25.120
I think we can also determine sort of the cost

22:25.120 --> 22:27.720
of not being empathetic, right?

22:27.720 --> 22:31.720
So if you, for instance, if you are going against

22:31.720 --> 22:33.120
somebody's interests

22:33.120 --> 22:37.440
and I don't mean interest in terms of relevance, right?

22:37.440 --> 22:40.880
So what a lot of marketing is currently doing, right?

22:40.880 --> 22:42.640
They're spamming you with stuff,

22:42.640 --> 22:44.760
they're wasting your time on things

22:44.760 --> 22:47.320
that are actually not that relevant to you.

22:47.320 --> 22:50.640
It would be good to not only know the percentage

22:50.640 --> 22:53.840
of events where that happens,

22:53.840 --> 22:56.360
it would also be really good if you had a sense

22:56.360 --> 23:00.160
of the money you're actually losing

23:00.160 --> 23:02.440
and you can calculate that mathematically

23:02.440 --> 23:04.200
by, for instance, saying, hey,

23:04.200 --> 23:06.360
this is what we actually talked about to this customer.

23:06.360 --> 23:09.160
We talked about this mortgage and the customer said,

23:09.160 --> 23:11.600
no to it because it wasn't relevant.

23:11.600 --> 23:13.720
What we could have been talking about

23:13.720 --> 23:18.720
is this particular issue that we spotted in a different channel

23:19.680 --> 23:23.160
or last week or an hour ago,

23:23.160 --> 23:25.840
or maybe a much more relevant offer

23:25.840 --> 23:27.920
that we didn't think we wanted to do

23:27.920 --> 23:32.560
because the margin was not as big as on the mortgage, right?

23:32.560 --> 23:35.640
Having, making that a transparent thing,

23:35.640 --> 23:40.640
having people own the kind of empathy level of the brand

23:42.400 --> 23:47.400
I think is a really important thing going forward.

23:47.520 --> 23:50.280
It strikes me that that latter point

23:50.280 --> 23:55.280
around quantifying the cost of these non-emphathetic

23:55.280 --> 24:00.040
non-emphathetic actions is really a big part

24:00.040 --> 24:02.760
of where the problem lies.

24:02.760 --> 24:07.760
It's easy to know the cost of the expected revenue

24:10.520 --> 24:13.600
or profit from offering something,

24:13.600 --> 24:16.120
but a lot harder to know the cost

24:16.120 --> 24:19.360
of just wasting the customer's time

24:19.360 --> 24:23.120
or reducing the brand goodwill

24:23.120 --> 24:27.600
because of some series of less relevant

24:27.600 --> 24:29.800
or poor experiences.

24:29.800 --> 24:32.960
How do you overcome that gap?

24:32.960 --> 24:34.800
Yeah, well, I think some of the math

24:34.800 --> 24:36.920
actually works out quite nicely, right?

24:36.920 --> 24:39.680
So remember that when we do this next best action,

24:39.680 --> 24:44.600
and I said before, the best is a function of the value

24:44.600 --> 24:47.400
that is created in the relationship.

24:47.400 --> 24:52.400
So the math works out that you actually can know

24:52.400 --> 24:54.240
that if you go against the propensity,

24:54.240 --> 24:55.400
because for instance, you're selling,

24:55.400 --> 24:58.840
let's use this mortgage example that works really well,

24:58.840 --> 25:02.680
if you are thinking this is not particularly relevant,

25:02.680 --> 25:05.920
but if the customer says yes, this is the margin,

25:05.920 --> 25:10.920
this is the money I as a bank in this case will make, right?

25:11.520 --> 25:15.320
If you calculate all the times a customer said no,

25:15.320 --> 25:17.240
because you're basically offering stuff

25:17.240 --> 25:18.600
that's not particularly relevant,

25:18.600 --> 25:22.920
just you know, you're hoping that the customer will say yes.

25:22.920 --> 25:25.920
What, how could we have used that moment?

25:25.920 --> 25:27.760
How could we have used that interaction

25:27.760 --> 25:29.880
with the customer in a better way

25:29.880 --> 25:32.320
that would have created more value?

25:32.320 --> 25:34.640
If you just multiply that with a hundred of millions

25:34.640 --> 25:38.400
of decisions you make, you get to a monetary value.

25:38.400 --> 25:40.280
And you find in examples like this

25:40.280 --> 25:43.000
that there's some explicit decision

25:43.000 --> 25:46.920
that where the customer is saying,

25:46.920 --> 25:49.040
hey, let's offer this more profitable thing,

25:49.040 --> 25:51.400
even though we know it's not as relevant,

25:51.400 --> 25:56.400
or does that happen in more subtle ways?

25:58.000 --> 26:01.560
No, I don't think these ways are particularly subtle, right?

26:01.560 --> 26:05.480
So the way, because the way we work is like,

26:05.480 --> 26:08.240
so to do this next best action, right?

26:08.240 --> 26:11.960
We would calculate every single thing,

26:11.960 --> 26:15.000
this is also part of this hyper-personalization vision,

26:15.000 --> 26:17.160
to be completely one to one,

26:17.160 --> 26:19.680
it means that of all the possible conversations

26:19.680 --> 26:22.560
you could have, right, you're going to rate them

26:22.560 --> 26:24.880
in real time based on the context,

26:24.880 --> 26:26.040
and you're going to say,

26:26.040 --> 26:28.360
this is the thing we are going to talk about

26:28.360 --> 26:31.080
as a combination of what the AI thinks

26:31.080 --> 26:34.040
is particularly appropriate and relevant,

26:34.040 --> 26:37.240
as well as my rules that I have around profitability

26:37.240 --> 26:41.720
and inventory and all of those kind of things, right?

26:41.720 --> 26:43.000
So it's that combination,

26:43.000 --> 26:45.400
but we calculate them all in parallel.

26:45.400 --> 26:48.360
So it's relatively easy to see, okay,

26:48.360 --> 26:51.600
this is what we chose to actually talk about,

26:51.600 --> 26:54.800
but this is what we could have talked about

26:54.800 --> 26:58.800
if we had weighted suitability higher,

26:58.800 --> 27:02.640
or if we didn't overrule this very low,

27:02.640 --> 27:04.360
or this very high propensity and said,

27:04.360 --> 27:05.960
well, even though that's relevant,

27:05.960 --> 27:08.240
it's not what we want to talk about, right?

27:08.240 --> 27:11.360
So you can see how you get a drop off of typical,

27:11.360 --> 27:14.520
or of specific conversation topics

27:14.520 --> 27:17.760
that you decide not to pursue for other reasons,

27:17.760 --> 27:20.800
and that's what you can calculate.

27:20.800 --> 27:26.000
Is the task then starting to build awareness

27:26.000 --> 27:29.600
on the part of customers or users

27:29.600 --> 27:34.440
or kind of the industry as a whole

27:34.440 --> 27:39.440
to incorporate these types of empathy metrics,

27:39.440 --> 27:44.440
if we call them that, into their systems,

27:45.720 --> 27:49.040
their rules, their algorithms,

27:49.920 --> 27:54.240
and start to, is it as, I don't know if simple is the right word,

27:54.240 --> 27:58.280
but as simple as starting to try to put numbers

27:58.280 --> 28:02.320
around these suitability context

28:02.320 --> 28:06.160
to where relevance risk and then feeding them

28:06.160 --> 28:11.160
into your automation tooling with kind of appropriate weights

28:12.240 --> 28:14.600
or does it go beyond that?

28:15.800 --> 28:17.520
I think the way you describe it,

28:17.520 --> 28:19.840
so what we're trying to do is, first of all,

28:19.840 --> 28:22.440
we want to do that like an easy task.

28:22.440 --> 28:23.440
No, no, no.

28:23.440 --> 28:25.760
We're not conclusion or anything like that.

28:25.760 --> 28:27.480
Yeah, yeah.

28:27.480 --> 28:29.000
No, that's not easy in itself,

28:29.000 --> 28:31.160
but what we're trying to do is, first of all,

28:31.160 --> 28:32.520
make it very explicit.

28:32.520 --> 28:35.840
So when we talk about next best action,

28:35.840 --> 28:39.640
we have like, there are patterns that we've seen

28:39.640 --> 28:43.440
that are repeatedly successful,

28:43.440 --> 28:47.720
and they include things like relevance,

28:47.720 --> 28:51.400
suitability, mutual value, risk mitigation, right?

28:51.400 --> 28:56.400
So the tooling and the methodology already encourage

28:56.400 --> 28:58.920
companies to at least think about it, right?

28:58.920 --> 29:01.160
They may think, okay, well, for suitability,

29:01.160 --> 29:03.360
we really don't care about it, right?

29:03.360 --> 29:07.040
Or not as much, or we let profitability,

29:07.040 --> 29:09.960
Trump suitability anytime, right?

29:09.960 --> 29:12.160
But at least the product,

29:12.160 --> 29:14.160
well, if you follow the product guidance,

29:14.160 --> 29:17.320
you will have to take all of these considerations

29:17.320 --> 29:18.600
into account, right?

29:18.600 --> 29:23.600
So there is an ethical framework built into the software,

29:24.040 --> 29:26.880
into the strategies that it will generate.

29:26.880 --> 29:28.320
So that's one aspect of it,

29:28.320 --> 29:31.200
and then the other aspect of it are like these,

29:31.200 --> 29:33.640
the dashboard that you will show, right?

29:33.640 --> 29:36.160
So it's basically shaming companies a little bit,

29:36.160 --> 29:37.560
if they won't, right?

29:37.560 --> 29:42.320
Having them self-shame them into appropriate behavior,

29:42.320 --> 29:44.280
right, where they would say, hey, listen,

29:44.280 --> 29:46.120
we cranked up profitability,

29:46.120 --> 29:50.640
but it's at the expense of suitability or customer interest.

29:51.600 --> 29:53.920
And at least, I think the awareness

29:53.920 --> 29:56.040
and the transparency around these things

29:56.040 --> 30:01.040
will be leading to better behavior.

30:01.360 --> 30:06.360
How does the company begin to put tangible numbers

30:07.800 --> 30:11.680
and costs around things like mutual value

30:11.680 --> 30:15.960
and suitability and contexts awareness and relevance?

30:15.960 --> 30:17.520
And relevance is maybe easier

30:17.520 --> 30:20.280
because it impacts like propensity to buy.

30:20.280 --> 30:24.320
Risk is something that's kind of fundamentally numerical,

30:24.320 --> 30:28.840
but like some of these others are a little bit squishier, maybe?

30:28.840 --> 30:32.240
Yeah, well, but I think, you're right.

30:32.240 --> 30:35.080
And I think the, from what we've seen,

30:35.080 --> 30:39.040
is that sort of the less squishy things, right?

30:39.040 --> 30:44.040
Are once you are aware that you need to put them in,

30:45.040 --> 30:49.200
and at the AI or the decisioning in general,

30:49.200 --> 30:54.200
touching 100 million, making 100 million decisions,

30:54.200 --> 30:56.040
and all the different general with all of your customers,

30:56.040 --> 30:58.520
that that is part of your brand, right?

30:58.520 --> 31:00.440
And you need to protect that.

31:00.440 --> 31:04.560
I think that's a very important thing.

31:04.560 --> 31:06.880
I think for the squishier things,

31:06.880 --> 31:10.960
I think what we also encourage and also make possible

31:10.960 --> 31:13.640
is continuous experimentation, right?

31:13.640 --> 31:16.240
So there's always, there's control groups,

31:16.240 --> 31:18.720
there is all sorts of things where you can, you know,

31:18.720 --> 31:21.840
for a small percentage, a small sample of the customers,

31:21.840 --> 31:24.520
you can actually measure if you're having an effect, right?

31:24.520 --> 31:27.680
If they have a positive response to the brand,

31:28.720 --> 31:31.200
and you can see if, you know,

31:31.200 --> 31:34.760
what kind of strategy changes would improve that?

31:34.760 --> 31:37.760
And that is a best practice kind of thing to do.

31:39.280 --> 31:44.280
Do you have any examples of folks that you've worked with

31:44.840 --> 31:47.080
that you can kind of walk us through

31:47.080 --> 31:48.600
how this all plays out,

31:48.600 --> 31:53.600
and how they want about making, you know,

31:54.000 --> 31:55.760
kind of incorporating these ideas

31:55.760 --> 31:57.600
into the way they make decisions?

31:58.720 --> 32:02.080
Yeah, so I think even before we sort of, you know,

32:02.080 --> 32:06.160
invested in all this around, you know, empathy

32:06.160 --> 32:09.640
and also before the transparency and opacity,

32:09.640 --> 32:12.640
it's not like, you know, these big brands

32:12.640 --> 32:16.400
are not aware of these issues, right?

32:16.400 --> 32:20.720
I mean, if I think 10 years ago, maybe longer ago,

32:20.720 --> 32:22.880
I had long conversation with banks

32:22.880 --> 32:27.560
that were sued for, like, misscelling, right?

32:27.560 --> 32:30.680
That I think, you know, we've seen more recent examples

32:31.880 --> 32:34.960
where obviously these companies

32:36.480 --> 32:39.000
want to sort of, you know, control that,

32:39.000 --> 32:41.120
even if just for their own sake, right?

32:41.120 --> 32:43.440
To not be part of some class action.

32:43.440 --> 32:47.240
And in the next best action methodology,

32:47.240 --> 32:51.160
stuff like relevance, appropriateness, failure and risk,

32:51.160 --> 32:54.040
have always been sort of first class citizens, right?

32:54.040 --> 32:58.720
What we're now trying to do is to make sure

32:58.720 --> 33:02.360
that it's much harder to break those patterns,

33:02.360 --> 33:06.160
or if you are, you know, don't want to be compliant

33:06.160 --> 33:10.920
with these kind of ethics practices,

33:10.920 --> 33:13.680
you at least have to make the effort.

33:13.680 --> 33:17.560
I think to your question, I think especially the banks,

33:17.560 --> 33:19.600
I think we've seen it in other industries as well,

33:19.600 --> 33:23.600
are getting very worried about their brand image

33:23.600 --> 33:26.280
in that regard, and they are putting, you know,

33:26.280 --> 33:28.320
suitability criteria, for instance,

33:28.320 --> 33:29.960
is a pretty hot topic right now.

33:29.960 --> 33:33.120
They're putting that as part of their next best action

33:33.120 --> 33:34.200
strategy.

33:34.200 --> 33:38.240
We just want to help them by showing the cost of that

33:38.240 --> 33:39.800
and the benefits of that.

33:39.800 --> 33:43.920
Okay, you mentioned compliance in there.

33:43.920 --> 33:47.040
Do you envision a time where an enterprise

33:47.040 --> 33:51.120
will have a formal empathy compliance regime?

33:51.120 --> 33:52.240
Will it be called that?

33:52.240 --> 33:53.560
Will it be called something else?

33:53.560 --> 33:56.080
Does it already exist under some other guys?

33:57.720 --> 34:00.760
I think that definitely will happen

34:00.760 --> 34:02.120
in some cases already happens.

34:02.120 --> 34:03.720
I don't think it's called empathy

34:03.720 --> 34:06.600
is probably more on the ethics board,

34:06.600 --> 34:11.600
where they, and again, it's not only about the company

34:14.480 --> 34:18.400
itself, it's also about basically a compliance issue

34:18.400 --> 34:21.080
out of self-interest, especially now,

34:21.080 --> 34:24.160
and this is what where the AI is so important,

34:24.160 --> 34:26.560
where there's so much self-learning going on

34:26.560 --> 34:31.560
on this incredible scale that there could be a bias,

34:31.560 --> 34:34.440
and there could be all sorts of things happening

34:34.440 --> 34:39.440
that may not be so easy to control or even spot.

34:41.840 --> 34:44.760
So I know that some of the companies I talk to,

34:44.760 --> 34:47.000
and these are larger companies,

34:47.000 --> 34:52.000
but they have a board already for all the algorithms

34:52.640 --> 34:55.840
that are involved in customer interaction.

34:55.840 --> 34:58.800
And I think that's a sensible thing to do.

34:58.800 --> 35:02.240
It's part of what inspired this transparency

35:02.240 --> 35:05.520
or trust switch in the software to make sure

35:05.520 --> 35:09.800
that all AI is at least you can control

35:09.800 --> 35:13.480
the level of transparency that you require

35:13.480 --> 35:18.240
in certain circumstances, talking to customers.

35:18.240 --> 35:20.400
Do you envision like a chief empathy officer?

35:20.400 --> 35:22.080
It sounds like no, it's probably going to be

35:22.080 --> 35:24.840
if anything, it'll be a chief ethics officer

35:24.840 --> 35:27.280
or some other role.

35:27.280 --> 35:30.320
Where do you see this all sitting?

35:30.320 --> 35:33.440
Yeah, well, I think this is a very interesting topic

35:33.440 --> 35:37.200
because I think this will become very, very important

35:37.200 --> 35:39.160
if it isn't already.

35:39.160 --> 35:41.840
And I think you will get into a situation

35:41.840 --> 35:46.840
where you have at the first level AI trying to check

35:49.440 --> 35:54.440
other AI for biases or an ethical behavior,

35:54.640 --> 35:56.640
because it's just a lot,

35:56.640 --> 36:00.880
and it would only escalate if such a bias

36:00.880 --> 36:05.200
or other irregularities is detected.

36:06.080 --> 36:09.440
But it's certainly, and again, I'm talking about

36:09.440 --> 36:11.960
the larger companies with tens to hundreds

36:11.960 --> 36:16.640
of millions of customers that are very worried about,

36:16.640 --> 36:18.640
especially with the level of automation

36:18.640 --> 36:20.520
that's now available, and then AI

36:20.520 --> 36:23.560
that is dynamically learning new things

36:23.560 --> 36:28.800
or evolving new things to have an ethics board like that.

36:28.800 --> 36:31.840
And we try from a product perspective,

36:31.840 --> 36:35.840
we try to make sure that like you have QA tests

36:35.840 --> 36:38.560
on quality assurance tests, on performance

36:38.560 --> 36:41.520
and other things that as a matter of course,

36:41.520 --> 36:45.840
you would do the same thing around bias detection

36:46.840 --> 36:50.240
or other irregularities before you release

36:50.240 --> 36:54.160
the next version of your corporate brain,

36:54.160 --> 36:58.400
so to speak, to make that easier.

36:58.400 --> 37:03.400
I think the ideas of kind of making these

37:03.720 --> 37:08.720
more empathetic types of qualities of your various offers,

37:11.800 --> 37:15.160
you know, as you've suggested throughout this conversation,

37:15.160 --> 37:18.160
it's very much kind of connected to this idea

37:18.160 --> 37:19.560
of transparency, right?

37:19.560 --> 37:22.440
There are, you know, these dollars and cents things

37:22.440 --> 37:25.360
that we kind of build into decision-making algorithms

37:25.360 --> 37:28.240
all the time, but there's all this other stuff

37:28.240 --> 37:30.480
that goes into the customer experience

37:30.480 --> 37:32.360
and what we're doing here, you know,

37:32.360 --> 37:35.600
we're calling empathy is really the idea

37:35.600 --> 37:40.600
of making a lot of those non-premafacy financial aspects

37:44.200 --> 37:48.200
you know, A, more transparent and then B,

37:48.200 --> 37:50.960
like putting, trying to make them more financial

37:50.960 --> 37:54.320
or putting numbers against them and then, you know,

37:54.320 --> 37:57.120
incorporating them into decision-making,

37:57.120 --> 38:00.120
dashboarding them so that there's some awareness of them

38:00.120 --> 38:05.080
and so that the organization can manage against them.

38:05.080 --> 38:09.760
Really interesting set of ideas around how to make,

38:10.800 --> 38:15.800
how to make this idea of A, I think it's a lot more tangible.

38:15.800 --> 38:18.840
Yeah, I think, yeah, yeah, tangible, I think it's the right word.

38:18.840 --> 38:22.440
So can we, are there straightforward ways to, you know,

38:22.440 --> 38:25.760
make sure that in our customer's strategies, you know,

38:25.760 --> 38:29.120
empathy is well represented and we can choose to ignore it,

38:29.120 --> 38:30.880
but then there are these, as you say, these,

38:30.880 --> 38:34.360
these desporting, these gauges, these dials

38:34.360 --> 38:39.360
that show you, that shame you into some compliance.

38:39.600 --> 38:41.360
And also, let's not forget that, like,

38:41.360 --> 38:44.800
I think the reason we as humans have empathy,

38:44.800 --> 38:48.240
you know, there can be lots of different theories around that,

38:48.240 --> 38:51.760
but personally, I think that evolved, right?

38:51.760 --> 38:55.320
It evolved out of a desire to collaborate, right?

38:55.320 --> 38:58.440
So it's empathy is not like a cost to the company.

38:58.440 --> 39:01.840
Empathy is actually establishing your, you know,

39:01.840 --> 39:05.880
better relationship and a longer term relationship

39:05.880 --> 39:07.720
with your, with your customers.

39:09.400 --> 39:12.000
Well, it would be interesting to kind of follow along

39:12.000 --> 39:16.280
with this work and see, you know, I'd love to hear a case study

39:16.280 --> 39:21.280
of, you know, how a customer kind of implements this end

39:21.280 --> 39:23.800
and once you've got this out in the market

39:23.800 --> 39:27.040
and have folks working with it.

39:27.040 --> 39:29.600
Yeah, I mentioned PegaWorld earlier,

39:29.600 --> 39:32.080
any besides from your own keynote,

39:32.080 --> 39:35.400
other things that you're looking forward to at the conference?

39:35.400 --> 39:37.960
Yeah, well, I mean, this will be the biggest effort.

39:37.960 --> 39:42.120
So it's always just very exciting about, you know,

39:42.120 --> 39:47.120
to show people, you know, where we are at,

39:47.400 --> 39:49.760
and it's not just about empathy, right?

39:49.760 --> 39:53.400
It's about, it's about also making decisions in general

39:53.400 --> 39:56.640
at a huge scale, you know, with this real-time AI

39:56.640 --> 39:58.880
on the one hand, and then on the other hand,

39:58.880 --> 40:00.400
and that's also part of empathy,

40:00.400 --> 40:02.040
although we didn't talk about it right now,

40:02.040 --> 40:07.800
but then following up on it, right, with the processes, right?

40:07.800 --> 40:11.120
So we are really, and you will hear a lot about that at PegaWorld,

40:11.120 --> 40:12.880
we're trying to, you know, have that,

40:12.880 --> 40:15.280
that combination very strongly.

40:15.280 --> 40:18.560
So it's, we have the AI and a decisioning

40:18.560 --> 40:21.240
to decide what to do, right?

40:21.240 --> 40:24.560
And then we have sort of the end-to-end automation

40:24.560 --> 40:27.520
that will tell the company how to do it,

40:27.520 --> 40:29.880
and to do it fast and efficiently,

40:29.880 --> 40:32.240
which also plays into empathy.

40:32.240 --> 40:34.760
So I really lost that interplay

40:34.760 --> 40:37.440
between sort of decisions and processes.

40:37.440 --> 40:42.160
So I'm expecting a lot of really good inter-discussions

40:42.160 --> 40:44.920
and presentations from my customers.

40:44.920 --> 40:46.120
Awesome, awesome.

40:46.120 --> 40:49.400
Well, Rob, I'm looking forward to seeing you once again

40:49.400 --> 40:50.400
at the event.

40:50.400 --> 40:53.160
Thanks so much for taking the time to jump on

40:53.160 --> 40:55.360
and talk this through with us.

40:55.360 --> 40:56.960
Okay, well, you're very welcome.

40:56.960 --> 40:57.800
Thank you.

40:57.800 --> 40:58.960
Thanks, Rob.

41:01.120 --> 41:03.680
All right, everyone, that's our show for today

41:03.680 --> 41:06.600
for more information on Rob or any of the topics

41:06.600 --> 41:08.000
covered in this episode.

41:08.000 --> 41:13.000
Visit twimmolai.com slash talk slash 248.

41:13.000 --> 41:15.320
Be sure to leave your thoughts on AI empathy

41:15.320 --> 41:16.720
while you're there.

41:16.720 --> 41:45.240
As always, thanks so much for listening and catch you next time.

