WEBVTT

00:00.000 --> 00:18.000
All right, everyone. I am here with long time friend of the show, John Bohannon, who is director of science at primer AI.

00:18.000 --> 00:33.000
If you recognize John's name, it may be from his May 2018 interview or his appearance in our Twimal Fest office hours, which we're focused on NLP back in October of last year.

00:33.000 --> 00:36.000
John, it is so great to have you back on the show. Welcome.

00:36.000 --> 00:56.000
Great to be back. I'm really looking forward to digging into our conversation. This is part of our AI rewind 2021 series and you are going to help us review all of the amazing things that happen in the NLP sphere this past year.

00:56.000 --> 01:09.000
Yeah, I, first of all, this has been so fun over the past week. I've been preparing for this. I don't think anyone's keeping up with everything in NLP. It's so much happening.

01:09.000 --> 01:14.000
So I really had to dig in, talk to my team, really review, and so I've learned a ton.

01:14.000 --> 01:29.000
The big picture that emerged for me at least was two things. One is that what we used to call NLP, you know, just text text in stuff comes out, but nothing else.

01:29.000 --> 01:45.000
That seems to be chilling out that like for the past few years, it didn't seem like a month would go by without some huge revolution, some brand new architectures, some totally new way of, you know, dealing with the data.

01:45.000 --> 02:00.000
And now that's kind of like flattening out and we're in what I think you can call the incremental phase of the science. So the big explosion. And now it's just like heads down trying to make it more efficient, you know, just better.

02:00.000 --> 02:19.000
And then the second big thing that emerged for me was that NLP is kind of like eating up the rest of ML. You know, they used to, they used to say like software is eating the world. And then it was ML eating software. Now I think NLP is eating ML because computer vision and language are just coming together.

02:19.000 --> 02:30.000
So I think that was the most like freakishly new really cool stuff. The rest is just like getting to business, making it work.

02:30.000 --> 02:42.000
Yeah, that's awesome. And in fact, that theme was the court that ladder of the two themes that you mentioned was kind of the core of the conversation in our computer vision.

02:42.000 --> 02:58.000
But in the AI rewind series, I think we'll talk about it from well, you know, obviously a slightly different take in this conversation, but so far the consensus is that that has absolutely been the case.

02:58.000 --> 03:11.000
Let's maybe dive into that particular point. What were some of the specific things that you saw that kind of led to this feeling of NLP eating the world.

03:11.000 --> 03:30.000
So, I mean, it was right in January of this year that Dolly came out, open eyes just crushing it, open eye, open AI is just owning this weird new hybrid space. And they're clearly having a blast. I mean, you almost giggle while you read their papers, you can just tell how fun it is to do this new work.

03:30.000 --> 03:37.000
I mean, it's, you know, we were talking earlier about how hard it is to keep up with the space, but it's hard to keep up with just open AI.

03:37.000 --> 03:47.000
I know, I know it's, God, it's just a really exciting time. So, you know, Dolly was followed by Clip and most recently we have this new thing called glide.

03:47.000 --> 03:51.000
That just came out like in the recent like past week or two.

03:51.000 --> 03:58.000
And I actually played with it this morning and I emailed you some of the output. I was just playing with it this morning.

03:58.000 --> 04:08.000
It's, it's just getting so dang good. You, you just literally use language to ask a model to generate images that you want.

04:08.000 --> 04:15.000
And one way that we've been using it, you can actually make it do useful work too. You can do image search.

04:15.000 --> 04:21.000
So, if you've got a whole bunch of images, let's say that you harvested from Twitter or something.

04:21.000 --> 04:29.000
And you want to say, Hey, find me blah. And you just describe it like write the caption for an image you think exists.

04:29.000 --> 04:35.000
So, you just take all those images and you put them into this embedding space that lives in the same world as the text.

04:35.000 --> 04:43.000
And so, it takes your text. It finds that high dimensional address. And then it just goes finds images that are kind of in that neighborhood and shows it to you.

04:43.000 --> 04:49.000
So, like semantic search on images is suddenly really, really working. So, that's just cool.

04:49.000 --> 04:56.000
Maybe contextualize glide relative to dolly and clip. What are the differences between the.

04:56.000 --> 05:06.000
Yeah, so dolly generates images clip is this classifier that that's the thing that knows how to take captions and images.

05:06.000 --> 05:15.000
That's the training data originally and put them into a common high dimensional space so that you can go from text to image or image to text.

05:15.000 --> 05:25.000
And so, glide is a new innovation where rather than guiding the generator of the images to try and satisfy the caption you gave it.

05:25.000 --> 05:31.000
With a classifier, it uses a noising technique so that you actually don't even need the classifier.

05:31.000 --> 05:40.000
The result, which I'm pretty convinced by even sending you those low res images is that it's much better at making photo realistic images.

05:40.000 --> 05:50.000
You know, like up till now, most of the fun that people have been having with VQGAN and clip systems has been like dreamy stuff.

05:50.000 --> 05:58.000
You know, something crazy. Like I did a whole holiday themed one at primer where I was just doing variations on Christmas trees.

05:58.000 --> 06:12.000
And one of them was a Christmas tree growing in a bathroom. And sure enough, it created this surreal image of a Christmas tree growing out of a toilet with a kind of a hand towel looking like a Santa hat.

06:12.000 --> 06:14.000
You know, just like.

06:14.000 --> 06:19.000
You're not going to you're not going to like make anything useful for the world, but it's fun.

06:19.000 --> 06:31.000
But with glide, they're getting so good that I think it actually will actually get commercial use and the really cool innovation is in painting.

06:31.000 --> 06:44.000
So now you can use this system to make an image. Let's say you're a designer and you want to make like a mock up of a living room or landscape or whatever it is you're trying to like visualize.

06:44.000 --> 06:57.000
You describe it and then you want to change something about it. What you can do is literally like finger paint. You can kind of circle a little zone of the image and then you can add an element to be like and a red barn.

06:57.000 --> 07:03.000
And then you could go in on that red barn. You'd be like with yellow windows. So it's this is the future of Photoshop.

07:03.000 --> 07:10.000
There will be a language interface to Photoshop. Either Adobe is going to do that or someone's going to eat their lunch.

07:10.000 --> 07:14.000
That's that's what we're going to be using in the future.

07:14.000 --> 07:18.000
And so is glide using clip to.

07:18.000 --> 07:21.000
No, you don't need it. That's the whole point. You don't need it anymore.

07:21.000 --> 07:23.000
Okay.

07:23.000 --> 07:24.000
Yeah.

07:24.000 --> 07:31.000
So it's pretty cool. And they haven't released the full weights model, but they released a small version, which is what I use this morning to send you the.

07:31.000 --> 07:40.000
Why are you talking about the, they're probably in a robot costume and robot for giant friendly robot visiting St. Louis.

07:40.000 --> 07:51.000
Nice. And those images are you were just mentioning this, but their generated images, based on the text as opposed to the way you originally describe it.

07:51.000 --> 07:55.000
It was almost like a image search where it's finding the images. What's the relationship?

07:55.000 --> 07:59.000
that knows how to take text and try and generate an image.

07:59.000 --> 08:02.000
But the way that you do it in the back end,

08:02.000 --> 08:06.000
I think from now on, we're not going to be using clip

08:06.000 --> 08:08.000
to guide that generation.

08:08.000 --> 08:11.000
So it starts off with just a random bunch of pixels.

08:11.000 --> 08:15.000
And then it tries to move in the direction of an image

08:15.000 --> 08:18.000
that matches your caption, because there's that common space.

08:18.000 --> 08:20.000
So clip was doing all that guidance before.

08:20.000 --> 08:23.000
Now with glide, you don't need clip to do that.

08:23.000 --> 08:25.000
So that's that's the idea.

08:25.000 --> 08:27.000
And it's pretty, it's pretty fast too.

08:27.000 --> 08:29.000
I mean, even though they're not very high res,

08:29.000 --> 08:33.000
it was like a minute per image this morning to make those.

08:33.000 --> 08:34.000
So I could just play around.

08:34.000 --> 08:36.000
That's awesome. That's awesome.

08:36.000 --> 08:41.000
And along the same line, do you mentioned a few other models

08:41.000 --> 08:45.000
and papers that were in similar vein?

08:45.000 --> 08:46.000
What were some of those?

08:46.000 --> 08:50.000
Yeah. So on the theme of NLP eating the world,

08:50.000 --> 08:54.000
papers and usable systems have started to emerge

08:54.000 --> 08:59.000
in chemistry, in medicine,

08:59.000 --> 09:02.000
and I think it's just going to keep on going.

09:02.000 --> 09:05.000
The general utility of the transformer architecture

09:05.000 --> 09:10.000
and the approach of feeding in data in a similar way

09:10.000 --> 09:15.000
that we teach these models in an unsupervised fashion.

09:15.000 --> 09:18.000
Language is just starting to pay off.

09:18.000 --> 09:21.000
So we've seen protein structure prediction.

09:21.000 --> 09:25.000
We've seen chemical formula,

09:25.000 --> 09:28.000
you know, like manipulation.

09:28.000 --> 09:30.000
I think it'll just keep on going.

09:30.000 --> 09:32.000
We're going to just keep, it's like a little asset

09:32.000 --> 09:34.000
that's eating through problem space.

09:34.000 --> 09:36.000
And I, you know, I don't know where it'll stop.

09:36.000 --> 09:41.000
But I, I also don't think it's going to stop with these

09:41.000 --> 09:43.000
things that are like clip and glide.

09:43.000 --> 09:47.000
There's no, there's nothing special about images

09:47.000 --> 09:50.000
other than we just happen to have a whole bunch of data.

09:50.000 --> 09:52.000
We have a whole bunch of images with captions.

09:52.000 --> 09:53.000
That's super convenient.

09:53.000 --> 09:58.000
But I think like what's coming next, of course, is video.

09:58.000 --> 10:00.000
And, you know, with robotics,

10:00.000 --> 10:01.000
you've got other senses.

10:01.000 --> 10:03.000
What about tactile?

10:03.000 --> 10:05.000
What about movement?

10:05.000 --> 10:10.000
You know, I think that the trend is going to be towards

10:10.000 --> 10:13.000
multi-modality with a single system.

10:13.000 --> 10:16.000
So you'll have, you'll be able to just like

10:16.000 --> 10:21.000
describe things in text or show like a video example

10:21.000 --> 10:24.000
of what you're trying to get at or a sketch.

10:24.000 --> 10:27.000
You can come in from any angle and there will be this

10:27.000 --> 10:31.000
massive, common embedding space for all modalities.

10:31.000 --> 10:33.000
I think that's, that's probably where we're headed.

10:33.000 --> 10:36.000
And a lot of ways this is, you know,

10:36.000 --> 10:41.000
we're approaching this world that NLP folks have been

10:41.000 --> 10:45.000
evangelizing for a while in the sense of, you know,

10:45.000 --> 10:50.000
we create language around all these concepts that we care

10:50.000 --> 10:51.000
about.

10:51.000 --> 10:56.000
And so NLP, you know, is a fundamental currency of thought,

10:56.000 --> 10:58.000
right?

10:58.000 --> 11:02.000
And so the NLP community has been arguing that, you know,

11:02.000 --> 11:05.000
all the work in that field is going to pay off because

11:05.000 --> 11:08.000
that's the way we, you know, we think we think using

11:08.000 --> 11:10.000
language, we communicate using language.

11:10.000 --> 11:14.000
And so, whereas before computer vision was an isolation,

11:14.000 --> 11:17.000
you know, that wasn't grounded by language.

11:17.000 --> 11:21.000
And so now we, you know, we're starting to bring it all

11:21.000 --> 11:22.000
together.

11:22.000 --> 11:25.000
And what you're saying is we're just getting started.

11:25.000 --> 11:26.000
Oh, yeah.

11:26.000 --> 11:27.000
Oh, yeah.

11:27.000 --> 11:29.000
This is just like the very beginning.

11:29.000 --> 11:32.000
A lot of people are saying that videos coming this year.

11:32.000 --> 11:33.000
I don't think so.

11:33.000 --> 11:35.000
I think that's really hard.

11:35.000 --> 11:38.000
I think it's going to be a little while before we have a

11:38.000 --> 11:40.000
clip or glide equivalent for video.

11:40.000 --> 11:43.000
But what I'm waiting for is that first moment,

11:43.000 --> 11:46.000
kind of like a, you know, like the very first moments of

11:46.000 --> 11:51.000
audio recording when you get that first hello world moment,

11:51.000 --> 11:56.000
a hello world moment for computer generated video,

11:56.000 --> 11:58.000
you know, like,

11:58.000 --> 12:01.000
GAN style generated video.

12:01.000 --> 12:04.000
Will probably be like a little cartoon animation,

12:04.000 --> 12:06.000
like an animated GIF.

12:06.000 --> 12:08.000
I don't know why has anyone hasn't done it yet,

12:08.000 --> 12:10.000
but, you know, just come.

12:10.000 --> 12:11.000
Giffy.

12:11.000 --> 12:13.000
I haven't seen a thing.

12:13.000 --> 12:15.000
The closest I've seen is you take a real video and then you

12:15.000 --> 12:18.000
basically apply, you know,

12:18.000 --> 12:20.000
something like transfer kind of thing.

12:20.000 --> 12:21.000
Yeah.

12:21.000 --> 12:23.000
You basically take frame by frame and you just transform it.

12:23.000 --> 12:26.000
So you can find Reddit just flooded with these cute short

12:26.000 --> 12:29.000
clips of real video that have been transformed into weird

12:29.000 --> 12:30.000
things.

12:30.000 --> 12:35.000
But I mean, we're sitting on a gold mine of data called Giffy.

12:35.000 --> 12:38.000
It's just someone's got to be doing that right now.

12:38.000 --> 12:43.000
Just take that data set and make a version of clip for Giffy

12:43.000 --> 12:46.000
so that we can have animated GIFs on demand.

12:46.000 --> 12:48.000
Yeah.

12:48.000 --> 12:50.000
So that's going to be the hello world.

12:50.000 --> 12:53.000
And from there, you know, it's onwards to Hollywood.

12:53.000 --> 12:58.000
You mentioned the work that's happening proteins and chemistry

12:58.000 --> 12:59.000
and medicine.

12:59.000 --> 13:01.000
What's the.

13:01.000 --> 13:05.000
What's kind of the common theme behind these papers?

13:05.000 --> 13:07.000
What are they trying to do?

13:07.000 --> 13:10.000
It's, it really all boils down to the original trick of the

13:10.000 --> 13:12.000
transformer.

13:12.000 --> 13:16.000
It's, it's, it's amazing how valuable that idea was, you know,

13:16.000 --> 13:21.000
a paper in 2017 has just completely taken over the world.

13:21.000 --> 13:25.000
You know, the idea is if you have a sequence of some kind,

13:25.000 --> 13:31.000
language could be pixels, it could be sound, could be DNA or

13:31.000 --> 13:34.000
protein sequences.

13:34.000 --> 13:38.000
You just, you make the, what we call a language model and NLP.

13:38.000 --> 13:40.000
It'll probably, maybe it'll always be called language model,

13:40.000 --> 13:42.000
even when it's not language.

13:42.000 --> 13:45.000
I see that in papers like I link two papers and what I sent you.

13:45.000 --> 13:48.000
One's called a protein language model.

13:48.000 --> 13:50.000
So maybe that'll be the trend is you just call that a language

13:50.000 --> 13:52.000
model, even if it's not human language.

13:52.000 --> 13:55.000
But the idea is you just.

13:55.000 --> 13:59.000
You teach the system to learn the patterns and the data by

13:59.000 --> 14:03.000
having itself supervised at a massive scale is just essentially

14:03.000 --> 14:06.000
doing, you know, we call it tokens by tokens.

14:06.000 --> 14:09.000
You just take this attention when you stick it in there and the

14:09.000 --> 14:12.000
usual trick is you do a mass token task where you hide some

14:12.000 --> 14:16.000
of those tokens and you teach it to fill them in or autoregressive.

14:16.000 --> 14:19.000
You cut it and you say, what would you, you know, continue

14:19.000 --> 14:21.000
writing from here?

14:21.000 --> 14:23.000
It's all amounting to the same thing.

14:23.000 --> 14:26.000
You're forcing this very, very massive function.

14:26.000 --> 14:28.000
That's all a neural network is.

14:28.000 --> 14:32.000
It's just a huge function with a like ridiculous number of variables

14:32.000 --> 14:37.000
that we call parameters to, you know, essentially satisfy this

14:37.000 --> 14:40.000
objective of being able to predict.

14:40.000 --> 14:45.000
And the weird thing is all of these cool skills just seem to

14:45.000 --> 14:48.000
emerge from that, I think dumb trick.

14:48.000 --> 14:50.000
I think of it as a, and that's not a pejorative.

14:50.000 --> 14:52.000
I think it's a beautiful dumb trick.

14:52.000 --> 14:55.000
And that's another theme, by the way.

14:55.000 --> 14:57.000
We still don't know understand.

14:57.000 --> 15:00.000
We don't understand how it works, which blows my mind.

15:00.000 --> 15:02.000
ML used to be a branch of math.

15:02.000 --> 15:04.000
Now it feels more like biology.

15:04.000 --> 15:07.000
It feels like we're studying things that we don't fully understand

15:07.000 --> 15:10.000
and we're just taking an empirical approach, probing and prodding,

15:10.000 --> 15:12.000
trying to figure out how they work.

15:12.000 --> 15:17.000
Do you follow the more theoretical side of NLP and the efforts

15:17.000 --> 15:21.000
that are taking place to try to better understand how the model

15:21.000 --> 15:22.000
will work?

15:22.000 --> 15:25.000
The extent of my engagement with that, you're full honesty.

15:25.000 --> 15:29.000
Full honesty is here at Primer, we have a Slack channel called

15:29.000 --> 15:32.000
Algorithms, just for his throw of reasons.

15:32.000 --> 15:35.000
And people drop papers in there and start discussions.

15:35.000 --> 15:40.000
And every once in a while, I will see something that's a theoretical

15:40.000 --> 15:42.000
paper that just seems to be making some bold claim.

15:42.000 --> 15:45.000
Usually it's like, it sounds like physics to me.

15:45.000 --> 15:48.000
And I'll just tag tag in people, usually people with a physics

15:48.000 --> 15:52.000
background who are on my team and say, please explain this.

15:52.000 --> 15:53.000
Is this a big deal?

15:53.000 --> 15:55.000
And so we like dig into it.

15:55.000 --> 16:05.000
And usually the consensus is maybe, even people on my team who

16:05.000 --> 16:10.000
can really come to grips with these theoretical papers are always

16:10.000 --> 16:12.000
like, well, let's see.

16:12.000 --> 16:13.000
It feels very nascent.

16:13.000 --> 16:19.000
There hasn't been any big reduction on reductive understanding

16:19.000 --> 16:22.000
that you would get in physics with these models.

16:22.000 --> 16:24.000
It's still very empirical.

16:24.000 --> 16:25.000
People take a theoretical take.

16:25.000 --> 16:27.000
It's like intriguing.

16:27.000 --> 16:32.000
But I'm just, you know, like, I'm just waiting for someone to say,

16:32.000 --> 16:34.000
oh, yeah, this is the breakthrough we've been waiting for.

16:34.000 --> 16:36.000
This explains a lot.

16:36.000 --> 16:38.000
And I just haven't heard that.

16:38.000 --> 16:39.000
Kind of along those lines.

16:39.000 --> 16:46.000
One comment that was recently made to me relating to NERPs was this

16:46.000 --> 16:53.000
feeling that the NERPs, and let me be clear.

16:53.000 --> 16:59.000
And individuals experience with NERPs was that, you know, they're,

16:59.000 --> 17:07.000
they were able to take less practical, practical.

17:07.000 --> 17:12.000
They got less out of it, I guess, is the only way to say it.

17:12.000 --> 17:16.000
And I was reflecting on that.

17:16.000 --> 17:21.000
And this idea that, you know, over the past few years,

17:21.000 --> 17:25.000
what, one of the things that's been really interesting about this field

17:25.000 --> 17:31.000
is that you, to, to stay on top of the field and to implement,

17:31.000 --> 17:36.000
you know, useful things that were beyond something that was already packaged

17:36.000 --> 17:41.000
in a, in a library, like you, you had to read papers to understand what

17:41.000 --> 17:46.000
folks were doing, because the field was evolving so quickly.

17:46.000 --> 17:52.000
And I think you kind of alluded to this in your opening and that the

17:52.000 --> 17:56.000
field is in some sense slowing down.

17:56.000 --> 18:01.000
And I wonder if there's a corollary to that that the kind of the hard core

18:01.000 --> 18:08.000
research is, you know, is diverging from practice more so than in the past.

18:08.000 --> 18:09.000
Yeah, I agree.

18:09.000 --> 18:14.000
And it does resonate and it manifests in a bunch of different ways.

18:14.000 --> 18:20.000
And one of them, here's a really practical way that this manifests.

18:20.000 --> 18:25.000
Early on, and by early, I mean, like, circa 2018.

18:25.000 --> 18:29.000
NLP, you know, modern NLP is so new.

18:29.000 --> 18:31.000
These benchmarks emerged.

18:31.000 --> 18:33.000
You know, it was like, okay, let's get down to business people.

18:33.000 --> 18:37.000
We need, you know, natural language processing needs to take the,

18:37.000 --> 18:40.000
you know, measurement of these models seriously, because we started to

18:40.000 --> 18:45.000
create these giant language models that had these amazing capabilities and

18:45.000 --> 18:49.000
everyone wanted to play the game of is my model better than yours.

18:49.000 --> 18:53.000
And so, you know, you had glue and then that got beaten.

18:53.000 --> 18:54.000
We had super glue.

18:54.000 --> 18:59.000
Now we have something called gem and like a whole zoo of benchmarks out there.

18:59.000 --> 19:06.000
And something I've noticed is that they are just less and less useful.

19:06.000 --> 19:10.000
I think of them as academic benchmarks, academic data sets.

19:10.000 --> 19:13.000
So what you do is you, you know, you take.

19:13.000 --> 19:16.000
If you want to know how good name density recognition.

19:16.000 --> 19:19.000
It's one of the like the old school core tasks of NLP.

19:19.000 --> 19:23.000
Find me the people places things in this text.

19:23.000 --> 19:27.000
There's this old data set floating around called con LL.

19:27.000 --> 19:28.000
It's really old.

19:28.000 --> 19:31.000
It's just a little sample of news.

19:31.000 --> 19:34.000
You know, just from I think like one source.

19:34.000 --> 19:36.000
From one period of time.

19:36.000 --> 19:38.000
And we've been using it ever since.

19:38.000 --> 19:41.000
To measure how good you are at, you know, any are.

19:41.000 --> 19:43.000
And.

19:43.000 --> 19:47.000
It wasn't that long before, you know, when we built our own any our model,

19:47.000 --> 19:51.000
we started to notice that performance on that benchmark that academic con LL

19:51.000 --> 19:56.000
benchmark started to drift away from the performance on real customer data

19:56.000 --> 19:59.000
that we had gold labeled that we cared about.

19:59.000 --> 20:02.000
And that was the trend that we just saw happening over and over again is that.

20:02.000 --> 20:07.000
You know, if you're chasing state of the art on these on these big benchmarks.

20:07.000 --> 20:11.000
You're actually often driving down the performance on something you care about.

20:11.000 --> 20:13.000
And so you have to make your own internal benchmarks.

20:13.000 --> 20:15.000
And I think we're not alone.

20:15.000 --> 20:19.000
I think everyone across the industry is quietly maintaining their own internal benchmarks.

20:19.000 --> 20:20.000
I could keep track of stuff.

20:20.000 --> 20:21.000
And that's bad, right?

20:21.000 --> 20:24.000
That's the thing that came up in the computer vision conversation.

20:24.000 --> 20:29.000
Also, we've we've we're focused on the flip side of that, which was the academic tendency

20:29.000 --> 20:33.000
at least in CV to, you know, quote unquote overfit on image net.

20:33.000 --> 20:34.000
Yeah.

20:34.000 --> 20:43.000
And you're describing the, you know, you know, you know, the flip side of yours will be overfit on blue or blue or whatever it is, right?

20:43.000 --> 20:44.000
Yeah.

20:44.000 --> 20:51.000
So, I mean, that's one of the manifestations though of this problem is that, you know, early on when NLP just started to explode.

20:51.000 --> 20:54.000
You know, a big conference like Neurips.

20:54.000 --> 20:58.000
You know, and these kind of shared resources like these benchmarks made sense.

20:58.000 --> 21:00.000
Like we didn't we didn't even have a map. We didn't have a compass.

21:00.000 --> 21:02.000
We were just, okay, let's get together for some now.

21:02.000 --> 21:03.000
Yeah.

21:03.000 --> 21:05.000
Now we're past the explosion.

21:05.000 --> 21:10.000
And we're in the like settling like stable phase of the science where it's like, okay, let's make it good.

21:10.000 --> 21:12.000
Let's make it efficient.

21:12.000 --> 21:15.000
And now it's like very applied.

21:15.000 --> 21:24.000
Now like the performance of some base language model on some, you know, completely kooky task like natural language inference.

21:24.000 --> 21:28.000
It's kind of beside the point that's like not even helpful anymore.

21:28.000 --> 21:33.000
So, yeah, no one solved this, but that's a, that's a big story of this year is like,

21:33.000 --> 21:39.000
turmoil over benchmarks, how to measure the performance, what's relevant, how to improve things.

21:39.000 --> 21:44.000
And all the big companies came out with their own new big approach to this.

21:44.000 --> 21:50.000
The only thing that I've seen this year that's a truly new take on this is something from Facebook.

21:50.000 --> 21:55.000
Meta called Dynabench. And that's an experiment that's running now.

21:55.000 --> 22:00.000
So there, rather than just have a frozen gold label set and, you know, like a test.

22:00.000 --> 22:02.000
And everyone takes the same test.

22:02.000 --> 22:05.000
It's an ever evolving test that's adversarial.

22:05.000 --> 22:08.000
The whole point is to have humans in the loop try and trick models.

22:08.000 --> 22:11.000
And so you're collecting all these adversarial examples.

22:11.000 --> 22:13.000
That models find really difficult.

22:13.000 --> 22:16.000
And so it's just constantly evolving.

22:16.000 --> 22:22.000
And as a side virtuous side benefit, you're generating all this useful, truly instructive adversarial data.

22:22.000 --> 22:24.000
That's, that's a really fresh take.

22:24.000 --> 22:28.000
I'm going to be really interested this year to see what results come out of that.

22:28.000 --> 22:30.000
That is really interesting.

22:30.000 --> 22:37.000
Do you kind of on this benchmark that this idea that in industry,

22:37.000 --> 22:40.000
folks are collecting their own benchmark data sets.

22:40.000 --> 22:44.000
Do you, do you see that changing in the sense that of, you know,

22:44.000 --> 22:50.000
industry consortia or groups of organizations kind of sharing.

22:50.000 --> 22:53.000
This information of credit tried to create better data sets.

22:53.000 --> 22:57.000
Or is it more, you know, either some combination of, you know,

22:57.000 --> 22:59.000
this is our proprietary advantage.

22:59.000 --> 23:03.000
And so, you know, we're going to keep it or it's, you know,

23:03.000 --> 23:05.000
it's really only relevant to us anyway, because it's, you know,

23:05.000 --> 23:08.000
our specific problem, our specific customers.

23:08.000 --> 23:10.000
That kind of thing.

23:10.000 --> 23:15.000
No, I don't, I don't think there's a lot of goodwill around in this face anymore.

23:15.000 --> 23:21.000
The trend I'm seeing is use benchmarks to be able to brag about achieving state of the art,

23:21.000 --> 23:23.000
even if it's not really relevant.

23:23.000 --> 23:32.000
And quietly curating your own high value data that actually gets you models that solve the problems that people pay money for.

23:32.000 --> 23:38.000
The big, the big like exception to this that's going against that tide is Ellie Uther.

23:38.000 --> 23:41.000
That's how you pronounce it. I've never actually never heard that name.

23:41.000 --> 23:46.000
The creators of the GPT three alternative, the GPT neo folks.

23:46.000 --> 23:51.000
So I love that they're out there open sourcing nonstop.

23:51.000 --> 23:54.000
They put out a huge data set called the pile.

23:54.000 --> 23:55.000
They put out GPT neo.

23:55.000 --> 24:00.000
And I think, you know, they're, they're a force for, for good going against the tide of,

24:00.000 --> 24:03.000
basically closed, closed ML development.

24:03.000 --> 24:06.000
Because the problem is like that's going to slow down the science.

24:06.000 --> 24:14.000
And the majority of shared resources and standards is like we can all march to the same drum beat and make progress more quickly.

24:14.000 --> 24:17.000
But there's just a lot of money to be made right now.

24:17.000 --> 24:20.000
I mean, NLP is where all the investments going.

24:20.000 --> 24:25.000
So you're going to see a lot of, a lot of anti scientific trends.

24:25.000 --> 24:31.000
Along those lines, are you seeing a decrease in.

24:31.000 --> 24:36.000
Publishing by, you know, corporate research teams.

24:36.000 --> 24:38.000
No, I haven't seen any trends like that.

24:38.000 --> 24:43.000
They're still publishing, you know, solder chasing papers.

24:43.000 --> 24:45.000
We got to have the art in this.

24:45.000 --> 24:47.000
Even when it's just crazy.

24:47.000 --> 24:51.000
I mean, in, in, you know, a lot of NLP.

24:51.000 --> 24:55.000
They're still using this, this standard called Rouge.

24:55.000 --> 24:58.000
So it was invented for, you know, machine translation.

24:58.000 --> 25:03.000
And now it's used to evaluate summary quality.

25:03.000 --> 25:08.000
And then everyone just openly knows that it's completely flawed.

25:08.000 --> 25:15.000
But people just keep on reporting state of the art results, you know, with the Rouge metric, even though we have alternatives now.

25:15.000 --> 25:22.000
But it just reveals that a lot of the publishing that's happening, whether it's in universities or big companies is really just bragging rights.

25:22.000 --> 25:23.000
Yeah.

25:23.000 --> 25:33.000
Other than true scientific sharing, I mean, there's a mixture, there's a lot of, there's a lot of goodwill still, but yeah, it's a troubling trend.

25:33.000 --> 25:42.000
So maybe going back to this idea of incremental progress within NLP.

25:42.000 --> 25:48.000
You know, it sounds bad, but you know, when you think about kind of the hype cycle and, you know, the money markets.

25:48.000 --> 25:49.000
I think it's beautiful.

25:49.000 --> 25:54.000
It sounds less exciting, less sexy than NLP eating the world.

25:54.000 --> 25:57.000
But, you know, I think we're saying the same thing here.

25:57.000 --> 25:59.000
It's a, it's a necessary phase.

25:59.000 --> 26:07.000
And it's where, you know, we talk, we throw around this idea of democratization and stuff like that is hard.

26:07.000 --> 26:12.000
And this is related to the point I was making about, you know, everyone needed to be a researcher to use NLP.

26:12.000 --> 26:21.000
Like it's not the case anymore because things are settling down, the tools are standardizing, like you can pull state of the art stuff off of hugging phase and, you know, just use it.

26:21.000 --> 26:23.000
You don't have to implement the paper.

26:23.000 --> 26:24.000
Yeah, exactly.

26:24.000 --> 26:25.000
I think it's great.

26:25.000 --> 26:27.000
I mean, that's the business I'm in.

26:27.000 --> 26:30.000
You know, like we're trying to build stuff that people put in money for us.

26:30.000 --> 26:33.000
So you got to make that stuff efficient and actually high performance.

26:33.000 --> 26:38.000
So yeah, it's just settling down into normal science, normal engineering.

26:38.000 --> 26:49.000
But the revolution is still ongoing, you know, it's just that this first big, you know, explosion that was made possible by transformer based language models.

26:49.000 --> 26:50.000
It's kind of cooling down.

26:50.000 --> 26:55.000
It's like a lot of alternatives to transformers have been proposed.

26:55.000 --> 26:58.000
It was a paper, you know, you can forget transformers.

26:58.000 --> 27:00.000
You can use Fourier transform.

27:00.000 --> 27:01.000
Forget this.

27:01.000 --> 27:05.000
You can use this, you know, and none of them seem to have taken off.

27:05.000 --> 27:08.000
Transformers seems to be here this day.

27:08.000 --> 27:13.000
And so what are you seeing in the dimension of improving efficiency?

27:13.000 --> 27:21.000
Well, the really exciting thing is it started with something called the switch transformer that Google implemented successfully.

27:21.000 --> 27:23.000
Apparently, the idea is a lot older.

27:23.000 --> 27:24.000
I didn't realize that.

27:24.000 --> 27:31.000
But in a nutshell, rather than having one gigando neural net where when you feed in something at the bottom,

27:31.000 --> 27:36.000
you have to do Pachinko through the whole network to get your answer out the other side.

27:36.000 --> 27:41.000
You instead fragment it into what's called a mixture of experts.

27:41.000 --> 27:43.000
So bunches of little neural networks.

27:43.000 --> 27:46.000
And then you have something at the bottom that does the routing.

27:46.000 --> 27:51.000
So when little piece of information comes in like language token or whatever,

27:51.000 --> 27:52.000
it gets routed to the right expert.

27:52.000 --> 27:57.000
Sometimes these systems routed to a couple experts and let them kind of do it out.

27:57.000 --> 28:02.000
But the point is you can have a gigantic system that's very sparsely activated.

28:02.000 --> 28:06.000
So it's way cheaper to train way cheaper to do inference on.

28:06.000 --> 28:10.000
And you can do it with distributed architecture very naturally.

28:10.000 --> 28:15.000
Whereas if you have your whole neural network that needs to be in memory, maybe even on a chip, you know,

28:15.000 --> 28:18.000
there's a hard limit to how big you can make these things.

28:18.000 --> 28:26.000
The size, the parameter size of these models has been outstripping Moore's law by a healthy, healthy helping over the past few years.

28:26.000 --> 28:32.000
And that's got people worried, you know, money, power consumption, climate change impact.

28:32.000 --> 28:37.000
Only a few giant companies being able to even operate with these things.

28:37.000 --> 28:42.000
So the switch transformer has been a really exciting development this year.

28:42.000 --> 28:49.000
And two models, one from DeepMind, one from Google, came out just recently.

28:49.000 --> 28:51.000
Gofer and Glam.

28:51.000 --> 29:00.000
Bamsman around the longer that implemented and also there's a really amazing result from Facebook AI.

29:00.000 --> 29:04.000
They go by Facebook AI or meta AI now.

29:04.000 --> 29:07.000
I guess it's meta AI then.

29:07.000 --> 29:18.000
Where they had a multi-lingual translation model that beat all of the bilingual models for the first time on the big WMT annual machine translation contest.

29:18.000 --> 29:23.000
So it used this mixture of experts architecture.

29:23.000 --> 29:25.000
So ran a lot more efficiently.

29:25.000 --> 29:29.000
So that idea seems to be taken off.

29:29.000 --> 29:33.000
And I can expect that this year, it'll just keep going.

29:33.000 --> 29:41.000
And maybe eventually we'll say goodbye to single neural network, you know, systems.

29:41.000 --> 29:47.000
Is there anything special required at the infrastructure hardware?

29:47.000 --> 29:48.000
For sure.

29:48.000 --> 29:49.000
For sure.

29:49.000 --> 29:50.000
And that will come.

29:50.000 --> 29:55.000
We'll get, you know, the pie torch for distributed, you know, mixture of expert transformers.

29:55.000 --> 29:56.000
Indie guy.

29:56.000 --> 29:57.000
I'm sure.

29:57.000 --> 29:58.000
Yeah.

29:58.000 --> 29:59.000
But yeah, for now it's pretty bespoke.

29:59.000 --> 30:03.000
I think you need a real team of experts to implement this.

30:03.000 --> 30:04.000
Got it.

30:04.000 --> 30:08.000
But can't be long before you know, tools get good.

30:08.000 --> 30:12.000
Well, it's, I think it's the same process that we've been talking about.

30:12.000 --> 30:22.000
We've, you know, there's an innovation and the switch transformers kind of standardized and demonstrated to be effective across or at least from several different groups.

30:22.000 --> 30:32.000
And, you know, now the pie towards community and others can, you know, take it and kind of push it down into their level and take advantage of it.

30:32.000 --> 30:38.000
What are you seeing around kind of building.

30:38.000 --> 30:43.000
Operational pipelines around transformers or transformers and production.

30:43.000 --> 30:45.000
That kind of thing.

30:45.000 --> 30:50.000
You know, this relates somewhat to the.

30:50.000 --> 30:56.000
The broader problem or your challenges around like fairness and.

30:56.000 --> 31:04.000
And predictability of language models and, and ethics that kind of thing.

31:04.000 --> 31:12.000
And thinking about the, the guardrails that an organization needs to put in place around large language models.

31:12.000 --> 31:14.000
If they're productizing them.

31:14.000 --> 31:16.000
But also the.

31:16.000 --> 31:21.000
Kind of ML ops processes that are required to operation wise them.

31:21.000 --> 31:26.000
Are they, are they diverging from any other kind of model.

31:26.000 --> 31:33.000
Or are they, you know, you're seeing the same practices applied to transformers.

31:33.000 --> 31:36.000
This, this touches on a pretty rich vein.

31:36.000 --> 31:39.000
We could take this from a lot of different angles.

31:39.000 --> 31:43.000
There's the whole issue of.

31:43.000 --> 31:46.000
ML ethics and responsible.

31:46.000 --> 31:47.000
ML.

31:47.000 --> 31:51.000
And that, that's been a sea change over this past year.

31:51.000 --> 31:56.000
And that really is starting to have really practical effects on, as you say, ML ops.

31:56.000 --> 31:59.000
Like how do you actually operationalize this stuff.

31:59.000 --> 32:09.000
And protecting against harm, increasing the, you know, the quality of data by accounting for things like bias and toxic language.

32:09.000 --> 32:11.000
So all of that is happening.

32:11.000 --> 32:21.000
There's also a kind of more boring side of this, which is just how do you make these systems cheaper to, you know, cost assert drive down cost to serve.

32:21.000 --> 32:24.000
Make it more data efficient to train these models.

32:24.000 --> 32:29.000
So there's a couple of bunch of new tricks that are emerging on that side, too.

32:29.000 --> 32:33.000
So there's like the side of spicy spicy side and the mild side, which, which, which do you want to.

32:33.000 --> 32:37.000
Yeah, let's start with the, the mild side.

32:37.000 --> 32:39.000
And we'll get to the spicy side.

32:39.000 --> 32:40.000
Sounds good.

32:40.000 --> 32:41.000
All right.

32:41.000 --> 32:43.000
So for the mild appetizer.

32:43.000 --> 32:47.000
One thing that's emerging is.

32:47.000 --> 32:54.000
So there's a lot of different kinds of clever tricks for increasing data efficiency.

32:54.000 --> 33:02.000
So when I say data efficiency, I mean like how many training examples did you need to get your model to some target performance.

33:02.000 --> 33:06.000
So, you know, you have some target in mind, how long did it take you to get there.

33:06.000 --> 33:10.000
And the whole trend has been that that number is getting lower and lower and lower.

33:10.000 --> 33:15.000
And the most exciting thing over the past year has been a few shot learning.

33:15.000 --> 33:23.000
When GPT two and GPT three came out, people started using the phrase few shot learning and zero shot learning.

33:23.000 --> 33:25.000
Aroniously.

33:25.000 --> 33:31.000
So it's like really confusing because you know, when you when you do prompt engineering and you and you write.

33:31.000 --> 33:34.000
There's no learning happening here.

33:34.000 --> 33:37.000
You've just changed the format of the input.

33:37.000 --> 33:41.000
But true few shot learning has started to actually emerge.

33:41.000 --> 33:50.000
So all kinds of really clever tricks for driving down that cost in terms of data to get some performance measurement.

33:50.000 --> 33:53.000
Here's one of the like really cute tricks.

33:53.000 --> 33:55.000
So when you want to make a class fire.

33:55.000 --> 33:59.000
So the document comes in and you tell me whether it belongs in bucket a B or C.

33:59.000 --> 34:00.000
Yeah.

34:00.000 --> 34:03.000
You know, this is like NLP task 101.

34:03.000 --> 34:07.000
Usually what we do old school is you give it a whole bunch of label data.

34:07.000 --> 34:12.000
And the first thing the model has to do during training is figure out what the task even is.

34:12.000 --> 34:14.000
You don't give it instructions.

34:14.000 --> 34:16.000
You just punish it every time it gives a wrong answer.

34:16.000 --> 34:21.000
And so the whole beginning of the of the training is just figuring out what the task is.

34:21.000 --> 34:23.000
And that's not efficient.

34:23.000 --> 34:28.000
You know, that's like administering a test to students where you don't even tell them what's going on.

34:28.000 --> 34:32.000
You know, and they just have to kind of figure out what it is first.

34:32.000 --> 34:37.000
And so one of the cute tricks I've seen emerge is just make it multiple choice.

34:37.000 --> 34:40.000
Just literally spell it out right there in the input data.

34:40.000 --> 34:42.000
What the possible choices are.

34:42.000 --> 34:44.000
It's just like such a nice simple trick.

34:44.000 --> 34:47.000
And that's what I mean when it comes to incremental science.

34:47.000 --> 34:52.000
You know, it's like bunches and bunches of little artisanal advanced, you know,

34:52.000 --> 34:58.000
improvements to how we do things that squeezing out efficiency and performance.

34:58.000 --> 35:08.000
And that is that an example, and as your broader point related to you've already got a trained large language model.

35:08.000 --> 35:13.000
And you're trying to figure out the, you know, basically how to get it to do what you want.

35:13.000 --> 35:14.000
Yep.

35:14.000 --> 35:19.000
It's just old fashioned now old fashioned fine tuning of a language model, the same old same old.

35:19.000 --> 35:24.000
Another neat trick that's emerged is the pipelines themselves.

35:24.000 --> 35:27.000
So this is something we've been grappling with it primer.

35:27.000 --> 35:34.000
You know, if you're just a some person at home wanting to do a little batch of data using some hugging face model.

35:34.000 --> 35:36.000
You don't worry too much about the cost.

35:36.000 --> 35:37.000
It's just sort of a quick one off.

35:37.000 --> 35:38.000
It's no big deal.

35:38.000 --> 35:46.000
But if you need to do millions of documents with let's say dozens of different machine learning models for a customer.

35:46.000 --> 35:49.000
And you're on the hook for the cost to serve.

35:49.000 --> 35:50.000
It really matters.

35:50.000 --> 35:53.000
You look at your monthly AWS bill and you're shocked.

35:53.000 --> 36:00.000
So, you know, there's a lot of motivation out there to try and figure out how do we drive down the cost to serve of these systems once we scale it.

36:00.000 --> 36:04.000
And so one of the things that you can do.

36:04.000 --> 36:08.000
And this is borrowed so true of of NLP these days.

36:08.000 --> 36:12.000
Take an old concept from like computer vision and just figure out how to reapply it.

36:12.000 --> 36:14.000
So it's the idea of model cascades.

36:14.000 --> 36:18.000
That's what they call it over in computer vision.

36:18.000 --> 36:23.000
You have them just have your big, great model doing all the work. You have a cascade of models.

36:23.000 --> 36:27.000
Maybe sort of like not that good, better, better, best.

36:27.000 --> 36:31.000
And you basically do what I call inference triage.

36:31.000 --> 36:38.000
So what we found and we're about to publish a paper on this is you can with some really expensive classification tasks.

36:38.000 --> 36:54.000
You can have a old school scikit learn CPU driven no GPU's needed model to 99% of the work and all it has to learn how to do is take the ambiguous tricky cases and pass those on to the big model.

36:54.000 --> 37:02.000
Yeah, it's stuff like that just like really simple tricks. That's that's what engineering evolution looks like just an accumulation of tricks.

37:02.000 --> 37:08.000
Yeah, I mean, it, you know, I guess all the best tricks sound obvious once you say them, right?

37:08.000 --> 37:12.000
Yeah, although I got to say transformers. That's clever.

37:12.000 --> 37:18.000
It's still such a cool idea. It's to me, it's never going to feel obvious. That's just such a cool idea.

37:18.000 --> 37:22.000
Language models, even like cool idea. So that's the mild side.

37:22.000 --> 37:32.000
Let's talk about the evolution of the way we're thinking about ethics and responsibility in the context of NLP. What are you seeing there?

37:32.000 --> 37:38.000
Well, I'll start by making a prediction for 2022 since we're supposed to be doing some of those, right? We're going to do some of those. Yeah, absolutely.

37:38.000 --> 37:50.000
Alright, I'm going to throw one in. I don't think that Amazon Alexa or Apple Siri or Google Assistant are going to get dramatically better this year, even though they could.

37:50.000 --> 38:00.000
And I think it's because they are, these companies are rightly cautious about the language models that would power that.

38:00.000 --> 38:10.000
It just doesn't feel safe enough yet. It just takes one really, really egregious, you know, public relations disaster to nuke the whole effort.

38:10.000 --> 38:14.000
And I think they're going to hold back until that stuff is all ironed out.

38:14.000 --> 38:24.000
So I don't think you're going to, even though like you're going to have hugging face models and maybe little startup, you know, efforts that feel like they're miles ahead.

38:24.000 --> 38:32.000
And I think that in some regards with human interaction, whether it's speech or text or just making sense of things.

38:32.000 --> 38:39.000
I think that the big companies are going to lay low a lot longer. So that's my prediction. And rightly so.

38:39.000 --> 38:54.000
So on the spicy side, in retrospect, this no one should have been surprised by any of this. Like, what are these models? They're just big mathematical objects trained on data or that data come from us.

38:54.000 --> 39:02.000
It's our written text on the internet mostly. Where did that come from? Well, a very biased point of view.

39:02.000 --> 39:10.000
Very, very biased point of view. English speaking almost entirely Western industrialized male dominated.

39:10.000 --> 39:21.000
And, you know, it comes from a human history that is only now starting to, you know, on the time scale of human history, kind of yesterday woke up to equality.

39:21.000 --> 39:41.000
And so it shouldn't have surprised anyone that if you put a language model behind a free form interface where you can just like talk to it or make it say stuff that you can very easily make it say things that are just like politically unacceptable and sometimes ethically shocking.

39:41.000 --> 39:52.000
And yet the past year, I mean, really it started the year before, but it continued this year. People just keep on stepping in that landmine.

39:52.000 --> 40:04.000
The one that I noticed this year was the Allen Institute for artificial intelligence made this cute little cute little experiment called Delphi.

40:04.000 --> 40:13.000
And man, that blew up in their face. And, you know, I don't think it was a mistake to do the science. It was a cool question.

40:13.000 --> 40:22.000
Could you, if you made a big data set of ethical statements, you know, things that are right and wrong. Could you generalize at all?

40:22.000 --> 40:29.000
Could you get a model to like take statements as input and say whether it's like sounds ethically right or wrong?

40:29.000 --> 40:40.000
Interesting idea. It's like basically a toy experiment. And the one mistake they made was they put it online in a way that allowed anyone to say anything to this model.

40:40.000 --> 40:47.000
And of course, if you're motivated, you want to make the model say something racist, it'll take you like 15 seconds.

40:47.000 --> 40:59.000
And then they, you know, this is the part I don't like, then they absolutely mob bullied the, you know, these poor scientists who had good intentions. Come on, like we weren't out to harm anyone.

40:59.000 --> 41:09.000
And so that's the trend we've been seeing is just sort of people making these self-inflicted mistakes using language models and, you know, like Twitter beating up on them.

41:09.000 --> 41:23.000
And I mean, there was a time when we talked, when it came to machine learning and ethics, we talked about systems that were deployed in the world, like things that determined parole, things that determined whether you got hired or not, that we're having impacts today.

41:23.000 --> 41:32.000
And people were truly being harmed and like those, you know, facial recognition systems being used, you know, to, you know, like hassle people that clearly had racial bias.

41:32.000 --> 41:42.000
Suddenly that whole conversation moved to, can you make a language model say something racist. And like, so what's come of that some very good things.

41:42.000 --> 41:51.000
Complete shift, I think there are still people that are there are there are, but like it has made a lot of heat noise. And I think some good things are finally coming out of this.

41:51.000 --> 42:05.000
One is that you, you can't not talk about the ethics of your work anymore, like papers, you know, routinely will have an ethics section. It's just expected now.

42:05.000 --> 42:15.000
When you dream up some project, you're not going to be able to avoid the conversation. Hopefully before you even get started of like, well, what what are the possible harms.

42:15.000 --> 42:25.000
That's a sea change that, you know, just a couple of years ago that that just wasn't the case. And I think that's the biggest positive impact of all this is that we're talking about it.

42:25.000 --> 42:43.000
And then the second one is people are starting to get practical about it. Deep mind I saw put out a blog post describing practical ways that they try and reduce these kind of biases in data and also ameliorate like the impact on the other side.

42:43.000 --> 42:53.000
So we're just starting to get really practical one of the really cool results from the recent glam and go for papers. I can remember which one, but one of the two of them.

42:53.000 --> 43:00.000
They claimed that they got a lot of bang for buck by actually practically addressing the problem of data quality.

43:00.000 --> 43:14.000
So they had all kinds of clever filters on huge gobs of internet data to try and get rid of some of the worst stuff like, you know, internet chat and toxic language, you know, they pre filter that out.

43:14.000 --> 43:26.000
And they got better performance on the task that they cared about practical. So it's not just it's not just a bunch of heat noise. It's it's actually productive.

43:26.000 --> 43:35.000
I don't think I saw the deep mind paper that you referred to have to get that link from you.

43:35.000 --> 43:50.000
I think you're you're touching on the medium spicy that that I was referring to earlier and the example that comes to mind for me was this.

43:50.000 --> 43:57.000
It was a tweet from a while back if this was an October and a October.

43:57.000 --> 44:08.000
Someone did a search on Google about, you know, what should you do when someone's having a seizure and you know how it has the feature snippets.

44:08.000 --> 44:23.000
The feature snippets basically gave us some bullet points, which if you get went to the text of the page that it was referring to were the things not to do.

44:23.000 --> 44:51.000
You know, we're starting to see more kind of model monitoring and governance tools. But I wanted to get your take, you know, for an organization that's putting these models out into the wild, like what are the tools and processes that you're using to try to constrain their behavior.

44:51.000 --> 44:55.000
Let's just don't put them, don't put them up.

44:55.000 --> 45:14.000
Another is, you know, we see, you know, we've seen with Google like they'll put like, they'll filter like you can't, you know, search this, you can't use some set of search turns because it knows the model breaks under those searches.

45:14.000 --> 45:34.000
All, you know, create lists of filter terms, you know, that's one thing. I'm curious what, you know, what are the other things that you're seeing folks doing in the kind of the full spectrum of, you know, sophistication, you know, or is it just kind of a, you know, an exclusion list of front kind of.

45:34.000 --> 45:53.000
I can tell you as an outsider to this because, you know, where I am, primer, primer is working with big, a small number of very big customers. And so the relationship we have when we deploy models, we can talk directly to the very small, manageable number of people who actually need it.

45:53.000 --> 46:10.000
So there's a trusting relationship there that's very dynamic, we can, you know, fix problems as we occur them. So in a certain sense, we can be a little less, I guess you could say just less cautious, but you don't need to be as cautious.

46:10.000 --> 46:16.000
When you unleash something onto the internet and anyone at scale is using it, you got to be really cautious.

46:16.000 --> 46:17.000
So it's a different game.

46:17.000 --> 46:23.000
I've observed looking at, looking at how people deal with that.

46:23.000 --> 46:29.000
I think we're still stepping in the do-do phase, honestly.

46:29.000 --> 46:32.000
I mean, I just see it happening over and over again.

46:32.000 --> 46:35.000
You know, things going horribly wrong.

46:35.000 --> 46:40.000
And I guess that's probably because it still pays off.

46:40.000 --> 46:47.000
The game out there is to be the first to do something, and there's a huge payoff for doing that.

46:47.000 --> 46:56.000
And so people are still in this era of just cowboy it, just like make something and put it out there and see, you know, what sticks.

46:56.000 --> 47:01.000
And then when it blows up too bad for you, but it was still worth the risk.

47:01.000 --> 47:03.000
And I think that's going to change.

47:03.000 --> 47:04.000
I think that's going to change.

47:04.000 --> 47:11.000
For one thing, like the number of new things seems to be decreasing over time.

47:11.000 --> 47:17.000
Like, I don't think people are that impressed by a chatbot on Twitter anymore.

47:17.000 --> 47:20.000
You know, like, okay, yeah, we've seen that.

47:20.000 --> 47:23.000
So I think we're going to be moving into like practical stuff.

47:23.000 --> 47:25.000
Like, can you solve problems?

47:25.000 --> 47:29.000
And the nature of solving practical narrow problems.

47:29.000 --> 47:36.000
I've noticed is that it forces you to be more cautious just for good engineering.

47:36.000 --> 47:44.000
You know, so we're going to see less and less of interfaces where there's just a raw language model willing to say anything.

47:44.000 --> 47:49.000
Because that it's just there aren't that many problems that require that it's going to get more and more narrow.

47:49.000 --> 47:58.000
And I think that, you know, the things that are left where it's still dangerous.

47:58.000 --> 48:01.000
Yeah, we're going to have to figure out how to do it.

48:01.000 --> 48:06.000
And to your point, like, I think we are still essentially filtering.

48:06.000 --> 48:16.000
I think that is the best tool we have right now is to just make big exclude lists of terms and keep on adding to it.

48:16.000 --> 48:17.000
I've done that myself.

48:17.000 --> 48:29.000
I, you know, not not for toxicity, but we were building a system once and it wasn't dealing very well with certain famous people that, you know, just pollute your data.

48:29.000 --> 48:32.000
And so we would just exclude them because they weren't relevant.

48:32.000 --> 48:36.000
And so we're just like clean up clean up results. That way it's an old school trick.

48:36.000 --> 48:38.000
And I think we're still doing that.

48:38.000 --> 48:40.000
And I think that'll continue.

48:40.000 --> 48:49.000
Ultimately, though, well, a lot of people think that the solution is to train better language models, make them more polite and, you know, helpful and cautious from the get go.

48:49.000 --> 48:52.000
Other people feel like that's too hard.

48:52.000 --> 48:54.000
Just train them on the raw world as it is.

48:54.000 --> 48:58.000
And then do something downstream to clean up their output.

48:58.000 --> 49:00.000
I don't think that's been settled.

49:00.000 --> 49:02.000
You mentioned chat bots.

49:02.000 --> 49:06.000
Have you seen anything?

49:06.000 --> 49:10.000
Nope, no matter what you're going to say, the answer is nope.

49:10.000 --> 49:13.000
I don't think chat thoughts have improved at all.

49:13.000 --> 49:21.000
I mean, you would think they would inherit some some improvement due to transformers and that whole thing.

49:21.000 --> 49:26.000
But they are more fluent in terms of the, you know, they're not, it's not word salad.

49:26.000 --> 49:30.000
But they still can't keep track of what you're talking about.

49:30.000 --> 49:33.000
They're still not, they still haven't achieved utility.

49:33.000 --> 49:36.000
I don't think so.

49:36.000 --> 49:41.000
I've one thing I loved this year was AI dungeon.

49:41.000 --> 49:45.000
Have you played with this game or at least heard about it?

49:45.000 --> 49:47.000
I don't remember.

49:47.000 --> 49:48.000
Do you remember?

49:48.000 --> 49:49.000
Do you remember Zork?

49:49.000 --> 49:50.000
Yep.

49:50.000 --> 49:54.000
So it's just a text adventure game like Zork, but it's powered by like GPT two and three.

49:54.000 --> 49:55.000
I think I remember this.

49:55.000 --> 50:01.000
So it's just, you know, it's a text adventure that you get to basically be the player and dungeon master simultaneously.

50:01.000 --> 50:09.000
And that what I think that's a glimpse at what the really cool hello world of tomorrow's chat box is going to be.

50:09.000 --> 50:15.000
I think that's that that kind of like you're not trying to solve a problem that, you know, you're stressed about and monies on the line.

50:15.000 --> 50:18.000
It's going to start with games, something fun.

50:18.000 --> 50:20.000
And where it's a little more freeform and can be cooking.

50:20.000 --> 50:21.000
It's okay.

50:21.000 --> 50:28.000
And I think that's where the first really sophisticated chat bots are probably going to get used.

50:28.000 --> 50:32.000
I don't think it's going to be customer service. Everyone thinks it's going to be chat bots.

50:32.000 --> 50:38.000
You know, helping you with questions, you know, that you have of a company or your phone company or something like that.

50:38.000 --> 50:46.000
You can solve those kind of problems with much simpler systems because you don't need just to have a conversation with those bots is really just a fancy menu.

50:46.000 --> 50:59.000
But with games, you really do want to have a conversation. And so something that really bugs me is like they haven't figured out co reference resolution, which is if I'm talking about something.

50:59.000 --> 51:02.000
And then I say, what do you think about that?

51:02.000 --> 51:07.000
That is a reference to something that we've talked about earlier, you have it in memory, you know what it is.

51:07.000 --> 51:11.000
These these models really struggle with co reference resolution.

51:11.000 --> 51:17.000
And so someone's got to crack that code. Someone's got to figure out how to make a chat bot that can keep track of things.

51:17.000 --> 51:26.000
One of the really cool papers that came out this year that was a really truly innovative idea is something called scratch pads.

51:26.000 --> 51:27.000
It was a Google paper.

51:27.000 --> 51:32.000
And the idea is you let the model literally take notes as it processes stuff.

51:32.000 --> 51:41.000
So you can imagine, you know, they did it for mathematics. Their paper was all about having this model work on a mathematical problem and it could take notes.

51:41.000 --> 51:52.000
But the sparks flying in my head were like, oh, it's not going to stop there. Imagine you have a model that has to deal with a book length document or an hour long conversation.

51:52.000 --> 52:02.000
We can't fit all that stuff into a standard transformer attention window. It's just got this little keyhole that it looks at the world and it forgets everything as soon as you move that window.

52:02.000 --> 52:12.000
And so, you know, this might be a way forward to sort of give it a space where I can keep notes of the most important things and keep referring to it as it goes.

52:12.000 --> 52:32.000
And thinking about trying some version of this for longer documents. That's the something I care about in the chatbot world. Maybe it'll be something like that where you literally have a model that's keeping track of the most important information, erasing it and changing it as it goes so that it can remember what that is when you say that.

52:32.000 --> 52:43.000
I don't know. Had some interesting conversations on the general topic of fusing various types of memory with, you know, modern deep learning models.

52:43.000 --> 52:56.000
And that seems to be one of the, well, hey, there's a lot of activity and B seems to be high potential. And, you know, if we get that right, we open up a whole other dimension of performance.

52:56.000 --> 53:05.000
Yeah, absolutely. Because right now we live in the world of the attention window. It's, it's only, it's, it's only about 500 words.

53:05.000 --> 53:12.000
It's really small. You know, imagine you had amnesia where you forgot, you know, everything you read before 500 words ago.

53:12.000 --> 53:14.000
Pretty tough.

53:14.000 --> 53:19.000
I think I've seen that movie and the solution of writing things on your hands.

53:19.000 --> 53:26.000
That's scratch that we reinvent a memento.

53:26.000 --> 53:31.000
There's going to be a paper called memento. Actually, there are a strong prediction.

53:31.000 --> 53:34.000
That's a strong prediction.

53:34.000 --> 53:37.000
I feel like there is one though.

53:37.000 --> 53:42.000
I'm going to have to look that up. It can be rebranded. I think I can be appropriate for sure.

53:42.000 --> 53:50.000
You briefly mentioned multi-lingual results this year.

53:50.000 --> 54:05.000
Yeah, that's something that the community has been kind of advocating for or elements in the community have been kind of advocating for for a long time kind of pushing NLP beyond English or English dominant.

54:05.000 --> 54:10.000
What were the highlights in that in that domain?

54:10.000 --> 54:21.000
Well, so this year, Chinese has just completely blown up. Chinese NLP is just amazingly amazingly rich now.

54:21.000 --> 54:33.000
Multiple models, language models, bigger than GPT-3, whole new dialogue type models that is like Chinese based.

54:33.000 --> 54:43.000
So there's just no doubt like the language that's going to be on par with English if it isn't already is Chinese.

54:43.000 --> 54:45.000
It's huge investment.

54:45.000 --> 54:48.000
And then a bunch of other languages are starting to play catch up.

54:48.000 --> 54:56.000
I noticed that a Korean version of glue came out and a GPT-3 Korean model called hyperclova.

54:56.000 --> 55:07.000
A whole bunch of other languages are also playing catch up mostly by utilizing English models and then kind of doing tricks to make the multi-lingual.

55:07.000 --> 55:20.000
The most exciting thing in multi-lingual NLP was this result from Facebook meta where a multi-lingual model beat all the bilinguals. That's a paradigm change.

55:20.000 --> 55:31.000
But I think actually the thing that's going to have the bigger impact long term potentially is this other thing also from meta called XLSR.

55:31.000 --> 55:34.000
So this is really new and intriguing.

55:34.000 --> 55:37.000
The idea here is forget about the written text.

55:37.000 --> 55:41.000
Let's just do language and audio the way humans learn it and use it.

55:41.000 --> 55:52.000
And besides opening up an unbelievably vast potential data source, you know, most human speech is actually audio.

55:52.000 --> 55:57.000
We NLP has just been grounded in text because we can easily deal with it with the computers of today.

55:57.000 --> 56:12.000
But meaning we have this is it's this is an interesting problem to kind of formulate. So if you, you know, certainly the volume in terms of bits of audio data is going to be greater than written data.

56:12.000 --> 56:14.000
But that's not relevant.

56:14.000 --> 56:20.000
Are you saying that we have more recorded words and audio formats than written formats?

56:20.000 --> 56:34.000
It's slightly weaker claim, which is that every human alive today who utters language in any form is mostly doing it with their with their throat, not not their typing, you know, fingers.

56:34.000 --> 56:49.000
And I'm not sure that's that's an interesting and different question is if we had to do this today, how much data do we actually have counted by words, let's say, you know, written versus recorded.

56:49.000 --> 56:53.000
Probably written.

56:53.000 --> 56:58.000
If I had to guess, just the libraries, you know, accumulated.

56:58.000 --> 57:09.000
But with all the listening devices we now have in our pocket, in our home, in our meetings, that could be eclipsed very, very quickly.

57:09.000 --> 57:14.000
Yeah, but do we even know how to build models that are kind of channel characteristic, agnostic.

57:14.000 --> 57:27.000
But that's that's the exciting promise here is like maybe, but I think that the well, the thing that excites me about it is this unlocks low resource languages in a way that we never could with written text.

57:27.000 --> 57:35.000
You just don't have enough written text from all of the African languages and all of the Aboriginal languages of South America.

57:35.000 --> 57:43.000
And even a bunch of European languages, I hear Icelandic has this problem that just isn't enough written text to train a language model the way we do today.

57:43.000 --> 57:46.000
But you could easily get a whole bunch of recording.

57:46.000 --> 57:51.000
And so that's pretty exciting that that might be longer term, the bigger deal.

57:51.000 --> 57:54.000
We'll see very cool.

57:54.000 --> 58:01.000
The multilingual model that you referred to.

58:01.000 --> 58:03.000
I think it was meta.

58:03.000 --> 58:08.000
Yeah, they did English to many and many to English was the paradigm.

58:08.000 --> 58:16.000
So yeah, and by the way, they used this nice, efficient, new distributed mixture of experts architecture to pull it off.

58:16.000 --> 58:19.000
But yeah, it was English to many and many to English.

58:19.000 --> 58:21.000
So they still English at the heart of this effort.

58:21.000 --> 58:25.000
You know, it's like the, it's the node connecting all the other languages data wise.

58:25.000 --> 58:27.000
Got it, got it.

58:27.000 --> 58:46.000
So we've covered NLP eating the world, we've covered kind of this transition to incremental progress and driving efficiency multilingual benchmarks ethics.

58:46.000 --> 58:57.000
And you've got this section here and some bad things that did not happen.

58:57.000 --> 59:00.000
People are always talking about, you know, the things that did happen.

59:00.000 --> 59:04.000
I think it's worth talking about the things that didn't happen, even though I expected them to.

59:04.000 --> 59:13.000
So one of those was, man, there was so much concern about GPT three generated text flooding the internet.

59:13.000 --> 59:16.000
I'd let a whole team working on this problem for a whole year.

59:16.000 --> 59:19.000
Yeah, that concern about GPT two also.

59:19.000 --> 59:23.000
The whole thing is too powerful to share with anybody.

59:23.000 --> 59:29.000
And I've been looking real hard and I just don't see it happening yet.

59:29.000 --> 59:31.000
So we don't, we don't Twitter.

59:31.000 --> 59:34.000
Could that be because it's so good that you can't detect it?

59:34.000 --> 59:36.000
Of course, of course.

59:36.000 --> 59:45.000
I don't think so, though, because I can tell you as someone who's done this before, the first thing that'll happen is someone will announce a prank.

59:45.000 --> 59:48.000
And I haven't even seen a large scale prank yet.

59:48.000 --> 59:57.000
I haven't seen anyone maintaining a well, you know, regarded or widely read blog that turns out to be all GPT text.

59:57.000 --> 01:00:03.000
I haven't seen a Twitter account that's had a big account, you know, a big impact on the world that turns out to be all generated.

01:00:03.000 --> 01:00:06.000
And often, it's just like a big whimper.

01:00:06.000 --> 01:00:12.000
And I think having used these models myself to solve problems with text generation.

01:00:12.000 --> 01:00:15.000
The reason I think is that they're just not that easy yet.

01:00:15.000 --> 01:00:24.000
It's not the case that you can command an army to generate, you know, things that are actually going to achieve some goal.

01:00:24.000 --> 01:00:29.000
It's still too weird, too much hallucination, too much fuss.

01:00:29.000 --> 01:00:39.000
But I do think that troll farms, state sponsor troll farms, and they continue to toil away, trust me.

01:00:39.000 --> 01:00:47.000
If they aren't already, they will soon be using, you know, GPT style models to accelerate the human effort.

01:00:47.000 --> 01:01:00.000
And so, you know, there are going to be efforts. I'm part of, you know, I'm part of one of surely many such efforts to try and make detection systems to combat these.

01:01:00.000 --> 01:01:03.000
But that will be a problem, but it didn't seem to happen this year.

01:01:03.000 --> 01:01:11.000
I just, I didn't see any evidence that any bad actors were using these text generation models at scale.

01:01:11.000 --> 01:01:13.000
So that's nice.

01:01:13.000 --> 01:01:18.000
The other thing that didn't happen was reinforcement learning didn't take over NLP.

01:01:18.000 --> 01:01:25.000
A lot of people thought it was inevitable because RL is just so amazingly impactful in other fields.

01:01:25.000 --> 01:01:27.000
Just didn't make any inroads.

01:01:27.000 --> 01:01:39.000
There was one exception that I noticed, which was this open AI paper, which was using reinforcement learning to shape a summarization model to human preferences.

01:01:39.000 --> 01:01:44.000
I mean, the results weren't like mind blowing, but worked.

01:01:44.000 --> 01:01:49.000
But yeah, hasn't has an RL is not eating ML.

01:01:49.000 --> 01:01:52.000
That's not happening yet.

01:01:52.000 --> 01:01:58.000
The other big surprise, of course, is that we still don't know how language models work.

01:01:58.000 --> 01:02:08.000
Still big mystery. We're still in this era of, you know, treating these things like cells that you have to do experiments on to figure out how they what they can do and how they do it.

01:02:08.000 --> 01:02:17.000
You know, one of the things that we've, we've talked about language models, of course, throughout this conversation.

01:02:17.000 --> 01:02:20.000
And we've mentioned open AI specifically.

01:02:20.000 --> 01:02:29.000
We haven't specifically talked about GPT three and all the things that have happened just in that world.

01:02:29.000 --> 01:02:37.000
Do you think maybe a place to start is do you think it's worthy of talking about do you think GPT three?

01:02:37.000 --> 01:02:51.000
We'll have an outsized impact relative to the innovation that is large language models or is it just, you know, a hosted example of which, you know, there are many.

01:02:51.000 --> 01:02:55.000
That is the million dollar question.

01:02:55.000 --> 01:03:09.000
It is crazy how, how big the gap is between the excitement about these large other aggressive language models like GPT three and the posity of applications.

01:03:09.000 --> 01:03:12.000
Unbelievable divide there.

01:03:12.000 --> 01:03:27.000
Do you say the posity of applications, do you mean the, you know, in fact, actual, you know, in the wild applications or the posity of use cases for which they could be kind of reliably applied.

01:03:27.000 --> 01:03:29.000
Well, I think they're related.

01:03:29.000 --> 01:03:32.000
You put your finger right on the problem.

01:03:32.000 --> 01:03:43.000
I think of GPT three and it's many cousins as wild horses, it's clearly an awesome horse, really powerful.

01:03:43.000 --> 01:03:48.000
But good luck writing that thing to get like to town.

01:03:48.000 --> 01:03:55.000
They're just very hard at this point to control to do the things you need.

01:03:55.000 --> 01:04:04.000
We have some tasks and trust me, we were trying like others to use them to try and make progress on the problems you know we care about.

01:04:04.000 --> 01:04:20.000
You usually better off with a smaller, simpler model, just training data is more important than parameter size for the narrow practical applied ML problems of today.

01:04:20.000 --> 01:04:28.200
But one way we tried to use, we haven't been using GPT-3, but GPT Neo, the 16 billion

01:04:28.200 --> 01:04:36.440
per amber one, or a 6 billion, it's called GPT-J6B, so that's the biggest that was open

01:04:36.440 --> 01:04:38.400
source from Eleuther.

01:04:38.400 --> 01:04:43.000
We've been trying to use it for data augmentation, so we were like, okay, well, if you can't put

01:04:43.000 --> 01:04:49.440
this model literally into production to do a task, maybe you can use it as a tool to train

01:04:49.440 --> 01:04:51.760
a simpler model more efficiently.

01:04:51.760 --> 01:04:57.360
So we've been trying to find all kinds of ways to get it to, you know, teach another model

01:04:57.360 --> 01:04:59.320
to do cool things.

01:04:59.320 --> 01:05:03.440
You can't even get it to do that yet, we have to admit, we just have not cracked the code

01:05:03.440 --> 01:05:04.440
on that.

01:05:04.440 --> 01:05:09.040
What you can do with it is all kinds of fun stunts.

01:05:09.040 --> 01:05:16.800
So I mean, it's just irresistibly fun to engineer a prompt to do some brand new trick

01:05:16.800 --> 01:05:23.720
all in the space of 10, 20 minutes, you know, where else in ML can you, can you do that?

01:05:23.720 --> 01:05:30.320
I've used it to generate new neologisms, urban dictionary style neologisms where you give

01:05:30.320 --> 01:05:35.280
it a description of a word you wish existed and it just spits out a weird word, fun.

01:05:35.280 --> 01:05:41.920
I've used it to generate movie plots or movie names given a plot.

01:05:41.920 --> 01:05:49.240
I just made a prompt from stuff I copy pasted from Amazon, prime, super easy.

01:05:49.240 --> 01:05:53.800
And I've recently used it to generate what I call deep talk questions.

01:05:53.800 --> 01:05:56.600
So this is a real practical use case.

01:05:56.600 --> 01:06:01.800
So COVID came and we were all like a bunch of people who used to work in an office and

01:06:01.800 --> 01:06:08.560
breathe the same air and feel connected and, you know, it's, it's very hard to stay connected

01:06:08.560 --> 01:06:13.040
when you just, you know, everyone's a little tiny face on a grid.

01:06:13.040 --> 01:06:18.480
And so I have this Monday ritual with my team now that we start with deep questions.

01:06:18.480 --> 01:06:25.480
We used to use a, a list that was floating around the internet of a bunch of deep questions,

01:06:25.480 --> 01:06:31.880
you know, stuff like what is something you regret from your childhood?

01:06:31.880 --> 01:06:36.680
What is, what is, you know, what, who's the most influential person in your life?

01:06:36.680 --> 01:06:40.480
If you could go back in time and change one thing, what would it be, you know, these kind

01:06:40.480 --> 01:06:43.720
of questions, they elicit real conversation.

01:06:43.720 --> 01:06:47.960
And we blew through that list because we were doing it every week and we would randomly

01:06:47.960 --> 01:06:49.880
choose three and then vote on one.

01:06:49.880 --> 01:06:52.480
So we got exposed to all the questions pretty fast.

01:06:52.480 --> 01:06:58.920
So what I did was I just made a prompt of actual deep questions and used GPT Neo to just

01:06:58.920 --> 01:07:02.000
generate tons of these things and I put it on the internet.

01:07:02.000 --> 01:07:03.280
It's like taking a life of its own.

01:07:03.280 --> 01:07:04.800
It's on the internet now.

01:07:04.800 --> 01:07:10.240
Like, yeah, you can use them for certain things, but no one's figured out how to tame

01:07:10.240 --> 01:07:11.240
that horse.

01:07:11.240 --> 01:07:18.720
Do you follow and have you played with the various kind of incremental things that OpenAI has

01:07:18.720 --> 01:07:24.040
done with the API, like the instruct engine?

01:07:24.040 --> 01:07:27.360
I think technically it's not 2021.

01:07:27.360 --> 01:07:33.480
It was actually a year ago from the recording of this conversation that I haven't, but

01:07:33.480 --> 01:07:41.440
only because I'm so busy using GPT Neo and trying to figure out how to make it useful.

01:07:41.440 --> 01:07:46.960
So I have access to GPT 3 have for a long time, but I didn't find myself using it because

01:07:46.960 --> 01:07:52.440
every trick that GPT 3 could do, I could do myself with an open source model.

01:07:52.440 --> 01:07:56.680
But for all I know that they've improved it by leaps and bounds, I just wouldn't know.

01:07:56.680 --> 01:08:08.240
Yeah, the instruct model allows you to kind of bypass a lot of the prompt creation and

01:08:08.240 --> 01:08:13.160
just say in natural language what you want the model to create.

01:08:13.160 --> 01:08:21.360
That's kind of interesting and they've got fine tuning and specific ways to use it for

01:08:21.360 --> 01:08:25.760
classification and question answering and other things.

01:08:25.760 --> 01:08:32.760
The idea of being to make it easier and more accessible and to try to reduce the need

01:08:32.760 --> 01:08:36.680
to be a prompt engineering ninja that's something done.

01:08:36.680 --> 01:08:39.880
I'll give it another try.

01:08:39.880 --> 01:08:43.920
Yeah, I mean, OpenAI is doing such wonderful things.

01:08:43.920 --> 01:08:48.120
I wouldn't be surprised if it made some really cool advances on that front too.

01:08:48.120 --> 01:08:54.200
One thing that we have been using that I think should go in the list of most successful

01:08:54.200 --> 01:09:05.880
applied ML, NLP, most successful examples of actual day-to-day useful NLP is co-pilot.

01:09:05.880 --> 01:09:12.840
I know it's gotten a lot of complaints, especially that copyright, but people on my team actually

01:09:12.840 --> 01:09:15.120
use it.

01:09:15.120 --> 01:09:20.360
If you know what you're doing and you understand its limitations, it's pretty wonderful.

01:09:20.360 --> 01:09:26.480
It takes a lot of dreary boilerplate code writing out of your life.

01:09:26.480 --> 01:09:31.640
You switch to a mode of taking a template that's generated by the model and making sure

01:09:31.640 --> 01:09:34.000
it's correct in a lot of cases.

01:09:34.000 --> 01:09:39.800
That is like 10 times faster than writing that boilerplate yourself.

01:09:39.800 --> 01:09:40.800
How are they using it?

01:09:40.800 --> 01:09:43.320
They're in VS code.

01:09:43.320 --> 01:09:44.320
In VS code?

01:09:44.320 --> 01:09:45.320
Yep.

01:09:45.320 --> 01:09:50.240
I haven't done it myself, but it's top of my to-do list now to try it.

01:09:50.240 --> 01:09:54.560
But I only discovered this past week that they were using it.

01:09:54.560 --> 01:09:57.840
I was like, oh, wow, so it's gotten to the point where it's actually useful.

01:09:57.840 --> 01:09:59.560
I'm going to check it out.

01:09:59.560 --> 01:10:03.640
Yeah, I haven't checked in a while, but the thing that I was most looking for was like

01:10:03.640 --> 01:10:06.640
some kind of Jupyter Notebook plug-in.

01:10:06.640 --> 01:10:10.680
I think it's through VS code because you can imagine you have to make these API calls.

01:10:10.680 --> 01:10:11.680
Yeah.

01:10:11.680 --> 01:10:15.160
There's a lot of guts that have to be set up for that.

01:10:15.160 --> 01:10:16.160
Yeah.

01:10:16.160 --> 01:10:17.160
Interesting.

01:10:17.160 --> 01:10:18.160
Interesting.

01:10:18.160 --> 01:10:27.040
And that's one that I can relate to quite a bit, just that I'm not a professional engineer,

01:10:27.040 --> 01:10:29.800
data scientist, whatever day to day, I forget.

01:10:29.800 --> 01:10:34.120
What's the specific format for doing such and such in Python or pandas or whatever?

01:10:34.120 --> 01:10:37.960
And you do what anyone does, which is you go to Stack Overflow, you find some version of

01:10:37.960 --> 01:10:41.920
the problem, and then you have to copy paste and then change it because that's actually

01:10:41.920 --> 01:10:45.320
not quite the problem I was solving.

01:10:45.320 --> 01:10:49.920
No pilot is just doing exactly that, but as a neural network, it's already got all of

01:10:49.920 --> 01:10:50.920
GitHub.

01:10:50.920 --> 01:10:51.920
Exactly.

01:10:51.920 --> 01:10:56.520
And so it just knows how to spit out boilerplate.

01:10:56.520 --> 01:10:57.520
Yeah.

01:10:57.520 --> 01:11:00.360
So I'm definitely going to have to go look at that plug-in.

01:11:00.360 --> 01:11:02.160
I had not played with it yet.

01:11:02.160 --> 01:11:06.840
If anyone knows about a way to do that inside of Jupyter, let me know.

01:11:06.840 --> 01:11:10.360
That would be a really cool little gadget.

01:11:10.360 --> 01:11:14.520
If someone in the open source Jupyter community would make that, that would be really cool.

01:11:14.520 --> 01:11:15.520
Yeah.

01:11:15.520 --> 01:11:16.520
Yeah.

01:11:16.520 --> 01:11:17.520
Absolutely.

01:11:17.520 --> 01:11:18.520
Absolutely.

01:11:18.520 --> 01:11:24.520
So also on your list of fun things that did happen in 2021, my little pony, G-P-T.

01:11:24.520 --> 01:11:25.520
What is that?

01:11:25.520 --> 01:11:27.520
I think I missed that one.

01:11:27.520 --> 01:11:28.520
Yeah.

01:11:28.520 --> 01:11:35.760
So I think, like, of the things that happened this year, that I noticed and remembered were

01:11:35.760 --> 01:11:38.160
the delightful things.

01:11:38.160 --> 01:11:42.200
And so one of the delightful things was someone, of course, someone did this.

01:11:42.200 --> 01:11:46.800
They basically trained a language model on my little pony text so that, you know, it speaks

01:11:46.800 --> 01:11:50.960
in the language of this kid's toy universe.

01:11:50.960 --> 01:11:56.320
And I think we're just going to see a lot more of that playfulness before people start

01:11:56.320 --> 01:12:02.760
caching in and figuring out how to make industrial scale text generation, which is coming.

01:12:02.760 --> 01:12:07.760
But I think it's going to be the artists and pranksters who are going to lead the way.

01:12:07.760 --> 01:12:13.560
And you see that already with Clip and these things, it's play, play is leading the way,

01:12:13.560 --> 01:12:14.560
which is cool.

01:12:14.560 --> 01:12:15.560
That's great.

01:12:15.560 --> 01:12:16.560
That's great.

01:12:16.560 --> 01:12:21.560
I think we've talked about a lot of your predictions already.

01:12:21.560 --> 01:12:26.400
Are there ones that we've not mentioned yet?

01:12:26.400 --> 01:12:27.400
Let me see.

01:12:27.400 --> 01:12:28.400
I mean, myself a little list.

01:12:28.400 --> 01:12:33.080
One thing we didn't mention is that I think that we're going to have the emergence of AI

01:12:33.080 --> 01:12:35.160
first gaming companies.

01:12:35.160 --> 01:12:42.400
So AI dungeon, I think, can claim the mantle of being the first that I know of.

01:12:42.400 --> 01:12:45.840
But there's going to be many AI first gaming companies and they're going to be taking

01:12:45.840 --> 01:12:52.480
advantage of this concept of an infinite game, you know, where NPCs are actual agents that

01:12:52.480 --> 01:12:59.120
evolve and you can talk to and the world itself is evolving in response to your decisions,

01:12:59.120 --> 01:13:02.640
not just through procedural generation, but more sophisticated things.

01:13:02.640 --> 01:13:05.240
So that's going to be really fun.

01:13:05.240 --> 01:13:11.760
While you're describing this, I'm looking for this interesting company that a friend

01:13:11.760 --> 01:13:19.600
just told me about transforms.ai, have you heard of that?

01:13:19.600 --> 01:13:26.320
They're kind of, you know, 30,000 foot bolting language models onto ARVR as kind of a

01:13:26.320 --> 01:13:28.320
metaverse gaming play.

01:13:28.320 --> 01:13:29.320
Interesting stuff.

01:13:29.320 --> 01:13:30.320
Cool.

01:13:30.320 --> 01:13:34.920
Well, it looks like a lot of this stuff is already arriving then.

01:13:34.920 --> 01:13:35.920
Great.

01:13:35.920 --> 01:13:39.440
Yeah, that's going to be fun.

01:13:39.440 --> 01:13:46.200
The other thing is there are a bunch of very narrow application AI startups in the NLP

01:13:46.200 --> 01:13:50.760
space that are only going to get better like Grammarly.

01:13:50.760 --> 01:13:55.680
I think if it's expense, I think expense is kind of an NLP startup, even though it's

01:13:55.680 --> 01:14:00.200
a mixture of computer vision and NLP AR kind of, yeah, but they're just so good.

01:14:00.200 --> 01:14:06.480
I use them all the time and it's like a beautiful example of narrow, but really nailed it applications.

01:14:06.480 --> 01:14:11.400
And I think all of those very narrow startups are going to get better at what they're doing,

01:14:11.400 --> 01:14:16.600
but they're going to start to expand inevitably, like you're only just a walk away from this

01:14:16.600 --> 01:14:19.720
neighborhood and you can kind of support that workflow.

01:14:19.720 --> 01:14:27.200
And I think they're going to get acquired real fast and those that remain are going to

01:14:27.200 --> 01:14:28.200
be contenders.

01:14:28.200 --> 01:14:33.600
It'll take more than a year, but the ones that don't get acquired and stick around and

01:14:33.600 --> 01:14:37.680
keep expanding are going to be new fan companies eventually, and that'll be cool to see.

01:14:37.680 --> 01:14:40.880
Let's find out.

01:14:40.880 --> 01:14:46.360
Let's see, the fan companies are going to be very, very cautious about anything involving

01:14:46.360 --> 01:14:52.560
a language model because of PR and rightly so.

01:14:52.560 --> 01:14:56.680
But I do think they're going to be putting more and more AI muscle behind their existing

01:14:56.680 --> 01:14:57.680
features.

01:14:57.680 --> 01:14:58.680
It'll be invisible progress.

01:14:58.680 --> 01:15:04.200
You won't realize it, but more and more of the stuff you take for granted will be taking

01:15:04.200 --> 01:15:08.080
advantage of AI.

01:15:08.080 --> 01:15:15.560
And then I guess the other thing that I haven't mentioned is we're not going to stop at language

01:15:15.560 --> 01:15:19.000
to image, you know, clip and glide and dolly.

01:15:19.000 --> 01:15:20.680
That's just the first shot across the bow.

01:15:20.680 --> 01:15:26.720
I think all the rich multimodality will start to get eaten up.

01:15:26.720 --> 01:15:31.680
I just can't wait for movement to be part of it.

01:15:31.680 --> 01:15:36.720
So, you know, like, first of all, of course, you'll have videos where you can say what

01:15:36.720 --> 01:15:41.680
you want, you know, a boy walking down the street, you know, right now you get an image

01:15:41.680 --> 01:15:43.600
of a boy walking down the street.

01:15:43.600 --> 01:15:47.800
Next step, of course, is to get a little video clip, little animation of a boy walking

01:15:47.800 --> 01:15:49.280
down the street.

01:15:49.280 --> 01:15:53.560
But where it gets interesting is where it gets really rich, where you could just write

01:15:53.560 --> 01:15:59.600
a little scene, a little movie plot, and it'll just make that coherently.

01:15:59.600 --> 01:16:01.360
I can't wait for that.

01:16:01.360 --> 01:16:07.800
And then also some really weird stuff, like a mechanical hand, you know, the data coming

01:16:07.800 --> 01:16:12.840
in and out of this, you know, electronic gadget is just data.

01:16:12.840 --> 01:16:18.640
And so you can imagine, you know, like essentially doing the trick that computer vision has been

01:16:18.640 --> 01:16:25.720
using with, you know, text as a kind of data, a brand new data paradigm with robotics.

01:16:25.720 --> 01:16:33.200
I don't see why NLP won't start to eat robotics, where you, you know, rather than just having

01:16:33.200 --> 01:16:37.160
kinematics where you have to like do all the vector math to figure out how to pick up

01:16:37.160 --> 01:16:43.000
something, you know, or use reinforcement learning to somehow train it to do this.

01:16:43.000 --> 01:16:45.840
I think language will somehow come into the mix sooner or later.

01:16:45.840 --> 01:16:49.440
I don't know if that'll be this year, but it feels inevitable.

01:16:49.440 --> 01:16:55.240
Language is just so good for encapsulating information at the highest level about the

01:16:55.240 --> 01:16:56.920
goals that we care about.

01:16:56.920 --> 01:17:07.480
The thought that that prompted me was, do you see or anticipate a role for artificial

01:17:07.480 --> 01:17:08.480
language?

01:17:08.480 --> 01:17:17.360
Well, like, you know, well, is what's happening in NLP going to enable like lay people DSLs

01:17:17.360 --> 01:17:21.440
that allow us to accomplish tasks better?

01:17:21.440 --> 01:17:30.880
It's kind of a half-form thought, but do you mean like you could say, do my taxes and

01:17:30.880 --> 01:17:33.520
you don't have to be more technical than that?

01:17:33.520 --> 01:17:36.240
Yeah, I'm not sure what I mean.

01:17:36.240 --> 01:17:38.960
I thought you were going someplace even weirder.

01:17:38.960 --> 01:17:44.600
You know, maybe one thing in the back of my head is like really, really early on in the,

01:17:44.600 --> 01:17:51.480
it's probably like five or six years ago, like, there was some crazy article about, hey,

01:17:51.480 --> 01:17:55.200
Facebook had these two chatbots talking about, and they invented their own language.

01:17:55.200 --> 01:17:56.920
Yeah, that's what I thought you were talking about.

01:17:56.920 --> 01:17:57.920
For transacting.

01:17:57.920 --> 01:17:58.920
Yeah.

01:17:58.920 --> 01:18:06.800
You know, I've always kind of remembered that plus, you know, what a cool concept in engineering

01:18:06.800 --> 01:18:13.560
and computer science, you know, to me has always been like the DSL like, you know, you can

01:18:13.560 --> 01:18:19.280
write code to do something, you can configure an engine to do something, or you can create

01:18:19.280 --> 01:18:26.680
a language that allows someone to do the task at a higher level of abstraction than code

01:18:26.680 --> 01:18:33.720
and a more kind of meaningful and fluid and visualizable and understandable way than configuration.

01:18:33.720 --> 01:18:39.960
And so, and I find myself like, you know, just everyday productivity, trying to kind of

01:18:39.960 --> 01:18:46.560
put together, you know, using tools like Alfred on the Mac, like, you know, how can I put

01:18:46.560 --> 01:18:53.240
together a bunch of like three letter things that will do some task for me.

01:18:53.240 --> 01:18:56.640
You're talking about compression.

01:18:56.640 --> 01:19:03.160
But I think there's, you're hinting at something even cooler, which is making thoughts possible

01:19:03.160 --> 01:19:04.880
that aren't yet possible.

01:19:04.880 --> 01:19:09.960
So, you know, this is all theory and linguistics, yeah, that, you know, like thought is truly

01:19:09.960 --> 01:19:10.960
composed of language.

01:19:10.960 --> 01:19:13.760
And if you don't have the language for it, you truly can't think it.

01:19:13.760 --> 01:19:19.640
I don't know if that's true or not, but I do know that practically, it's very difficult

01:19:19.640 --> 01:19:26.480
to do the mental gymnastics for certain things without the vocabulary and the kind of,

01:19:26.480 --> 01:19:30.680
you know, linguistic structures that support that.

01:19:30.680 --> 01:19:34.320
And I think what it comes down to, if you're talking about machine learning is, do you

01:19:34.320 --> 01:19:36.120
need a human in the loop?

01:19:36.120 --> 01:19:40.800
So, you know, the example of two machines talking to each other, evolving their own language.

01:19:40.800 --> 01:19:49.800
I mean, imagine you took the data set of a whole bunch of audio and a whole bunch of good

01:19:49.800 --> 01:19:50.800
transcription of that audio.

01:19:50.800 --> 01:19:53.400
So, you've got speech and you've got text.

01:19:53.400 --> 01:19:59.240
I've been thinking about this quirky, pointless experiment where you could then take that

01:19:59.240 --> 01:20:01.960
system that's been trained on that.

01:20:01.960 --> 01:20:03.760
And then, in fact, let's just do English.

01:20:03.760 --> 01:20:07.640
You've got a ton of English recording and the transcription.

01:20:07.640 --> 01:20:09.960
You've got a model that's good at that.

01:20:09.960 --> 01:20:13.520
And now you give it a language that's never heard.

01:20:13.520 --> 01:20:18.520
It'll try and write it in English with English phonemes, you know, and it'll make a sort

01:20:18.520 --> 01:20:25.440
of, like, an anglicized version, I imagine, of that language, right?

01:20:25.440 --> 01:20:27.800
So, you know, why stop there?

01:20:27.800 --> 01:20:34.680
Imagine, like, going from, let's say, mathematical descriptions to formulas, you know, like English

01:20:34.680 --> 01:20:41.160
to math, or maybe from between any two domains, and you just, you basically have a grounding

01:20:41.160 --> 01:20:47.640
vocabulary, and you have a system discover a brand new language for some new domain.

01:20:47.640 --> 01:20:48.640
What's the utility of that?

01:20:48.640 --> 01:20:52.200
I couldn't tell you, but I think it would be hilarious.

01:20:52.200 --> 01:20:56.520
The thing that it made me think of, you may have seen this video a few years ago.

01:20:56.520 --> 01:21:00.160
It's like, what English sounds like to non-English speakers on YouTube?

01:21:00.160 --> 01:21:01.160
Yes.

01:21:01.160 --> 01:21:02.160
You remember that?

01:21:02.160 --> 01:21:03.160
Yes.

01:21:03.160 --> 01:21:04.160
Exactly.

01:21:04.160 --> 01:21:07.560
So, you know, do you need a human in a loop?

01:21:07.560 --> 01:21:13.360
Not necessarily, but I think we should care about that more than, you know, anything else.

01:21:13.360 --> 01:21:19.160
So, you know, how can we, I had a really simple, simple idea I implemented once.

01:21:19.160 --> 01:21:20.840
This is not ML.

01:21:20.840 --> 01:21:23.040
This is old-school NLP.

01:21:23.040 --> 01:21:30.120
I made this algorithm that detected jargon, and specifically abbreviated jargon, which

01:21:30.120 --> 01:21:33.120
the scientific papers I was reading was just so full of, and I was trying to learn new

01:21:33.120 --> 01:21:34.120
fields.

01:21:34.120 --> 01:21:36.920
You get all these big acronyms, and you have no idea.

01:21:36.920 --> 01:21:42.080
So I just wrote an algorithm that would find all those things and find their expanded forms.

01:21:42.080 --> 01:21:45.000
Usually, regular expressions, it was nothing fancy.

01:21:45.000 --> 01:21:51.320
And it occurred to me that you could use the opposite of this to invent new jargon that

01:21:51.320 --> 01:21:56.320
text seemed to need, because phrases seemed to be just used over and over again, so you

01:21:56.320 --> 01:22:02.560
could just invent an algorithm, invent an acronym, you know, an abbreviation that you didn't

01:22:02.560 --> 01:22:07.720
realize you needed, and you could, you know, even invent, if you want to get machine learning

01:22:07.720 --> 01:22:09.400
involved, you could define it.

01:22:09.400 --> 01:22:12.040
You could have something that will generate definitions of these things and just make

01:22:12.040 --> 01:22:14.880
a glossary of invented jargon.

01:22:14.880 --> 01:22:15.880
Yeah.

01:22:15.880 --> 01:22:16.880
Yeah.

01:22:16.880 --> 01:22:20.960
Well, I think the last few minutes of this conversation kind of illustrates the delight

01:22:20.960 --> 01:22:26.360
aspect of NLP, and then to some degree machine learning.

01:22:26.360 --> 01:22:27.360
Absolutely.

01:22:27.360 --> 01:22:29.960
NLP is finally fun.

01:22:29.960 --> 01:22:34.200
I'd say it was not fun in 2017, and before, it was hard.

01:22:34.200 --> 01:22:35.200
Yeah.

01:22:35.200 --> 01:22:37.360
Like, now it's just fun.

01:22:37.360 --> 01:22:38.360
That's awesome.

01:22:38.360 --> 01:22:45.080
Well, that sounds like a great way to finish up this year's AI Rewind.

01:22:45.080 --> 01:22:51.040
John is so great reconnecting and talking through what you've seen this year and what

01:22:51.040 --> 01:22:53.320
you expect to see looking for.

01:22:53.320 --> 01:22:56.840
Happy Holidays, and looking forward to 2022.

01:22:56.840 --> 01:22:57.840
Absolutely.

01:22:57.840 --> 01:22:58.840
Happy Holidays.

01:22:58.840 --> 01:23:15.720
Thank you.

