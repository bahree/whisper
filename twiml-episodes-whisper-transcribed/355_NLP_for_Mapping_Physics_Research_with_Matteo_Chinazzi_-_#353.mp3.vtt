WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:21.920
I'm your host Sam Charington.

00:21.920 --> 00:25.160
Alright everyone, I am on the line with Mateo Kinazi.

00:25.160 --> 00:29.680
Mateo is an associate research scientist at Northeastern University.

00:29.680 --> 00:32.760
Mateo, welcome to the Twimal AI Podcast.

00:32.760 --> 00:34.280
Hi Sam, thank you for having me.

00:34.280 --> 00:36.600
It's great to be part of this show.

00:36.600 --> 00:37.600
Thank you.

00:37.600 --> 00:43.200
I'm looking forward to jumping into our conversation about some work that you've done recently

00:43.200 --> 00:48.480
to apply machine learning in particular word embeddings and word-to-vec to the physics

00:48.480 --> 00:49.480
research space.

00:49.480 --> 00:53.480
In fact, we've covered similar applications.

00:53.480 --> 01:00.280
First recently, the Twimal AI Podcast number 291 took a look at some applications of machine

01:00.280 --> 01:07.080
learning to identify new materials by looking at materials research publications.

01:07.080 --> 01:10.720
But very interested in learning a little bit more about your application.

01:10.720 --> 01:15.920
But before we do that, tell us a little bit about your background and how you came to work

01:15.920 --> 01:19.600
on kind of physics and machine learning.

01:19.600 --> 01:25.400
And you're also doing some work in computational epidemiology as well, if I understand correctly.

01:25.400 --> 01:26.920
Yeah, correct.

01:26.920 --> 01:31.840
So actually, my background is a bit strange in a sense because my former background is

01:31.840 --> 01:33.400
actually in economics.

01:33.400 --> 01:38.560
So I hold a PhD in economics from Santana University in Italy.

01:38.560 --> 01:42.280
But actually, during my career path, I've always worked, let's say, the intersection

01:42.280 --> 01:46.520
between I would say economics, physics, and computer science in a sense because I'm

01:46.520 --> 01:50.520
always being involved in this kind of field of research, which is called complex systems

01:50.520 --> 01:54.120
and applied network science, typically.

01:54.120 --> 01:59.200
And so what I've been working lately actually are essentially two main kind of broad area

01:59.200 --> 02:00.200
of research.

02:00.200 --> 02:04.880
One is, as you mentioned, now computational epidemiology, which means this idea of actually

02:04.880 --> 02:10.320
building these large-scale simulators that can allow us to kind of model the spread of

02:10.320 --> 02:12.400
a disease at the global scale.

02:12.400 --> 02:17.000
So you can imagine, essentially, what we're doing at our lab is kind of have this essentially

02:17.000 --> 02:22.080
complex piece of software that is able to kind of simulate in a realistic manner the evolution

02:22.080 --> 02:24.200
of an epidemic at the global scale.

02:24.200 --> 02:29.800
So imagine, let's say, I've been Zika starting a given point in Brazil and then spreading

02:29.800 --> 02:33.760
it on the world or having, for example, to try to model, let's say, the patterns of

02:33.760 --> 02:37.080
the seasonal flu, or the pandemic flu, let's say, in the United States.

02:37.080 --> 02:39.720
So this is the kind of research we are doing.

02:39.720 --> 02:44.200
And on the other hand, the other, let's say, focus of research was actually to work on

02:44.200 --> 02:46.840
a discipline, which is called science of science.

02:46.840 --> 02:51.840
And the idea there is essentially to kind of look under a microscope of how science works,

02:51.840 --> 02:56.600
meaning, for example, how it evolves over time, how collaborations occurs between different

02:56.600 --> 03:01.360
scientists and between different fields, how scientists speak their research problems,

03:01.360 --> 03:07.200
how they, for example, move across different institutions, how nations develop expertise

03:07.200 --> 03:09.360
in different fields of research and so on.

03:09.360 --> 03:13.200
So those are kind of my main two topics of research at the moment.

03:13.200 --> 03:17.120
How did you get started, in particular, on that science of science?

03:17.120 --> 03:22.040
What led you to start exploring that area in your research?

03:22.040 --> 03:27.120
Well, actually, that kind of comes in a sense from my economic background, because the

03:27.120 --> 03:31.280
reason why I was attracted to economics in the first place was this idea of actually

03:31.280 --> 03:37.080
trying to understand, as human behavior and all the interactions occurs between individuals.

03:37.080 --> 03:41.800
And so translating that to science of science, the idea is that, okay, you have all these

03:41.800 --> 03:46.640
many people working on producing new innovative research, you are trying to understand how

03:46.640 --> 03:51.640
you can, for example, accelerate science and, let's say, redirect funding, so you can

03:51.640 --> 03:55.400
get, let's say, the most out of it, and you can maybe, let's say, promote the specific

03:55.400 --> 04:00.400
set of expertise in specific institutions, or pressing the word, so how will you do that?

04:00.400 --> 04:05.000
And one way, which I approached the problem was actually to first try to understand how

04:05.000 --> 04:08.960
you can actually map a research space in a given field.

04:08.960 --> 04:12.120
In our case, we started actually looking at physics as a work.

04:12.120 --> 04:16.880
Let's say, baseline, if you wish, because I mean, the people with whom I'm working most

04:16.880 --> 04:20.120
of the time are actually people training physics or computer science, so that was kind

04:20.120 --> 04:22.720
of the natural way to go in a sense.

04:22.720 --> 04:25.520
Well, let's dig into this research.

04:25.520 --> 04:34.360
So you start with the goal of mapping the physics community via the research.

04:34.360 --> 04:41.520
Did you define it more specifically than that, or did you set out broadly to see what

04:41.520 --> 04:45.440
you could see by applying machine learning to the space?

04:45.440 --> 04:49.920
No, actually, we are using kind of a process definition given that this kind of, due to

04:49.920 --> 04:52.000
the kind of data we are using.

04:52.000 --> 04:56.800
So in our case, we are using publications from the American Physical Society.

04:56.800 --> 05:00.840
So essentially, we have a data set with all the publications, all the papers that were

05:00.840 --> 05:05.640
published in the journals of the American Physical Society, and we use that as our corpus

05:05.640 --> 05:07.160
in a sense.

05:07.160 --> 05:13.160
And the reason why we started looking at this is because there is, let's say, a very specific

05:13.160 --> 05:18.480
classification that allows you to kind of look at each paper and know if that paper, which

05:18.480 --> 05:22.400
kind of topics are covered in that specific paper, given that they have this classification

05:22.400 --> 05:27.000
that we different essentially cause that tells you, okay, this paper is about nuclear physics

05:27.000 --> 05:32.680
and specifically this subtopic of nuclear physics versus astrophysics or something different.

05:32.680 --> 05:37.680
And so the fact that we were able to kind of map simultaneously publications, authors,

05:37.680 --> 05:42.640
locations through the affiliations of the authors and topics, such topics, that's kind

05:42.640 --> 05:47.320
of what we needed to be able to generate and then infer this knowledge and research space

05:47.320 --> 05:48.640
in physics.

05:48.640 --> 05:57.640
So the technique that you applied is word-to-vec specifically or word embeddings more generally?

05:57.640 --> 06:02.040
The word embeddings, in a sense, that the model that we're actually using is star space,

06:02.040 --> 06:06.880
which was developed by Facebook Research AI Group.

06:06.880 --> 06:11.440
And but is one of, let's say, of the many, in a sense, variations of the word-to-vec approach.

06:11.440 --> 06:16.360
So the idea, in our case, is that we are treating each author as a bag of topics.

06:16.360 --> 06:20.200
So as a bag of research fields in which that author has worked.

06:20.200 --> 06:25.880
And then we use this bag of topics to kind of infer the embeddings for each specific

06:25.880 --> 06:31.200
research sub-area, in a sense, and create this mapping of the overall nor space by essentially

06:31.200 --> 06:36.320
looking at the expertise of authors to guide us on what it means to be, for two topics

06:36.320 --> 06:37.800
to be similar to each other.

06:37.800 --> 06:39.240
So that's the general idea.

06:39.240 --> 06:44.920
So for example, if we, let's say if we go back to the word-to-vec idea there, I mean,

06:44.920 --> 06:50.040
the main assumption is to use this distribution hypothesis idea, then the fact that essentially

06:50.040 --> 06:54.480
you can infer the meaning of a word by looking at the context of that word.

06:54.480 --> 06:58.080
And in our case, let's say the analogy in our case is that we're saying, okay, we actually

06:58.080 --> 07:00.840
observe the publication record of scientists.

07:00.840 --> 07:05.960
So we know what each scientist is able to actually produce in terms of research.

07:05.960 --> 07:11.120
And so we use that as our, let's say, way to infer what is the context of a specific

07:11.120 --> 07:12.120
sub-topic.

07:12.120 --> 07:15.760
So we're saying, okay, given that this author has published, for example, in nuclear physics,

07:15.760 --> 07:20.320
we can look at what other topics that author has published on, and we use that as a context

07:20.320 --> 07:24.600
to kind of predict the embedding for the former, for example.

07:24.600 --> 07:25.600
Okay.

07:25.600 --> 07:30.320
In that sense, does it mean that the approach that you've taken is more of a supervised type

07:30.320 --> 07:36.160
of an approach using the labels that were available in the journals for the different

07:36.160 --> 07:43.360
research than word-to-vec, or would you still consider it unsupervised or semi-supervised

07:43.360 --> 07:45.000
or something like that?

07:45.000 --> 07:49.800
So yeah, actually, I mean, it can be considered if you wish supervised learning in a sense

07:49.800 --> 07:51.720
that we do have the code.

07:51.720 --> 07:56.160
So we do have this Pax code that stands for Physics and Astronomy Classification Scheme,

07:56.160 --> 07:58.160
the data attached to each paper.

07:58.160 --> 07:59.160
Okay.

07:59.160 --> 08:03.120
But actually, the way in which we use them, they are as if they were simple keywords.

08:03.120 --> 08:07.600
In a sense that we are not trying to actually, for example, create an embedding of the author,

08:07.600 --> 08:12.000
given the labels that we have access to the author, we are actually treating each bag of

08:12.000 --> 08:14.960
labels in a sense as a standalone document.

08:14.960 --> 08:18.760
So for example, the idea is that you can, I mean, our idea actually is that you then you

08:18.760 --> 08:25.000
can apply this approach in general to any kind of keyword list that you have associated

08:25.000 --> 08:30.000
to papers, which can be even inferred, for example, semi-automatically from titles or abstracts

08:30.000 --> 08:31.360
of papers.

08:31.360 --> 08:35.880
But the reason why we chose to use this specific data set with this specific taxonomy already

08:35.880 --> 08:40.800
embedding into it was because this will give us a way to actually have a ground truth

08:40.800 --> 08:43.440
baseline to compare our results with.

08:43.440 --> 08:48.240
So the idea is that we know that this taxonomy exists, we know that we have these hierarchical

08:48.240 --> 08:53.280
schemes that tell us that physics is divided, for example, in 10 big sections, and then

08:53.280 --> 08:58.760
each section has many different sub-fields and so on, but we are not using that kind of

08:58.760 --> 09:00.400
information at training time.

09:00.400 --> 09:05.600
So the idea is that once we have our embedding, can we actually see that same, in a sense,

09:05.600 --> 09:07.160
classification scheme exposes?

09:07.160 --> 09:10.320
Can we use that to actually validate what we are going to observe?

09:10.320 --> 09:12.680
And that's why we're using it in that sense.

09:12.680 --> 09:13.680
Got it.

09:13.680 --> 09:19.480
Are you also using the full texts of the research papers or are you only using these keywords

09:19.480 --> 09:21.680
as kind of your document?

09:21.680 --> 09:26.760
No, we are only using these keywords because really our idea was to kind of just, in a sense,

09:26.760 --> 09:31.680
start with a minimal level of information we could get, in a sense, but yes, really

09:31.680 --> 09:35.600
I mean, our goal actually is try to kind of extend it beyond physics, actually, where

09:35.600 --> 09:38.920
we do not have any sense of classification scheme.

09:38.920 --> 09:42.640
And then at that point, yes, we will go towards the direction of using, for example, text

09:42.640 --> 09:47.320
of abstracts or titles to actually infer automatically, in a sense, these keywords.

09:47.320 --> 09:48.320
These keywords.

09:48.320 --> 09:49.320
Okay.

09:49.320 --> 09:54.720
And so with that as the backdrop, what are some of the things that you've been able

09:54.720 --> 10:01.040
to kind of identify within this embedding space that you were able to create?

10:01.040 --> 10:02.040
Yeah.

10:02.040 --> 10:07.920
So what we were able to do essentially was to kind of fingerprint the scientific production

10:07.920 --> 10:12.600
of cities and recover what is, let's say, an effect that is already observed in literature

10:12.600 --> 10:18.840
using different approaches, which is this idea of this so-called principle of relatedness.

10:18.840 --> 10:20.080
Principle of relatedness?

10:20.080 --> 10:21.080
Yes.

10:21.080 --> 10:22.080
That's correct.

10:22.080 --> 10:28.320
And so that's something that comes, let's say, from the economic geography literature

10:28.320 --> 10:30.160
in the first place.

10:30.160 --> 10:33.960
And that's the idea, for example, that if you look, let's say, the production and exports

10:33.960 --> 10:38.160
of the nation, so the country, and you look, let's say, what kind of product category is

10:38.160 --> 10:41.320
the export to their trade partners?

10:41.320 --> 10:46.080
You can kind of predict what kind of, what new categories they are going to be able to

10:46.080 --> 10:51.560
export next by looking at, by first, getting a measure of the relatedness between the

10:51.560 --> 10:53.720
different product categories.

10:53.720 --> 10:57.840
And then by essentially kind of checking where is the overall production, the overall level

10:57.840 --> 11:02.800
of expertise of the country, and then use that to kind of predict what is the next item

11:02.800 --> 11:04.520
you're going to export.

11:04.520 --> 11:09.720
And the idea there is that if you are working in a part of, let's say, this product space

11:09.720 --> 11:14.360
in which we have, let's say, you are able to, you have skills and expertise in different

11:14.360 --> 11:18.280
product categories, and you have a way to measure relatedness within another product category

11:18.280 --> 11:21.120
that will help you kind of predict.

11:21.120 --> 11:23.520
If that's where you're going to go next or not.

11:23.520 --> 11:27.360
And in our case, essentially, this kind of, the analogy in our case is that now at this

11:27.360 --> 11:29.360
point, we're looking at cities.

11:29.360 --> 11:33.800
So we kind of collect all the publications at the geographical level.

11:33.800 --> 11:38.240
And then we use our knowledge space, our research space, build using your buildings to measure

11:38.240 --> 11:43.240
this level of relatedness, and then kind of predict where each city would go next in

11:43.240 --> 11:46.480
terms of their scientific expertise.

11:46.480 --> 11:51.280
So for example, we might look at the publication records in a specific time window, let's say

11:51.280 --> 11:57.440
from 2000 to 2003, they were going to kind of predict using this embedding space, whether

11:57.440 --> 12:02.080
or not the specific city will have a so-called comparative advantage, a revealed comparative

12:02.080 --> 12:08.480
advantage in specific subfielding physics in the next time window, for example.

12:08.480 --> 12:13.360
And presumably, you were able to identify those kinds of trends with embedding space you

12:13.360 --> 12:14.360
created?

12:14.360 --> 12:15.360
Yes, that's correct.

12:15.360 --> 12:22.960
So what are some examples of any examples of cities and research topics come to mind?

12:22.960 --> 12:24.120
Yeah, me.

12:24.120 --> 12:28.560
So what we do essentially is, on one hand, what we do is, for example, look at specific

12:28.560 --> 12:34.480
cities and look at how their activities cluster in this embedded space.

12:34.480 --> 12:38.640
And so, for example, as we show in the paper, we can, let's say, see that cities like

12:38.640 --> 12:45.360
Brassos, for example, they have a clear level of expertise and expert knowledge in nuclear

12:45.360 --> 12:49.360
physics, for example, while you might have, let's say, other cities like Grenoble to

12:49.360 --> 12:52.320
instead specialize more on condensed matter physics.

12:52.320 --> 12:57.720
So this is what you can actually really, really observe by just using this embedding space

12:57.720 --> 13:03.360
and using an effort to kind of represent the interactions between different, between

13:03.360 --> 13:06.800
these different vectors representing the topics.

13:06.800 --> 13:11.360
And then what we show is that we can then use this model to kind of rank the probability

13:11.360 --> 13:13.760
of a given city to enter in a specific field.

13:13.760 --> 13:18.200
And they use that if you want as your classifier that will tell you whether yes or no whether

13:18.200 --> 13:22.040
or not that activity is going to be developed at the next time step.

13:22.040 --> 13:26.160
And for that, we kind of show that let's say, distribution of our predictions is actually

13:26.160 --> 13:31.240
better than a simple random model where, let's say, all these expertise will evolve randomly.

13:31.240 --> 13:39.720
And so is the model primarily good at identifying new research topics that are closely adjacent

13:39.720 --> 13:45.400
to existing research topics as a direction for a given city?

13:45.400 --> 13:51.520
Or is it, yeah, there's some ability to take more leaps if you all.

13:51.520 --> 13:55.720
So that's actually one of the things that are studied in the literature.

13:55.720 --> 14:01.440
And the idea is that when you build this space, you compute a so-called measure of knowledge

14:01.440 --> 14:02.520
density, in a sense.

14:02.520 --> 14:06.880
So imagine that now you have this very, in our case, very complex and dimensional space

14:06.880 --> 14:11.520
where you have different research topics, so different points placed in this and dimensional

14:11.520 --> 14:12.520
space.

14:12.520 --> 14:17.240
And you want to look if there is any specific area of this space where specific entity

14:17.240 --> 14:23.680
in our case, let's say, city has a high density of topics that are active.

14:23.680 --> 14:29.600
So what you show is that actually, that city is more, let's say, likely to be actually

14:29.600 --> 14:32.880
able to develop an expertise around that area.

14:32.880 --> 14:37.360
So in areas in which it actually did the density of points is high.

14:37.360 --> 14:41.720
And like, and that is also what is found in the literature using essentially also different

14:41.720 --> 14:42.720
techniques.

14:42.720 --> 14:44.480
And that is like a pretty robust result.

14:44.480 --> 14:48.600
The only, let's say, exception to that is that, for example, in the case of thing that

14:48.600 --> 14:54.160
was of, I think, economic activities, what they show is that if you are, for example, a

14:54.160 --> 14:58.760
country that is in an intermediate level of development, your more chances of actually

14:58.760 --> 15:01.880
jump to a different part of the space, in a sense.

15:01.880 --> 15:07.560
And there might be ways for you to kind of optimize your trajectory in the space precisely

15:07.560 --> 15:13.960
by kind of knowing what is them up around you and knowing where you have your level of expertise.

15:13.960 --> 15:18.960
You mentioned earlier that you used an algorithm that was developed at Facebook, what was that

15:18.960 --> 15:19.960
algorithm?

15:19.960 --> 15:22.320
The name is star space.

15:22.320 --> 15:23.320
Star space?

15:23.320 --> 15:24.320
Yes.

15:24.320 --> 15:26.320
And can you go into a little bit more detail?

15:26.320 --> 15:29.400
How does it differ from word to vac?

15:29.400 --> 15:30.640
Yeah, sure.

15:30.640 --> 15:34.400
So the main difference that here we actually don't have a neural network.

15:34.400 --> 15:40.680
So the embeddings are not trained as in word to vac as like an even layer in a very simple

15:40.680 --> 15:41.680
neural network.

15:41.680 --> 15:47.880
But in this case, instead, you kind of directly treat the problem as an optimization problem.

15:47.880 --> 15:52.520
So you have these metrics that represent essentially your dictionary.

15:52.520 --> 15:57.520
So in our case, let's say, all the different research topics that we have in our papers.

15:57.520 --> 16:01.680
And then the other dimension will be the actually embedding dimension that you picked.

16:01.680 --> 16:08.080
And then you have a loss function that kind of tries to optimize essentially these embeddings

16:08.080 --> 16:13.200
and optimize these metrics, but essentially allow you to play with essentially what is

16:13.200 --> 16:15.240
to be considered similar to each other.

16:15.240 --> 16:20.880
So essentially, the way the loss function is written is that you have a generator of

16:20.880 --> 16:25.720
positive and negative pairs, which will change depending on the kind of problem that you

16:25.720 --> 16:27.120
are trying to address.

16:27.120 --> 16:32.640
So for example, in our case, let's say we have a list of topics for a researcher.

16:32.640 --> 16:36.640
So the positive pairs will be, for example, the fact that I leave one of these topics

16:36.640 --> 16:38.280
out and that would be my target.

16:38.280 --> 16:40.800
So what I actually want to kind of predict.

16:40.800 --> 16:46.120
And then all the other topics within that pack will actually be something that I can use

16:46.120 --> 16:47.920
to generate these positive pairs.

16:47.920 --> 16:52.600
So these pairs that you have in our case, for example, are very high cosine similarity.

16:52.600 --> 16:57.240
And then on the other side, you have a negative pair generator, which instead kind of throws

16:57.240 --> 17:03.640
randomly other topics, for example, other bag of topics, or you already actually dictionary.

17:03.640 --> 17:07.080
And those are the ones in which you kind of want to stay away from, in a sense.

17:07.080 --> 17:11.560
And that is done in a similar way as it is done, or some implementation work to act using

17:11.560 --> 17:16.200
negative sampling, meaning that you indeed sample randomly what are, let's say, the negative

17:16.200 --> 17:20.720
cases, so that the points in the space from which you want to be far in a sense.

17:20.720 --> 17:25.960
And so all of this essentially is treated as just an optimization problem, which tries

17:25.960 --> 17:28.680
to directly learn these metrics.

17:28.680 --> 17:34.960
So do you think that a neural network-based approach would get you different results

17:34.960 --> 17:35.960
in any significant way?

17:35.960 --> 17:40.480
Is that something that you're interested in or not so much?

17:40.480 --> 17:45.880
Now, actually, I think they will probably give different results in a sense that what

17:45.880 --> 17:52.200
we are not playing with right now is actually this idea of having, in a sense, a time-spend

17:52.200 --> 17:56.480
list of topics, like, let's say, when you use, let's say, word-to-back in a word idea,

17:56.480 --> 18:00.520
is that you have, for example, this kind of rolling window that you define your context,

18:00.520 --> 18:01.520
right?

18:01.520 --> 18:04.880
So you can say, OK, look at two words before or afterwards.

18:04.880 --> 18:08.400
And then depending on whether you're using the continuous bag of words or the skip-gram

18:08.400 --> 18:10.720
approach, one is the target or the other.

18:10.720 --> 18:14.960
But essentially, the idea is that you have this kind of rolling window in your sentence

18:14.960 --> 18:16.640
that speaks up what is the context.

18:16.640 --> 18:21.400
Well, in our case, what we are doing is that we are treating everything as if all the

18:21.400 --> 18:23.600
production of a given scientist is the context.

18:23.600 --> 18:27.480
And there is no idea of, in a sense, temporal distance embedded yet.

18:27.480 --> 18:34.040
So one idea would be to maybe try to apply this algorithm maybe using a word, even just

18:34.040 --> 18:39.320
using a continuous bag of word implementation word-to-back, for example, to adapt this idea

18:39.320 --> 18:41.760
of having an early window.

18:41.760 --> 18:46.400
And the other thing is that, for example, if we're to use word-to-back in this skip-gram

18:46.400 --> 18:49.360
approach, then actually we'll give us something probably maybe that might be different

18:49.360 --> 18:51.280
than what we're observing right now.

18:51.280 --> 18:55.160
So of course, let's say, in the paper, what I show is that actually this method seems

18:55.160 --> 18:58.520
to have better results in the use cases they test.

18:58.520 --> 19:03.840
Then, for example, fast text, word-to-back, and I think also gloves, as far as I remember.

19:03.840 --> 19:06.440
But yes, it's definitely something we are willing to explore.

19:06.440 --> 19:10.680
So our idea is in the action not to try to kind of come up with what would be the best

19:10.680 --> 19:15.520
way of creating these embeddings, giving that we have a very specific problem in mind.

19:15.520 --> 19:19.720
So we are not teaming at creating embeddings that should work for, let's say, whatever

19:19.720 --> 19:24.800
task at hand, but actually the shoework for very specific, to test very specific ideas

19:24.800 --> 19:26.440
in a sense.

19:26.440 --> 19:34.560
Are you incorporating, in any way, some kind of waiting of a given researcher's volume

19:34.560 --> 19:41.080
of work or significance of work in a given area, or is the, their kind of bag of topics

19:41.080 --> 19:44.560
just based on anything that they've been active in?

19:44.560 --> 19:50.800
No, actually, it's based on anything that they've been active in, and that's actually

19:50.800 --> 19:55.080
by design, and that's something we wanted to do like that.

19:55.080 --> 19:59.400
And the reason for that is that, let's say, the other approaches that do not use embeddings

19:59.400 --> 20:03.160
to kind of address the same problem have that kind of filtering.

20:03.160 --> 20:10.000
So have this idea of actually first filtering, for example, what are the topics in which

20:10.000 --> 20:14.920
information or an institution or an entity have what is called a reveal comparative advantage?

20:14.920 --> 20:19.880
So which putting in your words would be something like, okay, identify where that scientist

20:19.880 --> 20:21.200
is most active on.

20:21.200 --> 20:25.680
So remove, for example, all the site papers that have topics that come out maybe of site

20:25.680 --> 20:29.560
projects or feeds the abandoned or something like that.

20:29.560 --> 20:33.280
Well in our case, what I said, no, we actually don't want to embed any of that information

20:33.280 --> 20:34.280
in our embeddings.

20:34.280 --> 20:39.440
So we want to try to see, okay, what if we are applying, we just keep and get a big pile

20:39.440 --> 20:44.160
of papers from an author, we just list all the topics in which has worked on, and train

20:44.160 --> 20:45.160
on that.

20:45.160 --> 20:49.880
So we don't for the filtering without trying to kind of pick the data to get the best

20:49.880 --> 20:50.880
results possible.

20:50.880 --> 20:55.560
Our idea was to show that you don't need to do that to have the same results that you

20:55.560 --> 20:59.360
have with a lot of this manual, let's say, checks and optimizations.

20:59.360 --> 21:05.000
Given that you're focused on a very specific task, what was your performance metric against

21:05.000 --> 21:06.440
that task?

21:06.440 --> 21:11.680
So okay, there's different in a sense checks and validations with it.

21:11.680 --> 21:16.040
So the first validation, which was not in a sense quantified, but was just in a sense

21:16.040 --> 21:21.920
of visual inspection, was what I was saying before, of actually comparing our results from

21:21.920 --> 21:24.760
this box classification scheme.

21:24.760 --> 21:29.360
So the idea is that what we did was that we obtained these embeddings, then we used

21:29.360 --> 21:34.920
them to create actually a research network space.

21:34.920 --> 21:38.600
In the idea, there is that we are borrowing, let's say, techniques and tools from network

21:38.600 --> 21:39.600
science.

21:39.600 --> 21:45.600
And at that point, what we're doing is that we are treating each topic as a node in a network

21:45.600 --> 21:50.960
where the nodes are connected to each other through edges and each hedge as a way has

21:50.960 --> 21:56.680
actually the amount of similarity relatedness to those two topics share, which in our case

21:56.680 --> 22:00.040
would simply be because of similarity of two topics.

22:00.040 --> 22:04.360
Then after building this network, we kind of use what are colleagues spring layouts

22:04.360 --> 22:08.600
in this kind of literature, which essentially the idea, the basic idea is that imagine that

22:08.600 --> 22:13.240
you have these balls into space that are representing your different topics, and you have these

22:13.240 --> 22:15.120
different edges connecting them.

22:15.120 --> 22:20.320
Now, imagine those edges are actually springs that have different force depending on the

22:20.320 --> 22:24.080
level of similarities of those nodes, so those entities.

22:24.080 --> 22:29.520
And so the strong of the similarity, the closer those nodes will be in your visual representation.

22:29.520 --> 22:34.320
And so we use that to build essentially this research network space.

22:34.320 --> 22:38.440
And in a sense, that is our way to kind of make the projection from the embedding space

22:38.440 --> 22:42.000
to a 2D dimensional plot in a sense.

22:42.000 --> 22:46.720
And by doing that, we can kind of clearly identify the 10 macro areas in which we know that

22:46.720 --> 22:52.520
Pax classification divides the different branches of physics.

22:52.520 --> 22:58.240
And that was kind of our first in a sense validation after running the embedding problem,

22:58.240 --> 23:02.520
because we knew that we weren't providing that kind of information to the algorithm.

23:02.520 --> 23:07.400
So we know that that kind of information of the existence of this hierarchical classification

23:07.400 --> 23:10.000
of topics was not used at training time.

23:10.000 --> 23:14.680
So that was our first way to benchmark ourselves and say, okay, look, actually our results

23:14.680 --> 23:16.960
in the look promising.

23:16.960 --> 23:22.680
And the second way in which we tested was actually to indeed measure the predictive power of

23:22.680 --> 23:23.680
the algorithm.

23:23.680 --> 23:29.120
And at that point, what we did was essentially to look at each city at a given point in time.

23:29.120 --> 23:34.520
So in a given time window, list the topics in which that city had a comparative advantage.

23:34.520 --> 23:39.760
So list the topics in which the research topics in which that city was particularly good at

23:39.760 --> 23:42.240
producing new research.

23:42.240 --> 23:46.960
And then try to predict what will happen next as if it was, let's say, a classifier problem

23:46.960 --> 23:51.040
where you have that you have one, see if you're going to develop in that new field and

23:51.040 --> 23:57.720
zero otherwise, and then use a standard metric like rock curve or something like that to

23:57.720 --> 24:02.720
actually kind of get a sense if our model was better than a random model when instead

24:02.720 --> 24:06.160
this evolution of expertise was occurring randomly.

24:06.160 --> 24:10.600
So those are the two ways in which we tested our results.

24:10.600 --> 24:18.520
And did you have any external comparisons available to you in the first of those two cases

24:18.520 --> 24:24.960
you're kind of comparing against common sense, but as you've mentioned previous research

24:24.960 --> 24:30.880
on the economic side, for example, are the results compatible such as you can kind of compare

24:30.880 --> 24:35.360
what you're seeing to what previous papers have shown?

24:35.360 --> 24:36.360
Yes.

24:36.360 --> 24:40.960
So for example, in terms of the kind of, in a sense, so let's say the map of the knowledge

24:40.960 --> 24:45.960
based with finding physics is actually very similar to the maps that are obtained.

24:45.960 --> 24:50.080
For example, by other researchers using different techniques, for example, looking at citation

24:50.080 --> 24:51.320
networks.

24:51.320 --> 24:55.040
So how different fields, subfields cite each other.

24:55.040 --> 24:58.680
And yes, actually, what we have found is that our results are well in line with what

24:58.680 --> 25:03.920
has been found using other methodologies and other approaches and actually even other

25:03.920 --> 25:06.080
data sets as far as I remember.

25:06.080 --> 25:08.520
And so that that was, let's say, confirm in a sense.

25:08.520 --> 25:13.640
So on the physics side, on the, let's say, more, let's say, theoretical side of whether

25:13.640 --> 25:17.560
or not we could like reproduce this principle of relatedness effect, in the dose in that

25:17.560 --> 25:22.640
case, we are getting results that are well in line with what has been found in the literature.

25:22.640 --> 25:23.640
What do you say this going?

25:23.640 --> 25:28.400
What's next in this line of inquiry for you and what do you think it opens up?

25:28.400 --> 25:32.720
Well, first of all, our idea is to actually extend this and this and we were already doing

25:32.720 --> 25:35.720
to other things rather than physics.

25:35.720 --> 25:39.680
So for example, now there is, let's say, a lot of data available to researchers in this

25:39.680 --> 25:40.680
field.

25:40.680 --> 25:45.120
So Google Scholar, Microsoft Academic Graph and other big databases that could allow you

25:45.120 --> 25:50.320
to actually extend this approach in a sense of scale, look at different disciplines.

25:50.320 --> 25:53.600
And then one, and we're already working on that, we already have some preliminary results

25:53.600 --> 25:54.600
on that.

25:54.600 --> 25:58.800
And for example, what we have seen is that by using the same approach in the Microsoft

25:58.800 --> 26:03.600
Academic Graph, which is essentially another of these bi-burematic data sets that kind

26:03.600 --> 26:08.200
of included at that point all the, all the different fields in science.

26:08.200 --> 26:12.280
And for example, using it there, we kind of can replicate fully that it's us we get in

26:12.280 --> 26:15.080
physics to actually all the other fields.

26:15.080 --> 26:22.680
But then actually our goal is to start using this embedding space as our, let's say, additional

26:22.680 --> 26:27.360
tool in our tool kit to actually try to something new.

26:27.360 --> 26:31.680
So for example, you know all these papers that, let's say, for example, crack the evolution

26:31.680 --> 26:34.240
of the semantics of a term over time, for example.

26:34.240 --> 26:38.640
So you can imagine that given that we have this space in which we can embed topics, but potentially

26:38.640 --> 26:43.200
also authors, fields, and so we can kind of try to understand, for example, how authors

26:43.200 --> 26:44.680
move in this space.

26:44.680 --> 26:48.520
Of, for example, can we find the same kind of analogies that we have in the classical,

26:48.520 --> 26:53.960
let's say, examples of King, Queen, man, woman, and so on.

26:53.960 --> 26:56.200
Arigmatics can we do the same things in science, for example.

26:56.200 --> 27:02.000
So can we kind of understand where they might be gaps into the, into science by actually

27:02.000 --> 27:03.360
try to play with these analogies?

27:03.360 --> 27:08.360
So try to see, for example, what happens if you use, or if you apply a specific, sub-fielding

27:08.360 --> 27:14.040
computer science to maybe a field in which that was never considered as kind of technique.

27:14.040 --> 27:18.680
So once that you get a sense of how to navigate this space, then you can try all these kind

27:18.680 --> 27:23.720
of scenarios and thought experiments that maybe can give you more of an intuition of how

27:23.720 --> 27:25.680
science really works underneath.

27:25.680 --> 27:30.080
So what is the actual structure of the knowledge that is kind of keeping everything together?

27:30.080 --> 27:36.640
It strikes me that it's possible that the, you know, the shifts that we see in research

27:36.640 --> 27:42.120
have less to do with changes happening, you know, in, you know, new research in a city,

27:42.120 --> 27:47.920
and more to do with changes in the way we describe our research and these tags that we applied

27:47.920 --> 27:48.920
to them.

27:48.920 --> 27:51.680
And I'm wondering if you've explored that at all.

27:51.680 --> 27:56.280
Knowing a sense of that, I mean, the way I think I understand your point is, is not

27:56.280 --> 28:01.000
a problem for us, so the way in which we define strength in a sense of a city in a specific

28:01.000 --> 28:07.160
topic is by using this concept borrowed from economics of trivial comparative advantage,

28:07.160 --> 28:12.400
which put in a very simple terms is like saying, okay, that your podcast, for example, if

28:12.400 --> 28:17.840
you compare it to other shows, as a comparative advantage in machine learning simply because

28:17.840 --> 28:22.600
they let's say the share of time that you devote to this topic compared to all the other

28:22.600 --> 28:28.760
possible topics that you might consider is larger than the share than the average share

28:28.760 --> 28:31.640
of podcasts that talk about machine learning.

28:31.640 --> 28:32.640
So it's a relative measure.

28:32.640 --> 28:38.960
So it's like saying, okay, imagine, for example, now from T2T plus 1, you have an explosion

28:38.960 --> 28:42.040
of papers using Word2Vac, for example.

28:42.040 --> 28:48.440
So let's say that your share of Word2Vac papers goes from 5% of the publications to 10%.

28:48.440 --> 28:49.440
Okay.

28:49.440 --> 28:53.040
That's not really going to affect the results because what we're going to look is what

28:53.040 --> 28:59.120
is your own, as a country, as a city, share of publication in that topic compared to what

28:59.120 --> 29:00.480
the Word is doing.

29:00.480 --> 29:07.880
So for example, if a 10% of your publications that were in Word2Vac in the Word was 5%, then

29:07.880 --> 29:13.400
you kind of have a value of this trivial comparative advantage that is true, which means that you

29:13.400 --> 29:17.440
are above average on how much you're publishing in that topic.

29:17.440 --> 29:21.520
If at the next time step, let's say that when, let's say, for example, Word2Vac publications

29:21.520 --> 29:25.040
increase, your share of publication doesn't change.

29:25.040 --> 29:31.320
So you still have that actual share of publications in Word2Vac papers is 10%, but now the Word

29:31.320 --> 29:36.440
share is 10% as well, then your kind of strength have decreased.

29:36.440 --> 29:41.600
So the effect of controlling for the actual absolute size of a different topic, sort of different

29:41.600 --> 29:47.640
fieldings, embedding in our definition of how we define, in a sense, strength and weakness

29:47.640 --> 29:51.040
or expertise in different fields of research.

29:51.040 --> 29:55.520
So what we observe is actually indeed a relative measure of how good you are with respect

29:55.520 --> 29:59.480
to all the other players in the arena in a sense.

29:59.480 --> 30:03.160
I think the scenario is trying to point out, and this may be way in the weeds and not

30:03.160 --> 30:06.560
at all irrelevant, but what I was trying to get at is, you know, what if we have this

30:06.560 --> 30:12.920
machine learning podcast and we talk about embeddings all the time, and then the term

30:12.920 --> 30:18.960
Word2Vac suddenly becomes more popular and so we start using that as opposed to another

30:18.960 --> 30:25.520
way of describing the same thing, or the field matures and we get more and more specific.

30:25.520 --> 30:32.560
Your approach, out of necessity, is really concerned with the way we're describing what

30:32.560 --> 30:37.520
we're doing and what we can learn about the way we describe what we're doing as opposed

30:37.520 --> 30:44.840
to what we're actually doing that your research isn't really involved in kind of mapping

30:44.840 --> 30:51.280
that to the cities themselves and trying to project, for example, economic value that

30:51.280 --> 30:59.560
comes out of any of these changes in research areas to validate the model in any way.

30:59.560 --> 31:06.800
So in terms of, let's say we, as we said, maybe we get more specific in like talking

31:06.800 --> 31:11.360
on, let's say, discussable specific topics, so we might kind of switch the way in which

31:11.360 --> 31:12.360
discuss that.

31:12.360 --> 31:19.040
That's actually the whole reason why we introduced the embedding idea, embedding model around

31:19.040 --> 31:25.880
this problem precisely because if your embeddings work well, then that kind of transition

31:25.880 --> 31:30.240
shouldn't really matter because, for example, if now you start from talking, I don't know,

31:30.240 --> 31:35.880
from word-to-back tool, let's say, using only glove, in theory, your, let's say, system

31:35.880 --> 31:39.160
so your, let's say, knowledge space should have already accounted for that.

31:39.160 --> 31:43.200
So it should have already placed, for example, the two vectors very close to each other.

31:43.200 --> 31:50.960
So I don't think that's actually a problem given how we are setting up the whole system,

31:50.960 --> 31:55.200
which is kind of why we decide to go with the embedding approach rather than, for example,

31:55.200 --> 32:00.080
do probability counts or quick current counts as others, what authors have been doing precisely

32:00.080 --> 32:02.520
because of the problem you're saying.

32:02.520 --> 32:06.840
And regarding the economic impact, actually, indeed, that's actually the next step in

32:06.840 --> 32:07.840
the system.

32:07.840 --> 32:13.680
That's actually why we want to kind of extend our approach to different, to different

32:13.680 --> 32:14.680
areas.

32:14.680 --> 32:18.080
For example, because another thing that we have been working on and we can actually,

32:18.080 --> 32:22.480
you can imagine doing is that you don't only have academic publications in the world,

32:22.480 --> 32:24.480
but for example, you also have patents.

32:24.480 --> 32:28.960
So you actually also have recordings of what, for example, cities, if we want to stick

32:28.960 --> 32:33.640
with the geographical dimension, what cities have been publishing both in terms, for example,

32:33.640 --> 32:37.840
of technology, so patent applications and also publications.

32:37.840 --> 32:43.280
And so you can imagine that if you actually are able to build this big knowledge space,

32:43.280 --> 32:47.960
that includes not only knowledge as publications, but actually, for example, technology, then

32:47.960 --> 32:51.600
maybe you're actually able to kind of throw a line between the expertise that you might

32:51.600 --> 32:58.720
be developing in a specific field, research-wise, and the kind of maybe patenting activity might

32:58.720 --> 33:00.760
open up for you in the future.

33:00.760 --> 33:04.160
And then the actually link between the patent and the activity and the actual economic value,

33:04.160 --> 33:07.960
I would say is pretty straightforward, and it's still out of work to do, but I mean,

33:07.960 --> 33:09.720
it's something that any way can be done.

33:09.720 --> 33:15.240
So that's actually the reason why we are kind of trying to refine this approach, because

33:15.240 --> 33:21.000
once you have this space that can tell you essentially how science works and how technology

33:21.000 --> 33:25.080
works as an extension depending if you put those patent applications in or not, then you

33:25.080 --> 33:27.800
are actually able to answer those kind of questions.

33:27.800 --> 33:28.800
Fantastic.

33:28.800 --> 33:32.840
Mateo, thanks so much for taking the time to chat with us about what you're up to.

33:32.840 --> 33:35.120
Thank you for having me.

33:35.120 --> 33:41.080
All right, everyone, that's our show for today.

33:41.080 --> 33:46.880
For more information on today's show, visit twomolai.com slash shows.

33:46.880 --> 33:53.880
As always, thanks so much for listening, and catch you next time.

