Welcome to the Twomo AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone, a few quick updates from Twomo HQ.
As some of you may have noticed, we recently relaunched the Twomo newsletter.
If you've already signed up, awesome.
If not, well, you're missing out on weekly podcast updates, recommendations from the team,
and much more.
Head over to twomoAI.com slash newsletter to get signed up.
After you check out today's podcast, I encourage you to check out our latest demo casts.
This time around, I was joined by Vila Toulos, who you might remember from our conversation
late last year about Netflix's recently open source library, Metaflow, for building
and managing real-life data science projects.
Based on the data we've seen with our previous couple of demo casts, we've decided to no
longer publish the audio-only versions of those here in the main podcast feed.
So you'll need to jump over to twomoAI.com slash Metaflow to check it out.
And now on to the show.
All right, everyone.
I am on the line with David Adybo.
David is co-founder and CTO of Analytical AI, as well as a Kaggle Grandmaster.
David, welcome to the TwomoAI podcast.
Yeah, thank you, Sam.
Happy to be here.
You know what?
I am really looking forward to this chat.
I think this is the first time I've had a sibling on the show.
I interviewed your brother, Steven Adybo, back in July of last year, where we were talking
about retinal image generation.
And he was up on Twitter recently, talking you up and your conquests on Kaggle.
And I thought, man, I've got to get this conversation as well.
So congratulations for being the first sibling, at least to my knowledge on the podcast.
Yeah, thank you.
Why don't we start out by having you share a little bit about your background.
How did you come to work on machine learning?
When I started my PhD, I was interested in machine learning.
I wasn't quite sure where to go back then, so there was a lot of literature review, a
lot of just digging around.
And back then, you know, I read a lot of papers, I kind of tried to get a sense of where
to go to get useful information.
And Kaggle kept creeping up and coming up in various things I read.
And so I finally joined Kaggle, and it kind of put, it gave me an opportunity to put
some of that theoretical background into practice.
And a lot of things made a lot more sense when you actually practice.
So what were you studying for your PhD?
I was looking to investigate deep learning and medical imaging and how the advances in
deep learning at the time could be applied to various medical imaging problems.
And that sounds like an interest that runs in the family.
Yeah, it does.
You know, besides from any kind of genetic contribution, what sparked your interest in
that?
I wanted, actually, the first neural network I ever, I wanted to see if I could build
something to recognize a face in an image.
And this was actually before I started my, I selected my thesis topic.
And so that was really how I got it really interested in machine learning because it
was something I was kind of blindly doing.
And you know, as you're digging around, I wrote my first neural network in C sharp, which
was it worked horribly.
And this was in 2012.
And you know, that kind of made me realize that you read about how neural networks should
work and all these things.
But you never really, I never really knew at the time what, what the appropriate frameworks
to use and, and, and how you go about it, basically.
So.
And so what, why C sharp, were you a Microsoft developer?
Yeah, I was a, I was a Microsoft enterprise developer, I had done all the Microsoft
specifications and stuff.
So I was pretty good at C sharp at the time.
So I said, let me, let me try to do this as C sharp.
And it was just the absolute wrong idea and wrong approach to use at the time.
But, you know, if you, if you have a hammer, you know, you tried to, so if I'm putting
the pieces together correctly, you were the Microsoft enterprise developer before you went
back for your PhD, what prompted you to go back for your PhD?
Well, it's something I always wanted to do, you know, after I got my master's, I worked
in industry a little bit and I always knew I, I needed to kind of pivot back and, and
complete the PhD.
So that was kind of, it was something that was in the plans before I, but I wanted to,
after being in school for so long, you want to work and gain some experience.
So.
Yeah.
Nice.
And so you started working on neural networks in C sharp and kind of, you know, banged
away with that hammer that you, you had.
And it was, it was the absolute wrong, wrong thing to do.
I just need nowhere to go or to start back then.
You kind of were pursuing, you know, this formal, you know, research and education with
the PhD, but then you realized that there was kind of a practical compliment to that in
Kaggle.
Yeah.
Um, how did you start with, you know, once you, you know, created an account and joined
Kaggle, quote unquote, like how did you actually start did you just jump in and do a competition?
So I actually created my account for the first time in, I think it was 2014 or 2015.
And I didn't actually, I looked around, I read the, I didn't actually start competing
until a year later.
Okay.
So I first joined and then I, I just spoke around and then later on, I kept working
on my stuff and I got better at it.
I kind of identified the right frameworks to use.
I got a little bit more comfortable.
And then I think I did my first competition in, in 2016.
And, and that was, I think the data science bowl at the time that one was for trying to
detect the volume of, of, of ejection in, in heart, um, cardio grams or, so that was
my first competition.
Okay.
And that competition, I did okay, you know, but I didn't do really well, but it was good
experience to kind of get my feet wet.
Mm-hmm.
Yeah.
And were you, we had you partnered up with folks for that one or were you working independently?
Yeah.
We have a guy that we're, Jason Zeng, we're both in the same program and we're both doing
kind of deep learning and medically mentioned.
So we both went into that competition together.
Okay.
Yeah.
Cool.
And so that was 2016 fast forward to 2020 and you've seen quite a bit of success and
Kaggle competitions, can you talk about some of the, uh, your more recent results or at
least the results that you're most proud of?
Yeah.
So, um, shortly after that competition, there was another one that was, uh, for also medical
image and related competition.
It was for segmenting ultrasound images in the neck.
And there was, uh, fortunately for me, um, that competition was actually the competition
I did best in.
So there was a, there was a new architecture, a new paper that was written on this, uh,
thing called, it's an encoder, the quarter network is called a unit.
And it hadn't really been used on Kaggle before that architecture.
And I'd read the paper about the architecture.
And when I read the paper, it, because I thought about this too, you know, like how do you segment
things to something I've been thinking about, but when I read that paper, the, the ideas
in the, in the paper made a lot of sense, right?
It was like, and it almost seemed so obvious that what, that was what you were supposed
to do to segment images with convolutional neural networks where you try to preserve, um,
you try to preserve the localization information from the original images and things.
So that paper really sparked my interest.
I read it.
I was, I was a fan of it.
And this competition came up and somebody mentioned it.
And back then, there was a framework called LaZange and, uh, the, and those frameworks
are deprecated now, but back then, that was actually what I did, what I used.
And I, I, I, I said, okay, I'm going to give it a shot.
I'm trying, I'm going to try to implement this unit architecture.
And I did.
And I had no hopes of actually doing well in that competition.
But I said, let me give it a shot.
I implemented it, I trained the network and to my surprise, it worked.
And I was so, I was just so happy that it worked.
I had no idea I was going to finish a second place in the competition.
So, um, so that's probably the one I was most proud of because, um, I had to hack a lot
of things back then, you know, there was, even in 2016, there was, there was not, there
are not a lot of best practices of how you do segmentation, even back then.
So there was this, a lot of the frameworks out there had not specified how you do data
augmentation, you know, for image and masks because when you train, when you train segmentation
networks to avoid overfitting, you have to augment the image and the mask together.
And you have to find the right kind of augmentation strategies and things like that.
So I hacked, there was, I created a good augmentation strategy, a good augmentation framework.
And, you know, I saw a lot of people in the challenge were struggling with this idea,
you know, and they were not probably implementing the right augmentation strategies because,
you know, like I said, this was the first time they used this unit in this competition.
But I was kind of a little bit ahead of the game back then.
And so, yeah, so I finished in second place and it was, it was, it was probably my, the
proudest challenge, I'm, that's awesome.
And that was out of like 950 teams, right?
Yeah, that was out of 950 teams.
Were there other teams that used the unit architecture in that competition?
I think a lot of the teams used the unit, but they were implemented in, they used, they
probably all used the same unit.
And probably that was something that gave me an advantage because I don't think people
had taken the time to learn how to build that unit architecture back then.
So somebody posts that, hey, this is a great implementation of the unit and everybody
just follows the crowd and uses it.
And I think that kind of, if you did something different or you implemented your own and
you kind of improved it a little bit, maybe more than what everybody else was using, you
had a chance of doing a little better than they did if you kind of implemented it.
That's, that's one interesting thing that I find about Kaggle is that it is both at the
same time, you know, kind of inherently this competition, right?
That's what it is, but there's also a lot of collaboration and people sharing kind
of results and kernels and things like that.
Yeah, yeah, yeah.
Yeah, talk a little bit about that dynamic from the perspective of someone who participates
in it.
Yeah, so the kernels are great because it gives, it gives a good starting point like you
can take a kernel and utilize it and kind of quickly get up to speed in what the challenge
is about.
And it gives you, it kind of does a lot of the preliminary hard work you might have
had to do to kind of kind of figure out how to get started.
The next level, yeah, great.
So the kernels are great in that it gets you started.
But being a competition, it's, it's, my approach is generally that if you do what everybody
else does, you're not going to win.
So usually you usually have to take those kernels and try to try to improve them and add
your own unique improvement, optimization just to them because it's a competition after
all.
Yeah.
So the kernels are great and the sharing is great and it gets people up and running
fast.
Yeah.
Like you're, you had the success with the ultrasound nerve segmentation competition
relatively early in your Kaggle journey.
Yeah, it was actually my, my, my second or third competition.
So I was pretty, and that's why I'm also proud of it.
That's pretty encouragement.
Yeah.
It was, I got up there pretty quick.
I think I was, like I said, I was really surprised.
I said, let me implement this.
And it kind of gave me the confidence that, you know, if you just apply your ideas, you
know, you might, you should give it a shot.
Just just give it a shot and, and you never know how it turns out.
Mm-hmm.
You know, it's, it's amazing to think that, you know, just three years ago was kind of
the wild, wild west when it, you know, comes to these kinds of problems.
How was the game evolved over the past three years and not to say that that what your,
your accomplishment was easy, but is it, you know, is it as easy?
Is it still possible to, you know, do the kinds of things that you did in that competition?
Yeah.
You know, talk a little bit about how it has evolved.
Yeah.
So, it's evolved because now, you know, like then it was the wild west.
Now things, a lot of things you could have done in the past again and edge are now standardized
well understood.
So that's good because it moves the field forward.
You know, people have settled on best practices and best strategies.
So it definitely has evolved things that have improved.
It gets harder now to win competitions without really innovating or really looking into
percular things about the data sets or the problem that you can exploit.
So there's definitely, it's not as, it's, I think it's easier to start now than it was
back then because you have a lot of kind of examples and things to go by.
But still being a competition platform, it still requires pushing the limits a little
bit in terms of innovating and finding kind of things that could give you an edge.
So can you give us some examples of, you know, how you've innovated in competitions and,
you know, where you found patterns in the data that you were able to exploit?
How has this played out for you?
Okay.
So the next really good competition I did in was one sponsored by the Department of Homeland
and the TSA for detecting, for detecting threats in the provisioned body scanners.
You seeing the airports?
AKA the newtoscopes.
Yeah.
Yeah.
So, and that was the largest competition on Kegel in terms of price money.
So it was kind of like the highest profile competition.
And we came in, I came in third place in that one.
And that was another one that requires requires money.
The overall purchase was about $1.5 million.
Wow.
So, um, so in that one, we had to innovate because the data set was peculiar.
It was very large.
It was the largest data set ever on Kegel.
And so that required the barriers on that one were required, kind of solving the problem
of dealing with really, really large data and terabytes and, and, you know, you had
to get an edge in that.
And then you had to, the data was also three-dimensional.
And you had to come up with architectures that could deal with three-dimensional data.
So the, so in that, that was another competition where there were no best practices in terms
of how you go about solving this problem.
So if you could, it was one of those, another one of those low-hanging fruits where if you
could quickly come up with an optimal approach, you were more likely to do really well in
it.
So we came up with a very creative architecture that allowed us to perform well.
And so did you join that competition because it looked interesting or did you join that
competition because you saw that there were these issues that would provide kind of a
direction for you to innovate in?
Yeah.
So that competition, like I said, I'll still in my PhD program back then.
My part of my research was dealing with medical imaging data and a lot of the medical imaging
data are three-dimensional CT or MRI.
And I already started thinking about three-dimensional imaging.
So I already started, the architecture that you used was one I already started doing research
in.
I wrote about my thesis for handling the difficulties with the three-dimensional imaging are the
large volumes, and even with the state of the RGPUs we have today, if you want to process
at the original resolution of all these medical images, it's often impossible, right, without
sound sampling or reducing the size of the data to a point where you actually lose useful
information.
So I had started working on some of these problems.
And I'd actually already used the architecture I used on Parkinson's disease data so that
just to see if we could detect or classify Parkinson's disease in brain scans.
So it was something I was working on.
And when that challenge came around, I quickly identified that the data looked very similar
to the medical imaging data I was working with and I already had an approach I'd been investigating
so I said we're just going to crack this method loose on the data and it worked out well.
Nice.
Can you walk us through the method?
Yeah, so it's usually most of the CNNs out there are work with two-dimensional data and
a lot of the methods in deep learning work with two-dimensional imaging and that's well
understood.
But when you come to 3D, the memory requirements, they grow a lot.
So this method I used kind of combined, so you want to learn features in the two-dimensional
space.
But there's also a third dimension which is the third axis where you also can correlate
features across multiple two-dimensional images because the volume is essentially a
step of two-dimensional images.
So what we did was we combined the convolutional neural network, the 2D convolutional neural
network with an LSTM.
And LSTM is a different kind of architecture that kind of models sequences of data.
So and it requires less the input vectors of much smaller than images.
So if you could use the convolutional neural network to kind of learn the two-dimensional
vectors that you need to feed into the LSTM, you could reduce the memory requirements required
to train a three-dimensional volume.
And that was kind of the innovation in the architecture where we were able to.
So it sounds like you're using the, if I get this right, you're using the CNN to do something
along the lines of dimensionality reduction of your two-dimensional images and you're
eating that into an LSTM.
Yes, exactly.
That's what it is.
Interesting.
So by doing that, we didn't have to reduce the resolution of the input images.
So we're able to learn rich two-dimensional futures.
And also, you know, the body scanners, they give you this kind of three-dimensional view
of the person.
And there's kind of a correlation from frame to frame.
If you see something in one frame and you don't see it in another frame, in the next frame,
is it a false positive?
Is it a false negative?
So that kind of temporal effect of kind of going around something, kind of also benefited
that architecture.
So that was one where we could innovate and do well in a challenge.
Nice.
And was your, was the challenge to classify the images or to do segmentation or...
No, it was to classify the image.
So if you had something on your ankles hitting on your clothing, they wanted to know that
there's a threat underneath the left ankle.
There is 17 body zones around the person, the lower legs, backs, and you just have to
tell...
Then you're on that, just have to predict a probability of a threat in a specific body
part.
Okay.
Yeah.
Any other tricks that came to bear there?
You mentioned that part of the challenge had to do with just dealing with the volume of
data.
What did you end up doing there?
Yeah.
So that dealt with just writing a lot of, you know, code to deal with processed data
fast, multi-threaded, find ways to optimize your training so you could load as much data
as possible, as fast as possible.
So that was more like an engineering challenge, you know.
How do you train faster?
How do you load data faster?
How do you get through your training epochs faster?
How do you do your inferencing faster?
So I...
And where did you do all this?
Did you have, did you use university equipment or did you have your own equipment or do
you use cloud?
Because I use AWS Amazon, I write GPU instances on Amazon.
Okay.
And what did you end up spending on the competition?
So this competition we spent probably maybe $2,000 in GPU compute costs over three months.
Would you end up winning?
What is third place?
Third place was worth $200,000.
Nice.
So good profit margin.
Yeah.
That's good.
That was a good profit margin.
I guess I'm curious like the way you thought about the spend on AWS, you know, you know,
obviously before the competition ended, you didn't know if you were winning, but you
had signal that you were doing well, like, did you kind of managing it or did you?
Yeah, yeah.
You definitely manage it.
You definitely manage it.
You have to weigh the cost and the benefit and fortunately, usually what I do is, as
soon as I enter, I want to see myself in the top 30 to give, to know if it's worth,
you know, pushing harder, maybe spending more.
So that's usually how it works, you know, I usually come up with an approach.
I hardly ever pivot in this competition.
I kind of try to come up with a plan that before you even start, yeah, before I start,
but I wanted that plan to show signs of promise right away because oftentimes my strategy
is that I hardly ever pivot.
I hate pivoting because when in this challenge is usually you find out you waste a lot of
time experimenting with approaches that just lead to dead ends.
So I try to think out the problem well ahead of time and then just give it a shot at that
first approach and hope it shows good promise.
It's almost like because of the factors that you describe, like a lot of winning the competition
is being on the winning side of an information asymmetry.
And so the approach is one of, you know, figuring out a strategy.
And if you end up in the top 30, it means you're probably not on the losing side, dramatic,
you know, dramatically on the losing side of some information asymmetry, whereas that's
an interesting way to think about it.
Has that way of thinking about it evolved over the four years you've been doing this,
or did you, did you start pivoting like everybody else does?
Yeah, sometimes, you know, if a lot of time, it depends on the interest in the competition.
And there's a lot of interest, just my personal interest is a lot.
I will pivot if something is not working and I'll, because I don't mind, but if it's
not there, you know, yeah, so it's not like I don't pivot, but it's just, if the cost
of time and commitment you need, you can kind of gauge that from the first time you see
the competition is a lot, you don't want to spend too much time.
And also there's also this diminishing return effect where if you pivot, at least this
is just my approach, is that if you try too many ideas that fail, it's kind of defeating.
So you want to kind of throw your hard punch the first time and try to kind of get a good
result that way.
That's kind of how I approach it.
Is, are there some competitions that you join to play and others that you join to win?
Yeah, most of the competitions I do well in our computer vision.
So I usually try to do computer vision to do well in and win the other machine learning
competitions.
I do them to learn and to figure out things.
What are some other examples of this idea of kind of exploiting unique aspects of the
problem?
Yeah.
So, and these are some of my secrets.
I don't know if I want to give away too much, but I find the discussion forums on
Kaggle very, very useful, like what participants are talking about.
A lot of the times, you know, you reach through those forums and you can kind of, you can kind
of get a sense of an approach or some, you can find kind of connect threads.
So that's another part.
You try to find information you need that maybe people haven't seen.
But there's something they're all trying to say, but I don't know how to explain it.
But there is information in just reading the mindsets or trying to understand what people's
experiences are.
Because oftentimes that aspect of it too, you know, people who are doing well, sometimes
they're usually quiet.
That was one that I noticed in Kaggle too was when I started, the people at the top of
the leaderboard, I always try to look to see what they're talking about.
And they're, you never find anything because they're just quiet.
It's like they know something and they don't want to share.
You know, so there's that aspect of trying to dig in the forums to try to, you know,
try to learn approaches that could help in a challenge.
That makes me think a little bit about research in general and how, you know, often you'll
find that there are a bunch of, you know, more or less kind of parallel introductions
of innovative ideas because, you know, some period of time before, like there was something
in the air and nobody quite knew how to say it.
Exactly.
But there was something in the air and if you can kind of figure that out, that leads you
down the road to some innovation.
Exactly.
That's kind of what I'm trying to say.
So you also participated in a distracted driver competition.
That was what State Farm sponsored?
What was that one all about?
Yeah, that challenge was a challenge to identify distracted drivers.
I think there are different things.
It was the driver playing with the radio, was a driver looking, I forgot in all the categories
in that challenge, but it was kind of to identify distracted drivers.
And that was just they had a dash cam in the car and the task was to kind of indicate
what if the driver was paying attention to driving or was the driver eating or drinking
or using makeup or playing with the radio or doing something else.
So that was another image in competition I did well in.
The trick, one of the things I did in that one that really helped was we used a clever
data augmentation technique that enabled us to convince the data a lot.
We kind of, I don't know if it's relevant, I can explain the technique is we, so if
you, if we had two images of a driver that was playing with the radio, right, two different
drivers, we could take 75% of one image and combine it with the remaining 25% of the
other image and form a new image.
So that creates a kind of an additional image.
So it's, it's more likely some part of the driver, it just that augmentation approach
on how does create a lot more.
You had a picture of one driver playing with the radio and then a picture of the same
driver not playing with the radio, no, no, no, the picture of a different driver that
said was playing with the video.
So with the radio.
Two drivers that are both playing with the radio, you created other images and was this
like a linear interpolation of the two images.
Yeah, yeah, this is just a flat combination, you just combine it and you take some percentage
of this one and some, I mean, you, you join them horizontally or vertically.
And because you're just trying to learn, the neural network has to be able to learn,
you know, if it's a partial image of somebody doing something, there's still enough information
there to, to kind of try to estimate the best prediction.
So you're literally just like 75% of the image horizontally is one picture and 25% is
another picture.
Yeah.
And that worked.
Yeah.
Yeah.
Wow.
Yeah.
And we did that, we were able to, I think I don't know, we had maybe something like that.
What made you try that?
We're trying to deal with overfitting, right?
So the problem with the way the data was collected was that there were a few subjects
in the, in the training set and neural networks tend to overfit, they just memorize, right?
They just memorize the images, right?
Because once they see somewhere, the visor in a certain position or some unique future
in the image, they just latch on to that and they make the correct prediction every time
in training.
But that doesn't generalize to, it doesn't generalize to an unseen validation set, because
they're exploiting, you know, peculiarities of each.
So by doing this administration strategy, you kind of break all those false assumptions
that the neural network might have made to improve.
So it really has to learn the task of maybe identifying when to try and explain what
a radio.
So.
Wow.
Did you also innovate on the model architecture or something off the shelf?
No, we just used off the shelf in that one.
We didn't do anything special with the model architecture.
And a lot of Kaggle competitions are one by like these really kind of monstrosity ensembles
of things.
Is that true generally in vision or less so?
Yeah.
It's true in vision, but it's true in vision when you get to inserting competitions.
In other competitions like the Homeland Security Challenge, we could have kept our position
with one model.
So it's true.
But if one model is, if you really spend time can do, if you focus on one model, you can
almost do it as well as a massive ensemble.
But oftentimes the ensemble is the easy way out.
But the ensembles, there's a cost associated with that, at least for computer vision in
terms of GPU time.
If you have infinite compute resources, you might be able to get away with ensembling.
But oftentimes you have to wait the cost of training many models with focusing on one
and trying to get it as good as possible.
So if you were starting Kaggle today, we actually, we've got a study group, these are folks
that are kind of like an online meetup of folks that are doing Kaggle together.
They've been, I think it's been going for maybe eight weeks now, and they may be about
to start a new session of it.
But the sense is that one of the key things to do is to find other folks to work with.
Is that true in your experience?
Yeah, working with other people usually helps sharing ideas and kind of creating diversity
of approaches.
Most of my competitions have teamed up with somebody.
And everybody brings unique perspectives that help in challenges.
If you were talking to folks that are interested in starting, how would you advise them to kind
of go at it?
Okay, so one part of, at least I see, is that in AI, a lot of people make things a little
more complicated than they need to be.
Really?
I don't know if that's new information, but so the key is actually, the solutions for
problems, at least my approach, is that these solutions are usually simple.
They're not, they're not complicated.
And a barrier to starting Kagel is that there's this idea that it's hard, it's difficult.
If you look at AI, for example, and the progression of deep learning, at least from my experience
and my research, the key contributions have been really simple ideas.
So for example, before 2012, when Alex net won that computer vision challenge, there
was this idea that neural networks were hard to train, there was this vanishing gradient
problem, there was all this stuff going on, you just couldn't train neural networks.
And you should avenge your paper on the difficulties of training neural networks.
And what he found was that the activation function they were using, which was the sigmoid,
was causing, was a problem.
So all, in my opinion, all the breakthroughs we've seen has been a result of the regular
activation function that sort of eliminated that vanishing gradient problem.
So that idea is pretty simple.
It was a simple change in the activation function.
Between then and the other next key contribution, there are a lot of complex things that people
explain and try to cloud the actual real important things that push the ball forward.
And Kegel said of the same way, I kind of feel like a lot of challenges you just have
to kind of look at it with a creative approach and just open your mind that the solution
is simple.
It's just you're just a few ideas away from having kind of like a really good solution.
So I think this is a sort of joining teams is teaming up with people is a good idea.
But generally, you just have to believe that you can do well in a challenge and really
give it a shot.
Because Kegel can be discouraging, you know, it's, you can be discouraging if you go
in and you don't do well.
Fortunately for me, I didn't experience that well, maybe my first challenge, because you
know, my first challenge, I thought, yes, I'm going to blow this thing away.
And you know, you can be, you'll be really surprised, you know, how much you still have
to learn.
Yeah.
So teaming up is a good way.
But I still feel you can do it on your own, just have the motivation to to enter a challenge
and stick at it, don't get discouraged if you don't do well initially.
So teaming up persistence, keeping a simple, what else?
Yeah, that's, that's about it.
And, and reading top solutions, following up at the end of the challenge.
And oftentimes, Kegel, the winners usually share their approaches and follow up and kind
of look and correlate what the winners did with what you did and what you could have done
that or that you missed.
But I think that's it, it's really following up and just being persistent.
So those are the kind of the general tips for folks that want to get started.
What about the expert tips for folks that have been banging their head against it for
a while and haven't gotten to, you know, where they'd like to get?
Do you have any, you know, I think, I think, or ninja habits or anything like that.
I think those tips are universal.
Those tips works for the expert to the experts.
If they stay persistent, they look at the solutions of the top teams, what they did.
That's it.
Okay.
You just kept it simple.
Yeah.
Awesome.
You know, what we didn't talk about yet is analytical AI, which is your, your day job.
What, what is that?
What do you do there?
Yes.
So at Annalclei, we, due to the Homeland Security Challenge, we got a couple of projects.
We're developing various, we're working with various equipment manufacturers to that are
usually in the security space to help identify threats, either on person screening or in
baggage, CTE or X-ray.
And we're also working on various fintech products where we're trying to use AI for things
like technical analysis and things like that.
Interesting.
I did, I, this wasn't a full interview, but I've talked to a group down in Austin, I think.
And I've seen a couple of press releases about this.
I get a couple of year of groups that are using a bunch of sensors, maybe mounted on a police
vehicle or on a drone that are like trying to identify the presence of weapons, you know,
on a person, and the drone mounted ones are the ones that are kind of the most crazy
sounding.
Yeah.
Does that stuff work?
Is that kind of in the domain of stuff that you've been looking at?
That's not necessarily the domain of what we're looking at, like we're, we're looking
at working with teams that make things like passive screening, like tear hurts and, you
know, like, or X-ray for baggage at the airport.
It's not like drone mounted or these are like people screening devices, like the station
at somewhere and they try to make you walk through maybe at events or stadiums and identify
threats under clothing and things like that.
Okay.
Okay.
So opinion on the drone mounted viability.
Awesome.
Well, David, it has been great chatting with you.
Thanks so much for sharing a bit about what you've been up to and all of the great Kaggle
tips.
Yeah.
Thanks.
It was great talking to you too.
Thanks.
Thanks, David.
All right, everyone, that's our show for today to learn more about today's guests or
the topics mentioned in the interview, visit TwomoAI.com slash shows.
Don't forget to check out our demo cast with Vila at TwomoAI.com slash Metaflow.
And of course, be sure to subscribe to our YouTube channel while you're there.
If you like what you hear on the podcast, please subscribe, rate, and review the show on
your favorite pod catcher.
Thanks so much for listening and catch you next time.
