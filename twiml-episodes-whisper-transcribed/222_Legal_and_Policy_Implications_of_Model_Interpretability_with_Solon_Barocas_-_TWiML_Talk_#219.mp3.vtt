WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.400
I'm your host Sam Charrington.

00:31.400 --> 00:36.320
Today we're joined by Solan Barokas, Assistant Professor of Information Science at Cornell

00:36.320 --> 00:37.800
University.

00:37.800 --> 00:43.200
Solan is also the co-founder of the Fairness Accountability and Transparency in Machine Learning Workshop

00:43.200 --> 00:47.040
that's hosted annually at conferences like ICML.

00:47.040 --> 00:51.080
Solan and I caught up to discuss his work on model interpretability and the legal and

00:51.080 --> 00:55.160
policy implications of the use of machine learning models.

00:55.160 --> 01:00.320
In our conversation, we discussed the gap between law, policy, and ML and how to build

01:00.320 --> 01:04.600
the bridge between them, including formalizing ethical frameworks for machines to look at

01:04.600 --> 01:09.520
his paper, the intuitive appeal of explainable machines, which proposes that explainability

01:09.520 --> 01:14.480
is really two problems, inscrutability and non-intuitiveness, and that disentangling

01:14.480 --> 01:19.560
the two allows us to better reason about the kind of explainability that's really required

01:19.560 --> 01:21.960
in any given situation.

01:21.960 --> 01:26.560
And now on to the show.

01:26.560 --> 01:27.920
All right, everyone.

01:27.920 --> 01:30.360
I am on the line with Solan Barokas.

01:30.360 --> 01:35.800
Solan is an Assistant Professor of Information Science at Cornell University.

01:35.800 --> 01:39.080
Solan, welcome to this week in Machine Learning and AI.

01:39.080 --> 01:40.560
Thanks so much for having me.

01:40.560 --> 01:43.440
Why don't we get started by having you tell us a little bit about your background and

01:43.440 --> 01:46.640
how you got involved in machine learning?

01:46.640 --> 01:49.440
Yeah, it's a nice story, at least.

01:49.440 --> 01:53.520
I have a think unusual story in the way that I found my way to machine learning.

01:53.520 --> 02:00.200
So I got my PhD at NYU a couple years ago now, and I actually graduated from what is essentially

02:00.200 --> 02:01.720
a media studies department.

02:01.720 --> 02:06.880
I worked with a particular professor named Helen Nissenbaum, who for many years has been

02:06.880 --> 02:12.360
one of the kind of leading people doing work on the ethics of information technology.

02:12.360 --> 02:16.440
So I had spent time there specifically to work with her, but I knew early on when I was

02:16.440 --> 02:21.320
a graduate student, that I was interested in what was more commonly called data mining

02:21.320 --> 02:28.320
then, and decided to actually take some classes and got to know some of the faculty at NYU,

02:28.320 --> 02:32.440
who then really kind of brought me up to speed in the fundamentals of machine learning.

02:32.440 --> 02:37.200
And it became very clear to me that there were enormous number of interesting ethical and

02:37.200 --> 02:41.880
policy questions that were raised by machine learning, and that those were quite different

02:41.880 --> 02:47.040
than the kinds of topics that people were discussing quite speculatively without knowing

02:47.040 --> 02:50.560
very much about how machine learning actually worked in practice.

02:50.560 --> 02:54.360
And so I went off to do a dissertation that was really trying to translate some of these

02:54.360 --> 03:00.040
interesting questions that grow out of the technical aspects of machine learning into

03:00.040 --> 03:07.000
ethics and policy questions, and it has meant over a kind of a long period of time becoming

03:07.000 --> 03:11.880
more and more familiar with machine learning and both the community of researchers and practitioners

03:11.880 --> 03:17.440
in this area to the extent now that I feel pretty comfortable and at home in the broader

03:17.440 --> 03:20.280
machine learning community in general.

03:20.280 --> 03:25.360
And a lot of that work is also taking the form of trying to bring some of these normative

03:25.360 --> 03:32.680
concerns around privacy or fairness or transparency further into the machine learning community.

03:32.680 --> 03:39.240
So making these issues first order questions for the field and that has included staging

03:39.240 --> 03:44.920
workshops for about five years now attached to the main machine learning conferences like

03:44.920 --> 03:51.080
NIPS and ICML that were explicitly attempts to bring questions of fairness accountability

03:51.080 --> 03:53.920
and transparency into the mainstream of the community.

03:53.920 --> 03:58.560
In fact, the event we started was still called fairness accountability and transparency

03:58.560 --> 04:00.440
and machine learning.

04:00.440 --> 04:05.320
So that has been one of the main mechanisms by which I've kind of entered the community.

04:05.320 --> 04:09.240
I think people, if they know me at all, probably know me through that work.

04:09.240 --> 04:15.080
And it's been great actually because five years ago when we started this kind of conversation,

04:15.080 --> 04:22.240
it was a pretty niche group, some great people from the start but still quite a small community.

04:22.240 --> 04:26.680
And over the past two years in particular, these have just become kind of mainstream issues

04:26.680 --> 04:32.160
for the field, such that papers are now being accepted at these conferences, a bunch

04:32.160 --> 04:36.080
of best paper awards have gone to papers on fairness recently.

04:36.080 --> 04:39.640
And if you follow the news at all, a lot of companies, a lot of the big tech companies

04:39.640 --> 04:44.720
have also begun to take these issues really seriously and begin to work on them in practice.

04:44.720 --> 04:47.760
And I've had opportunity to kind of see how that's progressed.

04:47.760 --> 04:51.720
So that's really the role I've been filling in this community the past couple of years.

04:51.720 --> 04:58.040
And while I really appreciate that aspect of kind of bridging these two communities, I find often

04:58.040 --> 05:07.000
or found that a lot of people in machine learning care about these issues of ethics, but there

05:07.000 --> 05:10.720
aren't a lot of people that have both the machine learning background and a background

05:10.720 --> 05:11.720
in ethics.

05:11.720 --> 05:17.800
And so you often observe, appears to be reinventing the wheel, coming up with ethical frameworks

05:17.800 --> 05:18.800
from scratch.

05:18.800 --> 05:24.880
In fact, there's clearly been a lot of work put into that kind of thing on the ethics side.

05:24.880 --> 05:30.880
And it's just an issue of like connecting these dots or like connecting the two sides of

05:30.880 --> 05:34.040
the bridge or something like, do you experience that as well?

05:34.040 --> 05:35.400
It's a great question.

05:35.400 --> 05:42.400
I think what's been exciting about this area of research and policy discussion is people

05:42.400 --> 05:48.560
sort of recognized early on that at least when it comes to questions of fairness and discrimination,

05:48.560 --> 05:54.520
there were some existing ways of thinking about the issue which lent themselves to formalization.

05:54.520 --> 06:01.360
So just to give a kind of quick anecdote here, it's sort of a rule of thumb in discrimination

06:01.360 --> 06:07.480
law that if there's a disparity in outcome greater than 20% between, let's say, men and

06:07.480 --> 06:13.120
women who are applying for a job, that itself can be a trigger for a case to investigate

06:13.120 --> 06:15.480
whether in fact there is discrimination.

06:15.480 --> 06:20.800
It's called the four fifth rule, it comes from the, the regulator, the employment regulator.

06:20.800 --> 06:24.960
And even though it's not enough to establish that some kind of decision making process, there's

06:24.960 --> 06:30.400
discriminatory, it's often used as a heuristic to say like, well, we should try to minimize

06:30.400 --> 06:33.800
the disparity so that it's never greater than 20%.

06:33.800 --> 06:38.080
And because this is an existing way of thinking about this in quantitative terms, it really

06:38.080 --> 06:43.240
lent itself to the kind of work that machine-linear people like to do, which is this kind of constrained

06:43.240 --> 06:44.400
optimization problem.

06:44.400 --> 06:49.280
So you know, how can we build classifiers or predictive models that aim to achieve, you

06:49.280 --> 06:53.440
know, a maximum performance, subject to this additional constraints.

06:53.440 --> 06:57.360
And some of the earliest work on fairness and machine learning grew directly out of some

06:57.360 --> 07:00.040
of these principles from the law.

07:00.040 --> 07:04.480
Since then, what's happened though is that there's been a bunch of new ways of trying to

07:04.480 --> 07:08.720
formalize notions of fairness that are really different and pretty much detached from

07:08.720 --> 07:12.680
the way that the law and policy community think about them.

07:12.680 --> 07:17.480
And you could maybe think about that as being reinventing the wheel or not being sufficiently

07:17.480 --> 07:21.360
sensitive to how much work has been done already.

07:21.360 --> 07:27.680
But part of what's been kind of exciting and fascinating is that some of these new formalizations

07:27.680 --> 07:31.600
are actually just sort of interesting new ways of thinking about the problem that haven't

07:31.600 --> 07:36.120
come up in the law and policy discussion in part because perhaps people haven't had

07:36.120 --> 07:41.120
reasons to think about them in the way that these new formalizations actually parse the

07:41.120 --> 07:42.120
issue.

07:42.120 --> 07:47.800
And so an example of this is, you know, there's a debate now whether differences, differences

07:47.800 --> 07:52.080
not only in the accuracy of the model between different groups matter, but whether differences

07:52.080 --> 07:54.080
in the error rates matter.

07:54.080 --> 07:58.520
So, you know, it might matter very much in different settings that you're subject to a false

07:58.520 --> 08:04.920
positive or false negative and perhaps you want to equalize across these rates, but which

08:04.920 --> 08:08.840
one of those actually matters or do these matter at all.

08:08.840 --> 08:13.320
And that's sort of the direction that the field is going, proposing these new ideas that

08:13.320 --> 08:17.400
don't have a neat or obvious analog in law.

08:17.400 --> 08:19.120
And I think there's a lot that can come out of that.

08:19.120 --> 08:23.160
Part of it might be that maybe these aren't, in fact, good ways to think about it given

08:23.160 --> 08:28.480
what people actually care about from a normative perspective, but it also can potentially

08:28.480 --> 08:32.920
reveal that some of the ways that law and policy have thought about these problems are incomplete

08:32.920 --> 08:35.560
or incoherent or we could actually do better.

08:35.560 --> 08:37.160
So I think there's a lot of opportunity here.

08:37.160 --> 08:41.080
That's a fantastic take on that.

08:41.080 --> 08:48.920
What are, you kind of mentioned, you know, broadly different formalisms and ways to parse

08:48.920 --> 08:49.920
this issue?

08:49.920 --> 08:55.160
Are there some other examples that come to mind of the way folks have formalized ethical

08:55.160 --> 08:58.920
and fairness frameworks applying them to machine learning?

08:58.920 --> 09:03.680
Yeah, I think at a high level, there's a way to maybe categorize some of them.

09:03.680 --> 09:10.600
So I think a set of concerns have to do with just straight up differences in the performance

09:10.600 --> 09:14.600
of models for different groups, for men and women, for people of different races or what

09:14.600 --> 09:16.720
have you.

09:16.720 --> 09:20.360
And you could think about that using any number of different metrics, right?

09:20.360 --> 09:24.800
So not just accuracy, not just these kind of false positive or false negative rates,

09:24.800 --> 09:29.000
but a bunch of other things as well, including calibration and other terms that might be familiar

09:29.000 --> 09:30.320
to you folks.

09:30.320 --> 09:35.480
What's interesting about that work is that it sort of sets aside the question, whether

09:35.480 --> 09:39.200
the data, the underlying training data itself is reliable.

09:39.200 --> 09:44.480
It says, even if the data is perfect, there might be circumstances where we still observe

09:44.480 --> 09:48.960
disparities in the performance of these models between groups.

09:48.960 --> 09:50.880
And if that's the case, you know, what can we do about it?

09:50.880 --> 09:53.400
How do we actually try to avoid those situations?

09:53.400 --> 09:56.160
A separate bucket, though, actually starts from a different position.

09:56.160 --> 10:02.360
It says, we should never actually believe that the training data we have is reliable.

10:02.360 --> 10:06.720
And in fact, there's good reason to believe that it's systematically unreliable in a particular

10:06.720 --> 10:07.720
way.

10:07.720 --> 10:13.720
So an example of this is, you know, you might think about how does the accuracy of a predictive

10:13.720 --> 10:16.840
policing model differ between groups?

10:16.840 --> 10:19.800
That's the sort of question you get asked in the first camp.

10:19.800 --> 10:23.320
And the second camp, which is concerned with the underlying quality of the data, they might

10:23.320 --> 10:29.280
say, yeah, you actually don't have data that is a good representation of the true incidence

10:29.280 --> 10:30.800
of crime in society.

10:30.800 --> 10:35.040
So anything you might do around the edges concerning performance is really not addressing the

10:35.040 --> 10:39.760
fundamental problem, which is that you're learning from highly biased data.

10:39.760 --> 10:46.480
And a slightly more serious version of this concern might be that maybe you are even

10:46.480 --> 10:49.920
using data that encodes some kind of prejudice.

10:49.920 --> 10:54.280
So it's not just that the selection or sample of data is biased, but that the training

10:54.280 --> 10:59.760
data is itself some past decision made by someone, which was made in a prejudicial way.

10:59.760 --> 11:04.280
Though the example I tend to give for this is something like an employer trying to use

11:04.280 --> 11:09.640
machine learning to help find good people to hire in the future.

11:09.640 --> 11:14.440
And you might say, let's take a look at which employees that we've hired in the past

11:14.440 --> 11:17.080
have gone on to be particularly successful.

11:17.080 --> 11:22.800
The target variable we might select in this model is the annual review score that we've

11:22.800 --> 11:25.800
given our past employees.

11:25.800 --> 11:31.600
And so what you want the model to do is to predict what the likely annual review score

11:31.600 --> 11:36.960
would be for any new job applicant given the training data from your past employees.

11:36.960 --> 11:42.000
The problem, of course, is that even though one of these annual review scores is meant

11:42.000 --> 11:48.360
to be carefully considered and people are asked to go through a pretty systematic process

11:48.360 --> 11:52.760
to assign this score invariably, and there's even research empirical research to show

11:52.760 --> 11:58.120
this, that score is going to be inflected either potentially by conscious prejudice or

11:58.120 --> 12:02.840
implicit bias, where people don't even really realize that their assessment is somehow swayed

12:02.840 --> 12:06.800
by these preexisting beliefs about gender or race or what have you.

12:06.800 --> 12:11.240
And so what ends up happening is that the training data actually encodes that prior prejudice

12:11.240 --> 12:15.400
or bias, and so the model is not learning to predict how people would actually do on

12:15.400 --> 12:16.400
the job.

12:16.400 --> 12:21.200
The model is learning to predict how human assessors who had previously been given, who had

12:21.200 --> 12:26.920
been previously giving these annual review scores, would likely review this future person.

12:26.920 --> 12:31.720
So these give you a taste for some of the subtle differences, and each of these problems

12:31.720 --> 12:34.560
really require quite different responses, right?

12:34.560 --> 12:38.440
The very first type of problem that's describing, you're trying to figure out how to deal with

12:38.440 --> 12:43.400
the fact that there are situations where your model just won't perform as well, and these

12:43.400 --> 12:50.040
other situations where the underlying data is actually unreliable, require more creative,

12:50.040 --> 12:56.160
potentially kind of more ambitious solutions, which try to just compensate for what you

12:56.160 --> 13:00.880
believe to be the underlying problem with the data, even though you might have no direct

13:00.880 --> 13:03.120
way of measuring the thing you really care about.

13:03.120 --> 13:10.240
Just following that thread of the job performance reviews, to what degree has that specific problem

13:10.240 --> 13:15.720
been, explored, and what kinds of approaches or solutions have you seen folks taking

13:15.720 --> 13:16.720
with that?

13:16.720 --> 13:17.720
That's a great question.

13:17.720 --> 13:23.720
So I don't think there's any concrete case yet where we've been able to establish that

13:23.720 --> 13:32.200
the training data actually encodes the past discriminatory decisions of management.

13:32.200 --> 13:37.720
But as a kind of thought experiment, and given other empirical work on how humans actually

13:37.720 --> 13:43.080
do assign these kinds of scores to their employees, there's very good reason to believe

13:43.080 --> 13:46.040
that that training data would, in fact, whatever training data people might put together

13:46.040 --> 13:49.200
would suffer from that kind of problem.

13:49.200 --> 13:55.240
There's a lot of people who are becoming more and more familiar with this as a potential

13:55.240 --> 14:01.440
concern, and so that's resulted both in lawyers trying to find cases to potentially bring

14:01.440 --> 14:03.080
against employees.

14:03.080 --> 14:07.960
But separately, there's also a lot of companies who specialize in machine learning, who are

14:07.960 --> 14:13.920
trying to integrate these concerns into the products they then sell to clients.

14:13.920 --> 14:18.480
So these are sort of recruitment machine learning specialists to then develop tools that

14:18.480 --> 14:23.200
they license or sell to employers.

14:23.200 --> 14:28.440
And part of the solution to these problems can vary considerably.

14:28.440 --> 14:34.440
So on the one hand, a lot of the simplest interventions involve reporting performance

14:34.440 --> 14:39.800
in a way that just breaks apart overall performance into performance by group.

14:39.800 --> 14:45.600
So can we just observe that this model does a much worse job for certain people than others?

14:45.600 --> 14:48.040
And are there ways that we can try to compensate for that?

14:48.040 --> 14:52.480
And my sense is that that's becoming an increase in the more common approach.

14:52.480 --> 14:57.000
The problem with that approach, of course, is that you actually need to know those details

14:57.000 --> 14:59.320
about the people you're trying to score.

14:59.320 --> 15:04.640
So in order to separate out the performance for men and women or for people with different

15:04.640 --> 15:08.360
of different races, you actually have to collect information about that.

15:08.360 --> 15:15.680
And as a kind of standard practice, employers at least really have good incentive not to

15:15.680 --> 15:20.640
collect that information because they don't want to even create the possibility that of

15:20.640 --> 15:23.960
being accused of having considered it when making decisions.

15:23.960 --> 15:26.000
So it's an interesting tension, right?

15:26.000 --> 15:32.000
We might want the information to prevent discrimination, but historically, companies have been instructed

15:32.000 --> 15:36.680
not to collect it to limit the likelihood that they could ever even consider it.

15:36.680 --> 15:41.600
The problem with encoding or your training data is just past decisions which might have

15:41.600 --> 15:44.040
been influenced by human bias.

15:44.040 --> 15:45.480
That's much more difficult.

15:45.480 --> 15:49.920
And it's not even obvious what a principled approach to that would be.

15:49.920 --> 15:54.160
Some of the positions that people have put forward is to sort of just make certain assumptions

15:54.160 --> 16:01.160
about what you think is a more reasonable distribution of attributes across the population so that

16:01.160 --> 16:07.640
you sort of override what is the actual distribution and the training data you're dealing with.

16:07.640 --> 16:12.240
In a way, you're basically saying, I suspect the data to be systematically flawed and I'm

16:12.240 --> 16:16.280
going to kind of put my thumb on the scale to compensate for that.

16:16.280 --> 16:21.520
And some people now have pretty rigorous mathematical methods for doing this without necessarily

16:21.520 --> 16:23.960
sacrificing performance.

16:23.960 --> 16:28.680
But I'm just not so familiar yet with what is happening in practice on that front.

16:28.680 --> 16:32.920
And the final thing I would say is that, you know, in a situation where you have all

16:32.920 --> 16:37.840
their mechanisms to potentially measure these dynamics, you might take a different approach.

16:37.840 --> 16:41.760
You might say, I'm not going to treat this as a pure prediction problem.

16:41.760 --> 16:47.400
I'm going to try to do some kind of empirical study to see if in fact, you know, there's

16:47.400 --> 16:49.160
a problem in my workplace.

16:49.160 --> 16:55.240
So rather than just relying exclusively on these annual review scores, maybe you go and

16:55.240 --> 17:02.240
try to find some other thing to measure, which is not as vulnerable to human bias.

17:02.240 --> 17:05.840
So an example of this might be something like, well, it's pretty hard to argue with the

17:05.840 --> 17:10.640
fact that you've achieved some sales figure at the end of the year if you're a sales person.

17:10.640 --> 17:16.720
Like that seems like a metric that's much less vulnerable to this kind of bias assessment.

17:16.720 --> 17:20.280
You can challenge that, but the argument anyway is that maybe we can begin to measure

17:20.280 --> 17:25.560
other things and then compare that to the kinds of assessment people get at their annual

17:25.560 --> 17:26.560
review.

17:26.560 --> 17:31.560
And that might reveal some kind of underlying disparity or sort of misalignment between

17:31.560 --> 17:34.440
people's true performance and the score they're given.

17:34.440 --> 17:37.880
And then given that finding, you can go about trying to correct things.

17:37.880 --> 17:42.600
But there's, you know, even in this very long answer, I haven't even exhausted the list

17:42.600 --> 17:43.600
of possibilities.

17:43.600 --> 17:49.480
I mean, I guess it's obvious that a lot of these challenges are like fundamental human

17:49.480 --> 17:57.520
organizational people issues and technology is but a small part of the overall picture.

17:57.520 --> 17:59.040
That's right.

17:59.040 --> 18:04.760
And I think what's interesting about the current state of the research is that a lot of it

18:04.760 --> 18:10.520
is sort of head-to-head comparison between existing decision-making process and some

18:10.520 --> 18:13.240
model under perfect conditions.

18:13.240 --> 18:19.280
And I think a lot of progress will depend really ultimately, I think, on figuring out

18:19.280 --> 18:25.680
how to think more kind of formally and carefully about these fairness concerns and integrate

18:25.680 --> 18:28.040
them into the model development process.

18:28.040 --> 18:34.600
But separately, also think about how to then put that model into practice and potentially

18:34.600 --> 18:37.840
reform the institution itself, as you were saying, right?

18:37.840 --> 18:42.840
Like really consider how this fits into the bureaucratic decision-making process, how it

18:42.840 --> 18:46.560
figures into the dynamics of the workplace that would have you.

18:46.560 --> 18:54.000
So even if we are successful in trying to deal with these concerns within the model itself,

18:54.000 --> 18:58.840
that is certainly not sufficient to achieve these broader fairness or justice goals we might

18:58.840 --> 18:59.840
have.

18:59.840 --> 19:05.480
And so I asked about that, about an example in the context of performance reviews, but

19:05.480 --> 19:13.400
are there more broadly any examples that you've come across of kind of this process taking

19:13.400 --> 19:21.760
place full circle within an organization or some political structure where the algorithmic

19:21.760 --> 19:29.440
or data biases were observed, some sets of adjustments were made to modeling as well

19:29.440 --> 19:37.040
as the underlying organization, organizational practices, and that leading to a better

19:37.040 --> 19:38.040
outcome?

19:38.040 --> 19:39.560
Yeah, it's a great question.

19:39.560 --> 19:44.720
And I don't think there's some shiny and example to point to, unfortunately, at least

19:44.720 --> 19:45.720
not yet.

19:45.720 --> 19:51.600
I mean, there's certainly some examples of changes that have been made that were a little

19:51.600 --> 19:52.640
bit more straightforward.

19:52.640 --> 19:59.400
So some scholars were able to show that there were disparities in the performance of

19:59.400 --> 20:05.360
kind of off-the-shelf facial recognition software and that these disparities were along the

20:05.360 --> 20:09.920
lines of both race and gender, such that these systems did a much, much worse job, for

20:09.920 --> 20:13.240
instance, for black women than for white men.

20:13.240 --> 20:18.240
And the result of this research, which I certainly encourage your listeners to take a look

20:18.240 --> 20:25.720
at, was some pretty rapid changes on the part of the companies that provided these often

20:25.720 --> 20:30.560
API facial recognition services, so they just basically went about trying to figure out

20:30.560 --> 20:33.160
how to improve the performance for these populations.

20:33.160 --> 20:37.200
You know, that's a different story because it doesn't involve this entire kind of workflow

20:37.200 --> 20:42.280
and bureaucracy, but an example I could still point to, which I think is an interesting

20:42.280 --> 20:51.160
one, is in Allegheny County, which houses Pittsburgh, the county was working with some academic

20:51.160 --> 20:56.320
researchers to not only use machine learning, but just in general, you sort of more data-driven

20:56.320 --> 21:02.400
approaches to the way that it handled its child protective welfare agency.

21:02.400 --> 21:09.520
And this got written up as a kind of long future article in the New York Times magazine

21:09.520 --> 21:10.880
a few months back.

21:10.880 --> 21:15.680
It's also actually the focus and the chapter of an excellent book called Automate in

21:15.680 --> 21:23.080
Equity both described this pretty careful process by which researchers engage with the city,

21:23.080 --> 21:30.880
but also with agency workers, with advocates for children and families, for people affected

21:30.880 --> 21:35.600
by these systems, and really try to take into consideration all the different interests

21:35.600 --> 21:39.960
that this agency was charged with serving.

21:39.960 --> 21:44.000
And the people developing the tool actually observed that there were, in fact, some of

21:44.000 --> 21:47.520
these disparities and how well the model performed.

21:47.520 --> 21:53.680
It was likely to produce these kinds of kind of disparate impact in the way that it would

21:53.680 --> 22:00.000
suggest people for scrutiny when it came to potential child maltreatment or child abuse.

22:00.000 --> 22:04.880
And one of the interesting things to think about when focusing on this story is that despite

22:04.880 --> 22:11.280
the fact that this effort really involves a considerable amount of community engagement

22:11.280 --> 22:16.960
and consultation, and even really explicitly took into consideration some of these questions

22:16.960 --> 22:21.600
around fairness, that people will nevertheless still have, I think, some legitimate concerns

22:21.600 --> 22:23.480
around the use of these tools.

22:23.480 --> 22:30.800
So it's hard to say that this is like a clear model for what everyone should be doing

22:30.800 --> 22:37.480
in similar situations, but it gives some sense of just how difficult it may be to kind

22:37.480 --> 22:42.640
of more fundamentally address questions of fairness, even if you've gone to the effort

22:42.640 --> 22:45.680
of integrating them into the model development process.

22:45.680 --> 22:52.240
I think what it points to for me is the need for kind of lots of people and perspectives

22:52.240 --> 22:57.560
to get involved in understanding this issue and how it applies to the problems that they

22:57.560 --> 23:04.200
care about and taking on the little pieces of it that they can take on, even if they're

23:04.200 --> 23:08.520
not exposed to the full cycle, if you will.

23:08.520 --> 23:13.600
Yeah, you know, the way you think about this is heavily influenced by the fact that one

23:13.600 --> 23:19.240
of the people who introduced me to machine learning was someone who had more than a decade

23:19.240 --> 23:25.120
of experience and practice, this is Foster Provo who's a professor at NYU.

23:25.120 --> 23:29.400
And his way of teaching machine learning really emphasized this kind of first step in the

23:29.400 --> 23:33.400
process, very different than what you get in kind of traditional academic machine learning

23:33.400 --> 23:34.400
education.

23:34.400 --> 23:38.760
You know, he really emphasized that one of the main challenges is how do you translate

23:38.760 --> 23:41.440
a business problem into a machine learning problem?

23:41.440 --> 23:47.360
So, you know, how do you specify the target variable correctly or in a way that's reasonable?

23:47.360 --> 23:51.920
And for people who I think have experience and practice, that is a familiar problem, you

23:51.920 --> 23:57.720
know, it's like not that easy to know exactly how to solve some general business problem

23:57.720 --> 24:01.160
by figuring out how to specify the target variable.

24:01.160 --> 24:06.000
And my sense is that when people do think about that, they think about it in terms of, you

24:06.000 --> 24:10.960
know, what is the appropriate thing to try to predict when you're in online marketing,

24:10.960 --> 24:11.960
right?

24:11.960 --> 24:15.000
Clearly, like, click through is not the best thing.

24:15.000 --> 24:19.240
We want someone to convert and so you can have a pretty obvious debate around what is

24:19.240 --> 24:21.880
the right thing that you should be predicting.

24:21.880 --> 24:27.000
I think there's a similar thing that happens in these domains that involve much more high

24:27.000 --> 24:33.760
stakes decisions like, you know, child welfare or employment or credit, right?

24:33.760 --> 24:38.960
We can have, I think, a pretty straight forward conversation about what it is that we're

24:38.960 --> 24:45.320
actually trying to achieve and does the way the problem has been specified actually correspond

24:45.320 --> 24:47.760
to our normative goals.

24:47.760 --> 24:54.040
And the advice that you get from some of the early data money, literature, I think is

24:54.040 --> 24:58.920
really relevant here, it's sort of about, as you say, you really want to have a pretty

24:58.920 --> 25:02.720
deep and thoughtful conversation with your stakeholders.

25:02.720 --> 25:06.560
You want to understand the problem that you're being charged with solving and you want

25:06.560 --> 25:10.280
to understand whether or not it really, the way you've kind of set up the problem really

25:10.280 --> 25:13.680
reflects the concerns of the people it's supposed to serve.

25:13.680 --> 25:20.320
So my sense is that, you know, a lot of existing ways of doing machine learning well in practice,

25:20.320 --> 25:25.360
those insights, the kind of ideas people have from their experience on the ground, could

25:25.360 --> 25:29.320
be super helpful to the conversation people are having now about ethics.

25:29.320 --> 25:37.080
So you recently published a paper that looked at the applicability of some of the work that's

25:37.080 --> 25:46.280
happening around transparency and explainability to various regulatory frameworks like GDPR and

25:46.280 --> 25:47.600
others.

25:47.600 --> 25:50.400
Can you give us an overview of that work?

25:50.400 --> 25:51.400
Sure.

25:51.400 --> 25:56.960
So this is a forthcoming paper and Ford and Law Review and my co-author Andrew Selbst and

25:56.960 --> 26:02.120
I were trying to sort of bring together a couple different conversations that were happening.

26:02.120 --> 26:08.320
So in the law and policy world, there's a lot of anxiety around the use of machine learning

26:08.320 --> 26:12.760
for high-stakes decision making because the fear is that you won't be able to explain

26:12.760 --> 26:17.880
the outcome of some decision-making process and for people who come from machine learning,

26:17.880 --> 26:22.280
you'll know that there's a long history of working on interpretability in machine learning

26:22.280 --> 26:27.040
and that this has become especially hot area as deep learning has become more successful

26:27.040 --> 26:31.720
and more dominant and there's been some really interesting research breakthroughs in trying

26:31.720 --> 26:36.160
to be able to explain what is happening with deep learning models.

26:36.160 --> 26:40.600
So what we were trying to kind of show is that there's ways to sort of have these two

26:40.600 --> 26:43.880
things speak to each other a little bit.

26:43.880 --> 26:50.080
Part of it is about explaining exactly what it is that the existing laws and regulations

26:50.080 --> 26:56.440
actually require when they require explanations and then part of it is also trying to show

26:56.440 --> 27:01.040
that there are in many cases tools for satisfying those laws.

27:01.040 --> 27:08.840
So although GDPR which is the European general data protection regulation is the thing that

27:08.840 --> 27:14.520
has really generated a lot of attention around these issues recently, there are laws here

27:14.520 --> 27:19.880
in the United States that also will require explanations for automated decisions and

27:19.880 --> 27:25.600
the key example of that is the Equal Credit Opportunity Act and the Fair Credit Reporting

27:25.600 --> 27:26.600
Act.

27:26.600 --> 27:33.120
These are both laws that regulate credit scoring and credit decision-making and the acronyms

27:33.120 --> 27:40.520
are ECOA and FICRA and these laws when they stipulate that when a person applies for

27:40.520 --> 27:45.880
credit and the creditor denies that person, the creditor actually has to give reasons

27:45.880 --> 27:49.400
for why they denied the loan.

27:49.400 --> 27:54.160
And this law actually dates from the 1970s so this is not a new law, it's been around

27:54.160 --> 28:00.320
for a very, very long time and it has actually really structured the credit industry.

28:00.320 --> 28:04.480
If you speak to people in practice, you'll know that this really is the way that they've

28:04.480 --> 28:09.120
had to orient all the work that to make sure that they could always give reasons for

28:09.120 --> 28:14.520
their decisions regardless of the mechanism by which they got to their decision about

28:14.520 --> 28:16.320
whether they issued the loan.

28:16.320 --> 28:23.560
And the concern is that can you give reasons for a decision around credit if your model

28:23.560 --> 28:26.400
is using something like deep learning?

28:26.400 --> 28:31.240
Well, if you know the work in interpretability and machine learning at all, you'll know

28:31.240 --> 28:39.400
that a lot of the recent proposals involve for going any attempt to actually provide global

28:39.400 --> 28:46.080
transparency into the model, meaning forget any effort to describe the full relationship

28:46.080 --> 28:47.960
that the model maps out.

28:47.960 --> 28:54.760
And instead, let's use some other mechanisms to see if we can say what in any given decision

28:54.760 --> 29:00.080
actually seem to be the most salient variable or set of variables, so which features in

29:00.080 --> 29:06.240
the model really account for this particular classification or outcome.

29:06.240 --> 29:11.880
And those methods have proven pretty powerful in general for purposes of kind of any deep

29:11.880 --> 29:17.040
learning or machine learning model, but certainly it's not hard to imagine that they would

29:17.040 --> 29:23.440
be well suited to satisfying this existing requirement in eco and Fickra, which literally

29:23.440 --> 29:30.420
say you have to give specific and the actual reasons why someone was denied a loan.

29:30.420 --> 29:33.720
So what's interesting about this is it feels sort of like a silver bullet.

29:33.720 --> 29:41.400
It says like, oh, you can go off and build an arbitrarily complex model so long as you

29:41.400 --> 29:45.560
can provide specific reasons for any particular decision.

29:45.560 --> 29:51.840
And it feels that there might be a way to avoid the longstanding perceived tradeoff between

29:51.840 --> 29:56.720
the performance of the models you build and the interpretability of those models.

29:56.720 --> 29:57.920
So that seems like a good outcome.

29:57.920 --> 30:02.840
It seems like progress in kind of the research domain of machine learning has really helped

30:02.840 --> 30:08.920
solve a longstanding issue with regulation.

30:08.920 --> 30:13.960
The paper with my co-author kind of goes into some detail about why this might not always

30:13.960 --> 30:16.920
be so helpful in practice.

30:16.920 --> 30:17.920
And it's a bit complicated.

30:17.920 --> 30:21.160
So I'm not sure if I should carry on or I should let you ask another question.

30:21.160 --> 30:24.160
Well, we definitely want to dig into what makes it complicated.

30:24.160 --> 30:35.080
But I'm curious with what you stated, kind of the broader history of explainability or

30:35.080 --> 30:42.920
transparency requirement by other regulations, did you generally feel like all of the

30:42.920 --> 30:49.760
hullabaloo about GDPR and its implications for machine learning and deep learning and

30:49.760 --> 30:56.120
innovation and all this stuff like do you feel it was overblown or based on some of the

30:56.120 --> 30:59.920
challenges that you're aware of appropriate?

30:59.920 --> 31:07.560
So I think the reactions to GDPR in general is appropriate in the sense that the law

31:07.560 --> 31:12.680
is not radically different from the existing national laws that the regulation is meant

31:12.680 --> 31:13.680
to replace.

31:13.680 --> 31:18.520
So in the European Union, a regulation refers to a law that is standardized across all

31:18.520 --> 31:19.880
member states.

31:19.880 --> 31:24.280
There was something called the data protection directive for more than 15 years I think

31:24.280 --> 31:28.760
that was a sort of earlier version of what is now GDPR.

31:28.760 --> 31:33.040
The difference is that the directive was sort of guidance from member states and they

31:33.040 --> 31:37.720
were expected to sort of follow similar rules, but it wasn't standardized across all

31:37.720 --> 31:38.720
of Europe.

31:38.720 --> 31:44.360
The regulation was updated to kind of deal with some new problems, but the main radical

31:44.360 --> 31:50.920
change between existing laws and the regulation was financial penalty.

31:50.920 --> 31:57.200
So the reason is appropriate, I think, to actually think through these things is whether

31:57.200 --> 32:02.880
or not you care about these things from a kind of normative perspective or making sure

32:02.880 --> 32:05.200
that you obey the law.

32:05.200 --> 32:10.560
Companies actually now face genuinely significant financial penalty for failing to comply with

32:10.560 --> 32:11.560
the law.

32:11.560 --> 32:17.680
My sense is that the motivation for taking this serious, much more seriously anyway than

32:17.680 --> 32:22.160
it had been in the past is less that the law has changed dramatically.

32:22.160 --> 32:27.480
It's much more that the kind of consequences for failing to meet the law are now significantly

32:27.480 --> 32:28.960
more severe.

32:28.960 --> 32:34.120
So if you had been following the law already, which you should have been, this will not

32:34.120 --> 32:36.920
actually require a radical change.

32:36.920 --> 32:41.400
But the fact of the matter is that most people were not even really aware of the law, let

32:41.400 --> 32:43.320
alone following it.

32:43.320 --> 32:47.160
So to me, that really accounts for the difference.

32:47.160 --> 32:51.840
Having said all that, I still think that the main thing people at least were concerned

32:51.840 --> 32:58.360
about when it came to machine learning was this provision that required that you explain

32:58.360 --> 32:59.360
the decisions.

32:59.360 --> 33:06.080
So for certain types of decisions, automated decision making in general is forbidden

33:06.080 --> 33:08.960
unless people give consent.

33:08.960 --> 33:14.320
And even when they give consent, you still have to be able to explain the decision.

33:14.320 --> 33:20.520
And as I mentioned, there's a sense in which perhaps there is a trade-off between performance

33:20.520 --> 33:22.200
and explainability.

33:22.200 --> 33:28.480
So even if this wouldn't necessarily prohibit machine learning, it might be a constraint

33:28.480 --> 33:34.520
on the complexity of the model in order to make sure that it remains explainable.

33:34.520 --> 33:38.120
And that that might mean a degradation in performance.

33:38.120 --> 33:41.920
My sense is, though, that depending on how this law is interpreted, and this is also

33:41.920 --> 33:48.120
something we go into in the paper, you could potentially satisfy this requirement without

33:48.120 --> 33:54.000
necessarily building a model that is sufficiently simple that even a layperson could be able

33:54.000 --> 33:56.000
to look at it and understand it.

33:56.000 --> 34:00.840
There might be other ways of providing explanations, which still satisfy the law that don't require

34:00.840 --> 34:03.320
this potential trade-off.

34:03.320 --> 34:07.800
Can this bring us back to the point that you were about to make about looking at the details

34:07.800 --> 34:10.080
of complying with these various laws?

34:10.080 --> 34:12.120
Yeah, it's certainly in that direction.

34:12.120 --> 34:20.360
So I think the hope is that by explaining how decisions are made, you will know whether

34:20.360 --> 34:23.520
or not that is a good way of making decisions.

34:23.520 --> 34:29.240
So explanations are sort of a mechanism to check for other things you care about.

34:29.240 --> 34:35.760
Check to see that the decision making process, or in this case, the model, is taken to consideration

34:35.760 --> 34:39.080
things that you think of as being legitimate and relevant.

34:39.080 --> 34:41.080
So it should be considering these factors.

34:41.080 --> 34:43.560
So I can check, is it considering these factors?

34:43.560 --> 34:48.320
It could also be a way to check that it's not taken to consideration things that it shouldn't

34:48.320 --> 34:49.320
be.

34:49.320 --> 34:53.240
So it shouldn't be considering explicitly things like race, or it shouldn't be considering

34:53.240 --> 34:57.040
things that are arbitrary or clearly irrelevant.

34:57.040 --> 35:01.680
One of the challenges here is that sometimes you might be in a situation where even if

35:01.680 --> 35:08.720
you explain how the model makes the decisions, you as a human may not have any good intuitions

35:08.720 --> 35:13.640
for whether or not that is a reliable or sound basis for decision making.

35:13.640 --> 35:18.600
So one of the reasons people are interested in machine learning is that they can uncover

35:18.600 --> 35:22.760
patterns and data sets that would just escape humans' attention.

35:22.760 --> 35:27.000
And one would really be able to figure out that there are some kind of subtle signal across

35:27.000 --> 35:33.000
10,000 features that none of which on their face seem particularly relevant to the task

35:33.000 --> 35:34.920
at hand.

35:34.920 --> 35:39.880
So if it turns out, however, that the model has found such a signal, you could potentially

35:39.880 --> 35:44.000
try to explain the ones that are relevant to any particular decision, but even if you

35:44.000 --> 35:49.200
did, it would be potentially impossible for humans to know whether or not that's reasonable

35:49.200 --> 35:52.040
or really appropriate.

35:52.040 --> 35:56.080
So the example, I'll just come up with a kind of toy example here.

35:56.080 --> 36:02.160
If it turns out that the way you tie your shoes is predictive of some kind of performance

36:02.160 --> 36:05.680
on the job on its face that seems sort of laughable, right?

36:05.680 --> 36:09.400
Like why should that matter to my job performance?

36:09.400 --> 36:13.920
But let's just say for the sake of argument that it's a pretty robust finding that like

36:13.920 --> 36:18.320
the data really supported and wants you to play the model in practice, it actually shows

36:18.320 --> 36:20.920
that it does a pretty good job.

36:20.920 --> 36:25.000
On what basis do you actually say this is a good or bad model?

36:25.000 --> 36:29.640
It's good in the sense that it's potentially accurate, but it's not bad in the sense that

36:29.640 --> 36:34.400
it's choosing to pay attention to something irrelevant or obviously unfair.

36:34.400 --> 36:39.840
It's unsettling because it has discovered that there's something relevant in a feature

36:39.840 --> 36:43.680
that we as humans just cannot see is possibly relevant.

36:43.680 --> 36:46.040
And this is, I think, one of the real challenges here, right?

36:46.040 --> 36:51.000
So even if we use some of these awesome machine learning techniques to give explanations of models,

36:51.000 --> 36:55.440
they may not help us as humans to be able to assess whether those are even reasonable

36:55.440 --> 36:58.360
things to rely on to make important decisions.

36:58.360 --> 37:05.520
I think one of the interesting things here is some of the work that is going into kind

37:05.520 --> 37:12.040
of as opposed to trying to make your models, your fundamental fairness technique, being

37:12.040 --> 37:18.400
one of trying to make your models blind to factors like race or gender, actually taking

37:18.400 --> 37:26.600
part of this fairness challenge to build into your models, trying to predict these things

37:26.600 --> 37:33.280
like race and gender as part of the decision making and using whether the models can predict,

37:33.280 --> 37:40.840
race, for example, in the features that you kind of build into your model as an indicator

37:40.840 --> 37:46.760
that your model may be surreptitiously kind of making decisions based on race, right?

37:46.760 --> 37:50.040
It's there in the signal, it's just not obvious to you.

37:50.040 --> 37:53.680
Yeah, and so this is exactly the approach that some people are taking.

37:53.680 --> 37:55.440
It's an interesting problem, right?

37:55.440 --> 38:01.360
So you say, well, my model is non-discriminatory because it doesn't consider race or gender

38:01.360 --> 38:05.960
explicitly, but it turns out that other features are highly correlated.

38:05.960 --> 38:10.680
So maybe what you want to do is strip out those features that are highly correlated.

38:10.680 --> 38:15.160
This becomes a kind of impossible exercise at some point because with sufficiently

38:15.160 --> 38:21.720
rich data sets, it's almost certain that these kinds of details about you will be reflected

38:21.720 --> 38:24.480
in the other features that you have.

38:24.480 --> 38:28.440
And so it's not so obvious at what point you really are supposed to stop, right?

38:28.440 --> 38:33.400
Like how many things you'd remove from your model until you feel comfortable.

38:33.400 --> 38:37.800
And it's interesting because it actually relates to the other issue of explainability that

38:37.800 --> 38:39.520
we were just discussing.

38:39.520 --> 38:45.440
And the simple fact that some feature is correlated with race or gender on its own is not enough

38:45.440 --> 38:50.560
to say that it's illegitimate or illegal to actually consider that feature, right?

38:50.560 --> 38:54.480
And the reason for that is that there just happens to be inequality in society.

38:54.480 --> 39:00.200
Some people possess features at a certain value, at a higher or lower rate than others.

39:00.200 --> 39:02.480
That's a fact potentially of the world.

39:02.480 --> 39:08.000
And saying that this kind of difference in the value of this feature means that we shouldn't

39:08.000 --> 39:13.680
be actually considering that feature is not itself a sufficient argument, even when it

39:13.680 --> 39:17.680
comes to like cases around discrimination.

39:17.680 --> 39:18.920
And this is a complicated issue, right?

39:18.920 --> 39:23.760
Because it may well be the reason that some feature actually is correlated with race,

39:23.760 --> 39:28.120
for instance, is because of a long history of racial discrimination.

39:28.120 --> 39:32.840
So for instance, zip codes, right, they can be very informative when trying to predict

39:32.840 --> 39:38.800
the value of someone's home, but of course at the same time zip codes are highly correlated

39:38.800 --> 39:45.880
with race because of a long-term race-based discrimination in housing, right?

39:45.880 --> 39:47.840
So this is a tricky problem.

39:47.840 --> 39:52.320
In other situations, what happens is that people might understand if I'm talking to people

39:52.320 --> 39:57.200
in practice, is that they will want to find out which features are in fact correlated

39:57.200 --> 39:59.640
with race or gender in their model.

39:59.640 --> 40:03.040
And rather than just stripping them out because they happen to be highly correlated, they

40:03.040 --> 40:04.840
actually will just go look at them.

40:04.840 --> 40:10.080
They'll have a kind of rank-ordered list where the top is those features have been most

40:10.080 --> 40:14.240
correlated with the sensitive feature race or gender would have you.

40:14.240 --> 40:17.240
And then they look at it and they say, is it okay?

40:17.240 --> 40:23.600
Is it like reasonable to consider this feature, even though it is highly correlated with

40:23.600 --> 40:25.400
race and gender?

40:25.400 --> 40:28.320
And in some cases, like the zip code example, you might say no.

40:28.320 --> 40:32.000
You know, there's obvious historical reasons why this is not acceptable.

40:32.000 --> 40:35.440
But in some cases, you might say, yes, you might say, well, this doesn't seem to be the

40:35.440 --> 40:38.280
result of some kind of past injustice.

40:38.280 --> 40:43.320
It may just reflect some true difference in the distribution of this feature in the population.

40:43.320 --> 40:47.320
So we're going to stand by its relevance for the decision at hand.

40:47.320 --> 40:49.720
What's an example that falls into that category?

40:49.720 --> 40:52.680
A good example of this might be something like this.

40:52.680 --> 40:54.200
What university do you go to?

40:54.200 --> 41:00.400
I'm a person trying to figure out who to hire and my model, maybe unsurprisingly, assigns

41:00.400 --> 41:05.120
a lot of significance to people who graduated from, I don't know, the Ivy League, right?

41:05.120 --> 41:09.720
But it may well be that the actual people who graduate from the Ivy League tend to be

41:09.720 --> 41:12.680
disproportionately white, right?

41:12.680 --> 41:16.800
And you might say, well, let's actually not consider this because in a way, it's sort

41:16.800 --> 41:22.920
of like saying, if you're white, you have a better chance of succeeding on this job, right?

41:22.920 --> 41:27.960
If we wanted to tell some story yet, we could about like why it is that the population

41:27.960 --> 41:29.960
in these schools looks the way it does.

41:29.960 --> 41:33.160
And we could potentially get to a point where we feel like it's in fact reasonable to

41:33.160 --> 41:38.720
say, don't consider the university someone want to, but I think a lot of employers would

41:38.720 --> 41:46.000
make a reasonable and plausible case for the relevance of the university you graduated

41:46.000 --> 41:47.000
from, right?

41:47.000 --> 41:51.080
You would say, no, no, no, no, no, like these are actually like reasonable markers of

41:51.080 --> 41:55.040
some lines, performance, like the performance and potential.

41:55.040 --> 41:58.520
And so it's reasonable for us to consider it.

41:58.520 --> 42:04.440
And so, you know, with all that, like where, where does that leave us in terms of, you

42:04.440 --> 42:11.480
know, there's certainly a lot of kind of potential thick gray lines here in terms of the counterpoint

42:11.480 --> 42:17.080
to, I think that example was an example where there is some historical, you know, evidence

42:17.080 --> 42:22.840
of historical discrimination, and certainly that's the case in this university example

42:22.840 --> 42:23.840
that you gave.

42:23.840 --> 42:25.960
So it's not quite the opposite.

42:25.960 --> 42:31.280
Are there concrete examples of how folks have parsed through all of this with some kind

42:31.280 --> 42:36.320
of framework, or is it kind of everyone making a judgment call based on what they think

42:36.320 --> 42:37.320
is right?

42:37.320 --> 42:38.320
That's a great question.

42:38.320 --> 42:41.520
It's a really good way of putting, I think, the current state of affairs.

42:41.520 --> 42:50.480
So I think for some people, there should be some kind of bright line, and that the kind

42:50.480 --> 42:57.040
of university using, you know, your alma mater as a way to determine whether to hire you

42:57.040 --> 43:00.440
should be obviously reasonable, right?

43:00.440 --> 43:04.640
Then there are other people who I think take the view that we, you know, there's lots of

43:04.640 --> 43:07.960
reasons to be suspicious of the admissions policies of those places.

43:07.960 --> 43:12.680
There's lots of reasons to be even more suspicious of the quality of the high school education

43:12.680 --> 43:15.480
that people receive to prepare them to apply to college.

43:15.480 --> 43:16.920
And you can go back even further, right?

43:16.920 --> 43:21.440
The disadvantage you face as a person earlier in life that kind of sets you on this particular

43:21.440 --> 43:22.760
course.

43:22.760 --> 43:28.200
So you know, this may be frustrating, but I think ultimately there are going to be these

43:28.200 --> 43:33.600
ongoing debates around how to even parse this issue, right?

43:33.600 --> 43:37.800
For some people, it will be clear cut that there are certain factors that despite how

43:37.800 --> 43:43.360
correlated they are, that they are legitimate to consider when building these models.

43:43.360 --> 43:47.560
For other people, you know, they can make very strong arguments about the need to actually

43:47.560 --> 43:54.120
use the model development process to compensate for the unfair disadvantage that people had

43:54.120 --> 43:57.040
suffered earlier in their lives.

43:57.040 --> 44:00.680
And ultimately, this is not a machine learning debate, right?

44:00.680 --> 44:04.520
This is not something that is peculiar to building machine learning models.

44:04.520 --> 44:09.560
This ultimately is just the kind of longstanding debate that people have had in general about

44:09.560 --> 44:14.280
the fairness of decision making and certain settings about what is the appropriate role

44:14.280 --> 44:17.000
of discrimination law in general.

44:17.000 --> 44:22.120
So it's unsurprising ultimately that some of these things are not settled or are not

44:22.120 --> 44:26.160
going to be settled in part because people have been arguing about this for at least 50

44:26.160 --> 44:29.960
years when it comes to discrimination law and for millennia when it comes to questions

44:29.960 --> 44:30.960
around fairness.

44:30.960 --> 44:36.480
So did we cover all of the points that you wanted to cover with regards to your paper?

44:36.480 --> 44:42.560
Yeah, I mean, I think the final thing I would just say about is there's going to ultimately

44:42.560 --> 44:47.800
be, there's going to be situations in which the attempt to achieve fairness will require

44:47.800 --> 44:49.480
explanations, right?

44:49.480 --> 44:53.560
You actually know whether or not something is a reasonable thing to consider.

44:53.560 --> 44:58.520
You need to be able to explain what the model is doing and you need to let humans actually

44:58.520 --> 45:03.080
look at it and see if they can kind of weave some story that makes it feel like a reasonable

45:03.080 --> 45:05.760
basis for making these important decisions.

45:05.760 --> 45:13.040
In some cases, we might say the effort to get questions of fairness through explanations

45:13.040 --> 45:14.040
is misguided.

45:14.040 --> 45:18.640
Maybe what we should do instead is just abandon these requirements for explanations and

45:18.640 --> 45:22.240
focus on providing kind of formal fairness guarantees.

45:22.240 --> 45:27.040
Say, you know, we just want to ensure that whatever model we build, we can prove we'll

45:27.040 --> 45:30.160
not have certain problems, right?

45:30.160 --> 45:34.520
And we can do that more directly rather than relying on explanation.

45:34.520 --> 45:38.320
And this just sort of summarizes, I think, the point I was making a moment ago, which

45:38.320 --> 45:43.640
is that this, I think, just sort of ultimately depends on your perspective.

45:43.640 --> 45:47.400
For some people, it will feel inadequate to just provide guarantees.

45:47.400 --> 45:52.560
I can imagine myself actually feeling pretty dissatisfied with something that said, like,

45:52.560 --> 45:57.760
this system is certified fair, but we're never going to tell you how it makes its decisions,

45:57.760 --> 45:58.760
right?

45:58.760 --> 46:02.680
At the same time, I think there's a lot more you can achieve with these kind of formal

46:02.680 --> 46:07.480
approaches to fairness than what people expect they will get out of explanations.

46:07.480 --> 46:10.680
And so I just think there's a role here for both things and an opportunity to spend a

46:10.680 --> 46:14.000
lot more time figuring out when each of those is most appropriate.

46:14.000 --> 46:19.400
Well, Salon, thank you so much for taking the time to chat with me about this service.

46:19.400 --> 46:22.720
It's super interesting and super important as well.

46:22.720 --> 46:23.720
Great.

46:23.720 --> 46:24.720
Yeah, thank you.

46:24.720 --> 46:26.720
I really enjoyed it.

46:26.720 --> 46:28.520
All right, everyone.

46:28.520 --> 46:30.280
That's our show for today.

46:30.280 --> 46:36.560
For more information on Salon or any of the topics covered in this show, visit twimlai.com

46:36.560 --> 46:39.560
slash talk slash 219.

46:39.560 --> 46:57.800
As always, thanks so much for listening and catch you next time.

