All right, everyone. I am on the line with Beaty Chen. Beaty is a PhD candidate in the Department
of Computer Science at Rice University. Beaty, welcome to the Twomo AI podcast. Thanks.
So I had the opportunity to hear Beaty's presentation at the systems workshop.
MLs always get confused. ML for system systems for ML. This was systems for ML, correct?
Yeah. They changed the name recently to we submit this paper to the conference. They first
have it called MLs. And then they later change it to system because the trademarks stuff.
Yeah. Well, I think there are two different workshops with very similar with all the same
words, but in different orders. Yes. But in any case, you presented at the workshop on your
paper slide, which we will go into. But before we do that, share with us a little bit about your
background, what sparked your interest in machine learning, particular on kind of this intersection
of hardware and algorithms. How did this all come about? Sure. So before I study my PhD at rice,
I would have my anagra in University of California Berkeley. And I was actually doing some
network's research. It's kind of still computer science, but a little bit irrelevant.
But that kind of yeah, that actually I did my grad my grad work and networks also like
stochastic modeling and all that kind of stuff. Is that the stuff you were doing?
No, not the same different kind of networks. Yeah. Okay. Yeah. So we're kind of doing like a
software defined building. Yeah. So that's what's kind of close to the AI part because you want
to automate it in some process and you want to predict something for people. Yeah.
Yeah. Then when I come to rice, I get interested in the machine learning algorithms because I saw
I always have this in mind. All the problems we're trying to attack in computer science,
not all like most of it, the bottlenecks are usually the computation. Not all the problems are
showing this part, but if you think about it, we usually have a brute force way of doing things.
And if we do have the computation of power or we have the time to achieve it, then the problem
stops. But the problem is we usually don't have that time and computation of power. So for the
algorithm part, what is good is reducing from big O N square to big O N or big O N to cancel
in is usually making that possible. So on the way I'm doing research is kind of I always think
about what is the brute force way, what is the naive way I can chip this and can I speed it up?
Either in time wise or reducing the memory, any part that is blocking this happening
to produce the optimal solution and we can do approximation. And that's why I kind of doing all
the randomized algorithms in my PhD because that's going to help with the memory and time.
Meaning the algorithms are take advantage of randomization or you just are working on a bunch
of random different projects. That is also true because you need our special weapon,
this randomized the algorithm called locality sends to be hashing all kind of applications
to tackle different problems. Both. Got it. Got it. So this locality hashing is kind of well
obviously at the core of the paper you presented at the MSS workshop slide. What is the full name of
that paper? Let me see we changed a bunch of times. I think on the first version we're trying to
saying that we're defending for the smart algorithms over specialized hardwares. And then we found
a kind of offensive to some people because we're not saying hardware development is not good. We're
just saying that we also need more progress on the algorithm side. So we're just changing the name
back to we're beating like side trying to beat GPU over like CPU stuff. So it's like the result
of the paper. So nobody gets offended by the CPU GPUs beating up on CPUs except Intel, right?
Yeah, we actually have three more collaborators after the first version of the paper came out.
They also help us further speed it up and by using all those Intel tricks.
Before we get into the kind of the core innovation of the paper, what is the motivation? You've
hinted at this also. Everything takes too long but maybe talk a little bit more about the specific
problem that you're trying to solve with this paper. Right. So we all know that now all the
applications like vision and LP and they all kind of have the state of art by using the neural
networks. But the problem one bottleneck of the neural network is the computation. That's why
the inspiring GPUs were invented and then people have the computation of power to kind of do this
all complicated computations. So the major bottleneck is still the matrix multiplication.
And if you can do like large matrix multiplication really fast, then that's kind of solving
half of the computational problem in your networks. And that's why we started from those. The problem
which is like called extreme classification. So basically, for example, you have your Amazon
user. So you're typing something like I want like key shoes. And then in the back end, hopefully
you're going to have a black loss which can help you decide which products to show to you.
Then this kind of reduced to like extreme classification because you're typing a query that's
your query belong to for 1 million classes, which one is it? One class can be added as shoes.
One class can be like Nike run issues like that. So for this kind of problem, the major computation
is the last layer because you have 1 million classes. Then you have to do all this,
the major computation over there. And then we're thinking about is GPU the only way to solve this
problem. And because we realized for this kind of problem, even GPU, the speed up is not enough
because like the high memory excess. And then we're thinking about all the computations necessary.
Then the answer is actually no because majority of it is redundant because in the neural networks,
if like the heater layers, usually you care about the high activations, which is like to
inner product producing high results. And for the last layer is like even worse because for those
kind of extreme classification, what you care is about Nike shoes, Adidas shoes, but you don't
care about a headphone. So those irrelevance of what the computation is kind of wasted.
But the problem is before you do the computation, you don't know which part are important,
which part does not. And then the hatching algorithms is exactly good for these kind of situations,
kind of predict which computation are going to give you like high density so that you just compute
those to go through approximation of the few for the full computation. And then in this way,
all the computations in the neural networks will be sparse and including the backwards.
So that's how we come up with the problem and find the solution.
So you mentioned hatching algorithms are useful here and telling you which of your
computations are going to be useful. Why and how is that? Because usually locality sensitive
hatching is very useful for the search or ranking industry. The reason why it is good for finding
the nearest neighbors. And in the in the neural network space, what is near neighbors is like if two
vectors attend to have the similar direction, then the inner product will tend to be higher.
Let's assume that a uniform norm here. So in this way, vector A and B are neighbors
if they can produce high inner product. The hatching is kind of if you know vector A,
they will, they will, we can spend cash in time find all the b's which are close to a
neighbors of A and producing those b's which has the higher inner products with A. And that's why
I call it like a useful algorithm exactly for this kind of situation. Got it. So locality sensitive
hatching, that's something that has been around. That's not something you created. Yeah. That's
been like 20 years or so. Been around for 20 years. And what are some of the other applications
in which it tends to come up? You mentioned search. Yeah. Usually so for locality sensitive hatching,
one big application is if you have let's say 1 million images or 1 billion images, you can't
and then you have a image of query on hand, for example, a cat. You cannot search linearly for
the 1 billion images in a database because that's kind of cause you take a long time. Yeah.
So one, the only smart way is you save the 1 million images based on some kind of structures,
which we call it like neighbors goes to the same bucket so that whenever you have a query,
they will quickly use the same way to compute the hash code and go to the bucket which contains
all the neighbors so that your search time is bigger one. Otherwise, when you type something
on Google, it's probably take you ages to search for all the information currently we have now. So
we have to reduce the search time to big old lines that have big old end and that's all the
hashing algorithms are useful. Producing the hash is it kind of a deterministic process or is
there a formula or is it a learned process? Are you learning the hashes across the data set?
So there are two categories. One is the learned one and we're actually on the other side is
like data independent. But the hash functions are not deterministic is joined from some distribution
and all the hash functions are all in guarantee that the collate like the for example, you have two
item are exactly the same and then the probability that searching that element will be one. But if the
element has some minor differences, then the probability will be different. So by calling probability
is each time you generate the hash function is probably different ones. So we can only say that
it's with high probability we're going to retrieve the neighbors. It seems like if the hash function
is going to determine the proximity of different pieces of data it needs to know something
structural about the data and I'm wondering then does that mean that you have to do a lot of
work up front to create this hash function based on a given data set and then anytime you want to
look at a different type of data do you have to redo that? No, but we actually provide the freedom
for users to implement their own hash functions. So there are four things we're providing for
different kind of similarities. I think for hash functions, there are two lines over the first
one is the learned hash functions. We're currently not going to that line but people have freedom to
adding those solutions to our framework as well. But the second one is we're focusing on the
data independent one. By data independent, I'm saying that not only based on the data but also
based on different similarities. For example, the most useful ones in our framework currently
is cosine similarity. I would just talk about if you want two vectors to determine which vectors are
around one query vector, then cosine similarity might be the most useful one because that's kind of
determining the angle, which is the direction. And another one is like the ranking. So they don't
care about the normal vector or like the sequence of the vector, but they care about the rank like
for a given vector. The first one, the first dimension is larger than the second, the second is
larger than the foot. Like this kind of preserved order, that's extremely useful for images.
And there are a third one, which is called main hash, which is kind of preserved the
jacot similarities, like for a two given set is preserved the similarity between it. So it's
actually for different similarity and different kinds of data, you can use different kinds of
hash functions to achieve the same guarantees. And I'm recently also getting to the learn hash
function that part, but the one particular problem that is extremely trouble our framework is,
our framework is not, we're going to change the hash functions or review the hash tables once in a
while because we want to preserve the randomness, but the rebuilding part for those learned hash
functions will be very long. Yeah, so that's a good thing about the data independent hash,
because it is not time consuming. But the interest in the learned hash functions is that they
might be more accurate or allow you to express different types of similarity. Yeah, that is
correct. That's why we are seriously considering this case in our framework, because our frame
ultimately wants to spend very less time predicting which neurons should be active, and then do
all the computation on CPU. There's actually very hard tasks. You have to do it very quickly.
Otherwise, that overhead is going to kill you. So that's why I'm talking about like for the
learn hash function, although it's very accurate, it's going to help us like spend maybe 10 times, if the
current time of computing hash functions, and that's going to make the framework maybe comparatively
okay with GPU, but not obvious speed up. So the solution is doable. I think there's a time and
accuracy trade-off here, but we think the independent hash functions are along the sweet spots.
And so what's the relationship between the hash function and the idea of adaptive sparsity?
That's a concept that you mention in the paper. Right. So just considering the random dropout
is actually like each time you're just giving sparsity to the neural network randomly,
so that you're is highly possible that you're going to meet some important parts if you do that
randomly. So by adaptive sparsity here is that's why like for the random dropout, the sparsity can
maximum goes to like 50% or so, otherwise you will decrease your accuracy. But we do another
experiment and it's actually finding a lot of like previous papers to if we can compute all the
computations and we only take maybe 5% or 1% of nodes as the new of the active nodes, then the
accuracy will not drop. But the problem is you have to make the huge computation to know which ones
we're going to produce you the top ones. So that's inspiring us like we do approximate like 5%
or 10%. But so the speed is kind of close to the random dropout, but the effectiveness of it
is close to the adaptive top K. So that's why we find the sweet spot in this too.
Does that mean that you're applying the location sensitive hashing in two places?
One at the very end for your classifier and then also more deeply in the dropout.
So dropout is actually in parallel with our framework. I'm just giving an example of how the
hashing is going to help. If hashing is doing a random selection of the nodes is exactly the same
as the dropout. But if is computing the real top K, then that's called the the adaptive
sparsity. But it's somewhere in between it's using the same time is very efficient as the random
dropout because the query only take bigger one time. But it's going to retrieve you the nodes that
is highly possible in that top K. So I'm just like giving an intuition of what hashing with selecting.
It's nothing but just selecting adaptively the active nodes for each layer. So what is very
useful in the case of extreme classification is because of the full computing like the majority
of the computation is in the last layer because of the number of classes. So that's going to reduce
a lot of computation. But for the usual fully connecting neural network, each layer we can
build different hash tables and we can do is sparsity over each layer. So that's not restricted
to the last layer. Are there other elements of this approach that we should be sure to cover?
There's the application of the hashing. Do you have to do anything special in terms of your
data collection or preparation or should you just be able to apply this technique?
So we're actually exploring two directions of extension to this work. So this is our current
framework support the fully connected neural networks. But people are more interested in when they
have like the NLP or vision problem they want the CN or LSDM. Currently what we can do is at least
for last layer we can apply the same technique. But the problem is for CN or LSDM in a mid like those
heat layers we have to investigate there to see what advantage is the hash in the approach.
And for not a line of work we're thinking about the distributed because it's well known that for
the distributed setting the computational bottleneck is the communication. And our slide has
is natural advantage here because of the we're only selecting for example 1% or 5% of the active
nodes for each layer so that in the back propagation the gradient updates is also very sparse.
So that for the distributed setting this is natural very efficient.
For the same reasons that we've talked about previously intelligently using the hashing to kind
of identify and strip out the sparsity. Yeah. How have you kind of measured the results and
compared this approach to others and then what kind of results have you seen? Yeah for the results
part that's the most impact for people care about. So for GPU comparison we're using the current
state of art framework TensorFlow and also use also run the same program on a via 100 GPU which is
currently kind of the state of art people use to run to perform all the experiments for those
applications. And also for TensorFlow CPU we also try to see that for all those CPU tricks
they're using are we beating them? And obviously you will be slower than the TensorFlow GPU
but we still do some comparison there to see like there's three lines which ours is the most
efficient and then TensorFlow GPU on via 100 and TensorFlow CPU on the same CPU machine we're using
because we want to still compare with the CPU otherwise we can use a super powerful CPU machine
and then that's then using that to be the GPU is not necessary for people because we want people
to do the same thing on CPU which is not that costly as using like a via 100 or this kind of
expensive GPU machines. And also there's a nice finding about how many cores we include
in our CPU computation so that we can beat the TensorFlow GPU. It seems like 8 cores to 16 cores
is enough to outperform the same task on GPU via 100. So although we're in the in the paper
we're using like a 44 core machine but we do the obligation study to see where it intersects
the performance is actually 8 to 16 cores somewhere in the middle. So with 8 to 16 cores you're
able to to what outperform TensorFlow on a GPU? Yes. Okay. What's the problem that you're
that you're benchmarking this on? Those are extreme classification tasks. Oh, extreme classification.
And can you talk a little bit about are there established data sets and benchmarks for extreme
classification? Can you talk a little bit about those? Yes. So there's a whole repo about all those
for example the example that I gave earlier for Amazon data is usually like yeah you have query
and you have all the products that's like extreme classification or sometimes you have
a user typing query and then you have a search results for the finally the website you are
clicking. That's like another case. Yeah. So all those benchmarks including one repo, all the
extreme classification field like people use those as the benchmarks. For the Amazon you said
how many classes are there? For the data set we're using is 670K. Okay. And then use another
delicious data set that's like the one where you're predicting links. Yeah, it was like 200K.
So you've got a first set of diagrams where you've got the three lines and that one is looking at
the slide accuracy and then you have time on the bottom what is the time? Is that like convergence time
or else? Yes, like training time. Training time? That's a lot scale. Yeah. Okay. So you've got pretty
strong results here. Do you have you had anyone take this and implement it in practice?
Kind of an industry or for a kind of a live application? Yeah. I receive a couple of emails requesting
we have a license there and then people can implement it and Intel is extremely of course there will
be super interesting. Yeah, they're working on it to include to their some kind of libraries
in the future too. There's one thing that I want to share. So the all the speedouts we're getting
here is not about all those CPL plans or implementation tricks. We don't even use cool glass or this kind
of libraries for speeding up the computation which is use all the primitives and all the
computations we're saving here because of the smart algorithms. So because we're only choosing
five percent of the nose active nose so that the computation was saved. So I think this framework
if viewed like we can have further speed up if those techniques tricks are applying to this.
But this is just like a prototype which proves that we can do this with smart algorithms.
Kind of going back to the result you found that the I found the number here the convergence time
required is over two and a half times less than with TensorFlow GPU and then I was looking for your
accuracy number here. How does it compare in terms of accuracy? There's no accuracy drop.
Okay, so you're able to maintain accuracy but um, converge more quickly. Yes. So you've done your
benchmarking for this extreme classification task which has inherent sparsity to it. Are there
other types of problems that you think that this would easily lend itself to due to the because
they also have high degrees of sparsity or do you see this as you know are there ways to kind of
adapt this approach so that it you can apply it to use cases where they're not quite as inherently
sparse. So you're talking about the approach we're having, does that work? The approach you're taking
and the applicability and you know what are the requirements of the scenarios in which it would
apply is you know would they have to be as sparse as the extreme classification task or can
you see some ways that they can apply more broadly. You can be applied broadly as well as I
just mentioned for example, we're currently working on if we can use this speeding up on the
training of BERT which is very popular recently. So the difficulty of applying this to an
existing framework is we have to profile where is the computation bottleneck because speeding up
the non bottleneck won't help and because we're producing some overhead as well and that's not
gonna suite the whole process up anyway. So we have to find the bottleneck of the training and if
this technique can in this technique needs the sparsity. So if we do experiment saying that
we're replacing our algorithm with like the real top K and it's decreasing the accuracy too
much then it's not worth investigating. But if we're for example choose the real top 5% of the
active nodes and the accuracy still maintains, then in this case there's hope for us to do this
approximation because like the kind of like the ground choose works but now we just need to speed it
up. So it depends on the architectures of the network. So it's not something that you would
envision ever being kind of a generic tool in the toolkit that you would apply to different
types of problems. You have to kind of hand tailor the application of this technique to the different
to different algorithms and where they're bottleneck. Why I think this is still generic is because
in all those neural networks or architectures anyway reduced to the matrix multiplication problem.
Once that one is there this should be useful but it's just like we need to see which where do we
where does it worth it to apply to. And we're also looking for like a GPU implementation of the same
method as well because the major bottleneck for this technique currently going to GPU is because
of the hashing algorithm because for those many years hashing only has the CPU implementation
because it requires a lot of memory because it's remembering something right we save something
in the hash table. And recently we see a step forward to the dynamic hash tables and GPUs
from UC Davis folks. So for those I kind of were saying that it might be possible to implement
like a locality sensitive hashing on GPU and then this can further speed up GPU and that's like
also a possible future direction of the framework. Awesome awesome well maybe thanks so much
for taking the time to share a little bit about what you're working on. It's very cool stuff. Thanks.
