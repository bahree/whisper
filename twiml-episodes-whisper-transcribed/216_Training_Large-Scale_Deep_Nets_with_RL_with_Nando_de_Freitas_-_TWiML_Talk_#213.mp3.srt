1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,440
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,440 --> 00:00:34,520
I'm your host Sam Charrington. Today we close out our NURP series and our 2018 conference

4
00:00:34,520 --> 00:00:40,000
coverage with this interview with Nando DeFratus, team lead and principal scientist at Deep

5
00:00:40,000 --> 00:00:46,240
Mind and Fellow at the Canadian Institute for Advanced Research. In our conversation,

6
00:00:46,240 --> 00:00:50,600
we explore Nando's interest in understanding the brain and working towards artificial

7
00:00:50,600 --> 00:00:56,000
general intelligence through techniques like meta learning, fuchsot learning and imitation

8
00:00:56,000 --> 00:01:01,840
learning. In particular, we dig into a couple of his team's NURP's papers, playing hard

9
00:01:01,840 --> 00:01:07,480
exploration games by watching YouTube and one shot high fidelity imitation, training

10
00:01:07,480 --> 00:01:11,880
large-scale deep nets with reinforcement learning. Enjoy.

11
00:01:11,880 --> 00:01:17,920
Alright everyone, I am here with Nando DeFratus. Nando is a team lead and principal scientist

12
00:01:17,920 --> 00:01:25,640
at Deep Mind as well as a Fellow of the Canadian Institute for Advanced Research or CIFAR

13
00:01:25,640 --> 00:01:29,840
as you might know it. Nando, welcome to this week in machine learning and AI.

14
00:01:29,840 --> 00:01:32,880
Thank you. Thank you very much for having me here. It's such a pleasure.

15
00:01:32,880 --> 00:01:37,680
Absolutely. Absolutely. It's great to finally get a chance to speak with you. We've been

16
00:01:37,680 --> 00:01:43,720
trying to coordinate something for a while now, but fortunately NURP brings us all together.

17
00:01:43,720 --> 00:01:49,400
I'd love to hear a little bit about your background and your path into ML&AI.

18
00:01:49,400 --> 00:01:57,440
Yes, so I got into ML&AI a very long time ago. We're going back to prehistoric times here.

19
00:01:57,440 --> 00:02:03,280
So actually it was around, it was in 1994, when I was an underground at the University of

20
00:02:03,280 --> 00:02:10,440
it, Father's Rund in Johannesburg, South Africa. And Professor, my control professor, introduced

21
00:02:10,440 --> 00:02:18,040
me to this thing called Neural Networks. I was fascinated by neural networks and tried

22
00:02:18,040 --> 00:02:24,920
to learn as much about them as possible, 100 maps by propagation, and ended up implementing

23
00:02:24,920 --> 00:02:31,680
something that eventually went into hardware and it became a controller for pneumatic

24
00:02:31,680 --> 00:02:39,600
control valves. With that work, I eventually even made it to an international conference.

25
00:02:39,600 --> 00:02:49,960
That was, I still remember 26-hour trip from my home to my first conference in Washington.

26
00:02:49,960 --> 00:02:57,600
It was extremely intimidating because I was the skit from Africa. And there I was meeting

27
00:02:57,600 --> 00:03:04,720
all these big shots and people that I thought I would never be able to ever hang out with.

28
00:03:04,720 --> 00:03:12,960
They were like heroes to me. Some folks like David Hackerman, Underbarco and a few others,

29
00:03:12,960 --> 00:03:20,680
like very prominent people in the field. It was a great experience. It was intimidating.

30
00:03:20,680 --> 00:03:25,720
And when I went back to South Africa, that made me wanted to do a masters. And so I continued

31
00:03:25,720 --> 00:03:33,440
my work with neural networks. And eventually through that, I was able to apply to universities

32
00:03:33,440 --> 00:03:39,200
and I got lucky, got a scholarship to Cambridge, and I went there, and I continued working

33
00:03:39,200 --> 00:03:47,600
on based neural networks. And that was my PhD. And it became very statistical, ended up going

34
00:03:47,600 --> 00:03:53,840
to Berkeley. And by just fluke, my supervisor there, still at Russell, was in computer science.

35
00:03:53,840 --> 00:03:59,680
So I somehow became a computer scientist. And eventually after two years there, I moved

36
00:03:59,680 --> 00:04:04,680
to UPC where I was a professor of machine learning, introducing some of the undergrad courses

37
00:04:04,680 --> 00:04:11,480
that you probably have seen on YouTube. And it was there that I first started participating

38
00:04:11,480 --> 00:04:20,000
in the SIFER program. And this was organized chiefly by Professor Jeff Hinton. And the

39
00:04:20,000 --> 00:04:28,880
goal back then was to understand the brain. And this was so amazing because it was suicide

40
00:04:28,880 --> 00:04:36,640
in those days to put a grant with that title. You would not get tenure. This was like a crazy

41
00:04:36,640 --> 00:04:44,840
thing to do. But fortunately, some of the key leaders in Canada believed in that ambitious

42
00:04:44,840 --> 00:04:53,360
vision. They're still there and they continue inspiring us. And that's what we did. We embraced

43
00:04:53,360 --> 00:05:00,160
that vision. Our research sort of steered toward that at the time, more different, more

44
00:05:00,160 --> 00:05:07,200
difficult path. And now it seems it's very easy and retrospect. What we do. Yeah, and

45
00:05:07,200 --> 00:05:10,880
that's kind of how I got into deep learning and how I got here.

46
00:05:10,880 --> 00:05:15,920
Have you since returned to applications in control systems?

47
00:05:15,920 --> 00:05:21,240
Yeah, I continue doing a lot of that. So a lot of the knowledge that I gained from my

48
00:05:21,240 --> 00:05:29,600
undergrad in my masters in control, things about certainly the automatic thinking about

49
00:05:29,600 --> 00:05:34,960
our differential equations, knowing how to sort of map things of legacy domain and so on.

50
00:05:34,960 --> 00:05:41,800
But as well as being able to do things like PID controllers, those are still the things

51
00:05:41,800 --> 00:05:48,240
that we use today. Like when we try to control robots and we might use neural networks

52
00:05:48,240 --> 00:05:53,440
these days to sort of set up parameters, automatic, of different PID controllers. But you

53
00:05:53,440 --> 00:05:55,440
know, all that background has been very useful.

54
00:05:55,440 --> 00:06:01,680
Yeah, those are some of my favorite courses from grad school, PID controllers and hysteresis

55
00:06:01,680 --> 00:06:12,840
and all these different things. But I've had the impression that that stuff is, you know,

56
00:06:12,840 --> 00:06:18,800
works so well and there's such a need for more deterministic response for a lot of

57
00:06:18,800 --> 00:06:25,400
the applications of this that there hasn't been much in terms of neural network applications

58
00:06:25,400 --> 00:06:28,000
for that. But it sounds like you've been doing that for a while.

59
00:06:28,000 --> 00:06:36,760
Yes, no, there's quite a few people who work with this. So control and the sort of combination

60
00:06:36,760 --> 00:06:43,680
of classical control techniques with the new, sort of the reinforcement learning techniques.

61
00:06:43,680 --> 00:06:47,040
Yeah, it's actually a very fruitful area of research. And I think a lot of the work that

62
00:06:47,040 --> 00:06:52,880
folks like Peter Abel at Berkeley do, like Michael Thunderpan at UBC, Emma Chauderof at

63
00:06:52,880 --> 00:06:58,480
the University of Washington, it all sort of involves a sort of a bit of a marriage between

64
00:06:58,480 --> 00:07:03,560
the classical techniques and the new techniques. And I think there's still a lot we can learn

65
00:07:03,560 --> 00:07:09,480
from classical control as well in terms of robustness and constraints. Because as we build

66
00:07:09,480 --> 00:07:14,720
controllers, we want to make sure that they're safe, that they we can verify them and so

67
00:07:14,720 --> 00:07:20,440
on. So these, these continue being challenges that we have to address in order to deploy

68
00:07:20,440 --> 00:07:21,440
the technology.

69
00:07:21,440 --> 00:07:27,080
So tell me a little bit about your research interests. What are, it sounds like you kind

70
00:07:27,080 --> 00:07:32,120
of definitely a lot of different things, but how do you kind of characterize the general

71
00:07:32,120 --> 00:07:35,080
scope and direction of your research?

72
00:07:35,080 --> 00:07:43,120
I think fundamentally it's still the same that it's always been. It's trying to understand

73
00:07:43,120 --> 00:07:50,040
the brain, trying to understand what it is to be me, what it is for you to be you and

74
00:07:50,040 --> 00:07:59,040
for us to be engaged in this discussion. That's the real goal. And of course we have to

75
00:07:59,040 --> 00:08:04,880
sort of break this problem in order to be able to attack it. We have to think of like what

76
00:08:04,880 --> 00:08:09,280
question, how to formulate the question. And often that's what leads to things like

77
00:08:09,280 --> 00:08:14,760
RL, you know, you can specify tasks and cost functions. It's a language to talk about

78
00:08:14,760 --> 00:08:20,360
the problem. We can sort of think of it in terms of cognitive abilities that we have

79
00:08:20,360 --> 00:08:26,720
like memory or imagination, curiosity and so on.

80
00:08:26,720 --> 00:08:32,040
So there's many ways to attack the problem of understanding the brain. And I think this

81
00:08:32,040 --> 00:08:38,800
is a huge problem. And there's so much yet to be done. And I've picked a few directions.

82
00:08:38,800 --> 00:08:44,960
I see my colleagues have picked out the directions. And I think it's going to involve many

83
00:08:44,960 --> 00:08:53,000
perspectives for us to eventually come up with an understanding at many levels of abstraction

84
00:08:53,000 --> 00:09:02,400
of what it is in the sense to be human. Do you think that our understanding of the brain

85
00:09:02,400 --> 00:09:09,520
has benefited our understanding of machine learning or the practice of machine learning

86
00:09:09,520 --> 00:09:18,840
more or less than machine learning has benefited our understanding of the brain? That's an

87
00:09:18,840 --> 00:09:23,200
interesting question. So it is very clear that the understanding of the brain has helped

88
00:09:23,200 --> 00:09:31,640
us build a get here. It's a couple years, but actually four years ago, I went to Japan

89
00:09:31,640 --> 00:09:38,120
and I had the honor of meeting Professor Fukushima, who was the author of the New Cognitron,

90
00:09:38,120 --> 00:09:43,320
which is essentially the convolutional neural network architecture that we all use these

91
00:09:43,320 --> 00:09:50,880
days. Janakun added backpropagation to that architecture and that kind of is what brought

92
00:09:50,880 --> 00:09:59,960
us here. That was a success story and vision and so on. When I spoke to Professor Kitagawa,

93
00:09:59,960 --> 00:10:06,920
he mentioned that we're inspired him to do that work was the work of Hubel and Vizel

94
00:10:06,920 --> 00:10:14,400
and Neuroscience. When he worked at his institute, I forget the name of the institute, but they

95
00:10:14,400 --> 00:10:21,800
put neuroscientists and computer scientists together. It was through this combination

96
00:10:21,800 --> 00:10:27,320
of different disciplines working together that he found out about these results in Neuroscience

97
00:10:27,320 --> 00:10:32,280
and decided to code this model. So I think the neuroscientists had a much greater

98
00:10:32,280 --> 00:10:42,480
potential. I'm told a set of writers was also influenced by cortical models of cells and

99
00:10:42,480 --> 00:10:50,400
so on in order to propose LSTM, which also one of the key components of machine learning.

100
00:10:50,400 --> 00:10:56,600
Has it helped the other way around? That's a much harder question. I mean, clearly with

101
00:10:56,600 --> 00:11:02,880
the success of backpropagation and so on, a few researchers at MIT, for example, they

102
00:11:02,880 --> 00:11:07,520
went on and they were starting to use that new knowledge to explain how the visual cortex

103
00:11:07,520 --> 00:11:16,320
might be working and so on. I also think more recently, folks are starting to contract

104
00:11:16,320 --> 00:11:22,440
coding and so on. Trying to look at the research that has happened, not just in machine learning,

105
00:11:22,440 --> 00:11:28,760
but the combination of statistics and econometrics and machine learning and biostatistics, looking

106
00:11:28,760 --> 00:11:36,880
at things like causal modeling in order to go back and try to understand, to try to formulate

107
00:11:36,880 --> 00:11:42,080
the theory of how it is that we should approach neuroscience and what sort of findings we're

108
00:11:42,080 --> 00:11:47,440
looking for. So I think to some extent, it also has penned out the other way.

109
00:11:47,440 --> 00:11:54,080
Yesterday, at the poster sessions, I was checking out one of the posters that was making

110
00:11:54,080 --> 00:11:59,240
kind of a strong correlation between what's happening in the brain and image processing

111
00:11:59,240 --> 00:12:05,840
and CNNs and I always thought of neuro networks in general and CNNs as kind of quote-unquote

112
00:12:05,840 --> 00:12:11,920
inspired by the brain, but not really exhibiting a very strong correlation. But they were talking

113
00:12:11,920 --> 00:12:20,040
about layers as like V1, V2, V4, whatever, and IT, which presumably are related to a model

114
00:12:20,040 --> 00:12:25,240
of how we think the visual information is processed in the brain. I didn't realize that there

115
00:12:25,240 --> 00:12:28,680
was that strong correlation between these two fields.

116
00:12:28,680 --> 00:12:35,720
Yeah, there appears to be. Certainly, for example, it's SIFAR. So every year, before

117
00:12:35,720 --> 00:12:42,360
New Europe, there's another workshop that takes place. It's a two-day workshop called the SIFAR meeting.

118
00:12:42,360 --> 00:12:48,840
And actually, I just came from it. There, we always have a combination of invite people

119
00:12:48,840 --> 00:12:53,680
working in neuroscience as well as people working in computer science and engineering physics

120
00:12:53,680 --> 00:13:02,480
and some people from different areas, from causality, et cetera. And we try to sort of use

121
00:13:02,480 --> 00:13:10,000
everyone's insights to sort of advance what we're mainly to choose what to research areas

122
00:13:10,000 --> 00:13:17,160
to explore next. So I think neuroscience has always informed what we do in machine learning

123
00:13:17,160 --> 00:13:21,600
and machine learning, I think, also informed neuroscience. So certainly, at least at that

124
00:13:21,600 --> 00:13:23,600
meeting, that's the case.

125
00:13:23,600 --> 00:13:31,040
So the driving motivation is to try to understand the brain, so that kind of reverse part of

126
00:13:31,040 --> 00:13:38,560
the feedback loop. And one of the recent areas that you've been exploring to help get there

127
00:13:38,560 --> 00:13:46,240
is been imitation learning. And in particular, you contributed to a paper here that's focused

128
00:13:46,240 --> 00:13:51,080
on third-party imitation learning. Let's just dive in. Tell us about that paper. What's

129
00:13:51,080 --> 00:13:52,080
the motivation there?

130
00:13:52,080 --> 00:14:00,040
Okay, so I'm excited about imitation because imitation is something that we all do. A lot

131
00:14:00,040 --> 00:14:07,640
of the things we learn are through a process of imitation. And in particular, I'm interested

132
00:14:07,640 --> 00:14:15,760
in a form of imitation called fused shot imitation, which is sort of follows from, you can sort

133
00:14:15,760 --> 00:14:22,560
of think of it in terms of meta learning. So the idea of meta learning is evolution is

134
00:14:22,560 --> 00:14:28,680
a learning process that happens at a very slow scale and it produces biological machines,

135
00:14:28,680 --> 00:14:36,120
you know, us. Machines are capable of learning very rapidly with very few data. We don't

136
00:14:36,120 --> 00:14:43,880
experience that much data throughout our lifetimes. And so we kind of want to do the same thing.

137
00:14:43,880 --> 00:14:51,960
We want to sort of learn machines that can imitate very rapidly. So we trained them for

138
00:14:51,960 --> 00:14:58,720
a very expensive process, but then we can deploy this machine and someone can show, demonstrate

139
00:14:58,720 --> 00:15:05,560
something to this machine and then the machine is able to do that. Now then comes another

140
00:15:05,560 --> 00:15:13,080
question of when you demonstrate, your demonstration might be with different objects, so different

141
00:15:13,080 --> 00:15:20,600
hands. And so you need to now deal with the third person here. What we mean by that is

142
00:15:20,600 --> 00:15:26,240
that you might have learned to do the task using, I don't know, a robot hands. And now you

143
00:15:26,240 --> 00:15:33,320
have to watch human hands doing the task and then you have to, the robot has to imitate.

144
00:15:33,320 --> 00:15:39,520
So very much like when children can imitate a dog by using their foot to scratch behind

145
00:15:39,520 --> 00:15:46,240
their ear. Right. And they're able to remap from one type of body to another body quite

146
00:15:46,240 --> 00:15:56,200
easily. And the contrast is, I guess first person imitation learning, which is, for example,

147
00:15:56,200 --> 00:16:02,120
you're looking at the actions that someone is playing in playing a game and learning

148
00:16:02,120 --> 00:16:07,080
based on those actions as opposed to just watching them, quote unquote, from a distance.

149
00:16:07,080 --> 00:16:14,040
Correct. So you precisely said in first person in Atari, for example, if you're playing

150
00:16:14,040 --> 00:16:18,080
Atari, first person, you would collect data from playing the game in Atari. We tried

151
00:16:18,080 --> 00:16:25,440
to fit the model to it. In third person, what we do is we actually go to YouTube and we

152
00:16:25,440 --> 00:16:30,280
look at people playing the games. And it's third person because there's a bit of, there's

153
00:16:30,280 --> 00:16:35,520
a domain gap. You know, in YouTube, sometimes this video is, well, the video will definitely

154
00:16:35,520 --> 00:16:42,320
be of a different resolution, different color. It will have ads inserted. It has all sorts

155
00:16:42,320 --> 00:16:48,720
of other artifacts. And so we need to use, be able to remap from that thing in YouTube

156
00:16:48,720 --> 00:16:54,920
to our domain so that we can play a play the game. And the inspiration for this for me

157
00:16:54,920 --> 00:17:06,240
was like watching a child, my nephew. Spending a lot of time playing, what's that thing

158
00:17:06,240 --> 00:17:14,960
that gets played? Minecraft. So he spent a lot of time watching Minecraft. And I remember

159
00:17:14,960 --> 00:17:18,520
telling my brother, you're not worried about this and my brother's like, oh no, he's

160
00:17:18,520 --> 00:17:22,080
just actually very good. He does really well in school when he does this sort of thing.

161
00:17:22,080 --> 00:17:29,800
And what he does is he watches these videos and then he goes and plays. And so I realized,

162
00:17:29,800 --> 00:17:36,400
well, that makes sense. This is what we should do. And so then I, you know, I work with these

163
00:17:36,400 --> 00:17:41,920
very gifted scientists and engineers who took the idea and went and implemented this

164
00:17:41,920 --> 00:17:48,000
for in the context of Atari. I think this sort of an initial step, which is more than anything

165
00:17:48,000 --> 00:17:55,120
just a demo of what could be done. I think there's a lot of footage in YouTube for all sorts

166
00:17:55,120 --> 00:18:01,000
of things. And so the hope would be that we eventually could train, say, Robert Manipulate

167
00:18:01,000 --> 00:18:09,800
is or in simulation or with real robots to be able to watch YouTube and be able to even

168
00:18:09,800 --> 00:18:14,200
though there's a domain gap because it's, you know, it's, they're watching different

169
00:18:14,200 --> 00:18:21,960
human hands say tying knots. And then they would have to then tie knots themselves. That's

170
00:18:21,960 --> 00:18:26,800
not actually easier. Like, tying shoelaces, for example, is something that even six-year-old

171
00:18:26,800 --> 00:18:35,560
human struggle. Interesting. I think I've come across videos that or examples of research

172
00:18:35,560 --> 00:18:43,000
that is trying to do some of that kind of thing, but not with actual instantiated robots,

173
00:18:43,000 --> 00:18:49,880
but like animations. So like, watch a video of dancing and try to get the stick figure

174
00:18:49,880 --> 00:18:55,160
to dance or things like that. It seems like there are a lot of parallel activities kind

175
00:18:55,160 --> 00:19:00,000
of going after the same type of problem. Correct. A lot of this can be done in simulation

176
00:19:00,000 --> 00:19:07,680
in a sense. It's easier to do it in simulation. Robots are basically machines that complicate

177
00:19:07,680 --> 00:19:15,360
to this. And they're not produced by a typically a man. They're like cars, except that with

178
00:19:15,360 --> 00:19:21,240
cars, we have these big companies that produce these nicely engineered machines. And robots

179
00:19:21,240 --> 00:19:27,440
are more like cars when they started appearing in the world, you know, they sort of make shift

180
00:19:27,440 --> 00:19:33,400
Frankenstein type things put together. And so they break down a lot of the time. It's

181
00:19:33,400 --> 00:19:39,640
very hard to do robotics. The software is somewhat lagging and so on. But nonetheless,

182
00:19:39,640 --> 00:19:44,920
it's interesting. Manufacturing keeps being one of the automation and manufacturing

183
00:19:44,920 --> 00:19:50,680
keeps being one of the driving forces behind, you know, control theory. In fact, and control

184
00:19:50,680 --> 00:19:58,520
and machine learning. And so has your work in this area with playing the Atari games?

185
00:19:58,520 --> 00:20:04,520
Have you is that also still in simulation or do you have some robotic thing playing the

186
00:20:04,520 --> 00:20:11,320
game? So that's in simulation. But we have been doing other works also doing imitation.

187
00:20:11,320 --> 00:20:19,400
And in particular, doing this few short imitation, where we do, we use simulation. So physics

188
00:20:19,400 --> 00:20:27,400
accurate simulation through an engine called Majoko. It's kind of what the OpenAI GM uses.

189
00:20:27,400 --> 00:20:35,200
And then we also use real robots to do these sort of things. And so we've been very interested

190
00:20:35,200 --> 00:20:42,600
in this sort of few short imitation, which is basically imitation with a few data. The

191
00:20:42,600 --> 00:20:48,600
challenge here is and the challenge being the challenge of what I call few short metal

192
00:20:48,600 --> 00:20:57,520
learning is to get a machine and being able to sort of demonstrate something new to the

193
00:20:57,520 --> 00:21:03,920
machine with novel objects with novel motion and have the machine be able to do the same

194
00:21:03,920 --> 00:21:12,920
thing. So if you think so, I want to go beyond building machines like classifiers, like

195
00:21:12,920 --> 00:21:21,360
a CNN or and instead, they're learning machines. They're machines that I can give to someone

196
00:21:21,360 --> 00:21:27,920
at a factory. And that person can then modify the machine via demonstration or via some

197
00:21:27,920 --> 00:21:34,520
formal language or natural language and get the machine to do something new or do it slightly

198
00:21:34,520 --> 00:21:42,920
different. So I always think of the next generation of AI machines as adaptable machines or tools

199
00:21:42,920 --> 00:21:49,920
that learn. So it's very much like when we give tools to animators. I think to people in

200
00:21:49,920 --> 00:21:54,640
factories and so on, what we need to do is give them tools that they can adjust and adapt

201
00:21:54,640 --> 00:22:01,640
and become much more productive. So they don't have to do all this sort of low level coding

202
00:22:01,640 --> 00:22:08,200
in order to or change hardware, etc. In order to deal with the fact that maybe the

203
00:22:08,200 --> 00:22:16,160
lids of the water bottles have changed in size. So when we're learning as humans, one

204
00:22:16,160 --> 00:22:23,160
of the things that we get to take advantage of is this relatively vast amount of quote unquote

205
00:22:23,160 --> 00:22:29,920
common sense that I guess you can kind of think of as like transfer learning in a sense.

206
00:22:29,920 --> 00:22:36,000
In the context of a few shot learning is there in your research in particular, is there

207
00:22:36,000 --> 00:22:44,920
a starting place? Is there some base that we've taught the agent via you tell me or we

208
00:22:44,920 --> 00:22:48,840
just starting from scratch with a few examples and saying go figure this out like what's

209
00:22:48,840 --> 00:22:54,360
the base? Yeah, so you're absolutely right. Transition is very important. So we probably

210
00:22:54,360 --> 00:22:59,600
need to unpack what I mean by few shot learning. So few short I mean it has to learn from

211
00:22:59,600 --> 00:23:08,400
few data. And then meta learning is there's this assumption that the learning is going

212
00:23:08,400 --> 00:23:13,520
to happen at least in two phases. There's going to be a very expensive phase which involves

213
00:23:13,520 --> 00:23:18,840
many tasks. We want to build machines that can do many things or one thing. So we train

214
00:23:18,840 --> 00:23:24,680
it to do many things and there's different ways to do this. But we should be thinking

215
00:23:24,680 --> 00:23:32,320
about this as evolution and evolution leads to what some people call priors. It's an initial

216
00:23:32,320 --> 00:23:37,760
state in your brain when you're born, you're born with a faculty of language. You start

217
00:23:37,760 --> 00:23:42,640
with some sort of prior. There's still a lot of discussion on how to best do meta learning

218
00:23:42,640 --> 00:23:50,320
in practice. And so there is this long process and then there is also another process which

219
00:23:50,320 --> 00:23:56,240
is the fast learning which just with a few data you should be able to learn to do something

220
00:23:56,240 --> 00:24:02,280
new with new objects or with new behaviors. So this is very important for robotics because

221
00:24:02,280 --> 00:24:10,240
unlike simulation robots cannot afford to do or repeat the task a trillion times until

222
00:24:10,240 --> 00:24:15,600
they get it right. And so it's important that they learn rapidly. So the name of the game

223
00:24:15,600 --> 00:24:25,920
then is maximized generalization while minimizing the amount of time needed to learn. Which

224
00:24:25,920 --> 00:24:32,320
of the two phases or both? Actually in both phases. In particular in the second phase you

225
00:24:32,320 --> 00:24:37,280
sort of assume that you will be given very few data and you will have very little compute

226
00:24:37,280 --> 00:24:43,840
power and you will have to come up with a solution very quickly. And of course if you haven't

227
00:24:43,840 --> 00:24:50,000
in the first phase constructed a representation that allows for abstraction so that you can

228
00:24:50,000 --> 00:24:56,960
go to so you can sort of easily move to new tasks. Then this is not going to work. So it's

229
00:24:56,960 --> 00:25:01,680
important that whatever representation you come up with that there be sort of these

230
00:25:01,680 --> 00:25:08,240
causality people call this like Burner-Scholkhoff and a recently Professor Yoshio Benji here

231
00:25:08,240 --> 00:25:14,960
in Montreal they call this sort of the independent mechanisms. We also often refer to this as concepts

232
00:25:16,000 --> 00:25:25,040
abstractions. And so by having these concepts abstractions or programs I did some work before

233
00:25:25,040 --> 00:25:29,040
on something called neuro program interpreters that I'm so very excited about which is trying to

234
00:25:29,040 --> 00:25:37,520
understand these modular components of things in the world like behaviors, programs, objects,

235
00:25:37,520 --> 00:25:45,200
and so on. So if we have that and at the same time we don't want to hard code this we want to

236
00:25:45,200 --> 00:25:51,760
learn this so that I think that's important. Then we have some much more hope that we will be

237
00:25:51,760 --> 00:25:56,640
able to generalize to novel objects and so on. So we'll be able to reuse some of the knowledge.

238
00:25:56,640 --> 00:26:03,040
So we'll know if we had knowledge for example of physics then whether there's new task

239
00:26:03,760 --> 00:26:09,360
involves dropping something then it doesn't matter whether it's an apple or a watermelon or

240
00:26:10,160 --> 00:26:15,520
or a piece of chalk. If you open your hand it will drop. You can predict it because you have an

241
00:26:15,520 --> 00:26:20,880
understanding of the laws of at least at an intuitive level you understand the laws of physics.

242
00:26:20,880 --> 00:26:27,440
So we need to we definitely need abstraction. So it's a few shots with the learning

243
00:26:27,440 --> 00:26:33,360
but in order to get generalization we need abstraction. We need to have a causal understanding

244
00:26:33,360 --> 00:26:41,280
of the mechanisms in the world. Kind of taking a step back we've got few shot meta learning,

245
00:26:41,280 --> 00:26:47,520
imitation, all of these are part of you know they're brought together to try to solve this problem

246
00:26:47,520 --> 00:26:52,960
is it is meta learning kind of like decore challenge and if you figure that out then the other two

247
00:26:52,960 --> 00:26:59,120
are easy or do they each kind of contribute their own unique challenges to the overall picture.

248
00:26:59,600 --> 00:27:04,080
I think they're all different perspectives of looking at the same problem which is like

249
00:27:04,080 --> 00:27:09,200
oh god that's the brain do it. So the meta learning perspective is very useful but of course

250
00:27:10,320 --> 00:27:13,840
it still leaves open the question of how do we choose the representation.

251
00:27:13,840 --> 00:27:20,960
Like in likewise we do some reinforcement learning when we do imitation sometimes sometimes

252
00:27:20,960 --> 00:27:27,200
we do supervised learning but whether you do reinforcement learning supervised learning or whether

253
00:27:27,200 --> 00:27:31,680
you choose to think about the problems in terms of reinforcement learning and off-policy learning

254
00:27:31,680 --> 00:27:36,240
or you or you instead choose to think about them in terms of causality and contractuals.

255
00:27:38,240 --> 00:27:42,720
It's kind of a it's a matter of taste. It still leaves the question of what other

256
00:27:42,720 --> 00:27:47,680
representations. So we still have to figure out what are these sort of independent representations

257
00:27:47,680 --> 00:27:53,600
and how to combine them in order to solve tasks. So I think all these different perspectives on the

258
00:27:53,600 --> 00:28:00,560
problem are very useful. The multi-agent perspective is also very useful because the most interesting

259
00:28:00,560 --> 00:28:09,840
thing at Neurip's is not like the objects around it's the people. So people are the most

260
00:28:09,840 --> 00:28:14,320
interesting in our environment. A lot of these abstractions that I'm talking about that are

261
00:28:14,320 --> 00:28:21,280
only possible with people. We can talk about a podcast because even though podcasts are things that

262
00:28:22,320 --> 00:28:27,600
or perhaps a better example is we can think about today Wednesday. There's no such a thing as

263
00:28:27,600 --> 00:28:34,320
Wednesday in the world Wednesday is a construct of our minds. It's an abstraction that we humans find

264
00:28:34,320 --> 00:28:40,480
useful to communicate and so it's eventually we do need to the multi-agent perspective.

265
00:28:41,360 --> 00:28:45,920
And another sort of thing that sort of comes into this sort of important in third person

266
00:28:45,920 --> 00:28:52,000
imitation is I'm able to say observe you and you have a slightly different body than mine

267
00:28:53,280 --> 00:28:59,200
but and I have the sort of third person view of you. So I'm looking at you and I can see your

268
00:28:59,200 --> 00:29:04,720
whole body and parts of your body that you can't see. I also have my own sort of perspective

269
00:29:04,720 --> 00:29:10,400
and I know what I'm feeling for example in my fingers and I know how cold this room is and I

270
00:29:10,400 --> 00:29:19,360
apologies and always. So the interesting thing is that I see if you think of I have a third person

271
00:29:19,360 --> 00:29:27,680
of view of you and I have a first person view of me and through that I can have a first person

272
00:29:27,680 --> 00:29:33,760
view of you. So I can more or less predict how you're finding the temperature in the room. I can

273
00:29:33,760 --> 00:29:40,160
probably predict what you're feeling in your fingers right now and at the same time I can also do

274
00:29:40,160 --> 00:29:45,840
the third person view of me I can sort of step out of my head and imagine me sitting here in this

275
00:29:45,840 --> 00:29:55,680
chair or renting a tube and so we then end up with this two by two matrix and eventually I think

276
00:29:55,680 --> 00:30:02,640
this being able to sort of step outside of my mind and to know that it's me here talking to you

277
00:30:03,440 --> 00:30:11,520
to be aware of that I think it's very profound and as part of understanding what is intelligence

278
00:30:11,520 --> 00:30:20,480
and how how do our brains work because this is about knowing how I relate to the world. How do I

279
00:30:20,480 --> 00:30:26,560
know what I know it's to know what I don't know to know what I don't know it's we're getting to

280
00:30:26,560 --> 00:30:33,440
this question of awareness to be aware of your knowledge it's something none of our machines

281
00:30:34,480 --> 00:30:44,560
have at present kind of done and and I mean in fact if we go in this direction we are not too far

282
00:30:44,560 --> 00:30:55,360
from there I say having a stab at understanding consciousness because that's to to some extent

283
00:30:55,360 --> 00:31:02,240
in non-metaphysical sense but to a computational we're moving toward a computational definition

284
00:31:02,240 --> 00:31:09,280
of consciousness here and there's a few scientists in working say on attention scheme of theories

285
00:31:09,280 --> 00:31:14,800
and so on that are starting to actually believe that this it might be possible for us to start

286
00:31:14,800 --> 00:31:22,800
talking about we already talk about like like for example a neural network policy with R&N

287
00:31:22,800 --> 00:31:27,840
the internal state is a subjective state and some of the work that I've already done with

288
00:31:27,840 --> 00:31:33,360
Misha Deniel one of the researchers I collaborate with shows that the models have an internal

289
00:31:33,360 --> 00:31:38,800
representation of the world even after they've no longer in the presence of that world you know

290
00:31:38,800 --> 00:31:44,880
sort of they've they've touched an object and the hand lifts even a while later they're still a

291
00:31:44,880 --> 00:31:51,040
memory of the shape of that object in the in the internal representation of the recurrent neural

292
00:31:51,040 --> 00:31:59,280
network and so if we go on step further to there being awareness in that neural network of what it

293
00:31:59,280 --> 00:32:05,600
knows that it that it knows that there was this object and it had a representation when we start

294
00:32:05,600 --> 00:32:12,240
combining this with you know with abstractions with counter five thought thinking and so on I

295
00:32:12,240 --> 00:32:18,240
think we are getting very close to intelligence the problem is we still don't know how to get

296
00:32:19,120 --> 00:32:26,000
these abstractions in order to be able to do third person imitation and in order to be able to

297
00:32:26,000 --> 00:32:32,720
get good awareness models of the world so when you're pursuing a line of research like the third

298
00:32:32,720 --> 00:32:40,400
person imitation learning can you walk us through you know the various phases of a project like that

299
00:32:40,400 --> 00:32:46,560
how does it evolve so this one in particular is it's fun you go collect videos of that's our

300
00:32:46,560 --> 00:32:53,360
in YouTube and because the first thing you need is data yeah so one initial state is coming up

301
00:32:53,360 --> 00:32:59,600
with representations that whether it's the video in YouTube or whether it's the the video that you

302
00:32:59,600 --> 00:33:06,560
can generate by playing the game in your own emulator in the lab the representations in both the

303
00:33:06,560 --> 00:33:12,800
main to such that they that they are equivalent so we call this sort of self super so one of

304
00:33:13,440 --> 00:33:18,400
micro operators which is the two that are equivalent the representation so it's just you the

305
00:33:18,400 --> 00:33:25,840
ideas of build neural networks that will whether whether they look at a YouTube video or the actual

306
00:33:25,840 --> 00:33:31,600
video in the emulator that I have they will produce similar representations okay so the questions

307
00:33:31,600 --> 00:33:35,920
how do we train this because we won't have super we don't have supervision and it's not an RL

308
00:33:35,920 --> 00:33:42,880
problem it's something that we're now referring to a self-supervision okay and so my one of my

309
00:33:42,880 --> 00:33:52,080
collaborators you survived are went to the YouTube videos and he came up with a technique contrast

310
00:33:52,080 --> 00:33:59,040
of learning now that allows us to be able to learn these representations so see for example

311
00:33:59,040 --> 00:34:09,840
tries to predict whether a video frame appears before or after a recent video frame or he tries to

312
00:34:09,840 --> 00:34:18,080
classify how many steps ahead is one segment of video from another segment okay or whether the audio

313
00:34:18,080 --> 00:34:25,040
video just an important part of Atari that we never get to see much of whether the audio signal

314
00:34:25,040 --> 00:34:32,320
car happens at the same time or not as the video signal and through and through just these very

315
00:34:32,320 --> 00:34:39,200
basic checks you can sort of formulate a you know training labels essentially you can train a

316
00:34:39,200 --> 00:34:46,240
neural network the typical techniques that we use there the sort of contrasting what is right versus

317
00:34:46,240 --> 00:34:52,880
what is wrong is basically what we call contrast of learning or and there's been many techniques for

318
00:34:52,880 --> 00:35:00,480
doing this max margin maximum likelihood in fact it's a form of thing this and in fact this is

319
00:35:00,480 --> 00:35:07,200
what kind of led to also Gantt it was that this kind of research of being able to so try to

320
00:35:08,000 --> 00:35:13,440
have a classifier that is telling you what's real what's not real and so you collect the video

321
00:35:13,440 --> 00:35:17,440
and I'm still not very clear on that okay so we're representations right so one representation is

322
00:35:17,440 --> 00:35:21,920
the one you've built up from the YouTube videos what's the other so we because we're using videos

323
00:35:21,920 --> 00:35:28,320
of that are very different for different players with different I know the difference of all

324
00:35:28,320 --> 00:35:34,000
the videos will all have different appearance but we through learning the representations like

325
00:35:34,000 --> 00:35:40,160
this we're able to get an in to embed the videos into vectors okay that are the same for all

326
00:35:40,160 --> 00:35:45,760
different variations of the video and once you have such an embedding embedding on a frame-by-frame

327
00:35:45,760 --> 00:35:52,880
basis or segment or entire video or something else typically on a frame-by-frame basis but you

328
00:35:52,880 --> 00:35:59,200
could sort of do this also conceivably you could try to do this of a sequences also okay

329
00:36:00,960 --> 00:36:07,120
and with this now you essentially now if you have a trajectory in YouTube now you can map this

330
00:36:07,120 --> 00:36:14,960
to a trajectory in latent space in this sort of embedding space and now this is essentially

331
00:36:14,960 --> 00:36:20,960
the trajectory that you need to follow when you play Atari so then we use this trajectory

332
00:36:20,960 --> 00:36:29,040
as a reward signal and then we just do reinforcement learning and here Toby Favme the other

333
00:36:29,040 --> 00:36:35,280
collaborator just went and tried a couple reinforcement learning agents and then it sort of learns to

334
00:36:35,280 --> 00:36:43,360
it's using the trajectory as the reward signal and it tries to follow it by taking actions in the

335
00:36:43,360 --> 00:36:50,000
game so essentially all we've done is we've used the data that we've served in the world solving

336
00:36:50,000 --> 00:36:57,840
the task in a slightly different setting we call it with a domain gap and and we've

337
00:36:59,360 --> 00:37:02,960
we first learn the features and then once we have those features we can use those features to

338
00:37:02,960 --> 00:37:08,240
construct the reward function and then we just try and just maximize the discounted sum of

339
00:37:09,280 --> 00:37:17,280
returns in order to solve the game okay you mentioned that the videos that you come across have

340
00:37:17,280 --> 00:37:22,240
you know different levels of quality and I thought you said something like different you know

341
00:37:22,240 --> 00:37:31,040
colors or levels of noise or whatever do you do any pre-processing like domain adaptation or

342
00:37:31,040 --> 00:37:38,880
out as anything to augment the the data in some way yes so in fact the self-supervised learning

343
00:37:38,880 --> 00:37:45,040
is a form of domain adaptation that's what we're trying to find features are sort of common for

344
00:37:45,040 --> 00:37:51,520
all these domains in Atari the gap the domain gap is not that big so it's somewhat

345
00:37:51,520 --> 00:37:58,960
easy for us to do some processing of a video scale and so on okay what we will need to do is

346
00:37:58,960 --> 00:38:04,560
really address and there's been a few papers but this is still I think largely an unsoft problem

347
00:38:04,560 --> 00:38:12,000
is be able to just watch a few videos of someone in YouTube doing something like pouring liquids

348
00:38:13,360 --> 00:38:20,080
and then be able to get a robot whether a simulation or real robot to also pour liquids

349
00:38:21,840 --> 00:38:28,800
that is very hard to do and in terms of mapping human hands to robot hands

350
00:38:28,800 --> 00:38:36,080
think the task there's a few nice efforts recently Chelsea Finn has done some good work in this we

351
00:38:36,080 --> 00:38:43,440
we've sort of been looking at this as well open AI has also some works but I think we still have

352
00:38:43,440 --> 00:38:51,760
a long way to go before solving the problem and so what's your sense for what the next step is the

353
00:38:51,760 --> 00:38:59,600
next piece of that puzzle so I think it's one of the things that I'm going to be betting on will

354
00:38:59,600 --> 00:39:08,480
be through coming up with better representations and currently there's two sort of hypothesis

355
00:39:08,480 --> 00:39:16,640
here one is just make the networks bigger and so the big networks is something we explored recently

356
00:39:16,640 --> 00:39:24,160
on a paper we actually just put on archive recently with an algorithm called metamimic

357
00:39:25,040 --> 00:39:35,040
the idea of metamimic is it closely imitates the demonstration like we in fact call this

358
00:39:35,040 --> 00:39:39,680
high fidelity limitations so you're trying to do something precisely like a human we do

359
00:39:39,680 --> 00:39:48,800
okay so and the idea is we're going to learn to do what humans do precisely for many many tasks

360
00:39:50,480 --> 00:39:55,840
so far we have only done it for one task with variation in the task but the intention is to do

361
00:39:55,840 --> 00:40:04,560
this for many tasks if you can learn to do one short imitation so if imitate many tasks then

362
00:40:04,560 --> 00:40:09,360
a test time we show a new task with new objects and a new check whether we can still imitate

363
00:40:09,360 --> 00:40:16,640
so we've in effect test generalization now what we found when doing when training metamimic

364
00:40:16,640 --> 00:40:21,360
is we had to keep the net making the networks bigger in order to improve generalization on the

365
00:40:21,360 --> 00:40:29,680
test set typically our real researchers haven't focused much on generalization and that this has

366
00:40:29,680 --> 00:40:36,320
changed recently over the last few years and I'm actually glad to see our colleagues all sort of

367
00:40:36,320 --> 00:40:42,480
embracing these generalization tests and what we're finding is here it becomes really important

368
00:40:42,480 --> 00:40:49,200
to make the networks big however we didn't know whether we could train the massive network so

369
00:40:49,200 --> 00:40:55,680
we trained for perception for things like vision and so on to also do control but in in this work

370
00:40:55,680 --> 00:41:00,640
we found out that it is indeed possible if you have enough sort of these demonstrations it's

371
00:41:00,640 --> 00:41:07,120
possible to train like in our work is the largest ever-trained neural networks to do RL

372
00:41:09,040 --> 00:41:15,040
and by orders of magnitude larger than any pre-existing network most of what we did before with

373
00:41:15,040 --> 00:41:21,600
Atari and so usually was with two layers neural networks and in fact professor Imot Todorov even

374
00:41:21,600 --> 00:41:28,800
complained about this at one stage I think if we are going to be doing control from pixels

375
00:41:28,800 --> 00:41:40,080
or from pixels and ego emotion and so on it's very important to use large network architectures

376
00:41:40,080 --> 00:41:49,120
and to learn how to model them properly and how to train them so the reason why we chose to go

377
00:41:49,120 --> 00:41:56,080
this way with metamemic is because if you observe children when humans are solving a task

378
00:41:56,080 --> 00:42:03,280
if they introduce irrelevant steps in between children will solve the task but they will also

379
00:42:03,280 --> 00:42:11,280
do the irrelevant task whereas a few experiments have shown that I think with chimpanzees and

380
00:42:11,280 --> 00:42:16,480
volunteers that they will go and solve the task immediately they will not do the relevant steps

381
00:42:16,480 --> 00:42:22,800
okay so humans have this propensity to over what psychologists call over imitate

382
00:42:22,800 --> 00:42:30,480
okay and essentially this is the strategy we're following at metamemic so build a very big network

383
00:42:30,480 --> 00:42:39,440
that will imitate precisely millions of things okay and that will allow it to sort of build into

384
00:42:39,440 --> 00:42:46,240
this model the capacity to when shown a new thing that I had not seen before it can just from one

385
00:42:46,240 --> 00:42:54,480
single demonstration repeat what has been done okay we also found that these models can also

386
00:42:55,120 --> 00:43:01,600
be further trained with RL to solve tasks more efficiently so that's one way to go okay that I'm

387
00:43:01,600 --> 00:43:06,000
gonna explore just very big networks and of course there's a lot of engineering because it's

388
00:43:06,000 --> 00:43:11,840
in the big networks we have to put a lot of thinking into normalization and so on so it's not

389
00:43:11,840 --> 00:43:17,440
when we say big networks it should not be understood that this is brute force engineering

390
00:43:17,440 --> 00:43:22,800
on the other hand is one requires very precise engineering in order to train these big networks okay

391
00:43:24,480 --> 00:43:29,120
this certainly has been my experience with some of our work on liberating as well as some of

392
00:43:29,120 --> 00:43:35,280
the brilliant work that your mind has put out on big gants and using gants as generative models

393
00:43:35,280 --> 00:43:41,440
recently on the other hand I also want to sort of come up with the representations that are

394
00:43:41,440 --> 00:43:46,160
more compositional so that's like the neural programming interpreters work that I did with

395
00:43:46,160 --> 00:43:52,960
Scott read a few years ago and that some folks like in Stanford like folks in Stanford have used

396
00:43:52,960 --> 00:43:59,120
to build things like neural task programmers that are able to actually do very sophisticated

397
00:43:59,120 --> 00:44:07,760
control just by exploiting modularity sort of program modularity as well as to you know

398
00:44:07,760 --> 00:44:12,960
sort of being able to break tasks into some tasks and so on yeah that's going to be a continued

399
00:44:12,960 --> 00:44:20,960
effort and I think I'm also thinking that in terms of building many causal models many small

400
00:44:20,960 --> 00:44:26,640
modules that you can then combine and if the small modules are doing the right thing and they have

401
00:44:26,640 --> 00:44:33,280
a good understanding of I mean a good understanding in the sense that the causal representations

402
00:44:33,280 --> 00:44:39,920
are world and so by combining them and they're in ways which we still have to devise

403
00:44:41,920 --> 00:44:46,800
and we should be able to get much larger models that can still represent the world and

404
00:44:48,320 --> 00:44:53,040
and you know and be able to generalize much better because these models will be compositional

405
00:44:53,040 --> 00:45:02,880
they will be able to have this combinatorial reuse of the components when you describe the first

406
00:45:02,880 --> 00:45:09,280
of those two directions building out these much bigger networks one of you may reference to

407
00:45:10,320 --> 00:45:17,120
kind of staying in the pixel domain do you see efforts to are there what are the alternative

408
00:45:17,120 --> 00:45:24,240
approaches that are being explored if any there's two different philosophies there is the learning

409
00:45:24,240 --> 00:45:32,480
from scratch philosophy that tries to learn just from pixels yeah and I often find too much from

410
00:45:32,480 --> 00:45:37,520
just pixels okay in fact if you're doing grasping and so on it's important that you don't just

411
00:45:37,520 --> 00:45:44,800
have a monocular image but you have stereo okay so death is a very important cue in order to

412
00:45:44,800 --> 00:45:50,720
grab things and you just have to try to grab things from a single video monocular video to see

413
00:45:50,720 --> 00:45:58,640
how hard it is even for humans to do it so there's that there's of course touch half-text and

414
00:45:58,640 --> 00:46:02,960
there's the you know the fact that we have little hairs in our ear that allow us to know the

415
00:46:02,960 --> 00:46:07,760
accelerations of our head and so on so we have a lot of internal gyroscopic information

416
00:46:07,760 --> 00:46:14,880
there's some cares less about kind of multi-sensory and more like is there a direction to you know

417
00:46:14,880 --> 00:46:21,120
I think the weights and activations in these networks kind of form concepts but we don't necessarily

418
00:46:21,120 --> 00:46:29,760
force them to form concepts that are meaningful to us or abstract in any way and I'm curious if

419
00:46:29,760 --> 00:46:34,880
that is a direction that people are pursuing that makes sense indeed so it's indeed there's two

420
00:46:34,880 --> 00:46:41,760
orthogonal things here one is so sort of indeed go for multi-sensory perception and then learn

421
00:46:41,760 --> 00:46:49,360
these networks to conduct actions and this is the big open question like for example in robotics

422
00:46:49,360 --> 00:46:56,000
with like these competitions with flying drones in a way that's an easy problem because the drone

423
00:46:57,120 --> 00:47:03,760
doesn't have to touch anything so there's no contact force says this should be an easy control

424
00:47:03,760 --> 00:47:09,120
it's an easy control problem if you know the state of the world what makes it hard is that they have

425
00:47:09,120 --> 00:47:14,160
to do this if they if they have to do this from pixels navigate from pixels and if there's people

426
00:47:14,160 --> 00:47:18,320
in the background that are moving and lighting changes and so on there's then becomes incredibly

427
00:47:18,320 --> 00:47:26,320
hard and it's still by far and an open problem so perception has not been solved right and that's

428
00:47:26,320 --> 00:47:32,320
essentially what this tells us or perhaps we're not using the right sensors we should and so

429
00:47:32,320 --> 00:47:40,960
mm-hmm now the the other side of this of the argument is also what other folks are doing is

430
00:47:41,600 --> 00:47:46,400
they're sort of saying we shouldn't be learning from scratch all the time you should fight in

431
00:47:46,400 --> 00:47:52,320
fact use that we have already learned some modules some perceptual modules or some action modules

432
00:47:52,320 --> 00:47:59,440
some controllers and sort of learn to reuse the components and that indeed I think is a very

433
00:47:59,440 --> 00:48:06,560
promising area if you certainly if you've learned controllers to achieve tasks it's possible

434
00:48:06,560 --> 00:48:11,200
to reuse them and here controllers you can think of for them as also like sub-programs and we can

435
00:48:11,200 --> 00:48:17,680
combine them what we haven't done well yet is being able to learn a representation of something

436
00:48:17,680 --> 00:48:24,880
like an object and then being able to exploit it that representation keep it in new

437
00:48:24,880 --> 00:48:32,240
you'll be able to harness it for future to build future representations or to keep learning

438
00:48:32,240 --> 00:48:38,480
continually more and more complex representations and be able to manipulate to be able to manipulate

439
00:48:38,480 --> 00:48:44,720
those objects that is still an open problem we don't quite know how to build and this type of

440
00:48:44,720 --> 00:48:51,520
model architectures and there's two philosophies as people who try to model this by learning everything

441
00:48:51,520 --> 00:48:57,120
from scratch and then there's people who of course try to inject much more domain knowledge

442
00:48:57,120 --> 00:49:03,440
and they argue with each other as to who is right but we've seen a lot of these debates

443
00:49:03,440 --> 00:49:09,440
in Twitter especially but I think the verdict is still open it's good that there's different

444
00:49:09,440 --> 00:49:16,000
perspectives on this well Nando any advice or words of wisdom or pointers for folks that are

445
00:49:16,000 --> 00:49:22,640
interested in kind of digging in a little bit deeper into this area losing thoughts I guess come

446
00:49:22,640 --> 00:49:28,640
to come to new reps and I clear and then it's sort of get get to know watch this podcast

447
00:49:31,280 --> 00:49:38,080
and there's also brilliant courses online like Andruing and so on so you where you can sort of

448
00:49:38,080 --> 00:49:43,680
get into this sort of material and get a much better understanding of the material we also

449
00:49:43,680 --> 00:49:50,160
organize summer schools we as in many researchers volunteer throughout the year to teach summer

450
00:49:50,160 --> 00:49:56,640
schools all over the world so here in Canada through the SIFA program lately I've been involved

451
00:49:56,640 --> 00:50:02,640
with teaching a summer school in Africa called the Deep Learning in Dapa which is a great initiative

452
00:50:02,640 --> 00:50:09,600
because and I think that is a parting thought that I want to bring into this AI is such a very

453
00:50:09,600 --> 00:50:17,520
powerful techniques AI will be very influential in our world not just the tech world but AI

454
00:50:17,520 --> 00:50:25,280
will shape politics AI will shape our economies and so on it is essential it is of paramount

455
00:50:25,280 --> 00:50:33,360
importance that everyone has access to AI if currently there is very strong biases

456
00:50:33,360 --> 00:50:42,320
women are underrepresented in AI there's a huge under representation of certain races in AI

457
00:50:42,320 --> 00:50:49,840
of certain continents in AI and we need to address these unbalances we need to address them for

458
00:50:49,840 --> 00:50:56,320
two reasons one then we want to build tools that are fair we want to make sure that AI is for all

459
00:50:56,320 --> 00:51:04,160
of us not just for the few and the other reason is because some of the people that have been

460
00:51:04,160 --> 00:51:11,920
marginalized from this actually have a lot to contribute we often look at Africa as there's

461
00:51:11,920 --> 00:51:17,520
this wall as there hasn't participated in the machine learning conferences this year at NIPS we

462
00:51:17,520 --> 00:51:23,440
have at least two papers that I know of that came from Africa so by helping a bit going and

463
00:51:23,440 --> 00:51:32,400
volunteering there we can sort of start reaping the benefits of their contributions and there's

464
00:51:32,400 --> 00:51:38,240
lots of things that came from Africa Gaussian processes were invented at my university by Professor

465
00:51:38,240 --> 00:51:48,160
Krig and he used to be called Krigging EC2 if you actually look at the history of AWS Africa played

466
00:51:48,160 --> 00:51:55,760
a big role in it you know this is where people in South Africa working on this that have given us

467
00:51:55,760 --> 00:52:02,880
the infrastructure that not pretty much holds all the data from all banks and nation states and so on

468
00:52:04,000 --> 00:52:11,520
it's important to also address the problems in Africa that we haven't even thought about

469
00:52:11,520 --> 00:52:16,240
because we haven't gone there and things like translation in South Africa there's 11

470
00:52:16,240 --> 00:52:23,760
official languages in other countries in Africa there's like 50 languages and when people tweet they

471
00:52:23,760 --> 00:52:33,200
usually use three languages mixed so translation is a huge problem and so when you go there you learn

472
00:52:33,200 --> 00:52:38,240
this and then you realize oh it's important to sort of start working more on unsupervised machine

473
00:52:38,240 --> 00:52:46,960
translation and so on there's also other problems one and I'm really going off track here there's

474
00:52:46,960 --> 00:52:52,640
a very long apologize for having gone this long another problem that I thought was very

475
00:52:52,640 --> 00:52:58,240
interesting in going to Africa two years ago was when I learned about Mum Connect so with Mum Connect

476
00:52:58,240 --> 00:53:09,280
Mum's essentially get enrolled when they go to hospitals into this messaging service and throughout

477
00:53:09,280 --> 00:53:17,120
Africa people use these very cheap old phones these two have here and they can still text and

478
00:53:17,120 --> 00:53:22,400
communicate quite efficiently in fact that the service plans are not as expensive as here in Canada

479
00:53:22,400 --> 00:53:31,120
so in many ways they have leapfrogged out inefficiencies and what they can do is the doctors can send

480
00:53:31,120 --> 00:53:37,600
the messages reminders of what to do after the child is born and so on and they can also if they

481
00:53:37,600 --> 00:53:43,840
have questions they can text the doctors so for example they could text the doctor and this is

482
00:53:43,840 --> 00:53:50,720
like an example of what happens like I've given water to my child all day and all night but the

483
00:53:50,720 --> 00:53:58,880
child keeps throwing up the water at that state the doctor can simply say boil the water

484
00:53:59,920 --> 00:54:06,080
provide that sort of basic advice that will save a life and in fact most if we want to have a

485
00:54:06,080 --> 00:54:11,200
huge contribution to the health of people in our planet those little things other things that

486
00:54:11,200 --> 00:54:18,400
sort of matter the most and you can learn about them when you go and include people from other

487
00:54:18,400 --> 00:54:24,720
communities and then you see the wonderful things that have already done to address this problem

488
00:54:24,720 --> 00:54:34,080
so they actually can contribute a lot of insights into the development of a machine learning tools

489
00:54:34,080 --> 00:54:38,800
well Nando thanks so much for taking the time the child it's great to get to speak with you

490
00:54:38,800 --> 00:54:40,880
thank you very much thank you for listening

491
00:54:40,880 --> 00:54:51,280
all right everyone that's our show for today for more information on Nando or any of the topics

492
00:54:51,280 --> 00:54:58,000
covered in this show visit twimmalei.com slash talk slash two thirteen you can also follow along

493
00:54:58,000 --> 00:55:06,000
with our nirip series at twimmalei.com slash nirips 2018 as always thanks so much for listening

494
00:55:06,000 --> 00:55:13,920
and catch you next time

