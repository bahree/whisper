Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.
All right, everyone. I am on the line with Emily Bender. Emily is a professor of linguistics
at the University of Washington. Emily, welcome to the Twimal AI Podcast.
I'm really excited to be here. Thanks for having me on. I am super excited to chat with you.
We spent some time speaking earlier, kind of the obligatory, how are you doing?
Kind of age of corona conversation and kind of grounded on some of the things that we'll be
talking about. Ethics, practical considerations around AI and NLP ethics, computational
linguistics and a relationship to linguistics to name a few. But I'd love to start with hearing
a little bit about your background and how you came to work in the field of what linguistics
slash computational linguistics. Happy to. So my background in terms of training is linguistics
all the way. I studied at UC Berkeley. Didn't know what linguistics was until I got there and
he did some wonderful advice the summer before going off to read through the course catalog,
which you know back then was a book, looked like a phone book, and just circle anything that looked
interesting. And there was this one class called an introduction to language. And I thought,
that sounds great, because I really liked my language classes I took in high school.
And so I circled it. And then when I was looking for a general education
requirement class, I saw that it fulfilled one of them. So I signed up for it and I was hooked
on the first day. It was like, this is my thing. But it took me the whole term to convince myself I
could major in something that I perceived to be impractical, which is pretty funny in retrospect.
And the class was an intro to linguistics or? Yeah, intro to linguistics. And I majored in
linguistics. I took one computer science class when I was an undergrad. There was the intro
programming class that was taught in Scheme, which is a dialect of Lisp. And then I went on to do a
PhD in linguistics at Stanford, where I worked on. And did you field work in the whole nine? No,
I did some experimental work, but I didn't do like there's there's linguists who do really
interesting work going out and, you know, recording people speaking to observe something with
a variation or working with consultants to understand the structures of languages that are not
well documented. I'm not either of those. But I did do a little bit of experimental work. So my
dissertation was on the interface between sociolinguistics and syntax. So syntax is the study of
what are the rules behind the grammars of languages? How do we get to the strings that are allowed
and what they mean? How do we write those descriptions? And what do we need to be able to
create a formalism that'll work for any language? Because we know that any human can learn any
language if they're exposed to it. And so those are the questions of formal syntax. And there's
this interest in what knowledge do we have. And then sociolinguistics is looking at language
variation and how people speak differently in different contexts based on sort of their
social address and how they want to present themselves to the world. So register and all that kind
of stuff? Yeah, register code switching, right? Lots of people will have multiple different
varieties that they control and they use it to present themselves in different ways in different
contexts and all of that. And the syntax traditions were saying that's not linguistic knowledge.
Linguistic knowledge is just what it is that lets you know that in English we don't say I see cat.
We have to say I see a cat or I see that cat or I see cats, right? And that's a grammatical
constraint of English. And that kind of stuff is knowledge of language. And all of this other
knowledge that people have that allows them to decide when to speak in which way, that's separate.
And my intuition was, well, we're one person. And yet we know all of this and it's connected
with the same thing. I think it fits together. And so I did some research looking at variation in
what at Stanford, we referred to at the time as av, which is an acronym African-American
vernacular English, also known as ebonics. And black English is a whole bunch of names for
the variety, which is a surprisingly actually well studied variety of English among those that
are not considered standard. And so there's really good social linguistics studies of the ways
in which variation is used in app to different purposes. And so I did a perceptual study looking
at how people perceived the meaning associated with saying or not saying the verb B in different
contexts. So that's pretty far field for a podcast on machine learning. So that's what I was
doing in my studies. And alongside I was working as a research assistant on a project building
a computational grammar of English. Okay. So I go on the job market and don't get picked up as
either a syntactician or a sociolinguist. And so I go out into the startup world. I was working
at a startup called YY Technologies. I started there in 2001, which was not a good time to be going
into the startup world if you remember. It was just in time for the dot com first.
Company was gone within seven months of when I started working there that had been around for
almost a decade. Doing grammar engineering for Japanese. So building a grammar that modeled
the rules of Japanese so that we could get from strings of Japanese to semantic representations
in the context of a customer service application. Okay. Kind of precursor with customer service
chatbot. Yeah. Yeah. It wasn't set up in a chatbot mode. It was meant to be email response.
But very much that same idea. Okay. And the company had a product that worked for English and they
wanted to branch out into the Japanese market. And so they hired me to work on the grammar for
Japanese that would fit into that same product. So then I am on the strength of that basically
and a little bit of research I was doing around generalizing grammars across languages got hired
to start the professional master's program in computational linguistics at the University of
Washington. Okay. And yeah. Which felt like a huge stretch at the time I have to say. I was like
really because if you look at my CV, I'm pretty junior and I look like a syntactician slash
sociolinguists with this thin layer of computational linguistics on top. But part of what was going
on there was that the kind of syntax that I was doing was really grounded in computational
modeling of syntax. It was developed to be able to be written on a computer as well as understandable
by humans. And so the linguists in my department saw all of my syntax work as computational linguistics
too. And was that just the representation, the way you represented the syntaxes or something
about the analysis that you were doing? It's about the analysis and the theoretical framework.
So that HPSG is what it's called and it's a framework that's interested in getting down to all
the details so that we actually model not just the small subset of sentences that are interesting,
but everything you might find in running text eventually. And so that's very coherent with
the goals of doing natural language processing, but from a symbolic perspective. And on the other
hand, it's very formalized. It's precise enough that you can do it on a computer. And a lot of
other work that happens in syntactic theory is much more entrusted in theoretical questions to do
with similarities and differences across languages and doesn't put the same emphasis on getting
down to the nitty-gritty both in terms of all of the data and in terms of precise enough a computer
could do it. Is there a way to create an example of the two different tags that would be
elastic and restrictive? It's not so the best examples I can think of are visual examples in terms
of what the analyses look like. And just to give you a sense of it, when we build a grammar in HPSG
for English, there's English. When we do that, for those people just listening, that was a cat
that just jumped into the frame. That's why we're laughing. And the cat did it earlier and I was like,
I hope the cat does it again. So there's the cat. He's helping me out. So in order to get,
there's a resource called the English Resource Grammar that the best thing that I was involved
with as a research assistant in the 1990s. It's been under continual development since 1993.
It is a hand-built grammar of English on a computer that if you give it well-edited English text
from pretty much any genre or register. So chemistry, academic articles, Wikipedia, Linux
manpages, Norwegian hiking, tourist questioners, like broad range of genres, but well-edited.
Their cats went flying. It can come up with correct analyses that give you a good grammatical
structure and a good semantic representation. I can send text tree of the center or something like that.
Send text tree and then like predicate argument structure, like first-order logic type representation,
predicate logic, not first-order of the semantics, for something like 90% of those sentences.
It's a massive project. But in order to do that, its representations internally are actually
quite complex. And if we try to like print one out, not in sort of the abbreviated form that shows
you the general structure of the tree, but all the details, it's pages and pages and pages for one
sentence. If what you're interested in is sort of making generalizations across languages about
what happens with the expression of pronouns versus you just don't say the word at all
in different languages. And how does that correlate with how much morphology is on the verb?
That kind of representation can be cumbersome. And so that's where the theoretical syntax tends to
work more in broad brush, I think, than the kind of details you need to do it on a computer.
Is there an analogy there to like explainability of machine learning models, or is that too much
of a leap? Wow, it's an interesting question. But I don't think it connects to explainability,
because in both cases, it's humans are creating these things. Yeah, I'm not directly like I'm
envisioning the thing that with the pages and pages is like a, you know, a deep model with a lot
of primers that we don't really understand. I guess in this case, you can understand them if you
zoom in. Exactly. Whereas in the other case, it's a little bit more amenable to interpretation.
Yeah, I think at that level, the analogy works. It's that it, but it's not useful.
You can drill down, but it's hard. So when people approach this resource and haven't been involved
with it for years and years and years, it is opaque. And it's like, well, how do I,
like, why is this feature value this here and that there? And where is it coming from? And
why did you make that analytical decision? And there's a lot of work to be done still about,
how do we document this as an incredible resource? But how do we make it accessible and understandable
to people who haven't already been deeply involved with it? And so did you end up taking another
computer class programming class? That's a big question. I left that hanging. So I did take one
class in the computer science department in graduate school. It was a class with Terry Winigrad
on phenomenology. It was a philosophy class. We did not program anything. And it was a great class.
He made us read Heidegger, which was painful. But it was a really interesting class. And that's
the only other computer science class that I took. I did learn along the way how to code.
Done both working in the grammar engineering is a very high level programming language. That's
sort of declarative linguistic knowledge. But I've also done some work with some of the underlying
algorithms. But I am not, you know, not the person who's going to build or deploy a machine learning
system. Rather, my role in this dialogue about linguistics and NLP has been sort of saying,
okay, I see what you're doing with the machine learning. How does that relate to the questions
that we're asking in linguistics on the one hand? And that was sort of my first entry into it.
Was, okay, you're building systems. I see the input. I see the output. I see you're really
interested in the internals of the system. And that's cool. That's fine. But that's not the part
that I'm working on. Right? I'm interested in the framing of the task, the input, the output,
how it's evaluated. Where did that gold standard data come from? How did you create those annotations?
And then how does that task relate to either scientific questions that someone interested
in say how language works might be interested in? Or how does it relate to the practical
applications that you're selling it as a solution to? Right, right. Yeah, you had an interesting
tweet the other day. I meant to look it up so I could reference it here, but it was something
alone, the lines of, I forget the exact framing, but a lot of your perspective being informed by
the idea that, you know, in NLP, we've got this whole field of linguistics that came before it,
and now we're doing all this cool computational stuff. And that you contrasted that with
computer vision, where maybe we didn't have computer visionology or visionology or whatever the
analogous thing would be. Where were you getting out there? So that came out of a fascinating
conversation that I had with Deb Rajee and Emily Denton, who I only know because we've met
over Twitter. And this is one of the wonderful things about this research environment,
especially now that everyone's stuck at home is like, okay, yeah, sure, I have time to
have a meeting with people who are in very different parts of the country,
cool. And they are interested in the sort of benchmarks and how they're deployed to machine
learning broadly and sort of where, where do they come from? How, and this is also Emily Denton's
working with Alex Hanna on these questions of how does this, how does a benchmark become a
benchmark? Why do people care about some and not others? And I think we all share an interest in
how do these benchmarks relate to the broader thing they're supposed to represent?
Particular benchmarks generally or benchmarks like ethics benchmarks or
So it's a great conversation because all of the people in that conversation really care about ethics,
but that's actually not what we're talking about. So it's always a background and
it's about the, yeah, I was just extrapolating from the names.
So, you know, it's things like, so the super glue benchmarks for natural language understanding
and image net for computer vision and image labeling, right? These become these
tasks that then you try different algorithms on. And so one of the things that came out of that
conversation was a framing that I really like, which is this sort of three-part thing. You have
the task definition sort of in the middle. And on the one hand, you have the data set that the
task is represented by. And on the other hand, you have the thing in the world that the task is
supposed to correspond to, right? So if your task is image labeling and then you can say, okay,
how does the construction of image net actually relate to that task as sort of task internally that
I'm trying to do? But then also, how would that task get deployed? What are we using
image labeling for? And how does this particular conception of the task support or not support those
use cases? So we're talking about things like that. And in the context of that conversation,
I was shocked to hear them say that from their perspective, NLP is better off than other parts of
machine learning because NLP has linguistics, keeping it honest. And I was floored by that because
I feel like, yeah, that's what I'm trying to do. And there's a few of us in there kind of conversation,
but it does not feel like we're generally speaking being listened to, which is maybe a little bit unfair.
I think the dynamic there isn't that people are not listening or ignoring whatever, but it's
rather that the way things are set up right now, NLP for some people serves as an application
area of machine learning, right? So if you're, if what you're interested in is learning algorithms,
and that is a wonderful area of research, it's, you know, I'm glad people are working on it.
In order to test them and refine them and understand, you know, what's good at what you need things
to be learning. And a lot of people who are interested in learning algorithms seem to be interested
in as something that can learn genuinely. So they're interested in applying, you know, RNNs in
different contexts. And so you need different contexts. And it's not just sort of a NLP view of,
okay, different registers, different, you know, are we doing speech recognition or machine translation,
but language is one, vision is one, playing chess, you know, things that you would need in
autonomous driving. Like these are all different application areas. And so we get people coming
into NLP who are really interested in the machine learning, and they're just coming to publish
in NLP venues because they're applying it to NLP. And so there's this constant influx of people who
don't have long or deep training in either linguistics or really thinking about language.
And so I sort of feel like I'm almost like chatting into the void. And then I talk to other
people who say, yeah, but, but at least there is linguistics, like there's a field of study there.
And what would it be for computer vision? And I mean, I guess it's something to do, well,
the thing that's strange about is that, yes, there is.
For thing, no processing is the thing that comes to mind. We study that in DSP classes.
Yeah. And that's used also in speech technology, right, for the speech recognition.
But if you're looking at like trying to do what people do, and you're looking at vision,
there's something around both sort of perception. So how does the, and here I'm talking
way outside my expertise, just want to flag that. This is not anything I don't think about.
How does the, how does the eye and the, and the ocular nerve and all of that, like what happens
when the light hits the retina and what happens, and then what happens in the brain that's processing it,
to create maybe some sort of representation that just has to do with the visual stimulus,
but then that gets connected to categories of things.
Yeah, yeah.
And so it's not just about sort of psychophysics and like perception and stuff like that,
but it's also about categories and ontologies and how we understand our world.
And so I don't think one coherent field of study, like maybe it's different parts of psychology
that they should be talking to. But I don't see that interaction apparently happening,
the way at least NLP and linguistics do talk to each other.
Mm-hmm. Interesting. Interesting. Yeah, when I, when I heard you initially describe the,
kind of how the kind of year take on, you know, machine learning folks doing NLP, it almost sounded
a little bit like the, you know, I guess most recently I've come across this in the context of COVID,
like data scientists running off building models and don't know anything about the
virology. Like does it feel to you as like you might be the virologist and you've got all these
data scientists producing all these models that don't correspond to the thing that you actually
understand? Yes. And, and I've had the same reaction to seeing the data scientists jumping in
around COVID. I'm really glad that you had that panel on that. By the way, those were great
comments that your guests had because there, there seem to be something in CS education, I think.
It's so it's somewhere in the people who are attracted to machine learning or the way we train
them or both that basically says you are problem solvers, here are problems you solve them. And
also the way machine learning gets sold, there's this like packaging around it that's all about
denigrating the work it's supposed to be replacing. And so if we're going to apply machine learning
to NLP, it's because oh, it's far too expensive to do by hand, right? You wouldn't want to hire
somebody to write a grammar when you can just learn a grammar, right? So it's better to do it automatically
and that has a way of devaluing the work of the people who actually understand the shape of the
problem. And so part of my message as a linguist and NLP over these years has been, look,
it's great to do things by machine learning, but if you want to know that you've actually done them,
it has to be in conversation with people who understand the shape of the problem and can see
whether your solution actually works. And so when I see data scientists teaming up with
virologists and epidemiologists and clinicians and saying, hey, I have skills. Where do you need
my help? That's fantastic. When I see people jumping in and saying, I'm going to solve this and
they seem to be on their own, I get really worried. And so I don't know that things like language
models, which have shown incredible, you know, innovation and promise and all this kind of stuff of
late. I don't know the extent to which linguistics were involved in their creation, but they're still
interesting. Like what, how do you kind of parse the fact that innovation is happening and we're
doing, you know, good, interesting, useful, at least in some cases, things with kind of the viewpoint
we previously discussed. Yeah. So there's a saying, I wish I knew who to attribute to that
said refers to the unreasonable effectiveness of n grams. And this is before the transformers,
this is old, right? You can do a lot of useful stuff by just counting co occurrences of words.
And as a linguist, that's kind of a bummer. Because when I look at a string of words, it's like,
yeah, the words occur in some order. That's not the interesting part, but it turns out that if
you have enough data and you count words and look at the orders they occur in, that's really useful.
It's extremely useful for speech recognition. It's extremely useful for machine translation.
It's useful for information retrieval. So there's a lot there. And that's fine. And it's useful.
And it sort of comes back around to this, you have the task. And how does it relate to the world?
Right? So if your goal is better transcription of, you know, open domain, a noisy environment,
and you've set up a task that models that well, and you're honest about which language varieties
are represented, and you don't claim to be solving speech recognition in general when it's actually
speech recognition for a particular variety of English. Exactly. Exactly. That's all fine.
And then there's interesting questions of, okay, why is that working? And there's room for
linguistic knowledge to come in and say, okay, what is it that makes this effective? And also,
let's do some error analysis. When it's not working, is there any pattern to that? Is there anything
about what it's missing by only looking at sentences as strings of words that can predict some of
these things? There's a bunch of really interesting work going on right now under the rubric of
Bertology, looking at the transformer language models and trying to figure out how much linguistic
structure they are picking up by doing this essentially to the language modeling task.
And Bertology, the, when you say that, is that the study of those models, or is it the,
kind of the explosion of related models, Roberta, and things like that?
So, Bertology, I think I usually see it used to refer to the study of the models.
Okay. So, Bert and its kin, how is it that they're working and what is it that they're learning
about language structure? So, there's work by, I can't do this off the top of my head. Sorry,
I could look up references for you, but people do probing tasks. Whether it's a, you know,
we're going to take a slice of this model and we're going to use it as a classifier and we're
going to see if that model just sort of as trained as part of Bert has learned something about
predicate argument structure or has it learned something about syntactic categories or syntactic
dependencies. Interesting. And is that the line of work happening primarily from a linguistic
perspective or primarily from a machine learning perspective, or is it both? So, the folks doing
that tend to be teams of people who are, all of them have an interest in linguistics. I think
primarily they tend to be folks who started off with machine learning training and then learned
the linguistics, some of them have linguistics earlier in their training. And then occasionally,
you'll get people who like bring linguists on as collaborators. So, that's, I mean, that's,
that's sort of what I'm usually pushing for when I'm saying, you know, that you can't do
machine learning without domain expertise. It's not a kind, I could sometimes get accused of
doing gatekeeping, which is not my intention. I think that collaboration is a great, more
perspective is great. And what I'm seeing is a lack of inclusion of the domain expertise perspective
sometimes in the machine learning work. And when you look at something like Bert with the
linguistic perspective, are there, is there kind of a laundry list of obvious things that,
you know, you think need to be done or that, you know, should be looked at or that, you know,
things that, you know, shouldn't have been done like that? Or, you know, I'm wondering, is it,
are you speaking primarily out of kind of opportunity? Like, if we pull domain experts in,
then, hey, maybe we could achieve some, you know, greater thing? Or is it because of the specific
things that you see when you look at Bert? Yeah, yeah. So, I don't, can't say I have specific
examples off the top of my head. And it's not, I mean, this is, this is a, a soap box I've been
creeping from for years now, like, prevert. And in prevert, it's more about the, the claims of
what's happening, right? So, if someone creates Bert and they say, hey, this is helping me do great
speed, right, speed recognition, it's helping me do great machine translation. And then you can,
you know, you can test that. That's a product that's a, you know, sort of a practical application,
and it works better. Excellent. If somebody is saying, hey, Bert understands language,
then that's a scientific claim. And I want to say, okay, show me what your tests are that allow
you to make that claim that it understands language. And also, I can tell you on first principles
that it doesn't. Right. Right. And you talked about this in your paper, climbing towards NLU on
meaning form and understanding and the age of data. Yeah. And the basic premise there is that
Bert isn't at all trained on meaning. So there's no way that it could display, you know, meaning or
understanding of meaning. Yeah. Exactly. Exactly. That was a super fun paper to write with Alexander
Thor. We had a blast. And it came out of actually Twitter arguments. So I had this, I think it's
back in the end of 2018, this unending Twitter thread thread is the wrong word for it. So that
makes it sound very linear. And it wasn't where I was basically stating the thesis of that paper,
which is that if your training is only language modeling. So all of the training data is only form
and your task is to predict the next bit of form or, you know, in Bert style masked bits of form
in the middle, meaning as you say, it wasn't in the input signal. So you're not learning meaning.
And there was an unending parade of people who wanted to pick up the other side of that argument
with me on Twitter. And one new paraphrase, was there a best of those arguments? Because when I've
read that, you know, I have often been in a situation of trying to explain to people, you know,
technical or non-technical, whatever. The extent of these models and what they're really capable of
and what they're not capable of, that kind of thing. And I really appreciated the idea of, hey,
we're not the input, the input is not meaning. It's not understanding. So that's not going to be
what comes out. Yeah. So I don't, I don't have a best of because it is sort of this like,
it's not there. So how are you going to learn it? But it tends to take the form of, well,
what if you give it lots and lots and lots of data, right? Or sometimes the arguments were,
I think if I were to try to argue against that, I would try to say that meaning actually was
there, but not in the form that you're used to seeing it, like in a quality, but rather implicit
meaning as opposed to explicit meaning. Yeah. So that is one shape that the argument sometimes
takes, which is, well, the sentences themselves represent meaning. And that one is, well, yes,
they do, but only if you know the linguistic system behind them so that you can pull it apart.
I was going to say, this is a Sam gets creamed by a linguist's segment of the interview.
So at the end of the paper, we actually have a bunch of the counter arguments.
I'm going to say this, but you're right. And so it's, it's in there. But part of what makes it
difficult is we have to really pin down terms, right? So what do we mean by meaning? And then what
do we mean by language model? Because you can certainly imagine language models that are modeling
meaning as well as form. So we say, hey, language model is the only input data it has is the
form of language. And that could be the written form. It could be spoken. It could be signs and
signed language. And that's, that's all it sees. And meaning is a relationship between those
linguistic forms and something outside of language. And we actually break that down into two parts.
So the sort of biggest relation is between the form that I say and my communicative intent.
I'm trying to communicate something to you. So for example, when the cat went jumping and we're
both laughing, and I'm thinking of the people listening to this, like when they're out running,
like, which is how I usually listen to your podcast, I thought, oh, wait a minute, those people don't
know why Sam and I are both laughing. What's going on? I better say something to clue them in,
right? So I have this communicative intent to make apparent to these people who are only listening
audio that there was a cat in the frame. All right. So that's what I want to get across. And so then
I figured out better podcasts than I am because I were just laughing.
I actually have a lot of experience with online teaching. We've been doing our classes, not just
recently, in the master's program, we've had classes in a hybrid online in person format since 2007.
Oh, wow. So I've long years decades, almost experience with this now.
All right. So that goes flying. I want to get that across. So then I think, okay, how can I convey
that to somebody who's listening? And then I pick a string of words that doesn't directly contain
catflies over my shoulder, right? It doesn't, it doesn't, it's not a picture. It's a string of words.
To provide a clue to the listeners that would allow them to reconstruct my communicative intent.
And so the thing about language is that it is this collaborative experience. And interesting,
like you and I are doing some collaboration right now, right? And we're doing a lot in terms of
the general communication stuff, the turn-taking. It helps that we can see each other, right? It makes
the turn-taking race smoother. But we're also collaborating with our listeners. So people who are
not here in this present moment who are listening later, hi people. They are, they are working with
us across that time to work out, okay, given what I know about English and what these sentences can
conventionally mean. What is Emily trying to get across to me as a listener? What is Emily trying
to get across to Sam in the moment and vice versa? So that's what we're saying meaning is about.
And hopefully that makes it clearer to people that it's not there if all you have are the pixels on
the page. Are you aware of any efforts that are trying to get there? I mean, I'm envisioning
something like, I mean, I guess we're all trying to get there or everyone that's in this research
field of NLU is trying to get there in some way. But more specifically, I mean, I'm envisioning
something like a cross between a, you know, a bird that's kind of anger and focused and a
normal machine translation that is pairing, you know, meaning and sentences. Anything interesting
happening there? So, yes. And I think that a lot of the ways in which Burke gets deployed,
you have the pre-training, which is just language modeling. And then you have the fine tuning,
which is giving a tiny little signal about the kind of meaning that's relevant for the present task.
And so it's there in that sense. There's a wonderful paper, this paper by Jonathan Biskin
colleagues, is talking about sort of the stages of the world scope of NLP. So really early on,
you might have had just like hand constructed word lists. And then you got to corpora. So things
like the pen tree bank, which has Wall Street Journal plus the brown corpus in it. And that's
a couple million words of text. And then you get to like web scale. So enormous corpora.
But it's all that is still just form. And then they're saying, well, where can we go next? And so
there's, there's starting to be work on embedding the, the naturalization processing system in some
sort of embodied context where you are talking with a robot. And the robot has to figure out
what motions in the world it should be taking. So there's sort of, there's small amounts of text,
large amounts of text, text plus embodied interactions and grounding in things like vision and
whatnot. And then the biggest one is adding in the social. And so I think that's a really
fantastic vision. And I'm super excited. This is not actually an ACL paper. This is a preprint
right now. I think they're going to keep working on it and publish it somewhere else later. But ACL
has this wonderful theme this year. ACL is the association for computational linguistics.
Supposed to have been in Seattle in July. And I'm sad that we don't get to host people in person.
They'll be online. And the theme session is taking stock of where we are and where we're going.
And so there's a bunch of papers sort of looking at. Interesting. You know, how do we go in a lot
of them seem to be focused on meaning and semantics. So people are thinking about it.
And do you think that that particular theme is driven by the kind of recent progress with
Bert and the like or is that is something that we talk about at these conferences every once in a
while? I guess I'm trying to get a sense of, you know, does it feel from a linguistic and
computational linguist? I guess as a linguist. And then maybe the next level is computational
linguist. Does it, does it? Do you have the same feeling of excitement around what's happening,
you know, with Bert and language models and the like as, you know, folks from a machine learning
perspective have? Because I think from a machine learning perspective, it's kind of akin to
2012 with, you know, deep models and computer vision. It's like, wow, we're making a lot of
progress here. Or is it just trade? So it's actually as a computational linguist in NLP,
it's kind of shouting for it actually. Because when I when I entered the field,
the what I was doing was considered very old school, right? So I came into computational
linguistics in, you know, early 2000s working on grammar engineering and that was already the
height of statistical processing in NLP. And so everyone would frame their papers as well,
you know, in the olden days, people would have to do this by hand. But now we can do it
and I'm like, some of us are still here, still doing it by hand. And you know,
that's okay to have both. Like we don't have to, you know, this room, this should be, you know,
the world is big enough for all of us, right? And then when deep learning sort of swept through
NLP, starting in about 2015, 2016, a bunch of the statistical learning people who have built up all
this expertise around feature engineering were told, Oh, no, no, no, you don't have to do that by
hand anymore. We can do that automatically. And so that's where the shouting for it comes in a
little bit. So it's not, there was a period where it got a little boring because basically,
you had all these existing tasks and turns out you could build those the model for it. And so
there's all these papers that basically take existing tasks and first word embeddings in general
from like word to back or something and then bird into the mix. And hey, look, say to the art
paper. And it's like, does anybody actually enjoy reading these? Like,
what do you get out of this? Right? That was my reaction to that. So,
Bertology is really welcome. The people who start saying, okay, but why? Right? What is it about
the pairing of this methodology and this task that makes such a big difference? That's interesting.
And sometimes it is, hey, look inside of the deep neural net, you can see that this layer is
picking up this kind of information. And then that sort of opens up questions. Okay, so why is
that information available just from distribution and text? Those are scientifically interesting
questions to me. And in other cases, there's a great paper by Niven and Kau at ACL 2019 that said,
actually, guess what? Bert is cheating. And we can construct a data set that sort of hides
the clues that Bert was using that were just artifacts. And then it falls to chance. Really? Yeah.
Yeah. Interesting. Can you elaborate on that a little bit more? I haven't come across that paper.
So this or maybe I should just interview the. Yeah, that would be great. I recommend it.
But in brief, my non-author non-expert understanding of it was that there was,
this was a question. I think it was, it was like textual entailment or reading comprehension or
something. And the way the task was set up, the there was artifacts and the data that made particular
words really good cues for a certain kind of answer. And if you construct a data set that washes
that out, then those cues are gone. And Bert doesn't do well at all. So it wasn't necessarily a
general statement across all tasks. It was for a particular task. For a particular task. Yeah,
exactly. No sort of saying, hey, look, say to the art with Bert, yay, but wait a minute, that makes
no sense. Why should Bert be doing well in this task? Let's look deeper. And for that task,
they found that problem. Okay. Interesting. Yeah. And I think that that kind of result
informed the design of super glue. So Sam Bowman and his colleagues are, there was the glue
task that had to do with a language understanding. And then super glue was, okay, let's bring together
more tasks, but let's pick the ones that are hard for Bert. Okay. So I think that's a that's a
promising way to try to push towards tasks that better represent the questions that we're interested
in. But you asked, you know, why do we have this theme at ACL? And I think you really have to
ask the program chairs. So that's Joyce, Chai, Natalie, Schluter, and Joel, Tetral, who are the
incredibly hardworking program chairs for this conference. And early on, they said, hey, it will
make this more interesting if we solicit papers on this theme. If we actually encourage people
to sort of lift their heads up and look a little bit more broadly at what's going on here. And
I think I recall them saying that that was actually because of the date, basically. It was like,
hey, 2020 round number, it's time to like sort of take stock and think about it more broadly.
Got it. Got it. Interesting. So I'm trying to think if we've kind of fully captured the relationship
between linguistics and computational linguistics, or maybe that's not possible to do it now, we certainly.
Yeah, probably not possible in an hour, but I think that there's there's lots of ways in which
linguistic knowledge can inform work on NLP. And it doesn't have to be directly in terms of encoding
the linguistic knowledge in the system. And when I do grammar engineering, that's what I'm doing.
And I think that's a valid mode of research, but it's not the only mode of research. But it's also
not the only mode of research that needs linguistic knowledge. So if you're doing statistical
machine learning, or if you're doing neural processing for NLP, there's still room for linguistics
on the one hand in feature design. If you're doing the kind of thing, we're not trying to learn
your features automatically. But especially in task design and error analysis and an understanding
sort of the task data set connection and the task world connection. And we barely talked about
the ethics stuff at all, but there's a connection there too, because one of the things that
linguistics tells us from social linguistics is that variation is the natural state of language.
Number one, number two, the fact that a variety is anointed as the standard is all about power
and nothing else, nothing inherently variety. And so that means that if you've got technology that's
working for the standard variety and not others, then you are likely to be setting up a situation
where you're further disempowering, further disempowering marginalized people. So there's a lot
that linguistics can do to inform the ethical deployment of NLP, especially social linguistics.
And is your research in this area looking at the, I know a big part of it is trying to understand
how to engage with people around this idea of ethics in NLP. Are you also looking at the kind
of fundamental problems themselves? So the fundamental problems are broad and diverse and
so I am interested in understanding like as we sort of get these case reports of what's
what's gone wrong or what might go wrong, I'm always interested in how that connects the
language variation. So when I think I mentioned to you beforehand that I've got some unpublished
work trying to do a typology of the kinds of risks of adverse societal impact of NLP,
and that is very much informed by what I know about language variation.
So yeah, so working on that, but also working on, yeah.
Does unpublished mean that you can't talk about it yet?
No, it just means that I can't point to anybody to it.
When you, how do you, what are some of those categories?
So what I'm looking at it in terms of direct versus indirect stakeholders, and this comes
out of value-sensitive design. So work about your freedom and colleagues sort of conceptualizing
the people that we need to worry about as both the users, the people who actually interact with
the technology directly, and other people who are affected. So we're taking that as the overarching
thing and then thinking about the different ways that you can interact with a system as a direct
or indirect, so the direct stakeholders might be doing it by choice or not by choice.
And then the indirect stakeholders, and it could be, well, my words are part of a broad
corpus, or I am a person who's subject to societal stereotypes that the technology might be perpetuating.
Or, dang it, I got a third one over there right now that I can't do off top of my head,
but that's the kind of category. And none of those are specifically about
sociolinguistic variation, but rather within each of those, I say, okay,
if I'm a direct stakeholder choosing to use something, but it doesn't represent my language
variety well, what happens. So that's sort of an example of that.
So the speech recognition does not work well for all Englishes.
And so if I'm trying to use a virtual assistant and it doesn't recognize my variety,
or it only recognizes the variety that I code switch into usually outside the home,
but I have to use it inside the home with this virtual assistant. How does that make me feel
about my language and how I fit into the world, right? Yeah. So there's the things like that.
So I felt like I was going to go somewhere. So sociolinguistics informs that.
And I think that that's an important kind of linguistics that needs to be in the
conversation and visible to people who are working on machine learning and NLP. And I think it's
pretty remote. It's not the sort of thing that maybe people even necessarily know about. They
may have heard of linguistics and they give linguistics as like Chomsky maybe. So.
And so when you talk about kind of techniques for getting folks to engage in these kind of
discussions and think about these issues, are you primarily
thinking about or targeting kind of the laypeople or researchers and builders?
And so both. And I should I can't give you a pointer to a video of a talk that I gave on this.
And it sort of ends with whose job is this, right? And I've got a bunch of categories of people.
And we all fill multiple categories. So the first one is the researchers and developers, right?
Then there's consumers. So when we use a product, we either buy it or we just choose to use it
in a, like, you know, pain with our eyeballs kind of a way. The third one is for cures. So people
who decide to buy and install and stand up a system based on NLP or machine learning technology
more broadly, I suppose. Then we have the role of members of the public who are advocating for good
policy and then we have policy makers. Okay. So you can see that any individual might be in multiple
ones of those roles. And I think it's important to sort of get this knowledge out in all of those ways.
And so that includes, you know, public engagement, things like this, right? It includes teaching.
So I'm running a tutorial at ACL this summer with their coveans and a scotheld on integrating ethics
into the NLP curriculum so that people hopefully early on in their training will start to see this
stuff. And then yeah, talking about it sort of on Twitter, say. How well do you feel the
core, like failure modes, ethical failure modes of NLP, you know, not even a mentioned machine learning,
but NLP are understood. Like I think we throw around the example, like these word-to-vec examples,
you know, man is to doctor as woman is to whatever. You know, I think we understand those failures
pretty well or at least conceptually. But I'm trying to think of other ones that I've heard
thrown around and they're very few. Yeah. So, so they're right. So you have one failure mode,
failure mode, I'm not so it's one of these like it's actually working as intended and we don't
want that kind of failure mode, right? So of these, when the word vector is feature not a bug,
you commented on that podcast recently. I love that episode. Yes, exactly. So it's like that.
It's a feature not a bug because it's doing what we told it to do and maybe we need to rethink
what we're telling it to do, right? Because there is an awful lot of bias in our conceptualization of
the world that comes out and how we talk about the world. And then if you say learn the patterns,
guess what? That's one of the patterns, right? And it's not even necessarily a pattern in the
world that it's learning. It's a pattern in how we talk about the world. And I think that a
mistake that often gets made in these discussions of machine reading, naturally understanding,
is that the text is the world and it's not. It's how some collection of people decided to talk
about the world. So there's a further distinction there. So that's a quote unquote failure mode.
You have cases, right? I mean, it's right because it's a feature not a bug, but we don't want a feature.
You have cases where the system just doesn't do the thing it's supposed to do, right? So
speech recognition and it gives you gibberish instead of what the person said or speech recognition
and it just doesn't give you any words for a little while. Machine translation and it gives you
something that maybe sounds perfectly fluent because the language model is doing its job,
but it's a bad representation of the source language intent. And that's I think a particularly
insidious kind of failure mode because you don't know that it's failed. It looks fluent. And so if
if as a consumer of that technology, you're not savvy to the fact that it's just making a guess,
you might believe that the person whose words you've translated actually said the thing that you're
reading when in fact they didn't. Interesting. And then so what I'm hearing here is you know,
you're saying things that sound like systems or models just breaking. But then when you take those
and put those, you know, let's call them technical failures into the world, they are having effects
and that creates ethical issues, but they're also potentially caused by things like the
style of language that someone's using or accents or things like that. And that also creates
ethical issues. Yeah. So the general framework that I am working towards for thinking about this
is basically whenever we are moving away from this is a fun abstract toy that we're playing with
to this is something that's going to be in the world. We have a responsibility to think about a few
things and one of them is what are the failure modes? That's a question you're asking. Another one
is when this is working as intended, who benefits and who's possibly harmed? And when it's not working
as intended, who was harmed? Because those harms are not going to be evenly distributed
in many, many cases. So this thing about social linguistics and the fact that people speak
differently and that people who are marginalized tend to then be told that their language
variety is not standard and it's not the one that's getting modeled means that when the system fails
to work because of a difference in in accentual linguistic variety, that's going to fall differently
across different groups of people. But I have to always when I get down this part of the line of
reasoning, refer to Alvin Grissom's talk that I heard last summer where he said, you know,
sometimes working as intended is really not anything we want. And so sometimes it's better to
have it not work for you, right? If it's being used for surveillance, then you're happy when it
can't understand you, provided that it's not understanding you just means it says there's
nothing here, as opposed to misunderstanding and then attributing, you know, bad intent when
you've said something completely innocuous. Right, right. I'll refer folks to my interview with
Alvin from February of last year, mythologies of neuromodils and interpretability.
Great, great talk. So when you talk about, again, kind of going back to this promoting engagement
around these issues, have, you know, part of the challenge I think is, you know, generally getting
folks to care about impacts on classes of people that don't involve them. Like, is that something that
you, you know, study or at least study the people that are studying that and can kind of talk
about how folks are thinking about that or who we should look to. So I don't have the expertise
there. I can tell you just what I've learned by teaching this stuff. So I can tell you sort of my
own experience reports about talking to people about this, but you're absolutely right that we
should be looking to, you know, people who know about movement building and about sort of bringing
people in towards working towards shared causes like that. And that's, that's not me. But certainly,
like there was a wonderful podcast episode of Radical AI with Ruha Benjamin on and she had wonderful
things to say about how to connect with existing work going on in the community rather than thinking
you're the one solving it. So I think there is expertise out there that should be called on.
What I've seen is that most people that I talk to and, you know, sample bias, I tend to talk to
people who are interested in this. Both people that I talk to would like to be sure that the
technology they're creating is making the world a better place and not causing harm and sort of
feel uncomfortable with engaging with these issues. And so it's easier to just not think about it.
It's easier to say, oh, I'm only working on the algorithm. And so one strategy is to say, well,
what can you do to make it a little bit better? And that's what we're going with the data statements
work is if you make clear documentation of what's in the data, then you have set it up so that people
can say, hey, guess what? This user is not a good match for my use case. And so that's sort of a
a positive step that's also close to our own research expertise, this dataset creators,
and ought to be close to a research expertise as system developers who are using the data, right?
You'd better understand what's in that dataset that you're using to test the machine learner if
you're going to claim that you're solving some task. So all of that I think is very in domain
and a positive step. And I have my hope, and this is now just like, you know, pure speculation and
hope, is that when people feel like they can do something like this, they then feel more attached
to the longer term goal of making sure that they're building things that don't exacerbate inequities
and maybe actually start being a force for good overall. Yeah. And you just mentioned the data
statements, right? When I haven't looked into that in detail, but when I hear that, envisioning
something very similar to the data sheets for datasets work, the Timnick Ebru is done.
Similar to that, what are the key differences? Absolutely similar. And in fact, it was sort of
like there was something in the air in 2018. So you have Timnick's group doing the data sheets,
Meg Mitchell's group doing model cards. And there was another group at the MIT Media Lab doing
something called nutrition labels for datasets. And yes, absolutely very similar ideas. I think the
thing that distinguishes data statements is that it was very focused on language datasets. So
we're thinking about what do you need to document about a language dataset. The data sheets work
is absolutely wonderful in terms of thinking about what do you need across machine learning datasets
broadly and very, very congruent. And I think I mean, maybe Timnick and Meg were talking to each
other, but there was sort of this moment we all kind of looked around and went, oh same.
So yeah, absolutely, absolutely that sort of the same idea. Got it. Got it. Awesome. Well, Emily,
it has been so wonderful having an opportunity to chat with you. Any parting thoughts
that you'd like to leave us with? This has been a blast. I'm sorry, we only saw the cat the
once. And I really, you know, I hope that your listeners are remain interested in both how do you
create a clever machine learning solution? And how do you think about how it fits into the world?
And if that second part is hard, then who do you need to talk to to figure out how it fits into
the world for the particular task that you're working on? Great. Thanks so much. Thank you. All right,
everyone, that's our show for today. To learn more about today's guest or the topics mentioned in
this interview, visit twimmelai.com. Of course, if you like what you hear on the podcast, please
subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for listening
and catch you next time.
