1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,960
I'm your host Sam Charrington.

4
00:00:31,960 --> 00:00:36,200
As many of you already know, one of the exciting parts of my work involves keeping tabs on

5
00:00:36,200 --> 00:00:40,800
the way large companies are adopting machine learning, deep learning, and AI.

6
00:00:40,800 --> 00:00:44,920
While it's still fairly early in the game, we're at a really interesting time for many

7
00:00:44,920 --> 00:00:45,920
companies.

8
00:00:45,920 --> 00:00:50,400
With the first wave of ML projects at early adopter enterprises starting to mature, many

9
00:00:50,400 --> 00:00:56,720
organizations are now asking how they can scale up their efforts to support more projects.

10
00:00:56,720 --> 00:01:01,160
Part of the answer to successfully scaling ML is supporting data scientists and machine

11
00:01:01,160 --> 00:01:05,480
learning engineers with modern processes, tooling, and platforms.

12
00:01:05,480 --> 00:01:10,100
This is the topic that we're super excited to address here on the podcast with the AI

13
00:01:10,100 --> 00:01:14,880
Platforms podcast series that you're currently listening to, as well as a series of ebooks

14
00:01:14,880 --> 00:01:17,400
that we'll be publishing on the topic.

15
00:01:17,400 --> 00:01:21,480
The first of these ebooks takes a bottoms up look at AI platforms and is focused on the

16
00:01:21,480 --> 00:01:27,000
open source Kubernetes project, which is used to deliver scalable ML infrastructure at

17
00:01:27,000 --> 00:01:31,680
places like Airbnb, booking.com, and open AI.

18
00:01:31,680 --> 00:01:35,640
The second book in the series, which looks at scaling data science and ML engineering

19
00:01:35,640 --> 00:01:41,560
from the top down, explores the internal platforms companies like Airbnb, Facebook, and Uber

20
00:01:41,560 --> 00:01:45,400
have built, and what enterprises can learn from them.

21
00:01:45,400 --> 00:01:50,240
If this is the topic you're interested in, I'd encourage you to visit twimmelai.com slash

22
00:01:50,240 --> 00:01:56,600
AI platforms and sign up to be notified as soon as these books are published.

23
00:01:56,600 --> 00:02:02,000
In this episode of our AI Platforms series, we're joined by Atul Kale, engineering manager

24
00:02:02,000 --> 00:02:06,120
on the machine learning infrastructure team at Airbnb.

25
00:02:06,120 --> 00:02:10,960
Atul and I met at the Stradded Data Conference a while back to discuss Airbnb's internal

26
00:02:10,960 --> 00:02:13,840
machine learning platform, Big Head.

27
00:02:13,840 --> 00:02:19,880
In our conversation, Atul outlines the ML lifecycle at Airbnb and how the various components

28
00:02:19,880 --> 00:02:21,720
of Big Head support it.

29
00:02:21,720 --> 00:02:26,180
We then dig into those components, which include Red Spot, their supercharged Jupyter

30
00:02:26,180 --> 00:02:32,680
Notebook service, Deep Thought, their real-time inference environment, Zipline, their data management

31
00:02:32,680 --> 00:02:35,560
platform, and quite a few others.

32
00:02:35,560 --> 00:02:39,800
We also take a look at some of Atul's best practices for scaling machine learning and

33
00:02:39,800 --> 00:02:44,280
discuss a special announcement that he and his team made at Stradda.

34
00:02:44,280 --> 00:02:48,040
And now on to the show.

35
00:02:48,040 --> 00:02:50,640
All right, everyone.

36
00:02:50,640 --> 00:02:54,520
I am in New York City here for the Stradda Conference, and I've got the pleasure of being

37
00:02:54,520 --> 00:02:56,800
seated with Atul Kale.

38
00:02:56,800 --> 00:03:02,480
Atul is an engineering manager at Airbnb on the machine learning infrastructure team.

39
00:03:02,480 --> 00:03:05,880
Atul, welcome to this week in machine learning and AI.

40
00:03:05,880 --> 00:03:06,880
Thanks, Sam.

41
00:03:06,880 --> 00:03:07,880
Thanks for having me.

42
00:03:07,880 --> 00:03:09,640
It's a pleasure to be here.

43
00:03:09,640 --> 00:03:16,440
So yesterday you did a presentation on Airbnb's machine learning platform, which is called

44
00:03:16,440 --> 00:03:17,440
Big Head.

45
00:03:17,440 --> 00:03:21,880
It was a great presentation, and I'm really looking forward to diving into some of the

46
00:03:21,880 --> 00:03:22,880
details with you.

47
00:03:22,880 --> 00:03:26,680
But before we do that, how did you get into ML and AI?

48
00:03:26,680 --> 00:03:27,680
Yeah, sure.

49
00:03:27,680 --> 00:03:30,160
So I'm trained as a computer engineer.

50
00:03:30,160 --> 00:03:35,560
I went to the University of Illinois, and right out of college, I actually started my

51
00:03:35,560 --> 00:03:40,320
career in the trading industry at a firm called DRW Trading.

52
00:03:40,320 --> 00:03:45,960
And DRW, they're proprietary trading firms, so they trade their own money for their own

53
00:03:45,960 --> 00:03:46,960
profit.

54
00:03:46,960 --> 00:03:48,200
They don't have any customers.

55
00:03:48,200 --> 00:03:53,640
And you can imagine for a trading firm like DRW, it's really important to have a data

56
00:03:53,640 --> 00:03:57,800
warehouse that's stockful of information about the financial markets.

57
00:03:57,800 --> 00:04:04,320
So imagine you're working on a particular trading strategy, and you need something kind

58
00:04:04,320 --> 00:04:07,280
of like Tivo for the markets, right?

59
00:04:07,280 --> 00:04:11,280
So you need to be able to understand that a very fine granularity exactly what was going

60
00:04:11,280 --> 00:04:15,600
on on a particular exchange or in the markets for a particular instrument.

61
00:04:15,600 --> 00:04:22,560
So my first job at DRW was to create that data warehouse, you know, everything from kind

62
00:04:22,560 --> 00:04:28,080
of the basic ETL and even like the data capture and all that sort of stuff, as well as

63
00:04:28,080 --> 00:04:31,120
like that back end database that's required.

64
00:04:31,120 --> 00:04:37,200
But then after I completed working on that, I made a transition towards working on trading

65
00:04:37,200 --> 00:04:41,320
strategies, and that's kind of where I got more into machine learning.

66
00:04:41,320 --> 00:04:46,120
So we went through kind of two different trading strategies.

67
00:04:46,120 --> 00:04:52,160
One was kind of light on the machine learning, and one was really heavy on machine learning.

68
00:04:52,160 --> 00:04:57,040
And in that latter one, I started really focusing on infrastructure.

69
00:04:57,040 --> 00:05:03,000
I think in general, my background is in back end work and infrastructure engineering.

70
00:05:03,000 --> 00:05:06,160
So that's really, you know, where my passion is.

71
00:05:06,160 --> 00:05:10,440
And for the trading strategies that we were working on, we really needed a high degree

72
00:05:10,440 --> 00:05:15,240
of automation and sophistication around training our models.

73
00:05:15,240 --> 00:05:22,040
So I spent quite a bit of time working on building infrastructure for my individual team.

74
00:05:22,040 --> 00:05:27,680
And then, you know, about a year and a half ago, I was kind of looking to make a transition,

75
00:05:27,680 --> 00:05:34,080
and I got a good opportunity at Airbnb, you know, working on their machine learning infrastructure

76
00:05:34,080 --> 00:05:40,800
team, kind of combining this passion for infrastructure with the interests I have in machine learning.

77
00:05:40,800 --> 00:05:47,520
So I've been working there for about the last year or so, and I'm currently leading the

78
00:05:47,520 --> 00:05:48,520
team.

79
00:05:48,520 --> 00:05:53,640
Where you explained the motivation for big head in your presentation, I thought was very

80
00:05:53,640 --> 00:05:54,640
well put.

81
00:05:54,640 --> 00:06:00,120
You talked about this notion of inherent complexity and incidental complexity.

82
00:06:00,120 --> 00:06:01,120
What are those?

83
00:06:01,120 --> 00:06:02,120
Right.

84
00:06:02,120 --> 00:06:03,120
Yeah.

85
00:06:03,120 --> 00:06:07,600
So I think the term that we use is intrinsic complexity and incidental complexity.

86
00:06:07,600 --> 00:06:13,200
We kind of just maybe use terms up, but the idea is pretty simple that the intrinsic

87
00:06:13,200 --> 00:06:19,600
complexity with machine learning is all about, you know, kind of understanding the latest

88
00:06:19,600 --> 00:06:24,720
modeling techniques, picking the right model, picking the right features for your model.

89
00:06:24,720 --> 00:06:29,240
And then really fine tuning that, you know, machine learning in some ways is kind of an

90
00:06:29,240 --> 00:06:34,920
art of, you know, really understanding your problem domain and fitting the right models

91
00:06:34,920 --> 00:06:36,320
to it.

92
00:06:36,320 --> 00:06:41,720
And that's really what we found that our ML practitioners at Airbnb, by that I mean,

93
00:06:41,720 --> 00:06:48,360
now in the data scientists and engineers that are working on deploying ML in our product,

94
00:06:48,360 --> 00:06:52,200
we found that they're really interested in solving those problems.

95
00:06:52,200 --> 00:06:57,520
But on the other hand, there's this other side of getting a machine learning project

96
00:06:57,520 --> 00:06:58,520
off the ground.

97
00:06:58,520 --> 00:06:59,920
And that's incidental complexity.

98
00:06:59,920 --> 00:07:01,440
That's what we call it.

99
00:07:01,440 --> 00:07:06,560
And that has to do with all like the messy details of, you know, getting access to your

100
00:07:06,560 --> 00:07:12,480
data warehouse and then making sure that what you prototype with is, you know, what is

101
00:07:12,480 --> 00:07:15,920
consistent with what you actually go into production with.

102
00:07:15,920 --> 00:07:22,960
And, you know, just having consistency also between your, your trained model and what

103
00:07:22,960 --> 00:07:27,360
you're going to actually see in production at inference time, for example.

104
00:07:27,360 --> 00:07:33,720
So there's a lot of like messy details there, you know, you also have to deal with managing

105
00:07:33,720 --> 00:07:37,240
all the various experiments that you've got, the versions of your model.

106
00:07:37,240 --> 00:07:40,760
And you know, all of that just adds up to a lot.

107
00:07:40,760 --> 00:07:45,440
And we, what we found is that when teams were having to deal with intrinsic complexity

108
00:07:45,440 --> 00:07:50,760
and incidental complexity together, that's just, you know, overwhelming for them.

109
00:07:50,760 --> 00:07:57,520
And a lot of teams at Airbnb were just not even that interested in machine learning just

110
00:07:57,520 --> 00:08:00,320
because the cost of it was so high.

111
00:08:00,320 --> 00:08:06,160
So part of the goal is to reduce the barrier to entry so that a greater proportion of

112
00:08:06,160 --> 00:08:09,840
teams could incorporate machine learning into what they're doing.

113
00:08:09,840 --> 00:08:10,840
Exactly.

114
00:08:10,840 --> 00:08:11,840
Yeah.

115
00:08:11,840 --> 00:08:16,640
I mean, our goal as a team, the machine learning instructor team is really to scale ML

116
00:08:16,640 --> 00:08:18,440
at Airbnb.

117
00:08:18,440 --> 00:08:22,800
And in order to reach that goal, of course, you know, we're going to need to reduce the

118
00:08:22,800 --> 00:08:24,240
barrier to entry.

119
00:08:24,240 --> 00:08:26,720
So that's really important to us.

120
00:08:26,720 --> 00:08:33,560
What did it evolve? Was it initially a collection of specific tools that solve specific problems

121
00:08:33,560 --> 00:08:38,200
or was it architected from the beginning as kind of a broad platform?

122
00:08:38,200 --> 00:08:39,200
Oh, yeah.

123
00:08:39,200 --> 00:08:40,200
Definitely.

124
00:08:40,200 --> 00:08:42,160
We did not get this right from the start.

125
00:08:42,160 --> 00:08:49,280
I think that, yeah, as with many products, it's just like, you know, cycle of iteration

126
00:08:49,280 --> 00:08:55,320
and, you know, lessons learned and then applying those lessons learned and building something

127
00:08:55,320 --> 00:08:56,320
new.

128
00:08:56,320 --> 00:09:01,400
The history for us specifically is that, you know, big heads components, sort of the

129
00:09:01,400 --> 00:09:08,160
subcomponents of big head are their pieces that Airbnb had kind of collected over the

130
00:09:08,160 --> 00:09:14,360
years, all to solve, you know, really specific problems with machine learning, with getting

131
00:09:14,360 --> 00:09:16,400
machine learning into production.

132
00:09:16,400 --> 00:09:20,160
And we found that that was kind of piecemeal, you know, of course, those problems were

133
00:09:20,160 --> 00:09:26,520
solved, sometimes not in the most sustainable way, but it solved nonetheless.

134
00:09:26,520 --> 00:09:30,360
But then, you know, when we looked at the entire system and we looked at the shortcomings,

135
00:09:30,360 --> 00:09:35,160
the problem was really that it wasn't cohesive, all these piecemeal components didn't really

136
00:09:35,160 --> 00:09:36,600
fit well together.

137
00:09:36,600 --> 00:09:41,600
So big head was an attempt to kind of make one more pass over the entire system, the

138
00:09:41,600 --> 00:09:48,200
entire architecture, and really focus this time on cohesiveness and end time consistency.

139
00:09:48,200 --> 00:09:52,160
Maybe let's walk through the various components of the system.

140
00:09:52,160 --> 00:09:53,160
Where do you start?

141
00:09:53,160 --> 00:09:55,720
Oh, you went through this in your presentation yesterday.

142
00:09:55,720 --> 00:09:57,720
What was the first component that you presented?

143
00:09:57,720 --> 00:09:58,720
Yeah, sure.

144
00:09:58,720 --> 00:10:01,480
So we start with where our users start, right?

145
00:10:01,480 --> 00:10:03,400
So our users need to prototype.

146
00:10:03,400 --> 00:10:08,840
They need kind of a development environment for machine learning and for that we have

147
00:10:08,840 --> 00:10:13,520
a red spot, which is really just a Jupyter Notebook service.

148
00:10:13,520 --> 00:10:18,000
It's a Jupyter Notebooks as a service, but you know, they're really not just, you know,

149
00:10:18,000 --> 00:10:21,640
the same as running Notebook on your laptop.

150
00:10:21,640 --> 00:10:23,040
It's pretty high-powered.

151
00:10:23,040 --> 00:10:30,160
You can plug it into more or less, you know, any EC2 instance type on AWS that we have

152
00:10:30,160 --> 00:10:31,680
available.

153
00:10:31,680 --> 00:10:38,040
So you can get access to things like GPUs and really high-memory instances.

154
00:10:38,040 --> 00:10:41,840
So you can be kind of unblocked to get your work done.

155
00:10:41,840 --> 00:10:48,200
But then in addition, it's really cleanly integrated into our data warehouse.

156
00:10:48,200 --> 00:10:51,560
So you have access to all the wealth of data that's going to actually make your machine

157
00:10:51,560 --> 00:10:53,280
learning project powerful.

158
00:10:53,280 --> 00:10:58,840
Yeah, one of the things that I've run into just with personal experimentation is you

159
00:10:58,840 --> 00:11:03,980
choose an instance type, you start running some experiment and then you realize that you

160
00:11:03,980 --> 00:11:06,080
get to a point where you wish you had more hardware.

161
00:11:06,080 --> 00:11:10,880
Like, have you managed to decouple the notebook itself from the underlying infrastructure?

162
00:11:10,880 --> 00:11:14,120
Yeah, yeah, and that's a great point.

163
00:11:14,120 --> 00:11:18,960
I think this is actually something that we're looking to address in the future.

164
00:11:18,960 --> 00:11:24,240
It's not just that you might not know exactly what hardware you need, but also it's pretty

165
00:11:24,240 --> 00:11:32,200
efficient or inefficient rather for every user that might need to train a deep neural network

166
00:11:32,200 --> 00:11:39,440
to fire up a GPU machine and have it sitting around for weeks maybe while they tinker

167
00:11:39,440 --> 00:11:41,680
around with their model, right?

168
00:11:41,680 --> 00:11:44,920
So that's definitely an area of concern.

169
00:11:44,920 --> 00:11:50,160
You can't vertically scale individuals' development environment for long.

170
00:11:50,160 --> 00:11:56,000
So what we're doing, and this is kind of on our roadmap, is to build out what we're calling

171
00:11:56,000 --> 00:11:58,040
Big Q right now.

172
00:11:58,040 --> 00:12:02,800
The idea is that your notebook environment can then become really thin and instead what

173
00:12:02,800 --> 00:12:09,200
you do is you just submit your job for training your model off to a work queue that's

174
00:12:09,200 --> 00:12:15,920
powered by GPUs or potentially the latest in distributed training algorithm, something

175
00:12:15,920 --> 00:12:16,920
like that.

176
00:12:16,920 --> 00:12:21,160
This is sort of a follow-on to the rest of our infrastructure where the goal is to really

177
00:12:21,160 --> 00:12:23,520
neatly encapsulate models.

178
00:12:23,520 --> 00:12:30,040
Once we have that powerful abstraction, now we can create these high-power tools like

179
00:12:30,040 --> 00:12:36,320
Big Q where we understand enough about your model that we can train it really efficiently.

180
00:12:36,320 --> 00:12:39,880
So it's definitely kind of out there on our roadmap.

181
00:12:39,880 --> 00:12:44,040
I think it was one of those things where we took an initial stab and we looked at our

182
00:12:44,040 --> 00:12:49,240
user base and we said, well, yeah, this kind of works for most people, and so it's definitely

183
00:12:49,240 --> 00:12:55,240
kind of minimally viable there, but we have a lot of plans to make that more efficient.

184
00:12:55,240 --> 00:13:02,520
And then underlying the notebooks, all of the code is stored in Git repositories, and

185
00:13:02,520 --> 00:13:10,760
is that transparent to the user, or is the user kind of manually checking things and

186
00:13:10,760 --> 00:13:13,560
checking things out?

187
00:13:13,560 --> 00:13:17,760
The user is indicating that they want to check things in, but have you built kind of

188
00:13:17,760 --> 00:13:21,440
in UI into the notebook experience where they're doing that or they're doing that kind

189
00:13:21,440 --> 00:13:22,440
of traditionally?

190
00:13:22,440 --> 00:13:24,440
They're doing that more traditionally.

191
00:13:24,440 --> 00:13:30,960
I think that we can try to hide away some of the details of version control, but ultimately

192
00:13:30,960 --> 00:13:38,000
it's kind of just a complicated messy process, version control in general.

193
00:13:38,000 --> 00:13:42,360
So we don't really want to try to reinvent the wheel on that.

194
00:13:42,360 --> 00:13:48,320
What we focus on is just making sure that it's easy to get your notebook or Python code

195
00:13:48,320 --> 00:13:54,280
into production, and that it's neatly version just like any other production code.

196
00:13:54,280 --> 00:14:00,680
And now Python code typically has, or the notebook code typically has annotations and

197
00:14:00,680 --> 00:14:08,640
other types of artifacts that you don't necessarily want to, you know, they're there for experimentation,

198
00:14:08,640 --> 00:14:14,040
but they're not there for, you know, when you're actually trying to put a model into production,

199
00:14:14,040 --> 00:14:16,120
do you separate those somehow?

200
00:14:16,120 --> 00:14:17,120
Yeah, yeah.

201
00:14:17,120 --> 00:14:23,080
So I think just to start, I would say that we don't necessarily focus on notebooks.

202
00:14:23,080 --> 00:14:28,520
We really just treat notebooks as Python code, and what we do is pre-process those notebooks

203
00:14:28,520 --> 00:14:35,640
to kind of strip out the annotations and strip out some of the exploratory work that

204
00:14:35,640 --> 00:14:38,280
you might be doing while you're prototyping.

205
00:14:38,280 --> 00:14:41,360
And you know, right now, of course, that's pretty simple.

206
00:14:41,360 --> 00:14:48,120
You just have to tag your notebook cells as prototype or not, and we just strip them

207
00:14:48,120 --> 00:14:49,120
out.

208
00:14:49,120 --> 00:14:50,120
Maybe who knows?

209
00:14:50,120 --> 00:14:53,000
We'll come up with a machine learning model to do that automatically or something.

210
00:14:53,000 --> 00:14:58,520
But yeah, it's pretty much manually tagged for now, and we've actually been doing that

211
00:14:58,520 --> 00:15:02,240
for a while, and it seems to work reasonably well.

212
00:15:02,240 --> 00:15:08,400
But I think that one thing to note is that notebooks are kind of a mixed bag.

213
00:15:08,400 --> 00:15:14,640
On the one hand, you've got this ability to annotate things, to show your work, and

214
00:15:14,640 --> 00:15:19,400
to, you know, really show the process of experimentation.

215
00:15:19,400 --> 00:15:24,680
And that's great for machine learning projects where, especially when you get to kind of code

216
00:15:24,680 --> 00:15:29,320
review time, what you're reviewing when you look at somebody else's project isn't really

217
00:15:29,320 --> 00:15:30,480
just their code.

218
00:15:30,480 --> 00:15:34,920
You're reviewing kind of their thought process, their experimentation process.

219
00:15:34,920 --> 00:15:40,720
So it's almost kind of more like a research paper or something like that than it is just,

220
00:15:40,720 --> 00:15:42,280
you know, a normal code review.

221
00:15:42,280 --> 00:15:44,920
So notebooks are great for that.

222
00:15:44,920 --> 00:15:52,040
But they're not great for is kind of the traditional software engineering process.

223
00:15:52,040 --> 00:15:57,320
Composition is in great, you know, of course, you can jam all of your code right into the

224
00:15:57,320 --> 00:16:03,200
notebook, but, you know, a lot of times we find people repeating themselves.

225
00:16:03,200 --> 00:16:09,240
And testing, unit testing, this is super important for software engineering, but it's equal

226
00:16:09,240 --> 00:16:12,400
important for some of this machine learning code.

227
00:16:12,400 --> 00:16:19,960
And I think we're trying to strike the right balance between having those really solid

228
00:16:19,960 --> 00:16:26,920
software engineering practices as well as having that flexibility of the notebook.

229
00:16:26,920 --> 00:16:29,760
So what's the next component?

230
00:16:29,760 --> 00:16:35,640
So next up in sort of the lifecycle for getting your model into production, that would be

231
00:16:35,640 --> 00:16:36,640
Big Head Service.

232
00:16:36,640 --> 00:16:43,560
So the idea with Big Head Service is to manage all of the different versions of your model,

233
00:16:43,560 --> 00:16:47,760
maybe the different experiments that you have, or you can imagine that say you have a model

234
00:16:47,760 --> 00:16:49,840
that needs to be trained weekly.

235
00:16:49,840 --> 00:16:54,200
All of those versions of that model are stored in Big Head Service.

236
00:16:54,200 --> 00:16:59,880
And it's got this nice convenient UI where you can go in and click deploy on your model

237
00:16:59,880 --> 00:17:02,600
and voila, it's in production.

238
00:17:02,600 --> 00:17:07,160
And kind of, you know, for a lot of users, we're hoping that, you know, they start with

239
00:17:07,160 --> 00:17:12,080
their notebook, they go into Big Head Service, they hit deploy, and end of story.

240
00:17:12,080 --> 00:17:14,960
That's sort of the happy path, of course.

241
00:17:14,960 --> 00:17:20,160
You know, and maybe you don't have to go even further than that, but, of course, things

242
00:17:20,160 --> 00:17:21,960
do go wrong.

243
00:17:21,960 --> 00:17:26,840
And when they do, Big Head Service also kind of serves as a portal into some of our model

244
00:17:26,840 --> 00:17:28,320
debugging tools.

245
00:17:28,320 --> 00:17:32,040
And those are more related to how we run our models in production.

246
00:17:32,040 --> 00:17:39,200
So the next pieces in the process are to actually get your model into production.

247
00:17:39,200 --> 00:17:45,480
So say, you've got a model that takes an image of, I don't know, like a listing photo,

248
00:17:45,480 --> 00:17:49,000
and predicts whether it's a bathroom or not.

249
00:17:49,000 --> 00:17:54,040
And you need to be able to do this live in real time in production.

250
00:17:54,040 --> 00:18:00,200
Maybe it's hooked up to the Airbnb website, and you want to, when you, when a host uploads

251
00:18:00,200 --> 00:18:04,760
a listing photo, you want to see if it's a bathroom to ask them something like, you

252
00:18:04,760 --> 00:18:08,880
know, how many towels are there, do you have a hair dryer, you know, little details

253
00:18:08,880 --> 00:18:09,880
like that.

254
00:18:09,880 --> 00:18:13,600
So say there's some kind of feature, and you need just sort of a live prediction.

255
00:18:13,600 --> 00:18:17,680
That's where our component deep dot comes in.

256
00:18:17,680 --> 00:18:24,960
It's just designed to be scalable, offer like a rest endpoint for your model.

257
00:18:24,960 --> 00:18:28,880
And that's, that's something I think that's pretty standard we've seen across the industry.

258
00:18:28,880 --> 00:18:33,560
And then we've also got another productionization component that's called ML Automator.

259
00:18:33,560 --> 00:18:38,400
The idea here is to automate batch training and batch inference.

260
00:18:38,400 --> 00:18:45,920
And I think at Airbnb, maybe more than other, other places batch inference is actually a

261
00:18:45,920 --> 00:18:48,040
pretty common thing.

262
00:18:48,040 --> 00:18:50,760
So there's a lot of times where you don't really need a live score.

263
00:18:50,760 --> 00:18:57,200
You just need to kind of backfill a lot of predictions or scores across your data.

264
00:18:57,200 --> 00:19:00,560
And so that's where ML Automator comes in.

265
00:19:00,560 --> 00:19:06,640
And so when you're, when the developer is working in the big head service interface and

266
00:19:06,640 --> 00:19:11,080
they click deploy, that's kind of pushing the model to deep thought deploying it out on

267
00:19:11,080 --> 00:19:14,360
some actual machines and exposing the rest endpoint.

268
00:19:14,360 --> 00:19:19,080
And then they can build that into whatever they're trying to use the model for.

269
00:19:19,080 --> 00:19:20,080
Yeah, exactly.

270
00:19:20,080 --> 00:19:25,320
If they have their model set up for live inference, then when you click deploy, it's just like

271
00:19:25,320 --> 00:19:28,960
deploying a service at Airbnb, right?

272
00:19:28,960 --> 00:19:33,640
It's kind of a new version of that service that's going out there and that endpoint is now

273
00:19:33,640 --> 00:19:34,640
available.

274
00:19:34,640 --> 00:19:41,760
And you know, we do some things to try to make sure that that endpoint stays available for,

275
00:19:41,760 --> 00:19:48,880
you know, spikes in traffic or, you know, potentially if that endpoint, that particular model needs

276
00:19:48,880 --> 00:19:52,800
a lot of memory, maybe to load a bunch of parameters into memory.

277
00:19:52,800 --> 00:19:57,320
So we do a lot of fine tuning there to make sure that more or less we kind of hide that

278
00:19:57,320 --> 00:20:03,120
away from our users who don't necessarily want to deal with the details of scaling their

279
00:20:03,120 --> 00:20:04,120
model.

280
00:20:04,120 --> 00:20:05,120
Right.

281
00:20:05,120 --> 00:20:08,360
And hopefully, you know, since we're leveraging Kubernetes for that, we're hoping that

282
00:20:08,360 --> 00:20:14,600
we can really get some of the benefits of potentially auto scaling algorithms there.

283
00:20:14,600 --> 00:20:16,600
And has that, has that worked well so far?

284
00:20:16,600 --> 00:20:20,000
Yeah, Kubernetes has been definitely working well so far.

285
00:20:20,000 --> 00:20:25,600
I think auto scaling is a little bit trickier, but that's something that right now we're

286
00:20:25,600 --> 00:20:28,440
just kind of getting into.

287
00:20:28,440 --> 00:20:37,200
And are you using, are you kind of finally managing GPU requirements and things like that

288
00:20:37,200 --> 00:20:42,880
that a given model might have and placing those on specific pods or workers in the Kubernetes

289
00:20:42,880 --> 00:20:43,880
environments?

290
00:20:43,880 --> 00:20:44,880
Yeah, yeah, definitely.

291
00:20:44,880 --> 00:20:50,840
So when we ask our users to specify their model, we ask them to specify their requirements

292
00:20:50,840 --> 00:20:53,000
for a computation up front.

293
00:20:53,000 --> 00:20:59,200
So that includes like memory requirements or a number of cores or potentially GPU.

294
00:20:59,200 --> 00:21:02,720
And then we use that throughout the rest of our system to make sure that their model

295
00:21:02,720 --> 00:21:09,680
runs an environment suitable for their resource requirements in production.

296
00:21:09,680 --> 00:21:15,200
I think earlier in the conversation, you've mentioned experiment management and things like

297
00:21:15,200 --> 00:21:16,200
that.

298
00:21:16,200 --> 00:21:20,560
But I don't think that came up in big head service deep thought or ML automator.

299
00:21:20,560 --> 00:21:22,640
Is it some place else in the system?

300
00:21:22,640 --> 00:21:24,120
No, actually, that's right.

301
00:21:24,120 --> 00:21:25,920
And that's very built into big head service.

302
00:21:25,920 --> 00:21:31,840
So yeah, I think that like this whole process of getting your model from prototype and

303
00:21:31,840 --> 00:21:34,680
clicking the deploy button and its introduction, that's great.

304
00:21:34,680 --> 00:21:39,120
But it's actually kind of scary, you know, just getting your model in a production with just

305
00:21:39,120 --> 00:21:41,960
a quick deploy, you know, is it, is it ready?

306
00:21:41,960 --> 00:21:43,280
Is it predictive?

307
00:21:43,280 --> 00:21:46,280
Do you know whether it even works or not?

308
00:21:46,280 --> 00:21:52,040
So that's where big head service offers a whole lot of kind of introspection tools into

309
00:21:52,040 --> 00:21:53,560
your model.

310
00:21:53,560 --> 00:21:59,640
And you know, it's like some of the examples of that, you know, if we wrap a particular

311
00:21:59,640 --> 00:22:06,520
library called, like let's say, XGBoost, we're able to produce a lot of details about the

312
00:22:06,520 --> 00:22:14,400
train model that you have right from XGBoost, you can of course, like, get details like

313
00:22:14,400 --> 00:22:22,120
ROC or PR curves, and you can also, you know, for XGBoost, get feature importance information.

314
00:22:22,120 --> 00:22:28,320
And both of those types of information, both the actual performance of your model and

315
00:22:28,320 --> 00:22:32,840
the, you know, explainability of your model, that's really important before you click

316
00:22:32,840 --> 00:22:33,840
deploy.

317
00:22:33,840 --> 00:22:36,680
And you can kind of surface that to our users in big head service.

318
00:22:36,680 --> 00:22:42,800
And of course, all of these tools that we have, particularly visualizations around this,

319
00:22:42,800 --> 00:22:45,200
they're all available in your notebook environment.

320
00:22:45,200 --> 00:22:50,880
So we kind of use the visualizations that we see people doing in their notebook environment.

321
00:22:50,880 --> 00:22:55,840
And the most common ones, we kind of elevate them to the status of being incorporated into

322
00:22:55,840 --> 00:22:57,320
our infrastructure.

323
00:22:57,320 --> 00:23:01,640
If they kind of reach that point where we really see everybody doing this all the time,

324
00:23:01,640 --> 00:23:05,720
or we might be thinking, you know what, feature importance, that's probably something useful

325
00:23:05,720 --> 00:23:07,640
that everybody should see.

326
00:23:07,640 --> 00:23:10,120
So we'll just automatically incorporate it.

327
00:23:10,120 --> 00:23:13,240
So this is all built right into the UI.

328
00:23:13,240 --> 00:23:21,000
Are you doing anything in big head service or ML automator that is doing automated hyperparameter

329
00:23:21,000 --> 00:23:22,600
optimization?

330
00:23:22,600 --> 00:23:25,480
So that's kind of an area that we're hoping to get into.

331
00:23:25,480 --> 00:23:30,960
Right now, the training phase is, and especially hyperparameter optimization, that's sort of

332
00:23:30,960 --> 00:23:34,680
left to the user way at prototype time.

333
00:23:34,680 --> 00:23:38,240
But one of the things that we want to do with Big Q, which I mentioned before, is offer

334
00:23:38,240 --> 00:23:40,040
hyperparameter optimization.

335
00:23:40,040 --> 00:23:46,840
And I think that part of what we need to do here is just have people specify their model

336
00:23:46,840 --> 00:23:51,680
to us in a little bit more structured of a way than, all right, here's a Python function

337
00:23:51,680 --> 00:23:53,520
that trains my model.

338
00:23:53,520 --> 00:23:57,040
For us to be able to do something like hyperparameter optimization.

339
00:23:57,040 --> 00:24:03,640
So once we have kind of better adoption of that, then we can move on to that next level

340
00:24:03,640 --> 00:24:08,160
of abstraction on top of this, where we can automatically train your models.

341
00:24:08,160 --> 00:24:12,920
And that includes hyperparameter optimization, but also like I mentioned before, using

342
00:24:12,920 --> 00:24:18,240
like advanced training algorithms, whatever you need.

343
00:24:18,240 --> 00:24:26,120
So one of the other platforms in the Big Head ecosystem is Zipline, which handles a lot

344
00:24:26,120 --> 00:24:29,880
of the training data, pipeline data acquisition.

345
00:24:29,880 --> 00:24:31,800
Can you talk a little bit about that?

346
00:24:31,800 --> 00:24:32,800
Yeah, yeah, sure.

347
00:24:32,800 --> 00:24:38,200
So this entire process that I mentioned, you know, going from prototype into production,

348
00:24:38,200 --> 00:24:41,560
there's a lot of ways that it can just go wrong on you.

349
00:24:41,560 --> 00:24:45,600
And what we found is like consistency across the board is really key.

350
00:24:45,600 --> 00:24:51,920
So in addition to the four components I mentioned already, we've got three other components

351
00:24:51,920 --> 00:24:56,440
whose job is purely just to maintain consistency.

352
00:24:56,440 --> 00:25:01,320
Some of that is around consistency of your environment, and maybe the code that you're

353
00:25:01,320 --> 00:25:08,360
executing, the actual logic of your model, and some of it is around data.

354
00:25:08,360 --> 00:25:13,720
Now data is super important for machine learning projects, having the right feature data.

355
00:25:13,720 --> 00:25:19,840
And so we created Zipline as sort of a solution for keeping consistency across the board.

356
00:25:19,840 --> 00:25:25,880
And I think that, you know, this is probably one of the areas I think that we see sometimes

357
00:25:25,880 --> 00:25:31,720
overlooked, and it's definitely a piece that's really hard to get right.

358
00:25:31,720 --> 00:25:38,360
But when you do it, when you have a single system that owns getting feature data for the prototype

359
00:25:38,360 --> 00:25:43,920
phase when you're just kind of playing around, and you don't, you know, want to wait three

360
00:25:43,920 --> 00:25:49,000
days for your backfill job to generate all your data, but also having that same system

361
00:25:49,000 --> 00:25:55,840
be responsible for materializing your data when you need to train, you know, the entire

362
00:25:55,840 --> 00:26:01,160
large scale model, and maybe perform batch inference on the model, or even real time inference

363
00:26:01,160 --> 00:26:07,160
with the model, having one system that is responsible for features that's really key.

364
00:26:07,160 --> 00:26:12,640
And it's a pretty hard goal to get to, but we're taking a stab at it with Zipline.

365
00:26:12,640 --> 00:26:18,960
Yeah, I think one of the things that I remember picking up on in your talk, maybe

366
00:26:18,960 --> 00:26:22,920
it was another talk, I don't know if I should attribute this to you or if it was someone

367
00:26:22,920 --> 00:26:29,800
else, but there was a comment that someone made at this event about data scientists not

368
00:26:29,800 --> 00:26:32,080
wanting to share their features.

369
00:26:32,080 --> 00:26:33,800
Do you run into that?

370
00:26:33,800 --> 00:26:34,800
That's interesting.

371
00:26:34,800 --> 00:26:41,600
You know, I guess maybe the culture is very from company to company, but at least at

372
00:26:41,600 --> 00:26:45,040
Airbnb, we've gotten a lot of value out of sharing.

373
00:26:45,040 --> 00:26:51,680
Now of course, there's situations where, you know, it's not so much about wanting to

374
00:26:51,680 --> 00:26:56,440
share your features, but you know, machine learning and features in general, it's all

375
00:26:56,440 --> 00:26:59,200
really tied together.

376
00:26:59,200 --> 00:27:04,120
You can't really separate exactly how your features are calculated from how your model

377
00:27:04,120 --> 00:27:05,120
works.

378
00:27:05,120 --> 00:27:10,720
And let's say you create some features and I want to use those features and maybe you're

379
00:27:10,720 --> 00:27:14,360
not quite happy with that implementation of those features and you want to change them

380
00:27:14,360 --> 00:27:15,360
down the line.

381
00:27:15,360 --> 00:27:20,360
Well, now I've already got my machine learning model in production.

382
00:27:20,360 --> 00:27:22,320
That's going to be really disruptive to my flow.

383
00:27:22,320 --> 00:27:27,920
So I think that maybe some data scientists might be a little bit more cautious about sharing

384
00:27:27,920 --> 00:27:33,440
what they're working on because, you know, as soon as you share something, you kind of

385
00:27:33,440 --> 00:27:36,200
need to keep it a little bit more constant.

386
00:27:36,200 --> 00:27:40,200
But what we found is that, you know, as long as we're able to appropriately version our

387
00:27:40,200 --> 00:27:45,280
feature definitions and kind of deal with it in the same way that you might have a library

388
00:27:45,280 --> 00:27:46,520
that you depend on.

389
00:27:46,520 --> 00:27:52,960
You know, maybe you depend on TensorFlow version, you know, X, but TensorFlow version Y has

390
00:27:52,960 --> 00:27:53,960
been released.

391
00:27:53,960 --> 00:27:57,080
And as long as you don't upgrade, you know, you're fine.

392
00:27:57,080 --> 00:28:01,040
And I think that's a pretty important part of our system.

393
00:28:01,040 --> 00:28:08,000
And the way that works, you know, every single time there's a new version of a particular

394
00:28:08,000 --> 00:28:12,240
feature, we had to go and back the level of a bunch of data up front before you could

395
00:28:12,240 --> 00:28:13,240
use it.

396
00:28:13,240 --> 00:28:14,400
That would be pretty painful.

397
00:28:14,400 --> 00:28:22,480
So this only works when you kind of lazily evaluate your feature data sets, just kind

398
00:28:22,480 --> 00:28:23,480
of on demand.

399
00:28:23,480 --> 00:28:27,280
And that's something that's like a key focus for zipline.

400
00:28:27,280 --> 00:28:31,000
So how are the features defined?

401
00:28:31,000 --> 00:28:37,600
The basic way that you define a feature in zipline, you'll define kind of some information

402
00:28:37,600 --> 00:28:42,800
about where to fetch the events for that feature and potentially some aggregation that you're

403
00:28:42,800 --> 00:28:45,320
going to apply over those events.

404
00:28:45,320 --> 00:28:49,720
And we kind of try to model roughly everything as events, but sometimes that's a little bit

405
00:28:49,720 --> 00:28:51,520
of a tough fit.

406
00:28:51,520 --> 00:28:57,080
And then, you know, so now you've kind of told zipline how your feature needs to be calculated

407
00:28:57,080 --> 00:28:59,560
at any given point in time.

408
00:28:59,560 --> 00:29:05,440
But zipline doesn't know what points in time, you know, you actually need your feature

409
00:29:05,440 --> 00:29:07,040
information.

410
00:29:07,040 --> 00:29:10,400
So what it does at that point is not much.

411
00:29:10,400 --> 00:29:11,760
It just kind of keeps track of that.

412
00:29:11,760 --> 00:29:16,480
But then when you come to it and you say, all right, I need a training data set.

413
00:29:16,480 --> 00:29:23,160
And, you know, I need to be able to see my features over the last year.

414
00:29:23,160 --> 00:29:27,560
And here's all the particular timestamps I need, my features calculated at.

415
00:29:27,560 --> 00:29:32,760
At that point, zipline gets to work, gathering all the information it needs and efficiently

416
00:29:32,760 --> 00:29:35,400
aggregating everything and materializing your data set.

417
00:29:35,400 --> 00:29:42,480
So you can imagine, you know, let's say that you have a page on the Airbnb website and

418
00:29:42,480 --> 00:29:47,160
you're trying to predict whether a user visiting that page is likely to book.

419
00:29:47,160 --> 00:29:52,880
And you want to know maybe the like last seven days of history in the last seven days,

420
00:29:52,880 --> 00:29:55,760
how many bookings did that user make?

421
00:29:55,760 --> 00:29:57,680
Maybe that's like a feature into your model.

422
00:29:57,680 --> 00:30:03,560
So specifying, you know, where you get the information about bookings and that you want

423
00:30:03,560 --> 00:30:09,400
a seven day sum or average or something like that, that's sort of the feature definition

424
00:30:09,400 --> 00:30:10,640
side of it.

425
00:30:10,640 --> 00:30:16,320
And then the next side of it is when you actually need to materialize your training data set,

426
00:30:16,320 --> 00:30:22,520
you actually give zipline a series of timestamps and say, okay, for these users, at these

427
00:30:22,520 --> 00:30:28,640
times, give me the information about what their seven day booking history was.

428
00:30:28,640 --> 00:30:32,800
So there's kind of that two step approach and that lets us materialize the data only when

429
00:30:32,800 --> 00:30:34,160
we need to.

430
00:30:34,160 --> 00:30:35,880
And where are those timestamps coming from?

431
00:30:35,880 --> 00:30:43,000
Is this so that you can compare models generated at different times by kind of rolling back

432
00:30:43,000 --> 00:30:46,720
the clock and looking at the training data that the models saw when they were trained,

433
00:30:46,720 --> 00:30:49,600
or are there other use cases?

434
00:30:49,600 --> 00:30:57,520
So I think the way to think about it is the training data set that we're creating, the

435
00:30:57,520 --> 00:31:01,720
columns in that training data set are the features that you might define.

436
00:31:01,720 --> 00:31:05,600
Maybe like your seven day booking sum or something like that.

437
00:31:05,600 --> 00:31:11,080
And the rows are really all of your data points for your machine learning model, right?

438
00:31:11,080 --> 00:31:19,480
So say that a user goes and views a particular page and at that point in time, we want to

439
00:31:19,480 --> 00:31:22,960
know what the state of all these various features were.

440
00:31:22,960 --> 00:31:27,600
And we know at another point in time in the future, what actually happened with that user,

441
00:31:27,600 --> 00:31:28,600
right?

442
00:31:28,600 --> 00:31:30,920
Like did they book or were they just viewing the page?

443
00:31:30,920 --> 00:31:34,840
So each of those data points, you can think of like imagine you have like millions of

444
00:31:34,840 --> 00:31:41,000
page views over the course of, you know, some time period, each of those data points ends

445
00:31:41,000 --> 00:31:45,280
up being an input into your machine learning model, right?

446
00:31:45,280 --> 00:31:49,600
So does the event that actually happened plus all of these features that you think are

447
00:31:49,600 --> 00:31:50,600
relevant to that?

448
00:31:50,600 --> 00:31:51,600
Exactly.

449
00:31:51,600 --> 00:31:52,600
Exactly.

450
00:31:52,600 --> 00:31:57,200
So you can think of it as zipline materializing that data set for you.

451
00:31:57,200 --> 00:32:02,800
And there is of course kind of like the separate question of machine learning model evolution

452
00:32:02,800 --> 00:32:07,200
over time and how training data is impacting that.

453
00:32:07,200 --> 00:32:11,800
And I think that's sort of somewhat of a higher level problem than just directly the

454
00:32:11,800 --> 00:32:15,000
future data that involves the model as a whole as well.

455
00:32:15,000 --> 00:32:18,560
And that's something that we're hoping that the UI visualization tools and think that

456
00:32:18,560 --> 00:32:23,000
service help you do is to kind of see how your model is trending over time.

457
00:32:23,000 --> 00:32:33,000
And it sounds like zipline's ability to access training data relative to these timestamps

458
00:32:33,000 --> 00:32:40,400
could feed into the ability to, you know, if he's into reproducibility, like can I,

459
00:32:40,400 --> 00:32:46,560
you know, can I reproduce the steps that went into creating this model, you know, if I'm

460
00:32:46,560 --> 00:32:54,680
training it at a different time and it sounds like the having an ability to have just having

461
00:32:54,680 --> 00:33:00,560
the flexibility to manipulate training data with respect to time is feet would feed into

462
00:33:00,560 --> 00:33:01,560
that.

463
00:33:01,560 --> 00:33:02,560
Yeah.

464
00:33:02,560 --> 00:33:03,560
Yeah.

465
00:33:03,560 --> 00:33:04,560
Exactly.

466
00:33:04,560 --> 00:33:07,360
And I think like one way to explain zipline is we like to use this analogy.

467
00:33:07,360 --> 00:33:10,160
It's like a time machine for your data warehouse, right?

468
00:33:10,160 --> 00:33:14,200
You know, you have all these events that are data points into your machine learning model

469
00:33:14,200 --> 00:33:19,680
and they happen, you know, all throughout the continuous space of time that you have in

470
00:33:19,680 --> 00:33:21,000
your data warehouse.

471
00:33:21,000 --> 00:33:27,480
But your data warehouse isn't really suited for time in as much of a continuous sense.

472
00:33:27,480 --> 00:33:34,000
You know, it's more suited for kind of large level, large scale aggregation.

473
00:33:34,000 --> 00:33:39,520
And at least at Airbnb, our data warehouse is very much oriented in like sort of a day

474
00:33:39,520 --> 00:33:43,200
by day snapshot of the world.

475
00:33:43,200 --> 00:33:45,440
And that works really well for humans.

476
00:33:45,440 --> 00:33:50,200
You know, humans can't really process things at a millisecond or but second by second level

477
00:33:50,200 --> 00:33:51,200
anyways.

478
00:33:51,200 --> 00:33:54,200
So we of course need to aggregate it.

479
00:33:54,200 --> 00:34:00,040
But for machines, you really need that high precision at every single point in time for

480
00:34:00,040 --> 00:34:02,920
your feature data to actually be useful.

481
00:34:02,920 --> 00:34:10,120
So having that ability to kind of like rewind time and go to any particular point and say

482
00:34:10,120 --> 00:34:13,960
what was the exact state of the world at that moment that my machine learning algorithm

483
00:34:13,960 --> 00:34:18,640
would have seen when it's trying to make this prediction, that's really, really valuable.

484
00:34:18,640 --> 00:34:24,080
It's actually something that kind of is parallel to what I worked on before in the trading

485
00:34:24,080 --> 00:34:28,240
industry where you want to be able to rewind the clock and go exactly to a point in time

486
00:34:28,240 --> 00:34:30,800
and see the state of the market.

487
00:34:30,800 --> 00:34:37,240
And then of course, there's that other part of using your model and that is actually

488
00:34:37,240 --> 00:34:39,600
performing inference live, right?

489
00:34:39,600 --> 00:34:45,600
So if you have a great system set up to rewind to any particular point in time, that same

490
00:34:45,600 --> 00:34:51,480
system, if it's responsible for calculating your features live right now, then you have

491
00:34:51,480 --> 00:34:58,200
that absolute consistency between getting gathering all your trading data and actually

492
00:34:58,200 --> 00:35:02,320
running your model in production and creating live predictions.

493
00:35:02,320 --> 00:35:08,120
And what we found is that when people don't have one system to do both those things,

494
00:35:08,120 --> 00:35:13,920
they end up kind of doing a little bit of manual ETL in their data warehouse and then maybe

495
00:35:13,920 --> 00:35:16,960
in their live service, they try to replicate that.

496
00:35:16,960 --> 00:35:18,760
And that leads to a lot of problems.

497
00:35:18,760 --> 00:35:23,760
Now you've got two code bases doing basically the same thing, maintained by potentially

498
00:35:23,760 --> 00:35:25,720
different people.

499
00:35:25,720 --> 00:35:29,720
So there's a lot of opportunity for those things to become inconsistent and besides, it's

500
00:35:29,720 --> 00:35:31,800
just kind of wasted effort.

501
00:35:31,800 --> 00:35:43,480
So having that kind of single system that encompasses both your offline data warehouse work and

502
00:35:43,480 --> 00:35:48,160
your online live service work, that's really important.

503
00:35:48,160 --> 00:35:54,360
You mentioned in constructing these features, you're specifying aggregations that are happening

504
00:35:54,360 --> 00:36:01,920
over the underlying events, in the case of, for example, a deep learning pipeline, you

505
00:36:01,920 --> 00:36:08,720
may also want to do some stand that I said of data augmentation or data transformations,

506
00:36:08,720 --> 00:36:15,440
things like that, image transformations, does big head or zip line support those types

507
00:36:15,440 --> 00:36:16,440
of things?

508
00:36:16,440 --> 00:36:17,440
Yeah, yeah.

509
00:36:17,440 --> 00:36:21,080
And this is actually an interesting question.

510
00:36:21,080 --> 00:36:27,680
In general, when it comes to forming our feature data, like before we actually go and submit

511
00:36:27,680 --> 00:36:32,360
it into a machine learning algorithm, we do a lot of pre-processing to it.

512
00:36:32,360 --> 00:36:39,120
And some of that pre-processing is things like I've talked about before, potentially doing

513
00:36:39,120 --> 00:36:48,960
aggregations, maybe sums across several days of data, maybe if you're trying to get a

514
00:36:48,960 --> 00:36:53,680
history of seven day bookings, like I mentioned before, but then some of it is just like

515
00:36:53,680 --> 00:36:59,080
extra-processing that we're doing, imputation or normalization.

516
00:36:59,080 --> 00:37:06,720
And we're trying to find the right place to draw the line where that actually happens.

517
00:37:06,720 --> 00:37:11,280
Does it happen in zip line where you're materializing your feature data data set, or does it

518
00:37:11,280 --> 00:37:16,080
happen in what we call your model's pipeline?

519
00:37:16,080 --> 00:37:23,880
And that means what's built into your model directly before your data enters into a machine

520
00:37:23,880 --> 00:37:25,320
learning algorithm.

521
00:37:25,320 --> 00:37:33,320
So that's a tricky balance, but I think that one lesson we've learned there is that if

522
00:37:33,320 --> 00:37:40,880
your model is seeing those same features and doing that same imputation or normalization

523
00:37:40,880 --> 00:37:46,040
in production, maybe on live queries that it's getting, then that needs to be more

524
00:37:46,040 --> 00:37:48,800
built into sort of the model pipeline.

525
00:37:48,800 --> 00:37:53,720
And if in production, and if you're trying to reproduce something that in your backend

526
00:37:53,720 --> 00:37:59,800
data warehouse that maybe the model would have just seen directly from the queries it's

527
00:37:59,800 --> 00:38:03,400
getting from a live service, then that kind of belongs more in zip line.

528
00:38:03,400 --> 00:38:10,800
So it's sort of a balance between kind of doing some computation upfront in feature

529
00:38:10,800 --> 00:38:14,480
calculation and doing some of it as part of the model.

530
00:38:14,480 --> 00:38:17,640
And I think we're still kind of trying to figure that out.

531
00:38:17,640 --> 00:38:26,160
And so for those types of transformations that should happen in the model, we've talked

532
00:38:26,160 --> 00:38:33,480
about this Jupyter Notebook experience is there also some kind of workflow or pipeline

533
00:38:33,480 --> 00:38:39,560
engine, or have you thought about and decided against that?

534
00:38:39,560 --> 00:38:46,840
Do you are there templates or a library of sorts that data scientists and ML engineers

535
00:38:46,840 --> 00:38:47,840
have access to?

536
00:38:47,840 --> 00:38:51,280
How do you address those kind of abstraction types of issues?

537
00:38:51,280 --> 00:38:53,080
Yeah, this is an excellent question.

538
00:38:53,080 --> 00:39:01,480
So in the early days for us, we did something kind of really lightweight in terms of the

539
00:39:01,480 --> 00:39:03,520
interface that we provided our users.

540
00:39:03,520 --> 00:39:09,040
We asked them to define basically a Python function that trains your model and another

541
00:39:09,040 --> 00:39:12,600
one that kind of performs inference with your model.

542
00:39:12,600 --> 00:39:18,120
It's just like, hey, there's two Python functions, fill these in, you're good to go.

543
00:39:18,120 --> 00:39:21,480
And there's some trade-offs with that.

544
00:39:21,480 --> 00:39:25,320
So on the one hand, it's great with the flexibility.

545
00:39:25,320 --> 00:39:29,040
You can really almost plug in almost anything.

546
00:39:29,040 --> 00:39:35,680
And it's really simple for people to take what they had already been doing likely in their

547
00:39:35,680 --> 00:39:40,920
notebook and just get it more or less straight into production by just kind of filling out

548
00:39:40,920 --> 00:39:41,920
these functions.

549
00:39:41,920 --> 00:39:47,400
But what we found was that it was kind of lacking.

550
00:39:47,400 --> 00:39:54,880
We found that people were reinventing the wheel a lot when they were writing these models

551
00:39:54,880 --> 00:39:56,160
for production.

552
00:39:56,160 --> 00:40:01,960
They're really redoing a lot of work that had been done before by themselves in another

553
00:40:01,960 --> 00:40:03,480
model or by somebody else.

554
00:40:03,480 --> 00:40:09,760
So we decided to go kind of add a little bit more structure to that workflow and have

555
00:40:09,760 --> 00:40:12,760
them instead create what we call an ML pipeline.

556
00:40:12,760 --> 00:40:18,360
It's really kind of just a pipeline of transformations that happened to your input data before it's

557
00:40:18,360 --> 00:40:19,360
output.

558
00:40:19,360 --> 00:40:27,000
And a model you can think of the output, sometimes it's a prediction, but really it's just

559
00:40:27,000 --> 00:40:29,840
a function that's transforming some input.

560
00:40:29,840 --> 00:40:35,240
So what we've done is we've tried to create a really generic interface for just specifying

561
00:40:35,240 --> 00:40:40,080
these transformation pieces and plugging them in together in kind of this like really easy

562
00:40:40,080 --> 00:40:46,080
to compose way that you can just create a entire end to end pipeline to do some stuff

563
00:40:46,080 --> 00:40:52,880
like image preprocessing or imputation or normalization and then have that be just

564
00:40:52,880 --> 00:40:57,800
enough structure that we can actually understand a little bit more about your model and do

565
00:40:57,800 --> 00:41:04,520
some more intelligent things than we could if we just had a function to call for training.

566
00:41:04,520 --> 00:41:10,560
So for example, if we know that your model is a deep neural network and maybe on the

567
00:41:10,560 --> 00:41:15,680
back end we've enabled this feature that gives us maybe distributed training or something

568
00:41:15,680 --> 00:41:21,920
like that, just having a little bit extra information from our users that slightly more

569
00:41:21,920 --> 00:41:28,280
onerous interface for them to specify their model, that's really useful, that's powerful.

570
00:41:28,280 --> 00:41:34,200
But that's definitely a trade off because our users are looking at that interface and

571
00:41:34,200 --> 00:41:36,520
they're wondering why do I need to use this.

572
00:41:36,520 --> 00:41:44,640
So I want to just, I already have my model or this is really complicated, I know TensorFlow

573
00:41:44,640 --> 00:41:47,200
or I know Keras, can I just use that.

574
00:41:47,200 --> 00:41:53,400
So we have to be really careful about adding value in that interface and keeping it really

575
00:41:53,400 --> 00:41:55,200
lightweight.

576
00:41:55,200 --> 00:42:00,440
And we try to add value of course through our infrastructure but you then end the prototyping

577
00:42:00,440 --> 00:42:01,440
phase.

578
00:42:01,440 --> 00:42:07,880
We need our users to adopt these tools up front, this is something we've learned and

579
00:42:07,880 --> 00:42:13,360
we found that other companies have also come to this realization that those tools that

580
00:42:13,360 --> 00:42:19,680
you use to productionize your model can't just be incorporated at that point where you're

581
00:42:19,680 --> 00:42:20,680
ready to productionize.

582
00:42:20,680 --> 00:42:25,560
They need to be used up front as much as possible because that way you'll really lower that

583
00:42:25,560 --> 00:42:28,680
barrier to entry into production.

584
00:42:28,680 --> 00:42:33,320
What's the experience or the interface for these pipelines?

585
00:42:33,320 --> 00:42:39,160
Yeah, so you can think of it kind of like plugging in Lego blocks together, you hopefully

586
00:42:39,160 --> 00:42:47,120
the blocks that you need already exist, things like imputation or normalization and you kind

587
00:42:47,120 --> 00:42:54,280
of have an easy way to, I talk about it, we call it a pipeline but it's really kind of

588
00:42:54,280 --> 00:43:00,960
a dag of computation that's happening on your input feature vector before it produces

589
00:43:00,960 --> 00:43:01,960
some output.

590
00:43:01,960 --> 00:43:05,640
So really what you're doing, we're trying to create a lightweight way for you to create

591
00:43:05,640 --> 00:43:12,800
that computation dag, that having to think too much about the details of what dag are.

592
00:43:12,800 --> 00:43:20,080
But we create this pipeline for you and fortunately I can't, the best way to explain it is really

593
00:43:20,080 --> 00:43:26,080
to look at a code sample of how you create a pipeline but I can try to describe it badly.

594
00:43:26,080 --> 00:43:31,760
That answers the question for me, it was really is this something that's defining code

595
00:43:31,760 --> 00:43:42,600
or is there some user interface and is there some notion of a repository of steps that

596
00:43:42,600 --> 00:43:48,400
I might want to plug in that I'm doing visually or do I have to know what those are and

597
00:43:48,400 --> 00:43:52,680
like I'm using a library, a standard library of some sorts and call them from within code.

598
00:43:52,680 --> 00:43:55,440
It sounds like it's the code orientation.

599
00:43:55,440 --> 00:44:01,040
Yeah, definitely, there's several different ways to do things you could prefer a config,

600
00:44:01,040 --> 00:44:06,800
you could prefer code, you could prefer UIs for managing this but what we found is code

601
00:44:06,800 --> 00:44:09,200
seems to be the best way to go.

602
00:44:09,200 --> 00:44:14,280
A lot of times what we see is people doing powerful stuff like programmatically creating

603
00:44:14,280 --> 00:44:21,400
these pipelines or composing them and I think that's a lot easier with code and I think

604
00:44:21,400 --> 00:44:27,440
that to address the piece about sharing, yeah of course, we don't want all of our users

605
00:44:27,440 --> 00:44:33,360
to be reinventing all of these different LEGO blocks so we have our inventory of pipeline

606
00:44:33,360 --> 00:44:38,400
transformations that they can just plug into but they can also kind of share those pieces

607
00:44:38,400 --> 00:44:45,000
that they've created so that they can easily maybe if somebody on another team is working

608
00:44:45,000 --> 00:44:50,560
on a particular model that is sort of relevant, maybe it's an image model and you're also

609
00:44:50,560 --> 00:44:52,440
working on an image model.

610
00:44:52,440 --> 00:44:58,640
You can actually go to the UI and browse and click in and see what pipeline transformation

611
00:44:58,640 --> 00:45:04,080
components they're using and say, hey, oh yeah, you know what, I actually do need a piece

612
00:45:04,080 --> 00:45:08,440
that kind of like resizes my image or something like that so I'll just go ahead and use

613
00:45:08,440 --> 00:45:09,440
what you have.

614
00:45:09,440 --> 00:45:14,840
So that was something really important for us to incorporate into our kind of discoverability

615
00:45:14,840 --> 00:45:16,920
and sharing aspect of our UI.

616
00:45:16,920 --> 00:45:18,840
Okay, awesome, awesome.

617
00:45:18,840 --> 00:45:27,480
So for folks that are maturing in their use of machine learning, AI, but don't have

618
00:45:27,480 --> 00:45:32,920
a platform in place, are there kind of a handful of core principles that they should be

619
00:45:32,920 --> 00:45:38,320
keeping in mind as they start to coalesce different tools into, you know, something that's

620
00:45:38,320 --> 00:45:39,320
more coherent?

621
00:45:39,320 --> 00:45:41,280
Yeah, yeah, absolutely.

622
00:45:41,280 --> 00:45:47,320
So I mean, we've talked to a lot of companies and they're all on this, on various stages

623
00:45:47,320 --> 00:45:51,720
of this journey in scaling ML at their organization, right?

624
00:45:51,720 --> 00:45:57,440
You've got, you know, the big tech giants like Google and Facebook who are really far

625
00:45:57,440 --> 00:46:01,640
along have been doing this for years and, you know, I think they're getting closer to

626
00:46:01,640 --> 00:46:07,360
that ideal of, you know, engineers or data scientists, everybody feeling like they have

627
00:46:07,360 --> 00:46:10,560
ML in their toolkit.

628
00:46:10,560 --> 00:46:16,040
But, you know, and I think Uber is actually coming along on that as well with Michelangelo.

629
00:46:16,040 --> 00:46:19,760
Michelangelo is a major inspiration for us.

630
00:46:19,760 --> 00:46:25,000
And we've talked to other companies as well who are, like, kind of going down that journey.

631
00:46:25,000 --> 00:46:27,720
And, you know, we're seeing a lot of the same problems.

632
00:46:27,720 --> 00:46:35,880
I think that two things that I would mention, one is consistency, be thinking about consistency

633
00:46:35,880 --> 00:46:41,520
in, you know, all aspects of the lifecycle of deploying your machine learning model.

634
00:46:41,520 --> 00:46:46,440
And that's what pieces like the big head library that we talked about and a zip line are

635
00:46:46,440 --> 00:46:48,280
meant to address.

636
00:46:48,280 --> 00:46:54,720
And I think that data is, you know, really probably one of the hardest things to get right.

637
00:46:54,720 --> 00:47:02,880
So, my other point is about, you know, understanding the value of making sure that data and the

638
00:47:02,880 --> 00:47:07,880
entire pipeline leading up to machine learning at your company is really well set up for

639
00:47:07,880 --> 00:47:08,880
machine learning.

640
00:47:08,880 --> 00:47:14,360
I think that, you know, something that we've experienced at Airbnb is that machine learning

641
00:47:14,360 --> 00:47:18,560
is kind of at the tail end of a whole lot of work that's happening upstream.

642
00:47:18,560 --> 00:47:23,440
You know, you've got your production services, the databases they use, maybe the data warehouse,

643
00:47:23,440 --> 00:47:27,240
and then lastly, you've got machine learning tacked on to the end of that.

644
00:47:27,240 --> 00:47:31,680
And so, one of the things we've noticed is that there's sort of a butterfly effect, you

645
00:47:31,680 --> 00:47:37,360
know, upstream changes can have a really outsized impact on machine learning.

646
00:47:37,360 --> 00:47:44,240
And one of the things we're trying to change is just getting more visibility around machine

647
00:47:44,240 --> 00:47:49,680
learning as something that's being used downstream throughout the company.

648
00:47:49,680 --> 00:47:56,560
And I think that that's like kind of a broader organizational type problem that you need

649
00:47:56,560 --> 00:47:58,120
to be able to solve.

650
00:47:58,120 --> 00:48:03,440
Getting awareness about machine learning, what the benefits of it are, and then also having

651
00:48:03,440 --> 00:48:08,360
people incorporate it into their workflow and their understanding of kind of, like, what's

652
00:48:08,360 --> 00:48:09,680
downstream.

653
00:48:09,680 --> 00:48:15,360
So, yeah, data is, of course, like, very critical and very hard to get right, and really

654
00:48:15,360 --> 00:48:20,960
kind of unique to every different organization, the types of data that's important to you,

655
00:48:20,960 --> 00:48:23,520
the pipelines you have set up for it.

656
00:48:23,520 --> 00:48:28,120
But I think there's certainly some tools and best practices that are emerging throughout

657
00:48:28,120 --> 00:48:29,120
the industry.

658
00:48:29,120 --> 00:48:35,560
I think it all kind of boils down to this analogy of using some of, like, the best practices

659
00:48:35,560 --> 00:48:41,000
from software engineering that we've accumulated over the decades and applying them to machine

660
00:48:41,000 --> 00:48:42,000
learning.

661
00:48:42,000 --> 00:48:47,640
I mean, sometimes this analogy doesn't work great, but as with all analogies, but I think

662
00:48:47,640 --> 00:48:49,200
it can kind of go a long way.

663
00:48:49,200 --> 00:48:55,800
So, making sure that you have a good developer environment, making sure that, you know,

664
00:48:55,800 --> 00:49:04,600
you have solid versioning, unit testing, continuous build deployment, monitoring, observability,

665
00:49:04,600 --> 00:49:08,600
these are all things that we've kind of arrived on for software engineering, traditional

666
00:49:08,600 --> 00:49:10,320
software engineering.

667
00:49:10,320 --> 00:49:14,720
And one of the things we're trying to do is just translate those concepts to machine

668
00:49:14,720 --> 00:49:15,720
learning.

669
00:49:15,720 --> 00:49:22,320
Understand, like, what does it mean to unit test your model or integration test your model?

670
00:49:22,320 --> 00:49:24,520
What does it mean to, like, code review a model, right?

671
00:49:24,520 --> 00:49:29,000
This is also a best practice for software engineering, but we need to kind of reinvent

672
00:49:29,000 --> 00:49:32,120
it for machine learning.

673
00:49:32,120 --> 00:49:35,320
And yeah, what does observability look like for a model?

674
00:49:35,320 --> 00:49:42,920
So that's some of, like, the, in kind of a high level way, how we're going about approaching

675
00:49:42,920 --> 00:49:44,920
scaling ML.

676
00:49:44,920 --> 00:49:50,680
And we should probably also mention that you announced in the talk that Airbnb is planning

677
00:49:50,680 --> 00:49:52,080
to open source big head.

678
00:49:52,080 --> 00:49:53,080
Yeah, that's right.

679
00:49:53,080 --> 00:50:00,160
So Airbnb, we've got kind of a culture of being a host to our community.

680
00:50:00,160 --> 00:50:05,080
And I think that's why there's been several open source projects from Airbnb, like Air

681
00:50:05,080 --> 00:50:07,160
Flow and Super Set.

682
00:50:07,160 --> 00:50:12,040
And we're hoping to actually open source big head as well in the coming months.

683
00:50:12,040 --> 00:50:13,720
We're really excited about that.

684
00:50:13,720 --> 00:50:20,560
It's kind of something that our entire team has been thinking about since the start.

685
00:50:20,560 --> 00:50:23,720
It certainly takes a lot of work to actually make happen.

686
00:50:23,720 --> 00:50:27,920
So we're hoping to get to that in Q1 of 2019.

687
00:50:27,920 --> 00:50:28,920
Awesome.

688
00:50:28,920 --> 00:50:29,920
Awesome.

689
00:50:29,920 --> 00:50:31,240
Well, I'll be keeping track of it.

690
00:50:31,240 --> 00:50:34,200
And it's an exciting step.

691
00:50:34,200 --> 00:50:35,200
Thanks so much.

692
00:50:35,200 --> 00:50:36,200
I'll appreciate it.

693
00:50:36,200 --> 00:50:37,200
Yeah.

694
00:50:37,200 --> 00:50:42,200
Thanks for having me, Sam.

695
00:50:42,200 --> 00:50:43,200
All right, everyone.

696
00:50:43,200 --> 00:50:45,000
That's our show for today.

697
00:50:45,000 --> 00:50:50,520
For more information on Atul or any of the topics covered in this podcast, visit twimmelai.com

698
00:50:50,520 --> 00:50:54,080
slash talk slash 198.

699
00:50:54,080 --> 00:50:59,880
To learn more about our AI platform series, or to download our eBooks, visit twimmelai.com

700
00:50:59,880 --> 00:51:02,400
slash AI platforms.

701
00:51:02,400 --> 00:51:29,160
As always, thanks so much for listening and catch you next time.

