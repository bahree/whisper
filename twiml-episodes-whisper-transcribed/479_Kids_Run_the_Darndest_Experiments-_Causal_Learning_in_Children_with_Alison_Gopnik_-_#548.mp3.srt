1
00:00:00,000 --> 00:00:15,840
All right, everyone. I am here with Allison Gopnik. Allison is a professor at the University

2
00:00:15,840 --> 00:00:20,560
of California at Berkeley. Allison, welcome to the Twoma AI podcast.

3
00:00:20,560 --> 00:00:25,880
I'm very happy to be here. I'm really looking forward to our conversation. We will be focusing

4
00:00:25,880 --> 00:00:31,640
in on a presentation that you'll be delivering at this year's NERVS conference focused on

5
00:00:31,640 --> 00:00:38,040
causal learning in children and how that relates to some of the things that we're doing in AI

6
00:00:38,040 --> 00:00:42,760
and deep learning. Before we dive into that though, I'd love to have you share a little bit about

7
00:00:42,760 --> 00:00:48,920
your background and you come at things from a psychology perspective. Tell us a little bit

8
00:00:48,920 --> 00:00:54,280
about your background and how you came to that field. Yeah, so I actually started out my career

9
00:00:54,280 --> 00:00:59,160
in philosophy and I'm still an affiliate in philosophy. So I have sort of three different

10
00:00:59,160 --> 00:01:04,040
hats at Berkeley. I'm in the psychology department and affiliate in philosophy and then part of the

11
00:01:04,040 --> 00:01:10,040
the Bear, the Berkeley AI research group. And the big question that I've really wanted to answer

12
00:01:10,040 --> 00:01:15,720
my whole career is this, how is it that we can know so much about the world around us from so

13
00:01:15,720 --> 00:01:22,120
little information? All we have is the photons in the back of our retinas and the disturbances of

14
00:01:22,120 --> 00:01:28,600
air at our ears and yet people can figure out that the world is full of objects and people and

15
00:01:28,600 --> 00:01:35,400
thoughts and ideas and, you know, quirks and black holes. How is that possible? And of course,

16
00:01:35,400 --> 00:01:39,960
that's the big problem of epistemology and philosophy, which is where I started out. How could

17
00:01:39,960 --> 00:01:44,360
you possibly do that? And it's the big problem of machine learning. It's the sort of central problem

18
00:01:44,360 --> 00:01:51,080
of machine learning. How could we get representations from data? And in a way, the way that the big

19
00:01:51,080 --> 00:01:57,800
problem, I mean, a way of putting this is we seem to have these very powerful abstract structured

20
00:01:57,800 --> 00:02:02,440
representations of the world around us that let us make great generalizations and predictions.

21
00:02:02,440 --> 00:02:07,880
And yet the data that we're getting doesn't seem to is concrete and particular and doesn't seem

22
00:02:07,880 --> 00:02:12,040
to have those characteristics of being abstract and unstructured. So the question is how do we get

23
00:02:12,040 --> 00:02:19,320
there here from there? Well, very early in my career, I realized, look, the people who are doing that

24
00:02:19,320 --> 00:02:23,720
most effectively are actually young children. They're the ones who are going out into the world,

25
00:02:23,720 --> 00:02:29,240
taking what they see in here and the actions they perform and figuring out what the world is like.

26
00:02:29,240 --> 00:02:35,000
So if we want to answer that question, either as philosophers or as people in AI, the people we

27
00:02:35,000 --> 00:02:39,560
should look to, the people who are really solving that problem are young children. And basically,

28
00:02:39,560 --> 00:02:44,280
that's what I've been doing for my whole career is trying to figure out how is it that young children,

29
00:02:44,280 --> 00:02:50,520
you know, two, three, four year olds who don't aren't, you know, don't have PhDs in computer science,

30
00:02:51,480 --> 00:02:56,760
aren't philosophers. Nevertheless, seem to be solving these problems that have really stumped

31
00:02:57,880 --> 00:03:04,520
the smartest minds in philosophy and AI. And about 20 years ago, I started actually collaborating

32
00:03:04,520 --> 00:03:09,080
with people in both philosophy of science and in computer science to try and figure out,

33
00:03:09,080 --> 00:03:13,800
could we say something computationally? What kinds of representations and algorithms could the kids

34
00:03:13,800 --> 00:03:19,960
be using that let them learn as much as they do? And that's basically been the project ever since.

35
00:03:19,960 --> 00:03:24,600
And I think it's interesting that there's been, just in the past few years, there's been this,

36
00:03:24,600 --> 00:03:32,600
this real explosion of interest within AI in trying to look at development and look at

37
00:03:33,480 --> 00:03:39,880
look at children and use that as a clue to solve some of these really tough problems. And I think

38
00:03:39,880 --> 00:03:45,640
it's because, you know, so much of the new work has depended on the idea of designing systems that

39
00:03:45,640 --> 00:03:49,960
learn rather than trying to build things in in the first place. And if you're interested in systems

40
00:03:49,960 --> 00:03:56,040
that learn, kids are really wonderful, probably the best example we have of, of creatures that are

41
00:03:56,040 --> 00:04:01,480
really, really good at learning very accurately from small amounts of data. So that's kind of the

42
00:04:01,480 --> 00:04:08,040
big overarching, that's the big overarching picture about what I've done. Fantastic. It sounds like you

43
00:04:08,040 --> 00:04:13,960
think about the problem of learning in children, you know, broadly at the level of kind of these

44
00:04:13,960 --> 00:04:23,640
broad representations, but your talk at NURPS is focused on a particular aspect of that. And that is

45
00:04:23,640 --> 00:04:29,880
the way that children and for causal relationships and structure in the world. Tell us a little bit

46
00:04:29,880 --> 00:04:36,920
about your talk and your research in that area. Yeah. So if you're asking this question, you know,

47
00:04:36,920 --> 00:04:42,840
how do we get these big abstract, powerful representations of the world? One of the most

48
00:04:42,840 --> 00:04:47,400
important, not the only one, but certainly one of the most important kinds of ways we have of

49
00:04:47,400 --> 00:04:51,320
representing and thinking about the world is thinking about causality, thinking about what makes

50
00:04:51,320 --> 00:04:58,200
what happen. And the first line of work that I did with children was what's come to be called

51
00:04:58,200 --> 00:05:02,680
the theory theory. And that's the idea that children are being building everyday theories of the

52
00:05:02,680 --> 00:05:07,000
world that are a lot like the way that scientists build theories of the world. And that's become,

53
00:05:07,000 --> 00:05:13,960
you know, one of the most prevalent theories about how children solve this problem. They do something

54
00:05:13,960 --> 00:05:17,720
like build everyday theories of the world around them. But of course, then the question is, well,

55
00:05:17,720 --> 00:05:22,200
how do they do that, right? If you say that they're doing something like scientists, how do scientists

56
00:05:22,200 --> 00:05:28,440
build theories of the world around them? And back in the arts, there was a whole bunch of very

57
00:05:28,440 --> 00:05:35,480
exciting work showing that you could build causal pictures, causal representations of the world

58
00:05:35,480 --> 00:05:39,960
from data. And that's not the only thing scientists are doing. It's not the only thing involved

59
00:05:39,960 --> 00:05:44,520
in theory formation, but it's a really important thing in theory formation. Because if you have a

60
00:05:44,520 --> 00:05:49,480
causal model, if you have a theory, if you have a representation, then you can solve these out

61
00:05:49,480 --> 00:05:58,520
of distribution generalization problems. You can say, okay, well, if we, you know, if we do this

62
00:05:58,520 --> 00:06:03,880
to the vaccine, even if it's something we've never done before, we can predict what will happen

63
00:06:03,880 --> 00:06:10,600
because we have a causal model of how the vaccine works. And the advantage of thinking about

64
00:06:10,600 --> 00:06:15,880
causal models, so there's an example of something that lets you generalize really broadly.

65
00:06:15,880 --> 00:06:20,600
And it isn't just letting you generalize about a specific area like, you know, the way that

66
00:06:20,600 --> 00:06:25,960
having a representation of everyday physics or a representation of the visual system could help

67
00:06:25,960 --> 00:06:31,000
you to generalize. Because that really covers everything. It covers the way that you interact with

68
00:06:31,000 --> 00:06:36,120
other people. It covers the way that objects work. It covers things that you've never seen before.

69
00:06:36,120 --> 00:06:41,400
So if you had a way of figuring out what makes what happen. If you had a way of figuring out

70
00:06:41,400 --> 00:06:46,520
causal structure, you'd have a really, really powerful tool for going out into the world and

71
00:06:47,480 --> 00:06:51,880
going out into the world and solving new problems and making new predictions.

72
00:06:51,880 --> 00:06:57,640
One thing that's really important about causality, and a lot of people think of it as sort of being

73
00:06:57,640 --> 00:07:02,200
the thing that makes causality different from, say, just correlation of the sort that typical

74
00:07:02,200 --> 00:07:09,640
deep learning programs have done, is that causality lets you intervene, is the word that people use.

75
00:07:09,640 --> 00:07:14,760
It lets you decide what to do and think about what the consequences of that are.

76
00:07:14,760 --> 00:07:21,640
And causality lets you make counterfactual inferences. So it lets you, if I know that smoking

77
00:07:22,840 --> 00:07:28,440
causes long cancer, for instance, if I think that that's right, then I'll know that even though

78
00:07:28,920 --> 00:07:34,040
smoking is correlated with yellow fingers, washing your fingers isn't going to help change the

79
00:07:34,040 --> 00:07:40,600
cancer rate, but not smoking, getting people to not smoke is going to change the cancer rate.

80
00:07:40,600 --> 00:07:45,960
And it also means that I can, say, do some counterfactuals. So I can say, well, look, if we had had

81
00:07:46,600 --> 00:07:52,360
not anti-smoking programs earlier, we would have saved more people from cancer.

82
00:07:52,360 --> 00:07:57,960
So that ability to do interventions and counterfactuals is a very powerful aspect of causality

83
00:07:57,960 --> 00:08:05,640
and causal representations. And back in the arts, people like Clark Gleymor and Peter Spirties

84
00:08:05,640 --> 00:08:12,760
at CMU, and notably, probably most famously, Judea Pearl in UCLA, started developing these

85
00:08:12,760 --> 00:08:20,360
formal models for how those kind of causal representations could work computationally.

86
00:08:20,360 --> 00:08:25,880
So causal-based, graphical probabilistic graphical models were the kinds of representations

87
00:08:25,880 --> 00:08:31,000
that they had. And back in the arts, we started trying to see our children doing something like

88
00:08:31,000 --> 00:08:36,280
inferring causal-based nets from data. And amazingly, much to everyone's surprise,

89
00:08:36,280 --> 00:08:40,360
it turns out that even if you're looking at two, three, and four-year-olds, they're really good at

90
00:08:40,360 --> 00:08:48,440
doing that. You can give them a pattern of data of conditional probabilities, and they'll pull out

91
00:08:48,440 --> 00:08:54,360
the right causal consequences from that data. So they seem to be doing something that looks like

92
00:08:54,360 --> 00:09:04,840
a very effective causal inference. Going back to the theory theory, what were the alternatives

93
00:09:04,840 --> 00:09:12,120
to the theory theory before that theory? And how did you demonstrate that the theory theory was

94
00:09:14,200 --> 00:09:22,200
you know, was predictive and had merit? Yeah, right. So that's a great question. And in fact,

95
00:09:22,200 --> 00:09:29,480
I think people in ML and AI should be very familiar with what the alternatives work,

96
00:09:29,480 --> 00:09:34,440
because they're the alternatives that we have in ML2. So one alternative, again, going back to

97
00:09:34,440 --> 00:09:40,040
really play-do in Aristotle, is look, it just looks as if we have all this abstract structure

98
00:09:40,040 --> 00:09:44,840
and representations. Really, it's just you've looked at a whole bunch of data and you pulled out

99
00:09:44,840 --> 00:09:49,800
the statistics of the data, and that's letting you make predictions. And that was one thing that

100
00:09:49,800 --> 00:09:55,000
the sort of what's sometimes called the empiricist option was, okay, maybe it's not that children

101
00:09:55,000 --> 00:10:01,320
have these abstract representations. Maybe they're just just following the data. And it just looks

102
00:10:01,320 --> 00:10:05,160
like they have the abstract representation, because they have a whole lot of data and a whole lot of

103
00:10:05,160 --> 00:10:11,720
observations. And then the other option, which again is very active in AI now, is well, look,

104
00:10:11,720 --> 00:10:16,200
maybe they aren't actually learning the representations. Maybe they're just built in. So

105
00:10:16,200 --> 00:10:23,080
work that's being done in AI now suggests that you might have built-in

106
00:10:24,760 --> 00:10:29,720
constraints, inductive constraints that are like assumptions about how the world works,

107
00:10:29,720 --> 00:10:33,080
assumptions about how physics work, assumptions about how people work. And if you just build those

108
00:10:33,080 --> 00:10:38,040
in in the first place, then you can help to solve the problem. So those were the two options that

109
00:10:38,040 --> 00:10:44,280
were and still are on the table. And I think those of us who actually look at young children learning,

110
00:10:44,280 --> 00:10:48,760
you know, there are probably elements of both of those that are right, but it doesn't look like

111
00:10:48,760 --> 00:10:54,040
either of those is what's happening, because from the time we can test, and this is where the great

112
00:10:54,840 --> 00:11:01,160
methodological and experimental advances in developmental psychology kick in, even very little babies

113
00:11:01,160 --> 00:11:06,840
already seem to have these abstract, powerful representations of the world. That's very different

114
00:11:06,840 --> 00:11:13,960
from what a previous generation thought about about babies. But at the same time, even three

115
00:11:13,960 --> 00:11:18,600
and four year olds, this is work that we did back in the eighties, are really changing what they

116
00:11:18,600 --> 00:11:24,760
think about the world based on their experience. So they seem to be have abstract representations

117
00:11:24,760 --> 00:11:30,200
and be learning and changing those representations from the time they're very little. And theory of

118
00:11:30,200 --> 00:11:34,520
mind, for example, which is something that people are thinking about in AI as well, that ability to

119
00:11:34,520 --> 00:11:39,000
understand what's going on in someone else's mind, that was the work that we did back in the eighties,

120
00:11:39,000 --> 00:11:45,640
that I think really clearly showed that children have this succession of every day of every day

121
00:11:45,640 --> 00:11:50,680
theories in the world. And then in the more recent work about causal inference, what we could show

122
00:11:50,680 --> 00:11:55,080
is not just that they, you know, will have an abstract theory and then another one based on the

123
00:11:55,080 --> 00:11:59,960
data, but we can actually say something about what those look like, what the representations look

124
00:11:59,960 --> 00:12:06,440
like, and how the representations change. Does the theory theory imply that children are

125
00:12:06,440 --> 00:12:12,760
taking an active role and kind of recognizing that a theory is a theory and testing the bounds

126
00:12:12,760 --> 00:12:20,920
of that theory and experimentation? Yeah, exactly. So that's the thing that is the new work that we're

127
00:12:20,920 --> 00:12:26,440
really doing now. So we showed over an extended period that children could could do this, they could

128
00:12:26,440 --> 00:12:32,440
get statistics, they could infer causal, infer causal structure. And you might wonder how on earth

129
00:12:32,440 --> 00:12:38,200
could you do this with like two year olds, right? I mean, if you asked most grownups, what does

130
00:12:38,200 --> 00:12:43,240
conditional, this pattern of conditional dependency is indicate a causal chain or a common effect,

131
00:12:43,240 --> 00:12:48,600
they would be not know what you were talking about. And the way we did it is we have a little

132
00:12:48,600 --> 00:12:52,760
little machines, one of them is called the blanket detector that you'll see in my talk,

133
00:12:52,760 --> 00:12:57,080
the little box that lights up when you put things on it plays music sometimes and doesn't,

134
00:12:57,080 --> 00:13:02,120
other times. So it's a new causal system. And the kids challenge is to figure out how it works,

135
00:13:02,120 --> 00:13:07,080
figure out which things are blankets. Well, blankets will make it go, which things are blankets,

136
00:13:07,080 --> 00:13:13,240
and then they have to make it go themselves. So without actually asking about causal structure,

137
00:13:13,240 --> 00:13:17,880
we can see what kinds of inferences they're making and we can control what kind of data we get

138
00:13:17,880 --> 00:13:21,960
from. So we can give them different patterns of data and then we can see, then we can see what

139
00:13:21,960 --> 00:13:27,800
they do, we can see what kinds of inferences they make. And as I said, to a remarkable degree,

140
00:13:27,800 --> 00:13:33,720
the kids are making the right kinds of inferences. If they were little Bayesian hypothesis testers,

141
00:13:34,520 --> 00:13:39,320
again, to get back to the theory theory, give them two hypotheses and they're picking the ones with

142
00:13:39,320 --> 00:13:45,320
the best posterior probability. But the big question is, so that that's really impressive. But then

143
00:13:45,320 --> 00:13:49,240
we still have this question about how are they doing that? What's happening that's letting

144
00:13:49,240 --> 00:13:58,040
them solve that problem? Because of course, the big issue with Bayesian reasoning and with probabilistic

145
00:13:58,040 --> 00:14:02,760
generative models in general is if they're interesting at all, the search space is enormous.

146
00:14:02,760 --> 00:14:08,520
So if you think about even a Bayesnet with four or five causes, you very quickly have a very,

147
00:14:08,520 --> 00:14:14,280
very big space of possibilities. And the question is, how do you limit that? How do you search through

148
00:14:14,280 --> 00:14:19,000
that space? How do you solve that problem? An idea that we've had and a lot of people in AI have had

149
00:14:19,000 --> 00:14:24,120
now is that active learning, to get back to your question, active learning experimentation,

150
00:14:24,120 --> 00:14:32,200
that's the way that scientists solve that problem. They are not just stuck in your main frame

151
00:14:32,200 --> 00:14:38,200
with data pouring over you. You can actually decide which kinds of data you want,

152
00:14:38,200 --> 00:14:43,640
depending on what kind of hypothesis you're testing. And for causal work, again, because of this

153
00:14:43,640 --> 00:14:50,280
intervention quality of causation, you can specify pretty clearly, here's the kind of experiment

154
00:14:50,280 --> 00:14:56,680
you should do. Here's what you should do. Here's how you should wiggle X to see if Y works.

155
00:14:56,680 --> 00:15:02,760
And if you think about little kids, I mean, like that's their entire life, right? I just,

156
00:15:02,760 --> 00:15:10,920
I just, it's a kind of nice, it's a kind of nice convergence. I just spent a, a, a sabbatical at

157
00:15:10,920 --> 00:15:16,440
Mela in Montreal, where, yeah, she was Benjiro and his colleagues are doing fantastic work just

158
00:15:16,440 --> 00:15:21,320
about this question about causal inference and causal models. And at the same time, I was

159
00:15:21,320 --> 00:15:28,360
visiting my one-year-old grandson. And if you watch my one-year-old grandson, basically all he

160
00:15:28,360 --> 00:15:34,120
does is do experiments. I mean, you know, he, he will occasionally eat if his mom gives him some food

161
00:15:34,120 --> 00:15:39,320
and, and other than that, what he's doing constantly is doing experiments. And what all the rest

162
00:15:39,320 --> 00:15:45,080
of us are doing is trying to keep him from killing himself by, by doing experiments. And, you know,

163
00:15:45,080 --> 00:15:49,160
it's funny, we just sort of take for granted. Oh, okay, look, I'm looking at this one-year-old and

164
00:15:49,160 --> 00:15:53,640
look what he does. He takes the spoon and then he bangs it on the pot and then he turns the pot over

165
00:15:53,640 --> 00:15:59,480
and then he sees if he can stick the spoon in the light socket and so on and so forth, we just take

166
00:15:59,480 --> 00:16:05,400
that for granted. But why would they be doing that, right? I mean, that's a lot of, a lot of physical

167
00:16:05,400 --> 00:16:11,240
energy going on to just go out and try things in the world. But if you think of them as being

168
00:16:11,240 --> 00:16:16,520
these causal inference, active causal inference engines, that's exactly, that's exactly what they

169
00:16:16,520 --> 00:16:21,000
should be doing. And what we're doing now is we're in collaboration with some people at Mela and

170
00:16:21,000 --> 00:16:29,560
at Berkeley and actually at Google Leap Mind as well. What we've done is to set up these environments

171
00:16:29,560 --> 00:16:37,240
in which you can find out about causal structure by doing experiments. We have our kind of virtual

172
00:16:37,240 --> 00:16:43,160
version of our blanket detector. And what we can do is see what do kids do when you just let them

173
00:16:43,160 --> 00:16:49,720
loose in this kind of environment. And how does that compare to various kinds of causal learning

174
00:16:49,720 --> 00:16:54,920
algorithms you might have? And if, you know, if you think about it like a classic RL algorithm,

175
00:16:54,920 --> 00:16:59,000
for example, is not going to do the kinds of things that are the best experiments because the

176
00:16:59,000 --> 00:17:03,640
classic RL algorithm is just going to try and find the outcome and maximize it. And what you

177
00:17:03,640 --> 00:17:09,400
need for experiments is to try lots of different things, change what you do based on what happened

178
00:17:09,400 --> 00:17:17,000
before. So, but some of the things, again, like my colleagues, Deepak Prathak and Polkad

179
00:17:17,000 --> 00:17:22,040
Agrawal and others coming out of Berkeley have these curiosity-based algorithms that seem to be

180
00:17:22,040 --> 00:17:28,040
closer to what the kids are doing. And I think if you kind of combine the curiosity-based idea,

181
00:17:28,040 --> 00:17:33,080
the idea that what you're trying to do is get a system that will make predictions and frustrate

182
00:17:33,080 --> 00:17:41,720
them and explore, and the causal models idea, that could be a very powerful mechanism for solving

183
00:17:42,360 --> 00:17:48,200
some of these search problems. So, what degree does your work begin to

184
00:17:50,440 --> 00:17:56,600
kind of articulate, explore the structure of the or the complexity of causal relationships

185
00:17:56,600 --> 00:18:02,920
that children are able to deal with at various ages? And what does that tell us about our

186
00:18:02,920 --> 00:18:13,160
ML models? Yeah, so the first pass, both with Basinets and with our work, was just pretty simple

187
00:18:13,160 --> 00:18:17,560
causal relationships. So, you know, here's one variable and another variable and does variable

188
00:18:17,560 --> 00:18:25,080
X cause variable Y. And you can see the experiments you do to try and find that out. And then what

189
00:18:25,080 --> 00:18:31,640
what Pearl and Glamour and colleagues had done was to look at more complicated things like,

190
00:18:31,640 --> 00:18:35,560
is it a causal chain or is it a common effect structure or a common cause structure?

191
00:18:36,680 --> 00:18:43,080
But starting in the sort of in the late arts, we started collaborating with people like the

192
00:18:43,080 --> 00:18:47,880
cognitive scientist Tom Griffiths, who's at Princeton, Chris Lucas, who was a student of mine,

193
00:18:47,880 --> 00:18:54,760
who was at is now in Edinburgh, to try and see if we could also make inferences about more abstract

194
00:18:54,760 --> 00:19:00,680
features of causal systems. So, for instance, could I infer not just, is this blanket detector,

195
00:19:00,680 --> 00:19:06,760
did this block make the detector go or not? But is the detector deterministic or stochastic? Or

196
00:19:06,760 --> 00:19:13,160
does the detector work with a conjunctive logic? You need two things to make it go in combination,

197
00:19:13,160 --> 00:19:19,400
or does it have a disjunctive logic? Each cause is separate. And what we've shown is that kids are

198
00:19:19,400 --> 00:19:25,560
quite good at, even again three and four year olds, are quite good at making inferences about

199
00:19:25,560 --> 00:19:31,320
those more abstract features of the system as well. And to get back to machine learning, you know,

200
00:19:31,320 --> 00:19:35,640
the more abstract your representations are, the more powerful your generalizations are going to be.

201
00:19:35,640 --> 00:19:39,880
So, the kids seem to be quite good at even making those more abstract inferences. And something

202
00:19:39,880 --> 00:19:44,280
that's really interesting is that the kids are actually better at doing that than adults are. So,

203
00:19:44,280 --> 00:19:51,320
well, here's the thing, and this is another point about children and childhood in general.

204
00:19:51,880 --> 00:19:59,000
So, if you give the adults a structure that's really common, that they have a strong prior

205
00:19:59,000 --> 00:20:03,960
for, then they're good at making inferences. But how about if it's something that's kind of weird

206
00:20:03,960 --> 00:20:08,520
and unusual? So, it's an abstract feature that isn't as obvious, and you don't have a stronger

207
00:20:08,520 --> 00:20:14,200
prior form. When you do that, the kids are actually better than the adults. And in a sense, the kids

208
00:20:15,320 --> 00:20:21,080
lack of knowledge, lack of previous knowledge is really an advantage when you want to explore

209
00:20:21,080 --> 00:20:27,480
the space, explore the space more widely. And I think that leads to another big point that I've

210
00:20:27,480 --> 00:20:32,200
made that I'll be making in my talk, which is that actually just being, it gets, gets back to my

211
00:20:32,200 --> 00:20:39,080
one-year-old, just being a kid, just the fact that you have this period of childhood before adulthood,

212
00:20:39,640 --> 00:20:45,240
that in and of itself may be something that humans use to solve this problem. And the argument

213
00:20:45,240 --> 00:20:50,280
that I've made is, again, think about that search problem. One of the reasons the search problem

214
00:20:50,280 --> 00:20:54,440
is so challenging is because there are always these explore exploit trade-offs, right? So,

215
00:20:55,080 --> 00:20:59,400
one of the things that we learned at the very beginning of computer science was that explore,

216
00:20:59,400 --> 00:21:07,000
exploit trade-offs are a bear, and there isn't any simple or optimal way of resolving them.

217
00:21:07,000 --> 00:21:11,880
But one idea that people often have is, okay, when you look at the actual algorithms that are

218
00:21:11,880 --> 00:21:16,280
trying to deal with explore exploit tensions, is start out exploring. And in particular,

219
00:21:16,280 --> 00:21:23,640
start out with these very wide, high temperature, bouncy, noisy kinds of searches that get through

220
00:21:23,640 --> 00:21:30,520
a whole bunch of the space. And then, once you've done that, narrow in and exploit. And the idea is

221
00:21:30,520 --> 00:21:36,120
that that keeps you from, you know, getting stuck in local optima, it keeps you from, it keeps you from

222
00:21:36,120 --> 00:21:42,440
settling on a particular option too quickly. And what I've argued is you could think about human

223
00:21:42,440 --> 00:21:48,040
life history as they call it a biology, human development, as being evolution's way of, so one of

224
00:21:48,040 --> 00:21:55,160
the ideas is often a simulated annealing kind of idea. So you start out looking really widely,

225
00:21:55,160 --> 00:22:00,760
and then you really, with a really high temperature, and then you cool off. And my slogan is that

226
00:22:00,760 --> 00:22:07,560
childhood is evolution's way of solving explore exploit tension and doing simulated annealing.

227
00:22:07,560 --> 00:22:13,640
So if you sort of say, who looks like they're bouncy and random and noisy and trying lots of

228
00:22:13,640 --> 00:22:19,080
things that are not very effective versus who looks like they're narrowing in, using a lot of their

229
00:22:19,080 --> 00:22:24,360
prior knowledge to do something effectively, you'll see that first, you know, that's what a one-year-old

230
00:22:24,360 --> 00:22:30,920
looks like, and as opposed to what an adult looks like. So part of the idea is that if we actually

231
00:22:30,920 --> 00:22:37,880
built in a developmental sequence, we sort of let AIs have be children for a while, that might,

232
00:22:37,880 --> 00:22:43,880
we might also get some clues about how to learn more effectively. And this kind of trade-off that

233
00:22:43,880 --> 00:22:49,960
you see where the children need a lot of care, they need a lot of people around them looking after

234
00:22:49,960 --> 00:22:54,920
them, but that gives them this chance to go out and explore. That might be a trade-off that's

235
00:22:54,920 --> 00:23:02,200
relevant for AIs as well. And I should say in some of the other things that I'm going to talk about,

236
00:23:02,200 --> 00:23:05,880
what we've, so one thing that we've been doing is looking at how children are using

237
00:23:05,880 --> 00:23:12,200
experimentation and exploration to try and solve these problems. So one thought that

238
00:23:12,920 --> 00:23:21,400
that brings to mind for me is in some ways could you say that the natural machine learning

239
00:23:22,200 --> 00:23:27,880
cycle kind of resembles this childhood adulthood in a sense that training is kind of like childhood

240
00:23:27,880 --> 00:23:33,560
and inference once the model is kind of built and fixed and that's kind of adulthood.

241
00:23:33,560 --> 00:23:38,360
Something like that I think is right. Yeah, I think the idea is that you have a period of learning

242
00:23:38,360 --> 00:23:44,200
and then you have a period of actually using what you've learned to go out and make inferences.

243
00:23:45,240 --> 00:23:49,000
But the kind of learning that the children are doing seems to be much more

244
00:23:50,040 --> 00:23:56,760
wide-ranging than the kind of learning that a typical machine learning system is doing.

245
00:23:56,760 --> 00:24:00,760
And something that I think is really interesting is when you actually look at the practicalities

246
00:24:00,760 --> 00:24:05,800
and talk to people about, well, how do you solve these exploit problems? A kneeling shows up

247
00:24:05,800 --> 00:24:10,360
again and again and often in multiple cycles where you'll, you know, do heat things up,

248
00:24:10,360 --> 00:24:15,000
cool things down, heat things up, cool things down. But there isn't a kind of general theory

249
00:24:15,000 --> 00:24:20,360
about how to do that as far as I can tell or about why that works. And again, looking at

250
00:24:20,360 --> 00:24:26,040
kids might give us some clues about, you know, how does a kneeling work in the wild when you see it

251
00:24:26,040 --> 00:24:31,320
in children? One of the things that I think is really cool about this idea is if you think about

252
00:24:31,320 --> 00:24:39,240
exploit trade-offs, things that look like bugs from the exploit perspective might actually be

253
00:24:39,240 --> 00:24:43,640
features from the explore perspective. So, for instance, having a system that's noisy that has

254
00:24:43,640 --> 00:24:50,840
a lot of variability is not good if what you want is to make inferences and act effectively,

255
00:24:50,840 --> 00:24:56,440
but it is good if what you want is to be able to learn as much as possible. So, many of these

256
00:24:56,440 --> 00:25:01,960
things about kids that have seemed like defects, like the fact that they're curious all the time,

257
00:25:01,960 --> 00:25:07,880
that they're kind of unpredictable, that they are variable, that they're noisy, those might

258
00:25:07,880 --> 00:25:12,520
actually be advantages from the perspective of learning. And I think we don't have a good

259
00:25:12,520 --> 00:25:16,440
theoretical account of how that all works and thinking about the kids could help.

260
00:25:16,440 --> 00:25:21,800
What would you say are the closest ways that we're approximating that in the machine learning world?

261
00:25:22,760 --> 00:25:32,600
People like Joshua Benjo and his colleagues and others are trying to, now, and I think this

262
00:25:32,600 --> 00:25:37,240
is going to be the way for the future, is to try and design hybrid systems that have some of the

263
00:25:37,240 --> 00:25:42,680
power of the, that can use some of the power of machine learning, but then can also have some of

264
00:25:42,680 --> 00:25:51,480
the structure and generalization of a causal system. So, Joshua has these flow nets that are

265
00:25:51,480 --> 00:25:58,920
trying to do that, so they're trying to add a layer of, a layer of further structure on top of

266
00:26:00,600 --> 00:26:05,320
a kind of classic machine learning system. And RL is interesting from this perspective too,

267
00:26:05,320 --> 00:26:10,760
because you can argue that, and people have argued that reinforcement learning in the psychological

268
00:26:10,760 --> 00:26:17,560
sense is like the most primitive form of causal learning. As opposed to just picking out correlations,

269
00:26:17,560 --> 00:26:21,400
what happens in reinforcement learning is that you do make an intervention and you see what the

270
00:26:21,400 --> 00:26:26,360
outcomes are. And it's important that you're actively going out and making interventions and

271
00:26:26,360 --> 00:26:34,440
seeing outcomes. But typically in RL, what you're, the outcome of that isn't a model as much as

272
00:26:34,440 --> 00:26:39,400
just the fact that you're more likely to make those inferences or those policies later on.

273
00:26:39,400 --> 00:26:45,480
So something like model based RL ends up looking a lot like causal inference, it ends up looking

274
00:26:45,480 --> 00:26:50,520
a lot like causal structure, and that might be a relevant, that feels like that's a relevant

275
00:26:52,200 --> 00:26:57,480
outcome. But I think it's important that the objective functions for a system like that

276
00:26:57,480 --> 00:27:02,200
would have to be things like information gain or like knowledge or like curiosity,

277
00:27:02,200 --> 00:27:08,920
rather than being things like how well you're scoring on, on some mention. And there's

278
00:27:08,920 --> 00:27:13,240
really elegant work in our lives and others that show, for instance, that if you look at the kids

279
00:27:13,240 --> 00:27:19,720
playing around, they seem to be acting in a way that will get them information. That information

280
00:27:19,720 --> 00:27:23,960
gain seems to be sort of an objective function that describes what it is that they do.

281
00:27:23,960 --> 00:27:28,440
What's an example of that? Well, there's beautiful work by my colleague Celeste Kid,

282
00:27:28,440 --> 00:27:34,200
and she looked at really young babies, you know, 10-month-olds. And what they did was they showed

283
00:27:34,200 --> 00:27:40,120
the babies different sequences of events that had different amounts of information in the

284
00:27:40,120 --> 00:27:44,920
technical information of theoretic sense. And what they discovered was that there was this kind

285
00:27:44,920 --> 00:27:49,960
of sweet spot, and they just measured how long the babies looked at each of these events,

286
00:27:49,960 --> 00:27:55,880
and they discovered there was this sort of sweet spot if something was too random, too far removed

287
00:27:55,880 --> 00:28:01,240
from where you were now. The babies wouldn't look, but they also wouldn't look at things that

288
00:28:01,240 --> 00:28:05,480
didn't give them very much new information. There was this kind of sweet spot of just where the

289
00:28:05,480 --> 00:28:11,720
information gain was going to really help you to make progress, and the babies looked the most

290
00:28:11,720 --> 00:28:17,400
at those events. And we've been thinking about that, too. One of the problems with using

291
00:28:18,120 --> 00:28:23,240
just using information gain, this is something that comes up in a lot of these curiosity-based

292
00:28:23,800 --> 00:28:29,880
RL kind of algorithms, is what's called, you know, the TV problem. The problem is if you just

293
00:28:29,880 --> 00:28:35,880
use technical information gain, then if you just put someone in front of a random TV with

294
00:28:35,880 --> 00:28:39,800
a random static at it, you're getting lots of information in the information through

295
00:28:39,800 --> 00:28:42,680
theoretic point, but you're not getting anything that's actually going to be very useful.

296
00:28:43,240 --> 00:28:48,760
So I think one of the real interesting frontiers is this balance between

297
00:28:49,400 --> 00:28:54,360
noise and structure, right? So the kids are noisy, but they're not just completely noisy. They're not

298
00:28:54,360 --> 00:28:59,000
just acting like random agents. They're doing things that make sense, given the kinds of problems

299
00:28:59,000 --> 00:29:02,920
they're trying to solve, the kinds of causal structures that they're trying to infer,

300
00:29:02,920 --> 00:29:08,920
and how you get that balance between introducing noisiness and variability, and then also having

301
00:29:09,880 --> 00:29:13,960
interventions and experiments that are relevant to the problems you're trying to solve.

302
00:29:13,960 --> 00:29:18,280
That's something the kids seem to be really remarkably good at doing, and we don't quite know how to

303
00:29:18,280 --> 00:29:26,040
characterize that computationally. I had cut you off to ask a question earlier, and you about to

304
00:29:26,040 --> 00:29:33,960
mention some additional points from your talk over those. Yeah, so what we've been doing now,

305
00:29:33,960 --> 00:29:38,120
so one thing that we've been doing is looking at how children are using active learning to

306
00:29:38,920 --> 00:29:43,640
figure out the causal structure of the world in these online environments. Another thing that we're

307
00:29:43,640 --> 00:29:48,280
doing is trying to see if, I mentioned that we've done work showing that children could get these

308
00:29:48,280 --> 00:29:54,360
more abstract, what are some of us called, over hypotheses about causal structure, but one thing

309
00:29:54,360 --> 00:30:00,680
that we're doing now is trying to see if kids can do things like actually decide what the right

310
00:30:00,680 --> 00:30:06,920
causal variables are. So a big problem, so it's easy to say, okay, look, I'll tell you, you know,

311
00:30:06,920 --> 00:30:11,000
here's variable X, and here's variable Y, and then you can see whether they're dependent, and see

312
00:30:11,000 --> 00:30:15,080
if there's a causal relationship between them, but how do you decide which variables to look at in

313
00:30:15,080 --> 00:30:20,840
the first place? And this is a big problem for ML as well, where it turns out that, you know, in the

314
00:30:20,840 --> 00:30:25,560
classic adversarial examples for something like ImageNet, it turns out, well, wait a minute,

315
00:30:25,560 --> 00:30:30,280
no, the system is actually not even dividing up the world in the right way. It's paying attention

316
00:30:30,280 --> 00:30:35,480
to, you know, fine details of the texture instead of paying attention to the objects,

317
00:30:36,600 --> 00:30:40,680
and it might look as if it's making the right kinds of inferences, but it isn't really,

318
00:30:40,680 --> 00:30:46,040
because it just hasn't divided up the world in a way that makes sense from the perspective of

319
00:30:46,040 --> 00:30:52,840
different variables. You know, if you're a scientist, there's all sorts of classic examples like,

320
00:30:52,840 --> 00:30:57,960
you know, it turns out that if you're looking at the relationship between cholesterol and heart

321
00:30:57,960 --> 00:31:02,760
disease, oh, there's actually two different kinds of cholesterol. One of them makes heart disease

322
00:31:02,760 --> 00:31:07,640
more likely, one of them doesn't. So if you just didn't, if you didn't have the right measures,

323
00:31:07,640 --> 00:31:11,160
if you just looked at cholesterol overall, you wouldn't see the relationship, and then it turns

324
00:31:11,160 --> 00:31:17,400
out that, then it turns out that you can. And it turns out that kids are actually very good.

325
00:31:17,400 --> 00:31:22,200
We have some really beautiful experiments where kids are actually good at picking out, oh, okay,

326
00:31:22,200 --> 00:31:28,280
this is the variable, this is the object, or this is the property that is the one that I should

327
00:31:28,280 --> 00:31:33,880
be looking at for purposes of trying to do causal inference. So they're simultaneous. Yeah,

328
00:31:34,440 --> 00:31:40,840
so this is, this is my brilliant graduate student, Marielka, do. So what we do is we set up a

329
00:31:40,840 --> 00:31:47,160
situation where the kids have to figure out their little Shelley, the turtle, wants to grow

330
00:31:47,160 --> 00:31:52,040
cactuses. And he likes some cactuses. He likes the ones with round things on them, but not the

331
00:31:52,040 --> 00:31:57,240
spiky ones. And we're trying to decide how can we help him grow his cactuses? And there's two

332
00:31:57,240 --> 00:32:01,960
different things we can do. We could put the seeds in different colored flower pots, or we could

333
00:32:01,960 --> 00:32:06,840
have different colored water incants that are water and seeds. And we're trying to figure out

334
00:32:06,840 --> 00:32:11,480
how do we make sure that Shelley gets the cactuses that he does? So we've set up, here's these two

335
00:32:11,480 --> 00:32:17,640
potential variables. It could be the water incants, it could be the pots. And what we can show is that

336
00:32:17,640 --> 00:32:22,840
if we show children that the water incants make a difference and the pots don't, and now we give

337
00:32:22,840 --> 00:32:28,120
them a new case, this is a new cactus, a new water incant, a new pot, they'll say, okay, the water

338
00:32:28,120 --> 00:32:31,800
incant is the thing, I hope I got that right, the water incant is the thing I should be paying

339
00:32:31,800 --> 00:32:36,520
attention to. I should be playing with that water incant. I should be changing it. We should

340
00:32:36,520 --> 00:32:41,480
be doing things to that water incant, because I've already figured out that the pot really isn't

341
00:32:41,480 --> 00:32:46,920
relevant to these differences. So in philosophy, they talk about this as causality being

342
00:32:46,920 --> 00:32:51,160
difference-making. The causal thing is the thing that makes a difference. And the kids already

343
00:32:51,160 --> 00:32:55,880
seem to be saying, okay, which things in my world are the things that make a difference? Which are the

344
00:32:55,880 --> 00:33:01,080
things that I can control and change and make a difference? And again, if you think about an AI

345
00:33:01,080 --> 00:33:07,640
system, if it was paying attention to, think about, you know, RL, part of the problem with RL is

346
00:33:07,640 --> 00:33:14,840
it's just paying attention to everything in the image, right? And if it could say, oh no, these

347
00:33:14,840 --> 00:33:19,800
are the things that I should be wearing. These are the things that, you know, think about like the

348
00:33:19,800 --> 00:33:26,440
classic example where you see a robot being trained with RL. And it's doing all this ridiculous

349
00:33:26,440 --> 00:33:32,440
stupid stuff that isn't going to make any impact at all before it kind of stumbles on the thing

350
00:33:32,440 --> 00:33:37,960
that might work. If you could start out having that robot say, oh, okay, I know these are the things

351
00:33:37,960 --> 00:33:42,440
that are going to make a difference and these things aren't. It would be much better all. And if it

352
00:33:42,440 --> 00:33:55,000
could learn that, that would be even better. Does your research venture into the role of biases learned

353
00:33:55,000 --> 00:34:02,840
by children in the process of their exploration? Yeah, one of the things that we've, one of the things

354
00:34:02,840 --> 00:34:08,440
that I mentioned before was about the fact that children aren't just using these causal inferences

355
00:34:08,440 --> 00:34:15,080
to make draw conclusions about, you know, blanket machines. They're using them a lot to draw

356
00:34:15,080 --> 00:34:21,800
inferences about other people. And actually, something that we haven't published yet were just in

357
00:34:21,800 --> 00:34:28,040
the middle of doing it. For example, suppose kids see that there's a bunch of kids who are playing

358
00:34:28,040 --> 00:34:36,840
together and kids who have, you know, a particular kind of funny glasses are being welcomed into

359
00:34:36,840 --> 00:34:42,760
the group and kids who have a funny hat are being shunned, for instance. So they just would be seeing

360
00:34:42,760 --> 00:34:48,600
those, seeing those patterns. Are they going to conclude that there's a difference between the

361
00:34:48,600 --> 00:34:52,360
people who have the glasses and the hats? Even if, I mean, that's a nice example of variable

362
00:34:52,360 --> 00:34:57,320
selection, right? You know, kids start out not thinking that this difference is going to be important.

363
00:34:57,320 --> 00:35:01,320
But if they see that people are being treated differently, they might very well end up

364
00:35:01,320 --> 00:35:06,360
concluding. And in fact, we have some evidence that they do conclude that, that those people are,

365
00:35:06,360 --> 00:35:12,120
those, those are different groups. Those are people who should be treated differently, for example.

366
00:35:12,840 --> 00:35:18,040
So I think it's quite plausible that part of what's happening is that kids are paying attention

367
00:35:18,040 --> 00:35:24,520
to these social differences. And then they end up having various kinds of biases as a result.

368
00:35:24,520 --> 00:35:29,880
And it's an interesting question about would, you know, it seems plausible that an AI might very

369
00:35:29,880 --> 00:35:36,280
well reproduce that. And we might be worried about how we could counter how we could counter that

370
00:35:36,280 --> 00:35:43,560
both for the children and for the AI's. So what are the main takeaways that you want to leave

371
00:35:43,560 --> 00:35:49,000
the folks that are hearing your talk at the causal inference and machine learning workshop?

372
00:35:49,000 --> 00:35:53,880
So I think there's two. One of them is, this may be a little preaching to the choir,

373
00:35:53,880 --> 00:35:59,160
is that causal inference is really important. It's a really powerful, it's a really powerful

374
00:35:59,160 --> 00:36:05,320
technique causal representations give us lots of advantages. And, and we sort of made some progress

375
00:36:05,320 --> 00:36:10,440
computationally on causal inferences and representations. And that, that's a really,

376
00:36:10,440 --> 00:36:15,160
that that's exactly the, the technique we'd need to solve some of the limitations of our current

377
00:36:15,160 --> 00:36:20,280
systems. But then in a way, the even more important idea is that looking at little kids,

378
00:36:20,280 --> 00:36:25,880
which is not something that typically people in AI have done that feels like, oh, wait a minute,

379
00:36:25,880 --> 00:36:28,680
you know, the world of people who are sitting in little chairs and playing with three

380
00:36:28,680 --> 00:36:33,400
year olds is completely different from the world of computer scientists. And I think it's kind

381
00:36:33,400 --> 00:36:38,200
of wonderful that that computer scientists have realized, oh, wait a minute, you know,

382
00:36:38,200 --> 00:36:42,120
these little kids who we weren't paying any attention to, we thought they were just, you know, kind of,

383
00:36:42,840 --> 00:36:49,880
I don't know, um, uh, mushy stuff that wasn't like what we do in AI. That those are,

384
00:36:49,880 --> 00:36:56,120
that those are really, uh, you know, those are those kids might really have the clue to designing

385
00:36:56,120 --> 00:37:02,120
new systems. I think that's the really big point that I want to make. And, and I think back and forth,

386
00:37:02,120 --> 00:37:07,240
by looking at kids who are such great learners, we can figure out how to make more effective AI.

387
00:37:07,240 --> 00:37:11,640
But also, by looking at AI, we can figure out what's going on in those kids brains that makes

388
00:37:11,640 --> 00:37:16,840
them such effective learners. I think that's a really, really promising, exciting, uh,

389
00:37:17,880 --> 00:37:21,080
line of research and one that we're starting to do when I hope we'll continue to do.

390
00:37:21,960 --> 00:37:26,200
That's awesome. So, AI researchers go play with your kids. Exactly. Yeah, I mean, I think,

391
00:37:27,000 --> 00:37:30,520
and, you know, I think this might be an example as well where,

392
00:37:31,560 --> 00:37:35,720
you know, the diversity issues are really relevant, right? So, you know, I think part of the

393
00:37:35,720 --> 00:37:40,360
reason to be frank, why kids haven't played a bigger role in AI is because kids are kind of girl

394
00:37:40,360 --> 00:37:47,400
stuff, right? Um, uh, you know, they're, they're things that people who are off raising families are

395
00:37:47,400 --> 00:37:52,840
paying a lot of, of paying a lot of attention to. And the people who were doing AI and the people

396
00:37:52,840 --> 00:37:57,080
who are raising the families haven't necessarily been the same people. And I think it's a kind of

397
00:37:57,080 --> 00:38:02,040
tribute to the way that we have a much wider, more diverse group of people being involved in AI,

398
00:38:02,040 --> 00:38:08,280
including more women, more people who are raising families, that that's a nice example where that

399
00:38:08,280 --> 00:38:13,320
actually turns out to contribute to something that's really basic to the science. Yeah, yeah.

400
00:38:13,880 --> 00:38:19,080
Awesome. Awesome. Well, Allison, thanks so much for joining us and sharing a bit about your talk and

401
00:38:19,080 --> 00:38:33,720
what you've been up to on the research front. That's great. Thanks for having me.

