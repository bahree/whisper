WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:28.720
I'm your host Sam Charrington.

00:28.720 --> 00:33.040
Before we get going, I'd like to send a huge thanks to our friends at HPE for sponsoring

00:33.040 --> 00:38.080
this week's series of shows from the O'Reilly AI Conference in New York City.

00:38.080 --> 00:43.160
At the conference, HPE presented on InfoSight, which is the company's cloud-based AI ops

00:43.160 --> 00:49.240
solution for helping IT organizations better manage and ensure the health of their IT infrastructure

00:49.240 --> 00:50.240
using AI.

00:50.240 --> 00:55.440
I've previously written about AI ops and it's definitely an interesting use case for machine

00:55.440 --> 00:56.440
learning.

00:56.440 --> 01:05.720
To check out what HPE InfoSight is up to in this space, visit twimbleai.com slash HPE.

01:05.720 --> 01:08.560
All right everyone, I am on the line with Dellip Rao.

01:08.560 --> 01:13.280
Dellip is the vice president of research at the AI foundation.

01:13.280 --> 01:20.560
Previously, he founded the research consulting company Juiceware and the fake news challenge

01:20.560 --> 01:21.560
as well.

01:21.560 --> 01:26.120
He's one of my favorite AI people to follow on Twitter where he tweets him at Dellip Rao.

01:26.120 --> 01:28.920
Dellip, welcome to this week in machine learning and AI.

01:28.920 --> 01:30.400
Thank you for having me.

01:30.400 --> 01:33.320
I am really looking forward to our conversation.

01:33.320 --> 01:39.960
You recently joined the AI foundation where again, you'll be leading at research.

01:39.960 --> 01:46.120
Tell us a little bit about your background and what sparked your interest in AI in a

01:46.120 --> 01:48.880
particular natural language processing.

01:48.880 --> 01:50.840
Yeah, absolutely.

01:50.840 --> 01:57.040
So, my background is in research, particularly in natural language processing research.

01:57.040 --> 02:08.480
And I grew up in India and did typical engineering track that a lot of kids in India do.

02:08.480 --> 02:16.720
And I was pretty sure I was going to become some kind of an academic with very early on

02:16.720 --> 02:26.640
in my career, in my college life and because I found teaching very inspiring and I really

02:26.640 --> 02:28.880
wanted to be a teacher.

02:28.880 --> 02:31.760
And it was not AI, it was not my choice.

02:31.760 --> 02:38.280
In the beginning, I actually wanted to become, to do research and distributed computing.

02:38.280 --> 02:47.120
And I realized that, you know, there was this one school where I went to IT Madras and

02:47.120 --> 02:51.640
this one, there was a faculty who was working on distributed computing and he was on the

02:51.640 --> 02:52.640
sabbatical.

02:52.640 --> 03:05.720
They kind of put me in charge in care of another faculty who was working on AI and this

03:05.720 --> 03:11.160
person was supposed to babysit me while the actual professor, you know, returned back

03:11.160 --> 03:15.240
from sabbatical and he did a very dangerous thing.

03:15.240 --> 03:21.360
So, he gave me a couple of books, thinking that, you know, it'll keep me busy.

03:21.360 --> 03:24.760
And I just got really hooked.

03:24.760 --> 03:29.320
So, after reading this couple of books, I just felt like, okay, this is what I should

03:29.320 --> 03:32.280
be working on and sort of distributed computing.

03:32.280 --> 03:36.640
And that's when I started working on AI.

03:36.640 --> 03:41.840
And this person was actually like a very hard core, like old school AI person, like we

03:41.840 --> 03:45.880
were working on like planning problems and so on.

03:45.880 --> 03:51.720
So I actually started looking into AI planning actually to begin with and actually looking

03:51.720 --> 03:54.560
at search algorithms and so on.

03:54.560 --> 04:01.000
And it was only like around that time, I just got like curious about natural language

04:01.000 --> 04:07.280
processing because the data that we were working with involved text and natural language

04:07.280 --> 04:11.640
processing was supposed to be some kind of like pre-processing I was supposed to be doing,

04:11.640 --> 04:14.760
but instead I found the pre-processing itself super interesting.

04:14.760 --> 04:19.840
So, I kind of moved into natural language processing and then I went to graduate school

04:19.840 --> 04:27.320
the class of that, like, you know, I felt like I had to study more in AI and this is all

04:27.320 --> 04:29.400
ancient history, by the way.

04:29.400 --> 04:36.240
So, do you happen to remember the name of the books that your babysitter professor gave

04:36.240 --> 04:37.240
you to read?

04:37.240 --> 04:46.160
Yeah, I mean, I think one of them was actually the book by Nilton, who recently passed

04:46.160 --> 04:50.960
away, like I think that's the end of yesterday or day before.

04:50.960 --> 04:57.120
And yeah, he was a professor at Stanford and he wrote one of these very early AI text

04:57.120 --> 05:05.700
books, and then there was also the classic book by Peter Narveg, a story, a Russell and

05:05.700 --> 05:07.200
Narveg, right?

05:07.200 --> 05:11.840
And so, yeah, I started off with those two books.

05:11.840 --> 05:20.000
The Nilton, Nilton book is like a really great gateway drug for anyone, I like men.

05:20.000 --> 05:25.760
And then, of course, the Peter Narveg book, you know, puts you on a more serious route.

05:25.760 --> 05:29.640
And so, you, I think we veered off at grad school.

05:29.640 --> 05:32.640
Did you start juiced where right after grad school?

05:32.640 --> 05:38.600
Now, far from it, actually, I worked at a bunch of different places, you know, I worked

05:38.600 --> 05:49.800
at Google Research as a kind of like a repeat intern, and then I went to Twitter, and at

05:49.800 --> 05:55.120
that time, Twitter was pretty early, and it was way before that IPO.

05:55.120 --> 06:03.880
I was like, you know, employee number 500 something, and yeah, I was one of the first machine

06:03.880 --> 06:09.840
learning hires to join their anti-spam team.

06:09.840 --> 06:15.720
And they were that until that point, and they just formed this anti-spam team.

06:15.720 --> 06:22.480
When I joined, like, you know, a few months ago, and they were writing all sorts of interesting

06:22.480 --> 06:29.200
rules to catch them, and I was like, oh, no, that's not how it should be done, but, but

06:29.200 --> 06:35.760
actually, you know, I got, I got schooled because I felt like, oh, I came in the wild

06:35.760 --> 06:40.880
saying, thinking that here, I, here I am with all my training, I'm going to go change

06:40.880 --> 06:42.560
the way things work.

06:42.560 --> 06:46.600
But then, that's when my first humbling experience happened.

06:46.600 --> 06:51.120
I felt I discovered that, you know, real products don't work that way.

06:51.120 --> 06:56.080
You can't build, like, simple models that will, like, that you can just unleash it on

06:56.080 --> 06:59.440
the world and expect it to work.

06:59.440 --> 07:05.560
And they were doing a lot of things coming from deep experience, product experience.

07:05.560 --> 07:12.000
And that's when I kind of, like, completely shifted from my academic researcher mindset

07:12.000 --> 07:15.120
to a more product-based researcher mindset.

07:15.120 --> 07:20.880
Are there some specific examples you can give of where that disconnect really fell for

07:20.880 --> 07:21.880
you?

07:21.880 --> 07:22.880
Oh, absolutely.

07:22.880 --> 07:29.040
I mean, like, for example, like, data set drift is something that we kind of barely refer

07:29.040 --> 07:31.680
to in academia, right?

07:31.680 --> 07:39.240
And we, the model drift, model drift is something that we barely refer to in academia,

07:39.240 --> 07:42.680
but it actually happens all the time in production.

07:42.680 --> 07:47.400
Your model's stopping relevant, and you have to kind of, like, figure out, like, some kind

07:47.400 --> 07:52.280
of a feedback loop to start, like, to keep retraining your models.

07:52.280 --> 07:58.880
And even sometimes, you know, there is this, even you don't know much, there is a kind

07:58.880 --> 08:04.840
of a cockiness that comes in where you say that, oh, you can build this model, you can

08:04.840 --> 08:07.920
solve this easily by building a model.

08:07.920 --> 08:14.400
But then you realize that, you know, getting the model done and building all the engineering

08:14.400 --> 08:21.960
around the feedback loop for the model itself is so much more expensive than, like, you know,

08:21.960 --> 08:28.560
sometimes even writing a few rules or writing something very simplistic, which, to a researcher

08:28.560 --> 08:33.880
especially coming from an academic background, it almost feels like, oh, why are we doing

08:33.880 --> 08:34.880
this?

08:34.880 --> 08:35.880
Right?

08:35.880 --> 08:36.880
Like, this is not science.

08:36.880 --> 08:37.880
Why are we doing this?

08:37.880 --> 08:43.920
And this is more importantly, I think, in academia as well as in research, we're always

08:43.920 --> 08:49.360
encouraged to stay, you know, push down a lot of times, stay ahead of the game, right?

08:49.360 --> 08:56.400
And when you see some heavily engineering systems built using rules, the natural feeling

08:56.400 --> 09:02.360
comes into thinking that, oh, this is all, like, outdated, and this, and there is, it's

09:02.360 --> 09:07.680
kind of very natural for somebody coming from academia to look at it and feel a little,

09:07.680 --> 09:11.000
like, this is all outdated stuff.

09:11.000 --> 09:19.240
But I think with experience, there is this enlightenment that creeps in, where you realize

09:19.240 --> 09:25.920
that no, actually, that the so-called outdated stuff is actually, like, the most appropriate

09:25.920 --> 09:30.040
thing to do in this context.

09:30.040 --> 09:37.520
And that's when, you know, I kind of, it took me a few experiences like that, right?

09:37.520 --> 09:42.320
And, of course, I mean, I gave you an example of model draft, another is, like, you know,

09:42.320 --> 09:48.320
you build a model, the model does really well, then you're very proud of it, and then you

09:48.320 --> 09:58.560
try, and then, you know, in a company like Twitter, everything gets, baby-tested, carefully,

09:58.560 --> 10:04.840
and you end up also not just getting abtested, but also you're tested for how much compute

10:04.840 --> 10:08.280
that your setup is taking, right?

10:08.280 --> 10:12.440
Because you're operating at that massive scale.

10:12.440 --> 10:18.880
And if a simple rule-based thing is accomplishing pretty much everything that your model thing

10:18.880 --> 10:23.360
is doing, and your model is just adding, like, you know, a few decimal point improvement

10:23.360 --> 10:29.400
or even, like, a percentage point improvement, it gets, it becomes a wash, but it comes

10:29.400 --> 10:32.800
at an enormous cost to the business, right?

10:32.800 --> 10:35.200
And so, it just does not make sense.

10:35.200 --> 10:39.360
And as a researcher, you don't think about that, because you're always looking at, like,

10:39.360 --> 10:44.600
oh, can I improve on state of the art, even if, as long as the improvements are statistically

10:44.600 --> 10:48.320
significant, and I can publish, I can call it a win.

10:48.320 --> 10:54.960
And that's where, like, you know, a disconnect happened for me, and, like, kind of, I felt,

10:54.960 --> 11:01.000
like, oh, there is a different way to do science that is relevant in the real world, and

11:01.000 --> 11:03.240
I want to practice that.

11:03.240 --> 11:08.500
Have you developed that to the point where you've got some kind of fundamental tenets

11:08.500 --> 11:13.640
of science in the real world, or is it more a general idea and approach?

11:13.640 --> 11:16.640
Yeah, I did.

11:16.640 --> 11:24.640
You know, some of it, I kind of, I alluded to in the book that we published, we can talk

11:24.640 --> 11:25.960
about it later.

11:25.960 --> 11:29.800
But, you know, I was going to say that I worked at Twitter, and then I worked at Amazon,

11:29.800 --> 11:35.280
and Amazon was also another amazing experience, and it taught me a whole bunch of different

11:35.280 --> 11:40.400
things on similar, along similar lines.

11:40.400 --> 11:44.720
And I, after spending time at these two places, I started to use Twitter, which was a research

11:44.720 --> 11:51.280
consulting company, and it was a research consulting company kind of built with that mindset,

11:51.280 --> 11:52.280
right?

11:52.280 --> 11:59.200
When you do science, that is, that will be baked into products, it has to be developed

11:59.200 --> 12:06.440
differently, then the science that we do typically in graduate school labs that's meant for

12:06.440 --> 12:12.840
writing a paper, and that was the mindset with which, you know, just where it was created,

12:12.840 --> 12:19.160
and in fact, all our clients, we worked out, we worked with, we actually built solutions

12:19.160 --> 12:25.680
that could be deployed, not a single solution that shipped from us, was like, you know, a

12:25.680 --> 12:28.400
paperware or a shelfware, right?

12:28.400 --> 12:35.760
So, you mentioned the book, and I had this on my list of things to mention in your intro,

12:35.760 --> 12:37.120
so I'm remiss in that regard.

12:37.120 --> 12:42.600
You just published a book, in fact, that's sitting here on my desk, Natural Language Processing

12:42.600 --> 12:47.560
with PyTorch, which you co-authored with Brian McMahon.

12:47.560 --> 12:52.280
And so, we'll definitely dig into that a little bit more.

12:52.280 --> 12:59.440
You recently joined the AI Foundation, which I hadn't previously heard about, but read

12:59.440 --> 13:05.680
a little bit about, and it's got this interesting kind of for-profit, non-profit structure,

13:05.680 --> 13:11.400
reminding me a little bit of kind of some of open AI's recent announcements, in terms

13:11.400 --> 13:15.880
of the direction they're going, can you tell us a little bit about the AI Foundation

13:15.880 --> 13:20.320
and kind of that structure and what the organization is up to in general?

13:20.320 --> 13:27.120
Sure, in fact, the funny thing is now people are referring to, I mean, using open AI

13:27.120 --> 13:32.840
as an analogy, but we were doing, we established this structure almost like more than a year

13:32.840 --> 13:33.840
ago.

13:33.840 --> 13:40.440
And in a way, I think this is great for mission-oriented companies.

13:40.440 --> 13:47.080
AI Foundation is a hybrid for-profit and non-profit, and at the for-profit, we are actually

13:47.080 --> 13:52.240
interested in building all kinds of synthetic content.

13:52.240 --> 13:56.560
And in the non-profit, we are actually interested in detecting the synthetic content.

13:56.560 --> 14:04.480
So actually, we started off with, or at least the founders, they were interested in solving

14:04.480 --> 14:06.640
the detection problem.

14:06.640 --> 14:11.600
But then, you can't just go off and build a non-profit and keep it sustainable, right?

14:11.600 --> 14:19.440
You need something to power the non-profit, and a far-profit is like a very sustainable

14:19.440 --> 14:29.080
way to power a non-profit, like before I started working at AI Foundation, I started

14:29.080 --> 14:36.280
the fake news challenge in a very non-profit mode, and that was not easily sustainable

14:36.280 --> 14:42.520
just based on grant money, sure, we want some grant money from night foundation, but

14:42.520 --> 14:48.240
it becomes a massive exercise, almost like, and it's not a sustainable way to build

14:48.240 --> 14:49.720
a non-profit.

14:49.720 --> 14:54.560
So when I learned about AI Foundation, and I learned that about their mission, which

14:54.560 --> 15:02.120
was to detect any kind of synthetic media on the internet or anywhere, and it includes

15:02.120 --> 15:10.520
generated video, generated audio, as well as generated text, I was obviously very excited

15:10.520 --> 15:17.040
about that, and then, you know, I was curious how they were actually approaching it.

15:17.040 --> 15:22.400
And actually, it was their way of approaching that may solve me more than, you know, the

15:22.400 --> 15:28.440
mission itself, because I was already sold on the mission, I kind of, even before I met

15:28.440 --> 15:32.280
a disability AI Foundation.

15:32.280 --> 15:38.800
So the far-profit is building products, like, you know, is doing all these interesting

15:38.800 --> 15:44.320
synthesis work in the context of AR and VR, right?

15:44.320 --> 15:49.720
And this is great because one of my experiences is that, you know, in order to be really good

15:49.720 --> 15:55.960
at detection, you also need to be really good at generation, and the way the far-profit

15:55.960 --> 16:02.280
and non-profit structure is that, like, you know, the far-profit is actually building technology

16:02.280 --> 16:09.760
that is so strong and generation, that the non-profit, the technology that non-profit

16:09.760 --> 16:14.520
is building also gets strengthened, kind of like a real-world GAN, if you will, right?

16:14.520 --> 16:18.280
Like, they're both reinforcing each other.

16:18.280 --> 16:24.360
And of course, it helps for the far-profit folks to, like, go spend some hours working

16:24.360 --> 16:29.440
on the non-profit stuff, and it's a really great model.

16:29.440 --> 16:35.040
Sounds like there's some commonalities between the kind of problems you're trying to solve

16:35.040 --> 16:40.640
with the fake news challenge and what you'll be working on at AI Foundation.

16:40.640 --> 16:47.400
I'm curious, what were the key takeaways from your experience launching the fake news

16:47.400 --> 16:49.840
challenge and conducting that?

16:49.840 --> 16:55.560
Oh, yeah, absolutely. Fake news challenge was another one of those humbling experiences

16:55.560 --> 17:00.040
in life, and I think this is great.

17:00.040 --> 17:04.000
Everybody should go through multiple such humbling experiences, and that's where, like,

17:04.000 --> 17:06.240
a lot of growth sports happen.

17:06.240 --> 17:13.080
So when I started seeing, around the 2016 elections, when I started seeing a lot of misinformation

17:13.080 --> 17:18.080
on Twitter, I'm very active on Twitter, you know, and I have been for quite some time.

17:18.080 --> 17:23.120
Now, I felt like, oh, there is all this misinformation coming.

17:23.120 --> 17:27.040
Why is Twitter not doing anything about it?

17:27.040 --> 17:29.880
Why are these platform companies, especially Facebook?

17:29.880 --> 17:37.000
I mean, at some point, I used to be on Facebook, and I was seeing a whole bunch of random,

17:37.000 --> 17:42.680
like, complete conspiracy theory, like, not stuff being shared on Facebook.

17:42.680 --> 17:47.800
I mean, I'm no longer on Facebook, but I just felt like the platform companies were

17:47.800 --> 17:53.000
letting us down by not working on this problem, and there was also a time where, like, you

17:53.000 --> 17:58.280
know, the, like, Mark Zuckerberg went on stage saying that his Facebook does not have

17:58.280 --> 18:00.040
a misinformation problem, right?

18:00.040 --> 18:06.680
Like, this was in 2016, and around that time, there was not just me, but a whole bunch

18:06.680 --> 18:13.920
of other people who are interested in this problem started talking on Twitter, and I ended

18:13.920 --> 18:18.640
up partnering with Dean Palmer-Lew.

18:18.640 --> 18:21.400
He was a professor at Carnegie Mellon.

18:21.400 --> 18:30.040
He was, like, one of the pioneers of the autonomous driving program developed by DARPA.

18:30.040 --> 18:37.720
And he was doing self-driving cars in the 80s, which is crazy, think about that.

18:37.720 --> 18:44.320
So, anyway, Dean and I started the fake news challenge, and we both were, like, you

18:44.320 --> 18:46.320
know, we are both entrepreneurs, right?

18:46.320 --> 18:52.840
And we have this sort of, like, entrepreneurial optimism, and at the same time, we are academics,

18:52.840 --> 18:57.680
so with academic background, you know, we think that we can solve anything.

18:57.680 --> 19:05.040
And we thought that, oh, we can totally, like, build some kind of natural language processing

19:05.040 --> 19:10.720
system to kind of detect fake news.

19:10.720 --> 19:18.600
And it didn't take us too long to realize how nuanced this problem is, and the more

19:18.600 --> 19:23.840
we kept, we, and then, you know, we started talking to fact checkers and journalists.

19:23.840 --> 19:26.120
I spent, I don't know, much.

19:26.120 --> 19:31.640
I can't even count how many hours with fact checkers and journalists, and I realized

19:31.640 --> 19:37.840
that gosh, this problem is so complicated, and the work they're doing is so complicated.

19:37.840 --> 19:45.160
It's not going to be any one system or approach that will solve this, but it will be, like,

19:45.160 --> 19:54.000
you know, a combination of, you know, approaches involving, modeling, it involving human in the

19:54.000 --> 19:56.000
loop, et cetera.

19:56.000 --> 20:02.880
And that's, that's when, like, we, when we started fake, fake news challenge, we thought

20:02.880 --> 20:05.600
we were going to come up with a solution, right?

20:05.600 --> 20:11.560
And that was like the kernel of the idea that will come up with a solution.

20:11.560 --> 20:15.480
And then, you know, we thought, oh, maybe it's not one solution, it's multiple solution.

20:15.480 --> 20:18.120
And then we thought, no, it's not even multiple solution.

20:18.120 --> 20:22.840
What do you want is the community of people talking to each other.

20:22.840 --> 20:30.680
And that will create, like, a factory for coming up with multiple solutions and create

20:30.680 --> 20:33.120
more conversations, right?

20:33.120 --> 20:39.880
And so fake news challenge ended up becoming this sort of, like, a community where, you

20:39.880 --> 20:45.080
know, researchers, and natural English processing, computer vision, et cetera, behind also

20:45.080 --> 20:50.360
the fake news problem could come and interact with journalists and fact checkers.

20:50.360 --> 20:54.400
And basically, these were two, until the fake news challenge happened, there were two

20:54.400 --> 21:00.160
different communities that were not talking with each other as much as they are today,

21:00.160 --> 21:01.160
right?

21:01.160 --> 21:02.160
Right.

21:02.160 --> 21:05.480
And I'm not claiming that, you know, the fake news challenge completely changed that,

21:05.480 --> 21:07.480
but I think it set the tone.

21:07.480 --> 21:13.200
And as a consequence, what happened was there were a lot of people who ended up meeting

21:13.200 --> 21:14.200
there.

21:14.200 --> 21:18.040
Some of them ended up starting their own companies around this space.

21:18.040 --> 21:23.080
Some of them ended up starting other events around this.

21:23.080 --> 21:28.280
And it created a whole bunch of conversation.

21:28.280 --> 21:35.360
And I think there is, like, a snowballing effect that happened after that very shortly.

21:35.360 --> 21:40.280
So I think your original question was, what were the key takeaways from the fake news

21:40.280 --> 21:41.280
challenge?

21:41.280 --> 21:48.120
I mean, the one takeaway was, like, you know, always be humble, like, keep the world

21:48.120 --> 21:52.200
is more complex than you initially think at us.

21:52.200 --> 21:59.160
And there are surprising ways in which we keep learning that and relearning that.

21:59.160 --> 22:07.120
And the other takeaway is, we found that, okay, we put together this community, this community

22:07.120 --> 22:11.600
needs to be engaged, we need to do something about it.

22:11.600 --> 22:19.920
So we created a shared task and I would, like, advise anybody to not take up this job

22:19.920 --> 22:22.680
of creating a shared task.

22:22.680 --> 22:30.120
It's one of those really complicated jobs that require multiple people working together

22:30.120 --> 22:33.480
in order to pull it off, like, really well.

22:33.480 --> 22:38.080
And despite me working on it full time, and even Dean spending a fair amount of which

22:38.080 --> 22:45.120
time on it, there was, it was super challenging to pull it off, right?

22:45.120 --> 22:51.960
And the shared task in this case is the creation of the community or the creation of a solution

22:51.960 --> 22:53.360
to the problem.

22:53.360 --> 23:00.640
So what we did was I put my consulting hat on and then I looked at, okay, I'm going

23:00.640 --> 23:05.280
to talk to a whole bunch of fat checkers and I'm going to find out what are the core problems

23:05.280 --> 23:06.720
that they are dealing with.

23:06.720 --> 23:11.920
And if there is any science problem that needs to be solved in order to solve this problem,

23:11.920 --> 23:12.920
right?

23:12.920 --> 23:18.720
And I, you know, it was obvious that one of the common problems that kept coming over

23:18.720 --> 23:22.640
and over it was, was that of volume.

23:22.640 --> 23:28.440
So almost all fat checkers are, like, inundated with articles and they need to fact check and

23:28.440 --> 23:33.800
many of these articles are like, you know, pretty much rehashing the same thing or different

23:33.800 --> 23:36.120
variations of the same theme.

23:36.120 --> 23:42.360
And therefore, you know, it's not, they can't be very efficient about how the fat check,

23:42.360 --> 23:43.360
right?

23:43.360 --> 23:50.040
And how they assign labels to these articles or any kind of additional context around

23:50.040 --> 23:52.040
these articles.

23:52.040 --> 24:00.920
And what we did was we created a dataset where, if I give you two articles or two, let's

24:00.920 --> 24:11.120
say, two pieces of text, can you tell me if the two are related, unrelated, discussing

24:11.120 --> 24:20.440
about each, you know, about the same topic or they contradict each other, right?

24:20.440 --> 24:25.760
And it's a surprisingly hard natural language processing problem, right?

24:25.760 --> 24:35.000
Like, it's, I mean, maybe not unsurprisingly hard, natural language processing problem.

24:35.000 --> 24:39.920
Because you're not dissolving textual entailment, you're solving a whole bunch of other things

24:39.920 --> 24:42.080
in order to get it right.

24:42.080 --> 24:48.240
And a lot of teams competed on it, again, you know, it's not one of those things where

24:48.240 --> 24:54.160
if the dataset is still out there, it's not like an MNEST or something where, you know,

24:54.160 --> 24:57.200
people have solved that dataset and moved on, right?

24:57.200 --> 25:00.240
It's a really hard challenging problem.

25:00.240 --> 25:06.680
In fact, the problem was so interesting and challenging that it got used in a lot of

25:06.680 --> 25:13.320
university and LP courses, ranging from MIT to Stanford, like, you know, every big and

25:13.320 --> 25:19.440
small university departments offering, you know, key courses in class projects and so on.

25:19.440 --> 25:28.920
People have published on the dataset and, yeah, so the shared task was actually like a

25:28.920 --> 25:36.600
great way to keep the community engaged, but man, it's really time consuming to actually

25:36.600 --> 25:42.040
get that done and to make sure the evaluation, et cetera, was happening, right?

25:42.040 --> 25:49.480
And we kind of, like, I wanted to withdraw in a little more excitement to this, so just

25:49.480 --> 25:55.840
where I decided to sponsor, like, prices for the top three.

25:55.840 --> 26:03.840
And it became quite successful, like, you know, there were more than 1,000 people who registered

26:03.840 --> 26:11.040
for the challenge and forget the exact numbers now, but there were like a few hundred teams

26:11.040 --> 26:14.880
who were actually competing in it.

26:14.880 --> 26:21.960
And yeah, I mean, we had to, it was all automated, the evaluation, et cetera, and we ended up,

26:21.960 --> 26:28.520
you know, finding the top three and awarding them prizes and so on.

26:28.520 --> 26:37.000
And I would say the learnings from the fitness challenge were, you know, one half was

26:37.000 --> 26:44.120
technical, I would say as it was as much non-technical as it was technical, the non-technical or

26:44.120 --> 26:51.560
softer aspects were like the domain related problems, like how complicated the domain is

26:51.560 --> 26:58.480
and who are the key people working on the domain and how we can facilitate and how to

26:58.480 --> 27:04.040
build a community and how to sustain the community and so on.

27:04.040 --> 27:10.760
And the technical parts of the thing is actually, to me, it somewhat least interesting.

27:10.760 --> 27:17.800
I guess it was more like, how do we come up with the data set for this situation and

27:17.800 --> 27:22.600
we came up with some interesting tricks to build the data set, but yeah.

27:22.600 --> 27:26.840
And now the competition ended a couple of years back.

27:26.840 --> 27:34.000
I'm curious, do you think that the data set and the competition is still relevant?

27:34.000 --> 27:41.240
And are the solutions that were proposed at the time still relevant or do you think if

27:41.240 --> 27:45.480
you miraculously could clone yourself to run this thing again, like would you see dramatically

27:45.480 --> 27:48.760
different results two years later?

27:48.760 --> 27:55.360
So the shared task by itself as a natural language processing task is still highly relevant

27:55.360 --> 28:00.520
and the approaches that were proposed, they were all unsurprisingly deep learning based

28:00.520 --> 28:04.600
approaches, they're all still relevant, maybe today if you were to do this, you would

28:04.600 --> 28:12.400
use like something like Bert or some of these beefier models, but other than that, the

28:12.400 --> 28:16.240
fundamental approaches are still relevant.

28:16.240 --> 28:27.440
That said, when we created this shared task, we called it FNC1 for a reason because we

28:27.440 --> 28:35.640
realized that none of the tasks that we propose is going to be representative of a fake news

28:35.640 --> 28:37.160
solution, right?

28:37.160 --> 28:43.680
And in fact, any solution to the fake news problem is actually coming from a multi-pronged

28:43.680 --> 28:47.480
approach and there are so many problems to be solved.

28:47.480 --> 28:51.520
So the volume is just one aspect of the problem.

28:51.520 --> 28:55.360
So we call it as FNC1 for that reason.

28:55.360 --> 29:01.160
So which automatically implies will there be an FNC2, FNC3 and so on?

29:01.160 --> 29:07.920
And the answer is yes, because it takes such a lot of effort and thought to put together

29:07.920 --> 29:15.200
an FNC2, I am actively working behind the scenes, talking to people around what this

29:15.200 --> 29:22.640
FNC2 should be like and how to make that relevant to the new kinds of problems that are facing

29:22.640 --> 29:31.480
because the kinds of vectors that are popping up, especially around deepfakes and things

29:31.480 --> 29:35.760
like that, there are different kinds of misinformation vectors coming up.

29:35.760 --> 29:42.600
How do we select a task that faithfully reflects what is happening today?

29:42.600 --> 29:51.600
Yeah, let's jump into the current state of fake content generation and detection.

29:51.600 --> 29:56.720
Because fake news, as you kind of traditionally defined it in FNC1 and there's a bunch

29:56.720 --> 30:00.720
of subproblems there, you mentioned deepfakes to you.

30:00.720 --> 30:07.080
When you think about the content generation and detection landscape, what do you see as

30:07.080 --> 30:12.080
the primary challenges and how do you organize all of that in your head?

30:12.080 --> 30:17.680
I guess practically every modality can be faked.

30:17.680 --> 30:20.120
That is audio, video, text.

30:20.120 --> 30:26.960
I guess maybe tomorrow if tactile becomes another modality, you can fake that too, I don't

30:26.960 --> 30:27.960
know.

30:27.960 --> 30:35.400
So right now, I think our foundation, we are interested in detection of all three dominant

30:35.400 --> 30:42.120
modalities, like audio, video, and text, and each of these have their own approaches for

30:42.120 --> 30:43.120
generation.

30:43.120 --> 30:50.640
So for example, with video, you might already know about a whole bunch of approaches based

30:50.640 --> 30:55.640
on GAMS, so these are the narrative model approaches, and then there are all these approaches

30:55.640 --> 31:04.880
based on image-to-image transfer, like face swap and deepfakes and so on, so that's one

31:04.880 --> 31:14.480
category of misinformation, another category of synthetic misinformation is like in audio,

31:14.480 --> 31:23.400
you can talk or turn, set the trend, but like latest wave net models have become so good

31:23.400 --> 31:31.000
that you have to be not a human to detect if an audio is coming from a model or if it

31:31.000 --> 31:38.200
is coming from a human person, right, and I would say three ways to generate fake audio.

31:38.200 --> 31:45.760
One is synthesis, that is obvious, another is wise conversion, where I can take your

31:45.760 --> 31:52.080
wise and then kind of pass it through a deep network and then do a style transfer to

31:52.080 --> 31:58.160
somebody else's wise, right, and so that way I can give you a power to say in anybody

31:58.160 --> 31:59.960
else's wise.

31:59.960 --> 32:06.800
And the third is, of course, replay attacks, right, where I can splice a piece of audio

32:06.800 --> 32:16.240
from a previous conversation in a different context and kind of like create confusion

32:16.240 --> 32:25.160
because I can make you say things out of context, right, and the replay, so the first two are

32:25.160 --> 32:32.160
actually, you need a model to detect that because especially with lots of compute and

32:32.160 --> 32:40.360
a lot of parameters, like really deep and wide models, you can model, like you can generate

32:40.360 --> 32:49.520
audio with really high fidelity, that the human ear basically cannot distinguish, right,

32:49.520 --> 32:56.640
like, and we are doing some experiments around that too, but what we see is many of these

32:56.640 --> 33:01.640
generation algorithms, right, like synthesis or wise conversion algorithms end up leaving

33:01.640 --> 33:09.240
some high frequency artifacts in the data, sorry, in the audio that whatever your ear misses

33:09.240 --> 33:18.760
the detection model can easily pick up on, right, so I mentioned two of the three audio

33:18.760 --> 33:24.120
based, like we're two of the three methods to generate fake audio.

33:24.120 --> 33:30.200
There is also a third method, which is the replay attack, and interestingly, the replay

33:30.200 --> 33:37.960
attack has become such, I mean, it is one of the oldest attacks, right, because you don't

33:37.960 --> 33:46.880
need a model to do that and it's been happening forever, there has been a lot of work on detecting

33:46.880 --> 33:53.600
replay attacks, and in fact, I cast bin last year, they had a challenge for detecting

33:53.600 --> 34:02.280
replay attacks, and basically, not last year, in 2017, they had a challenge in detecting

34:02.280 --> 34:11.760
replay attacks, and 2018, there are rates were around 2.5%, right, by, you know, they

34:11.760 --> 34:17.320
were able to ensemble all the top models, and then this, let's say, the ensemble error

34:17.320 --> 34:23.440
rate was like around 2.5%, and in 2018, there was this one paper I can give you, the reference

34:23.440 --> 34:31.720
if you want, where the error, you know, basically went down to like zero, and so interestingly,

34:31.720 --> 34:39.240
we can now with the help of models, almost always detect replay attacks, and that's

34:39.240 --> 34:45.760
possible because whenever audio recording happens, there's always like a whole bunch of,

34:45.760 --> 34:54.720
you know, differences that are channel and room, like ambience, related variables that

34:54.720 --> 35:01.240
are kind of low level acoustic qualities that are undetectable by us, but the model can

35:01.240 --> 35:07.880
easily pick up that change in that when you kind of like splice another audio into an

35:07.880 --> 35:14.600
existing audio. So on the audio side, are there, what does the data set landscape look

35:14.600 --> 35:19.680
like for, you know, folks want to play with this, are they creating their own data sets,

35:19.680 --> 35:26.680
or are there some interesting? So, like, there hasn't been any other data

35:26.680 --> 35:32.920
set I know other than ASV Spoof, which was put together by a bunch of other researchers

35:32.920 --> 35:45.920
in Google. I think the Google synthesis team is behind the ASV Spoof data set, and basically

35:45.920 --> 35:51.520
what they're doing is they're collecting a whole bunch of different synthesized audio,

35:51.520 --> 35:59.320
and from an audio that has been synthesized by a whole different kinds of methods.

35:59.320 --> 36:08.920
I think they're only using synthesis and wise conversion, not necessarily replay, because

36:08.920 --> 36:16.320
replay I feel is like probably nobody wants to work on it. Probably easier creature on

36:16.320 --> 36:25.000
data set for that too. I know it's a replay is kind of like a solid problem, like, you

36:25.000 --> 36:32.200
know, with whatever caveats, right? But sufficiently over parameterized network, we can easily

36:32.200 --> 36:42.320
detect a replay attack, but the other two require some amount of work. And I think this

36:42.320 --> 36:48.240
is actually the current ASV Spoof, like 2019, where, you know, detecting whether the audio

36:48.240 --> 36:58.920
came from the, whether it came from a synthesis system or a wise conversion system, or another

36:58.920 --> 37:04.920
interesting task is actually, can I, you know, detect change in the room parameters,

37:04.920 --> 37:13.800
right? So imagine, let's say I have your Alexa, right? I'm able to record you in a different

37:13.800 --> 37:20.400
room, and then let's say I'm talking to you, and I make you say something, and it's,

37:20.400 --> 37:26.760
you know, clandestinely recorded. And then I go off and play that recording to your Alexa

37:26.760 --> 37:35.200
and make it purchase like, I don't know, 10,000 Barbie dolls or something like that. So

37:35.200 --> 37:42.080
if you do that, how can you detect that, right? So can I change, so you can kind of like

37:42.080 --> 37:49.160
generate audio by automatically changing the room parameters, like different kinds of

37:49.160 --> 37:57.760
audio by generating, sorry, you can, by varying the different room parameters, like the size

37:57.760 --> 38:05.720
of the room, the position of the speaker, and so on, and change the reverberation, and

38:05.720 --> 38:09.160
that in turn changes the quality of the audio and so on.

38:09.160 --> 38:13.760
It's interesting to think that, you know, a retailer, for example, particularly a commerce

38:13.760 --> 38:19.560
retailer, you know, has long since had, you know, huge anti-fraud teams that are working

38:19.560 --> 38:26.760
on credit card fraud and other types of transactional fraud. It's interesting to think of how, you

38:26.760 --> 38:33.400
know, an Amazon or Google Now might resource audio fraud because of the kinds of scenarios

38:33.400 --> 38:39.120
you're describing with Alexa's that are putting in everyone's house. Yeah, so and this is

38:39.120 --> 38:45.880
a big problem for like a lot of banks, like, you know, banks in accepting and credit card

38:45.880 --> 38:52.400
companies and using audio based or vice-based authentication, right? So any speaker verification

38:52.400 --> 39:00.120
system can, can be filed if it's not carefully planned, it can be filed by doing some kind

39:00.120 --> 39:05.600
of a replay attack. Because most of the time, it's something like my vice is my password

39:05.600 --> 39:12.160
or something like that that you have sayings and if it is recorded under sufficiently

39:12.160 --> 39:19.640
high quality conditions, then I can use a replay attack and basically file any vice-indication

39:19.640 --> 39:20.640
system.

39:20.640 --> 39:25.840
Well, let's, we'll come back to some of the other modalities, but you previously alluded

39:25.840 --> 39:34.240
to this kind of the GAN relationship between generation and detection. In the context of

39:34.240 --> 39:41.280
audio, but I imagine this will apply generally, you know, how do you see this interplay kind

39:41.280 --> 39:50.360
of evolving between the detection and generation approaches and systems? Like now that, you

39:50.360 --> 39:56.160
know, we're getting so, so much better at synthesis and conversion, detecting synthesis

39:56.160 --> 40:01.880
and conversion attacks. Are we already seeing, you know, feeding that back into the synthesizers

40:01.880 --> 40:05.000
and converters and trying to make better systems?

40:05.000 --> 40:15.040
Yeah, and that's, that's going to be an, an arms race and we have to accept that. So

40:15.040 --> 40:24.480
the better the, better the detectors become, you know, you can imagine somebody with sufficient

40:24.480 --> 40:30.520
motivation and it's not difficult to find them, actually engineer systems to overcome that

40:30.520 --> 40:39.000
or figure out exactly, okay, how do I file these detectors? I think an interesting challenge

40:39.000 --> 40:45.440
that would have, that's not yet been conducted by anyone else, look at all the approaches

40:45.440 --> 40:51.000
to detection and come up with ways to break them, right? And I think that will give us

40:51.000 --> 40:57.960
a lot of interesting lessons into building more robust detectors. And this is true with

40:57.960 --> 41:05.920
any kind of adversarial setting, like, you know, starting from a fighting spam, like anti-spam.

41:05.920 --> 41:11.880
So you've, you come up with one approach to, like, let's say, you figured out that, you

41:11.880 --> 41:21.200
know, a lot of the Nigerian spam emails that you get, they are using poor language, right?

41:21.200 --> 41:27.040
And you try to, let's say you train some in-gram based thing that detects it, you say,

41:27.040 --> 41:34.080
okay, the, the perplexity is too high. Therefore, this, this could be like a signal for spam.

41:34.080 --> 41:39.360
But then guess what happens? They will start with copy-pasting. They won't even have to

41:39.360 --> 41:47.400
do, like, fancy generation. They will copy-paste, like chunks of good text from different places.

41:47.400 --> 41:53.760
I am file the detector, right? And now that you know that is happening, you'll start moving

41:53.760 --> 42:01.480
the detector in a different direction. I think this is the game of working in an adversarial

42:01.480 --> 42:08.840
learning setting that your attack vectors keep changing and you have to constantly keep

42:08.840 --> 42:15.600
monitoring the attack landscape and then keep evolving the solution.

42:15.600 --> 42:21.280
We're starting to explore a lot of this ground in the context of adversarial examples

42:21.280 --> 42:28.080
on the video side. There's a whole body of knowledge for, I guess, kind of meta knowledge

42:28.080 --> 42:33.120
for dealing with this kind of these kind of adversarial scenarios in the security

42:33.120 --> 42:41.880
research world. Like, do we have enough of that DNA within the kind of AI and NLP realm?

42:41.880 --> 42:45.800
Or are we starting to get enough of that that we can, you know, we're not reinventing

42:45.800 --> 42:47.520
the wheel?

42:47.520 --> 42:54.680
We, we have for some, like, you know, NLP, I can say, because of things like anti-spam,

42:54.680 --> 43:03.040
etc. There has been that kind of work happening already, right? Like, you know, around fishing

43:03.040 --> 43:10.520
or in around span. People have been constantly building systems that involve, that involve

43:10.520 --> 43:16.480
some kind of a feedback loop and preferably a human in the loop that way it can sort

43:16.480 --> 43:23.800
of deal with this shifting adversary. And we'll see similar things happening with these

43:23.800 --> 43:30.040
other modalities as well, right? Like, yeah, so that's not, I mean, that's pretty much

43:30.040 --> 43:32.680
on the road map for any serious company.

43:32.680 --> 43:40.520
So we dove into audio on the video side. What's the landscape looking like there? I think

43:40.520 --> 43:47.240
deepfake is the thing that comes to mind. But even that is somewhat ill-defined and includes

43:47.240 --> 43:49.280
a bunch of different types of things.

43:49.280 --> 43:56.040
Right. So, like, for example, like anything that creates generator or sampled from a

43:56.040 --> 44:04.560
GAN, today, even the best GANs still end up leaving some kind of artifacts that makes

44:04.560 --> 44:13.320
it possible to do good quality detection, right? But I can imagine a future where compute

44:13.320 --> 44:23.760
becomes so easily available and maybe a different kind of modeling paradigm where we are basically

44:23.760 --> 44:30.560
able to generate hyper-realistic image or like even photorealistic, I mean, very uncanny

44:30.560 --> 44:40.040
images, right? And the thing about detection is, I have this thesis that all detection

44:40.040 --> 44:47.920
models are always better than humans at detection when it comes to sophisticated attacks, right?

44:47.920 --> 44:54.760
And as things become more, so initially, you know, we will have, we will operationalize

44:54.760 --> 45:00.760
that, we will have more and more people looking at things and so on. But then once the adversaries

45:00.760 --> 45:06.640
evolved and then they decide to like, you know, have more launch, more and more sophisticated

45:06.640 --> 45:15.520
attacks, then models will become indispensable for doing that, right? And so, I think we

45:15.520 --> 45:21.240
are seeing some of that happening with GANs. And now with GANs, you can, there are all

45:21.240 --> 45:30.320
these temporal GANs where you can actually generate a sequence of images that are tied

45:30.320 --> 45:36.840
to each other so that it becomes like a video as opposed to like, you know, individually synthesizing

45:36.840 --> 45:44.480
each frame, right? I think that was one of the criticism with that with GANs. Then there

45:44.480 --> 45:52.880
is also, you don't need a GAN, right? Like with things like celibate, etc. There are things

45:52.880 --> 46:00.960
happening, which are a class of algorithms, which are doing this image-to-image transfer,

46:00.960 --> 46:10.600
right? Where I can basically take your body and then just swap out your face or sometimes

46:10.600 --> 46:20.640
even just swap out the lips, right? And then make you say things that you have not said,

46:20.640 --> 46:27.440
right? And that's interesting because, you know, we are a lot of the algorithms that we're

46:27.440 --> 46:32.640
working with, even in the state of the art methods, are still leaving some artifacts that

46:32.640 --> 46:41.360
models can detect. Like, you know, at AI Foundation, we internally, we have researchers,

46:41.360 --> 46:47.840
as well as like a product that we are working on and from the nonprofit side, where we are

46:47.840 --> 46:53.440
detecting manipulated faces in video and showing a heat map over it, right? And so on.

46:54.720 --> 47:00.880
Efficient frontier may be overloading a term, but there's a kind of a frontier where the, you know,

47:00.880 --> 47:09.520
the costs to generate these things and the quality, the resulting quality, you know, versus a

47:09.520 --> 47:15.520
human's ability to detect, it seems like we're in, we're still in a different place than where

47:15.520 --> 47:21.680
audio is. Most of the things that are accessible that I see are pretty, you know, it's pretty

47:21.680 --> 47:28.880
easy to tell as a human that, you know, this is a bad fake. Yeah, but I would say that, you know,

47:28.880 --> 47:36.080
with GANS, I mean, when I say human, when you say humans, we have to be careful, right?

47:37.440 --> 47:41.520
You and I are probably looking at people like us who tend to be like, you know,

47:41.520 --> 47:47.520
more in the machine learning, yeah, I would. And we are used to seeing a lot of these things,

47:47.520 --> 47:54.480
and it kind of like have this in-net expertise to tell them apart. But we have experiments,

47:54.480 --> 48:00.000
we have done with human subjects where, you know, there's like human subjects who are like

48:00.000 --> 48:06.640
completely non-technical when they look at GAN images, there's literally like a 50-50 chance

48:06.640 --> 48:13.600
that, you know, they get it, right? Yeah. And this is where, like, you know, I think a model

48:13.600 --> 48:22.160
becomes highly valuable. It gets worse when it becomes audio, with audio practically everyone

48:22.160 --> 48:30.080
will fail. You made a comment and refer back to it that we're going to be dependent on models

48:30.080 --> 48:38.400
because humans are, you know, we're being outclassed by models. And it makes me think a little bit of,

48:38.400 --> 48:44.800
there's been some research on medical imaging that says that, you know, a model is, you know,

48:44.800 --> 48:52.480
X percent accurate at, you know, say 80 percent accurate at detecting some cancerous, you know,

48:52.480 --> 48:59.120
growth in an image. A human is 80 percent accurate. I'm making up the numbers here, but together,

48:59.120 --> 49:03.600
they're 95 percent accurate, or something along those lines. Do you think the same thing

49:04.240 --> 49:12.960
will evolve in fighting artificial content where humans plus models become the ultimate solution?

49:13.680 --> 49:19.760
Absolutely. I feel like that's, that's even, that's an inevitable case where humans and models

49:19.760 --> 49:28.720
work together, except that I would make a small note that in that situation, the kind of

49:28.720 --> 49:33.280
information that the human might look might be different from the kind of information the model

49:33.280 --> 49:42.720
is looking at, right? I have some examples. Come to mind. So, for example, in, in this case,

49:42.720 --> 49:49.120
you know, the model might be let's say, take fake audio, right? The, the human may not be able to

49:49.120 --> 49:57.840
teleport the fake audio from a generated one, let's say. And sometimes, you know, with compression

49:57.840 --> 50:04.000
in audio, it can get even worse. Like compressed images and compressed audio introduce

50:04.000 --> 50:10.880
all sorts of artifacts that you cannot distinguish between that from the artifacts that you get from

50:10.880 --> 50:16.720
a generated system, from a system like this synthesis, right? So, we're kind of conditioned to

50:16.720 --> 50:24.480
accept some degree of compression artifact. And so, exactly, got it, got it. Exactly. And, and, and

50:24.480 --> 50:31.520
folks really have trouble telling that just from the content, right? And so, the models will like

50:31.520 --> 50:38.080
look at things that are here, listen to things that people can't listen to or see things that people

50:38.080 --> 50:45.760
can't see. But at the same time, you know, let's say the model flags something as, as, you know,

50:45.760 --> 50:54.080
doctored or forged, then you can, we can imagine a human in the loop, actually, like, you know, looking

50:54.080 --> 50:59.600
into it and not necessarily looking at the content, but looking at everything else, looking at the

50:59.600 --> 51:06.720
context, like, you know, where is this coming from? What is the user behavior, and so on, right?

51:06.720 --> 51:13.920
To large extent, it can be automated, but, you know, but even the context, like, it requires a lot

51:13.920 --> 51:21.360
of, like, background knowledge, as well as, like, knowledge of the world, to, to be able to say,

51:21.360 --> 51:26.960
to make, to make us a definite pronouncement. And we see this all the time in, like, I, you know,

51:26.960 --> 51:33.440
I will give you an example from the spam world where I come from. So, if you have training topics,

51:33.440 --> 51:39.760
right? So, there is, like, a lot of things appear like spam. The model can flag it as spam.

51:39.760 --> 51:45.680
But if you know that, you know, this thing is actually related to a training topic,

51:46.640 --> 51:55.040
then a human can make a judgment call whether it is market as spam and take a retaliatory action,

51:55.040 --> 52:02.960
like, by suspending the account or something like that, or whether to just treat it as simply

52:02.960 --> 52:12.800
a high volume traffic, because this training topic is just so engaging and so controversial,

52:12.800 --> 52:19.440
right? Right. And this is like a call that a human, and only a human can make it, at least as far

52:19.440 --> 52:29.840
as now, because the context is where it requires all sorts of, like, reasoning and unification

52:29.840 --> 52:35.920
with the world that it's hard to bake that into a model. I feel like we were just scratching the

52:35.920 --> 52:44.400
surface here, but we're running along on time. I do want to briefly mention once again the book.

52:44.400 --> 52:49.600
If I remember correctly, there was a tweet, there were probably multiple tweets along these lines,

52:49.600 --> 52:54.960
but I remember you tweeting something along the lines of, like, your philosophy with the book,

52:54.960 --> 53:01.840
or at least one aspect of it, relating to the code examples being real examples, or something

53:01.840 --> 53:07.440
like that. Can you elaborate briefly on your philosophy for the book and what makes it different

53:07.440 --> 53:12.320
from, you know, picking up a tutorial on the web, or on YouTube, or any of the other books out there?

53:13.360 --> 53:24.080
Yeah. So when, when Brian and I set out to write this book, we decided to bake in all the best

53:24.080 --> 53:31.120
practices that we were, you know, actually practicing on a day-to-day basis, and also we were

53:31.120 --> 53:40.880
not seeing them on any of these tutorials on the web, and so on, and also on the original

53:40.880 --> 53:48.320
documentation. So we decided that, you know, there are, like, very good software engineering patterns.

53:48.320 --> 53:52.240
I mean, I've always believed because I come from a very engineering background,

53:52.240 --> 54:02.320
I was an engineer first, and then became a researcher. I think really good science requires good

54:02.320 --> 54:10.640
engineering, and good engineering is, like, indistinguishable from good science. Like, and it's important

54:10.640 --> 54:17.280
to have the two together. So we baked in a whole bunch of best practices that we knew of

54:17.280 --> 54:27.600
from the software engineering world to actually make modeling good, modeling successful, right?

54:27.600 --> 54:34.960
And so if you follow some of the practices that we suggest, you would, like, not make a typical,

54:34.960 --> 54:43.040
you know, modeling mistake that we would spend days if not weeks trying to chase down, right?

54:43.040 --> 54:52.400
So that's what I meant by real world, and then, of course, a lot of the actual problems that we

54:52.400 --> 55:00.080
have chosen, we intentionally chose problems that were not, like, toy problems, right? Or toy

55:00.080 --> 55:08.800
data sets. Any of our data sets could be comparable to the real data sets that you,

55:08.800 --> 55:16.400
anyone can deal with. And it's just a question of, like, you know, how do we come up with a set of

55:16.400 --> 55:25.280
representative tasks for the book that pedagogically it will expose the readers to a variety of

55:25.280 --> 55:33.680
NLP deep learning algorithms. And at the same time, not take the reader too far away from their

55:33.680 --> 55:40.960
home court, which is the world of practice where the world of real problems. And I want them to be

55:40.960 --> 55:48.800
situated next to each other. And hopefully, if we did our job, right? We have shown the readers

55:49.440 --> 55:58.560
a lot of good software engineering practices around building models. And we also talk about,

55:58.560 --> 56:04.800
like, good design patterns, not just from building models, but for even for the product building

56:04.800 --> 56:13.280
NLP products itself. So in a variety of ways, this book is actually built for practitioners.

56:13.280 --> 56:21.120
It is a book that is built for anybody who is good at Python development to pick it up

56:21.120 --> 56:28.080
and quickly become familiar with national language processing and deep learning and the combination

56:28.080 --> 56:37.680
of the two and become enough proficient enough to go do their own research. And this is something

56:37.680 --> 56:43.360
I strongly believe in. A lot of people have also believed it. You know, research is something that

56:44.560 --> 56:51.760
you can learn on your own. And you don't need to go to graduate school and all that, like,

56:51.760 --> 56:58.240
you know, the fast AI folks and so on. And we believe in the same thing except we believe

56:59.120 --> 57:06.240
we express it differently. And in the sense that we think there is a lot of value in graduate

57:06.240 --> 57:12.880
school and of course, and in all the research that is out there. And there is a lot of value in

57:12.880 --> 57:20.800
reading papers and actually even writing papers. But there is all that has to be done in the context

57:20.800 --> 57:28.320
of real world product development setting. And every single course that I am looking at

57:29.200 --> 57:36.320
today is failing on that, right? And I am hoping that, you know, the book that we wrote is just

57:36.320 --> 57:42.160
scratching the tip of that iceberg. Well, Delic, thanks so much for taking the time to chat with us

57:42.160 --> 57:47.920
about what you're up to. Very interesting stuff. And we will be sure to continue to follow along.

57:47.920 --> 57:54.400
Absolutely, this is such a pleasure. And, you know, my god, time just flies. Yeah, yeah, my pleasure.

57:54.400 --> 57:56.400
Thanks so much. Yeah, thank you.

58:00.000 --> 58:04.720
All right, everyone. That's our show for today. If you like what you've heard here,

58:04.720 --> 58:09.840
please do us a huge favor and tell your friends about the show. And if you haven't already

58:09.840 --> 58:14.720
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes

58:14.720 --> 58:20.240
we've got in store for you. For more information on any of the shows in our AI conference series,

58:20.240 --> 58:28.240
visit twemolei.com slash AINY19. Thanks again to HPE for sponsoring the series.

58:28.240 --> 58:35.200
Make sure to check them out at twemolei.com slash HPE. As always, thanks so much for listening

58:35.200 --> 58:45.200
and catch you next time.

