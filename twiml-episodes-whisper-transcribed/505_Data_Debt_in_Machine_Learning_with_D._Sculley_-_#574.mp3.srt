1
00:00:00,000 --> 00:00:12,000
If we're doubling the amount of data that we throw at our models, it is reasonable to think that we might also be doubling the amount of verification that we need to do for our models and to be sure that they're doing the right thing.

2
00:00:12,000 --> 00:00:26,000
All right, everyone. Welcome to another episode of the Twemble AI podcast. I'm your host, Sam Charrington. And today I'm joined by Dee Scully.

3
00:00:26,000 --> 00:00:35,000
Dee is a director at Google Brain. Before we get into today's conversation, be sure to take a moment to head over to Apple Podcasts or your listening platform of choice.

4
00:00:35,000 --> 00:00:44,000
And if you enjoy the show, be sure you subscribe and leave us a five star rating and review. Dee, welcome to the Twemble AI podcast. Welcome to the show.

5
00:00:44,000 --> 00:00:46,000
Hi, Sam. Thanks for having me.

6
00:00:46,000 --> 00:00:54,000
I am looking forward to diving into our conversation. We're going to be talking about data centric AI and data debt.

7
00:00:54,000 --> 00:01:12,000
This topic of debt is one that at least in our community, you are well known for, you wrote a paper, the hidden technical debt in machine learning systems that at one point when we had our first Twemble Con conference all about ML ops and AI platforms.

8
00:01:12,000 --> 00:01:21,000
I heard later that some folks had a bingo game for when that little diagram that you created showed up in in someone's talk.

9
00:01:21,000 --> 00:01:29,000
So we'll talk a little bit about that, but before we do, I'd love to have you share a little bit about your background and how you can work in the field.

10
00:01:29,000 --> 00:01:35,000
Oh gosh, how I came to work in the field. So I had a bit of a non traditional path.

11
00:01:35,000 --> 00:01:44,000
So as an undergraduate, I was an art major and I was deeply concerned with issues of visual narrative.

12
00:01:44,000 --> 00:01:58,000
And I guess I graduated in the late 90s and discovered that I needed a job and became a teacher. I was a high school teacher in high schools around the world taught in in the Middle East and Abu Dhabi for a while.

13
00:01:58,000 --> 00:02:02,000
And then I was in Venezuela and Switzerland and California.

14
00:02:02,000 --> 00:02:05,000
Went back to grad school for education.

15
00:02:05,000 --> 00:02:17,000
It was really concerned about, you know, was it mean for a subject to be learnable? What does it mean for for a concept to be learnable or some concepts inherently more more difficult to learn than others.

16
00:02:17,000 --> 00:02:22,000
And I found that the education people didn't have a great set of concepts to draw on for that.

17
00:02:22,000 --> 00:02:38,000
You know, this was the early 2000s, you know, 2001 or so. I found the machine learning people that actually thought about this a lot. And they had wonderful concepts like VC dimension to talking about concept complexity and things like that. I just kind of got hooked and realized, oh no, I studied the wrong stuff.

18
00:02:38,000 --> 00:02:43,000
So I went back and read it, you know, undergraduate courses that I hadn't done.

19
00:02:43,000 --> 00:02:52,000
I was learning a master's in a PhD at Tufts and machine learning, sort of following over the field that way. Then came to Google soon after that.

20
00:02:52,000 --> 00:03:02,000
At that time, my focus area was very much applied machine learning started off like kicking out scammy and spammy advertisers from our systems.

21
00:03:02,000 --> 00:03:17,000
And this was in 2008 or so at a time when machine learning wasn't necessarily the go to tool for things like that, you know, there were plenty of rule based systems that were created by really smart people thinking carefully about, you know, what does it mean for something to be to be spam.

22
00:03:17,000 --> 00:03:32,000
And so, you know, I was able to to be part of kind of the transition of, you know, systems moving from being hand engineered code based systems to to being machine learning systems and been sort of part of that journey ever since it's been a been a wild ride.

23
00:03:32,000 --> 00:03:38,000
I bet is that what led to the interest in technical debt as a concept.

24
00:03:38,000 --> 00:03:48,000
Yeah, so over time at Google, I worked on this problem of finding scammy and spammy advertisers and kicking it out for several years.

25
00:03:48,000 --> 00:04:06,000
And then for a number of years after that, I worked on Google's ad click through prediction models, which turns out to be just a fascinating problem domain that has all of the problems of, you know, being very large scale production critical latency sensitive.

26
00:04:06,000 --> 00:04:17,000
With all kinds of feedback loops and, you know, other interesting machine learning and statistical problems, but also has the problem that it needs to work every single second of every single day.

27
00:04:17,000 --> 00:04:29,000
And so I came to really appreciate, if you will, the importance of having systems that can be maintained over a long period of time.

28
00:04:29,000 --> 00:04:37,000
We certainly ran into a bunch of the problems that ended up writing about in the technical debt papers in these production settings.

29
00:04:37,000 --> 00:04:53,000
I'm seeing, okay, well, what happens when you have a feature source that changes its semantics over time under the hood without someone telling you, or when you have to migrate from one version to another of a given data dependency.

30
00:04:53,000 --> 00:05:02,000
What happens when there is a feedback loop that exists through some other related system where the only connection is what data is being consumed.

31
00:05:02,000 --> 00:05:14,000
You know, getting to really cut my teeth in a production critical setting exposed me to a lot of those problems and helped to sort of motivate, you know, telling, telling others about these things.

32
00:05:14,000 --> 00:05:20,000
And it turns out that these problems, you know, they weren't unique to us and they weren't, you know, one off problems. They seem to be, you know, pretty general.

33
00:05:20,000 --> 00:05:27,000
They've resonated pretty strongly, I think, and pretty broadly for folks that are trying to do machine learning and production.

34
00:05:27,000 --> 00:05:37,000
Yeah, you know, it's funny because, you know, when you write a paper, you, you really kind of wanted to stand the test of time and for, you know, for to, you know, be relevant for many years.

35
00:05:37,000 --> 00:05:44,000
And this is one that I really wish kind of wasn't, you know, you know, here we are, you know, seven years later.

36
00:05:44,000 --> 00:05:50,000
And we're still like talking about, you know, machine learning and technical debt and things like that.

37
00:05:50,000 --> 00:05:54,000
I would like to live in a world where these problems are kind of capitalists.

38
00:05:54,000 --> 00:06:04,000
And they seem, you know, not to be at least not in any sort of full or final sense, although obviously, you know, folks are doing a much better job with these problems than we were, you know, back in the day.

39
00:06:04,000 --> 00:06:21,000
So this podcast will be published in the context of a series of shows focused on data centric AI. And you've recently applied this idea of technical debt or debt to thinking about data.

40
00:06:21,000 --> 00:06:34,000
Maybe let's start by having you talk broadly about this idea of data centric AI is it, you know, what does it mean to you? How, in what ways do you relate to it based on your experiences?

41
00:06:34,000 --> 00:06:45,000
Yeah, so the phrase data centric AI, you know, I came in contact with that sort of phrase through Andrew Ming who reached out to me and, you know, we had some interesting discussions on this.

42
00:06:45,000 --> 00:06:55,000
I think that the overall point is that in the end creating machine learning models just isn't that hard.

43
00:06:55,000 --> 00:07:07,000
And it's not that hard because a large number of people has spent a huge amount of time creating infrastructure and tools and toolkits and frameworks for helping people train and create machine learning models.

44
00:07:07,000 --> 00:07:17,000
And so packages like TensorFlow and PyTorch and Jax and all these things exist now in wonderful ways and they're supported by, you know, fantastic cloud based offerings.

45
00:07:17,000 --> 00:07:28,000
And so sort of the problem of how do you create a model is one that maybe 10 or 15 years ago was a very difficult one has now become a relatively simple one.

46
00:07:28,000 --> 00:07:39,000
A model framework platform cloud service by itself isn't useful at all until you have data that you're feeding into that modeling system.

47
00:07:39,000 --> 00:07:52,000
And so to a first approximation, you know, in the current day, 100% of the problem is gathering the appropriate data and figuring out what is the right data to train our models on.

48
00:07:52,000 --> 00:08:09,000
You can see, you know, just the influence of even a really simple thing like getting more data and when we're, you know, seeing things like, you know, the large language model world, you know, exploding in capabilities by exploding in data set sizes.

49
00:08:09,000 --> 00:08:24,000
And obviously when we're we're gathering larger and larger data sets, it's not because we're gathering the same data pointed same information many times, it's that we're gathering and ever more diverse ever broader set of sort of experiences and pieces of knowledge to throw out our model and systems.

50
00:08:24,000 --> 00:08:34,000
And so because it's no longer really, you know, a particularly hard challenge to create models.

51
00:08:34,000 --> 00:08:45,000
Starting to focus on what is the big problem, which is to create the data to curate the data to figure out, okay, are we, you know, gathering the most appropriate data.

52
00:08:45,000 --> 00:08:52,000
Do we have data that is the most representative that's going to handle all use cases, perhaps all users of different backgrounds.

53
00:08:52,000 --> 00:08:56,000
These are really the questions that are coming to the fore now.

54
00:08:56,000 --> 00:09:08,000
For a while, we've kind of taken it as a given that to train larger and larger models, we just need to create, you know, just need to collect more and more data.

55
00:09:08,000 --> 00:09:24,000
And this whole idea of data centric AI is starting to maybe shift the conversation away from quantity alone to quality and other characteristics and this concept of debt plays into into that.

56
00:09:24,000 --> 00:09:27,000
Can you talk about where debt comes into play?

57
00:09:27,000 --> 00:09:33,000
Yeah, sure. So, so we talk about debts, you know, the most common one that we talk about is technical debt.

58
00:09:33,000 --> 00:09:49,000
And so technical debt is sort of this nice metaphor that software engineers and software engineering community have developed to talk about sort of the costs that you incur from basically writing more code or creating more stuff.

59
00:09:49,000 --> 00:09:55,000
And those technical debts are often incurred, especially when you're trying to move quickly.

60
00:09:55,000 --> 00:10:04,000
And so, you know, if you've ever written some code super fast, maybe on a tight deadline, you might have noticed that maybe didn't have all the comments that you would like it to have.

61
00:10:04,000 --> 00:10:12,000
Or maybe it wasn't quite as well tested as you might have preferred or maybe you did a little bit more copy and paste and a little less refactoring into an ice API.

62
00:10:12,000 --> 00:10:26,000
So those are costs, you know, their choices that you make under a certain set of circumstances, often when you need to move quickly, you know, now, and you're willing to maybe go back later and kind of pay off those costs.

63
00:10:26,000 --> 00:10:39,000
At a later time that will allow you to do a better job of making sure that your, your system is maintainable over time and can be modified, maybe by others who aren't you or things like that.

64
00:10:39,000 --> 00:10:50,000
The future use, I find that the future me is often a really great person who's willing to take on all kinds of tasks, but the past me as a real jerk has given me a lot of work to do.

65
00:10:50,000 --> 00:10:56,000
When we think about concepts of technical debt as they apply to, to data sets for machine learning.

66
00:10:56,000 --> 00:11:19,000
The sort of this nice phrase that has kind of made the rounds, which is that in machine learning systems data takes the place of code, and it's, you know, kind of this nice profound thing to think about because you're saying, OK, we're defining the behavior of our systems that are learning from data on the code on the data that we're providing.

67
00:11:19,000 --> 00:11:47,000
Data is taking the place of code, the data that we're providing, then also has many of the qualities of code, which is that if we were to say double the amount of code that we're putting into a system, we might assume that we're also maybe doubling the number of behaviors that we're asking our system to create or to exhibit and thus maybe doubling the number of unit tests that we might need to put in or maybe doubling the amount of documentation that we might have to put in.

68
00:11:47,000 --> 00:12:00,000
For doubling the amount of data that we throw out our models, it is reasonable to think that we might also be doubling the amount of verification that we need to do for our models and to be sure that they're doing the right thing.

69
00:12:00,000 --> 00:12:10,000
So one of my favorite examples of this is in line with the GPT three models, so one of the first, you know, large language models to really make waves.

70
00:12:10,000 --> 00:12:30,000
And in the original GPT three paper, they had this really wonderful result, which is that just training on their large language model on text from the web with no special instructions over time it learned to do simple addition and arithmetic.

71
00:12:30,000 --> 00:12:42,000
And they had wonderful plots showing how these behaviors emerged as more and more data was given to the system and there are a couple of interesting things there. One is that they didn't design the system to do arithmetic.

72
00:12:42,000 --> 00:12:50,000
This was learned from the data that they gave it another is that the behaviors emerged at certain amounts of data.

73
00:12:50,000 --> 00:13:10,000
So you needed quite a lot of data to see these things emerge. And the last thing is that if you look at the plots in that paper, you'll notice that they weren't actually 100% accurate at doing three digit addition, even with many billions of examples, many billions of parameters.

74
00:13:10,000 --> 00:13:33,000
And so this is, you know, I'm not picking on GPT three here. It's a fantastic model. It was a great advance for the field. But in terms of the kinds of testing and verification that you would need to do to be confident that your model was going to do a great job at three digit addition or four digit addition or more complex mathematics.

75
00:13:33,000 --> 00:13:49,000
You can imagine needing to go ahead and creating some formal tests for this of one form or another and needing to get some level of assurance about what the qualities that this model is going to behave you give over time.

76
00:13:49,000 --> 00:14:03,000
And obviously addition is just one example. It's a really nice one to think about. But, you know, any model that we're throwing ever more amounts of data. We're essentially doing so because we wanted to have ever more fine grained and specialized behaviors.

77
00:14:03,000 --> 00:14:12,000
And each of those specialized and fine grained behaviors is one that it might do well or it might not and made if we really think it's important, we might want to check.

78
00:14:12,000 --> 00:14:33,000
And so in this sense, adding more and more data to our models, in some sense, creates a, you know, an opportunity for debt that will, you know, want to pay off through all of the careful activities of, you know, testing and documentation and, you know, understanding, you know, what are the capabilities and models in the long term.

79
00:14:33,000 --> 00:14:59,000
For example, also explain why machine learning is much more difficult than traditional software from testing and validation perspective. I'm thinking, you know, if you had an API that was adding to three digit numbers, you know, you'd probably tested on a few examples because if you get it right for, you know, a few three digit number pairs, you probably, it's probably working.

80
00:14:59,000 --> 00:15:06,000
Whereas in a probabilistic system or statistical system like a machine learning model, who knows where the errors are.

81
00:15:06,000 --> 00:15:13,000
Yeah, it's a great question, right? And so, I think it's worth reflecting on the question of why do we do machine learning at all.

82
00:15:13,000 --> 00:15:28,000
And it's not because it's, you know, better or easier than traditional code based systems. It's because we use machine learning in situations where we don't actually know how to write down an algorithm in code.

83
00:15:28,000 --> 00:15:38,000
And to have all of the correct behaviors, you know, we can't necessarily write all the if the analysis for every single situation that say a large language model is going to encounter.

84
00:15:38,000 --> 00:15:50,000
People tried that in the 80s 90s and it just, it doesn't work. Right. And so we use machine learning in situations where we can't have a formal verification of what it is we want our models to do.

85
00:15:50,000 --> 00:16:00,000
And that's the reason why it's so hard to then go back and say, okay, well, now what are the unit tests to apply to this? Because in some sense, the unit tests are.

86
00:16:00,000 --> 00:16:09,000
Unit testing overalls an idea of saying, okay, what are the behaviors that we've specified in advance and we're going to make sure that those those are going to exist.

87
00:16:09,000 --> 00:16:27,000
Right, you know, the whole field of machine learning has come or, you know, developed and sort of founded on this idea of ID test sets. So, you know, in traditional machine learning, you know, if you want to write a nurse paper, typically what you do is you have a data set of training data.

88
00:16:27,000 --> 00:16:39,000
You randomly sample, you know, some portion of that to hold out as your test data. And both of those are from the same identically and independently drawn distribution.

89
00:16:39,000 --> 00:16:56,000
And the idea is that, okay, if we know that that distribution is the one that we care about, then doing these two independent draws from that same distribution means that if the if a behavior is important, it'll be represented very likely in both our test and our training data.

90
00:16:56,000 --> 00:17:03,000
We'll be able to verify that our model is doing the right thing on this distribution by having these two draws from them.

91
00:17:03,000 --> 00:17:06,000
And this is a really good idea.

92
00:17:06,000 --> 00:17:20,000
It works well when the distribution that we're drawing our training data from matches the distribution that it will be used on practice in nervous papers. This happens, you know, all the time except for specialized cases where people are really probing.

93
00:17:20,000 --> 00:17:31,000
In the world, it's quite difficult to have a training data set that matches exactly the data that will be encountered in practice, right.

94
00:17:31,000 --> 00:17:47,000
And we've seen, you know, all kinds of different ways that this can hold true, but, but just, you know, the simple observation that tomorrow is not necessarily a specialized case of yesterday is, you know, is evidence that we're going to have some amount of data set shifts.

95
00:17:47,000 --> 00:18:05,000
The problem for this is that the IID test setting that is traditional was was developed by the machine learning folks initially because there is this problem from statistics that correlation is not equal to causation, right.

96
00:18:05,000 --> 00:18:08,000
We all know this to be true.

97
00:18:08,000 --> 00:18:17,000
And the models that we build typically are ones that pick up exclusively on correlations in the data.

98
00:18:17,000 --> 00:18:23,000
And this would of course be terrible if those correlations didn't hold at test time.

99
00:18:23,000 --> 00:18:35,000
The IID test setting is one in which because all of our data is IID, we don't have to worry about the distinction between correlational and causal factors.

100
00:18:35,000 --> 00:18:49,000
But the moment we have any amount of data set shift, now we do run into situations where the differences between correlations and causation become much more relevant, much more meaningful.

101
00:18:49,000 --> 00:19:14,000
Sort of the fun underlying assumption of machine learning, you know, typically that we're drawing, you know, IID test and training data is sort of so steeped into us as machine learning researchers and people that it's one that we really need to force ourselves to recognize and recognize on a regular basis.

102
00:19:14,000 --> 00:19:27,000
There are specific patterns or or failure modes that you see repeatedly in practice that are sources of data debt.

103
00:19:27,000 --> 00:19:35,000
You've mentioned kind of this fixation on historical data and the assuming IID of their other patterns that you see.

104
00:19:35,000 --> 00:19:44,000
And I think that looking at things like, you know, representativeness is a really nice nice lens to, to view some of these pieces on.

105
00:19:44,000 --> 00:19:56,000
And so, you know, representativeness, you know, we often talk about this in sort of like a fairness or a bias sense, where perhaps there's a group of users that might not be as well represented in the data.

106
00:19:56,000 --> 00:20:06,000
These issues are incredibly important and there's a whole, you know, field and, you know, literature on this, you know, folks out of the mirror, much more qualified to talk on.

107
00:20:06,000 --> 00:20:19,000
But as a lens for thinking about the kinds of problems that can come up when our data is not IID and when our data is not drawn in a representative way, like that's a really good important one to be thinking of.

108
00:20:19,000 --> 00:20:31,000
The representative can also be thought of in sort of a counterfactual sense, imagine that, you know, we're dealing with data that's that's coming from some recommender system.

109
00:20:31,000 --> 00:20:40,000
I don't know, you know, we're building a shoe website when two users type in certain kinds of shoes, we show them some shoes they might consider buying, you know, that kind of thing.

110
00:20:40,000 --> 00:20:57,000
In this world, we'll often only get data on shoes that we've shown to users and ones that, you know, ranked not quite highly enough to show at the top of the page or not quite highly enough to be shown to users, we probably don't have data on.

111
00:20:57,000 --> 00:21:18,000
But we certainly need to its own form of representation bias that is, you know, the sort of thing that can happen not only in shoe recommenders, but in many different systems where we're looking at observational data to try and understand the impacts of things that might not happen in practice as often or as well.

112
00:21:18,000 --> 00:21:38,000
The ideas of randomization can be really helpful there. So, you know, adding a little bit of explorer to exploit can be extremely important in such situations to make sure that we're actively seeking out parts of the data space that wouldn't otherwise people represented.

113
00:21:38,000 --> 00:21:54,000
You kind of think about this kind of debt that machine learning systems are accruing what are some of the things that we as a community or that you in particular in your projects have done to try to mitigate these issues.

114
00:21:54,000 --> 00:22:10,000
So, I think there's a lot that can be done. One that I really like to highlight is from to Nick or brew her idea of data sheets for data sets data cards is, I think, utterly fantastic and everybody should be doing it.

115
00:22:10,000 --> 00:22:31,000
So, this is the idea that when you buy an electronics component, there's a data sheet that comes with it that you can tell you look, what are the qualities of your op amp, where the ranges of inputs and outputs where you can hit the rails and those kinds of things and those data sheets, those data sheets are often dozens or even hundreds of pages.

116
00:22:31,000 --> 00:22:40,000
This set should also have a sheet that accompanies it, talking about the qualities of the data, where is it sourced, how is it sourced, what are its shortcomings, what kinds of things are in it.

117
00:22:40,000 --> 00:22:50,000
So, the people can have a really clear understanding of what kinds of blind spots a model might encounter if it was trained on that data, what an appropriate use of that data might or might not be.

118
00:22:50,000 --> 00:23:01,000
The second is, I think it's really important that we have good processes for auditing data for quality. For a machine learning person, it can feel like, oh gosh, I have to get my hands dirty with the data and the answer is, well, yeah, you do.

119
00:23:01,000 --> 00:23:19,000
And so, we've seen everything from folks working on, say, protein discovery and wet labs using data that comes from wet labs can often be incredibly noisy and understanding, what is the quality of the data, what's the level of noise, where they're confounding factors.

120
00:23:19,000 --> 00:23:38,000
And that's true for things that look like wet lab data, it's equally true for credit scoring or for recommender systems or for anything in between, you know, having a deep sense of what's in the data and, you know, whether it's matching the appropriate levels of quality and quality that we need.

121
00:23:38,000 --> 00:23:57,000
And when I hear you describe that, I think of kind of an exploratory, you know, EDA type of process where you're learning about the data, do you also see more structured approaches that folks are taking to ensure high levels of data quality.

122
00:23:57,000 --> 00:24:20,000
I think in terms of structured pieces, you know, like the data sheets are a good way to bring structure to that exploratory piece. There's a whole additional approach that we can think about drawing from the causal literature, you know, who said before, hey, you know, our typical machine learning models don't know the difference between a causal factor and a non causal factor.

123
00:24:20,000 --> 00:24:34,000
And, you know, at the same time, there's this whole world of causal learning, you know, driven by folks like Judea Perl and other contributors in that area, who have an entire literature built up around this idea of a causal inference graph.

124
00:24:34,000 --> 00:24:50,000
And a causal inference graph is basically a way of writing down all of the ways that different factors might be causally linked to each other in a data regime.

125
00:24:50,000 --> 00:24:58,000
And if you have one and it's complete, and you can pay the computation cost for using it, then you're going to be all set.

126
00:24:58,000 --> 00:25:10,000
The problems are that typically a fully complete causal inference graph needs to come from an omniscient oracle, those are often in short supply.

127
00:25:10,000 --> 00:25:21,000
And for it to be truly reliable, it also needs to be complete. And so these things may have, you know, many, many factors, including things like, you know, what is the temperature of the sun that day or who does what.

128
00:25:21,000 --> 00:25:28,000
And in reality, it is much more common that we don't have a fully complete causal inference graph.

129
00:25:28,000 --> 00:25:48,000
But we do maybe have ways of talking with domain experts or other folks, and at least writing down some useful number of nodes in a causal inference graph that might be imperfect, might be incomplete, but can still tell us some interesting things about a causal structure of our data world.

130
00:25:48,000 --> 00:26:02,000
It gives us a way to sort of look at our data and say, okay, what are the correlations that we might need to make sure are broken in our data so that we can make sure not to be picking up on a specific correlation.

131
00:26:02,000 --> 00:26:10,000
And where might we want to create a stress test that specifically inverts a common correlation in our data to make sure that our model is still robust.

132
00:26:10,000 --> 00:26:24,000
Making of a causal inference graph, not so much as, you know, a source of truth that our model is going to rely on exclusively, but as a useful formalism for writing down, at least what we know about a, a given problem.

133
00:26:24,000 --> 00:26:33,000
And making sure that we're being systematic about generating ways for, for testing those brought assumptions and in some sort of stress test kind of format.

134
00:26:33,000 --> 00:26:40,000
Is the, that approach of using causal inference graphs, is this something that you've worked with in your projects.

135
00:26:40,000 --> 00:26:43,000
Yeah, so we've done a bit of this.

136
00:26:43,000 --> 00:26:55,000
Some folks in my group, Victor Vich and his teammates in my group, I had a paper at NURPS this last year on using causal invariants in this way.

137
00:26:55,000 --> 00:27:03,000
In that paper, they, they applied it to large language models as a, as a nice way to, to do stress testing. And I think it's a, it's a pretty good idea.

138
00:27:03,000 --> 00:27:05,000
It's also one that I think can be applied much more generally.

139
00:27:05,000 --> 00:27:11,000
Can you talk a little bit about how it's applied in the context of large language models, if you're familiar with that paper.

140
00:27:11,000 --> 00:27:17,000
One of the nice things about text data is that it's reasonably easy to create counterfactual data.

141
00:27:17,000 --> 00:27:32,000
So, you know, if the data that you had at hand had the phrase, you know, he is a doctor appearing many, many times, it is relatively easy to create a, you know, quote unquote counterfactual piece of data that has the words she is a doctor.

142
00:27:32,000 --> 00:27:50,000
And so, using causal inference graphs to write down a set of invariants that we think should hold true, such as whether somebody's a doctor shouldn't depend on the gender of the pronoun in the sentence, then allows you to go through and, and,

143
00:27:50,000 --> 00:28:02,000
uh, invert some of those, uh, commonly occurring pieces in the data set, um, and allow you to, to stress test according what's the kind of the end product of that type of stress testing.

144
00:28:02,000 --> 00:28:15,000
Yeah, so there are a couple of pieces. Um, one is that, uh, you can use this to create a suite of stress tests and stress testing in general is one that you don't have to have a causal inference graph to start thinking of some useful stress test for your model.

145
00:28:15,000 --> 00:28:37,000
But, um, in general, putting together a suite of stress test data sets that explore and probe, um, different behaviors or different edge cases for, for a model that are maybe different than what might be represented in an IID test set or, you know, one way to, to give yourself sort of like the moral equivalent of a set of unit tests that,

146
00:28:37,000 --> 00:28:47,000
um, you can be, uh, working with and, and using to verify your moments. The second is that, you know, what this paper showed is that you can also then use some of these ideas of, uh,

147
00:28:47,000 --> 00:28:59,000
uh, causal invariance as, as an additional regularizer in your model and, and maybe not just discover that your model wasn't so good in some of these situations, but then also, you know, use that as a way to help improve the model, um, in the next generation.

148
00:28:59,000 --> 00:29:09,000
You were talking through some ideas around how we can, uh, address, um, you know, data, data, data and machine learning system from a data perspective.

149
00:29:09,000 --> 00:29:11,000
Are there others that come to mind?

150
00:29:11,000 --> 00:29:19,000
I think in general, really focusing on the discipline of model evaluation is something that I think is incredibly important.

151
00:29:19,000 --> 00:29:32,000
Um, you know, when you look at, uh, you know, some of the recent papers that have come out on large language models, so I quickly released one on, on poem, uh, just a couple days ago, and it's, um, forget exactly how many pages was, but it's something like 80 pages.

152
00:29:32,000 --> 00:29:39,000
And most of it was, uh, really insightful, very detailed evaluations of different stress test use cases.

153
00:29:39,000 --> 00:30:03,000
Um, I think this is kind of the model of how we should be, um, looking at not just large language models, but, but all of the models that we create, um, and being very careful and very thoughtful about, um, using specialized data sets, uh, where we at least know what it is that we're asking for, and the kinds of behaviors that we're looking at, um, to, uh, to probe and ensure what our models are doing.

154
00:30:03,000 --> 00:30:18,000
Um, and you might ask, okay, well, what happens if we don't have one of these special use cases around, um, there are some, uh, good ways to use, um, the data that we already have to help probe, um, there are two that I quite like.

155
00:30:18,000 --> 00:30:38,000
One is, uh, so you're, you're familiar, um, with cross validation and the idea of, you know, leaving out, you know, uh, one out of many different folds, um, from a data set and, and using your overall cross validation is a way to get more resolution on, um, goodness of the model.

156
00:30:38,000 --> 00:31:02,000
Um, there's a slightly different technique, um, that we could call, uh, leave one cluster out, where instead of, uh, creating a cross validation folds by randomly, we first cluster, cluster our data, um, before doing any training, and then do the moral equivalent of cross validation, but instead of holding a fold out, we hold one of these clusters out.

157
00:31:02,000 --> 00:31:13,000
And so we're creating a world in which our models, um, explicitly have some blind spots and, um, that blind spot corresponds to a cluster that we then use as, as test data.

158
00:31:13,000 --> 00:31:26,000
If over the course of these, you know, evaluating over all of these different, uh, held out clusters, we're doing a good job that can be reasonably sure that our models is probably pretty robust and not just picking up on, on spurious correlations.

159
00:31:26,000 --> 00:31:39,000
A second form of this looks like slicing our data, our evaluation data based on how close a given test example is to its nearest neighbor in the training data.

160
00:31:39,000 --> 00:31:55,000
And so you can imagine that if, um, uh, we're, one of our given test examples is like an identical copy of an example in our training data that it, you know, being good on that is nice to see that isn't really that useful and impressive.

161
00:31:55,000 --> 00:32:05,000
Um, if it's a little bit further away that it gives us a little bit more confidence, if it's very far away from its nearest neighbor in the training data, then that's probably a pretty tricky example to get right.

162
00:32:05,000 --> 00:32:12,000
And if we're doing a good job on that, um, you know, that kind of data, then that our model is probably pretty robust.

163
00:32:12,000 --> 00:32:31,000
And so, you know, bending up our evaluation data based on how far it at each of those examples is from its nearest neighbor in the training data can be a way to help us, you know, begin to answer some of these questions at the level of robustness of our models, even if we don't have any specialized data around.

164
00:32:31,000 --> 00:32:43,000
Assume a certain level of smoothness in the reward function or the relationship between the, the points in the data.

165
00:32:43,000 --> 00:32:48,000
So, you know, it's machine learning, right? There's always one hidden assumption that gets you.

166
00:32:48,000 --> 00:33:02,000
So, um, in this case, and I guess, you know, I guess I'm asking you more in practice, like, do you find that that is overly limiting a sanction or no more so than, you know, differentiability or other things that we have to deal with anyway.

167
00:33:02,000 --> 00:33:15,000
Yeah, so, um, I think the, the thing to focus on for this kind of evaluation is, you know, what is the distance function that you're using to judge, you know, how far away.

168
00:33:15,000 --> 00:33:18,000
The point is from its nearest neighbor in the training.

169
00:33:18,000 --> 00:33:26,000
Sometimes we have a distance function that makes a lot of sense, like if we're dealing with proteins using something like edit distance that's reasonably well established is great.

170
00:33:26,000 --> 00:33:30,000
In other cases, if we're dealing with images, we probably want to use some sort of image embedding.

171
00:33:30,000 --> 00:33:43,000
And there, you know, we probably don't want to have the embedding that we're using be like the embedding that we're learning at the same time we probably wanted to be independent in some way just so that we're not, you know, overly favoring one model or another.

172
00:33:43,000 --> 00:33:47,000
But yeah, we will need to have some sort of distance function that makes sense.

173
00:33:47,000 --> 00:33:54,000
If we don't have a distance function that makes any kind of sense, then we probably have some more thinking to do about our problem overall.

174
00:33:54,000 --> 00:34:00,000
Have you explored techniques like active learning to address some of these issues?

175
00:34:00,000 --> 00:34:06,000
Active learning, I think, is one of the great underused machine learning methods.

176
00:34:06,000 --> 00:34:17,000
Learning we're often gathering more data to label based on some amount of uncertainty or disagreement or things like that from our current model.

177
00:34:17,000 --> 00:34:28,000
So we're trying to find the most informative data to add into a given model for things like model evaluation using ideas from active learning in an evaluation sense can be extremely helpful.

178
00:34:28,000 --> 00:34:36,000
So there, you know, we're not necessarily using say model uncertainty to tell us which examples to go off and get new labels for.

179
00:34:36,000 --> 00:34:51,000
But maybe we're using some ideas of model uncertainty to tell us which examples will be most useful to focus on when trying to understand model failures or which examples might be most indicative of problem areas.

180
00:34:51,000 --> 00:35:05,000
You know, using active learning to go off and gather additional data for helping to augment our models capabilities for areas of the data space that might be underrepresented are of course extremely helpful in use as well.

181
00:35:05,000 --> 00:35:15,000
I think one last thing to say is that active learning can break down in areas where the things that we're looking for are incredibly rare.

182
00:35:15,000 --> 00:35:21,000
Active learning in a, you know, I came across this in the world of machine learning for spam detection.

183
00:35:21,000 --> 00:35:34,000
You know, when when spam is a 10% problem, then active learning can save you a tremendous amount of problems when your spammers are like crazy clever and weird and rare and they're more like 0.001% of the world.

184
00:35:34,000 --> 00:35:52,000
Then active learning can still show you an awful lot of unuseful or uninteresting data before you finally find a couple of hits. And there, I think Josh Attenberg had a paper on this maybe 15 years ago, but one of the things that can be helpful there is is not so much.

185
00:35:52,000 --> 00:36:15,000
Appearly model based, you know human agnostic active learning, but more of a model assisted exploration and using machine learning or maybe some learned distances to help you build exploratory tools to help you know smart humans go and find the most useful examples, you know, either a stress test or as counterfactuals or things like that.

186
00:36:15,000 --> 00:36:28,000
And all this sort of speaks to the idea that just as, you know, a couple of decades ago, people realized that if you have computers and you have humans, maybe there should be a field of human computer interaction.

187
00:36:28,000 --> 00:36:52,000
That helps to figure out how these these two very different entities should be interacting with each other, we're probably getting towards a world where there should be some sort of field of human data interaction that really focuses on, you know, tools and best practices and methods for people to be interacting with data in a way that that helps individual humans makes sense of datasets of size billions or larger.

188
00:36:52,000 --> 00:37:01,000
How far along do you think we are in, you know, having the tools that we need to approach the kinds of problems you're envisioning.

189
00:37:01,000 --> 00:37:18,000
I think we're still relatively early days talking to a fully licensed statistician can be really illuminating experience because, you know, the statistics people have known about the importance of data for forever and as with most things in machine learning, the statistics people were right all along.

190
00:37:18,000 --> 00:37:39,000
But one of the differences is that, you know, the sizes of datasets that folks are dealing with, you know, in current machine learning are so large that I think that they they make it difficult to apply many of the traditional practices of exploratory data analysis and things of that form.

191
00:37:39,000 --> 00:37:49,000
I think that we do need to really focus on tools and techniques that allow us to, you know, explore and comprehend datasets of the current size.

192
00:37:49,000 --> 00:38:08,000
Thinking about debt in the financial sense, you know, it's a number in the code sense, there are proxies like code coverage and other things are there metrics that you think can be proxies for understanding the level of data debt in the system.

193
00:38:08,000 --> 00:38:11,000
Does that make sense? Is there anything there? Yeah, it's a great question.

194
00:38:11,000 --> 00:38:17,000
You know, like, and I think it's important to say that, you know, technical debt is a metaphor. It's not a measure for me.

195
00:38:17,000 --> 00:38:25,000
The best proxy is, you know, how are you feeling about your model?

196
00:38:25,000 --> 00:38:36,000
Are you starting to get worried? And, you know, like, what are the issues that are keeping you up at night when you think about your model's performance or ability to deploy in your production setting?

197
00:38:36,000 --> 00:38:45,000
Overall, you know, things that are indicative of technical debt and code based systems are often long term metrics.

198
00:38:45,000 --> 00:38:55,000
Like, how easy is it to create a change in the system? How easy is it for someone else to modify the system or improve the system?

199
00:38:55,000 --> 00:39:07,000
You know, how often do we get, you know, paged in the middle of the night when something breaks or when there's an outage? And I think all of those kinds of questions can be reasonably asked in machine learning systems and models as well.

200
00:39:07,000 --> 00:39:14,000
Any any additional thoughts on data debt that we haven't covered yet?

201
00:39:14,000 --> 00:39:40,000
You know, more than anything, I think it's, it's pretty exciting to be living in the world in which training model isn't that hard and the hard part is putting the data together because I think that this is actually going to be a world that opens the field up to folks with all kinds of different backgrounds and insights, you know, you need a reasonably specialized set of skills to, you know, build a modeling platform.

202
00:39:40,000 --> 00:39:54,000
I think it's a relatively different set of skills to think about collecting and curating the appropriate data sets and to think about the right ways to be evaluating stress testing models.

203
00:39:54,000 --> 00:40:01,000
And I think that it's up to us as a community to make sure that we're being open to to a broad variety of perspectives on that.

204
00:40:01,000 --> 00:40:10,000
Awesome. Well, Dee, thanks so much for taking the time to share a bit about your work you've been doing on characterizing data debt.

205
00:40:10,000 --> 00:40:39,000
Thanks so much. Appreciate the time.

