1
00:00:00,000 --> 00:00:08,720
If you have to spend time on designing and optimizing manually for every problem you have

2
00:00:08,720 --> 00:00:12,240
and deploy it onto the device, that's not going to scale, right?

3
00:00:12,240 --> 00:00:19,040
So our researchers focus not on manually figuring out how to optimize the neural network itself.

4
00:00:19,040 --> 00:00:23,200
They focus on innovative ways of doing AI.

5
00:00:23,200 --> 00:00:34,160
All right, everyone. Welcome to another episode of the Twomo AI podcast. I am, of course,

6
00:00:34,160 --> 00:00:39,920
your host, Sam Charrington. And today I'm joined by Marali Akula, Senior Director of Software

7
00:00:39,920 --> 00:00:45,920
Engineering at Qualcomm. Before we get going, be sure to take a moment to hit that subscribe button

8
00:00:45,920 --> 00:00:52,400
wherever you're listening to today's show. Marali, welcome to the podcast. You began your career

9
00:00:52,400 --> 00:00:57,440
with more of a traditional software engineering focused. Tell us a little bit about how you found

10
00:00:57,440 --> 00:01:02,080
your way into machine learning. Thanks Sam, excited to be on the show. Thanks for having me.

11
00:01:02,880 --> 00:01:11,120
Like you said, I'm a software guy and I started working on a wireless software stack. I worked in

12
00:01:11,120 --> 00:01:19,600
the beginning couple of commercial products and commercial teams. Then after that, I joined

13
00:01:19,600 --> 00:01:25,600
the Carpet R&D group, which is a research group. And I was part of the 4G research team.

14
00:01:26,640 --> 00:01:32,640
So at Qualcomm, when you were talking of 4G research, we build the entire system.

15
00:01:33,680 --> 00:01:38,720
We bring it up OTA, which is over there and test it out properly.

16
00:01:40,480 --> 00:01:47,600
And make sure it works in the real world constraints. So we have this mindset,

17
00:01:47,600 --> 00:01:53,600
unless you build it, it's not real. So we build things that can scale into the scalable and

18
00:01:53,600 --> 00:02:00,160
deployable, for example. You start with the radio and the hardware, the firmware, the software,

19
00:02:00,160 --> 00:02:08,800
all the layers, talked to bottom. And we do this before even you go to standardize or commercialize it.

20
00:02:08,800 --> 00:02:14,080
So this is an entirely a pre-commercial setting. We build the entire thing. So that's what I was

21
00:02:14,080 --> 00:02:24,640
doing for a while, part of the 4G research team. Then late 2000s, the first of the Android smartphones

22
00:02:24,640 --> 00:02:31,200
was coming out. My boss was Jeff Gellar at that time. He came on this podcast a couple of times.

23
00:02:31,920 --> 00:02:37,120
So he suggested that I look into a smartphone user experiences and he connected me to a team

24
00:02:38,160 --> 00:02:43,600
which was looking into latency in wireless protocol and how we can improve the smartphone user

25
00:02:43,600 --> 00:02:49,280
experience. I started looking into the smartphone. And beyond wireless, there were so many components

26
00:02:49,280 --> 00:02:55,600
which you built into it. And also during that time, new applications were showing up and

27
00:02:56,400 --> 00:03:03,760
the so much going on. And I was really amazed about so much technology we packed into this platform.

28
00:03:04,480 --> 00:03:10,000
And I really wanted to work on the entire smartphone user experience. So that's what happened.

29
00:03:10,000 --> 00:03:17,040
And it was a lot of fun. After that, I continued on machine learning projects. And we did that.

30
00:03:17,040 --> 00:03:26,080
It was before the deep learning took off. And over time, it's become more and more deep learning.

31
00:03:26,960 --> 00:03:30,720
So that's where I am. Doing software engineering and AI research today.

32
00:03:30,720 --> 00:03:36,080
Awesome. Awesome. And so tell us a little bit about your role today and the team that you

33
00:03:36,080 --> 00:03:43,280
that you work with. I have a broader organizational responsibility as the head of the Carpet R&D

34
00:03:43,280 --> 00:03:49,840
software engineering team that is looking into forward-looking efforts across the AI spectrum.

35
00:03:50,960 --> 00:03:59,120
My day-to-day where I spend most of my time, I lead a software team in a cross-discipline effort

36
00:03:59,120 --> 00:04:08,400
for AI research. So researchers applied ML, folk, software, hardware, testers. We all sort of work

37
00:04:08,400 --> 00:04:15,360
together in a cross-discipline effort. And we apply the same mentality which we I talked to

38
00:04:16,000 --> 00:04:22,640
talked about earlier in other research efforts. If you unless you build it, it's not real. So we

39
00:04:22,640 --> 00:04:29,760
build out the technology that comes out of our AI research and prove that it can be deployed

40
00:04:29,760 --> 00:04:38,400
onto devices. So that is broadly the role we have. And on one side to make that happen,

41
00:04:38,400 --> 00:04:43,840
we are working with researchers jointly. And on the other side, we are working with the commercial

42
00:04:43,840 --> 00:04:50,080
teams to be always ready. So by doing this, looking at the entire system, building it out,

43
00:04:50,080 --> 00:04:59,440
we are in a unique position to figure out how or when our innovations can go into our products.

44
00:04:59,440 --> 00:05:04,320
So we'll get the entire full stack for that. Can you elaborate a bit on the full stack?

45
00:05:04,320 --> 00:05:09,840
I know from our past conversations, that's a concept and a terminology that's important to you

46
00:05:09,840 --> 00:05:16,800
and the team. But it's also something that we throw around in the industry quite a bit.

47
00:05:16,800 --> 00:05:24,080
So when we talk about full stack, let me start about what happens when Qualcomm is looking into AI

48
00:05:24,080 --> 00:05:32,560
technology. We have a lot of research going on to advance fundamental AI for applications that

49
00:05:32,560 --> 00:05:38,880
are going to be running on Snapdragon platform, either now or in future in different products.

50
00:05:38,880 --> 00:05:44,560
When you are innovating new ways of doing AI, researchers run the experiments on

51
00:05:44,560 --> 00:05:50,960
servers, which run in racks in a data center. And it's all good because you can run lots of

52
00:05:50,960 --> 00:05:57,680
experiments and get good results. Now the innovations which come out of that from this big compute,

53
00:05:59,280 --> 00:06:05,840
how do you make them run on the resource constraint devices? The software which runs on the

54
00:06:05,840 --> 00:06:11,280
device, the hardware architecture, the software, the tools which you use to deploy them onto the

55
00:06:11,280 --> 00:06:18,080
device, all that is different from what you did the software which you used when you actually

56
00:06:19,680 --> 00:06:27,120
ran the experiments. So that's the one which I call the full stack that is needed to deploy

57
00:06:28,160 --> 00:06:35,600
your AI innovations onto the devices. So we will have to look into the entire full stack.

58
00:06:35,600 --> 00:06:42,560
So research into the full stack is extremely important for us. We need to continuously improve

59
00:06:42,560 --> 00:06:50,880
every layer of this full stack and build it out and also show that it is real and it can be

60
00:06:50,880 --> 00:06:55,760
deployable and this keeps us ready all the time so that any innovation we have, we can actually

61
00:06:55,760 --> 00:07:03,600
deploy into the real world devices. So that's what we do. We continuously look at this hardware

62
00:07:03,600 --> 00:07:09,120
software full stack and figure out how to deploy it onto this resource constraint that

63
00:07:09,120 --> 00:07:14,640
mobile devices. When I hear you talk about that lifestyle, I think a little bit about I guess

64
00:07:14,640 --> 00:07:19,920
Docker and where the container technologies came from, you know, developers running things on

65
00:07:19,920 --> 00:07:24,960
their desktops and you know, trying to get it in a prod and not working and say, hey, it worked on

66
00:07:24,960 --> 00:07:33,200
my machine. And you know, in a lot of places, those same kind of container technologies have been

67
00:07:34,800 --> 00:07:41,920
adopted for machine learning, you know, workloads. But there's this whole other set of complexities

68
00:07:41,920 --> 00:07:50,720
when you're talking about mobile and devices, you know, not just kind of the runtime, but the

69
00:07:50,720 --> 00:07:55,760
constraints that you alluded to earlier in terms of power and other things. Can you elaborate a

70
00:07:55,760 --> 00:08:05,280
bit on when you think about devices in particular for machine learning, you know, what are the

71
00:08:05,280 --> 00:08:09,840
constraints and maybe, you know, I think we're all kind of familiar with, you know, them as users,

72
00:08:09,840 --> 00:08:15,520
like, hey, not a lot of memory, not a lot of compute, bandwidth issues, but, you know, there's some

73
00:08:15,520 --> 00:08:19,440
non-obvious things or like, how do you think about that as an engineer who works on this stuff

74
00:08:19,440 --> 00:08:26,560
constantly? Yeah, it's interesting. You mentioned how people build applications and deploy them and

75
00:08:26,560 --> 00:08:33,600
Docker containers. There is no Docker container on the device. But there are applications,

76
00:08:33,600 --> 00:08:40,480
applications which are running in real time, right, very efficiently. And with the neural network,

77
00:08:41,040 --> 00:08:47,600
efficiently mapped to the hardware, there's many constraints on the device. It's not, first of all,

78
00:08:47,600 --> 00:08:54,160
all these mobile devices, they're on batteries. They're mobile, right. If you take a smartphone or a

79
00:08:54,160 --> 00:08:59,760
VR headset or an autonomous driving car, power efficiency is extremely important. That is,

80
00:08:59,760 --> 00:09:04,960
I would say almost number one, real time latency is important. When you're running experiments on

81
00:09:04,960 --> 00:09:11,840
the server or even applications, for example, you can run it in batch mode on data. But when you're

82
00:09:11,840 --> 00:09:19,120
running on the device, as a signal comes from cameras, you need to be making inferences in real time.

83
00:09:19,120 --> 00:09:22,720
Again, you need to know that this is not the only thing which is running on the device. There's

84
00:09:22,720 --> 00:09:26,160
a lot of other things which are running through. And they all have to run in real time.

85
00:09:26,960 --> 00:09:37,040
Latency is extremely important. Area is a big concern. The chip size, for example, the size of the

86
00:09:37,040 --> 00:09:43,200
block accelerator which you're using, the memory sizes, they're all important because in a way,

87
00:09:43,200 --> 00:09:51,040
they impact power efficiency and latency. You can add more area and get the better latency,

88
00:09:51,040 --> 00:09:57,040
but you'll be trading off against power, right. So area is a big concern. The

89
00:09:58,640 --> 00:10:05,120
number of sensors, the resolution, the number of applications which you're running, the tasks,

90
00:10:05,120 --> 00:10:10,480
everything is just increasing. The size of the neural networks, and we need to pack more and

91
00:10:10,480 --> 00:10:15,200
more compute and you can't just keep increasing the chip size, right. So area is very important.

92
00:10:16,000 --> 00:10:24,080
And how you run it on the machine learning accelerator, using the on-chip memory versus going off-chip.

93
00:10:24,080 --> 00:10:28,000
When you actually run the neural network, you have limited on-chip memory. You can increase,

94
00:10:28,000 --> 00:10:31,760
but it's going to cost and area and power, you know, all those things come into picture.

95
00:10:31,760 --> 00:10:37,280
So on-chip memory. So there's many constraints like that when you run and it becomes extremely

96
00:10:37,280 --> 00:10:45,360
important that how you optimize the neural network to deploy and properly map it to your hardware

97
00:10:45,360 --> 00:10:53,840
resources and the entire, you know, software hardware to look at, you know, we look at all that

98
00:10:53,840 --> 00:10:58,080
and develop techniques on an ongoing basis how to optimize your neural network for deployment.

99
00:10:58,080 --> 00:11:05,040
So this is the neural networks are going to continue to get complex and complex, right. And we need

100
00:11:05,040 --> 00:11:12,400
to make them smaller and efficient and deploy them. You know, no from prior conversations with,

101
00:11:13,600 --> 00:11:19,680
you know, folks like Jeff who you mentioned, they're Qualcomm as well as folks on the research side

102
00:11:19,680 --> 00:11:29,280
that techniques like quantization and compression and related techniques are, you know, often,

103
00:11:29,280 --> 00:11:37,280
often a subject of research there. Are those the main types of things that you are applying when

104
00:11:37,280 --> 00:11:42,400
you're talking about getting these complex models working on mobile devices or are there other

105
00:11:42,400 --> 00:11:47,520
things? How do you think about that landscape and process? Yeah, I think what you mentioned,

106
00:11:47,520 --> 00:11:54,320
they are very important, those and lot more. We are looking at, when I say,

107
00:11:54,320 --> 00:12:02,240
entire full stack, you know, it starts from the top. We work very closely with the researchers,

108
00:12:02,240 --> 00:12:06,880
so our researchers when they design the network itself, you know, they are looking at what are the

109
00:12:06,880 --> 00:12:15,200
parallelism in this design, which we can exploit and how do we design this so that it can run

110
00:12:15,200 --> 00:12:20,400
efficiently on a resource constraint device, right? It starts from there. And after that,

111
00:12:22,560 --> 00:12:27,200
you come up with a good design for your architecture and how do you train? How do you train so that

112
00:12:27,200 --> 00:12:35,520
you can deploy it onto the device? Normally, if you train freely, it becomes pretty complex.

113
00:12:37,040 --> 00:12:42,320
One example of how to train for deployment onto the device is neural architecture search,

114
00:12:42,320 --> 00:12:49,120
while you are designing your neural network, you could use neural architecture search to get

115
00:12:49,120 --> 00:12:56,160
the best optimal architecture for your design, right? So that is next. And once you have the

116
00:12:57,120 --> 00:13:02,320
neural network, then how do you deploy it? You get the best accuracy and how do you deploy? When

117
00:13:02,320 --> 00:13:08,960
you deploy, these are the couple of things which you mentioned already. We run on integer hardware.

118
00:13:08,960 --> 00:13:16,960
And when you train, you are training in floating point. So quantization techniques are important.

119
00:13:16,960 --> 00:13:23,920
And compilation, how do you optimally map the tensors to the memory? And how do you map the compute

120
00:13:23,920 --> 00:13:29,840
to the instructions of the hardware accelerators? So those are important. So that is the next step,

121
00:13:29,840 --> 00:13:36,000
right? How do you deploy? And then how do you run on the device? When you optimally compile this

122
00:13:36,000 --> 00:13:45,120
onto the device, the application stages the sensor data coming from the camera, how does it get

123
00:13:45,120 --> 00:13:50,640
moved into the accelerator? What are the preprocessing stages? Then you run the neural network,

124
00:13:50,640 --> 00:13:57,040
what are the postpassing stages? So that entire application architecture is next and then the hardware

125
00:13:57,040 --> 00:14:05,520
architecture, right? So all of it. So that's what we look at when we say, you know, looking into

126
00:14:05,520 --> 00:14:11,600
full stack. So I'll also mention one other thing which is extremely important, right?

127
00:14:13,120 --> 00:14:20,880
I might have alluded to this in, you know, previous efforts which I mentioned too. So it takes a

128
00:14:20,880 --> 00:14:29,360
continuous research effort not just to design the neural network, but also to design tools and

129
00:14:29,360 --> 00:14:37,040
techniques which can make deploying them onto the device important. So we do a continuous research

130
00:14:37,040 --> 00:14:47,600
into full stack with a mindset that our research in a way is running on real devices on an ongoing

131
00:14:47,600 --> 00:14:53,680
basis, okay? We sort of connect that being in the middle. We look at everything hardware software,

132
00:14:53,680 --> 00:15:00,000
algorithmic layers and all that and also the we collaborate, we collaborate with everybody because

133
00:15:00,000 --> 00:15:06,240
research is happening in universities, research is happening in our research teams and even in

134
00:15:06,240 --> 00:15:10,880
product teams, a lot of innovations are happening. So we sort of connect the entire dot.

135
00:15:10,880 --> 00:15:19,520
Can you maybe take us through an example of, you know, when you think of a complex model that's

136
00:15:19,520 --> 00:15:26,400
coming out of research, you know, what that, what that looks like and how you kind of walk this

137
00:15:26,400 --> 00:15:34,720
process that you identified for getting, you know, into the device? Yeah, sure. Monocular depth

138
00:15:34,720 --> 00:15:45,440
estimation is an application which we recently implemented and demonstrated. I can walk you through

139
00:15:45,440 --> 00:15:54,640
that. Monocular depth estimation is about creating a depth map of the scene in front of you. So

140
00:15:54,640 --> 00:16:01,920
this is very important for many applications. Thinking like autonomous vehicles or as one example.

141
00:16:01,920 --> 00:16:08,240
Autonomous vehicles is one example. Even mobile phones when you're, you know, pointing at something

142
00:16:08,240 --> 00:16:13,360
and you need to detect features of the face. For focusing for in the camera, for example.

143
00:16:13,360 --> 00:16:19,440
Yeah, a lot of applications like that and VR headsets drones, you know, drones are going at high

144
00:16:19,440 --> 00:16:25,120
speed. So you need to detect obstacles very fast. So in your autonomous driving example.

145
00:16:26,560 --> 00:16:33,280
How far is the car in front of you in the scene? And how far is a pedestrian or obstacle?

146
00:16:33,920 --> 00:16:40,480
So all those are very important and depth estimation is creating that depth map.

147
00:16:40,480 --> 00:16:46,800
We need to infer for every pixel in the scene a depth value. How far it is?

148
00:16:47,840 --> 00:16:56,000
And Monocular depth estimation is about applying a neural network and inferring this depth map

149
00:16:56,000 --> 00:17:02,320
from a single camera image. Monocular depth estimation is about singular single camera image.

150
00:17:02,320 --> 00:17:07,520
As opposed to like you've got some controlled environments with cameras all over the place

151
00:17:07,520 --> 00:17:15,280
that you can interpolate to figure out depth. Exactly. Yeah. So that is the application and we implemented

152
00:17:15,280 --> 00:17:21,280
it on the device and demonstrated, you know, we took it around the time for Newrips.

153
00:17:23,040 --> 00:17:31,040
So when we implemented that, then I can explain how we went through the process. So what happens

154
00:17:31,040 --> 00:17:40,560
is when accuracy is extremely important in this use cases, right? Researchers can get the

155
00:17:40,560 --> 00:17:46,000
best accuracy, but through those experiments you can end up creating very complex models.

156
00:17:46,000 --> 00:17:51,760
Then what ends up happening is it becomes so complex, it becomes almost invisible to

157
00:17:51,760 --> 00:17:57,440
deploy it on the device and there are a bunch of state of the art neural networks which can achieve

158
00:17:57,440 --> 00:18:06,000
that. So what our researchers did is they working closely with us and also working on an ongoing

159
00:18:06,000 --> 00:18:16,800
basis with this device mentality. Our researchers designed a neural network where at training time

160
00:18:16,800 --> 00:18:23,680
they increase the complexity, but that doesn't translate to inference time. So because of that,

161
00:18:23,680 --> 00:18:29,840
you can get best accuracy at training time, but you can still run it on the device. So that's

162
00:18:29,840 --> 00:18:34,480
where it starts, right? As I explained, you know, I think part of what you're saying is it starts

163
00:18:34,480 --> 00:18:45,440
of research like there are ways to do this kind of depth estimation, but the complexity of the

164
00:18:45,440 --> 00:18:51,200
resulting models is such that they're not feasible starting places for something that you eventually

165
00:18:51,200 --> 00:18:57,200
want to run on device. So you're kind of starting at the algorithm level with the thought of,

166
00:18:57,200 --> 00:19:03,760
hey, this eventually needs to run on the device. That is correct. Yeah. I mean, you could just say,

167
00:19:03,760 --> 00:19:09,120
oh, I can just increase add more layers, right? I'll increase the complexity and get, but we know

168
00:19:09,120 --> 00:19:15,600
what happens if you do that. So our researchers have that mindset. So it's a very important innovation

169
00:19:15,600 --> 00:19:21,600
to be able to increase the training time complexity, but not let that go to the device.

170
00:19:22,320 --> 00:19:29,920
So that's what our researchers did. And after that, we have a neural network and we take the design

171
00:19:29,920 --> 00:19:41,760
and the full stack team next took that even after that, right? Our inference per second speed was

172
00:19:41,760 --> 00:19:51,680
23 FPS. So that's not going to make it real time. So the next step was to make the model smaller

173
00:19:51,680 --> 00:19:59,680
and our full stack team used neural network architecture search NAS techniques to reduce the size

174
00:19:59,680 --> 00:20:05,440
we have NAS tools which we use. So that is the next step. Now you make the model smaller. The next

175
00:20:05,440 --> 00:20:14,960
step is to deploy it on the device. To do this, you need to quantize it. Some models quantize

176
00:20:14,960 --> 00:20:20,080
well, some models don't quantize well. In this particular case, we have to apply the our

177
00:20:20,080 --> 00:20:25,280
aim at techniques post training quantization techniques to make sure we don't lose the accuracy.

178
00:20:25,280 --> 00:20:33,920
So that is the next step. And after that, we use our compiler tools and compile it properly so

179
00:20:33,920 --> 00:20:40,880
that it can be mapped properly to the hardware which we have. And the next step is application design,

180
00:20:40,880 --> 00:20:48,400
right? We design the application properly and run it on the device, measure on device performance,

181
00:20:49,360 --> 00:20:54,800
and we have an integration team which touches it properly and makes sure everything is stable.

182
00:20:55,680 --> 00:21:00,080
The application, the architecture, the tools, any improvements which we made.

183
00:21:00,080 --> 00:21:07,840
And that is the entire process of how you take one of the use cases like monocular depth estimation

184
00:21:07,840 --> 00:21:15,920
all the way to the device. Let's dig into all that. You mentioned the first step is the algorithm.

185
00:21:15,920 --> 00:21:19,920
Can you share a little bit more detail about the algorithm that was developed?

186
00:21:19,920 --> 00:21:27,120
It's called, again, that the technology is monocular depth estimation. Our researchers came up with

187
00:21:27,120 --> 00:21:35,680
a method called x-digital. x-digital? Yeah, I'll summarize it. That's a lot of

188
00:21:36,400 --> 00:21:40,000
and we'll of course link to the full paper in the show notes.

189
00:21:41,680 --> 00:21:47,520
So what they did is they I earlier gave a hint that you know our researchers

190
00:21:48,560 --> 00:21:55,680
increased the complexity of the network while training, right? So the way they did it is they

191
00:21:55,680 --> 00:22:04,080
added semantic information. That increases the complexity of the network, but that semantic

192
00:22:04,080 --> 00:22:10,560
information is not needed on the device. So you don't need to transfer that network down and

193
00:22:11,440 --> 00:22:18,000
the training also happens in a self supervised manner. So this is a summary of what they ended up

194
00:22:18,000 --> 00:22:25,440
doing. We use extra information, increase the complexity of the network, but that extra information

195
00:22:25,440 --> 00:22:29,680
which is semantic information is not required to be transferred onto the device.

196
00:22:30,800 --> 00:22:36,320
So the method is called x-digital method. So that is the first step, you know, which our

197
00:22:36,320 --> 00:22:41,360
researchers were able to come up with this innovation. You mentioned the next step was

198
00:22:41,360 --> 00:22:48,160
applying neural architecture search. Even before you do that, you invest all this time and research

199
00:22:48,160 --> 00:22:54,240
like coming up with a more efficient algorithm. Like why do you need to go through the next step

200
00:22:54,240 --> 00:23:00,480
is it you know do you have predefined performance targets that you're trying to hit and you kind

201
00:23:00,480 --> 00:23:08,160
of go as far as you can with algorithm but you're not quite there or is there a you know I'm thinking

202
00:23:08,160 --> 00:23:13,920
of like some kind of efficient frontier of like you know where you get all your efficiencies from

203
00:23:13,920 --> 00:23:20,400
to overuse efficiency maybe. I think that's an excellent question. So think of it this way. This

204
00:23:20,400 --> 00:23:26,320
thing but we already have that in our aim at toolkit. So we were able to we didn't have to get into

205
00:23:27,440 --> 00:23:31,680
you know they were able to design the end-to-end system in a way where it can actually run on the

206
00:23:31,680 --> 00:23:36,320
device. They're looking at the complexity of the network while they're designing. When we are

207
00:23:36,320 --> 00:23:40,240
working jointly with them you know being in the same team software engineers working with the

208
00:23:40,240 --> 00:23:49,040
researchers, we looked at the end-to-end architecture and we found that this entropy encoding and

209
00:23:49,040 --> 00:23:54,160
entropy decoding could be a bottleneck. The reason it is is we can exploit the parallelism in

210
00:23:54,160 --> 00:23:58,880
the neural network and map it to our accelerator but when you do entropy decoding,

211
00:23:59,680 --> 00:24:04,400
when you need to decode a symbol you need to you need the previous symbol that means you need to

212
00:24:04,400 --> 00:24:08,800
decode the previous symbol it's sort of serialized the entire thing. So you have this wonderful neural

213
00:24:08,800 --> 00:24:14,800
network part which can be paralyzed but the resulting latent you end up serializing it. So we

214
00:24:14,800 --> 00:24:23,040
discussed with the researchers and then they looked at the latent and the mathematical properties

215
00:24:23,040 --> 00:24:29,440
of it and figured out how can we paralyze it and they figured out that latent are actually either

216
00:24:29,440 --> 00:24:34,720
independent or conditional independent and they were able to like figure out how to design a parallel

217
00:24:34,720 --> 00:24:41,520
entropy encoder and parallel entropy decoder. So once they did that we took that and implemented

218
00:24:41,520 --> 00:24:48,000
it on the CPU. SnapDragon platform is a heterogeneous platform we have the accelerator, we have the

219
00:24:48,000 --> 00:24:54,560
CPU, we have the GPU, we have DSPs. So we were able to implement the parallel entropy decoder

220
00:24:54,560 --> 00:25:00,320
on the CPU and match the parallelism which is happening on the accelerator and then we were

221
00:25:00,320 --> 00:25:05,200
able to get it to real time. A lot of our conversation thus far has been focused on

222
00:25:05,200 --> 00:25:14,000
trying to achieve like inference latency targets and other inference oriented

223
00:25:16,320 --> 00:25:24,240
objectives on devices. Is there anything that you're aware of looking at more of the

224
00:25:24,880 --> 00:25:32,160
like training on devices? Is that is that a thing yet? It is a thing. So are the past

225
00:25:32,160 --> 00:25:42,000
number of years the full stack on the device was more about running inference workloads

226
00:25:42,800 --> 00:25:49,680
while training was mostly centralized and on the servers. So the full stack accounts for

227
00:25:49,680 --> 00:25:57,280
optimizing the inference the forward pass on the device but lately past couple of years

228
00:25:57,280 --> 00:26:05,040
are even more we're seeing more and more training on the device using the local data.

229
00:26:05,760 --> 00:26:10,000
Primarily for personalization reasons you have some data which is very unique to you you can

230
00:26:10,000 --> 00:26:15,120
personalize the model for yourself. So those kind of use cases have already been deployed and

231
00:26:15,120 --> 00:26:23,680
they're showing up. So training is becoming an important part of the full stack and we are looking

232
00:26:23,680 --> 00:26:32,640
at it. So we at Neurip's 21 we actually implemented a federated learning system

233
00:26:33,600 --> 00:26:38,960
for distributed training completely on the devices. Nothing is happening on the server except for

234
00:26:39,680 --> 00:26:46,560
getting the incremental updates from various devices and combining them. So we implemented

235
00:26:46,560 --> 00:26:53,200
end-to-end federated learning training system. So that is also part of our full stack effort

236
00:26:55,360 --> 00:26:59,920
and it requires a lot of software engineering. It's a distributed system. So you need to make

237
00:26:59,920 --> 00:27:05,600
sure the entire architecture works well and it works for both PyTorch and TensorFlow users.

238
00:27:06,160 --> 00:27:11,280
We created libraries on the device. We created connections to a coordinated running in the cloud

239
00:27:11,280 --> 00:27:20,480
so that everything scales up to thousands of devices. And is the idea that like Amit and Dana and

240
00:27:20,480 --> 00:27:26,000
some of these other tools that it kind of just goes into the bag of tricks and the next time you

241
00:27:26,000 --> 00:27:33,360
need to build a model that could benefit from distributed training you can apply this or do you

242
00:27:33,360 --> 00:27:39,840
think for in the near future it will require that manual kind of hand crafting that we've talked

243
00:27:39,840 --> 00:27:45,600
a little bit about. Yeah so the goal is to avoid anything manual which is not going to scale.

244
00:27:45,600 --> 00:27:52,640
So you need to build these tools. They are going to become automatic more and more. And the federated

245
00:27:52,640 --> 00:27:59,760
learning framework we package it in a way that our teams who want to do federated learning can

246
00:27:59,760 --> 00:28:05,120
just take it and run the experiment but also be able to deploy without having to do a lot of work

247
00:28:05,120 --> 00:28:11,040
downstairs. Awesome, awesome. So thinking back to the examples that we've talked about

248
00:28:12,960 --> 00:28:20,000
the monocular depth estimation, the neural video codec, we've talked about kind of this process

249
00:28:20,000 --> 00:28:27,760
up from getting from research to real world kind of touch it, see it on a device. Does that mean

250
00:28:27,760 --> 00:28:35,760
that they're products and that they're productized or is that a further step that you take these

251
00:28:35,760 --> 00:28:41,840
products or these projects? There is a further step. So we are still part of the research team.

252
00:28:43,120 --> 00:28:51,200
Whatever we create is deployable. But our focus is on creating a solid research pipeline.

253
00:28:51,200 --> 00:28:59,200
A research pipeline of fundamental AI improvements and also how they can be optimally

254
00:29:00,080 --> 00:29:07,280
deployed onto the device. So across the full stack. So we innovate, implement and create

255
00:29:07,280 --> 00:29:13,440
this research pipeline. But while doing that, the product teams also have innovative advances

256
00:29:13,440 --> 00:29:20,240
in the architecture, both hardware and software. And we work very closely with the product teams.

257
00:29:20,240 --> 00:29:28,960
And by doing a full stack approach in the research team, we are always ready. When the need arises,

258
00:29:30,000 --> 00:29:37,840
the product teams can immediately decide to take some of the innovations. Some go very early,

259
00:29:37,840 --> 00:29:46,160
some go a little late. But immediately as a very soon, take it and put it into their commercial

260
00:29:46,160 --> 00:29:51,920
roadmap, product roadmap, then after that, it's about scaling. Scaling across Qualcomm business.

261
00:29:51,920 --> 00:29:59,280
That's a pretty big effort. So we focus on the research side, building a solid research pipeline

262
00:29:59,280 --> 00:30:06,480
and our product teams are focusing on the product roadmap. We work very closely and collaboratively.

263
00:30:06,480 --> 00:30:16,480
And the form that these products end up taking, like does that, say, monocular depth estimation,

264
00:30:16,480 --> 00:30:24,320
would that be built into a silicon or would that be software that's part of a toolkit or

265
00:30:24,320 --> 00:30:28,800
kind of all of the above, depending on what makes the most sense? Monocular depth estimation is one

266
00:30:28,800 --> 00:30:34,000
example. We have a lot of AI technology in our platform, in Snapdragon platform. We have the entire

267
00:30:34,000 --> 00:30:38,640
camera, everything which is needed to do camera, everything which is needed to do video processing.

268
00:30:39,360 --> 00:30:46,080
So the answer is all of the above. All these innovations, like applications,

269
00:30:46,880 --> 00:30:51,840
our product teams will take integrated into the products. Same thing with the

270
00:30:54,080 --> 00:30:59,520
full stack innovations, our software team, product software team takes it and puts it into the

271
00:30:59,520 --> 00:31:07,680
roadmap. But we also make sure that we figure out the best way we can put it out into the community,

272
00:31:07,680 --> 00:31:15,440
into the world. Like for example, a model efficiency toolkit. We felt, you know, it's important that

273
00:31:15,440 --> 00:31:23,200
that's an open source project. So anybody can use it to quantize to run an integer hardware.

274
00:31:23,200 --> 00:31:28,960
We also have a model zoo project. Some of the applications will just put it out in the model zoo.

275
00:31:30,320 --> 00:31:34,800
And with some of them, we just directly give it to the customer. So they can integrate. So it's

276
00:31:34,800 --> 00:31:43,440
sort of all of the above. Okay. Awesome. Awesome. Well, Marali, thanks so much for taking the time to

277
00:31:43,440 --> 00:31:51,360
share a bit about how you see kind of this full stack ecosystem and getting some of the research

278
00:31:51,360 --> 00:31:59,520
ideas into product. Thank you. Thanks, I'm just so really grateful. Thanks.

