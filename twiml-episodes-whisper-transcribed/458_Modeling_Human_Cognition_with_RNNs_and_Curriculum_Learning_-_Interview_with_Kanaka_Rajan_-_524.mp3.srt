1
00:00:00,000 --> 00:00:16,640
All right, everyone. I am here with Kanaka Rajan. Kanaka is an assistant professor at the

2
00:00:16,640 --> 00:00:22,400
Icon School of Medicine at Mount Sinai. Kanaka, welcome to the Twimal AI podcast.

3
00:00:22,400 --> 00:00:28,320
Thank you so much for having me. I'm really looking forward to jumping into our conversation. We'll

4
00:00:28,320 --> 00:00:34,000
be talking about your work in the field of computational neuroscience. But to get us started,

5
00:00:34,000 --> 00:00:38,000
I'd love to have you share a little bit about your background and how you've come to bridge

6
00:00:38,000 --> 00:00:45,920
the worlds of biology and artificial intelligence. Thank you. I sort of took a circuitous route

7
00:00:45,920 --> 00:00:51,840
to getting into neuroscience and specifically computational neuroscience. I was trained in

8
00:00:51,840 --> 00:00:58,080
engineering and physics. And then, you know, I sort of collected fields along the way.

9
00:00:59,760 --> 00:01:04,480
I thought I was going to be an experimental neuroscientist when I first got to graduate school.

10
00:01:05,360 --> 00:01:11,840
And then I was very quickly dissuaded of that particular delusion of mine. And I thought,

11
00:01:11,840 --> 00:01:16,320
okay, I should stick to at least the tools that I have acquired in my various

12
00:01:16,320 --> 00:01:21,280
dabblings across fields. And so computational neuroscience seemed like the perfect melding

13
00:01:22,480 --> 00:01:26,320
sphere for such a thing. So, essentially, how I've gotten here.

14
00:01:26,960 --> 00:01:31,600
And what was it about the experimental approach that you didn't find appealing?

15
00:01:32,560 --> 00:01:38,800
So, I do find it incredibly exciting and challenging. So, that wasn't the issue. The issue

16
00:01:38,800 --> 00:01:43,760
resident hands either, right? Once somebody showed me how to do experiments, I could do them.

17
00:01:43,760 --> 00:01:48,080
Yeah. And the reason I know this is because, you know, we do rotations. In graduate school,

18
00:01:48,080 --> 00:01:52,640
you show up and you spend eight weeks or 10 weeks in somebody's lab. So, I did, like,

19
00:01:52,640 --> 00:01:58,080
four experimental rotations at Brandeis University with, you know, world-class experimentalists.

20
00:01:58,640 --> 00:02:04,080
My issue was the complete inability to design experiments. So, you know, I would show up

21
00:02:04,080 --> 00:02:09,200
the next day and it would still be completely blank slate. I'd be like, okay, now tell me what to do.

22
00:02:09,200 --> 00:02:15,920
I didn't have the same sorts of issues with computational stuff. So, you know,

23
00:02:15,920 --> 00:02:21,040
it was really a fork. I did decide should I spend the next 10 years trying to get better at designing

24
00:02:21,040 --> 00:02:28,640
this whole other complicated, incredibly rich field. Or should I just take my tools and attack

25
00:02:28,640 --> 00:02:36,880
a slightly different sliver of the same problem? Nice, nice. Tell us a little bit about your research

26
00:02:36,880 --> 00:02:42,480
writ large. What are the broad questions and issues that interest you?

27
00:02:43,920 --> 00:02:50,320
So, I work exactly at the interface between, you know, AI, artificial intelligence, ML,

28
00:02:50,320 --> 00:02:57,120
and experimental neuroscience. So, the field that I work in straddles both. What I do for

29
00:02:57,120 --> 00:03:05,280
living can be described as building LEGO models of the brain. So, what I want to do is to build,

30
00:03:05,280 --> 00:03:10,080
essentially, you know, a model that, you know, makes like the Death Star and looks like a Death Star,

31
00:03:10,080 --> 00:03:16,880
but it's actually engineered by me as an artificial system. I have built it to mimic something

32
00:03:16,880 --> 00:03:23,840
the biological brain does. My goal is then to reverse engineer this LEGO Death Star and see what

33
00:03:23,840 --> 00:03:28,000
makes it tick. And then, of course, the rest is a little bit of hope and prayer and a little

34
00:03:28,000 --> 00:03:33,120
bit of hard science to say, well, is this the same operating principles that the biological brain

35
00:03:33,120 --> 00:03:39,520
uses. So, I build essentially artificial models that make like the biological brain, but are

36
00:03:39,520 --> 00:03:48,400
much more simplified and engineered. And it sounds like the, you're referring to the brain as a

37
00:03:48,400 --> 00:03:54,960
whole, meaning you're trying to build what I might call like end-to-end models of the brain or

38
00:03:54,960 --> 00:04:01,360
comprehensive models of the brain as opposed to simplified models of particular subsystems within

39
00:04:01,360 --> 00:04:08,960
the brain or the idea. I would say both. So, there are models of the brain that I build which are

40
00:04:08,960 --> 00:04:13,680
more like behavior level models, right? I could take an artificial, you know, I build neural

41
00:04:13,680 --> 00:04:20,080
network models within this class of essentially computational modeling. And I can train these

42
00:04:20,080 --> 00:04:26,880
neural network models to capture a behavior of the whole organism. Like, for example, you know,

43
00:04:26,880 --> 00:04:31,520
if you're standing in front of an elevator bank and, you know, both doors open, which one do you

44
00:04:31,520 --> 00:04:37,520
pick, that kind of decision making, or counting while you're walking, estimating time, learning,

45
00:04:37,520 --> 00:04:43,520
remembering, deciding, those types of behaviors. So, that would be in the category that you just

46
00:04:43,520 --> 00:04:49,280
mentioned of brain rich large, because they're capturing behavior of the whole organism. But within

47
00:04:49,280 --> 00:04:55,520
it, I also build multi-region neural network models, which would capture the interactions between

48
00:04:55,520 --> 00:05:01,600
interconnected brain areas. And so, that is a much more granular, more biologically grounded,

49
00:05:02,480 --> 00:05:08,160
sort of system. And that may or may not capture behavior, but what this model does is capture

50
00:05:08,160 --> 00:05:14,880
the dynamics or the neural activity over time from all of these brain regions. So, we can certainly

51
00:05:14,880 --> 00:05:21,920
build whole brain models at both the behavior and the neural level. And I think I build both kinds of

52
00:05:21,920 --> 00:05:28,960
models and any combination of the two. Got it, got it. One of the things I found fascinating about

53
00:05:28,960 --> 00:05:36,080
your work is particular to this, well, I don't even know if it's limited to behavior or dynamics,

54
00:05:36,080 --> 00:05:42,640
you know, one versus the other, but this idea of looking at cognitive processes that, you know,

55
00:05:42,640 --> 00:05:49,280
may manifest as kind of a split second decision about an elevator and then drilling into how they

56
00:05:49,280 --> 00:05:55,520
play out all kinds of different timescales. Can you elaborate a little bit on that idea and how you

57
00:05:56,960 --> 00:06:04,080
how you dig into that? Sure. So, there's a tension in neuroscience in general, right? When you

58
00:06:04,080 --> 00:06:09,360
want to understand something about the biological brain, there's a tension. The building blocks the

59
00:06:09,360 --> 00:06:14,640
Lego that I just mentioned, the building blocks are much, much faster, right? The neurons are at,

60
00:06:14,640 --> 00:06:20,400
you know, working at millisecond timescales. They're connected with synapses and those dynamics are

61
00:06:20,400 --> 00:06:26,080
at most hundreds of milliseconds. And yet, you know, I have to show a my niece or nephew or

62
00:06:26,080 --> 00:06:31,440
godchild how to solve this elevator problem, which elevator button press and how to go to. But this

63
00:06:31,440 --> 00:06:36,560
kid is going to remember this for the duration of their lifetimes. So, there's essentially a

64
00:06:36,560 --> 00:06:43,440
tension between the timescales of the building blocks and the timescales of behaviors and learning

65
00:06:43,440 --> 00:06:50,480
of behaviors over lifetimes essentially. So, that somehow the biological system bridges seamlessly.

66
00:06:50,480 --> 00:06:58,080
Artificial systems are less good at this, but both feels like AIML and neuroscience are both

67
00:06:58,080 --> 00:07:04,720
kind of geared towards this fundamental tension. And so, one of the things that, you know, my work has

68
00:07:04,720 --> 00:07:12,000
been able to do is to exploit a certain feature of a type of neural network models, which are called

69
00:07:12,000 --> 00:07:17,920
recurrent neural network models, sometimes RNNs for short. And those are characterized by, you know,

70
00:07:17,920 --> 00:07:23,520
connections between active neuron-like units going forwards and backwards. So, they have feed-forward

71
00:07:23,520 --> 00:07:29,200
and feedback connections. This lets them have long-range dynamics. So, what that means is that they

72
00:07:29,200 --> 00:07:37,680
can have ongoing patterns of neural-like activity, essentially in perpetuity. However, I can

73
00:07:37,680 --> 00:07:44,960
engineer this RNN to have model elements that are still fast, like the neural timescale.

74
00:07:44,960 --> 00:07:51,520
So, there's a fundamental feature of these types of models that has a convenient feature.

75
00:07:52,160 --> 00:07:57,840
So, one of the things that me and several people who work on RNNs are working on is to say,

76
00:07:57,840 --> 00:08:04,160
well, can we use this natural feature of this type of neural network model as a substrate?

77
00:08:04,160 --> 00:08:10,160
On which to build this LEGO model that has other functionality? Like, can I get this recurrent

78
00:08:10,160 --> 00:08:15,600
neural network model to solve the elevator problem? But, you know, the brain doesn't just do the

79
00:08:15,600 --> 00:08:21,040
elevator problem, right? Like this kid recognizes, you know, elevators from a picture book, even if it

80
00:08:21,040 --> 00:08:27,200
isn't their real elevator, it can solve escalators and it can, you know, there's a lot of flexibility.

81
00:08:28,080 --> 00:08:33,680
That is a harder problem. So, yes, I can exploit the mathematics of recurrent neural network models

82
00:08:33,680 --> 00:08:40,160
and their ability to produce long-time scales, but I still have to engineer them. So, they have

83
00:08:40,160 --> 00:08:47,760
the flexibility to do many different things all over many, all over long-time scales, using the same

84
00:08:47,760 --> 00:08:53,680
machinery that only biology has access to, which, inconveniently for us, biology is annoying.

85
00:08:53,680 --> 00:09:06,480
You know, in talking about the human side of this equation, one of the things that comes to mind

86
00:09:06,480 --> 00:09:14,400
is memory. And, you know, certainly RNNs have a form of memory, but then you also talk about

87
00:09:15,040 --> 00:09:21,120
kind of dynamics. You know, we think of memory in a lot of ways as kind of the static snapshot of,

88
00:09:21,120 --> 00:09:27,520
you know, either, you know, some rules or a view of the world or something like that. Dynamics

89
00:09:28,400 --> 00:09:33,920
sounds, well, sounds more dynamic, right? It's like the evolving state of the system, and I'm wondering,

90
00:09:34,480 --> 00:09:39,520
you know, can you kind of put some color on that, like the relationship between memory and

91
00:09:39,520 --> 00:09:44,720
dynamically evolving system states? So, that's an outstanding question, and you've really hit the

92
00:09:44,720 --> 00:09:49,840
nail right on the head. So, this is a fundamental thing, like what you're asking about is a fundamental

93
00:09:49,840 --> 00:09:56,160
feature, right? We have to reconcile the ongoing nature of this, you know, neural activity, right?

94
00:09:56,160 --> 00:10:01,360
Even if my eyes are closed, there's patterns of activity swirling in my brain, with the fact

95
00:10:01,360 --> 00:10:05,600
that I still remember the elevator problem from when I was like five years old, but that has to

96
00:10:05,600 --> 00:10:11,200
be the static piece. And that's kind of how people thought memory used to work. So, in fact,

97
00:10:11,200 --> 00:10:17,120
the earlier model, earliest models of memory were, you know, like the hubfield model, all involved

98
00:10:17,120 --> 00:10:24,640
this idea of what are known in my field as fixed points. But that just involves saying, let's say

99
00:10:24,640 --> 00:10:30,960
I want to remember two things, right? What I would do is take a group of neurons over here, elevate

100
00:10:30,960 --> 00:10:36,560
their firing rate to a certain static value, and that's a memory. If I want to remember another

101
00:10:36,560 --> 00:10:42,240
thing, I take another group of neurons over here, I elevate their firing rate to some other number,

102
00:10:42,240 --> 00:10:48,240
and I leave it there. So, that number doesn't change, and for however long it does not change,

103
00:10:48,240 --> 00:10:53,520
I have remembered object A and object B. That's how people used to think memory used to work.

104
00:10:54,160 --> 00:11:00,160
And so, then experiments were designed to test this model. So, they would have animals that were,

105
00:11:00,160 --> 00:11:05,200
you know, making movements based on remembering a number or making decisions and so forth,

106
00:11:05,200 --> 00:11:10,400
in very, because experimental technologies like Way Back When were still, you know, pretty,

107
00:11:10,400 --> 00:11:15,520
um, pretty simple compared to what they're able to do now, and they observed such activity

108
00:11:15,520 --> 00:11:21,760
anyway in the brain. Then came this revolution in neuroscience where people were able to record

109
00:11:21,760 --> 00:11:26,480
electrical activity from the brain while animals were doing much more complicated versions of the

110
00:11:26,480 --> 00:11:30,720
same problem. So, there's a version of this elevator problem where you've taken this kid,

111
00:11:30,720 --> 00:11:35,760
strap them down and only force them to make a choice with their eyeballs, which elevator to get

112
00:11:35,760 --> 00:11:42,160
into, or you have this version of the problem where this child is walking towards the elevator and

113
00:11:42,160 --> 00:11:48,160
making a decision while all this ongoing activity is occurring, all this dynamics of the movement

114
00:11:48,160 --> 00:11:53,520
are occurring, and then, you know, the door opens, there's a pause, and the the kid can get into

115
00:11:53,520 --> 00:11:58,800
the elevator. So, there is dynamics, but the kid hasn't forgotten the elevator problem. How might

116
00:11:58,800 --> 00:12:06,480
this work? So, one of my postdoc papers involved proposing this idea of what if it's not fixed

117
00:12:06,480 --> 00:12:13,040
points? What if it is waves of activity? Or in other words, sequences? So, this is, you know,

118
00:12:13,040 --> 00:12:18,800
individual neurons only fire for a short duration, but if you look at the whole population,

119
00:12:18,800 --> 00:12:24,560
there's like a wave that goes through, where each neuron has a temporarily kind of sparse bump,

120
00:12:24,560 --> 00:12:30,560
but if you tile all of the bumps together, you get the same duration, but now it's essentially

121
00:12:30,560 --> 00:12:37,760
doing what the fixed point used to be doing, but it's a much more robust way that has both features.

122
00:12:37,760 --> 00:12:43,360
It has steady representation during the period for which you want the memory to be represented,

123
00:12:43,360 --> 00:12:50,560
but you also have dynamics because this activity is actually moving. And so, now we're proposing

124
00:12:50,560 --> 00:12:55,920
more advanced versions of this hypothesis, and we're saying, what if it's not even sequences?

125
00:12:55,920 --> 00:13:00,560
In our kinds of tasks, right, like I'm talking to you, but I also remember, I've got to keep an

126
00:13:00,560 --> 00:13:06,720
eye on the time, because how long I've ever ambled about sequences. So, so now I'm thinking,

127
00:13:06,720 --> 00:13:11,520
if you were to record electrical activity in my brain, it's unlikely to be just sequences.

128
00:13:11,520 --> 00:13:18,000
Maybe it's some other high-dimensional repeating pattern. So, we have advanced these types of

129
00:13:18,000 --> 00:13:23,600
theories to incorporate both features, the stability of the memory that you asked about,

130
00:13:23,600 --> 00:13:28,080
along with the ongoing dynamics that is true of biological brains.

131
00:13:28,880 --> 00:13:35,280
So, you proposed as an example, this very simple model of memory, where you're trying to,

132
00:13:36,320 --> 00:13:42,560
your subject is remembering two quantities, and those kind of map one-to-one to firing rates of

133
00:13:42,560 --> 00:13:49,040
neurons. You know, that's compelling from an experimentation perspective, but then you think about,

134
00:13:49,040 --> 00:13:54,000
okay, these abstract concepts like elevators and dog and cat and all these things that we're

135
00:13:54,000 --> 00:14:02,720
amazing at remembering. What do we know about how memory works at that level? Like, it's starting

136
00:14:02,720 --> 00:14:09,200
to come to call to mind, you know, ideas like theory of mind and stuff like that that are

137
00:14:09,200 --> 00:14:12,560
kind of abstract. How close are we to understanding how all that machinery works?

138
00:14:15,040 --> 00:14:20,400
Great question, to which I only have a waffly answer. Not a whole lot.

139
00:14:23,760 --> 00:14:29,360
So, unfortunately, the state of the field, well, and I shouldn't be so super pessimistic about it.

140
00:14:29,360 --> 00:14:36,800
So, let me try that answer. So, we're all trained in the physical sciences to some degree,

141
00:14:36,800 --> 00:14:42,160
and so our instinct is to clear the deck first. So, you present me with the problem like this,

142
00:14:42,160 --> 00:14:50,240
what I, my instinct is to clear the deck and produce the most simple form of this question that I

143
00:14:50,240 --> 00:14:57,680
can handle with some nice mathematical formulation. And that's what, you know, I attempted to do with,

144
00:14:57,680 --> 00:15:02,080
with talking about working memory and so forth. And I think experimentalists have a version of

145
00:15:02,080 --> 00:15:07,440
the exact same issue, right? They do, they take this very complicated animal with all of its

146
00:15:07,440 --> 00:15:12,400
inner richness and theories of mind. And then they essentially kind of strap it down and force it

147
00:15:12,400 --> 00:15:17,520
to make one little movement or another, right? So, there's a version of this that's true of both

148
00:15:17,520 --> 00:15:24,400
sides of the aisle. Right. So, there's that instinct. That instinct has the benefit that I can say

149
00:15:24,400 --> 00:15:30,640
something general now about memory, right? I can talk to you about the same or similar problem being

150
00:15:30,640 --> 00:15:36,400
solved in multiple different nervous systems. So, I can say, well, this is how a larval zebra fish

151
00:15:36,400 --> 00:15:41,120
would do this, how mouse would do this, rat would do this, macaque would do this, humans would do this.

152
00:15:41,120 --> 00:15:46,400
And then I can, as a theorist, I have the privilege to sit back and say, is there something fundamental

153
00:15:46,400 --> 00:15:51,920
that unifies all of these different models that I have built into something that starts to look

154
00:15:51,920 --> 00:15:59,200
like a theory? Is there a conserved or unified theory of memory that is true of nervous system

155
00:15:59,200 --> 00:16:05,520
independent of this details of it? That's the advantage. The disadvantage is I lose all of the

156
00:16:05,520 --> 00:16:10,560
other details that make biology interesting. So, for example, I'm not talking about any kind

157
00:16:10,560 --> 00:16:18,640
of synaptic dynamics, right? I'm not talking about neuro-modulators. I'm not talking about evolution

158
00:16:18,640 --> 00:16:24,640
even, right? The models we build start from this kind of random soupy thing and then are overtrained

159
00:16:24,640 --> 00:16:31,040
to solve this elevator problem to perfection. But that's clearly not how biology works at all.

160
00:16:31,040 --> 00:16:38,560
So, yes, it's got the pros and the cons to it. But I'm saying, like, in order to span all of those

161
00:16:38,560 --> 00:16:44,800
levels, sort of vertical and horizontal integration to produce a satisfactory answer,

162
00:16:45,600 --> 00:16:53,920
we're not even close to those big questions. You mentioned that your research kind of led to this

163
00:16:53,920 --> 00:17:04,880
idea of RNNs as the substrate for building more complex models of memory and I think maybe

164
00:17:04,880 --> 00:17:12,960
more importantly behavior. When I hear substrate, I think of, you know, platform building blocks,

165
00:17:12,960 --> 00:17:17,760
you're combining them in different ways, elaborate on that idea a bit more.

166
00:17:17,760 --> 00:17:25,360
So, when I said, so I find RNNs to be a convenient substrate to capture several different features

167
00:17:25,360 --> 00:17:31,520
of the biological brain or neural circuits that I'm interested in, right? So, one convenient

168
00:17:31,520 --> 00:17:36,160
piece of this is the fact that they have forward going and backward going connections, which are

169
00:17:36,160 --> 00:17:42,240
true of biological brains, independent of species and granularity and so forth. They are also

170
00:17:42,240 --> 00:17:47,440
mathematically simple, which helps me. But then also with reference to the previous question, I have

171
00:17:47,440 --> 00:17:54,800
let go of a lot of details. RNNs have this ongoing dynamics. They can be coarsed and trained to do

172
00:17:54,800 --> 00:18:01,520
many different behaviors. So, there's those conveniences. But I can also expand a single module

173
00:18:01,520 --> 00:18:06,000
into multiple regions, right? I can pretend. So, you know, when you look at, you know,

174
00:18:07,040 --> 00:18:13,840
the complex nervous systems like mammalian nervous systems, mouse and humans and macoxins

175
00:18:13,840 --> 00:18:20,320
so forth. There are anatomical partitions and there are functional partitions of neural circuits

176
00:18:20,320 --> 00:18:27,200
into sometimes brain areas, right? But how those brain areas are defined, you can abstract that idea

177
00:18:27,200 --> 00:18:33,840
into each area as an RNN. You can say I have an RNN that makes like, you know, a region that

178
00:18:33,840 --> 00:18:39,680
senses stress. I have another RNN is a region that says, okay, I've had this much stress and I'm

179
00:18:39,680 --> 00:18:45,520
going to shut down the system now. And those two in conjunction, we'll start to get at questions

180
00:18:45,520 --> 00:18:51,120
in the brain like how do two regions interact, sometimes compete and sometimes cooperate. So,

181
00:18:51,120 --> 00:18:59,840
you've suddenly gone from a circuit level model of one RNN as the brain to multi region RNNs,

182
00:18:59,840 --> 00:19:06,640
which have now gone to a multi circuit level. Now, you train that multi circuit RNN to do a behavior

183
00:19:06,640 --> 00:19:10,800
and pretty soon you've spanned three different levels. You've got the individual circuits,

184
00:19:10,800 --> 00:19:18,640
multi area circuits to the whole organism. Now, expecting this model to also capture details

185
00:19:18,640 --> 00:19:24,160
of the molecular machinery, maybe beyond us at the moment. It's a little bit of a kitchen sink

186
00:19:24,160 --> 00:19:31,200
flavor of modeling, but you know, this is what you can do with them. You can treat the RNN as its

187
00:19:31,200 --> 00:19:38,080
own module and explore its features, or you can treat it as a building block on which to build

188
00:19:38,080 --> 00:19:43,360
other functionality. Like, you can take the RNN and train it to do many different tasks.

189
00:19:43,920 --> 00:19:48,640
You can take the RNN and hook more of them together to build multi region RNNs.

190
00:19:49,680 --> 00:19:54,560
And so, that's the sense in which it's a versatile and convenient substrate.

191
00:19:54,560 --> 00:20:01,680
I got it. And when you talk about training RNNs to do different tasks in this context,

192
00:20:01,680 --> 00:20:08,400
what are some of the types of tasks that you're training these RNNs to do, these networks to do,

193
00:20:09,040 --> 00:20:14,160
and what are, well, what's the training procedure look like? Are you,

194
00:20:15,280 --> 00:20:21,840
what's the data look like? Are you training them into N versus subsystem? Tell us a little bit more

195
00:20:21,840 --> 00:20:29,040
about how you're using the RNNs. So, let me do this question sort of more specifically,

196
00:20:29,040 --> 00:20:34,560
right? So, one of the things that we're working on in the lab is this project called curriculum

197
00:20:34,560 --> 00:20:41,280
learning. And so, this came about because we were frustrated by the limitations of current

198
00:20:41,280 --> 00:20:46,560
training algorithms. And, you know, we didn't invent this machine learning and AI engineers

199
00:20:46,560 --> 00:20:52,240
innovated this amazing concept. It was originally even inspired by psychophysics where, you know,

200
00:20:52,240 --> 00:20:57,680
when you train an experimental animal or watch a child learn by imitation or something,

201
00:20:57,680 --> 00:21:02,080
there's a shaping to this behavior. The behavior is shaped by means of reinforcement.

202
00:21:02,080 --> 00:21:06,640
And that then led to the idea of using curriculum learning, like different syllabi,

203
00:21:07,280 --> 00:21:13,920
where the task you want a system to learn increases in complexity slowly. And so,

204
00:21:13,920 --> 00:21:19,200
you can find that you can train networks to do things better. Now, in the work that I had done

205
00:21:19,200 --> 00:21:26,640
before we came upon this, we used to use standard training algorithms, which didn't resemble biology

206
00:21:26,640 --> 00:21:31,520
and didn't do that well anyway. So, there's two flavors of the types of things.

207
00:21:33,040 --> 00:21:38,080
One, involved training networks to do tasks using something like that propagation.

208
00:21:38,080 --> 00:21:43,680
So, you can, you know, turns out that if you scale up network sizes, you can get these networks

209
00:21:43,680 --> 00:21:49,760
to do a lot of things. However, when tasks get complicated or over long periods of time,

210
00:21:49,760 --> 00:21:57,040
back propagation wasn't doing so hot. So, then we do the other flavor of work where we train

211
00:21:57,040 --> 00:22:03,920
the individual units inside the RNN to match experimental data collected from individual neurons

212
00:22:03,920 --> 00:22:09,760
in the biological system. That target sort of a different problem, but they're both wildly

213
00:22:09,760 --> 00:22:14,400
unrealistic. They don't even smell like something the biological system may have,

214
00:22:15,040 --> 00:22:20,320
may have used to get the animal into the state where it can perform these things.

215
00:22:20,960 --> 00:22:26,960
Meaning, we know that we don't learn on a module by module basis and we learn more holistically.

216
00:22:26,960 --> 00:22:32,160
Right. Exactly. Right. So, we learn by doing a simple version of the task first.

217
00:22:32,160 --> 00:22:36,560
Then we stack on a slightly more complicated thing if the first thing paid off.

218
00:22:36,560 --> 00:22:41,600
If the, if the escalation step in task complexity is kind of, you know, very mini-school,

219
00:22:41,600 --> 00:22:48,480
then you have the option of testing out. So, we thought what we would do is to train RNNs using

220
00:22:48,480 --> 00:22:54,400
curricula. So, you know, my team and I are designing different curricula. So, for example,

221
00:22:54,400 --> 00:23:00,000
one of the tasks, let's go back to our famous elevator problem, right? Our elevator problem has

222
00:23:00,000 --> 00:23:06,480
features of navigation because you have to walk from your apartment or your office to the elevator.

223
00:23:06,480 --> 00:23:11,840
It has, it has indications of working memory because you want, you know, when the doors open,

224
00:23:11,840 --> 00:23:17,280
you want to be able to estimate which side has more people or fewer people. And then you want to

225
00:23:17,280 --> 00:23:22,240
hold that in memory while you make a decision to make a turn and go into the elevator that has,

226
00:23:22,240 --> 00:23:28,320
let's say, few people. So, it's almost like a two alternative choice feeling, but it also has

227
00:23:28,320 --> 00:23:33,360
evidence accumulation. So, what we're trying to do is instead of taking a network and saying,

228
00:23:33,360 --> 00:23:40,080
okay, here's the full blown elevator problem, solve it. What we're doing is training the network,

229
00:23:40,080 --> 00:23:46,000
as though it were a person being trained. So, we can have multiple different curricula, right? One

230
00:23:46,000 --> 00:23:51,840
in which, you know, first both elevators are blank, right? Which elevator bank you choose,

231
00:23:51,840 --> 00:23:57,040
you get a reward. So, that's the first step. The second one is you put one person in the elevator.

232
00:23:57,040 --> 00:24:02,240
Once, let's say the network has mastered this, then you put one person in the elevator.

233
00:24:02,240 --> 00:24:07,360
Then if the, if the, if the network goes to the side that has no people, you give it a reward.

234
00:24:08,080 --> 00:24:12,960
Then you escalate the slowly and slowly and then it starts to get at things like, let's say there

235
00:24:12,960 --> 00:24:18,720
were a thousand people in one elevator and a hundred in the other. Thus, this network really need to

236
00:24:18,720 --> 00:24:23,120
count one through thousand in one elevator and one through hundred in the other.

237
00:24:23,120 --> 00:24:29,920
Brookforce will suggest yes. But what turned, what, what we found is that if you train networks

238
00:24:29,920 --> 00:24:35,040
using a curriculum, not only can it do much more complicated tasks than the network,

239
00:24:35,600 --> 00:24:41,200
than the naive version of the training, it can also do many more of these elevators simultaneously.

240
00:24:41,760 --> 00:24:48,000
So, network starts to intuit something like, feels like. If you're standing in front of the elevator

241
00:24:48,000 --> 00:24:53,680
and one has a thousand people and one has a hundred, you'll kind of feel like the one that has a

242
00:24:53,680 --> 00:24:57,520
hundred is easier to get to. We don't know what feels like looks like in the brain.

243
00:24:58,560 --> 00:25:03,760
But I can tell you because we now can build these networks that have gone through the same

244
00:25:03,760 --> 00:25:11,840
or at least qualitatively similar shaping. So, I can say you've got a record from region X

245
00:25:11,840 --> 00:25:19,120
where the signal should look like this particular signal to indicate what feels like, feels like.

246
00:25:20,160 --> 00:25:26,400
So, these are, you know, this powerful class of models that now be now building

247
00:25:26,400 --> 00:25:30,000
and using for all kinds of biological mechanisms.

248
00:25:31,440 --> 00:25:37,360
Now, when I hear you describe this particular task and how the network approaches it

249
00:25:37,360 --> 00:25:43,120
under this curriculum learning regime, like I think about the, you know, that ultimate task of,

250
00:25:44,640 --> 00:25:53,440
you know, counting or deciding based on the number of people in the elevator, which one to choose

251
00:25:53,440 --> 00:26:00,400
as being, I'm imagining it to be very different from the penultimate step, right? It's not like

252
00:26:00,400 --> 00:26:07,440
that, I'm imagining that that step immediately before was not, you know, about the same problem,

253
00:26:07,440 --> 00:26:14,720
the number of, the number of inhabitants or people in the elevator, which is

254
00:26:16,000 --> 00:26:25,600
suggesting to me that the, you know, the, this feel like is maybe akin to like the, you know,

255
00:26:25,600 --> 00:26:33,920
the deep levels in a, you know, CNN where you're like training structure in some way and that

256
00:26:33,920 --> 00:26:40,960
structure is useful to the ultimate problem solving task. I'm trying to think through and want to

257
00:26:40,960 --> 00:26:48,000
talk through the relationship between that kind of low level structure and, you know, feels and

258
00:26:48,000 --> 00:26:53,760
like how those things connect to one another. So, you've again, like, it's an incredible question

259
00:26:53,760 --> 00:26:59,040
to ask. So, really, you're right, like the penultimate step before this problem is solved is not

260
00:26:59,040 --> 00:27:08,480
999 people and 199. So, really what you're asking is what makes a good curriculum and what makes

261
00:27:08,480 --> 00:27:15,600
a bad curriculum. So, given that I want to teach this, you know, animal that I've just adopted to

262
00:27:15,600 --> 00:27:20,160
get into the elevator unassisted, for example, then what, what would make a good curriculum?

263
00:27:20,160 --> 00:27:26,880
The argument we're making is just by watching two completely trained, let's say dogs,

264
00:27:26,880 --> 00:27:32,960
because you know, dog person, two completely trained dogs getting into the elevator will not be

265
00:27:32,960 --> 00:27:40,640
able to tell you the roots that they took to get there. So, we don't have all of the answers to

266
00:27:40,640 --> 00:27:48,960
this problem, but we know that by testing different curricula all of which end in the dog being

267
00:27:48,960 --> 00:27:56,880
able to solve the elevator problem, we are able to discern how the biological circuitry's inside

268
00:27:56,880 --> 00:28:05,600
structure would have been shaped in this case, fun and tender. But it's the analogy that you set

269
00:28:05,600 --> 00:28:11,360
up with the structure of the weights or the weight space within a CNN passing through a training

270
00:28:11,360 --> 00:28:17,040
protocol is exactly what we're looking at. So, we're looking at, you know, in epoch by epoch or

271
00:28:17,040 --> 00:28:22,720
asked the task complexity increases, what happens to this weight matrix? Are you learning something

272
00:28:22,720 --> 00:28:29,200
that looks, are you solving the navigation problem first, the working memory problem next,

273
00:28:29,200 --> 00:28:33,840
and then the counting problem last? Right. In fact, the structure you see evolving,

274
00:28:33,840 --> 00:28:39,040
or do you see the counting emerging first? Because counting is kind of like a lookup table,

275
00:28:39,840 --> 00:28:45,120
and then, you know, doing the navigation, then doing the working memory piece, and the answer I'm

276
00:28:45,120 --> 00:28:52,240
going to give you is there isn't a one step. The simpler a task is, one might argue that this

277
00:28:52,240 --> 00:28:57,280
counting task is simple compared to the kinds of tasks that humans and macaques can do,

278
00:28:58,400 --> 00:29:03,840
the number of roots by which you can learn, and therefore the number of curricula that can still

279
00:29:03,840 --> 00:29:11,920
approach the final state are big, are many. So, the more complex something is, the smaller the

280
00:29:11,920 --> 00:29:19,360
solution space appears to be. Therefore, if there are multiple curricula or multiple different

281
00:29:19,360 --> 00:29:26,480
learning trajectories that can approach that solution space, then, you know, then they will,

282
00:29:26,480 --> 00:29:34,320
then the solutions will also look kind of similar. So, it kind of depends on the task that you wanted

283
00:29:34,320 --> 00:29:41,120
to solve, the roots that the structure will take to approach it. But without putting networks

284
00:29:41,120 --> 00:29:46,000
through something like this, you wouldn't have the foggiest idea, right? Like if two dogs walked

285
00:29:46,000 --> 00:29:53,520
into a bar and say we can both juggle, our current go-to involves recording from their brains and

286
00:29:53,520 --> 00:29:59,120
looking at squiggles in state space. But you won't be able to tell if the first dog learned and

287
00:29:59,120 --> 00:30:05,280
Boston, the second dog learned in New York and how. They were successful. But by following their

288
00:30:05,280 --> 00:30:10,720
trajectories, learning trajectories, or even mimicking their learning trajectories in

289
00:30:10,720 --> 00:30:20,560
network models, we might be able to make progress. And so in this work are the curricula that you're

290
00:30:20,560 --> 00:30:30,080
presenting these networks with are those explicitly designed by you or are the the curricula

291
00:30:30,080 --> 00:30:35,200
themselves learned? Or, you know, can you look, do you look at like a curricula, like a hyper

292
00:30:35,200 --> 00:30:41,680
parameter, and you're, you know, maybe doing some automated search across them?

293
00:30:41,680 --> 00:30:46,400
Oh, see, that's a very good, so I feel like now I'm going to open my notebook and take notes.

294
00:30:47,440 --> 00:30:52,720
Right now we're limited by sadly my imagination. So we're hand designing this curriculum.

295
00:30:52,720 --> 00:30:58,480
Okay. Because our first goal, so what we discovered when we first did this was, yes, we can tell

296
00:30:58,480 --> 00:31:03,520
those two dogs apart that walked into a bar and can juggle identical. So that was the big win.

297
00:31:03,520 --> 00:31:08,640
Then we said, let's look in literature. We have this vast network of experiments.

298
00:31:08,640 --> 00:31:14,800
And by just to put a point on that, by telling these two dogs apart, what you mean more

299
00:31:14,800 --> 00:31:20,800
concretely is you've built these models, you've trained them using these different curricula to be

300
00:31:20,800 --> 00:31:27,120
successful at the same end task. And you can distinguish between kind of the things that they've

301
00:31:27,120 --> 00:31:33,280
learned. That strategies that they would use, yeah, did one learn by means of reward and the other by

302
00:31:33,280 --> 00:31:38,480
means of imitation. Those would be sort of two different types of learning that these animals can

303
00:31:38,480 --> 00:31:43,600
experience and still get to mastery. Right. Okay. But doing curriculum learning, I'll be able to

304
00:31:43,600 --> 00:31:49,200
distinguish those two because different learning rules will benefit to different degrees to different

305
00:31:49,200 --> 00:31:57,840
curricula. Yeah. So the first observation was this different learning principles will benefit to

306
00:31:57,840 --> 00:32:03,520
different degrees to the choice of the curriculum. So you can use that curriculum learning as a tool

307
00:32:04,240 --> 00:32:09,920
to disambiguate learning principles in the biological brain. So then we went to look in literature

308
00:32:09,920 --> 00:32:15,440
to say, well, have people looked at this type of, you know, have people systematically put their

309
00:32:15,440 --> 00:32:21,520
animals through shaping protocols and can they share these types of data with us? We found that

310
00:32:21,520 --> 00:32:29,760
people rarely system systematically collect curate or publish curriculum learning type data.

311
00:32:29,760 --> 00:32:33,840
Right. Because different labs have different shaping protocols that they put their experimental

312
00:32:33,840 --> 00:32:39,200
animals through. A lot of it is trial and error like the senior post dog designed the shaping

313
00:32:39,200 --> 00:32:43,760
protocol that happened to work for the mouse. So we're all going to do it. There's also tricky

314
00:32:43,760 --> 00:32:49,120
features because as I had mentioned, biology is gnarly. So a lot of the actual shaping involves

315
00:32:49,120 --> 00:32:54,400
just getting the animal to behave while being handled. So there's all these complications that

316
00:32:54,400 --> 00:33:00,240
were not interested in modeling with our models. What we wanted to do with the first paper

317
00:33:01,040 --> 00:33:06,400
is to say, well, we're just going to say curriculum learning as a tool to disambiguate learning

318
00:33:06,400 --> 00:33:14,240
principles. Inspire experiment lists to much more carefully collect curate and then hopefully share

319
00:33:14,240 --> 00:33:20,960
with us details of these experiments, collect, you know, what syllabus worked, what syllabus did

320
00:33:20,960 --> 00:33:29,040
not work. I mean, just because 75% of the animals learned and 25 failed to learn the task you were

321
00:33:29,040 --> 00:33:33,600
trying to get them to learn, the start mean that they've, you know, failed. That means that the

322
00:33:33,600 --> 00:33:39,360
curriculum could have been tweaked in a better way. So that's the first goal. The next goal is

323
00:33:39,360 --> 00:33:45,360
certainly to go in the directions that you have suggested, which is to say, well, can we say something

324
00:33:45,360 --> 00:33:52,800
more general about which types of tasks will respond to which types of curriculum? And then can we

325
00:33:52,800 --> 00:33:59,200
auto engineer those? Can we train networks to follow some kind of low energy path through the

326
00:33:59,200 --> 00:34:08,080
space of possible curriculum? That would be a home run. Yeah, I'm not sure if there's a question

327
00:34:08,080 --> 00:34:16,800
at the end of this, but it strikes me that the way you're approaching the problem starts to

328
00:34:16,800 --> 00:34:23,760
poke at issues of like nature versus nurture and like, how can we capture what nurture really means

329
00:34:23,760 --> 00:34:30,160
and that kind of thing? Is that something that you think about? I do, but not in a terribly coherent

330
00:34:30,160 --> 00:34:38,640
way. Me neither. Yeah, no, you know, this is not really my real house at all. I mean, my knee

331
00:34:38,640 --> 00:34:44,000
jerk has to say, yes, of course, they're both important. Yeah. I think for me personally, the key

332
00:34:44,000 --> 00:34:51,120
is not to take my, my tools and my models so seriously that I conflate them with reality. Yeah.

333
00:34:51,120 --> 00:34:57,200
So, you know, for me, the win is not that I can write the, even though I was trained in that

334
00:34:57,200 --> 00:35:02,160
school of thought, is to write down the most elegant, simple mathematical solution and so forth.

335
00:35:02,160 --> 00:35:07,200
Yes, there's a, there's a little kick to it. The better kick is if I make this prediction about

336
00:35:07,200 --> 00:35:13,200
the juggling dogs and one of my experimental collaborators said, oh, yeah, now this really worked,

337
00:35:13,200 --> 00:35:21,680
or this really didn't work. That's the win. If is, if it is validated or falsified by biological data,

338
00:35:21,680 --> 00:35:27,600
because that's where reality is, is the, is the biological system. Yeah. Now, if I were an engineer

339
00:35:27,600 --> 00:35:33,440
still, or a male practitioner or died in the world, I would be like, well, what is the smallest

340
00:35:33,440 --> 00:35:41,120
network I can build that can do the most things? I'm not. So, I'm interested in with all the

341
00:35:41,120 --> 00:35:48,800
works and wrinkles of biology. How can, how, what in the brain is tracking, you know, which

342
00:35:48,800 --> 00:35:54,080
task is currently happening and when it's time to switch, right? I'm having this conversation

343
00:35:54,080 --> 00:35:58,480
with you. I'm watching the time, you know, you're doing the same thing. And so, we're like,

344
00:35:58,480 --> 00:36:04,000
somebody in the brain is tracking this in the biological brain. And we're not terribly good at

345
00:36:04,000 --> 00:36:08,640
either, right? As you can see from some of my rambly answers, the time keeping and answering

346
00:36:08,640 --> 00:36:15,520
coherently. So, with all these wrinkles, I want to understand the functioning of the biological brain.

347
00:36:16,720 --> 00:36:21,920
I am aware that the tools aren't perfect. They weren't evolved. And this gets to your question.

348
00:36:21,920 --> 00:36:26,960
They didn't go through a process of evolution. They're missing a whole lot of biological

349
00:36:26,960 --> 00:36:32,240
details. So, you know, the, I call them units. I don't even call them neurons within the network.

350
00:36:32,240 --> 00:36:39,280
And so, I think it's another one of those questions where we have to say, well, can I tackle a

351
00:36:39,280 --> 00:36:46,400
sliver of this problem? And a sliver of that problem is now we're able to train these networks

352
00:36:46,400 --> 00:36:52,320
over very long periods of time. As I explained with the curriculum learning, it takes weeks to train

353
00:36:52,320 --> 00:36:58,080
an experimental animal to do one of these tasks that they do in the lab. So, can I play that game

354
00:36:58,080 --> 00:37:05,680
further over a developmental trajectory? I don't have terribly clever ideas for that because

355
00:37:05,680 --> 00:37:11,360
the hardware and the software are changing constantly in such a problem, but it's one that I'm excited

356
00:37:11,360 --> 00:37:17,360
to dive into in the next phase of my career. Yeah, yeah. I particularly appreciate the,

357
00:37:18,160 --> 00:37:24,720
you calling out the distinction between developing a tool that may have some predictive value,

358
00:37:24,720 --> 00:37:31,520
you know, versus kind of getting caught up in, you know, your tool and taking it to be

359
00:37:31,520 --> 00:37:37,200
representative of the actual thing itself, you know, you mentioned closed form kind of equations

360
00:37:37,200 --> 00:37:43,760
around this, you know, simplified model that you make. I find it, you know, in conversations

361
00:37:43,760 --> 00:37:50,560
around the biology in particular, I often, like, I want to go there. It's like, I need to like,

362
00:37:50,560 --> 00:37:57,440
really reign myself in and make the distinction between the model and the thing being modeled.

363
00:37:58,320 --> 00:38:02,720
Yeah, no, I appreciate you saying that you put it extremely eloquently. I think anyone that's

364
00:38:02,720 --> 00:38:07,280
trained in the physical sciences has that tension. Because you want to write me, you want to write

365
00:38:07,280 --> 00:38:12,480
down the formula and be done with it. But I think to understand, I mean, I remember this,

366
00:38:12,480 --> 00:38:17,760
like feeling of shock as I walked into an experiment lab as a postdoc, as a legitimate

367
00:38:17,760 --> 00:38:24,240
theorist, right? And I go, oh my god, nothing is as I thought it would be at all. I mean,

368
00:38:24,240 --> 00:38:31,120
what even is going on? Yeah. And you realize, okay, you're in a whole other game now, biology is

369
00:38:31,120 --> 00:38:36,640
not. It's going to, it, I, my prediction is we're not going to have anything that looks like a

370
00:38:36,640 --> 00:38:42,720
grand unified theory of name of brain function. I think we're going to have a pile of models

371
00:38:42,720 --> 00:38:50,320
and then a holistic understanding will emerge from such a thing. That's so disappointing. I've

372
00:38:51,200 --> 00:39:00,240
spoken and written about how I'm a sucker for grand unified theories. I want to kind of go back

373
00:39:00,240 --> 00:39:09,200
to a point that you made earlier. The relationship or distinction between,

374
00:39:09,200 --> 00:39:17,280
I, there's a lot of ways we could come at it. But, you know, training these kind of complex

375
00:39:17,280 --> 00:39:25,440
hierarchical systems of RNNs module, you know, modular, module by module or unit by unit versus

376
00:39:27,040 --> 00:39:37,680
kind of end to end. And I asked about that unit by unit versus end to end or in isolation versus

377
00:39:37,680 --> 00:39:44,240
end to end. And you responded with, you train them in curricula. But I don't, to me, those don't

378
00:39:45,200 --> 00:39:51,840
come as like orthogonal things. You could train, you could use curricula, you know, the idea of

379
00:39:51,840 --> 00:40:00,400
curricula learning, but unit by unit as opposed to end to end. Is that tell us a little bit more about,

380
00:40:00,400 --> 00:40:03,840
you know, which of those or do you experiment with all of those or like,

381
00:40:03,840 --> 00:40:09,440
I think it depends on the problem. So the answer is closer to experiment with

382
00:40:10,400 --> 00:40:15,440
So one of the key distinctions I need to draw here is that the word learning is used

383
00:40:16,160 --> 00:40:21,200
by multiple different communities to mean different things, right? When, when experimental

384
00:40:21,200 --> 00:40:26,880
neuroscientists, biologists talk about, talk about learning, they're talking about the process,

385
00:40:26,880 --> 00:40:32,080
the animal has experience, it's all experiential state. When you and I talk about learning,

386
00:40:32,080 --> 00:40:37,040
we're talking about a training algorithm really. So in some sense, it's kind of the details of

387
00:40:37,040 --> 00:40:44,160
the problem. And the answer is, I'm getting these networks into a state where once I stop

388
00:40:44,160 --> 00:40:50,000
training them, they can autonomously produce the relevant dynamics that manifest in the time

389
00:40:50,000 --> 00:40:56,160
varying behavior consistent with biology. And so it kind of depends. Sometimes I do them

390
00:40:56,160 --> 00:41:01,360
unit by unit, sometimes you would have to train the units activations along with the behavior,

391
00:41:01,360 --> 00:41:04,880
or sometimes you want to abandon all of them and put them through a curriculum.

392
00:41:05,520 --> 00:41:10,000
But within the curriculum, you can again play with both of those. So it sort of depends on the

393
00:41:10,000 --> 00:41:17,520
problem that one is tackling. And in my case, the problem isn't sort of cooked up in vacuum.

394
00:41:18,080 --> 00:41:24,640
It is also inspired by something weird in experiments. So, you know, an experimental observation

395
00:41:24,640 --> 00:41:30,000
that I've read about or one of my collaborators tells me about, that's how usually a problem gets

396
00:41:30,000 --> 00:41:35,760
sparked. And I'm like, oh, I wonder if I can get networks to do something like that.

397
00:41:36,960 --> 00:41:43,120
And what's an example of that? So an example of that would be something that looks like sequences,

398
00:41:43,120 --> 00:41:48,720
right? This was one of my earliest, I would say, you know, discoveries if I may be so bold

399
00:41:48,720 --> 00:41:54,880
as to use that term. People were seeing sequences everywhere. They see sequences. By sequences,

400
00:41:54,880 --> 00:42:00,720
I mean, you know, time varying patterns of activity every neuron is only firing a small-ish bump.

401
00:42:00,720 --> 00:42:06,000
But over the population, it looks like a wave going through. People see that in the hippocampus.

402
00:42:06,000 --> 00:42:10,240
They see that in the prefrontal cortex. They see that in the striate. And they see this

403
00:42:10,240 --> 00:42:16,960
in anatomically wildly different areas. And they see them all over the place. They see them during

404
00:42:16,960 --> 00:42:21,680
working memory. They see them during, you know, place fields and place cells. They see them during

405
00:42:21,680 --> 00:42:27,680
navigation, learning, remembering any function. There's a ubiquitous nest to this, the fact that

406
00:42:27,680 --> 00:42:33,280
people see sequences. And that led me to wonder, well, what if it's an accident? Like we can't stop

407
00:42:33,280 --> 00:42:37,840
sorting things. Is that why we're seeing sequences? That's how the problem first started.

408
00:42:38,880 --> 00:42:46,000
Then we came up with this theory of they're doing a fundamental function, which is remembering

409
00:42:46,000 --> 00:42:52,240
how long a task is occurring and when it's time to switch. So I'm having this conversation with

410
00:42:52,240 --> 00:42:58,320
you. I'm also thinking about time, let's say, right? Something in the brain has to have steady

411
00:42:58,320 --> 00:43:04,160
representation during the performance of this answer. It has to switch when it's time to ask the

412
00:43:04,160 --> 00:43:10,000
next question or answer the next question. Something that has that feature, independent of the underlying

413
00:43:10,000 --> 00:43:14,960
anatomy, is sequences. Because they're steady. They're the way it goes through, but it has a start

414
00:43:14,960 --> 00:43:20,480
time and an end time. Now, of course, if I zorch the neurons that are making the sequence,

415
00:43:21,040 --> 00:43:26,800
I'm not able to do this task properly, right? I keep sipping my cup of coffee and put it down

416
00:43:26,800 --> 00:43:34,720
repeatedly. But why am failing? Maybe different from what we think, right? Maybe we're failing because

417
00:43:34,720 --> 00:43:40,960
the state tracker is broken. And so we're now trying to get this into a framework we're starting

418
00:43:40,960 --> 00:43:46,880
to talk about things like addictions and obsessive problems that what if we're looking at the problem

419
00:43:46,880 --> 00:43:53,200
sort of much more narrowly than we need to? What if these disorders share the feature that,

420
00:43:53,200 --> 00:43:57,600
you know, if I take a sip of a glass of wine and put it down, I know the action is finished.

421
00:43:58,560 --> 00:44:03,440
Because once I put the glass down, well, what if I didn't? Or there's something that is tracking

422
00:44:03,440 --> 00:44:09,120
that I've taken a sip and put it down is broken? Then I keep doing it obsessively for length of time.

423
00:44:09,120 --> 00:44:16,400
And so that's an example of of a weird biological feature, repetitive action until something is

424
00:44:16,400 --> 00:44:23,520
finished or inability to stop performing an action that could be cast as a as a fundamental

425
00:44:23,520 --> 00:44:29,840
property of neural circuits and therefore neural circuit dysfunction. And that was this observation

426
00:44:29,840 --> 00:44:37,040
that we're seeing sequences everywhere. GA wonder why? Yeah, and I was going to ask about the

427
00:44:37,040 --> 00:44:46,560
kind of the end goal of the research you identify this, you know, quirk, let's say, an experimental

428
00:44:46,560 --> 00:44:54,480
result. You try to create a model for it. So now you have this tool and it sounds like at least one

429
00:44:54,480 --> 00:45:02,320
of the objectives here is that the tool can then be a launching point for deeper inquiry.

430
00:45:02,320 --> 00:45:08,960
Is that how you see the work playing out? That's right. So one we can better leverage existing data,

431
00:45:08,960 --> 00:45:13,680
right? I mean, we can say like people should share data with us because they're collecting it

432
00:45:13,680 --> 00:45:19,120
anyway by process of training these animals. You better leverage existing data. You can use

433
00:45:19,120 --> 00:45:26,720
these models as essentially a bottomless pit of hypotheses. So you can do experiments on them

434
00:45:26,720 --> 00:45:31,440
that you wouldn't be able to either ethically or technologically or financially do in the real

435
00:45:31,440 --> 00:45:39,600
system. So you can in some sense fail faster if you tried all those ideas on these substrate models.

436
00:45:39,600 --> 00:45:46,640
Yeah. And so yes, they are essentially tools for deeper inquiry. At the very, very least, they

437
00:45:46,640 --> 00:45:52,640
generate predictions for the next experiment, which can then, you know, validate or falsify.

438
00:45:52,640 --> 00:45:57,920
And then I go back to the drawing board, refine the model. So all of the collaborations that I have

439
00:45:57,920 --> 00:46:05,760
involved is kind of, you know, intimate recursive back and forth. And is that is the use of the tool

440
00:46:05,760 --> 00:46:12,560
necessary necessarily? I'm envisioning like a simulation type of an approach to use the tools.

441
00:46:12,560 --> 00:46:16,960
Is that the way it tends to work or is it something else? That's right. That's exactly right.

442
00:46:17,520 --> 00:46:23,120
So these models are mathematical models that are then simulated in essentially computer programs,

443
00:46:23,120 --> 00:46:27,840
right? And so we can do the manipulations in the program that you wouldn't be able to

444
00:46:27,840 --> 00:46:34,560
necessarily do. Use them to extract features from data that are inaccessible from just measurements

445
00:46:34,560 --> 00:46:40,320
alone. Yeah. So that kind of inquiry these models lend themselves to.

446
00:46:41,040 --> 00:46:46,720
Great. Great. Well, Connaca, thank you so much for sharing a bit about what you're working on with

447
00:46:46,720 --> 00:46:52,400
us. Very fascinating. And looking forward to keeping in touch and learning more.

448
00:46:52,400 --> 00:46:54,400
Thank you so much for having me.

