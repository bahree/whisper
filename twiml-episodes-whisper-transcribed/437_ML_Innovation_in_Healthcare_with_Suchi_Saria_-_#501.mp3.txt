All right, everyone. I am here with Suci Saria. Suci is the founder and CEO of Bayesian Health,
the John C. Malone Associate Professor of Computer Science, Statistics and Health Policy,
and the Director of the Machine Learning and Healthcare Lab at the Johns Hopkins University. Suci,
welcome to the Tuomo AI Podcast. Thanks, Sam. The long affiliation that made me very nervous.
It was quite a mouthful, but I am super excited to have you here on the show. This is an
interview that I've been looking forward to for a very long time. I think I remember seeing one of
your very early presentations on machine learning for sepsis, and this was how long ago was that work?
I've been working on it for over six years now, so I don't know when which presentation you saw,
but yeah, it's been a while. That's awesome. The podcast has been going strong for five,
so it was probably early days for for both of us. So nonetheless, I'm excited to have you here
on the show and would love to start out by having you introduce yourself to our audience,
share a bit about your story and kind of give us a sense for how you came to work at this convergence
of machine learning, AI, healthcare, medicine, all these great things.
So I grew up in India, in like a tiny little town in India, and it just so happens, you know,
India is a very nerdy place. It's people are totally encouraged to be engineers and computer science
nerds at a young age, and I got into computer science very early and actually got fascinated by AI
as a field, and just really got lucky and trained at a very young age with people who are luminaries
in the field, which means got tons of opportunities that were uncharacteristic for someone my age and
background. And in terms of for me, actually around 12 years ago, 2006, 2007, eight around then,
I was kind of going through an early midlife crisis where I realized a lot of the kinds of ideas
we were exploring in AI and machine learning, the applications at the time were advertising,
or like, you know, personalization on a phone or personalization on a desktop, you know,
email foldering. And what that made me think about was, you know, like, is that I wanted to sort of
do something with more social immediate social impact. And that meant I considered everything,
and around that time also got introduced to colleagues at Stanford who were physicians. So these were,
like physicians who took care of premature babies, these babies at risk of major complications.
And if you can turn out, because they're very, very tiny premature,
treating them in a timely way is very important for being able to impact the health outcomes.
Like, they're much more at risk for declining deteriorating, had poor having poor neurodevelopmental
outcomes, and not surviving if you don't catch them in a timely way. So that was sort of my first
introduction, actually, to moving from just sort of understanding and studying methodological problems
in machine learning and AI, broadly applied to, like, hard messy time series data sets to thinking
harder about real world applications, but he really could make an impact. And, you know, that
and healthcare is just really hard. I didn't realize how hard it is and what I was getting into.
But I really didn't. It's got me so many sleepless nights.
But yeah, but, you know, it sort of also made me realize, like,
wholly, majorly, like, there's so much opportunity. But, you know, it's going to require the
right types of efforts to make progress. And that's how I got started and kept going down that path
and actually have considered the whole nine gamut from, like, advising companies to
the previous seeding particle research companies to now spinning out this company out of Hopkins.
But to obviously being a professor, faculty, you know, innovating on research. And all throughout
sort of with one singular focus, which is healthcare is moving from, you know, in 2009, there was the
particularly big event that happened, which was the passage of the high tech act. That made it so
that systems were going from no data to data, data were now going to be stored electronically at
scale across hospitals and clinics around the country, which means it was entering this era,
like almost like a 1999 where the web came. So in medicine, electronic data is coming,
electronic infrastructure is coming. And since 2009, in the last 10 years, there's been widespread
adoption because of policy changes off this electronic infrastructure. But the use of this data
is still extremely limited in healthcare delivery. Like today, the way physicians practice is still the
way, you know, practice occurred 50 years ago, 100 years ago, 200 years ago. And so the singular focus
being there are so many opportunities for improving the quality of care if we could use data in a
more intelligent way correctly using the right type of AI. And so how do we make that happen?
Nice. And you kind of jumped directly to your recent history. But I noted that you earlier in
your career, you interned with Eric Corvitz, who was a recent guest on the show and you did your
PhD with Daphne Kohler, who's been on the show. You've had some amazing opportunities.
Yeah, it's true. Actually, I have a funny story about that. So I didn't actually want to be
faculty at all. My thought was, I'm going to go into industry and, you know, like I love the pace
at which industry moves. And I was doing a lot of work. You know, the work we did in new units,
we were able to show by using machine, like approaching this data from a new lens, you really
could actually predict outcomes and these little babies, which babies at risk for complications,
much earlier than physicians were recognizing them. So at the time, I remember there was opportunity
to start a company to build improved new natal care. And Eric's been a mentor of mine for a while
and Eric met me at Nurebs, where he's like, why aren't you becoming faculty sushi? And I sat
then I was like, I don't know, things that academia move at just a pace that feels a tad bit too
slow. And we had this sort of soul-searching conversation for like half an hour where
that got me to like reconsider where I was like, you know, what we're doing is very foundational.
I think this was back in 2011. Very, very foundational, the kind of work I was doing back then.
And realizing like, we're very early in our use of, we need novel methodological developments
that was really going to unleash this kind of data. Our technology wasn't ready yet at the time,
which meant it really needed to be in a very deep research environment, being at a place like
Hopkins, right, which is in a way the mech-off healthcare. Like, you get to sit next to people who are
some of the leading policymakers who study, you know, guideline policy change and got treatments.
And like, so in some sense, it felt like in order to be able to bring about any kind of change,
coming to the center of the activity and trying to bring change from within could be really
productive. And so yeah, so that was really exciting and really enjoyed, you know,
Daffney and Triedar have had a number of really amazing people who influenced me from a very,
very young age. That's awesome. You're, often when I'm talking to folks that are applying
ML in the medicine and healthcare field, there's like, I guess the point I'm getting at is like,
there's a distinction. You know, there's a set of folks that kind of think of it from a policy
perspective and healthcare. And there's a set of folks that think about it from a medicine
perspective. Your work seems to span the two. Is that true? That's right. So I think the way
to think about this is almost everything starts with a discovery, right? You want to first figure
out, you know, what is something? Where is there opportunity for change? And what would you do
differently? So in other words, are you inventing a new software based? So often in machine learning,
the kinds of interventions we'd be looking at is like new software based tools for being able to
do diagnosis more correctly. New software based tools for the moving, doing early detection of
adverse events, new software based tools for targeting drugs more precisely. Software based tools
to avoid adverse drug effects. So these are all examples of totally new opportunities that machine
learning and AI have opened up. And in order to scale any of them, they always start from a discovery
phase. So you're learning about you're using data to identify what's possible. Then you construct,
you know, just like you would construct a new drug, here you would construct a new piece of
software just like a drug happens to be bits and bytes that's using the data to do something
differently. You should ideally go through the same exact process of creation, validation,
evaluation, showing it works in a prospective setting upon, you know, when used. And once you've
done all that, then you move into policy. So when we think about policy, there are two levels of
policy. They're sort of at the level of like clinical guidelines, which means societies have to go,
there's a whole process in medicine dissemination of new ideas are much more rigorous, methodical.
I would even say somewhat slow in the sense that you're trying to convince, you know, and you
saw this with the vaccines, right? We had to think very hard about how is evidence communicated,
because you can have all the right stats and data supporting something. It still matters how
it's disseminated. And through what channels is it disseminated in order to build trust at scale.
And so that's sort of where you move into the policy realm, where you are thinking hard about
one, like clinical guidelines for a specific new invention. And then at the policy level,
also what is the general mechanism of framework by which these kinds of ideas get absorbed over and
over again at scale. And then of course, the implications of that on everyday practice, whether it's
cutting costs, improving outcomes, and everything that's needed to accelerate disciplined adoption.
And a big part of that is dealing with the whole pay system here, the payers and the insurance
companies. And I was having a conversation with a friend who comes at things from that perspective.
He's a pharmacist by training. And he noted that, you know, we're just now getting to the point
where, you know, the first algorithms are getting coded by the insurance companies.
I don't know that world very well. So I'm sure I'm butchering it. But it's taken a long time to get
to a level of progress where we're, where we're seeing the kind of impact. Yeah. So actually,
some, let me unpack that question, because I think there are like a couple of different things
you touched on. So the first thing is, why has it taken this long? What is hard about it?
What's taking long? And then the second question is, well, where do we stand now?
Yeah. So let's start with like, is it taking long? What is taking so long? Why?
So, you know, 10 years ago, electronic health records came to be, right? And the digital
infrastructure started to be. Now, turns out, the data collected within these and through variables,
which means there's been, and COVID's only accelerated this, right? There are, you know,
now data being collected in social format through devices, through measurements in the clinic,
in a hospital, also like any kind of billing data, reimbursement data, like lots of different
sources for really building what is kind of like a digital longitudinal picture of a person.
And how they've, you know, how they've evolved, but also how like different treatments have
impacted them, which means now in the last five years, what's been possible is, you know,
researchers like myself, you know, I was sort of relatively early in this movement, but like,
really going neck deep to understand, head deep, to understand like, what's hard? Like this data,
there are hundreds of different data streams. It's not like imaging data via one type of data,
you have hundreds of different data coming in, different kinds of bias, different kinds of
missingness, and you're integrating all these data to draw real-time clinical signals,
but these signals have to be much more trustworthy, safe and precise compared to safe, you were just,
you know, choosing whether to show somebody the ad for a shoe, right? It's like a whole different
ballgame. And so a big part of this was being able to build the kinds of methodology and
technology to be able to really draw safe reliable trustworthy inferences that could then power
specific applications. That's the first part. Second is then tying it to real concrete use cases
that are well supported by clinical users where there's naturally need for it, like hospitals
are struggling or providers are struggling, they actually need solutions to help them. As opposed
to, you know, often the way technologists start by solving problems is they start with the
problems that are technically hard, that they find technically interesting and not all of them
are technically useful. So in this case, we want to marry the heart, you know, we had to build
the heart technology, but we also had to deeply understand medicine. The practice of medicine to
understand where are their use cases where today people are struggling, where there is need,
and we can marry the two. And then the third part, that is actually really hard. And so I thought
we were like almost there back in 2016 and I was like, oh, this is so beautiful. We've written
all these papers in a number of different use cases and we've started to show how it's feasible
to take the technology and to even get it to a place where it's possible to show how it would be
used by providers. But then I realized like, you know, there are all these other barriers like how
the technology is delivered within a provider's workflow, like it's user experience. How do they,
you know, how will they use it? Is it easy to use? And how does it communicate? Like going back to
machine learning and AI, one of the super cool things in the field in the last couple of years
that, you know, our lab, my team at Bayesian and then others in the field have been thinking about
is how do we make machine learning amenable to collaboration with experts, right? So in my example,
if physicians and nurses and care team members are going to use the software, these are high stakes
decisions. We need them to be able to collaborate with these software outputs. It's not like a black
box system where the system can just, you know, say something and overrule what the provider is going
to do. It's actually, it requires teaming. It requires the ability for the software to identify
patients at risk for the providers to come in and agree, disagree, reason with it. And that means
it's a joint decision making process. And how do you facilitate that with machine learning in
high stakes environment? So that's been, those are the kinds of areas where we need to keep pushing
the field. And we've been able to make a lot of progress in terms of the quality of the underlying
methodological stack to be able to get to really high quality inferences that are trustworthy.
Of course, there's always room to improve. And then, you know, in terms of use, like we built
and deployed, for example, in sepsis, which is one of the leading causes of inpatient death.
I lost my nephew to sepsis. So it's sort of like a personal area that I've been working in for
almost now seven years. And in sepsis, for instance, you know, timely treatment is one of the most
effective ways to improve outcomes. Basically, the earlier you can catch a patient, evaluate and
give them the right treatment, the more you're likely to completely alter the clinical trajectory.
But and our very early work back in 2015 showed you could identify sepsis early using machine
learning, but getting it to a place where you could identify it, surface it, get providers to use it,
adopt it, act off of it to actually improve outcomes was like a whole five year, six year journey.
And like, and basically now we're we're recently going to release a study that shows, you know,
our experience deploying this with thousands of physicians and nurses using it over the course of a
two and a half year period, where we've been able to see, you know, very meaningful adoption.
So like 90% of cases that the software flags, the providers actually go in, look at what the
tool has to say, and they provide an evaluation. And then they treat patients if they agree. And
and that's lead to very meaningful shifts in in terms of in our cohort, what we found is
in sepsis, every hour is daily is associated with increased risk of associated with significant
increase in mortality and we've been able to meet very significantly impact how earlier these
patients are getting treatment. And so that's an area that you've worked very closely and
are you able to give us a sense for more broadly, you know, where are their pockets of success?
I mean, you know, we over the past few years or or many years at this point, you know, we've
gone through the waves of, oh, hey, we, you know, I can read, can read x-rays better than radiologists.
So, you know, we're done there, you know, x-rays can identify, you know, cancer and biopsies,
you know, we're done there. But, you know, when you talk to folks in the industry,
you know, we're far far away from done. How is ours? Excellent, excellent, excellent point.
So this is so hard as a researcher in the field, because I get to watch those headlines all the time.
One of the big, big, so it's sort of the innovation has gone through phases. The first phase was
hubris. We went in, there were people who went in and were like, we can do everything, because they
like can do everything and anything, so it can do everything. They underestimated how hard medicine
healthcare is. So that was hubris. Then phase two, a renewed pack of researchers who came in
wiser and went in and really rolled up their sleeves dug in deep and came up with methods that
actually work for this kind of data, integrating domain knowledge, causal reasoning,
thinking about safety, reliability, actionability, that sort of thing. So then the next phase was
a sequence of methods that were better, higher quality, which resulted in these kinds of headlines
you've seen where they're doing evaluation studies in the lab where they say, okay, let's compare
how the software does to how a human expert would do either by looking at historically
on a population, what the human did in comparing the software in the background or putting them in
front of the software and see if they would change their mind. What's new now is phase three,
where basically we've gone from experiments in the lab to the kind of example I'm talking about in
the last three years where we've now deployed in real life settings where providers are actually
using these software and actually making decisions with it. That's been very hard to come to and
it's been a long you know a long time coming and that's why this is so exciting to be able to get
to a place where VC providers adopting VC providers in engaging, interacting and actually a changing
practice in a meaningful way. So I think this phase three is going to be extremely exciting because
we're going to see more and more. So through Bayesian for example we've applied sort of a platform
that we've built which provides basically these kinds of real-time signals to empower providers
to catch life threatening complications early to save lives and we've done this in a number of
different clinical areas and my sense is like just like we've done it there are a couple of other
you know there are other groups around the country now there are also sort of like in imaging some
of these early results you saw where people are like I have software can do better in some of these
areas people have already started operationalizing it. So there's this in diabetic orthinopathy
which is an area where you know often diagnosis is missed because patients go to the prime and the
question was can the automate the diagnosis or screening of diabetic orthinopathy with primary care
providers and so there are groups now that have built software that gets deployed at primary
physician's office that can be used for screening automated screening and then if they're at
high risk they're sent to a specialist. So that's sort of an example there's already now in white
spread use and and in terms of billing and coding and reimbursement which is sort of what you alluded
to earlier that's now starting to happen for some of the treatments. So there are like a couple
different treatments are already where these AI type screening diagnostic workflow is tools
where you know they're getting reimbursed today by either from the health system paying for it
itself or the insurance company is paying for it for the use of it. Yeah yeah and that ends up
being a big accelerator for innovation in the space is the impression I'm under. You always have
to understand at the end of the day if you you can do things to make you know this was for me one
of the most rude of evenings and you know like back in 2011 I sort of thought well doesn't it make
so much sense we could save lives would not be enough but the reality is that's not enough that's
the way a provider would see it is or a physician would see it is there are so many opportunities
for saving lives you need to solve more than one problem for me need to help me save lives but you
also need me to help me do other things you need to save me time you need to make my job easier a
health system administrator will say you need to help us cut costs you need to help us improve our
you know reduce penalties that we get so this is where deep marriage of the domain and the
financial system and how it works and then marrying that to where the use cases are where there's
real opportunity for adoption near term and then obviously long term this is where policy plays
a role again right as we see more examples like this it's not my policies frozen there's always
opportunity for new policies to get adopted that incentivize the use of these kinds of technology
so for instance healthcare historically has been pretty reactive which means you know when a
problem happens you show up I look at what's happening and I fix it I think where AI can make a
big difference done right is moving it from being reactive to proactive we can fork we can look at
your data in a granular way we can forecast we can make it possible for you to anticipate these
complications and act on a timely way that is pretty exciting but today in some scenarios
moving to proactive care might actually reduce the amount systems are getting paid which means
there's a natural financial barrier to the adoption of these kinds of technologies that is also changing
this is you know awareness that's coming people are becoming aware you know there are new
financial models that are coming you know new sets of financial models like
band of payments and value-based care where there's incentive for systems to be more
more focused on preventative care proactive care and that all of that will also mean more
opportunities for AI to impact lives got it got it you in kind of describing this phase two you
rattled off a handful of methodological changes improvements that have happened over the
past few years it seems like an interesting area to maybe dig into a little bit deeper so that folks
you know that are thinking about entering the space have some ideas for the the way that they
need to approach problems in the space can you elaborate on what some of the big you know differences
that you've seen and you know the way folks need to approach machine learning problems in
healthcare nowadays yeah so one of the very very big differences is sort of in some of the other
areas there's this notion of like you have really good gold standards and really clear evaluation
metrics so for instance you could go into face recognition say and maybe there's a very nice
data set where everybody sat down and everybody can agree this person is this person and that person
that person doesn't so much debate about it and you can and if your goal is to do face recognition
or face detection the error metric is pretty clear so you can go in get a data set it's well annotated
there's in a whole lot of disagreement and you have a clear metric to optimize and then people can
go to town with all the creative ideas for optimizing that metric there are so many ways in which
health data sets are not bad so for example in most clinical areas the notion of like what is
the goal standard and what is the metric you're optimizing for is very unclear so as so when we first
frame the successio a success early detection problem the question was well how early do you want
to detect it because if it's too early maybe providers won't recognize it but if it's too late but
that's not very productive so that's one example the second example okay what is sepsis that's
an existential question people will sit down and debate like this person was treated for sepsis
because they likely were septic but somebody else might say yeah but this person was being a bit
conservative and treating them so what do you do do you treat that person as septic or not septic
or do you treat it so how do you think about that third you could take data set from one hospital
or one health system and you could learn a model that is very good at predicting there but as soon
but you know we've written numerous papers on the topic like when you move it to a different hospital
if you have these big rich deep models that are very flexible can learn anything they can easily
pick up patterns that are very specific to how people practice in that hospital when you go to a
different hospital that method may not generalize at all in fact there are papers showing you know
certain methods are very brittle easily break as you shift the underlying data and and you want
methods that are robust and to these kinds of shifts so can we so we need almost sort of like new
class of reliable learning methods or you know ship stable methods like they have a number of
different names in the field but basically methods that like where if there are new sense things
that change in the data they're not going to actually impact the quality of the learning system
or differently put up front you can get guarantees that if certain types of new sense changes
happen they're not actually going to hurt the software's performance in an unpredictable way
which is something that's very important in applications like you know social impact applications
right where it's often a question of life as opposed to say like advertising breaks by contrast
I could talk about a number of other methodological issues but you know all of these like
how do you like health data are like so messy so messy and there's a lot of messiness how do you take
into account the like by tackling the messiness and the messiness and measurement models in an
intelligent way you really can show 200 300 percent improvements in precision or sensitivity so
so yeah I think yeah I was actually going to ask about that because in the in the setups all
this you described high tech and the introduction of electronic medical records as kind of opening up
this you know huge opportunity but I still hear from folks that you know as much as you know we
digitized the data is still very very messy very dirty and that remains a huge constraint in this
particular set of applications I'm just curious so there if you can elaborate on that and what
kinds of examples you can give us to help us understand the state of healthcare data yeah so I
think healthcare data is definitely far more messy than any other domain I've ever worked with
the the way I think about it is there are certain things you can do with it and there are other
things you can't do with it which is why it's even more important to have deep expertise in understanding
the data the complexity of the data but also the problems you're looking to solve with it to
understand the risk profile right just like in a drug there's a notion of risk benefit analysis
right almost everything comes with like a some kind of side effect so how is it getting used on
home what's the side effect what's the benefit and there's a risk benefit trade off so in the same
vein in terms of how data sets there are some applications where today's technology is just
not there there are other applications where today's technology isn't there but you could build
the right technology to improve it and yet other applications where no amount of amazing
technology can help you because the information doesn't exist so I think there's a there's
um certainly a number of problems where the information doesn't exist and we just need new
modalities but the vast majority of problems are of the kinds of problems where the data exists
because today human experts like physicians nurses care team members are looking at this data
and making decisions right so the data exists and there's an opportunity to leverage the data in a
much more intelligent way to be able to so it's all to be able to improve the quality of decision
making and outcomes right so it's like a human expert is looking at it yeah and they're making
decisions so whether you like it or not it's happening and so now the question is how amenable
are these and the way and I think that's where we need the right kind of um machine learning
AI technologies that are you know humble where like the researchers building these tools are humble
they understand the difficulty of it and they're intelligently approaching you know which problems
too tackle and then doing very careful evaluation so I guess another team here is evaluation so
because this area is hard and correctness is so crucial I almost feel like unlike other fields where
people spend you know 10 units of time like 90% of the time developing a model and then 10 minutes
writing it up here it's the flip it's like whatever time you might spend the model you need to
spend 9x that much more time triangulating in many many ways to get to a place where you know it works
yeah and and that makes this really hard so um yeah so I think I think it's very promising I think
the data are exciting I think the loads of opportunities is just uh it requires more patience
more thoughtfulness more carefulness uh maybe back to methodology your you've named your company
Bayesian health uh you mentioned causality uh in that list of um tools yeah talk about the role of
causality and and um maybe by extension the approach that Bayesian is taking yeah so um when they hear
the name Bayesian they often think oh is it only using Bayesian methodologies um so um so let me just
sort of first quickly explain that which is um just like any smart human when we have a lot of
different data coming at us we integrate it over time to uh to update our view of what we think
is happening that's how the best physicians practice right they're continuously doing new tests
they're integrating the new piece of information coming in they're doing uncertainty quantification
they're thinking about how certain and uncertain they are about the different pieces of information
coming in and putting it all together to come up with the forecast which updates as new information
arrives that's a very Bayesian way of thinking so it's that's basically so the Bayes the Bayesian
health the name comes from the idea of building intelligence software that gives you the ability
to do that with largely with large scale health data in terms of the kinds of techniques it's it's
really um a comment you know so you ask me causal inference so when we have today for these models
to be able to be um intelligible and actionable it's so important for it to not capture
spurious correlations or spurious dependencies that are almost like hurt the user trust
so in some sense that's where knowledge of the domain the data generating process and as a result
uh techniques from causal inference are really helpful more more than in in being able to build
models that are going to be more intelligible and actionable turns out these models are also more
transportable or you know more likely to generalize as you go across sites or across uh you know even
within the same site across time where you know data collection methodologies might change or practice
patterns might change and so on and so forth by making models that aren't just learning memorizing
what's in the data but it's reasoning about what if like if this were to happen then what if that
were to happen then what i'll give you a simple example there was a really nice um paper a few years
ago where it's a very well cited paper now where um this team of researchers were trying to learn
a model for predicting patients who came in the pneumonia into a hospital emergency department
their risk profile with the idea that if they were high risk they would place them in the intensive
care unit which is the sicker unit or like the high acuity unit right like where the sickest patients
go and then if they were lower risk maybe they would go to the floor and uh they took historical
data sets and they learned using like you know classical supervised learning techniques a model
and what that model did is looked at retrospective data and said okay great if the patient came in
and they died then that's my training data for high risk if the patient survived that's my training
data for not high risk and then based on that they learned a model and then what the model learned
was turns out patients who had pneumonia with asthma were actually lower risk than patients with
just pneumonia which is uh for totally counterintuitive because you know asthma complicates the case
quite a bit and actually you tend to have poor outcomes now when they went and looked in the data
they realize actually the reason that was happening is because the people with pneumonia and asthma
were getting escalated to the intensive care unit versus the people with just pneumonia
were on the floor but in the intensive care unit they were just getting constant monitoring,
constant supervision, constant care which meant when you looked at the resulting models all it was
really like it was ignoring the fact that this person had been so you know like what if what the
model should have done is what would it would this person have been high risk if I didn't send
them to the ICU would this person be high risk if I went exactly you want to do counterfactual
reasoning so that's sort of an area where we you know very early on he's you know started realizing
and with health data sets where you really are trying to reason about a patient's risk profile
you really want to I mean this isn't just pertinent to health this is just pertinent across the
board when you're looking at any kind of temporal or sequential decision making problem or decision
making problem you want to ask you know under different interventions what would the trajectory
have looked like so you can basically figure out what's the right thing to do and the question
therefore one should be asking when developing these models is what would this person's risk profile
be had they not been to the ICU as opposed to what's the risk profile ignoring what was done to
them right right so that's sort of a very simple example of a place where you know how machine like
the next trend these the types of machine learning approaches we're using embrace causality
in order to be able to get more sensical models that are both more accurate more actionable but also
more transportable and safe so that by the time this show is out in the wild and folks are listening
to it Bayesian health will be announced and released and as part of that you're releasing a study
can you tell us a little bit about the study that you're that will be published by the time folks
yeah this yeah yeah so we're releasing a copy of a manuscript where basically starting in April
2018 across five different hospital sites we built and deployed the software for early detection
of sepsis and making it possible for care teams specifically providers and nursing staff to be able
to get access to these real-time machine learning inferences entirely within the workflow
to flag patients who are high risk and then make it possible and and so you know it we provided
these dynamic workflows within the EMR so EMR is the electronic medical record it's the infrastructure
the system they use for documenting for recording you know it's sort of the platform that they
you know care team like providers spend the vast majority of the time in and so what we did is
provided this real-time software that like pulls in in the background in real-time crunches it
puts information back into the EMR into the provider's workflow flags a patient when they're at
risk makes it very easy for the provider to come in and see why were they flagged more context
around the patient in terms of you know what was the risk for certain comorbidities certain
um certain adverse events and then also like what are the indicators that they could quickly look at
and um now they look at it they evaluate and then if they agree it septic they treat it
and what the study does is analyzes um and it's sort of a first study of a guy in the sense that
it's large it's analyzing physician adoption factors impacting physician adoption which is
really crucial for the purpose of building future CDS tools that have chance of getting increased
adoption and also like yes sorry clinical decision support so any kind of like AI driven decision
support decision augmentation tools and it's impact on outcomes and so what we find is one
89% of the so it was a you know upwards of nearly 500,000 patients was screened through the
software 10,000 cases of sepsis were analyzed and then providers when the software flagged a patient
90% of the times or 89% to be precise providers came in you know it was a passive tool but they
sorted out interacted with it put in an evaluation and then we found that basically when providers did
that in patients on whom providers came in and entered in evaluation and treated them a 1.9
hour median difference like so for the in the meeting case 9 1.9 hour difference in
movement and treatment timing so and then this obviously has subsequent impact in terms of
reductions in mortality, morbidity, length of stay, this manuscript only talks about
impact on clinical treatment practice and the key clinical metrics people care about like time to
antibiotics the next set of manuscripts will release will be more on clinical also discussing
then verifying like the mortality impact and morbidity impact and then one of the interesting
things was in sepsis being able to recognize it precisely is just really really hard and like
what we were able to see in our study is like with very high sensitivity we were able to detect
cases but also you know the like you know like the 10x higher precision than typically is seen
with widely available software we were able to like help them you know like reduce false
alerting quite a bit by basically improving the precision right so like in our study one in three
cases were confirmed as septic and you know it's a needle in a haystack problem only a small
number of cases in a given day gets sepsis so it's really a problem of like how do we go from
a hundred cases to the three or four we need to worry about yeah yeah I'm curious about the
adoption side of things what type or level of engagement did you have with the physicians who
were you know ultimately gained access to did they gain access to the tool passively where they
actively onboarded what goes into getting a physician on board and what have you learned about
that process yeah so actually I think so we very actively onboarded them when we initially launched
it in 2018 it was super funny we built it we were like all the papers here it shows it works
the data's there we built it we integrated it we launched it and then literally two physicians
used it at this particular site and it was so disheartening because I think that was like end of
2017 early 2018 it was sometime around then I can't remember maybe sometime in 2017 it feels like
forever ago but like I just remember distinctly like we launched it we thought it was going to be
such a big deal and it was so sad when we were monitoring the data coming in to see literally like
nobody was using it or like a small number of people who were deeply involved in the development
were using it so we had to do a lot of work to get it from like machine learning researchers and
engineers and launching a piece of software to people who understand human machine teaming and
collaboration and workflow integration and design and you know like that gap needed to be closed
and then part of that is also how you launched the software and in medicine this is really
important because it's part of the trust building process so we basically partnered with champions
in local sites to basically create materials that was really easy for them to read and you know
gave very and the tool was designed to be very intuitive easy to use and then and it's very simple
but basically making it so that we could basically have a little video where they could see
understand how to use it and then we also had a whole infrastructure for monitoring engagement
adoption which was super crucial because if you can monitor then we could understand like you know
both in terms of the health of the models itself and how they were working as we scaled across
sites but also like what was used looking like where there were barriers and how we could mitigate
those barriers and so we partnered with our champions to be able to then close barriers for adoption
right and in some and these were very barrier barriers some around software some around the
perception some around the understanding and we took different approaches to closing these barriers
based on you know what we learned awesome awesome um maybe to wrap things up you can share
a little bit about where you think the field is headed yeah um I mean I'm super it's I'm super
excited about where I think the next five years will be in this field I think we're really at a
place now where you know the data exists the infrastructure exists the ability to deliver these
inferences within workflow exists and the ability and our experiments and papers show the ability
for you know and providers are willing to engage and they're engaging and we can have meaningful
impact on practice so to me the next five years is about leveraging this whole stack to apply
this thoughtfully in a number of other clinical areas and start doing really thoughtful evaluations
like one of the very big things that's been missing is because there are no evaluations people don't
know if something is working and if they don't know something is working they they are not going to
adopt it and trust it and medicine this is such a key currency for anything so today a lot of
software and medicine software based tools people are just people deployed but they haven't had the
infrastructure to really evaluate measure efficacy um but in you know as we've sort of worked in this
area and I think um you know we've we've developed in front of monitoring so to me the next five
years is we others in the field release more and more studies showing efficacy that now accelerates
adoption learning around what increases adoption and then obviously if you have a good quality
intervention that is precise and gets adoption you're going to get outcomes.
Awesome awesome. Well Suci thanks so much for joining us and sharing a bit about what you're
up to and congrats on the launch of the company. It's been great to chat with you. Yeah like
why Sam I'm so glad we finally got to catch up and hopefully the next time we speak a few years
from now I'll have much more good news for you in terms of the number of different clinical areas
that are already being impacted by AI in that real time. Fantastic thank you.
