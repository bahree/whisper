WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.880
I'm your host Sam Charrington.

00:23.880 --> 00:27.880
This past week, the conference finally came to me.

00:27.880 --> 00:33.080
Over the weekend, the great and not-so-little anymore strange loop conference graced downtown

00:33.080 --> 00:34.400
St. Louis.

00:34.400 --> 00:40.240
I got a chance to meet with a bunch of the speakers, including Sumeet Chintala, a Facebook,

00:40.240 --> 00:44.160
Allison Parish of NYU, and Sam Richie of Stripe.

00:44.160 --> 00:50.120
I had a ton of fun and I can't wait to share some of these great interviews from the conference.

00:50.120 --> 00:54.880
Before we move on to the show, speaking of conferences, we're going into conference

00:54.880 --> 00:57.560
giveaway mode for a few days.

00:57.560 --> 01:02.800
Next week, October 10th through 11th, I'll be in Montreal for the rework deep learning

01:02.800 --> 01:08.040
summit and one lucky listener will get a chance to join me there.

01:08.040 --> 01:10.200
Entering the contest is simple.

01:10.200 --> 01:17.480
Just head on over to twimlai.com slash dl summit and choose any of up to four methods

01:17.480 --> 01:19.680
of entry and voila.

01:19.680 --> 01:25.280
While there are only four ways to enter this time around, by sharing the contest with friends,

01:25.280 --> 01:29.280
each participant can get up to 14 entries.

01:29.280 --> 01:34.680
This giveaway will only be open until noon central time on Wednesday the 4th, so make

01:34.680 --> 01:37.480
sure you get your entries and ASAP.

01:37.480 --> 01:38.480
Good luck.

01:38.480 --> 01:43.720
This week, I'd like to introduce a new sponsor, Nexosis, and thank them for sponsoring

01:43.720 --> 01:45.560
this week's show.

01:45.560 --> 01:50.760
Nexosis is a company of developers focused on providing easy access to machine learning.

01:50.760 --> 01:55.760
The Nexosis machine learning API meet to developers where they're at, regardless of

01:55.760 --> 02:01.200
their mastery of data science, so they can start coding up predictive applications today

02:01.200 --> 02:03.760
and their preferred programming language.

02:03.760 --> 02:08.520
It's as simple as loading your data and selecting the type of problem you want to solve.

02:08.520 --> 02:12.960
Their automated platform trains and selects the best model fit for your data and then

02:12.960 --> 02:15.040
outputs predictions.

02:15.040 --> 02:20.200
Get your free API key and discover how to start leveraging machine learning in your next

02:20.200 --> 02:24.320
project at nexosis.com slash twimmel.

02:24.320 --> 02:30.880
That's n-e-x-o-s-i-s.com slash T-w-i-m-l.

02:30.880 --> 02:34.880
Head on over, check them out, and be sure to let them know who sent you.

02:34.880 --> 02:41.680
Finally, before we dive into the show, a reminder about the upcoming twimmel online meetup.

02:41.680 --> 02:48.120
On Wednesday, October 18th, at 3pm Pacific time, we'll discuss the paper visual attribute

02:48.120 --> 02:54.320
transfer through deep image analogy by Jing Liyao and others from Microsoft Research.

02:54.320 --> 02:57.000
The discussion will be led by Duncan Stothers.

02:57.000 --> 02:58.520
Thanks, Duncan.

02:58.520 --> 03:02.760
To join the meetup or to catch up on what you missed from the first two meetups, visit

03:02.760 --> 03:05.720
twimmelai.com slash meetup.

03:05.720 --> 03:10.720
As you all know, a few weeks ago, I spent some time in San Francisco at the Artificial

03:10.720 --> 03:14.560
Intelligence Conference by O'Reilly and Intel Nirvana.

03:14.560 --> 03:20.120
While I was there, I had just enough time to sneak away and catch up with Scott Clark,

03:20.120 --> 03:25.680
co-founder and CEO of Sigopt, a company whose software is focused on automatically tuning

03:25.680 --> 03:29.880
your model's parameters through Bayesian optimization.

03:29.880 --> 03:35.400
We dive pretty deeply into how they do that through the course of this discussion.

03:35.400 --> 03:40.440
I had a great time and learned a ton, but before warned, this is definitely a nerd alert

03:40.440 --> 03:41.440
show.

03:41.440 --> 03:45.360
And so without further ado, on to the show.

03:45.360 --> 03:55.480
Hey, everyone, I am here with Scott Clark.

03:55.480 --> 04:03.440
Scott is the founder and CEO of a company called Sigopt and he was gracious enough to spend

04:03.440 --> 04:09.000
some time with me this morning to talk about his background, the company, and the topic

04:09.000 --> 04:13.800
that I am very interested in learning more about Bayesian optimization.

04:13.800 --> 04:16.400
We're sitting in his office in San Francisco.

04:16.400 --> 04:22.280
I happen to be in town for the AI conference and I'm really looking forward to this interview.

04:22.280 --> 04:23.280
So welcome, Scott.

04:23.280 --> 04:25.760
Thank you so much for really looking forward to it as well.

04:25.760 --> 04:26.760
Awesome.

04:26.760 --> 04:27.760
Awesome.

04:27.760 --> 04:31.680
So let's just jump right in and have you tell us a little bit about your background and

04:31.680 --> 04:34.040
how you got involved in machine learning.

04:34.040 --> 04:35.040
Definitely.

04:35.040 --> 04:38.400
I first got really excited about this while I was in grad school.

04:38.400 --> 04:42.800
So I was pursuing a PhD in applied math at Cornell University.

04:42.800 --> 04:43.800
Go upstate New York.

04:43.800 --> 04:44.800
Yeah, exactly.

04:44.800 --> 04:48.720
Cornell is great because there's not a lot to do and it's super bad weather all the time.

04:48.720 --> 04:52.040
So you just focus on studying and you graduate as soon as possible.

04:52.040 --> 04:56.120
I went to RPI undergrad, also upstate New York and had the same experience.

04:56.120 --> 04:57.120
Nice.

04:57.120 --> 04:58.120
Nice.

04:58.120 --> 05:00.280
I highly recommend it for efficient degrees.

05:00.280 --> 05:07.120
RPI had the added advantage of it was hugely skewed towards Nell students and so they

05:07.120 --> 05:11.560
were even less distractions and that's excellent.

05:11.560 --> 05:15.960
So basically I was applying math to a variety of different things.

05:15.960 --> 05:18.960
One of the focuses of my degree was bioinformatics.

05:18.960 --> 05:22.880
So I had a fellowship from the Department of Energy.

05:22.880 --> 05:27.560
So the problem I was trying to attack was genome assembly and you can think of this as

05:27.560 --> 05:30.360
trying to solve a jigsaw puzzle on a supercomputer.

05:30.360 --> 05:34.920
So basically we have a bunch of DNA and we have to reassemble it into some genome and the

05:34.920 --> 05:39.680
Department of Energy cares about this because if you know the genome it might be a path towards

05:39.680 --> 05:42.400
more efficient biofuels or something like that.

05:42.400 --> 05:46.640
The problem was lots of tunable knobs and levers with these various systems and we had

05:46.640 --> 05:51.040
to configure those to get the best possible performance out of them.

05:51.040 --> 05:54.200
And you were the grunt in grad school exactly to tune all these levers.

05:54.200 --> 05:57.200
We jokingly call this graduate student descent.

05:57.200 --> 06:01.120
The idea being we just need to get to the best configuration and it doesn't matter how

06:01.120 --> 06:02.120
you get there.

06:02.120 --> 06:03.120
Yeah.

06:03.120 --> 06:06.120
There's a good way to attack this problem and a lot of get your field value for your

06:06.120 --> 06:07.120
paper.

06:07.120 --> 06:08.120
Something like that.

06:08.120 --> 06:09.120
Yeah.

06:09.120 --> 06:12.120
I mean academic incentives are a completely different topic.

06:12.120 --> 06:13.120
Yeah.

06:13.120 --> 06:15.120
I just thought I tossed it in there.

06:15.120 --> 06:16.120
Fair enough.

06:16.120 --> 06:20.040
But the idea is there's a couple standard ways people go about attacking a problem like

06:20.040 --> 06:21.040
this.

06:21.040 --> 06:22.520
You can try to brute force the problem.

06:22.520 --> 06:28.000
So just lay down a grade of all possible options for every configuration and try them all.

06:28.000 --> 06:32.160
This was intractable for us because it took 24 hours on a government supercomputer.

06:32.160 --> 06:36.960
Every single time we wanted to try a single configuration, randomized search has become

06:36.960 --> 06:40.960
very popular, especially in the deep learning literature for trying to come up with different

06:40.960 --> 06:45.160
configurations of hyper parameters and architectures and things like that.

06:45.160 --> 06:49.600
Turns out much more efficient than grid search, but this is still like trying to climb a mountain

06:49.600 --> 06:52.960
by jumping out of an airplane and hoping you land at the peak.

06:52.960 --> 06:56.800
Not necessarily the most intuitive way to go about optimizing something.

06:56.800 --> 07:01.640
A lot of the different algorithms we use randomized initialization.

07:01.640 --> 07:03.440
That's different from randomized search.

07:03.440 --> 07:04.440
Correct.

07:04.440 --> 07:08.040
So when you're building a neural network, you might use randomized initialization on the

07:08.040 --> 07:13.040
individual weights and then use some sort of secastic gradient descent optimizer within

07:13.040 --> 07:14.720
that underlying system.

07:14.720 --> 07:19.040
This is more of a black box parameter optimization problem I'm talking about where we're not

07:19.040 --> 07:24.160
introspecting the underlying model, but just tuning the higher level configuration parameters.

07:24.160 --> 07:28.640
So some of those configuration parameters might have to do with that random initialization

07:28.640 --> 07:32.160
or the secastic gradient descent parameters or something like that.

07:32.160 --> 07:37.360
You definitely need to be able to bootstrap efficiently from no data, but doing purely

07:37.360 --> 07:42.560
randomized search is not necessarily the most efficient thing you can do.

07:42.560 --> 07:47.200
So maybe before we move on, since I think we're going to be spending a lot of time talking

07:47.200 --> 07:54.200
about hyper parameter optimization here, maybe dig into grid search a little bit more so

07:54.200 --> 07:59.080
that we're all starting from the same place.

07:59.080 --> 08:03.120
Basically, as I understand it, the idea is you've got some set of hyper parameters, those

08:03.120 --> 08:13.040
form an n-dimensional, n-dimensional, not a cube, but a lattice, thank you.

08:13.040 --> 08:17.920
Grid search is basically systematically going from point to point, like if you were searching

08:17.920 --> 08:23.520
for someone in a forest, you'd form a grid and attack all those points.

08:23.520 --> 08:28.560
And random is you're basically picking points, and the idea is statistically, if you pick

08:28.560 --> 08:34.280
enough points, you'll get some level of coverage of all of the combinations of these hyper

08:34.280 --> 08:35.280
parameters.

08:35.280 --> 08:36.280
Exactly.

08:36.280 --> 08:39.200
So back to your searching and a forest analogy, this is jumping out of a helicopter and

08:39.200 --> 08:42.760
seeing if the person's there, getting back in and like continuing to do that over and

08:42.760 --> 08:43.760
over again.

08:43.760 --> 08:44.760
That's random search.

08:44.760 --> 08:45.760
That's randomized search.

08:45.760 --> 08:47.760
Another popular method is just manual today.

08:47.760 --> 08:49.880
So trying to do this in your head.

08:49.880 --> 08:53.920
And in the forest example, when there's only two dimensions, you might have a lot of intuition

08:53.920 --> 08:57.800
about maybe the person's going to be up on a hill or something like that, it can actually

08:57.800 --> 08:59.280
be somewhat effective.

08:59.280 --> 09:03.280
But once you start to look at 20-dimensional problems, a lot of human intuition starts

09:03.280 --> 09:07.880
to break down, and you might not be able to have some of that expert knowledge in the

09:07.880 --> 09:13.320
searching for a human and a forest setting, how to set stochastic gradient descent parameters

09:13.320 --> 09:17.320
and number of hidden layers and learning rates, and all these sorts of things, it starts

09:17.320 --> 09:19.400
to get very convoluted very quickly.

09:19.400 --> 09:25.280
And so manual search, well, it can be effective to kind of resolve very localized solutions

09:25.280 --> 09:28.040
is not a great global optimization strategy.

09:28.040 --> 09:34.720
And for the typical model that you are seeing, like, how many hyperparameters are there?

09:34.720 --> 09:37.080
Yeah, so it really depends on the underlying system.

09:37.080 --> 09:40.880
So something simple like a random forest might only have a couple that you care about,

09:40.880 --> 09:45.240
number of trees, number of samples needed to split a node, something like that.

09:45.240 --> 09:49.320
As you start to advance, maybe integrate in boosting methods and also you have learning rates

09:49.320 --> 09:51.600
and other sorts of parameters you can tune.

09:51.600 --> 09:55.480
But once you get into the deep learning and reinforcement learning regimes, there can

09:55.480 --> 10:00.760
be dozens of individual parameters, especially if you start to think of the system as a whole.

10:00.760 --> 10:06.160
So when you're doing an NLP or computer vision type problem, all of a sudden you have different

10:06.160 --> 10:09.240
ways you can parameterize the data as well.

10:09.240 --> 10:14.480
And so by looking at that system and its entirety, all of a sudden there can be dozens of parameters

10:14.480 --> 10:19.200
and something that grows exponentially like a grid search is completely intractable.

10:19.200 --> 10:25.200
The human manual intuition starts to break down and randomize search is just too slow to

10:25.200 --> 10:26.960
luck into a reasonable solution.

10:26.960 --> 10:27.960
Okay.

10:27.960 --> 10:34.720
Can you give an example of in the case of NLP how the way you look at the data set changes

10:34.720 --> 10:36.760
and increases your parameter space?

10:36.760 --> 10:37.760
Yeah.

10:37.760 --> 10:39.560
So how you tokenize the text itself.

10:39.560 --> 10:42.760
So do you look at different in-gram sizes?

10:42.760 --> 10:47.160
The idea being do you look at one word at a time, pairs of words, triples words, do you

10:47.160 --> 10:51.480
maybe do different thresholds for the frequency within the corpus itself?

10:51.480 --> 10:56.800
So maybe cut out words like the because they're too common and then also cut out words like

10:56.800 --> 10:59.240
bananza because they're too rare.

10:59.240 --> 11:04.520
And so you can kind of change the actual feature representation itself before you even feed

11:04.520 --> 11:06.240
it into the machine learning algorithm.

11:06.240 --> 11:08.000
But these are all tunable moms and lovers.

11:08.000 --> 11:09.000
Got it.

11:09.000 --> 11:10.000
Okay.

11:10.000 --> 11:15.960
So you were stuck in grad school like again, twiddling these lovers and you know as all

11:15.960 --> 11:18.920
innovation happened, you thought there's got to be a better way.

11:18.920 --> 11:19.920
Exactly.

11:19.920 --> 11:23.160
So went around the department and found that this was a very common problem.

11:23.160 --> 11:27.000
People in machine learning, people in financial engineering, like everybody were building

11:27.000 --> 11:30.600
these these expert systems, but they needed to be fine tuned.

11:30.600 --> 11:33.400
But everybody was using these kind of standard techniques.

11:33.400 --> 11:37.840
So expanded my search outside the department and eventually found who would become my PhD

11:37.840 --> 11:40.680
advisor in the operations research field.

11:40.680 --> 11:43.280
So they've been attacking this problem for decades.

11:43.280 --> 11:47.000
If you have a time consuming and expensive disamples system, how do you most efficiently

11:47.000 --> 11:48.760
get to the best configuration?

11:48.760 --> 11:52.720
So this crops up if you're tuning a particle accelerator, it crops up if you're trying

11:52.720 --> 11:56.720
to decide where to place a gold mine, which is where some of the original research came

11:56.720 --> 12:01.800
from in the 50s, but it maps extremely well on to a wide variety of computational problems.

12:01.800 --> 12:02.800
Okay.

12:02.800 --> 12:05.520
You have some input that comes in, some output that you care about.

12:05.520 --> 12:09.920
How do you get to the best output in as few input attempts as possible?

12:09.920 --> 12:12.520
So I started working in this field of optimal learning.

12:12.520 --> 12:17.280
This is called an operations research, or sequential model based optimization or Bayesian

12:17.280 --> 12:18.280
optimization.

12:18.280 --> 12:22.160
A lot of fields have different names for it, but the idea is, how do you do this as efficiently

12:22.160 --> 12:23.160
as you can?

12:23.160 --> 12:27.760
Ended up pivoting my PhD towards working on this problem, ended up being one of the chapters

12:27.760 --> 12:29.560
of my thesis.

12:29.560 --> 12:34.200
And after graduating, I realized that a lot of different people in a lot of different industries

12:34.200 --> 12:35.200
had this issue.

12:35.200 --> 12:39.720
So I spent two and a half years at Yelp working on their advertising team, applying these

12:39.720 --> 12:43.560
same techniques to help do more perform into advertising.

12:43.560 --> 12:44.560
Okay.

12:44.560 --> 12:48.080
The idea being, if you think about it mathematically, an advertising system is very similar

12:48.080 --> 12:52.640
to a genome assembly system, insofar as a lot of experts spend a lot of time building

12:52.640 --> 12:53.640
something.

12:53.640 --> 12:56.520
There's a bunch of inputs, and there's an output you care about.

12:56.520 --> 12:59.480
In genome assembly, it's better papers because you get a better genome.

12:59.480 --> 13:02.280
In advertising system, a bunch of money comes out the other end.

13:02.280 --> 13:06.000
I mean, clearly, there are tons of problems that fit the general exactly.

13:06.000 --> 13:07.000
Exactly.

13:07.000 --> 13:10.040
And so you started Sigopt.

13:10.040 --> 13:11.920
How long have you been at it here?

13:11.920 --> 13:12.920
Yeah.

13:12.920 --> 13:16.760
So immediately after Yelp started Sigopt about three years ago, went through a Y Combinator

13:16.760 --> 13:20.200
and Winter 15, raised a few rounds of funding.

13:20.200 --> 13:23.040
Most recently, a series A led by Andreessen Horowitz.

13:23.040 --> 13:25.120
And now we're 16 people in San Francisco.

13:25.120 --> 13:26.120
Perfect.

13:26.120 --> 13:28.880
That sounded like a steamboat.

13:28.880 --> 13:35.240
I swear it's not normally this bad.

13:35.240 --> 13:41.280
And so I guess I want to kind of jump into the main crux of this interview, which is

13:41.280 --> 13:43.840
around this Bayesian optimization.

13:43.840 --> 13:49.000
Walk me through the way, folks like Pedro Domingo also talk about the Bayesians as this

13:49.000 --> 13:57.960
one tribe within machine learning, and as opposed to others, walk me through, I guess

13:57.960 --> 14:02.720
what I'm trying to get at is I've had a couple of conversations with folks about different

14:02.720 --> 14:11.160
aspects of Bayesian program learning and other things, but I feel like there's still

14:11.160 --> 14:17.960
some ethos of what it means to be Bayesian and think about things from that perspective

14:17.960 --> 14:20.400
that we haven't fully captured on the podcast.

14:20.400 --> 14:25.400
So if we can start there and then get to the optimization, that would be pretty cool.

14:25.400 --> 14:26.400
Definitely.

14:26.400 --> 14:31.320
So the way a lot of those other techniques work like grid search or random search is there's

14:31.320 --> 14:32.960
no learning happening.

14:32.960 --> 14:36.640
And I think that's one of the major differences between the Bayesian optimization approach

14:36.640 --> 14:40.720
or the Bayesian approach to this problem and some of those more traditional techniques.

14:40.720 --> 14:46.040
The idea being every single time I evaluate this underlying machine learning pipeline

14:46.040 --> 14:49.600
or whatever it is, it's extremely time consuming and expensive.

14:49.600 --> 14:54.240
And I want to be able to leverage that data to decide what to do next.

14:54.240 --> 14:59.320
And so a lot of the Bayesian methods rely on this concept of trading off exploration versus

14:59.320 --> 15:00.920
exploitation.

15:00.920 --> 15:05.480
So we want to be able to learn as much as we can about that underlying response surface,

15:05.480 --> 15:10.720
how it varies, how all the parameters interact over what length scales, how certain we are

15:10.720 --> 15:17.360
about specific configurations and how well they'll perform and learn about that while

15:17.360 --> 15:22.200
also exploiting localized information to drive you to better results.

15:22.200 --> 15:27.320
And by constantly trading off these two facets, we're able to exponentially faster than

15:27.320 --> 15:31.440
something like an exhaustive grid search, arrive at better solutions.

15:31.440 --> 15:35.920
And the main difference here is the fact that we're learning from the past and using that

15:35.920 --> 15:38.600
to influence what we do in the future.

15:38.600 --> 15:43.160
And now when I think about this kind of explore, explore exploit trade off one of the things

15:43.160 --> 15:46.440
that jumps to mind for me is reinforcement learning.

15:46.440 --> 15:53.400
Is that coming in play here or maybe less so because the environment itself, the problem

15:53.400 --> 15:57.560
itself doesn't necessarily change in response to the inputs?

15:57.560 --> 16:00.840
So the underlying system can change pretty dramatically.

16:00.840 --> 16:06.800
So you can think of this as this larger system that fits around any underlying pipeline.

16:06.800 --> 16:09.280
That could be a reinforcement learning pipeline.

16:09.280 --> 16:12.840
It could be just a standard deep learning or it could be something as simple as a logistic

16:12.840 --> 16:15.080
regression or a random forest.

16:15.080 --> 16:19.840
And you can think about the fact that every single time we try a new configuration, we

16:19.840 --> 16:22.960
want to observe some sort of output at the end that the user defines.

16:22.960 --> 16:27.520
It could be something simple like accuracy, could be the sharp ratio of a back test of

16:27.520 --> 16:30.280
an algorithmic trading strategy or whatever it may be.

16:30.280 --> 16:33.800
And so we use that to kind of influence what we do next.

16:33.800 --> 16:38.760
You can think of this as this kind of reinforcement loop as a whole over that entire system.

16:38.760 --> 16:41.320
But we're agnostic to what the underlying method is.

16:41.320 --> 16:49.280
And so the underlying method could be reinforcement learning or any number of other things.

16:49.280 --> 16:55.400
But it also sounds, I guess what I was asking was, are you or could you do reinforcement

16:55.400 --> 17:02.280
learning at the top level to optimize the thing that you're optimizing, which could be reinforcement

17:02.280 --> 17:03.280
learning as well?

17:03.280 --> 17:08.400
So reinforcement learning on the hyperparameter space as opposed to the actual model itself?

17:08.400 --> 17:09.560
Yeah, definitely.

17:09.560 --> 17:12.520
And there's a lot of different approaches to this underlying problem.

17:12.520 --> 17:16.080
There's a lot of very cool papers that are all the top machine learning conferences

17:16.080 --> 17:17.480
for attacking this.

17:17.480 --> 17:23.480
The way that we attack it is via this consequential model based optimization.

17:23.480 --> 17:25.520
And this is a very Bayesian approach.

17:25.520 --> 17:30.880
And the idea is we're sequentially learning as much as we can about this underlying system.

17:30.880 --> 17:35.640
So once again, using the history to decide what to do in the future, it's model based

17:35.640 --> 17:40.440
in the sense that we're building up different surrogate models for how we think individual

17:40.440 --> 17:46.200
configurations are going to respond when we actually sample the underlying system.

17:46.200 --> 17:51.080
We can use various different things here like Gaussian processes or other kind of Bayesian

17:51.080 --> 17:53.840
regression type systems.

17:53.840 --> 17:59.760
And we want to be able to say, given what we think is going to happen, how do we sample

17:59.760 --> 18:01.080
as efficiently as possible?

18:01.080 --> 18:06.040
Then we want to say, what do we think is going to improve in expectation the most?

18:06.040 --> 18:11.080
What's the highest probability of improvement in terms of that new configuration to suggest?

18:11.080 --> 18:15.440
And then that loops back into the underlying system after you sample it.

18:15.440 --> 18:21.520
And we learn, update the posterior of these individual surrogate methods, optimize on them,

18:21.520 --> 18:23.800
and repeat that entire process.

18:23.800 --> 18:32.320
So how do you get to the proposed model for the model based piece of this?

18:32.320 --> 18:36.800
In general, in Bayesian optimization, usually you pick a specific type of model and go

18:36.800 --> 18:37.800
from there.

18:37.800 --> 18:42.480
So in some of the open source work I did at Yelp, it was kind of very cut and dry, use

18:42.480 --> 18:47.640
a Gaussian process, use expected improvement to optimize, and go through kind of extremely

18:47.640 --> 18:48.640
sequentially.

18:48.640 --> 18:51.960
This is very similar to Spearment, another popular library.

18:51.960 --> 18:56.080
Let's say that one again, Spearment, it was an open source library out of Harvard, very

18:56.080 --> 19:00.720
similar to the metric optimization engine, or MO, which I wrote at Yelp, also similar

19:00.720 --> 19:03.840
to like G-Pie opt, which is kind of a more recent one.

19:03.840 --> 19:08.360
This is kind of the bread and butter Bayesian optimization approach, Gaussian process

19:08.360 --> 19:10.160
is expected improvement.

19:10.160 --> 19:13.680
What SIGUP represents, though, is this ensemble based approach.

19:13.680 --> 19:17.600
So different surrogate models, different acquisition functions, different covariance

19:17.600 --> 19:23.320
kernels for learning how the parameters interact, as well as not just kind of that standard

19:23.320 --> 19:28.800
build us a single sequential surrogate model based approach, but really taking all of these

19:28.800 --> 19:32.640
different optimizers and optimizing and making it automatic.

19:32.640 --> 19:38.200
So you can select something ahead of time because you know you want to take a very specific

19:38.200 --> 19:42.920
approach, or you can take the more generalized approach and say, we're not necessarily going

19:42.920 --> 19:46.120
to say, we're going to use this specific surrogate model.

19:46.120 --> 19:50.960
We want to learn along the way it was the best possible thing for that underlying system

19:50.960 --> 19:51.960
that we're optimizing.

19:51.960 --> 19:52.960
Right.

19:52.960 --> 20:00.320
So to take a step back, you are in the former case where you're picking a model, a specific

20:00.320 --> 20:01.320
model.

20:01.320 --> 20:07.360
You know, let's say we're assuming a Gaussian distribution, then basically we've got this

20:07.360 --> 20:14.000
hyperparameter space, we are, I'm trying to get at like how, you know, so the parameters

20:14.000 --> 20:19.560
of your Gaussian distribution will be your meaning, your standard deviation, and how

20:19.560 --> 20:26.760
are we, like what's the process for identifying those that is then, you know, that we're

20:26.760 --> 20:27.760
doing sequentially?

20:27.760 --> 20:28.760
Gotcha.

20:28.760 --> 20:33.480
So the way that a Gaussian process works is that it's assuming that the response of that

20:33.480 --> 20:38.000
underlying system that we're sampling is going to be Gaussian distributed at any given

20:38.000 --> 20:39.000
point.

20:39.000 --> 20:43.640
So it's not a single Gaussian distribution or something similar to like a Gaussian mixture

20:43.640 --> 20:48.960
model, what it actually is is an infinite number of potential Gaussian responses for every

20:48.960 --> 20:51.080
potential input.

20:51.080 --> 20:55.960
And then the way the Gaussian processes are analytically defined, once you start to sample

20:55.960 --> 21:02.160
underlying points, you can explicitly build up what that distribution is at sample points

21:02.160 --> 21:04.120
or unsampled points.

21:04.120 --> 21:08.280
The main thing that controls this is what's called a covariance kernel.

21:08.280 --> 21:14.440
And what that is is how much information do I get from sampling point A about some

21:14.440 --> 21:16.240
other point B?

21:16.240 --> 21:18.680
So does it decay exponentially?

21:18.680 --> 21:22.120
Is there some sort of high variance or noise associated with it?

21:22.120 --> 21:26.160
What are the length scales over which all the different parameters interact?

21:26.160 --> 21:30.920
This becomes doubly complicated once you start to look at heterogeneous configuration spaces

21:30.920 --> 21:35.920
with integers and continuous variables and categorical variables and things like that.

21:35.920 --> 21:38.720
Is this covariance matrix?

21:38.720 --> 21:41.720
Is this something that you're learning as part of the process?

21:41.720 --> 21:44.760
It's not something that you know our priority.

21:44.760 --> 21:45.760
Exactly.

21:45.760 --> 21:50.640
So you can set it, but you can also learn as you go.

21:50.640 --> 21:55.000
So there are tunable parameters around these covariance curves.

21:55.000 --> 21:57.280
And so it's, yeah, it's turtles all the way down.

21:57.280 --> 22:03.960
But the idea here is, once you can analytically define, this is maybe a surrogate function.

22:03.960 --> 22:06.200
I may use a Gaussian process.

22:06.200 --> 22:10.920
Here's a specific class of covariance kernels, like an ARD kernel or something like that.

22:10.920 --> 22:17.240
Then you can explicitly say, OK, how good is the fit given what I've observed so far?

22:17.240 --> 22:22.000
And because you're defining the system analytically and you've effectively mapped the problem

22:22.000 --> 22:28.920
from this extremely sparse time-consuming, expensive underlying system that you're sampling,

22:28.920 --> 22:32.280
and now you've mapped it over to this surrogate space, you can start to throw kind of the

22:32.280 --> 22:35.000
kitchen sink of mathematics at the problem.

22:35.000 --> 22:39.960
And use that to kind of optimize the underlying covariance kernels, pick the correct ones, find

22:39.960 --> 22:45.480
the right surrogate functions, and then ultimately leverage that information to decide what's

22:45.480 --> 22:49.720
the point that has the highest probability of improvement or expected improvement or whatever

22:49.720 --> 22:50.720
it may be.

22:50.720 --> 22:58.120
So is the surrogate space, in this case, the covariance kernel or the kind of this vector,

22:58.120 --> 23:00.920
this infinite vector of the distributions?

23:00.920 --> 23:06.840
So the covariance kernel defines that infinite vector or that functional distribution.

23:06.840 --> 23:08.920
So there's two ways to think about Gaussian processing.

23:08.920 --> 23:13.920
So your covariance kernel is infinite by infinite dimensions or something on that order?

23:13.920 --> 23:20.440
Or I mean, it can, how do you, is part of the goal to kind of constrain the dimensionality

23:20.440 --> 23:22.360
of this covariance kernel?

23:22.360 --> 23:26.760
So the covariance kernel itself will take in inputs in the configuration space and basically

23:26.760 --> 23:30.800
say how much covariance can I expect between these two points.

23:30.800 --> 23:33.320
So it does map into a real number.

23:33.320 --> 23:38.280
Technically for various types of covariance kernels, there are these tunable parameters

23:38.280 --> 23:39.720
that are continuous.

23:39.720 --> 23:44.320
So like technically, yes, there's an infinite number of different ways you can parameterize

23:44.320 --> 23:45.640
that.

23:45.640 --> 23:51.920
But what we're able to do is say, given what we've observed so far, what's the most likely

23:51.920 --> 23:57.800
parameterization or what's a distribution of likely parameterizations and leverage that

23:57.800 --> 24:00.760
to decide, okay, this is what we think is a.

24:00.760 --> 24:04.920
A reasonable surrogate function and then once again, do that across a wide variety of

24:04.920 --> 24:05.920
them.

24:05.920 --> 24:06.920
Okay.

24:06.920 --> 24:12.400
I'm still not fully getting where the infinite distributions come in.

24:12.400 --> 24:13.400
Yeah.

24:13.400 --> 24:15.360
So there's two ways to think about a Gaussian process.

24:15.360 --> 24:17.200
One is from the point wise perspective.

24:17.200 --> 24:22.520
And so the idea is at every single point, we're going to assume the response from this underlying

24:22.520 --> 24:26.280
system that we're sampling is going to be Gaussian distribution.

24:26.280 --> 24:31.440
But every single potential configuration has a different potential Gaussian response to

24:31.440 --> 24:32.440
it.

24:32.440 --> 24:34.040
So there's some mean in this.

24:34.040 --> 24:41.200
So you've got an input point and then you've got the space of configurations and each

24:41.200 --> 24:47.920
of those configurations translates this input point to a different distribution.

24:47.920 --> 24:51.040
So the input point is a potential configuration.

24:51.040 --> 24:55.120
So maybe I'll take a step back and do an example here.

24:55.120 --> 24:59.440
So let's say we're tuning some neural network and we want to find the optimal learning

24:59.440 --> 25:00.800
rate.

25:00.800 --> 25:06.120
So maybe initially we try something like just point five or something like that and we

25:06.120 --> 25:07.120
get a response back.

25:07.120 --> 25:08.120
Okay.

25:08.120 --> 25:11.400
And we're optimizing for the accuracy of a fraud detection pipeline.

25:11.400 --> 25:16.720
And so we're like, okay, we get.7 cross validated AUC that looks all right.

25:16.720 --> 25:22.920
So the thing that we're optimizing for is our learning rate and the input is, you know,

25:22.920 --> 25:26.600
we're not talking about inputs to our neural network and our neural network.

25:26.600 --> 25:28.720
We're talking about an aggregate, the error.

25:28.720 --> 25:33.600
Well, so the inputs are, we're going to be tuning this machine learning pipeline.

25:33.600 --> 25:38.360
And so at this high like meta optimization layer, we're going to be saying, okay, we're

25:38.360 --> 25:41.840
going to put in a learning rate and then we're going to go through the training and cross

25:41.840 --> 25:46.360
validation and all sorts of things and come up with some metric that we care about.

25:46.360 --> 25:48.960
So maybe cross validated AUC.

25:48.960 --> 25:53.920
And our goal is to find the learning rate that tunes this entire pipeline in such a way

25:53.920 --> 25:57.080
that it maximizes that output.

25:57.080 --> 26:02.400
And so the way that this works in the sequential model based optimization framework is, okay,

26:02.400 --> 26:07.440
so we sampled.5 learning rate and got.7 out as the result.

26:07.440 --> 26:10.720
And maybe there's a little bit of uncertainty associated with that.

26:10.720 --> 26:16.240
So then let's say we want to model what we think is going to happen if we try.6.

26:16.240 --> 26:19.720
So we have a little bit of information because we've already sampled.5.

26:19.720 --> 26:25.440
So what we do is we build up this Gaussian process that says, okay, I'm pretty sure that

26:25.440 --> 26:29.840
it's going to pass near this point that I've already sampled, but then maybe the information

26:29.840 --> 26:32.040
decays pretty rapidly.

26:32.040 --> 26:40.040
So I expect to see maybe.6 plus or minus.1 if I were to sample a point further away

26:40.040 --> 26:41.520
from it.

26:41.520 --> 26:47.600
And what you can think of is every potential input learning rate to tune this pipeline

26:47.600 --> 26:51.200
has its own Gaussian response that we're expecting.

26:51.200 --> 26:54.360
Has its own mean, it has its own variance.

26:54.360 --> 26:59.880
And so we can explicitly build that up once we define the covariance kernel.

26:59.880 --> 27:02.520
And of course, as you expand this out into more dimensions.

27:02.520 --> 27:07.080
And so in this example, we're talking about what does the covariance kernel look like?

27:07.080 --> 27:08.080
Yeah.

27:08.080 --> 27:13.200
So we would explicitly set a covariance kernel like an ARD kernel that says, okay, we're

27:13.200 --> 27:19.640
expecting some sort of like squared exponential decay of this kind of information from sampling

27:19.640 --> 27:20.840
these different points.

27:20.840 --> 27:25.360
And so is the covariance kernel, again, in this particular case, it's going to be, it's

27:25.360 --> 27:31.440
going to describe the relationship between the learning rate and the output.

27:31.440 --> 27:36.760
So it's going to describe the relationship between like individual samples of that learning

27:36.760 --> 27:37.760
rate.

27:37.760 --> 27:44.440
So let's set that very where we expect wildly different results after 0.01 increments.

27:44.440 --> 27:46.320
Or is it 0.1 increments?

27:46.320 --> 27:51.840
Do we expect it to be an extremely noisy response, or do we expect it to be fairly well behaved?

27:51.840 --> 27:55.600
There's various different parameters of this covariance kernel that basically say, how

27:55.600 --> 28:02.960
much information effectively do I get after sampling point A about some other point B?

28:02.960 --> 28:10.800
Is the dimensionality of the covariance kernel fixed when we start or does it increase in

28:10.800 --> 28:13.360
dimensionality as we sample?

28:13.360 --> 28:18.000
So it takes in the input, which is the actual configurations.

28:18.000 --> 28:21.640
So in this case, it would just be a one-dimensional, just the learning rate, but you can imagine

28:21.640 --> 28:23.200
us extending this out.

28:23.200 --> 28:28.240
So it takes in a vector, which is a specific configuration, or two vectors actually, and

28:28.240 --> 28:34.640
says, OK, how much covariance is there between these two points, these two potential configurations?

28:34.640 --> 28:39.400
That being said, you can parameterize that covariance kernel in different ways, depending

28:39.400 --> 28:42.400
on which specific kernel you've picked.

28:42.400 --> 28:47.280
So in something like an ARD kernel, which is the squared exponential drop off, there's

28:47.280 --> 28:49.520
various length scales that you can tune.

28:49.520 --> 28:52.080
So maybe we know how many of these drop off is that kind of?

28:52.080 --> 28:56.680
Yeah, does it vary over 0.1, but then something like the number of hidden layers might vary

28:56.680 --> 28:59.000
over orders of magnitude larger.

28:59.000 --> 29:04.920
So like 100 hidden layers is very similar to 101, but very different than 200.

29:04.920 --> 29:12.720
I'm so not sure that I'm very clear on the kernel, in this specific example, the dimensionality

29:12.720 --> 29:19.400
of the kernel is 1 by 1, like is it a scalar, or is it takes in a single value, so that's

29:19.400 --> 29:21.120
just the learning rate.

29:21.120 --> 29:28.200
So I guess I'm thinking of it as a matrix.

29:28.200 --> 29:31.880
Is it a function, or is it something, or should I not be thinking of it as a matrix?

29:31.880 --> 29:37.240
So you can define it as a matrix, where it's every point, the pairwise covariance of

29:37.240 --> 29:39.600
every point you've sampled so far.

29:39.600 --> 29:40.600
Right.

29:40.600 --> 29:43.920
So as you sample the dimensionality of this thing is growing.

29:43.920 --> 29:48.560
Of the underlying covariance matrix, but the underlying covariance function is just a function,

29:48.560 --> 29:51.080
so there's no kind of dimensionality associated with it.

29:51.080 --> 29:52.080
Okay.

29:52.080 --> 29:57.520
So it's basically, if I've sampled 10 different points, then I could have a 10 by 10 matrix,

29:57.520 --> 30:04.360
which is the covariance matrix, where every single actual instance inside that matrix is,

30:04.360 --> 30:08.960
how does 0.7 covariance with 0.3, or whatever it may be?

30:08.960 --> 30:16.200
And this, as a whole, helps us define the Gaussian process, which then gives us this

30:16.200 --> 30:21.400
sarcastic, surrogate function for what we think is going to happen if we sample outside

30:21.400 --> 30:23.840
of the points that we've already explicitly observed.

30:23.840 --> 30:24.840
Okay.

30:24.840 --> 30:28.280
And it does that by way of defining the kernel.

30:28.280 --> 30:32.280
So how do we get from the kernel, from the matrix to the kernel?

30:32.280 --> 30:33.280
Is that done explicitly?

30:33.280 --> 30:34.280
Yeah, the way around.

30:34.280 --> 30:37.760
So you start with a kernel, and then the kernel defines the matrix.

30:37.760 --> 30:44.320
So every single individual value within that matrix is defined as, I got it.

30:44.320 --> 30:49.080
So we're specifying the kernel, in this case, you said ADR is ARD.

30:49.080 --> 30:52.080
So it's the, what is ARD sample?

30:52.080 --> 30:53.080
I said that.

30:53.080 --> 30:54.080
Blinking on that, all right.

30:54.080 --> 30:55.080
Okay.

30:55.080 --> 30:56.240
But it's the squared Gaussian falloff.

30:56.240 --> 30:57.240
Yeah.

30:57.240 --> 30:58.240
Yeah.

30:58.240 --> 31:04.120
So what's now unclear for me is if you've picked a sample in your input space, and you've

31:04.120 --> 31:11.200
run your underlying process, and you have an output value from that sample, is the covariance

31:11.200 --> 31:15.840
kernel used to build up like what you expected to see, and then you push that all through

31:15.840 --> 31:18.400
and you get what you actually saw.

31:18.400 --> 31:21.240
And then you can update the covariance kernel.

31:21.240 --> 31:25.760
And then that covariance matrix gets one more row and one more column.

31:25.760 --> 31:32.720
Because now we have how this new point varies with all of the previously observed points.

31:32.720 --> 31:35.920
And then we can use that to update our Gaussian process.

31:35.920 --> 31:43.840
And then we have this new posterior result that we can use to decide what we sample next.

31:43.840 --> 31:47.920
And what we're doing is we're not just kind of doing naive optimization on that Gaussian

31:47.920 --> 31:49.400
process response itself.

31:49.400 --> 31:53.240
We don't just want to find the point with highest mean or something like that.

31:53.240 --> 31:58.800
What we want to do is apply an acquisition function to it and say, given this is what

31:58.800 --> 32:04.120
I think is going to happen if I sample any of these potential input points, how do I find

32:04.120 --> 32:09.160
the point with the highest expected improvement or the highest probability of improvement?

32:09.160 --> 32:13.720
Or which one's going to give me the most knowledge about the eventual optimise, the knowledge

32:13.720 --> 32:15.320
gradient method?

32:15.320 --> 32:19.240
And so acquisition function is the new term that you just introduced.

32:19.240 --> 32:25.640
Is that something that is model based like the covariance kernel is model based on this

32:25.640 --> 32:26.640
ADR?

32:26.640 --> 32:30.080
Do you pick a model that you use for your acquisition function as well?

32:30.080 --> 32:31.080
Yeah.

32:31.080 --> 32:35.840
This is the optimization part of, so the sequential part of sequential model based optimization

32:35.840 --> 32:39.400
is leveraging the history to build up these surrogate models.

32:39.400 --> 32:42.240
The covariance kernel is keeping it updated and all that stuff.

32:42.240 --> 32:47.520
The model based part is actually deciding, okay, this is what we think the response is

32:47.520 --> 32:51.000
going to be in these unsampled configurations.

32:51.000 --> 32:52.480
So that's the Gaussian process.

32:52.480 --> 32:59.320
Then the optimization component is given that surrogate model, what do we actually optimise

32:59.320 --> 33:03.320
for sampling next, before we repeat this entire process?

33:03.320 --> 33:10.320
And so that particular piece is really focused on, you know, you've got this massive potential

33:10.320 --> 33:15.840
state space for your hyperparameters, you know, how do we, how do we choose a sample

33:15.840 --> 33:21.080
path through the hyperparameter space that minimises basically wasting time and not adding

33:21.080 --> 33:22.600
information to-

33:22.600 --> 33:23.600
Exactly.

33:23.600 --> 33:27.520
And this is what really controls that Explorer exploit trade off.

33:27.520 --> 33:32.680
So a popular acquisition function is expected improvement, and that is basically how much

33:32.680 --> 33:37.080
do I think I'm going to beat the best thing I've seen so far by?

33:37.080 --> 33:42.000
So if I've seen a pretty good AUC in my fraud detection pipeline, now all of a sudden

33:42.000 --> 33:44.920
I want to be able to do it as well as possible beyond that.

33:44.920 --> 33:47.560
We're playing King of the Hill effectively.

33:47.560 --> 33:52.080
Another popular one that's kind of maybe a little bit more intuitive to grasp is probability

33:52.080 --> 33:53.240
of improvement.

33:53.240 --> 33:57.480
If I were to sample this unsampled point, what's the probability that I beat the best

33:57.480 --> 33:59.440
thing I've seen so far?

33:59.440 --> 34:04.640
And so these have different exploration, exploitation trade-offs, you know, so far as probability

34:04.640 --> 34:08.800
improvement might be a little bit more conservative, like we're going to kind of keep edging it

34:08.800 --> 34:13.960
up slowly, whereas expected improvement kind of takes the magnitude of the gain into account.

34:13.960 --> 34:17.600
So it might try something far away because it thinks there could be something great that

34:17.600 --> 34:19.120
it has just never seen before.

34:19.120 --> 34:20.120
Yeah, yeah.

34:20.120 --> 34:23.200
And are there other common examples?

34:23.200 --> 34:24.200
Yeah.

34:24.200 --> 34:28.160
And unfortunately, they get a little bit more complicated to internalize, but another

34:28.160 --> 34:29.840
popular one is Knowledge Gradient.

34:29.840 --> 34:33.560
This is what my PhD advisor worked on during his PhD.

34:33.560 --> 34:34.560
The idea is...

34:34.560 --> 34:38.520
I'm imagining from the name like that's kind of based on information theory and like how

34:38.520 --> 34:40.880
much we're going to learn by checking this point.

34:40.880 --> 34:41.880
Exactly.

34:41.880 --> 34:46.760
And the goal is to learn as much as we can about that eventual best point.

34:46.760 --> 34:50.040
And so there's more information theoretic acquisition function.

34:50.040 --> 34:55.200
And then you can kind of define anything that you want with the goal of eventually getting

34:55.200 --> 34:56.200
to this best one.

34:56.200 --> 35:00.040
So these are probably the three most popular, but you could imagine doing composites of this

35:00.040 --> 35:03.880
or some sort of like upper confidence bound based acquisition function.

35:03.880 --> 35:09.480
And the idea is you want it to as efficiently as possible trade off exploration and exploitation

35:09.480 --> 35:13.200
because learning about that underlying system and how it performs and things like that's

35:13.200 --> 35:14.200
important.

35:14.200 --> 35:16.840
But at the end of the day, you just want the best performing model.

35:16.840 --> 35:17.840
Yeah.

35:17.840 --> 35:22.840
Yeah, I think turtles all the way down strikes me as app like you've got hyperparameters

35:22.840 --> 35:23.840
for your model.

35:23.840 --> 35:26.360
You've got hyperparameters for your pipeline.

35:26.360 --> 35:30.200
And then you've got hyperparameters for your optimization system.

35:30.200 --> 35:31.200
Yeah.

35:31.200 --> 35:37.200
And presumably, I'm imagining that you are also trying to optimize the hyperparameters at

35:37.200 --> 35:40.760
that top layer for your optimization system as well.

35:40.760 --> 35:46.480
And this is exactly why Stegopt exists because there's some incredible research out there.

35:46.480 --> 35:50.440
A lot of members of our team have contributed to the academic research and a lot of the

35:50.440 --> 35:51.920
open source out there.

35:51.920 --> 35:54.800
There's a lot of promise that Bayesian optimization has.

35:54.800 --> 36:00.320
But unfortunately, a lot of expert time is wasted optimizing the optimizer, figuring out

36:00.320 --> 36:03.840
the best way to tune all of these turtles all the way down.

36:03.840 --> 36:07.200
And I think that's one of the places where at least the open source that I released to

36:07.200 --> 36:11.360
the metric optimization engine, even though it was very popular on GitHub, it kind of failed

36:11.360 --> 36:16.040
to deliver on that promise because it required an expert to sit and fine tune all these

36:16.040 --> 36:17.280
different things.

36:17.280 --> 36:22.240
So the goal of a company like Stegopt is, can we optimize the optimizer for you and create

36:22.240 --> 36:27.440
this automatic ensemble that makes all of these trade-offs so that you as an expert can

36:27.440 --> 36:33.800
focus on fraud detection and we'll focus on black box optimization for you.

36:33.800 --> 36:40.320
And so we've described a bunch of different kind of variants in this process.

36:40.320 --> 36:47.600
Are there specific, you know, invariants for Stegopt in your process like, you know,

36:47.600 --> 36:50.640
like for example, you know, basing everything on a Bayesian process.

36:50.640 --> 36:58.240
That's one way of doing this is the product based around that and what other kind of

36:58.240 --> 37:00.800
invariants are there in the way you approach this.

37:00.800 --> 37:01.800
Yeah.

37:01.800 --> 37:04.760
So at the very highest level, we're just black box optimization.

37:04.760 --> 37:06.440
So there's inputs to a system.

37:06.440 --> 37:08.880
There's an output or set of outputs that we want to optimize.

37:08.880 --> 37:12.200
And we're going to try to come up with the best set of inputs.

37:12.200 --> 37:17.480
So Bayesian optimization is an extremely efficient way to do this, especially when it's time-consuming

37:17.480 --> 37:19.840
and expensive to sample that underlying system.

37:19.840 --> 37:23.480
There's lots of different variants of Bayesian optimization.

37:23.480 --> 37:27.080
So instead of using like a Gaussian process, we could use a Bayesian neural network for

37:27.080 --> 37:29.200
the underlying circuit function.

37:29.200 --> 37:33.560
Instead of using Bayesian optimization, we could use a genetic algorithm or particle swarm

37:33.560 --> 37:38.600
or simulated annealing or even just a convex gradient based method.

37:38.600 --> 37:44.760
The idea being, SIGUP takes care of that optimization of the optimizer and automatically

37:44.760 --> 37:46.880
selects the best one for you.

37:46.880 --> 37:51.920
Most of our methods or almost all of our methods are Bayesian in nature, but we're not constrained

37:51.920 --> 37:52.920
to that necessarily.

37:52.920 --> 37:53.920
Yeah.

37:53.920 --> 37:55.920
I guess that was the question that I was trying to get at.

37:55.920 --> 37:57.720
How far do you go?

37:57.720 --> 38:07.640
Do you also now or envision a future where, because you're providing this black box capability,

38:07.640 --> 38:16.000
you may do the Bayesian optimization, but also sample or test the results that you get

38:16.000 --> 38:20.120
from particle swarms and other types of methods.

38:20.120 --> 38:21.120
Definitely.

38:21.120 --> 38:26.800
In-house, we built this very robust evaluation framework for deciding whether or not specific

38:26.800 --> 38:29.640
algorithms fare well in different contexts.

38:29.640 --> 38:33.560
This is what we use when we integrate a new paper and want to make sure that with high

38:33.560 --> 38:37.600
statistical confidence, it actually outperforms what we're currently doing.

38:37.600 --> 38:42.960
We use this as our internal metric for deciding what to do.

38:42.960 --> 38:44.760
But we're agnostic to the underlying methods.

38:44.760 --> 38:47.520
We just want the best possible thing for our customers.

38:47.520 --> 38:51.960
It turns out for the types of problems that we're attacking, Bayesian optimization is an incredibly

38:51.960 --> 38:57.320
good fit and it's underutilized because it's so difficult to get up and running and

38:57.320 --> 39:01.960
optimized, but we have and will continue to employ whatever the best method is for the

39:01.960 --> 39:03.920
problems that we're attacking.

39:03.920 --> 39:09.280
Because we define this barrier in this way where it's just black box optimization, the underlying

39:09.280 --> 39:14.640
system is a black box to us, but we're also a black box to our customers.

39:14.640 --> 39:20.040
This allows us to hotswap in the best possible technique to solve their problem and not be

39:20.040 --> 39:21.040
constrained in that way.

39:21.040 --> 39:22.040
Okay.

39:22.040 --> 39:23.040
Okay.

39:23.040 --> 39:24.040
Cool.

39:24.040 --> 39:26.440
Can you talk a little bit about the model evaluation framework that you built?

39:26.440 --> 39:27.440
Yeah.

39:27.440 --> 39:31.480
There's some I-C-Mail workshop papers from 2016 that go into quite a bit more detail

39:31.480 --> 39:36.400
and are available on our website, but the idea is, I've just told you that we have an

39:36.400 --> 39:41.000
optimization framework that can solve any underlying black box function.

39:41.000 --> 39:45.240
The first response should be, I don't know whether or not it's working, so internally

39:45.240 --> 39:50.000
we built up the system where traditionally to publish papers, and I'm guilty of doing

39:50.000 --> 39:56.000
this, is you would come up with some strategy, pick three to six of your favorite functions,

39:56.000 --> 40:00.240
show that you can outperform some specific techniques on those functions, publish a paper,

40:00.240 --> 40:01.560
rinse and repeat.

40:01.560 --> 40:05.320
So when we built this up internally, we took the superset of all of those different functions

40:05.320 --> 40:06.840
from the academic literature.

40:06.840 --> 40:09.480
We took functions that look similar to our customer's data.

40:09.480 --> 40:13.840
We took a bunch of open machine learning data sets and strategies, and we basically piled

40:13.840 --> 40:14.920
them all together.

40:14.920 --> 40:18.680
So instead of comparing against three or four different response surfaces, now we're

40:18.680 --> 40:21.120
looking at hundreds or thousands of them.

40:21.120 --> 40:25.040
In addition to that, we wanted to make sure against all of these different open source

40:25.040 --> 40:29.520
methods and against all of these other kind of different global optimization strategies

40:29.520 --> 40:32.840
that we could very robustly outperform them.

40:32.840 --> 40:38.520
So what we do in the internal evaluation framework is we independently optimize these hundreds

40:38.520 --> 40:45.440
of different pathological and real world problems, many times with sigups and many times with

40:45.440 --> 40:49.480
another method, and that other method might be just a new version of sigopt.

40:49.480 --> 40:54.240
And then with high statistical confidence, we can say, which one got to the best value

40:54.240 --> 40:59.880
fastest, which one got to the ultimate best result, which one was the most robust, so it

40:59.880 --> 41:04.560
didn't have an inter-core tile ranges or all above a specific value.

41:04.560 --> 41:08.560
It sounds like to draw an analogy from software engineering, you've built a regression testing

41:08.560 --> 41:10.520
framework for optimizer.

41:10.520 --> 41:15.120
Yeah, so we do use it for regression testing, it's run nightly, but it's also a way to

41:15.120 --> 41:17.720
basically AB test optimizers as well.

41:17.720 --> 41:18.720
Right.

41:18.720 --> 41:28.040
And not using it to, or to what extent are you using it to inform model choices, or I

41:28.040 --> 41:32.400
guess the, you know, what I'm struggling a little bit with is, you know, so you've got

41:32.400 --> 41:37.480
this, you've got this, you know, this heap of data sets and functions and things like

41:37.480 --> 41:38.480
that.

41:38.480 --> 41:46.120
And if you were trying to optimize across all of those, then you've got a least common

41:46.120 --> 41:50.680
denominator kind of problem, right, or in a local maxima or something like that.

41:50.680 --> 41:51.680
Yeah.

41:51.680 --> 41:54.440
So we do have to be wary that we don't overfit to this data set.

41:54.440 --> 41:55.640
That's definitely true.

41:55.640 --> 42:00.440
One thing that we found though is the reason why we built an ensemble based approach.

42:00.440 --> 42:04.440
So let me just, just poke at that, like I'm not sure, is overfitting the right word for

42:04.440 --> 42:10.360
what I'm thinking of is, is that, you know, some of it strikes me as the opposite of overfitting,

42:10.360 --> 42:13.520
whereas like if I were to just look at, I don't really care about all this other data,

42:13.520 --> 42:19.440
I care about my problem, like if you're optimizing for this kind of broad spectrum and I can,

42:19.440 --> 42:24.240
you know, outperform you by just focusing on my problem, you know, I'd probably do that.

42:24.240 --> 42:25.240
Yeah.

42:25.240 --> 42:26.240
That makes it complete sense.

42:26.240 --> 42:27.080
I see where you're coming at here.

42:27.080 --> 42:31.120
So this is why we take this ensemble based approach, because it turns out like the most

42:31.120 --> 42:35.840
popular approach to invasion optimization, like Gaussian processes with aarity kernel with

42:35.840 --> 42:41.360
expected improvement actually doesn't do super well in a wide variety of different contexts.

42:41.360 --> 42:46.080
So by slotting in the right tool for the job, we can actually hit all of these different

42:46.080 --> 42:49.760
facets of different types of problems extremely well.

42:49.760 --> 42:54.320
That being said, the no free lunch theorem and computer science still applies here in

42:54.320 --> 42:58.960
so far as if you do have expert knowledge about your underlying system and you build a bespoke

42:58.960 --> 43:04.760
optimizer to solve that one specific problem, you are going to outperform a general technique.

43:04.760 --> 43:09.000
That being said, you would have to repeat that for the next problem that you attack and

43:09.000 --> 43:10.520
the next one and the next one.

43:10.520 --> 43:15.600
And so the idea is by having an ensemble of different optimizers, we use the right one

43:15.600 --> 43:20.000
for specific contexts and then a different one for a different context, et cetera.

43:20.000 --> 43:23.760
So instead of having like the lowest common denominator, like you said, just the one size

43:23.760 --> 43:29.600
fits all, what we're doing is actually putting in the right tool and automatically learning

43:29.600 --> 43:31.000
when we trade it off.

43:31.000 --> 43:34.840
So when you're tuning a gradient boosted method, you're getting the right tool, but when

43:34.840 --> 43:39.040
you tune a neural network, it's still the same API and stay interface, but you're getting

43:39.040 --> 43:40.720
the right optimizer out of it.

43:40.720 --> 43:45.680
So what I'm hearing is in response to my question, like a little bit of both, right?

43:45.680 --> 43:50.680
Like you're, you've built this model evaluation framework because fundamentally you're not

43:50.680 --> 43:57.080
necessarily trying to outperform a handcrafted model that 50 PhDs has been five years developing

43:57.080 --> 44:05.040
whatever you're trying to build a system that can deliver good performance on, you know,

44:05.040 --> 44:07.040
in general what someone throws at it.

44:07.040 --> 44:11.120
And so you want to test it against a bunch of, you know, hey, these are things that someone

44:11.120 --> 44:15.840
might throw at it and make sure that you get good performance.

44:15.840 --> 44:21.600
And the way that you do that is under the covers, you're not just relying on, you know,

44:21.600 --> 44:26.440
one specific set of choices, but you're taking an ensemble approach and your optimizer

44:26.440 --> 44:34.120
can swap in and out different decisions to produce a result that that's best.

44:34.120 --> 44:38.640
That's exactly it because what we find more often than not is that people don't assign

44:38.640 --> 44:44.480
50 PhDs for five years for every single optimization problem that they have more often than not.

44:44.480 --> 44:49.640
They're using grid search, random search, manual tuning, maybe an open source solution,

44:49.640 --> 44:53.760
maybe they have part of their team part time working on an internal optimizer or something

44:53.760 --> 44:54.760
like that.

44:54.760 --> 44:57.400
And those are the things that we can vastly outperform.

44:57.400 --> 45:01.160
If you know it's convex and you have gradient information and you have a bunch of expert knowledge,

45:01.160 --> 45:05.280
like there is specific tools that you can use to get there, and this is probably a little

45:05.280 --> 45:09.720
heavy handed to use in that situation, but more often than not what we're doing is we're

45:09.720 --> 45:15.800
coming and replacing these very exhaustive, very expensive, very domain expert intensive

45:15.800 --> 45:20.000
systems, and we can generally outperform those to a high degree.

45:20.000 --> 45:24.600
Yeah, now often like to think of the the tool space in general is like there's, you know,

45:24.600 --> 45:31.920
for many enterprises, there's such a huge potential opportunity to apply ML that their

45:31.920 --> 45:36.640
ability to staff up, you know, is far outpaced by the opportunity.

45:36.640 --> 45:41.160
So at a given staffing level, like you've got this choice, you can either like, you know,

45:41.160 --> 45:47.000
take only the biggest opportunity and apply all your resources to that in a very manual

45:47.000 --> 45:54.080
way, or you can, you know, utilize tools that allow folks to be more effective and

45:54.080 --> 45:59.280
light off some of these, you know, some of the, you know, it's like the, a lot of,

45:59.280 --> 46:02.360
you know, I'll talk to folks and they'll talk about it like we only go after home runs

46:02.360 --> 46:08.200
versus, you know, base hits, right, and this, it sounds like this is a tool for allowing

46:08.200 --> 46:14.560
people to, you know, both go after home runs as well as try to increase their hit rate

46:14.560 --> 46:15.560
for bases.

46:15.560 --> 46:16.560
Definitely.

46:16.560 --> 46:19.360
And what we find with a lot of the firms that we work with is how they differentiate

46:19.360 --> 46:24.240
themselves from their competitors is not by black box Bayesian optimization.

46:24.240 --> 46:28.480
It's by creating a great recommendation engine or a great algorithmic trading strategy.

46:28.480 --> 46:34.240
And if you can hire five more PhDs to work on that core differentiator or free up five

46:34.240 --> 46:39.560
PhDs to do that, and then just use SIGOP to tune it, they work very additively and hand

46:39.560 --> 46:44.600
in hand, we can accelerate that time to market, accelerate the results, getting to the best

46:44.600 --> 46:46.760
performance and all of these different things.

46:46.760 --> 46:51.440
And I think more and more companies are becoming aware of this and using the right tool for

46:51.440 --> 46:52.440
the job.

46:52.440 --> 46:54.640
Why rewrite TensorFlow when you can use it?

46:54.640 --> 46:59.600
Why write your own Bayesian optimizer when you can use a best in class, easy, rest API?

46:59.600 --> 47:00.600
Awesome.

47:00.600 --> 47:01.600
Awesome.

47:01.600 --> 47:03.440
So what's the, what's the best way for folks to learn more?

47:03.440 --> 47:04.440
I'm assuming the website.

47:04.440 --> 47:05.440
Yep.

47:05.440 --> 47:10.760
SIGOP.com or just contact at SIGOP.com if you want to shoot us an email, we run a complimentary

47:10.760 --> 47:14.520
proof of concept pilot like we can throw these peer reviewed papers at you to prove that

47:14.520 --> 47:18.120
we're as good as we say we are, but at the end of the day, we want to prove it with their

47:18.120 --> 47:20.240
underlying models themselves.

47:20.240 --> 47:25.280
So we can work with any enterprise, any underlying system, cloud agnostic, model agnostic.

47:25.280 --> 47:28.800
It's also free for students, so if there are any people at universities or researchers

47:28.800 --> 47:33.280
at national labs or whatever it is listening to the podcast, SIGOP.com slash EDU gets you

47:33.280 --> 47:34.680
a free enterprise account.

47:34.680 --> 47:38.920
I wasted way too much of my PhDs on this problem, don't want to do that for anybody else.

47:38.920 --> 47:44.040
And what about for folks that are interested in learning about the theoretical foundations

47:44.040 --> 47:45.040
of the work?

47:45.040 --> 47:46.040
Where would you point them?

47:46.040 --> 47:48.840
Are there like three canonical papers or something like that that they should look

47:48.840 --> 47:49.840
for?

47:49.840 --> 47:50.840
Yeah.

47:50.840 --> 47:53.760
So if you go to SIGOP.com slash research, those all of our papers, we also have a Bayesian

47:53.760 --> 47:57.600
optimization primer there that kind of goes into more detail about some of the things

47:57.600 --> 48:01.480
I said verbally sometimes is a little bit hard to describe the action processes and things

48:01.480 --> 48:02.480
like that.

48:02.480 --> 48:03.480
The math is there.

48:03.480 --> 48:07.320
There's references for all those papers as well, so that can kind of take you down the

48:07.320 --> 48:10.320
rabbit hole of all the different ways that this has been applied to the story.

48:10.320 --> 48:11.320
Okay.

48:11.320 --> 48:12.320
Awesome.

48:12.320 --> 48:15.120
Glad it's been a great conversation and I've learned a ton.

48:15.120 --> 48:16.120
Excellent.

48:16.120 --> 48:17.120
Thank you so much.

48:17.120 --> 48:18.120
I really appreciate it.

48:18.120 --> 48:19.120
Thanks.

48:19.120 --> 48:23.080
All right, everyone.

48:23.080 --> 48:25.520
That's our show for today.

48:25.520 --> 48:31.920
Thank you so much for listening and of course, for your continued feedback and support.

48:31.920 --> 48:38.120
For more information on Scott and the topics covered in this episode, head on over to twimmolaii.com

48:38.120 --> 48:41.240
slash talk slash 50.

48:41.240 --> 48:46.240
This week on Tuesday and Wednesday, October 3rd and 4th, I'll be at the Gartner Symposium

48:46.240 --> 48:51.120
in Orlando, where I'll be on a panel on how to get started with AI.

48:51.120 --> 48:55.080
If you'd like to meet up there, please send me a shout.

48:55.080 --> 49:00.080
The following week, I'll be in Montreal for the rework, deep learning summit, and hope

49:00.080 --> 49:03.320
to be joined by at least one lucky listener.

49:03.320 --> 49:08.080
Remember to visit twimmolaii.com slash dl summit to enter.

49:08.080 --> 49:12.040
It ends at noon central on October 4th.

49:12.040 --> 49:40.480
Thanks again for listening and catch you next time.

