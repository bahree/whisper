Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Before we get going, I'd like to send a huge thanks to our friends at HPE for sponsoring
this week's series of shows from the O'Reilly AI Conference in New York City.
At the conference, HPE presented on InfoSight, which is the company's cloud-based AI ops
solution for helping IT organizations better manage and ensure the health of their IT infrastructure
using AI.
I've previously written about AI ops and it's definitely an interesting use case for machine
learning.
To check out what HPE InfoSight is up to in this space, visit twimbleai.com slash HPE.
All right everyone, I am on the line with Dellip Rao.
Dellip is the vice president of research at the AI foundation.
Previously, he founded the research consulting company Juiceware and the fake news challenge
as well.
He's one of my favorite AI people to follow on Twitter where he tweets him at Dellip Rao.
Dellip, welcome to this week in machine learning and AI.
Thank you for having me.
I am really looking forward to our conversation.
You recently joined the AI foundation where again, you'll be leading at research.
Tell us a little bit about your background and what sparked your interest in AI in a
particular natural language processing.
Yeah, absolutely.
So, my background is in research, particularly in natural language processing research.
And I grew up in India and did typical engineering track that a lot of kids in India do.
And I was pretty sure I was going to become some kind of an academic with very early on
in my career, in my college life and because I found teaching very inspiring and I really
wanted to be a teacher.
And it was not AI, it was not my choice.
In the beginning, I actually wanted to become, to do research and distributed computing.
And I realized that, you know, there was this one school where I went to IT Madras and
this one, there was a faculty who was working on distributed computing and he was on the
sabbatical.
They kind of put me in charge in care of another faculty who was working on AI and this
person was supposed to babysit me while the actual professor, you know, returned back
from sabbatical and he did a very dangerous thing.
So, he gave me a couple of books, thinking that, you know, it'll keep me busy.
And I just got really hooked.
So, after reading this couple of books, I just felt like, okay, this is what I should
be working on and sort of distributed computing.
And that's when I started working on AI.
And this person was actually like a very hard core, like old school AI person, like we
were working on like planning problems and so on.
So I actually started looking into AI planning actually to begin with and actually looking
at search algorithms and so on.
And it was only like around that time, I just got like curious about natural language
processing because the data that we were working with involved text and natural language
processing was supposed to be some kind of like pre-processing I was supposed to be doing,
but instead I found the pre-processing itself super interesting.
So, I kind of moved into natural language processing and then I went to graduate school
the class of that, like, you know, I felt like I had to study more in AI and this is all
ancient history, by the way.
So, do you happen to remember the name of the books that your babysitter professor gave
you to read?
Yeah, I mean, I think one of them was actually the book by Nilton, who recently passed
away, like I think that's the end of yesterday or day before.
And yeah, he was a professor at Stanford and he wrote one of these very early AI text
books, and then there was also the classic book by Peter Narveg, a story, a Russell and
Narveg, right?
And so, yeah, I started off with those two books.
The Nilton, Nilton book is like a really great gateway drug for anyone, I like men.
And then, of course, the Peter Narveg book, you know, puts you on a more serious route.
And so, you, I think we veered off at grad school.
Did you start juiced where right after grad school?
Now, far from it, actually, I worked at a bunch of different places, you know, I worked
at Google Research as a kind of like a repeat intern, and then I went to Twitter, and at
that time, Twitter was pretty early, and it was way before that IPO.
I was like, you know, employee number 500 something, and yeah, I was one of the first machine
learning hires to join their anti-spam team.
And they were that until that point, and they just formed this anti-spam team.
When I joined, like, you know, a few months ago, and they were writing all sorts of interesting
rules to catch them, and I was like, oh, no, that's not how it should be done, but, but
actually, you know, I got, I got schooled because I felt like, oh, I came in the wild
saying, thinking that here, I, here I am with all my training, I'm going to go change
the way things work.
But then, that's when my first humbling experience happened.
I felt I discovered that, you know, real products don't work that way.
You can't build, like, simple models that will, like, that you can just unleash it on
the world and expect it to work.
And they were doing a lot of things coming from deep experience, product experience.
And that's when I kind of, like, completely shifted from my academic researcher mindset
to a more product-based researcher mindset.
Are there some specific examples you can give of where that disconnect really fell for
you?
Oh, absolutely.
I mean, like, for example, like, data set drift is something that we kind of barely refer
to in academia, right?
And we, the model drift, model drift is something that we barely refer to in academia,
but it actually happens all the time in production.
Your model's stopping relevant, and you have to kind of, like, figure out, like, some kind
of a feedback loop to start, like, to keep retraining your models.
And even sometimes, you know, there is this, even you don't know much, there is a kind
of a cockiness that comes in where you say that, oh, you can build this model, you can
solve this easily by building a model.
But then you realize that, you know, getting the model done and building all the engineering
around the feedback loop for the model itself is so much more expensive than, like, you know,
sometimes even writing a few rules or writing something very simplistic, which, to a researcher
especially coming from an academic background, it almost feels like, oh, why are we doing
this?
Right?
Like, this is not science.
Why are we doing this?
And this is more importantly, I think, in academia as well as in research, we're always
encouraged to stay, you know, push down a lot of times, stay ahead of the game, right?
And when you see some heavily engineering systems built using rules, the natural feeling
comes into thinking that, oh, this is all, like, outdated, and this, and there is, it's
kind of very natural for somebody coming from academia to look at it and feel a little,
like, this is all outdated stuff.
But I think with experience, there is this enlightenment that creeps in, where you realize
that no, actually, that the so-called outdated stuff is actually, like, the most appropriate
thing to do in this context.
And that's when, you know, I kind of, it took me a few experiences like that, right?
And, of course, I mean, I gave you an example of model draft, another is, like, you know,
you build a model, the model does really well, then you're very proud of it, and then you
try, and then, you know, in a company like Twitter, everything gets, baby-tested, carefully,
and you end up also not just getting abtested, but also you're tested for how much compute
that your setup is taking, right?
Because you're operating at that massive scale.
And if a simple rule-based thing is accomplishing pretty much everything that your model thing
is doing, and your model is just adding, like, you know, a few decimal point improvement
or even, like, a percentage point improvement, it gets, it becomes a wash, but it comes
at an enormous cost to the business, right?
And so, it just does not make sense.
And as a researcher, you don't think about that, because you're always looking at, like,
oh, can I improve on state of the art, even if, as long as the improvements are statistically
significant, and I can publish, I can call it a win.
And that's where, like, you know, a disconnect happened for me, and, like, kind of, I felt,
like, oh, there is a different way to do science that is relevant in the real world, and
I want to practice that.
Have you developed that to the point where you've got some kind of fundamental tenets
of science in the real world, or is it more a general idea and approach?
Yeah, I did.
You know, some of it, I kind of, I alluded to in the book that we published, we can talk
about it later.
But, you know, I was going to say that I worked at Twitter, and then I worked at Amazon,
and Amazon was also another amazing experience, and it taught me a whole bunch of different
things on similar, along similar lines.
And I, after spending time at these two places, I started to use Twitter, which was a research
consulting company, and it was a research consulting company kind of built with that mindset,
right?
When you do science, that is, that will be baked into products, it has to be developed
differently, then the science that we do typically in graduate school labs that's meant for
writing a paper, and that was the mindset with which, you know, just where it was created,
and in fact, all our clients, we worked out, we worked with, we actually built solutions
that could be deployed, not a single solution that shipped from us, was like, you know, a
paperware or a shelfware, right?
So, you mentioned the book, and I had this on my list of things to mention in your intro,
so I'm remiss in that regard.
You just published a book, in fact, that's sitting here on my desk, Natural Language Processing
with PyTorch, which you co-authored with Brian McMahon.
And so, we'll definitely dig into that a little bit more.
You recently joined the AI Foundation, which I hadn't previously heard about, but read
a little bit about, and it's got this interesting kind of for-profit, non-profit structure,
reminding me a little bit of kind of some of open AI's recent announcements, in terms
of the direction they're going, can you tell us a little bit about the AI Foundation
and kind of that structure and what the organization is up to in general?
Sure, in fact, the funny thing is now people are referring to, I mean, using open AI
as an analogy, but we were doing, we established this structure almost like more than a year
ago.
And in a way, I think this is great for mission-oriented companies.
AI Foundation is a hybrid for-profit and non-profit, and at the for-profit, we are actually
interested in building all kinds of synthetic content.
And in the non-profit, we are actually interested in detecting the synthetic content.
So actually, we started off with, or at least the founders, they were interested in solving
the detection problem.
But then, you can't just go off and build a non-profit and keep it sustainable, right?
You need something to power the non-profit, and a far-profit is like a very sustainable
way to power a non-profit, like before I started working at AI Foundation, I started
the fake news challenge in a very non-profit mode, and that was not easily sustainable
just based on grant money, sure, we want some grant money from night foundation, but
it becomes a massive exercise, almost like, and it's not a sustainable way to build
a non-profit.
So when I learned about AI Foundation, and I learned that about their mission, which
was to detect any kind of synthetic media on the internet or anywhere, and it includes
generated video, generated audio, as well as generated text, I was obviously very excited
about that, and then, you know, I was curious how they were actually approaching it.
And actually, it was their way of approaching that may solve me more than, you know, the
mission itself, because I was already sold on the mission, I kind of, even before I met
a disability AI Foundation.
So the far-profit is building products, like, you know, is doing all these interesting
synthesis work in the context of AR and VR, right?
And this is great because one of my experiences is that, you know, in order to be really good
at detection, you also need to be really good at generation, and the way the far-profit
and non-profit structure is that, like, you know, the far-profit is actually building technology
that is so strong and generation, that the non-profit, the technology that non-profit
is building also gets strengthened, kind of like a real-world GAN, if you will, right?
Like, they're both reinforcing each other.
And of course, it helps for the far-profit folks to, like, go spend some hours working
on the non-profit stuff, and it's a really great model.
Sounds like there's some commonalities between the kind of problems you're trying to solve
with the fake news challenge and what you'll be working on at AI Foundation.
I'm curious, what were the key takeaways from your experience launching the fake news
challenge and conducting that?
Oh, yeah, absolutely. Fake news challenge was another one of those humbling experiences
in life, and I think this is great.
Everybody should go through multiple such humbling experiences, and that's where, like,
a lot of growth sports happen.
So when I started seeing, around the 2016 elections, when I started seeing a lot of misinformation
on Twitter, I'm very active on Twitter, you know, and I have been for quite some time.
Now, I felt like, oh, there is all this misinformation coming.
Why is Twitter not doing anything about it?
Why are these platform companies, especially Facebook?
I mean, at some point, I used to be on Facebook, and I was seeing a whole bunch of random,
like, complete conspiracy theory, like, not stuff being shared on Facebook.
I mean, I'm no longer on Facebook, but I just felt like the platform companies were
letting us down by not working on this problem, and there was also a time where, like, you
know, the, like, Mark Zuckerberg went on stage saying that his Facebook does not have
a misinformation problem, right?
Like, this was in 2016, and around that time, there was not just me, but a whole bunch
of other people who are interested in this problem started talking on Twitter, and I ended
up partnering with Dean Palmer-Lew.
He was a professor at Carnegie Mellon.
He was, like, one of the pioneers of the autonomous driving program developed by DARPA.
And he was doing self-driving cars in the 80s, which is crazy, think about that.
So, anyway, Dean and I started the fake news challenge, and we both were, like, you
know, we are both entrepreneurs, right?
And we have this sort of, like, entrepreneurial optimism, and at the same time, we are academics,
so with academic background, you know, we think that we can solve anything.
And we thought that, oh, we can totally, like, build some kind of natural language processing
system to kind of detect fake news.
And it didn't take us too long to realize how nuanced this problem is, and the more
we kept, we, and then, you know, we started talking to fact checkers and journalists.
I spent, I don't know, much.
I can't even count how many hours with fact checkers and journalists, and I realized
that gosh, this problem is so complicated, and the work they're doing is so complicated.
It's not going to be any one system or approach that will solve this, but it will be, like,
you know, a combination of, you know, approaches involving, modeling, it involving human in the
loop, et cetera.
And that's, that's when, like, we, when we started fake, fake news challenge, we thought
we were going to come up with a solution, right?
And that was like the kernel of the idea that will come up with a solution.
And then, you know, we thought, oh, maybe it's not one solution, it's multiple solution.
And then we thought, no, it's not even multiple solution.
What do you want is the community of people talking to each other.
And that will create, like, a factory for coming up with multiple solutions and create
more conversations, right?
And so fake news challenge ended up becoming this sort of, like, a community where, you
know, researchers, and natural English processing, computer vision, et cetera, behind also
the fake news problem could come and interact with journalists and fact checkers.
And basically, these were two, until the fake news challenge happened, there were two
different communities that were not talking with each other as much as they are today,
right?
Right.
And I'm not claiming that, you know, the fake news challenge completely changed that,
but I think it set the tone.
And as a consequence, what happened was there were a lot of people who ended up meeting
there.
Some of them ended up starting their own companies around this space.
Some of them ended up starting other events around this.
And it created a whole bunch of conversation.
And I think there is, like, a snowballing effect that happened after that very shortly.
So I think your original question was, what were the key takeaways from the fake news
challenge?
I mean, the one takeaway was, like, you know, always be humble, like, keep the world
is more complex than you initially think at us.
And there are surprising ways in which we keep learning that and relearning that.
And the other takeaway is, we found that, okay, we put together this community, this community
needs to be engaged, we need to do something about it.
So we created a shared task and I would, like, advise anybody to not take up this job
of creating a shared task.
It's one of those really complicated jobs that require multiple people working together
in order to pull it off, like, really well.
And despite me working on it full time, and even Dean spending a fair amount of which
time on it, there was, it was super challenging to pull it off, right?
And the shared task in this case is the creation of the community or the creation of a solution
to the problem.
So what we did was I put my consulting hat on and then I looked at, okay, I'm going
to talk to a whole bunch of fat checkers and I'm going to find out what are the core problems
that they are dealing with.
And if there is any science problem that needs to be solved in order to solve this problem,
right?
And I, you know, it was obvious that one of the common problems that kept coming over
and over it was, was that of volume.
So almost all fat checkers are, like, inundated with articles and they need to fact check and
many of these articles are like, you know, pretty much rehashing the same thing or different
variations of the same theme.
And therefore, you know, it's not, they can't be very efficient about how the fat check,
right?
And how they assign labels to these articles or any kind of additional context around
these articles.
And what we did was we created a dataset where, if I give you two articles or two, let's
say, two pieces of text, can you tell me if the two are related, unrelated, discussing
about each, you know, about the same topic or they contradict each other, right?
And it's a surprisingly hard natural language processing problem, right?
Like, it's, I mean, maybe not unsurprisingly hard, natural language processing problem.
Because you're not dissolving textual entailment, you're solving a whole bunch of other things
in order to get it right.
And a lot of teams competed on it, again, you know, it's not one of those things where
if the dataset is still out there, it's not like an MNEST or something where, you know,
people have solved that dataset and moved on, right?
It's a really hard challenging problem.
In fact, the problem was so interesting and challenging that it got used in a lot of
university and LP courses, ranging from MIT to Stanford, like, you know, every big and
small university departments offering, you know, key courses in class projects and so on.
People have published on the dataset and, yeah, so the shared task was actually like a
great way to keep the community engaged, but man, it's really time consuming to actually
get that done and to make sure the evaluation, et cetera, was happening, right?
And we kind of, like, I wanted to withdraw in a little more excitement to this, so just
where I decided to sponsor, like, prices for the top three.
And it became quite successful, like, you know, there were more than 1,000 people who registered
for the challenge and forget the exact numbers now, but there were like a few hundred teams
who were actually competing in it.
And yeah, I mean, we had to, it was all automated, the evaluation, et cetera, and we ended up,
you know, finding the top three and awarding them prizes and so on.
And I would say the learnings from the fitness challenge were, you know, one half was
technical, I would say as it was as much non-technical as it was technical, the non-technical or
softer aspects were like the domain related problems, like how complicated the domain is
and who are the key people working on the domain and how we can facilitate and how to
build a community and how to sustain the community and so on.
And the technical parts of the thing is actually, to me, it somewhat least interesting.
I guess it was more like, how do we come up with the data set for this situation and
we came up with some interesting tricks to build the data set, but yeah.
And now the competition ended a couple of years back.
I'm curious, do you think that the data set and the competition is still relevant?
And are the solutions that were proposed at the time still relevant or do you think if
you miraculously could clone yourself to run this thing again, like would you see dramatically
different results two years later?
So the shared task by itself as a natural language processing task is still highly relevant
and the approaches that were proposed, they were all unsurprisingly deep learning based
approaches, they're all still relevant, maybe today if you were to do this, you would
use like something like Bert or some of these beefier models, but other than that, the
fundamental approaches are still relevant.
That said, when we created this shared task, we called it FNC1 for a reason because we
realized that none of the tasks that we propose is going to be representative of a fake news
solution, right?
And in fact, any solution to the fake news problem is actually coming from a multi-pronged
approach and there are so many problems to be solved.
So the volume is just one aspect of the problem.
So we call it as FNC1 for that reason.
So which automatically implies will there be an FNC2, FNC3 and so on?
And the answer is yes, because it takes such a lot of effort and thought to put together
an FNC2, I am actively working behind the scenes, talking to people around what this
FNC2 should be like and how to make that relevant to the new kinds of problems that are facing
because the kinds of vectors that are popping up, especially around deepfakes and things
like that, there are different kinds of misinformation vectors coming up.
How do we select a task that faithfully reflects what is happening today?
Yeah, let's jump into the current state of fake content generation and detection.
Because fake news, as you kind of traditionally defined it in FNC1 and there's a bunch
of subproblems there, you mentioned deepfakes to you.
When you think about the content generation and detection landscape, what do you see as
the primary challenges and how do you organize all of that in your head?
I guess practically every modality can be faked.
That is audio, video, text.
I guess maybe tomorrow if tactile becomes another modality, you can fake that too, I don't
know.
So right now, I think our foundation, we are interested in detection of all three dominant
modalities, like audio, video, and text, and each of these have their own approaches for
generation.
So for example, with video, you might already know about a whole bunch of approaches based
on GAMS, so these are the narrative model approaches, and then there are all these approaches
based on image-to-image transfer, like face swap and deepfakes and so on, so that's one
category of misinformation, another category of synthetic misinformation is like in audio,
you can talk or turn, set the trend, but like latest wave net models have become so good
that you have to be not a human to detect if an audio is coming from a model or if it
is coming from a human person, right, and I would say three ways to generate fake audio.
One is synthesis, that is obvious, another is wise conversion, where I can take your
wise and then kind of pass it through a deep network and then do a style transfer to
somebody else's wise, right, and so that way I can give you a power to say in anybody
else's wise.
And the third is, of course, replay attacks, right, where I can splice a piece of audio
from a previous conversation in a different context and kind of like create confusion
because I can make you say things out of context, right, and the replay, so the first two are
actually, you need a model to detect that because especially with lots of compute and
a lot of parameters, like really deep and wide models, you can model, like you can generate
audio with really high fidelity, that the human ear basically cannot distinguish, right,
like, and we are doing some experiments around that too, but what we see is many of these
generation algorithms, right, like synthesis or wise conversion algorithms end up leaving
some high frequency artifacts in the data, sorry, in the audio that whatever your ear misses
the detection model can easily pick up on, right, so I mentioned two of the three audio
based, like we're two of the three methods to generate fake audio.
There is also a third method, which is the replay attack, and interestingly, the replay
attack has become such, I mean, it is one of the oldest attacks, right, because you don't
need a model to do that and it's been happening forever, there has been a lot of work on detecting
replay attacks, and in fact, I cast bin last year, they had a challenge for detecting
replay attacks, and basically, not last year, in 2017, they had a challenge in detecting
replay attacks, and 2018, there are rates were around 2.5%, right, by, you know, they
were able to ensemble all the top models, and then this, let's say, the ensemble error
rate was like around 2.5%, and in 2018, there was this one paper I can give you, the reference
if you want, where the error, you know, basically went down to like zero, and so interestingly,
we can now with the help of models, almost always detect replay attacks, and that's
possible because whenever audio recording happens, there's always like a whole bunch of,
you know, differences that are channel and room, like ambience, related variables that
are kind of low level acoustic qualities that are undetectable by us, but the model can
easily pick up that change in that when you kind of like splice another audio into an
existing audio. So on the audio side, are there, what does the data set landscape look
like for, you know, folks want to play with this, are they creating their own data sets,
or are there some interesting? So, like, there hasn't been any other data
set I know other than ASV Spoof, which was put together by a bunch of other researchers
in Google. I think the Google synthesis team is behind the ASV Spoof data set, and basically
what they're doing is they're collecting a whole bunch of different synthesized audio,
and from an audio that has been synthesized by a whole different kinds of methods.
I think they're only using synthesis and wise conversion, not necessarily replay, because
replay I feel is like probably nobody wants to work on it. Probably easier creature on
data set for that too. I know it's a replay is kind of like a solid problem, like, you
know, with whatever caveats, right? But sufficiently over parameterized network, we can easily
detect a replay attack, but the other two require some amount of work. And I think this
is actually the current ASV Spoof, like 2019, where, you know, detecting whether the audio
came from the, whether it came from a synthesis system or a wise conversion system, or another
interesting task is actually, can I, you know, detect change in the room parameters,
right? So imagine, let's say I have your Alexa, right? I'm able to record you in a different
room, and then let's say I'm talking to you, and I make you say something, and it's,
you know, clandestinely recorded. And then I go off and play that recording to your Alexa
and make it purchase like, I don't know, 10,000 Barbie dolls or something like that. So
if you do that, how can you detect that, right? So can I change, so you can kind of like
generate audio by automatically changing the room parameters, like different kinds of
audio by generating, sorry, you can, by varying the different room parameters, like the size
of the room, the position of the speaker, and so on, and change the reverberation, and
that in turn changes the quality of the audio and so on.
It's interesting to think that, you know, a retailer, for example, particularly a commerce
retailer, you know, has long since had, you know, huge anti-fraud teams that are working
on credit card fraud and other types of transactional fraud. It's interesting to think of how, you
know, an Amazon or Google Now might resource audio fraud because of the kinds of scenarios
you're describing with Alexa's that are putting in everyone's house. Yeah, so and this is
a big problem for like a lot of banks, like, you know, banks in accepting and credit card
companies and using audio based or vice-based authentication, right? So any speaker verification
system can, can be filed if it's not carefully planned, it can be filed by doing some kind
of a replay attack. Because most of the time, it's something like my vice is my password
or something like that that you have sayings and if it is recorded under sufficiently
high quality conditions, then I can use a replay attack and basically file any vice-indication
system.
Well, let's, we'll come back to some of the other modalities, but you previously alluded
to this kind of the GAN relationship between generation and detection. In the context of
audio, but I imagine this will apply generally, you know, how do you see this interplay kind
of evolving between the detection and generation approaches and systems? Like now that, you
know, we're getting so, so much better at synthesis and conversion, detecting synthesis
and conversion attacks. Are we already seeing, you know, feeding that back into the synthesizers
and converters and trying to make better systems?
Yeah, and that's, that's going to be an, an arms race and we have to accept that. So
the better the, better the detectors become, you know, you can imagine somebody with sufficient
motivation and it's not difficult to find them, actually engineer systems to overcome that
or figure out exactly, okay, how do I file these detectors? I think an interesting challenge
that would have, that's not yet been conducted by anyone else, look at all the approaches
to detection and come up with ways to break them, right? And I think that will give us
a lot of interesting lessons into building more robust detectors. And this is true with
any kind of adversarial setting, like, you know, starting from a fighting spam, like anti-spam.
So you've, you come up with one approach to, like, let's say, you figured out that, you
know, a lot of the Nigerian spam emails that you get, they are using poor language, right?
And you try to, let's say you train some in-gram based thing that detects it, you say,
okay, the, the perplexity is too high. Therefore, this, this could be like a signal for spam.
But then guess what happens? They will start with copy-pasting. They won't even have to
do, like, fancy generation. They will copy-paste, like chunks of good text from different places.
I am file the detector, right? And now that you know that is happening, you'll start moving
the detector in a different direction. I think this is the game of working in an adversarial
learning setting that your attack vectors keep changing and you have to constantly keep
monitoring the attack landscape and then keep evolving the solution.
We're starting to explore a lot of this ground in the context of adversarial examples
on the video side. There's a whole body of knowledge for, I guess, kind of meta knowledge
for dealing with this kind of these kind of adversarial scenarios in the security
research world. Like, do we have enough of that DNA within the kind of AI and NLP realm?
Or are we starting to get enough of that that we can, you know, we're not reinventing
the wheel?
We, we have for some, like, you know, NLP, I can say, because of things like anti-spam,
etc. There has been that kind of work happening already, right? Like, you know, around fishing
or in around span. People have been constantly building systems that involve, that involve
some kind of a feedback loop and preferably a human in the loop that way it can sort
of deal with this shifting adversary. And we'll see similar things happening with these
other modalities as well, right? Like, yeah, so that's not, I mean, that's pretty much
on the road map for any serious company.
So we dove into audio on the video side. What's the landscape looking like there? I think
deepfake is the thing that comes to mind. But even that is somewhat ill-defined and includes
a bunch of different types of things.
Right. So, like, for example, like anything that creates generator or sampled from a
GAN, today, even the best GANs still end up leaving some kind of artifacts that makes
it possible to do good quality detection, right? But I can imagine a future where compute
becomes so easily available and maybe a different kind of modeling paradigm where we are basically
able to generate hyper-realistic image or like even photorealistic, I mean, very uncanny
images, right? And the thing about detection is, I have this thesis that all detection
models are always better than humans at detection when it comes to sophisticated attacks, right?
And as things become more, so initially, you know, we will have, we will operationalize
that, we will have more and more people looking at things and so on. But then once the adversaries
evolved and then they decide to like, you know, have more launch, more and more sophisticated
attacks, then models will become indispensable for doing that, right? And so, I think we
are seeing some of that happening with GANs. And now with GANs, you can, there are all
these temporal GANs where you can actually generate a sequence of images that are tied
to each other so that it becomes like a video as opposed to like, you know, individually synthesizing
each frame, right? I think that was one of the criticism with that with GANs. Then there
is also, you don't need a GAN, right? Like with things like celibate, etc. There are things
happening, which are a class of algorithms, which are doing this image-to-image transfer,
right? Where I can basically take your body and then just swap out your face or sometimes
even just swap out the lips, right? And then make you say things that you have not said,
right? And that's interesting because, you know, we are a lot of the algorithms that we're
working with, even in the state of the art methods, are still leaving some artifacts that
models can detect. Like, you know, at AI Foundation, we internally, we have researchers,
as well as like a product that we are working on and from the nonprofit side, where we are
detecting manipulated faces in video and showing a heat map over it, right? And so on.
Efficient frontier may be overloading a term, but there's a kind of a frontier where the, you know,
the costs to generate these things and the quality, the resulting quality, you know, versus a
human's ability to detect, it seems like we're in, we're still in a different place than where
audio is. Most of the things that are accessible that I see are pretty, you know, it's pretty
easy to tell as a human that, you know, this is a bad fake. Yeah, but I would say that, you know,
with GANS, I mean, when I say human, when you say humans, we have to be careful, right?
You and I are probably looking at people like us who tend to be like, you know,
more in the machine learning, yeah, I would. And we are used to seeing a lot of these things,
and it kind of like have this in-net expertise to tell them apart. But we have experiments,
we have done with human subjects where, you know, there's like human subjects who are like
completely non-technical when they look at GAN images, there's literally like a 50-50 chance
that, you know, they get it, right? Yeah. And this is where, like, you know, I think a model
becomes highly valuable. It gets worse when it becomes audio, with audio practically everyone
will fail. You made a comment and refer back to it that we're going to be dependent on models
because humans are, you know, we're being outclassed by models. And it makes me think a little bit of,
there's been some research on medical imaging that says that, you know, a model is, you know,
X percent accurate at, you know, say 80 percent accurate at detecting some cancerous, you know,
growth in an image. A human is 80 percent accurate. I'm making up the numbers here, but together,
they're 95 percent accurate, or something along those lines. Do you think the same thing
will evolve in fighting artificial content where humans plus models become the ultimate solution?
Absolutely. I feel like that's, that's even, that's an inevitable case where humans and models
work together, except that I would make a small note that in that situation, the kind of
information that the human might look might be different from the kind of information the model
is looking at, right? I have some examples. Come to mind. So, for example, in, in this case,
you know, the model might be let's say, take fake audio, right? The, the human may not be able to
teleport the fake audio from a generated one, let's say. And sometimes, you know, with compression
in audio, it can get even worse. Like compressed images and compressed audio introduce
all sorts of artifacts that you cannot distinguish between that from the artifacts that you get from
a generated system, from a system like this synthesis, right? So, we're kind of conditioned to
accept some degree of compression artifact. And so, exactly, got it, got it. Exactly. And, and, and
folks really have trouble telling that just from the content, right? And so, the models will like
look at things that are here, listen to things that people can't listen to or see things that people
can't see. But at the same time, you know, let's say the model flags something as, as, you know,
doctored or forged, then you can, we can imagine a human in the loop, actually, like, you know, looking
into it and not necessarily looking at the content, but looking at everything else, looking at the
context, like, you know, where is this coming from? What is the user behavior, and so on, right?
To large extent, it can be automated, but, you know, but even the context, like, it requires a lot
of, like, background knowledge, as well as, like, knowledge of the world, to, to be able to say,
to make, to make us a definite pronouncement. And we see this all the time in, like, I, you know,
I will give you an example from the spam world where I come from. So, if you have training topics,
right? So, there is, like, a lot of things appear like spam. The model can flag it as spam.
But if you know that, you know, this thing is actually related to a training topic,
then a human can make a judgment call whether it is market as spam and take a retaliatory action,
like, by suspending the account or something like that, or whether to just treat it as simply
a high volume traffic, because this training topic is just so engaging and so controversial,
right? Right. And this is like a call that a human, and only a human can make it, at least as far
as now, because the context is where it requires all sorts of, like, reasoning and unification
with the world that it's hard to bake that into a model. I feel like we were just scratching the
surface here, but we're running along on time. I do want to briefly mention once again the book.
If I remember correctly, there was a tweet, there were probably multiple tweets along these lines,
but I remember you tweeting something along the lines of, like, your philosophy with the book,
or at least one aspect of it, relating to the code examples being real examples, or something
like that. Can you elaborate briefly on your philosophy for the book and what makes it different
from, you know, picking up a tutorial on the web, or on YouTube, or any of the other books out there?
Yeah. So when, when Brian and I set out to write this book, we decided to bake in all the best
practices that we were, you know, actually practicing on a day-to-day basis, and also we were
not seeing them on any of these tutorials on the web, and so on, and also on the original
documentation. So we decided that, you know, there are, like, very good software engineering patterns.
I mean, I've always believed because I come from a very engineering background,
I was an engineer first, and then became a researcher. I think really good science requires good
engineering, and good engineering is, like, indistinguishable from good science. Like, and it's important
to have the two together. So we baked in a whole bunch of best practices that we knew of
from the software engineering world to actually make modeling good, modeling successful, right?
And so if you follow some of the practices that we suggest, you would, like, not make a typical,
you know, modeling mistake that we would spend days if not weeks trying to chase down, right?
So that's what I meant by real world, and then, of course, a lot of the actual problems that we
have chosen, we intentionally chose problems that were not, like, toy problems, right? Or toy
data sets. Any of our data sets could be comparable to the real data sets that you,
anyone can deal with. And it's just a question of, like, you know, how do we come up with a set of
representative tasks for the book that pedagogically it will expose the readers to a variety of
NLP deep learning algorithms. And at the same time, not take the reader too far away from their
home court, which is the world of practice where the world of real problems. And I want them to be
situated next to each other. And hopefully, if we did our job, right? We have shown the readers
a lot of good software engineering practices around building models. And we also talk about,
like, good design patterns, not just from building models, but for even for the product building
NLP products itself. So in a variety of ways, this book is actually built for practitioners.
It is a book that is built for anybody who is good at Python development to pick it up
and quickly become familiar with national language processing and deep learning and the combination
of the two and become enough proficient enough to go do their own research. And this is something
I strongly believe in. A lot of people have also believed it. You know, research is something that
you can learn on your own. And you don't need to go to graduate school and all that, like,
you know, the fast AI folks and so on. And we believe in the same thing except we believe
we express it differently. And in the sense that we think there is a lot of value in graduate
school and of course, and in all the research that is out there. And there is a lot of value in
reading papers and actually even writing papers. But there is all that has to be done in the context
of real world product development setting. And every single course that I am looking at
today is failing on that, right? And I am hoping that, you know, the book that we wrote is just
scratching the tip of that iceberg. Well, Delic, thanks so much for taking the time to chat with us
about what you're up to. Very interesting stuff. And we will be sure to continue to follow along.
Absolutely, this is such a pleasure. And, you know, my god, time just flies. Yeah, yeah, my pleasure.
Thanks so much. Yeah, thank you.
All right, everyone. That's our show for today. If you like what you've heard here,
please do us a huge favor and tell your friends about the show. And if you haven't already
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes
we've got in store for you. For more information on any of the shows in our AI conference series,
visit twemolei.com slash AINY19. Thanks again to HPE for sponsoring the series.
Make sure to check them out at twemolei.com slash HPE. As always, thanks so much for listening
and catch you next time.
