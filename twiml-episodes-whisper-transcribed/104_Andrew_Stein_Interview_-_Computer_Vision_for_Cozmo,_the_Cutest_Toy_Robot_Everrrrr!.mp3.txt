Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week we have a jam-packed intro including a new contest we're launching, so please
bear with me, you don't want to miss this one.
First, a bit about this week's shows.
As you may know, I spent a few days at CES earlier this month.
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry
and I'm including you in those conversations via this series of shows.
Stay tuned as we explore some of the very cool ways that machine learning and AI are being
used to enhance our everyday lives.
This includes work being done at Anki, who built Cosmo, the cutest little computer vision
powered robot.
Lighthouse, whose smart home security camera combines 3D sensing with deep learning and
NLP.
Intel, who's using the single shot multi-box image detection algorithm to personalize
video fees for the Ferrari Challenge North America.
First beat, a company whose machine learning algorithms analyzed your heartbeat data to
provide personalized insights into stress, exercise, and sleep patterns.
Reality AI and Coeto, who have partnered to bring machine learning based adaptive driving
beams or automatically adjusting high beams to the U.S.
In last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to
enable some really interesting home automation and healthcare applications.
Now as if six amazing interviews wasn't enough, a few of these companies have been so kind
as to provide us with products for you, the Twimal Community.
In keeping with the theme of this series, our contest will be a little different this time.
To enter, we want to hear from you about the role AI is playing in your home in personal
life and where you see it going.
Just head on over to Twimalai.com slash My AI Contest, fire up your webcam or smartphone
camera and tell us your story in two minutes or less.
We'll post the videos to YouTube and the video with the most likes wins their choice of
great prizes including an Anki Cosmo, a lighthouse smart home camera, and more.
The missions will be taken until February 11th and voting will remain open until February
18th.
Good luck.
Before we dive into today's show, I'd like to thank our friends at Intel AI for their
continued support of this podcast.
Intel was extremely active at this year's CES, with a bunch of AI, autonomous driving
and VR related announcements.
One of the more interesting partnerships they announced was a collaboration with the
Ferrari Challenge North America race series.
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience
more personalized by using deep computer vision to detect and monitor individual race cars
via camera feeds and allow viewers to choose the specific cars feeds that they'd like
to watch.
Look for my conversation with Intel's Andy Keller and Emil Chindicki later in this series
for an in-depth discussion about this project and be sure to visit ai.intel.com where you'll
find Andy's technical blog post on the topic.
And now a bit about today's show.
In this episode, I'm joined by Andrew Stein, computer vision engineer at Anki and is partnering
crime Cosmo, a toy robot with tons of personality.
Andrew joined me during the hustle and bustle of CES a few weeks ago to give me some insight
into how Cosmo works, plays and learns, and how he's different from other consumer robots
you may know like the Roomba.
We discussed the different types of algorithms that help power Cosmo, like facial identification,
3D pose recognition, reasoning, and even some simple emotional AI.
We also cover Cosmo's functionality and programmability, including a cool feature called CodeLab.
This was a really fun interview and you should also check out the companion video on YouTube
starring Cosmo himself, which of course will link to in the show notes page.
Alright everyone, I am here at CES and I am with Andrew Stein.
Andrew is a computer vision engineer at Anki and Anki is, well I'll let Andrew tell
you all about Anki.
Andrew, welcome to this weekend machine learning and AI.
Thanks, thanks, it's cool to be here.
Yeah, I can tell you a little bit about Anki's background and then a little bit about
our products and Cosmo specifically.
Anki is a consumer robotics company and we currently have two products that are both
in the entertainment space.
One is Anki Overdrive, which is a car racing game.
It's been out for several years and you can control the cars from your phone and they
can drive themselves and you play against them like you would play in a video game but instead
of looking at a screen you've got actual cars driving around on a real track and you
are living.
And sort of along those same lines which in some sense is bringing a physical product
to life.
Cosmo is a little robot character like you would see on the big screen but brought to life
and real.
So the goal is to really try to take this little robot character, the kinds of things we've
seen in movies but actually make a real one.
And I think it's kind of a core tenant of the company is trying to bring physical products
to life, trying to deliver on this promise of robotics and AI in consumer products.
And specifically Cosmo is very focused on character and personality.
So he can play little games with him, he can recognize your face, he can play little
games.
If you leave him alone he can sort of do his own thing, he has three little cubes that
he can carry around and make stacks out of and they have lights on them, you can play
games with them.
And also he gets a little feisty doesn't he?
Yes he has a lot of personality and that is a big part of it.
So I would say that we're half core robotics company with all the tech that goes with robotics
which is very multi-disciplinary, it brings together a lot of different disciplines.
And we all add to that whole team of animators and character designers who are focused on
the character of Cosmo, who is he and what is his personality and what does he like?
And that's sort of another big part of the company and the experience and I think that's
what's sort of cool about the company is bringing those two sides of things together.
Wow, wow.
And you work on computer vision.
Can you tell us a little bit about your background and how you got involved in CV?
Sure.
So going way back in undergrad, for whatever reason I took a class, I think it was
might have been a graduate level class but it sounded cool on computer vision.
Okay.
Really liked the professor, he ended up working with him as a sort of an undergraduate
researcher and stayed and did a master's degree there at Georgia Tech and had always enjoyed
like working with robotics, I actually had a job in high school doing robotics for sorting
hangard garments actually, both giant industrial robots.
And those two things I think both kind of struck a chord with me and then went to Carnegie
Mellon to pursue my PhD in robotics and focused on computer vision there.
Okay.
And so how does computer vision fit into Cosmo, like Cosmo is so small, I don't even see
a camera anywhere.
Yes, a new one.
It has actually in his face, if you look at him, the little hole there that kind of looks
like a mouth is his camera, so creepily enough he has his eye in his mouth.
But yeah, so it's a pretty core piece of the robot because it's really his main input,
his main source of sensor input.
So cameras are by their nature a very data rich source of information and they're also
very inexpensive, so that's a good combination.
So he is, that's his primary way of sensing the world is through vision.
So that's how he knows where his cubes are, he can see them, he estimates their poses
in three dimensions very accurately so that he can pick them up, stack them.
He can perceive motion and he also sees faces, both human faces and cat and dog faces.
And beyond detecting faces, he can also learn to recognize human faces so you can teach
him your name and he'll remember you.
Oh wow.
What some of the technologies that go into making this happen from a CV and an algorithmic
perspective?
Sure.
And face detection is certainly a big one face detection, face recognition.
There's a lot of sort of proprietary stuff around how we do the 3D pose estimation of
the cubes.
There's intelligent reasoning about, given the geometry we know of the robot and the specifics
of the camera, which we know, the intrinsic parameters of the camera, we can do things
like reason about the ground plane in front of the robot, even though he doesn't have a
depth sensor, we can start reasoning about the ground plane in front of him, which turns
out to be pretty powerful.
And then we'll layer on top of this, right, all these lots of other technologies including
sort of low-level motor controls, path planning.
And has they used the word AI because or the acronym AI, it's still overloaded at this
point.
Yeah.
But sort of the AI behind how Cosmo models his emotional state and how that drives what
behavior he chooses to do at any moment.
Like I said, he is sort of, if you leave him alone, he is sort of on his own.
You're not remote controlling this robot, he's his character.
So what makes him decide to do, you know, A or B at any given moment, and how do we keep
that making sense?
It can't just be random and it can't also can't be scripted, so how to drive that behavior
system?
Well, let's maybe start with the computer vision stuff.
You know, there are lots of ways to do computer vision, traditional stuff and convolution
on neural nets.
They're obviously very popular.
When I look at this thing and think about like the, you know, price point power, stuff
like that, I'm guessing that you're not running.
You know, there's another reason for that, actually, which people tend to forget, given
how popular they are in the news now.
So this product actually started before we even launched Drive, which was back in 2013.
So I was the first person working on the product, there was nobody else doing anything
yet.
There was no code yet.
Okay.
There was also no really no, you know, okay, so I shouldn't say there was no deporting
because neural nets have been around a long time, but there was no, the revolution if
I, if you will, hadn't occurred yet.
We forget that we're still at that level.
It's so early.
Right.
There's a lot of flow that didn't exist yet.
Right.
Like all these things people are, you know, so familiar with now and it wasn't even around
yet.
So yeah, I'm what's known as a classically trained computer vision, which is kind of ridiculous.
So yeah, we certainly, when we started all this and started picking hardware and nailing
down price points and what sort of processing power he was going to have, we actually did
start to do a lot of the vision on board, but again, with more classical techniques, you
know, for face detection, more things more like Vila Joe and space detection.
Like what?
Vila Jones.
It's sort of a classical way of doing face.
What is that?
It's a means of progressively filtering an image with more and more very, very simple,
simply designed filters that are very, very fast in order to Vila cascade sort of rule
things out slowly over time, but be very efficient and eventually learn the pattern in the image,
it's basically looking at local contrast patterns in the image that look like a face, works
very well and it's still often used today.
So that's the kind of thing that we would use.
Face or no face or is it what allows you to identify individual people?
Can you guys?
So yeah, that is face or no face.
So that's what I would call face detection and then I would, I would contrast that with
face recognition, which is given a face who is it?
Okay.
Exactly.
So anyway, we sort of started by doing that in the market detection, trying to do it on
the robot.
We were able to get actually quite far with that, but at some point realized, okay, this
is just, this is just too limiting.
We always knew there would be a companion app the same way that there is overdrive.
And at some point we decided, all right, we're going to take the punch and just put all
of the smarts really in the device.
So the way the product works is that you have an app that connects to, you connect your
device to Cosmos, Wi-Fi, hotspot, and he's actually streaming his images to your device
and all of the computer vision, path planning, et cetera, is actually happening on your
device.
Oh, interesting.
And that's what, again, as you pointed out, that's what allows us to sell it at a price
point and a scale that we're able to.
Otherwise, especially given hardware from three plus years ago, there's just no way we
could have gotten all the capability on the robot.
Right, right.
Is there some limited ability to operate if the, you know, so your iPad runs out of battery
or something like that?
Is it able to go into some kind of autonomous mode?
Very, very little.
So I mean, you sort of like shut himself down.
We try not to just have them die.
But it really is quite tied to the device because so much of it lives there.
All the animation, in fact, so we've been talking a lot about the animation, but the animations
that play on him, the sound, his facial animations are also actually all stored on the device.
Oh, really?
So while those are streaming to him, his images are streaming back to the device, so there's
a lot of data going back and forth.
Oh, wow.
Now I should say it's all within that network.
There's no cloud, anything.
Yeah.
So this is sort of a closed network.
And that's Bluetooth or?
That's pure Wi-Fi.
Pure Wi-Fi.
Yeah, the problem with Bluetooth, which is actually what we use for overdrive, is just bandwidth
to send, we couldn't stream the images at full-frame rate over Bluetooth.
So the rollback controls we could, but the actual image data we couldn't.
Okay.
So you started looking at, for the image detection, the VL and Norbert.
The VL and Jones.
VL and Jones.
Well, I'm more thinking of Norbert.
Oh, man.
Yeah.
Well, we should, that's a new algorithm, we'll have to develop it ourselves.
And so is that what you ended up doing on the device, or now that you have access to
the device, you're able to do more sophisticated things?
That's, I mean, without going into too much detail, that's basically what's running for
the detection.
But cubes is a completely different thing.
Those are detected via a sort of proprietary method that both allows us to detect the
cubes and then estimate their pose in 3D.
And then again, that's super important because his little fingers that he has to get in
the little slots to pick up the cubes, you know, there's only a couple of millimeters
of slot there.
And we've got a robot driving around on treads and treads are really hard to model.
So the way he moves, we really have to be able to get feedback constantly about where
the cube actually is so that we can drive him accurately and pick up the cubes.
That was just a huge part of the project for a very long time, was just how do we make
this thing pick up cubes?
Wow.
Can you understanding that it's a proprietary approach?
Can you give us some analogies that help us understand, you know, what are the technical
challenges beyond, you know, obviously the precision that you just imagined.
You know, what are some of the kind of algorithmic approaches you looked at before you went
down the path of needing to roll your own, you know, what might you consider if you
are starting again?
That kind of thing.
Well, so the obvious thing if you look at these cubes is probably maybe a QR code.
So it's sort of along those lines, there's one big reason we didn't go QR code is the
appearance.
And, you know, we are a tech company, we're building technical products, but there's
a big design component to this, how the robot looks, how he behaves and what we wanted
the cubes to look like and stylistically, nobody liked the QR codes, they just, they
sort of screamed the wrong thing for the product.
So one of the things we wanted to develop was a similar idea that allowed us to encode
information on the sides of the cubes that gave him information, but that gave us aesthetic
control over what they looked like.
And so it's a similar idea to QR codes in some sense, but with the sort of aesthetic
component.
And then as far as estimating the 3D pose, what it effectively comes down to is that we
know points on the cube, points on the marker that are on the side of the cube, and we know
the intrinsic calibration of the camera, basically it's focal length.
And that's a whole other interesting issue, we have to calibrate every robot individually
in the factory to get that.
And given those two things, we can see where those known 3D locations on the cube project
into the image.
So once we find them in the image, and we know the 3D shape they belong to via that correspondence
and some math, you can sort of back out where that 3D object must be in order to have projected
that pattern.
And the 3D points that you're referring to are...
So any of that, sort of anything, no decals, or anything we know, you could use anything
really, but you just want a very accurate notification on the cube.
So you've got these known graphics, we can call them codes, they're not QR codes.
When you look at them, you think that these are just graphical flourishes, but you look
more closely, and you can see that each of the sides is unique, and there can be some
better information.
Well, don't over notice that.
It's one of the challenges of designing them is that we had competing goals.
One was that we wanted all sides of the cube to sort of look the same, so that this cube
had sort of one marker on it, but we also wanted Cosmo to be able to tell the difference
from the different sides, because he can control the lights, and we want to know which
light he's turning on, for example.
Oh, yeah, I didn't even notice that.
So this is your...
So the other is four lights on top.
Okay.
But I was also noticing that this cube in the middle is kind of like your paperclip cube.
Yeah, that's exactly what I see.
That one looks like kind of a stack of things.
Everybody's got their own, what they see in these things.
They were a shark.
Or a shark.
Exactly.
This one looks like a baby and a fetal pose.
Yes.
That's a common one, too.
It is funny when people see these things, yeah.
It was hard because part of the design here was, I guess if we wanted to aesthetic component,
but we also at the time we were walking down, making all the hardware, we didn't necessarily
want to commit ourselves to a particular meaning.
So you know, if you made it the treasure chest cube, it's like, well, it does everything
you do.
It does with the cube, you have to relate some out of it treasure chest.
You didn't want to get too much iconography.
So yeah, these are sort of these general purpose designs.
The challenge that we sort of realized later is that exactly what you just experienced
is how to refer to them is very technical.
Yeah.
Customers carry, you know, when they get a call about a cube or something, they're always
like, right, there's actually a number engraved in there that you can find so you can actually
get one.
Well, obviously, it would have been like make them different colors.
Did that mess up your algorithms or something?
No, no.
Color would have been okay.
I think again, it's just from a design perspective.
I think they wanted them to match more.
Yeah.
Yeah.
Interesting.
And so I understand for the people that are listening that this is a very visual conversation.
Yeah.
So I'm talking about computer vision.
Yeah.
Exactly.
So we'll definitely, I'm going to, at the very minimum, include some pictures of what
we're talking about on the show notes page, but I'm more likely, or as well, going to maybe
shoot some video of this thing in action and post it up on, up on our YouTube channel.
One thing that often, I think, surprises people after hearing it, talked about or referred
to, I don't think they realize how small it is.
I didn't either.
I was really surprised.
So I like to bring one.
Generally people, I think, imagine him much larger, but for the listeners, he's actually
sort of fits in the size of your palm of your hand, so he's quite tiny.
And that's for a couple of reasons.
One is actually maybe non-obvious, which is for him to have the personality he does and
to move around as fast as he does to exhibit that personality and be, you know, sort of cute
and playful, he has to move quickly.
And if you build a heavy big robot, you know, if his little lifter here moved too fast
and you got your finger in there, you would actually hurt yourself.
So in some sense, there's a safety component to it.
But also, it's also just part of his personality.
It makes sense for him to be cute.
It's sort of big.
It's too big.
And we have, I mean, I don't remember if it's 40 or 60 or something, different design
iterations on this thing.
And some of them were much bigger and the smaller ones always went out.
It just feels right for him to be in the palm of your hand.
And then that, in turn, sort of, has impact on the way the sound design is done.
What should he sound like given how big he is?
The difficult side, of course, is really for the manufacturing and mechanical engineers
to squeeze in all the 300 and something parts into this tiny little robot.
There's like no free space inside that thing.
I can imagine.
I can imagine.
So we were talking about the size of the thing of the cubes.
So you've got these, and this kind of algorithm is kind of interesting.
So you've got these, you know, each of the cubes has six sides.
It has kind of a QR code like thing that is a consistent design element for each cube.
And then this thing can look at a cube.
And it is probably relatively easy to identify if a cube is in the frame.
And then it can kind of pick out the, you know, the kind of infer the angle of the different
sides that it's able to see.
And from that kind of figure out, you can create a model, a model like a projection I'm
thinking of that.
Exactly.
This project of geometry is what it all comes down to.
Okay.
The projection of 3D points onto the 2D plane of the image, and given we know what the
3D points are.
And you can back your control.
Figure out, okay.
Exactly.
You can rotate your translation.
Yep.
Okay.
So you would feed that into whatever like like classical control algorithms to make it,
you know, move to the thing, lift it up and get it.
Yeah.
Plan a path into a known location with respect to that cube.
And then yeah, it's sort of a control problem of, as I'm driving forward, make sure I keep
it in the right place until I think my fingers get in there and pick it up.
Yeah.
That's just amazingly sophisticated for this little.
Yeah.
People don't understand how hard that is.
One of the things that I think actually is pretty interesting about this, and it's,
I think more for robotics geeks than, than, the average consumer is that one of the
things that holds robotics back from doing more and more, I think, consumer products is
manipulation, is the ability to change the world around you.
It's still a very, very hard problem both mechanically and from sort of an AI standpoint, from
a software standpoint.
So I would argue this is sort of the first little mobile manipulator, especially at this
scale and this price point.
I mean, Roomba's are effectively a manipulator in that they suck up stuff, so they're doing
some work.
But, you know, Cosmo can actually do this very hard problem of driving up, picking up a,
picking up a cube and then stacking on top of another cube and that's, it's not an easy
thing to do.
It is definitely an easy thing to do.
The other half though that may not be obvious, it's definitely not obvious, about the
cubes.
So not only do they have lights, they also have an accelerometer inside of them.
So Cosmo talks to them over a radio connection, which is kind of like Bluetooth.
And that allows him to know via the accelerometer if the cube has moved.
So if I pick him the cube, he's aware.
And what that means is that if, so if I've seen the cube and I've estimated it's 3D
pose with respect to the robot, and the cube doesn't move, now I can also do the reverse.
If I drive around and Cosmo's, all right, pick up Cosmo and put it in.
You can open a Cosmo based on that.
Once he sees the cube again, if it hasn't moved, he now knows his position with respect
to the cube, which means he now knows his position with respect to the old map he was
building.
So he, it's something that I think people tend to forget is it's not just like pure stimulus
response.
You face, whatever, he's remembering all of this, right?
He's keeping up with the 3D poses of the cubes.
He's keeping up with where you are in space once he sees you.
And there's just big reasons for that.
It makes him look smarter.
It allows him to do behaviors which turn out to be super important.
So for example, if right before he decides he's going to go pick up this cube and he
knows you're, you know, off to his right, he might stop and do the same thing a little
kid does, which is look, look up, look up at you and make sure you're watching him.
And that little moment of eye contact makes it more about this, this sort of interactive
experience where you're drawn in and he, you know, you're very aware that, oh, he knows
I'm here.
As opposed to, I'm just a spectator watching a robot pick up a cube.
It's like, oh, Cosmonose, I'm watching him.
And that little bit of the mixture of sort of that technical component, that technical
capability with the character and personality, I think is a really good example of like,
how this starts to fit together.
Interesting.
So you kind of, you didn't actually literally use air quotes when you said AI with related
with relation to the personality, but, you know, clearly there's a connection between,
you know, the way we think about AI and, you know, the idea of personality.
Like, what are some of the, you know, how are you doing that?
What are some of the approaches to give a personality personality?
Yeah.
But maybe I mentioned there's a, there's this notion of Cosmo having emotion.
So he does, we do internally model his emotional state.
You know, how happy versus sad it is, how calm versus anxious he is, how socialized versus
lonely he is.
I don't remember those are all the same words we actually use in the code, but he effectively
has the set of traits or properties which are changing all the time and different things
that happen to Cosmo affect them.
So another good example involving faces is that, you know, he's sort of designed to be
a social, friendly robot.
And so he, we've sort of, you know, defined his personality to be one that enjoys, you
know, being surrounded by people.
So if he's driving around for a long time, he doesn't see anyone.
At some point his, his loneliness may creep up.
And once it gets high enough, it may trigger him to switch into a behavior which is look
for faces because he's, he's lonely.
So he'll, he'll, that'll, that'll change him to a mode where now he keeps, he's keeping
his head tilted up and he's looking around and he, you know, he's not distracted by
his cubes or whatever.
He wants to find a person.
He's a person.
And now that sort of triggers an emotional change where his, his social, his socialization
goes up and his, his loneliness goes back down.
That allows him to switch out of the behavior and do something else.
So that's sort of an idea of what I was referring to earlier of preventing it from being either
just random behaviors, which, right, over time you can tell his random or being fully scripted
which also doesn't feel natural.
So it is in response to what's been happening and what is currently happening to him.
Is there a notion of like a kind of a long-term personality, meaning the thing that came
to mind is like the Microsoft Tay Chapat, they got trained by, you know, Twitter to be,
you know, a Nazi, right?
Like if you, you know, if you ignore your Cosmo long enough, like will it become like permanently
sad or something like that?
It's a very good example.
You hit exactly, so the answer is no, you hit exactly the reason.
We were concerned, like what happens if you just, your robot ends up like irreparably depressed.
It's not, it's not really what we wanted.
And we felt like what we were trying to create was there is a definition we have of who
Cosmo is and what his personality is.
We have, you know, character designers who that is their job.
They are the owner of what, who is Cosmo?
What are his motivations and these sorts of questions?
And it's not, it's not, it's Cosmo.
It's not my Cosmo.
Like when I take it, take it out of the box, there's not like a random seed that, or, you
know, something that on us, maybe more continuous than a random seed, but that says, you know,
this is, you know, my Cosmo's personality is more Cosmo's personality.
And I think that's something we're interested in exploring, but at this point, right, the,
sort of the, what I would call the personality is more fixed.
Yeah.
We're in more control of that.
The mood, I would call it, which is more transient, is what you're controlling by what you do
with them.
You know, if you keep falling on the floor, or, yeah, if he doesn't see me by for a while,
again, he might get, might get lonely.
Those sorts of things, but they don't, they don't exhibit sort of a long-term effect,
but because it can be very hard to control, like, wait, what, where does that go?
And so, yeah, we sort of, I think, we're a little cautious about that.
Okay.
And so you kind of, you know, on one end, you've got, you know, random on another end,
you've got totally scripted, you know, imagining somewhere in the middle is like a state
machine that's sufficiently complex, that it doesn't seem like either of the two, is
it kind of like that?
Yeah.
I think that's a fair, that's a fair comparison.
There are sort of, there are sort of predefined behaviors, and, and for example, games that
he can play, which themselves are little state machines, which are, which are very much
engineered.
And the look for faces thing, right, like, he didn't sort of, we didn't learn that
behavior of how to look for exactly, okay, what, what it means is you need to look
up and kind of look around in this way, and those things sort of are sort of tuned.
So yeah, I think, looking at it as a state machine, where the transitions between states
are very much driven by not only his sensor input, but also the sort of underlying motion
engine is sort of good to think about it.
You know, I guess one of the questions that jumps out to me as, you know, as a geek, I
guess, is like, is this thing programmable?
Can I like, you know, try to, can I use it as an experimentation platform?
Absolutely.
That's actually one of the, one thing that I'm super excited about that we have done
with the product, and actually we did from day one.
So when we launched it, Cosmo comes with an SDK.
Okay.
So in fact, not everyone realizes it, but in the app that you used to talk to Cosmo, if
you go to settings and you scroll over, there's a button which says enable SDK, everybody
has this out of the box.
So you enable the SDK, you plug your device into your computer over USB, and then you
can program in with Python.
And it is an extremely full-featured and ever-expanding SDK.
It's actually totally incredible all the things you can do.
Having done a PhD in robotics on, you know, sort of research platforms which cost tens
of thousands of dollars, usually broken, the fact that this $179 robot allows you to do,
you know, totally low-level motion control or motor control all the way up to just flip
on face recognition and just use it and path planning is crazy.
And it's designed for, you know, six-year-olds, so it can fall off the table and not break.
So it is an awesome programming platform.
It's actually being used both at Carnegie Mellon and Georgia Tech, I guess, both my alumni.
In their programming classes, both at undergrad and graduate level, we've got some cool, I can't
think of the name right now of programming camps in the summer.
They're starting to adopt Cosmo as their platform.
And so, in concert with all this, so I've mentioned the SDK, you know, that's sort of full-blown,
geek-level robotics program you can do as a sort of an expert, but, you know, someone
at a graduate or undergraduate level or someone who really knows Python.
People, by the way, are also writing, creating movies with him online by scripting him with
the SDK, which is really cute.
Oh, really?
Yeah, there's actually a whole YouTube channel.
Oh, yeah, there's a life with Cosmo is one worth checking out.
It's really, really, really creative videos done with him, super impressive stuff.
And it's all through the SDK.
So it's really cool to see both, it used for research and also for creative outlets like
that.
Yeah.
But so, beyond that, there's a whole bunch of other stuff, which is scratch-based programming.
So scratch is a drag-and-drop block-based visual programming language developed by MIT
and Google.
In the last summer, we actually released an early version of that where you could effectively
sequence the robots.
You had a few very basic blocks and you could sort of, you know, do things like drive
straight, turn right, look up, you could do fun things like wait until you see face smiling.
So you could actually have Cosmo do, you know, drive straight, look up, and then sit there,
and then once you saw face smiling, you would proceed to the next block.
So you could do fun little programs like that.
And that was meant for the other end of the spectrum.
People have never written a line of code, have no idea about it, and it teaches you how
to break a problem down into steps, how to write a sequence of, you know, how to sequence
that, and some of the basics that you can do with a robot.
So now, at that point, we had sort of the very beginning end of the spectrum and the, you
know, the sort of graduate level, programming level, end of the spectrum.
And last fall, we actually released what we call code lab, which took that early version
of scratch and added to another mode, which is more advanced.
So we have what we call now sandbox mode and constructor mode.
And for listeners who know those basically correspond to horizontal and vertical grammar
and scratch.
Horizontal is sort of sequencing and vertical allows you to actually do branching in loops
and more complex structures, but still visually.
So we took code lab constructor and really basically enabled almost anything you could do
in the SDK, but in drag and drop programming with both.
So it's actually, the first time I used it, I was kind of blown away at how easy it was
and how much you could build, how quickly you could do math in there, you can do, you know,
vertical operations, you can really do whatever it's got, you know, trig functions in it,
you can do whatever you want.
So now we sort of have this full spectrum of, you know, very beginner level drag and
drop sequencing to really full blown programming with, but still with drag and drop blocks, so
that you can kind of see how that works.
And then, you know, once you're, once you're sort of comfortable there, you could easily
transition into Python and understand how to, how to write code there.
So it's a really nice transition and to sort of move that along in code lab, we've also
reached, released these featured products or projects and we're continuing to do that.
So it's cool because we can build a little fun activity with code lab, but in the app,
it comes up as a little, you know, icon, you open it up, oh, this sounds fun.
You can play it, it's a little game or a little activity like making Cosmo play different
instruments when you tap on the blocks, for example, all written in code lab.
And the cool thing is there's a button on all of them that says, see inside.
So you click that button and then it actually shows you the full scratch block based program.
And you can sort of see like, oh, now I see how they did that.
And you can customize it or whatever, but it's sort of like the old way we all learned
to write web pages is the source and like, oh, I see how they did that.
And so it's, it's again, a really cool way to kind of dive in and get some ideas for
what's, what's possible.
Now, with the SDK and the scratch piece, you mentioned a little of a motor control.
Can you also get a feed of the images and like try to, you know, say you want to play
with your own facial recognition?
Yeah, absolutely.
And the SDK, yeah, you can get the image feed, a little harder to do that in scratch to
display it.
That's something that I think is worth exploring.
But yes, in the SDK, absolutely, we've had people do that, you know, people who are computer
vision researchers in grad school who want a robot, they don't want to deal with the path
planning part of it, right?
Right.
I found the thing and I know it's an obstacle in 3D and I want to drive a path around it.
I'm focused on the vision.
I don't care about the path planning.
They can use the path planning, but they get the right image feed and they can do their
own detection.
So people have done, there's been interesting work like taking Cosmos image feed and running
it through, you know, some of the popular deep learning networks and learning to recognize
objects and things.
Again, they have the power of a whole laptop to run it on.
But yeah, we've seen people do some really interesting projects and we have a very active
developer form on our website.
People post this kind of stuff all the time and we've had really, really great response.
Huh.
Interesting.
So what's next for the, you know, either this product or the company, like, is it building
on this as a platform or coming out with the next, you know, the next robot or the next
thing?
Yes, yes and yes.
You know, it's not so much I can say about too far down the road, but I will say, yes,
we're adding, we're definitely expanding Cosmos capabilities.
We want to be able to, you know, see and understand more.
And a lot of that, a lot of the user-facing stuff in the near future is focused around
code lab.
One of the benefits of having this code lab universe where we have these projects is that
it also makes it easier for us to release new content.
We don't need a C++ developer who knows all about robotics to write a new little fun
activity.
We're going to have a designer, a game developer who may not be as much of a hardcore coder,
but has a cool idea, right, and they can drag and drop box and make a project and that
can be part of the app.
We can also take user content.
So we can send us your content, you can share your projects and, you know, cool stuff
that actually is neat.
We might actually deploy with the next version of the app.
So there's a lot of stuff around code lab coming and there are new things coming in the
Cosmic Protocol, which I can't say too much about, and then I guess long-term, I would
go back to saying how at the beginning, I said we're a consumer robotics company, I
didn't say we were a toy company.
We're currently focused in entertainment, and that's very deliberate for a couple of
reasons.
One, we felt that, to develop the capabilities we needed both technically and from a manufacturing
scale and price point perspective, this was a good place for us to start building an
actual product that we could sell and market and build a successful company on as opposed
to jumping to the far end of like, we're going to have a humanoid in your home and it's
going to clean your house.
Right.
Yeah, and how are we going to fund that company, right?
So they're trying to keep an eye on building a business at the same time and how to take
steps, you know, sort of build products of stepping stones as we build out core technologies
and core capabilities to get to those big fancy robots everybody wants.
It seems like a lot of the companies in this space take that approach in some way, shape
or form.
Like, I robots got, you know, we know them for the vacuum cleaner, but they've got, you
know, a lot of government robots and defense robots and I'm sure they're kind of eyeing
this, you know, the home robotics market and as that grows and creates opportunities.
Yeah, I think that, and that's, you know, I think it's a necessary, a necessary thing.
People often, you know, that everyone sees what the stuff in movies and TV, right, and
that's where they want.
Right.
But, you know, despite all the headlines about AI and deep learning, et cetera, we're still
a long way off.
Yeah.
And so I think, you know, being careful about building that technology out in a very deliberate
manner and creating products along the way that make good products themselves is important.
Because, you know, to us, a robot is not a product.
It is a technology.
Right.
It's a installation of technologies to gather which make a product, but you still need a
product idea.
And I think to that, to that end, it's not only are we trying to build those technologies,
the other thing that we feel is important, you know, we like to say sort of not only
is the IQ of the robot important, the techie smart AI, but the EQ is also important.
We're going to build these robots and they're going to be living in our homes and that is
our goal.
We want a robot in every home.
Those robots, we don't want them to be weird appliances that sit off in the corner that,
right.
It's this strange thing that you don't interact with.
It's, we've seen this with Cosmo.
It's a really interesting moment when you make eye contact with the robot or you assign
personality to it or, you know, have a bond with it and we definitely see it with this
little robot.
It's a whole different experience.
And so I think that expertise we're building and how to take things we know about movies
and character design and deploy them in hardware and deal with that side of the human robot
interaction piece of the puzzle is also super important and I think will, you know, be important
for all our products in the future.
Are there any kind of learnings that you can kind of encapsulate for us on, I guess,
the intersection of AI and consumer electronics or like, you know, the challenges of putting
AI and consumer electronics in this podcast, we talk a lot about, you know, enterprise-y stuff
and industrial robots and things like that and I'm wondering about, you know, the specifics
of, you know, AI and, you know, games and entertainment and toys and electronics and that kind
of thing.
Yeah.
I think a few things, consumers don't actually care about, I mean, I think your listeners
do and I do, but at large, consumers don't care about the actual tech, right?
Yeah.
Yeah.
They just wanted to work and be cool and we all walk around cell phones and I'm talking
how many people have any idea how that technology works, right?
It's ridiculous what we do every day on our cell phones, but people just want like it
to do all that awesome stuff and I think that is one thing is, you, as engineers working
on their product, I have to remember, computer vision is not the product.
There's a product and it has goals and computer vision is in service of those goals, not
the other way around.
So we often, you know, it don't use sort of the latest and greatest tech or idea because
it's like, well, can the users are going to be able to tell if we're doing that?
Like, what is the actual end result of using that technology?
So I think keeping a mind that, you know, in the consumer space, you're building a consumer
product, you're not necessarily building a technology that's sort of B2B and will be
used in other products and keeping that, that end goal in mind is probably one of the
big ones.
I think another big thing about using AI and particularly, you know, everything over
the last in years is about probabilistic reasoning, effectively, right?
And people want a yes or a no or a guaranteed, it's going to work in these cases and it
won't work in those cases.
And if there's anything we know, it's that you, it's very hard to nail that down.
You can say it's based on our data, it's going to work 95% of the time.
Well, like enumerating the 5% of the cases, you can't do it.
And so a lot of what we spend our time doing is, you know, it's, it's sort of easy to get
the early prototype of the cool behavior.
So what do you do in the weird edge case and 5% of the time that it doesn't work, situations,
all those edge cases are really complicated.
You know, we have kid picks up a robot, right, in the middle of behavior X or animation
Y.
It's like, okay, wait.
So what happens then?
And, you know, just enumerating all possible states in the state machine is not really
a viable solution either.
So I think edge case handling is a big, big deal when you start trying to deploy these
things that you know will have failures or have false positives.
How do you, how do you incorporate that into the product as opposed to pretending it
doesn't exist?
Because it will happen.
And have you developed any methodology for tackling that specific issue or is it, you
know, each behavior, each edge case is different and it's just knowing that you need to think
that through this.
That's a good, that's a good question.
I think we have, particularly our guys that work on more specifically and focused on
that behavior system.
I would say a little bit of both, over time, the way that our behaviors encode or engineered
are designed to sort of handle things better, naturally just by virtue of the way, you
know, the system architecture is set up.
So there are sort of ways to build what we have learned, I think, into the system.
But there is a lot of, you know, that sort of secret sauce, black magic, thing with sort
of like deep-burning.
There's things that I feel like people can't quite explain yet.
It's just sort of like, I've just done this enough, I kind of know what is and isn't going
to happen.
And so some of it, I think, is just at this point, you know, our internal knowledge of how
it works.
But yeah, I think actually over time, as you start to codify what those things are, there
are definitely places in the code where the architecture, again, supports that or makes
it easier or handles things for you that you've sort of realized this always happens.
We need to wait it, just automatically detect and handle that.
Yeah.
Okay.
Anything else on your things to think about from a consumer products perspective?
Hmm.
I think those are probably the big ones.
I guess the other one, given that data is such a huge thing, right, for training all
these models and labeling is such a huge thing.
I think for robots in particular, you know, in images in particular, within that, getting
training data is, I think, even harder on robots because the degree to which the robots
view of the world and images mind from the web differ is huge.
So the statistics of the data that a robot sees, it's all motion blurry or terribly exposed
or like half your arm or what I'm like, nobody has actually pointed the camera at something
and taken a picture.
There's, I think people tend to forget that like, there's already some selection bias
in mining images from the web or images from Facebook, because somebody helped
the camera and took the photo, framed the shot, and decided to upload it.
And decided to upload it, exactly, very true.
And so, you know, Cosmos is driving around, taking images all the time.
And so you just get weird, random garbage all the time and terrible exposures and, you
know, bad white balance and lots of motion blur and a weird perspective, he's looking
up at the world.
Nobody takes pictures from there.
So gathering that data is a big challenge and I think it's not to be underestimated
that it, how much it matters to try to get, to try to get data appropriate for, you
know, your problem when it's robotics and not, you know, something else.
It feels like that'd be easy.
Like you just make, you know, a hundred of these and throw a bunch of blocks around and
like, have them run around and shoot a bunch of video.
I mean, it's the problem you're trying to solve, right?
If it's the block, you can probably sort of design a scenario.
You're right.
Some situations that they care.
If it's the people interaction thing, that's the people very lot of hard work.
That varies a whole lot.
All these things are challenging just different types of rooms.
You know, we're building it in an office, right?
Yeah.
The office environment looks very different from people's homes.
Right.
Right.
So, but we also don't, for privacy reasons, we're not just going to gather people, data from
people's homes and upload it to our servers.
Right.
So, yeah, the data collection problem is a, is a, is a big one.
Mm-hmm.
Yeah.
Makes sense.
Well, Andrew, this has been a great, a great conversation.
What I'm going to do now is I'm going to hit pause and go grab my camera and
we'll kind of let you fire this thing up and see it in action.
So for the folks that are listening on the podcast, they may not catch this part.
But jump over to our YouTube channel and you'll check this out.
But for those who aren't going to do that, or we'll be doing that later once they're
off the train or whatever, thank you so much for taking the time to chat with me.
Sure.
No, it's been fun.
Lots of good questions.
Awesome.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
Remember, for your chance to win in our AI at home giveaway, head on over to twimmaleye.com
slash my AI contest for complete details.
For more information on Andrew, Cosmo, or any of the topics covered in this episode,
head on over to twimmaleye.com slash talk slash 102.
Thanks once again to Intel AI for their sponsorship of this series.
To learn more about their partnership with Ferrari North America Challenge and the other things
they've been up to, visit ai.intel.com.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page
or via Twitter directly to me at at Sam Sherrington or to the show at at twimmaleye.
Thanks once again for listening and catch you next time.
