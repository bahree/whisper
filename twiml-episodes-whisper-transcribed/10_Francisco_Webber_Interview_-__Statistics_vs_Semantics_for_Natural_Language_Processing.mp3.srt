1
00:00:00,000 --> 00:00:16,280
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:16,280 --> 00:00:21,440
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,440 --> 00:00:24,280
I'm your host Sam Charrington.

4
00:00:24,280 --> 00:00:27,920
Once again the recording you're about to hear is part of a series of interviews I've recorded

5
00:00:27,920 --> 00:00:32,960
live from the O'Reilly AI and Stratocomferences in New York City.

6
00:00:32,960 --> 00:00:38,200
My guess this time is Francisco Weber, who is the founder and general manager of Artificial

7
00:00:38,200 --> 00:00:41,920
Intelligence Startup, cortical.io.

8
00:00:41,920 --> 00:00:47,560
Francisco's presentation at O'Reilly AI was called AI is not a matter of strength but

9
00:00:47,560 --> 00:00:49,360
of intelligence.

10
00:00:49,360 --> 00:00:54,540
To set the stage for my conversation with Francisco, recall that in the last interview Pascal

11
00:00:54,540 --> 00:01:00,460
Feng noted how recent advances in natural language understanding have been based largely

12
00:01:00,460 --> 00:01:04,580
on ignoring language structure and focusing on statistics.

13
00:01:04,580 --> 00:01:09,620
While in this interview you'll hear Francisco argue that the next advance in NLU will come

14
00:01:09,620 --> 00:01:14,500
from shifting our attention from statistical models to models based on a more sophisticated

15
00:01:14,500 --> 00:01:15,980
model of the brain.

16
00:01:15,980 --> 00:01:22,620
A warning in advance, this conversation is very technical and moreover rather abstract.

17
00:01:22,620 --> 00:01:26,060
Don't be afraid to listen to it a couple of times to allow the ideas and opportunity

18
00:01:26,060 --> 00:01:27,820
to sink in.

19
00:01:27,820 --> 00:01:33,580
You'll find this week's show notes at twimlai.com slash talk slash 10.

20
00:01:33,580 --> 00:01:38,660
You might be particularly interested in a link to Francisco's presentation slides which

21
00:01:38,660 --> 00:01:41,860
are helpful to review alongside the podcast.

22
00:01:41,860 --> 00:01:44,340
And now on to the show.

23
00:01:44,340 --> 00:01:51,380
Hello everyone here at the O'Reilly AI conference and I am with Francisco Weber, who gave a great

24
00:01:51,380 --> 00:01:56,660
talk earlier on AI is not a matter of strength but of intelligence.

25
00:01:56,660 --> 00:01:58,660
So welcome Francisco.

26
00:01:58,660 --> 00:02:03,740
Hello, great to be here and to talk about my talk actually.

27
00:02:03,740 --> 00:02:08,300
Nice, why don't we start out by learning a little bit more about you and hearing a bit

28
00:02:08,300 --> 00:02:09,620
about your background?

29
00:02:09,620 --> 00:02:18,500
Yeah, so I'm coming from the natural sciences, I'm trained in medicine in Vienna but I have

30
00:02:18,500 --> 00:02:27,660
since ever sort of a built-in affinity to technology and ended up sort of going into

31
00:02:27,660 --> 00:02:34,700
the natural language processing information retrieval domain where I'm in for like 20

32
00:02:34,700 --> 00:02:42,800
years now but I've been the so cortical I always actually my third company, I previously

33
00:02:42,800 --> 00:02:48,940
worked in the field of patent information which is also a sort of complex natural language

34
00:02:48,940 --> 00:02:56,980
issue and that was basically where I learned of the limitations of the current systems

35
00:02:56,980 --> 00:03:03,300
and that motivated me to actually try and find something substantially different.

36
00:03:03,300 --> 00:03:06,740
Okay, so tell us a little bit about cortical.

37
00:03:06,740 --> 00:03:14,580
Yeah, so cortical is all about two things in fact, so one thing is a theoretical framework

38
00:03:14,580 --> 00:03:23,820
that we have discovered I would say and that we explore that is about how the human brain

39
00:03:23,820 --> 00:03:32,980
to be more specific actually the new cortex supposedly handles language information and

40
00:03:32,980 --> 00:03:40,860
the other is that we basically use this theoretical framework to also create a real technology

41
00:03:40,860 --> 00:03:51,780
that we basically offer to the markets currently this is mainly for enterprise customers.

42
00:03:51,780 --> 00:03:59,700
They have a lot of problems out there and so say the effort to revenue ratio makes us

43
00:03:59,700 --> 00:04:06,780
work there for a while but we do also have a public API where basically everybody can play

44
00:04:06,780 --> 00:04:11,020
around with our technology for free.

45
00:04:11,020 --> 00:04:21,220
Yeah, so that's what the company sort of does and we try to find the really alternative

46
00:04:21,220 --> 00:04:28,700
way to deep learning and to the more traditional ways of statistical modeling and machine learning

47
00:04:28,700 --> 00:04:39,420
for the moment our approach doesn't actually use any statistics so which might not be

48
00:04:39,420 --> 00:04:45,860
the case in the future so there are some motivations to maybe team this up with deep neural

49
00:04:45,860 --> 00:04:51,500
networks or so so that's but in fact it's not our speciality so I leave this to others

50
00:04:51,500 --> 00:04:58,340
to try out what we basically do is that we have solved I would say or we have found a

51
00:04:58,340 --> 00:05:05,700
solution to the famous representational problem that exists in natural language understanding

52
00:05:05,700 --> 00:05:15,380
since decades basically a very fundamental issue is that basically says if you find out

53
00:05:15,380 --> 00:05:22,740
how to represent language which means text at some point in a way that you can actually

54
00:05:22,740 --> 00:05:32,780
compute with it then many of the big problems like ambiguity like vocabulary mismatch all

55
00:05:32,780 --> 00:05:42,460
the traditional problems we have in NLU basically are solved in one approach and that's what

56
00:05:42,460 --> 00:05:51,540
I actually presented today is that by this little shift in how to generate the features

57
00:05:51,540 --> 00:06:00,940
in falling place afterwards in a very convenient and most importantly efficient manner so tell

58
00:06:00,940 --> 00:06:11,740
us about this shift yeah so our approach basically is founded on the work of Jeff Hawkins

59
00:06:11,740 --> 00:06:19,820
who is a researcher in in the area of cortical processing so he works on finding out how

60
00:06:19,820 --> 00:06:27,660
the human new cortex actually processes data as a data in general because one of his findings

61
00:06:27,660 --> 00:06:36,780
was that regardless what kind of data so might it be hearing or seeing or touching all

62
00:06:36,780 --> 00:06:44,100
of that data when it comes to the new cortex looks the same it has the same format which

63
00:06:44,100 --> 00:06:49,220
is a what is called a sparse distributed representation so it's like a large vector

64
00:06:49,220 --> 00:06:56,220
of binary features where you have like two of two to five percent of those features are

65
00:06:56,220 --> 00:07:02,940
actually set to one and all the rest is zero and everything is encoded into such a such

66
00:07:02,940 --> 00:07:12,220
an SDR and that was basically our first goal is to find a systematic unsupervised because

67
00:07:12,220 --> 00:07:20,060
otherwise it's not doable in practice a systematic unsupervised way of converting text into

68
00:07:20,060 --> 00:07:27,540
such an SDR okay now I've heard a couple of times even at this event there were a couple

69
00:07:27,540 --> 00:07:33,220
of comments that were made that one of them was even I think in the key notes this morning

70
00:07:33,220 --> 00:07:37,820
there was a comment about how you know what we've got with neural nets are don't have

71
00:07:37,820 --> 00:07:44,900
the complexity and the nuance available to express what's actually happening in the brain

72
00:07:44,900 --> 00:07:51,380
and and in another talk the kind of follow on statement was so therefore we shouldn't

73
00:07:51,380 --> 00:07:55,180
try we should just use these as tools and now it sounds like you you have a totally different

74
00:07:55,180 --> 00:08:03,300
belief system around this well I mean fundamentally what we use as neurons nowadays has in fact

75
00:08:03,300 --> 00:08:11,780
very little to do with real neurons yeah so it was an abstraction that was made like 30 40

76
00:08:11,780 --> 00:08:19,780
years ago on a compared to today on a very rudimentary understanding of what neurons actually

77
00:08:19,780 --> 00:08:27,940
do nowadays we know more we know that for example the actual learning happens through the building

78
00:08:27,940 --> 00:08:37,940
and unbuilding of synopsis between them and if you actually model a neuron not not chemically

79
00:08:37,940 --> 00:08:43,540
so it's not about sort of creating all the molecules that are there because that's something

80
00:08:43,540 --> 00:08:50,260
that nature uses yeah so nature you know in evolution you always have the components from

81
00:08:50,260 --> 00:08:55,780
the previous evolution evolutionary state and you have to play with this kind of Lego bricks

82
00:08:55,780 --> 00:09:03,940
and do something which sometimes looks a bit inefficient but what is key on the other side is what

83
00:09:03,940 --> 00:09:13,220
is the mechanism that those real neurons create and that is what Jeff Hawkins actually has figured

84
00:09:13,220 --> 00:09:20,340
out and is about to even figure out in more detail okay and so certain aspects like the the

85
00:09:20,340 --> 00:09:28,740
sparse binary representation are actually key for this to work properly and by working on text so

86
00:09:28,740 --> 00:09:34,420
our approach is basically okay if Jeff is right with his theory everything he says about the

87
00:09:34,420 --> 00:09:40,500
general way how the cortex processes has also to be truthful language as the language is generated

88
00:09:40,500 --> 00:09:48,340
by the cortex too and so we basically took his theoretical framework as a set of constraints and

89
00:09:48,340 --> 00:09:54,180
we tried to say okay if that is the limitation how can I put everything I know about language

90
00:09:54,180 --> 00:10:03,780
within this limitation and it took a while actually 25 years or so in general I mean not I know

91
00:10:03,780 --> 00:10:09,540
Jeff's work since since a little bit over 10 years but everything that was sort of needed to

92
00:10:09,540 --> 00:10:18,580
me at least to sort of understand and to operate at this abstraction level took a while but then I

93
00:10:18,580 --> 00:10:24,260
had at some point while I was listening to his talks reading his book on intelligence and so

94
00:10:24,260 --> 00:10:34,100
and it was literally sort of taking a shower and in a second I had this visual idea so to say

95
00:10:34,100 --> 00:10:44,020
how this could happen okay because it boils down to a sort of visual aspect in the sense that

96
00:10:44,740 --> 00:10:52,820
as a necessity we have to find a representation where two words that mean similar things have to

97
00:10:52,820 --> 00:10:59,540
actually look similar and when I say look there as the hours have to be similar and literally

98
00:10:59,540 --> 00:11:06,580
similar so in fact and that's also what the brain is doing to put one word representation on top

99
00:11:06,580 --> 00:11:12,340
of the other word representation and by measuring the overlap how many of the bits actually stay

100
00:11:12,340 --> 00:11:20,100
at the same position you get two things one is how related are those words and the second is by

101
00:11:20,100 --> 00:11:26,180
looking where the overlap happens within this representation because this is a two-dimensional

102
00:11:26,180 --> 00:11:37,300
it's like a bitmap with 128 times 128 pixels and like 2% of those 16,000 bits are set to one

103
00:11:37,300 --> 00:11:44,980
therefore are like pixels dark pixels if you want and so it's actually a visual thing and

104
00:11:45,940 --> 00:11:52,660
you can try this out on our website when you take two words that are sort of have a common context

105
00:11:52,660 --> 00:12:00,020
or so you can actually literally see that they look similar yeah and interestingly I mean it's

106
00:12:00,020 --> 00:12:07,780
hard to or maybe even impossible to absolutely decode what it means but if you as I have done

107
00:12:07,780 --> 00:12:13,140
stare a lot into these representations you end up seeing the differences like in the blink of an

108
00:12:13,140 --> 00:12:20,260
eye you might not know the details but to identify that two words are similar in this representation

109
00:12:20,260 --> 00:12:31,300
takes a millisecond yeah and that is already a hint so to say that shows what the major gain is

110
00:12:31,300 --> 00:12:38,900
of this approach which is efficiency and that's what our brain is famous for so I know that

111
00:12:39,860 --> 00:12:45,620
on the deep learning in the deep learning community things like precision and so are the key

112
00:12:45,620 --> 00:12:53,220
metrics and they are of course important it's like having glasses that are blurry nevertheless

113
00:12:53,940 --> 00:13:01,620
at the very end the choice of algorithm is not so much on the precision but it relates to

114
00:13:03,540 --> 00:13:08,260
down to earth energy efficiency yeah I mean the brain works with something like 10 watts or something

115
00:13:08,260 --> 00:13:15,060
so I don't even want to know how much power the GPU servers

116
00:13:16,180 --> 00:13:22,260
each up and that is already a very good hint of how well a certain approaches yeah if

117
00:13:22,820 --> 00:13:30,820
and that's why I chose the title also of brute force because statistics especially if you do

118
00:13:30,820 --> 00:13:37,940
statistics of large combinatorial spaces like like language I mean you basically can create

119
00:13:37,940 --> 00:13:47,380
an indefinite number of combinations of words to make meaningful sentences so to do a statistics

120
00:13:47,380 --> 00:13:55,860
on such an open system it's a real hard work because you have to provide endless examples

121
00:13:56,660 --> 00:14:06,100
to have like a microbit of semantic payload in your representation yeah and it works up to a certain

122
00:14:06,100 --> 00:14:14,100
point no question I mean the statistical systems work but what you see is that in order to make

123
00:14:14,100 --> 00:14:21,860
the model a little bit smaller or to gain a tens of a percent in precision you have to put a lot

124
00:14:21,860 --> 00:14:29,060
of effort in yeah so from to get from 60 to 61 percent precision you might even double the effort

125
00:14:29,060 --> 00:14:36,580
of like going from one to 60 is the same as from 60 to 61 yeah if I could drag you drag you back

126
00:14:36,580 --> 00:14:43,780
a little bit just it sounds like understanding the Jeff Hawkins stuff is important to understanding

127
00:14:43,780 --> 00:14:52,820
what you guys are doing to some degree yes so they so he's defined the sparse data representation

128
00:14:52,820 --> 00:15:02,180
this SDR and is there also a different concept of a neuron that underlies that absolutely so he's

129
00:15:02,180 --> 00:15:09,220
modeling a real neuron but on the functional level so he's also modeling a neuron if you want

130
00:15:09,220 --> 00:15:15,140
but he's modeling everything that is relevant within the neuron for processing data is part of

131
00:15:15,140 --> 00:15:21,940
his model and everything that might be housekeeping building up proteins and stuff like that is not

132
00:15:21,940 --> 00:15:27,860
part of the actual data processing layer and therefore not represented there so he's basically

133
00:15:27,860 --> 00:15:37,700
tried to expand the simplistic concept of a neural net neuron to become a real neuron and sometimes

134
00:15:37,700 --> 00:15:46,180
if you face the problem the way it is the solution is much easier to understand yeah because it's

135
00:15:46,180 --> 00:15:53,940
basically a model-based approach versus a model-free approach so sort of bringing it into one

136
00:15:53,940 --> 00:16:00,820
sentence yeah and so on this base of this more robust model of a neuron there's this notion of

137
00:16:00,820 --> 00:16:07,220
the SDR which is capturing you know and I think of a neuron I think of the you know there's

138
00:16:07,220 --> 00:16:12,260
state plus action right and so this is capturing state even more it's state action and time

139
00:16:12,260 --> 00:16:20,500
that is key to what Jeff is doing okay because his networks have a time built in okay so it's not

140
00:16:20,500 --> 00:16:31,220
only of deciphering a pattern of input bits but it's rather memorizing a sequence of patterns

141
00:16:32,020 --> 00:16:39,460
because in reality things are interconnected so to say they have a semantics built into the system

142
00:16:39,460 --> 00:16:49,860
everything and therefore it is highly improbable if not impossible that by having an initial state A

143
00:16:51,060 --> 00:16:58,100
you can predict which are the let's say physical possible next steps and that's what the processing

144
00:16:58,100 --> 00:17:05,140
relays are yeah the fact that not like in statistics after state A any state could happen

145
00:17:05,140 --> 00:17:11,460
because I need to do the statistics for it but the reality is that after a step A there is a

146
00:17:11,460 --> 00:17:20,500
certain set of steps which have all to be possible in reality and what we learn as walking brains if

147
00:17:20,500 --> 00:17:28,260
you want is what are those potentials what are the potential outcomes and how many hints from the

148
00:17:28,260 --> 00:17:35,060
initial state could point me to the right next state and that's in the end what the brain is doing

149
00:17:35,060 --> 00:17:42,340
yeah the brain is nothing more than a sequence learning engine that does prediction based on what

150
00:17:42,340 --> 00:17:50,660
it has seen so far and if you think through that on a let's say philosophical level you will find

151
00:17:50,660 --> 00:17:59,380
out that you basically can solve or explain everything we do that basically follows this basic

152
00:17:59,380 --> 00:18:05,940
computation yeah so there are two interesting aspects so this one is there is no processor

153
00:18:05,940 --> 00:18:11,460
so the brain does this by being a memory system which is interesting I mean in computers it's

154
00:18:11,460 --> 00:18:17,380
exactly the other way around yeah the processing happens in the processor and the RAM is just a

155
00:18:17,380 --> 00:18:29,300
dormant store yeah and the brain obviously does this differently and the other aspect is that the prediction

156
00:18:29,300 --> 00:18:38,740
is in fact the condensed intelligence because the more I'm right in predicting the more I'm intelligent

157
00:18:38,740 --> 00:18:45,220
and by the way I mean there you know there have been very behavioral ways of looking at intelligence

158
00:18:45,220 --> 00:18:53,700
that's the reason why the dog looks intelligent to the dog owner because the dog owner knows the

159
00:18:53,700 --> 00:19:01,220
dog and knows what predictions the dog is making about things and is right in doing so and therefore

160
00:19:01,220 --> 00:19:06,820
the dog indirectly so to say looks more intelligent to the owner than to everybody else yeah

161
00:19:06,820 --> 00:19:18,180
yeah so does the SDR capture all of that are just the state so the SDR is all about getting a

162
00:19:19,620 --> 00:19:26,180
an explicit representation of the state so that's the other difference in the world of brains

163
00:19:26,180 --> 00:19:32,260
and the SDRs you only work with what is called semantically grounded information so every bit

164
00:19:32,260 --> 00:19:41,300
in representation of the SDR actually corresponds to something real and concrete so for the visual

165
00:19:41,300 --> 00:19:47,220
system it's pretty easy because in the end every bit of the image that is produced on the retina

166
00:19:48,340 --> 00:19:53,700
if you have two dots that are close to each other and have the same color or nearly the same color

167
00:19:54,660 --> 00:19:59,940
you are probably right in guessing that they are part of the same item in the physical world yeah

168
00:19:59,940 --> 00:20:05,380
if you have now a representation that gives you the same phenomenon namely that two

169
00:20:06,420 --> 00:20:12,740
bits that are set to one stay close to each other it's easy to guess that they are related and they

170
00:20:12,740 --> 00:20:18,980
are part of the same maybe subunit of the system the only thing you have to be sure is that

171
00:20:19,780 --> 00:20:26,980
the data that is provided is actually inherently semantic so it has to be part of a system in

172
00:20:26,980 --> 00:20:34,420
on a very abstract level yeah so the world is a system therefore any data that I can hear or see

173
00:20:34,420 --> 00:20:42,340
or so about the world is semantic because there are rules of physics rules of biology and so on

174
00:20:43,060 --> 00:20:50,260
and the same thing is truthful language language is data that is inherently tied together by a

175
00:20:50,260 --> 00:20:57,700
framework of grammar of syntax and all these aspects we know since a long time but we have

176
00:20:58,660 --> 00:21:06,660
we had the problem on how to actually store these mechanisms and the realities don't store the

177
00:21:06,660 --> 00:21:12,980
mechanism but just store the examples just store the detailed information the explicit information

178
00:21:12,980 --> 00:21:22,580
and are those words or those yeah so in our approach we declare the semantic atoms in language

179
00:21:22,580 --> 00:21:30,180
to be words I mean there are like smaller units like phonemes for example but they have no

180
00:21:30,180 --> 00:21:34,180
meaning but themselves so the first time you actually have a meaning is when you have a word

181
00:21:35,380 --> 00:21:41,700
and all the subsequent meaning of a sentence of a paragraph a document an utterance even

182
00:21:41,700 --> 00:21:49,940
comes out of the sequence of those words yeah and so what we do is basically we convert every single

183
00:21:49,940 --> 00:21:56,980
word into such a sparse representation we call this because it's so hard to say we call this

184
00:21:56,980 --> 00:22:04,260
a semantic fingerprint okay and the interesting thing is that through the way how we convert

185
00:22:04,260 --> 00:22:11,060
the semantic fingerprint you take advantage of some of the properties that are inherent of sparse

186
00:22:11,060 --> 00:22:17,940
binary vectors for example you can make a union of as many sparse vectors as you want and you

187
00:22:17,940 --> 00:22:24,740
don't lose information yeah you can always say from an unseen vector if it was part of the union

188
00:22:24,740 --> 00:22:31,460
or not if you try to do the same thing with a dense representation let's say the the ASCII encoding

189
00:22:31,460 --> 00:22:38,260
you have eight bits and every possible combination corresponds to another character if you make a

190
00:22:38,260 --> 00:22:46,100
union of a couple of them no way to say what was the initial part yeah and as I said the the

191
00:22:46,100 --> 00:22:54,260
generation of this pattern is done in a way that every single pixel of our fingerprints correspond

192
00:22:54,260 --> 00:23:01,620
to an explicit learned context and you can infect not a word a context it's a context which is

193
00:23:01,620 --> 00:23:08,580
basically technically it's a bag of words if you want it's a bag of words of utterances in which

194
00:23:08,580 --> 00:23:17,700
the word occurred and so is a what's the scope of an SDR in this model is it at the level of a

195
00:23:17,700 --> 00:23:21,860
corpus at the level of a language at the level of an utterance well not an utterance because

196
00:23:21,860 --> 00:23:28,500
an utterance in fact all of that so the the what we do is in principle we generate the atoms which

197
00:23:28,500 --> 00:23:38,100
is a fingerprint for a word but if I want to create a fingerprint for a sentence I just convert every

198
00:23:38,100 --> 00:23:44,340
single word into its fingerprint I make a stack and aggregate them together I make an ore of all of

199
00:23:44,340 --> 00:23:51,220
them and then I have depending on the location on the fingerprint you can do that's because of

200
00:23:51,220 --> 00:23:56,980
this union property exactly exactly yeah that's the reason why we have to stay on the sparse side

201
00:23:56,980 --> 00:24:02,980
yeah and if you make for example a union of let's say ten words that are in a sentence you of

202
00:24:02,980 --> 00:24:10,500
course fill up the representation therefore after making the union we we do what we call resparsify

203
00:24:11,380 --> 00:24:17,940
we introduce a threshold to cut away everything that fills the fingerprint on more than the two

204
00:24:17,940 --> 00:24:24,580
percent and so we end up with a fingerprint for a sentence that has basically the same topology that

205
00:24:24,580 --> 00:24:31,300
is directly comparable to a fingerprint of a word and we can do this with a sentence with a paragraph

206
00:24:31,300 --> 00:24:43,460
with a book yeah of course the SDR for book or for whatever let's say a book is it end dimensionality

207
00:24:43,460 --> 00:24:49,620
where n is the number of unique words in the book no so there is the topology and there is

208
00:24:49,620 --> 00:24:56,900
that's the name why why we call it semantic folding there is this semantic space folded into the

209
00:24:56,900 --> 00:25:05,140
representation so the way how we do this how we generate the word SDRs is that we take a collection

210
00:25:05,140 --> 00:25:11,300
of documents which are the reference documents that's for a human that would be everything you ever

211
00:25:11,300 --> 00:25:20,340
read and heard yeah all language elements that you got exposed to and we digest them and we do

212
00:25:20,340 --> 00:25:25,060
this of course using machine learning because we are not like humans we have not the time to wait

213
00:25:25,780 --> 00:25:32,180
20 years or so so that's in fact where we apply machine learning and what we do is that we

214
00:25:32,980 --> 00:25:40,580
first of all cut the training material in little pieces and then we define the size of our

215
00:25:40,580 --> 00:25:46,820
fingerprint which is a metric space so there is no dimensionality if you want it's a two-dimensional

216
00:25:46,820 --> 00:25:54,340
metric space and we position all of our training snippets on this space in a very simple rule

217
00:25:55,380 --> 00:26:02,180
two snippets that are similar stay close together and two snippets that are different stay far apart

218
00:26:02,180 --> 00:26:10,100
from each other and then it's you know one of these classical iterative algorithms similar to

219
00:26:11,140 --> 00:26:18,020
heavier learning a bit like this local inhibition mechanism and what you end up with is that you have

220
00:26:18,980 --> 00:26:26,420
all snippets about animals in one region all snippets about family in another region and so on

221
00:26:26,420 --> 00:26:33,780
and you get a semantic map and this semantic map is basically used to encode every word because

222
00:26:33,780 --> 00:26:40,260
I can take all the words that are in my training material and for each of the words I can say light

223
00:26:40,260 --> 00:26:45,300
up the positions of the snippets where this word occurs in and then you get this distributed

224
00:26:45,300 --> 00:26:54,180
representation and because you have to fold it in semantics so to say two similar words like cat

225
00:26:54,180 --> 00:27:01,380
and dog look similar if you look them on the on the semantic map representation of the fingerprint

226
00:27:02,740 --> 00:27:09,940
so you mentioned earlier kind of a you give an example of 128 by 128 matrix at that size matrix

227
00:27:09,940 --> 00:27:15,460
like what are you able to represent like is that a book all of the books I've ever read or

228
00:27:15,460 --> 00:27:24,980
so what it actually represents is a semantic space because it's the fundamental of the representation

229
00:27:24,980 --> 00:27:34,260
and if you just do the math of selecting 300 bits which is about close to 5% of 16,000 bits

230
00:27:35,540 --> 00:27:41,540
the number of combinations you can do is like the number of stars in the Milky Way so it's a huge

231
00:27:41,540 --> 00:27:49,540
combinatorial space and as you know we have not the same assumption as in statistics that in principle

232
00:27:50,180 --> 00:27:55,300
every word could be combined to every other word yeah so that's one of the central

233
00:27:57,140 --> 00:28:03,860
simplification methods is to say in the language statistics that every word is independent which

234
00:28:03,860 --> 00:28:10,020
is absolutely not true yeah if you have on the semantic level a certain set of adjectives that

235
00:28:10,020 --> 00:28:18,340
you associate to a certain noun yeah so there is semantic sort of glue between everything

236
00:28:18,340 --> 00:28:25,780
and in reality that shrinks the combinatorial space and that's precisely what we need to learn

237
00:28:25,780 --> 00:28:33,860
the semantics of it yeah okay okay so this there are elements of this that remind me of war

238
00:28:33,860 --> 00:28:44,500
Tevac I guess it's a natural development that we have started NLP and information retrieval

239
00:28:44,500 --> 00:28:51,300
with so-called document vectors everything was sort of derived from a document and we found out

240
00:28:52,340 --> 00:28:58,100
over the years that word vectors the representation of individual words seems to be more appropriate

241
00:28:58,100 --> 00:29:06,420
nevertheless there is a fundamental but crucial difference so word-to-veg like other word embedding

242
00:29:07,220 --> 00:29:14,820
mechanisms use they try to do dimensionality reduction and they end up with a dance vector

243
00:29:15,380 --> 00:29:21,940
and to put even more on it a dance vector of double of float numbers so sort of computationally

244
00:29:21,940 --> 00:29:31,300
expensive representations we don't do a dimensionality reduction we might even to an increase the

245
00:29:31,300 --> 00:29:37,780
dimensionality at some point if you want but we make it a sparse representation so we have

246
00:29:38,500 --> 00:29:45,620
sparse binary vectors versus dance floating point value the double vectors yeah which already

247
00:29:45,620 --> 00:29:55,060
sort of gives you a hint on where the efficiency will be right right so have we talked

248
00:29:55,060 --> 00:29:58,260
through have we got to what you talked about in your talker is this all been back

249
00:29:58,260 --> 00:30:05,700
I mean I was talking because my head already I understand I mean especially because you are

250
00:30:05,700 --> 00:30:10,660
listening to this without any visual support and this is a very visual thing yeah so

251
00:30:10,660 --> 00:30:15,860
typically when I when I show this and people see the the fingerprints on the screen and how they

252
00:30:15,860 --> 00:30:21,220
interact and how they overlap you can see in their faces ah I understand this yeah you don't

253
00:30:21,220 --> 00:30:27,780
need to know anything about machine learning or so it's so intuitive but if I imagine to sort of

254
00:30:27,780 --> 00:30:37,460
follow a description that is purely verbal then you're doing a great job yeah so the rest basically

255
00:30:37,460 --> 00:30:46,420
was that I gave a number of practical examples where we apply this okay and I can cite a few

256
00:30:47,220 --> 00:30:55,300
for example we do pro let's say we we have certain prototype ways of solving typical problems

257
00:30:55,860 --> 00:31:02,980
and what is the case is that we solve all of them with one unique operator which is similarity

258
00:31:02,980 --> 00:31:09,940
yeah so we only the only sort of verb we have in our universe is is similar or is not similar

259
00:31:10,660 --> 00:31:16,420
and so one thing you can do of course is search yeah so you can and since you're operating on

260
00:31:17,460 --> 00:31:23,540
essentially the sparse vector representations is when you hear similar like is it fair to think

261
00:31:23,540 --> 00:31:30,100
geometrically similar geographic yeah literally so so we actually measure this by calculating the

262
00:31:30,100 --> 00:31:36,420
overlap between two fingerprints which is the most generic way I mean we we do offer a number of

263
00:31:36,420 --> 00:31:41,620
distance metrics as I said this is a metric space so we have different ways of calculating a

264
00:31:41,620 --> 00:31:49,380
distance metrics like a Euclidean distance and others but I have to say that in fact the pure

265
00:31:49,380 --> 00:31:56,020
overlap count is fully sufficient to get the result out of it and it's very computationally efficient

266
00:31:56,020 --> 00:32:03,460
yeah so one of the prototypes as I said is search imagine you have a collection of documents you

267
00:32:03,460 --> 00:32:10,980
convert each of the documents into a fingerprint you have a user who types in a language-based query

268
00:32:10,980 --> 00:32:17,940
now I'm looking for information about red spot cars you create a fingerprint of that query

269
00:32:18,500 --> 00:32:23,620
and you just match how much overlap you have between all the documents and the query

270
00:32:23,620 --> 00:32:30,340
and you rank all your documents according to the size of the overlap yeah very generic it's

271
00:32:31,540 --> 00:32:38,180
it's a real search mechanism so what you get is really all the balanced aspects that you have

272
00:32:38,180 --> 00:32:44,980
in a document so it's not just does a document contain the word sports car but it's about

273
00:32:44,980 --> 00:32:50,340
the aspects that you might have developed in a document that make it match or less

274
00:32:50,340 --> 00:32:55,860
and then theory the document need not even say sports car and exactly are doing theory

275
00:32:55,860 --> 00:33:00,900
the similarity to the this conceptual yeah yeah yeah so it could be the race car it could be a

276
00:33:00,900 --> 00:33:05,860
text about the race car and my query could be about sport cars and it would still sort of give a

277
00:33:05,860 --> 00:33:11,940
good match yeah and how does it apply to non-English languages I didn't hear anything English

278
00:33:11,940 --> 00:33:20,020
completely independent of languages so as I used to say it gives me enough dictionaries and

279
00:33:20,020 --> 00:33:28,340
encyclopedias in klingon and I put you up a klingon system no problem the point is that we have

280
00:33:28,340 --> 00:33:35,620
even brought this to a step further because we were able to not only train in different languages

281
00:33:35,620 --> 00:33:43,620
the semantic spaces but to also topologically align them and as a result and I gave the example

282
00:33:43,620 --> 00:33:49,780
in my talk we take the word philosophy in English has a certain representation and the

283
00:33:49,780 --> 00:33:56,580
word philosophy in French has the same representation so the patterns are the same and what this means

284
00:33:56,580 --> 00:34:04,180
is you could have a system that contains English documents and you can post French queries and

285
00:34:04,180 --> 00:34:10,980
it will still work without any translation or or anything in between only for those words that

286
00:34:10,980 --> 00:34:18,420
have a fair degree of overlap or well that the word the words with the same meaning regardless

287
00:34:18,420 --> 00:34:25,620
of the language have the same fingerprint right yeah so a second prototype where I could give you

288
00:34:25,620 --> 00:34:32,500
an example is classification so our classifiers are actually just fingerprints I don't need to train

289
00:34:32,500 --> 00:34:41,460
my classifier if I say I want to get let's say all the tweets about mobile phones I can take the

290
00:34:41,460 --> 00:34:47,140
word mobile phone create a fingerprint and then compare the fingerprint of every incoming tweet

291
00:34:47,140 --> 00:34:54,740
to my fingerprint of the word mobile phone and even if it talks about iPhone it will have sufficient

292
00:34:54,740 --> 00:35:01,860
overlap for me to detect it and even if the tweet is in Chinese it will be converted into something

293
00:35:01,860 --> 00:35:07,300
that I can filter with my English mobile phone fingerprint even simultaneously in Chinese

294
00:35:08,260 --> 00:35:14,820
because you're fingerprinting that based on its language representation and there's the

295
00:35:14,820 --> 00:35:23,940
and the similarity exactly transferable from one to the next the Chinese description of the new

296
00:35:23,940 --> 00:35:29,300
iPhone generates the same fingerprint as the English description of the new iPhone

297
00:35:30,100 --> 00:35:37,140
why is that because the should we have surprised at that definitely you should be surprised

298
00:35:37,140 --> 00:35:42,180
yes and no I mean people who know two languages are able to do this in the same way yeah so there

299
00:35:42,180 --> 00:35:50,260
has to be a let's say mathematical way of doing this and the point is that we aligned the two

300
00:35:50,260 --> 00:35:56,900
semantic spaces so we have one topology that we generate in one language and we can then with a

301
00:35:56,900 --> 00:36:03,540
with a pure dictionary lookup mechanism which is the dumbest way of doing a translation

302
00:36:04,020 --> 00:36:09,860
we can convert all the distributed snippets in the vocabulary of the other language

303
00:36:09,860 --> 00:36:14,740
and use the same distribution that we have trained with for example the English method

304
00:36:15,860 --> 00:36:20,820
and therefore you have now the convenience to listen let's say to the Twitter

305
00:36:20,820 --> 00:36:27,460
firehose and regardless of what language message comes along you can filter it with an English

306
00:36:27,460 --> 00:36:35,380
example and I've done that just to give you a feeling on efficiency I've done that in real time

307
00:36:35,380 --> 00:36:43,780
on the firehose with my notebook yeah so it was sort of running locally I'm trying to run through

308
00:36:43,780 --> 00:36:50,260
the I'm trying to run through the physical analogy that were the biological analogy of this like

309
00:36:50,740 --> 00:36:56,500
and you know if the if the notion here is that you've kind of extracted this model that you know

310
00:36:56,500 --> 00:37:05,140
more closely represents what's happening in the the brain then and you can you have this kind

311
00:37:05,140 --> 00:37:12,660
of transferability across languages um is there some you know again we're kind of way beyond

312
00:37:12,660 --> 00:37:16,900
you know the pale of what's actually going to happen but like is there some like you take the

313
00:37:16,900 --> 00:37:21,460
you know some part of the brain from someone who learned Chinese and you transplanted into

314
00:37:22,660 --> 00:37:28,340
a person you know and then you know who has some other part of the brain that is kind of

315
00:37:28,340 --> 00:37:34,420
symbolically linked to English and they could then you know translate on the fly like

316
00:37:34,420 --> 00:37:41,060
yeah in theory in theory that would be possible the truth is that there has been research for

317
00:37:41,060 --> 00:37:48,340
example comparing the brain patterns of people who have who have been grown up with two languages

318
00:37:49,140 --> 00:37:55,540
they have they have a sort of speech area in the brain that is actually

319
00:37:57,060 --> 00:38:04,180
intricately mixed yeah so the two languages are represented in a mixed fashion whereas people

320
00:38:04,180 --> 00:38:09,700
who have been grown up in one language and who have then learned the other one they have added the

321
00:38:09,700 --> 00:38:17,060
second area so to say and that's the reason why a native speaker of two languages can actually

322
00:38:17,060 --> 00:38:24,420
easily do translation on the fly and can listen or read text in two languages without even noticing

323
00:38:24,420 --> 00:38:30,580
that there might be two languages right and someone who has just learned another language has

324
00:38:30,580 --> 00:38:37,860
always in his head to map from the one region into the other region now interestingly there is

325
00:38:37,860 --> 00:38:47,300
and I showed this also in my presentation there is sort of new research in brain science that

326
00:38:47,860 --> 00:38:56,100
supports our representations strongly so they were able to do an fMRI study to be precise so

327
00:38:56,100 --> 00:39:02,420
there has been an earlier version of this experiment where people were exposed to words and then they

328
00:39:02,420 --> 00:39:09,220
made like snapshots from their fMRI activity and what they found out that was in counting a

329
00:39:09,220 --> 00:39:18,900
melon if I remember well what they found out is that you can actually calculate starting with the

330
00:39:18,900 --> 00:39:23,940
picture an fMRI picture you can say what word this person was hearing when this picture was

331
00:39:23,940 --> 00:39:31,860
taken yeah so this is as you say wow but it comes better and they have sort of trained a

332
00:39:32,740 --> 00:39:38,260
machine learning algorithm to make this transfer to correlate the picture with the word that the

333
00:39:38,260 --> 00:39:45,860
persons have been hearing and the absolutely unbelievable thing is that let's say you have

334
00:39:45,860 --> 00:39:51,540
been in the fMRI first the model has been trained on your images to map to certain terms

335
00:39:51,540 --> 00:39:59,140
if now you present this very same model the images taken from my brain it will still recognize

336
00:39:59,140 --> 00:40:05,380
the terms properly and that is independent of whether we speak the same language no I'm talking

337
00:40:05,380 --> 00:40:12,980
about this has been done in English yeah I'm I'm pretty sure that even if I would do this with

338
00:40:12,980 --> 00:40:18,980
with the portuguese meaning of your English terms it still might work out but maybe not Chinese but

339
00:40:18,980 --> 00:40:28,660
but why not the the the the fact is that obviously if two individuals have been grown up

340
00:40:30,100 --> 00:40:34,900
sufficiently similar from a cultural point of view yeah so we both went to school for

341
00:40:34,900 --> 00:40:39,380
more or less the same time we more or less read the same stuff we've heard about the world

342
00:40:39,380 --> 00:40:48,740
in the same way the representation ends up being similar across individuals and in the end it's

343
00:40:48,740 --> 00:40:54,580
it makes a lot of sense I mean just imagine if we would really be wired completely different

344
00:40:54,580 --> 00:41:01,780
from one to each other it's it would be very hard to have a simple conversation yeah and in fact

345
00:41:01,780 --> 00:41:07,300
if you if you do the the investigation for example I'm pretty sure again this is just guessing

346
00:41:07,300 --> 00:41:13,860
but the fmri pictures from I don't know some distant tribe living somewhere in the Amazon and

347
00:41:13,860 --> 00:41:19,940
jungle they're the overlap between the two representations is probably less because they have

348
00:41:19,940 --> 00:41:28,820
just not been exposed to a very similar kind of environment yeah and and there is a newer

349
00:41:28,820 --> 00:41:35,220
publication which is I think it's from this year I think it's it's it's from a lab in the MIT

350
00:41:35,220 --> 00:41:44,820
and there they were actually able to create a map of about thousand words on the basically

351
00:41:44,820 --> 00:41:51,940
nearly the entire cortex and and what it shows is that every it's not like every word has a

352
00:41:51,940 --> 00:41:58,900
specific position but every word has a pattern of all sorts of positions all over the cortex that

353
00:41:58,900 --> 00:42:07,060
lights up which is in fact exactly what we are doing with our fingerprints yeah so I claim that

354
00:42:07,060 --> 00:42:14,580
we are the first NLP algorithm that gets support by fmri so this is fascinating stuff

355
00:42:15,540 --> 00:42:22,180
how do you help people make it practical like what if I'm you know if what what are the problems

356
00:42:22,180 --> 00:42:27,780
that hey if I have this problem I should be looking at this as a possible approach so so as I say

357
00:42:27,780 --> 00:42:34,180
earlier we are very strong with this approach in doing similarity calculation and therefore

358
00:42:34,180 --> 00:42:41,780
classification and as you might know in business natural language processing nearly all problems

359
00:42:41,780 --> 00:42:48,260
can be reduced to one or several classification problems okay so we do all sorts of things yeah

360
00:42:48,260 --> 00:42:59,380
I mean companies who say we want to classify our inbound emails in product requests complaints

361
00:42:59,380 --> 00:43:05,940
and I don't know looking for a person an individual in the company and believe it or not

362
00:43:07,620 --> 00:43:14,100
I haven't seen any working machine learning solution for that problem out there I mean I've

363
00:43:14,100 --> 00:43:21,540
been visiting like 150 companies over the last two years of course trying to sell our stuff

364
00:43:21,540 --> 00:43:29,220
right but I haven't seen a working solution for simple I mean this is really one of the most

365
00:43:29,220 --> 00:43:35,780
basic issues you could have and nearly nobody is actually using technology for that

366
00:43:35,780 --> 00:43:44,820
because the statistical approach has a lot of noise that comes in has false positives which is

367
00:43:44,820 --> 00:43:52,020
by the way the the biggest problem in business and we solve we solve this in a couple of weeks

368
00:43:52,660 --> 00:43:59,380
so we we make use of the efficiency of the approach in solving this kind of problems within very

369
00:43:59,380 --> 00:44:05,620
short time for people and so that's that's a specific use case are there is there like a higher level

370
00:44:05,620 --> 00:44:14,500
characterization like you know in terms of problem yeah so we have customers in the domain in a lot of

371
00:44:14,500 --> 00:44:21,300
customers for example are in the banking domain there we solve problems like compliance monitoring

372
00:44:21,300 --> 00:44:29,620
nor your customer activities or automation of business processes that depend on some text input

373
00:44:29,620 --> 00:44:39,380
at some point we have consumer good companies who want to know how to segment their customers for

374
00:44:39,380 --> 00:44:47,220
example we have a manufacturing industry where for example in technical products the documentation

375
00:44:47,220 --> 00:44:54,340
the manual of the product is so complicated good example is car industry for example a modern car

376
00:44:54,340 --> 00:45:00,500
is so complicated that if something breaks you need to visit the manual or to find out what is

377
00:45:00,500 --> 00:45:07,540
this funny light meaning there is this dangerous or can I just continue and people can't find

378
00:45:07,540 --> 00:45:15,060
anything because they have the problem that the person in the car doesn't speak the technological

379
00:45:15,060 --> 00:45:23,060
language so an example that I've that I've learned is the query where do I find the donut

380
00:45:24,020 --> 00:45:29,700
in in the US yeah so I didn't know that before but obviously the donut is a spare wheel that

381
00:45:29,700 --> 00:45:35,620
is sometimes pretty good hidden so if you look for donut in the in the manual you probably don't

382
00:45:35,620 --> 00:45:42,340
find it yeah and there is a lot of these issues yeah I mean to be even a more extreme case a person

383
00:45:42,340 --> 00:45:50,020
speaking only Spanish driving a US car and being unable to actually find the the right answer

384
00:45:50,020 --> 00:45:57,380
could use our system to sort of pose a Spanish query and be pointed to an English page for

385
00:45:57,380 --> 00:46:08,180
example so as I said I mean in principle we have solutions all across the domain we can do

386
00:46:08,180 --> 00:46:16,500
things like for example you have a LinkedIn profile you describe yourself in your LinkedIn profile

387
00:46:16,500 --> 00:46:22,580
I can make a fingerprint of your profile and if I do a fingerprint of my profile we probably have

388
00:46:22,580 --> 00:46:28,580
a lot of overlap as we are interested in the same kind of topics traditionally to make matching

389
00:46:28,580 --> 00:46:36,580
of people in HR for example you needed to actually if one person says I'm expert in G2E

390
00:46:36,580 --> 00:46:41,540
and the other person or the other job description contains Java Enterprise there was no way of

391
00:46:41,540 --> 00:46:49,300
matching it yeah in our case we matched this easily right wow so the very fascinating stuff how can

392
00:46:49,300 --> 00:46:58,020
folks learn more find out more about it contact you so basically on our website cortical.io

393
00:46:58,020 --> 00:47:09,380
what you find you can go there you find a white paper where you get basically a more in-depth

394
00:47:09,380 --> 00:47:17,140
introduction to the whole approach you find access to a public REST API that you can play around

395
00:47:17,140 --> 00:47:24,900
it's trained on Wikipedia and English Wikipedia data you can then even spin up an instance containing

396
00:47:24,900 --> 00:47:31,620
the software on Amazon or Azure to play around if you have more proprietary data so that you that

397
00:47:31,620 --> 00:47:39,620
you want to use and of course you can contact us if you need help to sort of get started I mean

398
00:47:39,620 --> 00:47:47,620
the problem is that many of us who have been struggling using conventional tooling sometimes it

399
00:47:47,620 --> 00:47:53,940
needs a little bit of help to sort of get the right angle on how to solve something yeah so we do

400
00:47:53,940 --> 00:48:01,140
for example offer a keyword extraction functionality you can throw in a text and you get like the 10

401
00:48:01,140 --> 00:48:08,260
most important keywords out of it and I've observed that many people try to systematically extract

402
00:48:08,260 --> 00:48:14,260
keywords and then try to do some magic with that and I just told them okay that keywords you

403
00:48:14,260 --> 00:48:20,020
need them if you want to show keywords at some point but you don't need them to make any computation

404
00:48:20,020 --> 00:48:26,500
because you can compare to fingerprints directly so yeah it's a bit of a mind shift change of

405
00:48:26,500 --> 00:48:32,580
mindset exactly yeah exactly right well thanks so much Francisco as it was great talking to you

406
00:48:32,580 --> 00:48:37,220
and amazing learning a little bit about what you guys are up to thanks a lot thanks

407
00:48:41,700 --> 00:48:46,500
all right everyone that's it for today's show please leave a comment on the show notes page at

408
00:48:46,500 --> 00:48:55,700
twimmaleye.com slash talk slash 10 or tweet to me at at Sam Charrington or at twimmaleye to discuss

409
00:48:55,700 --> 00:49:01,780
this show or just reach out let me know how you liked it thanks so much for listening and catch you

410
00:49:01,780 --> 00:49:20,020
next time

