1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,520
I'm your host Sam Charrington.

4
00:00:31,520 --> 00:00:36,600
In this episode I'm joined by Adam Wenzhel, Vice President of AI and Data Innovation at

5
00:00:36,600 --> 00:00:41,960
Capital One to discuss how machine learning and AI are being integrated into their day-to-day

6
00:00:41,960 --> 00:00:46,560
practices and how those advances benefit the customer.

7
00:00:46,560 --> 00:00:51,640
In our conversation we look into a few of the many applications of AI at the bank, including

8
00:00:51,640 --> 00:00:59,480
fraud detection, fighting money laundering, customer service, and automating back office processes.

9
00:00:59,480 --> 00:01:03,720
Adam describes some of the challenges of applying machine learning and financial services

10
00:01:03,720 --> 00:01:09,880
and how Capital One maintains consistent portfolio management practices across the organization.

11
00:01:09,880 --> 00:01:13,920
We also discuss how the bank is organized to scale their machine learning efforts and

12
00:01:13,920 --> 00:01:18,600
the steps they've taken to overcome the talent shortage in the industry.

13
00:01:18,600 --> 00:01:23,240
A big thanks to our friends at Capital One for their sponsorship of this show.

14
00:01:23,240 --> 00:01:27,480
As you hear in this conversation with Adam, Capital One is hosting their second annual

15
00:01:27,480 --> 00:01:31,760
Data Intelligence Conference, which will bring together machine learning practitioners

16
00:01:31,760 --> 00:01:36,160
and researchers for several days of presentations and talks.

17
00:01:36,160 --> 00:01:38,600
The conference will take place later on this summer.

18
00:01:38,600 --> 00:01:42,000
I'm really looking forward to it, so I'll keep you posted.

19
00:01:42,000 --> 00:01:47,200
A quick thanks to everyone who joined us for the first session of our Twomo Fast AI Study

20
00:01:47,200 --> 00:01:49,000
Group this past weekend.

21
00:01:49,000 --> 00:01:53,520
We had a great discussion, and I'm excited to get the group rolling.

22
00:01:53,520 --> 00:01:58,000
This first session was just an orientation, so there's still time to join in if you missed

23
00:01:58,000 --> 00:01:59,000
it.

24
00:01:59,000 --> 00:02:03,360
Keep an eye out for the recap video going up this week via the Slack group.

25
00:02:03,360 --> 00:02:07,600
You'll find more information at twomola.com slash fastai.

26
00:02:07,600 --> 00:02:14,520
Finally, our next Twomo online meetup is taking place next week on Tuesday, June 12th.

27
00:02:14,520 --> 00:02:19,720
Kelvin Ross will be presenting the paper cardiologist-level arrhythmia detection with convolutional

28
00:02:19,720 --> 00:02:24,760
neural networks, which is worked by researchers in Andrew Eng's lab at Stanford.

29
00:02:24,760 --> 00:02:29,000
For more information, visit twomola.com slash meetup.

30
00:02:29,000 --> 00:02:32,040
Alright, let's do it.

31
00:02:32,040 --> 00:02:34,360
Alright, everyone.

32
00:02:34,360 --> 00:02:39,920
I am on the line with Adam Wenzhel. Adam is vice president of AI and data innovation

33
00:02:39,920 --> 00:02:41,680
at Capital One.

34
00:02:41,680 --> 00:02:44,560
Adam, welcome to this weekend machine learning and AI.

35
00:02:44,560 --> 00:02:45,560
Thank you, Sam.

36
00:02:45,560 --> 00:02:47,040
I appreciate you having me on.

37
00:02:47,040 --> 00:02:48,040
Absolutely.

38
00:02:48,040 --> 00:02:51,680
Why don't we get started by having you tell us a little bit about your background and

39
00:02:51,680 --> 00:02:54,480
how you got involved in AI?

40
00:02:54,480 --> 00:02:55,480
Absolutely.

41
00:02:55,480 --> 00:03:00,560
I studied computer sciences and undergrad at the University of Maryland and took the only

42
00:03:00,560 --> 00:03:04,640
two AI courses that were available at the time, which is actually still the case fit at

43
00:03:04,640 --> 00:03:07,400
many undergraduate curriculums.

44
00:03:07,400 --> 00:03:10,480
When I graduated in the late 90s, I actually went to work.

45
00:03:10,480 --> 00:03:14,040
One of my professors had gone to DARPA to start a new program there, and so I was able

46
00:03:14,040 --> 00:03:18,680
to, you know, I immediately went to DARPA and was working in AI research there for the

47
00:03:18,680 --> 00:03:24,120
first couple years of my career, so I have been involved in this field for quite a while.

48
00:03:24,120 --> 00:03:28,280
It's funny to look around now and see all the, you know, how many jobs there are in this

49
00:03:28,280 --> 00:03:33,280
industry at the time, it felt like a very small community and was seen as somewhat quixotic

50
00:03:33,280 --> 00:03:34,280
at the time.

51
00:03:34,280 --> 00:03:41,760
So, this current, you know, massive surge in interest has been fascinating to watch.

52
00:03:41,760 --> 00:03:47,680
And from there, I went into a series of startups after working in DARPA for a couple years,

53
00:03:47,680 --> 00:03:52,520
some of which involved a little bit of AI, but a lot of more were straight, you know,

54
00:03:52,520 --> 00:03:54,000
web application startups.

55
00:03:54,000 --> 00:03:59,760
And then about five or six years ago, maybe a few more than that, I started, you know,

56
00:03:59,760 --> 00:04:05,880
with the kind of confluence of these really enabling technologies like GPU and big data

57
00:04:05,880 --> 00:04:11,120
and cloud computing that have kind of led to the recent resurgence of machine learning

58
00:04:11,120 --> 00:04:16,160
and coming together, you know, really started to get back into it in a big way, in particular

59
00:04:16,160 --> 00:04:22,200
in the cyber security domain, which is really ripe for machine learning.

60
00:04:22,200 --> 00:04:27,880
And so for the last six or so years, I've been really focused on machine learning.

61
00:04:27,880 --> 00:04:32,360
And then joined Capital One about two and a half years ago, initially to lead up a big

62
00:04:32,360 --> 00:04:37,680
project they had to create a new data platform and, you know, machine learning, suite of machine

63
00:04:37,680 --> 00:04:40,000
learning analytics for cyber security.

64
00:04:40,000 --> 00:04:44,800
And that has since kind of, my scope here is kind of expanded now, I help with machine

65
00:04:44,800 --> 00:04:47,040
learning implementations all across the company.

66
00:04:47,040 --> 00:04:52,880
Oh, fantastic. Well, maybe we can, we can have you give us a lay of the land and overview

67
00:04:52,880 --> 00:04:57,360
of the different ways ML and AI are being used at Capital One.

68
00:04:57,360 --> 00:05:02,560
Absolutely. So in financial, there's really kind of no end to, to the way machine learning

69
00:05:02,560 --> 00:05:06,480
can be deployed. There's a, there are a lot of possible use cases, huh?

70
00:05:06,480 --> 00:05:11,560
Absolutely. I mean, it's just a fundamentally quantitative field. And so everything from,

71
00:05:11,560 --> 00:05:16,160
you know, the way we fight fraud and there's many different flavors of fraud to financial

72
00:05:16,160 --> 00:05:22,320
crimes, like money laundering to, you know, to the way we service our customers, you know,

73
00:05:22,320 --> 00:05:26,240
making sure that they have the best experience as possible and he can get answers to the questions

74
00:05:26,240 --> 00:05:31,600
they need as quickly as possible, whether they're talking to a human or a bot or on their

75
00:05:31,600 --> 00:05:36,720
mobile app or the web app, we use it all over the place as well as for, you know, a lot

76
00:05:36,720 --> 00:05:41,640
of our internal back office processes, which for any large scale company, you know, bringing

77
00:05:41,640 --> 00:05:43,800
automation to that can create huge efficiencies.

78
00:05:43,800 --> 00:05:49,920
What would you say is an example of one of the more innovative projects that you're working

79
00:05:49,920 --> 00:05:50,920
on there?

80
00:05:50,920 --> 00:05:56,880
I think a lot of the way we're taking on fraud, both transaction fraud as well as account

81
00:05:56,880 --> 00:06:01,960
takeover fraud and, you know, identities after are really innovative. Similarly, that's

82
00:06:01,960 --> 00:06:07,840
the work we're doing and money laundering has really begun to bear some fruits recently.

83
00:06:07,840 --> 00:06:12,160
Can you walk us through one of those examples? You know, I'm curious about, you know, some

84
00:06:12,160 --> 00:06:17,760
of the data sources that come into play, you know, what you're, you know, the way you

85
00:06:17,760 --> 00:06:24,520
approach modeling there and, you know, there is related to those topics that might help

86
00:06:24,520 --> 00:06:29,200
us understand the way you go about applying ML and AI in your environment.

87
00:06:29,200 --> 00:06:34,200
Yeah, absolutely. So one of the ones that we like to talk about is an application we have

88
00:06:34,200 --> 00:06:39,640
called Second Look. And Second Look is for something that helps our customers out very

89
00:06:39,640 --> 00:06:44,600
directly. And it's, you know, I know that I'm sure all the listeners to your, to your

90
00:06:44,600 --> 00:06:50,160
podcast have are very dutiful about checking their, their credit card bill, you know,

91
00:06:50,160 --> 00:06:54,440
every day and making sure that all the transactions look correctly. But for those of us who are

92
00:06:54,440 --> 00:06:59,160
perhaps a little bit less diligent about it, you know, what we've done is we've actually

93
00:06:59,160 --> 00:07:03,840
trained a machine learning system to go and spot transactions that, you know, that, that

94
00:07:03,840 --> 00:07:07,120
we should highlight to our attention because everyone's busy. And so, you know, you don't

95
00:07:07,120 --> 00:07:10,920
always have time to review your bill. And so it's good to know that, that someone's kind

96
00:07:10,920 --> 00:07:15,600
of checking it for you, if you will. And so that can be things like spotting, say, a

97
00:07:15,600 --> 00:07:21,200
suspiciously high tip on a restaurant bill or getting double charge for the same service

98
00:07:21,200 --> 00:07:26,600
or product or just having sort of a recurring payment that, that one month, all of a sudden

99
00:07:26,600 --> 00:07:31,160
significantly higher than usual. Though there's a number of reasons why, you know, things

100
00:07:31,160 --> 00:07:36,440
that, that you might want to kind of be notified about on your credit card statement. And,

101
00:07:36,440 --> 00:07:41,160
you know, obviously there's a real, there's a real kind of a, um, precision that's needed

102
00:07:41,160 --> 00:07:46,360
because if you go lower down too many things, then it just becomes a frustrating user experience.

103
00:07:46,360 --> 00:07:50,760
But we also want to make sure that, um, that stuff doesn't slip through the cracks either.

104
00:07:50,760 --> 00:07:54,320
And so we put quite a lot of work and it's, it's really one of the, um, the kind of

105
00:07:54,320 --> 00:07:58,080
innovative ways that we're improving our customer experience. And it's funny. Like when

106
00:07:58,080 --> 00:08:01,000
I go, when I meet people at, you know, a party or something and tell my work at Capital

107
00:08:01,000 --> 00:08:04,920
One, by far and away, this is the, the thing that, you know, people mentioned the most

108
00:08:04,920 --> 00:08:08,960
consistently is, you know, how much they love this feature and how, and, and they always

109
00:08:08,960 --> 00:08:13,200
have a story about some, you know, some charge at cost that they wouldn't have caught otherwise.

110
00:08:13,200 --> 00:08:16,600
That was incorrect. They've been able to straighten out. So I think that's a great example of

111
00:08:16,600 --> 00:08:20,960
the way machine learning is, is, um, bringing some transformation into the financial services

112
00:08:20,960 --> 00:08:21,960
industry.

113
00:08:21,960 --> 00:08:28,720
Oh, that sounds like a feature I'd like to have. I imagine that the, the training data for

114
00:08:28,720 --> 00:08:35,880
something like this comes from the traditional customer reported issues. Yeah. So we do have,

115
00:08:35,880 --> 00:08:41,680
you know, there's a number of ways we look at it. Um, you know, we do have sort of a, um,

116
00:08:41,680 --> 00:08:46,440
a customer feedback loop in there where they can kind of tag stuff as being problematic

117
00:08:46,440 --> 00:08:50,920
or if alert, you know, not being problematic. I think with those kind of, um, those kind

118
00:08:50,920 --> 00:08:55,640
of systems that, that where you have kind of human generated tagging, um, they are, they

119
00:08:55,640 --> 00:09:00,440
are one source of ground truth. They're not the, the, you don't over rely on that kind

120
00:09:00,440 --> 00:09:05,080
of stuff, but it can be very helpful. I think there's also a lot of work from our, uh,

121
00:09:05,080 --> 00:09:10,240
our data scientists looking at, uh, things like, you know, using sort of, um, unsupervised

122
00:09:10,240 --> 00:09:14,560
learning and anomaly, uh, detection kind of approaches to kind of, uh, tease out some

123
00:09:14,560 --> 00:09:18,520
of the patterns and the data and, and, and that's another way that, um, through things

124
00:09:18,520 --> 00:09:23,720
like, uh, you know, label propagation, we can, we can, um, uh, begin to, you know, really

125
00:09:23,720 --> 00:09:29,240
hone in on the transactions that, that we want to alert off of. Oh, interesting. Uh, and

126
00:09:29,240 --> 00:09:34,560
you mentioned that you came in to, uh, capital one with a focused on cyber security.

127
00:09:34,560 --> 00:09:39,680
I imagine that that's a big deployment area as well. It is. Yeah. There's a, you know,

128
00:09:39,680 --> 00:09:44,800
that with, with cyber security, you just have such massive, um, amounts of data and even

129
00:09:44,800 --> 00:09:50,000
with a large team of analysts who staff are, um, cyber security operations center, uh,

130
00:09:50,000 --> 00:09:54,520
they're really constrained to only be able to look at a really small amount of the number

131
00:09:54,520 --> 00:09:58,960
of events that we have on our, our network and our computers every day. And so, you

132
00:09:58,960 --> 00:10:03,320
know, machine learning really gives, uh, a great opportunity to number one, cut through

133
00:10:03,320 --> 00:10:09,080
all that noise and really kind of surface the events that are, um, most that, that, that

134
00:10:09,080 --> 00:10:13,160
we really want to have the analyst focus on that may be suspicious that, you know, or

135
00:10:13,160 --> 00:10:16,120
maybe they're completely benign, but we need to have someone to check into and then we

136
00:10:16,120 --> 00:10:20,520
kind of get smarter, uh, along the process. And so that can be anything from a piece of

137
00:10:20,520 --> 00:10:24,960
malware on a computer to some sort of, uh, attempted data exfiltration. There's, there's

138
00:10:24,960 --> 00:10:29,880
a lot of, um, a lot of different attack vectors, malicious emails, whether that's phishing

139
00:10:29,880 --> 00:10:33,880
or spearfishing or extortion attempts or, you know, there's just quite a lot of, uh, threats

140
00:10:33,880 --> 00:10:40,800
that, that any large company faces and particularly one that, um, uh, manages a great deal of money.

141
00:10:40,800 --> 00:10:46,120
And a situation like cyber security where, you know, capital one certainly is far from alone

142
00:10:46,120 --> 00:10:52,400
and having to worry about these kinds of concerns. Do you find that, uh, your primary vector

143
00:10:52,400 --> 00:10:58,560
for kind of inserting machine learning into solving some of these problems is via off

144
00:10:58,560 --> 00:11:03,840
the shelf software, meaning your vendors kind of built, uh, ML and AI into their products

145
00:11:03,840 --> 00:11:08,800
or you kind of out in front of it with your own data scientist building kind of custom

146
00:11:08,800 --> 00:11:15,360
systems to help you stay, uh, ahead of, uh, your cyber adversaries.

147
00:11:15,360 --> 00:11:20,960
Yeah, it's a, it's a very good question. I think for us, we do buy a lot of kind of

148
00:11:20,960 --> 00:11:26,120
best-to-breed, um, cyber security products and I think it would be, um, uh, you know, it's

149
00:11:26,120 --> 00:11:29,760
certainly advantageous. It's really important to take advantage of, of kind of the rich

150
00:11:29,760 --> 00:11:35,480
product ecosphere that's out there. Um, there are, you know, when, when you kind of deploy

151
00:11:35,480 --> 00:11:39,520
a lot of these solutions, um, there are still opportunities that present themselves that

152
00:11:39,520 --> 00:11:44,720
are either, you know, unique to the, to the, to the, to capital one or just, um, that

153
00:11:44,720 --> 00:11:50,240
are created by the way that the, the product marketplace landscape, um, lines up to, uh,

154
00:11:50,240 --> 00:11:55,200
to, um, kind of even strengthen things beyond that. And that's, we, we focus on those opportunities.

155
00:11:55,200 --> 00:11:59,280
Are there any examples in that domain you can give us at least at the high level?

156
00:11:59,280 --> 00:12:03,960
Um, sure. One of the ones that we've, we've talked about, uh, at a couple conferences

157
00:12:03,960 --> 00:12:09,880
recently is on detecting malware callouts and a, and, um, specifically malware callouts

158
00:12:09,880 --> 00:12:15,640
that use, uh, DGA, DGA's domain generating algorithms. Um, we've done some pretty innovative

159
00:12:15,640 --> 00:12:21,760
work there that we've presented on a conferences and, and, you know, um, we'll potentially, uh,

160
00:12:21,760 --> 00:12:25,440
be sharing in the future. But that's, uh, you know, it's, it's been a fun challenge

161
00:12:25,440 --> 00:12:29,880
where we've applied some of our really, um, uh, interesting machine learning approaches

162
00:12:29,880 --> 00:12:34,680
using convolutional neural networks and other features, um, to, to bring kind of a, uh, this,

163
00:12:34,680 --> 00:12:37,720
this, this problem has been around for a while. And it's kind of one that, uh, a number

164
00:12:37,720 --> 00:12:41,120
of AI researchers have taken on and, and, you know, we have a pretty novel approach which

165
00:12:41,120 --> 00:12:46,120
has been, um, working well for us. So what's, uh, what's a malware callout?

166
00:12:46,120 --> 00:12:50,800
So you can imagine if you're, you know, if, if I'm sure it would be you, but if one of

167
00:12:50,800 --> 00:12:54,840
your friends' computers got infected because they clicked on the wrong link or, you know,

168
00:12:54,840 --> 00:12:58,320
double clicked on the wrong attachment and their email, uh, and then had a piece of malware

169
00:12:58,320 --> 00:13:02,400
on their computer, it might, you know, once it, once your machine's infected, the next

170
00:13:02,400 --> 00:13:07,400
stage is for it to reach out to a server controlled by the attacker and say, hey, I managed to affect

171
00:13:07,400 --> 00:13:11,520
the, in fact, this computer, what was you like me to do? And that can be anything from

172
00:13:11,520 --> 00:13:17,160
lock their computer to we pay, to they pay us a, a, a ransom and cryptocurrency to, um,

173
00:13:17,160 --> 00:13:21,800
look, search the computer for, um, any files containing with a title financial disclosures

174
00:13:21,800 --> 00:13:26,140
or something like that. And, and, you know, uh, download those to the, to the most

175
00:13:26,140 --> 00:13:30,080
a server, I mean, there's a wide variety of things you can do once the computer's infected.

176
00:13:30,080 --> 00:13:34,680
A lot of times it's participation in a botnet where you're, um, maybe trying to, you know,

177
00:13:34,680 --> 00:13:39,440
hack into another, a system controlled by another company, um, through some sort of

178
00:13:39,440 --> 00:13:46,680
brute force method. And so when it makes that call out to the, to the server, it, um, uh,

179
00:13:46,680 --> 00:13:51,800
you know, we, we focus on kind of detecting at that point. Um, uh, that's one of the areas

180
00:13:51,800 --> 00:13:56,840
that we've explored. And there, those, the, um, the way that those callouts are made,

181
00:13:56,840 --> 00:14:02,760
the way that those servers are effectively domain names they use, um, some of the attackers

182
00:14:02,760 --> 00:14:08,320
have gotten come up with some really complex algorithms for, um, basing the host names

183
00:14:08,320 --> 00:14:13,480
on either some sort of time-based algorithm where, you know, maybe there's like 20,000

184
00:14:13,480 --> 00:14:19,920
different domain names per given day. And you can say at any given, at any given time,

185
00:14:19,920 --> 00:14:24,400
there could be one that you decided to register that your malware will call out to, which

186
00:14:24,400 --> 00:14:28,200
makes, you know, the traditional kind of blacklist, whitelist approaches to blocking

187
00:14:28,200 --> 00:14:33,880
malware callouts very difficult. Um, and, and so we've focused, you know, that's one

188
00:14:33,880 --> 00:14:38,680
of the categories of attacks that we've focused on. And you mentioned that part of the solution

189
00:14:38,680 --> 00:14:46,440
here is involves the use of convolutional neuralness. CNNs are typically used in, like, image

190
00:14:46,440 --> 00:14:51,880
recognition types of tasks. How have you applied them to, uh, the cyber security use case?

191
00:14:51,880 --> 00:14:57,040
Yes. So CNNs are famous, certainly, for a kind of computer vision task, but more recently,

192
00:14:57,040 --> 00:15:01,160
there's been quite a lot of work. I think a real kind of, um, you know, surge of interest

193
00:15:01,160 --> 00:15:06,560
in using them for more NLP-based tasks. And that's, that's, this, this, this, this kind

194
00:15:06,560 --> 00:15:11,960
of category of, of, um, problem that I'm describing fits into that NLP category.

195
00:15:11,960 --> 00:15:15,600
Can you elaborate on that a little bit more? Sure. So, you know, really what we're trying

196
00:15:15,600 --> 00:15:22,040
to do is distinguish between, uh, you know, a host name that, um, that maybe is, let's

197
00:15:22,040 --> 00:15:28,240
say, phonetically plausible, but not a real word, like Google, um, to one that, uh, is just

198
00:15:28,240 --> 00:15:33,960
a string of jumbled up letters. Um, that's, that's kind of the first, uh, step. And then,

199
00:15:33,960 --> 00:15:37,760
you know, some of the more recent attacks actually use, uh, our dictionary-based attacks,

200
00:15:37,760 --> 00:15:40,720
where they'll generate the domain name from real dictionary words, but just put them in

201
00:15:40,720 --> 00:15:46,440
odd combinations. So understanding, like, what's an odd combination and what's not as Facebook,

202
00:15:46,440 --> 00:15:51,440
an odd combination, or does that make sense? And it really kind of gets into, uh, a decent

203
00:15:51,440 --> 00:15:57,320
amount of kind of language understanding to, to really, um, uh, to, to create a model

204
00:15:57,320 --> 00:16:02,000
that can accurately distinguish between those types of, uh, domain names. Ah, it sounds

205
00:16:02,000 --> 00:16:07,360
a bit like, uh, the challenge at Google and others face keeping up with, with spammers

206
00:16:07,360 --> 00:16:13,320
as spammers learn how to use proper grammar. Very, very much, very much so it's, you know,

207
00:16:13,320 --> 00:16:17,000
it's continually, uh, whether it's spamming or cyber security, it's a constant cat

208
00:16:17,000 --> 00:16:22,560
and mouse game, right? So, uh, it's, it keeps it fun. I never gets boring. And so one of

209
00:16:22,560 --> 00:16:29,920
the challenges that the industry has been, you know, faced with is, uh, is the talent

210
00:16:29,920 --> 00:16:36,800
shortage. Um, how have you managed at, at the bank to address that? Yeah, it's a good

211
00:16:36,800 --> 00:16:41,600
question. There is, there is a massive talent shortage going on right now. And, you know,

212
00:16:41,600 --> 00:16:45,200
there's no, there's no silver bullets to that answer to that question, but, um, there's

213
00:16:45,200 --> 00:16:50,040
a number of ways we're addressing it. So obviously, we, we are hiring, you know, going

214
00:16:50,040 --> 00:16:55,400
aggressively after recruiting people who have, um, good amounts of experience in that

215
00:16:55,400 --> 00:17:00,320
field. They're not a lot of those people, but, but we're certainly, uh, you know, very,

216
00:17:00,320 --> 00:17:03,840
uh, you know, work very hard to make sure that we can attract as many as possible. We

217
00:17:03,840 --> 00:17:07,680
also have a program at Capital One called the TDP, the technology development professional

218
00:17:07,680 --> 00:17:13,640
where we go out to primarily the top 20 computer science departments in the nation and, uh,

219
00:17:13,640 --> 00:17:18,520
and recruit heavily from that. And once they're here, they work all over in a bunch of

220
00:17:18,520 --> 00:17:21,880
different technology areas, but the ones that come to the center for machine learning,

221
00:17:21,880 --> 00:17:26,320
um, we actually have a pretty rigorous, um, process around, you know, training them

222
00:17:26,320 --> 00:17:29,920
and getting them up to speed on machine learning. And so we're, we're kind of famous for

223
00:17:29,920 --> 00:17:35,080
being the group that, uh, gives homework. And, um, we do things like we have a weekly

224
00:17:35,080 --> 00:17:39,760
paper sessions where we'll, we'll review kind of academic papers and, and, and sort of

225
00:17:39,760 --> 00:17:43,440
discuss their merits. And, you know, I think that's one of the things that really distinguishes

226
00:17:43,440 --> 00:17:49,120
the machine learning from, uh, other kind of software engineering computer science disciplines

227
00:17:49,120 --> 00:17:54,440
is it, it still has a very academic bent to it. Um, so even though there's like massive

228
00:17:54,440 --> 00:17:59,040
applicability across a wide range of business problems, um, there's still kind of a culture

229
00:17:59,040 --> 00:18:03,400
of, you know, making sure that you're keeping up with the latest and academic literature.

230
00:18:03,400 --> 00:18:08,400
And the academic literature is, is, is produced, you know, by, uh, and, you know, probably,

231
00:18:08,400 --> 00:18:13,400
this is much by commercial teams as it is by academic teams. And so, uh, it really creates

232
00:18:13,400 --> 00:18:17,440
a unique culture. So that's, you know, making sure that you have, you're continuing to do

233
00:18:17,440 --> 00:18:21,800
ongoing training and that you're, you're able to take people into the, into the group

234
00:18:21,800 --> 00:18:25,880
and train additional machine learning talent. That way is, uh, immensely important to

235
00:18:25,880 --> 00:18:29,960
being able to meet the, the talent needs that we have. It sounds like you're going after

236
00:18:29,960 --> 00:18:36,720
the same students that, uh, Google and a Facebook might go after. Do you find that you're challenged

237
00:18:36,720 --> 00:18:42,920
to convince them to, uh, to join a bank as opposed to, you know, one of these more sexy kind

238
00:18:42,920 --> 00:18:48,280
of internet-borne social media type of plays? Yeah, it's a good question. We, we definitely

239
00:18:48,280 --> 00:18:53,080
do compete head with, with a lot of the tech companies, um, for, for talent. And, you

240
00:18:53,080 --> 00:18:58,160
know, and that is a challenge. It's, fortunately, um, getting easier is more and more people

241
00:18:58,160 --> 00:19:02,880
become aware of, of, you know, really what a tech company, Capital One has become. Um,

242
00:19:02,880 --> 00:19:07,400
but, you know, the, the way I look at it is, uh, I, I, Capital One, you know, I think that

243
00:19:07,400 --> 00:19:12,040
we've seen disruption happen via machine learning across a wide range of industries,

244
00:19:12,040 --> 00:19:16,800
right? Whether we're talking logistics with self-driving cars or healthcare or commerce.

245
00:19:16,800 --> 00:19:20,560
And I, and I really think that, um, financial services is, is, you know, you're beginning

246
00:19:20,560 --> 00:19:23,520
to see the, you're seeing the beginnings of a similar kind of disruption. And so I

247
00:19:23,520 --> 00:19:28,120
think there are a lot of people who are really excited about, uh, you know, not just taking

248
00:19:28,120 --> 00:19:31,760
on marketing and advertising, but actually being able to, to apply their machine learning

249
00:19:31,760 --> 00:19:35,440
skill to something really impactful, like, how do people manage their money and, and how

250
00:19:35,440 --> 00:19:38,960
can we empower people more for their, to take better control of their financial lives

251
00:19:38,960 --> 00:19:44,720
and really kind of set them up for lifelong financial success? Uh, you mentioned Capital

252
00:19:44,720 --> 00:19:48,920
One kind of becoming a, a more and more like a tech company. And I think we kind of throw

253
00:19:48,920 --> 00:19:52,920
around in the industry that, you know, all companies are going to need to look more and

254
00:19:52,920 --> 00:19:57,680
more like software companies over time, sooner rather than later. In fact, uh, but Capital

255
00:19:57,680 --> 00:20:02,960
One actually has a pretty long history at this. I remember, um, Capital One was featured

256
00:20:02,960 --> 00:20:09,640
in, and a book that I read many years ago, competing on analytics by, uh, Thomas Devon

257
00:20:09,640 --> 00:20:14,560
Port, I think is the name of the author, uh, but really talked about how the company

258
00:20:14,560 --> 00:20:22,240
was before machine learning became cool, like it is now, um, really applying predictive

259
00:20:22,240 --> 00:20:28,400
analytics and other things to, uh, to the way it, you know, made decisions to the way

260
00:20:28,400 --> 00:20:35,400
the company made decisions. So my question is, in the company, do you find that having

261
00:20:35,400 --> 00:20:42,720
that history is kind of predisposed the, your business counterparts to kind of understand

262
00:20:42,720 --> 00:20:49,640
and be ready and willing to work with, uh, machine learning and, and predictive and statistical

263
00:20:49,640 --> 00:20:55,760
kinds of approaches. And, and algorithms already still find yourself, you know, with many

264
00:20:55,760 --> 00:21:00,720
of the cultural challenges that some of your counterparts and, and companies that don't

265
00:21:00,720 --> 00:21:06,000
have that history might have. Yeah, it's a, it's a great question. And, uh, and I think

266
00:21:06,000 --> 00:21:10,360
that's really good context about Capital One. Um, it's something that I didn't fully appreciate

267
00:21:10,360 --> 00:21:14,520
when I first started talking to them about joining, but, but I've really come to appreciate

268
00:21:14,520 --> 00:21:20,600
now, um, which is that the, the DNA of the company is very much in that, um, data analytics

269
00:21:20,600 --> 00:21:25,680
space. And that's how they, you know, that was their whole initial kind of founding hypothesis.

270
00:21:25,680 --> 00:21:30,560
And, um, Capital One is still founder led. And so Rich Fairbank, who's the CEO who started

271
00:21:30,560 --> 00:21:34,480
the company with that conviction still runs the company with that conviction. And it really

272
00:21:34,480 --> 00:21:40,240
trickles down. I think where, um, you know, machine learning is obviously a pretty, uh, pretty

273
00:21:40,240 --> 00:21:44,600
different shift from kind of the, some of the traditional, uh, stats and quants that have

274
00:21:44,600 --> 00:21:49,000
been done, um, and a lot of companies for, for many years, and it requires sort of a slightly

275
00:21:49,000 --> 00:21:54,000
different way of thinking about problems. And so I think that, yes, there's a long, um,

276
00:21:54,000 --> 00:21:58,960
it's, it's squarely in the company's DNA to think about, um, uh, you know, to think

277
00:21:58,960 --> 00:22:03,520
in a data driven way and data driven insights are worth their weight in gold. Um, I think

278
00:22:03,520 --> 00:22:07,920
we have had to evolve like a lot of people, um, along, you know, more machine learning

279
00:22:07,920 --> 00:22:11,600
lines. And that's something that we're, that we're embracing. But the DNA of the company

280
00:22:11,600 --> 00:22:16,840
is, is definitely there. Um, and it's been amazing to watch how much the company has, has

281
00:22:16,840 --> 00:22:22,520
embraced that other specific aspects that you can point to or examples you can give

282
00:22:22,520 --> 00:22:28,760
as to the way that that, uh, cultural shift is manifesting at Capital One. What are the,

283
00:22:28,760 --> 00:22:34,640
the things that you're finding, uh, your business counterparts kind of needing to hear the

284
00:22:34,640 --> 00:22:40,120
most and how are you articulating helping them come to terms with those things? Yeah, I

285
00:22:40,120 --> 00:22:44,200
think they're, you know, there are a number of changes. And I think, you know, everyone's

286
00:22:44,200 --> 00:22:48,920
very, um, I think everyone's really excited and enthusiastic about it. And so there's been,

287
00:22:48,920 --> 00:22:52,840
you know, what I've seen, people have really taken it upon themselves to kind of go out

288
00:22:52,840 --> 00:22:57,720
and educate themselves on, um, what machine learning can, can accomplish and, you know,

289
00:22:57,720 --> 00:23:02,840
separating the hype from their reality. I think, uh, one example is the, of, of kind of

290
00:23:02,840 --> 00:23:07,880
the evolution that's needed is, um, you know, traditionally, I think in a lot of large

291
00:23:07,880 --> 00:23:13,560
companies, uh, data science and, and software and systems engineering were sort of distinct

292
00:23:13,560 --> 00:23:20,720
activities. And a lot of the, the really disruptive, um, machine learning, uh, systems that

293
00:23:20,720 --> 00:23:25,520
you're seeing being built at, you know, in any industry are really kind of, uh, being

294
00:23:25,520 --> 00:23:30,400
produced when you have that software engineering, data engineering and, um, the, the kind of

295
00:23:30,400 --> 00:23:37,280
data science all working together as one, um, whole. Um, so if you talk about like a reinforcement

296
00:23:37,280 --> 00:23:41,680
learning system or, you know, any, any sort of interactive system machine learning system,

297
00:23:41,680 --> 00:23:46,720
you know, it's, it's critical that, uh, it's, it's not just a standalone model, but, but

298
00:23:46,720 --> 00:23:51,760
it's part of a system. And so I think that's been, um, a big, uh, you know, it's been kind

299
00:23:51,760 --> 00:23:57,040
of an evolution for us to, to really make sure we're building teams and, and setting up projects

300
00:23:57,040 --> 00:24:01,360
in that way, um, to take advantage. But that fundamental belief that, that data driven

301
00:24:01,360 --> 00:24:08,080
insights or, or, or data analytics can really, um, power great games is, is, is, has always

302
00:24:08,080 --> 00:24:15,040
been in the company's DNA. One of the big things that came out in, uh, an event that I recently

303
00:24:15,040 --> 00:24:23,600
held, uh, AI summit was that, you know, often companies, particularly large companies get caught

304
00:24:23,600 --> 00:24:32,720
up and comparing traditional software engineering with, uh, data science and, uh, data science driven

305
00:24:32,720 --> 00:24:38,240
efforts. And, uh, the particular area that was mentioned was just this notion of the, you know,

306
00:24:38,240 --> 00:24:44,800
the keyword and data science being science and it's a much more exploratory type of, uh, type of

307
00:24:44,800 --> 00:24:51,040
process and doesn't lend itself to, you know, agile, for example, uh, in the way that traditional

308
00:24:51,040 --> 00:24:55,280
software development does, is that, uh, is that your experience there as well? And how,

309
00:24:56,080 --> 00:25:00,960
how have you been kind of working to, you know, what are your methodologies there? And how have

310
00:25:00,960 --> 00:25:08,080
you been working to kind of fuse software engineering and, and, uh, data science and the other,

311
00:25:08,080 --> 00:25:13,040
you mentioned data engineering as well into a process that kind of works well for Capital One.

312
00:25:13,040 --> 00:25:18,800
Yeah, it's a good question. There's a couple, a couple big themes, um, wrapped up in that. The,

313
00:25:18,800 --> 00:25:23,920
the first one is you're talking about the science and data science and a lot of times it has been,

314
00:25:23,920 --> 00:25:29,120
essentially humans kind of experimenting with the data, uh, looking for, for insights, right,

315
00:25:29,120 --> 00:25:33,040
like trying to find, you know, in search of an insight and then finding that insight and then

316
00:25:33,040 --> 00:25:38,320
turning it into some sort of model that, that gets, um, uh, that can be, that can be put into

317
00:25:38,320 --> 00:25:43,840
production and make predictions. Um, the, uh, with machine learning systems, what we're really

318
00:25:43,840 --> 00:25:50,400
trying to do is create a system that sort of, uh, continually generates these insights in an

319
00:25:50,400 --> 00:25:56,000
automated way. And that's, you know, a pretty, pretty big shift from sort of a human looking for

320
00:25:56,000 --> 00:26:00,640
insights for themselves to generate building a system that can kind of, uh, continually look for

321
00:26:00,640 --> 00:26:04,800
insights and, and generate them. And so it does require a slightly different skill set in a slightly

322
00:26:04,800 --> 00:26:10,720
different way of, of thinking. Um, and then in terms of the methodology, you know, agile, uh,

323
00:26:10,720 --> 00:26:15,600
is great for software development. I think a lot of modeling is much less deterministic. Like,

324
00:26:15,600 --> 00:26:21,200
if you're trying to achieve a certain degree of accuracy on a model or efficacy on a model, um,

325
00:26:21,200 --> 00:26:26,320
it's really tough to, to predict and plan out, you know, how quickly you'll get there and,

326
00:26:26,320 --> 00:26:30,880
and what you, you know, you may have theories, but, um, it's just not that deterministic about

327
00:26:30,880 --> 00:26:35,280
when you're going to achieve the kind of results that you need to make a model really valuable.

328
00:26:35,280 --> 00:26:40,560
And so you, you know, the process, I think that, that people end up adopting is a little bit

329
00:26:40,560 --> 00:26:46,000
different than the traditional kind of agile methodology because, uh, you just, you have to take

330
00:26:46,000 --> 00:26:51,760
a really exploratory, um, approach to it because you're not sure, you know, what each step,

331
00:26:51,760 --> 00:26:56,000
you, whenever you take a step forward, it's going to, you're going to learn sort of on the fly,

332
00:26:56,000 --> 00:27:00,480
what the next avenues of exploration are going to be and you need to be prepared to react with that.

333
00:27:01,280 --> 00:27:08,320
Right, right. Uh, yeah, one of the, the interesting slides that came out of this session was that,

334
00:27:08,320 --> 00:27:14,720
you know, whereas you can kind of think of traditional engineering and, and agile as being more linear,

335
00:27:14,720 --> 00:27:18,960
right, we have these burn down charts and we're kind of creating linear value over time

336
00:27:19,600 --> 00:27:25,120
with, uh, machine learning is more of an S curve because of that exploration up front. And this is,

337
00:27:25,120 --> 00:27:31,360
uh, you know, kind of in mapping out ROI, that ROI takes longer to kind of get to and require

338
00:27:31,360 --> 00:27:39,440
some critical mass, but then when you do, you're, uh, able to, to, you know, get a significant ROI

339
00:27:39,440 --> 00:27:44,880
in your efforts relative to, again, traditional development. Do you see that kind of relationship

340
00:27:44,880 --> 00:27:50,960
between ROI and machine learning as well? I definitely agree that there's a lot of times a longer

341
00:27:50,960 --> 00:27:57,440
up front period of exploration. Um, we've seen uncertain projects and pretty dramatic ROI on these

342
00:27:57,440 --> 00:28:04,640
projects like, um, just because, uh, if you're, if you're coming into a system or, or, or a, uh,

343
00:28:04,640 --> 00:28:10,320
a business area that hasn't, you know, essentially right now with machine learning because

344
00:28:10,320 --> 00:28:14,960
it hasn't been applied broadly, there's a lot of kind of green field opportunities and

345
00:28:14,960 --> 00:28:19,040
kind of low hanging through. Yeah, a lot of low hanging fruit, um, but, but, you know,

346
00:28:19,040 --> 00:28:24,080
stuff that can be really impactful. And, and so, you know, some of the, some of the RF,

347
00:28:24,080 --> 00:28:29,760
the ROI on, on, on some of these initial like, on, on projects can be, can be pretty dramatic.

348
00:28:30,480 --> 00:28:34,480
I think that the, you know, the hard part is just scaling up the talent so that you, you can,

349
00:28:35,840 --> 00:28:40,720
you can, you can, uh, implement systems in all the, the opportunity areas that, that you have.

350
00:28:40,720 --> 00:28:46,240
And so given, uh, a constrained base of talent, how do you approach portfolio management across

351
00:28:46,240 --> 00:28:51,200
the business? Um, do you, uh, you know, I've talked to some folks that kind of take, you know,

352
00:28:51,200 --> 00:28:55,840
all their talent and apply it to, you know, their most pressing problems, kind of like a moonshot

353
00:28:55,840 --> 00:29:00,160
approach, you know, with the idea that, you know, if they solve or make a dent in that, they'll

354
00:29:00,160 --> 00:29:06,560
have a huge impact, you know, other folks, you know, go for kind of quick wins to help establish,

355
00:29:06,560 --> 00:29:10,320
you know, a machine learning way of thinking within organization. How do you balance that?

356
00:29:11,680 --> 00:29:15,520
Yeah, I think that's, you know, that, that's right on the mark for us. We started this center for

357
00:29:15,520 --> 00:29:19,120
machine learning about a year and a half ago. And I think when initially we were getting started,

358
00:29:19,120 --> 00:29:24,080
and we were sort of building some organizational muscle, um, we took on a fairly broad kind of

359
00:29:24,080 --> 00:29:29,040
organic set of projects that really allowed us to, um, help the business partners understand

360
00:29:29,040 --> 00:29:34,720
what machine learning could do and help us develop a little muscle around delivery. And, um, and,

361
00:29:34,720 --> 00:29:39,040
and that's what we got started. But, you know, once we've been doing that for a while, we sort of, uh,

362
00:29:39,040 --> 00:29:44,000
said, all right, we need to take a step back and actually sort of take a, a top-down assessment of

363
00:29:44,000 --> 00:29:49,680
the, of the, um, uh, of the enterprise and understand, like, where are the really high leverage

364
00:29:49,680 --> 00:29:54,640
opportunities for machine learning? And so that was, it's obviously, um, a conversation that we had

365
00:29:55,360 --> 00:30:00,800
with our business partners who have a lot more context around, you know, their business areas.

366
00:30:00,800 --> 00:30:05,840
And we really worked together to help understand and prioritize what those areas are. And we've

367
00:30:05,840 --> 00:30:10,720
devoted, you know, probably the lion's share of our resources against those really transformative

368
00:30:10,720 --> 00:30:16,560
opportunities. Um, that said, we still do hold back a team, uh, our internal consulting team

369
00:30:16,560 --> 00:30:22,000
that's available for sort of, like, the, the broader ones that may not be, um, the moonshots,

370
00:30:22,000 --> 00:30:26,560
but that, uh, but that are really important. Um, so we want to make sure we're servicing

371
00:30:26,560 --> 00:30:30,800
both. And we're, we have the right balance there between going after these really transformative

372
00:30:30,800 --> 00:30:36,480
opportunities, but also not ignoring, like, the, the hundreds of efficiency gains that collectively

373
00:30:36,480 --> 00:30:42,400
can be, uh, really add up to something pretty powerful. How are you organized around data science?

374
00:30:42,400 --> 00:30:46,160
It sounds like you've got a center. So there's some central centralization there. But what's the

375
00:30:46,160 --> 00:30:51,120
relationship between, uh, that center for machine learning and the various business units?

376
00:30:52,640 --> 00:30:57,440
Um, yeah. So, you know, it is, we have a center of excellence, but it's the machine learning

377
00:30:57,440 --> 00:31:02,400
expertise is, you know, across the company and the data science community is, is there's,

378
00:31:02,400 --> 00:31:05,920
you know, data sciences spread all across the company. So I don't, I certainly don't want to give

379
00:31:05,920 --> 00:31:10,960
the impression that it's all central, because that's, that's not the case. I think having,

380
00:31:10,960 --> 00:31:15,600
having the center of excellence has allowed us to have a, a small core of that, like,

381
00:31:15,600 --> 00:31:22,800
you know, really work closely together to sort of, um, help define best practices and really,

382
00:31:23,600 --> 00:31:27,040
you know, it's nice when you have a tight cluster of people because the amount of knowledge

383
00:31:27,040 --> 00:31:32,240
share and shared learnings, um, is, is tremendous. And it really kind of accelerates the, the growth

384
00:31:32,240 --> 00:31:38,080
uh, professionally of, of that group. Um, but, but we are, you know, we have people who rotate

385
00:31:38,080 --> 00:31:43,360
out from that center and go into the lines of business and, and help kind of see teams that way.

386
00:31:43,360 --> 00:31:48,720
And so there's a number of ways that we kind of manage that balance and, um, and so machine

387
00:31:48,720 --> 00:31:54,400
learning is really like our, all the, the business teams have embedded data science groups in them

388
00:31:54,400 --> 00:31:58,400
as well that are, that are really doing great things with machine learning. And, um, so the center

389
00:31:58,400 --> 00:32:02,480
of excellence is kind of one of the, one of the mechanisms we have towards building out

390
00:32:02,480 --> 00:32:10,480
machine learning systems, the capital one. So for an enterprise, you know, in, uh, financial services

391
00:32:10,480 --> 00:32:17,360
or elsewhere, um, that, you know, recognizes the importance of machine learning, you know,

392
00:32:17,360 --> 00:32:22,240
and is somewhere on the, the spectrum, but doesn't yet have a center of excellence of some sort.

393
00:32:22,240 --> 00:32:27,040
Is that something that you recommend? And, and if so, what, what do you think? Well, groundwork has

394
00:32:27,040 --> 00:32:34,720
to be in place before, uh, one can do that. Um, I, I do think it, uh, it's something I recommend.

395
00:32:34,720 --> 00:32:39,200
I think that it, one of the reasons why we've been able to attract a very high level, uh, you

396
00:32:39,200 --> 00:32:44,640
know, three of talent and a very, very competitive, um, talent segment is that working in the center

397
00:32:44,640 --> 00:32:49,280
of excellence is, is very, um, it's a very compelling opportunity for machine learning professional.

398
00:32:49,280 --> 00:32:55,760
And, and so, you know, from that perspective, I think it's been, uh, enormously helpful. Um, so I

399
00:32:55,760 --> 00:33:00,880
think that the, you know, as to what the groundwork you need to lay is, essentially, if you need

400
00:33:00,880 --> 00:33:05,680
to have a strong core of people and initial core, and because that's what's going to attract, you

401
00:33:05,680 --> 00:33:11,760
know, the, the additional talent. And, and so that's for us a big part of the reason why we initially

402
00:33:11,760 --> 00:33:16,800
created the center was to, to, to create that really kind of attractive place for, for talented

403
00:33:16,800 --> 00:33:22,320
machine learning professionals to come work at. When you look out across your industry,

404
00:33:22,320 --> 00:33:28,960
what unique challenges do you see to applying machine learning within the financial services

405
00:33:28,960 --> 00:33:33,760
context? I think the, the big challenges are just, you know, it's a heavily regulated environment,

406
00:33:33,760 --> 00:33:40,160
right? And so, um, it's one of the reasons we are focusing or, or investing heavily in areas like,

407
00:33:41,600 --> 00:33:49,600
explainability and fairness, um, because, you know, there's obviously many, many, uh, years of,

408
00:33:49,600 --> 00:33:55,280
of regulation and, and, um, working with the regulators to understand best practices for managing

409
00:33:55,280 --> 00:34:00,000
models and, and making sure that you're managing the risk and that's generated from those models

410
00:34:00,000 --> 00:34:06,000
appropriately. And so I think that's a, um, you know, it's, it's, it's a really kind of unique

411
00:34:06,000 --> 00:34:10,880
to financial services, um, practices around that and, and understanding how to navigate that.

412
00:34:10,880 --> 00:34:14,000
You know, it's tough. I think one of the reasons why I think we've seen less disruption from

413
00:34:14,000 --> 00:34:18,080
startups in the financial services industry is because of those type of challenges. And it really

414
00:34:18,080 --> 00:34:25,200
takes, you know, a, a larger bank that has the, the, um, all the teams and the personnel and the

415
00:34:25,200 --> 00:34:30,880
experience of having dealt with, uh, you know, those type of regulatory processes for many years to,

416
00:34:30,880 --> 00:34:36,720
to, uh, to be able to push more aggressively into, um, deploying machine learning in, in, in some

417
00:34:36,720 --> 00:34:43,520
of these, uh, heavily regulated areas. Uh, so on the one hand, financial services has had to deal

418
00:34:43,520 --> 00:34:50,000
with a lot of these issues before, but at the same time machine learning is, is changing things.

419
00:34:50,000 --> 00:34:54,960
What are some of the things that are changing in the way a bank would have to think about or deal

420
00:34:54,960 --> 00:34:59,760
with the issues that you've mentioned, you know, ethics, fairness, transparency, those kinds of things?

421
00:35:00,480 --> 00:35:04,960
Yeah. So I think that the, the, we could, we could talk for hours just about this subject, uh,

422
00:35:04,960 --> 00:35:09,680
uh, but I think, um, with, with a lot of these models, there's, they're effectively,

423
00:35:09,680 --> 00:35:13,440
you, there are more complex than, than the models that have been traditionally used and what that

424
00:35:13,440 --> 00:35:19,200
complexity becomes, comes additional power, but there also makes it, uh, a little bit harder to,

425
00:35:19,200 --> 00:35:25,440
to, uh, really kind of fully understand everything that the model is doing. And so having to invest in

426
00:35:25,440 --> 00:35:31,360
kind of automated ways of bringing that level of transparency, uh, it has been a big focus for us.

427
00:35:31,360 --> 00:35:36,080
And so, you know, I think that that's, um, that's a pretty big shift and then working with the

428
00:35:36,080 --> 00:35:41,280
regulators to make sure that these new ways of kind of understanding the models are, um,

429
00:35:41,280 --> 00:35:45,920
sufficient and, and kind of meet the, you know, meet, meet the, the spirit of the regulation. And

430
00:35:45,920 --> 00:35:49,360
it's not just working with the regulators, we're also working with kind of academic partners and

431
00:35:49,360 --> 00:35:55,360
others to, to really, um, understand collectively as a group, like what the best way is to bring

432
00:35:55,360 --> 00:35:59,920
that same level of understanding that, that we've had from traditional quantitative models to,

433
00:35:59,920 --> 00:36:03,760
to machine learning. And that's, you know, it's very much a work in the progress, in progress,

434
00:36:03,760 --> 00:36:08,800
but it's also a process that we're, um, we're very committed to and, and, and putting quite a lot

435
00:36:08,800 --> 00:36:14,880
of effort into. You mentioned in their automation, can you talk about some of the ways that you've

436
00:36:14,880 --> 00:36:20,000
approached automation around explainability? Um, sure. Well, some, I think a lot of it is there,

437
00:36:20,000 --> 00:36:24,880
there's a number of, um, papers that have been written and, and we've done our own internal work

438
00:36:24,880 --> 00:36:31,200
around, um, you know, if you have this extremely complex model, how do you get it to kind of explain

439
00:36:31,200 --> 00:36:36,320
its actions when it makes a prediction and why it made the prediction? And so having some sort of

440
00:36:36,320 --> 00:36:41,200
automatic system that can, you know, it's not something you, these, obviously, these models are so

441
00:36:41,200 --> 00:36:45,440
complex, you're not going to manually trace through it and understand the, the decision logic.

442
00:36:45,920 --> 00:36:50,880
But this, uh, you know, but there are, there are sort of a growing set of techniques for,

443
00:36:50,880 --> 00:36:56,160
for dealing with this in a, in a automated way that, you know, the systems will actually generate

444
00:36:56,160 --> 00:37:03,440
explanations for you that do a pretty good job of talking about the major, um, uh, reasons why

445
00:37:03,440 --> 00:37:10,080
a particular prediction was made. So are these techniques like, uh, you, like the line paper or

446
00:37:10,080 --> 00:37:15,760
like fitting more transparent, more explainable models to more opaque models, or did you have some

447
00:37:15,760 --> 00:37:20,320
other, uh, things in mind? Those are certainly two big areas. You have the line paper was, is,

448
00:37:20,320 --> 00:37:25,360
is a very seminal one. And, uh, probably the best known in this field. And I think that, um,

449
00:37:25,360 --> 00:37:28,880
there's a, there's just a tremendous amount of interest in this topic. And so there's,

450
00:37:28,880 --> 00:37:33,360
there's a lot more work, uh, the newer work that's coming out as well on this topic. And,

451
00:37:33,360 --> 00:37:39,120
and so I think it's, uh, um, it's an exciting area to, to be in, and, um, uh, you know, I've been

452
00:37:39,120 --> 00:37:44,320
at a couple major ML conferences recently. And this has been a huge topic for, for people at them.

453
00:37:44,320 --> 00:37:50,080
And, uh, um, so I, so I'm really excited by what I see in terms of, uh, the progress that's being

454
00:37:50,080 --> 00:37:56,080
made in, in getting to a point where we do have that good, you know, a good degree of explainability

455
00:37:56,080 --> 00:38:02,640
from these really complex models. What's kind of the, the, the lay of the line in terms of, um,

456
00:38:02,640 --> 00:38:08,880
the, the spectrum of model complexity that you're using there, like, is it, um,

457
00:38:10,720 --> 00:38:14,320
I, I guess I'm pausing because I, yeah, I'm guessing that I know the answer that, you know,

458
00:38:14,320 --> 00:38:18,640
it's like a 80 percent, you know, relatively simple things. And then you've got some more complex

459
00:38:18,640 --> 00:38:24,400
stuff, but like how, I guess maybe the question is more, you mentioned using CNNs, like how much

460
00:38:24,400 --> 00:38:29,760
are you using CNNs? How much are you using deep learning? And, and you mentioned reinforcement

461
00:38:29,760 --> 00:38:33,920
learning. Like, do you have reinforcement learning based apps and production? I'd like to get a

462
00:38:33,920 --> 00:38:40,640
sense for the range of complexity of the things you're doing. Yeah. So, you know, a lot of the stuff

463
00:38:40,640 --> 00:38:46,080
we're doing is, so for starters, you know, anytime you have a system where you're bringing

464
00:38:46,080 --> 00:38:50,560
machine learning to it for the first time, a lot of times you can get a big benefit just comes

465
00:38:50,560 --> 00:38:56,480
from kind of the basic data engineering. And, and, and, you know, and kind of getting the system

466
00:38:56,480 --> 00:39:01,360
straightened out and, uh, and even applying a relatively simple, uh, non deep learning type

467
00:39:01,360 --> 00:39:06,640
model to it can, can, you know, gives you massive gains. Right. There's, you know, that's another

468
00:39:06,640 --> 00:39:11,600
reason for the pause, right? There's nothing wrong with like taking the simplest approach and

469
00:39:11,600 --> 00:39:14,800
spreading it far and wide, right? You can get a lot of benefit in doing that.

470
00:39:14,800 --> 00:39:19,280
Yes. And, and, and, you know, those simpler models are really good about like telling

471
00:39:19,280 --> 00:39:22,400
you feature importance and things like that. And so, that's really good when you're in your initial

472
00:39:22,400 --> 00:39:27,840
stages to make sure there's not like some weird quirk in the data that's causing some,

473
00:39:27,840 --> 00:39:33,040
some results in the lab that will never be reproducible in production data. And there's a number

474
00:39:33,040 --> 00:39:37,920
of reasons to start simple. But, all that said, like, we know that, you know, deep learning,

475
00:39:37,920 --> 00:39:42,320
like CNNs is, is really like, there's, there's, there's certainly, there's a reason there's a

476
00:39:42,320 --> 00:39:46,960
lot of excitement around that type of stuff. And, and, and then techniques like reinforcement

477
00:39:46,960 --> 00:39:51,200
learning, you know, where you, where you really kind of give the systems a little more autonomy

478
00:39:51,200 --> 00:39:56,960
to explore the, the space are just fascinating. Um, when you can kind of, when, when you have a

479
00:39:56,960 --> 00:40:02,720
problem that's appropriate for it. And so we, we are doing a lot of research or a lot of work on,

480
00:40:02,720 --> 00:40:08,080
you know, deep learning based models, CNNs for computer vision, NNLP type applications and,

481
00:40:08,080 --> 00:40:13,920
you know, LSTMs for a lot of, like, time series based type prediction and things like that. And

482
00:40:13,920 --> 00:40:18,880
so there's, there's, you know, there's a lot of excitement about what those techniques can do.

483
00:40:18,880 --> 00:40:23,840
I think that, as I mentioned, the center is relatively new. So a lot of those uses are,

484
00:40:24,480 --> 00:40:29,360
not yet in production, but, but certainly, you know, driving in that direction. And,

485
00:40:30,320 --> 00:40:32,960
you know, excited to see what that yields over the next couple of years.

486
00:40:32,960 --> 00:40:39,040
I'm curious. Are there any applications of how far have you gotten with reinforcement learning?

487
00:40:39,040 --> 00:40:43,680
Have you identified some potential applications within Capital One?

488
00:40:43,680 --> 00:40:48,800
Yeah. So, you know, I'm not going to go too much into specifics, but we, there are a couple that

489
00:40:48,800 --> 00:40:52,960
were, that were eyeing. You know, it's not right for every, every situation. And one of the things

490
00:40:52,960 --> 00:41:00,880
about the financial world is for many of the problems, the, you know, you may not know whether a

491
00:41:00,880 --> 00:41:06,480
prediction was successful for three or four years, right? And so doing reinforcement learning

492
00:41:06,480 --> 00:41:13,520
with, when your feedback cycle is that long is creates its own set of challenges. And so,

493
00:41:13,520 --> 00:41:17,200
you know, it's something that we're, that we definitely focus on, because we do think it's

494
00:41:17,200 --> 00:41:21,360
a really powerful technique. But, you know, you need to, you need to make sure you have the right

495
00:41:21,360 --> 00:41:23,840
problem to focus it on and, and, and are doing it in the right way.

496
00:41:23,840 --> 00:41:32,000
I imagine that, uh, the bank has come across that kind of issue. These attribution problems

497
00:41:32,560 --> 00:41:37,440
in, in lots of different areas, trying to attribute, you know, the success of marketing campaigns

498
00:41:37,440 --> 00:41:44,320
and other things. Do you, all right? So, what degree do you, uh, have you identified or you,

499
00:41:44,320 --> 00:41:49,760
do you think that, you know, a path forward is in bringing some of the techniques that you've

500
00:41:49,760 --> 00:41:55,600
learned in that traditional space and applying it to RL, or is that more the domain of research

501
00:41:55,600 --> 00:42:00,560
and your way to, like, gets figured out? Uh, you know, what's interesting is, as you mentioned

502
00:42:00,560 --> 00:42:07,200
earlier in the podcast, Capital One has a lot of, uh, uh, history, uh, in analytic space. And,

503
00:42:07,200 --> 00:42:12,880
in particular, testing, you know, which we're talking about here, um, the, the more I've kind of

504
00:42:12,880 --> 00:42:17,760
learned, and as we've, we've kind of gone into, you know, talked to, explored more business areas

505
00:42:17,760 --> 00:42:22,880
for applying machine learning. Um, it, it's really interesting to see, I think in, in the early days,

506
00:42:22,880 --> 00:42:27,360
so that that was something, Capital One's, like, always been very good at is, is really,

507
00:42:27,360 --> 00:42:31,600
you know, having kind of, uh, control groups and test groups and, and really being disciplined

508
00:42:31,600 --> 00:42:36,400
about that. And so, um, I think there's a lot of practices that we're able to, the kind of

509
00:42:36,400 --> 00:42:41,760
leverage that we're existing in Capital One for, for that type of testing. Hmm. Okay, so maybe

510
00:42:41,760 --> 00:42:46,640
shifting gears a little bit, your group is sponsoring an event at Capital One coming up soon,

511
00:42:46,640 --> 00:42:51,680
the data intelligence conference. Can you talk a little bit about the objectives for that conference?

512
00:42:52,320 --> 00:42:56,080
Yeah, absolutely. So we're, we're really excited about that. Um, we held the first one,

513
00:42:56,080 --> 00:42:59,760
last year was the first one we did. Uh, it's called the data intelligence conference. It's held in

514
00:42:59,760 --> 00:43:07,120
McLean, Virginia in June. And I think our website is data-intelligence.ai. Um, and what this is,

515
00:43:07,120 --> 00:43:12,320
is really kind of intended to be a, a hybrid between an academic conference and a practitioner

516
00:43:12,320 --> 00:43:17,200
focus conference, um, really kind of blending the best of both worlds. And they're, they're, uh,

517
00:43:17,200 --> 00:43:22,560
we really saw a need, um, in, in that space with that focus. Um, we had a great lineup of speakers

518
00:43:22,560 --> 00:43:27,280
last time. We're gonna have the lineup of speakers this time is shaping up to be even, even better.

519
00:43:27,840 --> 00:43:33,840
We have separate tracks on, uh, the couple key tracks on, um, uh, one on fairness and explainability.ai,

520
00:43:33,840 --> 00:43:38,320
another one on, uh, data and ML visualization that, that I think are going to be quite good.

521
00:43:38,320 --> 00:43:45,680
And, um, and so, uh, it's just an opportunity to really kind of convene a group to get together.

522
00:43:45,680 --> 00:43:49,840
And lots of great conversations happen. It's a very kind of collegial feeling conference, um,

523
00:43:50,560 --> 00:43:55,360
uh, conference. It's held being in the DC area. There really isn't a major machine learning conference

524
00:43:55,360 --> 00:44:00,720
in the, the mid-Atlantic. And so, um, it's really kind of filled a nice gap and, and attracted a

525
00:44:00,720 --> 00:44:05,680
pretty big crowd force. We sold out last year and we've increased the, um, the capacity, uh,

526
00:44:05,680 --> 00:44:09,680
significantly this year, but, but I'm still expecting it to, to sell out well in advance.

527
00:44:10,320 --> 00:44:12,800
Any particular session you're really looking forward to this year?

528
00:44:12,800 --> 00:44:16,720
Um, I think we're still kind of finalizing the list. So I don't want to, uh, I don't want to

529
00:44:16,720 --> 00:44:21,120
be my favorite quite yet at the risk of alienating anyone, but, uh, there'll be some good ones.

530
00:44:21,760 --> 00:44:26,000
Who's the target audience for it? So I think it's really, you know, it's really focused on

531
00:44:26,000 --> 00:44:30,400
practitioners, although we do have a lot of, um, you know, students, grad students and put,

532
00:44:30,400 --> 00:44:35,040
we have poster sessions and things like that. So there's, uh, um, there is an academic bent to it,

533
00:44:35,040 --> 00:44:39,840
but, but really it's for, for machine learning practitioners and financial services in particular

534
00:44:39,840 --> 00:44:44,960
or more broadly, more broadly. Um, yeah, I would say it's, I wouldn't say it's, um,

535
00:44:44,960 --> 00:44:49,680
primarily focused on financial services. There, there obviously is some content that are using

536
00:44:49,680 --> 00:44:54,480
examples from financial services industry, but, um, it's a, it's a, it's a full spectrum machine learning

537
00:44:54,480 --> 00:44:59,600
conference. Awesome. Awesome. Well, from what I've seen, it looks like a, a great event.

538
00:44:59,600 --> 00:45:05,120
Any kind of parting thoughts or additional perspective that you'd like to share with our audience

539
00:45:05,120 --> 00:45:09,120
before we close out? No, just, you know, I think that we're, uh, we're at a very interesting time.

540
00:45:09,120 --> 00:45:13,120
As I mentioned, the being of the podcast, having been a machine learning, uh, for the better part of,

541
00:45:13,120 --> 00:45:18,000
it's not continuously, but it's starting 20 years ago. Um, it, it's just amazing to see, and I think

542
00:45:18,000 --> 00:45:22,640
that, uh, um, you know, there's a lot of kind of important questions like we were talking about

543
00:45:22,640 --> 00:45:27,200
around fairness and explainability and ethics in AI that are, um, are critical to think about

544
00:45:27,200 --> 00:45:31,920
along with the, uh, all the exciting technology technological aspects. And, and so I think, um,

545
00:45:31,920 --> 00:45:35,920
the next few years should be very interesting for, for us and we're excited to be a part of it.

546
00:45:36,800 --> 00:45:40,720
Fantastic. Well, Adam, thank you so much for taking the time to chat with us.

547
00:45:41,680 --> 00:45:42,400
Great. Thank you.

548
00:45:45,520 --> 00:45:51,520
All right, everyone. That's our show for today. For more information on Adam or any of the topics

549
00:45:51,520 --> 00:45:59,600
covered in this episode, head over to twimlai.com slash talk slash 147. Thanks once again to Adam

550
00:45:59,600 --> 00:46:05,360
and capital one for their sponsorship of this episode. And thank you so much for listening.

551
00:46:05,360 --> 00:46:24,000
And catch you next time.

