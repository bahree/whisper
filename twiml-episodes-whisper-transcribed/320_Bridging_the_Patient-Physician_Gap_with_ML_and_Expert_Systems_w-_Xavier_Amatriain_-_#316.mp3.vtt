WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:24.240
I'm your host Sam Charrington.

00:24.240 --> 00:27.360
Did you miss TwimalCon AI platforms?

00:27.360 --> 00:32.800
If so, you'll definitely want to check out our TwimalCon Video Packages.

00:32.800 --> 00:38.320
Featuring over 25 sessions, discussing expert perspectives on ML and AI at scale and in

00:38.320 --> 00:43.800
production, you'll hear from industry leaders such as Facebook, Levi's, Zappos, and more

00:43.800 --> 00:50.400
about their experiences automating, accelerating, and scaling machine learning and AI.

00:50.400 --> 00:56.080
In each video package, you'll receive our keynote interviews, the exclusive team Teardown

00:56.080 --> 01:04.960
panels featuring Airbnb and SurveyMonkey, case studies, and more, over 13 hours of footage.

01:04.960 --> 01:10.920
Once again, visit twimalcon.com slash videos for more information or to secure your advanced

01:10.920 --> 01:17.520
purchase today.

01:17.520 --> 01:25.520
All right everyone, I am on the line with Chavier Amatria, Chavier is the co-founder and CTO of Curai.

01:25.520 --> 01:29.600
Chavier, welcome back to the Twimal AI Podcast.

01:29.600 --> 01:32.080
Yeah, thanks for having me, Sam.

01:32.080 --> 01:40.160
So for those that don't recognize the name, Chavier was actually our third guest after

01:40.160 --> 01:42.640
switching to the interview format.

01:42.640 --> 01:50.280
So this was over three years ago, and so much has happened for both of us.

01:50.280 --> 01:56.840
We last had an opportunity to catch up at the AWS Remarice Conference, almost back in June

01:56.840 --> 02:02.720
or so, and I thought it makes sense to get Chavier back on the show to get a little bit

02:02.720 --> 02:06.280
of an update as to what he's been up to.

02:06.280 --> 02:12.800
So when we last spoke to Chavier, he was leading the engineering team at Cora, doing a ton of

02:12.800 --> 02:19.040
work on recommendation systems and other machine learning use cases.

02:19.040 --> 02:24.960
Prior to that, he led the machine learning algorithms team at Netflix, and again, he's

02:24.960 --> 02:31.120
currently the co-founder of Curai, a startup in the healthcare AI space.

02:31.120 --> 02:35.880
Chavier, why don't we just jump right in and have you bring us up to date on Curai and

02:35.880 --> 02:37.560
what's your up to there?

02:37.560 --> 02:38.560
Sure.

02:38.560 --> 02:46.400
Yeah, so Curai, we are using state-of-the-art AI and machine learning for a very big

02:46.400 --> 02:54.320
and bold mission, which is to basically bring the world's best healthcare to everyone.

02:54.320 --> 03:02.240
And of course, that is, as I said, a very bold and very big mission, and we are making

03:02.240 --> 03:06.800
it concrete by basically focusing first on primary care.

03:06.800 --> 03:14.680
So we want to bring the cost down of providing good, not good, but best quality healthcare

03:14.680 --> 03:20.000
to everyone by using AI and machine learning to bring it down to a place where it can be

03:20.000 --> 03:22.520
affordable and it can be scalable.

03:22.520 --> 03:30.480
And everyone in the world who has a phone can have primary care in a very convenient

03:30.480 --> 03:34.640
accessible and affordable way.

03:34.640 --> 03:41.320
And so when you're talking about allowing people to use their phones for primary care,

03:41.320 --> 03:46.160
are we talking about like turning your phone into a tricorder, or are we talking about

03:46.160 --> 03:52.320
using your phone as a kind of a vehicle for accessing human physicians or something

03:52.320 --> 03:53.320
in between?

03:53.320 --> 03:55.680
Or something totally different?

03:55.680 --> 04:05.240
It's a combination of the above, plus something slightly different, but the realization

04:05.240 --> 04:16.920
is that a lot of what can be solved in primary care and in healthcare, it really boils down

04:16.920 --> 04:23.080
to having conversations between patients and physicians.

04:23.080 --> 04:29.640
And of course providing input to those conversations from different sensors and labs and other

04:29.640 --> 04:31.120
places, right?

04:31.120 --> 04:36.800
But the core of what happens in any kind of like medical visit is a conversation between

04:36.800 --> 04:38.760
the patient and the doctor.

04:38.760 --> 04:47.200
And that's the part that can be really automated and not only automated, but actually brought

04:47.200 --> 04:55.680
to a point where you can do it from anywhere that you have a phone with any kind of connection

04:55.680 --> 05:00.480
and you can start having a conversation and chatting with a doctor.

05:00.480 --> 05:01.480
That's all you need.

05:01.480 --> 05:06.400
And of course, there's always going to be things that you can do over the phone like you

05:06.400 --> 05:08.800
can get your medication over the phone, right?

05:08.800 --> 05:13.320
But that's okay because we can always deliver medication to your home or we can always

05:13.320 --> 05:19.760
refer you to a lab that is nearby and get the results from the lab and whatnot.

05:19.760 --> 05:25.680
But the really key issue in the part that we're focusing on is in that conversation that

05:25.680 --> 05:28.720
happens between patients and doctors.

05:28.720 --> 05:34.760
And we have a service where we employ physicians to basically be on the other end of the line

05:34.760 --> 05:36.920
to have those conversations.

05:36.920 --> 05:43.120
And what we're doing is applying this AI machine learning approaches to automate as much

05:43.120 --> 05:48.880
as possible this conversation so we can augment and scale the doctor.

05:48.880 --> 05:54.520
So an important piece here is we're not replacing the doctors with the AI machine learning.

05:54.520 --> 05:58.440
What we're making them is we're giving them, I usually say we're giving them superpowers

05:58.440 --> 06:07.080
that instead of being able to see say 100 patients a day, they'll be able to see 10,000 of

06:07.080 --> 06:12.320
the conversations and most of the easy stuff will be handled by the AI machine learning

06:12.320 --> 06:16.360
and they'll be able to focus only in the places that they're needed the most.

06:16.360 --> 06:22.600
Is 100 patients a day a typical metric for practicing physician?

06:22.600 --> 06:32.720
It really depends. In primary care, the numbers are roughly about, the average is I think 12 minutes

06:32.720 --> 06:40.280
per patient. So you can, it depends on what the working hours are for for doctors.

06:40.280 --> 06:46.000
That's actually that's in the, in the U.S. Okay, all the places like India, for example,

06:46.000 --> 06:52.240
where it's much less than that and doctors can see way more than 100 patients in a day.

06:52.240 --> 06:56.360
And the reality is that they're not even seeing them, the nurses are taking care of them

06:56.360 --> 07:01.120
before they get to see the doctor, but they count as having seen the doctor.

07:01.120 --> 07:07.520
So it's, yeah, it's not a, it's not a, but you can, obviously, you can imagine that

07:07.520 --> 07:14.280
a doctor with 15 minutes or less to see a patient and to remember, first of all, the

07:14.280 --> 07:19.560
history of the patient, what's going on, ask the right questions, get the right answers,

07:19.560 --> 07:25.600
remember everything they know about medical school and come up with a diagnosis and come

07:25.600 --> 07:33.240
up with a recommendation. It's really hard for any, you know, human being to do that at

07:33.240 --> 07:36.440
that rate, right? And of course, there's a lot of mistakes and a lot of things that happen

07:36.440 --> 07:41.560
because of this. What we're trying to do is say, hey, hand that off as much as possible

07:41.560 --> 07:46.280
to the algorithms and the machines and then make sure that when the doctor comes in,

07:46.280 --> 07:50.320
they come at the right time and they come at the point where they have all that information

07:50.320 --> 07:55.760
laid out for them and they can verify the decisions and make sure that they're saying

07:55.760 --> 08:00.840
the right thing. And at the same time, that's what we mean by augmenting, right, the

08:00.840 --> 08:06.640
doctor. We are, of course, giving them information that is state of the art and based on real

08:06.640 --> 08:12.120
science and they can get that information in a way that they can parse it and they can

08:12.120 --> 08:17.040
say, okay, yeah, this is the right decision. I agree. Instead of sort of like having to

08:17.040 --> 08:22.520
deal with all the messiness of gathering that information, parsing it, remembering things,

08:22.520 --> 08:27.480
going through the electronic health record and then making a decision, all of that in

08:27.480 --> 08:29.880
less than 15 minutes, right?

08:29.880 --> 08:38.320
Now there are aspects of this that sound very much like, you know, from the kind of technology

08:38.320 --> 08:47.280
I'd expect to see like other conversational agents where you've got some backend resources

08:47.280 --> 08:56.920
or team that you want to optimize the use of the time and allow some AI system to handle

08:56.920 --> 09:03.600
the kind of easy, easy responses. I've got to imagine that, you know, things get a lot

09:03.600 --> 09:10.760
more complicated and messier, different, certainly more important and healthcare side of things.

09:10.760 --> 09:16.360
Can you talk about some of the unique challenges associated with applying this kind of technology

09:16.360 --> 09:17.360
in healthcare?

09:17.360 --> 09:25.640
Yeah, yeah, definitely. There's a lot of challenges and you're right. You could think that, you

09:25.640 --> 09:32.200
know, the typical approach to dialogue systems and all the advances that we're having recently

09:32.200 --> 09:42.120
on this kind of chatbots and things like transformers and birds and GPG2s and things like that

09:42.120 --> 09:48.320
are useful and they are. I mean, we are using all of the above in different ways, but the

09:48.320 --> 09:56.120
reality is in a domain like healthcare medicine where the stakes are so high, you cannot

09:56.120 --> 10:08.360
leave things out to, you know, chance or just to a model to rely on this kinds of conversations

10:08.360 --> 10:12.720
to actually following the right path. And there's a lot of examples out there where you

10:12.720 --> 10:19.560
can trick any of these models to say things that seem reasonable for any human being, but

10:19.560 --> 10:24.200
they're medically completely wrong, right? And there's been a few examples of that. And

10:24.200 --> 10:32.640
of course, that's the key issue that we were tackling with is like, how do we combine

10:32.640 --> 10:37.760
prior knowledge about what's correct and incorrect in science and in medicine with some

10:37.760 --> 10:44.760
of this automation, right? We do have a key insight here. A very important thing of what

10:44.760 --> 10:52.640
we're doing is we control the end to end. So we have both sides of the conversation and

10:52.640 --> 10:58.680
we're meaning the patient and the expert, the doctor writing this case. So the interesting

10:58.680 --> 11:07.000
thing in our, in the way that we're applying this technology is that we can deploy the conversation

11:07.000 --> 11:14.560
helpers in both ways, right? We can decide to serve something directly to the user if we

11:14.560 --> 11:21.160
want to, but we can also serve it to the doctor and the doctor can use it in an assistant

11:21.160 --> 11:25.880
and make a call whether that makes sense or not if it's being helpful, right? That's

11:25.880 --> 11:31.480
a really important thing, right? Because then you go, you're basically walking the line

11:31.480 --> 11:36.560
between a chatbot and an assistant, kind of like a Gmail assistant, if you will, when

11:36.560 --> 11:42.120
you get an auto response suggestion, right? And the doctor can decide, okay, yeah, this

11:42.120 --> 11:46.880
makes sense. I'll just take it as it is or this doesn't make sense, but they're still

11:46.880 --> 11:55.240
sort of like making sure that that's medically correct. And at the same time, we are getting

11:55.240 --> 12:04.960
training data on how our model, the model that we're building are accurate or not and

12:04.960 --> 12:12.440
in what way they are or not. We have actually an upcoming publication in one of the works

12:12.440 --> 12:18.760
of Neuribs, which basically talked about this, how to constrain the flexibility of this

12:18.760 --> 12:26.320
sort of like deep neural dialogue systems with expert feedback in order to make sure that

12:26.320 --> 12:34.760
the information is accurate for a domain, particularly in medicine, in this case. So

12:34.760 --> 12:41.560
we need to combine the best of both worlds. And by the way, we do the same thing in other

12:41.560 --> 12:49.480
parts of our modeling strategies like diagnosis. In diagnosis, we're also combining expert

12:49.480 --> 12:56.200
systems, which is, you know, old school day eye with deep learning. And I think you cannot

12:56.200 --> 13:03.920
rely 100% on any of the two, but you could get much better if you combine those two strategies

13:03.920 --> 13:10.520
in some smart ways, which I think it's a key insight for medicine, but it's also something

13:10.520 --> 13:15.880
that will happen. And it's being advocated for by many people in machine learning in general,

13:15.880 --> 13:22.040
right? Like you can't blindly trust models that come only with it from the data with no prior

13:22.040 --> 13:27.160
knowledge or some form of knowledge constraints. And a lot of people are trying to figure out

13:27.160 --> 13:31.160
like how do you combine those two things, right? How do you combine all the power you get from

13:31.160 --> 13:39.480
models that are basically just being trained from lots and lots of data with knowledge that we have

13:39.480 --> 13:45.480
and structuring that knowledge and form of prior into the models? Right, right. That is a theme

13:45.480 --> 13:52.200
that continues to recur here on the podcast and in my conversations. One interesting thought

13:52.200 --> 14:01.240
there is, you know, certainly on the probabilistic side, we've benefited from a huge recent

14:01.240 --> 14:13.000
explosion in available tools and algorithms and the like. You've mentioned a bunch of those

14:13.000 --> 14:19.240
already, Bert, et cetera. And, you know, we've got tools like TensorFlow and PyTorch and many,

14:19.240 --> 14:27.160
many, many others. Whereas expert systems, you know, we think of as kind of a throwback to

14:27.160 --> 14:35.160
pre-winter, you know, AI. And I can't think of, you know, not being deep in that space. I can't

14:35.160 --> 14:41.880
think of kind of what the leading open source expert system software might be. Is there a tools

14:41.880 --> 14:46.600
ecosystem there or is it, you know, are people building, you know, when people have this realization

14:46.600 --> 14:50.840
that they need both and not one or the other, are they kind of building it from scratch?

14:50.840 --> 14:58.520
Yeah, I don't think there is such a thing. I don't think there's a, you know, there's a

14:58.520 --> 15:03.800
next-for-system component for TensorFlow or PyTorch. Or should there be, does that make sense?

15:03.800 --> 15:09.320
You know, would something like that have benefited you or is it, you know, is it basically just kind

15:09.320 --> 15:14.200
of rules that we know how to code them because, you know, it's not probabilistic?

15:14.760 --> 15:20.360
Not really. I mean, and by the way, they can be probabilistic, right? At the end of the day,

15:20.360 --> 15:25.080
what you have with these expert systems is a graph. And then you can do probabilistic

15:25.080 --> 15:29.480
inference on the graph and you can do different things on that graph. So basically,

15:30.840 --> 15:36.280
I'm thinking a generic tool for expert systems would be rather simple in the sense that all you

15:36.280 --> 15:45.800
need is a way to represent sort of like graphs and make inferences on those graphs. So it wouldn't be

15:45.800 --> 15:54.840
that complicated to sort of like have a component for TensorFlow or for PyTorch that basically

15:54.840 --> 16:04.040
does that for you. So the key thing here is those expert systems rely a lot still on sort of like

16:04.040 --> 16:11.560
manual labor. And just to give you an example, in the case of some of the expert systems we're using,

16:11.560 --> 16:19.960
we're using some that have been developed for over 50 years, right? So there's a couple of

16:20.520 --> 16:29.000
expert systems for medical diagnosis that go back 50 years. And we, we're using both of them,

16:29.000 --> 16:36.120
actually. And interestingly, you know, there's a lot of knowledge in there, right? You can think

16:36.120 --> 16:42.440
about, you know, 50 years of a bunch of hundreds of really well trained physicians encoding

16:43.320 --> 16:48.840
knowledge and information about medicine and a graph, right? And that's really valuable. And

16:48.840 --> 16:53.560
and it's really something that if you can then inject it into any learning system,

16:54.440 --> 16:59.560
you get a lot of a lot from it, right? To your question, there's no, you know, there's no

16:59.560 --> 17:06.920
tooling for that. On the other hand, you can do interesting things like one of the things we've

17:06.920 --> 17:14.120
done is use this expert system as a, basically a data generator to generate synthetic data and

17:14.120 --> 17:18.760
train learning models from the data that is generated from the expert system, right? That's an

17:18.760 --> 17:25.720
example of something that I think is very useful and really, really valuable because then you can

17:25.720 --> 17:31.640
you can even merge synthetic data with natural data and you can tweak it in ways that you can

17:32.360 --> 17:38.120
learn a model that actually now has some prior knowledge that has been injected in the form

17:38.120 --> 17:42.920
of a ground truth data, so to speak. Can you speak to that particular point in a little bit more

17:42.920 --> 17:50.520
detail? Yeah, sure. Okay, so in this case, the thought process was like that, right? Like,

17:50.520 --> 17:58.440
we know that, you know, if we have very good data and we train a deep learning neural net,

17:59.320 --> 18:06.280
we could get sort of like a really high accurate diagnosis system, the reality is that high quality

18:06.280 --> 18:13.560
data does not exist. If you go to electronic health records, which we have used ourselves, I mean,

18:13.560 --> 18:18.760
we have a project with Stanford where we have been working with them on using the

18:18.760 --> 18:23.320
electronic health records, and this has been something that others have done, like Google and

18:23.320 --> 18:28.360
DeepMind, and you name it, it's like learning predictive models from electronic health records,

18:28.360 --> 18:35.240
electronic health records, the data quality is really, really poor notoriously so. Yes, yes,

18:35.240 --> 18:39.960
and there's a lot of reason for it, but one of it is, you know, they weren't designed

18:41.000 --> 18:45.080
for the purpose of diagnosis, they were designed for the purpose of billing and to make sure

18:45.080 --> 18:51.080
the insurance company's got their money back, so there's a ton of issues with them. So,

18:52.040 --> 18:57.480
but again, that data is valuable, it's not like it's totally noise, there is something in it.

18:57.480 --> 19:05.960
So how can you generate some kind of data that is more, you know, solid and sort of like,

19:05.960 --> 19:10.520
can treat more as a ground truth? Well, you can go to this expert system, which again,

19:10.520 --> 19:17.240
they're, all they are is, you know, a graph, and you can start activating notes and generate

19:17.240 --> 19:24.280
data from that graph that basically becomes sort of other cases that you use to train your

19:24.280 --> 19:30.920
deep learning model, and that's what we showed in this, this is a paper we published last year,

19:30.920 --> 19:37.720
where we basically generated data from the expert system. We injected noise to that data,

19:37.720 --> 19:43.000
because an interesting and important thing is you want to train a model that is robust to noise,

19:43.000 --> 19:48.360
right? The problem with expert systems, one of the problems, is that they're not capable of

19:48.360 --> 19:53.800
dealing with noise. So in other words, if, you know, if the patient doesn't say exactly the

19:53.800 --> 19:58.200
symptom they have and they make a mistake, because they didn't understand the question, or the doctor

19:58.200 --> 20:05.960
enters a wrong thing, the expert system is basically doomed and and we're going to give you an

20:05.960 --> 20:12.280
incorrect output. That's not the case for, you know, you can train machine learning models are,

20:12.280 --> 20:16.840
you know, relatively robust to noise, because you can even do adversarial training and you can do

20:16.840 --> 20:23.240
a lot of different things to make them robust. So how do you combine both? Well, you can also

20:24.120 --> 20:29.240
inject noise to the expert system, that's basically what we did. So we generated data from the

20:29.240 --> 20:36.920
expert system. We injected different kinds of noise. One example, which I think will be very obvious,

20:36.920 --> 20:41.960
is you can inject noise by saying, hey, I'm just going to randomly inject things, symptoms that

20:41.960 --> 20:45.880
are very common, right? I'll just add coughing to everything, because, you know, coughing is

20:45.880 --> 20:51.000
something that people have in general, no matter whether they have one disease or not, right? It's like,

20:51.000 --> 20:57.400
it's, you know, you always can cough or sneeze or something like that that is very prevalent and

20:57.400 --> 21:03.880
very common. It's a typical thing and can confuse, really confuse an expert system, but it's,

21:03.880 --> 21:11.080
if you train a machine learning model on ignoring cough, because it's something that's very common,

21:11.080 --> 21:17.400
and it's not very, it's not going to have determined what the diagnosis is, well, then you build

21:17.400 --> 21:25.640
a robust model. So we again, generated synthetic data from, from these systems, injected noise

21:25.640 --> 21:33.000
in ways that we made the learn model more robust, and we also combined that synthetic data

21:33.000 --> 21:40.760
with natural data that we had from EHRs and other sources to also prove that you can, you don't

21:40.760 --> 21:45.720
need to constrain yourself to just one single kind of data, right? All you need to do is combine it

21:45.720 --> 21:56.120
in smart ways to sort of like understand, because there's obviously value to training from real world

21:56.120 --> 22:03.400
data. All you need to do is figure out how to combine it with more clean data and data that you

22:03.400 --> 22:10.520
can trust. You mentioned this kind of injecting noise via adding symptoms that are frequently

22:10.520 --> 22:15.880
recurrent. What are some other examples of the kind of noise that you're injecting in and more

22:15.880 --> 22:24.440
broadly? How do you quantify the value of this synthetic data in building out your models?

22:25.720 --> 22:34.440
Yeah, so, okay, so to the first question, I mean, I think that the key insight to adding

22:34.440 --> 22:38.840
noise in a domain like medicine is that you do need, you need to have some domain knowledge,

22:38.840 --> 22:43.880
right? When I give you the example of adding symptoms that are very common, that makes sense,

22:43.880 --> 22:48.360
right? Because it makes sense because we know about medicine like, okay, the explanation makes

22:48.360 --> 22:56.120
sense. Another example is like, well, you can remove symptoms that are very rare or are likely

22:56.120 --> 23:01.320
to be missed, right? That's another thing that makes sense once you explain it, right? But you need

23:01.320 --> 23:06.840
to have some insight and you need to talk to doctors, and that's something we do all the time,

23:06.840 --> 23:16.680
right? This kind of strategies don't come up by sheer imagination. They come up because we talk

23:16.680 --> 23:21.960
to our physicians and we talk to them and say, hey, what's, how do you deal with this issue? Where

23:21.960 --> 23:27.800
are issues that are common and that lead to mistakes in diagnosis? How can we make sure that our model

23:27.800 --> 23:35.960
doesn't make the same mistake? So I think that is a key and important thing is you need to work with

23:35.960 --> 23:40.680
the main experts and that leads me to answer your second question. Let me just pause there because

23:40.680 --> 23:45.560
that's a kind of an interesting point. I think, you know, and I think of noise, at least from a

23:45.560 --> 23:51.400
classical engineering perspective, I think of noises like this junk that's, you know, uncorrelated

23:51.400 --> 23:57.160
from your signal. But what you're suggesting is that at least when you're creating synthetic

23:57.160 --> 24:01.800
data, your noise needs to be correlated with your actual noise that you need to expect. You can't

24:01.800 --> 24:07.480
just have, you know, purely random noise because that won't help your model.

24:08.280 --> 24:15.800
Yeah, that's pretty much it. I mean, here it's slightly different, right? And notion of noise

24:15.800 --> 24:25.560
if you will. But what you have is synthetic data that is strictly true, if you will, because

24:25.560 --> 24:30.280
true in a scientific sense, because it's been generated by kind of like an expert system that

24:30.280 --> 24:38.280
has been designed on science. But what you need to do is inject noise that mimics more the

24:38.280 --> 24:45.240
reality of nature, right? And the messiness, right? But that noise needs to model some of the

24:46.760 --> 24:53.320
natural messiness that you see in real life. And you need to not inject it. Yeah, it's not white

24:53.320 --> 25:00.200
noise, right? In that sense, right? It's noise that tries to sort of turn that synthetic data into

25:01.320 --> 25:06.200
something that is more real, right? If you think about it, I mean, I use some time for the metaphor

25:06.200 --> 25:10.520
of like the self-driving cars, also use synthetic data that is generated from video games. And it's

25:10.520 --> 25:18.360
like, well, you can imagine that you're training your self-driving model on data from, from Grand

25:18.360 --> 25:24.920
Flip Auto, but you need to inject, I don't know, a flock and you need to inject rain and you need to

25:24.920 --> 25:31.640
inject things that are not maybe, you know, in your synthetic data. And they're adding noise to the

25:31.640 --> 25:38.200
capture of the image, but in a way that mimics real life situation, right? Not just white noise.

25:39.080 --> 25:44.680
And that sense, it's a bit like the concept of domain adaptation.

25:44.680 --> 25:51.640
Yeah, I mean, you could consider that for sure. And that's another, it is a very, I mean,

25:51.640 --> 25:56.040
domain adaptation in itself. I mean, we could go into that. It's another important thing that

25:56.840 --> 26:03.400
you need to do in many cases because, and yeah, you're right. It could be seen as that, right? Because

26:03.400 --> 26:10.600
sometimes you are training on ideal data, but then you're going to be faced with real life data that

26:10.600 --> 26:18.760
it's going to have to be interpreted in the context of the ideal data that you use for training. So,

26:18.760 --> 26:23.160
yeah, it is, yeah. Okay, so you're about to take on that second question.

26:23.160 --> 26:30.680
Yeah, the second question was about how do you even, you know, how do you know that the data is good

26:30.680 --> 26:39.080
or even the model that your training is good? And, and, you know, beyond that, the relative

26:39.080 --> 26:45.000
advantage of, you know, how do you compare with and without using this synthetic data, you know,

26:45.000 --> 26:51.480
is it a, is it a training time or is it a, you know, accuracy or some combination of all these things?

26:52.520 --> 27:01.240
It's mostly about accuracy, right? And, and the, the problem is that the definition of the accuracy

27:01.240 --> 27:09.960
is, again, really tricky and, and, and, and not that obvious, right? And accuracy in the context

27:09.960 --> 27:20.280
of medical diagnosis is a very, very tricky thing to define, particularly because you would hope

27:20.920 --> 27:28.440
that by asking physician, you would get a ground truth, but that's not the case, right? There's

27:28.440 --> 27:35.560
a studies out there, for example, the human DX project that published some studies that

27:37.000 --> 27:43.240
the, on their dataset, the average accuracy of a single physician was 60 percent, right?

27:44.280 --> 27:51.400
Which is really low. Now, if you, if you take the consensus of 20 physicians that got up to

27:51.400 --> 27:57.800
over 80 percent, which is much better, but then, of course, you need to have 20 physicians agree,

27:57.800 --> 28:02.760
and you still have to 80 percent, which is a lot better, but not necessarily comforting if

28:02.760 --> 28:10.200
you're the patient. Exactly. And, and, and I think that's, that's a key issue in like, what do we

28:10.200 --> 28:17.000
treat as ground truth? So, in our case, we, we, I mean, we use a combination of a lot of things,

28:17.000 --> 28:21.720
we use a combination of sort of, like, publicly known datasets, which there's not that many,

28:21.720 --> 28:28.760
unfortunately, for, for this domain, and they're just, you know, a few, what's called medical

28:28.760 --> 28:37.320
vignettes that you can use to evaluate. We also use our own physicians to QA, and we make sure that

28:37.320 --> 28:43.960
we have sort of, like, several of them agreeing on the cases, so we know that, that we're right.

28:43.960 --> 28:49.240
And then, at the end, it, there's also this kind of, like, synthetic data, right? It's like,

28:50.120 --> 28:56.120
you need to treat that synthetic data as pseudo ground truth, in the sense that, as I mentioned,

28:57.080 --> 29:02.040
if you think about it, that, that synthetic data is the result of, as I said before, 50 years

29:02.040 --> 29:08.280
of research from hundreds of physicians who have agreed that that's what, you know, that,

29:08.280 --> 29:14.120
particular disease should be defined as, and that's those are the symptoms that are related. So,

29:14.120 --> 29:20.680
it's, it's as good as a ground truth as you can get in many other cases, right? So, again,

29:20.680 --> 29:27.000
it's, I wish I had a, like, a great answer for this, but the reality is, I don't. It's like,

29:27.960 --> 29:33.160
it's a, it's kind of an iterative process where you, like, treat one data as a ground truth,

29:33.160 --> 29:37.400
but then you compare it to your other data, you let your physicians go through it and say,

29:37.400 --> 29:44.920
yeah, this is correct or it is not. And then you feed it back and you keep improving both over time.

29:44.920 --> 29:52.200
And I think that's, that's another very important lesson learned here is that you need to design

29:52.200 --> 29:59.640
all the systems as really learning systems, right? So, it's, it's not only about what's their

29:59.640 --> 30:06.920
accuracy today, it's more about how can you make sure that the accuracy and all the other

30:06.920 --> 30:13.000
methods you care about improve over time, right? And in the meantime, the, the, the, the important

30:13.000 --> 30:17.560
thing is like, we always default to humans, right? It's like, we'll always default to a human

30:17.560 --> 30:22.680
doctor and improve the model over time and, and just tell that human doctor, like, hey,

30:22.680 --> 30:27.160
our model thinks that these three things are important. You want to consider them and the doctor

30:27.160 --> 30:33.800
will say, yes or no, and it's their call. And, you know, we'll be as good as the, as the doctors

30:33.800 --> 30:41.400
are. But over time, we, we are pretty sure. Actually, even in our outline evaluation metrics,

30:41.400 --> 30:46.040
we think that we're already, our models are already at least as good as not better than the average

30:46.040 --> 30:50.680
doctor. But even with that, it's, it's not enough, right? It's like, they need to be better than

30:50.680 --> 30:59.000
the best doctor to even make it feasible to rely on, on, on them. But they're a good assistant

30:59.000 --> 31:08.120
and a good augmentation to the human physician for sure. Do you, have you made any attempts to

31:08.840 --> 31:18.280
benchmark the, the third party expert systems with regard to, you know, some elusive metric around

31:18.280 --> 31:27.080
accuracy or, you know, I guess that the thought is that, you know, even if we were confident that each

31:27.080 --> 31:35.400
of the elements in this expert system, you know, was vetted by the 20 doctors or whatever required

31:35.400 --> 31:42.920
to, you know, have a consensus that, you know, has some sufficient level of accuracy. You know,

31:42.920 --> 31:50.040
medical perspectives have changed significantly over 50 years. We may, I don't know the extent

31:50.040 --> 31:55.800
to which this is tracked in this expert system. But, you know, there are diagnostic practices that

31:55.800 --> 32:02.200
apply not equally across different groups of patients. So you have all the potential for all kinds

32:02.200 --> 32:07.960
of biases within a data set like that. Have you made any attempt at kind of evaluating that?

32:08.600 --> 32:16.040
I mean, we are constantly evaluating that with our data, but it's really hard to come up with a,

32:16.040 --> 32:24.520
you know, something that I, I would dare to publish, right? Because it's, the problem is the same.

32:24.520 --> 32:31.720
It's like there is no, no ground truth. There's a, there's a couple of papers on evaluating

32:31.720 --> 32:38.120
different systems and different online symptom checkers. And those are the ones that everyone

32:38.120 --> 32:47.560
is using as sort of like the benchmark. And there's a paper by semi-gram on evaluating symptom

32:47.560 --> 32:55.000
checkers. And there's some medical vignettes that she published, which are commonly used by a bunch

32:55.000 --> 33:01.000
of people, including some like Babylon in the UK and so on with they published things like,

33:01.000 --> 33:05.480
well, we use these vignettes because that's all we have that at least is commonly available and

33:05.480 --> 33:11.960
you can benchmark against. But they're far from, you know, something that it's that you could

33:11.960 --> 33:18.840
consider sort of like has good coverage of medical conditions and you can trust us as being

33:18.840 --> 33:28.760
comparable. But that being said, again, I think that the reality is as harsh as it may sound,

33:28.760 --> 33:35.880
it's not too hard to be better than the average physician. But again, that's not enough. That's

33:35.880 --> 33:41.000
not convincing. Like if I told you like, oh, I can build a self-driving car that is better than

33:41.000 --> 33:47.560
the average teenage driver, would you be okay? Like, well, probably not. Because the average

33:47.560 --> 33:56.200
teenage driver is not somebody I would trust on an automated driving machine. So I think here

33:56.200 --> 34:01.320
it's pretty much the same. It's not about being better than the average doctor. It's about

34:02.120 --> 34:09.320
being better than the best doctor and being able to augment and always sort of like fall back on

34:09.320 --> 34:20.280
humans. And I think that's exactly, I like that comparison to self-driving cars a lot because I

34:20.280 --> 34:26.920
think what we're trying to build is not completely autonomous vehicle, right? We're trying to build

34:26.920 --> 34:32.920
this AI automation as an assistant to the driver just like many cars do right now. But in this case,

34:32.920 --> 34:40.280
the driver is an expert who is a physician. One more question for you. You mentioned earlier that

34:40.280 --> 34:46.680
among the techniques that you're relying on, you do make some use of transformers,

34:46.680 --> 34:51.240
Bert, GPT-2, that kind of thing. How does that play out in what you're building?

34:52.120 --> 34:58.440
That plays out in many different ways. I mean, there's a lot of great things about

34:58.440 --> 35:05.960
those approaches that the one that I think is probably the most relevant in our case is the fact

35:05.960 --> 35:15.880
that it's all about transfer learning, right? It's about if you have a great model that has learned

35:15.880 --> 35:24.840
in general how to speak, sort of say, you can then fine tune it on some specific domain to become

35:24.840 --> 35:32.120
better about speaking about healthcare, right? So a lot of the approaches we take is we look at some

35:32.120 --> 35:39.240
of these models. We fine tune them on very specific data that we have that is focused on healthcare.

35:39.240 --> 35:45.480
And then we can use it to do a bunch of things. I mean, the output of those models can be used

35:46.120 --> 35:52.120
in the context of a chat bottle or a dialogue system, but you can also use them to generate

35:52.120 --> 36:00.680
features for anything, for a classifier or you name it, right? And I think they build a representation

36:00.680 --> 36:09.880
of language in general, right? So we use them as inputs to many of the things we do,

36:10.520 --> 36:16.440
but more directly, we also use them, as I was mentioned before, to generate

36:16.440 --> 36:23.160
assistance to the physicians as they're chatting and they're talking to the patient, right? So

36:23.160 --> 36:32.840
if you think about, and then that also, I think I dare to say pretty common in many applications

36:32.840 --> 36:39.800
of just customer service in general, like where customer service will have, sort of like assistance.

36:39.800 --> 36:46.840
Actually, there are some papers, I think, for example, from Airbnb, where they've done similar

36:46.840 --> 36:52.600
things for their customer service, where there's basically an assistant that is telling the customer

36:52.600 --> 36:58.840
service and suggesting things they could say, so they can basically accept them or not and decide

36:58.840 --> 37:05.400
whether they want to type them out or just simply select the suggested response. So that's an

37:05.400 --> 37:12.360
example where you can almost, you know, you can take one of these models fine-tuned training on,

37:12.360 --> 37:17.960
training on very specific data that it's more healthcare-oriented and you can generate sort of

37:17.960 --> 37:27.080
like an assistant for a physician or an expert in any given domain. Well, Chavier, it was

37:27.080 --> 37:32.360
absolutely wonderful catching up with you, really excited to learn more about what's your up to

37:32.360 --> 37:39.400
there at CURI and I'll definitely be following along. Okay, yeah, great. I would say that many of

37:39.400 --> 37:47.000
these things that we've, I've mentioned, we are publishing and we're, we have I think four papers

37:47.000 --> 37:52.600
in this machine learning for healthcare, workshop and new ribs and if people are interested in

37:52.600 --> 37:59.320
following up in some of the details of how we use this transformer model or how do we do diagnosis

37:59.320 --> 38:05.400
and so on, that's all, I mean, they can go to archive and find more details on some of these

38:06.440 --> 38:12.040
techniques and how we're using them and trying to solve sort of like this huge healthcare problem

38:12.040 --> 38:20.600
access. So yeah. Fantastic. We'll include some links to those papers on archive in the Shunuts.

38:20.600 --> 38:31.880
Great. So great talking to you. Thank you. That's our show for today. To learn more about today's show,

38:31.880 --> 38:39.560
visit Twomolai.com slash shows. Once again, if you missed Twomolcon or want to share what you learned

38:39.560 --> 38:46.680
with your team, be sure to visit Twomolcon.com slash videos for more information about Twomolcon video

38:46.680 --> 38:56.680
packages. Thanks so much for listening. Peace.

