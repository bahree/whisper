WEBVTT

00:00.000 --> 00:13.040
All right, everyone. I am here with Nestreen, most of the

00:13.040 --> 00:19.560
day. Nestreen is the co-founder of Vernique. Nestreen, welcome back to the

00:19.560 --> 00:20.880
Twonwall AI podcast.

00:21.160 --> 00:24.560
Thanks so much for having me again, Sam.

00:24.560 --> 00:30.880
For those that recognize the name, Nestreen is a long-time friend of the show and

00:30.880 --> 00:40.120
this is your third interview maybe. The last time we caught up was in January of

00:40.120 --> 00:47.080
2020 before so much in the world changed and we were talking about trends and

00:47.080 --> 00:52.480
natural language processing and today we'll of course be talking a little bit

00:52.480 --> 00:58.160
about that but we'll be focusing on some big changes in your world since we

00:58.160 --> 01:04.920
last spoke and starting with the company that you founded. So maybe catch us up

01:04.920 --> 01:09.160
a little bit on big changes in your world since the last time.

01:09.560 --> 01:15.080
Absolutely. Well, it's so interesting that the last time that I talked with you

01:15.080 --> 01:20.160
was January 2020. It was precisely third of January 2020 and the reason I

01:20.160 --> 01:26.480
remember this is that you may recall it was literally the day after U.S.

01:26.480 --> 01:31.720
basically went on the brink of having a war with Iran. So we were all extremely

01:31.720 --> 01:34.840
stressed. That was a night. I literally didn't sleep. We were all like checking

01:34.840 --> 01:40.680
our phones trying to see whether or not there will be literally a war and it

01:40.680 --> 01:47.200
was just a very bizarre time. So that's kind of how 2020 started for a bunch of

01:47.200 --> 01:52.400
us not even knowing where it's headed. So I'm even third of January was a very

01:52.400 --> 01:58.960
very weird year. So anyways since then that that very even actually played a

01:58.960 --> 02:05.720
major role in how the rest of my life went on from that point on. So

02:05.720 --> 02:11.080
basically diverse so many other things that happened like on eighth of January

02:11.080 --> 02:17.840
there was this plane that was downed with like 176 people on it. A lot of them

02:17.840 --> 02:22.600
Iranians some of whom I literally went to school with you know who could have

02:22.600 --> 02:27.000
been literally me. So after so much thinking I always you know I've been

02:27.000 --> 02:32.520
working in the startup scene for the last five years or so and I truly

02:32.520 --> 02:38.080
believe in wanting to make impact through the power of startups and through

02:38.080 --> 02:43.360
being able to laser focus of the particular fundamental problem in real

02:43.360 --> 02:49.400
world. So anyways I always wanted to do this and this whole series of events

02:49.400 --> 02:58.840
around like January and February 2020 made me kind of think how blessed I am as

02:58.840 --> 03:04.000
an individual and the kind of opportunities that I have and I'm not

03:04.000 --> 03:09.440
necessarily leveraging with older people that didn't even get to live to want

03:09.440 --> 03:14.080
to fulfill their dreams like it felt like it's time so I should just just do

03:14.080 --> 03:19.600
it and go for it. So anyways the process of starting learning was so many you

03:19.600 --> 03:23.120
know years of in progress basically but it was always about waiting for the

03:23.120 --> 03:29.040
right time for the right time. And then in February 2020 I was like that's it I'm

03:29.040 --> 03:34.080
gonna start it. And hilariously the first day basically official first day of

03:34.080 --> 03:42.720
Bernic is March 1st 2020. Not knowing what we are really up for so we had all

03:42.720 --> 03:48.920
these omit my co-founder and I've had a New York City. Yeah we had all these

03:48.920 --> 03:54.320
grand plans of starting this deep tech AI company in New York City with like

03:54.320 --> 03:59.120
you know a bunch of plans as to when we go fundraising, when we go hire our team

03:59.120 --> 04:07.920
and of course March 2020 the you know shutdowns started and then we in April

04:07.920 --> 04:14.560
so we realized that okay we are in the middle of a global pandemic so the rest

04:14.560 --> 04:21.680
is kind of history it's the timing was impeccable to say the least so we

04:21.680 --> 04:26.480
basically kind of put everything on hold for a couple of months just to see

04:26.480 --> 04:32.480
where the world is going like it wasn't necessarily clear that anyone wanted

04:32.480 --> 04:39.760
to even think about a brand new AI company while everyone was basically

04:39.760 --> 04:46.320
looking their booms. So yeah we just waited a couple of months and then around

04:46.320 --> 04:51.600
like late 2020 we were like okay let's just go for it so we started a fundraiser

04:51.600 --> 04:56.000
fundraising and in about a month or so we could like close around and then we

04:56.000 --> 05:01.440
started hiring and now we are based in New York City if you're in flat iron

05:01.440 --> 05:05.360
specifically it's like the dream neighborhood we're working in a physical

05:05.360 --> 05:12.800
office and this is our space and it's been just a really really great time to

05:12.800 --> 05:18.080
to want to do this honestly despite all the hurdles how hard it was to go

05:18.080 --> 05:22.240
through the pandemic to start a pandemic company basically I think it has

05:22.240 --> 05:26.480
helped us build all sorts of muscles that we never thought we can

05:26.480 --> 05:31.680
and hence we are really in a very good shape in terms of all the other

05:31.680 --> 05:35.760
hurdles that we have to overcome moving forward. That's awesome that's awesome

05:35.760 --> 05:45.280
congratulations on that. So the company is as yet stealth and not much is

05:45.280 --> 05:49.440
known publicly about what you're up to but you've promised that you're going to

05:49.440 --> 05:56.080
share a little bit of pull back the kimono a bit so to speak.

05:56.080 --> 06:00.560
What's the company up to? What are the challenges that you're hoping to take on?

06:00.560 --> 06:07.920
Absolutely so our mission is to enable anyone to make data informed decisions

06:07.920 --> 06:12.800
be for their personal matters or for their businesses without having

06:12.800 --> 06:17.600
any kind of technical background and what that means is that we are

06:17.600 --> 06:22.720
basically innovating in the human machine interfaces area where we want to

06:22.720 --> 06:28.560
replace hard to grasp things such as programming languages with other

06:28.560 --> 06:32.720
intuitive modalities of interaction including of course human natural

06:32.720 --> 06:36.480
language which you know given our backgrounds is abundantly clear but we are

06:36.480 --> 06:40.080
not just you know biased thinking that natural language is the

06:40.080 --> 06:45.200
answer to anything and everything in terms of intuitiveness we are thinking

06:45.200 --> 06:50.240
actively and working at simply towards other like modalities of interaction

06:50.240 --> 06:54.640
that would be really easy to use and intuitive in the given context.

06:54.640 --> 06:59.600
So basically this is like two degrees how much we usually share publicly but

06:59.600 --> 07:04.160
I love talking with you so I would love to start sharing a little bit more in

07:04.160 --> 07:09.360
terms of our vision for Verneak and what kind of a technology

07:09.360 --> 07:13.840
of your building so very deep tech company meaning that we are really

07:13.840 --> 07:17.680
we are overcoming various like scientific and engineering challenges

07:17.680 --> 07:23.120
that would ultimately enable us to build such a platform that I was talking

07:23.120 --> 07:26.800
for data informed decision making so what does data informed decision

07:26.800 --> 07:31.440
making means it means that there's all sorts of data out there they can come in

07:31.440 --> 07:36.080
any shape or form they can basically have any format they could be any small

07:36.080 --> 07:40.640
any large they could basically be anything and everything and

07:40.640 --> 07:45.520
when you want to put an interface on top of it and enable anyone to use it

07:45.520 --> 07:49.760
like let's say get and the little calls insights about it or like just ask

07:49.760 --> 07:53.600
any kind of questions that they can they have that can help them

07:53.600 --> 07:57.360
better their lives or their better their businesses that means that you have

07:57.360 --> 08:01.200
to have interoperability and that means that you have to be domain general

08:01.200 --> 08:05.200
we are not trying to build like something that will be just working in one

08:05.200 --> 08:08.160
sector we don't want to build something that is going to be just in a

08:08.160 --> 08:12.480
particular domain operation we want to build a truly domain general AI

08:12.480 --> 08:18.400
platform that can have interoperability across all sorts of data so it's a pretty

08:18.400 --> 08:23.200
grand mission in terms of the technology that it needs and we are actively

08:23.200 --> 08:27.760
working on all sorts of problems that in the academic sense has been

08:27.760 --> 08:32.560
you know an issue for the past couple of years as well so we are working

08:32.560 --> 08:37.680
on basically building for some foremost intuitive and easy to use

08:37.680 --> 08:41.520
interfaces including natural language building a domain general natural

08:41.520 --> 08:44.880
language interface in and of itself is very very challenging

08:44.880 --> 08:49.200
but mainly if you're trying to make it controllable we're trying to make

08:49.200 --> 08:53.200
sure to be assigned provenance as to where the data source comes from

08:53.200 --> 08:56.720
and what kind of data you're basically putting in the loop

08:56.720 --> 09:00.240
on top of it we're trying to make sure that these

09:00.240 --> 09:04.080
basically interfaces that we put are controllable

09:04.080 --> 09:08.000
meaning that we can have interaction with them and we can teach them

09:08.000 --> 09:11.440
or help them forget something that they've learned that was wrong

09:11.440 --> 09:15.760
you're also trying to make sure that these models are actually

09:15.760 --> 09:20.720
able to get all sorts of feedback and make better decisions down the road for

09:20.720 --> 09:25.440
the sake of the user so basically they should be able to not just

09:25.440 --> 09:30.000
base their decisions that they help the user make on one source but like just

09:30.000 --> 09:34.400
bring all the bits and pieces together and basically do reasoning

09:34.400 --> 09:38.960
but so many other characteristics that this kind of a fundamental AI

09:38.960 --> 09:42.240
platform of your building should have including the fact that it should be

09:42.240 --> 09:46.720
amenable to data privacy issues and so there's so many other things if you're

09:46.720 --> 09:51.520
actively working on but in terms of our lines of AI research we are

09:51.520 --> 09:55.840
of course working on conversational agents you're building basically dialogue

09:55.840 --> 09:58.560
systems that have these features that I was just

09:58.560 --> 10:04.320
outlining you're also working on like old sorts of zoning phenomena that are

10:04.320 --> 10:10.080
not data hungry that actually can help us go from it domain to another with

10:10.080 --> 10:13.920
like the least amount of time and the least amount of

10:13.920 --> 10:19.760
supervision of course but we also have problems on the data side itself so

10:19.760 --> 10:23.920
building a platform that has interoperability

10:23.920 --> 10:28.000
across the board requires like innovations in distributed systems

10:28.000 --> 10:32.000
requires innovations in building like runtime run time engines

10:32.000 --> 10:36.480
that can actually digest all sorts of data and understand it so

10:36.480 --> 10:41.520
it's not just AI research that is a problem that we are tackling but also

10:41.520 --> 10:46.480
you know other kind of computer science

10:46.480 --> 10:50.800
issues that we have to grapple with on a day-to-day basis and I will

10:50.800 --> 10:56.800
add that we are very mindful of having it like design

10:56.800 --> 11:00.960
elements and design driven thinking to be the front of the center of what we

11:00.960 --> 11:06.080
are building so that we don't kind of like repeat the mistakes that have been

11:06.080 --> 11:10.960
made in general in the texting of like bunch of technical people getting

11:10.960 --> 11:14.800
together thinking they have a intuitive solution and then turns out

11:14.800 --> 11:19.920
it's really not usable so that's that's our front and center we are really

11:19.920 --> 11:24.560
striving to build something that works for the masses as opposed to

11:24.560 --> 11:30.480
something that is just you know for the sake of pushing the boundaries of AI

11:30.480 --> 11:35.120
how big is the team so far? we were total of eight people we had like six or

11:35.120 --> 11:39.520
so interns also that loved us so we are very small we're

11:39.520 --> 11:44.960
hiring across the board we used to be just hiring on for like research team and

11:44.960 --> 11:49.760
the engineering team but now we're also hiring on the business side so we are

11:49.760 --> 11:54.800
hoping to grow to to over so more people as soon as possible we wanted it

11:54.800 --> 11:59.600
yesterday but hiring is the toughest one of the toughest things in

11:59.600 --> 12:05.760
my experience yeah I ask because the you've outlined a pretty broad

12:05.760 --> 12:11.360
set of areas not just that you want to build product in but that you need to

12:11.360 --> 12:17.520
do fundamental research in and that sounds like a ton for eight people

12:17.520 --> 12:24.480
oh my god how do you yeah before we get to kind of how you take that on

12:24.480 --> 12:30.320
maybe another way of coming out that is like how do you think about the

12:30.320 --> 12:36.960
MVP for the product like when I think of you know the simplest

12:36.960 --> 12:42.400
thing that you know kind of the simplest version of the the vision you

12:42.400 --> 12:47.280
outlined I think of something like a you know natural language query generator

12:47.280 --> 12:50.480
but those you know exist they're not without

12:50.480 --> 12:54.560
challenges like how do you think about the the MVP and how do you

12:54.560 --> 13:00.480
contrast with something like natural language query generation good question

13:00.480 --> 13:05.040
so I will just first inform us say something as a in like in

13:05.040 --> 13:12.240
parentheses we call MVP legum at Bernie so legum is this Swedish term I don't

13:12.240 --> 13:15.440
know if you're familiar with it that means just to write them out and just

13:15.440 --> 13:18.560
there's only like we take issues over the night we take

13:18.560 --> 13:25.280
issues on like minimum viable product often not be really minimum and not

13:25.280 --> 13:32.080
being really viable so anyway the question is what's just to write

13:32.080 --> 13:35.280
them on for us right just what we've been dealing with for the past

13:35.280 --> 13:40.480
like year now doing like R&D until we get to a point that we're

13:40.480 --> 13:43.760
comfortable with taking our product out so it's a very

13:43.760 --> 13:49.600
evasive complex question what the logon basically should be for our domain

13:49.600 --> 13:53.280
on the business side what we're doing is that we're very practical we are taking

13:53.280 --> 13:58.320
this technology one domain at a time we're like right now focusing on a

13:58.320 --> 14:02.800
particular domain for example that we're hoping to come out with in the next

14:02.800 --> 14:07.680
couple of months and in that particular domain then you have this

14:07.680 --> 14:11.280
you know flexibility of narrowing down the data that you're training on narrowing

14:11.280 --> 14:16.400
down basically the capabilities as well right so for example right now

14:16.400 --> 14:20.640
we we know that we want our system to be instantaneous and it is so we have

14:20.640 --> 14:24.080
like a threshold that we know okay if it's responding after one second

14:24.080 --> 14:28.880
it's like definitely a downer but if it is like we can just keep pushing the

14:28.880 --> 14:33.280
boundaries so there are such parameters that we are kind of tuning but

14:33.280 --> 14:38.320
anyways so in terms of the it taking it one domain at a time

14:38.320 --> 14:42.960
we are doing that but we have this rule in house that we are like if it is

14:42.960 --> 14:50.240
basically diverging more than 20% from the general domain general platform

14:50.240 --> 14:55.120
we are just working too much on one particular domain so basically we are

14:55.120 --> 15:02.720
making our logon or MVP to be an 80% of the same kind of code base same kind of

15:02.720 --> 15:07.840
technology that we would have and spending 20% of time in a particular

15:07.840 --> 15:12.560
domain without it generalizing out of it so it's it's been it's not easy I

15:12.560 --> 15:17.040
would say and I think I will have a more clear answer to that when we are out

15:17.040 --> 15:22.560
with our product I know what I said is a little vague as to what the MVP should

15:22.560 --> 15:25.680
be but there's so much to talk about in terms of how to

15:25.680 --> 15:30.560
curate the exact packaging of such a technology

15:30.560 --> 15:34.880
so that is still meaningful you can get enough

15:34.880 --> 15:40.640
signal back from the market and from the actual users and then keep iterating

15:40.640 --> 15:47.120
and is the example of some kind of natural language query is that

15:47.120 --> 15:55.920
directionally you know accurate accurate enough to give us some to provide some

15:55.920 --> 16:01.040
concrete grounding for the conversation you're going domain at a time some

16:01.040 --> 16:07.360
envisioning you pick a domain who knows what that domain is let's say

16:07.360 --> 16:13.200
contracts you know legal contracts right and so you know some lawyers want to

16:13.200 --> 16:17.440
do discovery and it's hard for them to find what they want and so

16:17.440 --> 16:21.200
you're essentially giving them a box to type in and you're doing smarter

16:21.200 --> 16:25.040
things with what they type in to get them to you know to turn that into a

16:25.040 --> 16:29.120
query that you can run against whatever systems they're trying to search

16:29.120 --> 16:33.840
against as an example but that's you know that's the unstructured text

16:33.840 --> 16:37.120
example you know there's the structure text example you're

16:37.120 --> 16:42.560
working with you know power companies and you're trying to help them manage

16:42.560 --> 16:46.320
their grids better or something and you give them a box to

16:46.320 --> 16:51.760
you know tell me the you know aggregate power across whatever whatever

16:51.760 --> 16:57.760
six regions I guess I'm you know the picture that

16:57.760 --> 17:02.880
is coming to mind is maybe you know

17:02.880 --> 17:10.000
AI or natural language interfaces for like business intelligence types of

17:10.000 --> 17:13.440
schools or problems yes that's that's very close

17:13.440 --> 17:18.160
that we can talk for hours and hours right about the state of the field like

17:18.160 --> 17:22.560
what are the relevant technologies how they're failing and why they're

17:22.560 --> 17:27.040
failing and why what you're doing is is really different but yes at the end of

17:27.040 --> 17:30.880
the day it's it resembles that so basically let me

17:30.880 --> 17:34.320
move you an example this is not what we are doing right now

17:34.320 --> 17:38.400
but one of the reasons we wanted to do what we are doing is that even as

17:38.400 --> 17:43.520
technologists it's so hard to make decisions based off of your data

17:43.520 --> 17:48.240
because it just requires so many bells and whistles so

17:48.240 --> 17:52.560
for example you just asked me right before this like what did you eat for breakfast

17:52.560 --> 17:56.400
right and I told you well I don't eat breakfast so as an individual I

17:56.400 --> 18:00.960
do intermittent fasting I have this app that I'm recording

18:00.960 --> 18:04.720
the number of hours that I'm not eating a day I've been just doing it

18:04.720 --> 18:08.480
abitually in the past couple of years then I have this other app for I'm

18:08.480 --> 18:13.040
recording what I'm eating every day and then just because I have some like

18:13.040 --> 18:16.800
underlying conditions that I have to be careful with whatever x, y and z

18:16.800 --> 18:21.680
that I'm like putting in my body basically and then I have this other app

18:21.680 --> 18:26.160
where I'm like basically recording my late I hop on a scale rate to scale

18:26.160 --> 18:29.600
every morning personally so there's this app that is capturing

18:29.600 --> 18:33.760
the data from my health on a day-to-day basis on my bait on a day-to-day

18:33.760 --> 18:37.440
basis so anyways as an individual like myself although I'm a

18:37.440 --> 18:41.680
technical person it's so hard for me to get the answer to a question like

18:41.680 --> 18:45.920
I don't know what is there any correlation between my weight loss and the

18:45.920 --> 18:48.960
number of hours that I'm intermittent fasting right

18:48.960 --> 18:53.200
and why is it hard because it requires me figuring out how to download the

18:53.200 --> 18:58.560
data from all of those apps if you even can if you even can exactly

18:58.560 --> 19:02.640
and then maybe I don't know going to a like opening up

19:02.640 --> 19:07.280
like a ipython book and trying to remember what is the correlation function or

19:07.280 --> 19:10.400
some sort and then trying to call it and then coming up with the

19:10.400 --> 19:13.440
answer right it just takes so much time and I am a

19:13.440 --> 19:18.240
technical person so imagine what the world looks like for other individuals and

19:18.240 --> 19:22.240
small businesses who have all these kinds of data and they are making wrong

19:22.240 --> 19:27.040
decisions on a day-to-day basis because of their lack of access to such

19:27.040 --> 19:30.960
easy-to-use interfaces so anyways we want to very much in line be what you

19:30.960 --> 19:34.880
said you're not like you know basically doing any of these things that I

19:34.880 --> 19:38.800
mentioned right now but in the long run for the company that's

19:38.800 --> 19:43.120
division we want it to be that we have this one

19:43.120 --> 19:47.040
interface that we can put on top of anything and everything and it can

19:47.040 --> 19:50.560
smartly navigate its way to find the right sources

19:50.560 --> 19:53.520
and then come back with the with their results

19:53.520 --> 19:57.520
but it definitely is basically like a natural language

19:57.520 --> 20:00.880
quarrying interface but as I mentioned we believe

20:00.880 --> 20:05.360
tremendous we believe in the tremendous value that other modalities of

20:05.360 --> 20:09.360
interaction bring into the scene and that's something that has been

20:09.360 --> 20:12.720
definitely neglected but on top of it you know I've worked on

20:12.720 --> 20:15.600
natural language understanding basically my whole

20:15.600 --> 20:20.800
like life like not even adult life like like whatever 15 years right there

20:20.800 --> 20:24.080
just no one knows how to build a domain general

20:24.080 --> 20:26.960
language interface like that we've had tremendous

20:26.960 --> 20:29.760
progress in the field in the past couple of years which is why

20:29.760 --> 20:34.640
likes of myself are motivated to want to finally take

20:34.640 --> 20:38.880
of you know this kind of technology to the market so that we can get

20:38.880 --> 20:43.040
you know clear signal as to the flaws right and the problems and the

20:43.040 --> 20:48.080
solutions of feedback into that basically academic AI world

20:48.080 --> 20:51.840
we could go down the rabbit hole of

20:51.840 --> 20:56.800
wearables and personal health tech and all that kind of stuff and maybe that's a

20:56.800 --> 21:01.280
a fourth conversation but for now I think the the thread that I want to pull a

21:01.280 --> 21:08.720
little bit on is the the state of AI research

21:08.720 --> 21:13.280
in the domains relevant to what you're trying to build

21:13.280 --> 21:16.720
because a lot of what you're I think your

21:16.720 --> 21:21.360
contention is is hey that you know the idea of a search box that

21:21.360 --> 21:25.040
you know lets you find things out right that's something we've been pursuing

21:25.040 --> 21:29.120
for a while but we've been failing and some of that is

21:29.120 --> 21:35.120
execution and you know or different execution and different vision

21:35.120 --> 21:41.280
but some of it is that the research or the technology just

21:41.280 --> 21:46.960
fundamentally isn't there and so maybe a next place to explore is

21:46.960 --> 21:51.920
you know where you see the gaps in the technology and how you're

21:51.920 --> 21:56.480
using you know that assessment to kind of prioritize the way

21:56.480 --> 22:00.960
you're approaching research at Verneak.

22:00.960 --> 22:04.720
Absolutely so I think this is very much actually

22:04.720 --> 22:08.800
continuation of the conversation we had in January 2020

22:08.800 --> 22:13.280
when you know I was reviewing basically the state of the field

22:13.280 --> 22:17.920
if you you may recall so my net was that we've come a very long way

22:17.920 --> 22:22.400
in natural language understanding and natural language processing in general

22:22.400 --> 22:27.600
in the past like now six years or so it's been tremendous

22:27.600 --> 22:31.840
how much progress we've made not on just down a stream past but also in real

22:31.840 --> 22:35.040
world products basically that have been impacted by these kind of

22:35.040 --> 22:41.440
technologies but I characterized like the flaws in a few ways that I think

22:41.440 --> 22:45.040
some of which are now kind of being

22:45.040 --> 22:48.560
addressed by the recent developments so like just to

22:48.560 --> 22:54.640
backtrack a little bit so it's kind of interesting so I started my personal

22:54.640 --> 22:58.560
work in natural language understanding

22:58.560 --> 23:03.520
because I was back in times is back in high school I used to work in robotics

23:03.520 --> 23:06.000
then I switched to natural language understanding and comments this

23:06.000 --> 23:10.960
reasoning in particular because I came across this motivating example that

23:10.960 --> 23:17.360
I saw in a random book that was like for an AI system for a machine to understand

23:17.360 --> 23:22.880
natural language it requires to put together lots of bits and pieces about

23:22.880 --> 23:26.640
the world that it is grounded in and hence it's a so-called AI

23:26.640 --> 23:30.000
complete problem so the motivating example was that

23:30.000 --> 23:33.680
the monkey ate the banana because it was hungry and

23:33.680 --> 23:38.080
is the it referring to the you know monkey or the banana

23:38.080 --> 23:42.080
basically was the the the riddle for the AI system

23:42.080 --> 23:47.040
so I literally literally started my life in AI in

23:47.040 --> 23:51.280
sorry natural language understanding for for that very same problem

23:51.280 --> 23:54.640
we're tackling that very same problem which I found fascinating

23:54.640 --> 23:59.120
and then fast forward these kind of problems were really not the focus for

23:59.120 --> 24:03.360
the field for a very long time until in 2015 and

24:03.360 --> 24:07.040
16 or so that these you know at the time it was like

24:07.040 --> 24:11.440
biolistiums that were suddenly starting to to to leave their mark on

24:11.440 --> 24:15.120
some comments this reasoning in natural language understanding benchmarks

24:15.120 --> 24:17.760
so much so that it was touted as the

24:17.760 --> 24:22.720
solution but then of course with the transformers in 2017 or so

24:22.720 --> 24:25.600
everything changed for the better so much so

24:25.600 --> 24:29.760
that it just kept kept going right we had all these other models and

24:29.760 --> 24:32.960
for me personally always I was on the camp of

24:32.960 --> 24:36.640
questioning but they're not any of this is real progress as someone that

24:36.640 --> 24:40.320
cared about comments and reasoning which is

24:40.320 --> 24:45.440
sort of historically been this line of thinking about reasoning which

24:45.440 --> 24:50.080
supposedly conflicts with like you know

24:50.080 --> 24:57.040
machine learning and stochastic and like look into to the data etc

24:57.040 --> 25:00.880
so anyways for me it was very natural to want to be biased against the

25:00.880 --> 25:05.760
progress so much so that in 2016 basically the work that I

25:05.760 --> 25:10.560
personally did was on building a the story close test benchmark

25:10.560 --> 25:14.080
which was basically trying to push these systems to showcase whether or not

25:14.080 --> 25:17.600
they have any sort of comments is reasoning by continuing this

25:17.600 --> 25:21.840
like a short story and basically finishing the story

25:21.840 --> 25:26.240
right in a right way that is like reasonable and logical

25:26.240 --> 25:31.280
so anyways then we then I did that work the intention was to

25:31.280 --> 25:35.760
assess whether or not these models of common sense and I wasn't that

25:35.760 --> 25:39.360
convinced that they do I wasn't that convinced that they have any reason in

25:39.360 --> 25:43.040
capabilities and then the transformers came of course GPT-1

25:43.040 --> 25:46.960
did a great job on story close test and then we were like oh is it just

25:46.960 --> 25:50.480
picking up on the intricacies of the data is it just

25:50.480 --> 25:54.400
basically learning the biases that exist in the data

25:54.400 --> 25:57.920
data sad not doing true national language understanding

25:57.920 --> 26:02.480
and then turned out that it maybe was to a degree but we even changed the test

26:02.480 --> 26:05.680
said to another version of story close test that was kind of

26:05.680 --> 26:13.680
debiased and then GPT-1 was basically the only model that could sustain its performance

26:13.680 --> 26:18.560
and anyways still I'm an skeptical let's say and I'm like okay I want to see

26:18.560 --> 26:22.880
deeper language understanding and deeper comments as reasoning

26:22.880 --> 26:29.440
and then GPT-3 came out basically this in 2020

26:29.440 --> 26:36.080
that was doing zero shot performance on story close test meaning using none of the

26:36.080 --> 26:40.560
training data at all and it was getting to 80 something percent performance

26:40.560 --> 26:44.400
which if you had told me in grad school I would not have

26:44.400 --> 26:49.440
taught as possible right in 2020 so these this is tremendous progress

26:49.440 --> 26:56.800
right I think it's really hard for us to try to

26:56.800 --> 27:00.400
sweep it under a rug that these models are not showing any

27:00.400 --> 27:05.360
fundamental language understanding and all they're doing is pattern recognition

27:05.360 --> 27:09.680
because one can even argue what it is that we do as

27:09.680 --> 27:13.680
understanding human beings and how much of it is recognizing patterns and

27:13.680 --> 27:18.400
the piers we have built on the world throughout our lifetime

27:18.400 --> 27:22.560
so anyways I think this is tremendous progress and I'll add one other note

27:22.560 --> 27:27.120
that since to me the progress of the field of natural

27:27.120 --> 27:30.720
language understanding has been so intertwined with my personal line of

27:30.720 --> 27:35.200
research as well so in 2020 I also did this really

27:35.200 --> 27:39.280
interesting work that I personally believe then with my great colleagues at

27:39.280 --> 27:43.520
elemental cognition called glucose that was basically

27:43.520 --> 27:49.680
about building these world models while you're reading a story so story

27:49.680 --> 27:54.240
closest was all about read for sentences predicting but now let's go

27:54.240 --> 27:59.120
maybe on that that's just not just predicting but also come up with these

27:59.120 --> 28:07.360
deep like a world model of the you know a person's kind of set of

28:07.360 --> 28:13.040
states and like events and their causal chains and like draw a coherent picture

28:13.040 --> 28:17.120
of the narrative that you're building let's make this as the new

28:17.120 --> 28:23.680
basically benchmark for evaluating whether or not a system is showing

28:23.680 --> 28:28.720
any sort of deep understanding so anyways this was we you know came up with this

28:28.720 --> 28:33.520
work and basically did all of it which you know we can delve deeper dive deeper

28:33.520 --> 28:38.720
into but it when it came out was like two or three months

28:38.720 --> 28:44.240
after the GPT-3 work and it's fascinating how GPT-3

28:44.240 --> 28:49.520
was doing better than GPT-2 on even in this particular basically task of ours

28:49.520 --> 28:54.000
and how far it has gone in showing that it has some sort of a world model so

28:54.000 --> 28:59.520
I personally think that whoever kind of claims that these models do not have

28:59.520 --> 29:04.560
any world model don't have any kind of a human like cognition don't have

29:04.560 --> 29:12.080
any deep understanding of language it's it's really incorrect to say the least

29:12.080 --> 29:16.480
of course these models are fundamentally flawed no question right we talked

29:16.480 --> 29:22.560
about this in the last session that these models are extremely biased

29:22.560 --> 29:28.960
they are really easy to get tricked which makes them in my opinion brittle

29:28.960 --> 29:33.840
you know how like it was the thing to call symbolic models back in time

29:33.840 --> 29:38.160
brittle I think these models are also quite brittle right it's easy to

29:38.160 --> 29:43.120
for them to get sidetracked and make really stupid mistakes although they

29:43.120 --> 29:48.320
work say well like often so these and you know of course these models are

29:48.320 --> 29:53.440
also really not controllable which as I mentioned is something that we are

29:53.440 --> 29:58.320
working actively at brain ink on so these are all flaws that they have

29:58.320 --> 30:02.800
no question but I think saying that these models do not have a world

30:02.800 --> 30:09.280
model do not have any understanding is is really not correct so those are the

30:09.280 --> 30:13.040
contentions of the stochastic parrots paper wasn't it?

30:13.040 --> 30:19.360
yes and I so it's it's we can talk for hours and hours about this as well so

30:19.360 --> 30:24.720
many things I personally think that look

30:24.720 --> 30:31.120
these these models are really great pattern recognizers

30:31.120 --> 30:35.840
and one can argue that recognizing patterns and then trying to

30:35.840 --> 30:40.800
stitch them together is not real understanding but I would

30:40.800 --> 30:45.760
refute that and I would say that look at the end of the day for me as a researcher

30:45.760 --> 30:49.520
old I could do throughout the past six years or so

30:49.520 --> 30:52.640
was to think about ways of evaluating

30:52.640 --> 30:58.720
and like NOU systems for deeper understanding and these models are proving

30:58.720 --> 31:01.600
consistently that they're making progress

31:01.600 --> 31:06.880
towards doing what constitutes as having understanding so we can of course

31:06.880 --> 31:11.840
argue what is a good benchmark I think benchmarking is one of the problems

31:11.840 --> 31:15.760
we've had for years and years we are making progress but

31:15.760 --> 31:21.200
for me one of the reasons that why I want to work in startups is to

31:21.200 --> 31:25.680
you know build something in real work that actually works for end users

31:25.680 --> 31:31.520
in the messy noisy real-world environments as opposed to our lab settings

31:31.520 --> 31:35.440
so that's definitely a problem we have that we have really narrow inherently

31:35.440 --> 31:41.440
narrow and bias benchmarks but setting data side I think that

31:41.440 --> 31:45.360
you know this is like a kind of theoretical right like

31:45.360 --> 31:49.280
there are people who don't believe in distributional semantics being the

31:49.280 --> 31:52.960
like expression kind of meaning that you can represent

31:52.960 --> 31:57.200
and believe that formal semantics and formal kind of meaning representation is

31:57.200 --> 32:01.280
the way to go which I would argue against I think having

32:01.280 --> 32:06.160
a way of representing meaning distributionally sort of representing a word

32:06.160 --> 32:13.200
by the context in which it just is often occurring at is

32:13.200 --> 32:17.360
is a viable representation of meaning so I think as at the end of the day

32:17.360 --> 32:20.640
if we have the right benchmarks for evaluating

32:20.640 --> 32:24.800
representation of meaning we have the right benchmarks for

32:24.800 --> 32:28.400
evaluating common sense reasoning and if these models pass

32:28.400 --> 32:32.720
past them that that tells you something and this is to take I had been

32:32.720 --> 32:38.240
GPT-3 work came out so many people were asking my opinion as to

32:38.240 --> 32:41.760
how I think about this because you know I was one of the

32:41.760 --> 32:46.080
proponents of less push-d systems for deeper understanding and like

32:46.080 --> 32:50.000
you know they're not they're really lacking it but the truth is we

32:50.000 --> 32:54.960
have made progress I would say that we need to move the

32:54.960 --> 33:01.520
basically the the bar you should you know raise the bar of course as to

33:01.520 --> 33:05.840
pushing these models to go further and further but they've definitely come

33:05.840 --> 33:10.160
very far already so I don't think that these models are

33:10.160 --> 33:14.080
parrots really it depends on the definition of course of

33:14.080 --> 33:18.000
a parrot but if it means that it's really just repeating

33:18.000 --> 33:23.520
without having any kind of an understanding I think that's

33:23.520 --> 33:29.040
that's not the case so going back to kind of the

33:29.040 --> 33:32.720
the state of research broadly and the gaps

33:32.720 --> 33:37.600
that need to be overcome for you to accomplish what you're trying to do at

33:37.600 --> 33:45.520
Vernique it sounds like you know the first one of those is

33:45.520 --> 33:49.600
or the first you know area you know of exploration is

33:49.600 --> 33:53.360
just understanding in general in order for you to do what you're trying to

33:53.360 --> 33:57.360
do you need a system to be able to understand to some degree or

33:57.360 --> 34:01.920
another or according to some you know metrics the

34:01.920 --> 34:06.000
what the user is trying to express in whatever box they're typing in or

34:06.000 --> 34:10.720
whatever interface they're using and it sounds like

34:10.720 --> 34:17.120
you're saying that models that you would lean on for that

34:17.120 --> 34:22.080
understanding are broken in lots of ways and

34:22.080 --> 34:25.440
that's part of what you need to research is how to fix

34:25.440 --> 34:29.120
the ways that they're broken you know bias and

34:29.120 --> 34:33.360
transparency explainability however you want to to put that

34:33.360 --> 34:38.000
but you are you have seen enough evidence of

34:38.000 --> 34:42.240
enough understanding for you to do what you're trying to do for them to be

34:42.240 --> 34:46.960
promising yes absolutely and honestly not even

34:46.960 --> 34:50.880
just talking about the particular technology that you're building at

34:50.880 --> 34:55.680
Vernique but generally for the field I think that

34:55.680 --> 34:58.640
well first and foremost I hope that so many people

34:58.640 --> 35:03.840
work on so many other directions of AI so that we really have diversity of

35:03.840 --> 35:06.800
thought we have diversity of thinking we have other

35:06.800 --> 35:14.560
models that may get to flourish if anything deep learning

35:14.560 --> 35:18.640
trend has taught us that by a couple of people not giving up on what they

35:18.640 --> 35:22.480
believe then they could prove us all wrong right and then

35:22.480 --> 35:26.320
like all the amazing progress that we're seeing is the

35:26.320 --> 35:30.400
fruits of that basically but anyways on our end in particular

35:30.400 --> 35:33.440
for the sake of natural language understanding yes I think these

35:33.440 --> 35:37.840
models are to have tremendous flaws but they've shown

35:37.840 --> 35:44.000
him enough evidence of being basically

35:44.000 --> 35:48.000
foundational to say the least so I know that this is also something

35:48.000 --> 35:52.080
pretty controversial right now like you know when

35:52.080 --> 35:55.600
Stanford start calling these this whole like transfer learning and

35:55.600 --> 36:00.160
pre-training and fine-tuning paradigm foundation models

36:00.160 --> 36:04.160
so many people started raising eyebrows as to okay fund foundation should be

36:04.160 --> 36:08.160
reliable foundations should be that you can poke in like

36:08.160 --> 36:11.040
just just change things which I completely agree

36:11.040 --> 36:16.720
but setting the kind of arguing over the terminology aside

36:16.720 --> 36:20.480
I think that these models have shown enough evidence that they can give us

36:20.480 --> 36:24.480
like lexical and word knowledge which I think are

36:24.480 --> 36:28.640
very foundational for for building natural language understanding and

36:28.640 --> 36:32.400
dialogue systems like you know I come from the school of

36:32.400 --> 36:35.840
thought of people who have spent their lifetime trying to build

36:35.840 --> 36:39.600
semantic parsers which is which has to do with these formal

36:39.600 --> 36:43.760
like semantics of representing each and every word

36:43.760 --> 36:47.440
in a way that like conforms to an ontology and then

36:47.440 --> 36:50.240
how you would go about like representing a verb and

36:50.240 --> 36:53.440
conjunction with something else and like connecting the dots and

36:53.440 --> 36:56.800
everything so that you can represent the meaning and then on top of it

36:56.800 --> 37:01.040
building a dialogue system that recognizes intents and like tracks and

37:01.040 --> 37:04.880
those planning and everything so I think that

37:04.880 --> 37:09.200
what these models are doing and you know they are even

37:09.200 --> 37:12.400
very promising in doing things in a multilingual sense

37:12.400 --> 37:16.960
but anyways these what these models are doing is that they're enabling us to

37:16.960 --> 37:20.960
really let go of some of these pipeline like

37:20.960 --> 37:24.960
things that we had in an LP and just not starting from scratch basically

37:24.960 --> 37:28.080
starting from a model that comes at some sort of a

37:28.080 --> 37:32.480
lexical and word knowledge and turns out comments this knowledge baked in

37:32.480 --> 37:37.440
inish you know I they have flaws as you mentioned and we've talked about

37:37.440 --> 37:41.040
the fact that they're not controllable the fact that they're not transparent

37:41.040 --> 37:44.480
and the fact that you can't let them like teach them

37:44.480 --> 37:48.320
intractively and let them forget about things that they're doing wrongly these

37:48.320 --> 37:53.920
are old problems that need to be fixed which may require its own paradigm shift

37:53.920 --> 37:57.040
could be architecturally could be on the data side

37:57.040 --> 38:02.400
but I think yes to answer your question in one final sentence

38:02.400 --> 38:05.600
these models have shown enough evidence that they could be used as

38:05.600 --> 38:08.480
foundations so that we don't start from scratch

38:08.480 --> 38:15.520
yeah yeah maybe in kind of keeping conscious of time

38:15.520 --> 38:18.720
maybe we can switch gears from talking about kind of the

38:18.720 --> 38:25.440
research broadly to the the research that you're pursuing

38:25.440 --> 38:31.120
and you know I think we can infer from there what you think is

38:31.120 --> 38:38.160
missing in the the research broadly so it sounds like some controllability of

38:38.160 --> 38:44.480
language models is one of the areas what are some of the other areas

38:44.480 --> 38:52.560
of research that you're digging into so yes controllability is one

38:52.560 --> 39:00.240
absolutely we are so our line of thinking is that we believe in

39:00.240 --> 39:04.000
I think I've seen people use this terminology so I'll try to repeat it

39:04.000 --> 39:09.600
a retrieval augmented generation so we want to make sure that we don't

39:09.600 --> 39:14.720
build you know like visci washi language models at the end of the day just

39:14.720 --> 39:18.240
you don't know where the information is coming from we want our models to be

39:18.240 --> 39:21.680
able to retrieve from existing data sources as I mentioned the

39:21.680 --> 39:24.320
soul about data and from decision making we want to

39:24.320 --> 39:27.840
you as a user to know where your information is coming from

39:27.840 --> 39:31.760
or if we are like actively like telling you what

39:31.760 --> 39:35.760
basically rethink we want you to know the source of it

39:35.760 --> 39:40.960
so we want to basically work on ways of retrieving what is out there

39:40.960 --> 39:45.200
with the resource and the provenance intact this is another thing that we're

39:45.200 --> 39:48.640
pursuing and in general generalization right we want to build a

39:48.640 --> 39:52.880
domain general platform how do you make it so that the models

39:52.880 --> 39:58.160
that you have are truly generalizable right this has been an ongoing line of

39:58.160 --> 40:02.400
work for in deep learning for many years but it's

40:02.400 --> 40:06.480
you know of course not solved I think last two years or so has been phenomenal

40:06.480 --> 40:12.960
in terms of how how much more flexible and generalizable these models are

40:12.960 --> 40:17.200
but it's still not you know there's a very very long way to go

40:17.200 --> 40:21.440
the other thing is a data array these models are extremely data hungry

40:21.440 --> 40:26.160
I love how there has been a lot of progress on kind of zero shot and in

40:26.160 --> 40:31.200
context learning kind of paradigm at paradigms but at the end of the day

40:31.200 --> 40:36.240
still the wherever you go to a new domain and you see this first of the

40:36.240 --> 40:40.320
first hand when you work in a real-world setting like in startups

40:40.320 --> 40:44.480
these models are truly data hungry right how can you make it so that these

40:44.480 --> 40:48.320
models are more sample efficient and they don't really need that much

40:48.320 --> 40:52.720
training data for adapting to a new domain and this is for us

40:52.720 --> 40:57.360
of utmost priority because we literally want to make a domain general model

40:57.360 --> 41:01.200
so how do we just go from a domain to another

41:01.200 --> 41:08.000
on the technology side without spending a lot of time just collecting data

41:08.000 --> 41:12.480
and then tuning models learning like new intricacies of that that

41:12.480 --> 41:20.000
existing domain and although I see a lot of value

41:20.000 --> 41:24.320
and someone like myself having spent time thinking about

41:24.320 --> 41:29.840
problems that like a pure deep learning person would not have had and the

41:29.840 --> 41:33.760
value that it kind of brings to building a real-world product so

41:33.760 --> 41:38.320
like we need to work on how to build conversational agents right it's not just

41:38.320 --> 41:42.000
about natural language understanding like one utterance at the time

41:42.000 --> 41:46.400
but it has a lot to do with word modeling really and

41:46.400 --> 41:51.440
kind of building a belief system right the old-school kind of dialog systems

41:51.440 --> 41:55.920
had this framework called BDI that was really cool so belief desire

41:55.920 --> 41:58.640
intention so when you have a full-fished

41:58.640 --> 42:03.600
conversation with someone you basically think about

42:03.600 --> 42:07.840
building models basically and you think about them about what the other

42:07.840 --> 42:12.080
party knows what they don't know what you know how you can help them reach

42:12.080 --> 42:16.720
to a certain goal and you plan right so the existing systems out there

42:16.720 --> 42:19.840
don't really do any planning right these are all

42:19.840 --> 42:24.560
kinds of things that need to to be worked on how do you basically

42:24.560 --> 42:28.480
and build a sustainable word model that includes the sets of belief desires

42:28.480 --> 42:32.800
and intentions and you dynamically basically

42:32.800 --> 42:38.720
monitor right what's happening and another thing is this is also very important

42:38.720 --> 42:42.400
is that we need to have memory right systems at the end of the day

42:42.400 --> 42:47.440
controllability comes from them being able to store what they've learned

42:47.440 --> 42:52.480
from the interactions separate from their actual

42:52.480 --> 42:56.080
like prior knowledge separate from their other kind of conditions so that they

42:56.080 --> 42:59.600
can make an important basically decision and these are also

42:59.600 --> 43:03.600
all kinds of things that are not in these so-called like foundation models

43:03.600 --> 43:08.880
right and need to be worked on and so given

43:08.880 --> 43:13.440
the again kind of the breadth of all of these things that you need to figure

43:13.440 --> 43:18.080
out in order to to build a product how do you

43:18.080 --> 43:23.920
how do you scope that down like you know each of those could be a you know a

43:23.920 --> 43:30.560
four-year PhD effort right or or many right how do you scope that down and

43:30.560 --> 43:35.280
connect that to you know your your mission as a startup founder

43:35.280 --> 43:38.400
to get a product out the door that meets a need

43:38.400 --> 43:44.800
yes so we are not of course I outlined our years of planning right

43:44.800 --> 43:48.240
outlined the kind of problems that we have to work on it doesn't mean that

43:48.240 --> 43:51.520
we're working on it at this moment or we need to work on it

43:51.520 --> 43:56.720
immediately so that just to answer you on the business side how we are

43:56.720 --> 44:02.560
practical but to be honest we do so last like year and half now has been

44:02.560 --> 44:07.840
truly the most rewarding part of my entire life literally in terms of

44:07.840 --> 44:12.320
just seeing firsthand how far you can go then you have no other choice but to

44:12.320 --> 44:16.960
innovate but to just keep working at what you need to and

44:16.960 --> 44:21.280
the truth is although you're very small and which is the nature of a lot of

44:21.280 --> 44:26.000
startups anyways there's so much you can do if you really

44:26.000 --> 44:30.560
you know gets scrappy and like know where to spend your time and

44:30.560 --> 44:34.320
effort so we have this thing and at very need that I don't know if you know

44:34.320 --> 44:37.760
what I'm gonna just mention so there used to be this meme going on

44:37.760 --> 44:42.000
that was the sketching of spider-man that was done in 10 seconds versus

44:42.000 --> 44:47.440
the sketching of it in 10 minutes by an actual artist so even if you are

44:47.440 --> 44:51.520
the master of your skill being art you can always produce something in

44:51.520 --> 44:55.520
10 seconds and you should be always able to to do it and then there is a

44:55.520 --> 44:58.640
10 minute version of course that would be your best work so that's the

44:58.640 --> 45:02.160
person we take at Vernon for anything and we say this to all of our

45:02.160 --> 45:06.480
employees and we even said it to all of our interns

45:06.480 --> 45:09.920
that look you have you don't have 10 minutes you have 10 seconds you have to

45:09.920 --> 45:14.400
have a first version of this go go for it you you've done it on the

45:14.400 --> 45:18.880
design side as well like we have an exceptional designer and then she

45:18.880 --> 45:22.560
started we were like you have two weeks you have to come up with a full

45:22.560 --> 45:27.520
pledge okay UI UX and just go for it and she really did it and this is a test

45:27.520 --> 45:32.080
we do even when we are doing interviews so anyways we we're not working on

45:32.080 --> 45:37.360
everything and anything at once a lot of what I outline or honor and in our kind

45:37.360 --> 45:41.920
of a longer term planning but I think we've gotten really good at

45:41.920 --> 45:47.520
trying to to do a 10 second version of everything just to to know that we

45:47.520 --> 45:52.000
have something in place you know if you were advising

45:52.000 --> 45:56.880
you know someone who was thinking about starting a deep tech type of

45:56.880 --> 46:01.360
start-up what are kind of the general principles that you would

46:01.360 --> 46:10.320
suggest or or put forth for translating you know going from this

46:10.320 --> 46:15.840
kind of gap in the research to product like what are the general ideas

46:15.840 --> 46:21.200
there to me honestly so there there there were so many reasons why I wanted

46:21.200 --> 46:25.040
to embed myself in this start-up scene as opposed to like other maybe

46:25.040 --> 46:30.640
industry research labs or even academia and I think the main driver

46:30.640 --> 46:34.640
is and should be the fact that you want to build an AI

46:34.640 --> 46:39.680
system that actually works so I think for anyone who is working on a

46:39.680 --> 46:44.160
fundamental like a research at a fundamental research setting

46:44.160 --> 46:47.680
they can see what kind of thing they're really passionate about and see how

46:47.680 --> 46:53.040
far it has gone in like you know lab settings and see if it makes sense for it

46:53.040 --> 46:58.160
to want to to be basically in an actual product so I don't know if I have

46:58.160 --> 47:03.040
necessarily principles but I can just talk a little bit more about

47:03.040 --> 47:07.040
my own frustration as to the progress that I couldn't make

47:07.040 --> 47:13.120
outside of this this kind of start-up world that may resonate with someone

47:13.120 --> 47:18.320
out there that we might see it themselves as well. I would say that so I

47:18.320 --> 47:22.800
spend about like a year and a half or so back when I was in grad school

47:22.800 --> 47:29.120
and industry research labs and to me at the end of the day

47:29.120 --> 47:34.080
publishing for the sake of publishing was not really satisfactory so as much

47:34.080 --> 47:38.160
as the we are all seeing the fruits of academic like scientific research

47:38.160 --> 47:42.160
right then sharing which we will you know do at Bernic as well as part of our

47:42.160 --> 47:46.000
I think duty right as researchers to contribute back

47:46.000 --> 47:51.440
like having it as the the main thing that drives you the main thing that you

47:51.440 --> 47:54.000
have to report on the main thing that you care about

47:54.000 --> 47:58.240
is very counterproductive to me right and I feel like okay I'm spending all of

47:58.240 --> 48:03.360
this time of mine on basically this this line of work

48:03.360 --> 48:06.560
where is it going right what kind of value am I bringing to the

48:06.560 --> 48:09.840
broader world so that's like one of the reasons I

48:09.840 --> 48:13.200
felt like personally the kind of research I was doing for the sake of

48:13.200 --> 48:17.760
publishing is not really a good like bar to have for my life

48:17.760 --> 48:21.920
I want to do it for the sake of making a progress but if you had something

48:21.920 --> 48:24.880
worthy of saying you should say it I think it's

48:24.880 --> 48:28.640
our you know duty as I mentioned to to contribute back

48:28.640 --> 48:34.000
so that's an argument maybe for having a product to focus

48:34.000 --> 48:39.680
your research and I think maybe put differently from principles

48:39.680 --> 48:43.840
trying to get at the kernel of like you have so much that you could possibly

48:43.840 --> 48:47.680
research like just how do you prioritize how do you

48:47.680 --> 48:52.160
lead or focus a research team that could go in lots of

48:52.160 --> 48:56.240
different directions like you can't be Bell Labs and just

48:56.240 --> 48:58.880
do a little bit of everything and kind of have it

48:58.880 --> 49:02.720
well I guess that's the the example that you were just talking about you know

49:02.720 --> 49:07.040
with a product in mind like how do you prioritize

49:07.040 --> 49:11.520
what you're going to spend your time on so I think actually so

49:11.520 --> 49:14.800
we should talk in a few months when we have the actual product and then I would

49:14.800 --> 49:18.720
love to be more specific about how we did that because I think that

49:18.720 --> 49:22.800
answers a lot of these questions so the truth is honestly having a

49:22.800 --> 49:26.080
product helps with narrowing down the focus so much we are

49:26.080 --> 49:30.720
we are a truly laser focus right now on a particular domain

49:30.720 --> 49:34.880
which drives a lot of our decisions and gives us a lot of insights as to

49:34.880 --> 49:39.280
what to prioritize and what not to so just talking about the issues that I

49:39.280 --> 49:43.440
was outlining that we have with the existing you know models which kind of

49:43.440 --> 49:46.720
applies to the internal and house models that we have as well

49:46.720 --> 49:51.600
like lack of control lack of transparency like not being able to for them to

49:51.600 --> 49:55.680
work fast enough right because your user has a certain level of

49:55.680 --> 49:58.800
expectation as to how quickly they should respond this completely

49:58.800 --> 50:02.640
change when you go to a new domain so that just dictates our

50:02.640 --> 50:08.960
basically priorities it's like we it's so funny about like

50:08.960 --> 50:12.400
year and a half ago or so when Omid and I we were sitting down to

50:12.400 --> 50:16.800
just outline these features that we want our AI platform to have

50:16.800 --> 50:20.400
we literally wrote down the list like it should be instantaneous in terms of

50:20.400 --> 50:23.840
response time we should have controllability it should be I don't know first

50:23.840 --> 50:27.840
start being like single turn and then multi turn and then it should be

50:27.840 --> 50:32.000
domain general from the get go blah blah and we literally on the notion

50:32.000 --> 50:34.560
page which is the software we use for knowledge

50:34.560 --> 50:38.880
management we have a priority assigned to them which is like high media

50:38.880 --> 50:43.600
blah and then it changes per domain so if I could share that with you that

50:43.600 --> 50:46.720
notion page you would see that we are literally doing it like the

50:46.720 --> 50:50.320
features that even we care about in terms of the

50:50.320 --> 50:55.440
the technology itself is very much dependent on the domain and then we keep

50:55.440 --> 50:59.200
going back and forth on them depending of what we're focusing on

50:59.200 --> 51:03.600
for a particular basically quarter got it got it got it

51:03.600 --> 51:10.640
so so summary there is you know if you're very focused on kind of product and

51:10.640 --> 51:14.640
features then that will tell you what you need to figure out to

51:14.640 --> 51:19.680
deliver those and in order to tell me more detail you'd have to talk about the

51:19.680 --> 51:25.120
specific features and research and that's going to come soon

51:25.120 --> 51:29.120
yes yeah because I think actually you would really like it

51:29.120 --> 51:32.960
for us we had this dilemma of what our first kind of

51:32.960 --> 51:36.800
sector would be first domain would be and probably will be even couple when we

51:36.800 --> 51:41.600
come up with it or regardless the the thing that is the closest to our

51:41.600 --> 51:45.440
hearts is the one that has a lot to do with some of the things that we were

51:45.440 --> 51:48.880
just discussing and it really applies to everyone's day-to-day

51:48.880 --> 51:52.880
decision-making and it's really exciting so that's another thing there's like

51:52.880 --> 51:57.280
this kind of hidden decision like a feature right when you're

51:57.280 --> 52:01.680
prioritizing this gigantic space in this gigantic space which has to do

52:01.680 --> 52:06.400
with your personal passion it really is right when you're especially

52:06.400 --> 52:11.680
so people call this product market fit but I think in our more or less

52:11.680 --> 52:17.120
called technology market fit that like founders like us you have a

52:17.120 --> 52:22.240
particular technology that could be fitted into any market but in any product

52:22.240 --> 52:26.080
and now you have this dilemma of even thinking about the technology being

52:26.080 --> 52:29.360
fitted into market not just to the product being fitted into markets it's

52:29.360 --> 52:35.920
really even a larger search of space for a founder to want to to navigate

52:35.920 --> 52:39.840
but yeah I think that personal passion and personal care is something that

52:39.840 --> 52:43.520
will play a role and has played a role then you know hopefully a couple

52:43.520 --> 52:48.960
months from now and be chattel happily spill all the beans

52:48.960 --> 52:54.320
awesome awesome well Nestreen it was wonderful catching up with you as always

52:54.320 --> 52:58.400
thanks so much for taking time and looking forward to next time

52:58.400 --> 53:02.560
absolutely pleasure with mine yeah looking forward to next time

53:02.560 --> 53:30.720
awesome thank you

