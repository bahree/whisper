WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.880
I'm your host Sam Charrington.

00:31.880 --> 00:37.240
Are you looking forward to the role AI will play in your life or in your children's lives?

00:37.240 --> 00:41.000
Are you afraid of what's to come and the changes AI will bring?

00:41.000 --> 00:45.360
Or maybe you're skeptical and don't think we'll ever really achieve enough with AI

00:45.360 --> 00:47.280
to make a difference?

00:47.280 --> 00:52.120
In any case, if you're a Twimal listener, you probably have an opinion on the role AI

00:52.120 --> 00:56.480
will play in our lives and we want to hear your take.

00:56.480 --> 01:01.320
Sharing your thoughts takes two minutes, can be done from anywhere and qualifies you to

01:01.320 --> 01:03.320
win some great prizes.

01:03.320 --> 01:13.560
So hit pause and jump on over to twimbleai.com slash myai right now to share or learn more.

01:13.560 --> 01:19.480
In this episode, the third in our Black and AI series, I speak with Iyana Howard, chair

01:19.480 --> 01:23.200
of the interactive school of computing at Georgia Tech.

01:23.200 --> 01:29.480
Iyana joined me for a lively discussion about her work in the field of human robot interaction.

01:29.480 --> 01:34.520
In our discussion, we dig deep into a couple of major areas she's active in that have significant

01:34.520 --> 01:40.000
implications for the way we design and use artificial intelligence, namely, pediatric

01:40.000 --> 01:42.840
robotics and human robot trust.

01:42.840 --> 01:48.520
I found the latter bit particularly fascinating and Iyana provides a nice overview of a few

01:48.520 --> 01:54.360
of her experiments, including a simulation of an emergency situation where, well, I don't

01:54.360 --> 01:59.880
want to spoil it, but let's just say that as the actual intelligent beings, we really

01:59.880 --> 02:02.560
need to make some better decisions.

02:02.560 --> 02:06.480
This was a really fun interview and I'm happy to share it with you.

02:06.480 --> 02:07.880
Let's get to it.

02:07.880 --> 02:13.960
All right, everyone, I am on the line with Iyana Howard. Iyana is chair of the school of

02:13.960 --> 02:18.360
interactive computing in the college of computing at Georgia Tech.

02:18.360 --> 02:21.640
Iyana, welcome to this weekend machine learning and AI.

02:21.640 --> 02:25.400
Thank you. I appreciate the invite. I think we're going to have a beautiful conversation.

02:25.400 --> 02:30.240
I am really looking forward to it and why don't we get it kicked off by having you tell

02:30.240 --> 02:36.280
us a little bit about your background and how you got involved and interested in artificial

02:36.280 --> 02:38.280
intelligence?

02:38.280 --> 02:43.280
I was an old early adopter of AI.

02:43.280 --> 02:45.280
What does that mean?

02:45.280 --> 02:49.520
Well, because it's cool. Everyone's like, oh, I do AI. I'm doing machine learning.

02:49.520 --> 02:54.120
It's like, no, no, no. My rusty faces where we had to draw stuff and try to figure out

02:54.120 --> 02:56.720
how to put it, had a neural network in it.

02:56.720 --> 02:57.720
Nice.

02:57.720 --> 03:05.560
I've been doing this since 1994. I think I co-ed up my first neural network.

03:05.560 --> 03:06.560
Oh, wow.

03:06.560 --> 03:11.760
So that's what I'm saying. I'm old school. It wasn't a thing. It was just a way to make

03:11.760 --> 03:17.360
my robots more intelligent.

03:17.360 --> 03:27.200
My background, I consider myself a robotics person. Robotics and AI and embodied AI is really

03:27.200 --> 03:34.360
what I do and started in this very, very early on. I knew I wanted to do robotics since

03:34.360 --> 03:39.880
like middle school. Of course, it was a different robotics then. It was basically remote control

03:39.880 --> 03:47.600
cars that you could figure out how to program. But then that evolved and when I was working

03:47.600 --> 03:56.680
on my PhD thesis, I was also working at JPL, NASA JPL and I was coding up. I'm trying

03:56.680 --> 04:03.520
to figure out how do I make rovers more intelligent? So that was the objective and I thought that

04:03.520 --> 04:09.000
the best way to do it was try to figure out how people think and behave and process and

04:09.000 --> 04:14.840
try to encapsulate that and put it into my robot brain. And so that's why I was doing

04:14.840 --> 04:23.240
AI. It wasn't necessarily AI-ish. It was these algorithms allowed me to take human data,

04:23.240 --> 04:31.640
human expertise and put it into a form that my robots could understand. So yeah, like I

04:31.640 --> 04:38.760
always tell my students, I'm cool now, right? With enough time, hopefully, we'll all

04:38.760 --> 04:52.960
be cool. Nice. So you mentioned kind of capturing human experience and kind of using,

04:52.960 --> 04:57.520
you know, encoding that in your robots. You also mentioned neural nets. Like when I think

04:57.520 --> 05:05.160
of the way we captured human experience and put them into systems at that time, a lot

05:05.160 --> 05:11.080
of it was like expert systems and kind of the preview, you know, that origin, that wave

05:11.080 --> 05:19.960
of AI from like the 80s. Is that the kind of thing you were doing or were you doing the

05:19.960 --> 05:26.800
neural net stuff in that context as well? I was doing so my thesis, my approach was I

05:26.800 --> 05:35.400
had to figure out how do I get a robot manipulator. So robot hands and arms to grab objects

05:35.400 --> 05:40.120
that be formed. So the thing was is we wanted to bring robots into the hospital and we wanted

05:40.120 --> 05:45.960
them to do things like pick up pillows and sheets and things like that. And these objects

05:45.960 --> 05:52.000
weren't, they weren't fixed, right? They put the apply force and they changed the shape.

05:52.000 --> 05:58.400
And there was really no way of mathematically modeling that. And so my thing was let's

05:58.400 --> 06:05.720
learn. And so what I did is early, early human demonstration is I had sensors on these

06:05.720 --> 06:10.960
manipulators and I'd have people grab objects with them. And I would kind of like that data

06:10.960 --> 06:15.680
and I look and see, you know, how much force did they have apply? And then I would model

06:15.680 --> 06:19.800
the object deformation, but the input was about the human. Okay, this is how much and

06:19.800 --> 06:24.280
lift it up. Oh, it fell. Okay, this is how much they applied and it was solved. So that's

06:24.280 --> 06:29.080
how I got the data and that data was then used to train on neural network. So that the robot

06:29.080 --> 06:36.240
then could visually see this object. It would map that information in terms of a model

06:36.240 --> 06:42.400
that I created. And then as it would grab, it would visually match that deformation

06:42.400 --> 06:46.280
to the model it had stored in a neural network and say, okay, this is the force I think I

06:46.280 --> 06:52.040
should have given the shape of the object that I learned before with previous knowledge.

06:52.040 --> 06:56.920
Oh, interesting. And how much data did you have to collect? Do you remember?

06:56.920 --> 07:01.360
Yeah, so this wasn't like the deep learning stuff. This was, I mean, I literally had ten

07:01.360 --> 07:08.200
objects. This was this was a big thing. The fact that I could even do ten objects was

07:08.200 --> 07:16.800
like amazing. So yeah, it's not the neural networks of today. And so, yeah, limited data

07:16.800 --> 07:23.600
set. I mean, then though, the number of observations you had to do was a lot, considering it was

07:23.600 --> 07:27.880
only ten objects. I remember we would be in the lab and I'd be like, okay, let's run

07:27.880 --> 07:36.000
through another. Let's lift it up. So that felt like a lot of data then. I mean, it wasn't,

07:36.000 --> 07:43.280
but at the time, it was a lot of data. Right, right, right. If we only knew back then.

07:43.280 --> 07:49.920
I know. So fast forward, fast forward some years, what are you working on nowadays?

07:49.920 --> 07:55.120
So now I'm working on two, and I would call them two buckets that are really interesting

07:55.120 --> 08:02.840
to me. So one is looking at pediatric robotics. How do you create robot coaches, therapists

08:02.840 --> 08:08.480
that can work with children with special needs in the home to do exercise? And why I really

08:08.480 --> 08:14.880
interest me is because we have to do things like, how does a robot adapt to different kids?

08:14.880 --> 08:20.960
So every child's unique. How does it you bring a robot in and it uniquely identifies the

08:20.960 --> 08:27.360
needs of that child in fairly real time? You put emotions on a robot and emotions is

08:27.360 --> 08:32.400
like, why do you need that? Well, emotions help with the bonding. So a child, you want

08:32.400 --> 08:36.600
them to do something that's hard. So how do you get them to do something they may not

08:36.600 --> 08:41.800
want to do? Emotions allows the child to connect with the robot. So then the robot says

08:41.800 --> 08:47.160
it and the child just wants to please the robot because this is, this is, it's friend.

08:47.160 --> 08:51.680
This is his or her friend. So that's really interesting because I get to play with all

08:51.680 --> 08:57.920
of these things and the kind of science space and the psychology space to get the robot

08:57.920 --> 09:03.640
to have this bond and guide. And then we, of course, use the classical things like, you

09:03.640 --> 09:08.720
know, computer vision to extract what the child is doing in terms of their body movements

09:08.720 --> 09:14.080
and eye gaze and facial expressions to see, are we getting the right emotional response

09:14.080 --> 09:18.200
from the child? So some of the classical things are incorporated in that. So that's, I

09:18.200 --> 09:26.600
would say, half of my life. And then the other is this work I'm looking at and involved

09:26.600 --> 09:34.680
in with respect to trust in robots or trusted and embodied agents trust in AI. We have some

09:34.680 --> 09:44.080
interesting experiments where we have evidence that people over trust robots. Yeah. It was

09:44.080 --> 09:48.240
a scenario and it's like, I talk about it all the time and it's, it's one of these scenarios

09:48.240 --> 09:52.760
where your own hypothesis were wrong and you're like, oh my gosh, my hypothesis was wrong.

09:52.760 --> 09:57.880
This is interesting. So this was research that you and your group did? Yes. So this is

09:57.880 --> 10:03.640
research I did with, I had a colleague at the time who was at GTI, which is our research

10:03.640 --> 10:09.880
ramen in my students. And where I started off, of course, was in, with robots, you start

10:09.880 --> 10:15.440
off in simulation because it's really hard to deploy real robots. So you all start off

10:15.440 --> 10:21.200
in simulation and we were doing an emergency evacuation. So imagine you're in a building

10:21.200 --> 10:29.120
and the alarm goes off and you need to evacuate. So as you're evacuating, imagine that a robot

10:29.120 --> 10:33.680
comes and you know, shows you the directions of how to get out because it's chaos and

10:33.680 --> 10:40.800
all these things. And so we were, that was the scenario and what we wanted to do was understand

10:40.800 --> 10:45.560
if a robot makes mistakes, what would the person do with this regard? So we weren't even

10:45.560 --> 10:51.680
looking at, looking at, you know, introducing mistakes and how optimal does a robot have

10:51.680 --> 10:57.320
to be for people to follow? So that was, that was the real focus. That's how it started.

10:57.320 --> 11:04.360
And what we found out was that very early on, the robots would make mistakes and the people

11:04.360 --> 11:09.400
would still follow guidance of the robot. So this is kind of, this was interesting.

11:09.400 --> 11:14.640
Can you give an example of the kind of mistake that the robot would make and that would

11:14.640 --> 11:19.520
be followed? So I'm in a, again, this is, we started in simulation. So I'm in a simulated

11:19.520 --> 11:24.600
building, you know, we have the fire, like a virtual reality game, a robot appears, you

11:24.600 --> 11:29.760
know, follow me and you follow and then the robot goes and bumps into a wall and then

11:29.760 --> 11:34.920
pops up and bumps into the wall again. And then bumps into the wall again, right? So,

11:34.920 --> 11:38.680
and you have a choice. So the instructions are, you can find your own exit or you can

11:38.680 --> 11:46.240
stay with the robot. And so you would think, oh, person would like, okay, the robot's

11:46.240 --> 11:51.200
broken. Let me go find the person. Right. Right. We would not expect the person to just

11:51.200 --> 11:57.680
stay there and just touch the robot, fascinated by this robot that was clearly not doing something

11:57.680 --> 12:02.880
that it was supposed to be. Right. And again, it was more accidents. Our, our original

12:02.880 --> 12:07.920
objective was not this trust objective. So we started to push that like, okay, there's

12:07.920 --> 12:12.440
something strange about this. We're not quite sure, you know, what happens if we expose

12:12.440 --> 12:17.520
you to a broken robot or a robot that has mistakes before you go into the building and then

12:17.520 --> 12:23.200
do you? And so we just kept pushing it and kept pushing it. And our final experiment,

12:23.200 --> 12:29.160
which was in, we got to hardware through this transition from simulation to hardware,

12:29.160 --> 12:38.160
the guidance and our, our experiment that just baffled us was we had a abandoned building

12:38.160 --> 12:45.040
that was off lab. This was one of these, these participants that we had, you know, fire

12:45.040 --> 12:50.000
marshal was involved and things like that. We had the robot guide the person through

12:50.000 --> 12:55.280
the building to an office room. And in the office, they had to close the door and, you

12:55.280 --> 12:58.880
know, there was an article about some survey robot navigation and fill it out. So we

12:58.880 --> 13:03.960
tried to prompt the user to think that this was the experiment, right? This survey and

13:03.960 --> 13:10.720
things like that. And while the person was in the room, we filled the building up with

13:10.720 --> 13:18.080
smoke. So smoke to simulate a fire alarm. And then we set off the fire alarm. And so what

13:18.080 --> 13:22.440
happens is the door was closed. So fire alarm goes off like typical human behavior. You

13:22.440 --> 13:27.440
get up and you walk to the door, you know, evacuate. But when you open the door, just

13:27.440 --> 13:34.440
what you see, you see smoke and you see fire alarms and you see, yeah, right? And so

13:34.440 --> 13:38.760
you, you, you, okay, what are you supposed to do? You're definitely going to find an exit.

13:38.760 --> 13:43.960
I'm going to find an exit. What we did is we introduced the robot and intentionally the

13:43.960 --> 13:51.640
robot was guiding you to an exit where you did not come in. So we intentionally did that.

13:51.640 --> 13:56.640
So you come from different entrants, for example, entrance exit. And we guide you to a different

13:56.640 --> 14:02.160
one. And we wanted to see, you know, what would you do? Now it's like it's a for real thing.

14:02.160 --> 14:07.920
It's not simulation. It's right. It's real. What would you do? People follow the guidance

14:07.920 --> 14:13.800
of robot. Meaning through the, the banging into the wall thing, did you, did you incorporate

14:13.800 --> 14:19.840
that? Yeah. So we incorporated robots that would turn in place. They wouldn't do anything.

14:19.840 --> 14:24.680
We incorporated robots that would guide you into dark rooms. Like there was no lights

14:24.680 --> 14:28.560
with furniture blocking. And you would see people moving the furniture to go into these

14:28.560 --> 14:34.280
dark rooms. We introduced mistakes, even when they entered the building. Like when you

14:34.280 --> 14:38.920
came in, let's have the robot break down and do things like, you know, circle with places

14:38.920 --> 14:43.440
up with the controls. And then later bring that same robot and see, you know, now you have

14:43.440 --> 14:48.080
this notion, this robot doesn't work, you know, here's the robot again. What are you going

14:48.080 --> 14:54.580
to do? Wow. And time and time and time again, it broke our hypothesis. Our

14:54.580 --> 15:01.680
hypothesis would be, at some point, trust would be broken. Right. Right. And it was not

15:01.680 --> 15:09.360
which, which actually surprised us. And if you look at the data, there was some suggestions

15:09.360 --> 15:13.760
of why and we're teasing that out. So some suggestions were, well, it's a robot. It

15:13.760 --> 15:18.560
can fix itself, right? Like, yeah, I knew it was broken before, but it's a robot. It's

15:18.560 --> 15:23.320
a program, right? So of course, it was fixed. Or the robot had more information than

15:23.320 --> 15:33.200
I probably did. So it kind of knew better. So things like this where people were following

15:33.200 --> 15:39.120
this guidance and they were explaining why they should, like after the fact, explaining

15:39.120 --> 15:43.520
why it was perfectly logical for them to do that. And then I look at things like, you

15:43.520 --> 15:48.840
know, Tesla and the autonomous vehicles and crashes. And people are like, oh, how did

15:48.840 --> 15:56.240
you run into? And I'm like, no, it now makes perfect sense. Interesting. So did you make

15:56.240 --> 16:04.000
any attempts to baseline this against human human behavior, meaning, you know, you've

16:04.000 --> 16:11.860
got a human that's playing the role of the robot in these scenarios. And that is, you

16:11.860 --> 16:17.400
know, either making mistakes or clearly lost or something like that and try to determine

16:17.400 --> 16:23.440
whether the results you saw were just, you know, based on kind of authority figures and

16:23.440 --> 16:27.720
us, you know, blindly following authority figures, whether or not we deem them competent

16:27.720 --> 16:33.880
objectively or, you know, is it specific to robots? No, we think it's the fact that we

16:33.880 --> 16:41.040
as humans place these robots in a higher state. That's, that's what we think it is. Because

16:41.040 --> 16:48.040
we did, so the human human in simulation, not in the real world. And, and we think that

16:48.040 --> 16:53.160
is this aspect of like the robot knows better. The robot isn't expert in this, in this

16:53.160 --> 16:57.080
scenario. And so what did you see when you did human human in simulation?

16:57.080 --> 17:03.720
It was, it was the same. It was, well, so interesting enough, we did, again, the human human

17:03.720 --> 17:08.600
is teleoperating. So it's, it's a, there's a human controller kind of thing. We also did

17:08.600 --> 17:13.640
static signs like trying to compare robots and signs. So it was more of a comparison than

17:13.640 --> 17:21.520
anything else. And we found in like the human aspect, peer pressure was more effective.

17:21.520 --> 17:28.560
So if you had more than, so if you had an influencer that was very dominant, they usually can

17:28.560 --> 17:34.240
influence the person. If not, you had to have like additional people to influence the

17:34.240 --> 17:39.680
person. And so like if one person is like, I think it's over there, right versus no, it's

17:39.680 --> 17:45.480
over there. Right. And again, that's exuding, I guess authority. And so there, I think it's

17:45.480 --> 17:50.040
about three, like you need three people to say, I'm uncertain, but I think we, it's over

17:50.040 --> 17:59.200
there for it to be fact in that regard. So we, we do think it's this, and I won't say

17:59.200 --> 18:07.120
necessarily authority, but this fact of, this is the expert in this situation. And yet

18:07.120 --> 18:15.200
that this is a totally human situation. And that's really what is, is a little bit disconcerting.

18:15.200 --> 18:20.120
So we did another study also in this trust where we, in the therapy related to therapy,

18:20.120 --> 18:25.320
where we looked at comparison between a robot therapist and a human therapist and looked

18:25.320 --> 18:30.080
at aspects of trust. And so would you follow the guidance? And there we didn't make, we

18:30.080 --> 18:34.920
didn't do the mistakes. You just wanted to get a baseline of this, would you talk about

18:34.920 --> 18:40.320
authority, but basically feelings of, you know, this trust and following. And interesting

18:40.320 --> 18:46.000
enough in that scenario, for the robot, they literally would self, you know, in terms

18:46.000 --> 18:52.560
of our server results, they claimed that they trusted the robot. In the human scenario,

18:52.560 --> 18:57.600
they said, trust has nothing to do with it. It was the same task. I mean, literally,

18:57.600 --> 19:01.640
we had the robot and the human do exactly the same behaviors, because we didn't want

19:01.640 --> 19:06.800
to put in any nuances like, oh, the human smiled instead of, you know, instance one versus

19:06.800 --> 19:11.880
instance two. So we basically scripted the behavior of the robot and the therapist exactly

19:11.880 --> 19:17.800
the same. And what was the, the task here? It was a therapy task. So they basically had

19:17.800 --> 19:24.320
to follow the guidance of moving their arms in a certain configuration. So very, very,

19:24.320 --> 19:29.000
not a, not a very strenuous task, just very simple exercise. So physical therapy.

19:29.000 --> 19:39.920
Physical therapy. Right. And so the humans, when they were being guided by another human,

19:39.920 --> 19:45.440
it was, you know, just this thing that they, you know, did, you know, whereas when they

19:45.440 --> 19:50.960
were, there was a robot involved, it kind of evoked this, you know, question of whether

19:50.960 --> 19:53.920
there was trust involved in the relationship. Is that the idea?

19:53.920 --> 19:59.400
Correct. Correct. Correct. They weren't the same, even though the outcomes, because

19:59.400 --> 20:03.840
we measure the outcomes, like, what did you actually do? And the, the people, participants

20:03.840 --> 20:08.000
followed the exact same, like, trajectory and rules. And there was very little variation

20:08.000 --> 20:13.680
in terms of even, you know, how well they did the task, but yet their perception of the

20:13.680 --> 20:19.520
agent different. And even that, we even, we even modeled the exact same speech. Like,

20:19.520 --> 20:25.760
here is what you say, like exactly the same with robot and the human scripted. And yet there

20:25.760 --> 20:30.160
was a difference when it was the person. Now that's interesting, but I'm not sure what

20:30.160 --> 20:33.840
exactly it tells you. What, what conclusions did you draw from that?

20:33.840 --> 20:43.600
So the conclusions we drew is that because there's this aspect of, it goes with bonding,

20:43.600 --> 20:52.000
but this aspect of trust, what that means is I think that when you have these scenarios

20:52.000 --> 20:57.600
with these humans and these robots, that if a robot says something or does something or

20:57.600 --> 21:05.440
tells you some information, you have this assumption that the robot must be correct. Because

21:05.440 --> 21:10.800
I trust the robot is going to do the right thing. I trust the programmers that are programming

21:10.800 --> 21:16.880
the robot to do the right thing. Whereas with a human, it's just a, and human's question

21:16.880 --> 21:23.200
of the humans all the time, right? And I think it's because as soon as a human does something

21:23.200 --> 21:28.240
wrong, you'll probably be like, oh, you're wrong. I'm not going to trust you. But I think because

21:28.240 --> 21:32.880
based on a previous work, if a robot does something wrong, because you started off with this condition

21:32.880 --> 21:40.480
of trust, it doesn't, it doesn't break. It's interesting. You mentioned Tesla in passing. And

21:41.600 --> 21:47.280
you know, in this context, I can't help to think about how, you know, a lot of us are notorious

21:47.280 --> 21:53.920
backseat drivers, like we wouldn't get in a car and just like totally see control over to someone

21:53.920 --> 21:59.360
else without, you know, be constantly thinking about what they should be doing better. But yet,

21:59.360 --> 22:05.520
so many of us would sit in a Tesla that tells us, you know, pay attention, keep your hands on the wheel.

22:05.520 --> 22:16.720
And so I don't know. Right. And then, you know, there's a recent accident where a Tesla like slammed

22:16.720 --> 22:24.000
into a fire truck and it was, I think it was at 65 miles an hour. And I don't, I didn't see in

22:24.000 --> 22:31.040
the article whether the driver, you know, was claimed to, you know, being just, you know, was not

22:31.040 --> 22:36.080
distracted or, you know, was like deeply engaged in something else or, you know, in this context,

22:36.080 --> 22:40.400
you almost wonder if, you know, someone's thinking, oh, the car's going to handle it or something.

22:40.400 --> 22:45.840
I don't know. I think I'll do a last minute maneuver. Right. Right. Is autonomous vehicles

22:45.840 --> 22:50.880
an area that you're, that you're getting involved in and applying some of this stuff? We are. We are.

22:51.600 --> 22:58.080
We've done our first study where we are looking at, again, you always have to have a baseline. So

22:58.080 --> 23:04.480
we're at the baseline. What is your, if you know that there's another driver on the road that's

23:04.480 --> 23:10.640
human and that human makes a mistake. So what is the baseline? No mistake and then the human

23:10.640 --> 23:16.960
makes a mistake. What is your, what is your driving behavior? Does it change? And then do the same

23:16.960 --> 23:22.960
thing with a self-driving car to see what happens. So that's our baseline and we just collected our,

23:22.960 --> 23:29.520
I can't tell you the secret sauce yet. But over trust is there. Let's just put it that way.

23:30.320 --> 23:34.960
And so we're going to push that. And ultimately, it's like, well, why are you doing? So yeah,

23:34.960 --> 23:41.120
you prove that it's over trust. Ultimately, what I want to do with this research and with my lab

23:41.120 --> 23:47.200
is then come up with methods to mitigate it. Because the fact is, is that we are going to be dependent

23:47.200 --> 23:51.280
on these AI systems. Right. We're going to be reliant on them and we're going to trust them to do

23:51.280 --> 23:56.720
what they're supposed to do. And I think as your bodice is, we also need to ensure that if there's

23:56.720 --> 24:01.040
a scenario where we're uncertain, for example, you know, I can look at the data and be like, oh, yeah,

24:01.040 --> 24:07.360
we're 85 percent. Oh, that's good enough. Give the answer, right? I know this. I know how accurate

24:07.360 --> 24:13.040
my stuff is or how certain are, you know, oh, this data said I really haven't seen, but it's close

24:13.040 --> 24:19.600
to math and I come up with a metric of what's close to math. I think that information would be valuable.

24:19.600 --> 24:26.720
If I'm in a self-driving car, for example, and I see a scenario, I should be able to get feedback.

24:26.720 --> 24:34.000
Like, there's something in front of me. I don't know what it is, right? It's not the driver

24:34.000 --> 24:38.480
now. So then the driver could be like, oh, well, let me pay attention. Okay, because there's

24:38.480 --> 24:42.080
something's wrong. You're giving me information. That's not that's a little bit different.

24:42.800 --> 24:49.200
What are those things that we can do as roboticists to basically, I would say, you know, kind of jump

24:49.200 --> 24:54.960
start a person to think a little bit about it. And I think it's our responsibility to do that.

24:54.960 --> 25:01.680
But then also ensure that the humans also still follow the directions when they want to. I don't want,

25:01.680 --> 25:05.840
you know, my, like, with emergency evacuation. I don't want the robot to come. You're like, oh,

25:05.840 --> 25:10.640
no, no, no, no, I'm not going to follow you. And it's like, no, no, no, no, no, we want you to

25:10.640 --> 25:19.520
leave. This is not the time to say no. It's interesting in that it's almost like there's this

25:19.520 --> 25:26.560
compounding effect where, you know, you've got this over trust issue. But the things that we're

25:26.560 --> 25:34.480
over trusting are, you know, increasingly probabilistic where we've traditionally associated

25:34.480 --> 25:41.440
computers with, you know, deterministic correctness, right? Right. Right. And, you know, just the way

25:41.440 --> 25:47.600
you're, you know, describing, I'm kind of thinking through like all different kinds of ways that

25:47.600 --> 25:55.040
one might convey, you know, this, you know, probabilistic notions via, you know, robots and systems

25:55.040 --> 25:59.600
like this. Like what, you know, what's the confused face on your Tesla or some other robot?

25:59.600 --> 26:08.080
Right. What does that look like? Right. And how far have you gotten with that? Have you come up

26:08.080 --> 26:16.000
with directions on in that, or have you come up with any initial research into, you know,

26:16.000 --> 26:25.280
directions on conveying these kinds of probabilistic outcomes or we have, but it's not. And so with

26:25.280 --> 26:31.520
anything, it's not statistically significant, which basically means we have more work. So one,

26:31.520 --> 26:37.440
we've found that the way that you provide information, there's a timing constraint to it.

26:38.560 --> 26:45.360
When you provide that information, it's important because we filter. So depending on the timing,

26:45.360 --> 26:50.960
you'll filter out the information. Figure out what that timing is. We still, you know, we've

26:50.960 --> 26:56.480
probably, and again, we're at the stage of, okay, here's the event, you know, what happens if we

26:56.480 --> 27:01.200
provide the information right before, like at the point where they can still make a decision,

27:01.200 --> 27:06.240
but maybe it's a split second type of decision, what happens? Okay. What happens if we put it so that

27:06.240 --> 27:11.760
they have to think about it? And so we're playing with that to see maybe, because I can give you the

27:11.760 --> 27:20.400
information, but I have to also give you the urgency to do something and also make sure that you

27:20.400 --> 27:27.440
can do something. So if you think about the Tesla, maybe it's a blocky, you know, a way up ahead,

27:27.440 --> 27:32.880
but I'm only going one mile per hour, right? Probably don't need to tell my user, I'm confused.

27:33.920 --> 27:41.360
It may not make sense, but maybe if I'm, again, I'm hazy, I don't know what's going on,

27:41.360 --> 27:47.120
and I have my map, and I know that, you know, one mile ahead is a really bad intersection,

27:47.120 --> 27:51.360
because I've crowdsourced this data, and everyone says it's a really bad intersection,

27:51.360 --> 27:57.360
and my sensors aren't, my sensors aren't certain at this point. Okay, I think the timing is about

27:57.360 --> 28:02.640
right, you know, at this point. And so it's not like a magic bullet that says, this is it,

28:02.640 --> 28:09.200
I think it's scenario dependent, I think it's timing is really, really important, and then,

28:09.200 --> 28:14.080
of course, how you provide that information. You can't say, I'm 80% accurate, actually, means,

28:14.080 --> 28:20.080
and we've done some studies on, again, related to the medical, like, when doctors and clinicians

28:20.080 --> 28:28.320
get information, saying something like 80% is not as effective as just saying, I'm wrong, right?

28:29.920 --> 28:34.800
There's even ways of providing that data, so that user will understand what that means.

28:34.800 --> 28:47.520
Do you think the creators of these kinds of systems will be, you know, open to conveying

28:47.520 --> 28:56.400
this uncertainty? Like, you know, is there a sense where, you know, you convey an uncertainty,

28:56.400 --> 29:00.720
and the user might think that the system is broken as opposed to understanding that it's

29:00.720 --> 29:09.200
inherently probabilistic, and thus the, you know, the system makers won't want to be able to

29:09.200 --> 29:16.480
convey uncertainty. Have you explored that at all? Yeah, well, so what's, I'm very positive

29:16.480 --> 29:22.800
about though, is that there's this whole push now on transparency and AI algorithms.

29:23.600 --> 29:28.640
And AI and robotics are so tightly linked that it basically affects us as roboticists as well.

29:28.640 --> 29:34.160
And this aspect of, you know, especially with respect to the deep learning algorithms, you know,

29:34.160 --> 29:39.520
this concept of, you have a black box and you sort of know what goes in, but you sure don't know

29:39.520 --> 29:44.240
what's going on inside. And how do you make that more transparent? And so I think that there is

29:44.240 --> 29:52.240
now consensus that algorithms should be transparent. I think that there's still disagreement on

29:52.240 --> 29:58.640
how transparent and how that information should be provided to the user. That's still an ongoing

29:58.640 --> 30:04.240
debate. Because again, there's this balance. You need to still optimize the benefit,

30:05.360 --> 30:11.520
but you want to minimize the risk. And so it's a balancing act. Because if you're fully transparent,

30:12.560 --> 30:18.160
they may not, may, may or may not listen. I always say if you ever install software at any

30:18.160 --> 30:24.240
point in your life, there's literally what three or four pages of text, fully transparent, right?

30:25.120 --> 30:31.040
I agree. Like, no, right? It's only after the fact that like, I didn't realize I just signed my

30:31.040 --> 30:37.840
life away. That was line number 10. So that's a full transparent, but that's not what we're trying

30:37.840 --> 30:42.320
to get to, right? And so I think there's this balance. We have to figure this out. It's not about

30:42.320 --> 30:49.120
being fully transparent is about providing the information that we need at the time that it's needed.

30:49.120 --> 30:56.640
That's really the underlying issue. And does this, you mentioned earlier, we hadn't had a chance to

30:56.640 --> 31:04.400
dive into it. The work that you are doing with pediatric robotics, does the, how does the trust

31:04.400 --> 31:14.240
play into the pediatric robotics scenario? Yeah. So pediatric robotics, it relies on a bonding.

31:15.120 --> 31:19.760
So having an established relationship, which basically means you have this aspect of trust.

31:21.040 --> 31:27.680
And it's important because we're working with kids where we're doing something that might not

31:27.680 --> 31:33.360
be very comfortable because we're doing physical therapy. We're doing exercise. And so it's,

31:33.360 --> 31:38.080
it could get uncomfortable. We may want them to do it for longer than they want to. And so the

31:38.080 --> 31:42.800
child has to trust that this robot has their best interests in heart and is, quote, unquote,

31:42.800 --> 31:49.520
their friend. So it was really, really, really important. And in fact, we have shied away from

31:49.520 --> 31:56.800
introducing the mistakes. Even though we're like, we're like, oh, let's see what happens. Only because

31:56.800 --> 32:01.600
we're talking about this vulnerable population. And so you make mistakes. And if you have this

32:01.600 --> 32:06.880
bonding of trust, you might actually, I mean, it's physical therapy. You might fundamentally

32:06.880 --> 32:11.920
change the way they think about what's right or what's wrong. And so we've shied away from that

32:11.920 --> 32:16.640
because we intentionally want this bonding. We intentionally want this trust to have an

32:16.640 --> 32:22.160
optimal outcome. And so we can't play around with at least with children with special needs. Now

32:22.160 --> 32:28.320
with adults, we're doing, like I talked about the human and the robot. For adults, we can do it.

32:28.320 --> 32:34.640
Yeah. But for kids, we're very, very conscious of this population. And so when we're doing that,

32:34.640 --> 32:40.240
we actually are going out to hopefully improve outcomes. That's our ultimate goal. And to improve

32:40.240 --> 32:44.720
outcomes, you have to have bonding. You have to have this relationship building. You have to have

32:44.720 --> 32:49.520
this, quote, unquote, friendship, which of course equals this aspects of trust.

32:49.520 --> 32:59.680
And the is pediatric robotics to what degree is this a thing? I guess what degree are we there? Are

32:59.680 --> 33:07.600
there systems that are commercial or production systems out that are doing this? Or is this more

33:08.720 --> 33:14.880
in the research domain? I would say this is more in, so it's a combination. It's a combination

33:14.880 --> 33:24.720
of in the research, but in the clinical research domain. And so for places that do clinical research,

33:24.720 --> 33:30.400
like say hospitals, they are bringing in these platforms to look at outcomes. But it's not

33:30.400 --> 33:35.520
something where say a local clinic that is just providing services, not doing research,

33:35.520 --> 33:42.240
is going to bring into their home. So it's in that in between translational stage of proving.

33:42.240 --> 33:47.840
And mainly is because it's well, the hardware itself is still difficult to use. But it's also

33:47.840 --> 33:52.720
proving out these outcomes and proving out the interventions and basically saying, you know,

33:52.720 --> 34:00.000
for this target population, this is the intervention that works. And so it's more becomes a prescription.

34:00.000 --> 34:08.000
It's like that's trying to figure that out. And what's the research frontier in that domain?

34:08.000 --> 34:11.200
What are the main problems you're trying to solve at this point?

34:11.200 --> 34:20.720
So the main problem is long-term engagement and adaptation. So again, I like our longest

34:20.720 --> 34:25.200
days or eight weeks, which is not long-term. But imagine you have this robot in the home with

34:25.200 --> 34:33.200
this child for years. I would even be a year. I'd be happy with even eight years at this point.

34:34.160 --> 34:40.720
So how do you ensure one that the system can adapt to the needs of the child? Because the child

34:40.720 --> 34:47.200
is going to grow. They're going to improve on some measures of these outcomes. Also, the relationship

34:47.200 --> 34:55.120
as well. How does the robot identify the progression of the child, even in terms of the emotional

34:55.120 --> 34:59.840
state? They become more confident because they are getting better and having more outcomes.

34:59.840 --> 35:05.760
Does the robot have to change its behavior because of that? And then the personalization

35:05.760 --> 35:13.680
which is I bring in the same robot into a home. I bring it into home one. I bring it home to two.

35:13.680 --> 35:19.680
I bring it into home three. And each child is different. How do you deal with that? The nuances

35:19.680 --> 35:26.160
of the child, which also leads to this long-term adaptation because you have to be able to adapt

35:26.640 --> 35:29.520
in that moment as your calibration routine.

35:29.520 --> 35:41.680
And what are some of the research results that you've seen? I guess I'm curious, for examples of

35:41.680 --> 35:46.160
things that you publish in this area. So our outcomes, so believe it or not, most of the stuff

35:46.160 --> 35:52.400
we now publish is in the clinical literature. So we're at the outcomes state. And so we've shown

35:52.400 --> 36:01.280
improvements in things like range of motion in movement time, so how fast you move. So we've

36:01.280 --> 36:05.360
shown outcomes with children. Our primary target demographic has been children with cerebral

36:05.360 --> 36:12.400
palsy. So we have shown improved outcomes. On the, I would say, more of the technical techy side,

36:14.160 --> 36:19.120
we published, like the stuff that we published, like now we're done. We're like, we're done with

36:19.120 --> 36:24.800
that. A long-term adaptation is really linked to the kids. It's directly linked to the kids.

36:25.680 --> 36:31.040
It's about collecting the data and things like that. So the, I would say, the technology

36:31.040 --> 36:36.880
infrastructure was things like, how do you create an expert system based on a knowledge base of

36:38.240 --> 36:42.560
therapists interacting with kids? So that's how we started our initial training as we looked at

36:42.560 --> 36:49.680
therapist child interaction and looked at how do they interact with the child? What kind of information

36:49.680 --> 36:56.640
did they provide in terms of both verbal as well as gestural feedback and took that and started

36:56.640 --> 37:04.320
with an initial system that we can then have the robot extract. Looking at correlating facial

37:04.320 --> 37:09.440
expressions to an emotional state, you know, what does that look like? How do is that information

37:09.440 --> 37:14.320
get fed to the robot so that the robot can then provide the right emotions, whether it's happy or

37:14.320 --> 37:20.800
frustrated and things like that? So that was all of the work that then tied into more of these

37:20.800 --> 37:27.120
pilot slash, I won't even call them clinical, but pilot studies with clinical collapse.

37:28.400 --> 37:36.000
Okay. And so these robots that you're using in this scenario are these, you know, humanoid robots

37:36.000 --> 37:43.120
or they arms or something different? Yeah, so they're humanoid robots. And I mean, I roughly say

37:43.120 --> 37:49.600
that as humanoid, we don't use the lower, like we don't do... They're probably not a base or

37:49.600 --> 37:55.200
something like that. So they have legs, but in all of our experiments, it's about the arm movement

37:55.200 --> 38:02.480
and we just walk. So in theory, the robot doesn't have to have legs. So, but it does have to have

38:02.480 --> 38:09.760
arms because we have to do the gesturing. So, for example, when a child needs more guidance on

38:09.760 --> 38:16.720
a proper form, the robot has to use the arms to basically explain what the child should be doing.

38:16.720 --> 38:22.800
So we have to have the upper arms to show that movement so the child can mimic. But that's really

38:22.800 --> 38:28.880
the only requirement. Of course, the head, because we need to express emotions somehow. So, you know,

38:28.880 --> 38:37.360
head turn and things like that. So, yeah, humanoid, they're not that. So, we use the now and the

38:37.360 --> 38:43.600
Darwin. So, in terms of height wise, that's like 18 to the now's a little bit taller,

38:44.400 --> 38:48.560
two-ish feet. So, they're small robots. So, they're not human size.

38:48.560 --> 38:52.240
Okay. And so, these are... Their role is primarily as

38:52.240 --> 39:01.280
exemplars for the children to follow as opposed to, you know, I recall from physical therapy being

39:01.280 --> 39:09.520
physically manipulated to the project pain. Correct. Correct. So, yeah. So, what we do is

39:10.560 --> 39:17.040
non-contact we have. So, contact-based rehabilitation would be, I grab your arm and I move it.

39:17.040 --> 39:26.000
So, we do non-contact. And it's mainly... Well, there's a couple of reasons. One is because we want

39:26.000 --> 39:32.400
kids to improve based on their own... Based on their own kind of intrinsic motivation, pushing

39:32.400 --> 39:38.000
themselves. It's just like with... If you think about sports, like the only way you get better at

39:38.000 --> 39:43.760
sports is practice, practice, practice, practice. And you get better. And yeah, you can stop

39:43.760 --> 39:48.800
one and say an exoskeleton to a baseball player and they will perform very well, but as soon as

39:48.800 --> 39:58.000
you remove the exoskeleton, may or may not perform as well. And so, our focus is on non-contact.

39:58.000 --> 40:04.160
It takes longer time and longer term to get the improvements, but then they are their own and

40:04.160 --> 40:16.800
they're retained. Got it. Super interesting. You also mentioned early on that your... Well,

40:16.800 --> 40:21.920
one of the terms that you mentioned in passing was embodied AI. And I've had some... A few

40:21.920 --> 40:28.800
conversations with folks about kind of the role, the relationship between AI and embodiment. And

40:28.800 --> 40:35.040
I'm curious with all the work that you're doing in this area, what your perspective is.

40:37.120 --> 40:44.960
And maybe the background is... One of the comments that was made was that really we'll never get

40:44.960 --> 40:53.760
to true AI without embodiment because it is so inherent to what intelligence means for us as

40:53.760 --> 40:59.360
humans. And I'm curious if that's your perspective as well. Interesting. That must have been a

40:59.360 --> 41:05.520
roboticist, that's it. It wasn't a roboticist, that's it. Thank you. Yeah, that sounds like something

41:05.520 --> 41:13.920
we would talk about in our closed room. So, I'm not going to talk about true intelligence.

41:13.920 --> 41:22.640
That's like that's going down a rabbit hole. But the kind of the embodied AI versus AI,

41:24.080 --> 41:32.160
there is a large overlap. So that's one thing as a fact. Now, maybe not say 15 years ago,

41:32.160 --> 41:38.320
maybe it was a little bit more removed, but now there's a large overlap. And there are some

41:38.320 --> 41:46.160
things that are unique about being embodied in the physical world. But I think, and I'm going to

41:46.160 --> 41:54.560
put on my other hat, I think when you think about AI in general, and if it's a true agent,

41:54.560 --> 42:01.200
like an agent that it's exploring a virtual world, I think you have some of the same characteristics

42:01.200 --> 42:09.520
of an embodied AI agent, but not all of them. So if you, for example, decide, we'll use a chat bot,

42:09.520 --> 42:15.920
a chat bot, but put the chat bot in a virtual reality environment that has to interact with people.

42:17.360 --> 42:23.280
And so therefore, you might chat with someone, but then they look really weird or they might turn

42:23.280 --> 42:28.400
their backs on you. And so you then have to, you know, so then you have to learn how to interact.

42:28.400 --> 42:33.680
And so you're using physical motions. If you do the virtual reality environment correctly,

42:33.680 --> 42:38.400
you know, you're realizing that, you know, if I say something and they bring me a cup,

42:38.960 --> 42:43.600
I have to do something with the cup. And so maybe I shouldn't say that I'm thirsty if I'm really not.

42:45.040 --> 42:50.640
So I think you can learn a lot in the virtual world that's similar to being embodied.

42:50.640 --> 43:00.880
But then there's the uniqueness of having a physical agent in our real world because then the, like they say,

43:00.880 --> 43:07.760
the world kicks you in the face, right? So you're like, I have the perfect intelligent algorithm.

43:07.760 --> 43:12.880
And it's perfect in the virtual world. And you come into the real world and you realize that,

43:12.880 --> 43:19.120
you know, the building that you have the beautiful, beautiful map of is actually not correct

43:19.120 --> 43:23.600
because a human created that map. And of course, they might have taken shortcuts.

43:24.480 --> 43:28.160
And so now you're in the physical world and you realize and therefore you then have to think

43:28.160 --> 43:33.840
about alternatives because your map is 100% guaranteed not to be correct. And so then the way

43:33.840 --> 43:41.840
you think about intelligence and adaptation becomes more about problem solving to get to a solution.

43:41.840 --> 43:47.840
So it's a different way of thinking about intelligence versus just taking all the data and coming

43:47.840 --> 43:53.840
up with a conclusion. But there's a large, large overlap between the two. And I hate to say that,

43:53.840 --> 44:00.000
you know, literally 15 years ago, I'd be like, oh, no, no. Robotics is special, right?

44:01.360 --> 44:07.760
I think it's like, no, there's so much overlap now because the computing has become so powerful

44:07.760 --> 44:14.560
in terms of the computational aspects and the brain that we can now take these AI algorithms

44:14.560 --> 44:20.480
and put them on a robotic platform. Because we can do that, we can now take advantage of some of

44:20.480 --> 44:27.760
these more powerful AI algorithms and then incorporate these differences of being in the real world.

44:28.960 --> 44:34.320
Super interesting. So what's next for you given all the things that you

44:35.360 --> 44:41.200
have talked about and shared with us? What is the kind of future directions for you and your work?

44:41.200 --> 44:49.600
So one is, of course, the pediatrics. And we're moving into the smaller space. We're now looking at

44:49.600 --> 44:56.320
infants, which is actually unique because they don't actually respond in the same way as kids.

44:57.040 --> 45:02.000
You know, they're nonverbal for the most part except that they do have emotions. So we're working,

45:02.000 --> 45:08.160
we're pushing more into that space, younger and younger. And then on the trust aspects,

45:08.160 --> 45:13.680
it's this one more study is about looking at the parameters of trust.

45:15.520 --> 45:21.760
Is trust tied to things like education? Is it tied to economics? Is it tied to gender? Are there

45:21.760 --> 45:27.200
certain things that we can start modeling about trust? Like, oh, this person here, if I get your

45:27.200 --> 45:33.680
demographics, I can maybe more identify that you're more susceptible or not. Kind of looking at

45:33.680 --> 45:38.880
that as ways of then being able to mitigate. Like, if I have someone, I'm like, oh, guess what?

45:38.880 --> 45:43.840
All engineers will never trust. Then it doesn't make sense for me to provide any type of intervention

45:43.840 --> 45:49.760
for trust. But if I'm like, oh, this type of demographic, like maybe teenagers of the age between

45:49.760 --> 45:56.560
16 and 20, they will always trust, you know, if I can identify that, then my intervention methods

45:56.560 --> 46:02.800
might be slightly different. So pushing that a lot more, pushing it in the healthcare domain,

46:02.800 --> 46:09.360
but also in this autonomous, and I don't say autonomous vehicle, but these autonomous robots that

46:09.360 --> 46:17.440
are on the road, that are on the road before we even realize, I think that we need to start

46:17.440 --> 46:24.560
looking at this aspect of trust. And only I say that mainly because if anything really bad happens,

46:24.560 --> 46:30.640
it would totally destroy the community. And so I think we need to get in front of it before it,

46:30.640 --> 46:36.560
before it gets to that point. Right. Right. Well, I want to thank you so much for taking the time out

46:36.560 --> 46:47.440
to chat with me. Great conversation as you predicted. And I really enjoyed learning about what

46:47.440 --> 46:52.480
you're up to. Oh, no, thank you. Thank you. This was a beautiful conversation. I enjoyed it.

46:52.480 --> 47:00.960
All right, everyone, that's our show for today. Remember, we want to hear your thoughts on

47:00.960 --> 47:05.680
personal AI. If you were too excited about the interview to hit pause before,

47:06.320 --> 47:12.640
now's your chance to head on over to Twomlai.com slash my AI to talk back to us.

47:14.160 --> 47:18.880
For more information on Ayanna Howard or any of the topics covered in this episode,

47:18.880 --> 47:27.040
head on over to twomlai.com slash talk slash 110. Thanks so much for listening and catch you next time.

