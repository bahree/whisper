1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,200
I'm your host Sam Charrington.

4
00:00:32,200 --> 00:00:36,600
Next Tuesday, May 15, the Twimble online meetup is back.

5
00:00:36,600 --> 00:00:43,040
Our main event will be a presentation by Santosh GSK on the paper YOLO9000 Better Faster

6
00:00:43,040 --> 00:00:46,760
Stronger by Joseph Redman and Ali Farhaddi.

7
00:00:46,760 --> 00:00:51,640
We'll also discuss the landscape of object detection, the current state of algorithms

8
00:00:51,640 --> 00:00:54,720
in that space, and the challenges ahead.

9
00:00:54,720 --> 00:01:01,480
If you aren't already signed up, head over to twimbleai.com slash meetup to register.

10
00:01:01,480 --> 00:01:03,640
See you there.

11
00:01:03,640 --> 00:01:05,520
And event season continues.

12
00:01:05,520 --> 00:01:10,480
Today in tomorrow I'll be at Figure 8 Train AI Conference in San Francisco.

13
00:01:10,480 --> 00:01:15,240
I'll be podcasting all day from the event, so if you're attending the conference, stop

14
00:01:15,240 --> 00:01:17,360
by and pick up a sticker.

15
00:01:17,360 --> 00:01:23,560
It's not too late to use our Twimbleai discount code if you'd like to attend.

16
00:01:23,560 --> 00:01:28,360
In this episode, I'm joined by Jose Hernandez Arayo, professor and the Department of Information

17
00:01:28,360 --> 00:01:34,640
Systems in Computing at the Polytechnic University of Valencia, and fellow at the Leverhume Center

18
00:01:34,640 --> 00:01:40,160
for the Future of Intelligence, working on the Kinds of Intelligence Project.

19
00:01:40,160 --> 00:01:44,440
Jose and I caught up at Nips last year after the Kinds of Intelligence Symposium that he

20
00:01:44,440 --> 00:01:46,440
helped organize there.

21
00:01:46,440 --> 00:01:51,000
In our conversation, we discussed the three main themes of the symposium, which were

22
00:01:51,000 --> 00:01:56,960
understanding and identifying the main types of intelligence, including non-human intelligence,

23
00:01:56,960 --> 00:02:02,000
developing better ways to test and measure these intelligence, and understanding how and

24
00:02:02,000 --> 00:02:06,240
where research efforts should focus to best benefit society.

25
00:02:06,240 --> 00:02:10,240
Alright, let's do it.

26
00:02:10,240 --> 00:02:17,240
Alright, everyone, I am here at Nips in Long Beach, California, and I have the pleasure of

27
00:02:17,240 --> 00:02:25,040
being seated with Jose Hernandez Arayo, who is a professor at the University of Polytechnica

28
00:02:25,040 --> 00:02:31,800
of Valencia, and is currently on sabbatical and visiting at the Center for the Future

29
00:02:31,800 --> 00:02:34,720
of Intelligence at Cambridge, UK.

30
00:02:34,720 --> 00:02:38,240
Jose, welcome to this week in Machine Learning and AI.

31
00:02:38,240 --> 00:02:39,720
Thank you for having me here.

32
00:02:39,720 --> 00:02:40,720
Absolutely.

33
00:02:40,720 --> 00:02:41,720
Absolutely.

34
00:02:41,720 --> 00:02:44,280
So, why don't we get started by having you tell us a little bit about your background

35
00:02:44,280 --> 00:02:47,400
and how you got involved in the Machine Learning world?

36
00:02:47,400 --> 00:02:52,920
Well, it's a long story, probably, that many people in the area would share.

37
00:02:52,920 --> 00:02:59,600
As a child, I had my first computers, and at some times I thought, how can I make this

38
00:02:59,600 --> 00:03:01,960
computer think?

39
00:03:01,960 --> 00:03:07,200
And at the time I had, well, probably it's not very common here in the States, but in

40
00:03:07,200 --> 00:03:13,080
Europe there was this ZX, which is kind of a Commodore, the early computer, the early

41
00:03:13,080 --> 00:03:15,080
computer, like the Sinclair.

42
00:03:15,080 --> 00:03:16,080
Yeah, Sinclair, yeah.

43
00:03:16,080 --> 00:03:20,800
I had that one with 48 Ks, memory.

44
00:03:20,800 --> 00:03:26,640
And at some point I started to play with some kind of basic ideas of logic, because you

45
00:03:26,640 --> 00:03:29,920
have that at high school and, okay, this is about thought.

46
00:03:29,920 --> 00:03:34,800
And then, well, even I just played a little bit with some kind of probabilistic versions

47
00:03:34,800 --> 00:03:40,040
of that, even now it's completely naive from the point of view of, well, you know, the

48
00:03:40,040 --> 00:03:41,920
things are much more complicated than that.

49
00:03:41,920 --> 00:03:44,320
They will try as a teenager and all of that.

50
00:03:44,320 --> 00:03:49,280
And then, well, because of all of that, I started the computer science, but at some point

51
00:03:49,280 --> 00:03:55,320
when I was, what I completed that, I felt that what I really liked was AI, that of course

52
00:03:55,320 --> 00:04:01,040
I was from the beginning, but I didn't see that much AI, just one subject during a five

53
00:04:01,040 --> 00:04:05,240
year degree, and I was like, can it be that you do computer science, no machine learning

54
00:04:05,240 --> 00:04:06,240
at the time.

55
00:04:06,240 --> 00:04:09,160
It was a wind, it was what was this, this the 90s, right?

56
00:04:09,160 --> 00:04:17,160
So, and I decided I have to do a PhD one, and at some time it was an MSC and a PhD on

57
00:04:17,160 --> 00:04:18,840
something different, more AI.

58
00:04:18,840 --> 00:04:21,840
It wasn't that easy to find something.

59
00:04:21,840 --> 00:04:29,640
It was all about these expert systems, and I said, this is not exactly AI, for me, so

60
00:04:29,640 --> 00:04:37,200
this is, and at some point, I did a PhD on something related to, I started with deduction

61
00:04:37,200 --> 00:04:42,320
I still, I thought the reason it was a lot about logic and all of that, and at some point

62
00:04:42,320 --> 00:04:47,560
to learn, this is more about inductive inference about learning.

63
00:04:47,560 --> 00:04:54,360
And then I moved a little bit to machine learning, at some point, then when I started as

64
00:04:54,360 --> 00:05:01,400
an academic, at some point, I was really concerned, and that was from the beginning, about

65
00:05:01,400 --> 00:05:09,560
how to certify at some point that the system has some capabilities, or how you would say

66
00:05:09,560 --> 00:05:14,920
that the system is able to do something, so that's an kind of evaluation of these systems.

67
00:05:14,920 --> 00:05:20,520
And of course, at that time, people already knew about the during tests, and I was also

68
00:05:20,520 --> 00:05:22,200
interested in psychometric tests.

69
00:05:22,200 --> 00:05:23,200
Okay.

70
00:05:23,200 --> 00:05:26,120
About 20 years ago, so it's a long time.

71
00:05:26,120 --> 00:05:31,320
And then I, well, because you, at that time, machine learning started to flourishing with

72
00:05:31,320 --> 00:05:39,440
applications, the terms data mining became fashionable at that time, and I also worked on building

73
00:05:39,440 --> 00:05:47,480
different kinds of classifiers, I was involved in ICML for some periods, in terms of trying

74
00:05:47,480 --> 00:05:56,800
to submit papers, so that's basically a traditional, let's say, a career in trying to get into

75
00:05:56,800 --> 00:06:04,040
the field of machine learnings in the 2000, and then at some point, about a few years

76
00:06:04,040 --> 00:06:09,800
ago, I recorded some of these ideas out, because I was working on evaluation of classifiers,

77
00:06:09,800 --> 00:06:17,960
okay, so things like ROC curves, and how to evaluate a system for a range of context rather

78
00:06:17,960 --> 00:06:23,040
so related to cost-sensitive learning, and things like that, and I said, okay, but some

79
00:06:23,040 --> 00:06:28,880
of these ideas could be linked to the evaluation of AI as well, and how to evaluate some other

80
00:06:28,880 --> 00:06:30,840
more general systems.

81
00:06:30,840 --> 00:06:38,720
And at some point, and I started to work on this area that I call AI evaluation, and

82
00:06:38,720 --> 00:06:43,480
I saw that there was a lot of things to do there, it's perhaps not an area where people

83
00:06:43,480 --> 00:06:49,840
prefer talking about building things, but I like to evaluate what you do is, well, you

84
00:06:49,840 --> 00:06:57,680
are on the other side, like you are not an engineer, but I thought that this was really

85
00:06:57,680 --> 00:07:03,200
important, and whenever I talk to people, well, we need to know where we are, where we

86
00:07:03,200 --> 00:07:07,680
are going, so it's important we have very good measurement instruments, and at the moment,

87
00:07:07,680 --> 00:07:11,960
we don't have very good instruments in AI, because it's evolving so far, and we are also

88
00:07:11,960 --> 00:07:19,760
focused on particular challenges and tasks that we are not really sure whether our systems

89
00:07:19,760 --> 00:07:21,200
are really progressing.

90
00:07:21,200 --> 00:07:24,440
We have that feeling, and of course, there's a kind of objectivity there that there

91
00:07:24,440 --> 00:07:29,160
were systems that are much better now, because we are able to solve more, but at some point

92
00:07:29,160 --> 00:07:34,040
I said that there's a gap here, and I tried to recover some of the old ideas with new

93
00:07:34,040 --> 00:07:39,160
ideas, tried to see what people were doing in terms of evaluating, not only machine learning,

94
00:07:39,160 --> 00:07:46,600
but AI in general, also robotics, and I started to work on this area, and then about

95
00:07:46,600 --> 00:07:52,040
one year ago I wrote a book about this, I've been involved in the organization of several

96
00:07:52,040 --> 00:07:59,000
events around AI evaluation, all of these things, and now I'm here at the last year, at

97
00:07:59,000 --> 00:08:06,600
this center for the future intelligence in Cambridge, where they are doing this idea of evaluation

98
00:08:06,600 --> 00:08:14,320
goes well with some of the general objectives of the center, and basically I'm still really

99
00:08:14,320 --> 00:08:17,800
enthusiastic about machine learning and AI, and see what's going on.

100
00:08:17,800 --> 00:08:19,560
Awesome, what's the title of your book?

101
00:08:19,560 --> 00:08:24,800
Well, the title is the measure of all minds, evaluating natural and artificial intelligence,

102
00:08:24,800 --> 00:08:33,320
so it covers AI evaluation, but also it puts that in the context of how natural intelligence

103
00:08:33,320 --> 00:08:38,960
is evaluated, animals, non-human animals, and humans, a little bit of animal commission,

104
00:08:38,960 --> 00:08:43,880
how tests that you can use, for instance, for development in children, or tests that

105
00:08:43,880 --> 00:08:49,400
you would use to test, or maybe IQ tests, what's the relation between IQ tests, and what

106
00:08:49,400 --> 00:08:56,200
we are doing in AI, and then the first answer is perhaps nothing, but there's something

107
00:08:56,200 --> 00:09:00,440
that you scratch a little bit on the surface, you find connections, and these interesting

108
00:09:00,440 --> 00:09:03,960
questions, some of these connections are developed in the book, for instance.

109
00:09:03,960 --> 00:09:07,400
I'd love to dig into that a little bit, in a little bit more detail, but first I'd like

110
00:09:07,400 --> 00:09:12,760
you to describe the symposium that you helped organize here at Nips, called the Kinds

111
00:09:12,760 --> 00:09:14,760
of Intelligence, right?

112
00:09:14,760 --> 00:09:21,760
Yes, so it was around a project that we have at the center of the future of intelligence

113
00:09:21,760 --> 00:09:26,800
in Cambridge, UK, this project is called Kinds of Intelligence, or at some point there

114
00:09:26,800 --> 00:09:33,520
was a suggestion to propose a symposium on some of the ideas of how to characterize the

115
00:09:33,520 --> 00:09:38,880
different kinds of intelligence that we know at the moment, not only human intelligence,

116
00:09:38,880 --> 00:09:47,320
but also non-human animal intelligence, natural intelligence, and how to locate AI in this

117
00:09:47,320 --> 00:09:49,160
landscape of intelligence.

118
00:09:49,160 --> 00:09:54,880
So that was the original idea of the symposium, and then at some point we developed that idea

119
00:09:54,880 --> 00:10:05,520
into three strands, the first strand was to understand this space, and to see where AI is

120
00:10:05,520 --> 00:10:10,640
just represented as a subset of human intelligence, or something completely different that is

121
00:10:10,640 --> 00:10:15,160
taking us to different places, so trying to analyze this landscape a little bit from different

122
00:10:15,160 --> 00:10:22,800
perspectives, and we wanted to have different perspectives from animal commission and from

123
00:10:22,800 --> 00:10:27,320
human commission as well, for human intelligence and development.

124
00:10:27,320 --> 00:10:35,320
And the second strand was about how to test all of this, and of course I had some influence

125
00:10:35,320 --> 00:10:42,160
in having this strand in the symposium, I was okay, now we have this landscape, but

126
00:10:42,160 --> 00:10:48,660
how can we locate where we are, so we are moving in some direction, but can we say okay,

127
00:10:48,660 --> 00:10:54,240
what are the dimensions of this landscape, and how can we certify that the system is

128
00:10:54,240 --> 00:10:56,360
moving in that direction.

129
00:10:56,360 --> 00:11:02,040
And the third one was okay, we are able to answer all these questions, which of course

130
00:11:02,040 --> 00:11:09,400
sounds very abstract and challenging, the third question is now that we are able to understand

131
00:11:09,400 --> 00:11:14,600
this landscape where we are, the question is where we want to go, and there was a third

132
00:11:14,600 --> 00:11:22,320
strand of this symposium about what are the priorities for society, and whether these

133
00:11:22,320 --> 00:11:29,680
priorities, I wouldn't say the low hanging fruits because some of the recent challenges

134
00:11:29,680 --> 00:11:35,520
are really, really challenging, but sometimes we are motivated by things that are doable

135
00:11:35,520 --> 00:11:43,360
in with current technology, rather than perhaps aiming at the really important problems,

136
00:11:43,360 --> 00:11:47,000
because you are not going to have a success in a matter of one, two years, so it's more

137
00:11:47,000 --> 00:11:51,600
like a long-term project that perhaps academia, or even government, can be interested,

138
00:11:51,600 --> 00:11:56,920
they have to ask companies, but this is changing because the tech giants are also interested

139
00:11:56,920 --> 00:12:05,520
now in long-term goals, and also some areas that we might reach at some point in the future,

140
00:12:05,520 --> 00:12:12,320
even in the near future, that are perhaps dangerous or unethical, but if we don't know where

141
00:12:12,320 --> 00:12:16,040
we are, it is very difficult also to assess the progress in the direction that we want

142
00:12:16,040 --> 00:12:17,040
to take.

143
00:12:17,040 --> 00:12:23,200
So we have that, and in the end, yesterday we had a fantastic lineup of speakers.

144
00:12:23,200 --> 00:12:24,200
I saw that.

145
00:12:24,200 --> 00:12:25,480
Yeah, from different areas.

146
00:12:25,480 --> 00:12:31,720
So it seemed very interdisciplinary, very diverse in terms of the viewpoints that folks came

147
00:12:31,720 --> 00:12:32,720
from.

148
00:12:32,720 --> 00:12:37,880
Yeah, we have people from, for animal cognition, from human intelligence, we have people

149
00:12:37,880 --> 00:12:46,200
from AI, for different perspectives from AI, people perhaps more in favor about more orthodox

150
00:12:46,200 --> 00:12:52,080
approach to AI, where you would like to learn from a lot of example of more in alignment

151
00:12:52,080 --> 00:12:58,240
with the, with nips, people are more contrarian to this view, where you're okay, what about

152
00:12:58,240 --> 00:13:05,280
the learning more human-like, or more with a few examples, or more hypothesis-driven rather

153
00:13:05,280 --> 00:13:07,400
than data-driven?

154
00:13:07,400 --> 00:13:12,640
And we have a very interesting discussion in the second session about this, the different

155
00:13:12,640 --> 00:13:13,640
perspectives.

156
00:13:13,640 --> 00:13:16,880
Also, we had people from the first session come in and ask questions to the people

157
00:13:16,880 --> 00:13:17,880
of the second session.

158
00:13:17,880 --> 00:13:19,040
It was very active.

159
00:13:19,040 --> 00:13:23,440
The third session was a little bit about this society, some of the risks, and some of

160
00:13:23,440 --> 00:13:28,360
the things that people are talking that we are going to see in the future, like at some

161
00:13:28,360 --> 00:13:32,960
point the discussion, it was, also indeed, it was at the right moment also to talk about

162
00:13:32,960 --> 00:13:38,760
at some point things like corporations having a lot of power, and there was a moment where

163
00:13:38,760 --> 00:13:46,120
there was this link about AI in the future, could resemble some of the corporations that

164
00:13:46,120 --> 00:13:50,840
we have at the moment, or that we have had in the past two centuries, and they have a

165
00:13:50,840 --> 00:13:52,440
lot of power.

166
00:13:52,440 --> 00:13:56,840
So that perhaps we can link some of the risks to some things that we have already seen,

167
00:13:56,840 --> 00:14:02,240
so not everything that is coming is new in terms of the effects on society.

168
00:14:02,240 --> 00:14:03,240
Interesting.

169
00:14:03,240 --> 00:14:08,160
Maybe we can take these three strands in turn and spend a little bit of time kind of

170
00:14:08,160 --> 00:14:13,360
having you characterize the landscape before diving a little bit deeper into the measurement

171
00:14:13,360 --> 00:14:18,720
in your research in that area, and then we'll finish up with some of the directional stuff

172
00:14:18,720 --> 00:14:20,760
that you discussed.

173
00:14:20,760 --> 00:14:22,600
So what does that landscape look like?

174
00:14:22,600 --> 00:14:26,280
How do you characterize the way folks are thinking about the kinds of intelligence?

175
00:14:26,280 --> 00:14:34,040
Yeah, that's a really challenging problem, because the first thing is we disagree on our

176
00:14:34,040 --> 00:14:35,920
notion of intelligence.

177
00:14:35,920 --> 00:14:41,520
Some people even say that the term intelligence should be eliminated from our discourse,

178
00:14:41,520 --> 00:14:49,360
and we should only focus on, or even just use the word learning and forget about intelligence.

179
00:14:49,360 --> 00:14:56,480
Some people, because we have these two different views of intelligence, I would call one extreme

180
00:14:56,480 --> 00:15:03,640
is we negate that there's such a thing as intelligence, and some of these people, you

181
00:15:03,640 --> 00:15:08,280
can also have some of these people from, even from machine learning, saying we have the

182
00:15:08,280 --> 00:15:14,720
no freelance theorem, so even if you design a system to solve this problem, there's some

183
00:15:14,720 --> 00:15:18,600
other problem for which there are other systems that will be optimal for these problems,

184
00:15:18,600 --> 00:15:20,480
but not for the first problem and the other way around.

185
00:15:20,480 --> 00:15:27,600
So this idea of just having more general systems or more intelligent systems is nonsense.

186
00:15:27,600 --> 00:15:31,880
Well, I would argue against that, but then you have the other extreme.

187
00:15:31,880 --> 00:15:37,320
It seems like being intelligent is very different from being optimal at everything, which is

188
00:15:37,320 --> 00:15:41,880
kind of what this no freelance theorem seems to argue against.

189
00:15:41,880 --> 00:15:46,040
Yeah, well, I think that that's assuming that our world is random in a way.

190
00:15:46,040 --> 00:15:50,720
Do you have this block uniformity as an assumption, which is, well, the special case is that

191
00:15:50,720 --> 00:15:55,760
the data you receive is random, so it just says, if that's a case, well, the theorem

192
00:15:55,760 --> 00:16:00,120
is a theorem, that's a proof, and you say, okay, so any machine learning algorithm will

193
00:16:00,120 --> 00:16:04,840
be equally good or equally bad at, but the thing is the assumption that there's assuming

194
00:16:04,840 --> 00:16:10,200
something very, very strong, and then basically that our university is completely chaotic,

195
00:16:10,200 --> 00:16:14,880
and it's not like, wow, there are patterns, and there are patterns not because they're

196
00:16:14,880 --> 00:16:23,320
laws of physics, patterns because what we receive goes through humans, animals, devices,

197
00:16:23,320 --> 00:16:28,920
so not just to explain this a little bit more technical, when you have a theory machine

198
00:16:28,920 --> 00:16:33,160
or any machine, it doesn't have to be a theory machine, but just any machine and you put

199
00:16:33,160 --> 00:16:37,960
random inputs, what you get as an output is not random at all, right, right.

200
00:16:37,960 --> 00:16:41,880
So what we see in our world, because it's just going through all of these filters and

201
00:16:41,880 --> 00:16:50,600
these filters are machines in a way that it's an animal or it's a thermostat or something

202
00:16:50,600 --> 00:16:56,680
like that, what you get as a result has patterns, and then you can explore these patterns,

203
00:16:56,680 --> 00:17:03,560
and if that's the case, then the assumptions for the North Korean land students don't hold

204
00:17:03,560 --> 00:17:07,640
in that scenario, but well, that will be like sometimes it gets a little bit philosophical

205
00:17:07,640 --> 00:17:14,280
about that question, but that's one extreme, so you have like infinitely many intelligences,

206
00:17:14,280 --> 00:17:19,560
and this links a little bit with to psychology where you say, okay, there's not such a thing

207
00:17:19,560 --> 00:17:25,640
as intelligence, and you will have just many of the multiple intelligences theory where

208
00:17:25,640 --> 00:17:30,440
you will be good at this, but you're not good at that. The other extreme is there's only one

209
00:17:30,440 --> 00:17:36,440
single intelligence, and we hear that occasionally, especially for some of these discussions

210
00:17:36,440 --> 00:17:40,840
about superintelligence, and people think of intelligence, something monolithic,

211
00:17:41,800 --> 00:17:48,200
and even some people try to assimilate intelligence in humans with IQ values, which is of course

212
00:17:48,200 --> 00:17:52,280
a simplification, and in psychomedic, people will say, no, no, no, no, this is just an indicator

213
00:17:52,280 --> 00:17:57,240
that each useful predicts this and that, but you can't assimilate that to intelligence,

214
00:17:57,240 --> 00:18:02,840
not people in. So the idea you have to infinitely many intelligences is just one,

215
00:18:03,480 --> 00:18:06,920
perhaps the virtue here is to think that intelligence is something with structure,

216
00:18:07,880 --> 00:18:12,840
so you can think of perhaps something like a journal intelligence, but is related to some other

217
00:18:12,840 --> 00:18:18,200
abilities, until you go down, you can see this as in a hierarchical well from the top where you

218
00:18:18,200 --> 00:18:24,440
would have something like a general ability like that or more, and you go to some kind of skills,

219
00:18:24,440 --> 00:18:31,640
until you reach the bottom when you have a very specific ability, sorry, a very specific task,

220
00:18:31,640 --> 00:18:35,640
so you go from very specific, as you aggregate tasks into, you put the, say,

221
00:18:35,640 --> 00:18:40,840
task skills generally, you have these, you can have many levels as you want,

222
00:18:40,840 --> 00:18:48,840
and that gives you a structure, so when we were going back to your question about the

223
00:18:48,840 --> 00:18:52,600
landscape of intelligence, there's one way of looking at this landscape of intelligence,

224
00:18:52,600 --> 00:18:57,960
so we can just look at the, at this landscape at the bottom, and we will have no structure,

225
00:18:57,960 --> 00:19:03,080
so there's no interest in this landscape, because we will have, okay, I'm able to solve this task,

226
00:19:03,080 --> 00:19:07,320
this task, this task, and I'm also able to solve these tasks, and these tasks are important

227
00:19:07,320 --> 00:19:13,000
commercially or whatever, that's interesting, but that's not, this is not going to give us a hint

228
00:19:13,000 --> 00:19:18,440
about what if I give you this new task, can you tell us something about your system solving these tasks?

229
00:19:18,440 --> 00:19:24,680
I mean, a lot of the research that we see today, it's kind of predicated on an assumption that if we,

230
00:19:24,680 --> 00:19:30,760
you know, build towards very task-based intelligence that will help us understand and get towards

231
00:19:30,760 --> 00:19:36,520
a general intelligence, I'm thinking of things like AlphaGo, right? That's, yeah, maybe task is too

232
00:19:36,520 --> 00:19:42,040
limited for what it's doing, but it's solving a very specific problem, but the only reason why

233
00:19:42,040 --> 00:19:46,840
they're doing that is because of this hope that it'll get us towards a higher level, more general

234
00:19:46,840 --> 00:19:53,400
intelligence. Yeah, I think that the program, the program is, they're there having research in

235
00:19:53,400 --> 00:20:03,240
deep mind with AlphaGo, AlphaGo 0, and Alpha0, we had that talk yesterday, Damage gave an excellent

236
00:20:03,240 --> 00:20:08,600
talk yesterday, and people were talking about AlphaStar at the end of the end of the session.

237
00:20:09,480 --> 00:20:15,160
He made a very good point in one of the questions about, okay, what we're interested in, perhaps

238
00:20:15,160 --> 00:20:20,600
we have to put a lot of bias to solve Alpha, or well, there was a discussion about

239
00:20:20,600 --> 00:20:28,440
quality bias or knowledge, or to solve AlphaGo, then we, and human knowledge, we can remove that

240
00:20:28,440 --> 00:20:35,080
and call it AlphaGo 0, still there's some knowledge or something, you have to put the rules of the

241
00:20:35,080 --> 00:20:40,840
game, for instance, but then it's a first stage that you can even make this general enough to cover

242
00:20:40,840 --> 00:20:46,920
some other games, and then you have this Alpha0, which is, so this line of progress that we see

243
00:20:47,640 --> 00:20:52,200
is really interesting because you're going for a very specific task, right? Using a lot of knowledge,

244
00:20:52,200 --> 00:20:59,560
which is very difficult to add up to just a small change of that, and you need a lot of effort

245
00:20:59,560 --> 00:21:05,960
to succeed for that task, to a system where you can have, even if you have to put still some

246
00:21:05,960 --> 00:21:11,000
knowledge, the rules and that, the system is able to do much more independently, much more

247
00:21:11,000 --> 00:21:16,280
autonomous ways, and then you have a third step in this progress where you have a system that

248
00:21:16,280 --> 00:21:21,480
does more general, still the system is only able to solve some kind of ball games if you give the

249
00:21:21,480 --> 00:21:27,640
rules, but that sounds amazing in terms of just 10 years ago, that you would have just one single

250
00:21:27,640 --> 00:21:33,000
system, just give the rules of any ball game, and perhaps we don't know because you need to do all

251
00:21:33,000 --> 00:21:39,240
of the experiments, but perhaps for a wide range of programs, you have a system that is much better

252
00:21:39,240 --> 00:21:45,160
than humans for all of them. This goes in that direction of bottom up, which is a very interesting,

253
00:21:45,160 --> 00:21:50,920
so you're getting more general. How general you can get, how interesting is to have something

254
00:21:50,920 --> 00:21:58,600
like a very general system without, by as I told it, was one of the topics for discussion

255
00:21:58,600 --> 00:22:05,240
yesterday. I think that in some case we have this discussion about the kinds of tasks we are

256
00:22:05,240 --> 00:22:10,600
interested in, and some people say we are interested in those tasks and humans are well at,

257
00:22:10,600 --> 00:22:16,920
a good at, but perhaps there are some other tasks that people are very bad at, and they are really,

258
00:22:16,920 --> 00:22:23,720
really interesting as complementary to what we are doing, so it's not easy to map this space,

259
00:22:23,720 --> 00:22:31,560
but I think that's perhaps a very important challenge, and it's, I wouldn't say that we are

260
00:22:31,560 --> 00:22:36,760
playing with fire, but we are doing a lot of great things with a really understanding where we

261
00:22:36,760 --> 00:22:43,320
are, and perhaps just train to analyze the connection between tasks. What the tasks have in common,

262
00:22:43,320 --> 00:22:49,880
because we say, okay, test and go have something in common, but can we extrapolate what alpha

263
00:22:50,600 --> 00:22:57,960
star is able to do with robotic navigation? It was a completely different system. You can,

264
00:22:57,960 --> 00:23:02,760
you can be used some of the ideas, but of course, this system is not meant to solve this kind of

265
00:23:02,760 --> 00:23:07,720
task, it's not generally not, but the human, even if it is not that good, is not optimal

266
00:23:08,440 --> 00:23:14,920
for any of the stacks, and so we go to this idea of intelligence being second best at everything,

267
00:23:14,920 --> 00:23:21,640
you just have to be the best, and we move away as well from this idea that your child is a genius

268
00:23:21,640 --> 00:23:26,120
at something, but very bad at all the rest, so intelligence is something in the middle, you are

269
00:23:26,120 --> 00:23:31,000
not the best for everything, but also it's not this idea, you are very good at just one thing,

270
00:23:31,000 --> 00:23:36,360
because you wouldn't call that intelligence, that's the narrow approach to AI, so I think this

271
00:23:36,360 --> 00:23:42,680
is basically the interesting part to categorize what it is, what we have in the middle,

272
00:23:42,680 --> 00:23:47,800
this kind of structure of intelligence and relation between tasks, and how can we,

273
00:23:47,800 --> 00:23:56,040
how we can extrapolate from what the system is able to do to other tasks, and it's a question of

274
00:23:56,040 --> 00:24:01,400
applicability in machine learning and AI, sometimes what about, I have this task, can I use your

275
00:24:01,400 --> 00:24:05,960
technique, and you have all these discussions you will need to train, it's not only that you have

276
00:24:05,960 --> 00:24:11,320
to train all the hyper parameters, sometimes you have to change the architecture completely,

277
00:24:11,320 --> 00:24:17,720
and we have all these discussions about general IT in AI, machine learning, and so this landscape

278
00:24:17,720 --> 00:24:25,000
tries to give some kind of conceptualization about this, of course, I don't think there's going

279
00:24:25,000 --> 00:24:30,840
to be one way of looking at this landscape, but if there are different proposals in some areas,

280
00:24:30,840 --> 00:24:35,960
at least there's some, you can say, okay, what about a number of examples that you need,

281
00:24:35,960 --> 00:24:40,840
a training course, you can put some dimensions that you can plot some systems according to

282
00:24:40,840 --> 00:24:46,280
these dimensions, okay, I'm here, and I want to go there, and humans are present at that corner,

283
00:24:47,160 --> 00:24:52,840
and we know where we are, but we need to do more of this, and that was one of the motivations of

284
00:24:52,840 --> 00:24:58,040
the symposium, and also some of the projects that we have in the CFI as well.

285
00:24:59,240 --> 00:25:05,960
And so where does measurement fit in, and how does that led you to a specific set of research

286
00:25:05,960 --> 00:25:12,200
that you work on? Yeah, I think measurement is everywhere in science and technology.

287
00:25:12,200 --> 00:25:20,600
Sure. So for me, it was a kind of a surprise to say, okay, how much we progress in AI,

288
00:25:20,600 --> 00:25:26,040
especially in machine learning, and we don't have very good measurement tools. Of course,

289
00:25:26,040 --> 00:25:29,720
we have tasks, and we can say, okay, accuracy, or some other metrics.

290
00:25:29,720 --> 00:25:33,560
Task performance, for example. Yeah, with different, but where is that lacking?

291
00:25:34,440 --> 00:25:40,120
Yeah, the thing is that the question about measurement here is measurement for which tasks,

292
00:25:40,120 --> 00:25:47,000
and that's the main question. Just for one single task, you can put some kind of a metric of

293
00:25:47,000 --> 00:25:52,600
performance, and this is going to work well. And there's no objection if even you have a utility

294
00:25:52,600 --> 00:25:58,760
function, or even just some cost function money associated with a task. That's perfectly okay,

295
00:25:58,760 --> 00:26:05,880
for instance, self-driving car. I think that's complex to find a good metric, but it's clear.

296
00:26:05,880 --> 00:26:13,000
You can say, okay, these routes are more frequent than others, accidents, spinny things like that.

297
00:26:13,000 --> 00:26:16,840
You can put all of these in a formula and say, yeah, I want to maximize this. Yeah.

298
00:26:16,840 --> 00:26:23,880
But for some other systems we want them to be more general, we don't have a, we want to say,

299
00:26:23,880 --> 00:26:29,080
we want a system that has very good variable abilities. This is too abstract.

300
00:26:30,120 --> 00:26:34,440
We don't really know how to do that. Well, so we can have a good translator.

301
00:26:34,440 --> 00:26:45,240
Even for machine translation, there's a strong debate about the evaluate metrics,

302
00:26:45,240 --> 00:26:51,720
whether what you get is readable, or you get the idea, or what kind of mistakes are worth.

303
00:26:51,720 --> 00:26:56,440
Because we are entering an area where we have to talk about meaning and interpretations,

304
00:26:56,440 --> 00:27:00,520
human interpretations, about whether I think that this is a good translation. Of course,

305
00:27:00,520 --> 00:27:06,680
you can always say, okay, I can just perform a lot of translations and have a human access

306
00:27:06,680 --> 00:27:10,280
these translations from zero to ten, or something like that. And you can have a metric.

307
00:27:10,280 --> 00:27:14,440
But in that case, you are not sure what you are really evaluating.

308
00:27:15,880 --> 00:27:21,240
In contracts where the, for instance, the self-driving car, which is kind of objective in a way.

309
00:27:21,240 --> 00:27:26,600
So we find that even in robotics, there were some discussions, for instance, in the,

310
00:27:26,600 --> 00:27:33,560
there were, there have been several European Union projects on robotics. And one of the big meetings

311
00:27:33,560 --> 00:27:40,120
that was someone who said, okay, we are progressing. And can we really measure that?

312
00:27:41,400 --> 00:27:45,720
You say you saw this task, but we still have this task without being solved. And the system

313
00:27:45,720 --> 00:27:49,720
solved this task, but not the other task. So we have more, because we solve more tasks,

314
00:27:49,720 --> 00:27:56,120
because we have more robots. So a more specialization, but not really because we have one robot

315
00:27:56,120 --> 00:28:03,240
that is able to solve. And in terms of efficiency and economy, what we want in machine learning is

316
00:28:03,240 --> 00:28:11,800
to have systems that are able to solve a range of tasks without a lot of tweaking and tuning

317
00:28:11,800 --> 00:28:18,120
and changing architectures, because that takes a lot of effort from teams. And we want to have

318
00:28:18,120 --> 00:28:24,600
systems that are easier to develop. They require, for instance, less data, all of these things.

319
00:28:24,600 --> 00:28:30,200
So having metrics around all of this is going to give us a better assessment of whether we are

320
00:28:30,200 --> 00:28:37,800
really progressing. And especially at conferences, or when we have one of these breakthroughs in AI,

321
00:28:37,800 --> 00:28:47,480
really understand whether this is a great breakthrough. For instance, when I saw Alpha, Alpha go,

322
00:28:47,480 --> 00:28:53,400
I was impressed like everybody had one. Come on, this is, wow, this is so good. And

323
00:28:53,400 --> 00:28:59,960
but that was, for instance, the other day, I was more impressed about this system just learning

324
00:28:59,960 --> 00:29:06,600
go test, and this Japanese go, this Japanese test as well. And I was really, I think for me,

325
00:29:06,600 --> 00:29:13,080
this is even more important than what we saw two years ago. Right. And well, this is my view.

326
00:29:13,080 --> 00:29:17,720
And at some point, we would like to have some kind of metrics where we can say, okay,

327
00:29:17,720 --> 00:29:21,960
can we say something about what these systems are able to do and put that some kind of

328
00:29:21,960 --> 00:29:30,280
even quantitative assessment. So talking more in terms of skills and abilities rather than

329
00:29:30,280 --> 00:29:39,320
specific tasks. So that's basically the interest in measurement. And now all of these new trends

330
00:29:39,320 --> 00:29:46,120
about evaluation using video games. I think this is really, really interesting. But again,

331
00:29:46,120 --> 00:29:51,880
grab that work. Is that like related to the reinforcement learning type of work? Yeah, we are having many

332
00:29:51,880 --> 00:30:02,760
platforms. For instance, we have Microsoft research, launch, my graph, which is my more platform.

333
00:30:02,760 --> 00:30:09,960
We have good AI release these, well, now it's integrated in the AI universe. Open AI universe.

334
00:30:09,960 --> 00:30:13,720
Open AI universe. Open AI universe. And then you have deep mind where there are also

335
00:30:13,720 --> 00:30:18,520
the evaluation platforms and also some competitions using video games. And we also have these

336
00:30:18,520 --> 00:30:26,760
the Atari games. And so we in a way, we see many evaluation platforms where we have a range of

337
00:30:26,760 --> 00:30:33,400
tasks as different video games. And we see that our systems have been evaluated according to how

338
00:30:33,400 --> 00:30:39,000
well they behave for this range of games. It seems like relative to some of the,

339
00:30:39,000 --> 00:30:45,560
you know, relative to more kind of higher level or conceptual definitions of intelligence,

340
00:30:45,560 --> 00:30:51,960
performance on a video game is more akin to evaluating a self-driving car. What's my score?

341
00:30:51,960 --> 00:30:58,040
How fast did I do it? How many times did I die? Things like that. What makes these video game

342
00:30:58,040 --> 00:31:03,240
platforms interesting for the types of measurement that you want to get to? Yeah, that's a very good

343
00:31:03,240 --> 00:31:10,040
question. But I think that in a way, a self-driving car is, even if it, perhaps, it requires

344
00:31:10,040 --> 00:31:14,440
more technology, more different technologies than, for instance, a video game. A video game is a

345
00:31:14,440 --> 00:31:20,680
mini-world in a way. It's an environment. Of course, the self-driving car is an environment,

346
00:31:20,680 --> 00:31:25,640
but the thing is that for video games, if you just define one video games, I see no difference,

347
00:31:25,640 --> 00:31:29,480
even, of course, that the self-driving car is much more complex, much more into it,

348
00:31:29,480 --> 00:31:34,760
in terms of the technology that you need to solve it. But the good thing of video games,

349
00:31:34,760 --> 00:31:41,400
you usually include 20, 50, 100 video games, and then you have a range of tasks. And then we have

350
00:31:41,400 --> 00:31:46,840
a range of tasks, different tasks, even if they're still, they are 2D or 3D, and there are some kind

351
00:31:46,840 --> 00:31:55,800
of a shared input output for these platforms. There's the kind of a... So you're speaking to

352
00:31:55,800 --> 00:32:04,040
being able to kind of demonstrate generalizability. Yeah, that's the idea. So you tried these games

353
00:32:04,040 --> 00:32:09,000
to be as diverse as possible, in order to show that you're able to learn these tasks.

354
00:32:10,520 --> 00:32:17,720
Then if your system is good at all the tasks, or just kind of, or even if it's not optimal at any of

355
00:32:17,720 --> 00:32:22,200
them, I think you have a kind of a general performance for these range of tasks. And the more general,

356
00:32:22,200 --> 00:32:30,200
you make these pool of tasks, you go up in this skill from bottom task specific performance to

357
00:32:30,200 --> 00:32:35,800
kind of skills, or maybe to towards you go up in this direction of more general intelligence.

358
00:32:35,800 --> 00:32:41,400
And that's what you can claim that even if you take 100 games, Atari Games, that's very specific

359
00:32:41,400 --> 00:32:47,160
subset of all the possible tasks that you might have. And that's true. I think you can explore these

360
00:32:47,160 --> 00:32:52,920
first steps from bottom up from very specific tasks to more general. And you can also analyze all

361
00:32:52,920 --> 00:32:58,200
of these ideas about why is it that this system is good at these tasks, but not at those set?

362
00:32:58,200 --> 00:33:05,000
Because then we'll know these tasks are more about abstract thinking or planning. So we see

363
00:33:05,000 --> 00:33:09,800
that for us how we're reinforcement learning techniques, even if we are using deep learning with

364
00:33:09,800 --> 00:33:15,400
them. They are not very good at generalizing for these, so they are not solving these tasks.

365
00:33:15,400 --> 00:33:21,160
Well, so we can learn much more than we've just focused on one single problem that in the

366
00:33:21,160 --> 00:33:26,920
end, we put a lot of effort, we'll ace at that problem, that perhaps how much of that effort

367
00:33:26,920 --> 00:33:32,200
is extrapolable to other problems, that's basically what we want. We want this generalization

368
00:33:32,200 --> 00:33:39,160
ability in machine learning. Have you published any research that specifically looks at

369
00:33:39,160 --> 00:33:46,920
applying measurement to these video game scenarios? I've taken two different approaches. There's

370
00:33:46,920 --> 00:33:52,440
the idealistic approach, which is basically going from first principles, which is basically

371
00:33:52,440 --> 00:33:59,880
in my book and some related publications. How can we define a set of tasks that, by definition,

372
00:34:00,680 --> 00:34:07,320
are necessary to show that the system has this ability? That will be great, because of course

373
00:34:07,320 --> 00:34:15,240
you can say, okay, why 100 games? Why don't you use 1000? Why are these 100 games sufficient

374
00:34:15,240 --> 00:34:21,720
for this? You can decompose the games to these set of map them to these set of tasks and

375
00:34:21,720 --> 00:34:27,560
determine this game framework's ability to even assess intelligence at all. Is that one of the

376
00:34:27,560 --> 00:34:36,120
things? Yes, there is. We can generate these tasks, rather than we use some of these platforms

377
00:34:36,120 --> 00:34:42,760
generating games. If you generate a game just randomly, you get something that is completely

378
00:34:42,760 --> 00:34:48,360
meaningless. Usually, and it's not fun to play. You have a lot of random noise. You have to

379
00:34:48,360 --> 00:34:53,320
pull some structure into an environment to make it appealing for humans and meaningful,

380
00:34:53,320 --> 00:34:59,080
or even for machine learning. The thing is, how can we generate these tasks in a principle way,

381
00:34:59,080 --> 00:35:05,400
so that we ensure that a key concept here is that you can define the notion of difficulty of this

382
00:35:05,400 --> 00:35:10,440
task, so you can have a scale of difficulty as well. You can relate these tasks in terms of

383
00:35:10,440 --> 00:35:17,240
that difficulty and what they have in common. The problem about this approach is that you can only

384
00:35:17,240 --> 00:35:24,040
generate, at least in principle or easily, you can generate some kind of very abstract environments,

385
00:35:24,040 --> 00:35:32,280
a very simple environment. That's a very good way, but perhaps it might take more efforts and more

386
00:35:32,280 --> 00:35:39,080
ideas just to make it work. In practice, and the other one, which is summarizing some papers,

387
00:35:39,080 --> 00:35:45,400
for instance, last year at the European conference on AI, we had a paper on that,

388
00:35:45,400 --> 00:35:51,480
and it's in some other machine learning conferences. It's how to use, for instance,

389
00:35:51,480 --> 00:35:56,440
one technique from psychology, from psychology, with this item response theory,

390
00:35:56,440 --> 00:36:00,840
where you can characterize from a range of tasks, for instance, you take a competition.

391
00:36:00,840 --> 00:36:05,880
This is the video game playing competition. You take that, you take the results from last year,

392
00:36:07,160 --> 00:36:13,560
and you analyze the, you have, for instance, a 40 task, and you have a 30 competitors,

393
00:36:13,560 --> 00:36:19,560
and you analyze, you just try to understand not only the performance of each of them,

394
00:36:19,560 --> 00:36:24,280
but you try to extract latent variables by analyzing these result matrix,

395
00:36:24,280 --> 00:36:28,040
and you try to analyze, okay, can I add to the difficulty of this task,

396
00:36:28,040 --> 00:36:33,560
or the relations between these tasks? Can I analyze the relation between the participants as well?

397
00:36:33,560 --> 00:36:37,480
And at the end, you can get, for some of these, and you can have something like the difficulty

398
00:36:37,480 --> 00:36:43,000
of the task, and then you can realize these tasks, according to this population of AI systems,

399
00:36:43,000 --> 00:36:47,640
or machine learning systems, this is the most difficult task, but not because a score is 101,

400
00:36:47,640 --> 00:36:55,080
the other is 80, but just how challenging it is for the systems, and you can also determine

401
00:36:55,080 --> 00:37:00,280
the ability of the agents. Of course, this is simplification because you can extract two latent

402
00:37:00,280 --> 00:37:05,880
variables that you could do 10 latent variables to be, but these latent variables explain the

403
00:37:05,880 --> 00:37:10,520
behavior of these systems for this set of tasks, and it gives you information, for instance,

404
00:37:10,520 --> 00:37:16,360
the next competition, which tasks are completely, you get some other preferences, you can extract

405
00:37:16,360 --> 00:37:20,520
a third parameter, which is discrimination, you say, okay, these tasks are completely useless,

406
00:37:20,520 --> 00:37:25,800
because, or sometimes you have some kind of a negative discrimination, this task is performed

407
00:37:25,800 --> 00:37:31,720
well by the, by the bad methods, and the other way around, they say, okay, so what's the point

408
00:37:31,720 --> 00:37:36,840
of having this task here? And that's, that's something inherited from psychomedic, we have a question

409
00:37:36,840 --> 00:37:41,960
that you say, okay, good students are typically bad at that question, you remove the question,

410
00:37:41,960 --> 00:37:46,760
if that doesn't make sense, there's something, there's a catch there or something, because people

411
00:37:46,760 --> 00:37:51,400
get confused, because otherwise you wouldn't get an explanation. So all of the things help us

412
00:37:52,200 --> 00:37:55,800
to understand a little bit better, what is going on, especially when we have competitions,

413
00:37:55,800 --> 00:38:00,520
when we have benchmarks, and we've also applied that to the Atari Games results,

414
00:38:01,320 --> 00:38:07,000
and see what's happening with some of the games and the difficulty of the games, and we can have

415
00:38:07,000 --> 00:38:12,280
a better understanding of the abilities on the problems of current technology. Of course,

416
00:38:12,280 --> 00:38:17,880
this is, this is not for first principle, this is experimental, but at least gives us more

417
00:38:17,880 --> 00:38:23,000
understanding of what's going on that just looking at the results and the winner of a competition.

418
00:38:24,200 --> 00:38:31,080
So it's a set of methods for essentially decomposing these results into kind of generating the tasks

419
00:38:31,080 --> 00:38:37,000
themselves, or inferring the underlying task and their difficulty from the, the results,

420
00:38:37,000 --> 00:38:42,920
is that the right way to think about it? It's a question of trying to describe the tasks,

421
00:38:42,920 --> 00:38:47,800
and also the participants, which is kind of a latent variable that you put into the models and

422
00:38:47,800 --> 00:38:52,520
that you estimate through the results, and you say, okay, and these latent variables, you can call

423
00:38:52,520 --> 00:38:58,600
them difficulty, discrimination, and on the side of the techniques, you can call that ability,

424
00:38:59,240 --> 00:39:04,280
these factors of variables help you understand what's going on. Is there something that you do in

425
00:39:04,280 --> 00:39:10,120
some other, in some other scenarios? It's nothing really new for it's, but it gives you more

426
00:39:10,120 --> 00:39:16,200
understanding just the, just looking at the matrix, or just a scale that you get from the performance.

427
00:39:17,320 --> 00:39:21,640
For many of these games, it doesn't make sense to make it an average of the results.

428
00:39:22,280 --> 00:39:27,720
Some other, in many papers, you have this comparison with humans, which is okay.

429
00:39:27,720 --> 00:39:34,440
For instance, you have 50 Atari games, and you have one technique, one machine learning technique,

430
00:39:34,440 --> 00:39:38,600
and you have the results for each of them, and then you have the humans, the average human.

431
00:39:39,240 --> 00:39:45,400
Basically, we have use, or we are on Amazon Turk, or whatever they have, and they have

432
00:39:45,400 --> 00:39:51,960
this average score, and you say, we are superhuman on task one, we are, we are subhuman on task two,

433
00:39:51,960 --> 00:40:01,640
well, that doesn't say too much to me in a way, because well, we are not able to solve this

434
00:40:01,640 --> 00:40:07,640
task in terms of the human level, but we are not comparing also the number of games that we

435
00:40:07,640 --> 00:40:12,520
are given to humans and the number of games that we agree with. So it's not a fair comparison

436
00:40:12,520 --> 00:40:19,160
in the first place. It doesn't allow us, either to say, we are, there are 50 games, we are

437
00:40:19,160 --> 00:40:26,680
above human on 40 of them, and we are below human on 10 of them. Perhaps as an assistant is 39

438
00:40:26,680 --> 00:40:31,080
11, and it's much better because the difference are much better. So we have a lot of discussion

439
00:40:31,080 --> 00:40:37,640
about how to integrate things that are not commensurate in a way, when we put many, many tasks

440
00:40:37,640 --> 00:40:43,000
together. Okay. And all of these questions appear again and again, especially benchmarks,

441
00:40:43,000 --> 00:40:48,040
competitions about people typically would like to have one kind of a single score or something like

442
00:40:48,040 --> 00:40:55,160
that. And looking at all these specific scores is you don't get a lot of meaning, but you just

443
00:40:55,160 --> 00:41:00,360
simplify over, simplify to a single average score, something like that, and you probably, you miss

444
00:41:00,360 --> 00:41:06,360
a lot of information as well. So something in the middle that you can summarize the behavioral

445
00:41:06,360 --> 00:41:12,040
of these systems for this set of tasks that help us to understand whether we are going in the right

446
00:41:12,040 --> 00:41:17,640
direction, we are still very far from solving the problem that we want to solve. Okay.

447
00:41:17,640 --> 00:41:22,920
And all of these things. Interesting. So how about quickly kind of a quick overview of the

448
00:41:23,560 --> 00:41:27,000
you know, the future directions and priorities? Where did that conversation end up?

449
00:41:28,040 --> 00:41:37,720
Yeah, it was a very open discussion yesterday. When you set up a session with what society needs,

450
00:41:37,720 --> 00:41:48,360
it's like, well, everyone has a say here and we have many questions as well. And we had a panel

451
00:41:48,360 --> 00:41:55,080
for 40, 50 minutes. We was a very long panel. A lot of things we touched upon many, many topics.

452
00:41:55,080 --> 00:42:04,760
But there was this idea of having more human-like AI rather than complementary or not to what

453
00:42:04,760 --> 00:42:13,720
humans are able to do. I think that there's no agreement there. Because computer science and AI,

454
00:42:13,720 --> 00:42:18,600
one of the main goals have been automation of tasks. So of course it's good that the machine's

455
00:42:18,600 --> 00:42:24,520
able to do things that we can't do. That's great. But we are also interested in some tasks that

456
00:42:24,520 --> 00:42:29,320
can be automated and perhaps we can do some other things. But of course you have a lot of

457
00:42:29,320 --> 00:42:35,880
implications about that. And that these idealists view that we want humans and machines to be

458
00:42:35,880 --> 00:42:42,520
complementary ethics. It's very idealistic. And there's a kind of that. But perhaps we can focus

459
00:42:42,520 --> 00:42:47,560
on these things rather than just so human-like intelligence is important. Another way finding

460
00:42:48,200 --> 00:42:54,920
things that machines can do that we can't that is not really human-like. This is also a very

461
00:42:54,920 --> 00:43:01,320
good direction. There was also some more political questions about domination, power,

462
00:43:02,360 --> 00:43:08,120
for instance, corporations and organizations having a lot of power. There was a discussion

463
00:43:08,120 --> 00:43:13,560
not really in terms of AI companies and all that, which is a different way that we didn't touch

464
00:43:13,560 --> 00:43:21,960
them. But about this idea that in the future intelligence has changed our planet.

465
00:43:21,960 --> 00:43:30,680
And having more of that intelligence or different kinds of ways of extending the intelligence

466
00:43:30,680 --> 00:43:35,960
that we know of at the moment is going to transform everything. So we have a debate on all of

467
00:43:35,960 --> 00:43:41,560
these things that might be transformed. And also ethical things about there was this discussion

468
00:43:41,560 --> 00:43:46,760
about buyers in machine learning. So we had a little bit of everything with about 40 minutes

469
00:43:46,760 --> 00:43:52,440
of discussion. Oh, wow. Do you know if the symposium was recorded? Will people be able to find

470
00:43:52,440 --> 00:43:56,200
it afterwards? Yeah, we've been told that it has been recorded. It was recorded. There was a

471
00:43:56,200 --> 00:44:03,640
camera there. So well, probably it will take a while to have it online. But we hope that everything

472
00:44:03,640 --> 00:44:10,120
will be online soon. Okay, great. It sounds like a really wide-ranging and interesting set of

473
00:44:10,120 --> 00:44:15,880
questions that it raised. And I hope to get a chance to take a look at it. Thank you Jose

474
00:44:15,880 --> 00:44:24,360
for spending some time to chat with us. Any final thoughts or ways that folks can catch up with you

475
00:44:24,360 --> 00:44:30,600
or find out more about what you're doing? Well, we are having some workshops next year. But

476
00:44:30,600 --> 00:44:36,040
just if you Google my name or you will find some of the things that we are working on and some of

477
00:44:36,040 --> 00:44:41,160
the projects in the past years and now with the center of the future intelligence. There are many

478
00:44:41,160 --> 00:44:46,200
things, many interesting things going on there. Okay, awesome. Well, thanks so much. That's for

479
00:44:46,200 --> 00:44:54,680
having me here and it's been a pleasure. All right, everyone. That's our show for today.

480
00:44:54,680 --> 00:45:00,120
For more information on Jose or any of the topics covered in this episode, head on over to

481
00:45:00,120 --> 00:45:14,840
twimmaleye.com slash talk slash 137. Of course, thanks so much for listening and catch you next time.

