1
00:00:00,000 --> 00:00:11,760
All right, everyone. Welcome to another episode of the Twimal AI podcast. I am of course your host,

2
00:00:11,760 --> 00:00:17,440
Sam Charrington. And today I'm super excited to be joined by Kate Sianco, associate professor at

3
00:00:17,440 --> 00:00:24,560
Boston University, and a consulting professor for the MIT IBM Watson AI lab. Before we get going,

4
00:00:24,560 --> 00:00:29,280
be sure to take a moment to hit that subscribe button wherever you're listening to today's show.

5
00:00:29,280 --> 00:00:36,400
Kate, welcome to the podcast. Thanks so much for having me. I'm looking forward to digging into

6
00:00:36,400 --> 00:00:45,840
our conversation. You are a very, very busy woman at CVPR. You've got a ton of workshops that you're

7
00:00:45,840 --> 00:00:54,080
speaking at, as well as numerous papers. We will scratch the surface of all that activity

8
00:00:54,080 --> 00:00:58,960
in our conversation today. But before we do that, I'd love to have you introduce yourself to

9
00:00:58,960 --> 00:01:04,000
our guests and share a little bit about your background and how you came to work in the field.

10
00:01:04,000 --> 00:01:11,760
Yeah, definitely. I am a professor of computer science at Boston University, as you mentioned.

11
00:01:11,760 --> 00:01:18,320
I lead a research group focused on deep learning, especially applied to visual recognition.

12
00:01:19,440 --> 00:01:27,600
And I'm specifically interested in many topics, some of which are things like vision and language

13
00:01:27,600 --> 00:01:36,560
models, and also data set bias and adaptation to out of distribution data, also efficient models,

14
00:01:37,280 --> 00:01:44,400
and AI forensics. And I would say that more broadly, one of my goals is to create AI that

15
00:01:44,400 --> 00:01:53,040
can see and can understand language and interact with humans in a natural way. And also help us

16
00:01:53,040 --> 00:01:58,880
solve problems in society like helping people who are visually impaired or helping the environment.

17
00:02:00,400 --> 00:02:10,320
So that's briefly about my research. How I came to the field, I got interested in

18
00:02:11,920 --> 00:02:22,400
AI pretty early on. I was always a fan of science fiction. So when I decided to go to graduate

19
00:02:22,400 --> 00:02:31,440
school, I started as a graduate student at MIT, actually studying speech recognition.

20
00:02:32,240 --> 00:02:43,840
And later on, I switched to computer vision, but I've always been really fascinated about robots

21
00:02:43,840 --> 00:02:51,680
and artificial intelligence and kind of related areas. So I got my PhD from MIT,

22
00:02:51,680 --> 00:02:59,360
and then I did a postdoc for a couple of years, and then I became a faculty member. So I've

23
00:02:59,360 --> 00:03:06,560
basically been working in the AI field for a long time. And it's been really amazing to see,

24
00:03:07,760 --> 00:03:12,320
especially in computer vision, the transformation that our field has gone through.

25
00:03:13,440 --> 00:03:20,160
Because when I started as a graduate student, you know, computer vision, I think I can say that

26
00:03:20,160 --> 00:03:30,400
it didn't work for most things. You know, certain things worked, face detection was working pretty

27
00:03:30,400 --> 00:03:35,600
well, but it's nothing like what we're seeing here. And it's just been so exciting to see this

28
00:03:35,600 --> 00:03:43,360
revolution. It hasn't really, it hasn't really been boring for me. So I'm very kind of, I think I'm

29
00:03:43,360 --> 00:03:48,560
lucky that I get to do this kind of work and this kind of research. Absolutely.

30
00:03:48,560 --> 00:03:58,800
You mentioned your interest in a vision and language together and how those, you know,

31
00:03:58,800 --> 00:04:05,840
you shifted from one to the next and now they're kind of coming together. And one of the things

32
00:04:05,840 --> 00:04:11,840
that you're doing at CVPR is speaking at the multimodal learning and applications workshop.

33
00:04:11,840 --> 00:04:16,160
And I guess the first thing that jumps out at me is, you know, your reference to kind of an

34
00:04:16,160 --> 00:04:22,320
exciting time for computer vision. It's also a super exciting time for this idea of multimodal

35
00:04:22,320 --> 00:04:29,040
machine learning. We've been kind of, you know, doing that for a bit, but the, you know, visual

36
00:04:29,040 --> 00:04:36,480
transformers and kind of the convergence of these two architectures over the past a little bit has

37
00:04:36,480 --> 00:04:44,640
really opened up a big opportunity there. And I'd love to hear you kind of riff on your view of

38
00:04:44,640 --> 00:04:50,640
multimodal as a field and the opportunity before us and kind of the research frontier.

39
00:04:50,640 --> 00:04:56,960
Multimodal learning has been around for a long time. Actually, I mentioned that when I was

40
00:04:56,960 --> 00:05:02,640
starting as a graduate student, I was actually looking into audio visual speech recognition,

41
00:05:02,640 --> 00:05:10,640
which is essentially lip reading. So using both audio and the movement of someone's lips to

42
00:05:10,640 --> 00:05:18,080
understand speech. So that's one example of multimodal learning. And then for my PhD, I actually

43
00:05:18,080 --> 00:05:26,880
worked on trying to learn about visual categories of different objects using text that we can

44
00:05:26,880 --> 00:05:37,520
scrape from the web. And this was before deep learning really took off, but kind of the ideas,

45
00:05:37,520 --> 00:05:43,120
ideas have been around for a long time, right? Can we learn something from both audio and video? Can

46
00:05:43,120 --> 00:05:51,120
we get free data, essentially free labeled data from the web? So people have been trying this

47
00:05:51,120 --> 00:05:59,920
for a while. And I think one of the really breakthroughs that we're seeing now is the qualitative

48
00:05:59,920 --> 00:06:07,920
scale in the amount of data that we can get from the web and train on, right? There was a very

49
00:06:07,920 --> 00:06:17,280
recent paper that I just saw talking about emergent properties of large models. And I do agree

50
00:06:17,280 --> 00:06:24,960
that when we reach a certain scale, you know, both in terms of training data and size. So,

51
00:06:24,960 --> 00:06:30,320
of course, those are correlated. You typically need a lot more data to train a very large model

52
00:06:30,320 --> 00:06:36,320
with lots and lots of parameters. I think that you start seeing properties that are emerging that

53
00:06:37,200 --> 00:06:41,840
you're not seeing with smaller models and smaller data sets. So I think that's what we're seeing now

54
00:06:41,840 --> 00:06:51,760
with, you know, things like Dali too. And kind of very cool results that combined vision and

55
00:06:51,760 --> 00:06:58,880
language. And so one one thing that I'm talking about at the workshop is how we can use this

56
00:07:00,320 --> 00:07:07,200
training data that is essentially free, right? So probably you've heard of Clip, which is an

57
00:07:07,200 --> 00:07:14,160
OpenAI model that really I would say transformed our field over the last little while because

58
00:07:14,160 --> 00:07:22,480
they were able to collect lots of training data by scraping photos and their captions from the web.

59
00:07:22,480 --> 00:07:31,040
So if we have access to this kind of data and we can learn not just from images with tags that have

60
00:07:31,040 --> 00:07:37,680
to be sort of manually labeled like ImageNet data. Well, we have an image and we have a category

61
00:07:37,680 --> 00:07:44,720
label for each image, right? So I mean, this data is good and we have been training on this data

62
00:07:44,720 --> 00:07:49,840
for all kinds of computer vision applications. Usually you pre-train on this data and then you

63
00:07:49,840 --> 00:07:55,040
fine tune on your own data set. Turns out that if you can pre-train on the captioned images from

64
00:07:55,040 --> 00:08:02,320
the web, you actually can do much better, especially on zero-shot learning where you have a new

65
00:08:02,320 --> 00:08:08,160
category that you haven't seen, you don't have any training data for this category. It turns out

66
00:08:08,160 --> 00:08:13,840
that these models, these vision language pre-train models actually generalize better than just

67
00:08:14,400 --> 00:08:21,840
traditional image classification data sets for pre-training. So you were talking about the

68
00:08:21,840 --> 00:08:27,520
opportunities presented by all of the information that's out on the internet that is visual

69
00:08:27,520 --> 00:08:33,040
information with associated text. You know, one of the obvious questions that that begs or,

70
00:08:33,040 --> 00:08:38,880
yeah, I guess it's obvious in light of the recent history of large language models is the

71
00:08:38,880 --> 00:08:48,640
inherent bias in the internet information. And I recently saw a tweet that was, I think someone

72
00:08:48,640 --> 00:08:54,480
with access to Dolly II or something like Dolly II typed in, you know, engineers and it was

73
00:08:54,480 --> 00:09:00,400
kind of these generated images of all male engineers standing around or, you know, doctors

74
00:09:00,400 --> 00:09:06,640
same kind of thing. Now, curious your thoughts on that. And, you know, really it's a question of

75
00:09:06,640 --> 00:09:13,840
like having access to, you know, that kind of volume of information is, you know, clearly beneficial

76
00:09:13,840 --> 00:09:18,800
in some ways, but it also has, you know, we also need to be conscious of those biases. How are you

77
00:09:18,800 --> 00:09:29,600
thinking about that? Yeah, I think that the bias in data sets is in some sense inevitable.

78
00:09:30,320 --> 00:09:35,680
And this is something that I've been working on also for a long time. Kind of how do we deal

79
00:09:35,680 --> 00:09:43,680
with the fact that our data sets are biased simply because they're finite, right? It's very hard

80
00:09:43,680 --> 00:09:51,920
to sample the entire visual world and devoid bias. And especially since, as I mentioned,

81
00:09:52,640 --> 00:09:59,360
we have been really trying to go to the internet for our data source in the last, you know,

82
00:09:59,360 --> 00:10:07,920
say decade or so, or a couple of decades. Because it's free, it's easy. Yeah, it's there. I mean,

83
00:10:07,920 --> 00:10:13,200
it's a lot harder to, you know, we used to actually have graduate students go to the lab and take

84
00:10:13,200 --> 00:10:17,280
pictures of objects. And that's, that was the training data or take pictures of people.

85
00:10:18,240 --> 00:10:24,080
I remember having to collect a data set in the lab and get a signed release form from every

86
00:10:24,080 --> 00:10:29,040
person that walked in that we recruited, you know, making sure that they're okay with us using

87
00:10:29,040 --> 00:10:35,520
their data. But we're in a different world now, you know, you go to the internet and you have

88
00:10:35,520 --> 00:10:43,840
billions of images and videos at your fingertips. And I think we're almost, it feels to me like

89
00:10:43,840 --> 00:10:52,480
we're in a gold rush kind of era with respect to, you know, how data is really king right now.

90
00:10:52,480 --> 00:10:58,240
And whoever can get their hands on the most data in some sense is winning the AI race, right?

91
00:10:58,240 --> 00:11:08,080
So I feel like these concerns are very much important. And we should definitely be worrying about them.

92
00:11:08,080 --> 00:11:17,200
That's not something that I particularly work on in, in my research. But there's great people

93
00:11:17,200 --> 00:11:24,400
out there who work on these kinds of more ethical questions. Although, you know, I had a paper a

94
00:11:24,400 --> 00:11:32,720
while back. The first time that I noticed really this kind of bias in image caption data was

95
00:11:34,080 --> 00:11:41,200
some of the very first times that we were able to get deep learning to generate captions for

96
00:11:41,200 --> 00:11:47,600
images, right? And we were working with a fairly small by today's standards data set.

97
00:11:47,600 --> 00:11:55,280
And, you know, at the time was a large scale data set. And, you know, when the model started

98
00:11:55,280 --> 00:12:01,760
working, it was amazing, you know, literally the year before that, the best captions that you

99
00:12:01,760 --> 00:12:10,240
could generate sounded very awkward, very robotic, you know, they would say things like there is one

100
00:12:10,240 --> 00:12:20,160
building and grass, you know, they were templated. And once we got the image captioning to work

101
00:12:20,160 --> 00:12:27,440
with language models using neural networks, you know, because one of the major advancements,

102
00:12:27,440 --> 00:12:33,840
of course, in image captioning is being able to generate fluent natural sounding language.

103
00:12:33,840 --> 00:12:44,080
And once we develop these models that no longer had two separate stages where in the first stage,

104
00:12:44,080 --> 00:12:48,880
you detect all the objects and the second stage of your template and you plug in the objects

105
00:12:48,880 --> 00:12:53,040
you detected in the sentence template, right? So that's how you get the robotic sounding captions.

106
00:12:54,400 --> 00:13:01,440
But so now, instead of that, we had an end-to-end model that you just feed it images and captions,

107
00:13:01,440 --> 00:13:08,080
and it just learns a single neural network that takes the raw image and spits out the entire

108
00:13:08,080 --> 00:13:14,000
sentence, the entire caption, right? And, you know, it was amazing to see, I remember this,

109
00:13:14,720 --> 00:13:19,920
you know, we had a paper on this and a few other labs had a paper on this, it made the New York Times,

110
00:13:19,920 --> 00:13:25,600
it was really amazing, you know, how fluent these captions sounded like. But then we started

111
00:13:25,600 --> 00:13:33,680
looking at this more and digging into results more and we didn't notice that these models definitely

112
00:13:33,680 --> 00:13:40,400
learn shortcuts. They learn to exploit the biases in the data. And so one of the examples

113
00:13:40,400 --> 00:13:49,600
that we wrote about in our paper is when you have a data set where most of the, for example,

114
00:13:49,600 --> 00:14:00,480
images of snowboarders have captions that say, you know, a man is snowboarding. Then the model

115
00:14:00,480 --> 00:14:05,840
sort of learns the shortcut that if you see something that looks like a snowboarder or perhaps

116
00:14:05,840 --> 00:14:15,440
even snow, you should start the sentence by a man is, you know, and then complete it according to

117
00:14:15,440 --> 00:14:20,800
maybe what else you detect in the image. But it's a pretty good shortcut because it will

118
00:14:21,520 --> 00:14:25,920
minimize your loss, your training loss, because you'll get it right, almost to the training

119
00:14:25,920 --> 00:14:32,560
examples. And that's all really that these models care about. So we try to address this kind of bias

120
00:14:32,560 --> 00:14:40,640
in the paper. But, you know, I think we still don't have a very good solution and we we definitely

121
00:14:40,640 --> 00:14:48,880
need more work to improve this kind of, you know, to improve our models so that they don't have

122
00:14:48,880 --> 00:14:55,440
these egregious biases. And it's only getting worse, right? Of course, now we have giant data sets

123
00:14:56,720 --> 00:15:04,400
hundreds of millions or even a billion images much, much harder to both audit them and to ensure

124
00:15:04,400 --> 00:15:12,080
that they don't take shortcuts like this. And the models are a lot more complex and difficult to

125
00:15:12,960 --> 00:15:21,120
intuit about and understand. That's right. They're very block box, right? Yeah. So your talk at

126
00:15:21,920 --> 00:15:26,880
this multimodal learning and applications workshop is more language less labeling,

127
00:15:26,880 --> 00:15:34,800
vision and language pre-training for visual tasks. It immediately calls to mind something

128
00:15:34,800 --> 00:15:40,400
that I've been spending a lot of time exploring recently, this whole idea of data-centric AI.

129
00:15:41,600 --> 00:15:46,800
And in particular, like you start out talking about the cost of labeling and the burden that that,

130
00:15:46,800 --> 00:15:52,480
you know, creates for folks that want to build applications in this base. How is that played out

131
00:15:52,480 --> 00:16:00,320
for you? This is something that I've been interested in the last few years. How do we train models,

132
00:16:00,320 --> 00:16:08,000
especially for new tasks or new domains without having to collect and label a lot of data? And I can

133
00:16:08,000 --> 00:16:15,440
talk about sort of applications where this comes up, but I think it's a lot of applications,

134
00:16:15,440 --> 00:16:22,960
actually. And so this idea of, okay, we're going to get a fixed data set and train on it and then

135
00:16:22,960 --> 00:16:28,720
that's it. That's all we need. That's really not how things work in real life. In real life, you want to

136
00:16:30,000 --> 00:16:36,000
build an application, you have certain things that you want your model to recognize and images.

137
00:16:36,000 --> 00:16:40,240
And there are often things that you don't have a lot of training data for. You have to

138
00:16:40,240 --> 00:16:50,480
pay someone to label it. It costs a lot of money. Depending on what the labels look like, you

139
00:16:50,480 --> 00:16:57,040
might actually need experts. You might need to find someone who is actually good at labeling and

140
00:16:58,720 --> 00:17:06,400
and the scale and it just doesn't scale very well. So I think with one of the really exciting things

141
00:17:06,400 --> 00:17:14,240
that people have been doing recently is figuring out, first of all, that these very large-scale

142
00:17:14,240 --> 00:17:20,640
models that are trained on this web-scale data, like Clip, for example, you can prompt them

143
00:17:21,280 --> 00:17:28,480
to get them to learn very quickly. Sometimes there's not even any learning involved. You just

144
00:17:30,480 --> 00:17:36,160
give them some textual input telling them what you want recognized. And then you don't need any

145
00:17:36,160 --> 00:17:44,320
additional training data to get a very good improvement already on your task. Of course,

146
00:17:45,360 --> 00:17:51,600
you can improve further if you had training data. But if you don't have any training data or

147
00:17:51,600 --> 00:17:57,600
only have a few labels, these kinds of prompting methods seem to be working quite well.

148
00:17:59,360 --> 00:18:04,400
So that's one of the things that I was going to mention in the workshop is some very recent work

149
00:18:04,400 --> 00:18:17,040
where we try to essentially prompt a model like Clip, pre-trained on a large web-scale data set

150
00:18:17,040 --> 00:18:26,240
of images and captions, and then prompt it a test time to classify categories in a zero-shot way

151
00:18:26,240 --> 00:18:32,880
so that where you don't have any labels for those categories, you just want the model to

152
00:18:32,880 --> 00:18:41,600
kind of generalize to any label that you can throw at it essentially. And it does do much better

153
00:18:41,600 --> 00:18:47,360
than previous work on this task. Can you talk about the methodology that you

154
00:18:48,480 --> 00:18:55,040
pursued in that work? Was a lot of the challenge figuring out the right way to prompt the model?

155
00:18:55,040 --> 00:19:04,320
So we actually have two lines of work in this direction. One is exactly this challenge of how

156
00:19:04,320 --> 00:19:15,440
to prompt the model correctly. For example, again, we start with a pre-trained model that already

157
00:19:15,440 --> 00:19:20,880
exists, very hard to retrain these models when you're experimenting with them. So oftentimes,

158
00:19:20,880 --> 00:19:28,880
you just download one that OpenAI has been available. So we start with a pre-trained model,

159
00:19:29,600 --> 00:19:36,880
especially Clip is the one that we often work with. And then we want to

160
00:19:39,280 --> 00:19:47,520
prompt it with essentially some textual tokens. It's already learned to take text tokens

161
00:19:47,520 --> 00:19:54,560
as input and the image as input, compare them, and then predict a score that basically says yes,

162
00:19:54,560 --> 00:20:00,080
these, this textual string and this image are highly related or no, they're not related.

163
00:20:01,600 --> 00:20:08,960
But during pre-training, of course, these models are learning to compare captions,

164
00:20:08,960 --> 00:20:14,960
right? So a caption is a more general kind of label for an image. It could say, you know,

165
00:20:14,960 --> 00:20:22,240
there's a group of people playing Frisbee in the park. But what we're trying to do is now take

166
00:20:22,240 --> 00:20:27,840
that model and just we just want to know, is there a Frisbee in this image? So we want to classify

167
00:20:27,840 --> 00:20:40,000
the label Frisbee or we want to classify the label car or person. So it presents a problem because

168
00:20:40,000 --> 00:20:44,960
the model wasn't trained for that specific task and we need to tweak it. We need to do something

169
00:20:45,840 --> 00:20:56,160
to get it to do this task. And so one of the contributions in this work is to figure out a way

170
00:20:56,160 --> 00:21:05,200
to prompt a model with both a positive and a negative prompt. So you think of the prompt as saying

171
00:21:05,200 --> 00:21:12,800
is there a blank in this image, for example, where the blank is the word for the class you're trying

172
00:21:12,800 --> 00:21:18,880
to detect like Frisbee. And what that does is it makes the task a lot more similar to what the

173
00:21:18,880 --> 00:21:26,000
models learned about because it seems to the model that you're giving it a caption essentially.

174
00:21:26,000 --> 00:21:31,760
But it's a fake caption, of course. You just created a fake caption and just inserted the word

175
00:21:31,760 --> 00:21:39,600
for the class that you want to recognize. And so what we did is in addition to this positive

176
00:21:39,600 --> 00:21:46,320
prompt where we say is there a blank in the image and we replace the blank with the with the label.

177
00:21:46,320 --> 00:21:53,760
So is there Frisbee? We also give it a negative prompt which is saying essentially think of it as

178
00:21:53,760 --> 00:22:03,680
the negative question. Is there no Frisbee in the image? And then we compare the score that the

179
00:22:03,680 --> 00:22:09,520
model gives for the positive and the negative. And if the positive score is higher, then we predict

180
00:22:09,520 --> 00:22:16,880
that yes, there is a Frisbee in the image. In a sense, it's a bit analogous to the kind of games

181
00:22:16,880 --> 00:22:22,880
that you might play with a human label or with human inhuman labeling where you ask multiple

182
00:22:22,880 --> 00:22:29,440
labelers to label an image and compare their responses. This is just kind of a clever way to do that.

183
00:22:29,440 --> 00:22:37,840
Not to minimize it, but yeah, yeah, exactly. And then we had to do some other tricks to extract

184
00:22:37,840 --> 00:22:49,520
the spatial information in a more fine grain matter. I don't want to. Yeah, it's actually not

185
00:22:49,520 --> 00:23:00,320
nothing, nothing fancy. But what the image and caption model does is it takes the whole caption

186
00:23:00,320 --> 00:23:07,680
and compares it to the whole image. And it makes sense, right? Because when you're training the

187
00:23:07,680 --> 00:23:11,680
model, you don't know which part of the caption corresponds to which part of the image, right? So

188
00:23:11,680 --> 00:23:16,960
a group of people playing Frisbee in the park, you don't know where the Frisbee is located in the

189
00:23:16,960 --> 00:23:24,640
image, right? But for our task, we are only looking for the Frisbee, not the entire scene.

190
00:23:25,280 --> 00:23:32,080
So we actually wanted to focus in a more fine grain way on the region where the object might be.

191
00:23:32,640 --> 00:23:41,760
So what we do is we do some surgery on this network to to instead compare our prompt at each location

192
00:23:41,760 --> 00:23:50,480
in the image. Okay. And then decide if the object is there instead of first kind of aggregating

193
00:23:50,480 --> 00:23:54,240
over the whole image and then comparing. So that turns out that works a little bit better.

194
00:23:55,440 --> 00:24:02,800
Kind of akin to a convolutional window, we're just taking slices and passing them into the model.

195
00:24:02,800 --> 00:24:13,120
Yeah, actually, the model already has features that are that it's computing at each sub window in

196
00:24:13,120 --> 00:24:18,880
the image. It's just that then that later on, it is pooling them and aggregating them into a

197
00:24:18,880 --> 00:24:24,960
single vector. And we just kind of go in and do some surgery and compare before it does. So actually,

198
00:24:24,960 --> 00:24:30,400
we don't learn, I think the cool thing about this work is that we don't learn any new parameters

199
00:24:30,400 --> 00:24:36,960
except for these prompt tokens. So all the surgery we do, we don't introduce any learnable parameters,

200
00:24:36,960 --> 00:24:44,080
which means there's very little overhead and learning. So we need very little data to actually

201
00:24:44,080 --> 00:24:49,680
tune the model to do this task. I think in our paper, we report something like

202
00:24:51,920 --> 00:24:57,280
you know, something like maybe 20,000 parameters, the extra that we're learning.

203
00:24:57,280 --> 00:25:05,360
And it's it's just these token embeddings that are fed in as the prompt. And the these token

204
00:25:05,360 --> 00:25:14,240
embeddings, is there a fixed set of labels that you like you're starting with a fixed set of labels,

205
00:25:14,240 --> 00:25:18,560
then you create these prompts and you know, that's where the token embeddings come from.

206
00:25:18,560 --> 00:25:26,160
Yeah, so the tokens are the prompt. So you can think of them as some words, but actually,

207
00:25:26,160 --> 00:25:32,640
we're just learning some arbitrary words. And each word is embedded into a continuous vector.

208
00:25:32,640 --> 00:25:37,840
So that's what I mean by token embeddings. So you can think of them as some words like is there a

209
00:25:38,720 --> 00:25:44,720
something in the image? We're not actually specifying what words the model should use. We're just

210
00:25:44,720 --> 00:25:53,840
letting it learn some words. Yeah, I think I wasn't following that part. I thought I heard you

211
00:25:53,840 --> 00:26:00,160
describing something more like you identified some relatively standard patterns or something

212
00:26:00,160 --> 00:26:05,360
like that, like a template for these prompts. But rather these prompts are learned and they're

213
00:26:05,920 --> 00:26:13,120
they're learned for each of the classes that you're trying to be able to predict like it's you

214
00:26:13,120 --> 00:26:19,280
know, if it's tree a tree, the prompt is specific to tree. Like it's the prompt that produces the

215
00:26:19,280 --> 00:26:25,040
best result for tree as opposed to what might work for car. Is that am I thinking about that

216
00:26:25,040 --> 00:26:34,720
correctly? Yes, you are correct. Although we also did it the other way where the prompt is generic

217
00:26:34,720 --> 00:26:40,640
and you just plug in the word that you want to recognize. So this is really zero shot learning

218
00:26:40,640 --> 00:26:48,720
because you're not even tuning the prompts for any specific class you you you're tuning the

219
00:26:48,720 --> 00:26:54,320
prompts on one set of classes and then the user gives you some new class. You just plug it into

220
00:26:54,320 --> 00:27:01,440
the prompts that you've learned. It turns out that also works and our approach also improved

221
00:27:01,440 --> 00:27:06,240
performance on this kind of zero shot recognition even though the prompts are learned in a generic

222
00:27:06,240 --> 00:27:15,920
way and not specific to these categories. The evaluation on the topic of evaluation. Are you

223
00:27:16,560 --> 00:27:27,760
what are you comparing against? We are comparing against existing models that try to do this kind

224
00:27:27,760 --> 00:27:39,280
of label classification and images. Some of which are probably not working as well because they're

225
00:27:39,280 --> 00:27:45,440
not utilizing these very large vision language models that we're utilizing like Clip.

226
00:27:47,200 --> 00:27:54,720
And this is something that you know often comes up now. Is it fair to compare a model that isn't

227
00:27:54,720 --> 00:28:03,600
using something that was pre-trained on 400 million images with captions? And you know I

228
00:28:05,840 --> 00:28:13,360
I really go back and forth. I think in some sense yes it's not fair and in fact we don't even know

229
00:28:13,360 --> 00:28:19,360
these huge datasets. We don't know what's in them. OpenAI hasn't released the dataset that they

230
00:28:19,360 --> 00:28:24,080
trained on. It could in fact include some of our test data because it's straight from the web.

231
00:28:25,760 --> 00:28:30,320
So it doesn't seem like the best experimental protocol to be using. But on the other hand

232
00:28:31,680 --> 00:28:37,040
they work so well you know it just you can't deny that you get much better performance

233
00:28:37,360 --> 00:28:40,320
and people are using them. So the cat's out of the bag so to speak.

234
00:28:41,520 --> 00:28:46,560
On the other hand models do tend to work well if they've seen the training day. Exactly.

235
00:28:46,560 --> 00:28:54,240
And we don't know if the model has seen the training day or not. I mean we also do compare

236
00:28:54,240 --> 00:29:03,200
a model to kind of an original clip that hasn't been tuned in the way that I described and we do

237
00:29:03,200 --> 00:29:09,280
perform better than that. So there is an advantage to what we're doing when we compare the more

238
00:29:09,280 --> 00:29:17,440
apples to apples way. But I think what's happening now is I think that researchers really are going

239
00:29:17,440 --> 00:29:27,200
to have to keep up with the latest and greatest pre-trained backbones and you know you just the

240
00:29:28,560 --> 00:29:37,920
the method you develop can improve performance on your dataset with respect to a backbone like

241
00:29:37,920 --> 00:29:43,040
ResNet for example. But someone could come along with a backbone like visual transformer

242
00:29:43,040 --> 00:29:46,640
and upperform your whole algorithm just because they have a much better backbone.

243
00:29:49,680 --> 00:29:53,760
It's like I said you know the gold rush or the space race or whatever you want to call it you know

244
00:29:53,760 --> 00:29:59,760
the bigger the bigger the better. And yeah it's a little bit hard I think for researchers to

245
00:29:59,760 --> 00:30:05,600
keep up with this. Yeah I was just going to ask how does that how does that land for you as a

246
00:30:05,600 --> 00:30:13,200
researcher like there's been a lot of talk around you know just the amount of resources that are

247
00:30:13,200 --> 00:30:20,080
going into these you know quote unquote foundation foundational models you know changes what avenues

248
00:30:20,080 --> 00:30:25,440
are available to what researchers just based on the resources of their organizations and

249
00:30:26,560 --> 00:30:31,120
changes the research directions that are accessible to them. What's your take on that?

250
00:30:31,120 --> 00:30:41,200
Yes I think that's a very real issue for many people especially in academic research labs

251
00:30:41,200 --> 00:30:48,480
but also in many industrial research labs there are really only a few labs or companies

252
00:30:49,360 --> 00:30:57,840
that can afford to even train these very large scale models anymore right. And it's great

253
00:30:57,840 --> 00:31:04,880
when they make them publicly available because then the rest of us can kind of experiment with them

254
00:31:04,880 --> 00:31:10,240
we can use them as the feature backbone and or new new algorithms that we're developing. I think

255
00:31:10,240 --> 00:31:20,000
that there are some things that probably are really capabilities that as I mentioned earlier

256
00:31:20,000 --> 00:31:26,880
could be emerging only when you have a very large training data set or a very large model that

257
00:31:26,880 --> 00:31:34,320
you're training. And so that means that certain kinds of research is now in the realm of only

258
00:31:34,320 --> 00:31:41,840
those lucky few that that have access to both the data and the compute. So the other paper is called

259
00:31:41,840 --> 00:31:49,760
prefix conditioning unifies language and label supervision and it's a collaboration with google

260
00:31:49,760 --> 00:32:01,360
and so here we're also looking at prefix conditioning or prompting rather but we have a slightly

261
00:32:01,360 --> 00:32:08,560
different goal where as I mentioned the large vision language models are pre-trained on images

262
00:32:08,560 --> 00:32:16,080
with captions right and that that gives us a lot of data that we get for free essentially from the

263
00:32:16,080 --> 00:32:23,280
web but we also do have some label data sets the traditional data sets like image net which

264
00:32:23,280 --> 00:32:32,480
have an image with a category label. And these two kinds of data are complementary right the

265
00:32:34,320 --> 00:32:40,240
captioned images have a long tail distribution over object so they may not

266
00:32:40,240 --> 00:32:48,080
cover all the categories that you want to cover or they may cover them unequally or like we mentioned

267
00:32:48,080 --> 00:32:54,800
before they might have some bias like all the snowboarders or mail in the caption at least.

268
00:32:54,800 --> 00:33:03,200
And so we might want to combine that data with just traditional labeled image data that is human

269
00:33:03,200 --> 00:33:09,920
annotated and clean or less bias free and I'm not saying image net is completely unbiased either

270
00:33:09,920 --> 00:33:16,960
so if we wanted to do that for pre-training it turns out that's what we look at in this paper

271
00:33:17,840 --> 00:33:24,320
it turns out that you can do better than just combining the two kinds of data you can

272
00:33:24,320 --> 00:33:35,280
so there's work from Microsoft that actually try to transform the labels in image net into

273
00:33:35,280 --> 00:33:41,280
fake captions right so if you have an image an image net labeled with Persian cat you can

274
00:33:42,320 --> 00:33:48,480
make a caption out of it by just plugging it into some template like a close-up of a Persian cat

275
00:33:49,760 --> 00:33:57,200
so so they did this and then when you train on both the fake captions and the real captions

276
00:33:58,080 --> 00:34:05,040
you do get a stronger model you get a more powerful model that combines the knowledge from

277
00:34:05,040 --> 00:34:10,240
these two kinds of data it's kind of like a data augmentation type of approach yes exactly

278
00:34:11,120 --> 00:34:17,600
and it generalizes even better to zero shot tasks so new data sets with novel categories

279
00:34:18,560 --> 00:34:28,000
it generalizes even better to them so so one issue though is that now you have two kinds of

280
00:34:28,000 --> 00:34:36,320
input data sets one with real captions and one with fake captions and these you know deep models they're

281
00:34:37,760 --> 00:34:42,320
they're powerful which means they take shortcuts as we've been saying and so what

282
00:34:43,280 --> 00:34:50,000
what we found is happening is the model actually knows more or less that it's learning from these

283
00:34:50,000 --> 00:34:56,080
are fake captions and these are real captions and it sort of tries to learn from both kinds of captions

284
00:34:56,080 --> 00:35:02,480
but then it ends up you know not really knowing at test time when you give it a new image

285
00:35:02,480 --> 00:35:09,840
what to do because it basically one way to think of it is it's a little confusing for the model to

286
00:35:09,840 --> 00:35:16,240
have you know real captions and fake captions and so what we ended up doing is just something super

287
00:35:16,240 --> 00:35:26,240
simple which is add a token a training time to the real captions that just the token is just saying

288
00:35:26,240 --> 00:35:32,560
this is a real caption and the fake captions we add a token that says this is a fake caption

289
00:35:33,840 --> 00:35:42,080
and when you do that the model actually learns much better it generalizes better it

290
00:35:42,080 --> 00:35:50,720
um essentially helps the model overcome this these two kinds of domains by telling it look

291
00:35:50,720 --> 00:35:55,040
these are two kinds of domains so you can process them a little bit differently right so they're

292
00:35:55,040 --> 00:36:00,400
there's some specialized processing in the language model that we see emerge from that where

293
00:36:00,400 --> 00:36:06,560
if you give it the the caption and then you prefix it with the real caption token it will do one

294
00:36:06,560 --> 00:36:11,520
thing but if you prefix that same caption with the fake caption token it will do another thing and

295
00:36:11,520 --> 00:36:20,320
so for example you know if you prefix with the fake caption token the model will mostly focus on

296
00:36:20,960 --> 00:36:28,240
kind of the noun because it's learned that these fake captions they're just fake so the only

297
00:36:28,240 --> 00:36:34,320
information there is the noun right so and everything else is just kind of a template to

298
00:36:34,320 --> 00:36:41,920
fool it into yeah it's for exactly but if you prefix it with the caption real caption token then

299
00:36:41,920 --> 00:36:48,640
it will start looking at the whole caption because it knows that in real captions there is useful

300
00:36:48,640 --> 00:36:55,520
semantic information in multiple words across the whole caption so so it's kind of cool

301
00:36:55,520 --> 00:37:01,120
it is an example come to mind of you know the illustrates kind of the richness of a real of a

302
00:37:01,120 --> 00:37:10,960
real caption relative to one of these fake captions yeah so this is an example from the paper it's

303
00:37:10,960 --> 00:37:20,000
not a particularly rich caption but if you have a caption that says a sculpture of an airplane

304
00:37:21,760 --> 00:37:29,120
and you prefix it with the you know this is a fake caption then the model ignores the word

305
00:37:29,120 --> 00:37:36,960
sculpture and just focuses on the word airplane right and then if you prefix it with the real

306
00:37:36,960 --> 00:37:44,000
caption token then the model will actually look at the sculpture as well as the airplane

307
00:37:45,440 --> 00:37:53,760
in some sense this seems counterintuitive in the sense that you know that I guess the the

308
00:37:53,760 --> 00:37:59,520
usual thought is hey more data different types of data that will kind of force the model to

309
00:37:59,520 --> 00:38:06,640
generalize and that's going to lead to better performance and you're kind of saying that you know

310
00:38:06,640 --> 00:38:12,880
more data different types of data the model is not generalizing it's just learning that the

311
00:38:12,880 --> 00:38:21,760
classes and cheating right I think it is a little counterintuitive but I think we've seen this

312
00:38:21,760 --> 00:38:31,200
in other work too that when you're training with different kinds of data heterogeneous data

313
00:38:32,080 --> 00:38:39,280
sometimes it's better to specialize the model you know make it aware that it's being trained with

314
00:38:39,280 --> 00:38:47,680
heterogeneous data and because you know I think if you throw everything together you're giving it

315
00:38:47,680 --> 00:38:52,800
also less guidance right adding this prefix token is sort of just giving the model more

316
00:38:52,800 --> 00:38:59,120
information look you know these two sets of images are coming from two different kinds of domains

317
00:38:59,760 --> 00:39:04,480
and then the model can use that or not use that and you know in this case it seems to use it to

318
00:39:04,480 --> 00:39:11,280
to its advantage kind of on the topic of domain generalization but maybe switching modalities a

319
00:39:11,280 --> 00:39:18,320
little bit one of the other papers that you're speaking about at CBPR is focused on domain

320
00:39:18,320 --> 00:39:25,920
generalization an unsupervised manner and this idea of kind of bridging across domains

321
00:39:25,920 --> 00:39:31,200
and visual domains in particular can you talk a little bit about that paper and the motivation

322
00:39:31,200 --> 00:39:37,600
there yeah definitely so this paper it's called unsupervised domain generalization by learning a

323
00:39:37,600 --> 00:39:47,920
bridge across domains and we are looking at this problem of generalization to new kinds of visual

324
00:39:47,920 --> 00:39:59,360
domains so an example is let's say your model was trained on driving data collected from a car

325
00:39:59,360 --> 00:40:08,480
in California and then at test time you're giving it data from let's say Boston or New York in the

326
00:40:08,480 --> 00:40:14,160
winter people look different because they're wearing heavy coats or hats or maybe it's raining or

327
00:40:14,160 --> 00:40:23,040
maybe the trees are look different right so the visual domain has changed even though the

328
00:40:23,040 --> 00:40:28,480
categories you're looking for are the same it's still people and cars and another example of this

329
00:40:28,480 --> 00:40:38,480
is when you train the model on real photos but then you get a clip art or a painting or some other

330
00:40:38,480 --> 00:40:45,040
type of drawing perhaps so this is a more extreme even domain shift in your input data

331
00:40:47,120 --> 00:40:55,760
and you know there's been a lot of interest in robustness where we want models to be robust to

332
00:40:55,760 --> 00:41:01,920
to kind of changes in the image so if you change the top left pixel you know your model shouldn't

333
00:41:01,920 --> 00:41:09,120
all of a sudden flip its answer right and and there's a lot of work on adversarial robustness where

334
00:41:09,120 --> 00:41:15,280
if an adversary changes the top left pixel in a certain way to make the model flip the answer

335
00:41:15,280 --> 00:41:24,800
we don't want that either but I also think that a very very practical model is not some adversary

336
00:41:24,800 --> 00:41:34,240
even or even degradation in the image quality but just just kind of a different viewing angle or

337
00:41:34,240 --> 00:41:41,600
slightly different lighting or you know a little bit of a difference in how that object looks

338
00:41:41,600 --> 00:41:47,200
like like a person with a hat on as opposed to no hat or a winter hat as opposed to summer hats

339
00:41:47,200 --> 00:41:53,200
okay so all of these kinds of variations we want models to generalize to them right we don't

340
00:41:53,200 --> 00:41:59,520
want them to break and they do break they they they currently you know models do not generalize

341
00:41:59,520 --> 00:42:09,760
well to out of domain data data that is distributed differently from the training set so in this

342
00:42:09,760 --> 00:42:18,480
paper we are trying to fix that by doing unsupervised learning where we have a bunch of images

343
00:42:18,480 --> 00:42:25,360
so one example again I'll go back to paintings real images and clip art and sketches right so these

344
00:42:25,360 --> 00:42:31,680
are images of the same classes but from different domains and what we want the model to do is we

345
00:42:31,680 --> 00:42:39,760
wanted to learn that for example a giraffe is the same as a painting as it is in a sketch or in

346
00:42:39,760 --> 00:42:46,400
a real photo right but if you train the model with self supervised losses you know this kind of

347
00:42:46,400 --> 00:42:55,200
contrastive learning losses or sim clear moco all of these popular unsupervised training objectives

348
00:42:56,720 --> 00:43:03,360
what the model does is it tries to learn how images are similar and it ends up actually learning

349
00:43:03,920 --> 00:43:09,120
the domain similarity before category similarity in this case so we'll learn that a sketch of a

350
00:43:09,120 --> 00:43:18,080
giraffe is closer to a sketch of a guitar than it is to a photo of a giraffe right because it's

351
00:43:18,080 --> 00:43:27,040
picking up on those more superficial features are are you making the general statement that unsupervised

352
00:43:27,040 --> 00:43:40,320
approaches have fared worse in multi domain or you know non-constant non-single domain scenarios then

353
00:43:43,360 --> 00:43:51,200
then supervised approaches because of this general tendency to favor domain similarity versus

354
00:43:51,200 --> 00:44:02,480
object similarity yes I think I think I will risk making that claim you know we have a couple papers

355
00:44:02,480 --> 00:44:08,880
that supported within certain parameters but yes I think what happens when you have multi domain

356
00:44:08,880 --> 00:44:15,760
data with class labels you know the class labels are telling the model that this is a giraffe even

357
00:44:15,760 --> 00:44:22,320
though it's a sketch and this is a giraffe even though it's a photo and so focus on the

358
00:44:22,320 --> 00:44:30,640
giraffeness of the thing as opposed to other you know other types of correlations exactly right

359
00:44:31,440 --> 00:44:38,640
so so just that supervision of you care about the category not the domain that helps you learn a

360
00:44:38,640 --> 00:44:47,040
more generalizable representation but when you have unlabeled data the only training single signal

361
00:44:47,040 --> 00:44:53,520
you're giving to your model is okay if you take this sketch of a giraffe and you augment it

362
00:44:53,520 --> 00:44:59,440
with some you know you add some noise or crop it or rotate it or whatever people do or even just

363
00:44:59,440 --> 00:45:04,480
find its nearest neighbor it should be closer to that nearest neighbor than to some other image in

364
00:45:04,480 --> 00:45:12,640
your data that's farther away and and there's no category information there so the model doesn't

365
00:45:12,640 --> 00:45:18,880
know that the giraffe should be close to the painting giraffe should be close to the sketch

366
00:45:18,880 --> 00:45:23,680
giraffe because in pixel space there's a huge difference between those two images

367
00:45:23,680 --> 00:45:38,240
yeah so just very briefly to describe the main idea in the paper we essentially learn this bridge

368
00:45:38,240 --> 00:45:45,840
domain so think of it as creating a version of each training image that tries to remove all the

369
00:45:45,840 --> 00:45:52,800
domain specific information and only keep the general kind of outline and the general features of

370
00:45:52,800 --> 00:46:00,480
the object right so for the giraffe example it ends up looking kind of like an edge image

371
00:46:01,520 --> 00:46:08,320
but that was my impression yeah yeah but unlike an actual you know cany edge detector image or

372
00:46:08,320 --> 00:46:16,080
traditional edge detector which also picks up on a lot of edges on the background seeing you know

373
00:46:16,080 --> 00:46:24,080
a lot of irrelevant edges or it may on the other hand remove edges that are important like the spots

374
00:46:24,080 --> 00:46:30,160
on the giraffe because you know that's just edge detection it doesn't know what's important

375
00:46:30,160 --> 00:46:37,360
and what isn't in our approach this bridge domain is edge-like but it keeps the semantically

376
00:46:37,360 --> 00:46:43,440
important features and edges like the outline of the giraffe and the spots in the giraffe but

377
00:46:43,440 --> 00:46:52,480
removes all the irrelevant edges and the edge domain is is learned as well kind of end-to-end

378
00:46:53,840 --> 00:47:00,160
as part of as part of training the whole model yes so and the important thing is that we don't just

379
00:47:00,160 --> 00:47:07,360
we don't actually use that edge domain to do the final classification it's actually

380
00:47:07,360 --> 00:47:17,840
use to compute which images are similar to each other and so I think one criticism of this

381
00:47:17,840 --> 00:47:22,320
approach initially might be wait a minute but if you're using an edge-like domain aren't you

382
00:47:22,320 --> 00:47:28,480
throwing away color information well in fact we're not the model is still learning color features

383
00:47:28,480 --> 00:47:34,880
but to learn which objects are similar and which ones are not we're using this bridge domain which

384
00:47:34,880 --> 00:47:44,880
is edge-like so does that mean like in the final in the loss function there's you know some factor

385
00:47:44,880 --> 00:47:52,320
relating to the you know distance in the you know the edge domain space and also another factor

386
00:47:52,320 --> 00:47:58,640
relating to distance in the the actual image space yes and in fact the distance in the edge domain

387
00:47:58,640 --> 00:48:08,240
space guides the the model in learning it tells it which which are the similar pairs and which are

388
00:48:08,240 --> 00:48:16,400
the dissimilar pairs so that it can train in the unsupervised way from the original images

389
00:48:16,960 --> 00:48:24,960
right so think of it as kind of this we're faking some supervision that is a little bit stronger

390
00:48:24,960 --> 00:48:30,640
hopefully than just the model by itself trying to figure out which images are similar and which

391
00:48:30,640 --> 00:48:36,240
ones are not and being confused by all of these extraneous kind of textures and domain specific

392
00:48:36,240 --> 00:48:49,440
features and so in creating this edge domain space like how I get how handcrafted is that in a

393
00:48:49,440 --> 00:49:00,480
sense like I'm imagining if you you know found a way to create some embedding of the you know similar

394
00:49:00,480 --> 00:49:08,400
classes that it's not particularly likely that you're going to come out with these cool looking

395
00:49:08,400 --> 00:49:16,320
edge images like that that's more more handcrafted is what I guess the feel that I'm having here

396
00:49:16,320 --> 00:49:26,720
so I think that it seems maybe handcrafted because we initialize the generator with an edge

397
00:49:26,720 --> 00:49:38,320
edge image so the part of the model that learns to map the original images to the bridge domain is

398
00:49:38,320 --> 00:49:47,680
initialized by computing edges of those images and so originally it learns just edge maps but

399
00:49:48,640 --> 00:49:55,280
then as we keep training it in the sense supervised way to kind of learn about objects essentially

400
00:49:56,560 --> 00:50:04,400
it starts to get away from the original just edge detection kind of images and starts to for

401
00:50:04,400 --> 00:50:12,000
example remove some of the relevant background edges and keep the relevant ones that are useful

402
00:50:12,000 --> 00:50:21,280
for computing kind of objects similarity well we'll be linking to your workshop presentations

403
00:50:21,280 --> 00:50:28,640
and slides and folks can kind of dig into those for the papers that you've worked on and that you

404
00:50:28,640 --> 00:50:38,320
recognize cited but I want to thank you for a great discussion once again sounds like you're

405
00:50:38,320 --> 00:50:45,600
going to be super busy as CVPR yeah but it'll be fun the first in-person conference in three years

406
00:50:45,600 --> 00:50:51,440
so I'm excited well Kate thanks so much for taking the time to share with us a bit about what you've

407
00:50:51,440 --> 00:51:00,640
been up to thank you for having me

