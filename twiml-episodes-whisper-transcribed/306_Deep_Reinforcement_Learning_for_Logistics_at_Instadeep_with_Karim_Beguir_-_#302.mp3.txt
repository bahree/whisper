We are just one week out from the premiere event focused on accelerating,
automating, and scaling, machine learning, and AI in the enterprise.
That's right, Twomokon AI Platforms.
Over the past few weeks, we've shared a ton of details about the conference,
including our live keynote interviews where I'll be onstage with Andruing
and leaders from Uber, LinkedIn, and Cruise.
Our Super Size 2 Track Agenda, complete with speakers from Levi Strauss, Zappos,
Capital One, Airbnb, and many more.
Our community experiences like our Day 2 Unconference,
where attendees will propose and vote on topics to further explore in small group sessions.
And the Twomokon Happy Hour, complete with food and drinks,
interview, and photo booths, and AWS Deep Racer Contest, and much more.
It's not too late to get your tickets, but you'll need to act fast
if you don't want to miss out.
Head over to twomokon.com now, and secure your ticket for what's shaping up
to be an amazing event.
And now, on to the show.
All right, everyone.
I am on the line with Kareem Baghear.
Kareem is the co-founder and CEO of InstaDeep.
Kareem, welcome to this week in machine learning and AI.
Hi, Sam. Thank you for having me.
You are very welcome to be on the show,
and I'm looking forward to diving into the conversation.
But before we get into our main topic,
which will be focused on AI innovation in the field of logistics,
I love to learn more about your background.
You are in a, can we call it a unique position
of having co-founded a deep learning company in Tunisia, North Africa.
How did you get to that point?
Yeah, actually, I grew up in southern Tunisia in a city
which is very familiar to Star Wars lovers,
because it's actually Tatooine.
So it's the same name that George Lucas used
for the first movie about Star Wars.
So grew up in southern Tunisia,
and my love of applied mathematics took me then
to graduate education in France,
called Polytechnic, and ultimately to the US
at Courant Institute at NYU in the US.
And, you know, in study,
is a project that, you know, I co-founded with Zora
that I met in Tunisia with the goal of, you know,
basically proving that we can do, you know, advanced AI
and deep learning in Africa.
And it's been a crazy adventure for us.
We started really with like two laptops and no funding.
It was a pure bootstrap.
And we're very excited to, you know,
how much we've come since then.
Awesome. And the company to this day has most of its employees
in Africa, all in Tunisia,
or are there other locations?
So, yes, majority of our employees are in Africa,
something like 70% of the company.
And after the original Tunis office,
we also have opened up setup shop in Lagos,
in Nigeria, as well as Nairobi in Kenya,
and looking forward to opening an office soon in South Africa.
So, it's been a very, very interesting ride
in terms of like discovering the potential
of African talent.
And we also have two offices in London and Paris as well.
Okay.
And so, what's the company's focus?
So, the company is focused on basically building
advanced decision-making systems for the enterprise.
So, if you look at, you know,
deep learning and uses of AI,
historically, you know, most companies
and most startups focus on pattern detection.
So, advanced uses of either machine learning or deep learning.
But at Instady, we actually believe that, you know,
technology is around reinforcement learning
and the like are coming now to a state of fusion
that make them actually deployable for the enterprise.
And we anticipate significant savings.
So, we are focusing on helping large companies
realize the promise of AI, which is, of course,
easier said than done.
So, our role is to help them, you know,
seize the opportunity in AI,
while at the same time, you know, as discussed,
building up a new generation of talent
and making people believe that, you know,
actually their dreams can come true,
they can be part of this.
Nice.
Nice. You mentioned reinforcement learning.
Yeah, that comes up on the show quite a bit.
And there are very, it can be a polarizing topic
in the sense that there are folks that I talk to
that are very excited about it.
And the promise that it offers not just
in these traditional RL applications like game playing,
but for enterprise applications.
And then there are other folks that say,
it's way too complicated.
We're nowhere near being able to put it to good use.
It sounds like you're in this ladder camp.
So, yeah, we believe, actually,
that the technology is sufficiently mature now
to get to practical applications.
I mean, if you see historically, you know,
DeepMind can start to become, like,
a deep RL started to become something impressive
with originally DeepMind and the progress they did
with DQN on Atari Games and the like.
But now with progress on multiple fronts,
it's actually possible to deploy these technologies
in real life.
And, you know, we expect this to become
more and more mainstream.
So, you know, we actually doing work in multiple sectors,
but in particular in logistics that confirms
that these technologies can actually be deployed
and can actually generate savings
versus more classical technologies.
Okay, and so we've referenced a couple of times
the field of logistics,
the connection being a presentation that you delivered
at the recent Nvidia GTC conference
on some of the work that you've seen
and that you're doing on AI and logistics.
Can you maybe frame up the problem for us?
What are some of the specific challenges
that enterprises are seeing in logistics
and what are the gaps between, you know,
logistics obviously has been a problem
for many, many, many hundreds of years.
And we've developed technology-based solutions
to try to solve these problems.
What are the gaps that are leading folks
to look at deep learning
and machine learning-based approaches?
So, you know, on logistics,
you have multiple problems
that actually require making decisions
in a complex environment
where you have a very large number of choices.
So, just a specific example,
if you look at a right-sharing, for example,
for a mobility company such as Uber or others,
effectively you have a situation
where you have hundreds of cabs in like a big city
and at the same time maybe, you know,
hundreds or thousands of riders requesting a ride
and with a right-sharing allowed,
basically you can onboard, let's say,
three to four people in a given cab.
This is a complex decision to make.
Which cab should cater to which requests?
It's not an easy decision.
And this is typical of what we believe
is the opportunity for, you know,
deep reinforcement learning-based systems.
So, you effectively have to look for a needle
in a hay sack and, you know,
one of the ways to do that is actually
to train a modern AI system
to look for good solutions.
So, it really comes down to,
you have a very large search space,
sometimes bigger than the number of atoms in the universe.
And you need to find a clever way
to go through this search space.
So, I'll tell you historically what enterprises
have been doing to solve this problem,
is, you know, if you look at operations research,
OR, typically all the algorithms
that are used in OR are clever ways
to make that search space simpler
and they are more amenable to, you know, algorithms.
So, that is, you know,
that works pretty well in practice,
but it's far from being optimal.
And typically, we're talking about problems
that, you know, have a very large search space
and can be NP-hard,
so it means that there is not an easy solution
to solve them.
So, by making the search space smaller,
you know, OR algorithms effectively
are introducing biases.
So, you know, they're built on heuristics,
and those heuristics mean that a system
that could learn from scratch
could go beyond what is the state of the art at the moment.
So, it's very similar to what happened in the game of chess
before Alpha 0 came,
so Alpha 0, the groundbreaking algorithm
that DeepMind developed in late 2017,
basically before Alpha 0,
the best chess player was a system
that had to, you know, to use lots of rules,
whether coming from the programming side of things
or coming from the chess side of things,
but like there were hundreds of rules
that were pretty much embedded into the program
to make it do, you know, to make it competitive.
Yet, Alpha 0 came and replaced all of that
with just two simple concepts,
searching and learning,
and it was able to outperform, you know,
stockfish this particular chess program
in, you know, as little as four hours
if you use distributed machine learning.
So, I was watching that.
I was actually at Nuribs for the first time
when David Silver presented these groundbreaking results,
and I was like, wow,
this is actually very exciting,
and it has applications that are beyond games.
Like, think about, you know, as I said,
the right-sharing example I gave you
or, for example, you know, how you fit objects
or packages in containers.
All of those have this property
that, you know, you need to look for the right solution
in a very large amount of, you know,
a very large space.
And so, all of those are potentially
amenable to this kind of solutions.
And, you know, since then,
we've been working very hard,
I think, to make this promise a reality for our customers.
When you're approaching a problem like
the right-sharing problem,
how do you transform, you know,
this problem that, you know,
kind of makes sense when you explain it
into something that you can apply directly
deep learning or RL2.
So, effectively, you need to build up
a model of the environment,
first, which comes from, you know,
the real world.
So, for example, what we use at InstaDeep
is open-street maps.
So, we take, actually, the exact, you know,
information on traffic,
on the topology of the city from open-street map.
And we transform the problem of,
you know, going from point A to point B
and selecting different, you know,
passengers into a set of possible actions.
And basically, you know,
build it into an RL framework.
That's exactly the same method
that we would use, for example,
on a problem like dinpacking.
You have a certain volume.
You have certain constants that you need to respect.
And your goal is to find optimal configurations
such that, you know,
the total volume that you're using
is the smallest possible,
while respecting all the other constraints.
So, basically, there is a significant work
which is done on the environment.
But the general principle remains the same.
At the end of the day,
you have, let's say,
a reward function that comes out of this.
Usually, on NP hard problems in logistics,
you know, it's very easy to verify
if a given solution is a good one or not.
What is not so easy is to find what is a good solution.
So, the problem of like scoring the result
is usually not extremely difficult.
You just need to build up the environment
to make, you know, this possible
and then find ways to train this escape.
So, sticking with this right-sharing example
is the assumption that for a given ride,
you kind of start with a starting place
and a destination may be determined by an initial rider
and you're trying to figure out what other riders to pick up.
Yes. Basically, you have to make decisions
about which riders to take into the car.
And, you know, the car can have two states
like it could either have, you know, already people
in the car and given destination where to go
or it could be completely empty.
So, you have to make a decision about who you want to onboard.
And obviously, if you onboard a given person,
a given client, then you have to take that person to destination.
So, and you do this, you know, progressively, you know,
given the capacity that you have left.
In a situation like ride-sharing,
a typical metric of, you know,
that you're trying to optimize for
is the total number, like the total delay
for the passengers you're onboarding,
meaning that, you know, for example,
if an open street map tells you that you could deliver that person
in five minutes,
if you deliver that person in seven minutes,
then it's a two-minute delay.
And your goal is to, of course, optimize the total amount of delays
to minimize it and have, you know,
satisfied, basically, client base.
How have you created a data set
to use to train these models?
Is it all synthetic or simulated data
or is there some set of data
that you're using to kind of bootstrap this process?
So, initially, we started by working with, you know,
New York City and Manhattan,
because there is significant amount of data,
which is available through the New York Taxi
and Limousine Commission.
So, that makes it, you know, interesting to start with.
But the way the OR approach is effectively
to use open street map
to extract as much data as possible
to model the environment of the city
we're looking at,
and open street map has also, you know, many cities available.
It's actually, I believe, the tool that you were used
when they started their operations.
But when it comes to this problem,
it's actually possible to train from zero data.
So, it's very similar in principle
to what DeepMind did with Alpha0.
Zero means that there is zero data.
What you're setting up is basically a full simulated environment
of, you know, of the city.
And you can learn by doing.
So, the system will initially make random decisions
and then progressively realize what is good
and what is less good and learn from there.
So, this is also what is interesting about this problem
and also why, you know, relatively small scale start-up
like in study today can tackle those problems.
It's because, you know, have not having necessarily a lot of data
is not a hurdle provided you can build a realistic environment.
So, the data components is really about, you know,
affecting your ability to build a realistic environment of the city.
But if you find a way to do that,
technically speaking, you actually do not need data.
You've got this environment,
beyond the open street maps, data.
What's the kind of shape of this environment?
Did you build kind of, feel like a custom simulator
or is it, is it just the data?
You know, and not a lot of kind of external kind of tooling
or did you use an open source simulation platform
that you plug the data into?
What does it all look like?
So, actually, we built our own, our own basically simulation
and giant for this problem.
And including visualization,
we actually also have visualization teams at InstaDeep.
So, effectively open street map is used to provide us with, you know,
metrics about, you know, how much time, for example,
it takes from going to point A to point B in a city.
And basically different, different, like, intermediate points.
For example, like, I give and write could have, like,
50 intermediate points or 13 intermediate points, you know,
that kind of order of range.
But once you have that, effectively,
everything else runs on our platform.
So, we've built, you know, our own platform
to tackle these type of problems.
And this is, you know, one of the things
that makes it exciting for us.
Like, you know, it's something that we have full control on.
So, we use basically, you know, Kubernetes and TensorFlow.
And train on Nvidia DGX1.
So, Nvidia super computers to be able to extract results
and continue to learn.
And one is also interesting about this type of, you know,
solution is that when they are deployed in the real world,
you have an ability to, you know, keep learning from the results
you're getting as well.
You're using DGX1s.
Those aren't cheap.
Because your end user need to have the DGX1
to continue to fine tune a model.
Or is that, you know, what you've got to build up these models
and then you can push the models out for inference?
So, these are, yeah, it's exactly like you said.
We basically use those for training.
But once a model is trained,
it would simply be, you know, about inference
and customer does not necessarily have to have, you know,
have you weaponry as such.
Exactly.
Exactly.
So, that kind of leads me to the question about generalization
and transfer learning in a sense.
You know, for example, if you train a model on,
or when you train the model on New York City,
can you apply that model to San Francisco?
If you do need to then look at San Francisco,
do you have to start from scratch?
Or can you do kind of a traditional transfer learning type
of fine tuning?
Does that, is there an analogy of that kind of fine tuning
and reinforcement learning?
How does that all work?
Absolutely.
Absolutely.
Actually, you know, transfer learning works up to an extent.
So, you are much better indeed using a trained model,
even though it's a different city,
to get started in another,
even though you're going to still require, you know,
significant training dependent on how similar the topology
of the city is.
You know, if the topology of the city has really nothing to do,
then of course, that's an issue.
If, for example, to coming back to Manhattan,
if you have another city which has a relatively speaking
rectangular shape for its streets and avenues,
then obviously, you know,
what you have learned in Manhattan would be
to a certain extent applicable.
So, we've seen actually this type of transfer learning
happening in different problems.
I mean, this is the right sharing problem.
When it comes to, for example, packing bins,
it's the same.
If you are packing bins in a certain configuration,
let's say, for 30 items,
what you learn can be directly redeployed to larger dimensions,
like 200 items under like.
Sometimes even without retraining.
So, you know, you nailed it on it.
It's actually, you know, there is the equivalent of transfer learning
in this type of problems as well.
Yeah, I guess when we think about transfer learning
applied to computer vision, right?
You're often training models on image net and you're,
you're trying to teach the lower layers of the model,
kind of primitives like textures and edges and things like that.
Is that a very problem-specific thing?
Or are there, you know, some equivalent of edges
at the, you know, just broad logistics level
such that you would get some transfer benefit going,
taking a model from bin packing to a right sharing, for example.
So, there wouldn't be lots of, you know, intersection between,
let's say, taking a bin packing model to right sharing.
But there would be between different bin packing problems
or different right sharing problems.
So, typically like to take the bin packing example, for example,
like if you're dealing with a problem that has, you know,
like multiple hundreds of items,
this is actually a large combinatorially exploding problem.
However, if you are already having a good performance
with a smaller set of, you know, items,
actually this set could be as small as 10 items,
then indeed the system has figured out something about
how do I fit packages together that can be redeployed
on larger models.
So, it's a very interesting topic, but like at InstaDeep,
we've been able to take models that been trained on,
let's say, 10 items and deploy them with almost no changes
at all to larger sets of problems, like 50 and 100 items,
and still getting, you know, a type of performance
which is better than other OR type algorithms.
So, indeed, you know, there is a transferability,
but I would say it's transferability among certain domains.
So, you know, it's things that still have some level of similarity
between them.
And within, for example, the domain of bin packing
are there dependencies on kind of the,
you clearly mentioned the number of items,
but like the shape of the items or the shape of the containers,
the bins?
Yes, of course.
Yes, of course.
If you have, you know, shapes that are quite similar,
you would get strong transfer learning features.
For example, like if it's the same shape for the general volume,
let's say it's a container,
and you learn to solve the problem for 10 items
and you're just adding up more items that are relatively similar
to the items you already have.
In certain cases, you would have very little extra training,
meaning that starting with the train model on smaller items
would still deliver to you an acceptable performance.
And this is actually surprising.
We were, when we started working on this with my team,
we were not necessarily expecting this type of properties,
and yet we got there.
And these are some of the results we published in an article
that the latest, the last no-rips,
which is called rank rewards.
We found ways to basically go beyond what had been done
for Alpha Zero and the game of chess and go
to apply this to real, to real concrete problems.
And yes, we were surprised that, you know,
the level of transfer, you know,
transfer ability from one problem or another was actually pretty high.
What can you say about the data efficiency
or sample efficiency rather of RL
for this particular type of problem?
Do you, you know, how much, how many,
how, I don't know what the,
if you want to talk about in terms of time
or compute cycles or, you know, runs or batches,
but how complex is, you know, getting to a model
that, you know, starts to perform well
compared to some of the other things that we might apply RL2.
So, I mean, there is definitely some complexity,
but with, you know, modern equipment,
you could get to results in a matter of, like,
something like 24 hours or, you know, a couple of days.
We're not talking about like super, super long jobs,
but it's not a couple of hours either.
So, to give you concrete examples on our Nvidia DGX one,
we would typically train a model overnight
and be satisfied with the results.
So, it's not something unsurmountable, which means that, you know,
if you're not Google or if you're not like a very large player,
you would not be able to compete.
Obviously, it's strongly dependent on the quality of your algorithm.
We spend a lot of time, for example, trying to find ways to,
we inject the concept of competitiveness,
even though these are like one player games,
you're just trying to solve a problem.
You know, one of the secret, the secret source, for example,
in Alpha Zero, is that you are playing against yourself,
and so you're constantly, you know,
it's a two-player game where you're constantly challenging yourself.
So, when you're really bad, like you start with random parameters,
it doesn't matter because the other person in front of you,
which is, you know, a replica of yourself, is equally bad.
What matters is to get slightly better.
And it's the same thing when you get to, like, superhuman performance,
you can still improve, because what matters, in a sense,
is the relative improvement than the absolute improvement.
So, we found ways to re-inject this type of principles
of, you know, adversarial competitiveness into this problem,
and you can get good results, as I said, in, you know,
a recent, a decent amount of time.
Interesting. Is that part of the work that you've published
at the recent NURPS?
Yes, absolutely. So, we have a paper called Rankewarts,
which was accepted at the DPRL workshop at the recent NURPS,
and it describes exactly the type of results and breakthroughs
we are describing.
We talked about how, historically, these types of OR problems
are solved using heuristic methods.
One of the trends that I see in deep learning is,
kind of, there's a pendulum where we kind of started
at these very model-based approaches to things,
and we swung to, like, pure statistical and learned approaches.
And now, there's a lot of effort happening
to try to fuse model-based and heuristics-based approaches
with statistical approaches.
Is that something that you've looked at for these types of problems?
Yes, absolutely. The general idea that you're going to inject
learning ability into a process is a very powerful one.
But, yes, in some cases, it actually can be pretty efficient
to take and existing heuristic,
but add learning ability into the mix.
And it comes to a very natural idea.
If you are a large company, for example,
you have lots of, like, you have a fleet of trucks,
you have lots of packages to deliver tomorrow,
and some of these companies would do overnight runs,
take the result, and use it to, you know,
process their operations for the day.
And the next day, they're doing the same.
It's kind of a waste not to learn anything new.
So, adding learning ability into the mix,
and especially constant learning ability,
as you get, you know, more experience through your operations,
is a natural idea.
So, there are two schools, one which is indeed saying,
you could, you know, you could use existing heuristics
and build learning on top, which, you know, which makes sense.
But, increasingly, and especially with, you know,
compute, compute power for machine learning,
doubling every three months and a half or so.
And this is a study by OpenAI.
In certain cases, it becomes possible to learn from first principles,
meaning that you do not necessarily need the heuristics.
So, the honest answer is very problem-dependent.
Some problems are still too big to be learned completely and to end.
But some of them can be actually solved using this type of techniques.
So, it sounds like your general perspective
is that where you can use pure learning-based solution that's preferable?
Yes, absolutely, because that removes significant sources of bias.
And, you know, by learning through first principles and direct experience,
you can get a lot done.
There is an additional advantage, you know,
sometimes a heuristics-based model would require specific constraints to be in place
and might not tolerate certain number of constraints.
When you're taking, like, a pure learning approach from first principles,
you can actually set all the constraints,
all the real world constraints that you care about.
And this is also an advantage, means, like, if these are the real world constraints,
and actually there are people in the real world running those operations with those constraints,
you know, solutions exist.
So, as a consequence, a model learning end to end could get to that point as well.
You mentioned that one of the considerations is the some inherent bias in the heuristics-based models.
You know, this might come from constraints or other factors,
and that a learning-based approach overcomes those.
Have you seen the flip side as well,
where the learning you've seen bias introduced by the learning-based approach
that disadvantages it relative to what an organization might want to do?
So, I mean, in general, like, if the problem is well formulated,
and, you know, the system will learn to crack that problem.
So, yes, in cases where you see the system not doing what you expect,
it really comes down to, there might have been some constraints that you forgot to include into the mix.
But if you really, you know, give to the system all the constraints that he should care about,
you know, the system ultimately will explore and learn to crack that problem with the tools you have.
So, it's more a question of defining the right set of objectives,
and what's the space of, you know, possible behavior?
If you do this well, there is no reason why a system which has sufficient capacity
would introduce significant biases.
Just to give you an example in the game of chess as well,
you know, there have been some recent publications, I believe the title of the book is Game Changer,
where, you know, chess experts have looked at the behavior of Alpha Zero,
and, you know, some of them said, you know, this is very interesting,
because, you know, we believe this is very close to the truth,
meaning that the system has explored so many configurations,
and developed an intuition which is really based on facts,
that, you know, some people would consider this, you know,
including actually Kasparov as, you know, the best manifestation of truth in chess
that is possible to achieve.
I'm also wondering about, I imagine when you're talking to enterprises
that are considering these types of approaches,
there are concerns like explainability and even robustness.
You know, when you've got a set of rules, the actions that those rules tell you
to take are relatively predictable, whereas with a deep learning solution,
I'm imagining, you know, there can be instabilities or your decision boundaries are very complex,
and you can have the system-recommend actions that are, you know, maybe, you know,
not actions that you would want to take in the real world,
or these behaviors that you see coming out of systems like these?
I mean, it's a fair point. I mean, explainability will always be, you know,
a concern at some level when using deep learning,
and even though there is a promising research going on in terms of, like,
getting better ways to understand what the system is doing,
and how we, how it can explain to humans what it is doing.
But typically, we don't operate into, with customers for whom explainability
would be extremely strong. So coming back to the examples we discussed,
whether it's right-sharing or been-packing, ultimately, the customer will care about,
you know, in the case of been-packing, how many containers he's using,
provided all the constraints about how to pack, have been respected.
Same thing with right-sharing, you know, the client is going to care about, you know,
what's the, you know, average, for example, a number of, number of miles driven
to deliver one customer and the number of customers delivered.
In this case, it's okay if a few decisions are suboptimal, provided the total,
you know, the average decision is better. So in a sense, you're not getting extremely
penalized for taking a decision which locally would look suboptimal.
So that is typically, you know, the kind of, you know, customers re-engaging
and it's a thing studied for logistics, but I mean, to your point, absolutely.
I mean, if you all have certain situations for which where the negative impact
of a bad decision can be massive, I don't know, for example, you know,
like in, you know, drug manufacturing or things like that,
obviously, explainability could be of interest, but we haven't seen this,
you know, with the type of problems we work on, because really about logistics
and it's about optimizing well-known constraints and, you know,
there's not something which, you know, like if you deliver, for example,
a person and it has taken too long for that particular person, you know,
it is a concern, it can be improved, but it's not a massive concern
that it would stop all your operation.
True, wouldn't stop all your operations, but as organizations mature
in their use of these types of technologies, they may want to, you know,
their perception of what's the right reward function evolves and matures,
and at some point, you know, maybe they wouldn't want, you know,
a customer to randomly sit in the car and be driven across town only,
be driven back because that's what the machine said.
Do you have a path to addressing that in this kind of approach,
or are they totally antithetical to one another?
You can, you can, you know, control this type of, like, spurious behavior.
For example, like typically, like in the example you take,
if somebody, if a car would take someone and really drive around town in crazy ways
before delivering that person, you could actually put a very significant constraint
on like in a penalty if you want on the outcome.
So instead of having just, for example, like the total delay
that this person experienced, you could say that, you know,
beyond a certain point, which is statistically driven,
and you can collect the statistics for a given city.
Hey, if you go beyond that, there's really a significant problem.
And as a consequence, let's say, you know, the penalty on the reward system
would be very high.
So that's one of the ways you can, you can actually remove spurious cases from your system.
And when you're engaging with someone around these types of problems,
do you find that there is very unique and specific problem definition
that needs to happen and kind of building out these reward functions
and penalties and things like that, or, you know,
the most people just want a been packing solution that,
for which you've already really defined the problem.
So typically when we work with large corporates,
those companies have already processes in place, including, you know,
and they have a good sense of what type of constraints they are experiencing.
So one of the cool things at INSADEEP is that we will adapt to their problem.
Our process is pretty general.
So we basically put ourselves in the customer's shoes.
So we'll take the problem with, like, look at it the way they look at it.
They tell us the constraints they have.
And yes, those constraints could change from one customer to another
because maybe they're not doing exactly the same type of operations
or maybe because they have different priorities and preferences.
So we'll incorporate their constraints and work from there.
And as I said, you know, what's pretty cool in the way we look at things
and these general systems is that they can actually learn from, you know,
the constraints you give them.
So you're not a prisoner of given constraints.
So it's not a problem in practice to be looking at different customers
and different constraints because the principles that are behind it
and this is the thing which I'm most excited about,
are very general. So no matter how you look, you know,
what are your constraints and some of those could be real physical constraints
and others could be preferences,
there is always a way to solve your problem.
Provided that it is actually a real world problem that has solutions
and it's typically the case and you want to improve your solutions.
And so we spent most of the time talking about reinforcement learning.
Are there other approaches that you use for these types of problems
or is that the primary area you're working with?
So I mean, we also use like, you know, classical algorithms,
but these are more for benchmarking.
You know, I think the people who believe that, you know,
the type of smart systems are there to stay.
But when we say reinforcement learning just to be very clear
and I know the audience is pretty technical.
What we believe the most at in in in in study is what we believe in the most is
the concept of having, you know,
a neural net that's going to work with planning type algorithms.
For each we look at a technology such as Monte Carlo research.
It is, you know, in the secret source of, you know,
what makes, for example, alpha zero or so good.
So, you know, I have a great analogy for this.
It's this idea and I presented this at GTC,
which is this idea of thinking, thinking fast and thinking slow.
So this is the same title as Daniel Kaneman's book, The Noble Price.
It's this idea that with planning, you know,
basically Monte Carlo research into this type of algorithms,
you're going to be able to think slow,
which means you're going to consider many parts
and incorporate those parts of flat planning into your decision.
And the thinking fast part is the neural net.
So to be very clear, it's not just a rel in the sense,
hey, I'm experiencing a reward and I'm kind of looking blindly for that reward.
I'm actually trying to find, you know, interesting paths.
So another way to look at it, which is an analogy I really like,
is, you know, in deep RL, you're like in the dark in a room
and you're trying to find the switch.
And that is the sparse reward, for example, that you're looking at.
But you're like kind of like looking blindly and trying to find the switch.
You know, if you add planning and CTS type approaches,
it's like you have a search light.
It's not a perfect search light.
So it's only covers a little bit of area.
But at least you're not completely blind.
And this will help you be more efficient, much faster.
So these are the two tech that are used.
And it makes a lot of sense.
And in a funny way, it's actually interesting that Daniel Kaneman's book,
by the way, which is a great book, talks about it
from the angle of, you know, human neuroscience
and human, you know, behavioral patterns.
But there are some equivalents in the machine learning and AI world.
So let me ask you this.
We've spent quite a bit of time talking about this topic
and what you presented at GTC.
What were the key takeaways that you were hoping to leave your audience there with?
So, yeah, for me, the key takeaway is that, you know,
machine learning systems are progressively, you know,
going to have an impact when it comes to making decisions
in the real world.
And, you know, while we have mostly seen, you know,
those in the context of games,
and those games have, you know, started like very simple games
at Aristotle.
And now it's more like Starcraft complex games.
I think we are about to see those reads
I think we are about to see those really spread out
and make an impact in the real world.
And this is due to, you know,
a few key properties of these systems.
First, these systems can learn end to end.
So you can, you know, in certain cases go from pixels to behavior
or from raw data to behavior, things like that.
But these systems get better with scale.
And we have reached a point in terms of maturity of, you know, hardware
and also like, you know, algorithms that makes, you know,
you know, deploying these systems at scale a real possibility.
So I would, you know, to the users, you know,
the listeners of the program, I would say that, you know,
expect a lot of action on these in the real world
and not in games anymore.
And to a certain extent, if you are, you know, having a decision-making
system that uses a compute and doesn't have learnability
deeply embedded in it, that means that probably you will have to change,
you know, the way you are operating to include, you know,
AI first approaches and, you know, learnability.
And that means that, you know, lots, lots of industries
and particular logistics, last mile delivery,
are going to be deeply impacted by those.
Also, well, Karim, thank you so much for taking the time to chat with us.
This is really interesting stuff.
Thanks a lot.
It was a pleasure.
And, you know, thanks a lot for having me.
All right, everyone.
That's our show for today.
For more information about this and every show,
visit TwomoAI.com.
Remember, just one week left to register for TwomoCon,
AI platforms.
So head over to TwomoCon.com now.
Thanks so much for listening and catch you next time.
