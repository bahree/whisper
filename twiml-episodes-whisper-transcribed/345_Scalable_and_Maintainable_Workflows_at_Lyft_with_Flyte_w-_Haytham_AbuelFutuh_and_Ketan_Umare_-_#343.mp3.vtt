WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twomo AI Podcast.

00:13.400 --> 00:15.920
I'm your host Sam Charrington.

00:15.920 --> 00:24.920
Hey everyone, producer Amari here.

00:24.920 --> 00:28.800
As we mentioned on the podcast last week, we've got some exciting new additions to our

00:28.800 --> 00:35.480
Twomo community programs, including the IBM AI Enterprise Workflow Specialization

00:35.480 --> 00:41.720
Study Group, which will be hosted by Sam, and the causal modeling in machine learning

00:41.720 --> 00:47.240
study group hosted by Robert Osa-Zua-Ness, who you heard from earlier this week on the

00:47.240 --> 00:48.840
topic of causality.

00:48.840 --> 00:54.080
Today, I want to remind you that Sam and Robert are hosting a course and study group overview

00:54.080 --> 00:58.960
session this Saturday, February 1st, at 8 a.m. pacific time.

00:58.960 --> 01:03.400
To get signed up for this course, or to learn more about the other upcoming and ongoing

01:03.400 --> 01:08.680
Twomo community programs, visit TwomoAI.com slash community.

01:08.680 --> 01:11.680
And now on to the show.

01:11.680 --> 01:21.640
Alright everyone, I am here with Heitem Abul Fatou and Katen Umare from Lift, Heitem

01:21.640 --> 01:24.520
and Katen, welcome to the TwomoAI podcast.

01:24.520 --> 01:25.520
Thank you.

01:25.520 --> 01:26.520
Thank you.

01:26.520 --> 01:29.880
So, we're here in once again, sunny San Diego.

01:29.880 --> 01:35.160
We got the two days of like horrible, miserable rainy weather, you know, earlier in this

01:35.160 --> 01:36.160
week.

01:36.160 --> 01:40.680
And we're here to talk about some of what you're doing in Lift, namely the flight project

01:40.680 --> 01:44.800
that you just presented on yesterday, it's open sourced.

01:44.800 --> 01:45.800
And it's open sourced.

01:45.800 --> 01:46.800
And it's open sourced.

01:46.800 --> 01:50.920
We're going to dig deep into this, but before we do, I'd love to have each of you kind

01:50.920 --> 01:54.520
of introduce yourself, share a little bit about your background, how you got to work

01:54.520 --> 01:58.120
on ML and Fra and, you know, what's your story?

01:58.120 --> 01:59.120
Talk to us.

01:59.120 --> 02:00.120
Katen?

02:00.120 --> 02:01.120
Yeah.

02:01.120 --> 02:02.440
Hey, my name is Katen.

02:02.440 --> 02:10.840
I lead the flight team and probably one of the founding person of the flight at Lift.

02:10.840 --> 02:18.240
So my background is I work across different industries from hedge funds to retail, to logistics,

02:18.240 --> 02:19.240
to cloud.

02:19.240 --> 02:25.920
I'm mapping and now finally back in sort of transportation area.

02:25.920 --> 02:32.160
And one of the things I've been interested in is just large scale data processing and

02:32.160 --> 02:40.200
solving business problems where data and computation comes together and machine learning is

02:40.200 --> 02:42.840
a place where, you know, that's really, really important.

02:42.840 --> 02:48.240
I started Lift in 2016, but I started on flight close to the end of 2016.

02:48.240 --> 02:52.840
And it was mostly like, you know, I started working on this team.

02:52.840 --> 02:58.320
We were trying to get ETAs, which are, I'll explain what an ETA is.

02:58.320 --> 03:04.000
So when you open up and lift up and you see, hey, three minutes to get your driver or if

03:04.000 --> 03:08.800
you are in a car and you see like it will take 15 minutes to reach the airport, sometimes

03:08.800 --> 03:10.320
it's accurate.

03:10.320 --> 03:14.600
And it's accurate mostly because of a ton of machine learning models that go in the

03:14.600 --> 03:19.720
background and including understanding how the road traffic is and understanding all

03:19.720 --> 03:25.560
kinds of things that are happening on in the current conditions on the road.

03:25.560 --> 03:33.880
So using, so I was leading the team and which I joined the team, there was another engineer

03:33.880 --> 03:34.880
on it.

03:34.880 --> 03:42.600
He used to run the models all on his laptop and he was like, how are you running this?

03:42.600 --> 03:48.200
And he's like, I have this script, I've just run this and it just runs and it figures

03:48.200 --> 03:49.200
out.

03:49.200 --> 03:52.480
And you know, then I run this other script and then I run this other script and I do that's

03:52.480 --> 03:53.480
crazy.

03:53.480 --> 03:54.480
Yeah.

03:54.480 --> 03:59.760
So, and we, I'm trying to decide whether to interrupt you and just like dive deep into

03:59.760 --> 04:00.760
that.

04:00.760 --> 04:01.760
That sounds crazy.

04:01.760 --> 04:02.760
It is crazy.

04:02.760 --> 04:08.440
Like running a model that's doing like live prediction of ETAs on a platform, like training

04:08.440 --> 04:12.480
training the model, training the model or collecting the data for the model aren't

04:12.480 --> 04:13.680
things like that, right?

04:13.680 --> 04:17.200
And still reproducibility issues and stuff like that.

04:17.200 --> 04:18.200
A lot of issues.

04:18.200 --> 04:19.200
Yeah.

04:19.200 --> 04:21.960
But you wouldn't be surprised how many times this happens in the industry, right?

04:21.960 --> 04:27.640
It's just like the, and this is actually how we lead into it because this is the current

04:27.640 --> 04:32.600
state of the infrastructure for machine learning and especially productionizing models is,

04:32.600 --> 04:33.600
it didn't exist.

04:33.600 --> 04:38.440
We didn't think about retraining these models at that time and quickly we wanted to retrain

04:38.440 --> 04:42.040
them and then I'm the laptop's not going to scale.

04:42.040 --> 04:46.400
Now, the other story that happened, so this is just leading into flight, right?

04:46.400 --> 04:50.640
And the other story that happened is there was a research scientist on my team.

04:50.640 --> 04:53.120
He created a model and that model is pretty cool.

04:53.120 --> 04:55.360
It served live for many years.

04:55.360 --> 05:01.000
But he left the company and the model went with him probably, so we lost it.

05:01.000 --> 05:06.520
Like we had no idea where the model was and as leader, they told me, hey, let's recreate

05:06.520 --> 05:11.680
this model and I, I don't know how to and I, we knew the algorithm.

05:11.680 --> 05:15.400
So we just rewrote it and we got everything done.

05:15.400 --> 05:18.320
It would not give the same results.

05:18.320 --> 05:23.280
And it, like it literally took us three months to get like to the same level of accuracy.

05:23.280 --> 05:24.280
Wow.

05:24.280 --> 05:29.800
And we were like, okay, so he had done all of this extra work that we really kind of lost.

05:29.800 --> 05:33.640
We didn't waste too much effort on it because we knew the algorithm, we knew some of the

05:33.640 --> 05:34.640
tricks.

05:34.640 --> 05:38.480
But still, it's like wasted effort in trying something out and then going and trying

05:38.480 --> 05:40.680
out the accuracy and you're like, oh, it's time to say.

05:40.680 --> 05:41.680
Yeah.

05:41.680 --> 05:42.680
Yeah.

05:42.680 --> 05:45.400
It's not like one person spending all the three months, but it's still like, it's like,

05:45.400 --> 05:46.880
it's wasted effort.

05:46.880 --> 05:51.320
So at that time, I decided that this needs to be, this needs to change.

05:51.320 --> 05:55.160
And delivering new models became slower and slower.

05:55.160 --> 05:57.720
So that was the birth of flight.

05:57.720 --> 06:00.040
At that point, we used to call it a bad name.

06:00.040 --> 06:07.800
I'm not even going to put it on the podcast, but I used to, and we wrote, like I wrote

06:07.800 --> 06:11.800
like a first draft proposal internally and everybody was like, you are crazy.

06:11.800 --> 06:14.640
This thing is not, never going to work.

06:14.640 --> 06:23.320
But somehow in like a couple months, we erected a V zero, V one of this thing.

06:23.320 --> 06:29.200
And we got a team to try it out and this team was also struggling a lot with delivering

06:29.200 --> 06:30.200
their models.

06:30.200 --> 06:36.120
And the intersection where flight really fits in is when you have a lot of data and you

06:36.120 --> 06:40.720
want to reproduce, reproduce your models again and again, like maybe every day, every week

06:40.720 --> 06:42.360
or every hour.

06:42.360 --> 06:47.320
And you want like the trace of what happened and the lineage between everything.

06:47.320 --> 06:49.400
And this team actually fit the bill.

06:49.400 --> 06:52.720
And they, for the first time, they were able to deliver a model in like six months.

06:52.720 --> 06:54.080
And this was a gigantic model.

06:54.080 --> 06:58.160
It affected the bottom line of lift and it was really, really meaningful.

06:58.160 --> 07:04.760
And it was not without a lot of, you know, stress and working hard through the night.

07:04.760 --> 07:10.280
But that was the starting of flight and that was in 2017.

07:10.280 --> 07:17.200
And then we didn't stop like the use of the company just skyrocketed.

07:17.200 --> 07:20.720
And we at that point, we are like, hey, we should open source this thing because it's

07:20.720 --> 07:26.640
such a big problem to solve that a small team at lift can probably never solve it on

07:26.640 --> 07:27.640
their own.

07:27.640 --> 07:31.680
And if you open sources, we should be able to work with the community here, more ideas

07:31.680 --> 07:33.600
and improve it all the time.

07:33.600 --> 07:42.480
So we actually rewrote everything from scratch, made it Kubernetes native, took, took like

07:42.480 --> 07:46.560
the primitives that we understood from looking at all the various use cases.

07:46.560 --> 07:53.680
And that's the amazing part of lift, like it's a rich ground of amazing use cases.

07:53.680 --> 07:58.440
And we used all of that and put like basically distilled that information into flight.

07:58.440 --> 08:01.600
And that's our first cut into the world.

08:01.600 --> 08:02.600
Hi, Tom.

08:02.600 --> 08:06.240
Tell us about your background and what you do at lift.

08:06.240 --> 08:07.240
Sure.

08:07.240 --> 08:08.240
Yes.

08:08.240 --> 08:10.040
So my name is Haytan Abouhtou.

08:10.040 --> 08:16.880
I have worked previously at Microsoft, Google, and I had a journey up and down the stack.

08:16.880 --> 08:24.040
I worked in like enterprise, create applications in low-level storage, like Azure storage.

08:24.040 --> 08:30.720
And I, you know, at some point, wanted to try out ML and I kind of found this sweet spot

08:30.720 --> 08:36.200
in ML Infra to fit the bill kind of thing for me.

08:36.200 --> 08:44.400
I joined lift two years ago, January, and at that time, it was that we're still stabilizing

08:44.400 --> 08:53.200
the prior incarnation of flight, the unnamed product.

08:53.200 --> 08:55.560
It was great and teams loved it.

08:55.560 --> 09:04.760
As Keeta was saying, at that point, I joined in the midst of this discussion about what

09:04.760 --> 09:07.840
do we do next.

09:07.840 --> 09:14.960
So I got to be part of the decision-bidening, going Kubernetes native, and all the very

09:14.960 --> 09:19.680
I see as critical design decisions.

09:19.680 --> 09:26.880
We took in flight to virtual decisions, all the, like going with a very strong type system

09:26.880 --> 09:32.440
and very strong language specifications through Protobov, like there is a lot of things

09:32.440 --> 09:39.080
we view, we are like very opinionated about in flight, and a lot of things we are not.

09:39.080 --> 09:43.520
We like explicitly decided to leave open.

09:43.520 --> 09:51.360
Based on the experience we had, we think we found a good path for where we give you the

09:51.360 --> 09:58.680
learnings, like enforce the learnings we have had before, in how we ask you to write

09:58.680 --> 10:04.000
your code or deliver your models or did the processing tasks or whatever.

10:04.000 --> 10:10.520
At the same time, leave it open for a variety of different workloads that can run the system.

10:10.520 --> 10:17.520
And there we are, I'm very proud with how the product turned out to be in the launch and

10:17.520 --> 10:21.000
the reception we have had during the conference.

10:21.000 --> 10:25.160
And just a shout out to the team, we wouldn't have been possible, because our like just crazy

10:25.160 --> 10:26.600
amounts of effort with the team.

10:26.600 --> 10:29.040
It's an amazing team at Live.

10:29.040 --> 10:34.800
And we are proud of all our users also at Live, just they have stayed with us through

10:34.800 --> 10:38.600
bad times, good times, and thank you for all the support.

10:38.600 --> 10:39.600
That's awesome.

10:39.600 --> 10:44.880
So, Kate, then you've given us a little bit of an overview of flight.

10:44.880 --> 10:51.680
Maybe take a step back, and what's the core value proposition that flight is offering

10:51.680 --> 10:58.680
and how do you mention that it's Kubernetes native, like how does it relate to Qflow?

10:58.680 --> 10:59.680
Yeah, for example.

10:59.680 --> 11:00.680
Good question.

11:00.680 --> 11:06.560
Let me start with the motivation of like, or what is it that we think is missing and

11:06.560 --> 11:08.480
what we were trying to address?

11:08.480 --> 11:12.840
One of the things, as I said, we started in 2017, so the landscape was very different

11:12.840 --> 11:13.840
at that point, right?

11:13.840 --> 11:16.000
So we evolved from that point.

11:16.000 --> 11:22.320
And this is a V2, even though actually I think this is the real V1, but this is a V2,

11:22.320 --> 11:26.560
so that means we went through a process of like actually making something and failing

11:26.560 --> 11:30.160
and then redoing it, that has a lot of learnings with it.

11:30.160 --> 11:35.160
So one of the learnings is that we feel that there is this artificial divide that's happening

11:35.160 --> 11:42.000
between ML and data, but actually they go hand in hand, like you, it's not that these

11:42.000 --> 11:47.240
companies have amazing data systems, they're not the Google's Facebooks or the Amazon's

11:47.240 --> 11:48.240
of the world, right?

11:48.240 --> 11:53.360
They are smaller companies, nimble, and they want, they are basically building their data

11:53.360 --> 11:55.120
stack too.

11:55.120 --> 12:02.960
So, and the other thing that we realize is there are teams cross-collaborate quite a bit.

12:02.960 --> 12:07.920
Learning models are built, let's say, by A team, but the team B probably provides the

12:07.920 --> 12:10.960
data that builds that machine learning model.

12:10.960 --> 12:19.240
And actually the fallacy of separating them is that many times in production, we use machine

12:19.240 --> 12:24.680
learning models to predict, and that creates data that becomes a fact in the fact tables

12:24.680 --> 12:26.760
in the data world.

12:26.760 --> 12:30.840
And then many times you use machine learning models to convert that fact to a dimension,

12:30.840 --> 12:31.840
which trains other models.

12:31.840 --> 12:36.920
So there is this cyclic nature that's happening, and this needs to be captured at that granularity

12:36.920 --> 12:42.840
of saying that there is data and processing and machine learning all interacting together.

12:42.840 --> 12:47.640
So, that was the motivation behind flight that we need a single tool and a platform that

12:47.640 --> 12:57.960
allows for collaborating, sharing, and MLOPS along with definite focus on orchestration,

12:57.960 --> 13:04.320
and that's why the core of flight is a workflow engine that actually runs all of these pipelines.

13:04.320 --> 13:08.520
But from the idea point of view, it was built for collaboration and sharing across the

13:08.520 --> 13:15.080
company various aspects, as well as processing and machine learning on the same tool.

13:15.080 --> 13:19.560
And so maybe to make that more concrete, we can kind of compare contrast to what Q flow

13:19.560 --> 13:20.880
is trying to do.

13:20.880 --> 13:21.880
Right.

13:21.880 --> 13:29.480
Are they orthogonal, are they complementary, does flight use Q flow?

13:29.480 --> 13:32.400
That's a great question.

13:32.400 --> 13:40.680
So Q flow started a while ago, started as one thing, initially they had just the TensorFlow

13:40.680 --> 13:42.800
operator.

13:42.800 --> 13:49.920
And as the product started maturing, it became not just a product, it became a collection

13:49.920 --> 13:57.720
of products under the name Q flow, Q flow serving, Q flow pipelines, TensorFlow became its

13:57.720 --> 14:01.520
own thing, you can use it from its own and so on and so forth.

14:01.520 --> 14:08.120
So we don't see the comparison between flight and Q flow as a collection of tools or products

14:08.120 --> 14:12.880
that the fear comparison I would say is between flight and Q flow pipelines, which is like

14:12.880 --> 14:21.920
one segment of Q flow that sits on its own, in a way, and underlying engine, the workflow

14:21.920 --> 14:26.280
engine under Q flow pipelines is not actually a reason why Google, it's an open source

14:26.280 --> 14:29.240
product, a different product, right.

14:29.240 --> 14:34.600
So it's also swapable that way, as it's, you can think of it as like a puzzle kind

14:34.600 --> 14:37.840
of thing and these are just a few pieces of that puzzle.

14:37.840 --> 14:44.000
And flight does things slightly differently for those few pieces that are comparable, if

14:44.000 --> 14:45.000
that makes sense.

14:45.000 --> 14:53.060
So then, like saying that flight might be a swapable alternative to Argo under Q flow

14:53.060 --> 14:54.060
pipelines.

14:54.060 --> 14:58.400
Actually, yeah, we might, it might be more vertical than that, but I'll give you an example

14:58.400 --> 15:04.840
like a Q flow that is PyTarge and TensorFlow to absolutely opinionated distributed or

15:04.840 --> 15:09.080
not distributed or deep learning frameworks, they both exist, right.

15:09.080 --> 15:13.280
And it should be the user's choice of what they want to use.

15:13.280 --> 15:19.160
And that's how I feel about how flight and let's take you flow pipelines work.

15:19.160 --> 15:24.960
They might, they could interrelate and that absolutely, it's the number best interest

15:24.960 --> 15:29.440
to make all of these tools play very well with each other, but they could be like completely

15:29.440 --> 15:32.640
vertically available as two alternatives.

15:32.640 --> 15:38.920
And you could use Q flow serving to serve your models, but build those models on flight.

15:38.920 --> 15:43.200
This flight offers a great abstraction on, on compute and it gives you big data with

15:43.200 --> 15:44.200
it, right.

15:44.200 --> 15:49.560
So that's essentially our differentiator where we think like Q flow pipelines doesn't even

15:49.560 --> 15:50.560
try to do that.

15:50.560 --> 15:55.040
And then the lineage and the cataloging that we do is further on built on top of it, which

15:55.040 --> 15:58.440
is also the other bit that we should talk about, I guess later.

15:58.440 --> 16:05.880
Well, you mentioned that one of the aspects of flight is that it's strongly typed, makes

16:05.880 --> 16:12.640
me think most immediately to like FB learner and kind of its approach to typing and you're

16:12.640 --> 16:16.400
nodding your heads, maybe they think, yeah, can you talk a little bit about that kind

16:16.400 --> 16:18.760
of design decision and implications?

16:18.760 --> 16:23.240
I can actually tell you the first time I saw the FB learner blog.

16:23.240 --> 16:29.960
It's uncanny, but it's also unbelievable at some level, so we looked at it and I shared

16:29.960 --> 16:30.960
it with the team.

16:30.960 --> 16:31.960
I'm like, did you see this?

16:31.960 --> 16:37.800
This looks exactly like R. And this was around the same, about 2017, 2016, that time.

16:37.800 --> 16:45.840
And we had the same like the annotation decorators in Python and I'm like, is it like they're

16:45.840 --> 16:52.120
people thinking about like us, but I think that yeah, that helped us, you know, they

16:52.120 --> 16:53.120
were earlier than us.

16:53.120 --> 17:01.760
I'm not saying that, but it's just that that kind of helped us believe like more in that

17:01.760 --> 17:11.200
we were probably on the right path, and so, yeah, so we, in the version one of flight,

17:11.200 --> 17:17.840
we did not have a very, we had a type system, but the type system only existed on the SDK,

17:17.840 --> 17:24.000
because we call it in the Python, where you, maybe take a step back for folks that, you

17:24.000 --> 17:28.760
know, here type system, or some only type, and there's no context for that.

17:28.760 --> 17:32.600
What does that mean from the perspective of the user experience for flight?

17:32.600 --> 17:39.720
Yeah, so let's tell you right, a function in any language, you, you have some inputs

17:39.720 --> 17:42.840
and outputs to that function.

17:42.840 --> 17:46.760
There are languages like Python, where you do specify inputs and outputs, but you don't

17:46.760 --> 17:49.400
know what those types are, barring the new typing.

17:49.400 --> 17:53.400
Whether they're strings, or integers, or fields, or whatever, or more complex things.

17:53.400 --> 17:55.400
Could be typing in there, and so on.

17:55.400 --> 17:59.440
But if you go to a language like Go, or Jow, or C++, it's a really strong type, you have

17:59.440 --> 18:05.520
to say this is an int, and this is a string, and that's the order and whatever, right?

18:05.520 --> 18:09.400
There are benefits of having types in the system, and that's why people love them to

18:09.400 --> 18:13.080
use them in programming languages, because you get compiled time safety.

18:13.080 --> 18:18.960
When you build the function, if you pass in an int, and try to use it like a string, you

18:18.960 --> 18:23.560
get an error at compile time, so you don't have to wait for it to run for 10 days, and

18:23.560 --> 18:29.040
then figure out, oh shit, but in Python, actually, there was a, there's more of a movement

18:29.040 --> 18:31.840
to add these kind of typing, compile time type safety.

18:31.840 --> 18:37.320
We think that that is the same thing that should be done for, why not do it for all functions.

18:37.320 --> 18:41.960
Like in our RPC systems today, like services, microservices, and people are using, we have

18:41.960 --> 18:47.400
types, and we have APIs, and people talk to them, and then they receive outputs.

18:47.400 --> 18:52.720
In flight, that's how we've designed, every single function is a task, we call them,

18:52.720 --> 18:58.400
and these have inputs and outputs, and by strong typing, I mean, these inputs and outputs

18:58.400 --> 19:02.920
have a known set of types, but what are the known set of types that you need to use with

19:02.920 --> 19:03.920
machine learning?

19:03.920 --> 19:08.040
So we had to come up with a type system that allows you to specify the various types of

19:08.040 --> 19:13.840
types that users use when they are pilling models, like an example could be a structured

19:13.840 --> 19:22.800
schema, which is like a row vector, or could be a tensor, could be a blob, which could

19:22.800 --> 19:28.240
be a serialized model, and it could be various formats, could be on X, could be, and those

19:28.240 --> 19:33.360
could be annotations on top of that blob saying that this is a serialized with the TensorFlow

19:33.360 --> 19:35.160
serialization format.

19:35.160 --> 19:40.240
So that's the type system that we are referring to.

19:40.240 --> 19:46.120
So when you declare a task, if it's a model building task, and you use joblib to serialize

19:46.120 --> 19:51.640
it, you can output a model that says it is joblib.dat, and then the next task that actually

19:51.640 --> 19:58.360
consumes it knows that this is a type joblib, so I can just load it into using joblib.

19:58.360 --> 20:01.960
And if you try to now put these two tasks together, they will work, but if you try to

20:01.960 --> 20:07.080
put a task that does not understand joblib, only uses TensorFlow, it will fail at compile

20:07.080 --> 20:08.440
time.

20:08.440 --> 20:13.640
And that's what we wanted to achieve in a pipeline, try to fail earlier if possible.

20:13.640 --> 20:20.200
And one of the things that that enables for FB Learner cases that they can take all of

20:20.200 --> 20:25.480
these tasks that are strongly typed, kind of created dependency graph, and then execute

20:25.480 --> 20:30.840
them in parallel, like as one completes spin off its descendants, are you doing some more

20:30.840 --> 20:31.840
things?

20:31.840 --> 20:32.840
Exactly.

20:32.840 --> 20:38.960
I just remember something you wanted to add to the timing system, so one of the decisions

20:38.960 --> 20:45.760
we made there was using Protobov to declare the, to specify the types, and then that might

20:45.760 --> 20:51.440
be one of the distinctions between the existing systems you might see, even like FB Learner

20:51.440 --> 20:53.560
and others, and flight.

20:53.560 --> 20:58.720
And that makes it not, like takes it one step further, it doesn't only allow you to put

20:58.720 --> 21:06.400
Python tasks together, because after you declare the task in whatever language you choose to,

21:06.400 --> 21:11.440
using our spec, you can, you know, let's say right go, function becomes one step in your

21:11.440 --> 21:16.840
graph, the next step might be a Python task in a different container or, you know, what

21:16.840 --> 21:17.840
you have you.

21:17.840 --> 21:22.960
Because at the end of the day, they would compile to this standard spec that has standard

21:22.960 --> 21:27.840
set of types that are compatible with, you know, our SDK in any language.

21:27.840 --> 21:33.680
And yes, I guess back to your question, it does allow us to, we once you declare the graph

21:33.680 --> 21:41.320
in this spec, we have a compiler that looks at the graph and figures out dependency graph,

21:41.320 --> 21:48.040
and we can go as parallel as we, as the system would want, you know, pairing the dependencies

21:48.040 --> 21:50.560
and all of that into accounts.

21:50.560 --> 21:51.560
Yeah.

21:51.560 --> 21:56.200
And there are other advantages, like what I think, I just wanted to add one more thing is

21:56.200 --> 22:01.680
the reason why we did this is because we saw use cases within Lyft, where users were

22:01.680 --> 22:07.840
writing models in Scala for spark processing, right, like they just want to do spark processing

22:07.840 --> 22:08.840
Scala.

22:08.840 --> 22:09.840
They don't want to write this in Python.

22:09.840 --> 22:14.400
They do sometimes, but sometimes they want to use Scala for higher performance.

22:14.400 --> 22:18.600
And we are like, how do we move the data between these two different languages?

22:18.600 --> 22:23.520
And so that's why we came up with, so we also using arrow underneath, this is an open

22:23.520 --> 22:25.320
search project.

22:25.320 --> 22:30.640
But like we wanted to create a layer on top of it, so that it's easy to construct these

22:30.640 --> 22:34.720
polyglot pipelines, if I may.

22:34.720 --> 22:41.440
So we're talking about stuff like type systems and protobuffs and arrow and not language

22:41.440 --> 22:48.360
that the typical data scientist is about a lot, yeah, is it, you know, have you built

22:48.360 --> 22:52.840
abstractions that make it more acceptable to data scientists or do you just have a different

22:52.840 --> 22:53.840
audience?

22:53.840 --> 22:57.040
I think that's a very good question.

22:57.040 --> 23:01.320
And actually sometimes we do debate amongst ourselves, what's our audience, right?

23:01.320 --> 23:11.720
But we do want to appear in the beginning, at least, to the savvy engineer like us who

23:11.720 --> 23:14.720
wants to get his hands a little dirty.

23:14.720 --> 23:19.200
But as we are progressing, we are creating layers on top of them.

23:19.200 --> 23:25.160
So think about it, we are like, we built the foundation and now building layers on top

23:25.160 --> 23:26.160
is much easier.

23:26.160 --> 23:30.000
So for example, like we're just about to merge full notebook support, so you can write

23:30.000 --> 23:31.320
tasks in notebooks.

23:31.320 --> 23:34.240
So like we're just going to add like support for Jupyter notebooks.

23:34.240 --> 23:39.080
The way and the way we are thinking about Jupyter notebooks is users like data scientists

23:39.080 --> 23:41.800
and researchers love to use Jupyter notebooks.

23:41.800 --> 23:47.600
They write code in Jupyter notebooks in a very different way than how engineers write

23:47.600 --> 23:49.520
code on like using IDs, right?

23:49.520 --> 23:50.520
It's very different.

23:50.520 --> 23:53.720
And we want to keep and preserve that semantic of writing code.

23:53.720 --> 23:57.520
So we actually found a project called Paper Mill, which is pretty cool.

23:57.520 --> 23:58.520
We like that.

23:58.520 --> 23:59.520
That's pretty cool.

23:59.520 --> 24:02.960
There's only out of Netflix now in a background independent company.

24:02.960 --> 24:03.960
Yes.

24:03.960 --> 24:07.400
And so we decided to adopt it.

24:07.400 --> 24:14.000
And we take that and we convert a notebook into a function into our system with the

24:14.000 --> 24:15.000
same inputs and output.

24:15.000 --> 24:17.680
We do the magic of like passing the inputs and outputs.

24:17.680 --> 24:20.840
You just write your code as if you're writing a regular notebook.

24:20.840 --> 24:26.960
You can try it out to whatever, drop it into a container, give it to flight.

24:26.960 --> 24:30.560
And then it'll take care and execute it and actually record the output notebook and

24:30.560 --> 24:31.720
store it for you.

24:31.720 --> 24:35.280
So you can go back in time and look at it even if you want to.

24:35.280 --> 24:44.040
So yes, our audience is was engineers in the beginning because we do have that sort of

24:44.040 --> 24:49.680
audience at lift, but we are, we also have like data scientists and research scientists

24:49.680 --> 24:54.280
using more and more and we are improving every day for them.

24:54.280 --> 24:58.240
So if you start today, you might look at the Python flight kit and see that it's a little

24:58.240 --> 25:06.000
more suited for people who write with IDs, but with the notebook introduction and our demo

25:06.000 --> 25:08.680
we showed like how to use everything with the notebook.

25:08.680 --> 25:11.040
We are going more towards research scientists.

25:11.040 --> 25:14.080
And they will not even see some of the things that we do, like for example, I'll give an

25:14.080 --> 25:19.440
example, in a task, you can return a data frame, a pandas data frame and we understand

25:19.440 --> 25:24.240
that a pandas data frame is actually the throw vector that we do underneath which is converted

25:24.240 --> 25:27.400
to a prototype and arrow and it's just sent through.

25:27.400 --> 25:28.680
You don't even have to think about it.

25:28.680 --> 25:34.000
You just work with pandas data frame and hopefully we are thinking we're not yet implemented

25:34.000 --> 25:37.600
this, but that goes into spark data frame and the other side.

25:37.600 --> 25:41.440
So we created the abstraction layer for that.

25:41.440 --> 25:43.200
Does that introduce a lot of latency?

25:43.200 --> 25:48.880
I was having a conversation with someone who was talking about reason why they don't

25:48.880 --> 25:54.200
use TF serving is because they're primarily doing inferencing on images and it requires

25:54.200 --> 25:58.360
that you have to put everything into a data frame and there's a bunch of latency that

25:58.360 --> 25:59.880
that introduces.

25:59.880 --> 26:07.280
Do you run into these kinds of issues where your abstraction hides kind of some nimbleness

26:07.280 --> 26:12.800
and what the underlying data format is and you're doing conversions underneath the covers?

26:12.800 --> 26:19.920
So actually, arrow in that case is an extremely interesting project, according to me, aim

26:19.920 --> 26:29.000
is to make zero copy abstractions from one format to pandas data frame, to spark data

26:29.000 --> 26:36.720
frames and I think the founder is, I think it's Wes, who the guy who has been following

26:36.720 --> 26:43.640
that project and I think that's more of that is required, where we just should stop wasting

26:43.640 --> 26:49.280
CPU cycles on transforming data from one format or one language to another and more use

26:49.280 --> 26:54.520
the zero copy abstractions that we can create and that just makes the open source ecosystem

26:54.520 --> 26:55.920
much nicer.

26:55.920 --> 27:00.360
And that's why we chose arrow, but you don't have to, for example, if you're emitting

27:00.360 --> 27:03.960
out images, you don't have to have a data frame for it in flight.

27:03.960 --> 27:08.880
You can just say, I am emitting a directory of million images and we will take the million

27:08.880 --> 27:13.600
images and upload them and download them onto the other machines and use them.

27:13.600 --> 27:18.560
So yeah, so there is, you can use data frames, but you don't have to, and that's where

27:18.560 --> 27:20.440
the type system comes in.

27:20.440 --> 27:23.040
It needs to be more granular then.

27:23.040 --> 27:24.040
Okay.

27:24.040 --> 27:28.680
So we've talked a bunch about the type system and the workflow that that enables and kind

27:28.680 --> 27:36.840
of some of the user experience in introducing flight you mentioned.

27:36.840 --> 27:44.480
This important idea of kind of connecting back to the data and, you know, enabling things

27:44.480 --> 27:49.400
like end-to-end data providence and this, you know, kind of loop that you pointed out

27:49.400 --> 27:54.880
where your inference actually is, you know, data for your next train or a future train

27:54.880 --> 27:59.280
at least, talk more about kind of how that works like I'm thinking, the thing that comes

27:59.280 --> 28:06.680
to mind as I'm hearing that is Airbnb has a project that's kind of an adjacent project

28:06.680 --> 28:13.520
to their big head platform that is focused on, like doing, you know, point in time feature,

28:13.520 --> 28:19.680
mapping and management and feature repository, that kind of thing, is that the kind of thing

28:19.680 --> 28:20.680
we're talking about?

28:20.680 --> 28:25.000
That could be one of the things, but it's not like exactly what we're talking about,

28:25.000 --> 28:30.640
but like I think your, it's feature service is, well, let's call it like a generic name,

28:30.640 --> 28:31.640
feature service.

28:31.640 --> 28:33.160
Features as a service, right?

28:33.160 --> 28:40.000
What is an implementation on top of this potentially that allows you to pass the, like,

28:40.000 --> 28:45.720
consume the data back into the model and also build and send it to feature service.

28:45.720 --> 28:53.440
What we are referring to is the causal dependencies between the compute and the actual production

28:53.440 --> 28:58.920
of that data and then further consumption of data by the next compute layer and production.

28:58.920 --> 29:01.840
So now let's take an example.

29:01.840 --> 29:09.240
You get a map from OSM, OpenTreatMaps, if we consume it and you build a graph, the road

29:09.240 --> 29:14.560
network out of it, then some other team actually analyzes the road network in real time that's

29:14.560 --> 29:20.080
happening and creates the traffic pattern that's that are happening at the moment on the

29:20.080 --> 29:25.760
road and annotates the road network with some speed profiles, that's what we call them.

29:25.760 --> 29:29.640
And then the third team actually consumes these two things and creates the final road network

29:29.640 --> 29:31.880
that's deployed to production.

29:31.880 --> 29:37.000
So when we did a prediction on ETA, we would want to know which version of the map did

29:37.000 --> 29:42.120
I use and what was the speed profile at that point in time and what were the traffic

29:42.120 --> 29:45.320
conditions that were led to that speed profile?

29:45.320 --> 29:47.080
That's the type of question that we want to answer.

29:47.080 --> 29:54.440
So to go to that, you need to have a full trace in the system of how the data was generated,

29:54.440 --> 29:59.200
when did it get generated, when did it get passed under the next bit and then so we call

29:59.200 --> 30:03.040
this typically lineage or prominence as you said.

30:03.040 --> 30:07.400
And the way we track this is our type system was the other reason why we had our type system

30:07.400 --> 30:16.200
is to have the central engine automatically publish all of this data as it's generated

30:16.200 --> 30:20.960
by every single task execution to a central service that actually just records a unique

30:20.960 --> 30:27.240
signature of the execution and what did it generate with the inputs and the outputs.

30:27.240 --> 30:32.000
So now you have a relationship between what got generated by what and now you can create

30:32.000 --> 30:37.480
a graph because you know the graph that ran like flight knows intrinsically that how what

30:37.480 --> 30:39.240
was the computation graph.

30:39.240 --> 30:46.480
So you can go in and create the causal dependency structure across these data sets.

30:46.480 --> 30:52.280
We do this today, the exposed thing that you get in open source is not the full, we don't

30:52.280 --> 30:56.440
show the dependency tree that will come out soon at some point, but we are using this

30:56.440 --> 31:06.920
for memorization. So if you recompute the same data set, let's say you took some data set

31:06.920 --> 31:12.080
and you transformed it and it produced an output.

31:12.080 --> 31:16.080
Some other user goes in and takes the same data set and transforms it and produces the

31:16.080 --> 31:20.400
same output potentially because the code really has not changed.

31:20.400 --> 31:23.080
In the past people were like we would spend money.

31:23.080 --> 31:31.720
So then the solution would be let's create a intermediate high table or whatever, right?

31:31.720 --> 31:32.720
Things like that.

31:32.720 --> 31:38.040
But that's not really what it was for many artifacts that are like images, right?

31:38.040 --> 31:41.480
If you did luminon sampling on some set of images and you're not going to store them

31:41.480 --> 31:42.480
in high.

31:42.480 --> 31:45.520
I think we can do that, but we don't.

31:45.520 --> 31:51.680
But what we do is because we can identify the compute process and the set of inputs that

31:51.680 --> 31:54.320
were given, we create a unique signature of that.

31:54.320 --> 31:58.040
And so next time when we observe that the same thing is being run, flight just smartly

31:58.040 --> 32:00.200
replaces it with the existing outputs.

32:00.200 --> 32:05.720
Now you have to tell flight that this process is reproducible because if it has side effects

32:05.720 --> 32:09.280
or it is like, you know, if you're using random number, the generator or something, that

32:09.280 --> 32:10.280
might not work.

32:10.280 --> 32:16.120
So we, you have to tell us, but if you tell us then we just go and replace any execution

32:16.120 --> 32:18.720
of that on the platform across the company.

32:18.720 --> 32:24.480
So that saves money and time because the iteration time now, like if you're running something

32:24.480 --> 32:30.320
that's 10 steps, like a 10 step pipeline and you fail on the 10th step, you just fix

32:30.320 --> 32:31.960
the 10th step and rerun it.

32:31.960 --> 32:32.960
Right.

32:32.960 --> 32:33.960
And the 9 steps just are cached.

32:33.960 --> 32:38.480
So you just automatically fright will fast forward you to the 10 step and go, go, let's

32:38.480 --> 32:40.280
run that guy again.

32:40.280 --> 32:42.720
And so this is what we call it, memorization.

32:42.720 --> 32:43.720
So flight.

32:43.720 --> 32:50.760
I think you've described it as kind of a workflow engine, it's not a data store or something

32:50.760 --> 32:55.000
that is like creating snapshots or anything, it's more like, you can think of it more

32:55.000 --> 33:01.200
like metadata and pointers to, you know, existing training data and interim data, transform

33:01.200 --> 33:03.160
data, output data, et cetera.

33:03.160 --> 33:04.160
Exactly.

33:04.160 --> 33:05.160
Yeah.

33:05.160 --> 33:10.240
And or presumably you're also kind of tracking model versions as they're trained.

33:10.240 --> 33:11.240
Yes.

33:11.240 --> 33:20.480
Yeah, we have, that's one of the things we are a bit opinionated about flight is every

33:20.480 --> 33:26.560
artifact in the system, all the data produced, all the definitions, all the tasks and workflows

33:26.560 --> 33:28.560
are immutable.

33:28.560 --> 33:34.680
So we have versions, strict versions, versioning scheme that you can use your own versioning

33:34.680 --> 33:42.760
scheme, but it has to be strict as in you can't mutate something and try to register it

33:42.760 --> 33:45.680
or produce it with the same version again.

33:45.680 --> 33:52.400
So we, when we produce any metadata about tasks, outputs or whatnot, they are always unique

33:52.400 --> 33:55.400
and like the signature is always unique.

33:55.400 --> 34:01.160
And that allows us throughout the system to always refer to very consistently to like

34:01.160 --> 34:06.120
executions, past executions in the history and the produced artifacts and produced models

34:06.120 --> 34:12.280
and, you know, anything that went through the system with very confidence that, you know,

34:12.280 --> 34:18.000
we know exactly which even like piece of code produced that.

34:18.000 --> 34:19.000
So yeah.

34:19.000 --> 34:20.000
Yeah.

34:20.000 --> 34:24.800
And you should be able to fully re-run, re-produce it, but it'll cause a new version.

34:24.800 --> 34:30.040
We don't even have one single update API in the entire code list, because you have like

34:30.040 --> 34:33.280
a functional system.

34:33.280 --> 34:38.440
What's the smallest kind of use case that you can envision someone using this for?

34:38.440 --> 34:45.280
Does it make sense and is it kind of approachable enough for kind of a, you know, a single person,

34:45.280 --> 34:48.880
a kind of clone or repo and like actually get some value out of it or do you need a team

34:48.880 --> 34:56.080
of, you know, 10 MLN for people or that's a great question, great question, yes.

34:56.080 --> 35:02.240
We, and I will say like our work there is not done, but we have put a lot of effort to

35:02.240 --> 35:11.880
make the first time experience and the maintenance for small projects, as you mentioned, very

35:11.880 --> 35:13.560
approachable.

35:13.560 --> 35:19.880
We have written, we have written docs that, like I would say maybe most of the time, like

35:19.880 --> 35:24.480
the efforts we put in the docs, we're in the docs that tell people how to get started.

35:24.480 --> 35:29.480
So it's like me that easy, even sometimes we went back to like our architecture and what

35:29.480 --> 35:33.120
not to try to, you know, make that easy when like we looked at the docs and it's like that's

35:33.120 --> 35:34.320
a lot of steps.

35:34.320 --> 35:35.320
This is not okay.

35:35.320 --> 35:38.320
Let's go back and redo things, right?

35:38.320 --> 35:39.680
So that's one part of it.

35:39.680 --> 35:45.680
I would say the other part I think is we have seen that in a lot of cases, people when

35:45.680 --> 35:52.240
they get started in developing a model or even doing some transformation or whatnot, they

35:52.240 --> 35:57.560
don't think initially, unless they have done that before, they don't think that they will

35:57.560 --> 36:00.000
need a workflow for it.

36:00.000 --> 36:05.640
They think you know, we'll spawn off a notebook and do my thing and be happy, right?

36:05.640 --> 36:09.640
Which usually is the case in the beginning, right?

36:09.640 --> 36:14.080
Until you know, somebody leaves the company or you started, like you want to go back to

36:14.080 --> 36:18.680
a model that worked, but you don't know which version of the code produced that and so

36:18.680 --> 36:24.120
on and then you start realizing the problems and the need for such systems.

36:24.120 --> 36:30.880
But by trying to make it as easy to get started as approachable, we hope that at one point

36:30.880 --> 36:38.520
it becomes like a standard given that you always start there and the system, like the friction

36:38.520 --> 36:47.560
between I have nothing to the first task or the first execution is almost not there.

36:47.560 --> 36:52.400
Like more people would get into the happy love doing this as the first step before I think

36:52.400 --> 36:57.720
your first piece of code and it sets you up for success, leader.

36:57.720 --> 37:02.000
We are basically saying that every company should grow and become great and you know, start

37:02.000 --> 37:07.640
with what we think is like the bare minimum and it will evolve with you, right?

37:07.640 --> 37:09.720
So it's an evolveable system.

37:09.720 --> 37:13.720
The other bit that I want to add to that is that I think that one of the reasons why we

37:13.720 --> 37:20.280
move to Kubernetes is to make that first case experience and for small company experience

37:20.280 --> 37:22.120
really, really good.

37:22.120 --> 37:23.720
Kubernetes is great to get.

37:23.720 --> 37:29.160
You can go to any of the clouds and get one Kubernetes cluster and we use customized,

37:29.160 --> 37:33.360
I don't know if you know of that project, but that's a pretty interesting project.

37:33.360 --> 37:40.400
All of flight is really one YAMO and you can say, CUP, CTO, apply minus F, that YAMO

37:40.400 --> 37:46.320
and boom, you get a flying cluster, including all of the things that we just talked about.

37:46.320 --> 37:47.320
Wow.

37:47.320 --> 37:48.320
Okay.

37:48.320 --> 37:52.480
Yeah, you can even do that on the Docker for this top or Kubernetes.

37:52.480 --> 37:56.720
Like on your machine, you don't even have to go to a CUP provider to get started and

37:56.720 --> 38:00.640
set it up even with menu for storage.

38:00.640 --> 38:06.160
So you will get the full experience, your manual tasks and once you're ready to take it

38:06.160 --> 38:15.600
to the next level, run on in AWS or GCP or whatnot, that you don't have to change how you

38:15.600 --> 38:18.480
were doing things or how you wrote your tasks.

38:18.480 --> 38:23.680
They will just seamlessly run on the bigger cloud within lift and we talked about this

38:23.680 --> 38:25.720
in the presentation yesterday.

38:25.720 --> 38:31.280
We run a multi cluster set of flights, a single cluster, but of course you will not start

38:31.280 --> 38:32.280
there.

38:32.280 --> 38:33.280
Right?

38:33.280 --> 38:34.280
And we did not start there.

38:34.280 --> 38:35.800
We started with single cluster.

38:35.800 --> 38:40.280
We soon outgrew that and started, you know, deploying multiple cluster to flight.

38:40.280 --> 38:42.560
And the user's multi cluster.

38:42.560 --> 38:48.040
What's the multi tenancy models like a single user has stuff running on multi clusters

38:48.040 --> 38:52.040
or you just have multi clusters and you have some users among cluster and some on another.

38:52.040 --> 38:56.400
We have multiple clusters and we for the user though, it's one cluster.

38:56.400 --> 39:02.640
So we abstract that entire thing behind the service and the way we spun off the work

39:02.640 --> 39:07.360
depends on the load of the system or priority classes or things like that.

39:07.360 --> 39:12.000
Is that all stuff that's happening in the open source or this way that you're operating

39:12.000 --> 39:13.000
it?

39:13.000 --> 39:14.000
No, it's all open source.

39:14.000 --> 39:15.000
Interesting.

39:15.000 --> 39:16.760
Yeah, you don't need to do this.

39:16.760 --> 39:20.280
You don't have to use multi cluster, but let's see your company grows further beyond

39:20.280 --> 39:21.280
that single cluster.

39:21.280 --> 39:22.680
Yes, flight will evolve with you.

39:22.680 --> 39:23.680
Yeah.

39:23.680 --> 39:24.680
Yeah.

39:24.680 --> 39:26.480
Yeah, we basically open source everything that we do at lift.

39:26.480 --> 39:27.480
Yeah.

39:27.480 --> 39:30.280
Some other thing that I just talked about like the lineage or whatever, it will come soon.

39:30.280 --> 39:31.280
Yeah.

39:31.280 --> 39:32.280
But most of the things.

39:32.280 --> 39:37.520
What's exciting about this for me is that there are many, many companies that are doing

39:37.520 --> 39:40.000
this kind of thing internally.

39:40.000 --> 39:46.480
We talked about Facebook, Airbnb, Uber, obviously with Michelangelo.

39:46.480 --> 39:54.840
Some have talked about open sourcing, but I'm not sure I can think of any name brand

39:54.840 --> 40:00.080
company that is open source their internal platform outside of like TFX if you consider

40:00.080 --> 40:02.560
that Google open sourcing their internal platform.

40:02.560 --> 40:03.560
Yeah.

40:03.560 --> 40:04.560
So this.

40:04.560 --> 40:06.840
The TFX doesn't come with an execution portion of it, right?

40:06.840 --> 40:08.800
It's just basically the library at the moment.

40:08.800 --> 40:09.800
Right.

40:09.800 --> 40:10.800
Right.

40:10.800 --> 40:11.800
Right.

40:11.800 --> 40:15.400
And so this is the library that does all the type stuff.

40:15.400 --> 40:21.720
There's the execution piece, workflow engine, you know, after the step after the CUBE

40:21.720 --> 40:25.920
CTL, like is it spinning up a web front end that I can see stuff?

40:25.920 --> 40:26.920
Yeah.

40:26.920 --> 40:27.920
Our web front end is pretty snappy too.

40:27.920 --> 40:30.640
We show like errors in the UI and graphs and things like that.

40:30.640 --> 40:35.360
You can click you get log links in the UI and like inputs and outputs and the artifacts

40:35.360 --> 40:36.360
produced.

40:36.360 --> 40:37.360
All of that is in the UI too.

40:37.360 --> 40:38.360
Yep.

40:38.360 --> 40:39.360
And then the CLI too, of course.

40:39.360 --> 40:43.840
And tell me a little bit about the like the process of open sourcing it.

40:43.840 --> 40:47.560
Like, was that the size of it all?

40:47.560 --> 40:48.560
Yeah.

40:48.560 --> 40:49.560
All right.

40:49.560 --> 40:59.720
So I had a baby five months ago, five months ago, and I had another baby in a flight.

40:59.720 --> 41:04.440
So they think having literally having two babies at the same time.

41:04.440 --> 41:11.280
It's a twin, a twin, and I have not slept almost for five months probably.

41:11.280 --> 41:14.320
And so as the team, like, been a fantastic job.

41:14.320 --> 41:18.080
So yeah, that's why we took this long, actually.

41:18.080 --> 41:21.640
We started the process last year.

41:21.640 --> 41:22.640
We could have been.

41:22.640 --> 41:24.640
I mean, let's start with why?

41:24.640 --> 41:25.640
Very good question.

41:25.640 --> 41:27.440
They have to like, why?

41:27.440 --> 41:29.280
And we did debate that a lot, didn't they?

41:29.280 --> 41:30.280
We did a lot, right?

41:30.280 --> 41:31.280
Yeah.

41:31.280 --> 41:32.280
Especially that, right?

41:32.280 --> 41:36.800
The previous system wasn't going to be open sourced, was sift only.

41:36.800 --> 41:41.960
So like, when this came, this, when the new, when you started deciding to redo it, I

41:41.960 --> 41:45.400
remember, even hit them giant and when I was one of the question, why do you want to open

41:45.400 --> 41:46.400
sourcing?

41:46.400 --> 41:52.600
And I think what happened is there was interest from outside was one of the reasons, I don't

41:52.600 --> 41:56.400
need to name, like who, where it was, but there was some interest.

41:56.400 --> 42:02.720
The other thing is we realized this is such a big, like, big problem to solve.

42:02.720 --> 42:09.320
And I said this in the, I think, before, but a small 9 or 10 people team cannot do this.

42:09.320 --> 42:16.120
It needs to be an industry wide effort, hopefully, if not, at least like a few, few tens

42:16.120 --> 42:19.160
of people working on this.

42:19.160 --> 42:26.640
And, and all with like, if we do set the right primitives, then we can let it evolve into

42:26.640 --> 42:29.080
the, into the piece that it needs to be, right?

42:29.080 --> 42:34.600
And that gets great stuff for life, like if we, we get, for example, we don't, we currently

42:34.600 --> 42:39.280
only have a Python SDK, but I know there are other companies are saying that we want

42:39.280 --> 42:43.640
to add a scala SDK for this and we're like, awesome, we will use it, right?

42:43.640 --> 42:48.440
There is demand for this at least, but we don't have the time to build this.

42:48.440 --> 42:53.840
So that's one, a definite biggest reason that get, basically, leverage.

42:53.840 --> 43:01.400
Second is lift open source on why it was a foundational technology for lift two.

43:01.400 --> 43:06.560
And it's been, it's been, you know, amazing success, right?

43:06.560 --> 43:08.560
We're in this community.

43:08.560 --> 43:13.400
So we're not saying we are going to be even 10% of that weight, that's just much.

43:13.400 --> 43:19.000
But from that we learned that people actually, like, it's a great hiring tool.

43:19.000 --> 43:25.040
You are not working on a technology in the company that's, that now you have to hire

43:25.040 --> 43:27.840
engineers and teach them.

43:27.840 --> 43:31.000
You are going to work on a technology that they have probably used at their previous jobs

43:31.000 --> 43:33.640
and that's great for lift.

43:33.640 --> 43:38.600
So, and, yeah, answer and just wait for the team potential, right?

43:38.600 --> 43:42.280
So, and I would like to just add to all of this.

43:42.280 --> 43:50.680
I think we, as we talked throughout the conversation today, there are a few things we strongly believe

43:50.680 --> 43:52.480
in.

43:52.480 --> 43:57.560
And we wanted to have that debate in the open, because I think it will not only influence

43:57.560 --> 44:01.880
like this product or similar products, it will influence like the entire ecosystem,

44:01.880 --> 44:07.000
how it, how it interacts with each other, how, how you do serving even after like the

44:07.000 --> 44:13.600
all of that, even if you don't build these pieces, the concepts, like the underlying concepts.

44:13.600 --> 44:19.200
And I think it, like, we, we see that it does bring something to the table that isn't there

44:19.200 --> 44:20.200
yet.

44:20.200 --> 44:25.280
So, we wanted to make sure that, you know, we have that conversation, like it, it will,

44:25.280 --> 44:32.800
we think, advance like the entire, like the email, link for a community overall, to hopefully

44:32.800 --> 44:34.640
a slightly better place.

44:34.640 --> 44:39.000
Well, hi, Thom Ketan, thanks so much for taking the time to chat with us.

44:39.000 --> 44:40.000
Thank you, Sam.

44:40.000 --> 44:41.000
Thanks for having us.

44:41.000 --> 44:42.000
Yeah.

44:42.000 --> 44:43.000
Yeah.

44:43.000 --> 44:44.000
All right, everyone.

44:44.000 --> 44:49.520
That's our show for today to learn more about today's guests or the topics mentioned

44:49.520 --> 44:54.400
in the interview, visit twomelai.com slash shows.

44:54.400 --> 44:58.880
For more information on either of our new study group offerings, causal modeling and machine

44:58.880 --> 45:07.200
learning or the IBM Enterprise AI workflow, visit twomelai.com slash learn 2020.

45:07.200 --> 45:11.960
Of course, if you like what you hear on the podcast, be sure to subscribe, rate, and review

45:11.960 --> 45:14.520
the show on your favorite pod catcher.

45:14.520 --> 45:31.680
Thanks so much for listening and catch you next time.

