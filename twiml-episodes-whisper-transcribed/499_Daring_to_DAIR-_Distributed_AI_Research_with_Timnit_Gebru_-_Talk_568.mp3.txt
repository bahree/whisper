Like I said, your experience teaches you a lot more than what anybody else writes or says.
Is that if you don't have the right institution and the right structure,
there's just no way that you can do things fairly.
All right, everyone. Welcome to another episode of the Twomo AI podcast.
I am, of course, your host, Sam Charrington. And today, I'm joined by a very special guest,
none other than Timnett Gebru, founder and executive director of Dare, the Distributed Artificial
Intelligence Research Institute, and of course, a great friend of the show. Before we dive into
today's conversation, be sure to take a moment to head over to Apple Podcast or your listening
platform of choice. And if you enjoy the show, please leave us a five star rating and review.
Timnett, it is wonderful to have you back on the show. It has been a bit.
I think this is actually your fourth time ish because you did a meetup that you probably don't
remember back in January 17 about your Google Street View work. And then your first time on
the show is in January of 18 episode number 88. We're probably at five 88 or something like that now.
And of course, you helped us cover trends in fairness and AI ethics in January of 20, kind of
looking back on 19. Wow, it's been a long two and a half years. Why don't we get started by
having you share a little bit about what's been going on for you? Welcome back.
Yeah, I can't even, it's really interesting being back, you know, because I remember our first
black night workshop, you all had, you were at like a hotel room, you had a whole set off. It was just
like, it just feels like such a long time ago. Yeah, that was long beach. Yeah, yeah, yeah.
And it's very interesting. It's kind of like chronicling a journey, you know, every time I come back
here. Well, I have to say that, you know, right now, I'm focused on dare, as you mentioned.
And I'm trying to take the time to calm down a little bit. And also think about, you know,
just take, take a step back. So one of the things I wanted to do was think about, you know,
there are all of these issues that we're talking about, right fairness, ethics, labor issues, etc.
And but what does the right model for doing things look like, right? What does the right
institute look like? What do the right incentive structures look like? How should we approach
the way we do research and what we build, what we don't build? And I am just kind of trying to
take the time to figure those out at this right now with there. Is it fair to ask you to give a 30,000
foot 30-second overview of your recent experiences to get some at Google to help folks get some
context if they've not heard any of the previous, where do we start? Well, so well, I got fired from
Google or as some of my former teammates have called it, actually Sami Bianjo. He coined the term
being resonated. He was like, in French, he said in French, you know, you have this word where like
someone resigns you. And so like they call it being resonated. So I was resonated from Google. And
it was a whole, to be honest with you, I still have not processed it because I don't, you know,
it was in the middle of a pandemic, in the middle of, you know, a war that just started in Ethiopia,
the most horrible war I have ever seen that is not really being talked about, that also gets us,
has gotten me to see all of the issues on social media. And in a way that I've never seen before,
you know, people talk about these issues. And it's like you never learn about it as much as when
you experience it. And so in the middle of that whole thing, and I wrote, you know, this paper on
the dangers of large language models. And the way this actually happened, believe it or not,
was not because I wanted to write a paper. But I saw that people at Google were basically saying,
you know, why are we not the leaders in like large language models? You know, this is, we should
be the ones doing these giant models. And you know, you see this race, just people are so fixated
on having larger and larger models. And I was, I was very worried about that because it seemed to
be this rush to a bigger thing without clarity on like why that bigger thing and also what are the
issues. And so I asked Emily Bender and I said, hey, you know, do you have papers on this that you've
written before that I can cite because right now I'm citing your tweets. And if I could cite a
paper that you've written that I can send to people because people are also internally at Google
asking me what are things we should worry about. And so she said, hey, why don't we write something
together? And I'm like, well, I don't know what I'd contribute, you know. And so then I, and we
each pulled in other people, I pulled in Meg and other people from that play team and we wrote this
paper. And honestly, I never thought it would be controversial. It was it, you know, I, I just
thought it was just going to be this paper. And that's it, right? I didn't think they would love,
I didn't think the Google people were going to be like super happy about it. But I didn't think
they were going to just, you know, do what they did, obviously. And so long story short, I found
myself basically disconnected from my corporate account in the middle of my supposed vacation.
And I found out from my direct report that I had apparently sent in my resignation. And that's
sort of a whole, you know, very, very stressful few months because then, you know, there was all
this harassment online. There was all of this, you know, you have to make sure you're safe.
There are literally like people from the dark web who made it a point, like a point to to
harass me come to all the talks I'm giving. And you know, just kind of harass anybody who is
coming to my defense, you know, a lot of other people found themselves writing documents,
having to talk to lawyers and things like that. People who don't even know me, by the way,
just because people just, you know, were coming to my defense on Twitter or something like that,
just because of that, I found myself being a thirst thrust into the public space. And so then that
also just that fact itself brings in more attention for more people. And then I was like really
worried about my team and what was going to happen to them. But then, you know, my co-lead
Mitchell was also fired. So it was a whole few months. It was a whole thing. And, you know,
that's what I mean, but I didn't have a chance to really process what has happened. In the midst of
that, of course, I was thinking, what is the next, what is the thing I could do next? Because I really,
you know, couldn't get myself to think about being at another large tech company and do that
fight again. I also know that I would not, there would be some companies that would be unwilling
to hire somebody like me after all of that. There's, you know, some members of my former team,
their office were rescinded from some places like after this publicity. And it's real, you know,
it really is real that people can, you know, by speaking up, just destroy their entire careers
and any options. But, you know, I had been thinking about creating a distributed independent
research institute. I didn't even think about like creating a university. Why can't we have a
distributed kind of different kind of, you know, I've been thinking about these things. But
if I hadn't been fired, probably what I would have done is slowly start something, you know,
maybe start something from on the side and grow it there, be very slowly, not like the way,
you know, we just started this. So anyhow, and after that, I decided to start there, the
distributed air research institute. That's awesome. And so what's the, how do you think about the
charter for dare? What's kind of in the, in the zone, in the scope versus out of scope?
Yeah. So, you know, dare is the air research institute, like, you know, like any other research
institute that you can think of. The thing that we are is we're an interdisciplinary research
institute. So, you know, Alexandra recently joined as our director of research. She's a sociologist.
And the distributed aspect was very important for me because I saw it even at Google in the
ethical AI team, you know, Meg was very good at retaining a distributed team. And, you know,
one of the last people we hired was Mahdi, who's a Moroccan. And he was raising the alarm
on social media, like no other person. And he was doing all this research, his friends were in
jail, their journalists. And I could see that nobody, you know, even the people in ethics or
whatever could not really grasp the gravity of the situation. And if you didn't have that person,
with that experience, there was no way you would, you would, you know, find out about that issue
and look into it, right? And that showed me the importance of, of having people, you know,
like that and not forcing them to move to Silicon Valley or whatever. I don't want to, you know,
I'm, what I'm thinking about is how not to consolidate power, right? Not how to, far,
there kind of contribute to the brain drain of different other locations. So, so that's why the
first word that came to my mind was distributed. And I called, you know, I told Eric Sears,
who's a program officer, a director at MacArthur, the MacArthur Foundation, I was like, hey,
look, you know, the first word that came to my mind is distributed. I want to call it dare,
like does it sound weird, you know? It's like, no, it's, it's cool. And so, so that's, that's dare.
And so when you say what's in scope versus out of scope, you know, that's honestly something
that we're still trying to figure out because it, I'd like it to be kind of a combination of,
of course, we have a few top-down directions, but I really feel strongly that it's, it's very
important to have a bottoms-up approach to research because you can't be the all-knowing person
who knows like what the next important thing is, right? So it's important to let other people
drive that too. But the thing we're focused on right now is, you know, what is our research
philosophy? And what, what do we care about, right? And so first of all, we care very much about
not exploiting people in the research process. One of the most, one of the things that is super
clear in research in general, and especially when you look at this field where you, you know,
there's a lot of knowledge that's extracted from people, a lot of data in different forms
that's extracted from people without compensation, without, you know, acknowledgement, etc, right?
Like you have that also in the fairness space. For instance, you have a group of researchers,
you know, they get tenure and they're ascending based on work on fairness or something.
And who are the subjects that they talk about? Oh, they'll talk about formerly incarcerated people
or people in prison currently. They'll talk about like different groups of people who are
harmed by this technology who are not, you know, getting the money, you know, for the research or
the fame or, or, you know, many times their lives are not changing because of this work, but they're
subjects of it, right? And so we're trying to figure out how do we not do that? You know, how do we
do the opposite of that? What does it mean to have research that that that incorporates
these people and actually is led by many times people like that? And how do you funnel resources?
And so one of our research fellows who just joined Meela is actually one of the things she's
doing is helping us figure that out, right? What is our research philosophy and how do we
operationalize it? So in terms of, you know, what's in scope and out of scope, so there's a self-selection
going on there where the people, you know, who do want to do research at their are people who
care about these kinds of things are somehow embedded in community building, not just, you know,
like research that has nothing to do with that. And, you know, like, for instance, if you want to work
on, you know, low, you know, so I'm, I'm, I'm advising on a workshop, which I had
coordinated before on practical machine learning and, you know, for developing countries or
practical machine learning in low resource scenarios. So if you want to, you know, kind of think
about what about, like, small data and small compute, right? Like that, I think you might want to
join, you know, we might want to think about working out there. But if you're interested in, like,
even larger models and even larger or something, then I don't understand what we would, you know,
provide in that sense. So that's kind of how I'm thinking about it right now. What I'm hearing in
part is that the, the areas that you've traditionally been working in and a researcher, ethics,
fairness, and that you're probably best known for, that is not necessarily a research focus for
Dare, but more like a undercurrent or a foundation. And Dare is going to be broader and encompass,
you know, like you said, all the things that another research institute might, like Amila might
be interested in depending on, you know, who it is that comes and starts out research programs
there. Exactly. So like, a lot, some people describe Dare as like an AI ethics research institute,
right? And I'm like, no, it's like, yeah, that's not what we're, we're hoping to do. And by,
by virtue of who we are, we will, so there's two ends of the spectrum that we were looking at,
right? And I think our advisory committee members, when you look at Safiya Noble and Shira,
Meina, they encompass those two ends of the spectrum. So the first end is how do you develop,
how do you do this research in a way that we think is beneficial to the groups of people that we
care about? And actually, when you say what's in scope and out of scope, our focus is, you know,
we're starting with thinking about, you know, people in, in Africa and the African diaspora, right?
Like so, you know, you know, you know, there's no kind of question. Like I don't have, I don't know
if I have to explain why, but like, you know, black people in general around the world who are
very much harmed by this technology and not necessarily benefiting from it. So when you look at
Shira, he's, in the area he's in, he's in Kenya, and a lot of his work is on how to, you know,
work on climate change and data science, right? He analyzes bird migration patterns to, to,
that tells you something about the climate and how it's changing. He, he was at the first black
name workshop. He probably covered his work, food security and conservation. He works on stuff
like the he co-founded data science Africa, right? So it's kind of like, you know, how to work on
data, quote unquote, data science or related fields in a way that is beneficial to start, you know,
to the groups of people that he cares about. On the other end, you have Sophia who's in, you know,
in, in the US, and she is more on the other end of the spectrum, how to, you know, raise the alarm
when we know there are issues that with technology that's already been built, right? So we, and,
you know, she's more from the social sciences side, right? So like, for me, that encompasses sort
of what I want to build with dare, right? Interdisciplinary have different groups of people
to, to be able to work on research that, you know, we think is beneficial to our communities.
And, you know, way that's not exploiting the people who are actually, you know, who might not
have PhDs or whatever, but have a lot of knowledge about the, the systems and how they're impacting
them. So I like what you said. Yeah, it is an undercurrent, right? Of like, how do we do this work?
Is, is, is that's how we're building this foundation? I mean, this institute.
One of the things that we chatted about before we started recording was that a lot of your focus
right now is on institution building. For obvious reasons, you're building an institution. Like,
I'm curious what that means for you. And also, well, afterwards, I want to relate that back to
your experience at Google and, and the, the idea around, you know, how to, how to ethics organizations
inside large companies? Like, how do we build those so that they have teeth, so to speak, so that
they can be effective? Yeah, that's a very good question. And so I've been going on this fairness
rabbit hole, as you know, and, you know, I've been like, I've worked on things related to math
and or documentation or auditing, community building, like Black Day Eye, Power Building, you know,
met all the different kind of ways in which I think you can attack the problem. And I have kind
of just kind of come to the conclusion, like many. And of course, this is not something new that
I'm saying. It's just like I said, your experience teaches you a lot more than what anybody else writes
or says is that if you don't have the right institution and the right structure, there's just no way
that you can do things, quote unquote, fairly, right? So, um, so that's why I'm, I'm kind of working
on institution building, right? I've, I've had experiences in academia. I've had experiences in
industry. And when I, after I got fired from Google, I was thinking, you know, a lot of people
were saying, well, you obviously won't have academic freedom in industry. If you want that, you should
go to academia. And I'll say, that's not true, right? To me, it's a pyramid scheme up here at the
top of the, you know, somebody just tweeted the other day that graduate students make $36,000 a year,
perhaps, right? And, you know, it's like they're in this weird position. Are they students? Are they,
are they workers? Like do they get vacation or not? But they're in this situation for years, right?
Very similar to college athletes. Oh, apps 100%, which also should get paid exactly. So that's
where we are, right? And so, um, yes, and it makes absolutely no sense. It's, I think it's very,
very exploitative. And so imagine you're doing that work as a graduate student at your advisor
controls your life. And then you're going to tell them, you know, whatever research they're doing
is not fair. You should have a different sort of direction. You, you're, you should stop. How are
you going to do that? You, you will lose your, your money. You will lose your, um, career,
like your future prospects, because they won't write you a recommendation. If people are on
visas, you will lose your visa. So, so, so, um, how are we telling people to do the right thing
when we know we're not setting them up, right? With the incentive structure to do the right thing.
And it's the same thing at work too, right? Like, um, again, what did I, I spoke up? I got fired.
So, um, then why, why would anybody do something differently then, right? Like, and so, so, that's
why I really believe we have to think about, um, the, um, incentive structures. And it's not just
about, for instance, labor practices that we're talking about, right? It's about what kind of work
is valued and what kind of work is not valued. Um, you know, so I, I think you have Mariel Gray.
Well, so her and Sudarsasri have this book called Ghost Work, um, how Silicon Valley is creating a
global underclass, and they're talking about data labor, right? So all of this automation that we
talk about is sort of pseudo, it's not, you know, real automation is that there's a lot of people
behind it labeling data, you know, doing all sorts of things, but they're being exploited. They're
not being paid, right? Um, and so in, in, in, in graduate school, if you're telling your PhD
student that they should spend all of this time working on data related work, data labor, that's
the very, the most important thing you should think about how you're gathering and annotating data,
take the time to do this right. But then they can't publish their work or they, it's not valued
or they can't get a job after they graduate. Again, that's an incentive structure and institution
building issue, right? So now, there's some people working on journals, for instance, to be able to,
for people to be able to publish on data. And there was this new rips, this new, new rips,
data sets and benchmarks to act where we actually published a paper too for dare. So that's what I
mean, like, this is exactly why I'm thinking about the, the, the incentive structures, right? Because
there's no way you could, you know, do quote unquote the right thing if you're in the wrong
incentive structure. Yeah, yeah, we, I did an interview with Safe Savage, who researches
that area as well. That was a future of work for the invisible workers in AI. Exactly.
I can episode 447. You know, if you kind of, you know, chart your path as
experimenting with different institutional structures to try to see what works,
um, is your decision to start dare, uh, you know, can we infer from that that support organizations
aren't enough, internal organizations aren't enough. There needs to be just an independent,
uh, alternative to kind of traditional research structures. What I'm exactly, what I'm thinking is,
so let's say if I went to academia, I'm, I told you, I'm trying to spend the time to think about
the meta questions. How do we build something, et cetera? How am I going to survive? Like,
I have to publish tomorrow. My students have to, you know, I'm, oh, no 10 year old, sorry,
what are you working on? Like, it's not even an option, right? So, so my, so my hope is that, yes,
um, we start these smaller independent institutes where we can actually say stuff. Um, Alex was
telling me about a talk that she gave the other day. And that might, might not have made a number
of people happy, but she's like, well, that's fine because I'm not looking to get tenure. And I'm like,
yeah, that's the kind of stuff you can do when you're not looking to get tenure. So I think, you
know, it gives us the opportunity to actually advocate for things that we think are important. And
maybe, um, slowly, those other larger institutions might change or, you know, have pressure to,
to do things differently. If they know that there are different options, like, like our institute,
and if other people create other institutes. And honestly, um, if you look at, you know,
even how I started Black Neye, before Black Neye, I had been involved in a lot of other organizations,
like for diversity or for this or for that. And I was like, you know, there's no way I can convince
you all to do the things that I think we need to do for Black people, you know, I just, I fought,
I tried at this and that. And I was like, let's start something new and do it the way we think it
should be done. And this is kind of similar to that, right? I tried this. I tried that. I tried
inside the organizations. I tried appealing to, you know, higher ups. I feel whatever. But, you know,
I found that that's not, you know, it's not working. And so what I want is to have an alternative.
And even when you look at Black Neye, right? What has, you know, there's now, um, Latinx,
and AI, queer, and AI, indigenous, and AI, disabilities, and AI, you also have a lot of Black
in X, I'm Black in robotics, Black in neuro, Black in physics, Black, I don't even know like there's
so many of them, right? And we can then, you know, build, there's like a network. Now you have,
you build power and you can advocate for things collectively. Um, and so hopefully that's what I'm
hoping with Dare, right? It's kind of an alternative to what we have right now. Um, and hopefully,
you know, other people can kind of replicate it in a way that not exactly replicated, but,
you know, in a way that works for their context. Um, and, you know, so with Dare, I can do things like,
you know, think about funding. Right? Where is our funding coming from? You know, honestly,
sometimes it feels like pick your poison, like there's no really clean money, like, you know, I'm learning
all those things, but, you know, you, I'm thinking about, right, like, again, like I said,
the meta questions, right? If we're thinking about AI and where the money comes from, you think
about technology in general, when the government really invests in technology, right? Um, it's doing
warfare, or when they're interested in something to do with warfare, like so the transistors and
silicon valley, right? Um, in World War II, you think about machine translation, why people
were interested in their research, has to do with Russia, Cold War, you think about DARPA
and self-driving cars. It wasn't because they were like, oh, we need, you know, to make cars more
accessible. You know, we need, we need to make sure that blind people can very freely move around.
So let's build that's not what they said. They said we, we care about, you know, autonomous warfare,
right? And so how do we expect to come to a different conclusion when from the very beginning,
our funding, our incentive structures, every, the paradigm that we're using has something to do
with warfare. And it's the same with industry too, like if, if all you're thinking about is how to
make money for this large, huge humongous company that, that, you know, affects the entire world,
controls the entire world, how do we have the space to think about a different paradigm? Right?
So like we're hoping to think about a different paradigm. I'm sure like, you know,
not that these paradigms don't exist. Other people are doing it too. But like, you know, kind of
take the time for ourselves to think about what paradigm should we follow starting from the funding
to how we do research to, you know, who we are hoping to serve.
You know, we, we both have a lot of kind of colleagues in the industry that are working within
larger organizations trying to help them use AI responsibly. You know, what does it say about
that work? Is it, you know, futile? Is it for not? Is it, you know, a pessimistic view? Or is it,
you know, do you, do you have examples of that process working correctly that you refer to?
And, you know, you're just offering an alternative or do you think that that is,
um, you know, yeah, you know, there's, um, there's this paper called, um, what is it? The Grey Hoodie
project from the university project? Yeah, from people at the University of Toronto. And, um,
and they talk about, they say how big text tactics are close to big tobacco. So they talk
about how, um, they give examples of how like, you know, the tobacco industries would give lots of
money to certain academics who talk at who write about how, well, you know, it's not, it's unclear
if smoking causes cancer or something like that. Or they would then internally retaliate against
people who actually have those kinds of funding, I mean, of conclusions, right? Or fossil
fuel industry who's scientists knew about climate change way back, but they were suppressing it.
And so why, you know, why wouldn't big tech be like that? I mean, what is their incentive not
to be like that? So I, I have seen it myself, how they capture, how they, you know, people
talk about industry capture, how they use, um, research in order to like, fight back against
regulation. So I do believe, honestly, that the number one, um, reason that these large tech
companies want to have these clinical ethics teams is to, um, in order to like, fight back against
regulation. So after I got fired, you know, uh, members of Congress and representatives sent a
letter to Google, and there was a number of letters they sent. First of all, they sent letters
about, you know, um, you know, the, the number of black people they have in these AI divisions,
do they have special like, uh, training, uh, in AI, except, you know, racial equity or in
the training or impacts that or anything. And they write back and they say, oh, we have, you know,
these, um, ERGs or whatever, you know, employee resource groups and we have lots of black people,
we have this event, that event. And similarly, um, they are, uh, wrote a letter to them about
Lara's English models and their impacts, et cetera. And they're like, we have had hundreds of,
uh, papers in ethics and fairness. You know what I mean? But I know for a fact, they are actually
suppressing more papers about the dangers of Lara's language models. You would think that they've
learned from, uh, their lessons. So when they're doing this, they, they are freely allowed to suppress
and persecute people with certain kinds of works, but not others, you have to ask why. And that's,
that becomes more of a propaganda than research, right? So I do think that this is their goal, but
so the people inside can know that and try to fight that, right? And, um, I think the way they can
fight that is through collective organizing. Like, people before them have done, right? Um,
Polaroid or a workers organized against Polaroid's, um, partnership with apartheid South Africa,
right? And that was, that had a, a big impact. So it's not that I don't think that people in the
inside cannot, um, cannot change things, but it's that they have to be vigilant to understand
why they want them there and how their work is being used, right? If your Meg Mitchell used to
call it fig lift, right? She's like, oh, fig leaf. She's like, I don't want to do a fig leaf work,
you know, because like they already do everything else. And you do, you do the fig leaf work,
right? Like you're like, just stamping what they say, oh, we're going to write your ethical
consideration section or we're just not changing the course, the direction that you're doing.
But we'll, we'll sort of do a fig leaf thing. That can do much more harm than good.
Do you, are there examples in the industry that you look to as you're creating there?
Um, well, I mean, there's examples of what I don't want it to be like and what that was a huge
motivation for, you know, like the open AI type stuff is not what I want. Like when open AI was
announced, and I've been so clear about this, I've never had kind of, um, I remember when open AI
was announced, I was at New York's. And I think it was in 2015. And that was before the name change.
And I had just gotten harassed at some, you know, bro, like, at a Google party. I was harassed.
I was having a horrible time. I just like, I don't ever want to come back to this conference.
And, you know, and it was after the whole Google gorillas incident. And they announced this company
that said this, that's supposed to save humanity from AI, like the whole world. It's all about
Elon Musk and Peter teal and all these people. They had 10 deep learning people 100% no interdiscipline
or whatever. Eight of them white men. Uh, one white woman, one Asian woman. Um, and I know for,
I like, I knew for a fact, this was not going to save humanity. And fast forward, what are we talking
about? We're talking about GPT three. We're talking about the dangers of large language models.
We're talking about how we're worried about things. We're talking about how, you know, there's
unsafe products out there, etc. Right. So of course, like, you start at the insect. This is exactly
what I'm thinking about institution building. Right. So you start at the inception. Who was at the
table? Where did the funding come from? Where were, you know, and so unless you think about that,
you can't not arrive at the kind of the state that we're in today. There are a lot of those
kinds of models. I am finding out about institutes left and right right now. One working on AGI,
one working on some other thing that has like 50 million dollar endowment or whatever.
I get irritated. I'm like, where is that 50 million dollars coming from? I wanted to
dominate. So I don't have to think, but you know, but then I would have to compromise on of course,
probably where we get the money or something like that. So that is, you know, on the one hand,
I have models of what I don't want, but I do have models of the kinds of grassroots organizing
that I've seen that I'm really excited about. Right. So for instance, I gave you an example of
Masakani, which is a network. And it was really beautiful to see, right, because it grew up
of a grew out of the deep learning in Daba, which is a convene, right. And so then they create a
people there who met there created Masakani network. And it's really cool. It's a whole bunch of
people focused on working on natural language processing tools for African languages. And
their values are very much kind of in line with the kinds of values I'm thinking about for
dare. And they grew it super slowly, you know. And I think now they have a foundation. I don't think
they have any full-time people. Before the show, I was talking to you about this article, this
wired article I had read about the Maori who created speech-related technology to benefit
their community. So they had this competition using their local radio for people to send in
kind of annotated speech for speech-to-text and other kinds of speech-related, you know,
language-related technology. And they had like hundreds of hours of data, right. And then all of
a sudden, this company, I think, was called Lion Bridge or something in the American company,
wanted to license their data. And they, you know, said no. And they published their reasoning. And
they said, you know, we think this is the last frontier for colonization. They beat the language
out of our grandparents. Literally, we're not allowed to speak this language. And they were
beat up for speaking it. And why is this company interested in like, you know, buying stuff for
now? It's not, it's obviously not because they want to benefit this community. So they're like,
we want to make sure that whatever we do with this data and how, you know, it's something that
benefits us. So those are the kinds of models I'm looking at. I'm like, oh, that's awesome. Like,
I like that, you know, how they're doing data stewardship, you know. And, you know,
unless it can't, I like their approach for grassroots organizing. Mijante, right? It's another
grassroots organizing. They're doing such great work. I just read their report on, for instance,
border technology. And they're talking, they're educating people about what are the different
companies involved in this like digital border, digital border walls? How should we organize?
They drove Palantir out of Palo Alto, right? And they had this no tech for ice campaigns. So, so I'm
looking at that them too. And how they're, they've been able to be so successful with their grassroots
organizing. So I'm looking at different kind of, you know, different kind of models to see what,
what it is that I like about each of these models and what makes sense for dare.
Yeah, yeah. One of the things that you mentioned, as we were chatting before, getting started was
this realization that you had that you can't reduce fairness to a mathematical problem. Like,
I think you saw that experience that. I see a ton of that elaborate on that a bit and kind of
your journey to realizing that and where you see it. How it occurs for you out in the industry?
You know, I mean, like my sisters have been saying, you know, doctors of Vienna was been saying this
forever. Dr. Ruha Benjamin has been saying this forever is Simone Brown. And, you know, like
the people not trained as engineers and computer scientists have been saying this for a long,
long time. And unfortunately, I was reading Philip Agres. It was the most depressing thing. In the
90s, he wrote, so there was even actually a Washington Post article about him. He was in AI and then
kind of became much more of a critic of it. He became a professor and he was like talking about a
number of issues that, for instance, like people are going to share their data much more
freely with, you know, for various applications, right? This is way before social media and stuff.
And he was like, it's not going to be so much of a big brother kind of thing, but people are
just going to like share it without knowing, without thinking carefully, talked about face recognition.
And so I was reading this, like, lessons from trying to reform AI or something like that. I'm like,
you got to be kidding me. What year was this in the 90s?
I don't remember when. I mean, but it was like lessons from trying to reform AI talks about how
the field is not reflective, how it's arrogant. You can't get people to think about disciplinary
norms and whatever. And I'm like, oh my god, like it's true, right? And that's what it is. It's
that when the field feels like it's better than other fields has a lot of power and money
thrown at it. At this point, you have money from the government, money from industry, money from
everywhere. You don't have to think about what anybody else says, even though these people have
been saying this stuff forever. And so what my own experience showed me, of course, we can talk
all we want about, we want to reduce fairness to mathematical equations, because first,
there's again, it takes me back to that incentive structure. When you think about how you ascend
in the academic world in computer science or in engineering in general, there is this hierarchy
of knowledge. I gave a whole talk about the hierarchy of knowledge, right? Certain kinds of knowledge
and contributions are valued. So if you spend five years working on data-related stuff, first of all,
in these conferences, if you have a data track, it's already inferior. Oh, it's just a data set paper
or whatever. Where's the engineer? That's how they talk. And actually, Kiri Wack staff gave
a keynote at ICML 2012 called machine learning that matters, which was basically about this kind
of stuff and how what conferences are valuing versus not. So that, to me, makes it such that you
want to reduce everything to the algorithm, to the math, to whatever. You want to not look at it
as a sociotechnical problem as many people in SDS have said. And when you do that, like the paper,
there's a paper called fairness and abstraction. They do a really good job of giving examples of
what kind of issues might arise when you do that, right? You're just like looking at the system
in isolation, not thinking about it as, you know, as part of a larger system, which is like, how
is it being used? What domain is it being used in? Who is using it against whom, et cetera? Then
your analysis becomes very different and much more complex. But you're not incentivized to do that.
Where you're not going to ascend, the people who want to give you tenure are going to be like,
oh, whatever, that's just data, or that's just fluffy, whatever. You know, that's how I'm telling
you what I talk. And so because of that, you're incentivized to be like, oh, you know, what does
fairness mean? I have no idea what fairness means, right? And, you know, Mimi Anoha, actually,
was Sita Pena, I think, who said she came to fact and she gave a talk about some of her
observations there at a some other workshop. And she said, what, why, you know, what does it
need to make systems fair that are punitive? That their job is to just be punitive, right? So,
for instance, when people talk about risk assessment, they jump to, you know, using the
compass data set and then, you know, Christian Lam and others, like, have written about all the
issues with that data set and why people shouldn't just jump to use it. And then they, they say,
okay, like, you know, we, we looked at that data set and we have this new algorithm and it makes,
you know, this other metric higher by x percent, right? What does exactly, what does that mean in
reality, right? Like what you're doing is you're still locking people up, like, you know, and you're
trying to figure out how, you know, what does fair mean in this case? Like, you're locking this
other person out of the same map. So a lot of people don't, you know, abolitionists don't even think
that whole system should exist, right? So when you're not looking at the entire system and you're
just focusing on this map, it's even, it's unclear like what, you know, if that's even something that
will help or not, and many times it can be very harmful. I've seen this in the face recognition
discussion, you know, after Joy and I wrote the paper, like gender shades, a lot of people were like,
oh, okay, you know, Microsoft came out with an announcement saying now that, like, they've changed
their training data, you know, data, and now it's much better. Now it's all, you know, accurate.
It doesn't have, you know, the, for darker skin women, the air rates are not as high,
but then when you look at all of this scholarship from especially trans and non-binary scholars,
they talk about how automatic gender recognition should not even exist. It shouldn't even be a thing.
So why are you jumping to making it go unfair, right? That's because you're not incentivized to
look at the whole system. So, you know, and then even if you want to do the right thing, even if you
want to do the right thing, like I tried to do at Google, if you're, you're going to get fired,
then what, what does that mean, right? Like, where are we writing papers about what to do if everybody's
going to get fired if they try to do the right thing? So that's what I mean. You just cannot
reduce it to this mathematical thing, but you could, you keep on doing it and our field people
keep on doing it because that's what they're incentivized to do and that's what they're rewarded for.
I think what I love so much about talking to you is that you have this very clear view of all of
the challenges, the systemic systematic challenges that are kind of inherent and baked into,
you know, all of these systems that, you know, we struggle against and, you know, that doesn't,
you don't get jaded. You just, oh, let me try something else. Let me try something else.
I'll try something else.
Honestly, I think sometimes it's because I don't really think about, you know,
the other option, I think, is way too depressing is what I think, right? The other option of, like,
you know, being like, I guess this is too big of a problem. We can't do anything. I just, you know,
it's too depressing, right? And then sometimes, like, when I started there, now I think about,
oh, my God, I was doing, I was doing this visualization of the plots of, like, how much money you have,
your burn rate, when there's a red line, when you run out of money. And I made a mistake,
and that red line was like literally like this October. And I just, my heart was just like,
and I knew I made a mistake, but I'm like, oh, my God, I'm doing something right now where this could
be, this could be the scenario. And it's not just my job. It's like all of these people's jobs
that are on the line, you know? And so when I think about those things, I'm like, oh, my God,
what am I doing? But you know, if I don't really, if I don't really, then, yeah, we have to be,
and I love this quote by Maryam Kaba, and I heard it from Ruha Benjamin saying it,
that hope is a discipline. It's, you know, we have to, yeah, what's the alternative? What do we,
you know, and it's not like a lot of times, I think we think that things are so far away,
they're not going to touch us, but we're seeing that that is not true, right? With the pandemic,
with the wildfires in California, with, you know, all of the ways in which we're all connected
in the world, like, you know, if you want a, just even a better, a better world for ourselves,
I'm not even thinking about the next generation, who should sue the hell out of all of the prior
generations for living them, like a world with, you know, with the climate catastrophe.
Even if we want a different alternative, you know, I think we should work for it, and for me,
I, it makes me feel better to do that, right? To feel like, at least, you know, I'm trying this
other thing, you know, this other alternative, you know, otherwise it's just too depressing.
I don't, I don't know how to not do that, you know? Yeah, yeah, yeah. So the initial funding
for Dare came through MacArthur, have you identified funding sources beyond that, or how that
all is going to work? Yeah, that's the big question that I'm working on right now. So the initial
funding came from MacArthur and Ford, and also the Kapoor Center gave us a gift, and I'm Rocket
Feller Foundation and Open Society Foundation. So it's all these foundations right now. And so
now we're applying for, you know, project-based funding for grants, like based on specific projects
that we're working on. And just like other people, you know, if there's an NSF grant, we're looking
to that and, you know, see if we can apply. But I am extremely worried about having a whole
institute that is only based on grants. So one of the things I'm doing right now is trying to
figure out how do we have our own revenue stream, and what does that look like? And really hoping
to have some things that we can experiment with in the next few months, because, you know,
we have a bunch of people with expertise, and I think we can provide that expertise in different
ways that are valuable to people, and that help us kind of generate revenue for our institute,
in a way that gives us a little bit more freedom of independence and flexibility, right?
Imagine right now, I say something wrong that one of the funders doesn't like, and they're all
know each other, and then everybody can just be like, sorry, bye. Like, you know, that can happen.
And it's really interesting, you know, the nonprofit world. You realize, you know, I mean,
it is because of wealth inequality that this world even exists. It's actually really sad.
And it's all the people I ran away from, like, you know, Eric Schmidt, you know, Chan Zuckerberg
Basils, and these are all the people who have these large foundations that want to fund tech-related
stuff. So it's, you know, so that's kind of what I'm thinking about right now. Like, we're
identifying different funding sources, thinking about how to diversify our funding sources,
what would our own revenue stream look like? And once I figure it, especially the revenue
stream part, and we have a few things to experiment with, I'll be much happier. I'll be, you know,
I'll feel much better about it. Nice, nice. Now, how far along are you? Are there folks that are
dare-affiliated that have research projects that are spun up and things that you can talk to?
So we have, we have Alex Hanna as a director of research, Dylan Baker, who used to be
under me at Google 2. As a, as a research research and such engineer, we have, I think, two research
fellows, Milla and Raseja. Milla just joined, like, this week. And one person who's probably going
to join us full-time in the next month or so. And, yeah, so we have, for instance, one of the
projects that I had been working on with, we've been working on with Raseja is this project to
analyze spatial apartheid, the impacts of spatial apartheid using satellite images and computer
vision techniques. And that's a project where, again, all the issues I talked about appear,
like, it takes you a long, long time to, the innovation is on figuring out the data, right? Like,
how to get the data and how to process it, how to annotate it. That's very hard to-
What does that mean, spatial apartheid?
Oh, spatial apartheid is basically, like, segregation, but it was mandated in 1950 by the
Group Areas Act in South Africa. So it's a big, like, it's a feature of apartheid, you know, and so
people of European descent could live in certain areas and everybody else had to live in,
and, you know, other areas like townships. And the budget application was a lot lower for townships,
of course. And so the question is, you know, supposedly apartheid has ended legally, right?
But when you look at these aerial images, it's so clear. Like, the delineation is so
clear. And so the question is, can we analyze the evolution of these neighborhoods and how things
are changing? Because we know, right? It passes the smell test in that you can look at these things
visually and do an analysis. We're not just trying to do magic, right? So the question is,
you know, how can we use computer vision techniques to do that? So Rassadja, when speaking of,
you know, exploitation versus not, et cetera, Rassadja, someone who grew up in a township,
I mean, so this is a very personal project for her. So it's like she's, you know,
investigating her own, you know, like stuff that's related to her. It's not like this,
what people say, parachute science, right? So that's one of the projects we're working on. We just
had a paper on Neuritz. We're working on releasing the data. That's one of the things I like
about being a dare because I, you know, we didn't just stop, you know, publish the paper and like,
really quickly release the data and we're done. We're like, okay, how do we release the data? How
do we create visualizations? How do we allow people to interact with the data? What art,
what follow up work are, you know, we're writing an article for Africa as a country. I don't know if
you know the outlet. It's it's one of my favorite outlets about about the work and one of the
things we want to say is that actually the South African government has to label townships
if they want to analyze the impacts of spatial apartheid. What they do is they end up in the census,
they lump it with just suburbs as formal residential areas. But, you know, that doesn't allow you to,
to because townships were created as because of apartheid that doesn't allow you to. And this is
interesting. It's, it's part of a larger kind of issue. Mimio Nguho was talking about how
some of her work, I think it's called Data Voids or something like that talks about how,
for instance, Google Maps didn't have Fabela in Brazil, you know, right? That's a huge, huge,
huge community of people. So it's part of this larger thing about, you know, who's data is, is
visible anyhow. But yeah, like, but that's an example of a project that we're working on and
there's a few others too. Well, how, how can the community support what you're doing?
Well, you know, follow us a dare. We're going to, you know, on Twitter, I think we're going to
also have more stuff on our website, just about more stuff we're working on. And you can donate
to dare if you're interested. We're going to have, you know, fellowships for people that we're,
you know, we have to think through how to do these fellowships too. And yeah, I think that's it,
you know, and advocate for more funding for these kinds of independent research institutes.
I don't want to have to cater to like a billionaire to get, you know, funding for our institute.
I'd rather apply for a grant that comes from, you know, public, you know, taxpayers and, you know,
be accountable to that. So that's another way I think in which people can advocate for these things.
And are you still hiring? Are you bringing on additional researchers?
Yeah, I mean, we have a lot of requests for hiring. And so we have to figure out, like I said,
we have to first build the initial foundational team. And so before I, we open it up for like
applications that will fly like that we've had like hundreds of people asking about internships
and volunteer and, you know, full-time jobs. So after we set up the initial team, then we're
going to be thinking, you know, thinking very carefully about what kind of internship fellowship
opportunities will have, what kind of, you know, other full-time opportunities will have.
I mean, that's the thing about having a small research institute and having to think about
funding sources as I can't grow it really fast, right? I can't, like, so that's the sad part. But,
and the thing about volunteer opportunities that I'm thinking about very carefully is,
who does that prioritize? Right? A lot of people can't do volunteer stuff because they have to
work. So I think I feel strongly about people being compensated for their work.
Very cool. Very cool. Well, Timnett, it has been wonderful, as always,
connecting, reconnecting with you and learning a little bit about
dare and what you're building there. Thank you for having me. It's a lot of fun to
come back periodically and kind of reminisce on like how much stuff has changed, you know.
Yeah. Yeah, we'll have to be sure to schedule the next one not quite as far out.
