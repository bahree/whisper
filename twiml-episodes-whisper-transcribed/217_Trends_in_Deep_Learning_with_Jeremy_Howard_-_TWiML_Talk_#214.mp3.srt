1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

4
00:00:35,920 --> 00:00:40,640
to present to you our first ever AI rewind series.

5
00:00:40,640 --> 00:00:45,040
In this series I interview friends of the show for their perspectives on the key developments

6
00:00:45,040 --> 00:00:49,720
of 2018 as well as a look ahead at the year to come.

7
00:00:49,720 --> 00:00:54,360
We'll cover a few key categories this year, namely computer vision, natural language

8
00:00:54,360 --> 00:00:59,200
processing, deep learning, machine learning and reinforcement learning.

9
00:00:59,200 --> 00:01:03,840
Of course we realize that there are many more possible categories than these, that there's

10
00:01:03,840 --> 00:01:08,840
a ton of overlap between these topics and that no single interview could hope to cover

11
00:01:08,840 --> 00:01:12,240
everything important in any of these areas.

12
00:01:12,240 --> 00:01:17,120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

13
00:01:17,120 --> 00:01:26,120
by commenting on the series page at twimbleai.com slash rewind 18.

14
00:01:26,120 --> 00:01:30,280
In this episode of our AI rewind series, we're bringing back one of your favorite guests

15
00:01:30,280 --> 00:01:36,480
of the year, Jeremy Howard, founder and researcher at fast.ai to discuss trends in deep learning

16
00:01:36,480 --> 00:01:38,520
in 2018 and beyond.

17
00:01:38,520 --> 00:01:43,440
We cover many of the papers, tools and techniques that have contributed to making deep learning

18
00:01:43,440 --> 00:01:47,920
more accessible than ever to so many developers and data scientists.

19
00:01:47,920 --> 00:01:48,920
Enjoy.

20
00:01:48,920 --> 00:02:00,160
Alright everyone, I am here in Montreal with Jeremy Howard and I'm speaking with Jeremy

21
00:02:00,160 --> 00:02:07,680
as part of a special series of shows we're doing about reflection of the machine learning

22
00:02:07,680 --> 00:02:13,560
in deep learning world for 2018 and some thoughts on predictions for 2019.

23
00:02:13,560 --> 00:02:15,160
Jeremy, welcome back to the show.

24
00:02:15,160 --> 00:02:19,720
Thanks Sam, nice to be here, I'm worried I'm not nearly cool enough to be able to tell

25
00:02:19,720 --> 00:02:24,640
you much about trends and whatnot but I'll do what I can.

26
00:02:24,640 --> 00:02:28,080
I am sure we're going to have a wonderful conversation.

27
00:02:28,080 --> 00:02:34,800
So this is the first of these that I'm doing and I expect that the format will shift

28
00:02:34,800 --> 00:02:42,200
a little bit as I do them but maybe we can kind of start by just getting your off the

29
00:02:42,200 --> 00:02:48,880
top of the head kind of reflections on 2018 before we focus on some of the things in particular

30
00:02:48,880 --> 00:02:52,840
around deep learning research that you found most interesting.

31
00:02:52,840 --> 00:02:59,480
Well, I guess it's easiest for me to talk about the year and the stuff that I care about

32
00:02:59,480 --> 00:03:08,360
which is increasing accessibility of deep learning for normal people to solve normal problems.

33
00:03:08,360 --> 00:03:16,560
So I guess our main work towards that this year has been trying to make it faster and

34
00:03:16,560 --> 00:03:22,520
easier and less expensive to train neural nets across more different areas.

35
00:03:22,520 --> 00:03:27,280
So there's a lot of people who have been doing work in that area as well which I could

36
00:03:27,280 --> 00:03:28,280
touch on.

37
00:03:28,280 --> 00:03:37,080
One is that when you look at the Stanford competition called Don Bench, it kind of kicked off

38
00:03:37,080 --> 00:03:41,680
increased interest in an area which had been really underappreciated before which is

39
00:03:41,680 --> 00:03:46,680
can you train accurate, large models quickly and cheaply.

40
00:03:46,680 --> 00:03:53,200
So there's a competition to measure those two things and so before the competition it

41
00:03:53,200 --> 00:04:00,560
took a few days generally to train image net to a reasonable accuracy, particularly if

42
00:04:00,560 --> 00:04:07,040
you're using kind of commodity hardware like stuff you would rent from AWS.

43
00:04:07,040 --> 00:04:18,320
And at the end of it we had gotten to first place on the leaderboard with 18 minutes, $40

44
00:04:18,320 --> 00:04:24,680
being nothing but Amazon AWS and then there's been other related work from other researchers

45
00:04:24,680 --> 00:04:28,680
who have got it down not on AWS commodity equipment but on more specialized equipment

46
00:04:28,680 --> 00:04:31,920
down to seven minutes.

47
00:04:31,920 --> 00:04:36,760
And so the kind of stuff that's allowed that to happen has been things like fantastic work

48
00:04:36,760 --> 00:04:45,000
from Leslie Smith on achieving superconvergence so using much higher learning rates, particularly

49
00:04:45,000 --> 00:04:51,480
a approach to learning rate scheduling called one cycle scheduling which we use as the default

50
00:04:51,480 --> 00:04:55,960
all the time now and makes life much faster and easier.

51
00:04:55,960 --> 00:05:03,520
I remember in the course and I believe our last conversation talking about cyclical learning

52
00:05:03,520 --> 00:05:04,520
rates is one cycle.

53
00:05:04,520 --> 00:05:06,840
Yeah, this is the next stage.

54
00:05:06,840 --> 00:05:11,360
So last year I guess Leslie did the CLI paper, cyclical learning rates.

55
00:05:11,360 --> 00:05:19,920
One cycle is just what it sounds like which is to spend about half of your epochs gradually

56
00:05:19,920 --> 00:05:24,040
increasing the learning rate, about half gradually decreasing it.

57
00:05:24,040 --> 00:05:27,840
But there's a few really important insights you have to combine with that.

58
00:05:27,840 --> 00:05:34,200
One in Leslie's paper is that as you increase the learning rate you should decrease momentum.

59
00:05:34,200 --> 00:05:36,160
So you have these two things happening at the same time.

60
00:05:36,160 --> 00:05:39,960
So if you use the first AI library that will happen for you automatically.

61
00:05:39,960 --> 00:05:45,360
And so it means that as you get to those really high learning rates, because the momentum

62
00:05:45,360 --> 00:05:50,240
is a lot lower, you're much less likely to kind of accelerate further than the model

63
00:05:50,240 --> 00:05:52,440
can handle.

64
00:05:52,440 --> 00:06:00,000
Something not quite related to Leslie's work, but stuff from Frank Hudders lab around being

65
00:06:00,000 --> 00:06:04,560
able to handle weight decay properly when you're using dynamic learning rates.

66
00:06:04,560 --> 00:06:09,840
So the momentum and Adam, so their techniques called Adam W, turns out to be super important

67
00:06:09,840 --> 00:06:11,720
as well for this.

68
00:06:11,720 --> 00:06:14,160
Was that Adam W this year?

69
00:06:14,160 --> 00:06:19,880
I can't quite remember, in the last 12 months or not, it may be slightly more, I don't

70
00:06:19,880 --> 00:06:20,880
know.

71
00:06:20,880 --> 00:06:24,160
But the combination I guess of all these things has been very much this year.

72
00:06:24,160 --> 00:06:29,280
So, and we combine those two together along with progressive resizing.

73
00:06:29,280 --> 00:06:34,680
So basically, I don't know if we were the first to do it.

74
00:06:34,680 --> 00:06:37,360
There was a lot of people that kind of did it all about the same time, but basically this

75
00:06:37,360 --> 00:06:43,520
idea of like, why don't you do most of your training on small images and then gradually

76
00:06:43,520 --> 00:06:49,960
resize, because modern convolutional neural nets are all, don't care what size your input

77
00:06:49,960 --> 00:06:50,960
is.

78
00:06:50,960 --> 00:06:53,200
And what's the advantage of doing that?

79
00:06:53,200 --> 00:06:54,720
Well, it's just super fast, you know.

80
00:06:54,720 --> 00:06:59,800
So two things, super fast, you do most of your training at 64 by 64, so you're decreasing

81
00:06:59,800 --> 00:07:02,120
your compute by more than 10x.

82
00:07:02,120 --> 00:07:06,680
But then the nice thing is you can do the last one or two epochs at a larger size than

83
00:07:06,680 --> 00:07:07,680
most people.

84
00:07:07,680 --> 00:07:10,960
So most people do 224 by 224 the whole time.

85
00:07:10,960 --> 00:07:13,760
So we'll go through to 288 or even bigger.

86
00:07:13,760 --> 00:07:19,480
So the intuition there being that you can teach the network, the basic things that it wants

87
00:07:19,480 --> 00:07:22,480
to learn about edges and textures and stuff.

88
00:07:22,480 --> 00:07:24,320
Well, you know, what does the cat look like?

89
00:07:24,320 --> 00:07:25,320
Yeah.

90
00:07:25,320 --> 00:07:30,160
And what a cat looks like at 64 by 64 is basically the same as 288 by 288.

91
00:07:30,160 --> 00:07:33,760
So the last couple of epochs, it's really just learning a few little tweaks around

92
00:07:33,760 --> 00:07:38,840
like, okay, this breed of cat has a, you know, this color of nose and this breed of cat

93
00:07:38,840 --> 00:07:41,560
has this slightly longer hair or whatever.

94
00:07:41,560 --> 00:07:44,040
So it can do most of its learning.

95
00:07:44,040 --> 00:07:52,480
And actually, that's been a real focus for us throughout the year is like this kind of

96
00:07:52,480 --> 00:07:56,680
and same with Leslie Smith, doing a lot more stuff dynamically, changing things during

97
00:07:56,680 --> 00:07:57,680
training.

98
00:07:57,680 --> 00:08:00,880
So for example, something we're going to release next week is doing the same thing for

99
00:08:00,880 --> 00:08:06,200
the GANs. So we've now got GANs training quickly and easily and reliably for the first

100
00:08:06,200 --> 00:08:07,200
time.

101
00:08:07,200 --> 00:08:13,080
And the trick again was basically to pre-train the critic and pre-train the generator using

102
00:08:13,080 --> 00:08:18,960
kind of simpler, fast approaches and small images and then at the very end, you kind of

103
00:08:18,960 --> 00:08:19,960
GANify it.

104
00:08:19,960 --> 00:08:20,960
Okay.

105
00:08:20,960 --> 00:08:24,800
So, you know, that's kind of been a common theme, I think.

106
00:08:24,800 --> 00:08:28,880
And so that all, and so a lot of that ties in then to kind of transfer learning because

107
00:08:28,880 --> 00:08:34,120
all these things of like gradually increasing the image size is kind of just a type of

108
00:08:34,120 --> 00:08:35,120
transfer learning.

109
00:08:35,120 --> 00:08:38,920
It's kind of transfer learning you're doing within the training learning.

110
00:08:38,920 --> 00:08:39,920
Yeah.

111
00:08:39,920 --> 00:08:43,920
So for our GANs, you know, we kind of do a similar thing of this kind of transfer learning

112
00:08:43,920 --> 00:08:47,800
as part of the process of training again.

113
00:08:47,800 --> 00:08:53,560
And so in general, transfer learning lets you generalize better, lets you train faster,

114
00:08:53,560 --> 00:08:57,680
generally lets you use less data.

115
00:08:57,680 --> 00:09:03,360
So we had a particular focus, as you know, on NLP for that.

116
00:09:03,360 --> 00:09:08,520
And so we show it with NLP, you can use like 100 times less data and still get state

117
00:09:08,520 --> 00:09:12,600
at the art results, sentiment classification.

118
00:09:12,600 --> 00:09:14,520
And so that's all about transfer learning.

119
00:09:14,520 --> 00:09:20,720
And then Alec Radford, at OpenAI, you know, built on top of that, replacing our LSTM with

120
00:09:20,720 --> 00:09:29,920
a transformer, and then Google built on top of that, making some tweaks, but mainly just

121
00:09:29,920 --> 00:09:35,520
doing it for longer with more data and, you know, we're now at a point where we know,

122
00:09:35,520 --> 00:09:39,320
I guess we know kind of every time people try and do transfer learning anywhere.

123
00:09:39,320 --> 00:09:40,320
Yeah.

124
00:09:40,320 --> 00:09:46,680
It tends to either let you get way better results on small datasets than people thought

125
00:09:46,680 --> 00:09:51,680
were possible or if you use big datasets like Google did with, but you know, it's kind

126
00:09:51,680 --> 00:09:55,400
of smashed the state at the art of what people thought was possible.

127
00:09:55,400 --> 00:10:01,840
So still really underappreciated area, frankly, still most people don't know how to do it

128
00:10:01,840 --> 00:10:02,840
properly.

129
00:10:02,840 --> 00:10:08,000
Had an interesting conversation here at NURPS, someone approached me and was, we were just

130
00:10:08,000 --> 00:10:12,480
kind of exchanging thoughts on what was interesting at the conference and they said they didn't

131
00:10:12,480 --> 00:10:16,160
see a lot of transfer learning like did that go away.

132
00:10:16,160 --> 00:10:18,320
And I haven't seen any.

133
00:10:18,320 --> 00:10:24,640
I both haven't seen it, but it's also, it's kind of, you know, being baked into a lot

134
00:10:24,640 --> 00:10:25,640
of things.

135
00:10:25,640 --> 00:10:26,640
It's kind of different.

136
00:10:26,640 --> 00:10:32,680
But also like, NURPS is very tainted by its history and culture.

137
00:10:32,680 --> 00:10:39,480
So the papers here, like, very tend to over-represent either things which are very mathematically

138
00:10:39,480 --> 00:10:43,120
intensive or also things that follow certain trends.

139
00:10:43,120 --> 00:10:48,400
So NURPS every second paper is either adversarial, blah or reinforcement learning, blah, you

140
00:10:48,400 --> 00:10:49,400
know.

141
00:10:49,400 --> 00:10:55,160
So like, yeah, it's not necessarily a conference you expect to see the most practically

142
00:10:55,160 --> 00:11:00,440
impactful stuff, unless some of the workshops are a little bit different.

143
00:11:00,440 --> 00:11:05,280
But I think that's been another feature of 2018 is like, people are putting adversarial

144
00:11:05,280 --> 00:11:10,880
into everything and putting RL into everything and not generally for good reasons, especially

145
00:11:10,880 --> 00:11:11,880
adversarial.

146
00:11:11,880 --> 00:11:16,120
This is huge literature now around avoiding adversarial attacks and I've yet to find

147
00:11:16,120 --> 00:11:21,080
anybody who's, and I've asked many researchers in the field, you know, directly, can you

148
00:11:21,080 --> 00:11:27,640
show me a actual practical example of where you would need to use this thing, you know,

149
00:11:27,640 --> 00:11:28,640
whatever they built?

150
00:11:28,640 --> 00:11:29,640
Right.

151
00:11:29,640 --> 00:11:30,640
No one's managed to get.

152
00:11:30,640 --> 00:11:37,040
What about on the generative side of you as bearish on the generative, on the GANS?

153
00:11:37,040 --> 00:11:44,080
So yeah, so I spent two weeks figuring out how to train GANS properly and finally, we

154
00:11:44,080 --> 00:11:51,280
now have something in first AI where you can train them in an hour on a single GPU reliably.

155
00:11:51,280 --> 00:11:54,520
And I'm very proud that we got to that point and we have this really flexible API that

156
00:11:54,520 --> 00:11:59,160
allows researchers to plug in things in ways they couldn't before, but at the same time

157
00:11:59,160 --> 00:12:05,080
I was also researching how to avoid GANS and I've figured out the generative stuff who

158
00:12:05,080 --> 00:12:10,640
actually get GAN level performance without using GANS, so I'm not sure.

159
00:12:10,640 --> 00:12:13,640
Is this stuff that you've written about or published somewhere?

160
00:12:13,640 --> 00:12:14,640
It's coming up.

161
00:12:14,640 --> 00:12:15,640
Okay.

162
00:12:15,640 --> 00:12:21,920
So we're particularly doing some work in microscopy, in collaboration with the SOC institute

163
00:12:21,920 --> 00:12:22,920
at the moment.

164
00:12:22,920 --> 00:12:23,920
Okay.

165
00:12:23,920 --> 00:12:26,160
Through analytic or through the fast idea.

166
00:12:26,160 --> 00:12:28,640
No, no, I haven't had anything to do with endotic for years.

167
00:12:28,640 --> 00:12:29,640
Okay.

168
00:12:29,640 --> 00:12:35,040
This is through a few things, but it's particularly through a new medical and life science

169
00:12:35,040 --> 00:12:41,200
research lab that I've just helped start, and I'm now the chair of, called RAMRI, with

170
00:12:41,200 --> 00:12:49,000
low AI and medical research institute, and it's at USF University of San Francisco.

171
00:12:49,000 --> 00:12:57,320
And we basically invite medical researchers and life science folks to partner with us

172
00:12:57,320 --> 00:13:01,920
and we'll help with the deep learning stuff and they'll help with the domain specific

173
00:13:01,920 --> 00:13:02,920
stuff.

174
00:13:02,920 --> 00:13:06,960
Yeah, so through that collaboration, one of the things we've been working on is helping

175
00:13:06,960 --> 00:13:13,240
the SOC institute to get better results from their microscopy because they're world leaders

176
00:13:13,240 --> 00:13:18,640
in this area and it turns out that if you can get really high resolution microscopy,

177
00:13:18,640 --> 00:13:25,600
then you can literally learn they have been publishing papers showing breakthrough understanding

178
00:13:25,600 --> 00:13:32,080
of how proteins fold and how they actually impact cells and stuff like that.

179
00:13:32,080 --> 00:13:38,600
So yeah, so initially we were kind of looking at GANs and getting pretty good results and

180
00:13:38,600 --> 00:13:43,600
particularly one of our previous students, Jason Antich, has created this awesome thing

181
00:13:43,600 --> 00:13:51,760
called de-oldify, which takes old black and white low resolution photos and turns them

182
00:13:51,760 --> 00:13:54,360
into beautiful color pictures.

183
00:13:54,360 --> 00:13:58,960
And so he's been helping us with some of this work of like getting GANs to work reliably

184
00:13:58,960 --> 00:14:03,840
because he does better practical work with GANs than anybody else that's seen.

185
00:14:03,840 --> 00:14:07,200
And how would GANs plan to this SOC institute use case?

186
00:14:07,200 --> 00:14:13,360
Well, you've got these microscopy results, you want to get the highest resolution outputs

187
00:14:13,360 --> 00:14:18,960
you can from whatever input comes out of your microscope.

188
00:14:18,960 --> 00:14:27,480
And so there's actually been some very high impact work in microscopy recently on using

189
00:14:27,480 --> 00:14:34,880
super res for that purpose, but it turns out not surprisingly that that research was not

190
00:14:34,880 --> 00:14:43,440
at all using kind of modern, deep learning methods, so it's clear we can do a lot better.

191
00:14:43,440 --> 00:14:47,000
But then, you know, as I say, then it turns out we've kind of realized that there are certain

192
00:14:47,000 --> 00:14:50,600
loss functions we can use which avoid the need for GANs entirely.

193
00:14:50,600 --> 00:14:56,000
So I'm now wondering if I wasted all that time.

194
00:14:56,000 --> 00:15:05,400
But I'm very interested in generative models, you know, and to see the stuff that's happening

195
00:15:05,400 --> 00:15:11,720
in the world of life sciences, you know, through better using whatever signal they can get

196
00:15:11,720 --> 00:15:17,440
from their microscopes, it's really exciting, you know.

197
00:15:17,440 --> 00:15:22,720
And you really need domain specialists and deep learning specialists working very closely

198
00:15:22,720 --> 00:15:27,440
together because like there's all these cool things that they can do that I wouldn't

199
00:15:27,440 --> 00:15:29,280
have otherwise known about in vice versa.

200
00:15:29,280 --> 00:15:34,640
So for example, they can like, while they're kind of taking the picture, they can like

201
00:15:34,640 --> 00:15:38,440
change the wavelength of light they're using, they can change the focal length they're using,

202
00:15:38,440 --> 00:15:41,880
they can change the angle that they're using kind of then they can kind of end up with

203
00:15:41,880 --> 00:15:47,080
this like long exposure almost like a video, and so we can then get this whole extra dimension

204
00:15:47,080 --> 00:15:53,600
and this is kind of sub-pixel resolution embedded in that.

205
00:15:53,600 --> 00:15:57,800
I mean this is what, this is exactly what we hope for, honestly, when we start a trust

206
00:15:57,800 --> 00:16:04,360
AI is that kind of domain experts would be able to use deep learning to do a better job

207
00:16:04,360 --> 00:16:08,840
of whatever it is they're doing, so it's nice to see that really happening.

208
00:16:08,840 --> 00:16:16,720
Have you seen any applications of GANs outside of the image domain, I kind of wonder

209
00:16:16,720 --> 00:16:21,280
conceptually you should be able to apply this to text and maybe do some of the things

210
00:16:21,280 --> 00:16:24,480
that people are using RNNs and LSTMs to do?

211
00:16:24,480 --> 00:16:28,600
Yeah, well like I say I feel like people are putting adversarial in everything and I'm

212
00:16:28,600 --> 00:16:35,200
not convinced it's helpful, I think often it's a bit of a lazy shortcut to like actually

213
00:16:35,200 --> 00:16:38,280
thinking about what your loss function should be.

214
00:16:38,280 --> 00:16:44,360
Yeah, I mean you mentioned text, it's a good question, there's, I think the more general

215
00:16:44,360 --> 00:16:52,320
question in text is what kind of like augmentation and stuff can we do in text to be able to use

216
00:16:52,320 --> 00:16:59,760
less data and get better results and I've kind of seen some sign of trying to use adversarial

217
00:16:59,760 --> 00:17:07,560
approaches there which I don't think is necessary, like there was a recent paper in the last

218
00:17:07,560 --> 00:17:14,760
few weeks which was basically, I don't know if you're familiar with the cutout paper,

219
00:17:14,760 --> 00:17:19,040
so you've got dropout which I think everybody knows which is like removing activations

220
00:17:19,040 --> 00:17:26,560
around them and then the cutout paper in vision specifically removed a whole contiguous

221
00:17:26,560 --> 00:17:33,000
sections of an image so kind of cut squares out of it and use that as regularization and

222
00:17:33,000 --> 00:17:35,840
then there's been a more recent paper in the last couple of weeks which is kind of basically

223
00:17:35,840 --> 00:17:41,320
done cut out at every layer of the neural net so it's basically dropout but instead of

224
00:17:41,320 --> 00:17:46,600
removing activations at random, you remove activations at the next to each other, stuff

225
00:17:46,600 --> 00:17:56,760
like that almost certainly will work equally well in text and don't require any of this

226
00:17:56,760 --> 00:18:00,320
kind of adversarial stuff.

227
00:18:00,320 --> 00:18:07,080
So yeah so I think my view is probably cans are a little overhyped and adversarial attacks

228
00:18:07,080 --> 00:18:12,320
are a little overhyped and it's certainly a great way to get published in Europe and you

229
00:18:12,320 --> 00:18:16,560
feel the same way about RL, have you also experimented with RL?

230
00:18:16,560 --> 00:18:26,040
Well, kind of I spent 10 years kind of studying optimization more generally and I never felt

231
00:18:26,040 --> 00:18:33,640
like these standard differentiation based approaches when you've got this kind of long-term

232
00:18:33,640 --> 00:18:41,120
credit assignment issue necessarily make a lot of sense and I still feel like there's

233
00:18:41,120 --> 00:18:46,560
a lot of the optimization literature that's being ignored by the RL community.

234
00:18:46,560 --> 00:18:51,720
You've seen a little bit of like evolutionary algorithms get touched on here and there

235
00:18:51,720 --> 00:18:56,960
like I think everybody's in work there but when I think back to like all the stuff that

236
00:18:56,960 --> 00:19:03,360
was going on in the early 90s, people just started to rediscover some of it.

237
00:19:03,360 --> 00:19:10,920
So for example, in the early 90s combining evolutionary algorithms together with kind

238
00:19:10,920 --> 00:19:15,960
of gradient-based methods was really common and I just saw a paper literally reinventing

239
00:19:15,960 --> 00:19:18,280
it like two weeks ago.

240
00:19:18,280 --> 00:19:22,200
The problem RL is trying to solve is great which is like hey let's not just try to predict

241
00:19:22,200 --> 00:19:27,840
things but let's actually try to figure out what action to take but I feel like currently

242
00:19:27,840 --> 00:19:37,280
the RL community is not quite treating it as enough as differently enough as it should

243
00:19:37,280 --> 00:19:38,280
be.

244
00:19:38,280 --> 00:19:46,360
When I spent a long time on optimization it became clear over time that it's a good idea

245
00:19:46,360 --> 00:19:51,920
to kind of recognize the differences between prediction tasks and more general optimization

246
00:19:51,920 --> 00:19:55,120
tasks.

247
00:19:55,120 --> 00:20:00,040
This is interesting because I had a chance to chat with Sergei Levine last night and

248
00:20:00,040 --> 00:20:08,040
we were talking about generally what he's found interesting over the past year in RL in

249
00:20:08,040 --> 00:20:09,040
particular.

250
00:20:09,040 --> 00:20:12,040
This was informal.

251
00:20:12,040 --> 00:20:17,400
But one of the things that he mentioned was a paper, TD3 paper, Twin Delay, Deterministic

252
00:20:17,400 --> 00:20:24,080
Policy Gradient which sounded like just the kind of hack that you would love.

253
00:20:24,080 --> 00:20:29,720
Like it's a tweak to the way they do the policy that he didn't even go into the details

254
00:20:29,720 --> 00:20:35,520
because it was such a knit but it gave him two X better training times.

255
00:20:35,520 --> 00:20:41,360
So if fast AI was doing RL it would be just the kind of thing that you bake into the library.

256
00:20:41,360 --> 00:20:48,120
Yeah, but I mean we only work on stuff which clearly works in practice for the kinds of

257
00:20:48,120 --> 00:20:54,960
problems most people have and Levine's one of the very few people who's using RL in

258
00:20:54,960 --> 00:20:57,920
very appropriate and useful ways at the moment.

259
00:20:57,920 --> 00:21:03,480
So for stuff involving robotics there's a lot more you can do with RL because any time

260
00:21:03,480 --> 00:21:09,080
you've got some physical system that you can actually model with physics pretty accurately

261
00:21:09,080 --> 00:21:14,800
you can kind of pull it apart and add appropriate constraints and an appropriate kind of

262
00:21:14,800 --> 00:21:18,680
auxiliary losses and there's a lot more that you can do.

263
00:21:18,680 --> 00:21:25,920
So I think the stuff he's doing is interesting and useful but a lot of people are using

264
00:21:25,920 --> 00:21:27,880
RL for stuff I don't know.

265
00:21:27,880 --> 00:21:33,640
I just went to the start of the health workshop here in Europe and there's all these people

266
00:21:33,640 --> 00:21:38,880
tackling various medical problems using RL which just seemed like slightly ridiculous

267
00:21:38,880 --> 00:21:39,880
in my opinion.

268
00:21:39,880 --> 00:21:43,840
Interesting, interesting.

269
00:21:43,840 --> 00:21:50,760
So there's been, we were chatting before we got started and you mentioned some of the

270
00:21:50,760 --> 00:21:57,960
work that's been happening to try to better understand the way some of our tweaks we've

271
00:21:57,960 --> 00:22:01,120
been doing like batch norm and other things are working.

272
00:22:01,120 --> 00:22:03,120
Can you elaborate on some of that work?

273
00:22:03,120 --> 00:22:04,120
Oh yeah, I'd love to.

274
00:22:04,120 --> 00:22:09,400
I feel like that's something where there's been some great progress this year and particularly

275
00:22:09,400 --> 00:22:11,880
in the last couple of months.

276
00:22:11,880 --> 00:22:18,200
There's a fantastic poster here at Europe that's been an archive for a while called Visualizing

277
00:22:18,200 --> 00:22:25,800
the Lost Landscape of Neural Nets and what they basically showed is it's something I've

278
00:22:25,800 --> 00:22:31,360
been talking about for ages but never actually did anything about it which is this idea that

279
00:22:31,360 --> 00:22:38,480
you can have this idea of sharp parts of the lost surface where if you're in a sharp

280
00:22:38,480 --> 00:22:42,880
part of the lost surface then the idea is it probably won't generalize very well because

281
00:22:42,880 --> 00:22:48,880
if you change anything a little bit now you're not in that kind of nice low area anymore

282
00:22:48,880 --> 00:22:59,440
and it's kind of overreaction to what I thought was a kind of obvious and kind of mathematical

283
00:22:59,440 --> 00:23:05,520
issue which is how you can like reparameterize the weights to make anything arbitrarily sharp

284
00:23:05,520 --> 00:23:10,640
which to me like if any time you read can reparameterize something why don't you just normalize

285
00:23:10,640 --> 00:23:12,560
out that reparameterization.

286
00:23:12,560 --> 00:23:17,920
So this Visualizing the Lost Landscape actually did that so they actually did the normalization

287
00:23:17,920 --> 00:23:21,600
and then they did this you know they built some beautiful software and some beautiful visualizations

288
00:23:21,600 --> 00:23:27,920
to show what happens and they found some really interesting things.

289
00:23:27,920 --> 00:23:32,080
One of the most interesting was they found that when you look at the actual trajectory

290
00:23:32,080 --> 00:23:40,880
as you train your own app for example if you take the PCA space of the weights there's basically

291
00:23:40,880 --> 00:23:47,920
only two dimensions you can basically that plot the entire or they said 40 to 90% of the variation

292
00:23:47,920 --> 00:23:55,600
in the direction of the gradient updates lies in just two PCA directions independent of the

293
00:23:55,600 --> 00:24:00,800
dimensionality of your well this was on image net so I mean this is obviously super high-dimensional

294
00:24:00,800 --> 00:24:08,800
so yeah over a hundred million weights and so you know that and you know one of the nice things

295
00:24:08,800 --> 00:24:13,200
that means is you can kind of plot exactly what pretty close to exactly what's going on which

296
00:24:13,200 --> 00:24:19,680
they did and you can also plot the Lance Lost Landscape that's being navigated and so one thing

297
00:24:19,680 --> 00:24:26,400
that they found was if you as soon as you add skip connections so Resnets versus the exact same net

298
00:24:26,400 --> 00:24:34,160
without the identity shortcuts Resnets basically make the whole surface incredibly smooth

299
00:24:34,880 --> 00:24:39,680
and dense nets make it even smoother by the way so when you see their pictures it just

300
00:24:39,680 --> 00:24:45,840
immediately makes you realize like okay you know this is why we've been loving Resnets so much

301
00:24:45,840 --> 00:24:55,040
and so you can kind of see similar stuff with normalization so one of the really interesting papers

302
00:24:55,040 --> 00:24:59,520
to come out in the last couple of months so there's been two papers coming out very similar times

303
00:24:59,520 --> 00:25:06,960
which both both both basically said hey you know how batch norm was meant to help with covariate

304
00:25:06,960 --> 00:25:11,840
shift where it turns out it doesn't help with covariate shift and it's got nothing to do with

305
00:25:11,840 --> 00:25:19,200
covariate shift and actually what it does is it makes the loss surface smoother which actually

306
00:25:19,200 --> 00:25:25,200
if you think about it makes perfect sense you know if your if your current activation is scaled

307
00:25:25,200 --> 00:25:30,960
really poorly then if you don't have batch norm the only way to fix that is to modify all of your

308
00:25:30,960 --> 00:25:36,480
weights on that layer where else if you do have batch norm you only have to modify the batch norm

309
00:25:36,480 --> 00:25:41,680
weights to fix the scaling so just makes the loss surface a lot smoother so then there's been other

310
00:25:42,480 --> 00:25:46,800
very nice kind of follow-ups to that well exactly follow-ups coming out at similar times to that

311
00:25:46,800 --> 00:25:52,080
saying hey here are some different ways of doing normalization which don't focus on the covariate

312
00:25:52,080 --> 00:26:00,080
shift but focus on the scaling so spectral norm and weight norm in particular which are now both

313
00:26:00,080 --> 00:26:05,120
built into pie torch and when you create a conflare in fast AI you can literally

314
00:26:05,120 --> 00:26:12,960
pick from an enum of what norm type you want and yeah they they they help a lot actually with

315
00:26:12,960 --> 00:26:19,520
against stabilizing training so yeah all these all these kind of insights into what's going on

316
00:26:19,520 --> 00:26:24,640
and so the neural network is helping lead to better ways to train the neural networks which means

317
00:26:24,640 --> 00:26:33,120
less hyper parameters less you know more resilient higher learning rates it's all making life

318
00:26:33,120 --> 00:26:40,080
easier in practice which is great so last year the big controversy at least one of them was

319
00:26:40,080 --> 00:26:46,160
Ali Rahimi's kind of call for greater rigor do you feel like a lot of this work is a response to

320
00:26:46,160 --> 00:26:51,600
that or no I never quite figured out what Ali meant and even though I had quite a few private

321
00:26:51,600 --> 00:27:01,280
chats with him I never quite figured out what he meant I tried to dig into it I think yeah so

322
00:27:01,280 --> 00:27:07,120
so I you know since I don't quite know what he was saying and haven't figured it out I can't quite

323
00:27:07,840 --> 00:27:11,920
I mean he's a great speaker and so a lot of people would like I really speak to that talk

324
00:27:11,920 --> 00:27:16,480
when I ask them like what exactly did he mean by rigor and exactly what are you going to do about it

325
00:27:16,480 --> 00:27:24,880
no one had a good answer to that question so for me you know as a kind of an experimentalist I think

326
00:27:24,880 --> 00:27:31,600
rigor is about ablation studies you know so if you look at the your LEMFIT paper that Sebastian and I

327
00:27:31,600 --> 00:27:37,360
did we spend a lot of time doing ablation studies so we said like what if you did this with more

328
00:27:37,360 --> 00:27:43,200
data versus less data what if you removed this training thing from this training thing from this

329
00:27:43,200 --> 00:27:47,040
training thing what if you tried this versus that data set and so we just had lots of tables and

330
00:27:47,040 --> 00:27:52,960
pictures saying here's you know here's the thing showing you which bits help and how much they help

331
00:27:52,960 --> 00:28:00,640
and what they help with so to me that's you know when when people don't have that I don't find

332
00:28:00,640 --> 00:28:05,120
their papers terribly useful because I don't know what's what's actually helping and often the

333
00:28:05,120 --> 00:28:11,360
things they thought were helping weren't helping even worse for me is when people don't use a

334
00:28:11,360 --> 00:28:16,160
strong baseline so they'll have some really crappy model and they'll say like oh look at our technique

335
00:28:16,160 --> 00:28:23,840
X improves that crappy model but when you look at it it's you know their technique X is just a

336
00:28:23,840 --> 00:28:30,320
crappy way of doing what the normal baselines would have done anyway so I think like to me in terms

337
00:28:30,320 --> 00:28:37,280
of experimental work strong baselines and good ablation studies is is what it's about but you know

338
00:28:37,280 --> 00:28:42,880
for the New York's crowd rigor often means great letters you know they want to see like

339
00:28:42,880 --> 00:28:51,360
you know convexity proofs and error bound proofs and all this stuff which I've just

340
00:28:52,320 --> 00:28:59,280
never seen useful like the only thing I've seen those kind of proofs do is to totally mislead people

341
00:28:59,280 --> 00:29:06,080
so like we had what I call the SDS pretty strong it's true we that what I call the SVM winter for

342
00:29:06,080 --> 00:29:13,920
like 15 20 years which is basically you know that Nick did this really to some people compelling

343
00:29:13,920 --> 00:29:20,000
papers it's just kind of like hey all you need is SVMs here's the mathematical proof and

344
00:29:20,800 --> 00:29:26,080
it you know when you actually look at it it's it's it's it's the the things that people took out

345
00:29:26,080 --> 00:29:34,400
of that are rubbish like it's the difference between like in theory here's what ought to work

346
00:29:34,400 --> 00:29:40,080
versus in practice here's what actually it works so it's it's it's nice this year

347
00:29:41,280 --> 00:29:48,160
leonbertos I can't remember the co-author but leonbertos work won a test of time award his 22 2007

348
00:29:48,160 --> 00:29:54,400
paper which basically I mean it's it's it's ridiculous that it was necessary for this paper to

349
00:29:54,400 --> 00:30:00,000
even happen but he basically said look or you people who were spending all this time on like

350
00:30:00,000 --> 00:30:09,920
fancy optimizers for SVMs actually SGD works better and and and like and then here's all the

351
00:30:09,920 --> 00:30:14,560
Greek symbols you need you know like here's all the proof and math and whatever else so like

352
00:30:14,560 --> 00:30:18,720
there's already plenty of experimental evidence to say like it works better which

353
00:30:18,720 --> 00:30:24,560
most of the community ignored and so it really took somebody to come along and like say it in

354
00:30:24,560 --> 00:30:29,920
their own words and to like prove it so to me I'm kind of like I don't like similar thing with

355
00:30:29,920 --> 00:30:34,240
like going back further minskie right minskie like proving that neural nets are a waste of time

356
00:30:34,240 --> 00:30:38,560
because they can't like solve the x-all problem and so the thing is all these mathematical proofs

357
00:30:38,560 --> 00:30:43,600
they're always of oversimplified versions of the problems we're actually trying to solve because

358
00:30:43,600 --> 00:30:48,320
the problems we're actually trying to solve are not amenable to that kind of analytic approach

359
00:30:48,320 --> 00:30:55,120
so you know took 20 years really for somebody to come along and say well we're not just using

360
00:30:55,120 --> 00:31:00,000
one layer you know we actually have a hidden layer if you have a hidden layer then we can solve

361
00:31:00,000 --> 00:31:09,520
any arbitrary problem to arbitrarily close given enough parameters so again like these kind of AI

362
00:31:09,520 --> 00:31:17,760
winters we had were really kicked off by people taking theoretical results far further than they

363
00:31:17,760 --> 00:31:24,160
should have ever been taken and ignoring all of the empirical evidence of like I don't know like

364
00:31:24,160 --> 00:31:28,480
guys like Jan LeCouin who's like saying like hey I've actually written this thing on that five

365
00:31:28,480 --> 00:31:35,680
that actually reads numbers you know it works it's this thing called a convolutional neural network

366
00:31:35,680 --> 00:31:42,960
and you know people like that just weren't getting published because you know because the theory

367
00:31:42,960 --> 00:31:50,000
had already proven that we should be using these other approaches so yeah I think that's um

368
00:31:50,720 --> 00:31:57,680
that's a concern I have about our field in general it also is a real problem for diversity

369
00:31:57,680 --> 00:32:06,160
and inclusion because there's lots more people in the world who know how to code or you don't know

370
00:32:06,160 --> 00:32:12,160
how to do some engineering but won't know how to prove error bounds on something or whatever

371
00:32:12,160 --> 00:32:17,200
and also like we'll be much more focused on like hey I want to actually solve this problem in

372
00:32:17,200 --> 00:32:25,120
in medicine or in you know disaster resilience or whatever and so they'll want to be

373
00:32:25,120 --> 00:32:31,840
publishing papers saying his thing A that works for thing B and the current focus in our field is

374
00:32:31,840 --> 00:32:43,600
yeah very unfriendly to that kind of stuff interesting yeah yeah um I'm curious having launched the new

375
00:32:45,040 --> 00:32:53,040
fast.ai course this year the deep learning course any you know specific 2018 learnings

376
00:32:53,040 --> 00:33:03,520
about deep learning education or what's required you know what's you know what's needed to kind

377
00:33:03,520 --> 00:33:09,200
of broaden the fold of folks that can do deep learning. I think I've been surprised by how far

378
00:33:09,200 --> 00:33:14,640
people have gone on the back of the fast.ai course like a lot of presenters at nureps have come up

379
00:33:14,640 --> 00:33:20,800
to me and said they got into deep learning through fast.ai and if you would ask me three years ago

380
00:33:20,800 --> 00:33:26,000
if our students are going to go on to be researchers presenting at nureps I would have been like

381
00:33:26,000 --> 00:33:31,040
that sounds like me I feel like we're just trying to get people to be reasonable practitioners so I

382
00:33:31,040 --> 00:33:39,040
think I think that's been a pleasant surprise um it's also been I must have been a bit surprised

383
00:33:39,040 --> 00:33:46,480
at how quickly the software has taken off um like for me that's kind of my focus now is like I would

384
00:33:46,480 --> 00:33:52,880
love for software to be as standalone as possible and not really require people to do the course

385
00:33:52,880 --> 00:33:56,960
because there's a lot of investment in time to do the course and yeah I'm increasingly running

386
00:33:56,960 --> 00:34:02,880
into particularly researchers now who have come from backgrounds in TensorFlow or pure pie

387
00:34:02,880 --> 00:34:08,560
torch or China or whatever and kind of saying like oh I started using the software and

388
00:34:09,440 --> 00:34:15,600
my research is going faster than it was before so that's been nice um but I think overall

389
00:34:15,600 --> 00:34:20,240
you know it's we've kept on finding the same things we've found in previous years it's just

390
00:34:20,240 --> 00:34:26,160
accelerating like I went to the black and ii dinner last night and the black and ii workshop which

391
00:34:26,160 --> 00:34:34,080
I saw you there as well um and uh yeah I mean it's just so great how many people came up to me and

392
00:34:34,080 --> 00:34:40,080
said like you know hey I'm from the ivory coast or I'm from Tanzania or whatever and

393
00:34:40,080 --> 00:34:46,640
I had no way to learn deep learning until your course came along and now I've done it and now

394
00:34:46,640 --> 00:34:52,960
I'm here at nureps presenting at this workshop and uh and they're always like kind of inspiring

395
00:34:52,960 --> 00:34:58,160
stories because often that you know they're telling me last night about the steps they had to go

396
00:34:58,160 --> 00:35:04,240
through to get a visa often having to go to other countries and contacting the console general or

397
00:35:04,240 --> 00:35:09,520
you know like as they're they're forging paths no one's ever forged before and now they're

398
00:35:09,520 --> 00:35:15,760
finding communities so like um it's so cool the way like three years ago and we did our first

399
00:35:15,760 --> 00:35:23,360
course we had one student from legos and he was like asking on the forum like hey anybody else

400
00:35:23,360 --> 00:35:29,520
from legos here anybody else from africa here you know tonight you're like can we have a community

401
00:35:30,320 --> 00:35:37,040
no nothing you know and and now uh legos is a second biggest faster your community

402
00:35:37,040 --> 00:35:44,880
long outside of the u.s so a bungalow is still the biggest bangalow that legos is huge so like

403
00:35:44,880 --> 00:35:50,960
it's been really cool to see how just a little bit of um and I know it's certainly not all

404
00:35:50,960 --> 00:35:55,680
thanks to fast ai there's lots of people doing great work here but I know like plenty of the people

405
00:35:55,680 --> 00:36:02,000
involved have got there thanks to fast ai so for example one of the um people presenting at the

406
00:36:02,000 --> 00:36:10,240
black and ai dinner last night uh judy jichoya she's a um a radiologist who got into deep learning

407
00:36:10,240 --> 00:36:18,320
through fast ai and now she's like a incredibly kick ass leader both in the radiology world and

408
00:36:18,320 --> 00:36:24,480
in the deep learning world in the black and ai world so um yeah I feel like there's now enough

409
00:36:24,480 --> 00:36:32,320
momentum going on that these uh underrepresented groups are not going to be underrepresented

410
00:36:32,320 --> 00:36:41,680
too much longer mm-hmm mm-hmm yeah I thought the I forget his full name but one of the presentations

411
00:36:41,680 --> 00:36:51,440
was kareem karen those an interesting uh slide where one of the speakers uh from tenizha

412
00:36:51,440 --> 00:36:59,280
kind of asserted that ai is both this incredible both uh existential threat for africa in some ways

413
00:36:59,280 --> 00:37:05,920
and and an awesome opportunity and kind of uh you know for for him it was a rallying cry to

414
00:37:05,920 --> 00:37:10,800
you know get more people engaged in the process i thought those were really interesting yeah

415
00:37:10,800 --> 00:37:15,680
I mean that's kind of why Rachel and I started this or it's exactly why Rachel and I started

416
00:37:15,680 --> 00:37:22,640
this is we both thought like this has the potential to massively increase inequality that's that

417
00:37:22,640 --> 00:37:28,880
is exactly what will happen if there's just a status quo right because all the people you know

418
00:37:28,880 --> 00:37:34,240
before we started fast ai all the people pretty much studying and working in deep learning were

419
00:37:35,760 --> 00:37:44,560
you know western white men who very very few of whom had any kind of domain expertise background

420
00:37:44,560 --> 00:37:51,440
so they were using deep learning to solve kind of bullshit problems um so we thought okay if

421
00:37:51,440 --> 00:37:56,480
nothing changes that's going to get worse and worse because those people keep getting money thrown

422
00:37:56,480 --> 00:38:03,280
at them and and they keep hiring more of the same people and investing education in the same

423
00:38:03,280 --> 00:38:12,720
places and but at the same time you know um it's deep learning is not at heart and it can be used

424
00:38:12,720 --> 00:38:20,400
to help so many areas with so many problems so if we could get kind of get this the the skills out

425
00:38:20,400 --> 00:38:28,720
there um then maybe uh people who otherwise would not be able to will be able to like make a big

426
00:38:28,720 --> 00:38:35,840
impact so i mean jute is a good example i think she's from Kenya uh and uh you know uh i'm

427
00:38:35,840 --> 00:38:41,200
sure she would have been a great radiologist regardless right but i feel like now you know we have

428
00:38:41,200 --> 00:38:49,680
somebody in the kind of senior thought leader community amongst radiologists who is black and who

429
00:38:49,680 --> 00:38:58,240
is a woman and who is you know both bringing ai to radiology and radiology to ai you know it's

430
00:38:58,240 --> 00:39:05,440
exactly the kind of perspective which yeah hope you know i think can create opportunities but

431
00:39:05,440 --> 00:39:12,640
they didn't exist before um let's talk a little bit about the the tools landscape obviously one

432
00:39:12,640 --> 00:39:20,160
of the big developments in 2018 was the launch of the 1.0 versions of both the fast that ai library

433
00:39:20,160 --> 00:39:30,960
and and uh pie torch is it too early to to talk about kind of momentum from the pie torch launch

434
00:39:30,960 --> 00:39:35,520
or it's not too early because one of the really interesting things is paying to see TensorFlow's

435
00:39:35,520 --> 00:39:41,360
reaction which is they've really got off their ass and doing good stuff you know they made some

436
00:39:42,000 --> 00:39:49,600
tough choices like getting rid of tf.contrib making tfo a much more community given exercise i think

437
00:39:49,600 --> 00:39:56,640
they realized that you know the the direction you know you talk to almost any Google TensorFlow

438
00:39:56,640 --> 00:40:01,600
engineer off the record at least and they'll tell you they all hate that code based you know it's

439
00:40:01,600 --> 00:40:08,880
full of technical debt and it's just not well put together um and it's not focused on the kind

440
00:40:08,880 --> 00:40:17,520
of the developer experience um it's really focused on using as many tp users possible so so tf2 is

441
00:40:17,520 --> 00:40:26,560
looking like a huge step forward in terms of the developer experience and um you know uh just

442
00:40:26,560 --> 00:40:32,160
kind of a piece of software that people will enjoy using breath of the news because they have to

443
00:40:32,800 --> 00:40:38,240
so and i really think that's been i mean i know it's been very heavily a reaction to pie torch

444
00:40:38,240 --> 00:40:44,400
which is not to say pie torch invented this approach uh it's i mean China certainly did it before

445
00:40:44,400 --> 00:40:52,560
pie torch and i don't quite know the history before that but um i think yeah realizing that

446
00:40:52,560 --> 00:40:58,320
the the tools need to be written for developers uh is just a really important insight so i think

447
00:40:58,320 --> 00:41:04,240
that's been so i think the impact on the on the TensorFlow gorilla has been important and interesting

448
00:41:04,960 --> 00:41:11,200
but then um yeah people just using pie torch uh particularly in the research community

449
00:41:11,200 --> 00:41:17,200
you're definitely seeing things you know more innovation as a result you're seeing faster innovation

450
00:41:17,200 --> 00:41:24,640
and then yeah you know the impact of fast ai is probably a little early to really know but um

451
00:41:25,520 --> 00:41:30,480
we're seeing that for a whole nother level for the you know quite a few researchers who are picked up

452
00:41:30,480 --> 00:41:39,760
fast ai um and you know fast ai kind of has two different user groups in mind there's the

453
00:41:40,320 --> 00:41:45,040
you know very much the research user group so that's all about the kind of

454
00:41:45,040 --> 00:41:51,120
lower level of abstractions where so for example with our gans you know we've made it so you can

455
00:41:51,760 --> 00:41:58,000
easily do research around like if you want some dynamic approach to switching between training

456
00:41:58,000 --> 00:42:04,320
the critic and the generator you know you you can just plug in your own gans which are class or if

457
00:42:04,320 --> 00:42:08,800
you want to pre-train a different kind of critic you can plug in a different pre-train critic class

458
00:42:08,800 --> 00:42:16,560
whatever and then of course there's the user group of people who just want to um get something

459
00:42:16,560 --> 00:42:20,960
working at their company so for example one person who came up to me at europe's last night said

460
00:42:20,960 --> 00:42:27,440
oh my company has three million documents um it's a pretty big international company and we've

461
00:42:27,440 --> 00:42:34,880
been using faster ai and urlm fit to basically tag all three million documents and they have a

462
00:42:34,880 --> 00:42:40,800
full-time taxonomist who sits there and classifies documents and then that gets fed off to urlm fit

463
00:42:40,800 --> 00:42:45,840
through fast ai and then they put the results and find you in the model and um so they're not

464
00:42:46,560 --> 00:42:52,640
doing research level customization or whatever they're just getting stuff done so I think yeah I

465
00:42:52,640 --> 00:42:59,120
think both of those groups so uh you know I think previously they would have been using

466
00:42:59,680 --> 00:43:03,760
keras if you wanted to do something like that but keras just doesn't really give you easy

467
00:43:03,760 --> 00:43:10,080
supportive things like nlp and things like that so um I think that yeah maybe that's one of the main

468
00:43:10,560 --> 00:43:18,640
impacts of fast ai in that area is easy deep learning isn't just in computer vision anymore

469
00:43:18,640 --> 00:43:23,120
it's also nlp and tabular and collaborative filtering

470
00:43:24,320 --> 00:43:30,000
any other interesting things on the tool side that you've that have caught your eye

471
00:43:30,000 --> 00:43:36,640
um yeah there's plenty of stuff going on I wish I had more time to dig into them than I did

472
00:43:36,640 --> 00:43:42,880
I mean you know in the pie-torch community well I mean obviously one big thing in pie-torch

473
00:43:42,880 --> 00:43:50,240
version one release which just came out yesterday is the just in time compiler so you can add a

474
00:43:50,240 --> 00:43:57,440
jet decorated to your code and get you know fused you know fast inference version of it so

475
00:43:57,440 --> 00:44:04,960
um it's quite a bit of that stuff going on there's another much less known library for pie-torch

476
00:44:04,960 --> 00:44:10,960
which is the galsion processes library and um so me it's not so much interesting because of the

477
00:44:10,960 --> 00:44:15,120
galsion processes but be interesting because they have to encode a lazy tensor which kind of

478
00:44:15,120 --> 00:44:22,480
takes the jet to a whole nother level which is you can as you basically say these are all the

479
00:44:22,480 --> 00:44:28,480
mathematical operations I want to do on my tensor it it's just storing the computation graph it's

480
00:44:28,480 --> 00:44:33,600
not doing the actual calculation at all until later on when you just say you know compute and then

481
00:44:33,600 --> 00:44:41,120
it compiles a kind of a fused version and then much faster handles kind of stuff like sparsity

482
00:44:41,120 --> 00:44:48,160
and stuff much better and they also have some kind of nice linear algebra identities which they

483
00:44:48,160 --> 00:44:55,360
use to you know just use arithmeticly much better choices when they know that you know you had

484
00:44:56,000 --> 00:44:59,840
these different shape matrices and you did these particular operations to them and you know

485
00:44:59,840 --> 00:45:04,160
this linear algebra identities so therefore we can replace that set of operations with this

486
00:45:04,160 --> 00:45:10,400
single faster one um at least with the lazy tensor is this an example of kind of pie-torch moving

487
00:45:10,400 --> 00:45:17,440
towards tensor flow while tensor flow is moving towards pie-torch I think so um I think so yeah exactly

488
00:45:17,440 --> 00:45:24,160
so the kind of the the the jet in pie-torch is starting to feel a bit like XLA I guess in tensor

489
00:45:24,160 --> 00:45:29,920
flow but with less technical depth yeah and they're kind of a lazy tensor stuff is starting to

490
00:45:29,920 --> 00:45:37,840
look a bit like the um uh kind of static graph approach but without all the horrible boilerplate

491
00:45:37,840 --> 00:45:45,760
the tensor flow so um I mean what we're I'm really excited though is what TNG chain is doing with

492
00:45:45,760 --> 00:45:51,120
TVM like hopefully that stuff with TVM where there you'll have right on that what's TVM

493
00:45:51,120 --> 00:45:58,640
our TVM is is something that's basically at a lot of level which is that you um take a it's

494
00:45:58,640 --> 00:46:06,400
basically a compiler for tensor expressions that will create an an optimized version of your

495
00:46:06,400 --> 00:46:13,760
tensor expression and um because at the moment one of the things I hate I as I hate it when I say

496
00:46:13,760 --> 00:46:18,720
to a student like okay let's dig into this code which calls this which calls this which calls this

497
00:46:18,720 --> 00:46:21,520
and let's understand all the stuff that's going on and they get to a point where it's like oh and

498
00:46:21,520 --> 00:46:28,080
this calls n videos kudian and library so at that point okay after that it's magic um so one of

499
00:46:28,080 --> 00:46:36,160
the cool things TVM does is it um kind of lets you see you know um it doesn't suddenly stop at

500
00:46:36,160 --> 00:46:43,040
kudian and it's now here's the TVM code you know okay and then um TVM ends up believe it or not

501
00:46:43,040 --> 00:46:50,240
faster than kudian and even although TVM is automatically creating that optimized kuda kernel

502
00:46:51,360 --> 00:46:56,880
so is it replacement or is it compiling down to kuda or something i mean it still has to

503
00:46:56,880 --> 00:47:03,840
compile down to what i'm not necessarily kuda but maybe ptx um so it's still you know so it's not

504
00:47:03,840 --> 00:47:10,320
business not just for Nvidia it can also target you know um or you know um various mobile devices

505
00:47:10,320 --> 00:47:19,200
or cpu um so i'm kind of like excited about this because i think it might mean that um particularly

506
00:47:19,200 --> 00:47:24,640
for stuff like swift for TensorFlow which is a project i'm really excited about you know

507
00:47:25,840 --> 00:47:32,640
you know hopefully we can see things where we can write everything we're doing in our kind of

508
00:47:32,640 --> 00:47:38,000
host language and um i was gonna ask are you excited about that because you're excited about

509
00:47:38,000 --> 00:47:43,920
Swift the language or because you want to see deep learning accessible via all the languages

510
00:47:44,480 --> 00:47:51,520
um there's a number of reasons i'm excited about it one is that i hate Python so great to you know

511
00:47:52,800 --> 00:47:58,800
get rid of it um it's just it's just incredibly frustrating to have to write in a language which

512
00:47:58,800 --> 00:48:06,640
has so many klachi things like with global interpreter lock and just so incredibly slow every

513
00:48:06,640 --> 00:48:13,840
time you actually do something in python and um so you know it's it's very very frustrating to

514
00:48:13,840 --> 00:48:19,440
to to work with it's fine if you're doing a web app or something but for numerical programming

515
00:48:19,440 --> 00:48:23,600
you spend all of your time trying to figure out how to have everything not being done in python

516
00:48:23,600 --> 00:48:30,320
you know you're basically always calling to see your kuda or whatever libraries um partly it's

517
00:48:30,320 --> 00:48:36,160
because Chris Latina everything he touches is awesome so it's just nice when somebody like that

518
00:48:36,160 --> 00:48:42,480
comes into your field and um i just can't you know i just love seeing what he's doing

519
00:48:43,280 --> 00:48:50,400
partly because Swift is just you know it's a good language um it's uh you know there's a few good

520
00:48:50,400 --> 00:48:56,080
languages in the world like i think f-sharp and julia and swift uh all examples of just good

521
00:48:56,080 --> 00:49:04,400
languages um but the thing about Swift uh and julia and f-sharp is they can all um you can write

522
00:49:04,400 --> 00:49:11,040
fast code in them and so like and so Chris's approach with with the swift potential flow team is

523
00:49:11,040 --> 00:49:18,800
that you'll be able to write all of your kuda kernels and stuff in swift you know and so be

524
00:49:18,800 --> 00:49:24,720
you know and and because like he's the llvm guy you know you know he's got the compiler chops to

525
00:49:24,720 --> 00:49:32,320
make sure that those swift kernels are not going to be any slower than the c kernels for me

526
00:49:32,320 --> 00:49:38,000
so also from a teaching point of view it'll be really nice to be able to show people every layer

527
00:49:38,640 --> 00:49:42,880
and from a research point of view it'll be really nice because we're we're able to

528
00:49:43,760 --> 00:49:49,920
swap out this lstm cell with our own lstm cell and not have to yeah worry about

529
00:49:51,440 --> 00:49:55,840
you know switching into c++ and comparing an extension and then dealing with a whole different

530
00:49:55,840 --> 00:50:06,000
debugging framework and all that how far along is it um not terribly far along um yeah it's not

531
00:50:06,720 --> 00:50:11,760
some of our students have tried to kind of play with it and it's not really usable yeah

532
00:50:12,320 --> 00:50:17,440
but i'm sure it'll get there okay and similar stuff going on in julia by the way julia also

533
00:50:17,440 --> 00:50:23,840
has similarly exciting stuff going on around writing kernels in julia and all that okay

534
00:50:23,840 --> 00:50:33,120
uh so maybe switching gears to 2019 and things you're excited about uh opportunities

535
00:50:34,160 --> 00:50:41,840
what what do you what are you looking forward to um well what do you think is going to happen if

536
00:50:41,840 --> 00:50:45,680
i don't know if you're one for kind of dusting off of crystal ball but

537
00:50:46,800 --> 00:50:51,520
no i don't know what i'm doing i'm doing another people i mean we're just keep doing what we're

538
00:50:51,520 --> 00:50:59,840
doing i guess um so i'm going to be doing a deep dive into um speech recognition kind of next year

539
00:51:00,560 --> 00:51:06,800
we're going to write a book about faster i and patorch kind of the book version of the course

540
00:51:06,800 --> 00:51:12,480
i guess early in the year with uh silvenko gogo who has been helping us with pretty much

541
00:51:12,480 --> 00:51:24,480
everything this year um you know i i hopefully will keep lowering the bar around um and we hopefully

542
00:51:24,480 --> 00:51:28,560
will start getting to the point sometime soon i don't know if it'll be in the next year where we can

543
00:51:28,560 --> 00:51:35,840
do useful stuff without any code that's that's really the the main bar that i want to get to is

544
00:51:35,840 --> 00:51:43,520
doing useful stuff without any code what does that mean for you so this is a startup i'm involved

545
00:51:43,520 --> 00:51:48,080
in called platform ai where we're trying to do exactly this for one particular sub domain which

546
00:51:48,080 --> 00:51:55,120
is image classification so you can uh don't even need labels you can import some some photos and we

547
00:51:55,120 --> 00:52:01,760
try and provide a uh kind of intuitive representation of what the model is learning to help a domain

548
00:52:01,760 --> 00:52:10,240
expert interact with it in an entirely visual way um so for me it's all about like people using

549
00:52:10,240 --> 00:52:17,520
stuff like that to solve scientific problems or to optimize their logistics or whatever it is

550
00:52:17,520 --> 00:52:22,720
they're trying to do you know like um it's all about recognizing that machine learning is

551
00:52:23,520 --> 00:52:30,960
computers learning from examples and so that should be all about getting rid of code we shouldn't

552
00:52:30,960 --> 00:52:35,760
need for loops and conditionals and stuff you should just be able to say here are my examples

553
00:52:35,760 --> 00:52:38,720
and the computer said here is what i'm learning and then you should say well here is some

554
00:52:39,520 --> 00:52:45,680
feedback about which ones are right and wrong so that's where we want to get to so yeah so we've

555
00:52:45,680 --> 00:52:52,080
uh already got the startup doing that for computer vision and i hope to do similar things in speech

556
00:52:52,080 --> 00:53:00,400
and nlp as well um as long as we rely on people knowing how to code we're missing out on something like

557
00:53:00,400 --> 00:53:08,480
99.9% of local population right right so maybe you know not kind of being so strict about

558
00:53:08,480 --> 00:53:14,560
2019 just you know looking ahead to the near future what um you know where you think the

559
00:53:14,560 --> 00:53:22,960
opportunities for us uh liar or are there specific um ideas you know or even some of the ideas

560
00:53:22,960 --> 00:53:27,120
that we've talked about in terms of you know better understanding of the way some of the training

561
00:53:27,120 --> 00:53:35,680
techniques are working or so the the big opportunity right now is nlp so um you know this year

562
00:53:36,800 --> 00:53:42,960
we and others showed that transfer for learning for nlp works specifically it works with that

563
00:53:42,960 --> 00:53:49,600
requiring an email labeling um um other than your target task and even then doesn't require

564
00:53:49,600 --> 00:53:59,280
much labels um we're gonna see i'm pretty sure similar things being shown for um generative models

565
00:53:59,280 --> 00:54:06,480
for text um so unfortunately that means one of the big opportunities will be that um spammers

566
00:54:06,480 --> 00:54:12,640
and trolls and people interested in disinformation will be able to use this technology to

567
00:54:13,680 --> 00:54:19,200
cause much more mayhem than they've been able to cause before um so i would say that's probably

568
00:54:19,200 --> 00:54:24,640
gonna happen in the next 12 months so i would not be surprised to see massive scale

569
00:54:25,440 --> 00:54:34,400
bots of generative text which are both appropriate enough to the thing it's responding to to seem

570
00:54:34,400 --> 00:54:43,280
reasonable and kind of reasonably believable stylistically that large numbers of people will

571
00:54:43,280 --> 00:54:49,680
be fooled by them large amounts of the time and they will not be easily automatically blocked or

572
00:54:49,680 --> 00:54:56,160
even analyzed to know the scope of them the way that things are now so i think the technology

573
00:54:56,960 --> 00:55:02,560
to fight that is harder than the technology to create it or is it just a will no it's much harder

574
00:55:02,560 --> 00:55:10,880
okay i i i i feel very confident that i you know if i had the reason or motivation to build such a

575
00:55:10,880 --> 00:55:19,120
bot now i feel very confident i could create one at scale which would you know be devastating to

576
00:55:19,760 --> 00:55:26,240
any social media platform uh and i think lots of people i mean not lots lots but you know

577
00:55:26,240 --> 00:55:33,040
anybody involved in the modern nlp kind of uh transfer learning stuff could um i would have

578
00:55:33,040 --> 00:55:38,000
know if somebody said to me like hey Jeremy Sebastian Root has just become an evil genius so this

579
00:55:38,000 --> 00:55:46,320
has written a twitter bot to spread russian disinformation can you go and help block it that'd be

580
00:55:46,320 --> 00:55:54,160
like no probably not you know i don't know that sounds really really really hard uh and that's

581
00:55:54,160 --> 00:56:03,120
often been the problem in these kind of areas is it's normally easier to create mayhem than to

582
00:56:03,120 --> 00:56:09,040
block it um and also when you're trying to block it you're always kind of being reactive

583
00:56:09,680 --> 00:56:14,560
particularly where we're not using more heuristic approaches to do the generative modeling but

584
00:56:14,560 --> 00:56:22,640
using kind of smarter approaches so yeah i think that's going to be a really big problem and one of

585
00:56:22,640 --> 00:56:30,000
the challenges is that the best way to fight that would would be to write your own generative

586
00:56:30,000 --> 00:56:35,520
text bots and you know use them to fight the disinformation but the kinds of people that would

587
00:56:35,520 --> 00:56:39,520
want to fix the problem would be much less likely to be the kinds of people who would be prepared to

588
00:56:40,320 --> 00:56:47,040
launch their own generative text box so i think that'll be difficult um but i then i think you

589
00:56:47,040 --> 00:56:54,560
know there's a lot of opportunities to uh you know in industry for for companies to be able to use

590
00:56:54,560 --> 00:57:02,240
text as a valuable resource i think it can certainly happen in medicine as well um like

591
00:57:02,880 --> 00:57:08,640
one of the challenges and radiology is that the the labels are all buried in radiology reports

592
00:57:09,280 --> 00:57:16,880
right and when you look at stuff like the data sets that the NIH should have provided for medical

593
00:57:16,880 --> 00:57:23,040
imaging the labels they've created have been using classic rules based NLP approaches and they're

594
00:57:23,040 --> 00:57:31,280
just terrible so having um that labeling done with these kind of modern transfer learning

595
00:57:31,280 --> 00:57:39,120
approaches will be awesome so yeah i think that's hopefully going to be one of the big at least

596
00:57:39,120 --> 00:57:45,120
the positive side of it will be one of the big things in 2019 um i think we're going to see

597
00:57:45,120 --> 00:57:56,160
these different approaches to normalization um probably start to take over from batch norm um i mean

598
00:57:56,160 --> 00:58:02,400
it's kind of unfortunate that we don't have a real image net competition anymore because like

599
00:58:02,400 --> 00:58:07,680
image net you know has plenty of flaws so i'm not saying we need image netback but the fact that

600
00:58:07,680 --> 00:58:12,880
there was our competition that quite a few of the more serious researchers decided to invest

601
00:58:12,880 --> 00:58:18,080
significant time in meant that each year you would see you know this year we've got

602
00:58:18,800 --> 00:58:24,320
measured three by three comms this year we've got resonance this year we've got you know

603
00:58:25,280 --> 00:58:31,760
Andrew Howard's data augmentation methods like they were like it made people who would otherwise

604
00:58:31,760 --> 00:58:38,400
be focusing on mathematically pure whatever's like actually focus on solving a problem properly

605
00:58:38,400 --> 00:58:45,360
mm-hmm so i i do worry that you know and also those solutions got published not just as papers but

606
00:58:45,360 --> 00:58:50,560
as you know um pre-trained models so we can download so i do worry a bit that we might continue

607
00:58:50,560 --> 00:58:55,760
to use pre-trained image net models with older architectures and older normalization approaches

608
00:58:55,760 --> 00:59:01,680
and stuff just because we don't quite have an image net competition anymore so i don't know

609
00:59:01,680 --> 00:59:08,800
what the fix to that is i mean there is like tf hub and now there's a torch hub which trans you

610
00:59:08,800 --> 00:59:14,800
know actually provide pre-trained models but people need to start realizing i think yeah that

611
00:59:14,800 --> 00:59:22,960
the image net models are increasingly out of date and then you know skip connections

612
00:59:22,960 --> 00:59:30,960
are still interesting the the unit paper is now the most highly cited mick i medical

613
00:59:30,960 --> 00:59:38,000
image and conference paper of all time resinat obviously is still kind of all powerful

614
00:59:38,000 --> 00:59:44,880
so two really important skip connection stuff so i think hopefully people will keep finding ways

615
00:59:44,880 --> 00:59:51,680
to better utilize skip connections mm-hmm and those two things skip connections and normalization

616
00:59:51,680 --> 00:59:57,920
yeah really help make models easier to train quickly and accurately mm-hmm

617
00:59:57,920 --> 01:00:06,640
the i think the rate at which new data sets are coming online is uh been uh increasing there

618
01:00:06,640 --> 01:00:17,040
have been a ton of these and like various domains in 2018 any do you think we see um or need for

619
01:00:17,040 --> 01:00:25,920
that matter like a kind of a better image net or a monster you know kind of the image data set

620
01:00:25,920 --> 01:00:32,560
to and all image data sets or do you think the domain specific direction is more that one better

621
01:00:32,560 --> 01:00:39,360
yeah yeah you know um so google to their credit just yesterday maybe it's this morning released a

622
01:00:40,160 --> 01:00:47,360
much more diverse version of open images um and that's really fantastic because the the previously

623
01:00:47,360 --> 01:00:53,440
the open images and image net data sets were incredibly biased in terms of they're basically all

624
01:00:53,440 --> 01:01:01,040
came from white western countries and so on that very very difficult to train models that could

625
01:01:01,040 --> 01:01:10,720
recognize Hindu weddings versus Christian weddings for example um so yeah i think we have a great

626
01:01:13,280 --> 01:01:20,720
photo object classification data sets um but we don't have great

627
01:01:20,720 --> 01:01:27,200
my cross-copy you know histopathology data sets we don't have great radiology data sets

628
01:01:27,760 --> 01:01:39,040
um on the text side and lp side uh on both yeah um particularly thinking of vision um yeah on the

629
01:01:39,040 --> 01:01:48,880
text side i mean it's ridiculous the it's there are almost no publicly available label text data sets

630
01:01:48,880 --> 01:01:55,040
that normal people can use as they wish um most of them are locked behind this thing called the

631
01:01:55,040 --> 01:02:01,280
linguistic data consortium which is part of you pen and um i'm sure originally when it was

632
01:02:01,280 --> 01:02:07,520
created it had every good plan to help research whatever but today it's basically this um

633
01:02:08,560 --> 01:02:13,920
thing that is increasing the exclusivity of the field so like it's my students can't replicate

634
01:02:13,920 --> 01:02:20,800
results in papers because so many of the data sets in an lp are locked behind all kinds of

635
01:02:20,800 --> 01:02:26,320
licensing agreements and like under the most popular ones the voters corpus you have to like

636
01:02:26,320 --> 01:02:32,480
download print sign with your organizational affiliation of form and send it to the to the

637
01:02:32,480 --> 01:02:37,760
government for approval you know you can't have a hundred thousand fast day ice students doing

638
01:02:37,760 --> 01:02:42,720
that so i can't use those data sets because they just aren't the like they were created in a time

639
01:02:42,720 --> 01:02:48,480
when people just didn't occur to them that like maybe there's more than just this like little

640
01:02:48,480 --> 01:02:52,560
research community of people who go to my little workshop but there's actually a there's a

641
01:02:52,560 --> 01:03:00,080
whole world of people out there who are wanting to do work right you know so that's a huge problem

642
01:03:01,760 --> 01:03:07,760
so i think um you know i think something else i would really like to see is in areas like

643
01:03:07,760 --> 01:03:15,520
medical imaging where data sharing is difficult um i want to see more model sharing

644
01:03:15,520 --> 01:03:19,920
so like there's lots of pre-trained image net models like more than we need

645
01:03:20,800 --> 01:03:25,520
so where's all the pre-trained histopathology models and the pre-trained

646
01:03:26,400 --> 01:03:32,320
CT models and the pre-trained MRI models like those are actually much more useful because you know

647
01:03:32,320 --> 01:03:38,320
if stanford releases their pre-trained prostate MRI model and then Boston picks it up and fine

648
01:03:38,320 --> 01:03:43,360
tunes it a bit and publishes those weights and then Harvard picks it up and fine tunes out a bit

649
01:03:43,360 --> 01:03:48,400
and then publishes those weights and then stanford can come back and fine tune that back in the

650
01:03:48,400 --> 01:03:52,720
circle right end up something better than they started with like it's actually it's been shown

651
01:03:52,720 --> 01:03:56,880
that you end up with just as good a model as you would have if you actually shared the data

652
01:03:56,880 --> 01:04:02,320
hmm but more you actually have to do is share weights so i would love to see people

653
01:04:03,760 --> 01:04:10,960
releasing pre-trained domain specific networks so another thing we saw quite a bit of this

654
01:04:10,960 --> 01:04:21,200
year was proposals for data sheets for data sets model cards you know different

655
01:04:21,200 --> 01:04:32,000
representations of the idea that we need to document the biases and and lineage in some cases

656
01:04:32,000 --> 01:04:40,480
of data sets and models do you see that stuff taking off? I don't see it taking off no I mean

657
01:04:40,480 --> 01:04:45,920
I think it's really cool the work that Jim Nick Gepparoo and all those folks did with the data

658
01:04:45,920 --> 01:04:52,480
data sheets for data sets work and it's a really interesting examples of other industries where

659
01:04:52,480 --> 01:04:58,480
that's happened so I've done some work with electronics and certainly I'm used to as they

660
01:04:58,480 --> 01:05:03,440
describe in the paper that every electronics component comes with a data sheet and a fairly

661
01:05:03,440 --> 01:05:12,800
consistent format and you rely on it having said that we're still suffering in the deep learning

662
01:05:12,800 --> 01:05:20,240
world from people not publishing the data at all or not publishing their code at all so I also

663
01:05:20,240 --> 01:05:25,360
worry about like if we say like well you can't publish your data set unless you create this data

664
01:05:25,360 --> 01:05:32,880
sheet maybe there'll be even less people publishing their data I do think that all the conferences

665
01:05:32,880 --> 01:05:39,520
need to say if you have experiments you need to publish the code and the data okay people claim

666
01:05:39,520 --> 01:05:46,000
that we have this great peer review system but when you actually look at it it doesn't really work

667
01:05:46,000 --> 01:05:53,600
like reviewers don't recreate the papers right the code to recreate the papers ever on the other

668
01:05:53,600 --> 01:06:00,240
hand where people put stuff up an archive there'll be many of our students all at the same time

669
01:06:00,240 --> 01:06:04,800
trying to replicate it and if they can they'll get up on GitHub and post issues and say what's

670
01:06:04,800 --> 01:06:10,320
going on which has happened just a couple of weeks ago it turned out that a widely cited paper

671
01:06:10,320 --> 01:06:15,360
as students went to replicate it and found they couldn't and discovered that the researchers

672
01:06:15,360 --> 01:06:21,600
had accidentally used the test the test set as part of the training data you know so like I don't

673
01:06:22,240 --> 01:06:30,800
at all agree with this idea that we have to keep this pure exclusive peer review system what we

674
01:06:30,800 --> 01:06:36,400
instead need is to be able to publish their code and publish their data and then get it out there

675
01:06:36,400 --> 01:06:43,840
so that the rest of us can can try it out and I think the academic community still doesn't realize

676
01:06:43,840 --> 01:06:52,240
how many of us do do that replication like every single algorithm that fast AI teaches or

677
01:06:52,240 --> 01:06:59,360
implements in our software we always re-implement from scratch and test from scratch for example and

678
01:06:59,360 --> 01:07:06,880
we're definitely not the only ones so I think like people are lots of people going to start

679
01:07:06,880 --> 01:07:13,200
publishing the data sheets for data sets let's first of all have the published data sets at all

680
01:07:14,240 --> 01:07:18,720
it would be great if they you know and I guess it's up to the kind of conference and venues

681
01:07:18,720 --> 01:07:24,080
and journals and stuff to start saying you know first yes you have to publish your code and data

682
01:07:24,080 --> 01:07:29,680
and then maybe once that's happening okay you actually have to also publish a data sheet to go

683
01:07:29,680 --> 01:07:35,360
with it we could at least encourage through the review process in the meantime to like if you've

684
01:07:35,360 --> 01:07:41,040
got a new data set to kind of say like can you you know reviewers could ask for information about

685
01:07:41,920 --> 01:07:49,280
how it was collected and you know what the diversity of people involved in was and stuff like that

686
01:07:49,280 --> 01:07:56,160
it certainly does seem like a good bar in a time when you've got 4,000 submissions to

687
01:07:56,160 --> 01:08:02,080
nerfs and a thousand papers being published to require that folks that are doing experimental work

688
01:08:02,080 --> 01:08:09,040
publish the code as well it does yeah well Jeremy it's been so great to chat with you once again

689
01:08:09,040 --> 01:08:12,960
great to see you here and thanks for taking the time thank you Sam

690
01:08:12,960 --> 01:08:22,960
all right everyone that's our show for today for more information on Jeremy or any of the topics

691
01:08:22,960 --> 01:08:30,080
covering in this episode visit twimmalei.com slash talk slash 214 you can also follow along with

692
01:08:30,080 --> 01:08:39,040
our AI rewind 2018 series at twimmalei.com slash rewind 18 as always thanks so much for listening

693
01:08:39,040 --> 01:08:46,240
and catch you next time happy holidays

