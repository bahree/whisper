WEBVTT

00:00.000 --> 00:18.160
All right, everyone. I am here with Greg Brockman. Greg is co-founder and CTO at OpenAI. Greg,

00:18.160 --> 00:24.880
welcome back to the Twomol AI podcast. Thanks for having me, Sam. Hey, it's been a while since we

00:24.880 --> 00:33.280
spoke. It was back in November of 2017. Believe it or not, episode 74 of the podcast, we're over 500

00:33.280 --> 00:40.640
now and we were then talking about AGI. I am really looking forward to this chat where we'll be talking

00:40.640 --> 00:48.960
about something new that OpenAI has been working on for a while, Codex. But before we do, why don't

00:48.960 --> 00:54.720
you reintroduce yourself to our audience and tell them how you came to work in the field of AI?

00:56.000 --> 01:01.200
Hey, everyone. I'm Greg as I, Sam said, and I am one of the co-founders of OpenAI.

01:02.880 --> 01:08.800
You know, for me, I've read the Alan Turing 1950 paper, Computing Machinery and Intelligence

01:08.800 --> 01:13.520
Paper back before I knew how to code. And I remember reading it. You know, it lays out the Turing

01:13.520 --> 01:18.960
Test. But then it says, look, you're never going to be able to program a solution to this test.

01:18.960 --> 01:23.840
The only way to do it is you have to have a learning machine. And he goes into quite some detail.

01:23.840 --> 01:27.040
You know, he says, like, look, you're going to have to do this, like, to have a little machine.

01:27.040 --> 01:31.120
It's almost like a child machine that you give rewards when it does good things, punishment when it

01:31.120 --> 01:36.720
does bad things. And from there, you can hope to build up a solution to this. Really visionary

01:36.720 --> 01:40.800
stuff, honestly. And for me, I was captivated by the idea that you could build a machine that

01:40.800 --> 01:45.840
could understand problems that you yourself could not. And I just saw being able to build machines

01:45.840 --> 01:50.160
that could themselves help you solve problems that were outside of your reach, be the thing I wanted

01:50.160 --> 01:56.320
to do. So I went to a professor and was like, hey, can I do some MLP research with you? And he's

01:56.320 --> 02:01.360
like, great. Yep. Here's these, like, parse trees and things like that. And sadly, it didn't look

02:01.360 --> 02:06.400
like that was going to quite get you there. So I got distracted by programming languages, which,

02:06.400 --> 02:10.320
you know, I think kind of captures the same idea, right? If you can build a compiler,

02:10.320 --> 02:14.400
you can kind of understand this program and can really amplify what a human can do.

02:16.800 --> 02:23.120
And then, you know, it did startups and it was really 2015 that I first encountered deep learning.

02:23.120 --> 02:28.000
And for me, I was watching hacker news every day. And I felt like there was a new deep learning

02:28.000 --> 02:31.520
for this, deep learning for that. But I didn't know what deep learning was. And it was actually

02:31.520 --> 02:36.400
surprisingly difficult to just Google around and learn what deep learning actually meant. So I

02:36.400 --> 02:41.360
asked some friends about it. And as I started going around, I realized all of my smartest friends

02:41.360 --> 02:45.920
from school were now in deep learning. And that for me was a real sign of, okay, maybe there's

02:45.920 --> 02:51.920
some real substance here. And the deeper I dug, the more it felt to me like the old direction,

02:51.920 --> 02:56.720
that it just didn't feel right, the new direction actually did feel right. And to me,

02:56.720 --> 03:01.040
you know, looking back at it now, the thing I find most fascinating is that really this neural

03:01.040 --> 03:05.680
in that direction, it's not a, you know, five, 10-year thing. It's really a 70-year journey to get

03:05.680 --> 03:10.400
to where we are. So it's just exciting to be pushing the frontier of what these neural networks

03:10.400 --> 03:13.280
can do. And that's basically what we've been doing in OpenAI the whole time.

03:14.160 --> 03:19.680
Nice. You mentioned your interest in programming early on and parse trees and all that kind of

03:19.680 --> 03:25.680
stuff. And you know, there's maybe a connection to what we're going to be talking about today.

03:25.680 --> 03:36.720
Again, which is Codex. OpenAI recently announced co-pilot, which is another project in the same

03:36.720 --> 03:42.560
vein. Maybe tell us a little bit about these projects, what they are and how they're related to one

03:42.560 --> 03:48.800
another. Yep. So we've been building the Codex model for about a year now. We really started

03:48.800 --> 03:55.920
when we saw GPT-3 be released. And people's, you know, most excited reactions were actually

03:55.920 --> 03:59.120
using it for programming. And we looked at that and we said, well, we didn't build this model

03:59.120 --> 04:03.520
of the program at all. What happens if we actually put some effort into it? And so we've actually

04:03.520 --> 04:07.440
teamed up with GitHub and Microsoft. You know, GitHub, I think, is probably best in the world that,

04:08.000 --> 04:12.960
you know, knowing what developers want and have great, your great community. And obviously,

04:12.960 --> 04:17.200
you know, that they have, they have lots of data as well. And so we worked really closely with

04:17.200 --> 04:23.040
them to try to build a product that people wanted, right, to really validate that what we were doing

04:23.040 --> 04:27.840
wasn't just a cool research project, but was actually useful from day one. So a month ago,

04:27.840 --> 04:35.440
we released co-pilot together with GitHub, which is the first product built on top of Codex.

04:35.440 --> 04:40.080
And that they use the Codex API that is the same API that we, I guess by the time people

04:40.080 --> 04:47.440
watch this podcast, will have released on Tuesday. And so, you know, talk a little bit about the

04:47.440 --> 04:54.720
relationship between Codex and GPT-3. Is it an entirely separate model? Are they the same model

04:54.720 --> 05:00.320
with different training data, different training processes? Yep, I would think of Codex as a descendant

05:00.320 --> 05:06.960
of GPT-3. So spiritually, you do the same kind of task. GPT-3 is take all of the text on the internet

05:06.960 --> 05:13.520
and just do an auto-complete task, predict what word is going to come next. Codex is take all

05:13.520 --> 05:19.200
the text on the internet and all of the public code and do that same process. And we made lots of

05:19.200 --> 05:23.760
improvements all across the board. Really, this has been an effort of a quarter of open AI to make

05:23.760 --> 05:29.440
it happen. So we've really had to put in efforts from everything to, you know, we have architectural

05:29.440 --> 05:33.840
improvements, we have training improvements, we have a lot of just like the good old-fashioned

05:33.840 --> 05:38.720
engineering, maybe these models be fast and responsive has been a huge amount of work as well.

05:38.720 --> 05:44.080
So it's really been improvements all across the board. So kind of talking about Codex relative

05:44.080 --> 05:49.360
to GPT-3, you mentioned, take all of the text on the internet and all of the code on the internet.

05:51.360 --> 05:54.800
In creating something like a Codex, are those given equal weight or

05:56.080 --> 06:01.440
is the code somehow, you know, more relevant for the task that Codex is likely to see?

06:01.440 --> 06:07.600
Yeah, the short answer is I think we're still figuring out exactly the right way of doing it

06:07.600 --> 06:13.920
ultimately. I mean, I think that, you know, right now our process is, you know, I think you basically

06:13.920 --> 06:22.240
end up with, you know, you end up seeing much more code more recently than text. But it's still

06:22.240 --> 06:26.000
an open question, I think, exactly what you want. And we've kind of found that when you actually

06:26.000 --> 06:30.560
look at the models in terms of how people want to use them, that part of what makes Codex really

06:30.560 --> 06:36.800
shine is the fact that it has all this world knowledge built in. And so you can actually end up with

06:36.800 --> 06:41.760
a model that's very, very good at doing just like, you know, sort of very narrowly defined,

06:41.760 --> 06:47.680
complete this programming, you know, this function or something, without actually being very useful

06:47.680 --> 06:53.600
to people. So I think that finding the right evaluations is actually one real trick to make

06:53.600 --> 06:58.560
this model work. And so, you know, what we really focused on is, and has actually got it,

06:58.560 --> 07:03.360
it's pretty well so far, is at the very beginning of the project, we've wrote down this data set,

07:03.360 --> 07:10.720
this now open source called, we call it human eval, which is a list of problems written by humans

07:10.720 --> 07:14.880
that are just programming puzzles. And we kind of designed them to be ones that are kind of,

07:14.880 --> 07:20.320
they're a little bit of tricky wording and a little bit like, you know, it's different from what

07:20.320 --> 07:26.560
you would find in, you know, some already out there in the training corpus. So kind of intentionally

07:26.560 --> 07:32.880
chosen to have some twist them and that sort of thing. And what we found is that by pursuing

07:32.880 --> 07:37.360
that metric, it is actually our best north star metric, like everything else, if you just look at

07:37.360 --> 07:42.160
perplexity, you know, basically like, how good it is exactly at predicting next token in text,

07:42.720 --> 07:46.880
that that particular metric breaks down a little bit for us, because you kind of want this holistic,

07:46.880 --> 07:51.680
not just how certain do you get that there should be a period here, but you really want just given

07:51.680 --> 07:56.240
a pretty natural description of what the problem is going to be, can you solve that problem?

07:58.240 --> 08:06.160
And so when you created that that data set and that metric, did you, was there a closed loop

08:06.160 --> 08:13.120
there where the things that the programs that Codex created against that training set had to

08:13.120 --> 08:18.800
actually run and produce the desired result? I understand, yes. I'm mostly asking just from

08:18.800 --> 08:24.480
the perspective of evaluating the formats of Codex in terms of producing runable code.

08:25.680 --> 08:31.360
An aspect of that data set. 100%, 100%. So you literally take, you provide the model with,

08:31.360 --> 08:35.120
you know, maybe a doc string and maybe a little bit of a function definition, it generates a bunch

08:35.120 --> 08:40.240
of code, you literally eval that code. Now the details of the eval actually I think are pretty

08:40.240 --> 08:46.320
interesting because you just had some code come out from your model. What's it going to do?

08:46.320 --> 08:50.400
Is it going to lead all the files on your computer? It's all possible, right? And so you really

08:50.400 --> 08:55.360
need to have a good sandbox. And so, you know, I think that one thing people miss in this field is,

08:55.360 --> 08:59.520
you know, it's all about the big idea, but what people miss is that actually it's about the small

08:59.520 --> 09:04.160
ideas, right? It's about getting the engineering really right. And so yeah, you want to actually train

09:04.160 --> 09:08.320
a model to run some arbitrary code and eval that makes sure it's doing the right thing. You need to

09:08.320 --> 09:13.360
have a world class sandbox to make that happen. And so you need to make sure both that the execution

09:13.360 --> 09:17.760
is like not able to do anything, you know, tamper with your system, but also that, you know,

09:17.760 --> 09:20.960
just even little things like resource consumption and being able to crash your system and things

09:20.960 --> 09:26.000
like that are held in check. And we actually have found multiple times that the model would generate

09:26.000 --> 09:32.160
code that kind of broke our current sandbox. So we've upgraded it since then. Interesting, interesting.

09:32.160 --> 09:38.320
So I think that is suggesting the folks that play this play around with this via the API that they

09:38.320 --> 09:42.720
take care to inspect the results they get before they just run them if they don't have a sandbox

09:42.720 --> 09:47.600
environment that they're. Yeah. I definitely, I definitely recommend that for any code you take

09:47.600 --> 09:52.480
from the internet. You know, if you just download some code from even my GitHub, I will not take

09:52.480 --> 09:57.360
a fence if you double check it before just running it. I think it's, it's just an important thing

09:57.360 --> 10:02.160
generally. But I would say that this, this like the model doing unpredictable things is really

10:02.160 --> 10:07.440
early in training, right? So when the model isn't very smart, isn't very capable. It's sort of

10:07.440 --> 10:12.240
less predictable exactly what it'll do. The more capable the model gets, the more it's going to be

10:12.240 --> 10:17.040
faithful to your instruction. So I've been, I've been using this model for, for, you know, I spent

10:17.040 --> 10:21.120
quite, quite a bit of time playing with it. And that I've actually, you know, found that it's,

10:21.120 --> 10:24.800
that it's quite reliable in contrast in some ways to GPT-3.

10:26.720 --> 10:31.680
Can you talk about those distinctions a little bit more in terms of the types of

10:32.480 --> 10:37.600
results that it tends to see versus GPT-3 relative to the, the prompts that you're giving it?

10:37.600 --> 10:43.200
Yeah, see the thing about GPT-3 is that I always, and I really, like when we get these new models,

10:43.200 --> 10:47.520
I really spend a lot of time with them trying to really understand them, trying to like just sort of

10:48.080 --> 10:53.840
feel like I get the personality of these models if you'll forgive the term because these models,

10:53.840 --> 10:57.040
you know, they're not one thing, right? They're really this whole distribution of things.

10:57.040 --> 11:01.920
But so for GPT-3, I really spend a lot of time trying to teach it. You know, I have this whole chat

11:01.920 --> 11:06.160
session where I was a teacher and I was explaining to it how to sort of list of numbers. And it would

11:06.160 --> 11:10.560
do one example, get it right, and I'd be like, wow, I've really taught it the process of sorting.

11:10.560 --> 11:13.520
And then I'd give it another example and it would totally go off the rails and do something wrong.

11:14.080 --> 11:18.880
And I think that the feeling that I had was GPT-3 didn't really want to list. And like it really

11:18.880 --> 11:22.960
felt like this, this, you know, this, this being that like had a short attention span and would

11:22.960 --> 11:27.680
just kind of like do random things sometimes. And I think that that's probably a reflection of

11:27.680 --> 11:31.280
the training data in some ways, right? If you're out there on the internet and you read some text saying,

11:31.280 --> 11:34.240
okay, now I'm going to sort of list of numbers. I mean, maybe you're in the middle of the fiction

11:34.240 --> 11:38.080
story, right? And then like, you know, that some aliens arrive or something. And so it's actually

11:38.080 --> 11:43.200
reasonable for GPT-3 to make pretty, pretty arbitrary predictions when it's not very confident

11:43.200 --> 11:50.000
what should come next. But by contrast and code, what I found with Codex is that when it fails,

11:50.000 --> 11:54.320
it does half my instruction, but not the full instruction, right? And sometimes, you know,

11:54.320 --> 11:58.640
sometimes you can end up with the traditional failure modes of autoregressive models where it fails

11:58.640 --> 12:02.800
by repeating a token over and over if that's the most certain one. And basically most of my

12:02.800 --> 12:07.600
experiments have been, I haven't had to fuss with hyper parameters and I really just set

12:07.600 --> 12:12.160
temperature equals zero, so it's just always picking the most likely token. And it's worked out

12:12.160 --> 12:16.720
way, way better than for any model that I've tried before. And I think that a lot of this comes

12:16.720 --> 12:21.680
back to the structure of the data, right? That in code, if I have a comment saying, now I'm going to

12:21.680 --> 12:25.360
sort some numbers, you're really going to sort numbers next, right? There's really nothing else

12:25.360 --> 12:30.160
that's about to happen. And so it's almost like we have this great data set that we've built up

12:30.160 --> 12:36.640
of instruction following. And I think that that idea we found in GPTland was pretty key to

12:36.640 --> 12:40.080
getting something that's even more useful to people. And in code, it's almost built in.

12:41.520 --> 12:50.640
Yeah, I'm very curious about this idea of, you know, the code is a data set and the self-documenting

12:50.640 --> 12:56.000
nature of it. When you think about just kind of raw code that you might find in GitHub,

12:56.000 --> 13:02.800
you know, there's documentation that's going to be at a, I would think a pretty low kind of

13:02.800 --> 13:09.040
semantic level, like, you know, this loop is going to do thing X. You know, I think of something

13:09.040 --> 13:15.840
like a stack overflow that's talking about the code that you might see in a post at a much higher

13:15.840 --> 13:25.680
level. And I wonder a little bit about, you know, as all of, you know, to what degree is the code

13:25.680 --> 13:30.400
that codex is trained on, you know, GitHub style versus something that might have some more like

13:30.400 --> 13:35.280
higher level semantic meaning. And, you know, just your thoughts on whether that matters and,

13:36.400 --> 13:43.520
you know, how codex might evolve with different types of data that you trained it on.

13:43.520 --> 13:47.200
Yep. I think the answer for this stuff has probably got to catch them all.

13:47.200 --> 13:52.080
Like, I think we're at a point with these models and I think GPT kind of set the stage for it,

13:52.080 --> 13:58.080
is that the broader you go, the more capable you're going to get. Yeah. And the part of it is that

13:58.080 --> 14:03.280
when we do a task, it's kind of impossible to predict exactly what skills people want to bring to

14:03.280 --> 14:08.880
bear, right? Like, that the, you know, it's almost like, if you rewind to the, before the, you know,

14:08.880 --> 14:13.840
general purpose computers were obviously the right solution, which by the way, they're not even

14:13.840 --> 14:16.480
obviously the right solution for all problems, but for most problems they are.

14:17.120 --> 14:21.920
There were specialized machines for each individual application. And people were always like,

14:21.920 --> 14:27.200
well, your general purpose computer is cool and all. It's great demo, but if you really want to do,

14:27.200 --> 14:31.840
like, you know, your, your, your contact book, you need to use this specialized, you know,

14:31.840 --> 14:38.240
IBM, whatever, you know, machine that existed at the time. And I think that the basically just turns

14:38.240 --> 14:44.400
out that many tasks require mixing and matching between lots of different things. And so it's kind

14:44.400 --> 14:50.480
of hard to pre-bake one answer to everything. And so where we've started has been, you know, again,

14:50.480 --> 14:54.720
kind of all the text out there and all the public code. But I think that within code, you know,

14:54.720 --> 15:02.080
it's not just like, you know, Django and projects like that. It's also think about all the ipython

15:02.080 --> 15:08.880
notebooks that people put on GitHub, right? And that those ones tend to be very much like a

15:08.880 --> 15:13.440
tutorial, right? There's lots of lots and lots of casual tutorials and things like that that

15:13.440 --> 15:18.880
are out there. And so you get kind of a very different slice of intelligence from those.

15:18.880 --> 15:22.960
And I think that what we've been looking at, like, I think kind of a big next step really is

15:22.960 --> 15:27.120
figuring out what are the best sources? You know, what do you learn from each one? How do you

15:27.120 --> 15:34.480
figure out what you want to balance in that model? And one thing that people probably might be

15:34.480 --> 15:39.520
surprised to hear is that codex, you know, it can do lots of things in lots of different languages.

15:39.520 --> 15:43.840
You know, it's probably pretty good at about it doesn't different languages. But we really trained

15:43.840 --> 15:47.200
it just for Python. Like, we actually were just like, we just want this thing to be as good

15:47.200 --> 15:54.000
as Python as we can. And all the other stuff kind of fell out as almost an accident. So I think that,

15:54.000 --> 15:59.120
you know, if you test it, it'll be interesting to see, you know, do people find it very useful

15:59.120 --> 16:03.680
for the broad range of languages? Or is, you know, sort of that focus on Python? Does that shine

16:03.680 --> 16:08.160
through? Nice. Nice. I can tell you that it does do hello world and list.

16:08.160 --> 16:12.400
Okay, good. There we go. Believe me, we did not try to make a good list.

16:16.560 --> 16:23.120
You know, also fascinated by this, this idea that we talked about earlier, you know, the

16:23.120 --> 16:28.400
language, the natural language, plus the code. And there's part of me that would love to like

16:28.400 --> 16:35.360
tweak some hyper parameter that lets you wait one versus the other. Any, you know, any thoughts

16:35.360 --> 16:41.760
on that? Or I suspect that your answer is going to be similar to the last one, which is kind of

16:41.760 --> 16:46.480
the more the merrier, all having all the data, you know, is going to get you better results and

16:46.480 --> 16:51.360
trying to over optimize or yeah, yeah, I think some of the stuff you're you're hitting on the

16:51.360 --> 16:55.680
right front here, as I think, right? And look, like just to zoom out to the big picture,

16:56.640 --> 17:01.360
to me, the most fascinating thing, first of all, is that this is all just a neural net, right? Like,

17:01.360 --> 17:06.960
you rewind back to the 40s and, you know, pits and McCullough, like that model of the information

17:06.960 --> 17:11.120
processing of the brain, like that's the thing we're still doing today. And so, you know, you can

17:11.120 --> 17:16.560
actually find this great paper on Wikipedia called like, you know, an interpretation of the

17:16.560 --> 17:22.240
history of the, of the perceptron. And the, you know, the story ever and always told about

17:22.240 --> 17:26.480
the perceptron was like, hey, in the 60s, these neural net people overhyped everything and,

17:26.480 --> 17:30.160
you know, all the funding went away. And if you actually look at the historical documents,

17:30.160 --> 17:33.440
kind of what was going on is there were two competing camps. There was the symbolic systems

17:33.440 --> 17:37.280
people. And then there were the neural net people. And that this symbolic systems people had a

17:37.280 --> 17:41.840
very concerted campaign to try to drive all the funding for the neural net people. And that they

17:41.840 --> 17:45.920
had all these disparaging things to say, like those neural net people, they have no new ideas.

17:45.920 --> 17:50.320
They just want to build a bigger computer. Like that's all they want to do. And, you know, here we are,

17:50.320 --> 17:55.120
you know, 40 years later, 50 years later. And yeah, we just want bigger computers and more data.

17:55.120 --> 17:59.520
And so I think that is actually the most core answer. Like, you know, I think that we all kind of

17:59.520 --> 18:03.440
want the great scientific insight of like, you know, to, to figure things out and to figure out

18:03.440 --> 18:08.000
the exact theory of mixing. And I think actually the funny thing is I think we can make progress

18:08.000 --> 18:13.200
on those problems. But the highest, highest order bit is you need to have a big machine with

18:13.200 --> 18:18.080
thoughts of compute and pour in all the data you can. And like, you know, at some point that that

18:18.080 --> 18:22.080
that the details that mix start to really matter. But the highest order bit is actually achieving

18:22.080 --> 18:29.440
that first thing. Does that, you know, to what degree does that like cap innovation? If you've

18:29.440 --> 18:34.160
already, you know, pulled all the language in the world, all the text in the world into GPT-3

18:34.160 --> 18:41.360
and all the code in the world into codex. And it's all about, you know, data and size of compute,

18:41.360 --> 18:46.240
where do you go to innovate? Yeah. So I think it's a great question. So on the one hand,

18:46.240 --> 18:51.040
you can look at what I said as a pretty depressing thing. I'm just like, okay, it's just, you know,

18:51.040 --> 18:55.360
just a simple matter of, you know, doing this, this large-scale engineering. And you need to have

18:55.360 --> 19:01.120
your particle accelerator equivalent in order to do it. But actually, if you dig into the source

19:01.120 --> 19:06.320
of progress in recent years, you know, we've published a couple studies on this. And so we have one

19:06.320 --> 19:10.800
study that shows the compute ramp is insane. Like, it's just faster than any exponential that I'm,

19:10.800 --> 19:17.360
that I'm aware of. But we also have another study showing the algorithmic ramp, showing the

19:17.360 --> 19:22.400
efficiencies due to algorithms is also exponential. And you know, rather than being like, you know,

19:22.400 --> 19:26.800
doubling every 3.5 months, it's like, you know, more like, you know, doubling every year, year and a

19:26.800 --> 19:31.520
half, you know, something much more like Moore's Law. I forgot the exact number. But that's still

19:31.520 --> 19:36.880
a pretty insane rate of progress. And so I think that the truth of all of this is that if you have

19:36.880 --> 19:41.600
a paradigm that is worthwhile, right, that like, making it a more capable neural net, clearly a

19:41.600 --> 19:46.640
worthwhile thing at this point, you're going to innovate to the max in all dimensions. And yeah,

19:46.640 --> 19:50.400
we've had a pretty big compute overhang because people just weren't willing to spend lots of money

19:50.400 --> 19:54.800
on computers. And now people are. So they just spend more money to get ahead of Moore's Law. So that's

19:54.800 --> 19:58.960
one dimension. Similar story for data, you know, that there's been lots of data out there, just like,

19:58.960 --> 20:02.560
it just wasn't really worth people's effort to collect it or people didn't really know to do it,

20:02.560 --> 20:07.360
whatever it is, there's an overhang of just gather all that data. But on the algorithmic front,

20:07.360 --> 20:10.480
you know, I think that's been the one that people have been pushing on. And so there isn't as much

20:10.480 --> 20:14.080
of an overhang, you know, there's not like low hanging fruit left around that just no one's

20:14.080 --> 20:17.520
thought of that just, you know, you just show up and you're just like, gold's at my feet,

20:17.520 --> 20:23.120
it takes effort. But I think that the fruit is still there, right, that it's still the case that

20:23.120 --> 20:26.720
we are making this exponential progress there. So I think it's like, just because we're making

20:26.720 --> 20:33.280
big progress in certain dimensions, you know, that's just temporal, right, that like we cannot

20:33.280 --> 20:39.200
keep up the rate of improvement from those dimensions. And so, yeah, once you've saturated them,

20:39.920 --> 20:43.040
the only thing left is going to be this other dimension. So I think it's really important,

20:43.040 --> 20:45.920
we as a community don't lose that muscle that we really build it up.

20:47.440 --> 20:52.560
And now if I asked you to comment on that algorithmic dimension, would it be,

20:52.560 --> 21:00.240
would it be asking you to speculate into the future or is there, you know, a set of kind of,

21:01.040 --> 21:05.840
you know, relatively low hanging fruit that, you know, things that you know that directions

21:05.840 --> 21:10.240
that you know that you want to head on the algorithmic side? Yep. Well, I want to talk again about

21:10.240 --> 21:15.360
just, you know, first of all, my personal philosophy, you know, is very much like, you know,

21:15.360 --> 21:20.560
greatness through a thousand small steps that I really, you know, I think that there are some

21:20.560 --> 21:25.680
people who are extremely good at the like one big idea to change everything. And I think that,

21:25.680 --> 21:30.000
you know, like Ilya, who's one of my co-founders is extremely good at that. You look at like,

21:30.480 --> 21:34.560
you know, I think he's, you know, with work like Alex, and I think he's very good at sort of

21:34.560 --> 21:39.520
setting the direction. But for me, I tend to think in terms of, okay, like what are all the small

21:39.520 --> 21:44.080
details we have to get right to make this happen? And if you look at the current models, you know,

21:44.080 --> 21:50.160
the funny thing about GPT-3 is that it actually used the same, the tokenizer that Alec Radford,

21:50.160 --> 21:56.320
who works at OpenAI, wrote kind of like overnight, right before the deadline, you know, three years

21:56.320 --> 22:03.440
prior for GPT-1. And like, you know, that thing is not optimal. It's actually become kind of the

22:03.440 --> 22:07.520
standard lots of people use it. I mean, you know, people have done a little bit to, to, you know,

22:07.520 --> 22:11.840
play with different organizations and retrain the things like that. But fundamentally, I think that

22:11.840 --> 22:16.400
that there's like a big, you know, big shift in some ways in kind of small detail in other ways

22:16.400 --> 22:20.480
of just, we should be really doing by level models, right? We shouldn't be doing this like, let's

22:20.480 --> 22:24.560
like, you know, sort of tokenize things and duck and chunk them up in this like way that kind of maps

22:24.560 --> 22:29.200
to, you know, it's almost like hard coding that's in the model that probably would do a lot better

22:29.200 --> 22:33.200
if it wasn't there. I think a lot of the story of neural nets has been removed the hard coded stuff

22:33.200 --> 22:38.400
and add in learning. So I think that's one example of the kind of thing that I would really love

22:38.400 --> 22:43.520
to see someone work on and just see, to see great results from and for us to incorporate that.

22:43.520 --> 22:49.680
So I think basically little bits of the architecture that are still like, yeah, we really should be

22:49.680 --> 22:54.400
doing this differently. I think that that for me is actually where I put a lot of focus.

22:55.280 --> 23:01.200
I wanted to kind of transition to, you know, how we should think about codex as like users and

23:01.200 --> 23:06.160
practitioners. Those folks that want to play with it, like, how should we, you know, think about

23:06.160 --> 23:11.440
interacting with this API to get the most out of it? And let's maybe start with, what is it best

23:11.440 --> 23:19.280
that versus where the, you know, the soft edges? Yes. Well, I will first say, I think that no one

23:19.280 --> 23:24.880
knows yet to the answer of, what is it best at? Like, I can tell you what I've discovered in my

23:24.880 --> 23:29.120
efforts, right? And I'll say, for me, I know I'm scratching the surface. Like, I know I am.

23:29.120 --> 23:33.760
Fair enough. But, and that's, that's the wonderful thing, by the way, you know, if you train a vision

23:33.760 --> 23:38.480
model on an image net, you know what it's good at, right? It's very, very good at all the dog breeds.

23:38.480 --> 23:44.320
Um, this model, general purpose, so it's quite good at lots of things. Um, I have, so for, for me,

23:44.320 --> 23:49.120
you know, I really latched on to this, uh, being able to provide instructions in natural language

23:49.120 --> 23:54.560
and have it generate an executable output, right? So basically talk to your computer, does it?

23:54.880 --> 23:59.680
Um, when, you know, when we first started playing with the model, like, it wasn't clear that it

23:59.680 --> 24:03.840
would be good at that. And I just kind of realized, like, hey, this model, when I give it these, like,

24:03.840 --> 24:06.960
because I actually started out the other, on the other side, I started out with trying to say,

24:06.960 --> 24:10.800
if I just want to provide one big instruction and have it write a whole program. And, you know,

24:10.800 --> 24:16.240
it's quite reliable at doing things like, I'd say make it to Kinter UI that, like, has a button that

24:16.240 --> 24:20.720
says, hello world, then you click it and send an email, like, that level of instruction, it could

24:20.720 --> 24:25.040
actually write, like, you know, the 30, 40 lines of Python to do it. And sometimes I make a little

24:25.040 --> 24:28.720
mistake. You'd forget to, like, wire up the button, or, you know, it kind of like a placeholder for

24:28.720 --> 24:33.040
whatever. Um, but the way it would fail was, again, very interpretable, right? Because look at

24:33.040 --> 24:36.720
it, I'd be like, oh, okay, just kind of forgot this piece. And so then I started thinking about, well,

24:36.720 --> 24:40.000
what I really want is I want to be able to chunk this instruction up into smaller pieces,

24:40.000 --> 24:44.160
because, you know, it did 80% of it. And so if I just had a 50% size instruction,

24:44.160 --> 24:50.560
maybe I'll do 100% of it. Um, and so I think that that's kind of the highest level picture of

24:50.560 --> 24:56.400
where we are is the model. I think it's not yet ready to do big things, right? On its own. But

24:56.400 --> 25:01.040
it's ready to do small things. And honestly, for programming, like, I like doing the big things.

25:01.040 --> 25:04.640
I don't like doing the small things myself, right? The, uh, the like, you know, okay, like,

25:04.640 --> 25:08.960
here's this very specific fiddley thing. And like, get the details of the indexing right. Like

25:08.960 --> 25:13.520
that kind of thing, the model, it knocks it out in the park, or memorizing the details of,

25:13.520 --> 25:17.360
you know, this, whatever framework, like, you know, I used to write and Ruby on Rails. And most

25:17.360 --> 25:21.440
of Ruby on Rails is just knowing what the Railsism is to do. Any particular thing. Right. Right.

25:21.440 --> 25:26.560
And, um, yeah. And by the way, I mean, like, ID's are just not very good at Ruby on Rails,

25:26.560 --> 25:30.080
because there's so much dynamism and things like that. But this model, I think, would be,

25:30.080 --> 25:34.080
would be quite good at it. So I think basically figuring out how to work with those strengths is

25:34.080 --> 25:39.360
important. And so part of it is, I think, like, one dimension that I think is very exciting is

25:39.360 --> 25:45.600
baking it in as an interface to lots of existing applications. So we have a example of baking to

25:45.600 --> 25:51.600
Microsoft Word. I think that for any website, really, you should now be able to very easily build

25:51.600 --> 25:56.080
an interface where you just say, like, you know, you know, what, if you're depending on what

25:56.080 --> 26:01.200
your web app is, you know, go and look up the, you know, go send an email to this person or,

26:01.200 --> 26:06.000
I, you know, like, yeah, any of the functionality that's in your website should become voice

26:06.000 --> 26:09.920
controllable or, you know, natural language controllable without having to nest or you click through

26:09.920 --> 26:15.520
a bunch of buttons in order to get there. You mentioned that some of your initial observations

26:15.520 --> 26:22.240
were that you, you construct this prompt and it will spit out results that, you know,

26:22.240 --> 26:29.440
were 80% there or missing some detail or something like that. And that you solve that by kind of

26:30.640 --> 26:37.520
chunking your prompts and making them smaller and more compact. Do you think the issue that you

26:37.520 --> 26:47.040
experienced originally was, you know, was on the input side or the the output side, if that makes

26:47.040 --> 26:52.880
sense, was it a, you know, issue in the, a fault in kind of the generation process or it, you

26:52.880 --> 26:58.160
know, couldn't pull the piece for that, you know, couldn't make the connection necessary required

26:58.160 --> 27:03.360
for that last 20% or was it, you know, forgot it in the parsing stage, using really rough language

27:03.360 --> 27:08.720
here to, yeah, yeah, you're asking the great mysteries of the what's going on inside the neural net,

27:09.280 --> 27:15.600
which, you know, I think is always very interesting. And, you know, for me,

27:15.600 --> 27:23.600
well, first of all, I also think that if you asked literally me to do the same task without

27:23.600 --> 27:28.720
access to an interpreter, so I just have to write the program once without ever being able to push

27:28.720 --> 27:34.320
backspace, I'm not going to do a good job either, like trust me, like I will not. Most of programming

27:34.320 --> 27:39.200
for me is I write a little bit and I run it and it doesn't work and I change it and I fix it and

27:39.200 --> 27:45.120
I iterate and I fix it, you know, and that, that other piece, this model doesn't get to do it at all.

27:45.120 --> 27:50.960
So I think that it's very possible that the model simply cannot, like, you know, just from reading

27:50.960 --> 27:55.200
all that text and really deeply thinking through all the details about the interface should work

27:55.200 --> 28:00.000
is a bottleneck. And then secondly, it's very possible that just like, it just as it's writing,

28:00.000 --> 28:04.080
it just runs, oh no, I really wish that I'd like implemented this function beforehand. So you

28:04.080 --> 28:08.080
know what, I'll just pretend that it's implemented later and like, you know, that never gets to it.

28:08.080 --> 28:13.280
So I don't know which of those stories is more true, my guess is that it's a mix of both.

28:14.240 --> 28:18.720
And partly, I just look at myself, like, you know, look, this is not a human-like intelligence,

28:18.720 --> 28:25.040
so it may be too, you know, a little bit too egocentric to think that I can look to what I'm good at

28:25.040 --> 28:32.160
and bad at to map to where the model makes mistakes. But I will say that for me, it's been actually,

28:32.160 --> 28:39.360
like, I feel much more in tune with the failures and successes of Codex than I did with GPT-3.

28:39.360 --> 28:43.200
For me, it does feel like when it fails, I'm a little bit like, you know, sometimes,

28:43.200 --> 28:46.320
sometimes the way it fails, by the way, is it'll just put in pass, you know, so it's like, you know,

28:46.320 --> 28:50.400
I have a nice Python, you know, death, whatever, and like, I put it in a doc strand, like, okay,

28:50.400 --> 28:53.840
model, you go now, and that's solution is just to put in pass or, you know, comment to do,

28:54.720 --> 28:58.960
something like that. And I get it, it's a little bit like, it's like, okay, I'm not going to be

28:58.960 --> 29:02.960
able to do this, so I'm not even going to try, right? And, you know, I don't think that's necessarily,

29:03.680 --> 29:08.400
the, you know, the only characterization. But it really feels like, you know, if you think of

29:08.400 --> 29:14.400
how code is usually structured, that I think that it actually starts to feel a little bit more,

29:14.400 --> 29:18.720
like, constrained in terms of the, the, again, you know, you have this pattern of comment,

29:19.280 --> 29:22.720
complete, or total out, and kind of nothing in between.

29:22.720 --> 29:34.000
Yeah, you just mentioned the structure that code tends to have the, you know, codex operates

29:34.000 --> 29:41.200
like GPT, GPT-3 in this kind of input, you know, process output paradigm. Have you done any

29:41.200 --> 29:48.800
playing around or experimentation to try to force fit structure into that input in a way that

29:48.800 --> 29:55.280
it understands that it can produce more, you know, structure on the output? Well, so I have one,

29:55.280 --> 29:59.360
so I have, I have a couple of different dimensions that I think are very interesting, right? So,

30:00.320 --> 30:05.040
look, there's one dimension that I think is kind of fun, which is translating between languages.

30:05.040 --> 30:12.000
And so I have a little demo of a writing, writing a Python program, so I wrote a Python program,

30:12.000 --> 30:17.520
that then you run it, make some calls to the API, generate some Ruby code, and that Ruby code is

30:17.520 --> 30:23.280
just a program that calls the API to generate some Python code, and you get this Python Ruby oscillator

30:23.280 --> 30:27.440
forever and ever. It's a little bit like writing a coin. It's just like kind of a fun, fun little thing.

30:28.560 --> 30:33.280
I actually tried doing the same thing for Python to Ruby to JavaScript to Python to Ruby JavaScript.

30:33.280 --> 30:39.360
I got it to do like six cycles of Python Ruby JavaScript before it broke. So it actually was like

30:39.360 --> 30:44.080
each time writing a little bit of unique code, which is kind of a cool thing to see. So setting it

30:44.080 --> 30:48.480
up for that, I think, was a very interesting challenge, because there you really have to make sure

30:48.480 --> 30:53.200
that your your prompt which is kind of contained within the program is something that kind of like

30:53.200 --> 30:58.400
gives the enough context to the API for it to actually generate the whole new program, but

30:58.400 --> 31:02.800
you know, it's like you really got to play some some some nice fiddley games to make it happen.

31:02.800 --> 31:06.800
So, you know, that I think is more of a proof of concept. It's more of like an interesting

31:06.800 --> 31:11.840
exercise than it is something very practical. But there's actually another direction that I was

31:11.840 --> 31:17.360
experimenting with that I I think it's like interesting and very fruitful. Someone can make it work of

31:18.240 --> 31:22.880
you know, look, programming is two things. It's understand the super hard problem and decompose it.

31:22.880 --> 31:28.160
Right. So it's basically problem decomposition and then mapping the small problem to code. We've

31:28.160 --> 31:33.120
already said codex is really good at that second thing, probably better than I am. That first one

31:33.120 --> 31:37.360
is it actually bad to it. And all I know is that the obvious ways of making it good at it,

31:37.360 --> 31:42.960
I haven't succeeded at, but I but using codex for task decomposition is something I've tried a

31:42.960 --> 31:48.240
little bit and got some interesting results on. And you know, you can do things like you have

31:48.240 --> 31:53.760
codex call into, you know, you basically tell it, oh, there's this like magical oracle function.

31:53.760 --> 31:58.640
And so oracle is you give it some natural language and then just like the machine will magically

31:58.640 --> 32:02.960
implement it for you. And then you say, okay, do this hard task and you get access to call

32:02.960 --> 32:08.160
the oracle thing. And then you can see it can codex generate good calls, subcalls to oracle.

32:08.160 --> 32:12.720
And I've actually gotten it to as a little bit of like a, you know, together working with codex

32:12.720 --> 32:16.800
to be able to get it to do things like, you know, go on Google and like download an image of,

32:16.800 --> 32:20.480
you know, a particular person and put into a website and things like that. And you know, you

32:20.480 --> 32:27.520
use Selenium to to orchestrate all of this. And I think that ideals like this are very interesting

32:27.520 --> 32:34.000
because maybe you can actually have codex as a tool that helps in more of the cognitive domain

32:34.000 --> 32:42.640
in addition to this like very mechanical like code emission domain. Is there a, you know, input

32:42.640 --> 32:49.520
pattern that you've seen or a hyper parameter that can kind of guide it towards a degree of

32:49.520 --> 32:58.080
complexity in the solution? Like there's a, the length of the output, you know, as a, as a, you

32:58.080 --> 33:03.440
know, one idea that might be that, you know, hey, if I say, you know, give me hello world and I

33:03.440 --> 33:07.680
want it to be, you know, 300 characters in length or a thousand characters in length, that's going

33:07.680 --> 33:12.640
to be, you know, one thing. If I say, you know, 10,000 like, is it going to give me the, you know,

33:12.640 --> 33:20.240
the J2EE enterprise? I mean, I think the best, the best starting point, by the way, for all

33:20.240 --> 33:24.160
these things, the only real answers you got to try it, right? Like you really seem to play with it.

33:25.840 --> 33:30.160
But I think the place to start is just by asking the model for what you want. And if the model

33:30.160 --> 33:35.760
doesn't quite seem to get it, you try to spell it out more clearly, expand how you're asking,

33:35.760 --> 33:40.000
like really think about if this were a junior programmer and I had to really hold their hand and

33:40.000 --> 33:44.160
walk them through it, how would I do it? Right? And sometimes that's breaking up into multiple

33:44.160 --> 33:49.760
instructions. Sometimes that's just expand more of what you're asking for. So I think that's

33:49.760 --> 33:54.160
definitely the starting point. Another very powerful thing is providing more examples, right? So

33:54.160 --> 34:00.400
one thing we really haven't done very much of yet is trying to do GPT-3 style prompt engineering

34:00.400 --> 34:05.280
and trying to provide prompts to the model that really show examples of the behavior you want.

34:05.280 --> 34:09.280
And like all the indications so far and all the times that we've tried is that it's quite good at

34:09.280 --> 34:14.000
that. But we just haven't really pushed it in the way that we push GPT-3. In part because

34:14.000 --> 34:19.920
it's already capable of the tasks we want simply by asking. So we just kind of didn't have to go

34:19.920 --> 34:24.880
down that road. And then the third thing, of course, is fine tuning. And so we have a GPT-3

34:24.880 --> 34:32.240
fine tuning API these days. We'll be wrong that out for codex. And I think that that will open

34:32.240 --> 34:38.640
a new dimension to what you're able to make it do. Awesome. One of the interesting examples I saw

34:38.640 --> 34:46.400
in some of the materials was a, you know, not your traditional kind of creative program like,

34:46.400 --> 34:52.960
you know, XYZ, but it was to solve this word problem like, you know, from an elementary school,

34:52.960 --> 35:02.400
you know, Jason has six apples and four apples, something like that. But it created a program to

35:02.400 --> 35:08.880
figure out this word problem. Yes, right? I thought that was really interesting. And it made me immediately

35:08.880 --> 35:16.160
think about the implications of something like this in education, you know, both coding education,

35:16.160 --> 35:22.240
but, you know, more broadly in education. Any thoughts on that? Yeah. So the funny thing is when

35:22.240 --> 35:28.240
we were starting open AI, I, you know, I left my previous job and I knew I wanted to start a company.

35:28.240 --> 35:33.760
And I had three possible domains on my list. Number one was AI, which turned out to pan out.

35:34.560 --> 35:40.000
Number two was VR slash AR and I kind of scratched that off very quickly. But number three was

35:40.000 --> 35:44.560
programming education. You know, this is an area that's very near and dear to my heart. I feel like,

35:44.560 --> 35:48.480
you know, for my programming education, it was, you know, I started out very self-taught,

35:48.480 --> 35:53.120
just building stuff that I was excited about. And it was just hard, you know, it's just not very

35:53.120 --> 35:57.040
much fun to like, you know, it's like, you do W3 school's tutorial back in the day. I'm sure

35:57.040 --> 36:01.200
there are better tutorials now. But then you're just stuck staring at an editor and thinking about what

36:01.200 --> 36:05.280
do I build, right? And you run your thing and it doesn't work. And what do you do, right? And you

36:05.280 --> 36:09.200
don't know about a lot of concepts, you know, I didn't know about serialization. And so I was building,

36:09.200 --> 36:12.800
actually I built one of the first things I built was a chatbot game. So it was that you had a

36:12.800 --> 36:17.520
little chatbot that you could train by talking to it. And then you can have a little chatbot

36:17.520 --> 36:22.240
battle where you would like play this game where one window that you were talking to with a chatbot,

36:22.240 --> 36:26.800
one was a person and you had to distinguish which which which which which was which before your

36:26.800 --> 36:31.760
opponent would. And all this stuff, you know, I didn't know what serialization was. So I just like

36:31.760 --> 36:35.360
had this like I came up with a magical identifier that or you know, like a string of characters

36:35.360 --> 36:39.760
that I thought no one else would whatever type. And I use that as my record separator.

36:41.200 --> 36:45.920
And just looking back, I just wish that someone was there to say, oh, you should probably use JSON here.

36:45.920 --> 36:50.240
And then I'd be like, what's JSON, right? I'd go around, I'd figure out how to use JSON. And I would

36:50.240 --> 36:54.720
have just sort of cut off this whole tree. You know, there's a little bit of the tree that was very

36:54.720 --> 36:58.880
useful for me to figure out why is it useful to have serialization? Like, you know, why don't you just

36:58.880 --> 37:02.560
want to do your own record separator? You know, what are the problems? But there's a bigger tree

37:02.560 --> 37:05.840
of really implementing it and building up the library and trying to make it work and like that kind

37:05.840 --> 37:11.280
of thing that was a little bit of wasted effort. And so what I am excited to see with Codex is that

37:11.280 --> 37:15.760
we have a model that for the first time you can show it code and can actually kind of understand it.

37:15.760 --> 37:19.120
And so we've done a little bit of playing around with code explaining, right? And actually can do

37:19.120 --> 37:24.000
a decent job of taking a function and explains how it works or can generate comments for it or

37:24.000 --> 37:29.360
generate doc strings, generate unit tests. I think that all those things really open up the

37:29.360 --> 37:34.720
possibility of having a personalized programming tutor, right? And that to me is just like, it's

37:34.720 --> 37:40.320
amazing. I would love to be able to see programming education fall out from, you know, pursuing the

37:40.320 --> 37:45.200
AI passion and we will get there. It's just a question of, you know, I'm hopeful that Codex is

37:45.200 --> 37:54.960
enough at least to take the first steps. Does there need to be an element of, I guess I made a

37:54.960 --> 38:00.480
mental connection to like explainability in these kinds of models and, you know, tutor, you want

38:00.480 --> 38:09.120
your tutor to be able to explain to you the connections beyond just, you know, showing you an example

38:09.120 --> 38:16.720
which is kind of what Codex does now. That kind of called a mind, the whole explainability around

38:16.720 --> 38:21.520
these kinds of models to you and do you think that's a piece that would be interesting in that

38:21.520 --> 38:26.480
context? Yeah. So I think maybe in a non-traditional way. Like I think that the traditional explainability

38:26.480 --> 38:30.400
has been, we want to look at the connections of the neural net and explain why it made a decision

38:30.400 --> 38:34.560
that it made, right? But I mean, if you think about the equivalent problem for humans, we're not very

38:34.560 --> 38:37.920
good at either, right? You know, we don't open up the neurons of the brain and be like, oh, wow,

38:37.920 --> 38:42.000
look at the connection between these two neurons, right? You asked someone, why did you make that

38:42.000 --> 38:47.120
decision? And I think most of behavioral science is basically realizing that our own explanations

38:47.120 --> 38:51.920
of our actions are quite poor, right? That like, you know, you kind of do something and you come up

38:51.920 --> 38:58.640
with some like back narrative or why you did it. So I kind of feel like the baseline we should

38:58.640 --> 39:03.120
shoot for is that we should shoot, you know, look, we should get to a better place than where we are

39:03.120 --> 39:07.760
with humans in terms of being able to explain why decisions were made. But at the very least, I

39:07.760 --> 39:11.920
think it's a good baseline to hit. And so I think that what we should be trying to focus on with these

39:11.920 --> 39:17.280
models is that, you know, they rate some, you know, they're given a function and that they

39:17.280 --> 39:20.880
should explain how it works, that they wrote their own function, they explain why they wrote it,

39:20.880 --> 39:26.320
and that explanation actually adds up. You know, and like, maybe it turns out that in fact, just like

39:26.320 --> 39:31.360
the human version, that it doesn't quite correspond to, you know, sort of objective truth in some

39:31.360 --> 39:34.240
ways and that it says, well, I mean, the decision because of this variable and that variable,

39:34.240 --> 39:38.080
I mean, it changed that variable and it still does the same thing. You know, that kind of experiment,

39:38.080 --> 39:42.720
I think would be very interesting to see. But on the other hand, I think that for the super

39:42.720 --> 39:46.880
complicated task, and let's not hit ourselves, I mean, like, even writing the simplest program is

39:46.880 --> 39:51.280
a super complicated task of like, you just got to understand so many different concepts, you need to

39:51.280 --> 39:55.520
know this whole library of all these different functions. Like, that is really hard and I think that

39:55.520 --> 40:00.320
to even fit in our brain exactly like, okay, like, you know, how do I translate, how would I even

40:00.320 --> 40:07.040
write a program for, you know, say, you know, say it five times, you know, like something like that,

40:07.680 --> 40:11.360
what is it supposed to reference? Like, you know, five, like, how is that represented? Like,

40:11.360 --> 40:16.560
all the different ways you could see it. I think that for us to write a program that can do that

40:16.560 --> 40:22.000
is just going to be such a giant complex tree that even a trace through it would be extremely

40:22.000 --> 40:26.560
complicated and probably, you know, something that's outside of humans, humanity's ability to

40:26.560 --> 40:32.320
understand. So I think the trick is number one, having the, you know, just focusing these models on

40:32.320 --> 40:38.880
being able to provide good explanations that feel right at an intuitive level, but the feel like

40:38.880 --> 40:42.400
they were written by a person. And I think that that, we're on trajectory four, you know, I think

40:42.400 --> 40:46.880
that you can ask codex for this stuff today, and maybe it'll do a good job, you know, maybe it's

40:46.880 --> 40:51.440
not exactly what it was trained for, so maybe it won't, but I think that you can at least get started.

40:51.440 --> 40:56.720
But I think there's a next step, and this is actually part of our alignment work at OpenAI,

40:56.720 --> 41:02.160
is thinking about models that themselves are really optimized for explaining what another model did,

41:02.160 --> 41:06.000
right? Because here we have these, you know, with the super complicated problem that this model

41:06.000 --> 41:10.160
I came up with a solution for, and that it did in a super complicated way that we can't understand,

41:10.160 --> 41:13.760
but hey, we know what to train models that can do super complicated things that we don't understand,

41:13.760 --> 41:18.160
and so maybe you can't explain your model to do it. And I think that really finding the right

41:18.160 --> 41:22.640
balance here where you can have a very trustworthy model, and you know, that there's ideas that we

41:22.640 --> 41:28.400
have for how to actually do it, but maybe you can bootstrap your way to models that can actually

41:28.400 --> 41:32.640
solve problems where we don't even understand the solution, but then they explain, and they have

41:32.640 --> 41:37.440
to really prove to these other models that what they're doing is legit. And I think that this

41:37.440 --> 41:42.080
kind of thing is in our, you know, might take a while to get there, but that isn't our future.

41:42.080 --> 41:52.000
Yeah, some of the broader societal issues that, you know, something like a codex, codex gives rise

41:52.000 --> 42:01.440
to or questions like jobs, copyright, and potentially fairness bias. Who may be dig into those

42:01.440 --> 42:10.320
really quick thoughts on kind of job implications? Yeah, so I think that the interesting thing about

42:10.320 --> 42:16.240
codex in particular, as an example of AI in general, is that it's just not playing out how people

42:16.240 --> 42:21.840
expected, right? I think that the expectation was that AI is going to take this job, and then that

42:21.840 --> 42:28.160
job, and then this job, and the only question is just ordering the jobs in order of automation.

42:28.880 --> 42:34.720
But in reality, I think AI is kind of taking no jobs, and it's taking a percentage of all jobs

42:34.720 --> 42:40.240
at once, and that percentage tends to be the kind of boring, drudge work stuff. And I think that's

42:40.240 --> 42:44.640
actually a pretty inspiring picture, right? You look at it in the case of codex that programming,

42:44.640 --> 42:50.320
you know, being a software engineer requires you to talk to users, understand what users want,

42:50.320 --> 42:54.960
come up with an idea of the thing that they are going to be excited to use, and have this picture

42:54.960 --> 42:58.400
of like how you're going to build it. So there's the architecture of the system. When you come

42:58.400 --> 43:03.360
to implementing, you want to design in a way that will be future compatible. So, you know, tomorrow

43:03.360 --> 43:06.720
users are going to ask you for something else, and you should make it so, you should make it so

43:06.720 --> 43:10.640
it's really easy to build that feature, right? So you kind of anticipate all the different ways

43:10.640 --> 43:16.320
that you might want to modify your system. None of that is, and then you also want to write, you

43:16.320 --> 43:19.680
know, you want to implement using a framework and, you know, know that exactly. After all that,

43:19.680 --> 43:26.160
it's API docs and Stack Overflow. Exactly. Exactly. Exactly. So we actually have very poor tools

43:26.720 --> 43:33.040
for those those that last piece, but that's not what we want to spend our time on. And so I think

43:33.040 --> 43:36.560
what we're going to see with codecs, and I think that this again, I think is representative

43:36.560 --> 43:42.400
of the kind of AI we're building is we're going to find that the kind of like the hard, you know,

43:42.400 --> 43:48.640
the drug work, the part that is like you need to know the whole encyclopedia of your field that,

43:48.640 --> 43:53.360
you know, just like even coming with an idea of where to start, like those problems that I think

43:53.360 --> 43:58.720
are real barriers to people getting started, those are going to start really melting away.

43:58.720 --> 44:02.240
And then that will free up people to actually work on the exciting stuff.

44:06.080 --> 44:10.480
Copyright is the the next one I know that, you know, the big issue here is that there are no

44:10.480 --> 44:15.840
answers, and the system hasn't quite figured it out yet. But I'm wondering what your quick take

44:15.840 --> 44:20.240
is on that. Yep. So, you know, I think that, you know, our position is definitely that, you know,

44:20.240 --> 44:26.640
training training on, you know, publicly available code and and text is very use. But I think that

44:26.640 --> 44:32.560
it's definitely the case that the technology here is running ahead of the law, right? I think that,

44:32.560 --> 44:37.200
you know, that's something that I think is has happened many times in the past. And so I think

44:37.200 --> 44:41.520
that it's time for a public conversation about this, like part of the reason that we're doing

44:41.520 --> 44:46.960
a preview here, you know, that this is a API that will be available starting to roll out now,

44:46.960 --> 44:50.400
is that we want that feedback. We want to start that conversation. And, you know, technologies

44:50.400 --> 44:54.480
like Codex, you know, I think they have a lot of potential. I think we would, you know, be doing

44:54.480 --> 44:59.920
it to service to ourselves if they weren't easy to build on the work, lots of people weren't able

44:59.920 --> 45:04.640
to use them. So I'm very hopeful that we can figure out how do we get the good of these systems

45:04.640 --> 45:09.600
and get lots of benefits. And, you know, just really help, help it, help it supercharge the economy

45:09.600 --> 45:13.040
in a way that we think is, you know, doing the right thing for everyone.

45:14.080 --> 45:20.480
And are there fairness bias types of issues that have come up through in the context of Codex?

45:20.480 --> 45:26.320
For sure. Yeah, I think that fairness and bias are kind of a key part of AI. And I think that,

45:26.320 --> 45:29.600
you know, one thing that, you know, first of all, I think that those issues themselves, I think,

45:29.600 --> 45:34.400
you know, deserve a lot of space because, you know, we're building these systems that, you know,

45:34.400 --> 45:39.840
that they are being trained on data that is generated by all of us, right? And that if you're,

45:39.840 --> 45:43.920
if you're, you know, sort of not careful, you're going to lash onto the wrong things or help amplify

45:43.920 --> 45:47.520
biases that exist in the system. So I think that this is always going to be an important thing.

45:47.520 --> 45:54.080
And the stakes are just going to raise as we go. But I also want to point out that I think that

45:54.080 --> 46:00.080
Codex also represents a bit of a raising of the stakes of the kinds of fallout that you can get from,

46:00.080 --> 46:04.960
from a misbehaving system, right? You know, that if you generate some code with Codex and it does

46:04.960 --> 46:10.560
decide to delete all your files, that's probably not something you want, right? So I think that we need

46:10.560 --> 46:16.320
to figure out what values go into these systems and that, you know, we have some preliminary work on

46:16.320 --> 46:22.400
this that I think we've, we've, you know, published, published a bit on already. But I think you

46:22.400 --> 46:27.360
also need to think about how do you really align these, how do you technically align these systems

46:27.360 --> 46:31.200
with whatever values should be in there? And I think that, you know, look like we've got some

46:31.200 --> 46:35.360
technical problems ahead of us, but I think the question of, you know, both who are the people who

46:35.360 --> 46:39.840
are actually building it and making sure that that that is diverse and representative enough,

46:39.840 --> 46:45.600
I think is pretty, pretty critical. But also the question of, you know, how exactly are those values

46:45.600 --> 46:49.920
chosen, you know, who makes that decision? I think one day that's going to be kind of the most

46:49.920 --> 46:54.720
important problem that we as a community and, you know, we as a society are facing. And so I think

46:54.720 --> 46:58.560
that, you know, it's never too soon to start really, really working hard on these problems.

47:00.160 --> 47:07.280
Related issue is access and accessibility. And that's maybe a segue to kind of the rollout plan

47:07.280 --> 47:13.120
for Codex. Yes. A little bit about that. Yeah. So we really want this technology to be out there

47:13.120 --> 47:17.280
and used. We think you can deliver a lot of value. And we think that it's like a little taste of

47:17.280 --> 47:22.800
a future to come. So that's really important to us. We're going to do the same kind of playbook

47:22.800 --> 47:26.800
we did with the GPT-3, where we're going to have a private beta. We're going to roll it out as

47:26.800 --> 47:31.680
quickly as we can safely. We're going to be scaling it up that the invites will start flowing on

47:31.680 --> 47:36.960
Tuesday. So again, whenever we seize this podcast, that the first invites will all be out. And,

47:36.960 --> 47:40.640
you know, honestly, we just want to learn, right, that we have a new technology here in the best

47:40.640 --> 47:45.280
way to understand how it will impact the world, is by actually seeing it impact the world.

47:45.280 --> 47:51.920
And our philosophy is very much try to get, you know, a broad slice of usage at smaller scale

47:51.920 --> 47:56.160
and scale it up as you go. And there's very particular things that we did for GPT-3. You know,

47:56.160 --> 48:00.320
we have an academic access program in order to make sure that the, you know, the academics are

48:00.320 --> 48:04.480
able to get access. I think that, you know, for this, I think that there's going to be different

48:04.480 --> 48:08.080
segments that are going to be excited about using it. You know, I think that people who are

48:08.080 --> 48:13.040
programming, you know, students, I think are like one segment who we want to make sure that this

48:13.040 --> 48:17.840
is accessible to. So we really want feedback. We really want to see how people are excited about

48:17.840 --> 48:22.240
using our technology. And we are very excited about you using it. And honestly, we need,

48:22.240 --> 48:27.920
need your help to understand it. Awesome. Awesome. And is there, was there something about a

48:27.920 --> 48:34.080
competition that you're hosting for this? Yes. So Thursday, 10 AM. So I don't know what time

48:34.080 --> 48:40.000
you're planning on releasing the podcast. But Thursday, 10 AM, we are going to have a new kind

48:40.000 --> 48:46.800
of programming competition. So you will be able to use Codex as both your teammate and a competitor.

48:46.800 --> 48:52.240
So everyone's going to get access to some number of queries to Codex while doing Python programming

48:52.240 --> 48:57.680
challenges. And it should be very exciting. There will be a leaderboard for the whole internet

48:57.680 --> 49:02.720
racing to solve these challenges. But really, the goal is to get a sense of what does it like to

49:02.720 --> 49:07.200
work alongside Codex. And this is one way we can really accelerate access to everyone and give

49:07.200 --> 49:12.880
them a chance to get a little taste of it. Awesome. Well, of course, we'll have pointers in the

49:12.880 --> 49:19.120
show notes for this episode. But Greg, thanks so much for taking the time to give us what is

49:19.120 --> 49:25.600
effectively a preview, a sneak peek, although it will be released by the time this shows public.

49:25.600 --> 49:35.600
Like, great to have you on the show once again. Yep, great to be back. Thank you so much.

