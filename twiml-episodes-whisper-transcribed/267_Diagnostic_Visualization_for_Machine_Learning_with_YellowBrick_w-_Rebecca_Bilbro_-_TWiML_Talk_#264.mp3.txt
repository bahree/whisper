Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're featuring a series of shows that highlight just a few of
the great innovations and innovators at the intersection of three very important and
familiar topics, data science, the Python programming language and open source software.
To better understand our listeners' views on the importance of open source and the projects
and players in this space, I'm conducting a survey, which I'd be very grateful if you
took a moment to complete.
To access the survey, visit Twimbleai.com slash Python survey.
Please hit pause now and we'll wait for you to get back.
That's twimbleai.com slash Python survey.
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this
series IBM.
Speaking of open source, IBM has a long history of engaging in and supporting open source
projects that are important to enterprise data science, projects like Hadoop, Spark,
Jupiter and Cubeflow to name just a few.
IBM also hosts the IBM data science community, which is a place for enterprise data scientists
looking to learn, share and engage with their peers and industry renowned practitioners.
Here you'll find informative tutorials and case studies, Q&As with leaders in the field
and a lively forum covering a variety of topics of interest to both beginning and experience
data scientists.
Check out and join the IBM data science community by visiting IBM.com slash community slash
data science.
All right, everyone, I am on the line with Rebecca Billbro.
Rebecca is the head of data science at ICX Media and co-creator of Yellowbrick.
Rebecca, welcome to this weekend machine learning and AI.
Thanks for having me, Sam.
I'm excited to be here.
I am excited to dive into our conversation as well.
This should be an interesting one.
Yellowbrick is a library focused on visualization and podcasts are not the best medium for conversations
that rely heavily on visualization, but I think we will do okay.
Why don't we get started by having you tell us a little bit about your background and
how you got involved in data science and what led you to create Yellowbrick?
Well, I am a practicing data scientist.
I specialize in applied machine learning applications and in natural language processing
usually specifically.
Like you said, I currently lead the data science team at a company called ICX Media.
So ICX Media is a social video intelligence company here in Washington, DC, which is
where I live.
And our focus is on using machine learning and natural language processing and data science
to understand what different audiences are currently watching and what they're talking
about.
And we use that to sort of spot trends and let our clients know how basically how to make
more of the kind of stuff that people are going to be excited to watch.
Well, we don't need natural language processing and machine learning to know what people
are watching and talking about today.
Well, that's right.
So yes, currently there's a lot of dialogue about end game and game of thrones, but we're
not going to talk about it because some people maybe are not up to speed and we don't want
any spoilers.
But you asked sort of how I got into data science.
I always love, you know, I love this part of the podcast.
It's so fun to hear kind of how everybody got here.
There's a million different stories of how people landed in this field.
And so for me, I've always worked in the space between I would say like natural languages
and formal languages.
So as an undergrad, I was a double major.
I'm double majored in mathematics and English.
And then when I went to graduate school, I decided to study communication and visualization
practices in engineering.
And so after I finished my PhD, I came to DC and that was in 2011.
And so this was, I joined the federal service as a presidential management fellow.
And so this was during the Obama administration, they were putting a lot of kind of focus on,
you know, analytics and kind of more data driven strategies inside the government.
And this was just around the time that data science had started really happening.
And so my job was basically analyzing like very, very messy data.
But I didn't have access to a lot of tools.
I mean, I think a lot of public servants can appreciate that, you know, the access to
tools is somewhat limited.
But in particular, I had a real hard time accessing tools to allow for like wrangling and modeling
unstructured data.
Okay. So I was working with a lot of like logs and reports and manifests and news stories
and speeches.
And there really just wasn't a lot of tooling out there.
And so I basically, I taught myself Python.
And I started building my own tools.
And eventually that was my full time job.
And I realized I could kind of take that and make that my full time job.
And I started teaching data science and teaching machine learning because I realized there were
a lot of other people who were kind of in the same boat and had a lot of the same kind
of needs that I that I had encountered.
Are you able to isolate what it is that prompted you to take the additional step to start building
your own tools?
Like I talked to a lot of people on the podcast that learn a programming language because
they have some problem.
But often, you know, you learn a programming language and the tool set that it offers
you, but then building your own tools and then further kind of publishing those and supporting
those for others.
Like what is it that even told you that you could do that?
Like what got you down that path?
Oh, that's that's a great question.
So I guess it would be, you know, one, one, one answer that I could give is that, you
know, I was already in public service, you know, when I started getting started doing
this.
I already kind of had the sense that what I was doing was for more than just me, that
it was for some greater good, but I think that really the longer that I've been working
in open source software and people who are, like you said, kind of building these tools
and putting them out there into the world, I would say that open source is a lot less like
public service and more like activism or maybe for a lot of people, it's more like art,
you know, that people are making the change that they want to see in the world.
And it's the sort of wild and organic space and it's just fun, you know, it's sort of,
it's fun.
And once you sort of start doing it, it feels like, you know, it feels like the right
way to do software.
Awesome.
So what is yellow brick?
Maybe it might help if I sort of give like a little bit of a history of like why we made
yellow brick.
So it was, it really has to do with actually moving to trying to do data science and moving
to try to teach data science.
And when I started teaching, I realized like how poorly I was able to describe why I was
making certain decisions, particularly around the machine learning workflow.
And you know, it's like there's that, you know, Richard Feynman quote, like if you, if
you can't explain it to your students or to your kid, you don't really understand it
yet.
So that was sort of like a moment for me.
I realized I was, I was thinking about this, let I was talking about this problem a lot
with another one of the machine learning faculty that I teach with, who's now my creative
partner, Benjamin Bangfort.
And essentially what we realized is like what we needed was a way to answer our students'
questions, the way we would answer them for ourselves.
And the way that we were answering them for ourselves was with plots, with plotting routines.
And so, and when I say the questions, I mean like the questions the students ask are things
like, you know, which features do I need to build this model?
Which ones should I remove?
You know, how do I know which model to pick?
Why is that the right model?
You know, why isn't my model doing a good job at modeling this data?
And so that's really why we decided to start working on yellow brick and so Ben and
I essentially we started this, this project, sort of in the beginning for our students.
But now it's sort of expanded out into being this tool that we see as being useful, not
just for students, but for engineers and for professional data scientists, who need
a way to diagnose and visualize how data is being fit and transformed throughout the entire
machine learning process and to end.
So it's a pure Python project, it's fully open source, it's licensed under the Apache
two license, you can find all of the source on GitHub.
But yeah, it's a, I can say more about it.
So it's, you know, it has an object oriented API.
If you are familiar with the scikit learn API, so scikit learn is a very popular Python
machine learning library that's open source.
So the scikit learn API has this notion of estimators and transformers that can either
change data or model data.
And so our API sort of wraps that API with common visual diagnostic plotting routines
that leverage pure Matplotlib.
So that is kind of basically in a nutshell what yellow brick is.
And so you reference that you you're using Matplotlib under the covers, what is yellow brick
offering that you can't do with Matplotlib is it kind of like pre configured templates
or way, you know, visualizations, you also mentioned in their diagnostic visualizations
as opposed to other types of visualizations.
Can you maybe elaborate on on all that?
Sure.
So to the question about what we offer on top of Matplotlib, it really is.
About capturing those routines that are kind of become best practices, I think, for people
who do machine learning kind of professionally or for their research.
So you know that you like to use residuals plots to diagnose certain kinds of error when
you're doing regressions.
Or you know that you like to use confusion matrices to know which of your classes you're
kind of failing to classify.
And so really all yellow brick does is it sort of says, okay, well, in order to do a you
know residuals plot, you need like these 50 or 60 lines of custom Matplotlib code or in
order to do a confusion matrix, you need these 50 or 60 custom lines of Matplotlib code.
And what we do is we wrap those into like an object.
And so instead of doing, you know, 50 lines of custom code each time, you just import
the object from, you know, you import the class from yellow brick and you create, you
instantiate an object that does, you know, it executes that work for you inside an object
oriented interface.
And so it learns from the data and then it tells you kind of what it learned using a plot.
Nice.
And I would really encourage folks to take a look at the gallery that you've got in the
documentation.
There are tons of different types of charts and graphs here, everything from feature
analysis to regression visual visualizers, classification visualizers, clustering, model
selection, text modeling, decision boundaries, and target visualizers.
Are there any particular of these tools that get the most use or kind of the bread and
butter for yellow brick users?
Oh, that's a fascinating question.
Part of me kind of wishes I knew more about how people are using it.
You know, one of the ways that we kind of try to keep a beat on this is just by like looking
at the blog posts that people are publishing.
You know, what people are using in those posts.
It does seem like the regression and classification visualizers are incredibly popular.
I think it's because, you know, those are two very common forms of, you know, supervised
learning is very common.
And so using things like classification heat maps or, you know, prediction error plots
or residuals plots are very, very popular.
We see those a lot in blog posts.
There was just a blog post that went out where somebody was using our stochastic neighbor
embedding plot, which is, which I really like to use when I'm doing a document modeling,
right?
So when I'm trying to understand a corpus and look at the different kinds of documents
in that corpus.
And so there's, you know, there seems like people are using kind of the whole gamut.
And then the other way that we sort of listen is just by looking in the issues, you know,
for future requests and bug reports to know where people are using different things and
how they're using them and what they're encountering as they're trying to use these different
kinds of visualizers.
Are there any particular ones?
Is there like a kind of lesser known hero among the different plots that you can do
that?
Yeah, I would have, you know, so I mentioned that I do a lot of NLP at my work and just kind
of in general is sort of, that's my sweet spot in terms of what I like to do.
And so as you might expect, there are, you know, a range of interesting text visualizers
that you might be surprised to see just because they sort of, you know, they happen because
Benjamin and I, my, at the other core maintainer, the other co-creator and I both do a lot
of natural language processing.
And so we've kind of evolved a lot of these sort of interesting different ways of thinking
about text and how what might help people to model text more easily.
And so stochastic neighbor embedding is a common one.
So that's, you know, it's like TSNE, TSNE plots.
So that's one that we have, we also have ones that people will probably recognize from
another library called NLTK.
So the natural language toolkit is kind of one of the first really robust Python libraries
for doing natural language processing.
So we have a visualizer that does like frequency distributions of words or tokens.
But we also have a few things that you might not see in other libraries.
So we have something called a dispersion plot.
And so what a dispersion plot lets you do is see the incidence of a certain word across
a corpus, which can be really useful for knowing it's sort of like a way of doing feature
analysis for when your features are words and tokens.
There's another one called a part of speech tag visualizer.
This is when, so I'm sort of selfishly maybe talking about this one because it's one
that I worked on recently.
But I think a lot about, you know, what kind of document is this and how can I guess
what kind of document this is without having to read every single document in my corpus.
And it turns out that one really kind of easy way to guess what kind of kind of document
it is is to just count the number of certain parts of speech.
So like how many adjectives are there, how many nouns are there, you know, how many
verbs are there.
And so our part of speech tag visualizer is a way that you can like scan over a corpus
and look at the proportion of different types of parts of speech.
And then you can compare different slices of the corpus or different classes from the
corpus and look at how much it varies, right, because our practices for using certain
kinds of words are really, really different when we're talking about blogging and when
we're talking about writing news articles when we're giving speeches.
And so you can see really distinct variations in the ratios of different parts of speech.
Can you give some examples of, you know, different ratios and what they signal in terms of
the type of document?
Oh, that's a good question.
Yeah, I mean, I guess there's that kind of like, you know, when you're writing the news,
you're supposed to not use any adjectives.
And so that's kind of an easy example, right?
So if you're scanning over the text and you do a part of speech tag visualization, let's
see, let's say, and your proportion of adjectives is extremely low.
There's a actually a pretty good chance that it's a news article.
Whereas for instance, if you have, you know, a lot of adjectives, you know, you might
be looking at something that's more akin to like poetry or music, like a song lyric,
because adjectives are a lot more common.
You know, so things kind of like that, I don't know if that helps to paint the picture
a little bit better.
It does help.
I'm not able to easily come up with a lot of different classes that this would easily
allow me.
It seems like the kind of thing that, and maybe this is, this is like everything else,
but you'd have to see a lot of examples to get an intuition for kind of how this might
translate to a particular class of documents, and maybe it doesn't generalize all that
well.
That's fair.
I think, you know, so you did.
And that's not a critique.
I'm just, it's very interesting that I'm intrigued that you can learn anything from
looking at this consistently.
Yeah.
An example is a great one.
I can definitely see that, and I'm just curious about others.
It's a very interesting analysis to apply to a corpus.
So here's another example.
So I sort of think of another example, which is that a lot of times with students, one
of the things that they'll try to do is they'll try to do a regression.
You know, so let's say, you know, they have a bunch of data where each sample is a sample
of concrete, and what we're trying to predict is how long the concrete is going to last
given, you know, its components, you know, how much of cement isn't it, how much ash,
how much water, et cetera, isn't it?
So let's say they're trying to regress this data.
And what they find is that their regression scores are really, really bad.
And so one of the strategies that we suggest is like, well, why don't you bend the values
into categories and then try to do classification?
So let's say we'll, you know, we'll look at all of the samples that, you know, lasted
between zero and five years, all the samples, the lasted between five and ten, but a lot
of times when you pick, when you sort of manually pick those bins, you end up with really
bad class and balance problems.
Class and balance is like the surefire way to end up with, you know, terrible classifiers.
And so we actually have a visualizer called balance binning.
And what it does is it recommends bins when you are kind of trying to turn a regression
problem that's, that's not successful into a successful classification problem.
Is the idea that you would use the parts of speech, visualizer in conjunction with the
binner to allow you to classify documents based on their parts of speech distribution?
Yeah.
So I guess I was, so for balance binning, I was just thinking about any regression
problem that you would want to convert into a classification problem, but I can't imagine
like a case where let's say we're looking at Yelp reviews.
And let's say, you know, we were, you know, trying to regress a score, like an exact score
for review is really hard.
So trying to predict like this is exactly a 4.32 or this is like a 2.17 restaurant is
really hard.
But when you bin things by stars, like, you know, this is a one star, this is a two star,
this is a three star, you can sort of get there a lot faster.
So if you don't already have those bins, you could kind of use balance binning to help
you decide on the bins.
And then you could use something like part of speech tags to see if the parts of speech
are a good way to differentiate positive reviews from negative reviews, let's say.
Yeah, I can envision something like fake reviews, use more adjectives, for example, than
real reviews or something like that.
I think that's a very good hypothesis.
Or more exclamation marks.
Is exclamation mark a part of speech?
Yes.
Yeah, punctuation is included in that.
This was originally built as a teaching tool.
Can you maybe talk through some of the ways that you use it or have seen it use in real
world?
And we've kind of talked through a bunch of those, but I'm wondering if you can walk
through and maybe more depth, kind of a workflow or use case that you're familiar with.
Sure.
I think that first, I would sort of say, I think a big part of how I think about the machine
learning process is by using this expression, the model selection triple, which comes
from a paper by Arun Kumar, which is actually a paper on sort of next generation databases
that anticipate machine learning from the start.
But he talks about the model selection triple as basically, this is what the machine learning
workflow sort of boils down to.
You do feature analysis, you do algorithm selection, and then you do hyperparameter tuning.
And you might have to do a triple, like many triples, to get to the optimal model, but
it generally follows that flow.
And so what you've got then is a search problem, right?
So you're searching, you've got this big grid of all of the possible features, all of
the possible algorithms, all of the possible hyperparameters, and you're trying to find
the best place in that search.
And so what students generally do is they just like Google examples, and they kind of copy
and paste.
But frequently, I think people in a production context, right?
So if you're in a company, you maybe are just buying machine learning as a service from
somebody else, right?
So you're paying some company to a black box kind of company, you give them your data,
they kind of do something magical, they pass you back a model, and maybe you try to deploy
that.
But I think that what I am seeing is that people are starting to become dissatisfied with
those black box models.
They can't tune them, they don't understand them, they don't have a lot of insight into
how they work.
And there's ethical concerns with how is this made?
How can we answer these questions about how this was made from our users, let's say?
And so what I'm seeing is that people are starting to adopt yellow brick to do steering
of the model selection triple, and kind of like taking that process back in-house so that
they can do it in-house rather than kind of delegating that or paying for that service
from somebody else.
I'm curious, are you seeing folks that are adopting yellow brick as a path towards
more explainable models or understanding the models better, you were just saying that?
Yes, yes, so I think that it is a very powerful tool for model interpretation and explainability.
It is not really designed for non-technical consumption, right?
So it's not really designed to kind of produce plots that you could just drop into a business
presentation and give to people who aren't already familiar with the process, but they
are a very good way to simulate kind of more directed decision making for your data scientists
so you can, you know, if your data scientists are using yellow brick to do feature analysis
and feature engineering model selection and hyperparameter tuning, you know that they are
doing this, you know, it's not a random walk, right?
They're not just picking things randomly, they're doing a directed, focused search, they're
not just using grid search, right?
So they're thinking about the choices they're making and because they're thinking about
the choices they're making and they have these artifacts of those choices, you can then
trace back through those choices and say, this is why we ended up with these features
and this is why we ended up with these models and you can kind of, you can tell that story
with these plots that have been generated in the course of modeling, you know, these visual
artifacts can be used, they can be used to explain how a model is working to the internal
engineering team and so that level of explainability is oftentimes a blocker to deploying models,
right?
Because the data scientists know how to make models but they don't understand maybe very
much about software engineering or deployment and the software engineers understand deployment
but they don't understand kind of, you know, how to, for instance, how to test non-deterministic
functions, they don't understand, you know, how to parameterize things so that they will
work in a production context but yeah, you can use yellow brick for logging, for visual
logging, once you've deployed a model, you can use yellow brick to generate plots on
a regular basis to look for things like model decay, you know, if all of a sudden the
classifier starts to perform poorly, that will show up in the plots and the engineers
can know what to look for and that can be very powerful for explainability and interpretability.
And so as visual logging in this use case where you're tracking model decay, is that are
there specific yellow brick plots designed for that or are you just using yellow brick
to display time series data? So I, that's a good question. I don't know of other people who
are using this in context other than Benjamin and I both use this in, you know, in our jobs.
And so essentially, you know, what you're just doing is you're, you're outputting, you know,
just, just the way that you would output like an an F1 score on a regular basis, let's say,
of your, your classifier and maybe you compute an F1 score on some regular, you know,
routine basis and you look for a change. Well, instead of outputting just the F1 score,
you could output a confusion matrix or a prediction error plot as another one of our classifier
evaluations. You could output a prediction error plot, which would allow you to see not just
the F1 score, which gives you the overall, you know, the harmonic mean of precision and recall,
but actually be able to see specifically which classes where the decay is happening. And that
is actually really helpful because you know, oh, it's this, you know, this specific class is where
I'm struggling to classify, you know, it's not just kind of overall performance decay, it's decay
in a specific way that I can use that information to make decisions about how to fix things. You
mentioned in discussing that the third element of that model selection trip, triple the hyperparameter
tuning that one of the advantages of using something like, well, this approach in general really
is that you're taking a more principled approach to optimizing your hyperparameter is what are some
of the plots that yellow brick offers that are there specific plots focused on hyperparameter
tuning or is it just, you know, tracking model performance as you're working your way through
the hyperparameter space? We do have a couple that are specifically for hyperparameter tuning,
and so one that is very popular is the alpha selection plot. And so, you know, I'm sure you know
this, but when you're doing regression, generally the problem is that you need to figure out how to
execute enough smoothing so that your model will generalize to unseen data. But there is this
sort of question about how much smoothing do I need? And generally the strategy is to grid search
that, right? So you say, okay, well, smooth between alpha equals, you know, 0.001 all the way to alpha
equals 10. That's a pretty big grid search, especially if you have a lot of data. That grid search
could be running for quite a long time. And the worst part is that you don't know maybe if you've even
picked the right range. And if you pick the wrong range to look inside, then you never get to the
optimal answer. And so one of the things you can do with yellow brick is use the alpha selection,
which allows you to visualize, you know, what the, let's say, regression score is at, you know,
some number of cuts of alpha. And what you're hoping to see is some point where your score, you know,
goes up, but starts to plateau. And if you don't see that, if you see kind of things hopping around,
moving around a lot, where you can, you can sometimes even see if you've picked the wrong range
to look inside. Another example that's sort of similar, but outside of regression is for when
you're doing a centroidal clustering. So how do you pick the right K when you're doing K means?
Well, you guess. There are, there are heuristics, let's say, for picking the best K. But
generally, I mean, people are mostly just guessing, right? And so in the same way as with the
smoothing coefficient, like you don't know how many K to pick. And so you can, you can grid search that,
you can, you know, pick all of the K from four to 400. That is a very long running grid search.
And so instead of kind of just trying to throw grid search at it, you can use the elbow plots.
So we have silhouette scores and elbow plots, which are two visualizers in yellow brick that are
very useful for centroidal clustering. And what a silhouette plot does is it computes the
silhouette score for each of the clusters. So you know, you pick, okay, K will seven, let's say,
will cluster to seven clusters. And then it will compute for you the silhouette score for
each of the clusters, which essentially is just a way of saying, like, how dense is this cluster?
And how far away is it from other clusters? And you want them to be as dense and as far away
from others as possible. Because that's what, you know, that's what centroidal clustering is sort of
for. And so you can use that to look for a single K. But because you can use that for a single K,
you can actually scale that up so that when you are, when you place like a range of K to look in,
you can compute the average silhouette score across all of the clusters at each number of clusters.
And what you hope to see in your plot is an elbow. So some point where you reach a peak, a peak
score. And if you get to like a peak score for a certain number of clusters where your average
silhouette score for all the clusters is really high, you know, you've got to the best K. But
you might also see something where there's no elbow and it's kind of jumping all around. And
usually when I see that, it tells me that centroidal clustering is not the best algorithm. It's
not the best model family. And I need to try to pick a different clustering methods, like maybe
something that's hierarchical, let's say. So those are two hyperparameter tuning visualizers
that I use regularly that I think are very popular. I'm curious about the community that uses yellow
brick. First of all, how long has yellow brick been out and what have you seen over time?
Well, so it's been open source for I think it's it's going to be three years this month or maybe
next month. So in that time, we have accumulated almost, I think we have 75 contributors now.
We have about 2,000 stars on GitHub. And, you know, it looks like we have about 5,000 downloads
a month, unique downloads a month via the Python package index pi pi. So it looks like we are kind
of we have reached a point where it is starting to become, you know, part of just the routine
way that people do machine learning diagnostics. You know, it's hard to tell what the breakdown is
between, you know, professional data scientists and students. But based on kind of what we're,
you know, the feedback that we're getting and kind of what we're seeing, it looks like this is
something that is being used across a lot of different communities. And we recently became a
non-focus affiliated project. So I'm not sure if you're familiar with non-focus, it's, you know,
it's a kind of open source umbrella organization that that helps promote open source projects. And
particularly a lot of the ones that are very popular like, SciPy and Matplot Live and Scikit
Learn. And so we recently became affiliated with non-focus, which is really nice. It helps us sort of
be kind of more hooked into the conversation and they help us sort of spread the word.
We work really hard to, you know, reach out to people. We give a lot of talks. You know,
we go to PyCon and you know, PyData events all over the country. Even, you know, we've been to
London. We've given talks in Argentina. So we really are doing our best to try to get this out
to as broad an audience as we can. Because I think that that like the diversity of usage
has really made yellow brick very interesting. So, you know, it's not just things for regression.
It's not just things for classifications, not just things for text, you know, not really because
we have users who are engaging in this conversation, who are doing all different kinds of things.
And it's really made the library much richer over time.
Oh, that's awesome. That's awesome. You know, one was the first feature that, you know,
major feature or different type of graph that went into yellow brick that you or Benjamin
didn't personally build. Oh, wow. That's been a long time.
It has been a long time. You know, one of, I mentioned the dispersion plots that was added
by one of our contributors, Larry. So Larry Gray is one of our core contributors and he does a
lot of text analysis too. And so he added the dispersion plot. We have a joint plot visualizer,
which is for doing pairwise feature analysis that was added by Prima, who is another one of our
contributors. We have a very kind of special approach to doing unit tests. You can kind of imagine
that when you're trying to do unit tests for visualizations, it gets a little bit hairy because
you have to compare, you know, different plots and see if they're close enough.
But one of our core contributors, Nathan, you know, essentially he found kind of a clever way
of doing this that we've adopted. That was a really important contribution. We have two other
contributors, Kristen and Adam, who have helped a lot with our documentation and getting it
basically to a forum where, you know, people really enjoy reading the documentation and know where
to find things because things are kind of easy, easily searchable and make sense.
Can you give a quick summary of the approach to unit testing? Yes. So the trick is that you have
to find a way to do visual comparisons. And so what we do is the library inside the test directory,
we have baseline images. And every time we add a new visualizer or a change of visualizer,
we update those baseline images. And those baseline images tell you, you know, the tests create
images. And the baseline images are what we compare them to. And so when the tests run either on
your machine or in CI on GitHub, they are building these plots. They're comparing them to the
baseline images that are part of the repository and they're looking at the diffs between
the two. So, you know, it's really just like black and white. Like if you actually look at the
diffs, it's just black and white and you're hoping that, you know, you won't see any diff at all.
But what actually happens in practice is that different operating systems have slight variations
in how they execute fonts or colors, let's say, or, you know, translucency. And so you have to
engineer, you have to use like Pytest parameterize or, you know, other tricks with Pytest
to say, okay, well, if it's within this range of similarity, we'll call it the same close enough.
Interesting. So yeah, it's a little bit tricky, but it's a pretty cool trick. Actually, if anybody out
there is doing any kind of visual library stuff and having trouble with tests, I would strongly
suggest looking at yellow brick. I don't really think that there's anything else out there.
Like that. So it can be a really good model for people who are having similar kinds of challenges.
Awesome. Awesome. Is that, have you put that visual compare into yellow brick or is it
separate thing? So it's in there in the sense that it's part of our test suite. So like if you go
into the test directory, yeah, it's so it's all in there. And we have actually a read me inside
the test directory that even just kind of goes pretty in depth into like what visual comparison
means and how it works. So if you're curious about that, you can dig in there. But yeah, so that is
part of the source code. Well, that's awesome, Rebecca. Thank you so much for taking the time to
chat with us about what you are up to and about yellow brick. Sounds like a really awesome
library. Thanks so much. Thanks. And yeah, everybody keep an eye out version 1.0 is going to be coming
out probably in the next month. So there's a lot of new stuff coming out with version 1.0. I think
people are going to be pretty excited to see it. Oh, that's a huge milestone. Congratulations.
Thank you very much. We are very excited. Awesome. Thank you.
All right, everyone. That's our show for today. If you like what you've heard here, please do us a
huge favor and tell your friends about the show. And if you haven't already hit that subscribe
button yourself, make sure you do so you don't miss any of the great episodes we've got in store for
you. As always, thanks so much for listening and catch you next time.
