Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Before we hop into today's interview, I'd like to send a huge shout out to everyone
who participated in the Twimble Online Meetup earlier this week.
In our community segment, we had a very fun and wide-ranging discussion about freezing
your brain, and if you missed that startup's announcement this week, you probably have
no idea what I'm talking about, as well as machine learning in AI in the healthcare
space and more.
Community member Nicholas Teague, who goes by underscore Nick T underscore on Twitter, also
briefly spoke about his essay, A Toddler Learns to Speak, where he explores connections
between different modalities in machine learning.
Finally, a hearty thanks to Sean Devlin, who presented a deep dive on deep reinforcement
learning and Google deep mind seminal paper in the space.
Be on the lookout for the video recording and details on next month's meetup at twimblei.com
slash meetup.
Now you all know I travel to a ton of events each year, and event season is just getting
underway for me.
One of the events I'm most excited about is my very own AI summit, the successor to the
awesome future of data summit event I produced last year.
This year's event takes place April 30th and May 1st and is once again being held in
Las Vegas in conjunction with the interop ITX conference.
This year's event is much more AI focused and is targeting enterprise line of business
and IT managers and leaders who want to get smart on AI very quickly.
Think of it as a two day no fluff technical MBA in machine learning and AI.
I'll be presenting a machine learning and AI bootcamp and I'll have experts coming
in to present many workshops on topics like computer vision, natural language processing
and conversational applications, machine learning and AI for IOT and industrial applications,
data management for AI, building an AI first culture in your organization and operationalizing
machine learning and AI.
For more information on the program, visit twimmolai.com slash AI summit.
In this episode, I'm joined by Ian Goodfellow, staff research scientist at Google Brain
and Sandy Huang, PhD student in the EECS department at UC Berkeley to discuss their work on the
paper, adversarial attacks on neural network policies.
If you're a regular listener here, you've probably heard of adversarial attacks and have
seen examples of deep learning based object detectors that can be fooled into thinking
that, for example, a giraffe is actually a school bus by injecting some imperceptible
noise into an image.
Well, Sandy and Ian's paper sits at the intersection of adversarial attacks and reinforcement
learning.
Another area we've discussed quite a bit on the podcast.
In their paper, they discuss how adversarial attacks can be effective at targeting neural
network policies and reinforcement learning.
Sandy gives us an overview of the paper, including how changing a single pixel can throw off
performance of a model train to play Atari games.
We also cover a lot of interesting topics relating to adversarial attacks in RL individually
and some related areas such as hierarchical reward functions and transfer learning.
This was a great conversation that I'm really excited to bring to you.
And now on to the show.
All right, everyone.
I am on the line with Ian Goodfellow and Sandy Huang.
Ian is a staff research scientist at Google Brain and Sandy is a PhD student at UC Berkeley.
Ian and Sandy, welcome to the podcast.
Thank you.
Thanks.
Fantastic.
I am so excited to have you both on the show.
You recently published a paper called Adversarial Attacks on Neural Network Policies and I'm
really looking forward to digging into some of what you worked on together.
But before we do that, why don't we take a moment to have each of you introduce yourselves
to the audience?
Ian?
Hi, I'm Ian.
I lead a team at Google where we study adversarial machine learning that can be generative adversarial
networks that can also be adversarial examples, which will be the main thing we talk about today.
I've been working on deep learning since about 10 years ago, really.
I got into deep learning when it was a very academic thing.
I spent my PhD studying deep learning and then when I came and did an internship at Google,
I got interested in the security side of machine learning when I helped Christian Zegity
write the first paper on adversarial examples.
And that's my main focus today is making sure that machine learning is secure.
Awesome. And what prompted you to study machine learning for your graduate degree?
I was really interested in figuring out how intelligence works.
As an undergrad, I started out taking a psychology class freshman year and then I decided that
it wasn't quite concrete and technical enough.
So I moved on to cognitive science and then neuroscience and then eventually I decided
that I'd be more likely to figure out how intelligence works by studying machine learning
directly.
It's really, really hard to reverse engineer the human brain partly just because so far
we haven't had the tools to look in and measure the activity of all the neurons.
We can't measure as many neurons simultaneously as we would like to.
During my PhD, I was really interested just in getting machine learning to work, just
making AI start to happen.
And now that that ball has started rolling, I'm a lot more interested in making sure that
there's a good outcome as AI develops further.
Like Google, we have a lot of different teams like the people and AI research group that
study different aspects of how AI relates to society.
I started a group that works on adversarial machine learning because I want to make sure
that systems with machine learning in them are secure.
That people on the outside can't intentionally mess with the machine learning system and
cause it to do what they would like it to do rather than what the designers would like
it to do.
Awesome.
How about you, Sandy?
How'd you get involved in machine learning and AI?
Yeah, I think like Ian, my interest also started when I was at undergrad.
I started off being really interested in computer science, but more from a bio computation side
of things, I was really interested in understanding how DNA works, understanding genetics, that sort
of thing.
But then, as I was thinking more biocomp classes, I realized that I was actually most interested
in the machine learning discoveries that had been made, like machine learning driven discoveries
that were made in biocomputation.
For example, clustering, breast cancer genes to figure out those two different types.
That's when, junior year, I started taking more AI, more machine learning classes, and
that's when I realized that, okay, there's a lot more to learn here, and I wanted to
do a PhD in this area and see what else I can figure out.
Awesome.
And you're advised by both Peter Biel, who's been on the show before and Anka?
Is that right?
Yes.
And Anka, drawing, yep.
And so does that mean that you spend a lot of time thinking about robotic applications
of machine learning and AI?
Yes, exactly.
Recently, I've been thinking a lot about how we can make machine learning systems more
interpretable and more predictable, and so that ties in very closely with helping human
robot interaction be more feasible in the future.
Fantastic.
So who wants to get started by telling me a little bit about the paper that you worked
on together?
Yeah, sure.
I can give a summary.
So the idea here was that there had already been a lot of work that shows that adversarial
examples are really effective at attacking classifiers.
So things like object recognition, if you train something to recognize objects in a scene
in an image, it's pretty straightforward to find a small perturbation that will
get your neural network to output a completely different label than what you had anticipated
and then what the correct label is.
And so we were thinking, we wanted to see if this would apply to neural network policies
that were trained with deep reinforcement learning.
And in particular, we were really interested in, to what extent, these adversarial perturbations
could disrupt the performance of these policies and how transferable they were.
And if you didn't know how a particular policy was trained, for example, which deep reinforcement
learning algorithm was used to train it, could you still attack that policy?
And so that was the overall question that we were trying to answer.
So you referenced the work on adversarial examples for classifiers and these are examples.
Like actually before spouting out some examples, do you do each of you have your own kind of
favorite example of adversarial attacks against classifiers?
Well, a lot of them are pretty similar.
I would say one of my favorite observations is a paper called Delving into Transferable
Adversarial Examples, where the authors found that if they fool several different classifiers
simultaneously, if they actually use an optimizer to search for an adversarial example
that fools very many different classifiers, then that input is extremely likely to fool
another classifier that wasn't involved.
You can design these attacks that will actually fool more or less anything without access
to the target model that you want to fool.
So in that example, if you somehow manage to create an example that visually looks like
an ostrich but is classified as a school bus for multiple classifiers, they've demonstrated
that it's likely to work on some broader number of classifiers.
Exactly.
Like suppose that you're a malicious attacker and you want to fool somebody's computer
vision system, you don't know what they're using.
Right.
Let's say for the sake of argument, they're using VGGnet, but the attacker doesn't know
that.
The attacker could do something like fool inception and fool a resonant with the same input
image.
And if they go ahead and fool those two and a few other models, it's much more likely
that they'll fool VGG, even if they never actually worked on fooling VGG specifically.
Is the fooling specific to the network architecture as opposed to the specific parameters of a given
model?
That's what this paper is able to overcome.
It's able to fool models regardless of their parameters or their architecture.
As long as the models are trying to solve the same task, like recognize school buses
and ostriches, this attack can find a reliable way of fooling pretty much any architecture
that we've tested.
And that's a lot of what Sandy was saying about how in this paper that we just published
on adversarial policies, we wanted to find out if this property of adversarial examples
transferring from one policy to another holds up in the same way that a transfer between
classifiers holds up.
And the context here is policies applied to reinforcement learning.
Can you give us a concrete example of what you've got in mind there?
Sure, maybe Sandy, do you want to talk about the Atari games?
Yeah, sure.
So I think when you think deeper reinforcement learning, the first really impressive example
of this was when DeepMind was able to train DeepQ networks, DeQin, use that to play Atari
games at human level performance.
And so the idea there is that you basically start from a random initialization network
that knows nothing.
And then very slowly over time, just by getting this reward as feedback, this network is
able to learn which actions will help it maximize reward.
What's the next step?
How does that apply or how does adversarial attacks apply in that example?
Yeah, so the kind of adversarial attacks we were looking at, we assume that we've already
got a policy that was trained with deep reinforcement learning.
So it's fully trained and it's able to get really high performance on the game.
So for example, a policy that's trained to play space invaders.
What we can do is compute these small adversarial perturbations in the image of the game.
So we do things like, for example, change a single pixel in the game.
And that's able to significantly decrease the performance of this fully trained policy.
And so the policy itself is fixed at that point.
All we're changing is the input that the policy is given.
Wow.
And so, and did you find that across, like how broadly did you find that this applies,
that you're able to change a single pixel value and dramatically impact the performance
of the model?
Yeah.
So we did look at a few different games.
All the only domain we looked at in this paper was Atari, but we looked at chopper command,
sequests, space invaders, and pong.
And so across all those games, and across three different ways of training these policies,
A3C, TRPO, DQN, adversarial examples are, you can pretty easily find adversarial examples
that will significantly decrease performance, like at least by half.
We looked at a range of different perturbations, there are graphs in the paper that show
this more concretely, but basically no matter which Atari game you're looking at or how
it was trained, it's definitely true that adversarial examples exist.
In some training methods, adversarial examples are more effective at decreasing performance,
but yeah, they're pretty prevalent.
Yeah, it's fascinating that you found this, you know, and I think about it in the context
of reinforcement learning, I guess my initial take is that it seems different from just
looking at an image, but you know, now that you've got me thinking about it, I can see
how because the training methods are so similar, you would expect to have similar occurrences
of adversarial examples in reinforcement learning types of models.
One thing that I find really exciting about Sandy's results is that they help us answer
a question that's almost more philosophical than technical.
Most of the previous work on adversarial examples was about object recognition, looking
at a photo and saying whether that photo is of an ostrich or a school bus.
But a lot of the time we were making up unusual photos.
They weren't photos made by taking a camera and snapping a picture of a school bus in
the real world.
They were made by a computer program and there's a deep philosophical question about how
you can say what the objectively true answer is in such a photo.
We've mostly evaluated our systems based on whether they agree with human judgment, but
maybe the human is making a mistake.
So it's hard to say that what the system does in the end is objectively wrong.
And are you, if I can interrupt, is that critique to the specific examples that have become
popularized of adversarial attacks like the ostrich and school bus?
I mean, I guess it's not clear to me.
Exactly.
Yeah.
I guess one of the strange things about adversarial examples is we're studying how to make computers
make mistakes.
And we happen to have done that mostly on, on kinds of data where we don't know objectively
what a mistake is or isn't.
If you make up an entirely new image, it's hard to say objectively how that image should
be categorized.
But for things like playing Atari games, it's, it's objective how the points are awarded.
For pong, you need to actually make the ball go through the opponent's goal post.
It's not really a question of whether a human observer thought the ball went through
the goal post.
It's just a question of how the pong game physics defined the scoring.
And so in these experiments, we can actually say that the machine learning system is objectively
compromised, that it really is doing worse.
It's not just that it's playing pong differently than a human would play.
It's actually playing pong in a way where it receives fewer points.
Were you able to produce specific failure modes via these attacks or are we only looking
at this from the perspective of subpar performance or reduced scores?
Could you always make, could you by manipulating a single pixel or some number of pixels always
make the pong paddle, the agent playing pong, missed the ball in the upper left corner?
There's some work that came out after ours that focuses on what they call an enchanting
attack.
Where they attract the agent toward a particular state.
We were just trying to reduce the score that the agent receives.
I don't know, Sandy, did you notice any particular failure modes like any specific kind of mistakes
that the agent would do over and over again?
No, I think because when we were computing the adversarial examples, all we were trying
to do is to get the agent to not do what it thought was the best action.
It could take any other action besides the best action, and that would be a successful
adversarial attack.
I didn't see any specific failure modes, although I did see patterns in terms of what particular
adverse profound and so in particular, you mentioned changing one pixel.
In something like chopper command, when you change one pixel in this game, the highest
impact you can get from that is by changing it in this small miniature map of the entire
game.
That's at the bottom of your screen.
When we found adversarial examples for this, actually, most of the ones where we only
changed one or two pixels, those pixels would get changed in that miniature map.
That's the optimal way to fold the agent.
That does shed some light on what the agent has learned to pay attention to, because you
could imagine that maybe if you're just training a deep reinforcement learning agent on images,
maybe it just would ignore this miniature map and not realize it's important.
In that way, adversarial examples are also kind of interesting because they can make
policies more interpretable in terms of what they're paying attention to in the scene.
Okay.
Other things that surprised you in terms of the things you learned in doing this?
I guess one other interesting result that we haven't actually posted yet, but we've
talked about at presentations and stuff, is it's actually possible to have dormant adversarial
examples as well.
We looked at policies that are recurrent, which means they have some sort of memory.
One canonical example is if you have an agent trying to navigate through a maze and you
show the agent, a particular indicator, say it's a certain color at the beginning of
the maze, it has to remember that in order to figure out which goal to go to at the end
of the maze.
If you just did playing adversarial attacks, you could have an agent that just while it's
navigating through this maze starts acting randomly and never reaches the goal at all.
But if you're using, if you compute dormant adversarial examples, which means that you
perturb a particular input, that's given to the agent, but then the agent keeps acting
correctly until some time point in the future.
In this maze, it would be the agent still navigating the maze correctly and then all
of a sudden at the very end actually goes to the wrong goal.
But the key there is that the point at which the adversarial example was introduced is
actually significantly earlier than the point at which the agent makes the mistake, which
is what makes it dormant.
And so these also exist for recurrent policies.
They're a little bit harder to find, but you can find them in very much the same way, just
frame you as an optimization problem.
What does harder to find mean?
Hard to find as in, it takes more computational power to find it.
The problem itself is a little bit, there are more local minima, for example, you'll have
a lot of examples that won't meet all the constraints of your authorization.
You're constrained to basically that you do the right thing for the next, let's say,
10 time steps and the wrong thing on the 11th time step.
You can think of dormant adversarial examples as being a little bit like post hypnotic
suggestion in a cheesy spy movie, where there's a character who has been pre-programmed to
suddenly carry out an assassination, and even that person themselves doesn't know that
they've been programmed in that way.
Kind of the mentoring.
Kind of.
Kind of.
Exactly.
Yeah, that was the movie we were talking about when we first had the idea for this project.
Interesting.
From a security point of view, dormant adversarial examples are more worrisome because they could
be presented to an agent before it enters the area that you've secured.
You could imagine if you have some kind of room where you're careful about what objects
are there.
You could make sure that nothing in that area can confuse your robot, but if your robot
could be confused by something it saw before it came into the secure environment, then you
actually have to secure it at the level of the machine learning software, rather than
securing it by making sure that there's nothing unusual in its physical environment.
Are you aware of any publicized examples of adversarial attacks in the wild?
I've heard people in finance say that they spend a lot of effort obfuscating their trading
algorithms so that their competitors don't reverse engineer their trading algorithm and
fool them into making unprofitable trades.
I don't know of anything very similar to the computer vision, object recognition examples
that we're studying so far.
Do you know of anything like that, Cindy?
I don't know of anything in the wild.
I mean, there have been more examples of real world adversarial examples where you do
something like print out a poster of a stop sign and paste that on top of a real stop
sign, and that's able to fool a classifier from a lot of different angles and distances.
So, I've been fortunate that the real malicious people don't seem to be using these techniques
yet.
I think that people probably well in the future.
At the moment, I think we're protected by a few factors.
One is I think there's a lot of other malicious things you can do that are easier.
And two, if you have the deep learning expertise, there are less risky ways to turn a profit.
Have you come across, I don't know if you would call these adversarial examples, but what's
the analog accidental adversarial examples, like natural adversarial examples, is there
such a thing?
I mean, I think that would just be called like training and test, well, it would be like
training and test distribution shift, but something that you saw it test time that you didn't
train on and you didn't expect to see, which I think does happen all the time.
Sure.
Yeah.
Yeah.
There are optical illusions and things like that that fool humans, even when they're not
really designed to.
I'm sure you've seen silly images posted on Reddit, where the first time you look
at it, you think you see something entirely different.
There's lots of party photos where people standing near each other, it looks like someone's
arms are actually someone else's legs.
So it looks like there's somebody with four legs or something like that.
That kind of thing comes up in machine learning too, just by chance.
So where do you see the work in this paper going?
One thing that I'd be really interested in is going further in the direction of what
you were alluding to earlier about controlling the agent to do complicated behaviors that
are different from what the designer wanted, rather than just doing worse at the main task.
One of the main things making that harder for researchers to do is that we only have access
to one reward function.
We have a reward function that says play pong very well.
And as an attacker, we can choose actions that make that reward go down.
But if we had two different reward functions for the same environment, like if we had a robot
that can cook, and it's been asked to cook scrambled eggs, but we also have a reward
function for making a birthday cake.
It would be interesting to show that we could trick it into making a birthday cake.
It's really easy to break things.
It's harder to create something that wasn't there already.
And so if as an attacker, we could show we have so much control that we're able to actually
create a birthday cake rather than just interfere with making scrambled eggs.
That would be a lot more impressive in terms of the capabilities of the attacker.
In the example that you use, the robot has multiple reward functions corresponding to what
are, you could view as largely different tasks are there.
Are you seeing a move in reinforcement learning towards something other than a single reward
function, something that's more kind of nuanced or complex?
I'm not sure exactly what that means or if the question makes sense, but you're saying
is there a different way to evaluate performance or to give a learning signal to the agent?
I don't work on reinforcement learning as much.
Sandy is probably better qualified to comment on that.
I personally feel like we need to move beyond the paradigm of just maximizing reward for
a lot of different reasons.
One reason that I have is that it's a strange way to communicate with an agent.
Imagine we wanted a reinforcement learning agent to plan a mission to Mars and we give it
a reward of one when it gets there and a reward of zero otherwise.
How would it even know we wanted it to go to Mars until it got there?
You could imagine having two different super intelligent agents and we want one to cure
cancer and one to go to Mars and we just let them both lose on Earth.
How would each one know which task it had been assigned until it finished its task?
Conceivably, one could cure cancer but it was supposed to go to Mars and get no reward
and the other vice versa.
I should say I'm saying all of this is someone who mostly studies classifiers in
generative models, so maybe I'm being very inferred at the reinforcement learning point.
Sandy, do you have any thoughts about the future of reward functions?
Well, I think there is a lot of complexity that can go into a reward signal.
I do sort of agree with Ian that it is a pretty severe constraint on how you can give
information to your agent if all you're giving it is this particular reward but you can
do things like shape the reward or do something more like curriculum learning where you start
off the agent with smaller tasks even if the reward is sparse and over time give it
harder task but it's already learned how to solve the easier smaller tasks so it should
be able to more easily solve more difficult ones.
I think, yeah, I mean there has been a lot of work recently on trying to do things like
transfer learning or meta learning in the context of reinforcement learning and so that's
also a promising direction.
You mentioned curriculum learning.
Can you elaborate on that?
Are you essentially iteratively training with more comprehensive or longer term rewards?
Is it related to transfer learning in that sense?
Yeah, it is related transfer learning in the sense that you're trying to transfer knowledge
from easier tasks to harder tasks.
So there's been some work Carlos Lorenza and Peter's group has done some work where you're
in a setting with sparse reward like Ian was talking about where you get a one if you succeed.
But you can do things like start your robot from starting states that are closer to your
goal.
For example, if you're trying to get a robot to insert a key into a lock and turn it, you
could start it with the key already inside the lock and it just has to turn the key and
then start with the key just a tiny bit outside the lock so it just has to figure out and
start the key and then turn it and so you just slowly you have a set of states where the
robot succeeds from and you slowly expand that set to starting states that are just a
tiny bit more difficult than the ones the robot can already succeed from.
And so that and then at the end you have a robot that can insert this key into the lock
and turn it from any point within a large range of different points.
It's called curriculum learning as an analogy to the way that we teach people in school
schools.
We start out teaching people how to read the alphabet in kindergarten and then build
up to very easy C spot run type books and then gradually to more and more difficult
reading tasks and then once people can read fluently we start teaching them subject matter
that they read in textbooks.
We don't on day one of kindergarten start throwing everyone questions sampled uniformly
from the set of all knowledge we expect them to have by age 18 we don't give anyone questions
from their algebra two class on day one of kindergarten.
We arrange the order of the experiences so that it gets harder and harder as they go through.
It seems really obvious in retrospect but in machine learning we actually usually do
uniformly sample all the experiences that we test the machine learning system on.
So curriculum learning is a pretty big change from what's the standard practice.
And I was thinking ahead a little bit to how the adversarial attacks might apply in
that example and what are the extent to which we've looked at transferability of adversarial
attacks in transfer learning cases in general have either of you looked into that.
I've been a co-author of some work and I've followed a lot of other work with a lot of
interest. In 2013 when Christian Zeggedy wrote one of the first papers on adversarial
examples he found that if you just make adversarial examples for one model and don't do anything
to try to make them transfer they will often fool other models just by chance without
needing to do anything special to cause it to happen.
And then later Nikola Paparno in a paper that we wrote together showed that if you train
one neural network to copy another neural network you can actually train it to copy the
behavior of the target network on unusual inputs that don't correspond to any kind of real
data as well as regular inputs that look like data.
But once you've managed to copy all the decision boundaries of this target network in that
way then you know that adversarial examples for your copy are very likely to fool the target
as well.
You can copy a network like that without actually having access to its parameters or its
architecture.
You can just send inputs to it and see what output it assigns them and then you train
your own model to copy that input to output mapping.
So is that you basically using the model that you're copying or the network that you're
copying is that essentially generating your label data so you've got some inputs you're
sending it to that and then you're training your model on the inputs and labels that
that thing generates.
Exactly.
The attacker doesn't even need to have enough resources to label their own data set.
Right.
And so any specific thoughts on how adversarial attacks might apply in this curriculum learning
type of use case?
One thing that's kind of interesting and related to curriculum learning is a machine learning
security problem called training set poisoning.
It's almost like the opposite of curriculum learning.
That curriculum learning is when a benevolent designer of the system structures the training
set to be really easy for the model to learn from.
Training set poisoning is when an attacker sneaks something into your training set that
you didn't know was there.
And then it can make the machine learning model do something that the attacker chose at
test time based on what it learned from the training set poisoning.
Another really similar idea was introducing it.
Before you move on from there, can you give a specific example of that?
Yeah.
There's a paper from Stanford that came out last year where they showed that they can
introduce a specific picture of a dog that has been altered a little bit just like an
adversarial example.
And if you include that dog in your training set, it will misrecognize lots and lots and
lots of dogs as fish at test time.
Wow.
Okay.
Interesting.
Yeah.
I've not come across that one.
Another similar thing is introducing a paper called Badnets and they call it adding a
backdoor to the network.
The idea is that the person who trains the network might intentionally add something weird
to the training set that then lets the network do something they want later on.
So for example, suppose that I was training a face recognition system that I would give
to other people to use to secure their facility.
If I was a bad person, I might put myself in the training set and tell it to always let
me in.
And then I could go break into their warehouse later without needing to do anything special
to get through security.
Detecting those kinds of backdoors in neural networks is a really interesting research
challenge because when a network does something unusual, it can be hard to tell whether the
network is making a random mistake.
The network is making a mistake that someone built into it as a backdoor.
Or in some cases, the network isn't even making a mistake, it's doing something smarter
than you, the human, have thought of.
And it's actually correcting one of your own mistakes.
So when you disagree with it, you don't actually know a priority who's right or wrong.
Yeah, the thing that this makes me think about is, I guess some of the conversations we've
had as an industry around code reuse.
I forget the specific example, but there was an example about a year ago or so of an NPM
library that I don't think anything malicious happened, but someone either changed it or
unpublished it or something like that.
And because so many people had used this library in their code, it had the potential to disrupt
a whole bunch of applications.
And I think the NPM folks, the node folks came in and did something extraordinary to make
sure this library didn't go away and break all these applications.
And we're seeing a lot of the same, the analogy of code reuse in the machine learning world
is like reusing these data sets.
But I don't know that we have any real standards for certifying data sets as being untempered
with.
And the idea that you can introduce back doors or make neural networks misbehave and
really bad ways by manipulating the training data sets suggests that we need these kinds
of standards.
And even a lot of people create their training data sets by crawling the web.
And we know that you can kind of poison web search results by creating linking architectures
and things like that so that certain things could make certain results become more popular.
And that could have a downstream impact on these models as well.
Exactly, yeah.
Sandy, do you have any war stories from training sets at Berkeley?
No, I guess in the, well, in the context of reinforcement learning, you don't really
have the training set like you do in supervised learning, you have your simulator.
And so I think, I mean, something similar does apply.
You could have a simulator that somehow gets the agent to learn some correlation that actually
impairs it when it's launched on, unlike a test simulator.
I mean, I think this is sort of related to reward hacking.
If there is something that the agent can exploit in your simulator, a lot of times if you
are training it with deep reinforcement learning, it will find that and it will exploit that.
But the difference is in the context of reward hacking, it's pretty obvious when your agent
has done that if you just watch a rollout of the agent, you can usually detect that, okay,
it's not actually doing what I want it to do.
And just to make sure folks are familiar with the term reward hacking, you know, these
are examples in the case of video games where the one that I remember was kind of this
agent that's a boat that figures out that if it swirls around in a circle that racks
up a whole bunch of points, even though it's not making progress towards its end goal,
or what you might want it to end goal to be.
Yeah.
Exactly.
That's a really popular example.
And the problem there is that you told the agent to maximize score.
And by swirling around, it's able to get all these points by getting these things in
the environment that appear periodically.
And so that's what's doing while swirling around.
But really, you wanted the agent to win the game, but you couldn't really specify that
particular reward function.
So that's why you gave it, just told it to max line points.
But that goes along with the discussion we were having earlier about how the reward function
is really important.
And you need to be really careful in selecting your reward function and make sure you're
actually telling the agent what you want it to do.
Yeah.
And I think the question that I was, the way I was trying to ask the question previously
was, is there a notion at all of either hierarchical reward functions or multiple reward functions
that you try to optimize simultaneously, or is that just beyond the frontier of complexity
for us right now?
There was a really interesting recent result from DeepMind with a system called Impala.
They showed that they could train on several different tasks at the same time and actually
do better on task A because they had also studied task B, C, and T. That was actually pretty
hard to get even that much working.
And so would you expect it to be easier or harder to, I'm assuming in this case, there are
separate rewards functions for each of these tasks.
But I'm envisioning at least conceptually that you can have a single task with multiple
reward functions.
Would you expect that to be easier or harder than what they did?
I would expect that to be easier.
It's mostly that for most of the simulators we have, there's really one thing that you
want to do.
In the pung simulator, you want to play pung by knocking them all through the opponent's
goal post.
Yeah.
I guess I'm thinking of something, maybe the simple example of the kind of thing I'm
thinking of is the whole explorer exploit.
Maybe there's, you know, you have a game that, you know, it's like a map based or world
based game and you want, you know, one reward function to be when the game.
But you also want to encourage your agent to explore the world.
And so you might have, at least I'm envisioning, you'd have one reward function that correlates
to the amount of the world that's been explored and another that correlates to winning the
game.
And, you know, what does it mean to kind of maximize both of those?
Yeah.
I think that that reminds me of hierarchical reinforcement learning, which is definitely
a topic that is being studied.
So in that sense, you have sort of different levels of agents.
You have a planner that essentially decides, okay, what do I want to do?
What is most important to do at this point in the game?
Do I explore?
Do I exploit?
Do I do something else?
And then that, they pass down that, I guess, that sub-reward to a policy that's been
trained to, for example, explore really efficiently.
And so you can learn this whole thing into end.
I mean, DeepMine also had a paper on this last year.
I forget what the, what the new hood it is, but yeah, where they're able to show that
it is possible to train a giant policy that does hierarchical reinforcement learning and
passes down these sub-rewards.
I think this is really important for complex tasks, like trying to win a game of starcraft,
for example, it might be, it might take a lot longer to train this end to end compared
to if you're doing it in a hierarchical way.
You know, we've talked about examples of, you know, fooling classifiers or fooling, you
know, reinforcement-trained agents, which could be robots.
But I'm wondering, given the focus of your work is on robotics, I'm wondering if there
are any more subtle examples that you've come across or things that, you know, are areas
of concern for the application of adversarial examples or adversarial training in the
context of robotics.
Yeah, I think one of the key problems is that adversarial examples make policies and
they make robots less predictable, and it's harder to anticipate their behavior.
And so if you are a human trying to interact with a robot or riding in a car, you have this
mental model of how you think this robot is going to behave in the next, like, one, two
seconds.
And so the dangerous thing about adversarial examples is that that basically breaks your
model and puts you in a position where you're not sure how to respond as a human, and so
that's really dangerous.
I think the fact that it does seem possible to introduce adversarial examples, whether
you train this policy with reinforcement learning or with supervised learning, does seem,
I mean, that's pretty, pretty scary.
Although it is hard to get adversarial examples in the real world, and like Ian said, there's
a lot of other ways in which robots can misbehave that don't depend on adversarial
examples.
And so the whole challenge of getting robots unpredictable and interpretable robots is
much bigger than just trying to solve the problem of adversarial examples.
What I'm really excited about is that adversarial examples give us a way of studying how robust
a robotic system is in very concrete mathematical terms.
We specify a model where we say all the things that an attacker can do, and then we try
to prove that our policy will still work, even in the last case where the attacker chooses
the thing that is the most likely to interfere with with the robot's abilities.
So far we're not able to defend against that kind of attack, but in the future when our
defense algorithms get better, if we're able to perform well in the worst case, it should
also mean that we're always able to guarantee good performance in the average case, that if
we're able to resist actual tempering, we can also be robust to things that interfere
with robot policies right now.
Like when you train a policy on one robot body and then run it on another robot body that
is slightly different due to mechanical imperfections, that can be enough to interfere
with that policy.
That's actually a good segue to what I think will be our last question here.
A lot of the work on the supervised learning camp of adversarial examples has been on
architectures or methodologies for creating robustness in the networks to these kinds of
attacks.
Have you done or seen anything in the reinforcement learning world along those lines yet?
Some of the follow-up work on our first paper actually used some of the techniques from
classifiers to increase the robustness.
So there was some work from CMU last year, it was called robust adversarial reinforcement
learning.
They were trying to, well, their definition of adversarial attacks was more physical,
so you're training this locomotion agent in Mujoko, and the adversary can apply these
forces to the agent.
And you train the adversary actually in parallel with the policy that you're trying to train
to get this agent to walk or make forward progress.
And what they were able to find is that by training this agent to be able to walk despite
these forces applied to it by the adversary, they're able to get an agent that was more
robust in terms of being able to locomot across many different parameters of the environment
in terms of friction or mass, different body parts of the agent, things like that.
And Mujoko, that's a simulator, that's right.
Yeah, that's a simulator.
So that's where you have things like the half-cheetah and the humanoid on the swimmer,
yeah.
Great, great.
Well, any final words from either of you, any parting thoughts or things that you'd
like to point folks to if they're interested in learning more about this stuff?
You could summarize a lot of what we talked about today as Goodheart's law in action.
Goodheart's law is an idea that came from economics that says, once you use some value as a metric
that you make it your target to optimize, it's no longer a good metric.
And we see that happen with both adversarial examples and with reward hacking.
If we use the output of a classifier as something that we're going to optimize, we find
an adversarial example instead of a good input from a particular target class.
And similarly, if a reinforcement learning agent optimizes its reward function too well,
it can find ways of obtaining rewards that are serious and not doing what we actually
hoped that our agent could do.
Yeah, I think that's a great summary.
Are there any specific implications of thinking of this stuff in terms of Goodheart's law?
I guess one thing is just that it lets us see that there are many different things that
all fall in the same category that you could think of reward hacking as a kind of adversarial
example or vice versa.
And you can see that solutions to one might help with the other.
Got it.
Awesome.
Well, Ian Sandi, thank you both so much for taking the time to chat with us about this
stuff is really interesting and important work.
Thank you.
Oh, thank you.
Thank you.
All right, everyone, that's our show for today.
For more information on Ian, Sandi, or any of the topics covered in this episode, you'll
find this show notes at twemolei.com slash talk slash 1-1-9.
If you have any questions for Ian or Sandi, please post them there and we'll make sure
to bring them to their attention.
If you're new to the podcast and you like what you hear or you're a veteran listener
and haven't already done so, please take a moment
to head on over to your podcast app of choice and leave us your most gracious rating and
review.
It helps new listeners find us, which helps us grow.
Thanks in advance and thanks so much for listening.
Catch you next time.
