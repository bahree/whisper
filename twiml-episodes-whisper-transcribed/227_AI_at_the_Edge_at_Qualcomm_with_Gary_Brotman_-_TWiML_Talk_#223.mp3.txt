Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Gary Brockman, Senior Director of Product Management at Qualcomm Technologies.
Gary, who got his start in AI through music, now leads AI in machine learning strategy
and product planning for the company with a focus that includes the Qualcomm Snapdragon
mobile platforms.
In our conversation, we discuss AI on mobile devices and at the edge, including popular
use cases and explore some of the various acceleration technologies offered by Qualcomm
and others that enable them.
We also dig into the state of AI on devices from the application developer's perspective
and how various acceleration technologies fit together to help developers bring new products
to market.
Before we get going, I'd like to send a huge thanks to Qualcomm for sponsoring today's
show.
As you'll hear in my conversation with Gary, Qualcomm has been in the AI space for well
over a decade now, powering some of the latest and greatest Android devices with their
Snapdragon chipset.
From their strong footing in the mobile space, Qualcomm now has the goal of making AI at
the edge ubiquitous.
To find out more about what they're up to and how they plan to get there, visit twimble
AI dot com slash Qualcomm and now on to the show.
Gary is the Senior Director of Product Management at Qualcomm.
Gary, welcome to this week in Machine Learning and AI.
Thank you and happy new year.
Happy new year to you.
I am excited for this new year and as I mentioned to you, as we're chatting before we started
rolling today is my birthday making this a, well, it's always very cool to kind of have
the new year on my birthday line.
But I'm excited to make this my first interview of the year.
And I'm happy to share your birthday with you, Sam.
Fantastic.
Fantastic.
Why don't we get started by having you tell us a little bit about your background.
You got into artificial intelligence initially by way of music, is that right?
Yeah, that's right.
I guess my, my interest in AI started way back.
I guess it would have been at the turn of the century.
I was working at a company called Music Match, which had developed a jukebox software application
for the PC.
But part of that offering was a personal music recommendation engine that was a proprietary
engine that Music Match had created.
And it was using collaborative filtering algorithms to monitor the listening behavior
of the users of our jukebox software and what they, what they were listening to, what they
skipped, what they listened to fully.
And we were able to really develop some really rich correlations between artists and listeners
that had, that transcended any traditional tagging data like genre or year or era would
have you.
Just looking, listening to or actually watching what people are observing, what listeners
were actually doing, not what they were saying, not what they said they liked or disliked.
But what they were actually listening to and then taking the correlations between individual
listeners and then myself or another somebody else.
And that combined, the combined influences came up or resulted in some very unique recommendations.
My background early on was in music and digital music and MP3 compression.
And when I came across, when I experienced what Music Match had to offer and help promote
their recommendation technology, which also powered personalized radio and on demand streaming
recommendations, that got me hooked.
And I carried that with me for a number of years and up until four years ago, when I had
an opportunity at Qualcomm to bring some AI machine learning technology at a corporate
R&D into the commercial side of the house and release it across our Snapdragon platform,
kind of tying those things together over the past four years has really been a great experience
and quite rewarding.
Why don't we maybe spend a little bit of time talking about Qualcomm for those who are
not familiar with the company and what you're up to?
I know in our newsletter, we've talked about Qualcomm quite a bit, particularly over
the last year, everything from the Snapdragon platform launch in December, prior to that,
the extended reality release that you did in June.
Some new chips in April and I think maybe in January or something, there was a neural
processing engine and a hexagon vector processor, all really interesting stuff that I'm excited
to talk to you about today.
But for folks who've kind of seen those pieces and don't really know what the big picture,
what's Qualcomm up to in this space?
Well, if you go back in time, we've been a mobile innovator for over 30 years now, most
of it's starting with cellular and since the early 90s, we've been focused on the connectivity
side with every G transition, we call it from 3G to 4G and out of 5G.
We also got into the chip business in the 90s because others were having some difficulty
developing 2G and 3G chips and then that was a springboard for us to then move the internet
from the PC onto a mobile phone.
We saw this trend happening very early on and tried to hasten that with investment in
both connectivity and silicon.
And then we introduced the Snapdragon mobile platform a little over a decade ago.
In fact, we're probably coming up on 11 years now and that and Snapdragon is now what
powers the majority of Android phones in the marketplace globally.
What we've done on the Snapdragon side specifically, when it comes to compute on a handset or in
any other embedded device, IoT, automotive, etc., we have leveraged the compute capabilities
that we developed over that past decade to drive what we have seen over the past four
years as being a very quick movement of an artificial intelligence and machine learning
based workloads that have been mostly relegated to the cloud on the server side or the data
center, but are matriculating to the edge.
Mobile phone is really kind of the primary focus for us, but we do look at other verticals
like IoT and automotive as I mentioned, but the activity from Qualcomm standpoint has
been very strong on 5G, but also an AI over the past four years and specifically on device
AI and ensuring that any device for the Snapdragon processor is able to efficiently run and accelerate
AI algorithms in a power efficient way.
Okay, so to make sure I understand that trajectory, the companies started out basically building
the or not started out, but one of the companies big moves was really building the chips that
allowed devices to connect to wireless networks like 3G, 4G, 5G, and then from that kind
of presence in the mobile space moved into, you know, as smartphones arrived, moved
into providing the compute platform or the compute chips for the smartphones and now
are providing an of the AI acceleration extensions to that compute platform.
Is that the right way to think about it?
Yeah, I think that's the right continuum.
Those are the three big pillars, if you will.
Okay, and your responsibility there is focused on AI strategy and product planning.
How long has the company been thinking about AI?
That's a good question.
We've been, it's interesting that most folks don't know how long we've been focused on
this, but it's been over a decade.
We really started investing in deep learning back in or machine learning back in 2007.
We've had a heritage in computer vision, but in the research side of the house, we've
been looking at, you know, everything from spiky neural networks to deep learning going
back as far as 2007, 2008.
Our research group has actually been driving most of the activity over that 11, 12 year
period, but in the past four years, as I say, four of the last 11 years, we've actually
been releasing commercial grade software as well as hardware acceleration to accommodate
what we see is at this present, a tidal wave of workloads that are arriving on mobile.
We started our first, or at least we introduced our first mobile SOC that was optimized for
on device machine learning with a Snapdragon 820 back in 2015, and we're now in our fourth
generation.
Taking a little quick step back, I mean, my role at Qualcomm and our team's role is to
look at what we need to do to accommodate this new class of software and algorithms on
device and all as well as in the device.
So at Qualcomm, we don't focus just on the end product.
We're actually looking at AI and machine learning as being integral to how we develop products
and also run our business.
So my team's responsibility is to look across the Snapdragon portfolio where we can optimize
up and down the tiers, whether it's in a lower tier all the way up to the premium tier
SOC, but also look at ways that as we develop our products within the organization, how can
we apply machine learning to make those processes more cost efficient, reduce time, and in
cases where it's possible, generate a creative revenue from that effort.
You mentioned SOC, and that's system on chip, which is basically the effectively the CPU
for these devices.
Yeah, it's actually, the SOC would be a complete system.
So the CPU, maybe the heart of the chip itself is the primary processor, but the SOC is
defined as all the components that are necessary to drive what that device is capable of doing.
So in the case of Snapdragon, the entire Snapdragon portfolio is an SOC portfolio and
each has at least our cryo CPU, our Adreno GPU, and in the mid to high to premium tier,
you'll find our hexagon.
We do have DSPs, or I say the hexagon DSP.
The DSP is pretty prevalent across the portfolio from a modem standpoint, but from an AI standpoint,
the hexagon family includes vector processors, which are present in our premium tier and
top tier SOC, so 800 down to 600.
And that vector processor up until recently has been a primary engine for running on
device AI.
What are the primary use cases that you're focused on from a on device AI perspective?
So there are a number that are I guess today and literally over the past 18 to 24 months
that have become kind of table stakes have been things like just basic object recognition.
So if you hold up your camera, you can detect whether something in the field of view is
a specific class of object, let's say a car, a cat, or a dog, or what have you.
And then taking that a little bit further to the facial recognition.
So now if you look at any phone today, either you're going to have a phone that has a specific
camera module that allows the device to take kind of a 3D depth mapped image of your face
so that you can unlock the phone, but also maybe even make mobile payments depending on
the type of phone you have and the region that you're in.
With AI, we're seeing that the specifically in facial recognition, all the extra hardware
that's been necessary to drive that really isn't necessary across the board because AI
algorithms specifically are able to do depth mapping on their own with single images
now and single camera.
So facial recognition for unlocking a phone and making mobile payments is becoming fairly
common voice activation.
So being able to do detecting keyword on a mobile phone or any other device, I think that's
becoming commonplace and again, that's happening locally, it's not happening in the cloud,
that recognition is immediate so that you can have pretty low latency.
There are other camera effects like bouquets, you can segment your portrait or they may
call it portrait mode, what have you, where you segment your body or the subject's body
from the background so you can blur that out or create a separate background and put
the individual in a completely separate environment visually.
The entire camera pipeline, most of the features that you see in a mobile camera today, whether
it's HDR or even being able to do things like super resolution where you bring clarity
to a picture that doesn't have, that isn't high resolution to start with.
All of those features are now, they started off as computer vision based and are now becoming
driven by deep neural networks on the device to solve for the same problem to achieve better
accuracy and in many cases give the developer a more generalized approach to how they
instantiate those features.
Those are probably the most common today.
It doesn't surprise me that the camera pipeline is all being processed on device but I'm
a bit surprised to hear that things like object recognition, face recognition, voice recognition
are being done on device.
The thing that I'm thinking of most is like OK Google or the facial recognition or the
people recognition and like Google photos and I just assume that that's the app and
it's talking back to Google's cloud and all of the machine learning AI is happening between
those two pieces of software but it sounds like more of it's happening on the device than
I might think.
I think that's right.
If you look at anything that deals with biometric data, whether it's your face or your voice,
that's very specific to you and that data needs to be kept secure and private and there
is today given the state of kind of compute on device as well as the ability of CNNs and
RNNs to be able to run locally in a pretty efficient way.
There's really no reason why you as an individual should have to rely on a cloud and present
your personal data to that, to the data center of the cloud in order to achieve some level
of utility or some utility on the phone.
So specifically with facial recognition and I guess I should take a step back and say
that there's a security and privacy of been probably one of the primary reasons we focus
so much on device as opposed to relying solely on the cloud for some of these functions.
That in performance, so low latency because everybody wants things immediate.
But there is again no reason why you should have to share your voice print or your voice
data or your facial information or any other biometric data for that matter with a third
party to let's say unlock a phone and do what you need to do on that device.
So what you see today, if you're looking at OK Google or if you're on an iOS device,
there's a balance between what's happening on the phone and what's happening in the cloud.
Some cases as a consumer you're not going to know and I don't think that and the intention
is that you shouldn't know which you should get as the best user experience.
What we try to do is focus on ensuring that whatever workload does run on the device
is executed in a power efficient and performant manner.
But yeah, there's a variance.
Some things do happen on the device and some things are happening in the cloud.
And how about from an application developer perspective, if I'm developing an app and
maybe everything's probably different if I'm Google, but if I'm just an app developer,
Android app developer or iOS app developer, and I want to use AI, either via your mobile
TensorFlow or something like that or the iOS extensions, do I have to think a lot about
on device AI and how to take full advantage of the hardware or we at the point where
that's all transparent to me and whatever framework I'm using, figures out the best way
to do things.
I think we're still at a point where there's experimentation, there's very little standardization
across the industry, whether it's from benchmarking or common APIs to access the underlying
compute on different devices, irrespective of the chip that is driving that device.
In our portfolio, we have right now we have a handful of tools that are all part of what
we call the Qualcomm AI engine and this AI engine is the sum of a number of parts.
It's the primary compute engines that we have on our chip, so the CPU, the GPU, our hexagon
vector processor, and something that we just recently introduced with Snapdragon 855,
we call a tensor accelerator, which is a dedicated AI processor embedded in the Snapdragon
855 chip, we can talk about that at another time or later, but those are the hardware
elements.
That's where the job gets done and each of those cores has a different power and performance
profile to accommodate what would be, let's say, KPIs that the developer has set out
for a specific user experience, let's say we're going to say facial recognition or detecting
a keyword or somebody's voice, each of those features has
a different tolerance for latency and a different tolerance for power, so by providing multiple
compute engines from a hardware standpoint, it gives the developer choice.
On the software side, specifically in our portfolio, we have SDKs like the neural processing
engine, which I think you mentioned at the beginning, which is a very easy to use tool for
a developer who has trained in neural network offline and wants to run it on a Snapdragon
device, when they bring that model to the device using the neural processing SDK, all they
have to do is make a simple API call to run on any of the compute elements, whether it's
the CPU, GPU, or the vector process, or tensor accelerator, and then a way they go.
It's very straightforward.
You don't have to get down to the lower level close to the metal and rule up your sleeves,
but we do have tools for those that want to get their hands dirty.
We have libraries like Hexagon NN, which is a neural network library for Hexagon, if
you want to just write directly to the Hexagon processor and not worry about the other cores.
We have math libraries for CPU, and then if you're familiar with OpenCL, the Adreno GPU,
our Adreno GPU supports OpenCL, so you can program directly with the GPU, so there's
a variety of different ways with our own technology that you can access the compute on Snapdragon,
and all of that is part of what we call the AI engine.
There's also in mobile specifically, and I'd say with Android, Google released the Android
NNAPI or neural networking API back in Android O, which we supported from the very beginning
on Snapdragon.
Another time we expect that that API will become the primary, the dominant way, that in Android
if a developer wants to run a built-in application that's running DNNs online, they'll use Android
NN or NNAPI as the primary interface.
That would be, I guess, the first movement toward standardization when it comes to how
you would access different chips in the Android environment.
But I'd say by and large, there's still quite a bit of activity.
I use the term Wild West a lot because it really is the Wild West, and it's exciting.
There's so much activity, the amount of innovation, the collaboration between organizations that
have found themselves to be the most contentious competitors are now collaborating to advance
overall machine learning and artificial intelligence at a pace that I think most of us, we look
at it.
There's no hyperbolic term that could be used that doesn't apply, it actually does apply,
it's moving so fast.
Again, putting the developer head on, it sounds like particularly in the Android world,
which kind of historically suffers with a very high degree of fragmentation.
It sounds like it may still be a frustrating experience to figure out which of these tools
and APIs and things like that I need to use to take advantage of the underlying, all of
the potential underlying hardware possibilities.
Yeah, I guess it depends too on the device class.
In mobile, it's probably a little bit more complicated because there are multitude of
SOCs that are out there that are driving different mobile phones.
That's where Android NN as an API or as an interface may actually solve a lot of that
problem because if you have a common way to interface with the underlying hardware and
each of the SOC manufacturers, chip manufacturers like Qualcomm are doing our job under the hood,
which we have on our side, developing drivers for each of our compute cores that are optimized
specifically for, let's say, NNAPI, then that frustration from a developer standpoint
is minimized.
In fact, it's reduced considerably.
When you deal with proprietary SDKs, they still have an impact and still have a benefit
in a broad mobile environment, but they then become a little bit more, I guess the
priority around those become increases when you get into dedicated devices like connected
cameras or speakers where you know that the SOC that's powering that is the same one
across the board and as a developer, you have less variability so you can be more confident.
But I do think that there's, for at least for the next few years, there's going to continue
to be a lot of experimentation, there'll be a balance between standardization and maybe
a slight degradation in performance versus proprietary software execution and the ultimate performance
that balance will always be there, but the good thing is that there will be choice.
If I go back where we started this conversation and music, now I've tried to develop an analogy
or a comparison when I was doing early on in my college days when I was doing digital
music production, everything was very manual, it was all purpose built, you didn't have
any way to generalize, cut and paste and sample and sampling was actually done with analog
devices like cassette tapes and vinyl records.
Over time, music production software became more advanced and abstracted a lot of that
heavy lifting and purpose built programming that you had to do as a producer and then cut
and paste became kind of the way that you went.
So if you want to replicate a specific phrase, you could do that, just cut and paste, cut
and paste, and now it's so easy to make music as a novice.
All the tools are there at your disposal, it's very, it's democratized, that's probably
the best way to put it.
The pace that we see in, let's say machine learning and AI tools and frameworks like TensorFlow
and PyTorch and the efforts like the OpenNural Network exchange is an example, which is another
way to give a developer the latitude to use whatever framework they wish to express
their model in and not have to worry about what the underlying hardware supports.
Monarchs as an interchange format is one that, again, makes that job a lot easier.
With all these different tools and advancements, I think you're going to see a democratization.
In fact, there's already a democratization of AI happening outside of just the technology
development, like an education, you know, Andrew Ng's courses on Coursera.
There are government entities around the globe that are pushing AI machine learning
education in high school and in college.
It's, it's, it's remarkable.
So I think this short term, there's certainly pain and, you know, a lot of experimentation.
But we're, I think you're already seeing a number of examples where things are consolidating
and I'll use that term, the democratization is already underway.
You made an interesting point earlier in, you know, when I, in kind of progressing through
this conversation, I've been thinking mostly mobile devices, i.e. handsets, smartphones.
But you reference kind of device classes and, quote unquote, IoT devices, whether they're
smart speakers or, you know, by extension, kind of industrial, IoT devices, can you give
us a sense of the relative size of each of those markets from, I don't know what makes
sense like a number of devices or do you have a sense of that?
I don't have, I don't have, you can slice and dice that in a number of ways.
I think the one, I think the, the one data point that keeps getting thrown around when
it comes to specific devices outside of mobile, I think the, the smart speaker market, I've
unfortunately, I have to forgive me, I don't have the number off the top of my head and
I don't want to give you an incorrect number.
But the smart, the smart speaker market driven it primarily by the echo in the Alexa platform,
I think is probably one of the standouts.
Security cameras are very prevalent, especially from an enterprise standpoint, but the smart
speaker is a consumer device is probably the standout category leader, if you will,
on IoT, even any other embedded device outside of mobile.
That particular device, and I have to speak for my own personal, I'll speak for my own
personal view first and then I'll talk about it from an industry standpoint.
I'm one of these individuals that despite my comfort with technology and an audio, which
is really at the core of what I've been focused on most of my career and my personal life,
talking to an inanimate object has been uncomfortable, whether it's a speaker or a mobile phone
or what have you, I see people do it and I had been awkward for me for the longest time,
but what that, you know, echo class device was able to do is kind of break down that wall
between a consumer and the device and create the beginning of what is a relationship.
And I think the relationship that we will have with the devices around us will be based
in large part on voice and how, you know, not just about command and control, where you
know, ask Alexa or Cortana or Siri or what have you, for a to provide something, so make
a request, over time that's going to become more real, more lifelike, today it's very,
it's still a little bit, I don't say awkward, but it's not, it's not like you're talking
to a human being, you know you're talking to a device, whether that device or whether
the intelligence is in the cloud or on the device, you know that it's not, it's not a
companion, it's not somebody you have a relationship with, but the movements toward
making a lot of that work happen on the device, a lot of the algorithm processing on the
device is going to allow for that conversation to become really more of a conversation.
And the gaps between the device providing a response or are you talking to the device,
those gaps will be reduced or lower latency, but then also being able to detect more about
what you as an individual or how you feel, like what is it in your voice that the device
itself or the algorithm can detect, so sentiment, like are you happy or you sad, angry, and
then being able to provide even richer, a richer response and maybe even proactively engage.
So that category and without providing a, without providing a number, which again, I don't
want to, I don't want to call attention to a number that may be incorrect because I
don't have that off the top of my head, but I think that device category specifically
and the size of it, which is I think the largest of all IoT devices, that's going to be one
of the most interesting ones to watch for all the reasons that I just mentioned.
I think that that interaction, that real relationship, the companionship that you're going
to have with a device over time is going to be largely driven by voice.
That's why that category is so hot.
It's interesting.
We thought about the lack of the ability to do robust on device, audio processing, AI
audio processing as a key limitation of the user experience, but now that you say
that I totally get it, like you say something to one of these devices, it kind of you're
waiting for a long time, and a lot of that is just kind of the latency of talking to
a cloud and having that then have to come back.
Exactly.
I don't know how many times your internet's going down at your house, but if you have
one of these devices, the minute you ask for something, you get a response, sorry, I
can't help you right now.
It's like, well, you should be able to help me.
You should at least engage with me in such a way that it minimizes that frustration.
There's enough processing capability.
The algorithms are becoming far more efficient to run on device.
You can handle more keywords, natural language processing, and overall voice UI as a category.
There's so much movement there.
There's no reason why the device could not provide more utility and that level of personal
interaction moving forward, and it'll just get better over time.
Yeah.
I imagine that's coming very soon.
There's no reason why the device shouldn't at least be able to turn off the lights or something
like that when it's disconnected.
So maybe taking a bit of a step back, one of the first things I mentioned was the Snapdragon
855 platform launch you did back in December.
I kind of want to use that as an on-trade of talking about what these platforms mean.
And specifically, when we, you mentioned some of the components of a platform like the
CPU GPU, hexagon, AI accelerator, I want to get a mental model for these sound like in
some ways like overlapping components in terms of functionality, or at least they could
be used in overlapping ways.
What are the different pieces and how to developers, how should developers or users think about
using them and what directions are each of them going?
So if we use the Snapdragon 855 introduction as the springboard for this one, Snapdragon
855 is what we consider to have our fourth generation AI engine.
I mentioned the AI engine as being the kind of the sound of the parts that make on-device
machine learning acceleration possible.
In Snapdragon 855, we've done a number of things to all the compute elements.
We've added more arithmetic logic units to our CPUs, about 50 percent more.
On the CPU side, we've incorporated dot product instructions that increase AI performance
specifically at 8-bit fixed by four times or 4x.
And then on the hexagon side.
If you followed our path over these fast forward generations, hexagon has had a vector
processor where we at least we've had the hexagon vector processor in our chips going
all the way back from Snapdragon 820 to 835, 845, and now 855.
And that's been one of the primary engines for doing on-device AI.
So that we actually doubled the size or doubled the number of hexagon vector extensions
as we call them between 845 and 855.
And then we added for the first time a dedicated AI accelerator.
We call it a tensor accelerator.
It's part of the hexagon family and its sole purpose is to process DNNs on device.
The hexagon vector extensions had been the prior in prior generations had been a primary
engine for that, but vector extensions also have utility and doing vision processing
and other compute functions.
So it wasn't dedicated solely to AI.
But even with the addition of this tensor accelerator, that does not mean that the rest of the
family of compute engines doesn't participate to your point, to your question, are there
specific reasons why you would want to use one engine versus another or one compute
element versus another to run an AI algorithm?
And the answer goes back to something, I think I mentioned earlier, and it has to do with
power and performance and what you want to get out of that particular user experience
or how you want that user experience to perform.
Also, what else is happening in the system?
So if the phone is doing other tasks or as part of that user experience, you're doing
graphics processing or you need the display to be highly performant.
You may want to choose a different place to run an AI algorithm if the GPU has a higher
workload or the same is true for the vector extensions.
You may want to have a little bit more flexibility in where you run that AI algorithm.
And then from a pure AI standpoint, what we've done to combine or in the system where
combining hexagon vector extensions and our new tensor accelerator is a developer, you
can utilize both of those, which gives you a kind of a balance between what would be
programmable AI processing in the form of the vector extensions and then your dedicated
acceleration with the tensor accelerator.
You can combine those two to give, I guess I'll say I don't like to use the term ultimate,
but the best performance possible by combining those two compute elements.
But again, taking one step back, we don't look at AI as a one-size-fits-all problem,
or at least it's not one-size-fits-all, so a one-compute architecture is not going to
solve all the problems that you see or satisfy all the use cases that you see on device.
It's just we're not at a stage and I don't know if we'll ever get to that stage to be
quite honest.
But at least with A55, we've provided a balance between what is our standard portfolio of
compute with different programming capabilities and then this new dedicated accelerator, again
each with a different power and performance profile to solve for the various use cases
and tolerances that the developer has for the, when they're developing those features.
In addition, and I guess I'll build on top of that, there's a big movement in the industry
across the board, whether it's in, whether it's in the data center or it's on device,
but dedicated acceleration, dedicated AI processors, there's various terms that are used.
There's NPU for neural processing or there's VPU or IPU.
All of these, there's a very big movement in that respect across the board in mobile,
in IoT and automotive, and there is a demand for that and a need for that today.
But that doesn't mean that that's where everything's going to go.
That doesn't mean that a dedicated processor, dedicated processor is the answer to all the
problems that you're going to see or satisfying all the use cases that you're going to see
in various device classes.
You do need a wide portfolio of compute to accommodate the level of variability.
That is certainly there today and it's going to be there to, it'll be there for the foreseeable
future.
When you speak about these dedicated processors, does the tensor accelerator, for example, fit
into that or are you specifically thinking about kind of an off board, off SOC processor?
No, I think the tensor accelerator is an example of that.
If you look at dedicated acceleration, there actually are two different vectors.
One is embedded, which would be in the SOC or in the chip.
In mobile, Qualcomm and others are or have introduced dedicated acceleration.
Some of those are kind of reworked digital signal processors.
In our case, we have a ground up architecture or from the ground up custom built IP.
It's specifically designed for DNN processing.
In tensor accelerator, but hexagon is kind of the reworked DSP.
Yes, so hexagon, yeah, hexagon is a, it does actually a number of DSPs in the hexagon
portfolio.
There's a modem DSP, there's a compute DSP.
In the past, hexagon has been associated with digital signal processing and even the
vector extensions are part of our compute DSP.
But the tensor accelerator is a totally unique architecture.
In the market today, there are embedded accelerators in SOCs, in ours and others.
There are also folks that are developing kind of off-chip dedicated AI chips that just
do AI processing.
They can augment what is on the primary application processor where they can be used independently.
But there's again, going back to the, there's such a big movement and innovation is happening
so fast that I guess the more, the bigger the sandbox that you provide a developer, the
more they'll use it.
In the case of mobile, there's a balance between what you provide in terms of overall compute,
the variety of compute and the cost associated with it.
You have less latitudes when it comes to kind of a primary SOC like Snapdragon to provide
a massive AI accelerator and there's really not any need to in mobile, but there might
be other instances where you do need a pure dedicated AI processing and quite a bit of
it outside of mobile.
On the software side of things, thinking about software platforms, I would call that
kind of idea like a very unopinionated platform versus platforms that have very, this is the
way to do it, very, very strong opinions.
Do you think there's that analogy apply here and there are some implications of that on
the software side.
Oftentimes, developers want more opinionated platforms because they, they're less choices
that they have to make.
Is that analogy apply and what are the implications of it here?
Yeah, I think if you had a going back in our conversation talking about developer pain
points and what makes the developer's life easier difficult.
We're not seeing a consistency across the board because there's still a ramping up of
expertise.
When we started this, I'll just speak from where I sit from four years ago when I began
this process in this project in Qualcomm, developer savvy was, well, there wasn't much.
A lot of abstraction and providing what would be turnkey solutions like an object classifier
or image classifier or seeing classifier was what we started off with because nobody
really understood how to do the job.
They could train a model, but they didn't know where they had to run it on target.
We've gone so far ahead or moved so far ahead just in four years that now developers are
very savvy.
The tools are there.
There's a variety of them.
The education is there and I think the desire across the desire from many of the developers
that we deal with today and customers that we have today, they want more flexibility.
They want more of an ability to experiment.
They don't want to be stuck in a box and focus on a very specific programming method or
way to access a particular compute engine.
They want more variability.
They want more flexibility.
The openness is becoming more prevalent.
It's not to say that proprietary tools and being very fixed in the approach to, let's
say, running a natural language processing algorithm or something on a purpose-built
device isn't helpful, but we are actually seeing more developers looking for choice and
flexibility and modularity.
They want to get closer to the metal when it comes to programming and they'd like to
be able to do it in multiple ways.
That's a trend, but I wouldn't say that that's a dominant trend.
I think that's still, there's nothing that settles out to say that a proprietary approach
and a fixed approach is the best, maybe over time that will be depending on the use case
for the device class, but I still think it's highly variable at this point.
We're speaking very early in January, it'll be a couple of weeks before folks hear this,
but we're at the very beginning of 2019.
What do you see happening in the space in 2019 and beyond?
There are a couple of areas that I think we're looking at, and we see kind of emerging
over time.
One is the area of on-device learning, so today the lion's share of all the workloads that
we see in mobile and in other devices, it's all inference, so it's the application or
the utilization of a train model running on target to deliver that use case or solve
that problem.
There's very little learning happening, but we see a kind of a movement toward taking
a train model, having it run on target, but also being able to take in data from the
various sensors on device and augment that model to become more aware or contentionally
aware of its environment to provide more personalization for consumers.
Many of different techniques that are being explored in the industry, there's reinforcement
learning and others, but I think that's going to be a trend that we see in 2019.
Hardware acceleration or dedicated hardware for AI processing will continue to be a big
movement, both in the embedded side as well as off-chip standalone AI processors.
Unchmarking is probably one of the areas that I've mentioned Wild West a minute ago, but
I think that certainly applies here.
There's a lot of, I guess, confusion in the marketplace today when it comes to what
is, first off, what is AI, and then secondarily, how do you measure its effectiveness?
How do you measure the performance of running a particular convolutional neural network
for a particular use case, or a specific network type or class like a ResNet or VGG or
what have you?
How do you actually measure that in a real-world setting and then compare hardware platforms
to each other?
Right now, there's a variety of entities that have sprung up to try to tackle that problem,
but there's no consistency with methodology or formula or the underlying network classes
that are used to actually do the measurement and ultimately what the benefit is.
What is the final outcome of this and why does it matter?
Benchmarks and graphics and CPU and in other cases are all very well settled, and there's
still a lot of gamesmanship that goes on in those categories, but specifically in AI,
that's an area that is quite fluid and it changes, I think, day-to-day.
We see some of that settling out this year, and it would be good because it will help
with OEMs making choices about who to pick as their SAC vendor, and it will help consumers
understand a little bit better what is the benefit of what's happening on the device?
Why does this matter?
Why is this making my life more interesting and compelling and enjoyable?
And then the last one, I think, is the movement that we're pushing very hard and something
that we've been leading the charge on for a few years is 5G.
So 5G connectivity isn't just going to make that connection between the data center and
the device more efficient and lower latency, but devices will be able to share information
with each other in ways that they haven't previously.
And the combination of 5G and AI, albeit 5G, will be at the starting point in 2019, you'll
probably see developers trying to take advantage of that connectivity platform in a way that
they haven't in the past and being able to make devices connect to each other in a more
intelligent way.
We see a big future with a combination of 5G and AI, in fact, that's where we're spending
most of our time and effort these days at Qualcomm.
One of the things you mentioned kind of peak my interest in the context of on-device learning
is reinforcement learning. I typically think of that as like running lots of simulations
taking a long time, which doesn't seem like something that I'd necessarily want to do
on a device. Are you aware of specific things that are happening to address that in the
context of on-device learning?
I don't have any examples to share with you now and you're right that a lot of what's
happening today and we look at different processes that we're trying to explore, like, reinforcement
learning, usage, off-target or not on-device, but improving processes that would end up
running on the device. I don't have any examples and I just pulled that out as one potential,
but nothing specifically that I could point to. But on-device learning specifically, the
desire or the demand from developers and even our OEM partners to be able to take better
advantage of the incoming data from the various sensors on the device, you know, microphone
and camera and accelerometer to be able to provide a more personalized experience in
general. There's a there's high demand there and I think you're going to see, you'll
see more effort, maybe some experimentation, but there'll be more emphasis place on trying
to make the device itself, not just a kind of fix with a specific intelligence level,
but one that would be heightened by way of additional context that it's able to grok
from the world around it. Well, Gary, thanks so much for taking
the time to chat with us. I really learned a lot and I enjoyed the conversation.
Yeah, thanks very much, Sam. I appreciate it and thanks for let me share your birthday
with you. Awesome. Thanks. Take care. You too.
All right, everyone. That's our show for today. For more information on Gary or any of
the topics covered in this show, visit twimmalei.com slash talk slash 223. Be sure to check out
what Qualcomm is up to at twimmalei.com slash Qualcomm. As always, thanks so much for listening
and catch you next time.
