WEBVTT

00:00.000 --> 00:26.000
All right, everyone. The holidays are upon us. And by the time you hear this, we will be in a new year. And so you know what that means. That means it's time for us to explore the best of machine learning in our annual AI rewind 2021 series.

00:26.000 --> 00:45.000
Today, I have the pleasure of being joined by Georgia Geoxade. Georgia is a research scientist at meta AI. And we're here to talk about all things computer vision. Georgia first joined us on the show last year for episode 408 where we talked about her work with PyTorch 3D.

00:45.000 --> 00:57.000
Georgia, it is so wonderful to see you again and welcome back to the 20 AI podcast. Thank you. Thank you for having me again. It's an exciting episode and I'm glad to be back for it.

00:57.000 --> 01:20.000
So Sam said, I am a research scientist of Facebook AI research and I work on a lot of things on computer vision with the focus and a passion for recognition and I feel that we're going to cover a lot of that today and see where what exciting work has happened in the last year and what is ahead of us.

01:20.000 --> 01:28.000
Awesome. Awesome. Well, this year has been a big year for the company that you work at.

01:28.000 --> 01:34.000
It's not Facebook anymore. It's meta. So there's that.

01:34.000 --> 01:46.000
And it's been a big year for computer vision as well. And looking forward to digging into that. I figured we would start by just talking a little bit about.

01:46.000 --> 02:02.000
What kind of broad brushstrokes? What's your what's your feel for how we did in 2021 with computer vision? What were the main accomplishments and sentiments in the field?

02:02.000 --> 02:27.000
And you know, this is a fantastic question. First of all, and of course, subjective. So I'm going to try to sort of give my input and and like in discuss exactly why I'm mentioning these works. But as you said, Sam, it's been I think a very good year with exciting new work, new work that has actually been very impactful that shifted the field in many directions.

02:27.000 --> 02:36.000
So I'm going to mention maybe three highlights that I have noticed being extremely important in the last year.

02:36.000 --> 02:46.000
And first, I'm going to I'm going to start perhaps with graphics, which is not exactly within computer vision, but very nicely tied to it.

02:46.000 --> 03:01.000
The explosion of nerves. So nerve, which stands for neural radiance fields, have had a tremendous impact in computer vision 3D and graphics.

03:01.000 --> 03:19.000
It's it's a wonderful work simple that has unlocked something that graphics people have actually been working for quite a while. And the innovation there has been a lot with introducing implicit functions and and volumetric rendering.

03:19.000 --> 03:38.000
And the goal is to reconstruct photorealistically a scene from a few images. Well, a few is actually a little under estimated. It's like it's actually 100 or 200 images. So it's still a lot of images.

03:38.000 --> 03:58.000
But the effect is that you can photorealistically synthesize the scene from any viewpoint, given these 200 images. I just wanted to jump in and make sure I'm understanding that the idea is that you've got some 3D scene and you're not building it up from kind of a CAD system or graphics tool or something like that.

03:58.000 --> 04:09.000
You collect just pictures of the scene from various angles and through the the nerve process, you're able to create a 3D model of the scene.

04:09.000 --> 04:21.000
So correct. So so you you basically take so let's say you have, you know, a scene, let's say your room right there. And so what you will do is that you will

04:21.000 --> 04:32.000
go around and collect many images. It actually works well when it's 360 views. So let's say you have a little scene at a center of the room and you just take pictures around it.

04:32.000 --> 04:39.000
And of course, this is going to be your own picture. So they're going to be from discrete set of viewpoints.

04:39.000 --> 04:51.000
So what Nerf will do is that it will take these images. It will try to extract the camera poses. This is going to happen offline. So with systems I call map, which have now been very well established.

04:51.000 --> 05:02.000
And having posed images of that scene, it will try to reconstruct a 3D representation. Now that 3D representation is not explicit. So you can't really.

05:02.000 --> 05:16.000
It's it's not a mesh or a voxel. It's an implicit representation. And it all it does is that it allows you to shoot race and then project those raised back from a noble viewpoint.

05:16.000 --> 05:26.000
So now you are building the system that can not only understand the scene from the discrete viewpoints that you collected with your with your photo with your camera.

05:26.000 --> 05:35.000
But it will allow you to reconstruct the scene from noble viewpoints outside of these of the existing ones.

05:35.000 --> 05:43.000
And it does that by maintaining, you know, the photo realism and the detail of the scene that you are capturing.

05:43.000 --> 06:01.000
And so you that the end in Nerf is for neural. Is it a deep learning based technique? Yeah. So so the authors there use a sequence of linear like actually perceptors. So it's an MLP.

06:01.000 --> 06:19.000
So you have a little neural network that maps a 3D point XYZ and a direction to color RGB and density. The density is the occupancy basically whether you know that point is occupied by the scene or not or it's empty space.

06:19.000 --> 06:35.000
And this is all that it's doing. And with this information, you're about, you're, you're able to collect that on array. So array is a collection of 3D points and render that from a noble viewpoint.

06:35.000 --> 06:51.000
And so how is this a technique that originated this, this past year in 2021 or is it a technique that kind of found its, its voice from, you know, prior years?

06:51.000 --> 07:07.000
Yeah, I mean, I think that so I think that the work came out in 2020. Of course, you know, this is something that we need all to realize that that thing actually comes, you know, gets to to to live out of nowhere.

07:07.000 --> 07:23.000
So of course, it's based on previous work. Of course, it's motivated by previous works. I think that where this work has actually innovated is this, this using this implicit functions along with volumetric rendering.

07:23.000 --> 07:38.000
And it has since actually created a huge impact because of course, it was the first paper came in 2020, but there was a lot of things to fix. You know, how can you do this with fewer views? How can you do this faster? How can you do this by consuming less memory?

07:38.000 --> 07:50.000
So it has actually created a lot of follow up works and step by step improving this method, even as we speak, there is like tons of papers coming out even on the daily basis.

07:50.000 --> 08:01.000
Are we seeing its impact primarily in terms of academic papers or is it something that, you know, we've already seen implemented in different applications?

08:01.000 --> 08:15.000
Yeah, that's so I think up for now, I, I definitely believe that it is mostly focus and for research, especially while we try to, while the community tries to improve it, make it more.

08:15.000 --> 08:30.000
We use it friendly, but also, you know, let's have a consumer memory or in faster, but I do believe that we will see a nurse will revolutionize the rendering.

08:30.000 --> 08:36.000
So any applications that now involves rendering, I believe will move into nerves very fast.

08:36.000 --> 08:51.000
Talking about Hollywood, you know, visual effects, making movies, I'm talking about gay, the gaming industry like video games, I think they will also move towards something like a newer representation for rendering.

08:51.000 --> 09:08.000
So for that is, I think, very simple, it's, so the method is simple, it does require a little bit of our expertise and so we might actually see a shift of sort of what knowledge you will need in order to make this happen.

09:08.000 --> 09:29.000
It is a lot easier to adapt to new scenes, so you need to do less work. If you want to do, you know, move to different scenes faster, so I do think that it will actually have a tremendous impact, not now, but in five years maybe if we do this podcast again, we can revisit that question.

09:29.000 --> 09:55.000
When I think of the of rendering, one of the things that I think about is in the case of games like rendering, like from light sources and things like that is the idea that this Nerf method solves this particular problem of reconstructing the scene from images or that the techniques that

09:55.000 --> 10:08.000
were developed for that problem also have implication and kind of the broader, more broadly graphics and rendering like point source rendering that kind of thing.

10:08.000 --> 10:37.000
I think they do, I think they definitely, they definitely are, we're seeing more and more actually we're seeing the impact of nerve expanding very much so moving beyond, you know, that initial application that we've seen and we are now seeing how they can incorporate even dynamic scenes like, you know, moving scenes, we are also seeing how to change lighting, how to change materials, we're also seeing how you can hallucinate with them.

10:37.000 --> 10:50.000
The field is ever expanding, so this is why it's an exciting, it's an exciting field to be in, I'm sure graphics people are very thrilled, because there's a lot of work to do.

10:50.000 --> 10:56.000
So yeah, I feel that we haven't even seen the full impact of mercy at and what they can do.

10:56.000 --> 11:09.000
But what we know is that representation is powerful, we know the volumetric rendering is is is impactful, so the combination of these two areas, I think, will lead to a lot more innovation.

11:09.000 --> 11:29.000
Awesome, so, so that is a innovation happening on the graphics side, I think your next point is maybe closer to home for machine learning and computer vision, and it's, it's a big one that has a lot of people talking.

11:29.000 --> 11:39.000
And it involves transformers, not the robots, of course, but the transformer networks that we all know and love. What are you saying there?

11:39.000 --> 11:49.000
Yeah, I mean, this has definitely been a highlight for 2021 computer vision, we have seen transformers have finally coming into computer vision.

11:49.000 --> 11:57.000
Transformers is actually not like in natural language processing, NLP researchers, there have been working with transformers for a few years now.

11:57.000 --> 12:03.000
But they have finally made it to, and to computer vision.

12:03.000 --> 12:20.000
And it has been an exciting times because we now sort of are seeing this line of work where we are slowly replacing CNNs with transformers and for, you know, various recognition tasks.

12:20.000 --> 12:30.000
And in particular, we've also seen what is exciting about transformers and computer vision is their impact when it comes to working with extremely large scale data.

12:30.000 --> 12:48.000
So it's still quite unknown to us whether transformers work well in the low data regime. This is something that is the answer is not out yet, but we definitely know that for large data, we're talking hundreds of millions of images and more transformers are quite impactful.

12:48.000 --> 12:58.000
And is the idea that transformers can create more robust representations based on lots of data than convolutional networks.

12:58.000 --> 13:22.000
Yeah, so I think that the, so again, this is we don't we don't quite know, you know, in like theory behind these things is kind of always unclear, but so the main sort of highlight is that to CNNs, which is the predominant sort of tools that we used before to represent our visual inputs.

13:22.000 --> 13:32.000
We had had inductive biases and this comes from exactly the structure of convolutions and the image grid. Now transformers take a completely different approach.

13:32.000 --> 13:42.000
They actually treat images like a sequence of tokens. So this can be patches of images, so like a small neighborhoods of the image.

13:42.000 --> 13:54.000
And they, and the only inductive by the comes into transformers is through this, this serialization, as you would say off the image.

13:54.000 --> 14:08.000
And the pose embedding that comes in when we are processing these inputs, but other than that, everything all operations are sort of global with our intentions, being cast into these representations.

14:08.000 --> 14:22.000
So there is no other inductive basis in these networks whatsoever. So that means that they are of the potential to be a lot more powerful because you're constraining them less, but in order to achieve that, you need more data.

14:22.000 --> 14:51.000
So, so it is a much more, it is much closer to actually having a true function, function approximators, like we, you know what we say, like MLPs are great function approximators, but with CNNs that was a little bit taken away because of that particular structure and transformers are bringing this back to life, which is an exciting, exciting, and has proven also to work in for images.

14:51.000 --> 15:20.000
And how would you characterize where we are? Are we in the, you know, just kind of demonstrating that it's possible to make it work stage or have transformers in computer vision demonstrated, you know, state of the art results or, you know, either better results on, you know, known tasks or tasks that we were doing pretty well on or, you know, allowing us to.

15:20.000 --> 15:28.000
To perform tasks that we weren't able to perform well with CNNs.

15:28.000 --> 15:42.000
I love that question, and I hope that my answer is not going to anger people, but, you know, I, it's a great, it's a great question, and I hope, and I wish we asked it actually more often.

15:42.000 --> 15:52.000
So, you know, before I answer that, I'm just going to go a little bit to the NLP world and say why I think transformers are have had and tremendous success.

15:52.000 --> 16:09.000
I think it's partly due to two reasons. First is that, you know, in NLP, you can actually get a lot of data, and that's what works like bird have shown where you, you know, kind of crawl the web and you get all all sources of text data.

16:09.000 --> 16:12.000
There's a lot of that there's a lot of that out there.

16:12.000 --> 16:13.000
Yeah.

16:13.000 --> 16:24.000
And now you show how it can be anything, you know, any, any text whatsoever, whatever content, and you feed that to the beast called transformers, and it builds a representation.

16:24.000 --> 16:50.000
So, that's fantastic and a first great milestone. And the second one is that they have all these fantastic tasks, diverse tasks like question answering text generation or text fill, like they they're task are endless, and they are very different from each other, and they have shown that even in those very diverse tasks, having a global representation coming from, you know, learning through.

16:50.000 --> 16:56.000
So supervised learning with a lot of data is helps all these tasks significantly.

16:56.000 --> 17:01.000
Now, in vision, in computer vision, things are not quite like that.

17:01.000 --> 17:05.000
Folks, the follow computer vision probably have a sense for where this is going.

17:05.000 --> 17:20.000
So, you know, we are kind of in, you know, we are awkward face where we kind of want us to sort of replicate that line of work from NLP, we're very much inspired and looking up to them.

17:20.000 --> 17:28.000
But we don't have exactly, first of all, we don't have a big data set, our biggest data set is something like image net.

17:28.000 --> 17:41.000
Which is one million images, and the image has been a fantastic data set has served the community for 10 years, but the problem with image net is that, first of all, it's been around for not quite a while.

17:41.000 --> 17:44.000
It's a frozen data set, so nothing changes.

17:44.000 --> 17:57.000
So, as we develop new ideas on a data set that is constant and kind of frozen in time, we tend to sometimes our ideas overfitting on that data set.

17:57.000 --> 18:03.000
And these, a lot of, a lot of the gains that we see in images are actually transfer.

18:03.000 --> 18:08.000
And that makes sense, you know, we've been, we've been dealing with the data set for a while now.

18:08.000 --> 18:15.000
I would actually say that maybe we've trained more models on pixels on image net, but, you know, I'm not going to get there.

18:15.000 --> 18:38.000
And then the other question, of course, is what are our downstream tasks that we're showing that performance in, and this is another sort of sore point for us computer vision people, where we're also seeing a little bit of, you know, kind of sad state, where we know we have our object detection and maybe our, our segmentation, but they're all essentially classification tasks.

18:38.000 --> 18:48.000
We don't have the richness in output and in tasks that NLP has all over the like our pre training is classification, our downstream task or classification.

18:48.000 --> 18:53.000
So, it's really hard to tell if we're actually making progress or not.

18:53.000 --> 18:59.000
So, this is something I think that it is for the community to think about.

18:59.000 --> 19:27.000
I guess a couple of reactions to that one is maybe the second one informs the first, but, you know, there's classification, but then is the idea that all of the complex problems that we think of like, you know, bounding boxes and all these other things that kind of just boil down to classification.

19:27.000 --> 19:33.000
But at the end of the day, it's all classification, even if it, you know, looks more complex.

19:33.000 --> 19:53.000
Correct. So, there isn't, that's, that's absolutely true. And, I mean, our most complex task, I would say maybe is object texture, because it involves sort of maybe predicting intermediate boxes, and then on top of that, you want to predict, classify the object type, or maybe predict other 2D properties by classifying the pixels within those boxes.

19:53.000 --> 20:04.000
Even that is cast as a classroom. And what about things like my VQA visual question answering, is that more do you think of that more of an NLP task than a vision to ask or.

20:04.000 --> 20:15.000
The only thing I have to say there is that, and this was also a point made by, you know, clip, a gray paper that came out last year is that even there, the data sets are very small.

20:15.000 --> 20:24.000
And we really can't tell how effective these methods are, because we really don't have a good sense of these tasks.

20:24.000 --> 20:35.000
And, you know, I mentioned clip, which is a prime example that showed that all the gains for transformers came when they downloaded that huge data set of theirs.

20:35.000 --> 20:46.000
I think it was 250 million image text pairs. We don't exactly know this source where the data set is from, but we know it's from the web.

20:46.000 --> 20:57.000
And this is how they were only able to show that amazing property of capturing jointly images and text when training on such a huge data set.

20:57.000 --> 21:06.000
So this is sort of the, what we aspire to get to, have a training on such large images.

21:06.000 --> 21:22.000
And of course, that comes with a lot of other questions about, you know, are we doing our due diligence and making sure that this is a good, you know, data set in terms of ethics in terms of like content, like are we making sure that it's unbiased data set and so forth, which brings, of course, new challenges.

21:22.000 --> 21:34.000
But I feel that for we need to move to that scale, or otherwise we're going to kind of be stuck in our little image net sort of standard regime, which is not a good state to be in.

21:34.000 --> 21:49.000
Yeah, yeah, I think the data set side of that was my second question, and that was we, you know, we hear about and see new image data on the one hand, we hear about and see new image data sets, you know, all the time.

21:49.000 --> 22:07.000
They tend to be specialized and fairly small, you know, on the other side, just like NLP had access to the web vision has access to the web, there are tons of vision, or there are tons of images on the web.

22:07.000 --> 22:26.000
Did you think that the in order to get beyond kind of image net and classification, you know, we need to define like unsupervised problems or semi supervised problems beyond just kind of simple label, you know, supervised learning types of problems is that the big, you know, one of the big barriers.

22:26.000 --> 22:48.000
Yeah, I think that's definitely one big barrier, you know, the problem with vision and actually with all data sets is that collecting a large scale data set by crawling the web and just releasing it is almost impossible and maybe for a good reason, as in, you know, you have to make sure that you have consent from the creators of the images to actually release them.

22:48.000 --> 23:17.000
You also want to make sure that, you know, we are now entering the stage with an AI where it's no longer opportunistic, we need to be very cognizant of the data set, we're using the models we're putting out there, how they can have like maybe they can have potential harm and these are all aspects of our work and we're responsible for it's not like someone else is responsible so there is actually releasing and collecting a really single large data set is is a huge responsibility.

23:17.000 --> 23:37.000
That comes in a lot of work and that might be a roadblock to seeing larger data sets coming out, I don't have a solution to that problem but I feel that you know if there is motivation to go there thing will make that happen and I think that we also need and this is maybe my own personal opinion we need to rethink.

23:37.000 --> 24:04.000
The problem that we're solving these pretext tasks that you know are commonly referred to sort of the task that you try to solve either in so supervised learning for example in bird it was filling in the like you know masking words and filling them in we've seen recently work from actually you know my lab and coming in colleagues where they do the same thing but they're trying to they mask out patches of the image and they try to fill them in.

24:04.000 --> 24:22.000
This is a great task but it is still still sort of you know constrained to be on just in predicting pixels we need to go we need to enrich and are these pretext tasks if we want to maybe so for richer tasks downstream task moving beyond classification.

24:22.000 --> 24:33.000
I don't think that having a simple pretext task will solve complicated tasks down the line like you know maybe 3D reasoning or 3D understanding which is a field that I've been working on.

24:33.000 --> 24:39.000
I do think that there's limitation and transferring that so we need to account for that and so.

24:39.000 --> 24:59.000
But I think it's a good first step I think that we just need to broaden our horizons a little bit on what problems are solving how we're solving on how we're thinking about building representations either through self supervised learning or through supervised learning with labels but move out of this you know image to single object label let's say.

24:59.000 --> 25:15.000
Here's where clip was fantastic because that's exactly the point where text is richer contains richer information than just a label and that's what they showed they showed that that creates a great representation if you try to you know jointly capture.

25:15.000 --> 25:25.000
Text with images yeah why don't you take a few moments to you know more deeply introduce clip and why you think it was so exciting.

25:25.000 --> 25:39.000
Clip is this work from the open item and basically their their premise that there's a lot of technical content that you know I will not maybe will not you know spend time describing here but big picture is that they.

25:39.000 --> 25:51.000
They wanted to build a representation a richer representation moving beyond your classical images to object labels type of you know training to get to those representations so.

25:51.000 --> 26:07.000
They know that text contains a lot of information and they also know that images are important visual descriptors of these text and so it makes sense to sort of combine them and build a joined representation and see what you can learn by.

26:07.000 --> 26:29.000
By you know reasoning about these two different modalities together that was the first innovation just thinking along those lines of you know pairing images with richer descriptors not than just single words and then the second innovation is that they they did this by.

26:29.000 --> 26:51.000
Optimizing with a contrastive by contrastive learning instead of prediction you know one way to do this is to build a presentation where you take in an image of input and maybe predict the sentence output and we know this is that has been the how things have been done predominantly in the field but there is difficulty there because the task is actually.

26:51.000 --> 27:12.000
It's difficult predicting and in order for you to predict predict right the right sentences you need a big capacity model so what is a lot more friendly and turns out also friendly for optimization and robust is to actually train embed these two modalities the lack this sentence and the image.

27:12.000 --> 27:41.000
Let them with you know separate neural networks and then train them so that these embeddings of the sentence of the image actually are close so by minimizing a similarity I think in this case I think they use local science similarity if i'm not mistaken so the pair the sentence and the image that correspond together should have should be close in that embedding space and any other pairs coming from the whole data set should be far in the embedding space so.

27:41.000 --> 28:07.000
And this was this was the this is what they did of course there's a lot of technical details on how to make that work at a train that of course there's the corpus is huge and then they showed that you can actually you can actually get really richer presentations to this way and this is sort of and why it's exciting for a person like me know i'm not an nLP is that it shows exactly that that we need to move beyond our single label.

28:07.000 --> 28:28.000
You know regime into richer either richer outputs or either in the text description space or maybe also in 3d space that's another model that I think is important to to start thinking about so that's why I actually think that after well I would say the clip is definitely my favorite paper of the last two years.

28:28.000 --> 28:44.000
Wow what are so so far clip and nerf there's sounds like several papers that have evolved the nerf method what were some of the other favorite papers in vision this year.

28:44.000 --> 29:07.000
I want to say that I've enjoyed a lot of the full work after clip i'm going to mention this awesome paper that came out recently from rana hanaka if i'm pronouncing her name right from Chicago where she is trying to if the team there they're using the clippers presentation and they're trying to build 3d models with texture that look like the the you know the sentence for example like.

29:07.000 --> 29:17.000
I think they have like lawyer as one potential like sentence and then they transform a human mesh to look like a lawyer.

29:17.000 --> 29:37.000
So that was like a really like I have enjoyed seeing all these fun applications coming up after clip than just you know it's not just about benchmark and we know meeting numbers but like fun things that you can do with these representations.

29:37.000 --> 29:52.000
And then of course I think that highlight in computer vision after beyond that was I definitely the transformers like world of VIT which was the first you know approach to make transformers work for images.

29:52.000 --> 30:13.000
So when transformers from Microsoft that showed how you can actually go beyond image net and make that work for other computer vision tasks and where you know the innovation there is that they're trying to do things in multiple scales and multiple resolutions in order to capture the variety of the objects in images.

30:13.000 --> 30:40.000
And kind of this analogy to the the bird task where you're kind of that close completion and text and you mentioned kind of patch completion and images is that the kind of the fundamental premise of the way that transformers are being applied and computer vision is that is that what the VIT papers about or.

30:40.000 --> 30:55.000
So the VIT paper is a is a purely supervised approach so you know where you're you're taking an image you're breaking it down into 16 for 16 patches and then you feed it through a transformer network transformer network.

30:55.000 --> 31:10.000
And encoder purely to get to clap for classification so it's supervised like on image net or you know the bigger I think they also show results in their internal internal Google data said but it's it's for classification supervised learning.

31:10.000 --> 31:18.000
We have also seen a lot of use of transformers it for self supervised learning and this is where the patch completion comes in that I mentioned.

31:18.000 --> 31:28.000
And this is how to make how to you know how to use transformers for self supervised learning which is actually a big topic and computer vision today.

31:28.000 --> 31:32.000
So these are the sort of two modes of work.

31:32.000 --> 31:54.000
And I mentioned that the way folks were evolving you know that we kind of started with there's an initial paper that kind of demonstrated that transformers could work in vision and there have been subsequent papers is there a clear.

31:54.000 --> 32:12.000
This thing that has to happen in order for it to be the breakout success that it was in an LP or is it more just lots of problems and the community kind of rallying around those problems and making progress paper by paper.

32:12.000 --> 32:22.000
So I'm going to come out and say it I think we need to rethink everything we're doing.

32:22.000 --> 32:41.000
Yes I think that it will entail us breaking away from that boring regime or ad with image net we like we need to move up beyond that we need to stop worrying less I'm kind of sick of seeing papers that are just table after table after table with one percentage point of.

32:41.000 --> 33:06.000
Performance you know sorry I don't want to be like better mean but like who cares to some degree right I mean benchmarks are not there for us to beat them to death and you know burn TPU hours and you know melt the ice and Antarctic in order to get 1% like on these data sets

33:06.000 --> 33:12.000
benchmarks are there for us to test ideas quickly see if the work and then move beyond that.

33:12.000 --> 33:17.000
So I would like I would like to I think we'll need to break out of that.

33:17.000 --> 33:35.000
Of that sort of desire that we have to produce more papers with more tables and more numbers that are not substantially we need to rethink a lot we need to be more creative and what we're solving how we're solving it how we're using these data sets we you know even simple data sets with single labels can be used in very creative ways.

33:35.000 --> 33:57.000
So and of course we need to also move beyond that I think video is a fantastic data source that we need to start exploring better again video right now computer vision is predominantly used for classification isn't that bizarre like video so you take this video that has so much information that there's so many things going on and you say running.

33:57.000 --> 34:11.000
You know it's so it's so sad there's more things to it so and we need to start exploring it's right there it's right in the pixel so and we have we we know maybe we have ideas how to do it.

34:11.000 --> 34:26.000
It might take a little longer together and not you know not meet the next CBR deadline but I do think that we need to rethink re evaluate and maybe get more creative and how we're working with these data sets.

34:26.000 --> 34:30.000
That's your question.

34:30.000 --> 34:54.000
No you did in a sense I guess and not to further the labor the image net point but I think it's interesting that you know this this data set that has you know so many ways kind of embodied the success and the moment that we're experiencing right the image net moment in

34:54.000 --> 35:04.000
you know machine learning deep learning computer vision all of that it's it's almost like it's a victim of its own success and a very strength.

35:04.000 --> 35:20.000
Yeah I think that image net is so I think that it's always important to sort of step away from our everyday and kind of see where we're at so image net was a data set and this is no attack to the data

35:20.000 --> 35:32.000
fantastic it's all it's an attack to us right as scientists so image net came out 10 years ago at a time when we actually didn't have neither good tools nor good data sets.

35:32.000 --> 35:44.000
And it also came out of the time when we had this explosion of the in the industry like the Google and the Facebooks that we're seeing a huge amount of user data being uploaded.

35:44.000 --> 36:07.000
And they had immediate needs to address specific problems like they wanted to see to make sure that they don't have by images don't contain violence pornography you know all these things that you need to understand in in contents immediately and of course also business purposes down the line you know how to cater to users better by understanding their content and what you know they post and stuff like that.

36:07.000 --> 36:32.000
So image net sort of the fantastic purpose at the time because we actually needed to build our content understanding you know research to get to that point and image that was a fantastic fit and it was it found itself even in like the progress are found found itself a spot in industry and you know it's being used and now industry has built.

36:32.000 --> 36:55.000
It's own internal data says to make that happen because you know based on the nature of their user data so but we're but in research was still kind of you know stuck with sort of that paradigm but now we're 10 years later now the needs for what we need to to get to we're moving past content understanding you know like my company just announced.

36:55.000 --> 37:05.000
They want to build metaverse metaverse is this extremely complicated task extremely like heart milestone to get to.

37:05.000 --> 37:24.000
Definitely not to kind of accomplish it by classification let me tell you so these are new needs right it sets a different environment and what we need to be working in and working on and i'm not saying that we should be working on like you know making metaverse happen but it sort of dictates what are the interesting problems you know ARVR that's another.

37:24.000 --> 37:33.000
Great topic that we hadn't really been discussing 10 years ago but now we we are there because we live in a completely different time.

37:33.000 --> 37:52.000
It is a clear to you what types of problems and and approaches we need in computer vision and machine learning broadly to enable a metaverse like vision.

37:52.000 --> 38:04.000
I think many many things i think and what is exciting about metaverse or the metaverse you know types of project is that it's definitely going to be a combination of software software and hardware.

38:04.000 --> 38:15.000
So which is sort of a new that builds a new potential collaboration in these completely different domains of science.

38:15.000 --> 38:28.000
I don't i mean i feel that we could be discussing this for hours and hours but it's definitely it's definitely will involve us moving into the 3d space us being able to understand in 3d space.

38:28.000 --> 38:48.000
As being able to generate content in 3d so it will i think i think we're going to be moving towards definitely towards 3d definitely towards dynamic sort of capturing just objects and scenes while they're moving and changing and time.

38:48.000 --> 39:01.000
And i think that it will also involve a different modality so we need it's not only going to be a matter of RGB streams it will sound will come into play.

39:01.000 --> 39:15.000
I think that you know depth maybe from depth sensors will come into play this a different modality so i think it's going to be sort of a combination of having to put in together a lot of work from different domains.

39:15.000 --> 39:20.000
Last point you raise is something that i also wanted to bring up and that is.

39:20.000 --> 39:43.000
And maybe clip is an example of this but one thing that i've heard a lot about this year is interest in multimodal types of problems you know whether it's text and images text and video audio and images.

39:43.000 --> 39:50.000
Are you seeing that as well and are there any interesting things that are on your rate are there.

39:50.000 --> 40:01.000
Yeah and i want to say i mean this is completely true and it makes sense right i mean these are all modalities that come from or derived from the same sort of underlying world.

40:01.000 --> 40:20.000
The text and the audio everything everything describes that world we are so desperate to understand and here i would say that the reason i always seen like a lot of work coming out is again thanks to transformers i think this is another benefit of transformers not just the gain in performance.

40:20.000 --> 40:39.000
That we can debate or you know it might not be a substantial but it's definitely the fact that it has unified how we are building models to consume these different modalities before audio had it's completely different line of work completely different architectures and.

40:39.000 --> 40:52.000
Video also had its completely different architectures images it completely different architecture text completely different and it was even hard to read papers from like i was was having a hard time actually papers from nLP because.

40:52.000 --> 41:05.000
I couldn't really understand the underlying you know structures of the network and all that stuff and now i have no problem papers from nLP or from any sort of speech and audio processing papers.

41:05.000 --> 41:24.000
So transformers are great because they have just unified all this and as a result it's really easy to also do joined research now with these modalities you know it's it's much easier to actually build without having the necessary skill set or expertise or anything to do research in that domain.

41:24.000 --> 41:38.000
These to advance a thing will will lead to a lot more work there hopefully and I think it makes sense and I think this is a direction that we should definitely keep pursuing you mentioned in the.

41:38.000 --> 41:59.000
Context of metaverse as well as in the context of nerf the problem of 3d understanding this is kind of you know one of your home problems spend a lot of time thinking about this can you talk a little bit about where we are relative to that problem and you know what.

41:59.000 --> 42:11.000
What that community is thinking about.

42:11.000 --> 42:37.000
From different perspectives for sure I think nerves and graphics see 3d from a different perspective and but it will also come into play when we when it comes to large scale learning I think that we are still not pursuing that as as much as we should because you know it's it's hard it's a harder task you're dealing with you know the modalities different 3d data or more complicated than 2d images so.

42:37.000 --> 42:47.000
And there is definitely a large data set missing in that field right now like a data set we could say oh this is a data set where I could do some sort of large field reading learning.

42:47.000 --> 42:56.000
I do you think that this is changing and we're run reason why this is changing in my view and why I'm excited is hardware like i'm going to.

42:56.000 --> 43:22.000
This is not product placement but like this is you know iPhone 13 and it has it actually has I don't know if you guys see but like the camera sensors in the back are tremendous and part of the sensors is a lighter camera which captures that so that means that now you are walking around your pocket holding this sensor that you would never imagine that you would actually be carrying some of you might not even know that this is what you actually having your pocket.

43:22.000 --> 43:35.000
We can now capture our gbd data in the wild so I do have a reason to believe that you know maybe in 2-3 years for now we might no longer talk about our gbd data sets so.

43:35.000 --> 43:49.000
And this is all thanks to hardware so I do think that this is going to open up a huge sort of new exciting direction for us this is maybe a basic question but is there a.

43:49.000 --> 44:02.000
An image format that includes RGBD like this the iPhone 13 and whatever spit out RGBD images or they two separate streams.

44:02.000 --> 44:07.000
Great question because I was looking into this the other day.

44:07.000 --> 44:25.000
So right now it's very obscure how the various companies are handling depth and whether they're releasing them and so this is actually a two full question is first I mean we know the camera captures it and we know it's just another channel right for you for you to store.

44:25.000 --> 44:36.000
The question is how whether we can host that so you know can you have like users upload that let's say somewhere and store this data and in what format.

44:36.000 --> 44:46.000
So these are not so this is kind of still obscure there's definitely a lot of apps that allow you to extract that information which is also why I've been playing around with them.

44:46.000 --> 44:52.000
I'm not going to name them like but if you you know look around a little bit you'll find them in app store.

44:52.000 --> 45:21.000
So and they extract that information they actually store it I'm the I am extremely confident that perhaps Apple or other companies will soon sort of do that by default and the reason for that is actually very simple it's not just all to you know satisfy Georgia because she needs not you did actually depth is very important to create a lot of effects with your images which a lot of us you know care about like the Instagrams and whatever to get the legs but like it does.

45:21.000 --> 45:37.000
That does a lot like dynamic focus things like that dynamic focus this 3d 3d photo sort of you know effect to give your images a little bit of a 3d aspect I think that's an app already fit in the Facebook.

45:37.000 --> 45:56.000
Just kind of look yeah like exactly you know like just give it a little bit of like interesting perspective. So we know that we want to store that because like we have you know there is also business partners behind it so I do think that we're going to see it being sort of become more commercially available.

45:56.000 --> 46:20.000
We've talked about needing new types of problems in computer vision broadly like do you does 3d create new types of problems like all the old you know problem formulations kind of transfer like 3d object classification 3d detection.

46:20.000 --> 46:26.000
But out there kind of fundamentally new problems that 3d gives rise to.

46:26.000 --> 46:49.000
Yeah I think that there's there's two maybe aspects to this one is whether 3d can help you build better or underlying representations so this is one you know exactly like we train representations again with our static you know 2d data sets can we build better presentations through 3d similar to clip where you know text maybe a different modality.

46:49.000 --> 46:55.000
So can you do better there and then yes the next thing is tasks and we need to get.

46:55.000 --> 47:18.000
And this is where you know this new era that we live in today with a focus on air VR the verses whatever verses those are they create these new tasks and challenges for us so you do have your regular object understanding in 3d you also have your you know nerf also requires 3d understanding to work so you also have your graphics related tasks.

47:18.000 --> 47:34.000
There and of course it's our responsibility to sort of build a better suit of tasks in 3d as well to measure performance better but there is a lot of tasks that will require 3d and without 3d they're just not going to be solved at all.

47:34.000 --> 47:57.000
And obviously most of the images that we're looking at that are 2d images are you know representations of 3d scenes does is there any evidence that you know solving 3d whatever that means like gets us to 2d and where you know maybe we're spending too much time on 2d and if we figured out 3d we would get 2d for free.

47:57.000 --> 48:16.000
I mean absolutely right I mean I think that I would only be satisfied and hopeful that by learning 3d visual representations that you are already solving for 2d I mean 2d is just like the you know a simple sort of flattening of that space so absolutely.

48:16.000 --> 48:31.000
I'm asking a more specific question then you know there's kind of the yeah that kind of stands to reason but I'm wondering like is there are there specific problems that you know we've solved in in 3d and then.

48:31.000 --> 48:58.000
We found that 2d improves to 2d yeah yeah no I mean there of course there is the pure 3d tasks which would not be solved by any means in 2d but I feel that in this more sort of image content understanding setting that we haven't seen this so far and the reason we don't have data sets to see this.

48:58.000 --> 49:22.000
And I think video is a fantastic actually first data set to like data format to attack going there video comes with so many other challenges that we just haven't seen enough research and I think we should I think we should overcome this so we've talked quite a lot about the research side of things have you seen anything interesting happening.

49:22.000 --> 49:38.000
And either the commercial side of things or in terms of tools and open source projects meaning you know less, less papers and more kind of concrete realizations of some of the things that we've talked about.

49:38.000 --> 50:07.000
I do think that it's it's an exciting time I feel for startups and product driven you know work because we there's a lot of stuff that it's already working I always go back and forth about like the self driving car industry I have an idea that they're actually doing a lot of progress which of course we don't know because they don't really publish any of the stuff which makes sense but I've been feeling that like they're actually sort of that is moving even though people were less hopeful or something but.

50:07.000 --> 50:21.000
Judging from the amount of startups and the amount of great people that I see join them I am only to believe that there's stuff happening there that is quite important I don't know if you have the same sense.

50:21.000 --> 50:39.000
Yeah I feel similarly in the sense that there there are definitely you know waves of enthusiasm about the space and it kind of ebbs and flows.

50:39.000 --> 50:55.000
One of the comments that you made earlier kind of created this model for me of you know early progress in computer vision was driven by content moderation and kind of you know maybe you call it internet 1.0 or whatever 2.0.

50:55.000 --> 51:15.000
The you know the next phase I think a lot a lot of focus on computer vision is there's been a lot of this work on autonomous vehicles that you know flows into the computer vision community.

51:15.000 --> 51:29.000
And so maybe the next big problem is metaverse maybe those are like the you know the three big problems that have driven computer vision that's not like a database assessment but correct I absolutely agree with that yeah.

51:29.000 --> 51:48.000
That whole space I think is one that you know historically and I think I feel the same now about it's like I've tended to be I've tended to be a bit more conservative than some folks.

51:48.000 --> 52:06.000
And it's mostly is less about the you know exciting things happening in computer vision and more about well two things one the you know the predominance and importance of edge cases in the real world.

52:06.000 --> 52:24.000
And then to the whole kind of regulatory legal insurance like all the other crap I think you know will limit our ability to get to

52:24.000 --> 52:45.000
where you walk outside or drive around and you know most of the vehicles you see are autonomous I still think that's a way right I mean actually I believe that we will solve the first one first like covering all the edge cases and then I think getting beyond the regulatory part of things I think that's like an impossible.

52:45.000 --> 53:09.000
Just impossible I don't know politics is hard you know I don't yeah people are hard people are hard yeah yeah definitely I mean I have the same sense I've also been very sort of conservative about it also the computer vision scientists I like would I ever try a car that I know you know so you know

53:09.000 --> 53:38.000
not too much is kind of difficult but yeah but I am mostly judging it by just sort of seeing all these like wonderful amazing brilliant people that are just joining and creating startups in that field so it makes me want to think that you know maybe there is really sort of progress happening there that I don't really have a deeper insight into things I do know though that there is you know in that similar similar space but there's tons of robotics startups.

53:38.000 --> 54:07.000
And you know vision is definitely one component but it's not I mean we're moving a little bit away from just purely vision now but it is I mean vision is definitely important especially for pick and place and all these other robotic tasks but we're seeing a tremendous amount of of startups in that space again made possible by the fact that we now have companies that make commercially available robots for relatively cheap prices so you can actually very fast sort of prototype and test various

54:07.000 --> 54:10.000
robotic driven applications.

54:10.000 --> 54:14.000
Are there any particular that come to mind?

54:14.000 --> 54:23.000
I mean yeah there is definitely the started by Peter Beagle that comes to mind which is actually very big I like all the name unfortunately.

54:23.000 --> 54:24.000
Covariant?

54:24.000 --> 54:36.000
Yeah Covariant definitely and I think they also have like a vision component two things and then there is a lot of other there is one that I recently found that I was very excited about.

54:36.000 --> 54:49.000
Which was a drone it was it's a startup but for drones for that they release it underwater to you know to capture coral and the reefs.

54:49.000 --> 55:01.000
Yeah map the ocean but also understand sort of the progression of file like the damage that's being done and map that and alert people so that like you know just proactive trying to make that.

55:01.000 --> 55:08.000
I found a wonderful I think a wonderful use case of sort of that technology.

55:08.000 --> 55:18.000
And yeah I think that these are sort of some of these startups that I've I you know hugging phase I think is a very up and coming one does well.

55:18.000 --> 55:29.000
You know relying on open source libraries making projects available fast for people to get to work on so it has definitely the open source component to it.

55:29.000 --> 55:35.000
And they also have like a scientific component where they're trying to be people together.

55:35.000 --> 55:42.000
I want to give a shout out also to to timnet with her new sort of research organization that she announced a few weeks ago.

55:42.000 --> 55:43.000
Absolutely.

55:43.000 --> 55:55.000
And that's you know we need that sort of those diverse types of research organizations as well not just within industry or academia but also sort of more independent and.

55:55.000 --> 56:00.000
So I'm very excited for her for that initiative.

56:00.000 --> 56:12.000
That's maybe a segue to talking about ethics and responsible AI and intersection with computer vision you've.

56:12.000 --> 56:27.000
So I alluded to the importance of that several times in our discussion already I'm curious if you would expand on your thoughts there and where you see the big challenges and opportunities.

56:27.000 --> 56:30.000
Yeah I think.

56:30.000 --> 56:49.000
I feel that it requires exactly like all this expertise I've been building in software and you know how to make models work we need exactly that same amount of energy and effort and emphasis to be put in ethics as well.

56:49.000 --> 57:04.000
I think that's a side project or something that by two people in like a you know 1000 people or no it should actually be a substantial effort try to understand because I we have a lot of work to do and even understanding the issues.

57:04.000 --> 57:13.000
And let alone being proactive about knowing and especially now in this era of huge data sets and you know crawling the web.

57:13.000 --> 57:19.000
We need to be extra careful what we put out and I feel it's our responsibility.

57:19.000 --> 57:34.000
So any you know we I think that Tim and husband and like impactful in this field where she is educate basically she's educating us how to do this I think that's her contribution it's a fantastic one.

57:34.000 --> 57:37.000
But we need we need more we need more.

57:37.000 --> 57:58.000
We would love to spend a few minutes having you look into your crystal ball and share what you see for computer vision you know kind of sprinkled a little bit of this throughout the conversation but you know looking forward where do you think the most exciting opportunities like.

57:58.000 --> 58:13.000
If I if I was you know coming out of you know school or was I wanted to see you know what is the most what should I be working on right now I would say that right now computer vision is definitely.

58:13.000 --> 58:33.000
I think it's route is defined by the applications that we are more excited about creating and the new experiences we want to create for for people so AR VR 3D I think the metaverse related topics are definitely a huge.

58:33.000 --> 58:59.000
Direction there I would also look into I think another major field is hardware and computer vision so how can we make computer vision fast and you know more things can train faster more energy efficient especially as we're moving to these transformer models that are huge and with a lot of parameters that require a lot of energy so I think this.

58:59.000 --> 59:27.000
I think that's a very exciting one and yeah I think that these are definitely two directions I would passionately pursue and very interested and with with the second one definitely being you know not just something that will happen in the next two three years but definitely very important especially when we talk about climate change and all and all these issues that we're faced with today.

59:27.000 --> 59:47.000
And do you are there particular is there particular research or company or hardware type that you are following that you find interesting it sounds like the you don't see the GPU is the final word and acceleration for computer vision.

59:47.000 --> 01:00:15.000
No I think that Nvidia is definitely on top of it and I feel that they're they will sort of make you know that technology will definitely improve you know Google is has their own TPUs like their own processing units I do think that we might see a revolution with maybe super computers maybe different types of computing.

01:00:15.000 --> 01:00:23.000
I'm not I this is hard to tell hard to predict.

01:00:23.000 --> 01:00:41.000
Are there areas that you think will see like the big paper drop in you know I guess the idea is there's something that like feels really really close that you think you know can you know we'll see a big drop in the next year or so.

01:00:41.000 --> 01:00:55.000
I think that immediately thinking ahead I think the biggest paper drop for me will happen if someone actually replicates that vert moment and CV.

01:00:55.000 --> 01:01:13.000
I'm just going on to you know a big you know as we discussed before just a huge huge data set and showing just like not just a couple of points and improvement but just a revolution and I'm not like line of improvements across many tasks.

01:01:13.000 --> 01:01:21.000
I think that's that's what's going to happen I'm not like we'll see if that's next year or two years or ever who knows.

01:01:21.000 --> 01:01:31.000
And feels within reach feels within reach I'm not sure if it's going to be I do I do really think that we're currently a little bit it feels like.

01:01:31.000 --> 01:01:47.000
Being before the image net moment in 2012 and where you know that just completely changed the field I feel that we are ready and we definitely need that paradigm shift definitely.

01:01:47.000 --> 01:02:16.000
So I'm excited I wanted to happen I hope I hope happens awesome awesome well Georgia it has been wonderful to reconnect and appreciate all of your insights and thoughts into what's been going on in computer vision and not just what's been going on over the past year but what we have to look forward to thanks so much for taking the time to chat about it.

01:02:16.000 --> 01:02:25.000
Thank you thinking I hope it was you know informative and find that and let's see let's see with fantastic thanks so much Georgia.

01:02:25.000 --> 01:02:54.000
Hi.

