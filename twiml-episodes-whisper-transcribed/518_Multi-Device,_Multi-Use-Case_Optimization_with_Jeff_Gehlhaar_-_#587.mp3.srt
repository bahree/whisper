1
00:00:00,000 --> 00:00:10,980
Alright everyone, welcome to another episode of the Twimble AI podcast. I am your host Sam

2
00:00:10,980 --> 00:00:17,840
Charrington, and today I'm joined by Jeff Gailhar, Vice President of Technology at Qualcomm.

3
00:00:17,840 --> 00:00:22,120
Jeff, welcome back to the podcast. Thank you very much. It's great to be back again.

4
00:00:22,120 --> 00:00:25,640
Great to have another conversation with you, Sam, about what's going on in AI.

5
00:00:25,640 --> 00:00:30,960
I am looking forward to digging into our conversation. It's always a lively one. We were talking

6
00:00:30,960 --> 00:00:37,560
a little bit about what was going on in your world in our pre-call, and one of the points

7
00:00:37,560 --> 00:00:41,960
that really jumped out at me that we'll be exploring in the conversation today is kind

8
00:00:41,960 --> 00:00:46,680
of this divergence that's happening in the industry between model complexity, increasing

9
00:00:46,680 --> 00:00:52,440
and device power in some cases decreasing, in the case of like IoT devices. But before

10
00:00:52,440 --> 00:00:55,680
we do that, it's been a while since you've been on. I'd love to have you share a little bit

11
00:00:55,680 --> 00:00:58,440
about your background and role with our audience.

12
00:00:58,440 --> 00:01:05,040
Sure. Thank you very much again for having me. Look, so I'm VP Technology. I had Qualcomm's

13
00:01:05,040 --> 00:01:10,280
AI software portfolio, and so we'll of course talk about the Qualcomm AI stack, and some

14
00:01:10,280 --> 00:01:16,480
of the challenges you and I have talked about. But my role really is to provide a platform

15
00:01:16,480 --> 00:01:22,000
of software solutions on top of our AI hardware that spans Qualcomm Silicon. Then

16
00:01:22,000 --> 00:01:28,400
enables us to address market segments from cloud all the way to IoT, including automotive,

17
00:01:28,400 --> 00:01:32,760
mobile and so on. So that's really what my team is focused on doing.

18
00:01:32,760 --> 00:01:40,560
Just for context, talk a little bit about the relationship between your focus and that

19
00:01:40,560 --> 00:01:47,520
of Qualcomm AI research. As you know, I'm fortunate to have the opportunity to talk to a lot

20
00:01:47,520 --> 00:01:54,000
of your colleagues and research about their latest projects and papers at conferences.

21
00:01:54,000 --> 00:01:56,920
How does that work relate to what you do?

22
00:01:56,920 --> 00:02:01,400
Really the way to think about it is that there's sort of three prongs for success for AI

23
00:02:01,400 --> 00:02:07,480
at Qualcomm. One is hardware. Of course, we're a Silicon First company in a lot of ways.

24
00:02:07,480 --> 00:02:12,240
Software, which we just talked about. And then the research, which really is, you know,

25
00:02:12,240 --> 00:02:16,920
drives our innovation. And so my job is to work with Qualcomm research, to work with

26
00:02:16,920 --> 00:02:22,360
the hardware teams, to kind of bring a lot of those innovations forward, either in the

27
00:02:22,360 --> 00:02:26,160
form of, you know, hardware software code design, where we're maybe changing hardware

28
00:02:26,160 --> 00:02:32,840
and software together to address a new challenge, a new innovation in AI, say, Transformers.

29
00:02:32,840 --> 00:02:38,600
And this kind of thing that's a new, relatively newer technology. Or we're bringing techniques

30
00:02:38,600 --> 00:02:43,760
for optimizing networks, you know, they have a big focus on power optimization and optimality,

31
00:02:43,760 --> 00:02:49,280
bringing those techniques into the software stack so that practitioners, developers and

32
00:02:49,280 --> 00:02:54,080
practitioners in general can take advantage of those techniques in a way which doesn't

33
00:02:54,080 --> 00:02:59,360
require them to read a paper or write their own code or whatever. That's part of my

34
00:02:59,360 --> 00:03:04,960
job is to kind of bring that, you know, mass market kind of way to our end users.

35
00:03:04,960 --> 00:03:10,360
One of the things that surprised me recently in, you know, having the benefit of conversations

36
00:03:10,360 --> 00:03:16,800
with the folks on the research side and the folks on the, the enterprise side, the product

37
00:03:16,800 --> 00:03:22,600
side is how quickly things can move from one to the next. I'm thinking in particular

38
00:03:22,600 --> 00:03:31,120
of conversation that I had about using neural nets to, you know, on the research side,

39
00:03:31,120 --> 00:03:37,920
using neural nets to kind of tune radio parameters that was, that was with Joseph Soriaga.

40
00:03:37,920 --> 00:03:47,080
And it seemed like, you know, just a couple of months later, I was speaking with folks

41
00:03:47,080 --> 00:03:53,920
about that being, you know, kind of in the wild or starting to make its way into the hardware.

42
00:03:53,920 --> 00:03:59,920
Any, any perspective that you can share on that is has that kind of velocity shifted over

43
00:03:59,920 --> 00:04:05,400
the years that you've been at Qualcomm or are there things that are impacting it or changing

44
00:04:05,400 --> 00:04:06,400
it?

45
00:04:06,400 --> 00:04:09,880
I think it's an area, thank you for the compliment because sometimes we feel like we don't move

46
00:04:09,880 --> 00:04:16,480
fast enough. I think that it's an area we have focused on really kind of tightening, trying

47
00:04:16,480 --> 00:04:24,880
to tighten the, the innovation cycle from research to product. And I think it's just a pleasant

48
00:04:24,880 --> 00:04:30,960
product of us focusing on it and us really trying to be more strategic about this hardware

49
00:04:30,960 --> 00:04:37,120
software co-design, which is kind of foundational to bringing these innovations quickly. We,

50
00:04:37,120 --> 00:04:43,880
we've had this sort of virtuous three way, you know, cycle going for a couple of while

51
00:04:43,880 --> 00:04:48,800
we're probably into our seventh rate generation product now. And that really bears fruit because

52
00:04:48,800 --> 00:04:54,320
we have a much clearer perspective on the use cases that our customers want in driven

53
00:04:54,320 --> 00:04:59,120
by the innovation and research. And then when time, you know, ripens to put one of these

54
00:04:59,120 --> 00:05:03,920
innovations into practice, we have found that we're pretty well prepared to make that happen

55
00:05:03,920 --> 00:05:09,600
quickly, right? So it's that whole sort of internal ecosystem we've developed and we have

56
00:05:09,600 --> 00:05:14,960
focused deliberately on trying to increase the cadence by which we can take research and

57
00:05:14,960 --> 00:05:16,720
bring it to the marketplace.

58
00:05:16,720 --> 00:05:22,200
So let's talk a little bit about the, this idea that I mentioned earlier, kind of increasing

59
00:05:22,200 --> 00:05:31,720
model complexity and kind of decreasing device power. I guess, you know, some devices in any

60
00:05:31,720 --> 00:05:36,440
particular class of devices, they're getting more powerful, but the diversity of the types

61
00:05:36,440 --> 00:05:43,240
of devices that you're trying to run machine learning models on is increasing, including

62
00:05:43,240 --> 00:05:49,560
lower power devices. Can you just talk a little bit about what you're seeing out in the ecosystem?

63
00:05:49,560 --> 00:05:54,840
Yeah. So, so I think, you know, traditionally, if you kind of rewind a little bit, you know,

64
00:05:54,840 --> 00:06:01,880
a lot of focus on CNNs. And of course, in their heyday, CNNs were, you know, very demanding

65
00:06:01,880 --> 00:06:06,120
workloads, right, for the hardware and the software that we had at the time. And it's not like

66
00:06:06,120 --> 00:06:10,760
that demand has gone away, but as you know, the industries move towards higher complexity,

67
00:06:10,760 --> 00:06:17,000
CNNs, higher resolution inputs, transformers, which are, you know, very powerful, but are

68
00:06:17,000 --> 00:06:23,800
higher complexity in a lot of ways than CNNs. And the, we've gone from, from delivering AI

69
00:06:23,800 --> 00:06:30,200
into mobile devices, which is kind of where Qualcomm, you know, has its heart and soul into IoT

70
00:06:30,200 --> 00:06:35,880
devices, which are obviously really constrained. So think about home robot, think about a ring doorbell,

71
00:06:35,880 --> 00:06:41,800
this kind of thing. And markets like XR are particularly challenging because you have high

72
00:06:41,800 --> 00:06:47,560
complexity networks involving hand tracking or gestures or super resolving the displays and

73
00:06:47,560 --> 00:06:53,240
stuff, right, foviated views, that kind of thing. And that's a thing, that's a device you want

74
00:06:53,240 --> 00:06:59,240
to be compact, you want it to sit on your head, you can't, you know, make your face or head too hot.

75
00:06:59,240 --> 00:07:04,520
So there's power constraints, right? So there's a lot of concurrency going on, which drives complexity

76
00:07:04,520 --> 00:07:10,440
and their battery limits, right, and thermal limits. And so at the margins of these, you know,

77
00:07:10,440 --> 00:07:16,360
let's call them simpler devices. We, you know, we find an increasing challenge to take these

78
00:07:16,360 --> 00:07:22,440
complicated and, and concurrent workloads and pack them into these devices. And at the other

79
00:07:22,440 --> 00:07:27,800
extreme, you can think about an automobile is having similar characteristics, tons of cameras,

80
00:07:27,800 --> 00:07:34,600
tons of concurrent workloads. You think relatively a lot more power, but still there, thermal and

81
00:07:34,600 --> 00:07:39,720
power is a consideration. And of course, other factors like safety, right, did you have to also factor

82
00:07:39,720 --> 00:07:45,880
into your workloads? So kind of a explosion of diversity, where we're trying to build a common,

83
00:07:46,760 --> 00:07:52,360
we'll talk about this, I hope a common architecture that lets the practitioners move workloads

84
00:07:52,360 --> 00:07:58,040
one the other. So you think XR, but you know, in cabin camera and an automobile that's tracking your

85
00:07:58,040 --> 00:08:02,120
eyes and making sure you're not texting when you're socially looking out the windshield is not in

86
00:08:02,120 --> 00:08:08,200
some way so dissimilar from an sort of outside in XR workload, right? So you see kind of analogies

87
00:08:08,200 --> 00:08:15,640
across these markets for different kinds of applications, right? So we want to build a common

88
00:08:15,640 --> 00:08:21,560
kind of architecture that lets us do that. From the developer perspective is the common

89
00:08:21,560 --> 00:08:30,280
architecture in place because you see the same types of applications across one device to the

90
00:08:30,280 --> 00:08:36,360
next or, you know, one use case like XR to audio or is it not so much the same applications,

91
00:08:36,360 --> 00:08:41,640
but you want the developer to be able to reuse their skills in the different environment.

92
00:08:42,520 --> 00:08:48,840
It's a little bit of both. So we can talk about it. We recently announced the Qualcomm AI stack

93
00:08:49,320 --> 00:08:54,760
at the kind of heart of that kind of the center of it is this Qualcomm AI engine direct layer,

94
00:08:54,760 --> 00:09:01,480
which provides a API abstraction that spans across all of our chips from our cloud product all

95
00:09:01,480 --> 00:09:06,680
the way to our, you know, IoT products. And like I said, all the sort of products in the middle.

96
00:09:06,680 --> 00:09:13,480
And that is directly aimed at, you know, developers, practitioners. And I use that word broadly because

97
00:09:13,480 --> 00:09:20,120
that could mean a hands at OEM. It could mean an application developer and so on. And that is the

98
00:09:20,120 --> 00:09:26,280
contract really that we're building between the runtimes and the training frameworks to get

99
00:09:26,280 --> 00:09:32,280
on to Qualcomm silicon. So we're going to provide sort of out of the box as part of that our own

100
00:09:32,280 --> 00:09:37,560
Snapdragon nor processing SDK, which is our traditional runtime that we provide on our silicon.

101
00:09:37,560 --> 00:09:44,840
It's built on this common block. We're of course providing like TensorFlow Lite delegate plugins

102
00:09:44,840 --> 00:09:50,600
where we're going to Microsoft and Onyx runtime plugins. And that API will be open to anybody who

103
00:09:50,600 --> 00:09:58,040
wants to build sort of a direct to Qualcomm silicon, you know, metal runtime of their own,

104
00:09:58,040 --> 00:10:03,880
whether their runtime is just an application or whatever. Now we do also see, you know,

105
00:10:03,880 --> 00:10:10,440
application providers that want to provide basically the same application on like compute devices.

106
00:10:10,440 --> 00:10:15,960
So think Windows on Snapdragon, I think ARM powered Windows devices and be able to move that,

107
00:10:15,960 --> 00:10:22,760
let's say to like a handset device, okay. And they want to be able to have basically the same

108
00:10:22,760 --> 00:10:27,640
application, right. And so we're providing the same silicon acceleration and now we're providing

109
00:10:27,640 --> 00:10:33,960
the same software platform and they can very easily move their application if you will from cloud

110
00:10:34,520 --> 00:10:41,480
or from device to device onto the device. And we've demonstrated that with one of our leading

111
00:10:41,480 --> 00:10:47,800
partners and and shown how the same application can be sort of cloud powered or device powered.

112
00:10:47,800 --> 00:10:51,560
And then you can split the workload using the same, you know, abstraction.

113
00:10:51,560 --> 00:10:57,320
Can you kind of walk us through it? You know, as you're talking to these developers,

114
00:10:58,680 --> 00:11:04,120
yeah, what are kind of the real real world challenges that they run into when they're trying to

115
00:11:04,120 --> 00:11:11,640
take advantage of neural networks on device? I mean, we talk about some of them, you know, power and

116
00:11:12,200 --> 00:11:19,560
the kind of compute constraints of those devices, but kind of take us to the next level of detail.

117
00:11:19,560 --> 00:11:24,040
Like what really, what are the pain points that the developers run into?

118
00:11:24,040 --> 00:11:29,400
Yeah, I think I think one of the key ones is when we think about network optimality,

119
00:11:29,400 --> 00:11:34,280
you know, there's a lot of dimensions to it. There's the size of the network.

120
00:11:34,280 --> 00:11:38,120
And it can take storage. It can take time to download it. It's kind of, of course,

121
00:11:38,120 --> 00:11:41,880
size is directly relevant. Let's say to the number of parameters, the number of weights and so on.

122
00:11:42,760 --> 00:11:47,000
Power like you mentioned, right? You know, other factors, performance, total performance can be a

123
00:11:47,000 --> 00:11:51,960
factor of model size, right? Of course. So all these factors are really, let's put them in a bucket

124
00:11:51,960 --> 00:11:59,080
of like achieving network optimality, right? And we have demonstrated, in fact, we just finished

125
00:11:59,080 --> 00:12:04,840
a project with one of our biggest customers where we were able to demonstrate that even their

126
00:12:04,840 --> 00:12:11,720
most advanced use cases can be quantized. We can quantize the networks to 8 bits or 16 bits,

127
00:12:12,520 --> 00:12:18,040
whereas their internal kind of research had shown that they needed like floating point operations

128
00:12:18,040 --> 00:12:24,040
in order to achieve the accuracy that they were, you know, targeting. And through our advanced

129
00:12:24,040 --> 00:12:28,840
techniques, which are all part of the, you know, the software offering that we're developing,

130
00:12:29,640 --> 00:12:33,480
we were able to work with them and show them, hey, look, you can get to 8 bit on some of these

131
00:12:33,480 --> 00:12:38,520
networks. You can get to 16 bit fixed point on some of these networks. The result of that,

132
00:12:39,080 --> 00:12:44,920
and we're, we have a lot of research in 4 bit as well. The result of that is tremendous,

133
00:12:44,920 --> 00:12:50,920
you know, improvements in power, performance, the ability to run things concurrently,

134
00:12:50,920 --> 00:12:57,640
because it can run faster, right? These are all kind of, I think, the most, the biggest pain

135
00:12:57,640 --> 00:13:01,960
points, because what's happened is it used to be that CNNs were sort of over-paramortized,

136
00:13:01,960 --> 00:13:07,560
and so it was relatively simpler to compress them or relatively simpler to sparsify them or whatever.

137
00:13:07,560 --> 00:13:13,080
And as sort of complexity has gone up, and the concurrency we talked about has gone up,

138
00:13:14,600 --> 00:13:19,720
getting them all jammed into a device and getting them to work efficiently and concurrently at

139
00:13:19,720 --> 00:13:23,400
whatever frame rate you need them to get it, becomes harder and harder. And so this bridge,

140
00:13:23,400 --> 00:13:28,920
if you will, from ML ops, like what happens if you will in the cloud, in the laboratory,

141
00:13:28,920 --> 00:13:36,520
and a data scientist invents a new network architecture or a new, whatever, and they say, hey,

142
00:13:37,160 --> 00:13:43,000
now deploy it, the deployment part turns out to be really pretty hard. And now you have this sort

143
00:13:43,000 --> 00:13:47,960
of impedance mismatch between that data scientist who says, hey, I've got this perfect model,

144
00:13:47,960 --> 00:13:52,280
and it solves a problem perfectly. Why can't you make it run on the device? That's really,

145
00:13:52,280 --> 00:13:57,160
I would say, one of the key pain points right now, and we're working very aggressively to address

146
00:13:57,160 --> 00:14:04,600
that and bringing tools that can allow more easily take that model out of the data scientist's

147
00:14:04,600 --> 00:14:11,640
hands and bring it to the device in a way that does not compromise the integrity, the basic design

148
00:14:11,640 --> 00:14:20,280
of that of that model. And are you, do those tools apply only to kind of custom models that

149
00:14:22,360 --> 00:14:27,880
ML engineer might be creating, or do they also apply to kind of the lighter weight models

150
00:14:27,880 --> 00:14:32,520
that someone might download off of hugging face or something like that? Sure. So hugging faces

151
00:14:32,520 --> 00:14:37,960
is a particular example where again, we can touch on Transformers. Quantizing Transformers is a

152
00:14:37,960 --> 00:14:43,080
bit of a new art, which we have quite a bit of research on. And you will see that kind of flow

153
00:14:43,080 --> 00:14:49,000
into our tools. But if we take the more general part of your question, no, these tools apply

154
00:14:49,000 --> 00:14:53,640
to all call it run of the mill, you know, models, resnets and those kinds of things that people

155
00:14:53,640 --> 00:15:01,240
are maybe familiar with, as well as these custom ones, right? And so we're working very hard to

156
00:15:01,240 --> 00:15:06,760
make sure that if you've got a model in TensorFlow or PyTorch, whether it's a stock resnet or you

157
00:15:06,760 --> 00:15:11,160
pull it off of GitHub or whether it's, you know, a proprietary model, that you can run it through

158
00:15:11,160 --> 00:15:17,720
this workflow and you can get, you know, consistent results to bring that to, in our case, Qualcomm,

159
00:15:17,720 --> 00:15:22,920
you know, Silicon and Qualcomm software. We think that's one of the challenges and I think it's

160
00:15:22,920 --> 00:15:30,360
evidenced by the fact that there are, you know, new companies surfacing all the time to deal with

161
00:15:30,360 --> 00:15:36,120
ML ops issues. And ML ops has its own set of complexities, managing the data, managing the

162
00:15:36,120 --> 00:15:40,520
pipeline, scaling up training. I mean, there's a lot of challenges there, right? We don't want to

163
00:15:40,520 --> 00:15:47,000
be one more challenge in the pipeline. We want to go, okay, you solve your ML ops problem or your

164
00:15:47,000 --> 00:15:51,480
training thing. That's fine. We don't want to be the, oh, but now it doesn't work on device. We're

165
00:15:51,480 --> 00:15:55,240
working really hard to make that transition, you know, as smooth as possible. So can you talk a

166
00:15:55,240 --> 00:16:01,320
little bit about the workflow that you just mentioned, kind of what are the steps or tools that

167
00:16:01,320 --> 00:16:07,320
you're providing that enable folks to do this? Yeah. So I think you've had some of our, some of my

168
00:16:07,320 --> 00:16:11,800
other colleagues talk about like the AI model efficiency toolkit. That's kind of at the center

169
00:16:11,800 --> 00:16:17,880
of this, right? So, so think about the workflow being like this. There are kind of maybe two aspects

170
00:16:17,880 --> 00:16:23,640
the model efficiency toolkit. One is an aspect that that pieces of it can plug directly into your

171
00:16:23,640 --> 00:16:28,520
training pipeline. So think quantization where training is a good example. So this is the process

172
00:16:28,520 --> 00:16:36,920
where during the training cycle itself in your training loop, you are conditioning the network to

173
00:16:36,920 --> 00:16:41,000
understand what the effects of quantization are going to be on that particular network. So it

174
00:16:41,000 --> 00:16:48,280
learns the noise, if you will, that gets introduced during that quantization process instead of waiting

175
00:16:48,280 --> 00:16:55,160
until you get all the way to the deployment phase and you now realize, hey, this model isn't very

176
00:16:55,160 --> 00:17:00,520
resilient to quantization, right? So some models really require quantization or training and we've

177
00:17:00,520 --> 00:17:06,200
got tools that plug into PyTorch and plug into TensorFlow, add a little bit of extra to your

178
00:17:06,200 --> 00:17:12,520
training pipeline and then condition the model during training to be quantization aware.

179
00:17:12,520 --> 00:17:17,000
Now, there's a whole category of models where you don't need that sophisticated

180
00:17:17,640 --> 00:17:23,240
approach. You can do what we call post training quantization, okay? So you've trained the model in

181
00:17:23,240 --> 00:17:29,720
your favorite framework. We've got tools that will read that native file format.

182
00:17:30,360 --> 00:17:35,000
We run it through a series of post training techniques. Again, a lot of this we have done

183
00:17:35,000 --> 00:17:41,480
quite a bit of innovation on and studying how do you do this in a way that preserves the integrity

184
00:17:41,480 --> 00:17:47,880
of the model and also reduces the precision which reduces the size, you know, reduces power

185
00:17:47,880 --> 00:17:52,840
consumption and so on, which are objectives. So you would run it through post training quantization.

186
00:17:52,840 --> 00:18:00,520
This produces a representation of the model in this Qualcomm AI engine direct format. Think

187
00:18:00,520 --> 00:18:07,160
about it like an intermediate representation. So it's a common representation. Any graph that comes

188
00:18:07,160 --> 00:18:12,520
from PyTorch or TensorFlow will be quantized and then reduced into this intermediate representation.

189
00:18:12,520 --> 00:18:18,040
From that intermediate representation, you can then as a practitioner, direct where you want

190
00:18:18,040 --> 00:18:23,160
that model to run. Do you want it to run in our AI hexagon accelerator? Do you want it to run

191
00:18:23,160 --> 00:18:28,600
on the GPU? Do you want it to run on the CPU? Once we have it in that common representation,

192
00:18:28,600 --> 00:18:32,920
you can then run it wherever you want. And a lot of our customers will go, oh, I want to run this

193
00:18:32,920 --> 00:18:38,120
one on the GPU and that one on the accelerator and so on. That one on the low power sub-system and

194
00:18:38,120 --> 00:18:45,560
so on. That's kind of the basic workflow. And then if your deployment is, you know, a smartphone,

195
00:18:45,560 --> 00:18:52,360
you can run the AI accelerator. If your deployment is a cloud device or a heavy-edged device,

196
00:18:52,360 --> 00:18:56,440
it's using our cloud processor, you can take the same representation and run it there.

197
00:18:57,240 --> 00:19:01,160
And that's a multi-core device. So you have a lot of performance so you can run there. So that's

198
00:19:01,160 --> 00:19:07,880
where this idea of having a common representation across our silicon ties in with that sort of

199
00:19:07,880 --> 00:19:13,000
developed workflow that a day of scientists might pursue. You've also been doing work in network

200
00:19:13,000 --> 00:19:17,880
architecture. So how does that plan to this workflow? You know, network architecture, again,

201
00:19:17,880 --> 00:19:23,880
is sort of an evolving technology, right? There was a big rush and Google was an early innovator

202
00:19:23,880 --> 00:19:29,000
and we're happy to be partnered with Google on our offering. Now, the way to think about it is,

203
00:19:29,000 --> 00:19:37,640
I put it in this sort of very advanced category, right? But again, it's one of the tricks when

204
00:19:37,640 --> 00:19:44,600
one of the tools that we're offering to our customers to bridge that gap from what a data

205
00:19:44,600 --> 00:19:50,760
scientist might want to do sort of in the laboratory, if you will, when they're designing a network

206
00:19:50,760 --> 00:19:57,800
and tying the design of our network to hardware-specific characteristics. So that's kind of one

207
00:19:57,800 --> 00:20:04,840
attribute. We're working with Google to make sure that Vertex NAS understands Snapdragon,

208
00:20:04,840 --> 00:20:09,080
if you will, that's the way to put it. So that the network architectures that are produced

209
00:20:09,080 --> 00:20:15,640
from neural architecture search are sort of inherently hardware aware. So they're inherently

210
00:20:15,640 --> 00:20:24,040
optimized for Snapdragon, okay? And so by tying those two things together, again, we are providing

211
00:20:24,040 --> 00:20:32,680
a way for the data scientists to produce a result that is sort of prepared to run in one of these,

212
00:20:32,680 --> 00:20:37,240
say, power constrained environments. And we've had good success with some XR and automotive

213
00:20:37,240 --> 00:20:44,600
workloads where we're seeing the, we're seeing NAS produce in a much quicker way than a data scientist

214
00:20:44,600 --> 00:20:50,920
could iterate on their own innovative network architectures that are tuned for Snapdragon

215
00:20:50,920 --> 00:20:55,560
that reduce the latency, increase the frame rate, reduce the network size, all, you know,

216
00:20:55,560 --> 00:21:00,440
characteristics that we're trying to achieve when we're helping like an XR customer or an automotive

217
00:21:00,440 --> 00:21:06,360
customer, you know, get a very demanding set of workloads onto, you know, our platforms.

218
00:21:06,360 --> 00:21:12,120
So it's another tool there, but I think that for the listeners, the key takeaway is this linking

219
00:21:12,920 --> 00:21:18,040
the speed at which you can search a lot of, you know, run a lot of experiments, if you will,

220
00:21:18,040 --> 00:21:23,560
automatically, and linking it to hardware awareness so that the final output

221
00:21:23,560 --> 00:21:30,120
is preconditioned to do well on our silicon. You talked about frame rate, are the primary

222
00:21:30,120 --> 00:21:35,720
workloads that these are being used for vision workloads? Yeah, so I mean, I think of that because

223
00:21:35,720 --> 00:21:39,240
that's the predominant workload. Of course, if this were like natural language processing,

224
00:21:39,240 --> 00:21:43,560
you know, you might say like how fast can it, you know, parse a sentence or whatever,

225
00:21:43,560 --> 00:21:48,840
right? So that has to do more with like, you know, word length, right? Or per length. But typically,

226
00:21:48,840 --> 00:21:55,480
you know, so many of the applications which are demanding are, you know, we can go across the

227
00:21:55,480 --> 00:22:00,440
whole spectrum. So of course, in mobile, it's typically, you know, low light video, let's say,

228
00:22:00,440 --> 00:22:05,960
is a pretty demanding workload, super resolution, right? We've been to, we've shown some super

229
00:22:05,960 --> 00:22:11,880
resolution in games, right? So I'm doing gaming, and I want to super resolve, you know, the action

230
00:22:11,880 --> 00:22:20,520
scene in the game to increase the, you know, quality and the experience XR. So maybe two kinds

231
00:22:20,520 --> 00:22:25,720
of workloads, not only the rendering part, right? And the sort of you're, you're doing

232
00:22:26,520 --> 00:22:31,240
some kind of augmentation or whatever. And also things like hand gesture, right? Which again,

233
00:22:31,240 --> 00:22:35,720
we talk about frame rate there because you want to sample it often enough that it's a smooth

234
00:22:35,720 --> 00:22:41,400
experience, right? That you don't miss gestures, you don't miss motion, right? So we oftentimes use

235
00:22:41,400 --> 00:22:46,440
frame rate a little loosely, meaning kind of sampling interval, right? For, for an experience.

236
00:22:46,440 --> 00:22:51,080
And like we said, the car, it's looking down the road, you don't want it to look down the road,

237
00:22:51,080 --> 00:22:55,880
you know, once a second, you want it to look down the road, you know, 30 times a second or 25 times

238
00:22:55,880 --> 00:23:04,520
a second, right? Do you have a sense for when, when folks know that they need to start exploring

239
00:23:04,520 --> 00:23:08,760
network architecture search? Like what are, what's the wall that they're bumping up against that?

240
00:23:08,760 --> 00:23:16,360
Oh, if you're experiencing that, and maybe now you need to go to this advanced level.

241
00:23:16,360 --> 00:23:21,720
Yeah, so that's kind of a complicated surface. You know, we didn't talk about, we didn't talk

242
00:23:21,720 --> 00:23:27,400
about, for example, mixed precision, right? Is another kind of technique of like you don't have

243
00:23:27,400 --> 00:23:31,640
to just do stuff in one precision, you can mix different layers and different precision,

244
00:23:31,640 --> 00:23:36,680
and we support that. You know, I think there's a couple drivers, the idea that

245
00:23:36,680 --> 00:23:39,640
that you can sort of algorithmically

246
00:23:42,040 --> 00:23:48,360
imbue this algorithm with an awareness of the hardware is something which is hard to sort of

247
00:23:48,360 --> 00:23:53,480
learn at scale. And in some cases, we don't want to tell, you know, all of our customers,

248
00:23:53,480 --> 00:23:57,720
all of our secrets, right? So there's a little bit of that going on. But it's also hard to learn

249
00:23:58,840 --> 00:24:03,960
a lot of the subtlety that something like a NAS system can learn on its own and can leverage.

250
00:24:03,960 --> 00:24:08,760
The other thing too is just raw experimentation. I mean, if NAS can produce a thousand

251
00:24:08,760 --> 00:24:15,000
candidate models in some unit time, how long would it take a data scientist to think out the same

252
00:24:15,000 --> 00:24:20,360
thousand experiments, right? And so some of it's just kind of a brute force, like search

253
00:24:21,000 --> 00:24:27,320
the solution space in an automated way, but it's directed, right? It's got considerations for

254
00:24:27,320 --> 00:24:34,280
hardware, it's got considerations for size, and so on, kind of baked into it. So it's a nice tool to

255
00:24:34,280 --> 00:24:39,800
increase the rate at which a data scientist can explore different approaches. So it sounds like

256
00:24:40,760 --> 00:24:50,600
given the relative cost and complexity of using it, there's a space of high value problems where

257
00:24:51,480 --> 00:24:56,600
they're close enough that they think that there's a solution, but not quite far enough that they're

258
00:24:56,600 --> 00:25:01,720
at the solution that, you know, this is something to try. Yeah, you can think about it kind of maybe

259
00:25:01,720 --> 00:25:06,920
in two areas. You can think about it like if you have kind of no idea how to solve a problem,

260
00:25:06,920 --> 00:25:12,680
right? Your search space is very large. This might be a way to kind of get to an idea, you know,

261
00:25:12,680 --> 00:25:18,360
automatically through basically, you know, wrote experimentation to solve your problem.

262
00:25:18,360 --> 00:25:23,560
The experience we've done so far have focused more on, I've got a network kind of to your point.

263
00:25:23,560 --> 00:25:28,600
I've got a network, and you know, if I apply this technique, how much better can I get it, right?

264
00:25:28,600 --> 00:25:34,360
And sometimes you go, oh, I saved, you know, 15% in latency or something, right? Which can translate

265
00:25:34,360 --> 00:25:38,760
into power or frame rate, depending on what you're trying to, you know, what effect you want.

266
00:25:39,320 --> 00:25:43,480
And that could mean, oh, I could run one more network with the same unit time because this

267
00:25:43,480 --> 00:25:48,520
one's going to finish sooner, right? So, you know, these projects, because they have so many

268
00:25:48,520 --> 00:25:53,720
networks and there's so much complexity are really sort of a puzzle fitting experience. You can't

269
00:25:53,720 --> 00:25:58,840
generally, at priori say, oh, this network absolutely has to run in such a amount of time,

270
00:25:58,840 --> 00:26:03,080
or use so much power because you're looking at a collection of networks that are together

271
00:26:03,080 --> 00:26:07,960
are solving a problem, right? Again, something people might, your audience might be experienced with

272
00:26:07,960 --> 00:26:12,360
this is, you know, virtual reality, you know, metaverse XR, whatever term you want.

273
00:26:13,720 --> 00:26:17,400
They realize, oh, it's following my hands and it's rendering the screen and they don't really

274
00:26:17,400 --> 00:26:22,920
think about all the things it's doing. But if it doesn't do all those things fast enough together,

275
00:26:22,920 --> 00:26:28,760
you just don't have that immersive experience. Right, right, right. You mentioned mixed precision.

276
00:26:29,880 --> 00:26:36,840
Can we dig into that a little bit more? Where is mixed precision now on kind of the adoption

277
00:26:36,840 --> 00:26:43,160
cycle? Is it being broadly used, do you think, or folks just starting to explore it?

278
00:26:43,160 --> 00:26:49,240
So I would say that there's been a period of exploration, say, maybe the last product cycle,

279
00:26:50,600 --> 00:26:56,520
where we enabled it in our products. But again, a little bit sort of to the nastery,

280
00:26:56,520 --> 00:27:02,440
there weren't good tools to sort of automate it. So it was a way to address point problems.

281
00:27:02,440 --> 00:27:07,640
Oh, this layer is having a problem quantizing, or we need more output resolution here. So we're

282
00:27:07,640 --> 00:27:13,160
going to use a higher precision layer that produces higher resolution activations or something.

283
00:27:13,800 --> 00:27:20,200
Now we're introducing automatic mixed precision. So again, bridging that the friction points

284
00:27:20,200 --> 00:27:26,360
from taking a model that works great on a bench somewhere for a data scientist and getting it

285
00:27:26,360 --> 00:27:31,320
onto device. Sometimes, again, thinking about post-training, quantization techniques,

286
00:27:31,320 --> 00:27:38,040
this would be one of them. I'm not really retraining the model. I'm looking, I'm analyzing the model

287
00:27:38,040 --> 00:27:43,880
and saying, oh, look, I can see that layer 42. That's the universal number of everything, right?

288
00:27:43,880 --> 00:27:52,200
Layer 42. That's the one that's causing the trouble if it's in say 8-bit integer precision.

289
00:27:52,200 --> 00:27:59,560
Let me move it to integer 16. And boom, my quantization problem goes away, right? That's the source

290
00:27:59,560 --> 00:28:05,880
of my accuracy. So again, providing tools for practitioners to automate that so that it's more

291
00:28:05,880 --> 00:28:12,120
push button. That's the road we're on. And well, those features are arriving now in our tool chain.

292
00:28:12,120 --> 00:28:19,320
What are the features that you are expecting to see looking forward? So again, I think we're early

293
00:28:19,320 --> 00:28:28,840
in NAS. I think we're still somewhat early in sort of quantization of transformers and all their

294
00:28:28,840 --> 00:28:34,520
variants, right? So we're seeing kind of an emergence of vision transformers. We're seeing

295
00:28:34,520 --> 00:28:39,880
transformers for like multi-modal kinds of applications, right? And so on. I would say we're,

296
00:28:39,880 --> 00:28:43,160
you know, we've done a bunch of research there, but I would say that that's an emerging

297
00:28:44,360 --> 00:28:48,760
area where we will see, you know, practitioners moving towards

298
00:28:49,400 --> 00:28:54,360
want to apply those and facing some of the same hurdles that they're facing now on sort of

299
00:28:54,360 --> 00:29:03,160
traditional architectures. And I would also say that, you know, there's a lot of smaller friction

300
00:29:03,160 --> 00:29:11,160
points in moving from, you know, the data scientists part of the ecosystem to the, you know,

301
00:29:11,160 --> 00:29:19,880
final deployed product. And so we can think about streamlining like the model conversion steps

302
00:29:19,880 --> 00:29:28,360
better. Int 4 is a really exciting, even more exotic data format. You know, we recently announced

303
00:29:28,360 --> 00:29:34,040
some discussions around FP8, which is another data format. So there's a lot of things coming with

304
00:29:34,040 --> 00:29:39,560
the continued, you know, challenges as, you know, like we started off this discussion. I don't see

305
00:29:39,560 --> 00:29:46,360
the challenge of increasing model complexity faced with, you know, struggles to reach optimality.

306
00:29:46,360 --> 00:29:51,640
In various forms, data format is away, NAS is away there, right, the tools we're providing. These

307
00:29:51,640 --> 00:29:59,800
are all roads to the same goal, which is more, you know, more complexity in, you know, packages that

308
00:29:59,800 --> 00:30:04,600
will also get more capable, but there'll be limits, right, that will always face kind of a

309
00:30:04,600 --> 00:30:09,400
innovation challenge. So I would say the tooling is really going to be directed at continuing to

310
00:30:09,400 --> 00:30:15,880
make that easier for practitioners. And at the same time, continuing in the direction of things

311
00:30:15,880 --> 00:30:21,560
like more NAS, lower precision and so on, right, which will pose our own challenges going forward.

312
00:30:21,560 --> 00:30:28,840
And do you see the tools themselves getting easier to use or are they based on what I've seen

313
00:30:28,840 --> 00:30:33,160
thus far? They're pretty low level tools. How did those, how did those evolve?

314
00:30:33,160 --> 00:30:38,600
Yeah, so I think, so the way they're evolving is we're not ready sort of sort of fully

315
00:30:38,600 --> 00:30:44,680
announced stuff, but you can think about we will also be introducing a graphically oriented tool.

316
00:30:44,680 --> 00:30:51,160
So again, moving from the, I'll call it advanced practitioner who maybe is part data scientists

317
00:30:51,160 --> 00:30:56,440
and part deployment engineer, right, to where, again, we want to make this simpler and simpler

318
00:30:56,440 --> 00:31:03,560
and more and more mass market. And so yes, we've started with, you know, with command line tools

319
00:31:03,560 --> 00:31:09,080
and sophisticated, you know, profilers and debuggers. What you will see from Qualcomm is,

320
00:31:09,080 --> 00:31:15,240
I think, increasingly steady set of announcements around, oh, now we've got a, you know,

321
00:31:15,240 --> 00:31:20,200
profiler or visual profiler for this. Oh, now we have a way to do, you know, network

322
00:31:20,200 --> 00:31:26,040
inspection. And so the way I think for the audience, I think about it is like source code debugging,

323
00:31:26,040 --> 00:31:30,280
if people, you know, still write code and they remember what that's like, you know, you want to,

324
00:31:30,280 --> 00:31:35,080
you want to write your code and you do not want to see it your debugger in a semi language.

325
00:31:35,080 --> 00:31:40,360
Like you don't want to see what the compiler turned your code into. You want to debug your code

326
00:31:40,360 --> 00:31:46,360
in the context that you wrote it that you understand it. And so the analogy, the way I think about

327
00:31:46,360 --> 00:31:51,880
it is the same here. A data scientist designed a network and they visualize it like a set of

328
00:31:51,880 --> 00:32:00,120
connected nodes. They don't want any, I think, hardware or compiler company to go, oh, now you have

329
00:32:00,120 --> 00:32:08,840
to debug your network that you, you know, conceptualize as a graph in a semi language. And so we're

330
00:32:08,840 --> 00:32:14,440
going to do the same sort of thing. We're going to provide debuggers that put that performance and

331
00:32:14,440 --> 00:32:20,920
data that the developer needs to understand how their network is performing on our silicon in

332
00:32:20,920 --> 00:32:26,600
the context of the original graph as it arrived, you know, at the start of that pipeline I described,

333
00:32:26,600 --> 00:32:31,240
right? And that's really the link we're trying to build is you think as a data scientist about

334
00:32:31,240 --> 00:32:36,920
your network and TensorFlow, and we want to show it to you on the device. Sure, with device-specific

335
00:32:36,920 --> 00:32:42,600
performance metrics and accuracy metrics, but in the context of this graph, the way you, the way

336
00:32:42,600 --> 00:32:47,640
you designed it, right? Not in some low level kind of, you know, a semi language, if you will,

337
00:32:47,640 --> 00:32:53,800
right? And that's what you'll see is more and more visually oriented tools that make the,

338
00:32:53,800 --> 00:33:00,520
you know, ML ops part of the equation and the deployment part a lot more equitable.

339
00:33:00,520 --> 00:33:06,120
When you talk about the relationship between kind of the higher level tools and programming

340
00:33:06,120 --> 00:33:11,480
languages that a data scientist or developer might want to use in the lower level hardware,

341
00:33:12,120 --> 00:33:18,120
it calls to mind comparisons to things like CUDA. Is that, you know, how should we think about the,

342
00:33:18,120 --> 00:33:25,640
what you're offering with SDKs relative to what someone might be familiar with in the GPU world?

343
00:33:25,640 --> 00:33:32,280
Yeah, so I think, I think, you know, I don't really want to do a bake off, but I think the analogy

344
00:33:32,280 --> 00:33:36,760
runs like this. You could think of CUDA and CUDA is a lot of things, right? It's a parallel

345
00:33:36,760 --> 00:33:41,800
programming language, it's a, you know, and so on. But you could think about our Qualcomm

346
00:33:41,800 --> 00:33:49,480
Engine Direct as being sort of Qualcomm's answer to CUDA in that it provides a common way

347
00:33:49,480 --> 00:33:54,760
for practitioners to either use existing runtimes, right, to deploy their solutions,

348
00:33:54,760 --> 00:33:59,240
or if they're advanced, and that's what their application demands, they can sort of program

349
00:33:59,240 --> 00:34:05,080
directly. So the analogy is Nvidia provides UDNN, which sits on top of CUDA, right? That's an

350
00:34:05,080 --> 00:34:10,120
example of a library. You know, our stack provides very similar kinds of capabilities.

351
00:34:10,120 --> 00:34:15,480
If you want to roll up your sleeves, you want to write a CUDA kernel for the GPU. Of course,

352
00:34:15,480 --> 00:34:21,560
there's a way to do that. The analogy is, if you want to write a custom application or a custom

353
00:34:22,120 --> 00:34:29,320
operation or layer for the Qualcomm AI stack, we provide a way to plug that into the Qualcomm

354
00:34:29,320 --> 00:34:35,000
Engine Direct. We have an API for that. We provide a debugger, we provide a compiler and tools

355
00:34:35,000 --> 00:34:42,680
for hexagon. We provide the same for our mobile GPU. So it's a very analogous offering. And then

356
00:34:43,400 --> 00:34:49,320
one thing which a lot of people like are these visual consoles that Nvidia provides that sit

357
00:34:49,320 --> 00:34:56,600
on top of their GPUs. We have offered that for like our gaming customers for a long time.

358
00:34:56,600 --> 00:35:01,960
We are expanding that kind of offering to the AI portfolio and really to provide

359
00:35:01,960 --> 00:35:07,880
the debug ability of Snapdragon as a whole. So you will see us working to integrate

360
00:35:09,400 --> 00:35:14,680
our debug tools, not just in the AI space and there will be AI-specific tooling, but more generally

361
00:35:15,240 --> 00:35:20,040
as Snapdragon is a development platform, you will see us kind of expand that offering.

362
00:35:21,320 --> 00:35:27,960
So to sort of maybe put a bow on it, I think at the library level and at the sort of programming

363
00:35:27,960 --> 00:35:35,880
and lower levels, we have a one-for-one corresponding kind of offering to Nvidia. And then you will

364
00:35:35,880 --> 00:35:42,280
see us kind of put the top of the story onto that offering in the next period. Over the years,

365
00:35:42,280 --> 00:35:49,320
we've talked a lot about the ecosystem and the way your tools support and the way you work with

366
00:35:49,320 --> 00:35:56,440
other ecosystem partners. You've already mentioned things like Onyx and PyTorch TensorFlow.

367
00:35:56,440 --> 00:36:04,360
Can you give us an update on where the ecosystem priorities are and any new and exciting updates?

368
00:36:04,360 --> 00:36:07,960
So yeah, I can maybe tease a little bit without getting into too many details.

369
00:36:07,960 --> 00:36:15,160
So you know the way I think about it is this, we continue and I think the Qualcomm Edge and

370
00:36:15,160 --> 00:36:25,160
Direct is a good concrete example of where we are trying to provide an abstraction of API surface

371
00:36:25,160 --> 00:36:33,480
onto which all these runtimes can plug in. So we want maximum access to our silicon and our

372
00:36:33,480 --> 00:36:39,160
software. We don't just leave it to them. We've invested working with Google on TensorFlow

373
00:36:39,160 --> 00:36:43,320
delegates working with Microsoft on Onyx runtime. So these are ways in which we're partnering with

374
00:36:43,320 --> 00:36:48,120
the broader ecosystem. Some of the work we do, some of the work they do, we share the work.

375
00:36:48,120 --> 00:36:55,880
An exciting thing to think about is the Cloud to Edge story. The idea that you could

376
00:36:57,000 --> 00:37:05,880
have like a Snapdragon Windows compute device and maybe also an XR device that run very

377
00:37:05,880 --> 00:37:11,240
similar applications or share and experience. These kinds of multi-device

378
00:37:11,240 --> 00:37:20,040
experiences are I think only really possible when you've got this common abstraction that we're

379
00:37:20,040 --> 00:37:25,480
building and you've got common runtimes. So that's why it's so important for us to work with

380
00:37:26,040 --> 00:37:32,440
like the Microsoft of the world and the Google of the world so that if you were Adobe or something

381
00:37:32,440 --> 00:37:37,080
and you're building Photoshop and you want a Photoshop experience on an Android device and you

382
00:37:37,080 --> 00:37:45,400
want a Photoshop experience on a Snapdragon ARM powered PC device, you don't have to write

383
00:37:45,400 --> 00:37:49,640
complete different pieces of software because underneath it the silicon is Qualcomm silicon.

384
00:37:50,440 --> 00:37:54,440
And so we want to kind of make that easier. So that's an example of the kind of thing we have in mind.

385
00:37:55,000 --> 00:38:00,600
And we're partnering with the operating system providers if you will, the main drivers of

386
00:38:00,600 --> 00:38:08,520
the broader ecosystem, but also like these ISVs, right? So as our portfolio of use cases expands,

387
00:38:08,520 --> 00:38:14,600
then the partners aren't just our traditional handset OEMs, they become Microsoft, Adobe,

388
00:38:14,600 --> 00:38:19,400
you know, these other kind of companies that have really compelling applications that they can bring

389
00:38:20,360 --> 00:38:25,960
to an assortment of Qualcomm devices. So that's kind of where I think we will see our ecosystem

390
00:38:25,960 --> 00:38:31,480
efforts expand. We've touched on throughout the conversation, some of the work happening in

391
00:38:31,480 --> 00:38:38,360
automotive as kind of this, you know, touchstone for, you know, a set of use cases that are kind of

392
00:38:38,360 --> 00:38:48,120
pushing the, pushing the edge. Can you talk a little bit about what you're seeing in automotive

393
00:38:48,120 --> 00:38:53,800
nowadays and kind of what's new and interesting in that world? Yeah, so new and interesting is

394
00:38:53,800 --> 00:39:00,920
automated cars are coming, but they're not here yet, right? So, you know, we have moved from,

395
00:39:00,920 --> 00:39:06,600
you know, connectivity, which is our traditional business. So this is like, you know,

396
00:39:08,360 --> 00:39:12,600
your car is connected to the internet so that you can, you know, get emergency services and stuff,

397
00:39:12,600 --> 00:39:19,080
to digital cockpit, right? So think about in cabin, right? And so we have very strong offerings there,

398
00:39:19,080 --> 00:39:24,280
and we talked a little bit about like gaze detection, you know, cameras inside the cockpit that

399
00:39:24,280 --> 00:39:30,200
are monitoring the driver as part of a safety protocol. Eight as, we recently want to

400
00:39:30,200 --> 00:39:36,040
a bid with with with BMW, for example, you know, to provide BMW, you know, eight as solutions.

401
00:39:36,040 --> 00:39:44,040
Think about this is, this is, you know, semi-autonomous safety oriented systems, right?

402
00:39:44,040 --> 00:39:49,640
This is lane to very advanced lane departure warning, lane keeping,

403
00:39:51,320 --> 00:39:57,880
automatic parking, backup. I mean, everything, everything related to making the automobile

404
00:39:57,880 --> 00:40:03,640
safer and more automated experience, right? And we're on this sort of trajectory towards

405
00:40:03,640 --> 00:40:07,720
sort of full automation, but before we get there, there's a lot of problems as all. So

406
00:40:07,720 --> 00:40:15,720
we want the car to be surrounded by cameras. So we're talking 10 cameras, 15 cameras, right?

407
00:40:15,720 --> 00:40:21,640
Completely surrounding. We're talking about immersive in cabin experiences. So

408
00:40:23,160 --> 00:40:31,080
displays, right? Perseete displays, perseete audio experiences, right? We think, oh,

409
00:40:31,080 --> 00:40:35,320
well, a car doesn't have to have NLP, but you could talk to your car. It could talk back to you,

410
00:40:35,320 --> 00:40:40,120
right? I mean, there's just layers and layers and layers of AI in an automated vehicle,

411
00:40:40,120 --> 00:40:46,120
both for the comfort of the passengers and the safety of the passengers. And so we're looking at

412
00:40:46,680 --> 00:40:49,800
all of those kinds of problems. And the most challenging ones are

413
00:40:51,640 --> 00:41:01,400
safety critical time critical kinds of problems. So viewing the car in the context of all the

414
00:41:01,400 --> 00:41:08,840
other cars on the road, very precise lane positioning for, you know, for lane keeping and so on.

415
00:41:09,960 --> 00:41:16,840
But things like, things like drive policy, like I want to tell the car, I want to go from here to

416
00:41:16,840 --> 00:41:22,200
there and I want to know it needs to know where I'm going, but it needs to be aware of other cars

417
00:41:22,200 --> 00:41:26,600
making lane changes. It needs to plan lane changes. It needs to do that in a safe and predictable

418
00:41:26,600 --> 00:41:33,400
manner, right? These are all very intensive, basically computer vision, you know, AI power

419
00:41:33,400 --> 00:41:39,880
computer vision problems that have to be completely seamless, you know, to the driver and are

420
00:41:39,880 --> 00:41:44,680
very demanding from a predictability point of view, you cannot be like, oh, I'm late, you know,

421
00:41:44,680 --> 00:41:48,840
with that frame, like that is could be a problem. And you have to, of course, design all that

422
00:41:48,840 --> 00:41:55,400
software in a safety critical kind of way. So that's really the challenge. And to come sort of full

423
00:41:55,400 --> 00:42:01,320
circle on the discussion, there's a lot of compute power that our platform brings to the automobile.

424
00:42:02,120 --> 00:42:07,800
But it's proportional to the demand of the workload, right? So the challenges that we see in XR,

425
00:42:08,360 --> 00:42:15,960
we see sort of analogous challenges in, in a vehicle, because the number of cameras is higher

426
00:42:15,960 --> 00:42:20,840
or the resolution is higher or the modalities are different, like a mix of LiDAR and radar and

427
00:42:20,840 --> 00:42:26,520
camera all working together to solve a problem. This introduces, you know, multimodal kinds of

428
00:42:26,520 --> 00:42:32,040
challenges. So there's a lot of very exciting challenges. And I think you'll see Qualcomm continue

429
00:42:32,040 --> 00:42:36,840
to, I know you will see Qualcomm continue to make strides on a mode of market. It's a very

430
00:42:36,840 --> 00:42:43,320
important market for us. Again, we do fundamental research there that drives our innovation. And

431
00:42:43,320 --> 00:42:49,800
it's a similar kind of cycle that we talked about with Qualcomm AI research. We do fundamental

432
00:42:49,800 --> 00:42:54,920
research in these automotive networks. And that drives, you know, our product line.

433
00:42:56,360 --> 00:43:03,880
Do you take a strong position on vision first type of automotive experience? Or do you

434
00:43:05,640 --> 00:43:11,880
also support and work with kind of these with other sensors and kind of a sensor fusion type of

435
00:43:11,880 --> 00:43:19,400
environment? Yeah. So we we're very agnostic. We do what our customers want, right? And so we're

436
00:43:19,400 --> 00:43:23,560
looking, like I said, it kind of three, the three primary modalities camera, which I think is sort

437
00:43:23,560 --> 00:43:30,200
of camera first in a lot of ways. I think pretty much for everybody. And then radar and LiDAR.

438
00:43:30,200 --> 00:43:35,160
So we look at sort of all, I'll call them the three major modalities, four sensors.

439
00:43:35,160 --> 00:43:44,920
Awesome. Any final thoughts, you know, as we close out, you know, what do you think we'll be talking

440
00:43:44,920 --> 00:43:50,440
about next year when we get together? Well, I hope, I hope, you know, thinking about your sort of

441
00:43:50,440 --> 00:43:56,200
practitioner, you know, audience, I hope, let's put this way, I am planning that we will be able to

442
00:43:56,200 --> 00:44:01,640
talk and a lot more concretely about let's say the tools that we talked about, right? These sort of

443
00:44:01,640 --> 00:44:07,000
visually oriented tools. We hope to be able to show them publicly by then. That's our plan.

444
00:44:08,600 --> 00:44:13,880
And I would like to talk a lot more about automotive probably. We will be another year into that,

445
00:44:13,880 --> 00:44:20,600
you know, evolution and hopefully, you know, of course, talk about a few more customers concretely,

446
00:44:20,600 --> 00:44:25,880
but we expect to, you know, win more business and make more advances there. I say that because I

447
00:44:25,880 --> 00:44:32,120
feel like it's perhaps one of the most compelling and demanding markets, you know, for AI,

448
00:44:32,120 --> 00:44:36,200
generally speaking, right? I think you can't build one of these safe vehicles without AI. I don't

449
00:44:36,200 --> 00:44:42,120
think there'll be a lot of debate about that statement. And I really hope we will be able to showcase

450
00:44:42,120 --> 00:44:48,760
more of these multi device use cases, right? We're working pretty hard on that. And I think we will

451
00:44:48,760 --> 00:44:55,320
see in the next year, the fruits of that come to bear like in a real kind of context that I can

452
00:44:55,320 --> 00:45:00,360
talk about publicly, but just know that a lot of work in that direction. And we're excited about

453
00:45:01,560 --> 00:45:06,760
we're excited about Windows on Snapdragon and, and, you know, putting that power of, you know,

454
00:45:06,760 --> 00:45:11,800
an all day device and people's hands that can also do AI, right? Because we take for granted

455
00:45:11,800 --> 00:45:16,280
background blur and these other things, but they can be even better with AI, right? And they can

456
00:45:16,280 --> 00:45:22,280
be done in a way that does not compromise battery life and, you know, makes the, the experience that

457
00:45:22,280 --> 00:45:28,520
we all, you know, live with every day a lot more, a lot more seamless. So I hope that these are

458
00:45:29,160 --> 00:45:32,680
a forecast of some of the things we'll talk about that would be more publicly, you know,

459
00:45:32,680 --> 00:45:38,840
announced in the next, you know, period. Awesome. Awesome. Well, Jeff, it's always a pleasure to

460
00:45:38,840 --> 00:45:43,720
reconnect with you. Thanks so much for joining us. I appreciate it. And you have a good rest of your day.

461
00:45:43,720 --> 00:46:11,720
I appreciate it. Thank you.

