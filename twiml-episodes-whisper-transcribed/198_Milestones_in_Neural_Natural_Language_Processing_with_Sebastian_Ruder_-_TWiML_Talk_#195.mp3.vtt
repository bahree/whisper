WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.400
I'm your host Sam Charrington.

00:31.400 --> 00:34.600
In this episode we're joined by Sebastian Rooter.

00:34.600 --> 00:38.960
Sebastian is a PhD student studying natural language processing at the National University

00:38.960 --> 00:44.440
of Ireland and a research scientist at Text Analysis Startup, Alien.

00:44.440 --> 00:49.120
In our conversation, Sebastian and I discussed recent milestones in neural NLP, including

00:49.120 --> 00:52.480
multi-task learning and pre-trained language models.

00:52.480 --> 00:57.680
We also discussed the use of attention-based models, tree RNNs and LSTMs and memory-based

00:57.680 --> 00:58.680
networks.

00:58.680 --> 01:04.320
Finally, Sebastian walks us through his recent UML fit paper, short for universal language

01:04.320 --> 01:09.060
model fine-tuning for text classification, which he co-authored with Jeremy Howard

01:09.060 --> 01:15.800
afast.ai, who I interviewed in episode 186.

01:15.800 --> 01:19.420
Before we dive into the conversation, I'd like to send a huge thanks to our friends at

01:19.420 --> 01:22.440
IBM for their sponsorship of this show.

01:22.440 --> 01:27.440
Interested in exploring code patterns leveraging multiple technologies, including ML and AI,

01:27.440 --> 01:29.440
then check out IBM Developer.

01:29.440 --> 01:34.600
With more than 100 open source programs, a library of knowledge resources, developer advocates

01:34.600 --> 01:40.400
ready to help, and a global community of developers, what in the world will you create?

01:40.400 --> 01:45.760
Live in at IBM.biz slash ML AI podcast, and be sure to let them know that Twimmel sent

01:45.760 --> 01:46.760
you.

01:46.760 --> 01:50.320
And now onto the show.

01:50.320 --> 01:53.320
All right, everyone.

01:53.320 --> 01:56.200
I am on the line with Sebastian Rooter.

01:56.200 --> 02:01.160
Sebastian is a PhD student studying natural language processing at the National University

02:01.160 --> 02:05.360
of Ireland, as well as a research scientist at Alien.

02:05.360 --> 02:08.160
Sebastian, welcome to this week in Machine Learning and AI.

02:08.160 --> 02:09.160
Hey, Sam.

02:09.160 --> 02:10.360
Thanks for having me.

02:10.360 --> 02:12.880
Yeah, I'm really excited about having you on the show.

02:12.880 --> 02:15.560
We've been trying to coordinate this for a while.

02:15.560 --> 02:20.120
And so thank you once again, that you've been doing some really interesting work at kind

02:20.120 --> 02:25.760
of the intersection of deep learning and natural language processing that I'm looking forward

02:25.760 --> 02:27.120
to diving into.

02:27.120 --> 02:30.640
But before we do that, I'd love to learn a bit about your background.

02:30.640 --> 02:35.920
You've been studying natural language processing and computational linguistics in particular

02:35.920 --> 02:38.400
since undergrad days.

02:38.400 --> 02:42.560
How did you get interested in into this field of study?

02:42.560 --> 02:48.240
Yeah, so yes, I said I did my undergrad in computational linguistics at the University

02:48.240 --> 02:50.520
of Highlandberg in Germany.

02:50.520 --> 02:57.440
And computational linguistics really is still quite a small field, or at least when I was

02:57.440 --> 03:01.680
studying that, there wasn't that much interest in it.

03:01.680 --> 03:09.560
And so how I initially got into that was that I was kind of doing high school, really interested

03:09.560 --> 03:13.320
in both mathematics and languages as well.

03:13.320 --> 03:17.360
So I really liked kind of learning different languages, but at the same time kind of the

03:17.360 --> 03:20.200
analytical side of math really appealed to me.

03:20.200 --> 03:26.440
And so I really was looking for something that like for some fields that were I could manage

03:26.440 --> 03:30.000
to combine both of those maybe quite different areas.

03:30.000 --> 03:35.240
And then in doing research, I really found that computational linguistics was really the

03:35.240 --> 03:41.200
field which is really at the intersection of yeah, like computer science and linguistics.

03:41.200 --> 03:48.160
And so for me, I tried that out and really seemed to be the perfect fit for me personally.

03:48.160 --> 03:53.480
And a lot of the kind of my classmates at that time, what I really liked about computational

03:53.480 --> 03:59.960
linguistics was that it seemed to because you kind of need such a like a peculiar

03:59.960 --> 04:02.360
set of interests.

04:02.360 --> 04:11.040
So there were a lot of people in that subject with like a variety of different interests,

04:11.040 --> 04:15.160
people who were interested in kind of other things, social sciences, practical linguistics

04:15.160 --> 04:16.160
as well.

04:16.160 --> 04:20.680
So really had kind of a very wide variety broad set of backgrounds.

04:20.680 --> 04:26.680
And I think that made studying and working in the field particularly interesting for

04:26.680 --> 04:27.680
me as well.

04:27.680 --> 04:31.440
Tell us a little bit about your graduate work.

04:31.440 --> 04:36.640
And yeah, so I did kind of the undergraduate in that there.

04:36.640 --> 04:42.520
And then afterwards I was really looking for, I really wanted to stay in the field as

04:42.520 --> 04:47.280
well, but I was looking for to get some industry experience first.

04:47.280 --> 04:54.120
But then in kind of researching for a job, I came across a program kind of an industrial

04:54.120 --> 05:00.080
PhD program where it could combine research and kind of industry work as well.

05:00.080 --> 05:02.400
And that really appealed to me.

05:02.400 --> 05:06.520
And yeah, I decided to go in that direction, consequently.

05:06.520 --> 05:12.400
And then doing my graduate work, I was really been looking for something initially I started

05:12.400 --> 05:15.440
working on kind of a lot of the applications of NLP.

05:15.440 --> 05:22.840
So as it's often used in the industry setting, like working on different applications of

05:22.840 --> 05:28.560
text classification, like sentiment analysis, or different forms of information extraction.

05:28.560 --> 05:34.000
And then kind of working on those very downstream tasks, I realized that actually the most kind

05:34.000 --> 05:39.200
of the challenge that permitted those or what was really at the heart of solving them was

05:39.200 --> 05:45.160
really because we had to create text classification applications for a lot of different domains,

05:45.160 --> 05:48.760
different languages, different data from different customers as well.

05:48.760 --> 05:54.920
So I really realized that actually we in order to solve those challenges better, we actually

05:54.920 --> 05:59.760
need to have better methods and better algorithms to learn from limited amounts of label data.

05:59.760 --> 06:04.680
So really having either models that analyze better or better ways to learn from either

06:04.680 --> 06:10.720
distant or distant supervision or from unlabeled data to.

06:10.720 --> 06:17.680
So this kind of idea of how can we more efficiently use label data or how can we learn from additional

06:17.680 --> 06:23.840
sources of supervision has really driven a lot of my interest and really a lot of my graduate

06:23.840 --> 06:24.840
work.

06:24.840 --> 06:30.600
And you're you're finishing up your PhD kind of any day now, right, you're on your last

06:30.600 --> 06:31.600
year?

06:31.600 --> 06:36.240
Yeah, I'm in my last year some currently in the process of just writing up my thesis and

06:36.240 --> 06:40.680
putting those different projects on which I worked on it together essentially.

06:40.680 --> 06:45.480
And I guess the last background question you are also working as a research scientist

06:45.480 --> 06:50.840
at Alien, which it sounds like is through this industrial PhD program?

06:50.840 --> 06:53.000
Yeah, exactly.

06:53.000 --> 06:55.280
And what does Alien do?

06:55.280 --> 07:01.960
So Alien is focused on the natural language for us in startup, which is mainly focused

07:01.960 --> 07:07.040
on analyzing and providing services around news.

07:07.040 --> 07:12.400
So we have some kind of a different number of products, one which aims at aggregating

07:12.400 --> 07:18.480
news and enriching news with different forms of kind of semantic information like entities,

07:18.480 --> 07:24.880
but also relations and so and sentiment to doing things like named entry recognition and

07:24.880 --> 07:28.120
to linking sentiment analysis around those.

07:28.120 --> 07:33.280
And then there's kind of a separate set of services which are which developers can integrate

07:33.280 --> 07:37.920
into their own applications to kind of use NLP in their own services.

07:37.920 --> 07:43.120
And then finally, we also do a fair amount of consulting to developing kind of specialized

07:43.120 --> 07:46.240
applications for particular customers.

07:46.240 --> 07:49.000
Okay, very cool.

07:49.000 --> 07:58.320
And so maybe to kind of dive into some of your work in NLP, you recently wrote I think

07:58.320 --> 08:05.360
on your blog post about kind of the history of natural language processing from your perspective.

08:05.360 --> 08:13.920
Maybe kind of walk us through the, maybe not the full history, but kind of the interesting

08:13.920 --> 08:18.640
recent historical developments in the field.

08:18.640 --> 08:19.640
Sure, yeah.

08:19.640 --> 08:26.040
So that blog post kind of arose in conjunction or as a byproduct of a session of natural

08:26.040 --> 08:30.840
language processing that we organized at the deep learning level, which is kind of a

08:30.840 --> 08:36.480
big initiative kind of a summer school like event in South Africa that kind of sought

08:36.480 --> 08:41.480
to bring together and strengthen the African machine learning community.

08:41.480 --> 08:47.160
And for a session of natural language processing, we essentially wanted to give kind of both

08:47.160 --> 08:52.480
beginners as well as maybe people who've already worked with NLP and overview of maybe

08:52.480 --> 08:58.320
some of the most important milestones that are still relevant today or that they can

08:58.320 --> 09:05.200
still like knowing them would still kind of help them tackle ongoing challenges.

09:05.200 --> 09:13.520
And in compiling this kind of this overview, Herman Camper, who co-organized the session

09:13.520 --> 09:19.800
with me, we essentially started out because we really wanted to have to get something

09:19.800 --> 09:25.000
that not only reflects our opinion, but also some opinions of a larger set of experts.

09:25.000 --> 09:30.200
So we started out just sending emails to a number of experts, NLP, just to get a bit

09:30.200 --> 09:35.040
more, more suggestions and more ideas.

09:35.040 --> 09:39.440
And then essentially we compiled kind of the eight milestones that were mentioned.

09:39.440 --> 09:42.400
And then we found relevant into a list here.

09:42.400 --> 09:49.600
And yeah, and so just because the main kind of category of methods are currently being

09:49.600 --> 09:56.040
used in NLP at this point, are really mostly using neural network-based approaches.

09:56.040 --> 10:01.920
So because of that, we mainly focused on kind of approaches or milestones that have relevance

10:01.920 --> 10:07.560
to that, although in the post there's also a lot of other kind of more traditional milestones

10:07.560 --> 10:13.720
that led up to that research in which kind of a lot of research later on is building on.

10:13.720 --> 10:19.480
Yeah, so to think so, essentially the milestones we arrived at were kind of the

10:19.480 --> 10:27.880
first neural language models that were proposed in 2001, kind of as a something that has been

10:27.880 --> 10:35.320
in many different ways kind of built upon and or co-opted by subsequent approaches because

10:35.320 --> 10:40.440
a lot of different tasks like sequence-to-sequence learning or even learning word embeddings can

10:40.440 --> 10:46.160
be expressed as some sort of variation of just doing language modeling.

10:46.160 --> 10:51.480
And something that is becoming more prevalent these days is multi-task learning, which

10:51.480 --> 10:59.520
was first proposed for NLP in 2008, as kind of a very milestone, very seminal paper of

10:59.520 --> 11:01.360
Colourbird and Destiny.

11:01.360 --> 11:05.680
And that paper also got a test of time award at ICNL this year.

11:05.680 --> 11:07.520
And then probably what was that paper called?

11:07.520 --> 11:12.680
Yeah, so it's like a unified architecture of four-netual language processing, I think.

11:12.680 --> 11:16.600
Yeah, deep neural networks with multi-task learning, I think that's the full title.

11:16.600 --> 11:20.840
Yeah, so that's definitely, if you haven't got that paper yet, that's really worth reading.

11:20.840 --> 11:27.200
And I'm surprised to hear that one was 10 years ago, multi-task learning seems to be just

11:27.200 --> 11:29.200
getting popular.

11:29.200 --> 11:36.000
Well, I should say I'm reading a lot more about it over the past, maybe six to nine months.

11:36.000 --> 11:41.080
So in that paper in particular, that introduced kind of a lot of different techniques like

11:41.080 --> 11:47.240
using CNNs for text, for instance, or even using something like unsuverse learning of

11:47.240 --> 11:48.240
word embeddings.

11:48.240 --> 11:50.160
That was in 2008.

11:50.160 --> 11:55.640
And only about five years later, people were really starting to use word embeddings.

11:55.640 --> 12:00.680
And even nowadays, yeah, so now like 10 years later, people are actually really starting

12:00.680 --> 12:02.480
to pick up multi-task learning.

12:02.480 --> 12:08.480
So in many ways, that particular paper was kind of a bit ahead of its time and not really

12:08.480 --> 12:16.640
widely kind of regarded or not a lot of people kind of took inspiration from that paper

12:16.640 --> 12:17.640
at that time.

12:17.640 --> 12:21.800
And then kind of probably what most of the experts we were talking to mentioned really

12:21.800 --> 12:28.200
as one of the biggest recent milestones was the use of word embeddings that were pre-trained

12:28.200 --> 12:36.440
and learned in unsuverse way on a large corpus so that really started out with word-to-vec.

12:36.440 --> 12:41.080
And kind of a whole separate branch of research that I've kicked off of really trying to

12:41.080 --> 12:45.200
better understand what these word embeddings capture.

12:45.200 --> 12:51.320
And word embeddings really are still very widely used, even word-to-vec is still often

12:51.320 --> 12:54.160
used in papers these days.

12:54.160 --> 13:00.600
And then kind of the last milestones you mentioned were essentially just the more prevalent use

13:00.600 --> 13:03.000
of using neural networks for NLP.

13:03.000 --> 13:07.880
And mainly then kind of in three different types, so essentially the main types people

13:07.880 --> 13:13.600
are using these days are recurrent neural networks and mainly these long short-term memory

13:13.600 --> 13:21.480
networks which are kind of a modification on this classic recurrent neural network.

13:21.480 --> 13:27.120
Then convolutional neural networks have been proposed for NLP and also used in particular

13:27.120 --> 13:33.760
for some text classification tasks like sentiment analysis.

13:33.760 --> 13:39.520
They have been used quite often and now more recently also for different tasks like machine

13:39.520 --> 13:43.880
translation for instance because people have realized that using these conclusions you

13:43.880 --> 13:52.840
can actually capture more efficiently long term dependencies in your data by using kind

13:52.840 --> 13:57.560
of wider receptive fields with daily latest convolutions in your models.

13:57.560 --> 14:03.760
So there's been some recent scene end-based models for machine translation.

14:03.760 --> 14:09.880
And then one particular category that I find quite interesting in NLP is that you have

14:09.880 --> 14:15.920
because linguistically if you look at a sentence it really, its sentence can be kind of split

14:15.920 --> 14:16.920
up hierarchically.

14:16.920 --> 14:22.080
So you have words or even like going on the sub word level you have different more themes

14:22.080 --> 14:27.720
that compose into a word and then you have words that form different clauses or chunks

14:27.720 --> 14:28.720
in a sentence.

14:28.720 --> 14:33.760
So instead of processing a sentence sequentially you can have models that take its input

14:33.760 --> 14:38.120
or act upon this tree structure as well.

14:38.120 --> 14:43.440
And these have been kind of called historically like recursive neural networks because they

14:43.440 --> 14:49.760
kind of recursively from the bottom up starting from the individual words builds the representation

14:49.760 --> 14:55.000
of the entire sentence using some sort of composition function.

14:55.000 --> 14:59.320
And more recently they're also called tree RNNs or tree LSTMs.

14:59.320 --> 15:04.760
So you can have an LSTM that doesn't act on a sequence but on entire tree.

15:04.760 --> 15:08.120
How much are those used and how well do they work?

15:08.120 --> 15:15.920
Yeah, so it really depends on the kind of tasks you use so they can be kind of useful if

15:15.920 --> 15:21.680
you want to, for instance, incorporate so for machine translation some of these some

15:21.680 --> 15:27.400
modifications using these kind of three-based methods have been proposed essentially to

15:27.400 --> 15:31.560
incorporate a bit more syntactic structure into the model.

15:31.560 --> 15:36.760
So just to make it a bit easier for the model to model this type of structure which helps

15:36.760 --> 15:42.360
when translating into certain languages or maybe if you translate from one language

15:42.360 --> 15:50.120
where, for instance, the verb comes, so with the object comes first to an language where

15:50.120 --> 15:55.000
this, where the position of object is reversed, for instance.

15:55.000 --> 16:02.720
So in some sense this can help you model yeah, some syntactic relations and or yeah another

16:02.720 --> 16:07.320
task like for sentiment, there's been some datasets where you can have kind of supervision

16:07.320 --> 16:12.680
at the word level where these for which task these have been originally proposed.

16:12.680 --> 16:18.520
So there's been a few things in general still if you have like an arbitrary NP task, probably

16:18.520 --> 16:26.040
like a bidirectional LSTM will still get you further probably but I would guess or I really

16:26.040 --> 16:33.040
hope to see, I personally would hope to see more of these models that can actually incorporate

16:33.040 --> 16:38.520
some linguistic information in kind of feel as a future direction.

16:38.520 --> 16:45.160
So I think this maybe while it's maybe not the most performant model yet, I think it's

16:45.160 --> 16:52.440
really a kind of useful way or useful kind of method or kind of class of methods to

16:52.440 --> 16:53.440
think about.

16:53.440 --> 16:59.480
And more recently you can have people have started using kind of graph convolution networks

16:59.480 --> 17:06.240
which basically apply like a scene in the convolution operation on on this tree as well.

17:06.240 --> 17:10.400
And they've shown some good results for yeah, for tasks we have some dependencies like

17:10.400 --> 17:16.160
semantic role labeling for instance, like semantic what like semantic role labeling,

17:16.160 --> 17:22.920
which is yeah, which is kind of a classic LP task and basically a form of so it's also

17:22.920 --> 17:29.720
people also refer to it as shallow semantic parsing and it essentially comes kind of from

17:29.720 --> 17:38.280
a and has like an underlying theory in that you have different frames of sentences.

17:38.280 --> 17:44.320
So depending what works, you use each verb put a vocal different frame and then you would

17:44.320 --> 17:51.080
essentially try to label the arguments of that verb that coca with that frame.

17:51.080 --> 17:55.720
So it's kind of a more linguistically informed or a classic linguistic task that people

17:55.720 --> 17:56.960
have been evaluating on.

17:56.960 --> 18:00.680
The next milestone that you pick up is sequence sequence models, right?

18:00.680 --> 18:02.880
Yes, exactly.

18:02.880 --> 18:06.400
Those have been popping up all over the place, not just NLP.

18:06.400 --> 18:11.520
Yeah, so it's really kind of like, yeah, like in doing that review, I looked back at

18:11.520 --> 18:15.920
the paper again and they in their paper, I think mainly evaluated on machine translation,

18:15.920 --> 18:21.520
which is I think really the kind of the biggest application these kinds of models have been

18:21.520 --> 18:23.760
applied to.

18:23.760 --> 18:28.000
But then, yeah, so for context sequence, the sequence models are essentially just a general

18:28.000 --> 18:31.920
kind of framework that tries to map an input sequence so it can be words, but can also

18:31.920 --> 18:39.320
be any other type of symbols to an output sequence using an encoder neural network that encodes

18:39.320 --> 18:44.240
this input sequence into a fixed access vector and then output sequence that's given that

18:44.240 --> 18:47.160
vector generates the target sequence.

18:47.160 --> 18:52.320
And yeah, and this has already been proposed in 2015 for machine translation, but since

18:52.320 --> 18:58.760
then basically a lot of applications in NLP, like as soon as you try to generate a sentence,

18:58.760 --> 19:04.720
most of these models would try to use kind of a sequence, a sequence model under the hood.

19:04.720 --> 19:10.960
So these have been very popular for things like chatbots or even in settings where you

19:10.960 --> 19:15.560
don't have an input sequence as an input, but just kind of another representation.

19:15.560 --> 19:21.160
Like for image captioning, for instance, you can imagine that you have a CNN model that

19:21.160 --> 19:27.400
generates a representation of an image and then based on that, you would use that as

19:27.400 --> 19:32.840
initialization for your LSTM and you would then try to generate the caption of that image.

19:32.840 --> 19:38.920
So this kind of framework has really turned out to be very versatile and really applicable

19:38.920 --> 19:40.600
in a lot of scenarios.

19:40.600 --> 19:43.120
You also mentioned attention-based methods.

19:43.120 --> 19:45.600
How do those fit into NLP?

19:45.600 --> 19:51.360
Yeah, so attention, probably like everyone who's kind of aware of sequence sequence model

19:51.360 --> 19:53.640
probably has also heard of attention.

19:53.640 --> 20:01.280
So attention was kind of the probably the main improvement that has really allowed machine

20:01.280 --> 20:10.400
translation models to exceed and overcome or outperform classic phrase-based or statistical

20:10.400 --> 20:13.080
machine translation models.

20:13.080 --> 20:16.720
And attention really, so as I mentioned before, in classic sequence to sequence learning,

20:16.720 --> 20:22.240
you would try to have a model that compresses the input sequence into a fixed size vector.

20:22.240 --> 20:27.400
But that, if you can imagine for machine translation, if you have a very long sentence or

20:27.400 --> 20:33.040
even entire paragraph that you might want to translate to, it can be very challenging

20:33.040 --> 20:37.240
or it can place a lot of computation burden on the model to try to compress the entire

20:37.240 --> 20:41.640
meaning of this paragraph into a fixed size vector.

20:41.640 --> 20:46.560
And attention essentially allows the model to kind of overcome this bottleneck.

20:46.560 --> 20:51.640
So in addition to having this fixed size vector, at every step of generating an output

20:51.640 --> 20:57.080
symbol, the model can also look back at kind of the input sequence and at the different

20:57.080 --> 20:59.160
hidden states of the input sequence.

20:59.160 --> 21:04.360
And essentially, it allows the model to, instead of having to compress everything into

21:04.360 --> 21:12.960
one state, essentially remember kind of the most similar past states, which then makes

21:12.960 --> 21:17.120
it easier for the model to generate the relevant output.

21:17.120 --> 21:22.400
And yeah, and attention also is really something that has turned out to be quite flexible.

21:22.400 --> 21:29.000
So that is really now used, I think in most sequences to sequence learning tasks as well,

21:29.000 --> 21:35.880
that has also been used in, for instance, in image captioning, so you could imagine that

21:35.880 --> 21:42.280
instead of kind of paying attention to different parts of your input sequence, when you just

21:42.280 --> 21:47.640
try to generate a caption or individual words with some image, you can also pay particular

21:47.640 --> 21:52.960
attention to certain parts of that image that might be relevant for generating that next

21:52.960 --> 21:53.960
word.

21:53.960 --> 22:00.640
Or yeah, generally so I think that now this is really common, the attention because this

22:00.640 --> 22:08.880
idea of just computing kind of a weight or kind of a weighted average of your input sequence

22:08.880 --> 22:11.840
or of certain states based on the similarity.

22:11.840 --> 22:17.960
So the doctorate to your current state really has allowed kind of models that can access

22:17.960 --> 22:25.360
memory based on similarity to kind of current state, or also these recent, basically kind

22:25.360 --> 22:30.960
of state of the app models for machine translation, which is a transformer, which essentially

22:30.960 --> 22:37.920
uses self attention, so is the attention not applied to previous sequence, but to the sequence

22:37.920 --> 22:44.120
itself to essentially allow the model to kind of look at and consider the surrounding

22:44.120 --> 22:50.080
words and the surrounding context to improve the representation of the current word.

22:50.080 --> 22:55.360
And by using multiple layers, this can be done multiple times so that you can have a kind

22:55.360 --> 23:01.160
of a more contextually sensitive and a hopefully more expressive or better representation of

23:01.160 --> 23:02.880
each word in your sentence.

23:02.880 --> 23:09.800
Is this related at all to the representations of networks like wavenet where you've got

23:09.800 --> 23:17.920
like these hierarchical layers that are folding upwards, and does that kind of relate to

23:17.920 --> 23:19.280
wavenet, for example?

23:19.280 --> 23:26.040
Yeah, so basically in kind of these conditional sequence models, you get, you can model like

23:26.040 --> 23:31.480
the kind of longer dependencies between different words, but essentially stacking these conditional

23:31.480 --> 23:33.080
layers on top of each other.

23:33.080 --> 23:41.440
So as you go higher, each word will be able to kind of access or consider kind of words

23:41.440 --> 23:47.520
that have become more, that have become, come earlier in the sentence.

23:47.520 --> 23:48.520
Yeah.

23:48.520 --> 23:54.760
So with just a few layers, if you use stylish convolutions, you can have more, you can,

23:54.760 --> 23:59.440
you're able to incorporate almost the entire sequence length.

23:59.440 --> 24:04.400
And with attention, essentially you can do that instead, you don't need to stack layers

24:04.400 --> 24:09.640
at all because the attention mask or this attention operation allows you to directly look

24:09.640 --> 24:12.640
at all of the words in the sentence.

24:12.640 --> 24:18.920
So essentially you kind of have this, you have like a global context, essentially every

24:18.920 --> 24:24.440
layer, whereas in this conditional one, you have a local context, which can very quickly

24:24.440 --> 24:27.360
scale to a global context.

24:27.360 --> 24:32.520
So it's an attention layer intuitively kind of makes it easier to model this global context,

24:32.520 --> 24:37.280
but on the other hand, you miss out on some of this local information again, because

24:37.280 --> 24:46.120
your mask or your attention operation is kind of irrespective of the position of the words.

24:46.120 --> 24:51.840
So because of that, people typically use kind of an embedding of the position or some

24:51.840 --> 24:56.520
function that indicates where the word in the sentence is situated to be able to kind

24:56.520 --> 24:58.960
of model this local context again.

24:58.960 --> 25:04.440
When you describe attention, you're talking about this attention mask, which essentially

25:04.440 --> 25:08.760
kind of weights where you're looking at in the input vectors, is that the right way to

25:08.760 --> 25:10.280
think about it?

25:10.280 --> 25:11.280
Exactly.

25:11.280 --> 25:16.040
So you generally have some, some attention weights that you, that you compute, and then

25:16.040 --> 25:17.360
you do softmax over that.

25:17.360 --> 25:23.440
So in the end, you get some, you essentially get kind of a probability distribution that

25:23.440 --> 25:30.160
tells you how relevant each of the words is or how similar they are to your current input.

25:30.160 --> 25:35.400
And then, and then in attention, the way these attention weights are computed that can

25:35.400 --> 25:39.720
differ a bit, and that usually includes some number of learned parameters so that model

25:39.720 --> 25:47.520
can kind of learn to pay attention or with regard to different aspects potentially.

25:47.520 --> 25:52.960
And so that the next milestone on your list are memory-based networks, and that's not

25:52.960 --> 25:55.760
a term that I've heard used.

25:55.760 --> 25:57.280
What is that referring to?

25:57.280 --> 25:58.280
Yeah.

25:58.280 --> 26:03.840
So we use that to refer to basically kind of a variety of different methods that essentially

26:03.840 --> 26:06.880
have some sort of memory mechanism, yeah.

26:06.880 --> 26:14.920
So essentially these methods are kind of very similar to attention in a way in that you

26:14.920 --> 26:20.640
can kind of see attention as essentially using the past instance of your model essentially

26:20.640 --> 26:22.760
as a memory or like a lookup table.

26:22.760 --> 26:27.480
So by using attention in a model essentially has like a memory that is the size of the

26:27.480 --> 26:29.600
input sequence.

26:29.600 --> 26:37.600
And these other methods are similar in a way, but some of them have kind of in addition

26:37.600 --> 26:43.800
a kind of a not only that you're able to read from this memory, but you can also write

26:43.800 --> 26:45.760
back at the memory.

26:45.760 --> 26:51.800
So probably I think deep-mind, they have, you know, researchers from deep-mind probably

26:51.800 --> 26:57.480
have produced most of these models that fall under this category with some coming from

26:57.480 --> 26:59.000
fair as well.

26:59.000 --> 27:04.600
And yeah, probably one of the most common ones is kind of this neural touring machine,

27:04.600 --> 27:10.000
which essentially which essentially tries to mimic kind of the classic touring machine,

27:10.000 --> 27:15.040
which has like this tape where you can read and write to and which essentially tries to

27:15.040 --> 27:21.280
use this concept and implement that in a neural network by having a memory where you

27:21.280 --> 27:27.240
can basically read and write to as well, but with a mechanism that is very similar to kind

27:27.240 --> 27:32.360
of attention that you can basically access content based on this on similarity, but you

27:32.360 --> 27:39.960
can also have some location based addressing in that you kind of know, okay, you've stored

27:39.960 --> 27:44.440
this information in a certain part of your memory, so you can add access that based

27:44.440 --> 27:47.600
on location as well.

27:47.600 --> 27:54.400
And these types of models really are kind of, I've found it kind of useful to include

27:54.400 --> 28:00.920
this, this particular category of models, because I think this, so firstly because this

28:00.920 --> 28:06.600
concept of memory is kind of closely tied to attention, but also because I think going

28:06.600 --> 28:12.840
forward having some sort of memory or some sort of kind of additional capacity of the

28:12.840 --> 28:19.840
model to access additional information in some form or another, so for instance having

28:19.840 --> 28:25.560
access in some way to a knowledge base that you can, that you can kind of query from

28:25.560 --> 28:34.400
or write to as well, or having some kind of more expressive way of storing, remembering

28:34.400 --> 28:40.000
like reading, storing facts that you've read about and then recalling them.

28:40.000 --> 28:45.960
I think going forward and really to really think we can probably talk about that later

28:45.960 --> 28:52.360
in more depth as well, but really to kind of have models that can kind of reason and

28:52.360 --> 28:59.040
have better reason abilities, we need some sort of memory of something, some mechanism

28:59.040 --> 29:01.360
that comes at least close to that.

29:01.360 --> 29:09.400
It does seem a bit like the previous milestones that you've mentioned, the impacts of those

29:09.400 --> 29:16.160
are fairly clear with these memory-based networks, it's a bit more speculative, do you

29:16.160 --> 29:17.160
feel that way?

29:17.160 --> 29:23.240
Yeah, I definitely agree with that, so most of these methods have either been evaluated

29:23.240 --> 29:31.040
on either kind of synthetic tasks, like counting, sorting, things like that, or on, so kind

29:31.040 --> 29:39.120
of the baby dataset from Facebook AI research, which is essentially a synthetically

29:39.120 --> 29:45.240
generated dataset for reading comprehension, so essentially they generate kind of stories

29:45.240 --> 29:51.880
which consist of different sentences, which involve multiple actors and kind of objects

29:51.880 --> 29:56.960
as people moving a ball or bringing a ball to different rooms in the end, the question

29:56.960 --> 29:59.280
would be, okay, where's the ball?

29:59.280 --> 30:06.120
And so most of these methods have been incorporated on this dataset, which is quite synthetic

30:06.120 --> 30:12.240
in the way it was created, so in practice people haven't really seen them scale too well

30:12.240 --> 30:16.920
to kind of more realistic datasets, yeah, so I think it's really safe to say that so far

30:16.920 --> 30:21.920
kind of the impact was probably limited, but I think kind of this, yeah, this mechanism

30:21.920 --> 30:24.720
I think is promising for the future.

30:24.720 --> 30:30.560
And so that brings us to 2018, the last milestone on your list, which is pre-trained language

30:30.560 --> 30:36.440
models, and more generally the idea of being able to apply transfer learning, which we've

30:36.440 --> 30:43.400
seen a ton of success with, and the image domain to NLP.

30:43.400 --> 30:48.280
And you've done quite a bit in that space, you know, tell us about what you've seen in

30:48.280 --> 30:54.760
that space that kind of inspired you to start working in that area.

30:54.760 --> 31:00.680
Yeah, so for me personally, so maybe I came on that from two directions.

31:00.680 --> 31:06.080
So first in my personal research, as I've worked on different tasks and kind of developing

31:06.080 --> 31:11.600
different, like domain annotation, multi-tasking methods for different tasks, that were like

31:11.600 --> 31:16.040
usually task specific and try to incorporate some features of the target task, kind of

31:16.040 --> 31:20.480
the logical or kind of the natural progression in that was really trying to work on kind of

31:20.480 --> 31:25.760
this more general inductive transfer learning setting, essentially, where you try to use some

31:25.760 --> 31:31.360
sort of pre-trained knowledge and generalize or use that to do better on an arbitrary target

31:31.360 --> 31:32.680
task, essentially.

31:32.680 --> 31:39.400
So in that context, you really want to have some pre-training task or some existing information

31:39.400 --> 31:45.360
that is as general as possible, so that it will be likely useful for kind of any number

31:45.360 --> 31:47.640
of target task, essentially.

31:47.640 --> 31:52.920
And then in the second direction, I've also been really following a lot of the progress

31:52.920 --> 31:55.360
made in computer vision.

31:55.360 --> 32:05.120
And just really been fascinated or I found it really cool to see that how transfer learning

32:05.120 --> 32:12.480
was really kind of commonplace there and we used a lot in practice by practitioners in

32:12.480 --> 32:22.320
computer vision that was really very easy to build models or to just collect a few hundred

32:22.320 --> 32:26.840
images for different classes that you care about, use a pre-trained model, find you those

32:26.840 --> 32:29.920
on those images and you could really already get reasonable performance.

32:29.920 --> 32:34.840
And I found that really kind of a factor that I think made it easy for a lot of people

32:34.840 --> 32:38.720
to just have an experiment or develop their own computer vision models.

32:38.720 --> 32:44.520
And I really wanted to have something or I really found thought it would be very useful

32:44.520 --> 32:50.440
to have something similar for a natural language as well, where you can similarly create

32:50.440 --> 32:56.920
collective view, 100 label examples and train your own models for your own task that

32:56.920 --> 32:58.240
you care about.

32:58.240 --> 33:05.040
And kind of in thinking about that, essentially I started kind of talking with Jeremy

33:05.040 --> 33:09.680
Howard from FASDI as well, which been thinking along similar directions.

33:09.680 --> 33:16.880
And concurrently there's been some related work coming out from the Institute for Artificial

33:16.880 --> 33:18.600
Intelligence as well.

33:18.600 --> 33:25.640
And so it really started to crystallize that kind of a like the ideal or useful pre-trained

33:25.640 --> 33:31.720
task for that would really be language modeling because that's already been used as a kind

33:31.720 --> 33:38.720
of a successful variant with Goetheveg, because all Goetheveg does essentially kind of

33:38.720 --> 33:43.400
a variation of language modeling, but instead of using that just to initialize the first

33:43.400 --> 33:48.520
layer of our models with Goetheveg meetings, why don't we just pre-trained a language

33:48.520 --> 33:53.920
model and use that to initialize the remaining parameters of our models.

33:53.920 --> 33:59.440
And that really kind of was, yeah, like this overall direction wasn't really like, wasn't

33:59.440 --> 34:05.280
completely new, so in 2015 there was like the first paper which actually used language

34:05.280 --> 34:08.920
models and then fine-tuned those on the target task.

34:08.920 --> 34:13.400
And we essentially kind of took that a step further, added like a step where we pre-trained

34:13.400 --> 34:20.520
that on a general domain purpose and then proposed some other techniques to improve the fine-tuning

34:20.520 --> 34:26.760
performance or to kind of to retain as much information as possible doing this fine-tuning

34:26.760 --> 34:27.760
process.

34:27.760 --> 34:31.320
I guess when we're talking about transfer learning, there's two pieces, one is being

34:31.320 --> 34:39.240
able to use models that are created by third parties, presumably on other data sets and

34:39.240 --> 34:44.760
the other element is disability to kind of fine-tune it to meet your specific needs.

34:44.760 --> 34:50.800
With kind of that in mind, like do you consider word embeddings to be transfer learning,

34:50.800 --> 34:54.200
you know, certainly we share like glove vectors and things like that, they definitely meet

34:54.200 --> 35:00.120
that first criteria, but maybe not the second, is that the key distinction that you make?

35:00.120 --> 35:05.840
So I definitely think which using like pre-trained vulnerabilities is one form of transfer learning.

35:05.840 --> 35:13.800
So it's one way or kind of a very simple way to use existing pre-trained knowledge.

35:13.800 --> 35:19.880
And kind of using pre-trained language models in the asset is really kind of an additional

35:19.880 --> 35:28.800
step or you basically use kind of that knowledge and for additional input in your model either

35:28.800 --> 35:34.040
to influence more and more layers or you use kind of these richer, these embeddings from

35:34.040 --> 35:40.840
language models which just capture richer relations than you could capture with word embeddings.

35:40.840 --> 35:45.600
So both are kind of forms of transfer learning, but I think using language models scores kind

35:45.600 --> 35:48.920
of a step further than just using word embeddings.

35:48.920 --> 35:53.880
When you say language models, what specifically are you referring to?

35:53.880 --> 35:58.800
When I think of language models, I think of things like LDA, is that what you're referring

35:58.800 --> 36:01.840
to or are you speaking about them more broadly?

36:01.840 --> 36:07.600
So LDA, so latent, usually allocation is an example of topic modeling, so topic modeling

36:07.600 --> 36:13.160
is typically just used for like the expiration where you have like a corpus and you want

36:13.160 --> 36:17.360
to find out what people are talking about in there, so that generally gives you like

36:17.360 --> 36:20.040
a list of topics.

36:20.040 --> 36:23.600
And so language modeling when people talk about language modeling is really, so language

36:23.600 --> 36:29.400
modeling really is the task of you have a sequence of words like a sentence and you want

36:29.400 --> 36:31.960
to predict the next word in that sequence.

36:31.960 --> 36:40.200
So it's a very clearly defined task because you only need kind of the raw text data essentially.

36:40.200 --> 36:43.840
So given the text you can always see, you always know what the next word is that you want

36:43.840 --> 36:48.840
to predict, so you can very easily train this kind of model on a large amount of data.

36:48.840 --> 36:54.280
The typical approach to solving this kind of task would be like an LSTM.

36:54.280 --> 37:00.200
Yeah, so the typical model really this days, so the more traditional model, so stepping

37:00.200 --> 37:05.320
back a bit, the more traditional model would be kind of using basically using Ngrams,

37:05.320 --> 37:15.400
so using kind of different windows of words and then computing the next word or predicting

37:15.400 --> 37:22.640
the next word based on how what words you've seen co-cur with your existing Ngrams essentially.

37:22.640 --> 37:28.480
And then these days, yeah, you have kind of a typical language model would be in LSTM,

37:28.480 --> 37:31.800
maybe with multiple layers.

37:31.800 --> 37:36.040
And yeah, so that would kind of be the most typical one, people have used other models as

37:36.040 --> 37:42.040
well using memory or kind of other variations on this classic LSTM, but recently actually

37:42.040 --> 37:47.120
people showed that if you're just a bit more careful about tuning the hyper parameters

37:47.120 --> 37:52.960
of your LSTM, you can actually get very good performance just using a very classic LSTM

37:52.960 --> 37:54.160
for language modeling.

37:54.160 --> 38:01.760
And so the method that you came up with with Jeremy is called ULM Fit, tell us a little

38:01.760 --> 38:08.240
bit about how it works and the approach you took there.

38:08.240 --> 38:14.760
Sure, so yeah, so our intuition was really with that method that instead of initializing

38:14.760 --> 38:18.400
only the first layer of the model with word embeddings, we really want to have something

38:18.400 --> 38:23.760
with which we can initialize our entire model so that we in the end can take our pre-tuned

38:23.760 --> 38:28.600
language model and we just need to put a new layer, like a new layer for the classification

38:28.600 --> 38:30.600
task on top.

38:30.600 --> 38:36.840
And essentially our kind of method repose here consists of three steps, so we first just

38:36.840 --> 38:42.400
train kind of a classic LSTM language model on a large general domain copus, so we really

38:42.400 --> 38:48.040
want to have a kind of a copus that can capture very general domain knowledge.

38:48.040 --> 38:53.720
And for that, we just use kind of a copus subset of Wikipedia that is quite commonly used

38:53.720 --> 38:57.880
for language model evaluation and training as well.

38:57.880 --> 39:02.640
And then in the second step, you want to get this language model to kind of learn about

39:02.640 --> 39:07.920
some of the characteristics of the data or of the copus you actually care about.

39:07.920 --> 39:12.840
So in the second step, we then find you in this language model, so still the same language

39:12.840 --> 39:18.760
model, only we can just continue training it on data of our new classification task.

39:18.760 --> 39:23.840
So for instance, if you want to do a movie review classification based on positive or negative

39:23.840 --> 39:31.640
reviews, we would just fine tune that and continue training it on movie reviews of that domain.

39:31.640 --> 39:39.840
And for that step, essentially, we realize that we don't really want to train the model,

39:39.840 --> 39:43.600
like use too high of learning rate or you want to kind of be careful in how you train

39:43.600 --> 39:48.360
the model so that you don't actually accidentally override the information that your general

39:48.360 --> 39:51.360
language model has already captured before.

39:51.360 --> 39:55.960
So we essentially proposed kind of two different techniques, certain learning rate schedule

39:55.960 --> 40:02.280
and using different learning rates for different layers, essentially for the model to allow to

40:02.280 --> 40:06.880
retain as much of the general information from the general language model as possible.

40:06.880 --> 40:13.840
And kind of only adapt the higher layers of the model for the current, the new domain.

40:13.840 --> 40:19.200
Because our intuition was with that, that from computer vision, people have shown that

40:19.200 --> 40:25.120
essentially the low layers of the model really capture the most general information.

40:25.120 --> 40:31.080
So in the lowest layers, you really have information about the edges and kind of very general

40:31.080 --> 40:32.080
features.

40:32.080 --> 40:36.000
And as you go higher, the information gets more specific to the task for image net you

40:36.000 --> 40:40.640
would have the higher layers really responding to particular parts of dogs or dog noses

40:40.640 --> 40:45.920
even so the information as you go higher in the model gets more task specific.

40:45.920 --> 40:50.160
And in a P people actually now recently have started to show that there's like a similar

40:50.160 --> 40:54.600
hierarchy of information in your model that was trained in text.

40:54.600 --> 40:58.440
So because of that, our intuition was really that we want to retain as much of the information

40:58.440 --> 41:02.320
in the lower layers as possible and then use higher learning rates for the higher layers

41:02.320 --> 41:06.200
to adapt them to a large extent.

41:06.200 --> 41:11.040
And then in the final step, we essentially remove the softmax layer that was used for the

41:11.040 --> 41:17.200
language modeling objective and replace that with layer, so just softmax cross entropy

41:17.200 --> 41:22.400
layer that just tries to predict the target classes for our task.

41:22.400 --> 41:28.720
And we just train that new layer on top of together with the remaining language model.

41:28.720 --> 41:33.120
And here in addition to the two previous techniques, we also propose to make it a bit easier

41:33.120 --> 41:38.560
for the model to adapt the top layer to the, which is randomly initialized to the other

41:38.560 --> 41:41.080
parameters which have already been trained.

41:41.080 --> 41:44.800
So we initially only train the top layer and freeze the remaining layers.

41:44.800 --> 41:50.760
And then at the second epoch, I'm freeze another layer from the top down to make it kind

41:50.760 --> 41:56.000
of easier for the model to adapt the new parameters to the existing ones that have already been

41:56.000 --> 41:57.000
trained.

41:57.000 --> 42:06.120
There's a lot there. So first, let's maybe start by noting that the method is specifically

42:06.120 --> 42:12.280
kind of targeting this classification type of a task as opposed to predicting the next

42:12.280 --> 42:15.320
word or a generation type of task.

42:15.320 --> 42:18.760
Where do you see those classification tasks come up?

42:18.760 --> 42:19.760
Yes.

42:19.760 --> 42:25.960
So we decided to initially focus on classification because we thought that was kind of the most

42:25.960 --> 42:29.680
or those had like the most real world applications.

42:29.680 --> 42:30.680
So yeah.

42:30.680 --> 42:38.680
So from, yeah, kind of a lot of applications from, yeah, classifying chat logs, customer

42:38.680 --> 42:45.720
requests, requests in different categories routing those to different relevant entities

42:45.720 --> 42:52.520
in legal classifying different legal documents, depending on different, yeah, different fields.

42:52.520 --> 42:59.440
Yeah. So I think classification is really something as soon as you have some sort of text

42:59.440 --> 43:03.600
that is generated and you want to have some information that you want to filter out that

43:03.600 --> 43:10.160
can be expressed as a classification task if you just are able just to define certain

43:10.160 --> 43:13.360
number of categories that you want to express.

43:13.360 --> 43:17.720
Or similarly, a lot of other tasks like even doing sequence labeling or so can also be

43:17.720 --> 43:24.160
expressed as classification. So we found that by focusing on classification first, we

43:24.160 --> 43:29.720
can really cover a lot of, a lot of the real world applications of NLP.

43:29.720 --> 43:35.400
And then you mentioned that recent research has shown that in neural networks that are

43:35.400 --> 43:40.080
kind of language models, there's a similar phenomenon to what we've seen in computer

43:40.080 --> 43:43.400
vision where the lower layers are more general.

43:43.400 --> 43:48.160
On the computer vision side, we've got some intuitive ways of explaining that and talking

43:48.160 --> 43:54.000
about low level features like edges and textures and things like that as being found in these

43:54.000 --> 44:01.240
lower layers. Is there a similar clean way to explain what we're seeing in the NLP side?

44:01.240 --> 44:06.440
Yeah. So there's not, so it's a bit, so in computer vision, it's always nice that

44:06.440 --> 44:10.920
you can actually get some sort of like intuitive feeling by visualizing that in NLP, it's not

44:10.920 --> 44:18.040
exactly as straightforward. But it's still somewhat intuitive. So so far, I think there's

44:18.040 --> 44:24.160
been kind of two ways in which people have shown that there's some hierarchy first when

44:24.160 --> 44:29.840
you do multi-task loading with different little p-task. So multi-task loading is when you

44:29.840 --> 44:34.560
share, when you have one model that performs multiple tasks at the same time. And then

44:34.560 --> 44:41.120
if you have your model, if you train a model with some kind of higher level or more semantic

44:41.120 --> 44:47.560
tasks, like named entry recognition, and if you train that together with another task,

44:47.560 --> 44:52.400
it's more syntactic. So kind of a more lower level linguistically, like a part of speech

44:52.400 --> 44:57.720
tagging, then people have shown that actually information that is relevant for the part

44:57.720 --> 45:01.520
of speech tagging tasks, so information that kind of maximizes the performance in that

45:01.520 --> 45:06.600
task is actually captured at lower level lower layers of model, whereas for named entry

45:06.600 --> 45:12.680
recognition, the information would be contained on a higher level layer. And then more recently

45:12.680 --> 45:21.400
in this kind of in some papers from AI2, where they had so in their L-mold paper and then

45:21.400 --> 45:26.800
in the follow-up paper as well, they showed that in having these embeddings that were

45:26.800 --> 45:35.360
pre-trained or trained in a language model, if you use just, if you use lower layer lower

45:35.360 --> 45:40.520
level layers of your model, then those actually give you the best performance for lower level

45:40.520 --> 45:48.280
tasks, like part of speech tagging, whereas for higher level tasks, the most, the best information

45:48.280 --> 45:53.800
is contained in higher layers of model. So, so far, yeah, so really the best way to look

45:53.800 --> 45:58.680
at kind of a downstream task that encodes some sort of some level of hierarchy, and then

45:58.680 --> 46:04.480
basically generalize from that. So, yeah, so, so far, we haven't really, but there has

46:04.480 --> 46:09.880
not been kind of more intuitive or like a more easily accessible way by just defining

46:09.880 --> 46:14.040
a task that kind of encodes some assumption or something that you want to measure and

46:14.040 --> 46:17.880
then measure the performance of different layers of the model on that task.

46:17.880 --> 46:26.280
And then you mentioned that the ULMFIT method includes some specific guidance around like

46:26.280 --> 46:30.840
setting your learning rates and learning rate scheduling and things like this. On the

46:30.840 --> 46:38.600
computer vision side, these types of things are often done very iteratively via experimentation

46:38.600 --> 46:44.520
to determine, you know, what the right learning rates are and how to schedule your learning

46:44.520 --> 46:50.440
rates or apply learning rates to different layers is what you're saying here that you can

46:50.440 --> 46:55.320
be more prescriptive about it because of some characteristic of the problem or just that

46:55.320 --> 46:58.840
you should take that kind of an approach when training these models.

46:58.840 --> 47:04.520
Um, yeah, so we try to come up with a, so I mean, the way we arrived at these methods,

47:04.520 --> 47:09.400
also, and there's some explanation and trial and error on kind of our validation data as well,

47:09.400 --> 47:15.640
um, but recently or kind of the parameters we arrived at, we found them to perform well on

47:16.200 --> 47:21.720
different or kind of a variety of text classification problems. So it's really kind of more of

47:21.720 --> 47:27.960
the guidance of these are like good rules of thumb or like a good range of parameters that

47:27.960 --> 47:33.880
give, um, that give good results in general, but then obviously for, for particular domains or

47:33.880 --> 47:39.240
particular tasks, it might still be useful to do some, um, some fine-tuning of these parameters or

47:39.240 --> 47:44.360
to slightly change them and see, um, particularly playing around with like the learning rate,

47:44.360 --> 47:48.840
and, um, kind of the number of epochs you train your model, um, can still give you boost in

47:48.840 --> 47:55.960
performance. And so what kind of results have you seen with this approach? Um, so, so with this,

47:55.960 --> 48:00.200
um, so with our approach that we proposed, we essentially were able to outperform the state of

48:00.200 --> 48:06.520
the art on, um, kind of the number of YT studied text classification datasets. And that was, um,

48:06.520 --> 48:11.880
particularly kind of exciting encouraging for us, um, because on a lot of these datasets, um,

48:12.600 --> 48:18.680
some of the architectures were really had either a lot of feature or architecture engineering,

48:18.680 --> 48:25.240
whereas we, um, really used a very simple, um, uh, LSM with just different numbers of dropout,

48:25.240 --> 48:31.320
and, um, only this, um, kind of pre-training step, um, essentially. Um, so it was really encouraging

48:31.320 --> 48:36.200
that, um, yeah, this very simple model was really able to outperform the state of the art on,

48:36.200 --> 48:42.280
on a variety of benchmarks. Um, and then I think to me personally, uh, very exciting was just to see

48:42.280 --> 48:48.120
that this type of approach, um, even trained on, um, limited number of examples. So we basically did

48:48.120 --> 48:56.040
some inflation studies, so we trained it on, um, smaller training exercises, going from 100 to, um,

48:56.040 --> 49:01.960
yeah, two in different steps to 1,000, 10,000, uh, number of examples. And we really saw that just

49:01.960 --> 49:07.080
by virtue of using this pre-trained, um, information and pre-training language model,

49:07.080 --> 49:12.760
we were actually really able to, um, outperform, kind of, training a model from scratch on the same

49:12.760 --> 49:18.440
number of examples by an order of magnitudes, um, or we were able to reach the same performance as a

49:18.440 --> 49:23.560
model that was trained from scratch on kind of an order of magnitude more data. And I think this,

49:23.560 --> 49:29.160
this particular finding that's something that, um, not only we, but, um, kind of other researchers

49:29.160 --> 49:34.040
working in one of a similar direction of using language modeling have found. And, uh, yeah, for,

49:34.040 --> 49:38.600
for me personally, I think that's very encouraging, um, because I think this will really help to unlock,

49:38.600 --> 49:44.840
um, a lot of the kind of potential for NLP and just make it easier for people to use it on, on their

49:44.840 --> 49:49.720
own data sets and just should make it easier for people to, to just collect a small number of

49:49.720 --> 49:54.920
examples and then train and apply these kind of models to their data. Are there any qualitative

49:54.920 --> 50:01.640
comparisons you can make between transfer learning and computer vision and transfer learning

50:01.640 --> 50:08.920
in NLP based on this approach? So, um, so I think maybe one, one comparison is that what we

50:08.920 --> 50:13.000
initially tried and what is, um, kind of commonly done computer vision is that you only train,

50:13.000 --> 50:18.040
kind of, freeze your, um, internet work and you only train like the, the top mostly in front

50:18.040 --> 50:23.240
scratch or you're only on trees, like a couple of the, the top layers of the model, um, but really for

50:23.240 --> 50:30.520
us in, um, at least at the moment, because the models people are using right now are still

50:30.520 --> 50:35.400
quite a bit more shallow than, um, kind of typical models like resonant or dense net that you

50:35.400 --> 50:40.600
would use for transfer learning computer vision. Um, so we really still found it useful to kind of

50:40.600 --> 50:47.400
train, um, the entire model or to fine tune the entire model. At the same time, we've seen or in the

50:47.400 --> 50:53.480
community, um, kind of one of the most promising approaches is, um, this Elmo approach from AI2

50:53.480 --> 50:57.960
and they actually use our, take kind of an orthogonal approach where they don't fine tune the

50:57.960 --> 51:02.920
model, but use the embeddings from the language model, um, kind of as fixed features in a,

51:02.920 --> 51:07.560
in a, um, kind of in a separate model that is still trained from scratch. So you kind of have your

51:07.560 --> 51:12.280
existing, um, architecture and you just add the embedding that you get from your language model

51:12.280 --> 51:19.640
for each word as an additional feature, um, as input, um, and they, they, um, yeah, we're able to

51:19.640 --> 51:24.600
achieve kind of very good and serial fair performance on, um, a large variety of tasks as well.

51:24.600 --> 51:31.480
And, um, kind of reasonably comparing, so in some ongoing work comparing against the

51:31.480 --> 51:38.520
approach, we actually find that, um, they're very kind of those, so our fine tuning approach versus

51:38.520 --> 51:43.400
just using the fixed features like an Elmo is quite, uh, actually is about like a similar performance.

51:43.400 --> 51:49.960
Um, whereas in computer vision, really the, um, kind of the prevalent or the, uh, current paradigm

51:49.960 --> 51:55.320
really seems that people are just, um, fine tuning these models, instead of using them as fixed features.

51:55.320 --> 52:01.560
So I think it's still, um, yeah, so I think we'll still, um, so that's an ongoing research direction

52:01.560 --> 52:07.000
essentially, um, to see what is kind of the best way going forward really to do this sort of fine

52:07.000 --> 52:13.000
tuning in LP. Do we want to use, uh, do we want to use fine tuning or do we want to use fixed features

52:13.000 --> 52:17.960
or maybe a combination of the two and what are actually, yeah, what are like the pros and cons of

52:17.960 --> 52:24.840
that? And in your work, did you train from scratch, your pre-trained language model that you then

52:24.840 --> 52:30.280
use later on to apply to different test code? Did you, uh, was that already available for you?

52:31.000 --> 52:37.240
Um, so we, we trained on language model from scratch. So we trained, uh, yeah, we, we tried

52:37.240 --> 52:41.640
or experimented with different ways to train the initial language model as well, um, yeah,

52:41.640 --> 52:45.480
just because we want to observe the effect of the data, um, on the language model.

52:45.480 --> 52:51.080
Um, but, yeah, there's like different types of, um, tasks like, um, so we trained on the

52:51.080 --> 52:56.360
subset of Wikipedia, but Elmo, for instance, was trained on kind of a larger, uh, news corpus.

52:56.920 --> 53:02.600
Um, and, um, they use, I think like a pre-trained language model that was available online,

53:02.600 --> 53:07.880
and more recently, there's been some work from, uh, OpenAI, where they also trained a language model

53:07.880 --> 53:13.240
that was based on this transformer architecture from machine translation on an even, uh, larger corpus.

53:13.240 --> 53:20.280
Um, so, and it's still not entirely clear how those different, what the actual impact or

53:20.760 --> 53:26.760
how much of a benefit it gives you when training on like a larger corpus. Um, I think one hypothesis

53:26.760 --> 53:32.600
might be that training on training a more expressive model on more data, as we've kind of seen all

53:32.600 --> 53:37.480
the history of deep learning that might work better in the long term, um, but we still need to kind

53:37.480 --> 53:43.240
of figure out what actually the, uh, the best ways to do that. And out of curiosity, what, uh, kind

53:43.240 --> 53:47.560
of order of magnitude were you experiencing in terms of training time for these models?

53:48.440 --> 53:54.120
Um, so in terms of training the initial, so training the first, uh, uh, language model on the

53:54.120 --> 54:00.760
large corpus, or fine tuning the language model, or, uh, well, both. Um, so yeah, so it kind of

54:00.760 --> 54:06.600
depends on your, um, like on the approach. So when we trained our language model, um, because the

54:06.600 --> 54:11.240
initial, like, the spring training set, because you have, um, a very, like, a lot of tokens,

54:11.800 --> 54:15.640
a large corpus that you train on, that's usually the most extensive step. So for us,

54:15.640 --> 54:21.560
in our early experiments, we trained that out for, like, 24 hours on one, uh, GPU, essentially,

54:21.560 --> 54:27.240
and then usually the fine tuning, um, and the final training set would be a lot faster, so maybe

54:27.240 --> 54:32.280
it would be like, uh, one hour, depending on the training set size, um, in the open AI paper,

54:32.280 --> 54:36.920
because they trained an even larger model on even more data, they trained for about a month.

54:37.880 --> 54:42.120
So it can really, if you scale that up, um, that can take a lot longer, but then fine tuning would

54:42.120 --> 54:47.160
probably, um, take similar long, maybe a bit longer, because, because model is steeper, um,

54:47.160 --> 54:52.440
and recently, kind of in the fast AI library, um, with some ongoing experience from computer vision,

54:53.560 --> 54:59.080
they've kind of integrated some methods where you can train, um, which are kind of, um,

54:59.080 --> 55:05.000
this, like, one technique called, uh, super convergence, that has shown, um, some good results for

55:05.000 --> 55:09.080
computer vision, where you essentially have a, uh, particular learning rate schedule, and you

55:09.080 --> 55:14.280
train a model with a very high learning rate, um, and in doing that, and if you're kind of careful

55:14.280 --> 55:22.200
about how you use the schedule, you can get to very high performance, uh, very fast, um, so using,

55:22.200 --> 55:26.920
like a technique like that might also, um, for language modeling, might also allow you to train your

55:26.920 --> 55:32.760
language models, um, yeah, significantly faster. With word embeddings, we've kind of, you know,

55:32.760 --> 55:39.000
those of matured and we've gotten to the point that there are, you know, multiple options for

55:39.640 --> 55:45.720
word embedding vectors that you can more or less use interchangeably. Is that the case for

55:45.720 --> 55:52.760
these pre-trained language models? Like, can you use either the open AI language models or what

55:52.760 --> 55:58.600
you've published kind of interchangeably in building out models and just experiment to see what

55:58.600 --> 56:06.040
works best for you? Or are they, are the, the language models more, you know, intertwined to

56:07.000 --> 56:14.360
how they are intended to be used? Um, yeah, so in a sense, you can still, um, you can basically kind

56:14.360 --> 56:22.440
of use them interchangeably at this point, um, although so we've recently seen, or there's been

56:22.440 --> 56:28.120
a recent paper which showed, okay, there's maybe some, um, differences in the representation that

56:28.120 --> 56:34.520
these models learn so that on, on some tasks, actually having, um, using, like an LSTM in contrast

56:34.520 --> 56:39.080
with a transformer as language model might actually give you better performance, but then you have

56:39.080 --> 56:45.080
the transformer which might be maybe more efficient than the LSTM because it doesn't require this

56:45.080 --> 56:52.440
temporal dependency, but this is again, like still very preliminary work, um, so I don't think, yeah,

56:52.440 --> 56:57.640
so I don't think at this point we really have a great understanding yet for which type of tasks,

56:58.520 --> 57:05.160
these particular models work, um, work really well, um, but I think what you want, so as long as you

57:05.160 --> 57:10.840
have a model, like a language model that is very expressive, that achieves kind of a good, um,

57:10.840 --> 57:15.640
performance on your data, then I think generally you would probably expect to get similar

57:15.640 --> 57:20.440
performance, and then probably going forward, there might be some particular guidance on maybe

57:20.440 --> 57:25.560
if you have a sort of, like, reasoning class or, um, something which requires maybe more

57:25.560 --> 57:31.320
long term, um, dependencies, maybe you won't rather want to use a transformer, um, but that's

57:31.320 --> 57:36.040
still a bit early stage to give kind of these more, um, more concrete rules of them.

57:36.040 --> 57:43.640
If we cover it all, there is a cover on ULM fit, uh, and, you know, and or, you know, what's next in

57:43.640 --> 57:49.720
kind of that research direction? Um, yeah, so, so I think in that, in that research direction,

57:50.360 --> 57:56.840
I think there's, like, a lot of interesting directions, so first I don't think we've yet,

57:56.840 --> 58:01.560
kind of come to the ceiling of what we can achieve, um, using language models in NLP,

58:01.560 --> 58:07.000
so I think, so I personally think that, um, because of the results I've seen so far are really

58:07.000 --> 58:11.400
encouraging and really significant improvements of what we've seen before. Um, people are going to,

58:12.040 --> 58:15.880
um, start using more and more instead of using pre-treatment meetings, using, um,

58:15.880 --> 58:20.120
pre-treatment language models in their own applications. So I think these will really kind of

58:20.120 --> 58:26.280
become a, uh, a mainstay in NLP going forward, at least for the foreseeable future. Um,

58:26.280 --> 58:32.360
and in, in that, I think there's still a lot of questions on how you actually, um, how you can,

58:33.160 --> 58:37.160
kind of compress and capture as much information in your language model as before,

58:37.160 --> 58:41.720
as possible, what are like the best language models to use, how you can incorporate these in that.

58:41.720 --> 58:46.840
So just, um, kind of, uh, maximizing the benefit from using this language modeling task,

58:46.840 --> 58:53.320
I think is kind of a need to, like, need a direction that will still or should still give us some

58:53.320 --> 58:59.240
boost in some tasks. Um, and then probably in the, in the more long term, um, and, and then kind

58:59.240 --> 59:05.880
of maybe tied to the former, um, just understanding what actually, um, this or how far we can actually

59:05.880 --> 59:10.680
go using these language models, um, because language modeling might, um, while it's still, while it

59:10.680 --> 59:15.320
gives us a boost on some tasks, it's still a similar objective to what we've been doing with,

59:15.320 --> 59:21.000
where to back. And just kind of this idea of language modeling of just, um, predicting the next word

59:21.000 --> 59:27.640
based on its previous words, um, is while it should give you kind of some, um, to capture some

59:27.640 --> 59:33.880
relations in text, there's still a lot of things that it can't capture. So it's, um, yeah, for

59:33.880 --> 59:38.920
instance, it's really hard to capture, um, kind of more like real world knowledge, actually about

59:38.920 --> 59:45.880
how the, how the world works or how different, um, kind of entities or things in the, in the real

59:45.880 --> 59:50.920
world, um, kind of relate to each other. It's really hard, it's often not mentioned in text, and then

59:50.920 --> 59:56.040
only like, uh, very rarely. So it's really hard to capture something like that, which would be

59:56.040 --> 01:00:00.920
useful for us like quest answering or reading comprehension with just using language model. So I

01:00:00.920 --> 01:00:07.320
think one really interesting direction going forward is really how we can, um, incorporate, um,

01:00:07.320 --> 01:00:11.800
information that allows us to do better sort of reasoning and natural language understanding,

01:00:11.800 --> 01:00:17.640
either in order to augment our language model, or, um, as additional forms of pre-training maybe,

01:00:17.640 --> 01:00:23.960
or maybe additional forms of knowledge has, um, like in the memory from a knowledge base,

01:00:23.960 --> 01:00:30.440
or through some other module that we can then, um, use or leverage in our, uh, downstream tasks.

01:00:31.400 --> 01:00:37.720
Well, Sebastian, thanks so much for taking the time to chat about this. Uh, it's really interesting

01:00:37.720 --> 01:00:42.040
work you're doing, and I'm looking forward to kind of keeping up with how it evolves.

01:00:42.760 --> 01:00:45.000
Uh, cool and congrats. Yeah, thanks for having me. Thank you.

01:00:48.840 --> 01:00:53.960
All right, everyone, that's our show for today. For more information on Sebastian or any of the

01:00:53.960 --> 01:01:00.440
topics covered in this episode, visit twimmelai.com slash talk slash 195.

01:01:01.480 --> 01:01:06.280
If you're a fan of the show, and you haven't already done so, or you're a new listener, and you

01:01:06.280 --> 01:01:12.600
like what you hear, please head over to your Apple or Google podcast app, and leave us a five-star

01:01:12.600 --> 01:01:17.640
rating and review. Your reviews help inspire us to create more and better content, and they help

01:01:17.640 --> 01:01:23.320
new listeners find the show. Thanks again to our friends at IBM for their sponsorship of this

01:01:23.320 --> 01:01:31.480
episode. Be sure to check out IBM developer at IBM.biz slash MLAI podcast. As always,

01:01:31.480 --> 01:01:39.720
thanks so much for listening, and catch you next time.

