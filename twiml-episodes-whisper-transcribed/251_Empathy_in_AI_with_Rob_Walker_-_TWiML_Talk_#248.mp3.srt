1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,040
I'm your host Sam Charrington.

4
00:00:32,040 --> 00:00:37,880
Today we're joined by Rob Walker, Vice President of Decision Management at Pegasystems.

5
00:00:37,880 --> 00:00:43,760
Rob joined us back in episode 127 to discuss hyper-personalizing the customer experience

6
00:00:43,760 --> 00:00:49,400
and today he's back for a discussion about the role of empathy in AI systems.

7
00:00:49,400 --> 00:00:53,560
In our conversation we dig into the role that empathy plays in consumer facing human

8
00:00:53,560 --> 00:00:59,800
AI interactions, the differences between empathy and ethics and a few examples of the ways

9
00:00:59,800 --> 00:01:05,240
that empathy should be considered when building enterprise AI systems.

10
00:01:05,240 --> 00:01:06,240
So what do you think?

11
00:01:06,240 --> 00:01:09,320
Should empathy be a consideration in AI systems?

12
00:01:09,320 --> 00:01:14,200
And if so, do any examples jump out for you of where and how it should be applied?

13
00:01:14,200 --> 00:01:16,280
We'd love to hear your thoughts on the topic.

14
00:01:16,280 --> 00:01:20,820
Feel free to reach out with a tweet at Sam Charrington or leave a comment on the show notes

15
00:01:20,820 --> 00:01:22,960
paid with your thoughts.

16
00:01:22,960 --> 00:01:26,800
Before we jump in, I'd like to thank Rob and the folks at Pegas for sponsoring this

17
00:01:26,800 --> 00:01:27,800
episode.

18
00:01:27,800 --> 00:01:33,240
If the topic of AI empathy peaks your interest, I'd encourage you to join Rob and I at Pegasworld

19
00:01:33,240 --> 00:01:39,840
June 2nd through 5th in Las Vegas, where he'll be delivering a keynote on this very topic.

20
00:01:39,840 --> 00:01:44,160
At the event you'll also hear great stories of AI applied to the customer experience at

21
00:01:44,160 --> 00:01:49,240
real Pegasystems customers, and of course, I'll be there and speaking as well.

22
00:01:49,240 --> 00:01:54,980
To register, visit pegaworld.com and use the promo code Twimal19 when you sign up for

23
00:01:54,980 --> 00:01:56,680
$200 off.

24
00:01:56,680 --> 00:02:00,320
Again, that's Twimal19, it's as easy as that.

25
00:02:00,320 --> 00:02:02,040
Hope to see you there.

26
00:02:02,040 --> 00:02:04,880
And now on to the show.

27
00:02:04,880 --> 00:02:10,760
Alright everyone, I am on the line with Rob Walker.

28
00:02:10,760 --> 00:02:15,760
Rob is the Vice President of Decision Management and Analytics with Pegasystems.

29
00:02:15,760 --> 00:02:23,240
You may remember Rob's name from TwimalTalk number 127 on hyper-personalizing the customer

30
00:02:23,240 --> 00:02:25,520
experience with AI.

31
00:02:25,520 --> 00:02:28,280
Rob, welcome back to this week in Machine Learning and AI.

32
00:02:28,280 --> 00:02:31,280
Yeah, thanks Sam, glad to be back.

33
00:02:31,280 --> 00:02:33,520
Absolutely, glad to have you back.

34
00:02:33,520 --> 00:02:39,200
Looking forward to what I believe will be a really interesting conversation on the role

35
00:02:39,200 --> 00:02:42,400
of empathy in AI.

36
00:02:42,400 --> 00:02:47,920
For folks that want to learn more about you, I'll refer them back to the previous episode,

37
00:02:47,920 --> 00:02:51,880
but give us a quick overview of your focus at Pegasystems.

38
00:02:51,880 --> 00:02:53,800
Yes, happy to do that.

39
00:02:53,800 --> 00:03:02,400
So with the impact, I'm responsible for our AI space, but we really try, I mean, there's

40
00:03:02,400 --> 00:03:07,400
so much hype around AI and we don't do AI just for AI sake.

41
00:03:07,400 --> 00:03:15,040
We really try to focus on making AI work for typically pretty large enterprises and

42
00:03:15,040 --> 00:03:18,120
typically in the area of customer engagement.

43
00:03:18,120 --> 00:03:19,120
Right?

44
00:03:19,120 --> 00:03:24,000
So in the previous episode, we talked about hyper-personalization and hyper-personalization

45
00:03:24,000 --> 00:03:31,920
is really trying to be one-to-one conversations with customers for companies to do that.

46
00:03:31,920 --> 00:03:35,960
And that requires a lot of AI.

47
00:03:35,960 --> 00:03:41,080
It also requires lots of other things, but AI is an important aspect of that.

48
00:03:41,080 --> 00:03:46,960
And that's what I mostly worry about, and also sort of areas around AI.

49
00:03:46,960 --> 00:03:50,720
It's not just, hey, AI is cool, let's use that in customer engagement to make customer

50
00:03:50,720 --> 00:03:55,920
engagement better, but it's stuff like, can we trust it?

51
00:03:55,920 --> 00:04:01,360
Who in your organization should be responsible for it if it makes weird decisions, if there

52
00:04:01,360 --> 00:04:03,960
is a bias, those kind of things?

53
00:04:03,960 --> 00:04:07,240
So that's typically what I worry about.

54
00:04:07,240 --> 00:04:12,840
The concept that kept coming up in our last conversation, I think this is pretty central

55
00:04:12,840 --> 00:04:21,720
to the way you think about applying AI to optimizing customer experiences, this idea of helping

56
00:04:21,720 --> 00:04:28,440
your customers figure out the next best action to take with their customers, is that right?

57
00:04:28,440 --> 00:04:29,440
That is correct.

58
00:04:29,440 --> 00:04:30,440
Yes.

59
00:04:30,440 --> 00:04:37,680
So the companies we work with typically implement like this customer decision hub, right?

60
00:04:37,680 --> 00:04:44,080
And it's a centralized decision authority that across all the different channels that

61
00:04:44,080 --> 00:04:52,320
companies may have, figures out the next best action during conversations, right?

62
00:04:52,320 --> 00:04:53,840
So what should we say?

63
00:04:53,840 --> 00:04:56,000
What should we not say?

64
00:04:56,000 --> 00:05:03,760
What price should we mention if it's commercial, basically trying to have very reasonable

65
00:05:03,760 --> 00:05:09,880
conversations, but at the same time, because most of the companies we work with are not

66
00:05:09,880 --> 00:05:11,840
just charities, right?

67
00:05:11,840 --> 00:05:16,640
So at the same time, you need to sort of improve customer value, right?

68
00:05:16,640 --> 00:05:22,400
So the next best action, the best in next best action, is typically some metric about customer

69
00:05:22,400 --> 00:05:28,960
value and trying to improve that over time by doing the best thing possible to optimize

70
00:05:28,960 --> 00:05:29,960
that.

71
00:05:29,960 --> 00:05:37,760
So this concept of empathy in AI is something that you'll be speaking about at the next

72
00:05:37,760 --> 00:05:43,120
Pegaworld, an event that your company hosts annually.

73
00:05:43,120 --> 00:05:47,040
I attended the last one and I'll be attending the next one.

74
00:05:47,040 --> 00:05:54,600
How did this idea of empathy, introducing empathy into these kinds of transactions or customer

75
00:05:54,600 --> 00:05:56,400
experiences?

76
00:05:56,400 --> 00:05:58,120
Where did that come from?

77
00:05:58,120 --> 00:06:00,600
Well, so I've always been interested.

78
00:06:00,600 --> 00:06:06,880
So before I joined Pegaw at some point before that I was a scientist, right, in AI, I did

79
00:06:06,880 --> 00:06:15,160
my PhD in that area and I've always been interested in not just all the cool things AI can do

80
00:06:15,160 --> 00:06:21,040
around predicting customer behavior and things like that, but also potentially the not so

81
00:06:21,040 --> 00:06:22,200
cool things, right?

82
00:06:22,200 --> 00:06:24,600
So can you trust it?

83
00:06:24,600 --> 00:06:25,800
Is it transparent?

84
00:06:25,800 --> 00:06:27,560
Is it opaque?

85
00:06:27,560 --> 00:06:29,400
Is there a bias?

86
00:06:29,400 --> 00:06:31,000
Can it go rogue?

87
00:06:31,000 --> 00:06:32,840
Those kind of things.

88
00:06:32,840 --> 00:06:38,960
So over the last years, we've really tried to sort of guard the moral high ground, if you

89
00:06:38,960 --> 00:06:44,920
will, around AI and now just look at what it can, the value it can bring, but also

90
00:06:44,920 --> 00:06:50,480
mitigate the risk that you can have with AI.

91
00:06:50,480 --> 00:06:54,720
And following sort of that path, empathy was a very natural thing.

92
00:06:54,720 --> 00:07:01,480
I mean, you know, the bigger thing is morality, you know, the morality of AI and AI decisions,

93
00:07:01,480 --> 00:07:04,960
which is, you know, that's a big beast.

94
00:07:04,960 --> 00:07:10,800
And sort of more ethical behavior and empathy seemed to be something that was just about

95
00:07:10,800 --> 00:07:17,480
tangible enough to try to really put it into the, into the product and into the vision

96
00:07:17,480 --> 00:07:20,600
of best practice around using AI.

97
00:07:20,600 --> 00:07:26,600
So we've been spending quite some time thinking about that and how you can operationalize that

98
00:07:26,600 --> 00:07:27,600
kind of thing.

99
00:07:27,600 --> 00:07:35,880
Now, when you start talking about the morality of AI, certainly, and even to large degree

100
00:07:35,880 --> 00:07:41,400
empathy, start to, you know, the picture in my head starts to form around, you know, what

101
00:07:41,400 --> 00:07:47,040
some of us will call AGI, artificial general intelligence, you know, what we talk about

102
00:07:47,040 --> 00:07:50,360
more commonly is like sci-fi AI, right?

103
00:07:50,360 --> 00:07:52,000
Is that what we're talking about here?

104
00:07:52,000 --> 00:07:57,360
Or are we talking about something that, you know, how can you make this more concrete

105
00:07:57,360 --> 00:07:58,360
for us?

106
00:07:58,360 --> 00:08:02,200
Yeah, because, because I'm definitely not talking about that kind of thing.

107
00:08:02,200 --> 00:08:05,840
I mean, I mean, that's really, well, it's, it's, it's interesting.

108
00:08:05,840 --> 00:08:06,840
Right?

109
00:08:06,840 --> 00:08:12,880
I mean, I think everybody in AI is thinking about how that, how that, how that works.

110
00:08:12,880 --> 00:08:17,600
But I think just as a human species, if I just, you know, just even reading the news, you

111
00:08:17,600 --> 00:08:23,560
know, today, I think I'm, I'm not sure we have morality, you know, very, very, very,

112
00:08:23,560 --> 00:08:27,520
very clear for, for, for everyone, at least there is a lot of discussion about what would

113
00:08:27,520 --> 00:08:32,080
be a moral judgment and not a moral judgment to even expect that of AI.

114
00:08:32,080 --> 00:08:36,640
I think it's already a tough act.

115
00:08:36,640 --> 00:08:41,520
But I'm certainly not talking about it in that kind of realm quite yet, although it's a

116
00:08:41,520 --> 00:08:42,840
very interesting topic, right?

117
00:08:42,840 --> 00:08:45,840
I was just thinking about, you know, self-driving cars, right?

118
00:08:45,840 --> 00:08:51,840
And, and, and sort of the moral judgments, they may, one day, have to make, right?

119
00:08:51,840 --> 00:08:54,040
In extreme circumstances.

120
00:08:54,040 --> 00:09:01,080
But this is very much sort of a smaller subset of those challenges where we're talking about

121
00:09:01,080 --> 00:09:04,360
customer engagement and, and those kinds of things.

122
00:09:04,360 --> 00:09:12,160
And I think in that area, empathy really shows in, in, in, in, in, in, in, in pretty clear

123
00:09:12,160 --> 00:09:17,320
dimensions, like stuff like, is this, if we are talking to me as a company, and it's an

124
00:09:17,320 --> 00:09:21,280
AI driven conversation, is that, is that a relevant, right?

125
00:09:21,280 --> 00:09:25,960
Or are you wasting my time stuff like, is it, is it appropriate?

126
00:09:25,960 --> 00:09:26,960
Right?

127
00:09:26,960 --> 00:09:27,960
So you're talking to me about something.

128
00:09:27,960 --> 00:09:30,200
And it might be interesting, it may be interesting to me,

129
00:09:30,200 --> 00:09:33,480
but it may not be still, it may not be appropriate, right?

130
00:09:33,480 --> 00:09:39,840
Maybe you shouldn't be selling me a gun or a car or a credit

131
00:09:39,840 --> 00:09:44,600
card that I actually can't pay back when I'm in debt, right?

132
00:09:44,600 --> 00:09:46,800
So it's those kind of things.

133
00:09:46,800 --> 00:09:48,760
And is there mutual value?

134
00:09:48,760 --> 00:09:51,040
Are you talking to me for something that, you know,

135
00:09:51,040 --> 00:09:53,760
can we have a transaction that has a mutual value?

136
00:09:53,760 --> 00:09:56,600
Or is it just about the company?

137
00:09:56,600 --> 00:09:59,600
And I think if companies implement those kind of

138
00:09:59,600 --> 00:10:06,040
considerations well, I think that will do pretty well

139
00:10:06,040 --> 00:10:09,240
on an empathy scorecard for starters.

140
00:10:09,240 --> 00:10:10,680
Now that's interesting.

141
00:10:10,680 --> 00:10:14,280
And actually somewhat different from the,

142
00:10:14,280 --> 00:10:15,880
I don't know if it's different from the direction

143
00:10:15,880 --> 00:10:20,400
that I thought we were going to go here or that, you know,

144
00:10:20,400 --> 00:10:23,960
the picture that formed in my mind when we were talking

145
00:10:23,960 --> 00:10:30,200
earlier about empathy or if one is part of the other.

146
00:10:30,200 --> 00:10:32,960
But I'll tell you, I'll kind of recount the picture

147
00:10:32,960 --> 00:10:34,080
that I have in my money.

148
00:10:34,080 --> 00:10:37,960
Let me know where it fits into this world.

149
00:10:37,960 --> 00:10:42,560
You know, I was envisioning primarily the kinds of interactions

150
00:10:42,560 --> 00:10:46,680
that you might have via a chat bot or, you know,

151
00:10:46,680 --> 00:10:48,120
chat kind of interface.

152
00:10:48,120 --> 00:10:51,760
And you often have, or even, you know,

153
00:10:51,760 --> 00:10:56,560
the extent to which AI is driving a call,

154
00:10:56,560 --> 00:11:00,160
a call center agent and their responses

155
00:11:00,160 --> 00:11:04,280
because that's some of that, or an IVR.

156
00:11:04,280 --> 00:11:07,120
You know, some of that is starting to happen.

157
00:11:07,120 --> 00:11:12,720
But I was envisioning kind of this set of capability

158
00:11:12,720 --> 00:11:14,880
where, you know, maybe the, you know,

159
00:11:14,880 --> 00:11:17,080
whether the chat bot or the IVR, you know,

160
00:11:17,080 --> 00:11:18,480
IVR is a great example, right?

161
00:11:18,480 --> 00:11:23,280
It's like, IVR should be able to tell from my voice,

162
00:11:23,280 --> 00:11:26,240
or could tell from my voice that I'm getting frustrated

163
00:11:26,240 --> 00:11:29,120
navigating the 50 million, you know, menus

164
00:11:29,120 --> 00:11:31,200
and maybe escalate me, you know,

165
00:11:31,200 --> 00:11:32,680
a little bit more quickly to someone.

166
00:11:32,680 --> 00:11:35,840
Or, you know, you can imagine the same kind of thing

167
00:11:35,840 --> 00:11:39,920
happening in a chat interaction where, you know,

168
00:11:39,920 --> 00:11:43,720
I'm interacting with this virtual agent.

169
00:11:43,720 --> 00:11:45,840
It's not getting what I need to do

170
00:11:45,840 --> 00:11:49,920
or it's, you know, needing asking me to repeat myself,

171
00:11:49,920 --> 00:11:52,040
you know, multiple times.

172
00:11:52,040 --> 00:11:55,400
You know, there's a degree of empathy and all that

173
00:11:55,400 --> 00:11:58,240
where it's understanding my,

174
00:11:58,240 --> 00:12:00,080
I guess I'm kind of simplifying that

175
00:12:00,080 --> 00:12:01,880
as understanding my emotional state

176
00:12:01,880 --> 00:12:05,400
and using that as part of the decisioning

177
00:12:05,400 --> 00:12:08,920
around what the next best action to take is.

178
00:12:08,920 --> 00:12:09,760
Yeah.

179
00:12:09,760 --> 00:12:12,400
But it sounds like maybe that's a piece of what you're saying,

180
00:12:12,400 --> 00:12:14,880
but you're also talking about, you know,

181
00:12:14,880 --> 00:12:17,840
maybe about the broader AI ethics conversation,

182
00:12:17,840 --> 00:12:20,800
your example around, you know,

183
00:12:20,800 --> 00:12:25,120
should we offer the credit card to the person

184
00:12:25,120 --> 00:12:28,640
who can't afford it is one that kind of,

185
00:12:28,640 --> 00:12:31,240
you know, resonates and kind of drives me in that direction.

186
00:12:32,240 --> 00:12:35,400
Yeah. Now, I think, so I think the example that you gave,

187
00:12:35,400 --> 00:12:38,600
right, like is somebody getting frustrated

188
00:12:38,600 --> 00:12:42,360
and those kind of things I think are a very important part

189
00:12:42,360 --> 00:12:44,200
of empathy, but I think it's part

190
00:12:44,200 --> 00:12:46,640
of the delivery mechanism.

191
00:12:46,640 --> 00:12:49,640
So I think what we're trying to do is sort of take that

192
00:12:49,640 --> 00:12:52,680
in two different layers, if you will, right?

193
00:12:52,680 --> 00:12:54,560
One is when you're talking to someone

194
00:12:54,560 --> 00:12:56,160
and somebody is getting frustrated

195
00:12:56,160 --> 00:12:59,760
or if you do voice detection and or inflection

196
00:12:59,760 --> 00:13:03,760
and you sort of notices that somebody is getting upset,

197
00:13:05,200 --> 00:13:09,480
you may want to change that may influence the next best action

198
00:13:09,480 --> 00:13:11,160
as part of the context.

199
00:13:11,160 --> 00:13:14,040
So I would call that, although it's cool,

200
00:13:14,040 --> 00:13:17,240
more of the sort of the superficial way of empathy, right?

201
00:13:17,240 --> 00:13:19,520
It's trying to feel somebody's mood

202
00:13:19,520 --> 00:13:21,640
and use that as a context,

203
00:13:21,640 --> 00:13:24,400
but that can become from a lot of different senses.

204
00:13:24,400 --> 00:13:26,480
It could be, you know, as I said,

205
00:13:26,480 --> 00:13:28,160
the inflection of a voice,

206
00:13:28,160 --> 00:13:29,960
it could be when this is face to face

207
00:13:29,960 --> 00:13:31,440
or you're in front of a camera,

208
00:13:31,440 --> 00:13:34,720
it can be that, you know, people can sort of read their face

209
00:13:34,720 --> 00:13:37,320
or the system can read the face of the customer

210
00:13:37,320 --> 00:13:39,160
and see that's not going well.

211
00:13:39,160 --> 00:13:44,080
But that's not the same as the underlying level

212
00:13:44,080 --> 00:13:46,840
of making sure that the next best action

213
00:13:46,840 --> 00:13:50,840
that you are contemplating is one that is empathetic

214
00:13:50,840 --> 00:13:52,760
or even moral, right?

215
00:13:52,760 --> 00:13:56,040
So I see that as two different, different thing.

216
00:13:56,040 --> 00:13:58,040
I think people think about empathy a lot

217
00:13:58,040 --> 00:13:59,680
like you were just describing it like,

218
00:13:59,680 --> 00:14:02,400
hey, I see this is making you upset.

219
00:14:02,400 --> 00:14:03,400
So I need to, you know,

220
00:14:03,400 --> 00:14:06,200
hurry this along or ask you what's wrong

221
00:14:06,200 --> 00:14:08,680
and that's all cool, very human stuff.

222
00:14:08,680 --> 00:14:12,320
But it's on the delivery of a particular action.

223
00:14:12,320 --> 00:14:15,200
But determining what you're going to do,

224
00:14:15,200 --> 00:14:17,440
that also requires empathy

225
00:14:17,440 --> 00:14:19,680
and that's more along the line of is this,

226
00:14:19,680 --> 00:14:21,640
is this relevant, is this appropriate,

227
00:14:21,640 --> 00:14:23,520
is this suitable, does kind of things.

228
00:14:24,600 --> 00:14:28,760
And do you specifically use the word empathy

229
00:14:28,760 --> 00:14:31,080
to distinguish it from ethics

230
00:14:31,080 --> 00:14:35,560
or are those ideas different in your mind

231
00:14:35,560 --> 00:14:40,040
or are they, you know, just a different word in this case?

232
00:14:40,040 --> 00:14:42,280
Yeah, now I think, I think the,

233
00:14:42,280 --> 00:14:44,920
if I think about ethics, that sort of, you know,

234
00:14:44,920 --> 00:14:49,120
ethical behavior, I think empathy is basically

235
00:14:49,120 --> 00:14:53,080
the step where humans, for now, maybe AI at some point,

236
00:14:53,080 --> 00:14:55,240
will basically be able to, you know,

237
00:14:55,240 --> 00:14:58,560
please themself in someone else's place

238
00:14:58,560 --> 00:15:02,280
and say, hey, if this is happening to me, you know,

239
00:15:02,280 --> 00:15:05,320
is this going to make me, you know, happier?

240
00:15:05,320 --> 00:15:06,440
And things like that.

241
00:15:06,440 --> 00:15:10,880
So I think they are, in that sense, very, very related.

242
00:15:10,880 --> 00:15:13,720
So in that sense, ethics is kind of relevant

243
00:15:13,720 --> 00:15:18,360
to some broader set of, you know, societal norms,

244
00:15:18,360 --> 00:15:21,720
whereas empathy, we don't have to figure all that out.

245
00:15:21,720 --> 00:15:24,520
It's just about, you know, this particular customer,

246
00:15:24,520 --> 00:15:26,880
am I doing the right thing by this customer?

247
00:15:27,880 --> 00:15:29,600
Yes, am I doing the right thing?

248
00:15:29,600 --> 00:15:32,680
And I think there are a few dimensions to that, right?

249
00:15:32,680 --> 00:15:35,000
Am I doing the right thing in terms of,

250
00:15:35,000 --> 00:15:37,960
am I forcing you to take like a risk

251
00:15:37,960 --> 00:15:40,440
that I actually think, you know, is too high?

252
00:15:40,440 --> 00:15:43,040
I already know that you won't be able to pay back

253
00:15:43,040 --> 00:15:45,080
that mortgage or that credit card or that thing,

254
00:15:45,080 --> 00:15:47,720
or you don't really need this kind of thing, right?

255
00:15:47,720 --> 00:15:51,080
So that's part of that decision,

256
00:15:51,080 --> 00:15:53,520
but also is it relevant in the first place, right?

257
00:15:53,520 --> 00:15:57,800
It's an empathetic thing to not try and waste somebody's time,

258
00:15:57,800 --> 00:15:59,840
right? I mean, if you don't, I don't know about you, Sam,

259
00:15:59,840 --> 00:16:02,760
but if you like all these ads, all the stuff that you get,

260
00:16:02,760 --> 00:16:06,000
if you, you know, browse the internet

261
00:16:06,000 --> 00:16:09,720
and look at all of these pages, it's, we're used to it.

262
00:16:09,720 --> 00:16:11,960
It doesn't show a lot of empathy, right?

263
00:16:11,960 --> 00:16:14,400
Everybody's trying to get your attention to do things

264
00:16:14,400 --> 00:16:18,000
that, you know, maybe 1% of the time,

265
00:16:18,000 --> 00:16:19,960
you're vaguely interested in, right?

266
00:16:19,960 --> 00:16:22,080
So that's part of it.

267
00:16:23,120 --> 00:16:23,960
Is it relevant?

268
00:16:23,960 --> 00:16:25,840
Is it not wasting my time?

269
00:16:25,840 --> 00:16:27,920
Is it, do you remember the context?

270
00:16:27,920 --> 00:16:31,560
Do you remember that I just spoke to you in another channel, right?

271
00:16:31,560 --> 00:16:34,920
I just walked into the branch, I just visited your website

272
00:16:34,920 --> 00:16:36,960
and now I'm going into the IPR.

273
00:16:36,960 --> 00:16:40,680
Do you even remember that or are you forcing me

274
00:16:40,680 --> 00:16:42,960
to basically repeat everything I did?

275
00:16:42,960 --> 00:16:45,760
I did, you know, in the previous channel.

276
00:16:45,760 --> 00:16:47,640
That's empathy, right?

277
00:16:47,640 --> 00:16:51,680
Empathy with customers that are trying to solve a problem

278
00:16:51,680 --> 00:16:55,560
or that want to get value out of their interaction

279
00:16:55,560 --> 00:16:56,600
with the company.

280
00:16:56,600 --> 00:17:01,600
This is clearly an issue that is much bigger than AI, right?

281
00:17:02,680 --> 00:17:06,960
We don't have to, you know, look very far to recognize

282
00:17:06,960 --> 00:17:11,680
that in many ways, the previous financial crisis

283
00:17:11,680 --> 00:17:15,760
with the mortgage bubble grew out of giving loans

284
00:17:15,760 --> 00:17:19,320
to people that, you know, weren't qualified for them.

285
00:17:19,320 --> 00:17:21,400
And there are many, many more examples

286
00:17:21,400 --> 00:17:25,320
where organizations, you know, fail to exhibit

287
00:17:25,320 --> 00:17:27,720
the kind of empathy that you are describing

288
00:17:27,720 --> 00:17:30,160
that have nothing to do with artificial intelligence

289
00:17:30,160 --> 00:17:34,240
or machine learning, you know, why take this on

290
00:17:34,240 --> 00:17:37,160
from an AI perspective?

291
00:17:37,160 --> 00:17:38,800
Well, I think that's a good question.

292
00:17:38,800 --> 00:17:42,040
And I think the answer to that is

293
00:17:42,040 --> 00:17:45,400
that the way we look at customer interaction in general

294
00:17:45,400 --> 00:17:48,720
is to always do this next best action kind of thing.

295
00:17:48,720 --> 00:17:51,880
And the next best action is actually collaboration

296
00:17:51,880 --> 00:17:55,480
between humans, you know, inside the company,

297
00:17:55,480 --> 00:18:00,480
deciding on, you know, rules or thresholds or policies

298
00:18:01,400 --> 00:18:05,840
working together with AI, where AI is maybe determining

299
00:18:05,840 --> 00:18:10,840
the risk, it's determining the level of likely interest

300
00:18:11,040 --> 00:18:12,480
from the customer.

301
00:18:12,480 --> 00:18:16,200
And it's that combination that creates the metric

302
00:18:16,200 --> 00:18:19,480
for this is the best thing to do right now, right?

303
00:18:19,480 --> 00:18:21,280
So you're quite right.

304
00:18:21,280 --> 00:18:23,680
It's actually that mortgage example or the bubble

305
00:18:23,680 --> 00:18:25,920
that you just described is a great combination, right?

306
00:18:25,920 --> 00:18:29,280
There are analytic models that should have said,

307
00:18:29,280 --> 00:18:32,600
listen, for this group of people,

308
00:18:32,600 --> 00:18:35,680
the risk is not really acceptable.

309
00:18:35,680 --> 00:18:38,360
And you shouldn't be pushing them

310
00:18:38,360 --> 00:18:42,560
on this level of mortgage, right?

311
00:18:42,560 --> 00:18:46,480
Suitability, for instance, is not taking into account, right?

312
00:18:46,480 --> 00:18:51,480
So it's that combination of AI and rules that I, you know,

313
00:18:53,440 --> 00:18:56,520
we call that decisioning or decision management

314
00:18:57,640 --> 00:19:02,640
that basically needs to represent empathetic behavior, right?

315
00:19:03,400 --> 00:19:08,400
So it's not just the AI, it's also the rules.

316
00:19:08,400 --> 00:19:11,960
But one of the reasons I think the AI aspect is so important

317
00:19:11,960 --> 00:19:14,600
is because the AI is learning, right?

318
00:19:14,600 --> 00:19:19,600
So it can, you know, have evolved a particular bias, right?

319
00:19:20,960 --> 00:19:23,680
It may be a very opaque algorithm

320
00:19:23,680 --> 00:19:27,240
that may have evolved at bias and you don't even know, right?

321
00:19:27,240 --> 00:19:29,680
So there are a lot of aspects of AI

322
00:19:29,680 --> 00:19:33,760
that I think really touch on ethics and empathy as well.

323
00:19:33,760 --> 00:19:35,880
When you're talking about this at Pegaworld,

324
00:19:35,880 --> 00:19:39,240
are you, you know, you raising this as an issue

325
00:19:39,240 --> 00:19:42,960
that customers should start thinking about this?

326
00:19:42,960 --> 00:19:46,360
Are you talking about new capabilities

327
00:19:46,360 --> 00:19:50,440
that you're unveiling at Pegas with, you know,

328
00:19:50,440 --> 00:19:54,520
with the product that will help them address these issues?

329
00:19:54,520 --> 00:19:58,800
Is it, you know, or is it something else?

330
00:19:58,800 --> 00:20:00,920
Well, I don't want to feel all my own thunder,

331
00:20:00,920 --> 00:20:04,800
but we'll definitely, so two years ago,

332
00:20:04,800 --> 00:20:07,560
one of the things that I was talking about in the keynote

333
00:20:07,560 --> 00:20:10,120
was about this thing called the T switch

334
00:20:10,120 --> 00:20:12,600
and the T was, you know, stands for transparency,

335
00:20:12,600 --> 00:20:14,080
but also for trust.

336
00:20:14,080 --> 00:20:16,840
And it's basically the ability that once you have

337
00:20:16,840 --> 00:20:20,160
this centralized next best action capability

338
00:20:20,160 --> 00:20:24,560
inside of your company, that you can have full control

339
00:20:24,560 --> 00:20:29,280
over where you allow sort of opaque algorithms

340
00:20:29,280 --> 00:20:31,800
like deep learning or genetic algorithms

341
00:20:31,800 --> 00:20:33,640
or that's kind of fancy stuff

342
00:20:33,640 --> 00:20:36,480
or where you insist on more transparent algorithms

343
00:20:36,480 --> 00:20:39,040
that you can actually explain to a customer

344
00:20:39,040 --> 00:20:42,920
and that you really explain or that you really understand yourself.

345
00:20:42,920 --> 00:20:44,280
So that was one aspect.

346
00:20:45,360 --> 00:20:49,160
Next up is the thing around empathy.

347
00:20:49,160 --> 00:20:52,880
I want, I think it's a really good thing if companies

348
00:20:52,880 --> 00:20:57,880
are aware at all times how empathetic their behavior is.

349
00:20:58,880 --> 00:21:03,560
So think about sort of a dashboard

350
00:21:03,560 --> 00:21:07,480
where you would see of all the actions my company is taking

351
00:21:07,480 --> 00:21:10,280
and these are, you know, the side of companies we work with,

352
00:21:10,280 --> 00:21:12,760
these are hundreds of millions a day, right?

353
00:21:12,760 --> 00:21:15,240
Hundreds of millions of interactions a day

354
00:21:15,240 --> 00:21:19,680
and then being able to see, okay, these are actions

355
00:21:19,680 --> 00:21:21,440
that we are taken automatically

356
00:21:21,440 --> 00:21:24,400
is a combination of AI and rules

357
00:21:24,400 --> 00:21:27,080
that are not empathetic

358
00:21:27,080 --> 00:21:30,240
and that means that they are going against the relevance

359
00:21:30,240 --> 00:21:33,280
or they are not appropriate, like not suitable.

360
00:21:33,280 --> 00:21:35,680
So we're talking about this credit card

361
00:21:35,680 --> 00:21:40,480
but it's not really suitable or it doesn't really create value

362
00:21:40,480 --> 00:21:41,680
for the customer, right?

363
00:21:41,680 --> 00:21:44,920
So imagine that while all of this is running,

364
00:21:44,920 --> 00:21:47,120
this combination of AI and rules

365
00:21:47,120 --> 00:21:50,000
and it's making hundreds of millions of decisions

366
00:21:50,000 --> 00:21:52,800
and having all of these conversations with customers

367
00:21:52,800 --> 00:21:55,680
that you can just see that as a real-time thing

368
00:21:55,680 --> 00:21:59,880
and say, hey, really, we're getting less empathetic.

369
00:22:00,840 --> 00:22:04,760
Let's see in our strategies, in our customers' strategies

370
00:22:04,760 --> 00:22:07,920
that we have in the algorithms that we use,

371
00:22:07,920 --> 00:22:09,720
where we're losing that, right?

372
00:22:09,720 --> 00:22:12,320
Are we pushing products that we shouldn't be pushing?

373
00:22:12,320 --> 00:22:16,360
Don't we have the rules that are determining suitability?

374
00:22:16,360 --> 00:22:21,360
It's those kind of things and then in addition to that,

375
00:22:21,360 --> 00:22:25,120
I think we can also determine sort of the cost

376
00:22:25,120 --> 00:22:27,720
of not being empathetic, right?

377
00:22:27,720 --> 00:22:31,720
So if you, for instance, if you are going against

378
00:22:31,720 --> 00:22:33,120
somebody's interests

379
00:22:33,120 --> 00:22:37,440
and I don't mean interest in terms of relevance, right?

380
00:22:37,440 --> 00:22:40,880
So what a lot of marketing is currently doing, right?

381
00:22:40,880 --> 00:22:42,640
They're spamming you with stuff,

382
00:22:42,640 --> 00:22:44,760
they're wasting your time on things

383
00:22:44,760 --> 00:22:47,320
that are actually not that relevant to you.

384
00:22:47,320 --> 00:22:50,640
It would be good to not only know the percentage

385
00:22:50,640 --> 00:22:53,840
of events where that happens,

386
00:22:53,840 --> 00:22:56,360
it would also be really good if you had a sense

387
00:22:56,360 --> 00:23:00,160
of the money you're actually losing

388
00:23:00,160 --> 00:23:02,440
and you can calculate that mathematically

389
00:23:02,440 --> 00:23:04,200
by, for instance, saying, hey,

390
00:23:04,200 --> 00:23:06,360
this is what we actually talked about to this customer.

391
00:23:06,360 --> 00:23:09,160
We talked about this mortgage and the customer said,

392
00:23:09,160 --> 00:23:11,600
no to it because it wasn't relevant.

393
00:23:11,600 --> 00:23:13,720
What we could have been talking about

394
00:23:13,720 --> 00:23:18,720
is this particular issue that we spotted in a different channel

395
00:23:19,680 --> 00:23:23,160
or last week or an hour ago,

396
00:23:23,160 --> 00:23:25,840
or maybe a much more relevant offer

397
00:23:25,840 --> 00:23:27,920
that we didn't think we wanted to do

398
00:23:27,920 --> 00:23:32,560
because the margin was not as big as on the mortgage, right?

399
00:23:32,560 --> 00:23:35,640
Having, making that a transparent thing,

400
00:23:35,640 --> 00:23:40,640
having people own the kind of empathy level of the brand

401
00:23:42,400 --> 00:23:47,400
I think is a really important thing going forward.

402
00:23:47,520 --> 00:23:50,280
It strikes me that that latter point

403
00:23:50,280 --> 00:23:55,280
around quantifying the cost of these non-emphathetic

404
00:23:55,280 --> 00:24:00,040
non-emphathetic actions is really a big part

405
00:24:00,040 --> 00:24:02,760
of where the problem lies.

406
00:24:02,760 --> 00:24:07,760
It's easy to know the cost of the expected revenue

407
00:24:10,520 --> 00:24:13,600
or profit from offering something,

408
00:24:13,600 --> 00:24:16,120
but a lot harder to know the cost

409
00:24:16,120 --> 00:24:19,360
of just wasting the customer's time

410
00:24:19,360 --> 00:24:23,120
or reducing the brand goodwill

411
00:24:23,120 --> 00:24:27,600
because of some series of less relevant

412
00:24:27,600 --> 00:24:29,800
or poor experiences.

413
00:24:29,800 --> 00:24:32,960
How do you overcome that gap?

414
00:24:32,960 --> 00:24:34,800
Yeah, well, I think some of the math

415
00:24:34,800 --> 00:24:36,920
actually works out quite nicely, right?

416
00:24:36,920 --> 00:24:39,680
So remember that when we do this next best action,

417
00:24:39,680 --> 00:24:44,600
and I said before, the best is a function of the value

418
00:24:44,600 --> 00:24:47,400
that is created in the relationship.

419
00:24:47,400 --> 00:24:52,400
So the math works out that you actually can know

420
00:24:52,400 --> 00:24:54,240
that if you go against the propensity,

421
00:24:54,240 --> 00:24:55,400
because for instance, you're selling,

422
00:24:55,400 --> 00:24:58,840
let's use this mortgage example that works really well,

423
00:24:58,840 --> 00:25:02,680
if you are thinking this is not particularly relevant,

424
00:25:02,680 --> 00:25:05,920
but if the customer says yes, this is the margin,

425
00:25:05,920 --> 00:25:10,920
this is the money I as a bank in this case will make, right?

426
00:25:11,520 --> 00:25:15,320
If you calculate all the times a customer said no,

427
00:25:15,320 --> 00:25:17,240
because you're basically offering stuff

428
00:25:17,240 --> 00:25:18,600
that's not particularly relevant,

429
00:25:18,600 --> 00:25:22,920
just you know, you're hoping that the customer will say yes.

430
00:25:22,920 --> 00:25:25,920
What, how could we have used that moment?

431
00:25:25,920 --> 00:25:27,760
How could we have used that interaction

432
00:25:27,760 --> 00:25:29,880
with the customer in a better way

433
00:25:29,880 --> 00:25:32,320
that would have created more value?

434
00:25:32,320 --> 00:25:34,640
If you just multiply that with a hundred of millions

435
00:25:34,640 --> 00:25:38,400
of decisions you make, you get to a monetary value.

436
00:25:38,400 --> 00:25:40,280
And you find in examples like this

437
00:25:40,280 --> 00:25:43,000
that there's some explicit decision

438
00:25:43,000 --> 00:25:46,920
that where the customer is saying,

439
00:25:46,920 --> 00:25:49,040
hey, let's offer this more profitable thing,

440
00:25:49,040 --> 00:25:51,400
even though we know it's not as relevant,

441
00:25:51,400 --> 00:25:56,400
or does that happen in more subtle ways?

442
00:25:58,000 --> 00:26:01,560
No, I don't think these ways are particularly subtle, right?

443
00:26:01,560 --> 00:26:05,480
So the way, because the way we work is like,

444
00:26:05,480 --> 00:26:08,240
so to do this next best action, right?

445
00:26:08,240 --> 00:26:11,960
We would calculate every single thing,

446
00:26:11,960 --> 00:26:15,000
this is also part of this hyper-personalization vision,

447
00:26:15,000 --> 00:26:17,160
to be completely one to one,

448
00:26:17,160 --> 00:26:19,680
it means that of all the possible conversations

449
00:26:19,680 --> 00:26:22,560
you could have, right, you're going to rate them

450
00:26:22,560 --> 00:26:24,880
in real time based on the context,

451
00:26:24,880 --> 00:26:26,040
and you're going to say,

452
00:26:26,040 --> 00:26:28,360
this is the thing we are going to talk about

453
00:26:28,360 --> 00:26:31,080
as a combination of what the AI thinks

454
00:26:31,080 --> 00:26:34,040
is particularly appropriate and relevant,

455
00:26:34,040 --> 00:26:37,240
as well as my rules that I have around profitability

456
00:26:37,240 --> 00:26:41,720
and inventory and all of those kind of things, right?

457
00:26:41,720 --> 00:26:43,000
So it's that combination,

458
00:26:43,000 --> 00:26:45,400
but we calculate them all in parallel.

459
00:26:45,400 --> 00:26:48,360
So it's relatively easy to see, okay,

460
00:26:48,360 --> 00:26:51,600
this is what we chose to actually talk about,

461
00:26:51,600 --> 00:26:54,800
but this is what we could have talked about

462
00:26:54,800 --> 00:26:58,800
if we had weighted suitability higher,

463
00:26:58,800 --> 00:27:02,640
or if we didn't overrule this very low,

464
00:27:02,640 --> 00:27:04,360
or this very high propensity and said,

465
00:27:04,360 --> 00:27:05,960
well, even though that's relevant,

466
00:27:05,960 --> 00:27:08,240
it's not what we want to talk about, right?

467
00:27:08,240 --> 00:27:11,360
So you can see how you get a drop off of typical,

468
00:27:11,360 --> 00:27:14,520
or of specific conversation topics

469
00:27:14,520 --> 00:27:17,760
that you decide not to pursue for other reasons,

470
00:27:17,760 --> 00:27:20,800
and that's what you can calculate.

471
00:27:20,800 --> 00:27:26,000
Is the task then starting to build awareness

472
00:27:26,000 --> 00:27:29,600
on the part of customers or users

473
00:27:29,600 --> 00:27:34,440
or kind of the industry as a whole

474
00:27:34,440 --> 00:27:39,440
to incorporate these types of empathy metrics,

475
00:27:39,440 --> 00:27:44,440
if we call them that, into their systems,

476
00:27:45,720 --> 00:27:49,040
their rules, their algorithms,

477
00:27:49,920 --> 00:27:54,240
and start to, is it as, I don't know if simple is the right word,

478
00:27:54,240 --> 00:27:58,280
but as simple as starting to try to put numbers

479
00:27:58,280 --> 00:28:02,320
around these suitability context

480
00:28:02,320 --> 00:28:06,160
to where relevance risk and then feeding them

481
00:28:06,160 --> 00:28:11,160
into your automation tooling with kind of appropriate weights

482
00:28:12,240 --> 00:28:14,600
or does it go beyond that?

483
00:28:15,800 --> 00:28:17,520
I think the way you describe it,

484
00:28:17,520 --> 00:28:19,840
so what we're trying to do is, first of all,

485
00:28:19,840 --> 00:28:22,440
we want to do that like an easy task.

486
00:28:22,440 --> 00:28:23,440
No, no, no.

487
00:28:23,440 --> 00:28:25,760
We're not conclusion or anything like that.

488
00:28:25,760 --> 00:28:27,480
Yeah, yeah.

489
00:28:27,480 --> 00:28:29,000
No, that's not easy in itself,

490
00:28:29,000 --> 00:28:31,160
but what we're trying to do is, first of all,

491
00:28:31,160 --> 00:28:32,520
make it very explicit.

492
00:28:32,520 --> 00:28:35,840
So when we talk about next best action,

493
00:28:35,840 --> 00:28:39,640
we have like, there are patterns that we've seen

494
00:28:39,640 --> 00:28:43,440
that are repeatedly successful,

495
00:28:43,440 --> 00:28:47,720
and they include things like relevance,

496
00:28:47,720 --> 00:28:51,400
suitability, mutual value, risk mitigation, right?

497
00:28:51,400 --> 00:28:56,400
So the tooling and the methodology already encourage

498
00:28:56,400 --> 00:28:58,920
companies to at least think about it, right?

499
00:28:58,920 --> 00:29:01,160
They may think, okay, well, for suitability,

500
00:29:01,160 --> 00:29:03,360
we really don't care about it, right?

501
00:29:03,360 --> 00:29:07,040
Or not as much, or we let profitability,

502
00:29:07,040 --> 00:29:09,960
Trump suitability anytime, right?

503
00:29:09,960 --> 00:29:12,160
But at least the product,

504
00:29:12,160 --> 00:29:14,160
well, if you follow the product guidance,

505
00:29:14,160 --> 00:29:17,320
you will have to take all of these considerations

506
00:29:17,320 --> 00:29:18,600
into account, right?

507
00:29:18,600 --> 00:29:23,600
So there is an ethical framework built into the software,

508
00:29:24,040 --> 00:29:26,880
into the strategies that it will generate.

509
00:29:26,880 --> 00:29:28,320
So that's one aspect of it,

510
00:29:28,320 --> 00:29:31,200
and then the other aspect of it are like these,

511
00:29:31,200 --> 00:29:33,640
the dashboard that you will show, right?

512
00:29:33,640 --> 00:29:36,160
So it's basically shaming companies a little bit,

513
00:29:36,160 --> 00:29:37,560
if they won't, right?

514
00:29:37,560 --> 00:29:42,320
Having them self-shame them into appropriate behavior,

515
00:29:42,320 --> 00:29:44,280
right, where they would say, hey, listen,

516
00:29:44,280 --> 00:29:46,120
we cranked up profitability,

517
00:29:46,120 --> 00:29:50,640
but it's at the expense of suitability or customer interest.

518
00:29:51,600 --> 00:29:53,920
And at least, I think the awareness

519
00:29:53,920 --> 00:29:56,040
and the transparency around these things

520
00:29:56,040 --> 00:30:01,040
will be leading to better behavior.

521
00:30:01,360 --> 00:30:06,360
How does the company begin to put tangible numbers

522
00:30:07,800 --> 00:30:11,680
and costs around things like mutual value

523
00:30:11,680 --> 00:30:15,960
and suitability and contexts awareness and relevance?

524
00:30:15,960 --> 00:30:17,520
And relevance is maybe easier

525
00:30:17,520 --> 00:30:20,280
because it impacts like propensity to buy.

526
00:30:20,280 --> 00:30:24,320
Risk is something that's kind of fundamentally numerical,

527
00:30:24,320 --> 00:30:28,840
but like some of these others are a little bit squishier, maybe?

528
00:30:28,840 --> 00:30:32,240
Yeah, well, but I think, you're right.

529
00:30:32,240 --> 00:30:35,080
And I think the, from what we've seen,

530
00:30:35,080 --> 00:30:39,040
is that sort of the less squishy things, right?

531
00:30:39,040 --> 00:30:44,040
Are once you are aware that you need to put them in,

532
00:30:45,040 --> 00:30:49,200
and at the AI or the decisioning in general,

533
00:30:49,200 --> 00:30:54,200
touching 100 million, making 100 million decisions,

534
00:30:54,200 --> 00:30:56,040
and all the different general with all of your customers,

535
00:30:56,040 --> 00:30:58,520
that that is part of your brand, right?

536
00:30:58,520 --> 00:31:00,440
And you need to protect that.

537
00:31:00,440 --> 00:31:04,560
I think that's a very important thing.

538
00:31:04,560 --> 00:31:06,880
I think for the squishier things,

539
00:31:06,880 --> 00:31:10,960
I think what we also encourage and also make possible

540
00:31:10,960 --> 00:31:13,640
is continuous experimentation, right?

541
00:31:13,640 --> 00:31:16,240
So there's always, there's control groups,

542
00:31:16,240 --> 00:31:18,720
there is all sorts of things where you can, you know,

543
00:31:18,720 --> 00:31:21,840
for a small percentage, a small sample of the customers,

544
00:31:21,840 --> 00:31:24,520
you can actually measure if you're having an effect, right?

545
00:31:24,520 --> 00:31:27,680
If they have a positive response to the brand,

546
00:31:28,720 --> 00:31:31,200
and you can see if, you know,

547
00:31:31,200 --> 00:31:34,760
what kind of strategy changes would improve that?

548
00:31:34,760 --> 00:31:37,760
And that is a best practice kind of thing to do.

549
00:31:39,280 --> 00:31:44,280
Do you have any examples of folks that you've worked with

550
00:31:44,840 --> 00:31:47,080
that you can kind of walk us through

551
00:31:47,080 --> 00:31:48,600
how this all plays out,

552
00:31:48,600 --> 00:31:53,600
and how they want about making, you know,

553
00:31:54,000 --> 00:31:55,760
kind of incorporating these ideas

554
00:31:55,760 --> 00:31:57,600
into the way they make decisions?

555
00:31:58,720 --> 00:32:02,080
Yeah, so I think even before we sort of, you know,

556
00:32:02,080 --> 00:32:06,160
invested in all this around, you know, empathy

557
00:32:06,160 --> 00:32:09,640
and also before the transparency and opacity,

558
00:32:09,640 --> 00:32:12,640
it's not like, you know, these big brands

559
00:32:12,640 --> 00:32:16,400
are not aware of these issues, right?

560
00:32:16,400 --> 00:32:20,720
I mean, if I think 10 years ago, maybe longer ago,

561
00:32:20,720 --> 00:32:22,880
I had long conversation with banks

562
00:32:22,880 --> 00:32:27,560
that were sued for, like, misscelling, right?

563
00:32:27,560 --> 00:32:30,680
That I think, you know, we've seen more recent examples

564
00:32:31,880 --> 00:32:34,960
where obviously these companies

565
00:32:36,480 --> 00:32:39,000
want to sort of, you know, control that,

566
00:32:39,000 --> 00:32:41,120
even if just for their own sake, right?

567
00:32:41,120 --> 00:32:43,440
To not be part of some class action.

568
00:32:43,440 --> 00:32:47,240
And in the next best action methodology,

569
00:32:47,240 --> 00:32:51,160
stuff like relevance, appropriateness, failure and risk,

570
00:32:51,160 --> 00:32:54,040
have always been sort of first class citizens, right?

571
00:32:54,040 --> 00:32:58,720
What we're now trying to do is to make sure

572
00:32:58,720 --> 00:33:02,360
that it's much harder to break those patterns,

573
00:33:02,360 --> 00:33:06,160
or if you are, you know, don't want to be compliant

574
00:33:06,160 --> 00:33:10,920
with these kind of ethics practices,

575
00:33:10,920 --> 00:33:13,680
you at least have to make the effort.

576
00:33:13,680 --> 00:33:17,560
I think to your question, I think especially the banks,

577
00:33:17,560 --> 00:33:19,600
I think we've seen it in other industries as well,

578
00:33:19,600 --> 00:33:23,600
are getting very worried about their brand image

579
00:33:23,600 --> 00:33:26,280
in that regard, and they are putting, you know,

580
00:33:26,280 --> 00:33:28,320
suitability criteria, for instance,

581
00:33:28,320 --> 00:33:29,960
is a pretty hot topic right now.

582
00:33:29,960 --> 00:33:33,120
They're putting that as part of their next best action

583
00:33:33,120 --> 00:33:34,200
strategy.

584
00:33:34,200 --> 00:33:38,240
We just want to help them by showing the cost of that

585
00:33:38,240 --> 00:33:39,800
and the benefits of that.

586
00:33:39,800 --> 00:33:43,920
Okay, you mentioned compliance in there.

587
00:33:43,920 --> 00:33:47,040
Do you envision a time where an enterprise

588
00:33:47,040 --> 00:33:51,120
will have a formal empathy compliance regime?

589
00:33:51,120 --> 00:33:52,240
Will it be called that?

590
00:33:52,240 --> 00:33:53,560
Will it be called something else?

591
00:33:53,560 --> 00:33:56,080
Does it already exist under some other guys?

592
00:33:57,720 --> 00:34:00,760
I think that definitely will happen

593
00:34:00,760 --> 00:34:02,120
in some cases already happens.

594
00:34:02,120 --> 00:34:03,720
I don't think it's called empathy

595
00:34:03,720 --> 00:34:06,600
is probably more on the ethics board,

596
00:34:06,600 --> 00:34:11,600
where they, and again, it's not only about the company

597
00:34:14,480 --> 00:34:18,400
itself, it's also about basically a compliance issue

598
00:34:18,400 --> 00:34:21,080
out of self-interest, especially now,

599
00:34:21,080 --> 00:34:24,160
and this is what where the AI is so important,

600
00:34:24,160 --> 00:34:26,560
where there's so much self-learning going on

601
00:34:26,560 --> 00:34:31,560
on this incredible scale that there could be a bias,

602
00:34:31,560 --> 00:34:34,440
and there could be all sorts of things happening

603
00:34:34,440 --> 00:34:39,440
that may not be so easy to control or even spot.

604
00:34:41,840 --> 00:34:44,760
So I know that some of the companies I talk to,

605
00:34:44,760 --> 00:34:47,000
and these are larger companies,

606
00:34:47,000 --> 00:34:52,000
but they have a board already for all the algorithms

607
00:34:52,640 --> 00:34:55,840
that are involved in customer interaction.

608
00:34:55,840 --> 00:34:58,800
And I think that's a sensible thing to do.

609
00:34:58,800 --> 00:35:02,240
It's part of what inspired this transparency

610
00:35:02,240 --> 00:35:05,520
or trust switch in the software to make sure

611
00:35:05,520 --> 00:35:09,800
that all AI is at least you can control

612
00:35:09,800 --> 00:35:13,480
the level of transparency that you require

613
00:35:13,480 --> 00:35:18,240
in certain circumstances, talking to customers.

614
00:35:18,240 --> 00:35:20,400
Do you envision like a chief empathy officer?

615
00:35:20,400 --> 00:35:22,080
It sounds like no, it's probably going to be

616
00:35:22,080 --> 00:35:24,840
if anything, it'll be a chief ethics officer

617
00:35:24,840 --> 00:35:27,280
or some other role.

618
00:35:27,280 --> 00:35:30,320
Where do you see this all sitting?

619
00:35:30,320 --> 00:35:33,440
Yeah, well, I think this is a very interesting topic

620
00:35:33,440 --> 00:35:37,200
because I think this will become very, very important

621
00:35:37,200 --> 00:35:39,160
if it isn't already.

622
00:35:39,160 --> 00:35:41,840
And I think you will get into a situation

623
00:35:41,840 --> 00:35:46,840
where you have at the first level AI trying to check

624
00:35:49,440 --> 00:35:54,440
other AI for biases or an ethical behavior,

625
00:35:54,640 --> 00:35:56,640
because it's just a lot,

626
00:35:56,640 --> 00:36:00,880
and it would only escalate if such a bias

627
00:36:00,880 --> 00:36:05,200
or other irregularities is detected.

628
00:36:06,080 --> 00:36:09,440
But it's certainly, and again, I'm talking about

629
00:36:09,440 --> 00:36:11,960
the larger companies with tens to hundreds

630
00:36:11,960 --> 00:36:16,640
of millions of customers that are very worried about,

631
00:36:16,640 --> 00:36:18,640
especially with the level of automation

632
00:36:18,640 --> 00:36:20,520
that's now available, and then AI

633
00:36:20,520 --> 00:36:23,560
that is dynamically learning new things

634
00:36:23,560 --> 00:36:28,800
or evolving new things to have an ethics board like that.

635
00:36:28,800 --> 00:36:31,840
And we try from a product perspective,

636
00:36:31,840 --> 00:36:35,840
we try to make sure that like you have QA tests

637
00:36:35,840 --> 00:36:38,560
on quality assurance tests, on performance

638
00:36:38,560 --> 00:36:41,520
and other things that as a matter of course,

639
00:36:41,520 --> 00:36:45,840
you would do the same thing around bias detection

640
00:36:46,840 --> 00:36:50,240
or other irregularities before you release

641
00:36:50,240 --> 00:36:54,160
the next version of your corporate brain,

642
00:36:54,160 --> 00:36:58,400
so to speak, to make that easier.

643
00:36:58,400 --> 00:37:03,400
I think the ideas of kind of making these

644
00:37:03,720 --> 00:37:08,720
more empathetic types of qualities of your various offers,

645
00:37:11,800 --> 00:37:15,160
you know, as you've suggested throughout this conversation,

646
00:37:15,160 --> 00:37:18,160
it's very much kind of connected to this idea

647
00:37:18,160 --> 00:37:19,560
of transparency, right?

648
00:37:19,560 --> 00:37:22,440
There are, you know, these dollars and cents things

649
00:37:22,440 --> 00:37:25,360
that we kind of build into decision-making algorithms

650
00:37:25,360 --> 00:37:28,240
all the time, but there's all this other stuff

651
00:37:28,240 --> 00:37:30,480
that goes into the customer experience

652
00:37:30,480 --> 00:37:32,360
and what we're doing here, you know,

653
00:37:32,360 --> 00:37:35,600
we're calling empathy is really the idea

654
00:37:35,600 --> 00:37:40,600
of making a lot of those non-premafacy financial aspects

655
00:37:44,200 --> 00:37:48,200
you know, A, more transparent and then B,

656
00:37:48,200 --> 00:37:50,960
like putting, trying to make them more financial

657
00:37:50,960 --> 00:37:54,320
or putting numbers against them and then, you know,

658
00:37:54,320 --> 00:37:57,120
incorporating them into decision-making,

659
00:37:57,120 --> 00:38:00,120
dashboarding them so that there's some awareness of them

660
00:38:00,120 --> 00:38:05,080
and so that the organization can manage against them.

661
00:38:05,080 --> 00:38:09,760
Really interesting set of ideas around how to make,

662
00:38:10,800 --> 00:38:15,800
how to make this idea of A, I think it's a lot more tangible.

663
00:38:15,800 --> 00:38:18,840
Yeah, I think, yeah, yeah, tangible, I think it's the right word.

664
00:38:18,840 --> 00:38:22,440
So can we, are there straightforward ways to, you know,

665
00:38:22,440 --> 00:38:25,760
make sure that in our customer's strategies, you know,

666
00:38:25,760 --> 00:38:29,120
empathy is well represented and we can choose to ignore it,

667
00:38:29,120 --> 00:38:30,880
but then there are these, as you say, these,

668
00:38:30,880 --> 00:38:34,360
these desporting, these gauges, these dials

669
00:38:34,360 --> 00:38:39,360
that show you, that shame you into some compliance.

670
00:38:39,600 --> 00:38:41,360
And also, let's not forget that, like,

671
00:38:41,360 --> 00:38:44,800
I think the reason we as humans have empathy,

672
00:38:44,800 --> 00:38:48,240
you know, there can be lots of different theories around that,

673
00:38:48,240 --> 00:38:51,760
but personally, I think that evolved, right?

674
00:38:51,760 --> 00:38:55,320
It evolved out of a desire to collaborate, right?

675
00:38:55,320 --> 00:38:58,440
So it's empathy is not like a cost to the company.

676
00:38:58,440 --> 00:39:01,840
Empathy is actually establishing your, you know,

677
00:39:01,840 --> 00:39:05,880
better relationship and a longer term relationship

678
00:39:05,880 --> 00:39:07,720
with your, with your customers.

679
00:39:09,400 --> 00:39:12,000
Well, it would be interesting to kind of follow along

680
00:39:12,000 --> 00:39:16,280
with this work and see, you know, I'd love to hear a case study

681
00:39:16,280 --> 00:39:21,280
of, you know, how a customer kind of implements this end

682
00:39:21,280 --> 00:39:23,800
and once you've got this out in the market

683
00:39:23,800 --> 00:39:27,040
and have folks working with it.

684
00:39:27,040 --> 00:39:29,600
Yeah, I mentioned PegaWorld earlier,

685
00:39:29,600 --> 00:39:32,080
any besides from your own keynote,

686
00:39:32,080 --> 00:39:35,400
other things that you're looking forward to at the conference?

687
00:39:35,400 --> 00:39:37,960
Yeah, well, I mean, this will be the biggest effort.

688
00:39:37,960 --> 00:39:42,120
So it's always just very exciting about, you know,

689
00:39:42,120 --> 00:39:47,120
to show people, you know, where we are at,

690
00:39:47,400 --> 00:39:49,760
and it's not just about empathy, right?

691
00:39:49,760 --> 00:39:53,400
It's about, it's about also making decisions in general

692
00:39:53,400 --> 00:39:56,640
at a huge scale, you know, with this real-time AI

693
00:39:56,640 --> 00:39:58,880
on the one hand, and then on the other hand,

694
00:39:58,880 --> 00:40:00,400
and that's also part of empathy,

695
00:40:00,400 --> 00:40:02,040
although we didn't talk about it right now,

696
00:40:02,040 --> 00:40:07,800
but then following up on it, right, with the processes, right?

697
00:40:07,800 --> 00:40:11,120
So we are really, and you will hear a lot about that at PegaWorld,

698
00:40:11,120 --> 00:40:12,880
we're trying to, you know, have that,

699
00:40:12,880 --> 00:40:15,280
that combination very strongly.

700
00:40:15,280 --> 00:40:18,560
So it's, we have the AI and a decisioning

701
00:40:18,560 --> 00:40:21,240
to decide what to do, right?

702
00:40:21,240 --> 00:40:24,560
And then we have sort of the end-to-end automation

703
00:40:24,560 --> 00:40:27,520
that will tell the company how to do it,

704
00:40:27,520 --> 00:40:29,880
and to do it fast and efficiently,

705
00:40:29,880 --> 00:40:32,240
which also plays into empathy.

706
00:40:32,240 --> 00:40:34,760
So I really lost that interplay

707
00:40:34,760 --> 00:40:37,440
between sort of decisions and processes.

708
00:40:37,440 --> 00:40:42,160
So I'm expecting a lot of really good inter-discussions

709
00:40:42,160 --> 00:40:44,920
and presentations from my customers.

710
00:40:44,920 --> 00:40:46,120
Awesome, awesome.

711
00:40:46,120 --> 00:40:49,400
Well, Rob, I'm looking forward to seeing you once again

712
00:40:49,400 --> 00:40:50,400
at the event.

713
00:40:50,400 --> 00:40:53,160
Thanks so much for taking the time to jump on

714
00:40:53,160 --> 00:40:55,360
and talk this through with us.

715
00:40:55,360 --> 00:40:56,960
Okay, well, you're very welcome.

716
00:40:56,960 --> 00:40:57,800
Thank you.

717
00:40:57,800 --> 00:40:58,960
Thanks, Rob.

718
00:41:01,120 --> 00:41:03,680
All right, everyone, that's our show for today

719
00:41:03,680 --> 00:41:06,600
for more information on Rob or any of the topics

720
00:41:06,600 --> 00:41:08,000
covered in this episode.

721
00:41:08,000 --> 00:41:13,000
Visit twimmolai.com slash talk slash 248.

722
00:41:13,000 --> 00:41:15,320
Be sure to leave your thoughts on AI empathy

723
00:41:15,320 --> 00:41:16,720
while you're there.

724
00:41:16,720 --> 00:41:45,240
As always, thanks so much for listening and catch you next time.

