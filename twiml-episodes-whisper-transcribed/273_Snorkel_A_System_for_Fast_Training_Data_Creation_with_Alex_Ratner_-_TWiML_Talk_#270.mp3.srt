1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,240
I'm your host Sam Charrington.

4
00:00:32,240 --> 00:00:36,980
If you missed our last show and if you did you definitely want to go check it out because

5
00:00:36,980 --> 00:00:42,340
it was a great conversation but if you missed that show you missed the first of the many

6
00:00:42,340 --> 00:00:45,760
exciting updates we have for you this summer.

7
00:00:45,760 --> 00:00:50,720
Last time we announced Twimble's third birthday and our 5 millionth download which happened

8
00:00:50,720 --> 00:00:53,080
right around the same time.

9
00:00:53,080 --> 00:00:58,320
To help us celebrate this occasion and to request your commemorative Twimble birthday sticker

10
00:00:58,320 --> 00:01:02,840
visit twimbleai.com slash birthday 3.

11
00:01:02,840 --> 00:01:09,280
This week we're continuing the action by kicking off volume 2 of our AI platform series.

12
00:01:09,280 --> 00:01:14,440
You recall that last fall we brought you AI platform's volume 1 featuring conversations

13
00:01:14,440 --> 00:01:21,420
with platform builders from Facebook, Airbnb, LinkedIn, OpenAI, Shell and Comcast.

14
00:01:21,420 --> 00:01:27,620
This series turned out to be one of our most popular series of shows ever and over 1000

15
00:01:27,620 --> 00:01:33,040
of you downloaded our first ebook on machine learning platforms, Kubernetes for machine

16
00:01:33,040 --> 00:01:35,920
learning, deep learning and AI.

17
00:01:35,920 --> 00:01:41,280
Well we'll be back at it over the next few weeks sharing more experiences from teams working

18
00:01:41,280 --> 00:01:46,880
to scale and industrialize data science and machine learning at their companies.

19
00:01:46,880 --> 00:01:51,320
And we've got even more in store on this topic so if it's an area you're interested

20
00:01:51,320 --> 00:01:53,880
in be sure to stay tuned.

21
00:01:53,880 --> 00:02:00,760
You can follow along with the series at twimbleai.com slash AI platforms 2 and by following us on

22
00:02:00,760 --> 00:02:07,720
Twitter at at Sam Charrington and at Twimbleai.

23
00:02:07,720 --> 00:02:12,800
Before we dive in, I'd like to send a giant thanks to our friends over at Sigopt.

24
00:02:12,800 --> 00:02:16,960
They've been huge supporters of my work in this area and I'm really excited to have them

25
00:02:16,960 --> 00:02:21,840
as a sponsor of this series of shows on machine learning and AI platforms.

26
00:02:21,840 --> 00:02:27,640
If you don't know Sigopt, I spoke with their CEO Scott Clark back on show number 50.

27
00:02:27,640 --> 00:02:31,480
Their software is used by enterprise teams to standardize and scale machine learning

28
00:02:31,480 --> 00:02:37,680
experimentation and optimization across any combination of modeling frameworks, libraries,

29
00:02:37,680 --> 00:02:40,560
computing infrastructure and environment.

30
00:02:40,560 --> 00:02:45,680
Teams like 2 Sigma who will hear from later in this series rely on Sigopt software to

31
00:02:45,680 --> 00:02:50,520
realize better modeling results much faster than previously possible.

32
00:02:50,520 --> 00:02:56,000
Of course to fully grasp the potential of a tool like Sigopt is best to try it yourself.

33
00:02:56,000 --> 00:03:01,280
That's why Sigopt is offering you the twimble community an exclusive opportunity to try

34
00:03:01,280 --> 00:03:06,280
their product on some of your toughest modeling problems for free.

35
00:03:06,280 --> 00:03:13,280
Take advantage of this offer, visit twimbleai.com slash Sigopt.

36
00:03:13,280 --> 00:03:16,160
All right, everyone.

37
00:03:16,160 --> 00:03:23,160
I am here in Montreal for the NERB's conference and I am with Alex Ratner, Alex as a PhD student

38
00:03:23,160 --> 00:03:24,160
at Stanford.

39
00:03:24,160 --> 00:03:26,640
Alex, welcome to this week in machine learning and AI.

40
00:03:26,640 --> 00:03:28,400
Thank you so much for having me.

41
00:03:28,400 --> 00:03:29,400
Awesome.

42
00:03:29,400 --> 00:03:32,880
We're going to talk a bit about one of the projects you're working on at Stanford, a project

43
00:03:32,880 --> 00:03:34,440
called Snorkel.

44
00:03:34,440 --> 00:03:38,240
Before we do, I'd love to hear a bit about your background and how you got started working

45
00:03:38,240 --> 00:03:39,240
in ML.

46
00:03:39,240 --> 00:03:40,240
Great.

47
00:03:40,240 --> 00:03:44,960
So like many at the NERB's conference, I was a reformed physicist for my undergrad days.

48
00:03:44,960 --> 00:03:49,360
Went out, financed for a bit and then crawled my way back in academia where I've been for

49
00:03:49,360 --> 00:03:53,800
the past four plus wonderful years at Stanford where there's a ton of exciting stuff going

50
00:03:53,800 --> 00:03:58,440
on and that kind of led to the current work which I guess we'll talk about today around

51
00:03:58,440 --> 00:04:00,280
a system called Snorkel.

52
00:04:00,280 --> 00:04:01,280
It's also about the system.

53
00:04:01,280 --> 00:04:03,360
What is Snorkel trying to do?

54
00:04:03,360 --> 00:04:07,640
So the idea that Snorkel's predicated on is that one of the biggest bottlenecks that

55
00:04:07,640 --> 00:04:13,720
people face right now in the current era of these highly automated deep learning algorithms

56
00:04:13,720 --> 00:04:16,920
is getting sufficient amounts of labeled training data.

57
00:04:16,920 --> 00:04:21,360
So if you want to train one of these, especially here at NERB's, if you want to train one of

58
00:04:21,360 --> 00:04:26,440
these fancy architectures, a lot of them require very, very complex.

59
00:04:26,440 --> 00:04:31,000
They do a lot of stuff like picking out all the features to look at and taking in the

60
00:04:31,000 --> 00:04:33,000
raw data and transforming it properly.

61
00:04:33,000 --> 00:04:36,440
They do this automatically, but this of course comes at a cost which is that they generally

62
00:04:36,440 --> 00:04:39,920
need lots of training data to learn from.

63
00:04:39,920 --> 00:04:44,480
And this training data often needs to be labeled by people who have some kind of domain

64
00:04:44,480 --> 00:04:45,480
expertise.

65
00:04:45,480 --> 00:04:50,800
So if you want to train a model to outperform a radiologist at some mammography task for

66
00:04:50,800 --> 00:04:56,520
example where there are some narrow results on things like these, you need months and

67
00:04:56,520 --> 00:05:01,880
months and months of or longer of radiologists sitting there labeling data.

68
00:05:01,880 --> 00:05:05,680
So this has become a significant capital expenditure for big companies.

69
00:05:05,680 --> 00:05:11,320
It's become a significant bottleneck for people like scientists or clinicians from actually

70
00:05:11,320 --> 00:05:14,000
applying these awesome new ML techniques.

71
00:05:14,000 --> 00:05:19,160
And so the question with Snorkel that we tried to address was, can we enable subject matter

72
00:05:19,160 --> 00:05:26,280
expert users to give kind of higher level inputs, things like rules or patterns or noisy

73
00:05:26,280 --> 00:05:30,680
signals from other models that they have lying around and use this to train up and leverage

74
00:05:30,680 --> 00:05:34,440
these awesome new machine learning models that are out there.

75
00:05:34,440 --> 00:05:35,440
Okay.

76
00:05:35,440 --> 00:05:41,200
I'm intrigued by this idea of allowing subject matter experts to work kind of in the domain

77
00:05:41,200 --> 00:05:48,040
of rules that their expertise, I often characterize one of the things I see in this space is kind

78
00:05:48,040 --> 00:05:55,040
of this swing from model based approaches to machine learning or to systems more broadly

79
00:05:55,040 --> 00:06:01,000
to kind of these purely statistical based approaches that have no, they don't really

80
00:06:01,000 --> 00:06:07,800
attempt to capture subject matter expertise and it sounds like Snorkel is part of a bit

81
00:06:07,800 --> 00:06:11,880
of the pendulum returning to the center which is trying to incorporate in some of this domain

82
00:06:11,880 --> 00:06:12,880
expertise.

83
00:06:12,880 --> 00:06:14,680
Yeah, I think that's a great way of putting it.

84
00:06:14,680 --> 00:06:18,960
I mean, this is, this is really an age old question in, you know, the field of AI more

85
00:06:18,960 --> 00:06:24,720
broadly is how do you inject domain expert knowledge into a system and, you know, ever

86
00:06:24,720 --> 00:06:29,680
since statistical learning techniques have become so powerful and useful in some areas,

87
00:06:29,680 --> 00:06:33,000
how do you inject this domain knowledge into them, right?

88
00:06:33,000 --> 00:06:34,000
Right.

89
00:06:34,000 --> 00:06:37,000
And, you know, more broadly than Snorkel, you can see this pendulum beginning to swing

90
00:06:37,000 --> 00:06:40,760
back a little bit when you go look around the poster session here at Nareps, you know,

91
00:06:40,760 --> 00:06:47,160
there's definitely more work out there around explicitly defined, say, generative models

92
00:06:47,160 --> 00:06:51,200
where, you know, someone has set some explicit structure rather than just learning all that

93
00:06:51,200 --> 00:06:54,040
structure from data.

94
00:06:54,040 --> 00:06:59,360
And, you know, Snorkel is definitely fitting into that narrative in that we're trying to

95
00:06:59,360 --> 00:07:04,200
bridge the gap between the two, we're trying to use domain expert knowledge that is,

96
00:07:04,200 --> 00:07:06,960
you know, expressed in a really simple way.

97
00:07:06,960 --> 00:07:11,040
We have experts or, you know, we have, you know, whether they're clinicians or journalists

98
00:07:11,040 --> 00:07:17,360
or, you know, whoever these subject matter experts are, they write what we call labeling functions.

99
00:07:17,360 --> 00:07:21,680
So just simple little functions that, you know, could express a pattern or a rule or they

100
00:07:21,680 --> 00:07:26,000
could call some other classifier and they just take a data point and say, you know, say

101
00:07:26,000 --> 00:07:29,800
it's a binary classification problem, yes, no, or I don't know, that's all.

102
00:07:29,800 --> 00:07:33,360
And they just, they can be Python functions, whatever they are, they just dump them into

103
00:07:33,360 --> 00:07:34,360
the system.

104
00:07:34,360 --> 00:07:35,360
Okay.

105
00:07:35,360 --> 00:07:37,760
And then we try to use that to take advantage of these statistical models.

106
00:07:37,760 --> 00:07:43,240
So we're trying to kind of bridge the two worlds via this very simple conduit of training

107
00:07:43,240 --> 00:07:44,240
data.

108
00:07:44,240 --> 00:07:49,960
Well, before we get too deep into Snorkel and how it works, it's a successor to an earlier

109
00:07:49,960 --> 00:07:51,840
project called Deep Dive.

110
00:07:51,840 --> 00:07:59,560
There's a clear theme that I hear somebody likes to swim or dive or snorkel or something.

111
00:07:59,560 --> 00:08:06,200
But maybe tell us a little bit about Deep Dive and its origins and trajectory.

112
00:08:06,200 --> 00:08:11,520
Deep Dive, like many, you know, great initiatives and computer science these days, it was kind

113
00:08:11,520 --> 00:08:17,440
of, in large part, based on or based around this big DARPA project, one that's still ongoing

114
00:08:17,440 --> 00:08:21,040
that actually snorkels being used in right now called Memic. So it's this big project

115
00:08:21,040 --> 00:08:24,280
that is run by DARPA around anti-human trafficking.

116
00:08:24,280 --> 00:08:25,280
Okay.

117
00:08:25,280 --> 00:08:28,120
And it's had actually, it's actually in, you know, the hands of law enforcement and making

118
00:08:28,120 --> 00:08:29,920
a really big impact.

119
00:08:29,920 --> 00:08:30,920
And the problem.

120
00:08:30,920 --> 00:08:34,120
So I'm familiar with like the DARPA, like the autonomous vehicles challenge and some

121
00:08:34,120 --> 00:08:36,000
of the others, I'm not familiar with Memic.

122
00:08:36,000 --> 00:08:37,000
So.

123
00:08:37,000 --> 00:08:38,000
Yeah, yeah.

124
00:08:38,000 --> 00:08:44,480
So the Memic challenge, brother has been focused on kind of targeted search trying to extract

125
00:08:44,480 --> 00:08:50,000
information from the dark web and use it to actually aid law enforcement and identifying

126
00:08:50,000 --> 00:08:54,200
individuals that are potentially being trafficked and interceding.

127
00:08:54,200 --> 00:08:59,320
So it's really an incredible project and there's a lot of teams, you know, across both

128
00:08:59,320 --> 00:09:01,080
academia and industry that have contributed.

129
00:09:01,080 --> 00:09:07,840
I know, you know, I wasn't so directly involved, but in our lab, Deep Dive had contributed

130
00:09:07,840 --> 00:09:13,400
around trying to map from what we sometimes call dark data or unstructured data.

131
00:09:13,400 --> 00:09:17,600
So things like websites and just this kind of messy data that's really tough for computers

132
00:09:17,600 --> 00:09:21,920
to deal with and actually pull out structured stuff like imagine pulling out an Excel spreadsheet

133
00:09:21,920 --> 00:09:27,600
with well-defined columns or a graph of entities and their relations from this messy unstructured

134
00:09:27,600 --> 00:09:28,600
data.

135
00:09:28,600 --> 00:09:32,280
And that's a really tough problem that machine learning often does really well at.

136
00:09:32,280 --> 00:09:36,560
But you know, when we tried to do this, it wasn't the, you know, lacking a fancy model

137
00:09:36,560 --> 00:09:37,560
architecture to do this.

138
00:09:37,560 --> 00:09:43,440
So it was really needing training data and, you know, asking, you know, people to look

139
00:09:43,440 --> 00:09:48,600
at these terrible websites for, you know, weeks or months on end to label a training set

140
00:09:48,600 --> 00:09:52,120
and then find out that actually one week later, we don't need that training set anymore.

141
00:09:52,120 --> 00:09:53,120
We need a different training set.

142
00:09:53,120 --> 00:09:54,920
So throw it out and start over again.

143
00:09:54,920 --> 00:09:58,800
You know, that, and this was somewhat surprising because, you know, we had been working on

144
00:09:58,800 --> 00:10:02,400
all these fancy modeling and inference improvements.

145
00:10:02,400 --> 00:10:07,480
That ends up being in that and many other projects that the biggest pain point, the biggest

146
00:10:07,480 --> 00:10:12,600
blocker, right, like when we started moving into snorkel, we would sit down with clinicians

147
00:10:12,600 --> 00:10:16,720
from the Stanford hospital system and, you know, we would be really eager to tell them

148
00:10:16,720 --> 00:10:20,320
about our new fancy model and they'd say, wait, hold up, you know, you said something

149
00:10:20,320 --> 00:10:22,040
about I have to have a large label training set.

150
00:10:22,040 --> 00:10:24,600
I don't have months to sit down and do that.

151
00:10:24,600 --> 00:10:25,600
What gives?

152
00:10:25,600 --> 00:10:27,120
Like, you know, what do I do?

153
00:10:27,120 --> 00:10:33,480
So we realized that all these incredible advances in the models and how they, how easy to

154
00:10:33,480 --> 00:10:38,320
use they were, were coming about in this wave over the last couple of years, but everyone

155
00:10:38,320 --> 00:10:42,040
was blocked on this first step of getting training data for them, right?

156
00:10:42,040 --> 00:10:46,600
You know, and people ignored this for a long time because you had things like ImageNet,

157
00:10:46,600 --> 00:10:50,320
which was a really, you know, incredible resource that jump started.

158
00:10:50,320 --> 00:10:54,320
That and other data sets jump started this, this current, you know, wave of deep learning

159
00:10:54,320 --> 00:10:59,120
progress took several years to create, you know, I know my advisor, Chris was involved

160
00:10:59,120 --> 00:11:03,680
a little bit, you know, downstairs from us was where a lot of this happened.

161
00:11:03,680 --> 00:11:07,720
If you're working on that, that's great, but then you want to take the models that are

162
00:11:07,720 --> 00:11:10,760
doing, you know, you want to take that incredible progress and you want to translate it into

163
00:11:10,760 --> 00:11:14,600
a real world problem where there's no labeled benchmark training set.

164
00:11:14,600 --> 00:11:15,600
It's a huge problem.

165
00:11:15,600 --> 00:11:21,080
So we sort of shifted gears a little bit, you know, deep dive went out into the world

166
00:11:21,080 --> 00:11:27,440
and then, you know, we decided with this new project snorkel to really tackle this problem

167
00:11:27,440 --> 00:11:31,880
of creating training data and rather than viewing training data as this thing that just sort

168
00:11:31,880 --> 00:11:36,320
of existed before you came to the problem, we decided we wanted to make it kind of the

169
00:11:36,320 --> 00:11:40,040
first class citizen of our new framework.

170
00:11:40,040 --> 00:11:44,920
And so in snorkel, really the main activity, you know, snorkel supports arbitrary models

171
00:11:44,920 --> 00:11:50,040
you can plug in your favorite model downloaded from online, plug it into PyTorch or TensorFlow

172
00:11:50,040 --> 00:11:55,240
or whatnot, but really the thing that users do in snorkels, they write these labeling functions.

173
00:11:55,240 --> 00:12:00,840
And the goal is to get subject matter experts to as quickly as possible, dump in all of

174
00:12:00,840 --> 00:12:05,280
the signal and the knowledge that they have in a much kind of higher density way than

175
00:12:05,280 --> 00:12:10,080
if they were just sitting saying yes, no, yes, no, on radiology images for months.

176
00:12:10,080 --> 00:12:15,920
And so you have subject matter experts producing these labeling functions, you know, so you

177
00:12:15,920 --> 00:12:20,760
see any, and so that provides, you know, some degree of abstraction over the labels themselves.

178
00:12:20,760 --> 00:12:21,760
Exactly.

179
00:12:21,760 --> 00:12:22,760
Right.

180
00:12:22,760 --> 00:12:25,240
I can imagine a bunch of different directions you might want to go with that.

181
00:12:25,240 --> 00:12:29,200
Like, are you using these probabilistically, are you like active learning, incorporating

182
00:12:29,200 --> 00:12:32,560
some kind of active, like what's next, what's next, what's next, what's next.

183
00:12:32,560 --> 00:12:33,560
Yeah, that's a great question.

184
00:12:33,560 --> 00:12:37,600
And actually, I mean, there are ongoing projects on kind of like all those and too many other

185
00:12:37,600 --> 00:12:38,600
fronts.

186
00:12:38,600 --> 00:12:39,600
Okay.

187
00:12:39,600 --> 00:12:41,400
This is such a thing is too much fun.

188
00:12:41,400 --> 00:12:45,600
You know, my advisor has a story called the, you know, we call it parable the burritos

189
00:12:45,600 --> 00:12:50,000
that he had a burrito eating contest and then he, you know, finally asks his friend,

190
00:12:50,000 --> 00:12:51,000
why are we eating all these burritos?

191
00:12:51,000 --> 00:12:52,000
What's their reward?

192
00:12:52,000 --> 00:12:53,000
His friend was like, well, it's more burritos.

193
00:12:53,000 --> 00:12:56,440
Kind of how research feels, you know, you work, you work and you work and you work and

194
00:12:56,440 --> 00:12:57,440
well, what's their reward?

195
00:12:57,440 --> 00:12:59,160
You get to do more work.

196
00:12:59,160 --> 00:13:02,000
So it's, you know, you hope that you like the work you're doing, right?

197
00:13:02,000 --> 00:13:05,880
But we have a ton of stuff we're trying to do on, on top of it, you know, I'll go back

198
00:13:05,880 --> 00:13:10,160
to the first thing you said, which is that you have this abstraction away from labels.

199
00:13:10,160 --> 00:13:14,400
And that's one of the things we're most excited about is that the big thing that we're trying

200
00:13:14,400 --> 00:13:21,280
to do with snorkel is turn training data labeling from a hand labeling, you know, hand

201
00:13:21,280 --> 00:13:23,720
annotation activity to a coding one.

202
00:13:23,720 --> 00:13:27,880
And this code is supervision paradigm, this shift, then, you know, we hope and we've seen

203
00:13:27,880 --> 00:13:32,240
with some of our prototypes allows you to use all the benefits of code, different abstraction

204
00:13:32,240 --> 00:13:36,840
layers, modularity, you know, increased interpretability.

205
00:13:36,840 --> 00:13:42,160
So when your training set is 20 labeling functions applied over unlabeled data rather than

206
00:13:42,160 --> 00:13:48,320
a million label data points, if you find out that your modeling goals change, you can

207
00:13:48,320 --> 00:13:51,680
try to change your labeling functions in half an hour rather than having to throw out

208
00:13:51,680 --> 00:13:56,040
your training data and start over again, which is a massive problem in real world deployments

209
00:13:56,040 --> 00:13:58,680
of ML as far as we see.

210
00:13:58,680 --> 00:14:05,360
And so does this apply most directly to a certain type of problem or use case I'm thinking

211
00:14:05,360 --> 00:14:10,440
of, for example, you know, the reason why deep learning has been so effective in the computer

212
00:14:10,440 --> 00:14:17,520
vision realm is because creating, you know, rules for these types of problems like, you

213
00:14:17,520 --> 00:14:19,240
know, where's the cat in this picture?

214
00:14:19,240 --> 00:14:20,480
It's like super hard.

215
00:14:20,480 --> 00:14:21,480
How do you do that?

216
00:14:21,480 --> 00:14:23,120
Well, it's a great question.

217
00:14:23,120 --> 00:14:24,120
It's a great question.

218
00:14:24,120 --> 00:14:28,400
So this, and it really ties in with that notion of like building higher level abstractions

219
00:14:28,400 --> 00:14:29,400
on top.

220
00:14:29,400 --> 00:14:30,720
So we started with text.

221
00:14:30,720 --> 00:14:34,120
We started with these, you know, this like, you know, Memex problem of, okay, we need

222
00:14:34,120 --> 00:14:38,960
to quickly classify our to pull out names of entities or relations, you know, we've

223
00:14:38,960 --> 00:14:42,760
done projects over electronic health records at the VA at Stanford where we want to pull

224
00:14:42,760 --> 00:14:48,000
out mentions of, you know, there was pain in this joint or this area of the body, right?

225
00:14:48,000 --> 00:14:51,320
This is actually kind of messy because language is messy, but you can imagine how you'd write

226
00:14:51,320 --> 00:14:52,560
rules over text, right?

227
00:14:52,560 --> 00:14:57,520
You, you know, you, you give some rules and the model then, you know, learns to generalize

228
00:14:57,520 --> 00:14:58,520
beyond it.

229
00:14:58,520 --> 00:15:01,720
Now images, we've done some interesting stuff.

230
00:15:01,720 --> 00:15:06,640
One thing that we've done, this was a Neurip's paper last year, actually, an extension

231
00:15:06,640 --> 00:15:12,120
called Coral built on top of snorkel where we basically applied a bunch of unsupervised

232
00:15:12,120 --> 00:15:17,280
algorithms to kind of get macro level features of images.

233
00:15:17,280 --> 00:15:21,160
So imagine that, um, and then we had people write labeling functions over these.

234
00:15:21,160 --> 00:15:25,360
So there's a tutorial we have online and all of this, there's a lot of material at Stanford,

235
00:15:25,360 --> 00:15:31,280
uh, snorkel.stanford.edu, um, so this exact demo is there as little toy demo, but imagine

236
00:15:31,280 --> 00:15:35,640
you want to classify, you want to use a, you know, a deep neural network to classify

237
00:15:35,640 --> 00:15:39,680
when people are writing bikes, like you want to do activity detection.

238
00:15:39,680 --> 00:15:44,080
So we just use an off the shelf, uh, bounding box thing to put, you know, bounding boxes

239
00:15:44,080 --> 00:15:48,320
around a person in a bike and then people would write labeling functions that say, okay,

240
00:15:48,320 --> 00:15:53,520
if person is, you know, above bike vertically and centered, then label true else label

241
00:15:53,520 --> 00:15:54,520
false.

242
00:15:54,520 --> 00:15:57,600
So you can write these labeling, and again, this is the abstraction level thing.

243
00:15:57,600 --> 00:16:01,760
You can write labeling functions over these kind of building boxes or if it's mammography,

244
00:16:01,760 --> 00:16:06,080
you know, we could pre tag all the blobs in the image and then the, you know, domain

245
00:16:06,080 --> 00:16:10,000
act, the clinician writes labeling functions about properties of those blobs.

246
00:16:10,000 --> 00:16:11,000
Mm-hmm.

247
00:16:11,000 --> 00:16:12,520
And then of course, this is a pipeline.

248
00:16:12,520 --> 00:16:16,320
So we're just trying to generate training data for some, some things, right?

249
00:16:16,320 --> 00:16:20,280
And then the goal, and this is what we see empirically, is that the deep neural network

250
00:16:20,280 --> 00:16:22,800
will learn to generalize beyond that training data.

251
00:16:22,800 --> 00:16:27,480
So you write these rules, capture some small portion of your data set, and then you use

252
00:16:27,480 --> 00:16:31,520
it to train a model that learns to cover the whole data set.

253
00:16:31,520 --> 00:16:36,960
And so you just, you're just trying to generate training data, are you actually generating

254
00:16:36,960 --> 00:16:43,760
the training data or have you integrated these labeling functions into the training process?

255
00:16:43,760 --> 00:16:44,760
That's a great question.

256
00:16:44,760 --> 00:16:46,960
There's definitely stuff where we want to close the loop.

257
00:16:46,960 --> 00:16:47,960
Yeah.

258
00:16:47,960 --> 00:16:51,680
Um, the basic, you know, the basic setup is that these labeling functions and generating

259
00:16:51,680 --> 00:16:55,600
is maybe a load of term, we're, we're trying to label training data sets.

260
00:16:55,600 --> 00:16:59,880
So what these methods rely on is, or the way that we mostly do is unlabeled data.

261
00:16:59,880 --> 00:17:03,080
To describe another project that I'm quite excited about at the moment, we're collaborating

262
00:17:03,080 --> 00:17:07,120
with the radiology department at Stanford and actually a bunch of teams around Stanford

263
00:17:07,120 --> 00:17:08,120
hospital.

264
00:17:08,120 --> 00:17:13,480
Um, but I'll focus on the radiology applications where we have, they have troves of unlabeled

265
00:17:13,480 --> 00:17:14,480
data.

266
00:17:14,480 --> 00:17:18,440
And what the sum label data looks like is generally comes in pairs of an image and then

267
00:17:18,440 --> 00:17:21,720
a text report that the doctor wrote, right?

268
00:17:21,720 --> 00:17:25,680
And so you can think of that text report as like kind of a messy, jumbled version of

269
00:17:25,680 --> 00:17:27,880
the label that you actually want.

270
00:17:27,880 --> 00:17:32,080
And so there were efforts, which are really cool, where, you know, I think there was one

271
00:17:32,080 --> 00:17:36,360
where they took, you know, a couple of years and we actually have a paper out in radiology

272
00:17:36,360 --> 00:17:42,040
from someone in our lab on using this labeling effort that took years of, you know, radiologist

273
00:17:42,040 --> 00:17:44,240
sitting and labeling data.

274
00:17:44,240 --> 00:17:48,800
With snorkelway access question, could we have a radiology fellow spend a week or two,

275
00:17:48,800 --> 00:17:53,440
write these labeling functions over those text reports and generate labels that were almost

276
00:17:53,440 --> 00:17:55,160
as good.

277
00:17:55,160 --> 00:18:00,080
And then if we piped in 10x the amount of unlabeled data, could we even do better?

278
00:18:00,080 --> 00:18:01,080
Mm-hmm.

279
00:18:01,080 --> 00:18:04,320
You know, this is this intuition, which is an old one in ML of like, well, maybe a larger

280
00:18:04,320 --> 00:18:08,520
amount of noisier data can be a smaller amount of ground truth data, especially if we use

281
00:18:08,520 --> 00:18:12,840
a technique like snorkel that kind of, you know, learns to reweight and combine the labeling

282
00:18:12,840 --> 00:18:15,400
functions, kind of denoises it automatically.

283
00:18:15,400 --> 00:18:19,320
And so the answer was yes, it's actually, you know, we just dump in as much unlabeled data

284
00:18:19,320 --> 00:18:23,720
we can get our hands on, we apply the labeling functions and dump it into snorkel, which

285
00:18:23,720 --> 00:18:26,600
kind of reweights and recombines them.

286
00:18:26,600 --> 00:18:31,160
And then you get a better training set that can be used almost as effectively or sometimes

287
00:18:31,160 --> 00:18:34,880
even more effectively than that hand labeled training set that would have taken months

288
00:18:34,880 --> 00:18:35,880
or years.

289
00:18:35,880 --> 00:18:36,880
Okay.

290
00:18:36,880 --> 00:18:43,120
So we've talked about defining these labeling functions and it sounds like the most

291
00:18:43,120 --> 00:18:49,520
basic uses to use these labeling functions to label data, but it's not a straight application

292
00:18:49,520 --> 00:18:50,520
of this labeling function.

293
00:18:50,520 --> 00:18:54,240
And there's this reweating that you've alluded to a couple of times, what exactly is happening

294
00:18:54,240 --> 00:18:55,240
there?

295
00:18:55,240 --> 00:18:56,240
Yeah, great question.

296
00:18:56,240 --> 00:19:01,440
And that's a part that we really like to get deep into the math on that front.

297
00:19:01,440 --> 00:19:06,840
So this was, I mean, I guess the first paper was at NURPS in 2016, it was called data programming.

298
00:19:06,840 --> 00:19:11,640
So we have a method which, you know, it's based on old intuition.

299
00:19:11,640 --> 00:19:17,640
So if you, you know, the intuition is this, is that if you have a bunch of sources, you

300
00:19:17,640 --> 00:19:22,320
have a bunch of, say, a bunch of crowd lablers and you don't have any ground truth, but

301
00:19:22,320 --> 00:19:24,480
you want to know who to trust.

302
00:19:24,480 --> 00:19:30,440
You can start under some assumptions and just say, okay, say all that they're all independent.

303
00:19:30,440 --> 00:19:32,320
None of them are colluding with each other.

304
00:19:32,320 --> 00:19:35,400
Just say, okay, well, I'm going to trust the ones that are in the majority more often.

305
00:19:35,400 --> 00:19:36,400
Sure.

306
00:19:36,400 --> 00:19:39,480
And if one of the crowd lablers is almost always in the minority, I'm going to discount

307
00:19:39,480 --> 00:19:40,480
them.

308
00:19:40,480 --> 00:19:41,480
Maybe think that they're adversarial.

309
00:19:41,480 --> 00:19:44,440
Whereas if someone is always in the majority on every data point, I'm going to trust

310
00:19:44,440 --> 00:19:45,440
them more.

311
00:19:45,440 --> 00:19:46,440
Okay.

312
00:19:46,440 --> 00:19:47,440
Yeah.

313
00:19:47,440 --> 00:19:48,440
Yeah.

314
00:19:48,440 --> 00:19:51,800
This is, you know, a big area of work in the past and crowd sourcing.

315
00:19:51,800 --> 00:19:58,040
So in our NURPS 2016 paper, what we were doing was kind of extending this to this more,

316
00:19:58,040 --> 00:20:03,520
this kind of more tangled setting of having functions rather than people because these

317
00:20:03,520 --> 00:20:06,680
functions can have all kinds of weird correlations, right?

318
00:20:06,680 --> 00:20:10,080
You know, two people we've had this happen before, two people can write two labeling functions

319
00:20:10,080 --> 00:20:14,000
that are near duplicates of each other or exact duplicates.

320
00:20:14,000 --> 00:20:15,920
And you don't want to double count their votes.

321
00:20:15,920 --> 00:20:19,400
You could have whole clumps of labeling functions that are using the same kinds of patterns

322
00:20:19,400 --> 00:20:21,120
or the same data resources.

323
00:20:21,120 --> 00:20:22,120
It's code, right?

324
00:20:22,120 --> 00:20:24,360
Code can have arbitrary weird correlations and overlaps.

325
00:20:24,360 --> 00:20:31,240
So is it fundamental to the model or the way you envision the model being used that for

326
00:20:31,240 --> 00:20:38,960
a given problem, you'd have multiple subject matter expertise, SMEs kind of creating labeling

327
00:20:38,960 --> 00:20:40,280
functions.

328
00:20:40,280 --> 00:20:43,440
It's almost like instead of crowd sourcing labels, you're crowd sourcing labeling functions.

329
00:20:43,440 --> 00:20:44,440
Yeah.

330
00:20:44,440 --> 00:20:46,880
We're actually doing a study on that idea like meta crowd sourcing that we actually

331
00:20:46,880 --> 00:20:52,960
had a workshop a couple of weeks ago at Stanford where we had 15 teams come and they were

332
00:20:52,960 --> 00:20:57,600
doing stuff around text extraction from in the bio domain.

333
00:20:57,600 --> 00:21:02,840
And we actually have now post hoc collected all 15, like the labeling functions from all

334
00:21:02,840 --> 00:21:07,760
15 teams and actually does better if you just dump them in all and all and together.

335
00:21:07,760 --> 00:21:11,760
But you know, practically, it's usually, you know, with a lot of our engagements, it's

336
00:21:11,760 --> 00:21:13,600
just one subject matter expert.

337
00:21:13,600 --> 00:21:16,760
But what is essential is that they write more than one labeling function, right?

338
00:21:16,760 --> 00:21:19,200
So they usually write for the same problem.

339
00:21:19,200 --> 00:21:20,200
For the same problem.

340
00:21:20,200 --> 00:21:21,200
Yeah.

341
00:21:21,200 --> 00:21:24,160
So, you know, we'll have the, in that example, I gave about, you know, classifying chest

342
00:21:24,160 --> 00:21:29,640
x-rays, for example, the radiology fellow wrote 20 labeling functions.

343
00:21:29,640 --> 00:21:33,200
And they were all, you know, they were a mix of things like looking for certain words

344
00:21:33,200 --> 00:21:37,920
or matching against certain ontologies of diseases or checking how many times the word

345
00:21:37,920 --> 00:21:40,800
normal is said, all these kinds of messy heuristics.

346
00:21:40,800 --> 00:21:47,640
And so, in a sense, you're, you know, it's that you're trying to apply, you know, what

347
00:21:47,640 --> 00:21:54,840
you consider to be like, it's just a good programming, like a single labeling function has

348
00:21:54,840 --> 00:21:59,080
like conceptual boundaries, it's trying to generate a label based on this as opposed

349
00:21:59,080 --> 00:22:04,720
to some, you know, single mango labeling function that takes in everything you know about

350
00:22:04,720 --> 00:22:07,320
the data set and tries to label it.

351
00:22:07,320 --> 00:22:11,200
And that is, that is such a great example because that speaks exactly the kind of our motivation

352
00:22:11,200 --> 00:22:17,240
in that, you know, we, you know, I was kind of glossing over a little bit like we didn't

353
00:22:17,240 --> 00:22:20,720
just go from, okay, people only hand labeled to now we're doing what we didn't snorkel.

354
00:22:20,720 --> 00:22:26,320
We saw that people were trying these, what's often called weaker ways of supervising models

355
00:22:26,320 --> 00:22:28,160
where they were writing code.

356
00:22:28,160 --> 00:22:32,040
But the big pain point there, including our lab with, with the previous system D type

357
00:22:32,040 --> 00:22:36,840
and the problem there was that exactly if you try to make one big mango program to produce

358
00:22:36,840 --> 00:22:40,640
the labels, this becomes the same kind of spaghetti code that you were trying to avoid in the

359
00:22:40,640 --> 00:22:43,760
first place by using a machine learning model.

360
00:22:43,760 --> 00:22:47,760
So the idea of these labeling functions is yes, you're, you're just writing little snippets

361
00:22:47,760 --> 00:22:51,480
and you're leaving it up to snorkel to figure out how to wait them, how to combine them.

362
00:22:51,480 --> 00:22:55,000
You don't have to sit there, again, you know, it's not magic, it doesn't work always

363
00:22:55,000 --> 00:22:58,640
right there, you know, rather than if I have two labeling functions and I want, you know,

364
00:22:58,640 --> 00:23:01,320
I rather than sitting there and being like, well, which one do I trust more?

365
00:23:01,320 --> 00:23:06,160
Or one example is say you have one labeling function that, you know, labels 10,000 points

366
00:23:06,160 --> 00:23:10,760
and you trust it way more and one that labels a million points and you trust it less.

367
00:23:10,760 --> 00:23:13,320
You know, how do you combine, what's the right way to combine them?

368
00:23:13,320 --> 00:23:14,320
Right.

369
00:23:14,320 --> 00:23:15,320
You don't need to worry about that.

370
00:23:15,320 --> 00:23:17,360
You just dump it into snorkel and snorkel waits them accordingly.

371
00:23:17,360 --> 00:23:21,480
Or you have 10 labeling functions and you don't, you don't want to like sit there thinking,

372
00:23:21,480 --> 00:23:24,640
okay, which one should override which one and which combination?

373
00:23:24,640 --> 00:23:26,680
You just dump it into snorkel.

374
00:23:26,680 --> 00:23:27,680
It's interesting.

375
00:23:27,680 --> 00:23:33,600
So I did an interview last week, I think, with Rich Zemel at the University of Toronto

376
00:23:33,600 --> 00:23:43,040
and he's doing some work on fairness and one of his papers is about a system that is trying

377
00:23:43,040 --> 00:23:51,400
to create an unbiased system by having the system refer decisions to kind of human participants

378
00:23:51,400 --> 00:23:57,320
in the system but start to learn their biases and refer things to them based on those biases.

379
00:23:57,320 --> 00:24:03,440
It almost sounds like there's some application of that here where the system can learn which

380
00:24:03,440 --> 00:24:10,000
of the functions work best given a certain type of input data and then dynamically use

381
00:24:10,000 --> 00:24:11,000
these.

382
00:24:11,000 --> 00:24:12,000
Yes.

383
00:24:12,000 --> 00:24:18,280
I mean, no, that's, your questions are uncannily good in this, you're also, you're also asking

384
00:24:18,280 --> 00:24:23,720
all about projects that were currently underway, so it's almost like my advisor paid you.

385
00:24:23,720 --> 00:24:25,560
This is one thing that we're very excited about.

386
00:24:25,560 --> 00:24:29,360
So the base thing, so I'm sure also, I haven't read that particular paper, but it sounds

387
00:24:29,360 --> 00:24:34,240
like I'm quite sure there are lots of parallels in the ideas and stuff.

388
00:24:34,240 --> 00:24:38,680
And in our setting, we start by learning an accuracy for every labeling function, right?

389
00:24:38,680 --> 00:24:45,560
And again, it's that we use some matrix completion style techniques in 2016, we're using

390
00:24:45,560 --> 00:24:51,800
Sephiron Gibbs sampling, we have our various fancy ways of doing it, but the intuition

391
00:24:51,800 --> 00:24:59,160
is that you try to learn the accuracy based on trusting the majority and having some

392
00:24:59,160 --> 00:25:03,480
assumption that they're kind of independent or learning which ones aren't, which ones

393
00:25:03,480 --> 00:25:04,920
are correlated with each other.

394
00:25:04,920 --> 00:25:08,760
But we're increasingly moving into this area where we are learning actually like biases

395
00:25:08,760 --> 00:25:10,040
that are conditioned on data.

396
00:25:10,040 --> 00:25:14,240
So you can learn that, you know, this labeling function is much better for daytime conditions

397
00:25:14,240 --> 00:25:16,160
than nighttime conditions, right?

398
00:25:16,160 --> 00:25:18,360
That's where it's kind of expertise is.

399
00:25:18,360 --> 00:25:22,240
So that's exactly something that we're, you know, playing around with right now.

400
00:25:22,240 --> 00:25:23,240
Okay.

401
00:25:23,240 --> 00:25:30,400
And I would imagine that some of these biases could be explicit, like the SME could

402
00:25:30,400 --> 00:25:38,040
consciously say, oh, if I was looking at this, you know, this radiological image, I guess

403
00:25:38,040 --> 00:25:41,400
that's not a great example, but if I was looking at this picture of a person on a bicycle

404
00:25:41,400 --> 00:25:46,280
and it was dark, you know, I'd look for reflectors, but then there are these implicit biases

405
00:25:46,280 --> 00:25:49,040
that you just kind of, it's better to learn.

406
00:25:49,040 --> 00:25:53,360
Anything that's simple for an SME to write down in code or even, and I should mention one

407
00:25:53,360 --> 00:25:56,360
of the other things we're doing going back to that point of abstraction layers, I guess

408
00:25:56,360 --> 00:26:01,200
I'll briefly go on that tangent, which is that, you know, one of the things we're really

409
00:26:01,200 --> 00:26:04,960
excited about is kind of building up this programming stack.

410
00:26:04,960 --> 00:26:08,000
So you know, if you think of a traditional programming stack, you go from like, you know,

411
00:26:08,000 --> 00:26:11,800
machine language all the way to increasingly higher and higher and more declarative levels

412
00:26:11,800 --> 00:26:14,840
until you get to like, I know, voice commands or GUIs, right?

413
00:26:14,840 --> 00:26:15,840
Yeah.

414
00:26:15,840 --> 00:26:17,120
We're trying to think about the same thing with training data.

415
00:26:17,120 --> 00:26:19,880
The training data is the sort of machine code that's like the common thing you compile

416
00:26:19,880 --> 00:26:20,880
to.

417
00:26:20,880 --> 00:26:24,080
Labeling functions are like the lowest level thing you can program in.

418
00:26:24,080 --> 00:26:25,680
How can you go higher and higher, right?

419
00:26:25,680 --> 00:26:26,680
So that's interesting.

420
00:26:26,680 --> 00:26:27,680
Yeah.

421
00:26:27,680 --> 00:26:30,280
And it's, I mean, a lot of these are still, you know, because they're kind of prototype

422
00:26:30,280 --> 00:26:35,080
level, but like, my lab made a presented a paper called, it's a great name, better than

423
00:26:35,080 --> 00:26:42,360
snorkeling and babble level at ACL, which is one of the big NLP conferences this year,

424
00:26:42,360 --> 00:26:46,920
where the idea is that you give explanations for why you're labeling data points.

425
00:26:46,920 --> 00:26:48,080
And these are compiled.

426
00:26:48,080 --> 00:26:52,440
They're parsed automatically into labeling functions and then dumped into snorkel.

427
00:26:52,440 --> 00:27:00,160
So we want to make the SME provides explanations as to why they're labeling manuals.

428
00:27:00,160 --> 00:27:02,960
Why they would label something, they don't actually have to do the manual, they're just

429
00:27:02,960 --> 00:27:06,720
looking at examples and they're giving explanations and those are then parsed using

430
00:27:06,720 --> 00:27:11,600
something called a semantic parser into labeling functions, which are, of course, are super

431
00:27:11,600 --> 00:27:14,480
noisy, but that's what snorkel is meant to deal with, right?

432
00:27:14,480 --> 00:27:19,480
So whatever the level that the SME can provide this stuff, the goal of snorkel is just to,

433
00:27:19,480 --> 00:27:23,320
you know, whatever they can provide, take it and whatever is too laborious to provide,

434
00:27:23,320 --> 00:27:28,720
like those exact conditions or those exact sort of if-then clauses, just kind of learn

435
00:27:28,720 --> 00:27:29,720
that, right?

436
00:27:29,720 --> 00:27:31,080
Kind of fill that in.

437
00:27:31,080 --> 00:27:32,080
Yeah.

438
00:27:32,080 --> 00:27:35,040
And another practical tool there is that these labeling functions kind of stain.

439
00:27:35,040 --> 00:27:40,360
So, you know, if you know the labeling function is really good for daytime, but not nighttime,

440
00:27:40,360 --> 00:27:44,400
and you have a way to express that, then you can just write a labeling function that says,

441
00:27:44,400 --> 00:27:49,960
if daytime, you know, vote on my, you know, output-alabel else, just abstain.

442
00:27:49,960 --> 00:27:50,960
Right, right.

443
00:27:50,960 --> 00:27:57,440
Yeah, that was another feature of this rich, simple paper that I was referring to, the,

444
00:27:57,440 --> 00:28:01,080
one of the things that they did with their models that it could say yes, nor pass, and kind

445
00:28:01,080 --> 00:28:05,280
of defer to another model or in their case of human.

446
00:28:05,280 --> 00:28:09,040
And we know in our setting, we find that this, you know, this abstention is actually like

447
00:28:09,040 --> 00:28:13,360
a very, very critical tool in practice, this ability to say, you know, I pass, or I don't

448
00:28:13,360 --> 00:28:14,360
know.

449
00:28:14,360 --> 00:28:18,360
And in their case, I imagine in your case, it's, you know, when I first heard that, I was

450
00:28:18,360 --> 00:28:22,840
like, okay, well, okay, if it's, you know, below a threshold of 50% confidence or something

451
00:28:22,840 --> 00:28:23,840
like that, you abstain.

452
00:28:23,840 --> 00:28:25,680
But it was way more nuanced than that.

453
00:28:25,680 --> 00:28:28,680
I imagine that it's similarly nuanced in your case.

454
00:28:28,680 --> 00:28:33,360
Yeah, it's, it's so much because the, the, the place where you pass on or the place

455
00:28:33,360 --> 00:28:35,720
where you abstain on does have some bias to it.

456
00:28:35,720 --> 00:28:39,320
It's where, you know, where the expert thinks this heuristic doesn't really apply, I guess.

457
00:28:39,320 --> 00:28:40,320
Yeah.

458
00:28:40,320 --> 00:28:43,560
And it's a, you know, it's a rough version of that.

459
00:28:43,560 --> 00:28:44,560
Right.

460
00:28:44,560 --> 00:28:45,560
Right.

461
00:28:45,560 --> 00:28:49,800
And again, you know, our goal here is to speed up in a human-in-the-loop process.

462
00:28:49,800 --> 00:28:54,240
And to fundamentally bring it to the level of coding rather than labeling, not to obviate

463
00:28:54,240 --> 00:28:55,240
it.

464
00:28:55,240 --> 00:28:56,240
Right.

465
00:28:56,240 --> 00:28:58,520
So, you know, people have to, you know, people iterate on these labeling functions.

466
00:28:58,520 --> 00:29:04,200
But this ends up taking, you know, days or a week or two, not months or years.

467
00:29:04,200 --> 00:29:08,440
And then it can be repurposed or reapplied when your next problem comes along.

468
00:29:08,440 --> 00:29:16,920
So this conversation about programming levels of abstraction and, and functions in particular

469
00:29:16,920 --> 00:29:22,400
is kind of pulling me towards like an infrastructure path and thinking of like serverless.

470
00:29:22,400 --> 00:29:23,800
Is that mean anything to you?

471
00:29:23,800 --> 00:29:27,400
Like lambda functions, AWS lambda and that kind of thing.

472
00:29:27,400 --> 00:29:28,400
Yeah.

473
00:29:28,400 --> 00:29:30,000
I wonder if there's an intersection here somewhere out of it.

474
00:29:30,000 --> 00:29:34,280
It's not obvious to me what it would be, rather than that functions are the primary currency

475
00:29:34,280 --> 00:29:35,280
of both systems.

476
00:29:35,280 --> 00:29:36,280
Yeah.

477
00:29:36,280 --> 00:29:41,120
There's a lot of interesting stuff we want to do, I don't have any directions we've explored

478
00:29:41,120 --> 00:29:42,120
around that.

479
00:29:42,120 --> 00:29:43,200
I mean, it's certainly an interesting area.

480
00:29:43,200 --> 00:29:45,960
We haven't tried that out with Snorkel yet.

481
00:29:45,960 --> 00:29:49,840
I will say this is, you know, a bit of a right-hand turn.

482
00:29:49,840 --> 00:29:53,040
But something I think is important is that, or that we think is important, is that Snorkel

483
00:29:53,040 --> 00:29:57,440
is very much what we think of as a system's ML type contribution, right?

484
00:29:57,440 --> 00:30:01,400
Like, you know, one of the initial papers or the initial paper was at NUREPS in 2016.

485
00:30:01,400 --> 00:30:02,600
It was about the algorithm.

486
00:30:02,600 --> 00:30:08,040
But the real contribution that we're excited about here is really a merger of sort of,

487
00:30:08,040 --> 00:30:12,680
you know, machine learning algorithms and theory, but with systems and framework constraints.

488
00:30:12,680 --> 00:30:16,600
And that's something that I, you know, a lot of us feel pretty excited about.

489
00:30:16,600 --> 00:30:21,560
We're actually, you know, there's a PC meeting for this new CISML conference here, you

490
00:30:21,560 --> 00:30:24,280
know, dovetailed on with NUREPS.

491
00:30:24,280 --> 00:30:25,800
And this is a conference.

492
00:30:25,800 --> 00:30:28,640
It's, I mean, unfortunately, the call for submission already happens.

493
00:30:28,640 --> 00:30:29,640
I can't advertise it.

494
00:30:29,640 --> 00:30:34,680
I can't say it's here, but, you know, at NUREPS, at the workshops, there are actually two

495
00:30:34,680 --> 00:30:39,520
different systems ML conferences, systems ML and ML for systems.

496
00:30:39,520 --> 00:30:45,280
And then there's this CISML conference happening at Stanford in the spring.

497
00:30:45,280 --> 00:30:47,040
And, you know, those of us involved things that-

498
00:30:47,040 --> 00:30:48,600
Separate from the scaled ML?

499
00:30:48,600 --> 00:30:49,600
Yes, separate, yeah.

500
00:30:49,600 --> 00:30:50,600
Oh, really?

501
00:30:50,600 --> 00:30:51,600
Okay.

502
00:30:51,600 --> 00:30:52,600
And I think a little bit of a broader umbrella.

503
00:30:52,600 --> 00:30:53,600
Okay.

504
00:30:53,600 --> 00:30:55,960
We're going to be releasing a white paper soon on sort of what we see the scope as.

505
00:30:55,960 --> 00:30:58,440
But I would kind of pitch it this way.

506
00:30:58,440 --> 00:31:02,840
There's so many fundamental core problems in core ML that still need to be solved.

507
00:31:02,840 --> 00:31:07,880
They're almost more so because, you know, deep learning came in and made such a splash

508
00:31:07,880 --> 00:31:13,200
and such an empirical impact on certain areas, but we still understand it so, so poorly.

509
00:31:13,200 --> 00:31:16,720
And, you know, we need to work on so many problems around that that are really fascinating.

510
00:31:16,720 --> 00:31:20,640
But it's kind of had an impact enough that a lot of us are saying, look, it's time to

511
00:31:20,640 --> 00:31:22,880
build the engineering infrastructure around it.

512
00:31:22,880 --> 00:31:23,880
Right?

513
00:31:23,880 --> 00:31:24,880
Right?

514
00:31:24,880 --> 00:31:29,400
It's valuable enough that people want to use it and they can't- and it's not because

515
00:31:29,400 --> 00:31:31,600
they don't understand- they don't have a fancine of algorithms.

516
00:31:31,600 --> 00:31:32,600
It reaches the core.

517
00:31:32,600 --> 00:31:33,600
Yeah.

518
00:31:33,600 --> 00:31:34,600
Yeah.

519
00:31:34,600 --> 00:31:39,120
So, you know, I- I think this was, you know, shot down by some of my core organizers.

520
00:31:39,120 --> 00:31:41,320
But I like the metaphor of like a sandwich almost.

521
00:31:41,320 --> 00:31:43,520
Like, you know, the core is the ML stuff.

522
00:31:43,520 --> 00:31:47,320
But, you know, on the bottom you have the, you know, you have the lower level concerns

523
00:31:47,320 --> 00:31:51,920
of how do you- how do you build the hardware for modern ML, you know, algorithms?

524
00:31:51,920 --> 00:31:53,440
How do you- how do you serve it?

525
00:31:53,440 --> 00:31:55,720
How do you set up the distributed systems?

526
00:31:55,720 --> 00:31:59,440
On the top level you have- how do you, you know, put together- put it together in end-to-end

527
00:31:59,440 --> 00:32:00,760
workflows?

528
00:32:00,760 --> 00:32:05,440
Things like snorkel or things for data preprocessing or interpretability or serving or monitoring,

529
00:32:05,440 --> 00:32:06,440
right?

530
00:32:06,440 --> 00:32:07,440
Right.

531
00:32:07,440 --> 00:32:12,040
So, you have these sort of types of considerations that that's not kind of what NERF specializes

532
00:32:12,040 --> 00:32:13,360
in, usually.

533
00:32:13,360 --> 00:32:17,920
And we think there's an exciting area to really fill out these engineering or systems aspects

534
00:32:17,920 --> 00:32:18,920
of ML.

535
00:32:18,920 --> 00:32:22,360
So, that's definitely kind of where we see a lot of the interesting questions around snorkel.

536
00:32:22,360 --> 00:32:23,360
I mean, yeah.

537
00:32:23,360 --> 00:32:27,680
This is the ML algorithm side, which we love too, but also these like, you know, we haven't

538
00:32:27,680 --> 00:32:30,600
dealt with a serverless question yet, but those kinds of questions.

539
00:32:30,600 --> 00:32:39,440
So, we started talking about the way that it kind of chooses between these labeling functions.

540
00:32:39,440 --> 00:32:42,000
And you mentioned that it's evolved.

541
00:32:42,000 --> 00:32:44,760
You started with Gibbs sampling and some other things.

542
00:32:44,760 --> 00:32:49,360
And- but it's an area that, you know, folks enjoy- folks on a team enjoy kind of geeking

543
00:32:49,360 --> 00:32:53,320
out about, like, can you go into some more detail on what exactly is happening there?

544
00:32:53,320 --> 00:32:54,320
Yeah, yeah.

545
00:32:54,320 --> 00:32:59,320
So, you know, I'd say that, again, you know, if you're into this area, one of the things

546
00:32:59,320 --> 00:33:04,920
that we're quite excited about is this handling correlations between the labeling functions.

547
00:33:04,920 --> 00:33:10,120
So, if you think about this as a modeling problem, you know, you could kind of- we call it

548
00:33:10,120 --> 00:33:12,120
weekly supervised.

549
00:33:12,120 --> 00:33:16,880
And this is one of the, you know, areas or little sub-communities that we're quite excited

550
00:33:16,880 --> 00:33:17,880
about.

551
00:33:17,880 --> 00:33:22,280
Ransom workshop to actually at last NERFs and at IClear coming up this year on week supervision.

552
00:33:22,280 --> 00:33:27,040
This idea of noisier or cheaper or higher level supervision.

553
00:33:27,040 --> 00:33:30,360
But the core model in Snarkle that learns the accuracy of the labeling functions, you

554
00:33:30,360 --> 00:33:35,440
could think almost of unsupervised in that we don't have any ground truth labels.

555
00:33:35,440 --> 00:33:38,840
But either way, what it looks like is you have- what you observe is you observe the votes

556
00:33:38,840 --> 00:33:40,360
of these labeling functions.

557
00:33:40,360 --> 00:33:44,480
What you don't observe is this latent variable of the ground truth.

558
00:33:44,480 --> 00:33:48,760
So it fits into this tradition of latent variable models, but in this new kind of way where

559
00:33:48,760 --> 00:33:52,560
these labeling functions can also be correlated with each other.

560
00:33:52,560 --> 00:33:57,360
And so we've done work on, you know, if you know the correlations, if your SME user says,

561
00:33:57,360 --> 00:34:00,240
look, these two labeling functions are basically the same.

562
00:34:00,240 --> 00:34:03,360
I just tweaked the number or I tweaked the threshold.

563
00:34:03,360 --> 00:34:07,200
And then our system can take that into account and learn the model knowing to expect that

564
00:34:07,200 --> 00:34:12,320
these two labeling functions are very correlated that we shouldn't double count their votes basically.

565
00:34:12,320 --> 00:34:13,320
We've also done work.

566
00:34:13,320 --> 00:34:17,560
There is an ICML 2017 paper and all this is up at snarkle.sanford.edu.

567
00:34:17,560 --> 00:34:24,480
Just before you get to the next paper, when you're talking about kind of these correlations,

568
00:34:24,480 --> 00:34:34,320
you've got these functions that are originally at least provided in the Python domain, the programming domain.

569
00:34:34,320 --> 00:34:39,960
Are you kind of projecting them into like a linear algebra domain?

570
00:34:39,960 --> 00:34:41,320
Yeah, so it makes sense to you.

571
00:34:41,320 --> 00:34:45,360
I think, yeah, I mean, that's a definitely a great way of putting it.

572
00:34:45,360 --> 00:34:52,600
I mean, but concretely, just to keep it simple, what we're doing is we're just taking their labels

573
00:34:52,600 --> 00:34:55,160
and then we basically just leave them as black boxes.

574
00:34:55,160 --> 00:34:58,880
Right, so you've got some transformation from some input to the label.

575
00:34:58,880 --> 00:35:00,400
We have a bunch of functions.

576
00:35:00,400 --> 00:35:01,800
We have a bunch of unlabeled data.

577
00:35:01,800 --> 00:35:04,000
That's another critical ingredient.

578
00:35:04,000 --> 00:35:05,480
Apply the functions to the data.

579
00:35:05,480 --> 00:35:08,400
And then we have a basically a matrix of noisy labels.

580
00:35:08,400 --> 00:35:09,800
Yeah, and that's what we work with.

581
00:35:09,800 --> 00:35:16,880
Although we have done one of the past nerfs papers last year was looking at what if we actually

582
00:35:16,880 --> 00:35:18,920
use information in these functions.

583
00:35:18,920 --> 00:35:20,960
So we can actually do static analysis on them.

584
00:35:20,960 --> 00:35:21,960
They're not black boxes.

585
00:35:21,960 --> 00:35:25,560
This is another interesting facet of our kind of unique facet of our setting.

586
00:35:25,560 --> 00:35:27,480
They're not black boxes, right?

587
00:35:27,480 --> 00:35:31,840
I can have a really pretty simple static analysis program that looks at two labeling functions

588
00:35:31,840 --> 00:35:36,720
and says, hey, they're nearly identical except that one number was tweaked.

589
00:35:36,720 --> 00:35:39,600
Therefore, I should model them as correlated.

590
00:35:39,600 --> 00:35:42,200
So we can open that black box.

591
00:35:42,200 --> 00:35:46,560
But at the end of the day, we just have this matrix of noisy labels.

592
00:35:46,560 --> 00:35:51,680
And we just need to know how to re-weight them according to the different sources they

593
00:35:51,680 --> 00:35:54,200
came, the different labeling functions they came from.

594
00:35:54,200 --> 00:36:00,920
Do you apply traditional PCA or things that dimensionality reduction or things that are

595
00:36:00,920 --> 00:36:07,720
trying to find these correlations or some of the things that you've mentioned, I guess

596
00:36:07,720 --> 00:36:14,720
are their unique aspects of this noisy matrix that make other techniques better than

597
00:36:14,720 --> 00:36:17,920
the more generic ways of finding these correlations?

598
00:36:17,920 --> 00:36:18,920
Yeah, yeah.

599
00:36:18,920 --> 00:36:22,400
So, I mean, that's a great intuition as well, and some of the techniques we're looking

600
00:36:22,400 --> 00:36:28,040
into are connections to a sign known as robust PCA for learning the structure.

601
00:36:28,040 --> 00:36:35,200
The ICML paper I mentioned from last year on learning the structure was building on classic

602
00:36:35,200 --> 00:36:39,840
techniques and probabilistic graphical models for learning the structure of the models,

603
00:36:39,840 --> 00:36:42,800
except our setting, we don't have ground truth labels.

604
00:36:42,800 --> 00:36:47,440
So that kind of fundament, that's like the fundamental change here is that we're missing

605
00:36:47,440 --> 00:36:48,440
the ground truth.

606
00:36:48,440 --> 00:36:53,080
And so then we show that we can still learn the structure from data.

607
00:36:53,080 --> 00:36:57,720
And so, you know, again, you either learn the structure or you're given it, and then

608
00:36:57,720 --> 00:36:59,600
you need to take it into account in the modeling.

609
00:36:59,600 --> 00:37:03,080
You don't want to effectively, if we go back to that majority vote intuition, you don't

610
00:37:03,080 --> 00:37:07,480
want to double count two labeling functions that are basically the same in Roots, and

611
00:37:07,480 --> 00:37:10,880
are always going to give the same answer, that'll throw your model off.

612
00:37:10,880 --> 00:37:15,240
And our motivation, once again, is really grounded in developer process.

613
00:37:15,240 --> 00:37:18,440
We've seen failure modes happen according to this.

614
00:37:18,440 --> 00:37:26,400
If you want to have three developers, or if you want to, we just post online about a

615
00:37:26,400 --> 00:37:28,560
report of some of our collaborations with Teams at Google.

616
00:37:28,560 --> 00:37:34,080
So you have a huge scale where you have Teams of multiple people working on these labeling

617
00:37:34,080 --> 00:37:36,440
functions, and they're all different types.

618
00:37:36,440 --> 00:37:39,880
You want to be able to just dump them in and not worry about if some of them are too correlated

619
00:37:39,880 --> 00:37:40,880
or not.

620
00:37:40,880 --> 00:37:44,320
You want to have the system take care of that for you, not because it's going to quadruple

621
00:37:44,320 --> 00:37:48,480
performance necessarily, but because it's going to avoid catastrophic failure modes.

622
00:37:48,480 --> 00:37:53,800
And so that kind of like increasing robustness in a human and the loop driven process is really

623
00:37:53,800 --> 00:37:55,360
our kind of motivation.

624
00:37:55,360 --> 00:38:00,880
You mentioned you just posted this paper or summary of some work done with Google.

625
00:38:00,880 --> 00:38:02,200
What can you share about that?

626
00:38:02,200 --> 00:38:09,440
Yeah, I mean, so at a high level, the interesting kind of academic thread there, I would say,

627
00:38:09,440 --> 00:38:17,800
is that we refer to this sort of how do you leverage organization scale resources?

628
00:38:17,800 --> 00:38:20,920
So I can talk high level about some of the ideas I think are interesting there, and the

629
00:38:20,920 --> 00:38:22,320
details are on our archive.

630
00:38:22,320 --> 00:38:30,600
But one aspect is this notion that if you go to any organization, or many organizations

631
00:38:30,600 --> 00:38:36,080
these days are dealing with this ML question.

632
00:38:36,080 --> 00:38:42,120
We have all these data resources, a lot of companies or labs or whatever, a lot of organizations

633
00:38:42,120 --> 00:38:48,040
they have troves of rules that their chunks of code that their experts wrote from before.

634
00:38:48,040 --> 00:38:52,480
They may have messy labels that are kind of outdated or noisy, but are sitting around.

635
00:38:52,480 --> 00:38:56,880
They may have even other classifiers that are too brittle to really be the solution,

636
00:38:56,880 --> 00:38:58,800
but have some signal.

637
00:38:58,800 --> 00:39:02,600
And then on the other end, you have these shiny new deep learning models, or they could

638
00:39:02,600 --> 00:39:09,480
be complex models of any sort, random, you know, forest, XG boost, whatever it is.

639
00:39:09,480 --> 00:39:11,680
And the question is, how do you bridge that gap there?

640
00:39:11,680 --> 00:39:14,600
And a lot of organizations are just told, okay, we'll throw out the old stuff, that's

641
00:39:14,600 --> 00:39:15,600
useless.

642
00:39:15,600 --> 00:39:17,760
It's legacy.

643
00:39:17,760 --> 00:39:23,000
And an enormous amount of resources, hand labelling training data, and then jump onto the new

644
00:39:23,000 --> 00:39:24,200
train.

645
00:39:24,200 --> 00:39:29,720
And I think what we showed, at least in some cases in this collaboration, was that you

646
00:39:29,720 --> 00:39:34,160
really can bridge the two using techniques like Snarkle, where you take that and use it

647
00:39:34,160 --> 00:39:38,600
as a weak supervision for training these modern models.

648
00:39:38,600 --> 00:39:43,280
And one of the other cool aspects I'll briefly mention there is this notion of going

649
00:39:43,280 --> 00:39:46,520
from non-servable to servable models.

650
00:39:46,520 --> 00:39:51,880
So you can write this weak supervision of these labeling functions over data that you

651
00:39:51,880 --> 00:39:53,400
don't want to serve in production.

652
00:39:53,400 --> 00:39:54,920
It could be like aggregate statistics.

653
00:39:54,920 --> 00:40:00,720
It might be models that are expensive to run, knowledge graphs, stuff that you can't

654
00:40:00,720 --> 00:40:02,800
serve efficiently in production.

655
00:40:02,800 --> 00:40:07,560
You could use that to train a model that can then run over, you know, cheap, public,

656
00:40:07,560 --> 00:40:08,880
servable features.

657
00:40:08,880 --> 00:40:13,280
And so this seemed to be another aspect of this kind of pipeline that is very useful.

658
00:40:13,280 --> 00:40:15,320
That's the one thing we learned.

659
00:40:15,320 --> 00:40:16,320
Interesting.

660
00:40:16,320 --> 00:40:17,320
Interesting.

661
00:40:17,320 --> 00:40:19,320
Awesome.

662
00:40:19,320 --> 00:40:20,320
What's next?

663
00:40:20,320 --> 00:40:27,600
Well, it sounds like you've got a ton of threads that are already kind of spun up about,

664
00:40:27,600 --> 00:40:30,960
you know, pushing this reach our research in different directions.

665
00:40:30,960 --> 00:40:31,960
Yeah.

666
00:40:31,960 --> 00:40:32,960
It's Snarkle.

667
00:40:32,960 --> 00:40:33,960
Is it open source?

668
00:40:33,960 --> 00:40:35,960
I can people play with it.

669
00:40:35,960 --> 00:40:36,960
Yeah.

670
00:40:36,960 --> 00:40:37,960
For sure.

671
00:40:37,960 --> 00:40:38,960
And, you know, we love feedback.

672
00:40:38,960 --> 00:40:43,200
Proversely, like many academics, we like negative feedback even more than positive at

673
00:40:43,200 --> 00:40:44,200
this point.

674
00:40:44,200 --> 00:40:48,800
We've been fortunate to have people have, you know, wins with Snarkle.

675
00:40:48,800 --> 00:40:53,480
We're very interested in, you know, interesting failure modes or requests for features and

676
00:40:53,480 --> 00:40:54,480
anything.

677
00:40:54,480 --> 00:40:57,000
So it's all in line at snarkle.stanford.edu.

678
00:40:57,000 --> 00:40:58,400
We have tutorials.

679
00:40:58,400 --> 00:41:01,720
We have blog posts, links to all papers and stuff.

680
00:41:01,720 --> 00:41:02,720
Okay.

681
00:41:02,720 --> 00:41:06,920
And again, you know, it's great when people, you know, find things that they have questions

682
00:41:06,920 --> 00:41:09,800
or they say, oh, could be used, could it be used in this setting or it didn't work in

683
00:41:09,800 --> 00:41:10,800
this setting?

684
00:41:10,800 --> 00:41:13,360
I think I have an interesting reason why we love that stuff.

685
00:41:13,360 --> 00:41:18,680
So, and then in terms of what's next, I mean, so there's a whole bunch of stuff.

686
00:41:18,680 --> 00:41:24,000
One thing that we're working on, it's actually in a separate repot just while it's in beta

687
00:41:24,000 --> 00:41:27,480
for now, which is this version of snarkle called snarkle metal.

688
00:41:27,480 --> 00:41:30,280
And it's supposed to be the multitask version of snarkle.

689
00:41:30,280 --> 00:41:35,440
So this is an idea, basically, people are getting excited again about this idea, they

690
00:41:35,440 --> 00:41:37,680
called multitask learning.

691
00:41:37,680 --> 00:41:39,480
This is an idea from back in the 90s.

692
00:41:39,480 --> 00:41:42,880
It's just, you know, if you have multiple, and it's something that's an old theme in

693
00:41:42,880 --> 00:41:43,880
the area, right?

694
00:41:43,880 --> 00:41:49,480
If you have multiple things you're trying to do, just like how humans learn, right?

695
00:41:49,480 --> 00:41:51,840
What if you, you know, kind of do it all jointly?

696
00:41:51,840 --> 00:41:55,440
You share the learned representation between these tasks.

697
00:41:55,440 --> 00:41:59,240
And so there's kind of been a resurgence of interest in these techniques, you know, kind

698
00:41:59,240 --> 00:42:03,120
of in the realm of these new architectures, right?

699
00:42:03,120 --> 00:42:05,480
And often what these look like is actually kind of conceptually simple.

700
00:42:05,480 --> 00:42:07,440
You are the base of the vanilla version.

701
00:42:07,440 --> 00:42:10,040
Now, you have some deep neural network.

702
00:42:10,040 --> 00:42:14,320
And you have the bottom layers are all shared across K different tasks.

703
00:42:14,320 --> 00:42:19,480
And then you kind of split off at the end and learn these like little top task specific

704
00:42:19,480 --> 00:42:22,080
bits.

705
00:42:22,080 --> 00:42:27,720
But you benefit from kind of sharing a representation of the world or of the data between all these

706
00:42:27,720 --> 00:42:28,720
different tasks.

707
00:42:28,720 --> 00:42:35,600
So what really we're really interested in exploring this from the perspective of weak supervision.

708
00:42:35,600 --> 00:42:40,320
And you know, one problem that we kind of detailed in a paper that's going to be presented

709
00:42:40,320 --> 00:42:44,880
this year at AAAI was, okay, how do we deal with the weak supervision from multiple

710
00:42:44,880 --> 00:42:45,880
tasks?

711
00:42:45,880 --> 00:42:49,920
People are writing labeling functions for K different tasks that, you know, have different

712
00:42:49,920 --> 00:42:52,200
kinds of relationships to each other.

713
00:42:52,200 --> 00:42:53,360
Like imagine a hierarchical thing.

714
00:42:53,360 --> 00:43:01,080
Like I have some labeling functions that say, you know, this is a lawyer versus a doctor.

715
00:43:01,080 --> 00:43:05,640
This is a bank versus a hospital and then some that say person or organization.

716
00:43:05,640 --> 00:43:07,040
So you have different levels of granularity.

717
00:43:07,040 --> 00:43:08,440
That's one example, right?

718
00:43:08,440 --> 00:43:12,120
How can we have people dump them all in just like as in snorkel, but now in this multi-task

719
00:43:12,120 --> 00:43:13,240
setting and handle it?

720
00:43:13,240 --> 00:43:15,280
So that was one extension we worked on.

721
00:43:15,280 --> 00:43:20,760
But moving forwards, we really think that, you know, most of the multi-task learning

722
00:43:20,760 --> 00:43:24,880
work, which has been really exciting to date, has been around a couple of hand labeled

723
00:43:24,880 --> 00:43:25,880
datasets.

724
00:43:25,880 --> 00:43:27,640
They're kind of static.

725
00:43:27,640 --> 00:43:31,480
We think that as people begin to move towards these weak supervision methods, the number

726
00:43:31,480 --> 00:43:35,480
of datasets is going to explode and you're going to have now these sort of massively multi-task

727
00:43:35,480 --> 00:43:41,240
models where, you know, rather than saying, oh, look, we can do better on these five different

728
00:43:41,240 --> 00:43:44,600
datasets, you know, that are there.

729
00:43:44,600 --> 00:43:49,480
You're going to have tens or hundreds of, you know, training sets for tens or hundreds

730
00:43:49,480 --> 00:43:52,760
of tasks that are all weakly supervised, they're all changing.

731
00:43:52,760 --> 00:43:53,760
Okay.

732
00:43:53,760 --> 00:43:56,600
And the question of how you manage that all in one big model, I think is fascinating.

733
00:43:56,600 --> 00:44:01,160
So there's the kind of the modeling aspect of that multi-task, but also what is that

734
00:44:01,160 --> 00:44:05,600
training data come from and these labeling functions could help contribute to that as

735
00:44:05,600 --> 00:44:06,600
well?

736
00:44:06,600 --> 00:44:07,600
Exactly.

737
00:44:07,600 --> 00:44:08,600
Yeah.

738
00:44:08,600 --> 00:44:09,600
Yeah.

739
00:44:09,600 --> 00:44:10,600
And I think that that's also, again, on the system side, what's really interesting is,

740
00:44:10,600 --> 00:44:16,320
how do you, you know, if you, if you have people, again, it's just a big shift, right?

741
00:44:16,320 --> 00:44:21,640
Right now, you know, training data is, a new training data set takes, you know, months

742
00:44:21,640 --> 00:44:24,960
minimum to put online for a real world problem, right?

743
00:44:24,960 --> 00:44:30,600
And a training set can be created in hours or days, and you're going to have tens or

744
00:44:30,600 --> 00:44:31,600
hundreds of them.

745
00:44:31,600 --> 00:44:32,600
How do you manage that?

746
00:44:32,600 --> 00:44:33,600
Right.

747
00:44:33,600 --> 00:44:36,720
And so that's, again, we think there are all these interesting questions around organizational

748
00:44:36,720 --> 00:44:40,280
level management of new training sets that are weakly supervised.

749
00:44:40,280 --> 00:44:41,520
How do you put them all together?

750
00:44:41,520 --> 00:44:44,000
How do you amortize costs across them?

751
00:44:44,000 --> 00:44:48,520
We think this, you know, MTL line of thinking is one answer to that, along with our existing

752
00:44:48,520 --> 00:44:49,520
work on snorkel.

753
00:44:49,520 --> 00:44:53,240
Well, Alex, thanks so much for taking the time to chat with me about this stuff.

754
00:44:53,240 --> 00:44:55,080
Thank you so much for having me on the show.

755
00:44:55,080 --> 00:44:56,080
Great stuff.

756
00:44:56,080 --> 00:44:57,080
Very fun.

757
00:44:57,080 --> 00:44:58,080
Thank you.

758
00:44:58,080 --> 00:44:59,080
Awesome.

759
00:44:59,080 --> 00:45:00,080
Thanks so much.

760
00:45:00,080 --> 00:45:01,080
All right, everyone.

761
00:45:01,080 --> 00:45:03,480
That's our show for today.

762
00:45:03,480 --> 00:45:08,360
For more information about today's guests, or to follow along with our AI Platform's Volume

763
00:45:08,360 --> 00:45:14,480
2 series, visit twimmelai.com slash AI Platforms2.

764
00:45:14,480 --> 00:45:19,680
Thanks once again to SIGUP for their sponsorship of this series and support of the show.

765
00:45:19,680 --> 00:45:24,840
To check out what they're up to and take advantage of their exclusive offer for Twimmel listeners,

766
00:45:24,840 --> 00:45:28,680
visit twimmelai.com slash SIGUP.

767
00:45:28,680 --> 00:45:58,640
As always, thanks so much for listening and catch you next time.

