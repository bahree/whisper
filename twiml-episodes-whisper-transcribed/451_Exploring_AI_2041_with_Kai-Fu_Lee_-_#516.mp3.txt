All right, everyone. I am here with Kaifu Lee. Kaifu is chairman and CEO of Innovation Ventures,
the former president of Google China, an author of the New York Times bestseller AI Superpowers.
And we're here to talk about his new book, which will be released next week, AI2041.
Kaifu, welcome to the Twomo AI podcast. Thank you. Thanks, Sam. It is great to have an
opportunity to speak with you. I'm looking forward to digging in and talking more about the book.
Before we do, though, I'd love to have you share a little bit about your background and
how you came to work in the field of AI. Sure.
I started with my excitement in AI back in 1979 when I started my undergraduate at Columbia,
and I worked on natural language and vision at Columbia, and then I went to Carnegie Mellon for
my PhD, at which I developed the first speaker independent speech recognition system,
based on machine learning, actually, one of the earlier thesis in machine learning in 1988.
I also developed a computer program that beat the world's off-the-all champion. It's all in the 80s,
very early years. After my graduation from CMU, I taught there for two years, then I joined Apple,
and led a lot of apples, AI, speech, natural language, and multimedia efforts. Later, I joined SGI,
and then Microsoft, where I started the Microsoft Research Asia in Beijing in 1998,
which kind of became one of the best AI research labs in Asia. Later, I joined Google,
and ran Google China for four years between 2005 and 2009. We did do a little bit for Hanei,
but mostly it was really developing Google's presence in China. In 2009, I left Google and started
my venture capital firm, Sinovation Ventures. At Sinovation Ventures, we invested in about 40 AI
companies. We were about the earliest and probably invested in the most companies. We invested in
about seven unicorns in AI alone, and with a few more yet to come. So, very excited to be in the
era of AI. It was not so hot during much of my career, but I'm glad to be able to catch the
recent wave and participate in it. Fantastic. Fantastic. So, let's maybe jump into the book.
The title is AI2041. If you just read that, having heard nothing of the book, you might think that
it's kind of a straight-up, you know, your vision for AI in 2041. But, and to some degree,
that is the case, you're asking interesting questions on that time horizon, but there's a little
bit of a twist. Tell us about that twist and the way the book is, you know, organized.
Sure. The twist is, we, I call this book, Scientific Fiction, because I collaborated with a
science fiction writer who wrote most of the book, probably three quarters. And they are 10 stories,
we call it, 10 visions of the future. I find that the impact of AI is misunderstood by a lot
of people. Some are too conservative, others are too optimistic, and others are just naive,
and some explanation, I think, could be helpful. AI will change our future, and more people need
to understand it. And having a fictional writer, right, in terms of stories, will make it all
more accessible to people. So, the book is organized in 10 stories, of which takes a place in a
different country, and in a different industry. So, we can see how AI will impact all countries
and all industries. And then, after each story, I write an analysis of the technologies embedded
in the chapter, how they will progress, and what challenges they may bring, and how we might
solve them, or what we should do now, to deal with the externalities or potential challenges that
they bring about. So, it's 10 stories going from relatively simple uses of AI to extremely
challenging and somewhat futuristic uses of AI, but in the whole set of 10 stories, I try to
at least have a high degree of confidence, like 80% or more, that this would work in the 10 to 20
year timeframe. You mentioned that when you talk to people, you get a range of
reactions or perspectives on AI ranging from very conservative to over optimistic. A lot of that
has to do with the time horizon that you're thinking about. You chose 20 years as the kind of central
time horizon for this book. Why is that? Because on the one hand, 20 years is pretty long. A lot
can happen in 20 years. 20 years ago, we didn't have the iPhone or the mobile internet, and look how
things have changed. So, imagine 20 years ago, if someone were to write AI 2021, it would be pretty
interesting and fantastic 20 years ago if it accurately described today. So, it's long enough,
futuristic enough, exciting enough, but not so long that we could hand wave and say, you know,
bring download as possible and we become cyborgs or we're doing teleportation or time travel,
so we stay away from that. And also, I factor in that the time it takes to develop the research
to perfect it, to reduce the cost, to implement it, to productize it, to make it acceptable to the
market, and also to deal with potential legal regulatory and accountability issues. So, it's not,
so some of the stories may look like, hey, we could almost do that today, but there are a lot of
other issues that we're coming to play. Sure, sure. I think one of the things that challenges
folks the most on this, you know, conservative optimistic from a mass perspective is autonomous
driving. Do you have a story in the book that talks about autonomous driving? Yes, yes, of course,
can't write the book in 20 years without it. Of course, by then, L5 will have worked.
It's kind of in transition, so I think that describes my view is that L5 is quite challenging,
and the path towards L5, as I describe in the story, will be incremental. As we know,
AI gets better with more deployment, with more data, with more learning, so basically L5 will be a
series of increasingly more challenging environments, perhaps starting with fixed routes, like buses,
and then trucks on highways, and then more and more cities, and that's kind of one path
as more data experience is gathered. It will face still a lot of challenges, even in 20 years,
and one of the predictions I make is that cities will have to modify some existing road infrastructure,
for example, to separate very dangerous cross sections with pedestrians and cars, so that there's
no risk of a car hitting a pedestrian in the most likely environments and crossroads in downtowns,
such as happened with the Uber Autonomous Vehicle in Phoenix, and also roads can be smart and
essentially work symbiotically with autonomous vehicles. Also, I predict that there will still be
environments in which AI will be lost and need a backup driver. Yet, we will need cars that
have no steering wheel, and really no place for a driver, so they can be smaller and
simpler, talk to each other, and even avoiding accidents as they communicate their location and speed,
and if you have a blown tire, you will tell cars around you, so I envision all of these will happen,
but one part of it that my partner Stanley who wrote the science fiction stories thought was
interesting was what would be the life of a backup driver in that case, because if the car got
into a natural disaster where the roads have disappeared, and you have to fall back on natural
instincts of a human driver to survive and navigate how they incredibly dangerous zone,
and obviously the passenger couldn't do it, there's no longer a steering wheel,
so the solution would have to be a remote center where super drivers jump in from one disaster to
another saving people's lives, and then the other interesting dramatic element is well what happens
to the life of such a backup driver, would it create so much stress that they can't live with
themselves because they will be watching people die from day to day, and so how would such an
arrangement be made, so without giving away the story that's kind of the dramatic element and
a technical element weaved into a story. That last note about the drivers and their welfare and
their life, even though we're talking about a scenario 20 years from now that calls to mind
the lives of folks that are working in like content moderation farms and centers today that are
dealing with those kinds of issues, so I imagine that part of what you're trying to do is to
point to future issues but also tie them to contemporary issues as well.
And also there are other interesting dramatic elements, for example, how do you recruit such
a amazing drivers, so part of the story is games are developed and then winners of these games
sometimes teenagers would be approached to see if they would be a backup driver, but of course
that's too much psychological burden for a young teenager to be put into the position of
helping and saving lives, so is it morally a problem to package a real job saving people's lives
as a game and not disclose it to the teenager who happens to be the best backup driver that
one can find, so we're saving lives to the purpose but can you lie to a teenager who's known
to be saving lives but also not fair to put a psychological burden for them to know they're
saving and not saving lives every day and do you tell them or don't you tell them it's a moral dilemma.
And do you answer those questions or do you just raise them?
We just raise them but I think the endings of the stories would give away how we feel
but we don't want to but I think it's an issue where reasonable people can and will disagree
so we don't presume that we know the right answer but I think we need to be aware such challenges
will come up and the book is probably for the people watching this podcast the book is less
about learning about technologies because you probably know most of what I have to say
but but thinking ahead about the externalities and implications that are up ahead and what we
technologists can possibly do about it to educate people and also to develop solutions.
One more question on the autonomous driving scenario you mentioned that you fully expect and
that the story presumes level 5 autonomy is in existence in 20 years does the the book or your
analysis project a degree of deployment or the degree to which it is in use at that time?
Yes yes I think the presumption is that in developed countries it's already popularly in use
and in countries that are proactively changing its transportation ecosystem it gets deployed
earlier and that's part of the technological prediction and hypothesis it's also predicting
that developing countries and underdeveloped countries would need the help of developed countries
to put this technology into place so this story takes place in Sri Lanka and in the story Sri
Lanka gets help from a Chinese technological company that is building a business out of helping
developed countries move from not having autonomous to autonomous and I think part of the
implication is that large countries will continue to have more advanced AI to put into other
countries and another assumption is the world will move towards autonomy one tier at a time
and that I feel first tier of countries may have it in the 15 year time frame with other countries
coming later 20 years plus and Sri Lanka was chosen at the place because not all the roads are
yet quite ready for autonomy with some very backward environments because it would not be
reasonable to put that scenario in US or China where by then I think the infrastructure as well
as the technology would perhaps have a very more minimal use of backup drivers by 20 years
you know it's hard to talk about AI in the future without raising the question of jobs and job
displacement it's probably one of the you know issue of one of the issues he was around which
there's the most concern when talking about the future use of AI do you take that up in you know
one particular story in the book or is this something that cuts across various stories
it cuts across all the stories there are probably three stories in which this covers
the one that's squarely on the topic is a story called the job savior and it's a story that
takes place in the US by watching phases and phases of routine jobs being taken over by AI a new
profession arises called a job reallocator and it's a company that would be funded either out
of government funding instead of paying social security or universal basic income the company
would take out take the funding and basically solve the problem of retraining and redeploying
workers whose jobs are being displaced by AI and this company faces some significant challenges
one is that AI is improving capability so more and more routine jobs are being lost people are
being retrained the three years later losing job again another major challenge that it faces
is that many entry jobs are being hollowed out because AI can do jobs of an entry level accountant
entry level architect entry level reporter but how do you advance people's careers and maintain
their motivation to grow and learn without having entry level jobs so it brings up the possibility
of creating a virtual job in which the person thinks he or she is working but perhaps is only
training is more like a training wheels not creating value to the economy but the training
will will help point out people's individual talents so they can be read that redirected later
so again a potentially a moral question of is it okay to let people work jobs which are not
real or maybe are real but could be done by AI and then how would the job reallocator deal with
these challenges so that's the more direct one there is another one on education is how would AI
be evolved so that it helps young people of home their soft skills skills like communication
teamwork and human to human interaction as well as train their creativity and critical thinking
which become all the more important because those are the skills AI cannot replace and also find
the voice of each individual person so that's kind of related to routine jobs being displaced
so people either have to find something AI cannot do or do things that only humans can do or find
something the individual is good at so that's another story a third story has to do with human
motivation it's it's more of a utopian outcome where so much money is being generated in an era
of plenitude where not only does AI do our much of the routine work for us but also the energy
costs have come down with green energy new materials the cost of goods are reduced and the
meaning of money needs to evolve so it asks the question the work isn't the work and money isn't
just a something to keep us busy but it's kind of people's motivation and and reason for living
so if jobs are largely gone routine jobs are largely gone how do people remain motivated so I think
it pushes the question into why do we live in this world perhaps it isn't just for work and pay
but also for for self-actualization finding what our lives are about and can those be measured
somehow can AI somehow measure whether we are improving ourselves where we're happily growing
where we're finding where we're creating more positive energy because as you move up the
mass law hierarchy it's not just about subsistence and not just about money for security but also
about love and empathy and companionship so can those things be measured and can people have
some kind of metric to improve a different metric than money and a different metric by job by
spending their time perhaps in sustainability volunteer jobs companionship as well as all the
creative professional jobs and it's an exploration of how that could develop in in the market in the
country in this case Australia which is doing a pretty good job in in energy efficient energy
that it might create enough of a small enough population to create and a lot of natural resources
to create a the first science the first economy of where universal basic income
plenitude and the moving people the higher purpose might be explored as an experiment kind of the
gamification of life purpose in a sense that's right I mean our life is a gamification now I mean
money is a virtual it's a silly virtual tool that keeps us you know in the rat race when it's
it's you know it's a fabricated human story and we're playing a big game now in chasing fame
and wealth so I think we need to find another which is perhaps more motivating it's interesting
you're hearing you talk about these these stories and reflecting on the other ones the book kind of
walks this line between you know presenting these potentially dystopic scenarios kind of like
black mirror-esque but you know trying to pull out I think trying to pull out an optimistic note
and for the most part you know to talk more about your your broad perspective on the book
are you you know are you kind of going into these stories looking specifically for the the
optimistic ending or you know does it vary do you have kind of different takes on where we'll go
for different scenarios yeah I'm a huge fan of black mirror and if you know if the book reviews
with find this book to be similar to the black mirror but more positive I there's nothing that would
please me more I think the black mirror does a great job describing possible dangers and they
usually end up with bad endings sometimes good ending so this so this is our effort to try to
also describe the challenges that could arise and but also I want to go an extra step to say
there could be a solution if only right if only we educated our kids differently if only we
regulated large companies in particular ways if only we thought ahead about job displacement
and provide the training if we deeply understand the meaning of money and how we can gradually
move towards a substitute so and and I think probably six stories or so have a happy ending and then
three have an ambiguous ending and one has a somewhat bad ending so I'm not I'm not being naive
to say all the problems will be hand-waved away and solved so yeah really their challenges yeah
yeah I think there's there are challenges with both the dystopic ending and the the utopian ending
the optimistic ending and I think you almost want a you know not quite a choose your own adventure
type of a story but a story that you know where there's a branch that says what the dystopic ending
could look like and what are some of the levers that could put you in down that path and also
presents the optimistic ending and you know what are some of the levers that would kind of drive
society towards an optimistic perspective do you do you take on any of that in your analysis of
the the various scenarios I do so so in the chapter about autonomous weapons it's a story it's
an issue that I think a lot of people in the AI community feel the same way that the physics
community felt about nuclear weapons and the chemistry community felt about chemical weapons
and so on so it is the the the clearest challenge that we face today because the cost of making
an autonomous drone with face recognition that can kill an individual as an automated assassin
that has so many dangers because it lowers the cost of it for the terrorists without having
to risk the terrorists lives and also it's very hard to to regulate because it's not like nuclear
weapons they're the good and bad thing about nuclear weapons is there is a principle of a short
mutual destruction so people have a deterrent countries don't do it because they're afraid of
retaliation and mutual destruction but that isn't the case with autonomous weapons
so so the one of the stories is about a terrorist model after the
unabomber and who decided to take revenge on a particular group of people in this case the elites
of the world as this model after the the unabomber so unabomber sorry so so the story ends up with
challenges of what happens when autonomous weapons are not regulated and the outcome is
somewhat negative one and the world isn't destroyed but it's still a somewhat negative one he
gets away with something of course and and of course he's caught but he creates causes a lot of
damage so in the explanation section I go into detail explaining the additional challenges
of autonomous weapons compared to conventional weapons compared to you know as well as the
nuclear weapons and why they really have to be regulated and what happens when you don't
regulated and and then I point out the challenges of regulating it because unlike nuclear weapons you
can't have UN going to a country and inspect uranium or nuclear facilities because someone can
build this in their garage but nevertheless regulation must take place and I point out a few
possible ways of regulating it as well as most AI scientists believe that it should be
regulated several letters have been written and and also the consequences of not not regulating it
so so I think that describes the both both possible paths of a negative outcome and that's an
example where both paths are are explored. In a section like that where you're talking about
something that's you know very clear and present danger do you is there a concrete call
the action for folks is that part of your perspective here to tell folks how to
you know if this is the issue that they're most passionate about where do they go?
Yeah no it's not a direct call to action but I think it hopefully removes all ambiguity
arguments have been made that autonomous weapons are in the early phases of development
so it's too early to regulate them and I give counter example on why that isn't the case
and also there are huge issues about how difficult it is to regulate it but I also point out
that we mankind have managed to largely control and contain chemical weapons and biological
weapons which are potentially equally difficult to to track and regulate so if we can do those
we should be able to do this one so I think people can draw their own conclusions some are
probably perhaps still not convinced by the argument but I wanted to to make the argument.
I've often had these exchanges with folks where you know they kind of present this scenario of
you know conscious AI that is belligerent in some way kind of like Terminator type of example
and you know or like a you know a Nick Bostrom superintelligence that's potentially you know
dangerous and you know often we'll say you know that is something that is potentially out there
in the future but you know autonomous weapons are much more kind of clear and closer and
concrete and scary for me personally than you know some AI that's you know acting on its own
against human interest. Yeah absolutely I totally agree that's why by absence there is no
singularity story in the book there is no AI with self-consciousness self-awareness that tries to
destroy the human race in the story so by its absence I'm totally agreeing with that in the
analysis section I do bring up the issue of singularity of why I don't rule out its possibility
in the future but I think it's too simplistic to say the exponential growth of compute power
also means an exponential growth that will drop the people behind in many of the stories we
still see many parts of the human intelligence that cannot be replicated by AI the fact that
many stories are saved by the saved by the hero or heroine of the story because of their
emotional and beliefs and conviction and love and that's something unique to people and
and also in stories where there are villains who do terrible things they're the ones who
cause the disaster using AI as a tool AI never in these 10 stories become the villain in itself
and and and then I clearly explained that singularity could happen when breakthroughs in algorithms
enable a fully taking advantage of the exponential growth but today we still have not had breakthroughs
that understand how our brain works why we have self-awareness and emotion and creativity
and can do analysis and strategic thinking so to think we can replicate AI on something that we
don't even know how we do it nor do we see AI approaching it and we know AI do not possess it
so we we don't have to worry about extrapolating the exponential curve and seeing super
intelligence and or singularity within the next 20 years that said I do believe the set of
things that AI can do better than human will grow dramatically and AI will do many things that
humans cannot imagine to do but there will always be a set that is about our core humanity
at least in a 20 year time frame that we can hold on to and it's exactly that set that defines
our humanity that causes are the stories the people in the stories to shine and save the day.
Speaking of the exponential growth in compute one of the stories touches on quantum computing
what you take on where that is in 20 years and the degree to which it enables a more powerful
artificial intelligence. Yeah I think quantum is one of the areas where I needed to make a
not 100% confident prediction right because there's too much understanding and variability
but but I do think looking at the maps that IBM, Google and other companies have
and the progress that's been made particularly in the last two years it seems like we can
extrapolate a story where the improvements in logical cubits will reach thousands probably
less than 20 years there are a lot of issues of how do you maintain stability and how many physical
cubits do you need to support logical a few thousand logical cubits so I'm not an expert in the
area but the experts seem to agree several thousand cubits are possible and there are useful
applications by that time with the major one being insecurity that is the existing asymmetrical
cryptography algorithms will no longer work the flip side of that is quantum computing will provide
a new unbreakable security system so in the story I did not go into how quantum and AI work
together because at a few thousand cubits I don't think it's enough to disrupt AI completely yet
so 20 years would be about when the security challenges would come up so in one of the stories
the villain achieved 4,000 cubits without anyone else knowing it and the villain went after
stealing bitcoins which is one commonly described the largest bank that's waiting to be robbed
so that was a part of the story but in the analysis I do go into the very nature of quantum
that can hold uncertainty in its head and pursue paths in parallel and dramatically reducing
the MP complete search problem will lead to a day where AI algorithms will be disrupted
but I don't think 20 years is quite when that will happen will probably take longer with more than
4,000 cubits so the book of course is focused on this 20 year you know 20 year forward time horizon
but there are a lot of AI technologies around which there are very contemporary issues in the
realm of ethics bias and others computer vision is one that comes to mind facial recognition in
particular it's a very contemporary issue the use of facial recognition by police organizations
the proliferation of cameras in you know quote unquote smart cities you know a lot of people
look at the you know it would be easy to look at the situation now and the frustration that many
people have with the situation now and find it difficult to project forward 20 years do you do that
in the book I do not facial recognition is one I did not speculate because we're in a bifurcated
world some countries are attempting to regulate others are not and it's it's not clear a bifurcated
world can work hopefully will reach a universal consensus at some point I do go into many other
aspects of externalities and I guess you can extrapolate from them to to all of the possibilities
so for example I talk about how objective functions need to be improved to go from
maniacally focusing on something like clickthroughs and revenue generating moving into longer term
metrics so our social media in 20 years ought to be showing us content that is making us better
over time that that we feel we're seeing content that is time well spent as Tristan Harris would
say or we're seeing content that is making us improving in some metric maybe it's our wealth
maybe it's our happiness maybe it's our how much knowledge we've gained and whether AI
objective functions can be turned more long term and more aligned with humans that's one aspect
I explored and I think technologists should spend more time on topics like that another is on
bias and fairness can we ensure that have tools that ensure that AI is being trained reasonably
balanced data so that it's not discriminating against any race gender individual etc.
And and also can can compilers alert warnings right now can AI tools do the same
and also can AI engineers be trained to be aware of the substantial power that they control
and therefore the responsibilities that must come with it so that's another aspect another one
related to privacy and and and and having our cake and eat it too can we have AI trained on a lot
of data but not everybody giving away data privately without consent so the stories in the book
talks about how technologies like federated learning homomorphic encryption and also
hardware environments that are self-contained where where data does not leak can these take these
technologies I predict in 20 years we'll be able to let us have our cake and eat it too
so that our data stays in devices to which we permit say our phone our computer or the computers
at the hospital which has our data but not beyond that so the models from a hospital is trained
on all the patients in that hospital who licensed their data to the hospital but not beyond
then the hospitals can jointly train by pulling their models together so so in the book I
point out the technological areas that I think are promising and the possible technological
solutions that could end up addressing many of the problems we see and I think the call to action
is for the technologists who read the book and watch the podcast to think about whether rather than
doing research on the next deep learning or tweaking a particular model is it useful for a sufficient
percentage of the AI community to think about these technological solutions that solve the problems
caused by our technology AI. On that note there are a number of labs focused on AI
safety as a research focus you know often they take the perspective of you know trying to
prevent the terminator scenario or making sure that we can control the terminator scenario.
Do you have a perspective on on those efforts are they asking the right questions looking at
the right things? Well there are risks that are clear and present danger I think those ought to
be addressed by the largest number of people issues related to bias fairness how to have our
cake in either two with respect to personal data there are a longer term lower likelihood
existential questions that one could ask and I think it makes perfect sense for a small number
of people in more like a think tank than a technology development to basically watch
for that possibility and to alert for the rest of us so yes I do think those labs should
continue to do what they do I don't think those existential threats will happen in the next 20
years but I think we should have think tanks that think about them and tell us when we really do
need to get involved and worried. In the chapter on autonomous weapons you call for regulation
you mentioned the objective function and you know there are many contemporary calls for regulation
of internet companies and advertising methods and you know privacy and many other things
for internet companies but how do you see regulation evolving over the next 20 years?
Yeah I think in the US people talk the most about breaking up companies I think that is
a two brute force and two you know 20th century it's not something designed for this kind of
monopoly I do think regulations are needed but I think we need to come up with newer and better
regulations and why do you think that's an effective for modern companies? Well let's say
Facebook got broken up into whatsapp.com and instagram.com and facebook.com it doesn't stop any of it
it wouldn't have stopped the Cambridge Analytica issue right it wouldn't stop any one of the three
products doing things that we don't want them to do it would reduce it but it's it's too brute force
it was specifically addressing monopoly extension by you know standard oil moving into gas gas
into gas stations and stop yeah and having the big bell company break broken up into baby bells
I think those were perhaps appropriate for telecommunications or traditional industries
but I think the issue that is fundamental is I think people the reason people go into such extreme
measures is they've given up hope that some companies can self-manage I've not given up hope
but I don't think today's reward and punishment systems give the large companies any incentive
to self-manage and those need to be created for example I think an idea called the AI audit
is something that could be pursued right it's very clear that the government can't go in and
and look at the code and data for each of the large internet companies but when there are
sufficiently serious complaints and repetitions of complaints there can be an audit just like
there can be a financial audit or a tax audit so with that as a deterrent I think companies
can be better behaved of course what are the metrics how does a complaint count should the
government get to look at the data in a large company these are all issues that need to be solved
but it seems more I think a more plausible way than just breaking up companies and more effective
another I think ultimately we need to get companies to really have aligned financial
incentives so that if they better behave for example can there be a third party watchdog
that publishes how much fake news how much you know false advertising how much wrong search results
or whatever things we have if we have a third party watchdog that publishes those and
enough consumer advocacy and a corporate ESG pressure for the companies to feel like
every quarter they have to report not just the financial results but how they do on the
you know a fake news metric fake news ranking then they're going to form internal teams
because they're being monitored from the outside so that would be one example but the ideally
we want to somehow have companies that can make even more money by aligning themselves with
user needs so as users become happier or learn more information or become wealthier or whatever
those metrics and it can somehow be attributed to companies that have created or helped
enable that situation then it can make more it can even make even more money so
so in other words are we willing to pay a company a lot more money to make us
more knowledgeable wealthier or happier in a three year horizon compared to the money that
the company would make us clicking and buying things so looking at natural financially aligned
metrics that connect the user and the the company this is a little bit abstract at right now
I was going to ask you say destiny such metrics in the book I suspect answers now
no well it's such like you know 30 years ago would people have come up with the ways that
you know Facebook does advertising or Google does at words or at sense some smart entrepreneur
will come up with some system that will create a new ecosystem that will create a new set of
companies that are even more profitable than Google and Facebook unfortunately I don't I don't
know the answer but entrepreneurs and VCs can take a step back and think about it it's not so much
out of a question right because how do you how do we measure people's happiness well there are
many metrics that can be used on our facial expressions micro expressions measures of our hormones
endorphin etc that could be one beginning of such a way our our wealth can be measured over time
and whether we've learned something and grown I think just like you know gpt3 today can remember
millions of words that it's read and pick out the ones that are relevant for the given current
context perhaps there will be AI that can look at all of our time spent on the internet and pick out
the epiphany moments that have caused us to grow and and and those moments if there is a software
technology or objective function that enable the moment they should be properly compensated for it
so I don't think it's out of the question um the technologies can be developed but I don't know
what the model is if I did I'd be either funding or creating that company myself I was just
going to ask was that part of your motivation for writing the book to kind of signal to
entrepreneurs that hey these are areas that need to be explored and if you are working in these
areas hey reach out to reach out to me that's not the primary purpose but if that were a side
effect I would be happy to look at those business plans awesome awesome well kayfoo thanks so much
for taking the time to chat with us and share a bit about the book congratulations on the book it
is really takes an interesting approach at raising some very important questions in the development
of AI so thanks and and congrats once again on that thank you thanks Sam for having
