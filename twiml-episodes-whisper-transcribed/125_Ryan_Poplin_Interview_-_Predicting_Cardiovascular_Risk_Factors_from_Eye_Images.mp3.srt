1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,960
I'm your host, Sam Charrington, in this episode I'm joined by Google Research Scientist

4
00:00:32,960 --> 00:00:38,640
Ryan Poplin, who recently co-authored the paper, prediction of cardiovascular risk factors

5
00:00:38,640 --> 00:00:43,040
from retinal, fundous photographs via deep learning.

6
00:00:43,040 --> 00:00:47,600
In our conversation, Ryan details his work training a deep learning model to predict various

7
00:00:47,600 --> 00:00:53,240
patient risk factors for heart disease, including some surprising ones like age and gender.

8
00:00:53,240 --> 00:00:57,240
We also dive into some interesting findings he discovered with regards to multitask

9
00:00:57,240 --> 00:01:02,800
learning, as well as his use of attention mechanisms to provide explainability.

10
00:01:02,800 --> 00:01:07,760
This was a really interesting discussion that I'm sure you'll enjoy.

11
00:01:07,760 --> 00:01:12,240
Before we get to the show, this past Saturday I spent some time at the scaled machine learning

12
00:01:12,240 --> 00:01:15,760
conference which was held on the Stanford campus in Palo Alto.

13
00:01:15,760 --> 00:01:21,520
I had a great time, heard some great speakers, met a few listeners, and tweeted a ton.

14
00:01:21,520 --> 00:01:26,280
So for more of my thoughts from the conference, head on over to my Twitter feed at at Sam

15
00:01:26,280 --> 00:01:28,280
Charrington.

16
00:01:28,280 --> 00:01:32,680
This week I'm at Nvidia's GTC conference, so be sure to hit that follow button for more

17
00:01:32,680 --> 00:01:34,280
news and views.

18
00:01:34,280 --> 00:01:37,240
And now on to the show.

19
00:01:37,240 --> 00:01:48,000
All right, everyone, I've got Ryan Poplin on the line, Ryan is a research scientist

20
00:01:48,000 --> 00:01:54,000
at Google and the lead author on the recent paper, prediction of cardiovascular risk factors

21
00:01:54,000 --> 00:01:57,360
from retinal fundous photographs via deep learning.

22
00:01:57,360 --> 00:02:00,600
Ryan, welcome to this week in machine learning and AI.

23
00:02:00,600 --> 00:02:01,600
Thank you so much.

24
00:02:01,600 --> 00:02:02,600
Thank you for having me.

25
00:02:02,600 --> 00:02:03,600
Absolutely.

26
00:02:03,600 --> 00:02:08,960
So tell us a little bit about how you got involved and interested in machine learning.

27
00:02:08,960 --> 00:02:09,960
What's your background?

28
00:02:09,960 --> 00:02:13,640
Sure, I'm a computer scientist by training.

29
00:02:13,640 --> 00:02:18,640
And what really drew me to machine learning was this idea that I could have an impact in

30
00:02:18,640 --> 00:02:25,840
a variety of scientific domains through statistics and through knowing about computer science

31
00:02:25,840 --> 00:02:27,040
and machine learning.

32
00:02:27,040 --> 00:02:32,720
And it allows me to sort of apply these methods to a variety of domains without having

33
00:02:32,720 --> 00:02:38,440
to sort of derive, you know, domain expert features about these problems.

34
00:02:38,440 --> 00:02:44,360
And so did you have a formal education in AI, where did you, how did you kind of come

35
00:02:44,360 --> 00:02:48,760
up to speed on the tools and techniques that you use today?

36
00:02:48,760 --> 00:02:49,760
Right.

37
00:02:49,760 --> 00:02:53,240
So I did a bachelor's degree in computer science in Indiana.

38
00:02:53,240 --> 00:02:57,080
This was at the Rose Home and Institute of Technology where I studied computer science

39
00:02:57,080 --> 00:02:58,560
and mathematics.

40
00:02:58,560 --> 00:03:04,120
And then for graduate work, I was at Carnegie Mellon University, where I was in a pretty

41
00:03:04,120 --> 00:03:09,920
fun computational neuroscience program, which was a fun way to combine sort of stats, computer

42
00:03:09,920 --> 00:03:12,760
science, and the study of neuroscience.

43
00:03:12,760 --> 00:03:16,080
That's sort of where I picked up the statistical machine learning techniques.

44
00:03:16,080 --> 00:03:17,080
Awesome.

45
00:03:17,080 --> 00:03:23,680
And does your work today, do you spend a lot of time applying the machine learning in neuroscience

46
00:03:23,680 --> 00:03:29,040
and in related fields, or do you work pretty broadly, you know, across domains?

47
00:03:29,040 --> 00:03:30,040
Right.

48
00:03:30,040 --> 00:03:32,240
So I like to work broadly across a variety of domains.

49
00:03:32,240 --> 00:03:36,160
So I've been having fun, you know, applying these, these methods, which are pretty general

50
00:03:36,160 --> 00:03:40,880
to problems in genomics, problems in medical imaging, which is a subject of this paper

51
00:03:40,880 --> 00:03:43,120
on a variety of other scientific domains.

52
00:03:43,120 --> 00:03:44,120
Awesome.

53
00:03:44,120 --> 00:03:45,120
Awesome.

54
00:03:45,120 --> 00:03:48,360
So why don't we get started by having you tell us a little bit about this paper?

55
00:03:48,360 --> 00:03:49,360
Sure.

56
00:03:49,360 --> 00:03:55,920
So this is, this was an effort to using the, the UK biobank, which is the set of fundus

57
00:03:55,920 --> 00:04:01,880
photography images that were annotated with patient metadata, try to take those images,

58
00:04:01,880 --> 00:04:05,280
send them through a neural network, and try to see what we could predict from it.

59
00:04:05,280 --> 00:04:07,280
And what we found was actually quite surprising.

60
00:04:07,280 --> 00:04:12,040
You could predict quite a wide variety of aspects of a person's health just by looking at this

61
00:04:12,040 --> 00:04:13,040
fundus photo.

62
00:04:13,040 --> 00:04:16,160
Why don't we get started by talking about what is a fundus?

63
00:04:16,160 --> 00:04:17,160
Yeah.

64
00:04:17,160 --> 00:04:18,160
Sure.

65
00:04:18,160 --> 00:04:24,080
So a fundus photograph is, is a pretty routine thing actually that I'm sure most of the listeners

66
00:04:24,080 --> 00:04:27,520
of this, of this show have actually had taken themselves.

67
00:04:27,520 --> 00:04:29,960
So it's simply a picture of the back of the eye.

68
00:04:29,960 --> 00:04:34,840
So, you know, a specialist would use a camera called a fundoscope, they, they take a picture

69
00:04:34,840 --> 00:04:38,600
of the back of the eye, and that's, and that's the retina.

70
00:04:38,600 --> 00:04:42,920
So if you go to the ophthalmologist, you probably have this taken, you're to routinely kind

71
00:04:42,920 --> 00:04:44,400
of once a year.

72
00:04:44,400 --> 00:04:48,520
So is this the thing that happens when they, like, blow the burst of air in your eye?

73
00:04:48,520 --> 00:04:49,520
Not quite.

74
00:04:49,520 --> 00:04:50,520
It is a camera.

75
00:04:50,520 --> 00:04:52,600
So you, you know, it's just a normal, you know, picture.

76
00:04:52,600 --> 00:04:55,680
It's actually, you know, very pleasant and non-invasive.

77
00:04:55,680 --> 00:04:57,360
You simply take a picture of the back of the eye.

78
00:04:57,360 --> 00:05:02,040
You can see in the image, you can see things like the optic disc and blood vessels sort

79
00:05:02,040 --> 00:05:08,080
of emanating out of that, and doctors use this to look for a wide variety of eye diseases.

80
00:05:08,080 --> 00:05:11,880
And what we're showing in this paper is that you can look at other diseases as well.

81
00:05:11,880 --> 00:05:12,880
Awesome.

82
00:05:12,880 --> 00:05:19,320
One of the things that I gleaned from the paper was that, you know, I've seen a bunch

83
00:05:19,320 --> 00:05:25,400
of articles about the paper and they all kind of focus on your ability to predict some

84
00:05:25,400 --> 00:05:30,080
interesting cardiovascular risk factors.

85
00:05:30,080 --> 00:05:35,320
But one of the things that's seen most interesting was that, and that didn't get picked up in

86
00:05:35,320 --> 00:05:40,760
a lot of these articles is that you didn't really go in with a direct correlation between

87
00:05:40,760 --> 00:05:45,120
a lot of the things that you were able to predict in the data set meaning, you know,

88
00:05:45,120 --> 00:05:50,840
it wasn't already known that looking at these retinal images, you could predict things

89
00:05:50,840 --> 00:05:56,360
like age and gender and stuff like that, but you discovered that through the research.

90
00:05:56,360 --> 00:05:57,960
Is that the correct interpretation?

91
00:05:57,960 --> 00:05:59,800
Yeah, that's exactly right.

92
00:05:59,800 --> 00:06:05,640
So we started, you know, the background work from my current team is that we've been

93
00:06:05,640 --> 00:06:08,800
using these images to look at a variety of eye diseases.

94
00:06:08,800 --> 00:06:14,200
So predicting diabetic retinopathy, for example, at the level of accuracy of a doctor.

95
00:06:14,200 --> 00:06:17,960
And so when we started using this data set, that's exactly where we started was trying

96
00:06:17,960 --> 00:06:20,000
to predict, look at eye diseases.

97
00:06:20,000 --> 00:06:24,360
And then what we noticed is that there's a wide variety of metadata about the patient

98
00:06:24,360 --> 00:06:26,200
available in this data set.

99
00:06:26,200 --> 00:06:31,240
And so we sort of added those variables to the model in really kind of a diagnostic kind

100
00:06:31,240 --> 00:06:32,240
of way.

101
00:06:32,240 --> 00:06:36,040
So these are things like age and gender and all kinds of other things.

102
00:06:36,040 --> 00:06:40,520
And we felt like those variables, they should be a, you know, very high fidelity in the

103
00:06:40,520 --> 00:06:45,040
data set because it's very easy to measure someone's age or ask their gender.

104
00:06:45,040 --> 00:06:48,680
And so we felt like it was a great sort of control or ground truth that we could add

105
00:06:48,680 --> 00:06:50,280
to the model.

106
00:06:50,280 --> 00:06:53,680
But then what we discovered as we were training the model was actually we were able to

107
00:06:53,680 --> 00:06:56,600
predict these things with remarkably high accuracy.

108
00:06:56,600 --> 00:07:00,640
And in fact, in the beginning, we thought it was a bug in the model, it was a problem

109
00:07:00,640 --> 00:07:07,560
because, you know, you look at a gender AUC of.97 and you show that to someone and they

110
00:07:07,560 --> 00:07:11,040
say to you, it must be, you must have a bug in your model because there's no way you

111
00:07:11,040 --> 00:07:14,360
can predict that with such high accuracy, right?

112
00:07:14,360 --> 00:07:18,240
And you can't ignore an AUC of.97, especially when the classes are balanced.

113
00:07:18,240 --> 00:07:20,600
This is quite a robust prediction from the model.

114
00:07:20,600 --> 00:07:24,720
And in fact, as we dug more and more into it, we discovered that this wasn't a bug in

115
00:07:24,720 --> 00:07:26,840
the model, it was actually a real prediction.

116
00:07:26,840 --> 00:07:32,240
And so drilling a little bit more on the kind of that, the realization, I'm imagining

117
00:07:32,240 --> 00:07:39,200
that you're at what I'm hearing you say is that you started by actually using this, you

118
00:07:39,200 --> 00:07:43,080
know, the things that you ultimately found you could predict as features of your model

119
00:07:43,080 --> 00:07:44,680
so as inputs.

120
00:07:44,680 --> 00:07:48,960
And then, you know, at some point along the way, you found that they were highly dependent

121
00:07:48,960 --> 00:07:50,520
on the images themselves.

122
00:07:50,520 --> 00:07:52,880
Is that kind of the way it came about?

123
00:07:52,880 --> 00:07:58,480
Sort of, so the input to the model was always simply the images in the fundus, so always

124
00:07:58,480 --> 00:08:01,040
using the pixels of the image.

125
00:08:01,040 --> 00:08:05,400
And then alongside it though, you can predict many things simultaneously.

126
00:08:05,400 --> 00:08:10,480
And actually what we found just through our research is that when you give the model many,

127
00:08:10,480 --> 00:08:13,960
many things to predict simultaneously, it actually does better overall.

128
00:08:13,960 --> 00:08:17,320
It's sort of like giving the model something to do with all of its capacity.

129
00:08:17,320 --> 00:08:21,040
And so as a result, we would throw in things like, why don't you try to predict the age,

130
00:08:21,040 --> 00:08:24,360
the gender, and actually all kinds of other things that we thought it probably couldn't

131
00:08:24,360 --> 00:08:27,360
predict, so it would serve as a nice control.

132
00:08:27,360 --> 00:08:30,720
But in reality, what we found is it's doing quite well.

133
00:08:30,720 --> 00:08:34,920
And so we started with gender, that was the most, that was a very surprising one.

134
00:08:34,920 --> 00:08:37,640
And we realized that it, in fact, was a real prediction.

135
00:08:37,640 --> 00:08:39,240
It was actually doing such a good job.

136
00:08:39,240 --> 00:08:40,800
And then we discovered age.

137
00:08:40,800 --> 00:08:44,680
And here, here it's slightly a different prediction, you're, you're regressing to a floating

138
00:08:44,680 --> 00:08:45,680
point value.

139
00:08:45,680 --> 00:08:50,360
And what we found is that age you were actually correct within something like three years

140
00:08:50,360 --> 00:08:54,800
up the person's actual age, which was just remarkable, because when we talked to doctors,

141
00:08:54,800 --> 00:08:59,240
they would tell us, sure, I can tell the difference between an old eye or a young eye.

142
00:08:59,240 --> 00:09:02,960
But then we tell them we're actually able to tell you the age within three years.

143
00:09:02,960 --> 00:09:05,120
And they're quite surprised by that.

144
00:09:05,120 --> 00:09:06,120
Yeah.

145
00:09:06,120 --> 00:09:10,840
So you said something in passing that I wanted to drill in on a bit.

146
00:09:10,840 --> 00:09:17,280
You found that the model was, you, you improve the model, maybe you can be more specific

147
00:09:17,280 --> 00:09:20,320
or kind of restate the, what you said.

148
00:09:20,320 --> 00:09:24,920
But it sounded like you were saying you improved the model's overall performance across a bunch

149
00:09:24,920 --> 00:09:25,920
of measures.

150
00:09:25,920 --> 00:09:30,880
Like each time you asked it to do something new, it got better overall.

151
00:09:30,880 --> 00:09:31,880
Is that what you're saying?

152
00:09:31,880 --> 00:09:32,880
Yeah, that's right.

153
00:09:32,880 --> 00:09:35,520
So, so this is called multi-task learning.

154
00:09:35,520 --> 00:09:40,720
What we found is that, so given a fixed model, as you add more and more features that

155
00:09:40,720 --> 00:09:44,440
you wanted to predict, you're sort of giving, giving the model something to do or letting

156
00:09:44,440 --> 00:09:45,800
it use its capacity.

157
00:09:45,800 --> 00:09:51,440
And so it has a tendency, you, it can't possibly overfit because it's being asked to predict

158
00:09:51,440 --> 00:09:52,760
so many things.

159
00:09:52,760 --> 00:09:57,120
And as a result, you sort of improve the performance of the model overall.

160
00:09:57,120 --> 00:09:58,520
Oh, interesting.

161
00:09:58,520 --> 00:10:04,600
And you didn't apply necessarily or go out of your way to apply kind of multi-task learning,

162
00:10:04,600 --> 00:10:08,200
you know, techniques or anything like that to try to improve the performance.

163
00:10:08,200 --> 00:10:12,360
But rather, this was just, you know, you got better performance for free by asking the

164
00:10:12,360 --> 00:10:14,080
model to do more stuff.

165
00:10:14,080 --> 00:10:15,080
That's right.

166
00:10:15,080 --> 00:10:18,920
And, and that's sort of a feature of this particular data set in which there are many,

167
00:10:18,920 --> 00:10:24,680
many metadata values that we could try to predict, you know, it probably may not work

168
00:10:24,680 --> 00:10:28,160
in every possible problem or every domain, but this was one and where to, and actually

169
00:10:28,160 --> 00:10:29,160
worked.

170
00:10:29,160 --> 00:10:36,040
Now that you have seen this, if you're working on a problem, you would, you know, would

171
00:10:36,040 --> 00:10:40,000
one of your steps in optimization be just trying to come up with other stuff for the network

172
00:10:40,000 --> 00:10:41,000
to do?

173
00:10:41,000 --> 00:10:43,040
Like is that a rational way of thinking about it?

174
00:10:43,040 --> 00:10:44,040
It is absolutely.

175
00:10:44,040 --> 00:10:46,920
This is a technique we're applying to a lots of different problems, actually.

176
00:10:46,920 --> 00:10:51,720
So another example where this worked really well was in our efforts to predict diabetic

177
00:10:51,720 --> 00:10:52,720
retinopathy.

178
00:10:52,720 --> 00:10:57,440
So in this case, you sort of have this grade that's given by the doctors, are you one of

179
00:10:57,440 --> 00:11:01,640
these five classes of the disease diabetic retinopathy, but there's all kinds of other

180
00:11:01,640 --> 00:11:05,320
things you can predict about the image, such as, is it the image quality?

181
00:11:05,320 --> 00:11:10,320
So would the grader say that this is a high quality image or a low or an unusable image,

182
00:11:10,320 --> 00:11:14,920
or things like, is this a left eye or a right eye, which is, you know, not that useful

183
00:11:14,920 --> 00:11:20,400
of a prediction, but adding this extra extra stuff into the into the model actually helped

184
00:11:20,400 --> 00:11:21,640
it a little quite a bit.

185
00:11:21,640 --> 00:11:22,640
Mm-hmm.

186
00:11:22,640 --> 00:11:27,160
I had a note to ask about kind of right versus left eye, and I didn't see that mentioned

187
00:11:27,160 --> 00:11:29,200
in the paper at all.

188
00:11:29,200 --> 00:11:34,000
Did that come into play at all in this recent paper?

189
00:11:34,000 --> 00:11:35,000
Yeah.

190
00:11:35,000 --> 00:11:39,640
So that's another feature that we try to predict alongside of, you know, age and gender

191
00:11:39,640 --> 00:11:43,560
and smoking status, you also predicted this image of a left eye or a right eye.

192
00:11:43,560 --> 00:11:44,560
Okay.

193
00:11:44,560 --> 00:11:50,440
And now how, you know, how arbitrary can you, can you get with this, like, can you, you

194
00:11:50,440 --> 00:11:56,440
know, could you, for example, I guess you'd need a label, but could you come up with, you

195
00:11:56,440 --> 00:12:03,440
know, some arbitrary metric like the, you know, the vascular density of a retina image

196
00:12:03,440 --> 00:12:09,400
or, you know, the color shading or something like that and try to, you know, maybe find

197
00:12:09,400 --> 00:12:16,320
some mechanical way to produce labels and then ask the network to try to match to that

198
00:12:16,320 --> 00:12:17,320
as well.

199
00:12:17,320 --> 00:12:18,320
Absolutely.

200
00:12:18,320 --> 00:12:22,400
I think it's a very fruitful avenue of research for when you have a problem when you're

201
00:12:22,400 --> 00:12:27,160
trying to optimize your model to do better, I think this is one way to try that.

202
00:12:27,160 --> 00:12:32,320
It doesn't work in every case, like the features need to be sort of informative or correlative

203
00:12:32,320 --> 00:12:38,400
or interesting, like I think creating synthetic features that actually aren't correlated

204
00:12:38,400 --> 00:12:42,200
with what you're trying to predict probably won't help as much.

205
00:12:42,200 --> 00:12:44,920
But it's definitely a fruitful array of research for sure.

206
00:12:44,920 --> 00:12:48,240
Can you give me a sense for like the effect that this had, you know, what are we talking

207
00:12:48,240 --> 00:12:54,880
about in terms of performance lifts ultimately, you know, is this, you know, a few percent

208
00:12:54,880 --> 00:12:57,800
or is this, you know, how significant does it?

209
00:12:57,800 --> 00:12:58,800
Right.

210
00:12:58,800 --> 00:12:59,800
It's something like a few percent.

211
00:12:59,800 --> 00:13:03,000
These are the kind of things where when you're, when you're sort of at the middle to

212
00:13:03,000 --> 00:13:06,480
the end of a project and you're trying to eke out, you know, all the possible gains

213
00:13:06,480 --> 00:13:09,640
in a data set, this would be a strategy to try to use.

214
00:13:09,640 --> 00:13:14,000
You know, it's not going to transform the problem because ultimately all your learning

215
00:13:14,000 --> 00:13:17,320
are sort of the correlations between the features anyway.

216
00:13:17,320 --> 00:13:20,360
But it does, you know, help a couple percent.

217
00:13:20,360 --> 00:13:24,120
And there are many other techniques that we try to use to try to eke out that kind of

218
00:13:24,120 --> 00:13:28,520
last percent in model performance.

219
00:13:28,520 --> 00:13:33,960
So I guess as often happens here, I kind of, you know, argued into a particular interesting

220
00:13:33,960 --> 00:13:37,960
detail, but we got a little bit off of track and talking about the project as a whole.

221
00:13:37,960 --> 00:13:43,920
So you, one of the things that I noticed in addition to predicting, you know, developing

222
00:13:43,920 --> 00:13:49,080
this model that predicted all of these factors, as well as the kind of core factors that you

223
00:13:49,080 --> 00:13:58,920
were looking at the cardiovascular risk factors, you also tried to address the explainability

224
00:13:58,920 --> 00:14:03,320
challenge that is often faced by deep learning models.

225
00:14:03,320 --> 00:14:05,760
Can you talk a little bit about how you did that?

226
00:14:05,760 --> 00:14:06,760
Yeah, absolutely.

227
00:14:06,760 --> 00:14:12,120
You know, so as you mentioned, it's often the case that criticism of these kinds of models

228
00:14:12,120 --> 00:14:14,440
is that they're so-called black boxes.

229
00:14:14,440 --> 00:14:20,920
And I think there's a very long and growing body of research showing that these models

230
00:14:20,920 --> 00:14:23,560
are actually shouldn't be called black boxes.

231
00:14:23,560 --> 00:14:27,800
They're actually quite explainable in many different ways.

232
00:14:27,800 --> 00:14:30,960
And the technique that we used here is called soft attention.

233
00:14:30,960 --> 00:14:36,600
And so the idea is to create a network such that you're sort of successively zooming in

234
00:14:36,600 --> 00:14:37,880
on the pixels.

235
00:14:37,880 --> 00:14:44,680
And then you kind of asked the model which were the pixels that made you make that prediction.

236
00:14:44,680 --> 00:14:49,080
And then you, then you can project those pixels onto the original image and get an idea

237
00:14:49,080 --> 00:14:53,400
for why the model made the choice that it did.

238
00:14:53,400 --> 00:14:57,800
And so when we did that with this paper, we looked at a variety of cardiovascular risk

239
00:14:57,800 --> 00:15:03,480
factors and created these heat maps of what were the pixels in the image that led you

240
00:15:03,480 --> 00:15:05,080
to make that prediction.

241
00:15:05,080 --> 00:15:06,600
And what we found was actually pretty cool.

242
00:15:06,600 --> 00:15:12,520
So there are some predictions in which the predominantly the blood vessels were where

243
00:15:12,520 --> 00:15:13,840
the informative features.

244
00:15:13,840 --> 00:15:19,120
And so you can see these as these, in the paper, we use these green heat maps and they

245
00:15:19,120 --> 00:15:24,680
sort of snake along and follow the blood vessels that they're emanating out of the altidisk.

246
00:15:24,680 --> 00:15:30,040
And so a few of those predictions in particular are the prediction of the age and the prediction

247
00:15:30,040 --> 00:15:31,040
of the blood pressure.

248
00:15:31,040 --> 00:15:32,560
And so the blood pressure makes a lot of sense.

249
00:15:32,560 --> 00:15:38,160
If you want to predict the blood pressure, you need to look at the blood vessels.

250
00:15:38,160 --> 00:15:40,000
And then the other predictions.

251
00:15:40,000 --> 00:15:44,160
So for example, the gender prediction were actually focused on quite different features.

252
00:15:44,160 --> 00:15:49,360
So there we saw things like the macula and the size of the altidisk.

253
00:15:49,360 --> 00:15:53,800
And when you go back and like show these images to doctors, they can sort of, you know,

254
00:15:53,800 --> 00:15:56,800
provide feedback and sort of back up some of these findings.

255
00:15:56,800 --> 00:16:02,000
So when you show these heat maps to doctors about gender, they say, oh, yeah, there's

256
00:16:02,000 --> 00:16:06,640
this research about the ratio of the sizes of the altidisk and the caliber of the blood

257
00:16:06,640 --> 00:16:07,840
vessels as they emanate out.

258
00:16:07,840 --> 00:16:09,920
And that's predictive of gender.

259
00:16:09,920 --> 00:16:14,680
And so this sort of gives a bit of validity to the predictions and also makes people feel

260
00:16:14,680 --> 00:16:17,040
much more comfortable.

261
00:16:17,040 --> 00:16:23,360
When you're applying this soft attention mechanism, do you, are you ultimately training kind

262
00:16:23,360 --> 00:16:28,960
of multiple models in parallel, or is it, you know, one big network that has a bunch

263
00:16:28,960 --> 00:16:30,680
of different outputs?

264
00:16:30,680 --> 00:16:31,680
Yes.

265
00:16:31,680 --> 00:16:34,200
So in this particular case, it's two separate networks.

266
00:16:34,200 --> 00:16:38,920
We had, we used the inception V3 architecture to do the majority of the predictions.

267
00:16:38,920 --> 00:16:43,160
And that network has the highest AUCs in the best accuracy.

268
00:16:43,160 --> 00:16:47,480
And then we trained a separate model, which is this smaller soft attention model to do

269
00:16:47,480 --> 00:16:49,680
the model explanation.

270
00:16:49,680 --> 00:16:53,120
For this particular paper, that was done just because the inception architecture had

271
00:16:53,120 --> 00:16:56,920
had a better performance and we wanted to report the best performance.

272
00:16:56,920 --> 00:17:00,960
But it could, it could have been the same model doing both actually.

273
00:17:00,960 --> 00:17:01,960
Okay.

274
00:17:01,960 --> 00:17:02,960
Yeah.

275
00:17:02,960 --> 00:17:07,640
So it is sort of a question of like what the goal of the, of the prediction is.

276
00:17:07,640 --> 00:17:12,240
So if your goal is to be able to show these heat maps and sort of give an idea to someone

277
00:17:12,240 --> 00:17:15,720
of why you made these predictions that maybe a network like the soft attention network is

278
00:17:15,720 --> 00:17:16,800
the way to go.

279
00:17:16,800 --> 00:17:20,360
And the, and the accuracy is actually only slightly worse than if you use a much bigger model

280
00:17:20,360 --> 00:17:23,360
like the inception V3.

281
00:17:23,360 --> 00:17:24,360
Okay.

282
00:17:24,360 --> 00:17:28,280
And did you, did you test a bunch of different models or did you start with, you know, start

283
00:17:28,280 --> 00:17:33,720
and finish with inception, you know, with some prior knowledge that it would probably

284
00:17:33,720 --> 00:17:35,120
perform the best?

285
00:17:35,120 --> 00:17:36,120
Right.

286
00:17:36,120 --> 00:17:42,600
So in terms of broad like architecture, what we found is that pretty much in every problem

287
00:17:42,600 --> 00:17:46,400
that we tried, the inception architecture simply works the best.

288
00:17:46,400 --> 00:17:50,320
And so it's hard to move away from it because it, it seems to predominantly always

289
00:17:50,320 --> 00:17:51,320
work the best.

290
00:17:51,320 --> 00:17:57,240
You know, and this is the network that's, you know, running lots of products all around

291
00:17:57,240 --> 00:17:59,280
Google, such as Google Photos and things.

292
00:17:59,280 --> 00:18:04,240
And so it's simply the case that when you send it a lot of images, that architecture seems

293
00:18:04,240 --> 00:18:05,560
to do the best.

294
00:18:05,560 --> 00:18:10,240
Now there's lots and lots of hyper parameter optimizations that can be done to that network.

295
00:18:10,240 --> 00:18:14,520
Things like, you know, the sizes of things as you grow out the layers and all kinds of

296
00:18:14,520 --> 00:18:18,760
things that we definitely explored as part of this, as part of this work.

297
00:18:18,760 --> 00:18:23,080
And predominantly, the inception architecture seems to work the best across a wide variety

298
00:18:23,080 --> 00:18:24,080
of problems.

299
00:18:24,080 --> 00:18:28,000
And that's true, not only in medical imaging, it's also working for us in genomics and

300
00:18:28,000 --> 00:18:31,360
radiology and other other things.

301
00:18:31,360 --> 00:18:34,800
One of the other things I noticed was that you use transfer learning here.

302
00:18:34,800 --> 00:18:36,480
Can you talk a little bit about that?

303
00:18:36,480 --> 00:18:37,480
Yeah.

304
00:18:37,480 --> 00:18:43,240
So it's a pretty standard technique nowadays for when you don't have as much data as you

305
00:18:43,240 --> 00:18:44,720
would training data as you would like.

306
00:18:44,720 --> 00:18:50,680
And so there's actually a benefit to training this model on cats and dogs and pictures of

307
00:18:50,680 --> 00:18:53,280
flowers and buildings and things.

308
00:18:53,280 --> 00:18:58,040
And then starting from those weights and telling the model, you know, forget about cats and

309
00:18:58,040 --> 00:19:00,440
dogs now and actually learn about retinas.

310
00:19:00,440 --> 00:19:05,320
And now you're going to start from those weights, but still update as you look at millions

311
00:19:05,320 --> 00:19:07,320
and millions of retinas.

312
00:19:07,320 --> 00:19:12,760
It sounds like the fact that that, you know, helped you, you know, bootstrap your training

313
00:19:12,760 --> 00:19:13,760
process.

314
00:19:13,760 --> 00:19:14,760
That's not a surprise.

315
00:19:14,760 --> 00:19:15,760
You do that a lot as well.

316
00:19:15,760 --> 00:19:16,760
Yeah.

317
00:19:16,760 --> 00:19:22,080
So that technique is used actually quite a bit across products here and across products

318
00:19:22,080 --> 00:19:23,720
basically everywhere now.

319
00:19:23,720 --> 00:19:28,720
And it's a, it's a small boost, certainly having more retinas pictures of retinas would

320
00:19:28,720 --> 00:19:33,240
be way better than having pictures of cats and dogs, but it is the fact that these retinas

321
00:19:33,240 --> 00:19:38,120
are actually closer to natural images than they are to random images.

322
00:19:38,120 --> 00:19:43,400
And so you do get, you do get some boost by training on doing this pre-training on other

323
00:19:43,400 --> 00:19:44,400
data sets.

324
00:19:44,400 --> 00:19:52,640
Yeah, I had a conversation with someone recently, I forget, I forget the specific context.

325
00:19:52,640 --> 00:19:57,160
We didn't have a chance to go into a lot of detail, but he made kind of a passing comment

326
00:19:57,160 --> 00:20:01,680
that transfer learning, you know, almost like transfer learning had been debunked or something

327
00:20:01,680 --> 00:20:04,400
like that or it doesn't work in practice.

328
00:20:04,400 --> 00:20:08,760
Clearly, you know, it does for these kinds of problems and you see it a lot.

329
00:20:08,760 --> 00:20:13,120
Do you have any, you know, care to guess at what that might be referring to?

330
00:20:13,120 --> 00:20:18,960
Have you seen, you know, frustrations in applying it to certain types of problems or anything

331
00:20:18,960 --> 00:20:19,960
like that?

332
00:20:19,960 --> 00:20:20,960
Yeah.

333
00:20:20,960 --> 00:20:21,960
A couple of things.

334
00:20:21,960 --> 00:20:25,440
So one is that the benefit you're going to get is actually much, much smaller than you

335
00:20:25,440 --> 00:20:26,440
would like.

336
00:20:26,440 --> 00:20:31,560
You know, it's not like you're adding millions of more training points to your data

337
00:20:31,560 --> 00:20:35,880
set, you're actually, it's a much smaller factor because it is the fact that you want these

338
00:20:35,880 --> 00:20:40,200
models to be optimized to learn about retinas, right, and the patterns in retinas.

339
00:20:40,200 --> 00:20:45,800
But it's just a matter of sort of bootstrapping those image like features and maybe the lower

340
00:20:45,800 --> 00:20:49,120
layers of the network and you just sort of get some benefit from that.

341
00:20:49,120 --> 00:20:52,720
So that's the first thing is that the benefits actually much smaller than you would hope for.

342
00:20:52,720 --> 00:20:58,440
The next thing is, it kind of depends on how close your problem is to the, to the problem

343
00:20:58,440 --> 00:21:00,520
in the of these natural images.

344
00:21:00,520 --> 00:21:06,760
So for example, if you're, if your data set is sort of synthetic images or experimental

345
00:21:06,760 --> 00:21:12,360
images or, or things that don't look like pictures of cats and dogs, then it will help

346
00:21:12,360 --> 00:21:14,520
much, much less.

347
00:21:14,520 --> 00:21:17,760
And that, I would expect that to be the case with these retinal images.

348
00:21:17,760 --> 00:21:24,400
But you, you kind of characterize them as, as being similar in some ways is, is it just

349
00:21:24,400 --> 00:21:28,360
kind of the naturalness of the image as opposed to, for example, like a computer generated

350
00:21:28,360 --> 00:21:29,360
image?

351
00:21:29,360 --> 00:21:30,360
Exactly.

352
00:21:30,360 --> 00:21:32,440
These definitely aren't random images.

353
00:21:32,440 --> 00:21:37,640
These are, they have lighting features that you could maybe learn, they have, they certainly

354
00:21:37,640 --> 00:21:43,000
have color correlations and spatial correlations that you could learn.

355
00:21:43,000 --> 00:21:47,720
So I do think it's the case that these images are closer to natural images than to just sort

356
00:21:47,720 --> 00:21:50,320
of starting from random weights, yeah.

357
00:21:50,320 --> 00:21:59,000
And when you say writing features, do you mean like metadata, like capture time and maybe

358
00:21:59,000 --> 00:22:03,400
capture location or things like that that are kind of burned into the image?

359
00:22:03,400 --> 00:22:07,360
You know, it could be because these are, you know, these are cameras, a fundoscope is

360
00:22:07,360 --> 00:22:08,360
a camera.

361
00:22:08,360 --> 00:22:13,040
And so there, there are definitely a camera related artifacts that you could probably

362
00:22:13,040 --> 00:22:17,560
learn and pick up on, even if they were, you know, this camera versus, versus other

363
00:22:17,560 --> 00:22:20,480
camera images of natural images.

364
00:22:20,480 --> 00:22:23,720
Maybe I, maybe I misheard you when you said writing.

365
00:22:23,720 --> 00:22:28,680
If in fact that, you know, there were like optical, you know, characters writing on these

366
00:22:28,680 --> 00:22:32,920
images like the, you know, patient name or whatever, you know, would you, would you have

367
00:22:32,920 --> 00:22:38,280
thought to do, you know, either mask those or, you know, do you worry about the network

368
00:22:38,280 --> 00:22:46,680
learning like, I don't know, you know, times, time of year, you know, types of names,

369
00:22:46,680 --> 00:22:47,680
that kind of thing.

370
00:22:47,680 --> 00:22:48,680
Yeah.

371
00:22:48,680 --> 00:22:52,640
So definitely when you're working with medical images, that's something to definitely worry

372
00:22:52,640 --> 00:22:53,640
about.

373
00:22:53,640 --> 00:22:57,800
So we have this process by which we try to de-identify those images.

374
00:22:57,800 --> 00:23:03,640
So we would first run it through a process to sort of ask, is there, is there writing

375
00:23:03,640 --> 00:23:04,640
on the images?

376
00:23:04,640 --> 00:23:06,080
Is one common way to do it?

377
00:23:06,080 --> 00:23:07,080
So.

378
00:23:07,080 --> 00:23:08,080
Okay.

379
00:23:08,080 --> 00:23:12,480
And these are standard sort of like OCR kind of techniques like, do you see any kind of

380
00:23:12,480 --> 00:23:13,480
writing?

381
00:23:13,480 --> 00:23:14,480
What is the writing?

382
00:23:14,480 --> 00:23:16,480
And if there is, we would mess that out.

383
00:23:16,480 --> 00:23:17,480
Yeah.

384
00:23:17,480 --> 00:23:18,480
Interesting.

385
00:23:18,480 --> 00:23:25,160
So do the, the transfer learning you mentioned kind of using the, using a network trained

386
00:23:25,160 --> 00:23:30,320
on image net and kind of bootstrapping with the, the weights from that network.

387
00:23:30,320 --> 00:23:34,560
I've also talked to folks about kind of a, you know, what I guess is another kind of

388
00:23:34,560 --> 00:23:38,240
transfer learning where you're kind of training a network and then kind of chopping off the

389
00:23:38,240 --> 00:23:44,520
last layers and replacing the last layers with something else that's more specific to

390
00:23:44,520 --> 00:23:45,520
your domain.

391
00:23:45,520 --> 00:23:48,280
Do you, I guess this is a bit of an aside.

392
00:23:48,280 --> 00:23:51,840
Are there kind of more specific names for each of those two things or would you, would

393
00:23:51,840 --> 00:23:54,320
you call them both transfer learning?

394
00:23:54,320 --> 00:23:57,920
I think they're both called transfer learning.

395
00:23:57,920 --> 00:24:02,320
We, we do that procedure as well where we're chopping off, you know, if you have a network

396
00:24:02,320 --> 00:24:06,760
trained on thousands of classes such as cats and dogs and buildings and flowers, then

397
00:24:06,760 --> 00:24:12,200
it's natural to sort of chop off that last layer and add sort of, you know, predictions

398
00:24:12,200 --> 00:24:18,320
for, you know, gender, which is, you know, a classifier prediction and then age, which

399
00:24:18,320 --> 00:24:22,160
is a floating point regression prediction, prediction, and you sort of add these extra layers

400
00:24:22,160 --> 00:24:23,600
to the top of the network.

401
00:24:23,600 --> 00:24:26,960
And then those layers have to be pre-initialized with sort of random weights.

402
00:24:26,960 --> 00:24:31,720
So those things definitely need to be learned as you optimize the, as you optimize the network.

403
00:24:31,720 --> 00:24:34,600
But I think in both cases, it's sort of called transfer learning.

404
00:24:34,600 --> 00:24:39,640
And there are lots of other machine learning techniques we try to do to augment the data.

405
00:24:39,640 --> 00:24:45,200
There's like random flipping of the images and, and brightness, random brightness changes

406
00:24:45,200 --> 00:24:46,200
and things.

407
00:24:46,200 --> 00:24:50,880
And these are ways to sort of augment your training data in small ways.

408
00:24:50,880 --> 00:24:58,440
And I imagine that you have a standard kind of script or, you know, or a pipeline that

409
00:24:58,440 --> 00:25:04,280
you send images through that does all this or do you, more kind of hand apply these things

410
00:25:04,280 --> 00:25:06,920
on a problem by problem basis.

411
00:25:06,920 --> 00:25:13,000
But now it's somewhat standard because these techniques like, you know, flipping and rotations

412
00:25:13,000 --> 00:25:17,720
and brightness changes, those are, those are can be applied to any kind of image, whether

413
00:25:17,720 --> 00:25:20,560
it's an image of a retina or other things.

414
00:25:20,560 --> 00:25:24,440
And so those techniques are pretty standard and everything we've done is built on top of

415
00:25:24,440 --> 00:25:25,440
TensorFlow.

416
00:25:25,440 --> 00:25:29,800
And these are using TensorFlow ops to do those image manipulations.

417
00:25:29,800 --> 00:25:32,760
But the code is shared amongst many, many teams here now.

418
00:25:32,760 --> 00:25:37,960
And so if things like flipping and changing brightness and like, are you also doing like

419
00:25:37,960 --> 00:25:43,440
kind of random crops and that kind of stuff is, you know, how, how many or how big is

420
00:25:43,440 --> 00:25:46,640
this pipeline of kind of augmentation operations?

421
00:25:46,640 --> 00:25:49,960
Is it a handful or is it a bunch of things?

422
00:25:49,960 --> 00:25:51,800
It's quite extensive.

423
00:25:51,800 --> 00:25:53,600
It can be, it can be a bunch of things.

424
00:25:53,600 --> 00:25:56,600
It sort of depends a little bit on the problem domain.

425
00:25:56,600 --> 00:26:01,000
So just one example is for us, the crop, the random crops and things didn't quite work

426
00:26:01,000 --> 00:26:05,720
out as well because the camera image is actually pretty standard in terms of its size and

427
00:26:05,720 --> 00:26:10,600
its field of view and things because, you know, it's a patient setting where the patient

428
00:26:10,600 --> 00:26:12,120
puts their face into the camera.

429
00:26:12,120 --> 00:26:15,760
And so the field of view is actually pretty, pretty set.

430
00:26:15,760 --> 00:26:19,240
And these images are taken by, you know, professional ophthalmologist.

431
00:26:19,240 --> 00:26:24,560
And so that kind of stuff, so adding crops doesn't quite help as much.

432
00:26:24,560 --> 00:26:29,480
So it's a matter of sort of capturing the natural variability of the training data.

433
00:26:29,480 --> 00:26:34,560
And so those sort of augmentations can be turned on or off depending on what your problem

434
00:26:34,560 --> 00:26:36,280
domain is.

435
00:26:36,280 --> 00:26:43,280
You briefly mentioned the data sets and kind of how that data was sourced.

436
00:26:43,280 --> 00:26:48,720
And the paper, you, there, I recall you mentioning one of the data sets, the UK data

437
00:26:48,720 --> 00:26:49,720
source.

438
00:26:49,720 --> 00:26:52,960
But in the paper, you mentioned two, one from the UK and one from the US.

439
00:26:52,960 --> 00:26:53,960
That's right.

440
00:26:53,960 --> 00:26:54,960
Yeah.

441
00:26:54,960 --> 00:27:00,040
Um, we started primarily by looking at the UK biobank because this is, this was a pretty

442
00:27:00,040 --> 00:27:03,800
fun data set that had lots and lots of metadata about the patient.

443
00:27:03,800 --> 00:27:08,800
And then when the, when we wanted to sort of validate these predictions, we used another

444
00:27:08,800 --> 00:27:13,840
data set called the I-PACS data set, which is a set of images from a US tele-optimology

445
00:27:13,840 --> 00:27:17,840
service where, um, you know, these images are sent out.

446
00:27:17,840 --> 00:27:20,040
They're graded by professional ophthalmologists.

447
00:27:20,040 --> 00:27:23,840
And they also had, they also had some of the same metadata associated with it.

448
00:27:23,840 --> 00:27:26,560
So we could validate some of these predictions.

449
00:27:26,560 --> 00:27:32,240
One of the things that I noticed in the paper was that you, in reporting performance and

450
00:27:32,240 --> 00:27:38,920
result and the like, you, uh, always treated these data sets separately.

451
00:27:38,920 --> 00:27:45,520
And I was wondering whether, uh, you, for example, combined the data sets and evaluated

452
00:27:45,520 --> 00:27:50,600
performance on or trained on the combined data set and evaluated performance on kind of

453
00:27:50,600 --> 00:27:55,400
a randomized sample from these data sets, you know, you know, and or kind of what your

454
00:27:55,400 --> 00:27:58,560
general thinking is about that whole line of thought.

455
00:27:58,560 --> 00:28:04,000
Yeah, so we had to be a little bit careful here because some of the data sets had some

456
00:28:04,000 --> 00:28:07,200
of the, uh, some of the variables and others didn't.

457
00:28:07,200 --> 00:28:12,240
So just for an example, systolic and diastolic blood pressure, those were available in UK

458
00:28:12,240 --> 00:28:13,240
biobank.

459
00:28:13,240 --> 00:28:15,680
And so we trained on those, but they aren't available in I-PACS.

460
00:28:15,680 --> 00:28:19,200
And so we couldn't report the results there.

461
00:28:19,200 --> 00:28:22,800
We actually, you know, we couldn't train the model on that because that data wasn't available.

462
00:28:22,800 --> 00:28:27,560
Um, and so that's one reason why it's sort of very carefully split between the two things.

463
00:28:27,560 --> 00:28:33,400
Um, there are actually looks like only ages common to the two data sets and gender, yeah,

464
00:28:33,400 --> 00:28:34,400
age and gender.

465
00:28:34,400 --> 00:28:35,400
Okay.

466
00:28:35,400 --> 00:28:36,400
Yeah, yeah.

467
00:28:36,400 --> 00:28:39,800
And so, um, yeah, that's just an unfortunate feature of the data.

468
00:28:39,800 --> 00:28:44,200
We were still working on trying to collect other data sets, which have these other metadata

469
00:28:44,200 --> 00:28:45,200
available.

470
00:28:45,200 --> 00:28:50,400
Um, one cool thing about I put the I-PACS data set was they had HBA 1C level measured

471
00:28:50,400 --> 00:28:51,400
for their patients.

472
00:28:51,400 --> 00:28:52,400
Mm-hmm.

473
00:28:52,400 --> 00:28:53,560
And so this is a measure of blood glucose.

474
00:28:53,560 --> 00:28:58,000
Um, and so it was really cool to be able to try to predict that, but that, but that,

475
00:28:58,000 --> 00:29:00,880
uh, feature is not available in UK biobank.

476
00:29:00,880 --> 00:29:03,640
So we had to play a little, a little bit there.

477
00:29:03,640 --> 00:29:04,640
Mm-hmm.

478
00:29:04,640 --> 00:29:11,720
Uh, and it looks like, uh, that was the, the only feature that, you know, for what you

479
00:29:11,720 --> 00:29:15,120
didn't consistently outperform the baseline.

480
00:29:15,120 --> 00:29:16,120
Is that right?

481
00:29:16,120 --> 00:29:18,480
And what, what's your intuition around that?

482
00:29:18,480 --> 00:29:19,480
Yeah.

483
00:29:19,480 --> 00:29:23,440
So I think the, uh, the mean is sort of slightly better than the baseline, but if you look

484
00:29:23,440 --> 00:29:27,320
at, um, 90% conference intervals, they're, they're basically overlapping.

485
00:29:27,320 --> 00:29:31,040
And so what I think it's saying is that there are some features in the images, which are

486
00:29:31,040 --> 00:29:36,480
correlated with HBA 1C, but, you know, it's not, it's not a slam dunk prediction that

487
00:29:36,480 --> 00:29:39,200
you would want to sort of, uh, bet the house on.

488
00:29:39,200 --> 00:29:44,560
Mm-hmm, as is the case with, you know, age where you've got this, you know, very accurate,

489
00:29:44,560 --> 00:29:45,960
uh, set of predictions.

490
00:29:45,960 --> 00:29:46,960
Right.

491
00:29:46,960 --> 00:29:47,960
Exactly.

492
00:29:47,960 --> 00:29:52,240
And we tried to be very careful about producing the 95% conference intervals to give someone

493
00:29:52,240 --> 00:29:56,960
an idea of how, how accurate we thought these predictions were and whether they were

494
00:29:56,960 --> 00:30:02,000
actually, um, you know, predicting something real or just sort of regressing to the, to

495
00:30:02,000 --> 00:30:03,720
the mean values in the data set.

496
00:30:03,720 --> 00:30:04,720
Mm-hmm.

497
00:30:04,720 --> 00:30:09,720
So how did you produce the confidence intervals and the p values and the statistical measures

498
00:30:09,720 --> 00:30:12,840
like that that you, um, that you mentioned in the paper?

499
00:30:12,840 --> 00:30:13,840
Mm-hmm.

500
00:30:13,840 --> 00:30:17,040
So all the confidence intervals are done through bootstrapping of the eVAL sets.

501
00:30:17,040 --> 00:30:21,080
You sort of randomly select from that eVAL set many, many times and then from that you

502
00:30:21,080 --> 00:30:23,640
have a distribution of AUC values.

503
00:30:23,640 --> 00:30:26,760
And then you can report the 95% confidence interval of those.

504
00:30:26,760 --> 00:30:32,960
And that's true for, um, the AUCs and, and, and also the mean squared errors and things.

505
00:30:32,960 --> 00:30:41,320
Um, um, you, you, and, and Google more broadly alone are doing a ton of work in this area

506
00:30:41,320 --> 00:30:48,120
specifically applying deep learning to not just image-based predictions, but, you know,

507
00:30:48,120 --> 00:30:50,320
healthcare predictions more broadly.

508
00:30:50,320 --> 00:30:56,600
But in the, you know, maybe starting specifically with the domain of image-based, uh, predict,

509
00:30:56,600 --> 00:31:01,480
predictions and diagnostics, you know, what are your, what are your, what's your sense

510
00:31:01,480 --> 00:31:06,000
of what the, you know, where are we, I guess, is the question, what are the limitations?

511
00:31:06,000 --> 00:31:10,040
What, what are we able to do very well right now and, and kind of, what do you think the

512
00:31:10,040 --> 00:31:16,040
path is to making this kind of a standard tool in the physicians toolkit?

513
00:31:16,040 --> 00:31:17,040
Mm-hmm.

514
00:31:17,040 --> 00:31:22,000
So one thing that I think that we're able to do very well right now in the medical imaging

515
00:31:22,000 --> 00:31:30,640
domain is to automate the diagnoses of diseases in which doctors can sort of look at an image,

516
00:31:30,640 --> 00:31:33,760
give you a diagnosis and be very confident in their diagnosis.

517
00:31:33,760 --> 00:31:39,520
So from that data, we can sort of collect many, many diagnoses from doctors, train models

518
00:31:39,520 --> 00:31:45,680
to replicate that performance and, and sort of automate that, that doctor-level diagnosis

519
00:31:45,680 --> 00:31:50,040
and make it sort of radically available, you know, throughout the world in which the people

520
00:31:50,040 --> 00:31:53,240
may not have access to that level of expert care.

521
00:31:53,240 --> 00:32:00,920
The places where we're still need work are in the sort of more researchy or experimental

522
00:32:00,920 --> 00:32:04,360
type predictions like the ones we're talking about here, in which we've shown that there's

523
00:32:04,360 --> 00:32:09,440
like some tantalizing correlations, but the data sets, they only have a couple hundred

524
00:32:09,440 --> 00:32:13,920
cardiovascular events, for example, and so we weren't able to prove that we're able

525
00:32:13,920 --> 00:32:17,880
to predict these things at high accuracy, but they're sort of tantalizing evidence that

526
00:32:17,880 --> 00:32:19,560
maybe there's something there.

527
00:32:19,560 --> 00:32:25,400
And so you could imagine a future in which this, this fundus image is actually, you know,

528
00:32:25,400 --> 00:32:30,000
taken like more like a, a vital sign like, like your, your, your blood pressure when you

529
00:32:30,000 --> 00:32:34,480
go into the doctor's office where you take this snapshot of the eye, it's a very non-invasive

530
00:32:34,480 --> 00:32:38,680
thing, very easy to do, and you get this sort of overall view of someone's health.

531
00:32:38,680 --> 00:32:42,320
And so we need, there's lots of work that needs to be done to sort of validate those predictions

532
00:32:42,320 --> 00:32:47,120
and those ideas, but that that's one, one area that I could go.

533
00:32:47,120 --> 00:32:52,320
And in terms of making this more sort of broadly accepted or available, I think one way to

534
00:32:52,320 --> 00:32:57,000
do this is to focus on this explanation of the predictions, and so it gives sort of

535
00:32:57,000 --> 00:33:03,720
get doctors more like on your side about the prediction, like by providing an explanation,

536
00:33:03,720 --> 00:33:10,240
they sort of can trust it, they can believe in what the model is doing.

537
00:33:10,240 --> 00:33:15,560
Did you out of curiosity, did you interface directly with the doctors for this study?

538
00:33:15,560 --> 00:33:20,600
Absolutely, yeah, we work with, we have lots of doctors that we work with through our,

539
00:33:20,600 --> 00:33:25,640
our eye disease initiatives, and we also work with cardiologists as well to sort of bounce

540
00:33:25,640 --> 00:33:31,600
ideas off of them, show them, and early results, and sort of get their feedback on the predictions,

541
00:33:31,600 --> 00:33:32,600
yeah.

542
00:33:32,600 --> 00:33:39,160
And were there, were there any particular, you know, conversations or reactions that

543
00:33:39,160 --> 00:33:46,400
jumped out at you for, you know, when you presented, maybe, you know, with and without presenting,

544
00:33:46,400 --> 00:33:53,000
you know, the, the soft attention results like, did you have those the entire time or

545
00:33:53,000 --> 00:33:58,560
did you have some conversations with doctors, you know, before you had the explanations

546
00:33:58,560 --> 00:34:03,280
from the attention mechanisms and some after and like, can you, did you see a market difference

547
00:34:03,280 --> 00:34:05,120
in kind of the way they react?

548
00:34:05,120 --> 00:34:06,640
Yeah, absolutely.

549
00:34:06,640 --> 00:34:11,760
So you can imagine trying to show a doctor, tell them that you have an AUC of.97 for

550
00:34:11,760 --> 00:34:15,480
gender, and they kind of, they kind of laugh at you, they don't believe it.

551
00:34:15,480 --> 00:34:19,880
But then when you show them that heat map and sort of show that it's focusing on the

552
00:34:19,880 --> 00:34:23,720
optic disc or maybe features around the optic disc, then they say, oh, yeah, of course we

553
00:34:23,720 --> 00:34:27,000
knew that, that, that, of course you can see that.

554
00:34:27,000 --> 00:34:33,440
And so it really does, by, by, by showing where in the image, the model is using to make

555
00:34:33,440 --> 00:34:38,840
this prediction, it really does provide a level of, of trust and also, you know, a level

556
00:34:38,840 --> 00:34:41,400
of validity to the results.

557
00:34:41,400 --> 00:34:45,880
Can you tell us a little bit about the, the broader research landscape in this area?

558
00:34:45,880 --> 00:34:50,040
What are, you know, you've mentioned a bunch of the research that you're doing.

559
00:34:50,040 --> 00:34:54,480
Where do you think the most interesting and important activity in this space is happening

560
00:34:54,480 --> 00:34:56,480
right now?

561
00:34:56,480 --> 00:35:02,360
So the future work for, for the cardiovascular effort is, is certainly in validating

562
00:35:02,360 --> 00:35:09,600
in more data, so more data sets, trying to find data sets which have more cardiovascular

563
00:35:09,600 --> 00:35:12,520
events so we can sort of validate those approaches.

564
00:35:12,520 --> 00:35:17,240
And I think as we gather together more and more data, that we'll be able to, you know,

565
00:35:17,240 --> 00:35:21,280
narrow those, those confidence intervals and try to decide if this is actually working.

566
00:35:21,280 --> 00:35:25,800
So that's the, so the future in that respect is certainly gathering more data.

567
00:35:25,800 --> 00:35:29,680
And that's true across a wide variety of domains actually.

568
00:35:29,680 --> 00:35:34,280
We have a lot of, you know, initial predictions and initial results, but we need lots and

569
00:35:34,280 --> 00:35:37,280
lots more data in order to validate those things.

570
00:35:37,280 --> 00:35:41,680
You know, that almost makes me think that, that the, the take is that, you know, we've

571
00:35:41,680 --> 00:35:45,840
kind of solves a lot of the machine learning bits of this.

572
00:35:45,840 --> 00:35:50,200
You know, we've got inception, we've got, you know, transfer learning from ImageNet,

573
00:35:50,200 --> 00:35:56,560
we've got, you know, attention mechanisms and the like to, to help with explainability.

574
00:35:56,560 --> 00:36:01,360
It sounds like you're, you're saying that, you know, in terms of kind of core, you

575
00:36:01,360 --> 00:36:06,360
know, evolving the way we think about our capabilities on the machine learning side,

576
00:36:06,360 --> 00:36:11,120
you know, we're kind of as far as we need to be and now we just need more data.

577
00:36:11,120 --> 00:36:14,560
Is that taking it too far or do you agree with that?

578
00:36:14,560 --> 00:36:21,560
I think in terms of medical imaging, I think that's more or less true.

579
00:36:21,560 --> 00:36:25,840
There's still research that's happening for sure on better ways, better and more valid

580
00:36:25,840 --> 00:36:31,520
ways to do the model explanation, ways to incorporate many more predictions simultaneously

581
00:36:31,520 --> 00:36:32,920
and things like this.

582
00:36:32,920 --> 00:36:38,080
But I think for, for medical imaging, I think there's definitely a path forward that we see

583
00:36:38,080 --> 00:36:41,280
that is more or less guaranteed to work at this point.

584
00:36:41,280 --> 00:36:43,600
We've done the early work to show that it's going to work.

585
00:36:43,600 --> 00:36:45,360
We just need to execute.

586
00:36:45,360 --> 00:36:51,520
I think the areas in which it's less clear are in other domains, so things like working

587
00:36:51,520 --> 00:36:55,880
with medical records and things like working with the genomics data and in other sort

588
00:36:55,880 --> 00:36:59,280
of health, health biomedicine data.

589
00:36:59,280 --> 00:37:05,760
There, there's still some more fundamental work that needs to happen around the network

590
00:37:05,760 --> 00:37:08,840
architectures and machine learning models and alike.

591
00:37:08,840 --> 00:37:09,840
That's right.

592
00:37:09,840 --> 00:37:10,840
Exactly.

593
00:37:10,840 --> 00:37:14,080
Because there, for example, on the medical record space, you're working with sort of time

594
00:37:14,080 --> 00:37:17,480
as a dimension that you need to add to the model in some way.

595
00:37:17,480 --> 00:37:19,640
There's a lot of unstructured notes and things.

596
00:37:19,640 --> 00:37:24,680
So there's sort of a much wider variety of model architectures that are needed to be

597
00:37:24,680 --> 00:37:27,200
explored there, for sure.

598
00:37:27,200 --> 00:37:28,200
Interesting.

599
00:37:28,200 --> 00:37:33,480
In terms of that data challenge, how involved were you in that?

600
00:37:33,480 --> 00:37:37,640
Did you come on this project and the data was there and it sounds like you've used this

601
00:37:37,640 --> 00:37:39,680
data set quite a bit.

602
00:37:39,680 --> 00:37:45,400
But do you have any experience trying to source this data and does that lead you, leave

603
00:37:45,400 --> 00:37:51,240
you with any advice for folks that are interested in doing work in this field, but need to get

604
00:37:51,240 --> 00:37:52,840
their hands on a data set?

605
00:37:52,840 --> 00:37:53,840
Yeah.

606
00:37:53,840 --> 00:37:56,960
So data is definitely the key.

607
00:37:56,960 --> 00:38:00,240
You can't really, can't do anything in this data.

608
00:38:00,240 --> 00:38:05,240
And so, you know, when this project started, it started with literally the word UK biobank.

609
00:38:05,240 --> 00:38:06,640
Is there something interesting there?

610
00:38:06,640 --> 00:38:07,640
Yes or no?

611
00:38:07,640 --> 00:38:08,640
We'd have no idea.

612
00:38:08,640 --> 00:38:13,440
And so it was a matter of sort of starting from that, with that data from scratch and sort

613
00:38:13,440 --> 00:38:18,920
of deciding what was in it and learning how all the different fields are encoded and

614
00:38:18,920 --> 00:38:23,640
learning like when the data was trustworthy and which fields were encoded in different

615
00:38:23,640 --> 00:38:24,640
ways and things.

616
00:38:24,640 --> 00:38:29,200
And so there's definitely a lot of work in terms of cleaning up the data, parsing and

617
00:38:29,200 --> 00:38:33,480
downloading and making it widely available to the team and things.

618
00:38:33,480 --> 00:38:37,760
So there's a lot of engineering work and data science work that goes into that for sure.

619
00:38:37,760 --> 00:38:43,800
Can you maybe give us a sense of the, you know, the timeframes involved in a project like

620
00:38:43,800 --> 00:38:44,800
this?

621
00:38:44,800 --> 00:38:48,920
It sounds like you didn't start from a cold start, so it may be a little bit difficult

622
00:38:48,920 --> 00:38:53,800
because you've got, you know, certainly a tool chain in place, this data set you've

623
00:38:53,800 --> 00:38:55,080
worked with for a while.

624
00:38:55,080 --> 00:39:02,600
But in terms of, you know, starting from, you know, the idea to research this project

625
00:39:02,600 --> 00:39:06,760
to publishing the paper recently, like, what was that time frame like?

626
00:39:06,760 --> 00:39:07,760
Mm-hmm.

627
00:39:07,760 --> 00:39:10,720
So it's something, I think, a little over a year.

628
00:39:10,720 --> 00:39:17,480
So it's roughly, you know, four to six months of, you know, digging through the data and

629
00:39:17,480 --> 00:39:24,560
deciding what's there, you know, understanding why when I load up the JPEG, it looks, you

630
00:39:24,560 --> 00:39:28,920
know, reversed or whatever and learning the specifics of the encoding and things like

631
00:39:28,920 --> 00:39:33,600
this, you know, that takes time diving into the data.

632
00:39:33,600 --> 00:39:38,280
And so that's, you know, roughly four to six months of digging in and trying things.

633
00:39:38,280 --> 00:39:45,000
I think our earliest predictions on the gender and AUC were within maybe three to four months.

634
00:39:45,000 --> 00:39:48,240
And then you dive into, then you branch out and look at these other predictions like

635
00:39:48,240 --> 00:39:51,840
blood pressure and BMI and cardiovascular events and things.

636
00:39:51,840 --> 00:39:56,120
And, you know, and doing this sort of statistical analysis to prove that that was actually working,

637
00:39:56,120 --> 00:39:59,400
that probably took another three to four months or something.

638
00:39:59,400 --> 00:40:04,480
And then there's, of course, a lot of work that goes into publishing manuscript.

639
00:40:04,480 --> 00:40:08,120
And so, you know, we went through rounds of peer reviews and things to get that out

640
00:40:08,120 --> 00:40:09,120
there.

641
00:40:09,120 --> 00:40:10,120
Sure.

642
00:40:10,120 --> 00:40:11,120
Sure.

643
00:40:11,120 --> 00:40:14,320
And this particular data set, the Biobank data set, or either of them, really, are they

644
00:40:14,320 --> 00:40:18,720
publicly available or are there hoops that you needed to jump through to get access to

645
00:40:18,720 --> 00:40:19,720
them?

646
00:40:19,720 --> 00:40:22,320
They're publicly available to qualified researchers.

647
00:40:22,320 --> 00:40:29,200
It's a process of applying for access and basically stating that you have a stated

648
00:40:29,200 --> 00:40:34,200
research goal in mind and then they decide to grant you access to that data and you can

649
00:40:34,200 --> 00:40:35,200
use it.

650
00:40:35,200 --> 00:40:36,200
Yeah.

651
00:40:36,200 --> 00:40:37,200
Interesting.

652
00:40:37,200 --> 00:40:42,200
And are there other interesting data sets on your radar?

653
00:40:42,200 --> 00:40:43,200
Oh, okay.

654
00:40:43,200 --> 00:40:50,520
So, just to talk about the UK Biobank again, they're adding, currently adding genomics data

655
00:40:50,520 --> 00:40:51,520
to the data set.

656
00:40:51,520 --> 00:40:57,520
And that'll be really exciting to see what's there and what we can predict from it.

657
00:40:57,520 --> 00:41:03,040
And in terms of fundus imagery, there are other data collections that are available.

658
00:41:03,040 --> 00:41:07,960
You know, they're these things like sort of Kaggle-like competitions or machine learning

659
00:41:07,960 --> 00:41:13,080
competitions and like a few of them are on fundus imagery and so those data sets are

660
00:41:13,080 --> 00:41:14,080
available.

661
00:41:14,080 --> 00:41:15,080
Awesome.

662
00:41:15,080 --> 00:41:18,360
Well, Ryan, this has been super interesting.

663
00:41:18,360 --> 00:41:23,760
Do you have any final thoughts or words for folks or anything that we didn't cover?

664
00:41:23,760 --> 00:41:25,960
Any questions that I should have asked?

665
00:41:25,960 --> 00:41:26,960
Yes.

666
00:41:26,960 --> 00:41:27,960
It's been super fun.

667
00:41:27,960 --> 00:41:28,960
Thank you so much.

668
00:41:28,960 --> 00:41:34,600
One parting thing maybe is I'm super excited about the fact that this work was done in

669
00:41:34,600 --> 00:41:41,240
sort of feature lists kind of unbiased way in which I am certainly not an ophthalmologist

670
00:41:41,240 --> 00:41:46,520
and don't know anything about retinus, but we were able to sort of tell the model to

671
00:41:46,520 --> 00:41:49,760
use its features to learn about a patient's health.

672
00:41:49,760 --> 00:41:54,760
And then we surprisingly learned about a bunch of different statistical correlations that

673
00:41:54,760 --> 00:41:59,960
were in the data that we wouldn't have found if we had sort of went into it like sort

674
00:41:59,960 --> 00:42:04,640
of presupposing what the features might be and looking at things like the width of the

675
00:42:04,640 --> 00:42:06,160
blood vessels or things.

676
00:42:06,160 --> 00:42:12,160
And so I do like that this is potentially a method of scientific discovery that could

677
00:42:12,160 --> 00:42:16,000
be used in the future and people should think about trying that.

678
00:42:16,000 --> 00:42:22,720
Yeah, that sounds like a general advertisement for deep learning and you know kind of a data

679
00:42:22,720 --> 00:42:26,080
first as opposed to a feature first approach.

680
00:42:26,080 --> 00:42:27,920
Is that is that kind of where you're headed?

681
00:42:27,920 --> 00:42:28,920
Yeah, that's right.

682
00:42:28,920 --> 00:42:29,920
Awesome.

683
00:42:29,920 --> 00:42:30,920
Awesome.

684
00:42:30,920 --> 00:42:31,920
Well, once again, Ryan.

685
00:42:31,920 --> 00:42:32,920
Thank you so much.

686
00:42:32,920 --> 00:42:33,920
I appreciate you taking the time.

687
00:42:33,920 --> 00:42:34,920
Thank you.

688
00:42:34,920 --> 00:42:38,920
All right, everyone.

689
00:42:38,920 --> 00:42:40,640
That's our show for today.

690
00:42:40,640 --> 00:42:45,240
For more information on Ryan or any of the topics covered in this episode, you'll find

691
00:42:45,240 --> 00:42:51,120
the show notes at twimmolai.com slash talk slash 122.

692
00:42:51,120 --> 00:42:55,560
We heard from a lot of you over the weekend about our last episode on the reproducibility

693
00:42:55,560 --> 00:43:00,400
crisis and the philosophy of data and we really appreciate your comments.

694
00:43:00,400 --> 00:43:04,880
If you want to get in on the conversation, be sure to hit us up at Sam Charrington or

695
00:43:04,880 --> 00:43:10,260
at twimmolai on Twitter or via the show notes page, which we'll link to in the notes for

696
00:43:10,260 --> 00:43:11,960
this show.

697
00:43:11,960 --> 00:43:22,600
Thanks once again for listening and catch you next time.

