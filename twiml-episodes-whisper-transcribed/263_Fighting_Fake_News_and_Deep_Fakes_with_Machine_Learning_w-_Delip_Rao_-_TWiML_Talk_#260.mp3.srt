1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:28,720
I'm your host Sam Charrington.

4
00:00:28,720 --> 00:00:33,040
Before we get going, I'd like to send a huge thanks to our friends at HPE for sponsoring

5
00:00:33,040 --> 00:00:38,080
this week's series of shows from the O'Reilly AI Conference in New York City.

6
00:00:38,080 --> 00:00:43,160
At the conference, HPE presented on InfoSight, which is the company's cloud-based AI ops

7
00:00:43,160 --> 00:00:49,240
solution for helping IT organizations better manage and ensure the health of their IT infrastructure

8
00:00:49,240 --> 00:00:50,240
using AI.

9
00:00:50,240 --> 00:00:55,440
I've previously written about AI ops and it's definitely an interesting use case for machine

10
00:00:55,440 --> 00:00:56,440
learning.

11
00:00:56,440 --> 00:01:05,720
To check out what HPE InfoSight is up to in this space, visit twimbleai.com slash HPE.

12
00:01:05,720 --> 00:01:08,560
All right everyone, I am on the line with Dellip Rao.

13
00:01:08,560 --> 00:01:13,280
Dellip is the vice president of research at the AI foundation.

14
00:01:13,280 --> 00:01:20,560
Previously, he founded the research consulting company Juiceware and the fake news challenge

15
00:01:20,560 --> 00:01:21,560
as well.

16
00:01:21,560 --> 00:01:26,120
He's one of my favorite AI people to follow on Twitter where he tweets him at Dellip Rao.

17
00:01:26,120 --> 00:01:28,920
Dellip, welcome to this week in machine learning and AI.

18
00:01:28,920 --> 00:01:30,400
Thank you for having me.

19
00:01:30,400 --> 00:01:33,320
I am really looking forward to our conversation.

20
00:01:33,320 --> 00:01:39,960
You recently joined the AI foundation where again, you'll be leading at research.

21
00:01:39,960 --> 00:01:46,120
Tell us a little bit about your background and what sparked your interest in AI in a

22
00:01:46,120 --> 00:01:48,880
particular natural language processing.

23
00:01:48,880 --> 00:01:50,840
Yeah, absolutely.

24
00:01:50,840 --> 00:01:57,040
So, my background is in research, particularly in natural language processing research.

25
00:01:57,040 --> 00:02:08,480
And I grew up in India and did typical engineering track that a lot of kids in India do.

26
00:02:08,480 --> 00:02:16,720
And I was pretty sure I was going to become some kind of an academic with very early on

27
00:02:16,720 --> 00:02:26,640
in my career, in my college life and because I found teaching very inspiring and I really

28
00:02:26,640 --> 00:02:28,880
wanted to be a teacher.

29
00:02:28,880 --> 00:02:31,760
And it was not AI, it was not my choice.

30
00:02:31,760 --> 00:02:38,280
In the beginning, I actually wanted to become, to do research and distributed computing.

31
00:02:38,280 --> 00:02:47,120
And I realized that, you know, there was this one school where I went to IT Madras and

32
00:02:47,120 --> 00:02:51,640
this one, there was a faculty who was working on distributed computing and he was on the

33
00:02:51,640 --> 00:02:52,640
sabbatical.

34
00:02:52,640 --> 00:03:05,720
They kind of put me in charge in care of another faculty who was working on AI and this

35
00:03:05,720 --> 00:03:11,160
person was supposed to babysit me while the actual professor, you know, returned back

36
00:03:11,160 --> 00:03:15,240
from sabbatical and he did a very dangerous thing.

37
00:03:15,240 --> 00:03:21,360
So, he gave me a couple of books, thinking that, you know, it'll keep me busy.

38
00:03:21,360 --> 00:03:24,760
And I just got really hooked.

39
00:03:24,760 --> 00:03:29,320
So, after reading this couple of books, I just felt like, okay, this is what I should

40
00:03:29,320 --> 00:03:32,280
be working on and sort of distributed computing.

41
00:03:32,280 --> 00:03:36,640
And that's when I started working on AI.

42
00:03:36,640 --> 00:03:41,840
And this person was actually like a very hard core, like old school AI person, like we

43
00:03:41,840 --> 00:03:45,880
were working on like planning problems and so on.

44
00:03:45,880 --> 00:03:51,720
So I actually started looking into AI planning actually to begin with and actually looking

45
00:03:51,720 --> 00:03:54,560
at search algorithms and so on.

46
00:03:54,560 --> 00:04:01,000
And it was only like around that time, I just got like curious about natural language

47
00:04:01,000 --> 00:04:07,280
processing because the data that we were working with involved text and natural language

48
00:04:07,280 --> 00:04:11,640
processing was supposed to be some kind of like pre-processing I was supposed to be doing,

49
00:04:11,640 --> 00:04:14,760
but instead I found the pre-processing itself super interesting.

50
00:04:14,760 --> 00:04:19,840
So, I kind of moved into natural language processing and then I went to graduate school

51
00:04:19,840 --> 00:04:27,320
the class of that, like, you know, I felt like I had to study more in AI and this is all

52
00:04:27,320 --> 00:04:29,400
ancient history, by the way.

53
00:04:29,400 --> 00:04:36,240
So, do you happen to remember the name of the books that your babysitter professor gave

54
00:04:36,240 --> 00:04:37,240
you to read?

55
00:04:37,240 --> 00:04:46,160
Yeah, I mean, I think one of them was actually the book by Nilton, who recently passed

56
00:04:46,160 --> 00:04:50,960
away, like I think that's the end of yesterday or day before.

57
00:04:50,960 --> 00:04:57,120
And yeah, he was a professor at Stanford and he wrote one of these very early AI text

58
00:04:57,120 --> 00:05:05,700
books, and then there was also the classic book by Peter Narveg, a story, a Russell and

59
00:05:05,700 --> 00:05:07,200
Narveg, right?

60
00:05:07,200 --> 00:05:11,840
And so, yeah, I started off with those two books.

61
00:05:11,840 --> 00:05:20,000
The Nilton, Nilton book is like a really great gateway drug for anyone, I like men.

62
00:05:20,000 --> 00:05:25,760
And then, of course, the Peter Narveg book, you know, puts you on a more serious route.

63
00:05:25,760 --> 00:05:29,640
And so, you, I think we veered off at grad school.

64
00:05:29,640 --> 00:05:32,640
Did you start juiced where right after grad school?

65
00:05:32,640 --> 00:05:38,600
Now, far from it, actually, I worked at a bunch of different places, you know, I worked

66
00:05:38,600 --> 00:05:49,800
at Google Research as a kind of like a repeat intern, and then I went to Twitter, and at

67
00:05:49,800 --> 00:05:55,120
that time, Twitter was pretty early, and it was way before that IPO.

68
00:05:55,120 --> 00:06:03,880
I was like, you know, employee number 500 something, and yeah, I was one of the first machine

69
00:06:03,880 --> 00:06:09,840
learning hires to join their anti-spam team.

70
00:06:09,840 --> 00:06:15,720
And they were that until that point, and they just formed this anti-spam team.

71
00:06:15,720 --> 00:06:22,480
When I joined, like, you know, a few months ago, and they were writing all sorts of interesting

72
00:06:22,480 --> 00:06:29,200
rules to catch them, and I was like, oh, no, that's not how it should be done, but, but

73
00:06:29,200 --> 00:06:35,760
actually, you know, I got, I got schooled because I felt like, oh, I came in the wild

74
00:06:35,760 --> 00:06:40,880
saying, thinking that here, I, here I am with all my training, I'm going to go change

75
00:06:40,880 --> 00:06:42,560
the way things work.

76
00:06:42,560 --> 00:06:46,600
But then, that's when my first humbling experience happened.

77
00:06:46,600 --> 00:06:51,120
I felt I discovered that, you know, real products don't work that way.

78
00:06:51,120 --> 00:06:56,080
You can't build, like, simple models that will, like, that you can just unleash it on

79
00:06:56,080 --> 00:06:59,440
the world and expect it to work.

80
00:06:59,440 --> 00:07:05,560
And they were doing a lot of things coming from deep experience, product experience.

81
00:07:05,560 --> 00:07:12,000
And that's when I kind of, like, completely shifted from my academic researcher mindset

82
00:07:12,000 --> 00:07:15,120
to a more product-based researcher mindset.

83
00:07:15,120 --> 00:07:20,880
Are there some specific examples you can give of where that disconnect really fell for

84
00:07:20,880 --> 00:07:21,880
you?

85
00:07:21,880 --> 00:07:22,880
Oh, absolutely.

86
00:07:22,880 --> 00:07:29,040
I mean, like, for example, like, data set drift is something that we kind of barely refer

87
00:07:29,040 --> 00:07:31,680
to in academia, right?

88
00:07:31,680 --> 00:07:39,240
And we, the model drift, model drift is something that we barely refer to in academia,

89
00:07:39,240 --> 00:07:42,680
but it actually happens all the time in production.

90
00:07:42,680 --> 00:07:47,400
Your model's stopping relevant, and you have to kind of, like, figure out, like, some kind

91
00:07:47,400 --> 00:07:52,280
of a feedback loop to start, like, to keep retraining your models.

92
00:07:52,280 --> 00:07:58,880
And even sometimes, you know, there is this, even you don't know much, there is a kind

93
00:07:58,880 --> 00:08:04,840
of a cockiness that comes in where you say that, oh, you can build this model, you can

94
00:08:04,840 --> 00:08:07,920
solve this easily by building a model.

95
00:08:07,920 --> 00:08:14,400
But then you realize that, you know, getting the model done and building all the engineering

96
00:08:14,400 --> 00:08:21,960
around the feedback loop for the model itself is so much more expensive than, like, you know,

97
00:08:21,960 --> 00:08:28,560
sometimes even writing a few rules or writing something very simplistic, which, to a researcher

98
00:08:28,560 --> 00:08:33,880
especially coming from an academic background, it almost feels like, oh, why are we doing

99
00:08:33,880 --> 00:08:34,880
this?

100
00:08:34,880 --> 00:08:35,880
Right?

101
00:08:35,880 --> 00:08:36,880
Like, this is not science.

102
00:08:36,880 --> 00:08:37,880
Why are we doing this?

103
00:08:37,880 --> 00:08:43,920
And this is more importantly, I think, in academia as well as in research, we're always

104
00:08:43,920 --> 00:08:49,360
encouraged to stay, you know, push down a lot of times, stay ahead of the game, right?

105
00:08:49,360 --> 00:08:56,400
And when you see some heavily engineering systems built using rules, the natural feeling

106
00:08:56,400 --> 00:09:02,360
comes into thinking that, oh, this is all, like, outdated, and this, and there is, it's

107
00:09:02,360 --> 00:09:07,680
kind of very natural for somebody coming from academia to look at it and feel a little,

108
00:09:07,680 --> 00:09:11,000
like, this is all outdated stuff.

109
00:09:11,000 --> 00:09:19,240
But I think with experience, there is this enlightenment that creeps in, where you realize

110
00:09:19,240 --> 00:09:25,920
that no, actually, that the so-called outdated stuff is actually, like, the most appropriate

111
00:09:25,920 --> 00:09:30,040
thing to do in this context.

112
00:09:30,040 --> 00:09:37,520
And that's when, you know, I kind of, it took me a few experiences like that, right?

113
00:09:37,520 --> 00:09:42,320
And, of course, I mean, I gave you an example of model draft, another is, like, you know,

114
00:09:42,320 --> 00:09:48,320
you build a model, the model does really well, then you're very proud of it, and then you

115
00:09:48,320 --> 00:09:58,560
try, and then, you know, in a company like Twitter, everything gets, baby-tested, carefully,

116
00:09:58,560 --> 00:10:04,840
and you end up also not just getting abtested, but also you're tested for how much compute

117
00:10:04,840 --> 00:10:08,280
that your setup is taking, right?

118
00:10:08,280 --> 00:10:12,440
Because you're operating at that massive scale.

119
00:10:12,440 --> 00:10:18,880
And if a simple rule-based thing is accomplishing pretty much everything that your model thing

120
00:10:18,880 --> 00:10:23,360
is doing, and your model is just adding, like, you know, a few decimal point improvement

121
00:10:23,360 --> 00:10:29,400
or even, like, a percentage point improvement, it gets, it becomes a wash, but it comes

122
00:10:29,400 --> 00:10:32,800
at an enormous cost to the business, right?

123
00:10:32,800 --> 00:10:35,200
And so, it just does not make sense.

124
00:10:35,200 --> 00:10:39,360
And as a researcher, you don't think about that, because you're always looking at, like,

125
00:10:39,360 --> 00:10:44,600
oh, can I improve on state of the art, even if, as long as the improvements are statistically

126
00:10:44,600 --> 00:10:48,320
significant, and I can publish, I can call it a win.

127
00:10:48,320 --> 00:10:54,960
And that's where, like, you know, a disconnect happened for me, and, like, kind of, I felt,

128
00:10:54,960 --> 00:11:01,000
like, oh, there is a different way to do science that is relevant in the real world, and

129
00:11:01,000 --> 00:11:03,240
I want to practice that.

130
00:11:03,240 --> 00:11:08,500
Have you developed that to the point where you've got some kind of fundamental tenets

131
00:11:08,500 --> 00:11:13,640
of science in the real world, or is it more a general idea and approach?

132
00:11:13,640 --> 00:11:16,640
Yeah, I did.

133
00:11:16,640 --> 00:11:24,640
You know, some of it, I kind of, I alluded to in the book that we published, we can talk

134
00:11:24,640 --> 00:11:25,960
about it later.

135
00:11:25,960 --> 00:11:29,800
But, you know, I was going to say that I worked at Twitter, and then I worked at Amazon,

136
00:11:29,800 --> 00:11:35,280
and Amazon was also another amazing experience, and it taught me a whole bunch of different

137
00:11:35,280 --> 00:11:40,400
things on similar, along similar lines.

138
00:11:40,400 --> 00:11:44,720
And I, after spending time at these two places, I started to use Twitter, which was a research

139
00:11:44,720 --> 00:11:51,280
consulting company, and it was a research consulting company kind of built with that mindset,

140
00:11:51,280 --> 00:11:52,280
right?

141
00:11:52,280 --> 00:11:59,200
When you do science, that is, that will be baked into products, it has to be developed

142
00:11:59,200 --> 00:12:06,440
differently, then the science that we do typically in graduate school labs that's meant for

143
00:12:06,440 --> 00:12:12,840
writing a paper, and that was the mindset with which, you know, just where it was created,

144
00:12:12,840 --> 00:12:19,160
and in fact, all our clients, we worked out, we worked with, we actually built solutions

145
00:12:19,160 --> 00:12:25,680
that could be deployed, not a single solution that shipped from us, was like, you know, a

146
00:12:25,680 --> 00:12:28,400
paperware or a shelfware, right?

147
00:12:28,400 --> 00:12:35,760
So, you mentioned the book, and I had this on my list of things to mention in your intro,

148
00:12:35,760 --> 00:12:37,120
so I'm remiss in that regard.

149
00:12:37,120 --> 00:12:42,600
You just published a book, in fact, that's sitting here on my desk, Natural Language Processing

150
00:12:42,600 --> 00:12:47,560
with PyTorch, which you co-authored with Brian McMahon.

151
00:12:47,560 --> 00:12:52,280
And so, we'll definitely dig into that a little bit more.

152
00:12:52,280 --> 00:12:59,440
You recently joined the AI Foundation, which I hadn't previously heard about, but read

153
00:12:59,440 --> 00:13:05,680
a little bit about, and it's got this interesting kind of for-profit, non-profit structure,

154
00:13:05,680 --> 00:13:11,400
reminding me a little bit of kind of some of open AI's recent announcements, in terms

155
00:13:11,400 --> 00:13:15,880
of the direction they're going, can you tell us a little bit about the AI Foundation

156
00:13:15,880 --> 00:13:20,320
and kind of that structure and what the organization is up to in general?

157
00:13:20,320 --> 00:13:27,120
Sure, in fact, the funny thing is now people are referring to, I mean, using open AI

158
00:13:27,120 --> 00:13:32,840
as an analogy, but we were doing, we established this structure almost like more than a year

159
00:13:32,840 --> 00:13:33,840
ago.

160
00:13:33,840 --> 00:13:40,440
And in a way, I think this is great for mission-oriented companies.

161
00:13:40,440 --> 00:13:47,080
AI Foundation is a hybrid for-profit and non-profit, and at the for-profit, we are actually

162
00:13:47,080 --> 00:13:52,240
interested in building all kinds of synthetic content.

163
00:13:52,240 --> 00:13:56,560
And in the non-profit, we are actually interested in detecting the synthetic content.

164
00:13:56,560 --> 00:14:04,480
So actually, we started off with, or at least the founders, they were interested in solving

165
00:14:04,480 --> 00:14:06,640
the detection problem.

166
00:14:06,640 --> 00:14:11,600
But then, you can't just go off and build a non-profit and keep it sustainable, right?

167
00:14:11,600 --> 00:14:19,440
You need something to power the non-profit, and a far-profit is like a very sustainable

168
00:14:19,440 --> 00:14:29,080
way to power a non-profit, like before I started working at AI Foundation, I started

169
00:14:29,080 --> 00:14:36,280
the fake news challenge in a very non-profit mode, and that was not easily sustainable

170
00:14:36,280 --> 00:14:42,520
just based on grant money, sure, we want some grant money from night foundation, but

171
00:14:42,520 --> 00:14:48,240
it becomes a massive exercise, almost like, and it's not a sustainable way to build

172
00:14:48,240 --> 00:14:49,720
a non-profit.

173
00:14:49,720 --> 00:14:54,560
So when I learned about AI Foundation, and I learned that about their mission, which

174
00:14:54,560 --> 00:15:02,120
was to detect any kind of synthetic media on the internet or anywhere, and it includes

175
00:15:02,120 --> 00:15:10,520
generated video, generated audio, as well as generated text, I was obviously very excited

176
00:15:10,520 --> 00:15:17,040
about that, and then, you know, I was curious how they were actually approaching it.

177
00:15:17,040 --> 00:15:22,400
And actually, it was their way of approaching that may solve me more than, you know, the

178
00:15:22,400 --> 00:15:28,440
mission itself, because I was already sold on the mission, I kind of, even before I met

179
00:15:28,440 --> 00:15:32,280
a disability AI Foundation.

180
00:15:32,280 --> 00:15:38,800
So the far-profit is building products, like, you know, is doing all these interesting

181
00:15:38,800 --> 00:15:44,320
synthesis work in the context of AR and VR, right?

182
00:15:44,320 --> 00:15:49,720
And this is great because one of my experiences is that, you know, in order to be really good

183
00:15:49,720 --> 00:15:55,960
at detection, you also need to be really good at generation, and the way the far-profit

184
00:15:55,960 --> 00:16:02,280
and non-profit structure is that, like, you know, the far-profit is actually building technology

185
00:16:02,280 --> 00:16:09,760
that is so strong and generation, that the non-profit, the technology that non-profit

186
00:16:09,760 --> 00:16:14,520
is building also gets strengthened, kind of like a real-world GAN, if you will, right?

187
00:16:14,520 --> 00:16:18,280
Like, they're both reinforcing each other.

188
00:16:18,280 --> 00:16:24,360
And of course, it helps for the far-profit folks to, like, go spend some hours working

189
00:16:24,360 --> 00:16:29,440
on the non-profit stuff, and it's a really great model.

190
00:16:29,440 --> 00:16:35,040
Sounds like there's some commonalities between the kind of problems you're trying to solve

191
00:16:35,040 --> 00:16:40,640
with the fake news challenge and what you'll be working on at AI Foundation.

192
00:16:40,640 --> 00:16:47,400
I'm curious, what were the key takeaways from your experience launching the fake news

193
00:16:47,400 --> 00:16:49,840
challenge and conducting that?

194
00:16:49,840 --> 00:16:55,560
Oh, yeah, absolutely. Fake news challenge was another one of those humbling experiences

195
00:16:55,560 --> 00:17:00,040
in life, and I think this is great.

196
00:17:00,040 --> 00:17:04,000
Everybody should go through multiple such humbling experiences, and that's where, like,

197
00:17:04,000 --> 00:17:06,240
a lot of growth sports happen.

198
00:17:06,240 --> 00:17:13,080
So when I started seeing, around the 2016 elections, when I started seeing a lot of misinformation

199
00:17:13,080 --> 00:17:18,080
on Twitter, I'm very active on Twitter, you know, and I have been for quite some time.

200
00:17:18,080 --> 00:17:23,120
Now, I felt like, oh, there is all this misinformation coming.

201
00:17:23,120 --> 00:17:27,040
Why is Twitter not doing anything about it?

202
00:17:27,040 --> 00:17:29,880
Why are these platform companies, especially Facebook?

203
00:17:29,880 --> 00:17:37,000
I mean, at some point, I used to be on Facebook, and I was seeing a whole bunch of random,

204
00:17:37,000 --> 00:17:42,680
like, complete conspiracy theory, like, not stuff being shared on Facebook.

205
00:17:42,680 --> 00:17:47,800
I mean, I'm no longer on Facebook, but I just felt like the platform companies were

206
00:17:47,800 --> 00:17:53,000
letting us down by not working on this problem, and there was also a time where, like, you

207
00:17:53,000 --> 00:17:58,280
know, the, like, Mark Zuckerberg went on stage saying that his Facebook does not have

208
00:17:58,280 --> 00:18:00,040
a misinformation problem, right?

209
00:18:00,040 --> 00:18:06,680
Like, this was in 2016, and around that time, there was not just me, but a whole bunch

210
00:18:06,680 --> 00:18:13,920
of other people who are interested in this problem started talking on Twitter, and I ended

211
00:18:13,920 --> 00:18:18,640
up partnering with Dean Palmer-Lew.

212
00:18:18,640 --> 00:18:21,400
He was a professor at Carnegie Mellon.

213
00:18:21,400 --> 00:18:30,040
He was, like, one of the pioneers of the autonomous driving program developed by DARPA.

214
00:18:30,040 --> 00:18:37,720
And he was doing self-driving cars in the 80s, which is crazy, think about that.

215
00:18:37,720 --> 00:18:44,320
So, anyway, Dean and I started the fake news challenge, and we both were, like, you

216
00:18:44,320 --> 00:18:46,320
know, we are both entrepreneurs, right?

217
00:18:46,320 --> 00:18:52,840
And we have this sort of, like, entrepreneurial optimism, and at the same time, we are academics,

218
00:18:52,840 --> 00:18:57,680
so with academic background, you know, we think that we can solve anything.

219
00:18:57,680 --> 00:19:05,040
And we thought that, oh, we can totally, like, build some kind of natural language processing

220
00:19:05,040 --> 00:19:10,720
system to kind of detect fake news.

221
00:19:10,720 --> 00:19:18,600
And it didn't take us too long to realize how nuanced this problem is, and the more

222
00:19:18,600 --> 00:19:23,840
we kept, we, and then, you know, we started talking to fact checkers and journalists.

223
00:19:23,840 --> 00:19:26,120
I spent, I don't know, much.

224
00:19:26,120 --> 00:19:31,640
I can't even count how many hours with fact checkers and journalists, and I realized

225
00:19:31,640 --> 00:19:37,840
that gosh, this problem is so complicated, and the work they're doing is so complicated.

226
00:19:37,840 --> 00:19:45,160
It's not going to be any one system or approach that will solve this, but it will be, like,

227
00:19:45,160 --> 00:19:54,000
you know, a combination of, you know, approaches involving, modeling, it involving human in the

228
00:19:54,000 --> 00:19:56,000
loop, et cetera.

229
00:19:56,000 --> 00:20:02,880
And that's, that's when, like, we, when we started fake, fake news challenge, we thought

230
00:20:02,880 --> 00:20:05,600
we were going to come up with a solution, right?

231
00:20:05,600 --> 00:20:11,560
And that was like the kernel of the idea that will come up with a solution.

232
00:20:11,560 --> 00:20:15,480
And then, you know, we thought, oh, maybe it's not one solution, it's multiple solution.

233
00:20:15,480 --> 00:20:18,120
And then we thought, no, it's not even multiple solution.

234
00:20:18,120 --> 00:20:22,840
What do you want is the community of people talking to each other.

235
00:20:22,840 --> 00:20:30,680
And that will create, like, a factory for coming up with multiple solutions and create

236
00:20:30,680 --> 00:20:33,120
more conversations, right?

237
00:20:33,120 --> 00:20:39,880
And so fake news challenge ended up becoming this sort of, like, a community where, you

238
00:20:39,880 --> 00:20:45,080
know, researchers, and natural English processing, computer vision, et cetera, behind also

239
00:20:45,080 --> 00:20:50,360
the fake news problem could come and interact with journalists and fact checkers.

240
00:20:50,360 --> 00:20:54,400
And basically, these were two, until the fake news challenge happened, there were two

241
00:20:54,400 --> 00:21:00,160
different communities that were not talking with each other as much as they are today,

242
00:21:00,160 --> 00:21:01,160
right?

243
00:21:01,160 --> 00:21:02,160
Right.

244
00:21:02,160 --> 00:21:05,480
And I'm not claiming that, you know, the fake news challenge completely changed that,

245
00:21:05,480 --> 00:21:07,480
but I think it set the tone.

246
00:21:07,480 --> 00:21:13,200
And as a consequence, what happened was there were a lot of people who ended up meeting

247
00:21:13,200 --> 00:21:14,200
there.

248
00:21:14,200 --> 00:21:18,040
Some of them ended up starting their own companies around this space.

249
00:21:18,040 --> 00:21:23,080
Some of them ended up starting other events around this.

250
00:21:23,080 --> 00:21:28,280
And it created a whole bunch of conversation.

251
00:21:28,280 --> 00:21:35,360
And I think there is, like, a snowballing effect that happened after that very shortly.

252
00:21:35,360 --> 00:21:40,280
So I think your original question was, what were the key takeaways from the fake news

253
00:21:40,280 --> 00:21:41,280
challenge?

254
00:21:41,280 --> 00:21:48,120
I mean, the one takeaway was, like, you know, always be humble, like, keep the world

255
00:21:48,120 --> 00:21:52,200
is more complex than you initially think at us.

256
00:21:52,200 --> 00:21:59,160
And there are surprising ways in which we keep learning that and relearning that.

257
00:21:59,160 --> 00:22:07,120
And the other takeaway is, we found that, okay, we put together this community, this community

258
00:22:07,120 --> 00:22:11,600
needs to be engaged, we need to do something about it.

259
00:22:11,600 --> 00:22:19,920
So we created a shared task and I would, like, advise anybody to not take up this job

260
00:22:19,920 --> 00:22:22,680
of creating a shared task.

261
00:22:22,680 --> 00:22:30,120
It's one of those really complicated jobs that require multiple people working together

262
00:22:30,120 --> 00:22:33,480
in order to pull it off, like, really well.

263
00:22:33,480 --> 00:22:38,080
And despite me working on it full time, and even Dean spending a fair amount of which

264
00:22:38,080 --> 00:22:45,120
time on it, there was, it was super challenging to pull it off, right?

265
00:22:45,120 --> 00:22:51,960
And the shared task in this case is the creation of the community or the creation of a solution

266
00:22:51,960 --> 00:22:53,360
to the problem.

267
00:22:53,360 --> 00:23:00,640
So what we did was I put my consulting hat on and then I looked at, okay, I'm going

268
00:23:00,640 --> 00:23:05,280
to talk to a whole bunch of fat checkers and I'm going to find out what are the core problems

269
00:23:05,280 --> 00:23:06,720
that they are dealing with.

270
00:23:06,720 --> 00:23:11,920
And if there is any science problem that needs to be solved in order to solve this problem,

271
00:23:11,920 --> 00:23:12,920
right?

272
00:23:12,920 --> 00:23:18,720
And I, you know, it was obvious that one of the common problems that kept coming over

273
00:23:18,720 --> 00:23:22,640
and over it was, was that of volume.

274
00:23:22,640 --> 00:23:28,440
So almost all fat checkers are, like, inundated with articles and they need to fact check and

275
00:23:28,440 --> 00:23:33,800
many of these articles are like, you know, pretty much rehashing the same thing or different

276
00:23:33,800 --> 00:23:36,120
variations of the same theme.

277
00:23:36,120 --> 00:23:42,360
And therefore, you know, it's not, they can't be very efficient about how the fat check,

278
00:23:42,360 --> 00:23:43,360
right?

279
00:23:43,360 --> 00:23:50,040
And how they assign labels to these articles or any kind of additional context around

280
00:23:50,040 --> 00:23:52,040
these articles.

281
00:23:52,040 --> 00:24:00,920
And what we did was we created a dataset where, if I give you two articles or two, let's

282
00:24:00,920 --> 00:24:11,120
say, two pieces of text, can you tell me if the two are related, unrelated, discussing

283
00:24:11,120 --> 00:24:20,440
about each, you know, about the same topic or they contradict each other, right?

284
00:24:20,440 --> 00:24:25,760
And it's a surprisingly hard natural language processing problem, right?

285
00:24:25,760 --> 00:24:35,000
Like, it's, I mean, maybe not unsurprisingly hard, natural language processing problem.

286
00:24:35,000 --> 00:24:39,920
Because you're not dissolving textual entailment, you're solving a whole bunch of other things

287
00:24:39,920 --> 00:24:42,080
in order to get it right.

288
00:24:42,080 --> 00:24:48,240
And a lot of teams competed on it, again, you know, it's not one of those things where

289
00:24:48,240 --> 00:24:54,160
if the dataset is still out there, it's not like an MNEST or something where, you know,

290
00:24:54,160 --> 00:24:57,200
people have solved that dataset and moved on, right?

291
00:24:57,200 --> 00:25:00,240
It's a really hard challenging problem.

292
00:25:00,240 --> 00:25:06,680
In fact, the problem was so interesting and challenging that it got used in a lot of

293
00:25:06,680 --> 00:25:13,320
university and LP courses, ranging from MIT to Stanford, like, you know, every big and

294
00:25:13,320 --> 00:25:19,440
small university departments offering, you know, key courses in class projects and so on.

295
00:25:19,440 --> 00:25:28,920
People have published on the dataset and, yeah, so the shared task was actually like a

296
00:25:28,920 --> 00:25:36,600
great way to keep the community engaged, but man, it's really time consuming to actually

297
00:25:36,600 --> 00:25:42,040
get that done and to make sure the evaluation, et cetera, was happening, right?

298
00:25:42,040 --> 00:25:49,480
And we kind of, like, I wanted to withdraw in a little more excitement to this, so just

299
00:25:49,480 --> 00:25:55,840
where I decided to sponsor, like, prices for the top three.

300
00:25:55,840 --> 00:26:03,840
And it became quite successful, like, you know, there were more than 1,000 people who registered

301
00:26:03,840 --> 00:26:11,040
for the challenge and forget the exact numbers now, but there were like a few hundred teams

302
00:26:11,040 --> 00:26:14,880
who were actually competing in it.

303
00:26:14,880 --> 00:26:21,960
And yeah, I mean, we had to, it was all automated, the evaluation, et cetera, and we ended up,

304
00:26:21,960 --> 00:26:28,520
you know, finding the top three and awarding them prizes and so on.

305
00:26:28,520 --> 00:26:37,000
And I would say the learnings from the fitness challenge were, you know, one half was

306
00:26:37,000 --> 00:26:44,120
technical, I would say as it was as much non-technical as it was technical, the non-technical or

307
00:26:44,120 --> 00:26:51,560
softer aspects were like the domain related problems, like how complicated the domain is

308
00:26:51,560 --> 00:26:58,480
and who are the key people working on the domain and how we can facilitate and how to

309
00:26:58,480 --> 00:27:04,040
build a community and how to sustain the community and so on.

310
00:27:04,040 --> 00:27:10,760
And the technical parts of the thing is actually, to me, it somewhat least interesting.

311
00:27:10,760 --> 00:27:17,800
I guess it was more like, how do we come up with the data set for this situation and

312
00:27:17,800 --> 00:27:22,600
we came up with some interesting tricks to build the data set, but yeah.

313
00:27:22,600 --> 00:27:26,840
And now the competition ended a couple of years back.

314
00:27:26,840 --> 00:27:34,000
I'm curious, do you think that the data set and the competition is still relevant?

315
00:27:34,000 --> 00:27:41,240
And are the solutions that were proposed at the time still relevant or do you think if

316
00:27:41,240 --> 00:27:45,480
you miraculously could clone yourself to run this thing again, like would you see dramatically

317
00:27:45,480 --> 00:27:48,760
different results two years later?

318
00:27:48,760 --> 00:27:55,360
So the shared task by itself as a natural language processing task is still highly relevant

319
00:27:55,360 --> 00:28:00,520
and the approaches that were proposed, they were all unsurprisingly deep learning based

320
00:28:00,520 --> 00:28:04,600
approaches, they're all still relevant, maybe today if you were to do this, you would

321
00:28:04,600 --> 00:28:12,400
use like something like Bert or some of these beefier models, but other than that, the

322
00:28:12,400 --> 00:28:16,240
fundamental approaches are still relevant.

323
00:28:16,240 --> 00:28:27,440
That said, when we created this shared task, we called it FNC1 for a reason because we

324
00:28:27,440 --> 00:28:35,640
realized that none of the tasks that we propose is going to be representative of a fake news

325
00:28:35,640 --> 00:28:37,160
solution, right?

326
00:28:37,160 --> 00:28:43,680
And in fact, any solution to the fake news problem is actually coming from a multi-pronged

327
00:28:43,680 --> 00:28:47,480
approach and there are so many problems to be solved.

328
00:28:47,480 --> 00:28:51,520
So the volume is just one aspect of the problem.

329
00:28:51,520 --> 00:28:55,360
So we call it as FNC1 for that reason.

330
00:28:55,360 --> 00:29:01,160
So which automatically implies will there be an FNC2, FNC3 and so on?

331
00:29:01,160 --> 00:29:07,920
And the answer is yes, because it takes such a lot of effort and thought to put together

332
00:29:07,920 --> 00:29:15,200
an FNC2, I am actively working behind the scenes, talking to people around what this

333
00:29:15,200 --> 00:29:22,640
FNC2 should be like and how to make that relevant to the new kinds of problems that are facing

334
00:29:22,640 --> 00:29:31,480
because the kinds of vectors that are popping up, especially around deepfakes and things

335
00:29:31,480 --> 00:29:35,760
like that, there are different kinds of misinformation vectors coming up.

336
00:29:35,760 --> 00:29:42,600
How do we select a task that faithfully reflects what is happening today?

337
00:29:42,600 --> 00:29:51,600
Yeah, let's jump into the current state of fake content generation and detection.

338
00:29:51,600 --> 00:29:56,720
Because fake news, as you kind of traditionally defined it in FNC1 and there's a bunch

339
00:29:56,720 --> 00:30:00,720
of subproblems there, you mentioned deepfakes to you.

340
00:30:00,720 --> 00:30:07,080
When you think about the content generation and detection landscape, what do you see as

341
00:30:07,080 --> 00:30:12,080
the primary challenges and how do you organize all of that in your head?

342
00:30:12,080 --> 00:30:17,680
I guess practically every modality can be faked.

343
00:30:17,680 --> 00:30:20,120
That is audio, video, text.

344
00:30:20,120 --> 00:30:26,960
I guess maybe tomorrow if tactile becomes another modality, you can fake that too, I don't

345
00:30:26,960 --> 00:30:27,960
know.

346
00:30:27,960 --> 00:30:35,400
So right now, I think our foundation, we are interested in detection of all three dominant

347
00:30:35,400 --> 00:30:42,120
modalities, like audio, video, and text, and each of these have their own approaches for

348
00:30:42,120 --> 00:30:43,120
generation.

349
00:30:43,120 --> 00:30:50,640
So for example, with video, you might already know about a whole bunch of approaches based

350
00:30:50,640 --> 00:30:55,640
on GAMS, so these are the narrative model approaches, and then there are all these approaches

351
00:30:55,640 --> 00:31:04,880
based on image-to-image transfer, like face swap and deepfakes and so on, so that's one

352
00:31:04,880 --> 00:31:14,480
category of misinformation, another category of synthetic misinformation is like in audio,

353
00:31:14,480 --> 00:31:23,400
you can talk or turn, set the trend, but like latest wave net models have become so good

354
00:31:23,400 --> 00:31:31,000
that you have to be not a human to detect if an audio is coming from a model or if it

355
00:31:31,000 --> 00:31:38,200
is coming from a human person, right, and I would say three ways to generate fake audio.

356
00:31:38,200 --> 00:31:45,760
One is synthesis, that is obvious, another is wise conversion, where I can take your

357
00:31:45,760 --> 00:31:52,080
wise and then kind of pass it through a deep network and then do a style transfer to

358
00:31:52,080 --> 00:31:58,160
somebody else's wise, right, and so that way I can give you a power to say in anybody

359
00:31:58,160 --> 00:31:59,960
else's wise.

360
00:31:59,960 --> 00:32:06,800
And the third is, of course, replay attacks, right, where I can splice a piece of audio

361
00:32:06,800 --> 00:32:16,240
from a previous conversation in a different context and kind of like create confusion

362
00:32:16,240 --> 00:32:25,160
because I can make you say things out of context, right, and the replay, so the first two are

363
00:32:25,160 --> 00:32:32,160
actually, you need a model to detect that because especially with lots of compute and

364
00:32:32,160 --> 00:32:40,360
a lot of parameters, like really deep and wide models, you can model, like you can generate

365
00:32:40,360 --> 00:32:49,520
audio with really high fidelity, that the human ear basically cannot distinguish, right,

366
00:32:49,520 --> 00:32:56,640
like, and we are doing some experiments around that too, but what we see is many of these

367
00:32:56,640 --> 00:33:01,640
generation algorithms, right, like synthesis or wise conversion algorithms end up leaving

368
00:33:01,640 --> 00:33:09,240
some high frequency artifacts in the data, sorry, in the audio that whatever your ear misses

369
00:33:09,240 --> 00:33:18,760
the detection model can easily pick up on, right, so I mentioned two of the three audio

370
00:33:18,760 --> 00:33:24,120
based, like we're two of the three methods to generate fake audio.

371
00:33:24,120 --> 00:33:30,200
There is also a third method, which is the replay attack, and interestingly, the replay

372
00:33:30,200 --> 00:33:37,960
attack has become such, I mean, it is one of the oldest attacks, right, because you don't

373
00:33:37,960 --> 00:33:46,880
need a model to do that and it's been happening forever, there has been a lot of work on detecting

374
00:33:46,880 --> 00:33:53,600
replay attacks, and in fact, I cast bin last year, they had a challenge for detecting

375
00:33:53,600 --> 00:34:02,280
replay attacks, and basically, not last year, in 2017, they had a challenge in detecting

376
00:34:02,280 --> 00:34:11,760
replay attacks, and 2018, there are rates were around 2.5%, right, by, you know, they

377
00:34:11,760 --> 00:34:17,320
were able to ensemble all the top models, and then this, let's say, the ensemble error

378
00:34:17,320 --> 00:34:23,440
rate was like around 2.5%, and in 2018, there was this one paper I can give you, the reference

379
00:34:23,440 --> 00:34:31,720
if you want, where the error, you know, basically went down to like zero, and so interestingly,

380
00:34:31,720 --> 00:34:39,240
we can now with the help of models, almost always detect replay attacks, and that's

381
00:34:39,240 --> 00:34:45,760
possible because whenever audio recording happens, there's always like a whole bunch of,

382
00:34:45,760 --> 00:34:54,720
you know, differences that are channel and room, like ambience, related variables that

383
00:34:54,720 --> 00:35:01,240
are kind of low level acoustic qualities that are undetectable by us, but the model can

384
00:35:01,240 --> 00:35:07,880
easily pick up that change in that when you kind of like splice another audio into an

385
00:35:07,880 --> 00:35:14,600
existing audio. So on the audio side, are there, what does the data set landscape look

386
00:35:14,600 --> 00:35:19,680
like for, you know, folks want to play with this, are they creating their own data sets,

387
00:35:19,680 --> 00:35:26,680
or are there some interesting? So, like, there hasn't been any other data

388
00:35:26,680 --> 00:35:32,920
set I know other than ASV Spoof, which was put together by a bunch of other researchers

389
00:35:32,920 --> 00:35:45,920
in Google. I think the Google synthesis team is behind the ASV Spoof data set, and basically

390
00:35:45,920 --> 00:35:51,520
what they're doing is they're collecting a whole bunch of different synthesized audio,

391
00:35:51,520 --> 00:35:59,320
and from an audio that has been synthesized by a whole different kinds of methods.

392
00:35:59,320 --> 00:36:08,920
I think they're only using synthesis and wise conversion, not necessarily replay, because

393
00:36:08,920 --> 00:36:16,320
replay I feel is like probably nobody wants to work on it. Probably easier creature on

394
00:36:16,320 --> 00:36:25,000
data set for that too. I know it's a replay is kind of like a solid problem, like, you

395
00:36:25,000 --> 00:36:32,200
know, with whatever caveats, right? But sufficiently over parameterized network, we can easily

396
00:36:32,200 --> 00:36:42,320
detect a replay attack, but the other two require some amount of work. And I think this

397
00:36:42,320 --> 00:36:48,240
is actually the current ASV Spoof, like 2019, where, you know, detecting whether the audio

398
00:36:48,240 --> 00:36:58,920
came from the, whether it came from a synthesis system or a wise conversion system, or another

399
00:36:58,920 --> 00:37:04,920
interesting task is actually, can I, you know, detect change in the room parameters,

400
00:37:04,920 --> 00:37:13,800
right? So imagine, let's say I have your Alexa, right? I'm able to record you in a different

401
00:37:13,800 --> 00:37:20,400
room, and then let's say I'm talking to you, and I make you say something, and it's,

402
00:37:20,400 --> 00:37:26,760
you know, clandestinely recorded. And then I go off and play that recording to your Alexa

403
00:37:26,760 --> 00:37:35,200
and make it purchase like, I don't know, 10,000 Barbie dolls or something like that. So

404
00:37:35,200 --> 00:37:42,080
if you do that, how can you detect that, right? So can I change, so you can kind of like

405
00:37:42,080 --> 00:37:49,160
generate audio by automatically changing the room parameters, like different kinds of

406
00:37:49,160 --> 00:37:57,760
audio by generating, sorry, you can, by varying the different room parameters, like the size

407
00:37:57,760 --> 00:38:05,720
of the room, the position of the speaker, and so on, and change the reverberation, and

408
00:38:05,720 --> 00:38:09,160
that in turn changes the quality of the audio and so on.

409
00:38:09,160 --> 00:38:13,760
It's interesting to think that, you know, a retailer, for example, particularly a commerce

410
00:38:13,760 --> 00:38:19,560
retailer, you know, has long since had, you know, huge anti-fraud teams that are working

411
00:38:19,560 --> 00:38:26,760
on credit card fraud and other types of transactional fraud. It's interesting to think of how, you

412
00:38:26,760 --> 00:38:33,400
know, an Amazon or Google Now might resource audio fraud because of the kinds of scenarios

413
00:38:33,400 --> 00:38:39,120
you're describing with Alexa's that are putting in everyone's house. Yeah, so and this is

414
00:38:39,120 --> 00:38:45,880
a big problem for like a lot of banks, like, you know, banks in accepting and credit card

415
00:38:45,880 --> 00:38:52,400
companies and using audio based or vice-based authentication, right? So any speaker verification

416
00:38:52,400 --> 00:39:00,120
system can, can be filed if it's not carefully planned, it can be filed by doing some kind

417
00:39:00,120 --> 00:39:05,600
of a replay attack. Because most of the time, it's something like my vice is my password

418
00:39:05,600 --> 00:39:12,160
or something like that that you have sayings and if it is recorded under sufficiently

419
00:39:12,160 --> 00:39:19,640
high quality conditions, then I can use a replay attack and basically file any vice-indication

420
00:39:19,640 --> 00:39:20,640
system.

421
00:39:20,640 --> 00:39:25,840
Well, let's, we'll come back to some of the other modalities, but you previously alluded

422
00:39:25,840 --> 00:39:34,240
to this kind of the GAN relationship between generation and detection. In the context of

423
00:39:34,240 --> 00:39:41,280
audio, but I imagine this will apply generally, you know, how do you see this interplay kind

424
00:39:41,280 --> 00:39:50,360
of evolving between the detection and generation approaches and systems? Like now that, you

425
00:39:50,360 --> 00:39:56,160
know, we're getting so, so much better at synthesis and conversion, detecting synthesis

426
00:39:56,160 --> 00:40:01,880
and conversion attacks. Are we already seeing, you know, feeding that back into the synthesizers

427
00:40:01,880 --> 00:40:05,000
and converters and trying to make better systems?

428
00:40:05,000 --> 00:40:15,040
Yeah, and that's, that's going to be an, an arms race and we have to accept that. So

429
00:40:15,040 --> 00:40:24,480
the better the, better the detectors become, you know, you can imagine somebody with sufficient

430
00:40:24,480 --> 00:40:30,520
motivation and it's not difficult to find them, actually engineer systems to overcome that

431
00:40:30,520 --> 00:40:39,000
or figure out exactly, okay, how do I file these detectors? I think an interesting challenge

432
00:40:39,000 --> 00:40:45,440
that would have, that's not yet been conducted by anyone else, look at all the approaches

433
00:40:45,440 --> 00:40:51,000
to detection and come up with ways to break them, right? And I think that will give us

434
00:40:51,000 --> 00:40:57,960
a lot of interesting lessons into building more robust detectors. And this is true with

435
00:40:57,960 --> 00:41:05,920
any kind of adversarial setting, like, you know, starting from a fighting spam, like anti-spam.

436
00:41:05,920 --> 00:41:11,880
So you've, you come up with one approach to, like, let's say, you figured out that, you

437
00:41:11,880 --> 00:41:21,200
know, a lot of the Nigerian spam emails that you get, they are using poor language, right?

438
00:41:21,200 --> 00:41:27,040
And you try to, let's say you train some in-gram based thing that detects it, you say,

439
00:41:27,040 --> 00:41:34,080
okay, the, the perplexity is too high. Therefore, this, this could be like a signal for spam.

440
00:41:34,080 --> 00:41:39,360
But then guess what happens? They will start with copy-pasting. They won't even have to

441
00:41:39,360 --> 00:41:47,400
do, like, fancy generation. They will copy-paste, like chunks of good text from different places.

442
00:41:47,400 --> 00:41:53,760
I am file the detector, right? And now that you know that is happening, you'll start moving

443
00:41:53,760 --> 00:42:01,480
the detector in a different direction. I think this is the game of working in an adversarial

444
00:42:01,480 --> 00:42:08,840
learning setting that your attack vectors keep changing and you have to constantly keep

445
00:42:08,840 --> 00:42:15,600
monitoring the attack landscape and then keep evolving the solution.

446
00:42:15,600 --> 00:42:21,280
We're starting to explore a lot of this ground in the context of adversarial examples

447
00:42:21,280 --> 00:42:28,080
on the video side. There's a whole body of knowledge for, I guess, kind of meta knowledge

448
00:42:28,080 --> 00:42:33,120
for dealing with this kind of these kind of adversarial scenarios in the security

449
00:42:33,120 --> 00:42:41,880
research world. Like, do we have enough of that DNA within the kind of AI and NLP realm?

450
00:42:41,880 --> 00:42:45,800
Or are we starting to get enough of that that we can, you know, we're not reinventing

451
00:42:45,800 --> 00:42:47,520
the wheel?

452
00:42:47,520 --> 00:42:54,680
We, we have for some, like, you know, NLP, I can say, because of things like anti-spam,

453
00:42:54,680 --> 00:43:03,040
etc. There has been that kind of work happening already, right? Like, you know, around fishing

454
00:43:03,040 --> 00:43:10,520
or in around span. People have been constantly building systems that involve, that involve

455
00:43:10,520 --> 00:43:16,480
some kind of a feedback loop and preferably a human in the loop that way it can sort

456
00:43:16,480 --> 00:43:23,800
of deal with this shifting adversary. And we'll see similar things happening with these

457
00:43:23,800 --> 00:43:30,040
other modalities as well, right? Like, yeah, so that's not, I mean, that's pretty much

458
00:43:30,040 --> 00:43:32,680
on the road map for any serious company.

459
00:43:32,680 --> 00:43:40,520
So we dove into audio on the video side. What's the landscape looking like there? I think

460
00:43:40,520 --> 00:43:47,240
deepfake is the thing that comes to mind. But even that is somewhat ill-defined and includes

461
00:43:47,240 --> 00:43:49,280
a bunch of different types of things.

462
00:43:49,280 --> 00:43:56,040
Right. So, like, for example, like anything that creates generator or sampled from a

463
00:43:56,040 --> 00:44:04,560
GAN, today, even the best GANs still end up leaving some kind of artifacts that makes

464
00:44:04,560 --> 00:44:13,320
it possible to do good quality detection, right? But I can imagine a future where compute

465
00:44:13,320 --> 00:44:23,760
becomes so easily available and maybe a different kind of modeling paradigm where we are basically

466
00:44:23,760 --> 00:44:30,560
able to generate hyper-realistic image or like even photorealistic, I mean, very uncanny

467
00:44:30,560 --> 00:44:40,040
images, right? And the thing about detection is, I have this thesis that all detection

468
00:44:40,040 --> 00:44:47,920
models are always better than humans at detection when it comes to sophisticated attacks, right?

469
00:44:47,920 --> 00:44:54,760
And as things become more, so initially, you know, we will have, we will operationalize

470
00:44:54,760 --> 00:45:00,760
that, we will have more and more people looking at things and so on. But then once the adversaries

471
00:45:00,760 --> 00:45:06,640
evolved and then they decide to like, you know, have more launch, more and more sophisticated

472
00:45:06,640 --> 00:45:15,520
attacks, then models will become indispensable for doing that, right? And so, I think we

473
00:45:15,520 --> 00:45:21,240
are seeing some of that happening with GANs. And now with GANs, you can, there are all

474
00:45:21,240 --> 00:45:30,320
these temporal GANs where you can actually generate a sequence of images that are tied

475
00:45:30,320 --> 00:45:36,840
to each other so that it becomes like a video as opposed to like, you know, individually synthesizing

476
00:45:36,840 --> 00:45:44,480
each frame, right? I think that was one of the criticism with that with GANs. Then there

477
00:45:44,480 --> 00:45:52,880
is also, you don't need a GAN, right? Like with things like celibate, etc. There are things

478
00:45:52,880 --> 00:46:00,960
happening, which are a class of algorithms, which are doing this image-to-image transfer,

479
00:46:00,960 --> 00:46:10,600
right? Where I can basically take your body and then just swap out your face or sometimes

480
00:46:10,600 --> 00:46:20,640
even just swap out the lips, right? And then make you say things that you have not said,

481
00:46:20,640 --> 00:46:27,440
right? And that's interesting because, you know, we are a lot of the algorithms that we're

482
00:46:27,440 --> 00:46:32,640
working with, even in the state of the art methods, are still leaving some artifacts that

483
00:46:32,640 --> 00:46:41,360
models can detect. Like, you know, at AI Foundation, we internally, we have researchers,

484
00:46:41,360 --> 00:46:47,840
as well as like a product that we are working on and from the nonprofit side, where we are

485
00:46:47,840 --> 00:46:53,440
detecting manipulated faces in video and showing a heat map over it, right? And so on.

486
00:46:54,720 --> 00:47:00,880
Efficient frontier may be overloading a term, but there's a kind of a frontier where the, you know,

487
00:47:00,880 --> 00:47:09,520
the costs to generate these things and the quality, the resulting quality, you know, versus a

488
00:47:09,520 --> 00:47:15,520
human's ability to detect, it seems like we're in, we're still in a different place than where

489
00:47:15,520 --> 00:47:21,680
audio is. Most of the things that are accessible that I see are pretty, you know, it's pretty

490
00:47:21,680 --> 00:47:28,880
easy to tell as a human that, you know, this is a bad fake. Yeah, but I would say that, you know,

491
00:47:28,880 --> 00:47:36,080
with GANS, I mean, when I say human, when you say humans, we have to be careful, right?

492
00:47:37,440 --> 00:47:41,520
You and I are probably looking at people like us who tend to be like, you know,

493
00:47:41,520 --> 00:47:47,520
more in the machine learning, yeah, I would. And we are used to seeing a lot of these things,

494
00:47:47,520 --> 00:47:54,480
and it kind of like have this in-net expertise to tell them apart. But we have experiments,

495
00:47:54,480 --> 00:48:00,000
we have done with human subjects where, you know, there's like human subjects who are like

496
00:48:00,000 --> 00:48:06,640
completely non-technical when they look at GAN images, there's literally like a 50-50 chance

497
00:48:06,640 --> 00:48:13,600
that, you know, they get it, right? Yeah. And this is where, like, you know, I think a model

498
00:48:13,600 --> 00:48:22,160
becomes highly valuable. It gets worse when it becomes audio, with audio practically everyone

499
00:48:22,160 --> 00:48:30,080
will fail. You made a comment and refer back to it that we're going to be dependent on models

500
00:48:30,080 --> 00:48:38,400
because humans are, you know, we're being outclassed by models. And it makes me think a little bit of,

501
00:48:38,400 --> 00:48:44,800
there's been some research on medical imaging that says that, you know, a model is, you know,

502
00:48:44,800 --> 00:48:52,480
X percent accurate at, you know, say 80 percent accurate at detecting some cancerous, you know,

503
00:48:52,480 --> 00:48:59,120
growth in an image. A human is 80 percent accurate. I'm making up the numbers here, but together,

504
00:48:59,120 --> 00:49:03,600
they're 95 percent accurate, or something along those lines. Do you think the same thing

505
00:49:04,240 --> 00:49:12,960
will evolve in fighting artificial content where humans plus models become the ultimate solution?

506
00:49:13,680 --> 00:49:19,760
Absolutely. I feel like that's, that's even, that's an inevitable case where humans and models

507
00:49:19,760 --> 00:49:28,720
work together, except that I would make a small note that in that situation, the kind of

508
00:49:28,720 --> 00:49:33,280
information that the human might look might be different from the kind of information the model

509
00:49:33,280 --> 00:49:42,720
is looking at, right? I have some examples. Come to mind. So, for example, in, in this case,

510
00:49:42,720 --> 00:49:49,120
you know, the model might be let's say, take fake audio, right? The, the human may not be able to

511
00:49:49,120 --> 00:49:57,840
teleport the fake audio from a generated one, let's say. And sometimes, you know, with compression

512
00:49:57,840 --> 00:50:04,000
in audio, it can get even worse. Like compressed images and compressed audio introduce

513
00:50:04,000 --> 00:50:10,880
all sorts of artifacts that you cannot distinguish between that from the artifacts that you get from

514
00:50:10,880 --> 00:50:16,720
a generated system, from a system like this synthesis, right? So, we're kind of conditioned to

515
00:50:16,720 --> 00:50:24,480
accept some degree of compression artifact. And so, exactly, got it, got it. Exactly. And, and, and

516
00:50:24,480 --> 00:50:31,520
folks really have trouble telling that just from the content, right? And so, the models will like

517
00:50:31,520 --> 00:50:38,080
look at things that are here, listen to things that people can't listen to or see things that people

518
00:50:38,080 --> 00:50:45,760
can't see. But at the same time, you know, let's say the model flags something as, as, you know,

519
00:50:45,760 --> 00:50:54,080
doctored or forged, then you can, we can imagine a human in the loop, actually, like, you know, looking

520
00:50:54,080 --> 00:50:59,600
into it and not necessarily looking at the content, but looking at everything else, looking at the

521
00:50:59,600 --> 00:51:06,720
context, like, you know, where is this coming from? What is the user behavior, and so on, right?

522
00:51:06,720 --> 00:51:13,920
To large extent, it can be automated, but, you know, but even the context, like, it requires a lot

523
00:51:13,920 --> 00:51:21,360
of, like, background knowledge, as well as, like, knowledge of the world, to, to be able to say,

524
00:51:21,360 --> 00:51:26,960
to make, to make us a definite pronouncement. And we see this all the time in, like, I, you know,

525
00:51:26,960 --> 00:51:33,440
I will give you an example from the spam world where I come from. So, if you have training topics,

526
00:51:33,440 --> 00:51:39,760
right? So, there is, like, a lot of things appear like spam. The model can flag it as spam.

527
00:51:39,760 --> 00:51:45,680
But if you know that, you know, this thing is actually related to a training topic,

528
00:51:46,640 --> 00:51:55,040
then a human can make a judgment call whether it is market as spam and take a retaliatory action,

529
00:51:55,040 --> 00:52:02,960
like, by suspending the account or something like that, or whether to just treat it as simply

530
00:52:02,960 --> 00:52:12,800
a high volume traffic, because this training topic is just so engaging and so controversial,

531
00:52:12,800 --> 00:52:19,440
right? Right. And this is like a call that a human, and only a human can make it, at least as far

532
00:52:19,440 --> 00:52:29,840
as now, because the context is where it requires all sorts of, like, reasoning and unification

533
00:52:29,840 --> 00:52:35,920
with the world that it's hard to bake that into a model. I feel like we were just scratching the

534
00:52:35,920 --> 00:52:44,400
surface here, but we're running along on time. I do want to briefly mention once again the book.

535
00:52:44,400 --> 00:52:49,600
If I remember correctly, there was a tweet, there were probably multiple tweets along these lines,

536
00:52:49,600 --> 00:52:54,960
but I remember you tweeting something along the lines of, like, your philosophy with the book,

537
00:52:54,960 --> 00:53:01,840
or at least one aspect of it, relating to the code examples being real examples, or something

538
00:53:01,840 --> 00:53:07,440
like that. Can you elaborate briefly on your philosophy for the book and what makes it different

539
00:53:07,440 --> 00:53:12,320
from, you know, picking up a tutorial on the web, or on YouTube, or any of the other books out there?

540
00:53:13,360 --> 00:53:24,080
Yeah. So when, when Brian and I set out to write this book, we decided to bake in all the best

541
00:53:24,080 --> 00:53:31,120
practices that we were, you know, actually practicing on a day-to-day basis, and also we were

542
00:53:31,120 --> 00:53:40,880
not seeing them on any of these tutorials on the web, and so on, and also on the original

543
00:53:40,880 --> 00:53:48,320
documentation. So we decided that, you know, there are, like, very good software engineering patterns.

544
00:53:48,320 --> 00:53:52,240
I mean, I've always believed because I come from a very engineering background,

545
00:53:52,240 --> 00:54:02,320
I was an engineer first, and then became a researcher. I think really good science requires good

546
00:54:02,320 --> 00:54:10,640
engineering, and good engineering is, like, indistinguishable from good science. Like, and it's important

547
00:54:10,640 --> 00:54:17,280
to have the two together. So we baked in a whole bunch of best practices that we knew of

548
00:54:17,280 --> 00:54:27,600
from the software engineering world to actually make modeling good, modeling successful, right?

549
00:54:27,600 --> 00:54:34,960
And so if you follow some of the practices that we suggest, you would, like, not make a typical,

550
00:54:34,960 --> 00:54:43,040
you know, modeling mistake that we would spend days if not weeks trying to chase down, right?

551
00:54:43,040 --> 00:54:52,400
So that's what I meant by real world, and then, of course, a lot of the actual problems that we

552
00:54:52,400 --> 00:55:00,080
have chosen, we intentionally chose problems that were not, like, toy problems, right? Or toy

553
00:55:00,080 --> 00:55:08,800
data sets. Any of our data sets could be comparable to the real data sets that you,

554
00:55:08,800 --> 00:55:16,400
anyone can deal with. And it's just a question of, like, you know, how do we come up with a set of

555
00:55:16,400 --> 00:55:25,280
representative tasks for the book that pedagogically it will expose the readers to a variety of

556
00:55:25,280 --> 00:55:33,680
NLP deep learning algorithms. And at the same time, not take the reader too far away from their

557
00:55:33,680 --> 00:55:40,960
home court, which is the world of practice where the world of real problems. And I want them to be

558
00:55:40,960 --> 00:55:48,800
situated next to each other. And hopefully, if we did our job, right? We have shown the readers

559
00:55:49,440 --> 00:55:58,560
a lot of good software engineering practices around building models. And we also talk about,

560
00:55:58,560 --> 00:56:04,800
like, good design patterns, not just from building models, but for even for the product building

561
00:56:04,800 --> 00:56:13,280
NLP products itself. So in a variety of ways, this book is actually built for practitioners.

562
00:56:13,280 --> 00:56:21,120
It is a book that is built for anybody who is good at Python development to pick it up

563
00:56:21,120 --> 00:56:28,080
and quickly become familiar with national language processing and deep learning and the combination

564
00:56:28,080 --> 00:56:37,680
of the two and become enough proficient enough to go do their own research. And this is something

565
00:56:37,680 --> 00:56:43,360
I strongly believe in. A lot of people have also believed it. You know, research is something that

566
00:56:44,560 --> 00:56:51,760
you can learn on your own. And you don't need to go to graduate school and all that, like,

567
00:56:51,760 --> 00:56:58,240
you know, the fast AI folks and so on. And we believe in the same thing except we believe

568
00:56:59,120 --> 00:57:06,240
we express it differently. And in the sense that we think there is a lot of value in graduate

569
00:57:06,240 --> 00:57:12,880
school and of course, and in all the research that is out there. And there is a lot of value in

570
00:57:12,880 --> 00:57:20,800
reading papers and actually even writing papers. But there is all that has to be done in the context

571
00:57:20,800 --> 00:57:28,320
of real world product development setting. And every single course that I am looking at

572
00:57:29,200 --> 00:57:36,320
today is failing on that, right? And I am hoping that, you know, the book that we wrote is just

573
00:57:36,320 --> 00:57:42,160
scratching the tip of that iceberg. Well, Delic, thanks so much for taking the time to chat with us

574
00:57:42,160 --> 00:57:47,920
about what you're up to. Very interesting stuff. And we will be sure to continue to follow along.

575
00:57:47,920 --> 00:57:54,400
Absolutely, this is such a pleasure. And, you know, my god, time just flies. Yeah, yeah, my pleasure.

576
00:57:54,400 --> 00:57:56,400
Thanks so much. Yeah, thank you.

577
00:58:00,000 --> 00:58:04,720
All right, everyone. That's our show for today. If you like what you've heard here,

578
00:58:04,720 --> 00:58:09,840
please do us a huge favor and tell your friends about the show. And if you haven't already

579
00:58:09,840 --> 00:58:14,720
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes

580
00:58:14,720 --> 00:58:20,240
we've got in store for you. For more information on any of the shows in our AI conference series,

581
00:58:20,240 --> 00:58:28,240
visit twemolei.com slash AINY19. Thanks again to HPE for sponsoring the series.

582
00:58:28,240 --> 00:58:35,200
Make sure to check them out at twemolei.com slash HPE. As always, thanks so much for listening

583
00:58:35,200 --> 00:58:45,200
and catch you next time.

