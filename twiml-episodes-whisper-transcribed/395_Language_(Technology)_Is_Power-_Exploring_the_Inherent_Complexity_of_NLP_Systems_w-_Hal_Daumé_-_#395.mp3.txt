All right, everyone. I am here with Hal Daume. Hal is a professor at the University of Maryland
and a senior principal researcher with Microsoft Research. Hal, welcome to the Twimal AI podcast.
Yeah, thanks for having me. I'm excited. I am super excited as well. We typically start these
conversations with a bit of background. What set you off on the path of exploring language
and machine learning and fairness and bias and ethics and all these cool things that we're going
to be talking about for the next few minutes? Yeah, I mean, I guess I've kind of always been
interested both in language and then also in sort of like math computer science stuff.
And it was really only in like my last year in college that I even discovered that these two
things could go together. Like I was majoring in math. I had originally started minoring and
creative writing, but it was really hard to get into the classes if you weren't actually a major.
And yeah, so then like my last year of college I was talking to one of my friends and he's,
he also was interested in math and computer science and language and he's like, hey, have you heard
of natural language processing? What is that? And I was actually at Carnegie Mellon at the time.
I mean, this was a long time ago, like way before, you know, NLP was kind of a known thing and
they have a really big group and they did it at the time and so I got involved and I basically
in like six months transitioned from like planning to apply to grad school in like pure math to
applying to grad school in like linguistics and computer science programs. Oh, wow. I thought
you were going to say planning to apply for, you know, fine arts programs. No, okay, not that.
That would be quite a swing in six months. Yeah, so yeah, so then I went to grad school,
started doing NLP and really didn't do anything in, you know, the bias car in a space until,
you know, the past three or four years, but I think one of the things that I guess they're sort
of two reasons. So one is, you know, sort of like obviously super important and the world and
as NLP techniques and systems start having more impact, like I think it's important to consider
like how that impact is actually affecting people. But then the other is that,
I think that, you know, like I said, like I've always been interested in language and
I think a lot of times, a lot of NLP work can end up a little bit like distant from the language
itself. Like I really like the machine learning side too and like the map aside and things like
that. But I think often that like the actual language gets lost. It's like, you know, okay,
let X be a sentence and then you sort of proceed from there. And I think for me, like a lot of
what makes, you know, sort of this like bias and NLP space really exciting is that it's like another
way to like start thinking again really deeply about like what does language mean? How is it used,
like how is it used in society and things like that? So I see it for me as like kind of a way of
like re-steering back toward my language interests rather than sort of just focusing on the more
mouthy side. At a great recent conversation with Emily Bender on just that topic, you know,
have linguistics been, have linguists been, you know, unfortunately absent from, you know, NLP
research and the like, my sense is that is your background kind of traditional linguistics or
you came up much more on the computational machine learning side. Yeah, I came up much more on
the computational machine learning side. Like I took a couple linguistics classes as an undergrad
and a couple as a grad student. And then basically like four years, three or four years ago,
when I first started getting into this, I basically bought like a giant stack of social
linguistics books and just like read through all of them. Which was rough going at the start,
because it's always hard to read stuff outside of your area. But yeah, so I definitely don't have
anything like a formal linguistics training. And you're also a program co-chair for this year's
ICML conference. I can only imagine that at the time you signed up for this, you had no idea what
you were getting into on multiple levels. Yeah, that's an understatement. Yeah, I mean, I think,
you know, so I'm doing this with Artie Singh as my program co-chair. And, you know, at the beginning
of the process, when we both agreed, we had this like big brainstorming document. It's like,
oh, here's like all the stuff we want to try like tweaking and experiments we want to do and stuff
like that. And then, you know, and so there's a lot of stuff that we've done intentionally.
There's also a lot of stuff that happened and that required adaptation. But, you know, I mean,
there's been a lot of work, but it's it's also been like pretty rewarding and interesting. And,
you know, working with all the committee members has been great. So when does that effort start?
Does it start when the previous conference ends or before that or? It basically, I can't
remember exactly when I signed on. I mean, it starts reasonably soon after the previous conference ends.
I think the last one was over like end of July last year. And I think I got asked probably in August
or September or something. So it's it's basically a year. Okay. Actually, no, that's,
it's not true. I think that's true. And for those who, well, whenever you hear this, it'll be
after today, we are speaking immediately before the conference starts. So congratulations on heroic
organization that you're doing something like this, you know, a conversation like this before the
the big show. Yeah. Sometimes you need breaks. Awesome. So it is a bunch of stuff that I'm curious
about about the conference. Maybe we'll talk about that at some point, but I want to dig a little
bit deeper into the work that you've been doing around language and machine learning and fairness
and bias. And you kind of mentioned that a good part of it comes from, you know, this belief around
the importance of language and kind of all that it, you know, means for us. Can you elaborate on that
a little bit? Yeah. So, you know, I think that there's lots of stuff that makes language really
interesting. I mean, I think it's generally considered that it kind of, you know, it's part of
what sets humans apart. It's, it's arguably sort of a major way that, you know, we've been able
to advance so far as a species because we don't have to like relearn everything immediately. Like
when we're born, like we can actually, you know, read and be told things and like learn things that
came before, you know, it's generally the primary way that we communicate, although there's
certainly lots of non-linguistic communication that goes on too. And I think from, you know, there's
sort of like two high level things I'm interested in. So one is like, you know, how do you use language to
do, like as a way of interacting with the world. So like, do you, you know, if I have like a
smartest assistant or a robot or something like that? And I want to communicate with it, like
language is certainly a very natural way to do this for most of the world's population.
So that's sort of like, you know, how do I use language to like operate devices? And then on
the other side, it's sort of like the bias fairness side. I think it's generally considered like
in social linguistics land that language serves kind of two roles in society. So one is it's a
way by which we construct our own social identities. So, you know, there's sort of like contextualized
things like, you know, if I'm sending an email to my department chair versus, you know, a grad
student I work with, like the language that I use in those is going to be different because of like
the respective roles that we have. But then, you know, more broadly, like, you know, I had this experience
like 17 years ago. I was an intern at Microsoft actually over the summer. And I was giving a talk.
I was like a second year grad student at the time. And one of the people in the audience,
who was like a relatively senior guy raised his hand, like 10 minutes in, he's like, how did you
grow up in Los Angeles? And I was like, yes. And then he proceeded to explain that he had heard
this story on NPR that morning about like the LA dialect and LA accent. And he was like, he thought
he was identifying it and he wanted to check that he had identified it properly. And so I guess
he had, but that also made me incredibly self conscious, don't even talk about like how I was
speaking. But like in general, like whether it's conscious or subconscious, like the way that we
speak, this is often called like how we do language. So the way we do language is as much a part of
what we're saying as like the actual content itself. So there's this whole sort of like constructing
identity side of language. And then the other side is, you know, like I was saying, you know,
we learn things from history. You know, not all of those things we might think of as like
normatively good. So we also learn like stereotypes. And I don't know, like sort of like, I don't
know, I can't think of another end. But yeah, so we learn like a lot of like group membership and
stereotype and things like that through language. And so like a lot of the stuff that you see in
sort of the bias and NLP space is kind of trying to tease apart like how much of those stereotypes
are being like picked up by language systems, whether they're gender stereotypes or racial stereotypes,
or whatever. And I think one of the things that makes language really interesting here is that like
because it serves these dual roles, it means that I think it like significantly complicates
like what you might want to do to mitigate harms that result from an NLP system. Because you
know, maybe you want to say like, okay, I don't want my system to contain these like gender
racial stereotypes. But then you run the risk in trying to like mitigate those, you run the risk
of like minimizing the ability of users of your system to like express their own identity,
because that's also embedded in language. And so like trying to figure out like, how do you
build systems that like let people talk how they want to talk as part of how they do language. But
then also minimize the harms that arise because of whatever societal stereotypes these systems
have picked up. I think that's like a really interesting divide to walk. And it makes everything
really complicated. And so that's why I like it. It does make a lot of things complicated. I'm
thinking of, well, there's, you know, there's, as you know, there's been a lot of discussion in
this area recently on Twitter and in other places. And I'm thinking of a tweet. I think Robert
Ness was just commenting about, you know, how it was in the context of the Jan LaCoon and the
front, I remember the name of the the upsampling algorithm conversation that
right, a couple of weeks ago, Pulse, I think. In any case, he was talking about Robert was
referencing the idea that, you know, it's hard enough for him as an individual to try to make,
you know, assessments around, you know, race and identity and these things to let alone,
you know, expect a computer system natural language processing. In this case, or computer vision,
in that case, to, you know, try to be able to do that based on today's technology, you know,
he was talking about it in the context of, you know, how many labels would you have to,
how many, you know, images would you have to label and, you know, how would you, you know, train
folks to label those images even? What is ground truth when we're talking about, you know,
identity and, you know, race and things like that? Is that, is that problem easier or more
difficult than an LP? I mean, I don't know that it's easier. I think it's like somewhat different.
You know, I think that, you know, if we let's take like races, an example, right? So if we
think about something like various Englishes that are spoken in the United States,
there's several that would be categorized as, for instance, like, let's say like Chicano
Englishes or African American English or, you know, what's sometimes called mainstream US English,
I've also heard it called hegemonic English. But, you know, and these things are closely related
with like how like US society constructs race. But they're not the same thing. You know,
they're African Americans in the United States who don't speak African American English.
They're white people in the United States who do. And so I think that like first we have to
like be kind of careful about like what categories we're talking about and like what they mean.
And, you know, certainly avoiding making assumptions about like how people do language as a result
of like how we've like categorized them. But I think like getting back to the point, I think the
the challenge that I see at least is like, I want systems that sort of work for whomever like
kind of regardless of how they want to do language. So, you know, one of the, sorry, I don't know if
you can hear the siren in the background. One of the things that like a handful of people
have studied. So I'm thinking of like work by Brandeis Marshall. So she's looked at, for instance,
the degree to which systems that don't work well on African American English on Twitter,
they're essentially mean that like voices are not counted because they're not identified as,
you know, even speaking English or other sorts of harms that come up. So, you know, if you have,
if you're running some word processing system and so this, and you,
I don't know what's a good example. Like even in like, I don't know if I can come up with a good
example right now. So I guess an example that Courtney and Appolice at Graham really gave me was
that, you know, if someone makes a typo and says something like IR space husband or something
and it wants to spell correct IR, like do you spell correct to her, which would probably be the
most common in data due to like heteronormativity? Or do you provide alternative spell corrections
for that? And so like all of these things I think are about like, you know, when we're designing
systems and, you know, they're being trained on say all the data you find on the internet or
something like all that data comes with, you know, sort of a bunch of social baggage. And some of
that is useful, but some of that serves to like either erase people and make them unseen,
or serves to make the systems just not work well for them, or make suggestions, like, you know,
sort of the classic example of like auto replies that, you know, if someone says, you know, I saw
a doctor yesterday, the auto reply is something like, what did he say? And so all these things that
sort of like reinforce social hierarchies that already exist in the world. I think that's sort
of the interesting question. Now, how you go about it? I think this is super hard. And like,
you know, this question of like, how much data do you need? I guess like my feeling on that is
I don't know that like more data is always the right answer. And, you know, in some cases,
I think we need to find other ways for domain experts to get their knowledge into machine learning
systems. Like, I think a lot of this democratization of machine learning where it's like, okay,
I have this black box. And, you know, there's like the machine learning experts on one side,
and they're doing all those like developments. And then there's the domain experts on the other
side. And like, all they're allowed to do is provide labeled data. Like, I think this is like
an incredibly reductive view of like what machine learning can do. And I think it's something that's
like much beyond fairness. Like, you know, when I was even in grad school forever ago at this point,
you know, I was in a natural language processing group. And one of the things that we would kind
of poke fun at the machine learning community for was that like, you know, to the machine learning
community, like your API is someone gives you a matrix and you have to do something with that matrix.
And this was just like a really weird way to conceptualize problems even as like an NLP person,
which is like not that far from machine learning. Because we're always talking about like, okay,
how do I develop new features? How do I collect new examples? How do I make better representations?
Like all of these things that like even just in a small way break outside of this like matrix
abstraction. And so I think that like in sort of like bias fairness space, this becomes like even
more problematic because the sorts of like knowledge you need to integrate are really like the societal
level. I don't know what the word is like societal level knowledge. And it's not clear to me that
trying to do that by having people label data is like an efficient way to do that at all.
Have you, yeah, what have you seen that's interesting that tries to get at this idea of incorporating
the domain experts more deeply into the process? I mean, I think there's a lot of stuff.
So, you know, back when I was in grad school, the technology du jour was Bayesian networks for
everything rather than neural networks for everything. And, you know, I think a lot of the hope then
was that, you know, domain experts could design these structured models that would then be useful.
I think in some domains where things are sort of easy enough that you only have like 10 variables
going on or 20 variables, you can do this. But I don't think it really panned out very much
for a lot of language problems. You know, I think in neural nets land like sort of, you know,
today's du jour, I think it's a lot harder because it's very difficult to understand these models.
And so, I think like the rather significant amount of work that's been going on in like
transparency and explainability is like super important here because like if we don't understand
what these models are doing, it's like not even clear how a domain expert would intervene.
But I think there's been also a bunch of work looking at like creative ways of
trying to get knowledge in one form or another into systems. So one is, you know, there's been
a handful of papers looking at, you know, okay, like let's say that I have some like logical rules
and I want to make sure that my neural net model maybe not always obeys them but like strongly
prefers things that are consistent with these rules I write down. So like how can I sort of compile
these into neural net systems? I think there's been a bunch of work looking at, well not a bunch,
but there's been a bit of work looking at like are there other forms of data that I can collect
sort of alongside typical labels. So I think a lot of this goes back to like the mid-2000s with
like annotator rationales so have annotators like mark the pieces of text that are most relevant
to the decision that they made. And then a variant of this that's come up recently
is instead of having them sort of highlight the relevant part have them change or relevant
part so that the labeling decision would change. So we tried this a couple of years ago in like
an evaluation context so this was I think we conceptualized it at the time as sort of like human
adversarial examples so we ran the shared task where people could submit systems for two tasks
sentiment analysis and semantic role labeling then so there this was this build it break at task
so those were the builders and then the breakers would come in and they'd take the outputs from
the systems on test data or on development data and then their job was to take a sentence from the
development data and change it in a minimal way so that the label change the true label changes
but perhaps the system prediction does not. And so then we score systems based on how hard it
is to break them and we score breakers based on how many systems they break and one of the I
think there was like an interesting outcome of this which was that we saw like a lot of the people
who were breakers were linguists we kind of build it as like you know hate linguists come show
these NLP people like that their systems are really fragile and don't learn like even basic sort
of syntactic properties and so so that was I don't know three years ago or so and then in the
past couple of years we've seen similar ideas applied to training data where people have elicited
data that where they say okay label this and then change it minimally so that the label would flip
and then it's sort of been observed that by using that type of training data you can learn something
like just as good with far fewer examples and so I think that's still like largely in the space
of having people label data but I think it's like making this interface between you know the machine
learning developers and the domain experts like a little bit broader and I think like if we can
continue pushing on that I think there's like a huge amount of space both for like progress and also
a lot of creativity like I think there's a lot of really interesting things you can think about when
you start like separating not separating but when you start trying to make this connection more
porous I was curious about a paper I think it was a survey paper that you did a while back on
language technology is power can you share a little bit about that one yeah I mean it wasn't a
while back it was like just this week actually at ACL it wasn't so it is top of mind yes so this
was with Suleyn Blodgett who is an intern at Microsoft last summer and then Suleyn Burkis and
Hannah Wallach so yeah so what Suleyn did to start with was look at basically every paper we could
find on quote unquote bias and NLP and that was basically 150 papers over the past couple of years
and what we set out to do was try to understand
and what are the like when people write papers on bias and NLP like sort of what is motivating them
so like what does the paper say like this is the problem in the world that we're trying to solve
what their sort of like normative commitments are so like they say it's a problem why do they say
it's a problem and then because most of these papers present some sort of new method that's either
a mitigation strategy or a measurement strategy you know what is that thing actually measuring and
like how much does it align with for instance the harms that were in the motivation and so a lot
of the initial work was sort of categorizing papers coming up with a taxonomy of different types
of harms that we largely followed the allocational representational harms perspective that like
Kate Crawford and Suleyn Burkis and colleagues have talked about before we had to create a couple
new categories and there were some that didn't really apply so the first ever was kind of a
categorization effort and then the second effort was really trying to take stock of like you know
okay so we found that like motivations were often super vague like of the form stereotyping is bad
and then we often found that papers sort of took normative stances for granted so
something would be seen as bad but it wouldn't really be explained why and actually in the poster
section what's an example of the previous yeah what's a good example of that
so for instance I'm making this up so like ask me to point to something that's in particular
but like you know something along the lines of you know word embedding and pick up
associations between gender and stereotyped occupations associated with that gender so that may
be true but the decision that this is bad is like fundamentally like a values question and so
like sort of what is like what are the values that you're leaning on in order to like decide
that this thing is bad and one reason why I think that's important is because like a lot of these
things are really tricky and like we need to debate them as a community but like until we make
them really Chris it's really hard to have a debate about like whether something's good or bad
without saying why it's good or bad and actually in the poster session earlier this week for this
paper Sharon Goldwater at Edinburgh brought up this this question about you know what does this
paper say about how to teach sort of ethics in NLP in classes and one of the comments that
Solon made that I thought was really nice was that and right and sorry the context in which Sharon
was asking this question is that she teaches in Edinburgh she's American many students in Edinburgh
are international we all come from like very different backgrounds and have different sets of
values so like how do we teach in that context and one of the points that Solon made in response to
this is that like if we made our normative commitments more explicit it would actually make
teaching easier too because then a paper would explain like okay this is the problem this is why
we say it's a problem and then go on and then even if you're not from the cultural context in which
it might be taken for granted that this thing is a problem you can still at least understand why
it's being couched as a problem so what you're looking for here is the explanation as opposed to
you know reference to some pre-existing ethical code that you know may have already established
that it's a problem yeah I mean I think both are good so you know there's a huge literature
outside of NLP on like how language and society works this is like linguistics anthropology
so it's the linguistics et cetera right and then like if you're looking at specific problems like
like for instance gender you know there's like decades of gender studies literature out there
I'm specifically thinking of the example you gave with the the you know predicting you know NLP
predicting gender or we you know throw this example around with the you know the word to veck
word embeddings like the professions and I think you're right that you know as was the case in
the paper the I guess it was the hypothetical paper you were referring to we often throw this
very example around as being you know so wrong that it doesn't need definition
and or doesn't need explanation as to you know why it's problematic and I'm thinking if we want
to be precise do we all need to keep recreating that definition of why that's a problem or is
there some something that we can refer to yeah I mean I think yeah this is why references exist
yeah yeah so I think we definitely don't need to like keep recreating it I think that
like for instance in this example you know I think the question that I would ask is like
you know okay so we've found that you know there's sort of these gender occupation stereotypes
and word embeddings like you know what harm does that create so for instance a lot of
I think even the original paper gives this example of resume filtering or something like that so
this is a sort of commonly leaned upon example where you could imagine a scenario in which word
embeddings are used in some sort of resume filter and then if you have you know I don't know if
the word embeddings pick up correlations between names and gender and then also pick up correlations
between occupations and gender then maybe it like sorts people according to whatever these
stereotypes were in the language that it was trained on so I think right so this would generally
be considered like an allocational harm in the sense that there's like a resource like a job or
a job opportunity that's being denied someone on the basis of like what the system has picked up
and so I think it's fine to lean on that but then I what would be I think really interesting to see
and I think there's like a lot of work to be done here is like how can we actually go about measuring
that and how can we build like mitigation strategies like for that like specific harm and so I think
like this is sort of like the mismatch issue that I mentioned earlier so you know there's a lot of
motivations along the lines of you know this resume filtering example but these are all sort of
in the context of this hypothetical resume filtering system that like we don't even know exists we
don't know if this actually happens and so we're thinking about like measuring whether we've been
successful in mitigating these harms like we have to be able to sort of measure them in this first
place but yeah I mean like on your sort of original question like definitely we don't need to
all reinvent the wheel every time but I think as things have and I think this is the gender-based
stereotypes is a more clear case I think there are less clear cases where it's I think much more
up for debate whether these harms are like or whether these whether these biases that are proposed
actually amount to some sort of harm or not in the world yeah I think the the question that
curious about is to what degree are you know folks to what degree should folks that are thinking
about these there's maybe too broad I'm curious if there's a cat to what degree there's a catalog
or a survey of you know the kind of common biases and harms you know versus individual papers
that have you know that are exploring specific harms so you know the word to veck word embeddings
thing that's been explored in a bunch of individual papers you know but is there like something
analogous to the the Wikipedia page of you know cognitive biases that just lists you know a
thousand of them and the the various you know harm arguments does the work that you reference
with Hannah Wollock and was that Hannah and Solon that yeah and Solon I don't know if something
like that I would love it existed you know like you know I was saying like three or four years ago
and I started doing all this stuff you know I read this giant pile or says you'll ingress
six bucks right and it would certainly have saved me a lot of time if if I could you know if this
was sort of like more distilled somewhere I think one of the challenges is that I think it's really
hard to think about these things abstractly even in word embedding it's like you know no one goes to
like word embedding.google.com and like asks for an embedding of a word like this is not
like an end user technology it's like embedded in in other systems yeah and I think you know the
further a technology is from like the use case though like it is much much harder to think about
so if you look at the work that's been done in sort of like bias and machine translation systems
or speech recognition or dialogue or something like that like these systems they're like much more
user-facing I think it's a lot easier to think about you know you could for instance do you
like a value-sensitive design type exercise and think about like who are the stakeholders involved
in this system you know what happens to each of them when different types of errors are made
and so on and it's just for these like these more component tasks whether like word embeddings
or syntactic parsing or something like that it becomes much harder to think about like
as errors are made like what sort of what sort of problems does this cause because they'll
inevitably be like downstream causes don't start downstream effects so yeah so I think the answer
is no I think I think that's something it would be great but I think it's also really tricky so
for instance like we had this other ACL paper this year on trying to move toward like
careference resolution that's more gender inclusive and particular for like binary and non-binary
trans people and you know like I think like work in that area for instance I think needs to lean
you know super heavily on queer studies and gender studies and topics like this and so I think
you know as you pick these areas and like both tasks and like populations you're thinking about
and things like that it's really hard to imagine anything other than like a full encyclopedia
that's gonna like have the answer to everything I totally see where you know it's wishful thinking
that another example that kind of comes to mind is like in you know writing you've got you know
like Chicago style or AP style I'm envisioning something where you know someone is writing a paper
can refer to you know it's you know that this paper is you know it should be evaluated in
XYZ ethical frame and you know that's the lens in which I'm you know conveying judgments about
what's right and what's wrong and etc etc I think you see a little bit of this so I think you see
it kind of with connections like political philosophy so sometimes you'll see motivations and
papers that say like you know we're gonna take like a Rawlsian view of justice and um
uh you know sometimes this is a little bit more like cell then content but like it often is content
and um you know and like I think a lot of people think like Rawls had interesting things to say I
think you know certainly uh he's also been contested and so um but like at least by saying like okay
this is like the framework that I'm using um it sort of lets other people say like well I either
disagree with that or I agree with it and like I can evaluate like you were saying like I can
evaluate the work in the context of like this framing right right right um so if you're if you're
not researching this but you're actually trying to build systems yeah how do you kind of
wade through all of these issues yeah that's really hard you know I think there's um I mean there's
like been a lot of work recently trying to like okay so I'm going to like kind of put like a
little bit I mean I think of this more in the context of like you know building systems not
building academic systems but building like real systems in the world right um I mean you know
so like there's been work trying to look at um you know what do what do the people who build
these systems need so like Michael Veele had this nice paper in Kai 18 um looking at the public
sector so systems that are developed um you know for things like uh well he was in the UK so like
hospitals um or social services and things like that um and then we had basically a follow-up
paper a year later looking at this in the context of private companies um and so you know I think
one of the things that was consistent in both of those is that a lot of the work on like bias and
fairness often comes down to sort of a single well motivated individual on a development team
um so one of the people we interviewed referred to these people as like fairness vigilantes um and
they're often working overtime uh to address issues like this um they're often not compensated
for doing this extra work um I mean in many ways it's kind of reminiscent of like diversity
and inclusion work in like companies and universities as well and so I think um you know I think
as like a bare minimum you know this has to be part of like assessment of tools and
assessment of success and like promotion criteria and raised criteria right so I think there's like
all these like social things and like how companies are run that kind of has to change so that it's
not this like you know one poor individual is like burdened with doing all this work um it's
interesting though that you the you point to a cultural solution a company cultural solution as
opposed to a process solution so I think it's both so I think there's the the cultural aspect um
because you can certainly argue that maybe the process won't come about if the culture's not there
to support it right but that's basically my concern yeah um you know so there's there's been more
sort of like technical work so like um Michael Medio who was an intern last summer working with
gentleman Vaughan and Hannah Wallach had this nice like checklist for fairness paper at CHI last
year um which was really sort of exploring this question of you know how can we um like once
people are bought into this either because they care themselves or because their boss tells them
they care now um you know how can they like more easily navigate this space um I think there's
also a lot of sharing of information that's needed I think this is like really hard but um
you know like for instance machine translation and speech recognition are different tasks um
they're often developed by different groups within if a company makes both they're often
developed by different groups but they share a lot of similar issues in the sense that both are
about writing down text as a function of some input um and it's just like is that input like
something in another language or something um in speech and like finding ways to like share
expertise about like what goes wrong like the thing everyone is like super concerned about is
is like blind spots right so these things that you don't think to test for and then you release your
system and then like 24 hours later there's a New York Times article about how your system is
terrible and you know and how are you supposed to know this ahead of time um so I think there's
a lot of process stuff so I think it's a lot of things so I think it's like cultural stuff I
think it's process stuff and then I think it's like technological stuff so um you know the the
sort of fairness in machine learning community has I think really gravitated and made a lot of
progress on um these sort of parity constrained machine learning problems right it's like
optimized classification accuracy subject to some maximum disparity between two predefined
um social subgroups um so that's I think like by far the dominant paradigm that people have been
thinking about um and we have pretty good tools for that for problems that fit in that category
at this point um I think the thing that we found in this kind of paper last year with Ken Holstein was
that like that is definitely not all that people need like people definitely need that but um you
know like we were talking about before a lot of this is about data so you know how do you know
say that I've discovered that my system has this gap like I can try to address it algorithmically
but like if I just don't have any data or I have like very little data from some uh from some
sub-population like no amount of algorithmic finessing is going to help me and so you know this
is where we need to start thinking about like how do I grow that matrix right so like how do I
like select new rows like how do I elicit more data from like um populations where I'm observing
like large disparities um so I think there's yeah so I think it's really these three things like
cultural process and then technical and I think um you know I'm a technical person so
it's easiest for me to think about the technical problems but you know I think it's also the case
that you can have the best technical thing in the world but if no one uses it and no one's encouraged
to use it then yeah um yeah ultimately if the the change is happening on the part of these
you know fairness and the vigilantes is the right word but you know advocates or whatever
yeah advocates is probably that's it then we need tools to enable the the advocates and
and it is it easy now you know is that tools you know here a bunch of links to papers go read them
and you know that's what you have or I think there's in conferences what what are the best
resources you think for folks that you know hear this and say oh I want to be that in my company
yeah so I mean on these on these problems of you know sort of like optimize classification
performance with respect to some parity metric I think you know sort of at least IBM Google and
Microsoft all have toolkits at this point that will do these I think you know they all have pros
and cons you know with my Microsoft hat on I'll be like the Microsoft ones but but you know and
but I think they they certainly all have strengths and weaknesses um the but they're kind of
I won't say all but they're like largely focused on sort of this like one very specific sub problem
um I think for other things like the technology hasn't matured that much yet so um you know like
I've been thinking a lot about data collection um and uh and in some ways this is a really natural
machine learning problem like machine learning people have been studying active learning which
is basically automated data collection for like 50 years probably at least um and uh so there's a
paper I apologize I can't remember the author's names but there's a there's a paper already on
archive on this topic um we've been doing some work on this topic basically figuring out you know
okay I have some disparate like I built a system I observed some disparity in its performance
um I have a giant pool of data that I could have labeled which data points should I label
um in order to close this parity gap as much as possible um and one of the things that I actually
really like about this conceptualization is that you know we very frequently hear about this um
um like accuracy fairness tradeoff um and I think to some degree like there's there's some truth
to that in the sense that like if I'm solving an unconstrained optimization problem versus a
constrained optimization problem like the constrained optimization problem is not going to do
it it's going to do it at best as well as the unconstrained one um but you know sort of like
caveat one is that you know accuracy with respect to like what test set so if your test set is
like supervised then like who cares if I trade off accuracy on a supervised test set um in order to
get parity but I think like I actually really prefer to think about this as a like accuracy versus
effort tradeoff so um like if we take for granted that some notion of fairness measured by parity
or measured some other way is just a constraint in how we build systems then the question is how
much effort do I have to put in in order to get my accuracy up to a sufficient level like under
that constraint and so I I I much prefer to think of this as like how much work am I willing to do
in order to get this accuracy when I demand that you know this thing is you know quote-unquote fair
and in however I've technically defined that which is great because it puts a responsibility on you
for deciding that you're not willing to put an effort to right yeah well not on me I hope
someone else yeah I mean data collection is you know it's it's tough it's it's important right like
I mean you know you can go back to the like the gender shades paper by Joy and Tim Nade and um
you know like that paper only was possible because they went in collective data um and you know
I think we've seen a lot of papers basically following up on that work where it's basically like oh
I collect another data set and this thing is terrible and I collect another data set and this
thing is terrible and you know we had a paper like that at ACL right so um you know I think that we
um I think that that's been like a really productive way of thinking about problems um
I would like to think that there's a way to do this without having to go out and like collect a
full test right this is an expensive process and so like what can we do to sort of like streamline
this so that we can find issues um like quickly and I don't think I mean going back to your original
tools question like um you know this like blind spots thing that comes up over and over again
I mean there's very few even papers on this um not to mention um like tools that exist so you know
people should work on that. What was the paper that you were referring to uh from ACL?
This is like with uh my PhD student Tristan Sal so this is on this is the gender
inclusive co-reference paper so um yeah the observation there is you know most so in co-reference
resolution I'm trying to determine like you know if I say like um you know I had a meeting with
Sam yesterday he blah blah blah I want to know that he refers to Sam um and um you know the vast
majority of data sets that have been collected for this are from newswire um you know like New
New York Times and Wall Street Journal it was only like last year I think that they their style
guidelines started allowing the use of third person singular they um to refer to non-binary people
I think otherwise they would just avoid I'm not actually sure what they did um but they must have
had some way of talking around this um and so like if your data doesn't even contain like third
person singular they not to mention like neo-pronouns like see here um you know how is your system
gonna learn to do anything like this so um you know our data collection exercise was basically like okay
what are like naturally occurring sources where we will see um uh like gender neutral pronouns
where we will you know unfortunately see things like dead naming and and misgendering um and then
I mean even how do we annotate those things is not entirely clear um and so we had like a handful
of data sources that we annotated and then you know not surprisingly systems don't do very well
because they've never seen um language used this way in their training time so did you after collecting
the data set did you benchmark existing uh systems yeah we ran like five yeah five systems um they
all performed like in terms of the main measure that we collected they all performed essentially
the same uh say systems are these systems that are used out in the wild by companies or algorithms
that were presented in a paper around some uh academic task yeah so certainly the latter
whether they're using companies I honestly don't know there's been work for instance looking at
you know how much does using careerference resolution help with like information retrieval systems
like web search and the idea there is like you know if I search for um like if I search for someone's
name I want to highly rank documents that mention that person a lot it shouldn't really matter
whether the document mentions them by name or whether it mentions them phenomenally or they
it mentions them as you know like uh you know the president or the professor or or whatever um and
I think there's kind of been mixed results like at the end of the day you know do major search engines
use this I have no idea um yeah so yeah so whether they're used in real systems or not I just don't
know um so it's if you know we were looking mostly at like academic systems yeah they don't do
super well so like on on more like uh I don't want to use the word standard but on
previous data sets um they get uh sort of these f scores in like the 60s on our data set it's like
in the 30s um and you know certainly not um the the errors that right this this gap between 30 and 60
the errors are not uniformly distributed over people like they're um much more peaky
uh for for non-binary people or uh people who are gender fluid and so on
um so there's you know clearly a ton of work to be done in this area are we generally heading
in the right direction I've had some conversations recently um and there's been some papers published
recently that try to save variations of kind of the you know the fairness community you know
isn't going in the right direction or is you know has a huge blind spot in area xyz kind of what's
your take on all that um it's hard to say like I think that you know I mean you know okay so like my
own bias is that um I think that like breaking outside of this sort of like parity constrained
classification um abstraction would be really good for the community and I think we're seeing that
um like I think it's happening um uh not because that problem's unimportant but like I actually
think we have pretty good solutions to that at this point um and I think that like the space of
problems is just so much bigger than that um I think that you know I know that people possibly
not possibly I know that you will disagree with me on this but like I really think it's hard to
do work in this space without engaging in um and like thinking closely about like what role does
this technology have in the world um so you know I think there's like especially in computer science
I think we're often trained to think like very abstractly and you know like um you know I build
this thing and you know people can use it for good people can use it for bad blah blah blah but like
I kind of feel like it's years to collect the right data to use this thing that I built
yeah exactly and I think at the end of the day it's like we live in a world and like that world
has properties and like some of those properties are good so these properties are bad but I think that
like it's you know if I build a technology that's like only useful in a world that doesn't exist
like it's like socially construct like if I build something that's only useful in a world that's
socially constructed like fundamentally different from how ours is um like at best that's neutral
right but like at worst that that's going to be bad and so you know this is something that you
know certainly I haven't thought about for my whole career right like um you know I really like
I've only been thinking about these things for like three or four years um but I do see a little
bit of a push in like some parts of the fairness community to try to maintain this like divide
between like the technical work that happens and the world in which that work is happening
and and I understand like that um that desire but I think that like for systems to or for like the
technology that we build to be like um to actually like support things like justice and equity
and stuff like that I I think it's really hard to do this without like engaging in like house
society is structured and do you how well do you think we're doing on that or what's
I actually I mean I'm in through like I'm I'm generally positive I mean I'm generally a
positive person but like yeah yeah and I really mean it to to to make it so reductive but I
guess more you know practically um again that you know if I'm kind of working in this space and
and I'm just I'm always curious with with I'm always curious around um you know how folks kind of
wrap their arms around the entire scope of um the technology that um you know this technology
that we're all working in like AI you know as huge potentials for good huge potentials for bad
and um you know how folks kind of parse that and um it is always interesting to me particularly
for folks that take on the challenge of parsing it as opposed to putting that in a box that
you know I'm just doing what I do and that's somebody else's issue how have you kind of grapple
with that or come to terms with that or assess that or you know what do you see other folks doing
I mean I think you see a broad range I mean I think the thing that makes me enthusiastic is you
know you see things like the fact conference and um AI ethics and society and like these have been
growing there's now you know sort of like fairness related workshops that essentially every AI
conference that happens um so I think there's clearly like momentum here um I think that it's
I don't know how to say this like I kind of so I remember like a bunch of years ago when like
this is gonna seem like a tangent but it's actually relevant like I remember a bunch of years ago
and like people were like kind of starting to think more seriously about like image captioning
um and so you know I'm thinking of like the work by like tomorrow burg um back in like the
early 2010 something like that maybe late 2000s um and I you know I think
we as a community and like I did a little bit of work in this space like we were kind of an
exploration mode like we didn't really know what was possible and um you know we hadn't quite
figured out like how do I evaluate captions like you know what makes a caption good what makes a
caption bad and you know over time as the field matured it became more and more important to like
tease these things out um and so I think you see a bit of this happening and sort of at least in
the fairness and NLP space these days where like I think for a while we kind of weren't really
sure what we were doing and what was possible and like what wasn't possible and like as we're
getting a better sense of the space um like I think things are starting to get like more concrete um
and uh like more grounded in like the real world I think the challenge with this analogy is that
like getting a caption wrong and like getting fairness wrong are like very different things um
and so I think that's like one of the reasons why I think it's like I mean I want to sound like a
broken record but like why it's like super important to like lean on all the work from uh you know
social linguistics and linguistic anthropology and particular like black feminist scholarship in
the space because like there's been so much written about this and like if we like I have this kind
of metaphor like when the NLP community started doing like syntactic parsing it's not like a bunch of
people like huddled in a room who didn't know anything about syntax and like tried to invent syntax
like we went and we read what linguists had to say about syntax and like you know we built I
mean not we are then involved in this of course but like you know the people who did this like
built you know like a 120-page annotation guideline for how to annotate syntactic trees on sentences
and we spent you know a million dollars annotating the pantry bank and you know an annotation has
continued like this and so I think in various parts of NLP there's been like really tight connections
between um you know I don't know maybe not quite what like Emily would like but um there has been
tight connections between how linguists conceptualize problems and how we go about annotating them
and building systems and I think like what we really need to see more of I mean not to say that
it doesn't exist but I think we could do a lot more in uh sort of the fairness for NLP space
is do a similar thing like um you know it would be kind of absurd to the community to think about
doing parsing without looking at what syntacticians have thought about like why should we be thinking
about doing like gender in NLP without looking at what gender studies people or sociolinguistic
the sociolinguists have have talked about so um you know I think people are digging more into this
literature it's kind of dense sometimes so it can be time consuming but like I think it's like
super worth it awesome well how thanks so much for uh sharing all that with us taking a few minutes
out of your clearly busy schedule about to launch a major research conference uh this weekend
a couple days one day yeah this is sunday or monday for this one um you know you'd think I
would know sunday is the expo um monday is tutorials and then that's also when we give out awards
and stuff like that and then the main conference program starts Tuesday uh awesome well thanks for
taking a few minutes out of your busy pre-conference schedule to share a bit with us um it was
wonderful thank you yeah thanks for having me
