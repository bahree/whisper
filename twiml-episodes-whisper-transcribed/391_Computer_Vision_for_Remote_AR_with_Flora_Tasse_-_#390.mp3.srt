1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:25,680
I'm your host Sam Charrington.

3
00:00:25,680 --> 00:00:29,920
Before we get to today's episode, I'd like to send a huge thank you to our friends

4
00:00:29,920 --> 00:00:36,280
at Qualcomm for their support of the podcast and their sponsorship of this series.

5
00:00:36,280 --> 00:00:42,640
Qualcomm AI research is dedicated to advancing AI to make its core capabilities, perception,

6
00:00:42,640 --> 00:00:47,440
reasoning, and action ubiquitous across devices.

7
00:00:47,440 --> 00:00:52,080
Their work makes it possible for billions of users around the world to have AI enhanced

8
00:00:52,080 --> 00:00:56,680
experiences on Qualcomm technology's powered devices.

9
00:00:56,680 --> 00:01:07,120
To learn more about what Qualcomm is up to on the research front, visit twimalai.com-qualcomm-qalcomm-al-c-o-m-m.

10
00:01:07,120 --> 00:01:10,280
And now onto the show.

11
00:01:10,280 --> 00:01:12,760
Alright everyone, I am here with Flora Tase.

12
00:01:12,760 --> 00:01:18,880
Flora is head of computer vision and AI research at Stream, a company that she joined

13
00:01:18,880 --> 00:01:23,840
through the acquisition of another company which she co-founded, Solario.

14
00:01:23,840 --> 00:01:26,680
Flora, welcome to the Twimal AI Podcast.

15
00:01:26,680 --> 00:01:28,880
Thank you Sam, it's a pleasure to be here.

16
00:01:28,880 --> 00:01:29,880
Thanks for the invite.

17
00:01:29,880 --> 00:01:33,720
It's a pleasure to have you on the show and I'm really looking forward to digging into

18
00:01:33,720 --> 00:01:38,160
your recent CVPR presentation.

19
00:01:38,160 --> 00:01:42,200
You did a keynote at the ARVR workshop at CVPR.

20
00:01:42,200 --> 00:01:49,120
I was excited to see that there's enough happening at the kind of intersection of ARVR and

21
00:01:49,120 --> 00:01:55,040
machine learning to, you know, that there's a workshop at CVPR on that topic.

22
00:01:55,040 --> 00:01:57,040
These tons happening.

23
00:01:57,040 --> 00:01:58,040
It was awesome.

24
00:01:58,040 --> 00:01:59,040
These tons happening.

25
00:01:59,040 --> 00:02:00,040
That's face.

26
00:02:00,040 --> 00:02:01,440
So it was definitely a good workshop.

27
00:02:01,440 --> 00:02:02,440
Yeah.

28
00:02:02,440 --> 00:02:07,320
Well, you can tell us all about that, but before you do, please share a little bit about

29
00:02:07,320 --> 00:02:14,400
your background and how you got started in computer vision, how you came to found Solario,

30
00:02:14,400 --> 00:02:16,160
how you ended up at Stream.

31
00:02:16,160 --> 00:02:17,760
I'd love to hear all of it.

32
00:02:17,760 --> 00:02:22,280
That's like a, that would be a long story, but I would try to be the shot.

33
00:02:22,280 --> 00:02:26,240
So I was born and raised in Cameroon.

34
00:02:26,240 --> 00:02:33,080
So Cameroon is in Central Africa and more precisely in the city of Duala.

35
00:02:33,080 --> 00:02:38,920
And so I was raised in the French speaking part of the country, so you might notice that

36
00:02:38,920 --> 00:02:39,920
for my accent.

37
00:02:39,920 --> 00:02:40,920
Okay.

38
00:02:40,920 --> 00:02:47,120
There's some French in there and yeah, so from a very early age, I was very much into

39
00:02:47,120 --> 00:02:53,240
special effects and movies and more precisely Jurassic Park.

40
00:02:53,240 --> 00:02:58,600
So I was a big fan of the, of the movies because of the dinosaurs.

41
00:02:58,600 --> 00:03:02,480
I would just like, I would sit like this close to the TV.

42
00:03:02,480 --> 00:03:07,760
I would introduce dinosaurs and wondering, how can it be like, why is this so real?

43
00:03:07,760 --> 00:03:13,120
And then I asked my dad, like, how can they make these extinct creatures look so realistic?

44
00:03:13,120 --> 00:03:18,000
And then he said, graphics, it's like, okay, yeah, that's what I'm going to do.

45
00:03:18,000 --> 00:03:25,280
That seems like my perfect dream, you know, making the impossible become possible.

46
00:03:25,280 --> 00:03:32,240
So fast forward a few years, I did my bachelor in maths in the English speaking part.

47
00:03:32,240 --> 00:03:38,200
So I had to learn English and then move and did maths because I couldn't, they had no

48
00:03:38,200 --> 00:03:40,320
course in computer science.

49
00:03:40,320 --> 00:03:47,760
So then move out of the country to South Africa, where I did a master in, in Cape Town, beautiful

50
00:03:47,760 --> 00:03:48,920
city in South Africa.

51
00:03:48,920 --> 00:03:55,920
Yeah, it's amazing, I recommend it's a great vacation spot.

52
00:03:55,920 --> 00:04:00,880
So I did my, my master's there then came to Cambridge for my PhD.

53
00:04:00,880 --> 00:04:08,560
So 2012, I arrived in the UK already to make my dreams come true.

54
00:04:08,560 --> 00:04:11,480
And so that was definitely like a good experience.

55
00:04:11,480 --> 00:04:18,000
So at the Cambridge, I was looking at how do you take real things in images and turn them

56
00:04:18,000 --> 00:04:19,840
into 3D content.

57
00:04:19,840 --> 00:04:23,880
So I was doing some 3D shape retrieval, shape analysis from images.

58
00:04:23,880 --> 00:04:30,160
I mean, you have a 2D image and you want to turn that into a 3D shape.

59
00:04:30,160 --> 00:04:31,160
Exactly.

60
00:04:31,160 --> 00:04:33,680
Yeah, that's exactly what I was doing.

61
00:04:33,680 --> 00:04:39,920
So for years or three and a half years, I stumbled upon like a great discovery that if you

62
00:04:39,920 --> 00:04:47,040
actually incorporate NLP, so if you're in corporate language information, you can basically

63
00:04:47,040 --> 00:04:52,680
get really, really accurate results going from 2D to 3D.

64
00:04:52,680 --> 00:04:59,840
And so the addition of that, yeah, so the concept was that rather than just looking at images

65
00:04:59,840 --> 00:05:05,600
as just, you know, pixels that you can then turn into some features and then use classification

66
00:05:05,600 --> 00:05:10,680
on them, it was a concept that those features can actually be semantic features.

67
00:05:10,680 --> 00:05:15,600
So there's some semantic meaning attached to those descriptors that you are, that you

68
00:05:15,600 --> 00:05:16,600
are generating.

69
00:05:16,600 --> 00:05:17,600
Okay.

70
00:05:17,600 --> 00:05:23,880
And so that means that of the text that would associate or would be associated with an image.

71
00:05:23,880 --> 00:05:29,880
So yeah, the very early version was just, so if I look at the chair, I know it's a chair.

72
00:05:29,880 --> 00:05:34,600
So I have like my data, my data set has labels like this is an image of a chair and this

73
00:05:34,600 --> 00:05:35,600
is a chair.

74
00:05:35,600 --> 00:05:40,480
And what we are trying to do is create a representation that capture both of those information.

75
00:05:40,480 --> 00:05:45,880
So it captures both the visual information and the text information, which in this case

76
00:05:45,880 --> 00:05:51,560
is just the semantic meaning of what the word share is.

77
00:05:51,560 --> 00:05:56,040
So in language processing, they have a lot of language models.

78
00:05:56,040 --> 00:06:01,880
So that's what they use for translation and other NLP tasks and they do this by representing

79
00:06:01,880 --> 00:06:06,640
those words as multi-dimensional vectors.

80
00:06:06,640 --> 00:06:12,160
And so what we were doing was creating a representation that would capture both the visual

81
00:06:12,160 --> 00:06:16,360
data and that language based representation.

82
00:06:16,360 --> 00:06:21,160
In that, it would capture semantic data inside the representation.

83
00:06:21,160 --> 00:06:26,600
And since that is actually, you have the same concept in 3D shapes, in hand drawn sketches,

84
00:06:26,600 --> 00:06:29,960
you can use that as a way of bridging the garbage in all of these domains.

85
00:06:29,960 --> 00:06:32,520
That was my PhD, basically.

86
00:06:32,520 --> 00:06:38,320
And so we had really good results, really increased the bench, the baseline by like 30% in some

87
00:06:38,320 --> 00:06:40,400
of the use cases.

88
00:06:40,400 --> 00:06:42,040
And so that was great.

89
00:06:42,040 --> 00:06:48,000
And the question is what happens next, what do we do with this?

90
00:06:48,000 --> 00:06:52,960
So there were some offers on the table, like big companies could go and work for, but

91
00:06:52,960 --> 00:06:58,360
I was really passionate about that specific discovery and how you could scale that to not

92
00:06:58,360 --> 00:07:01,920
just one object, but multiple scenes.

93
00:07:01,920 --> 00:07:08,360
Can I then take a picture of my living room and then turn each object into a hidden shape

94
00:07:08,360 --> 00:07:11,920
and build a scene graph of my surrounding?

95
00:07:11,920 --> 00:07:16,320
And so that became Celerio, that's what Celerio was doing.

96
00:07:16,320 --> 00:07:22,320
So it's fun, now that work out of the university created a startup called Celerio.

97
00:07:22,320 --> 00:07:28,360
That was looking at how do you take the real world and turn that into a digital format?

98
00:07:28,360 --> 00:07:33,080
Yeah, the story of that for how long before stream came along.

99
00:07:33,080 --> 00:07:34,400
Two years and a half.

100
00:07:34,400 --> 00:07:35,400
Okay.

101
00:07:35,400 --> 00:07:36,400
Yes.

102
00:07:36,400 --> 00:07:37,400
So it was a great journey.

103
00:07:37,400 --> 00:07:43,680
Like journey, we got to like build something I was running on the phone, had it tested by

104
00:07:43,680 --> 00:07:49,800
a bunch of like AR developers, it was a really cool and interesting journey.

105
00:07:49,800 --> 00:07:55,080
And during that testing phase, stream was one of the people who were testing our platform,

106
00:07:55,080 --> 00:07:57,400
looking to integrate that into their product.

107
00:07:57,400 --> 00:08:02,600
And we had you had met stream like a year ago because we have a common investor.

108
00:08:02,600 --> 00:08:04,360
And so we really like what they were doing.

109
00:08:04,360 --> 00:08:10,480
So stream is all about remote collaboration and how do you add value to that using open

110
00:08:10,480 --> 00:08:11,480
technology?

111
00:08:11,480 --> 00:08:16,760
So you take you take a video collaboration platform and then you incorporate it at a

112
00:08:16,760 --> 00:08:19,040
very core of it.

113
00:08:19,040 --> 00:08:23,680
And so they were doing that for like really amazing use cases like repairing, customer support,

114
00:08:23,680 --> 00:08:25,520
home repairs and so on.

115
00:08:25,520 --> 00:08:31,040
And so they came with us with an offer because they saw what we're doing as very close to

116
00:08:31,040 --> 00:08:36,760
like the vision of them of their product.

117
00:08:36,760 --> 00:08:39,760
And we were very interested in like bringing this tech to the real world.

118
00:08:39,760 --> 00:08:45,120
So we had been hard at work building the technology and we're really happy about the results,

119
00:08:45,120 --> 00:08:50,960
but we couldn't wait to actually see that being used by real customers and stream hello

120
00:08:50,960 --> 00:08:53,120
one of the best use cases for AR.

121
00:08:53,120 --> 00:08:56,520
And so that's how the acquisition happened.

122
00:08:56,520 --> 00:08:57,520
That's awesome.

123
00:08:57,520 --> 00:08:58,520
Yeah, go.

124
00:08:58,520 --> 00:08:59,520
Thank you.

125
00:08:59,520 --> 00:09:00,520
Thank you.

126
00:09:00,520 --> 00:09:07,960
So I'll say before we get very far, we will link in the show notes to your slides from

127
00:09:07,960 --> 00:09:10,840
the AR VR workshop.

128
00:09:10,840 --> 00:09:14,280
And folks who are listening should definitely check them out because there are a lot of

129
00:09:14,280 --> 00:09:20,440
cool videos and demos and things like that in the slides.

130
00:09:20,440 --> 00:09:27,760
So you know, maybe tell us a little bit about your talk and kind of what some of the main

131
00:09:27,760 --> 00:09:30,440
points that you're presenting.

132
00:09:30,440 --> 00:09:36,040
So the talk was entitled computer vision for remote AR.

133
00:09:36,040 --> 00:09:41,080
And so a lot of time people think of AR VR, they think of a mobile phone, they think

134
00:09:41,080 --> 00:09:48,600
of a headset, they think of, you know, like, yes, basically.

135
00:09:48,600 --> 00:09:53,640
And oh, like they would think of like an experience where you are looking at something and then

136
00:09:53,640 --> 00:09:56,800
you have something augmented on top of that.

137
00:09:56,800 --> 00:10:03,960
So remote AI is very different because someone else across the ocean is kind of experiencing

138
00:10:03,960 --> 00:10:08,320
what you are experiencing, but they are not in that same physical location.

139
00:10:08,320 --> 00:10:13,480
And so then I was talking about like, yes, what remote AR is and how computer vision can

140
00:10:13,480 --> 00:10:19,480
then help solve some of the issues that take place in that kind of scenario.

141
00:10:19,480 --> 00:10:23,960
And more precisely, I was looking at the stream use case, what we do and how we are using

142
00:10:23,960 --> 00:10:27,080
computer vision to solve some of the issues.

143
00:10:27,080 --> 00:10:30,800
One of them is something like measuring the environment.

144
00:10:30,800 --> 00:10:37,640
So a quick example, if I'm here in my living room and I have an issue with my fridge and

145
00:10:37,640 --> 00:10:43,480
I call one of these companies, I call their customer support and say I have this issue.

146
00:10:43,480 --> 00:10:49,480
Now they can use stream and the augmented reality platform that we build so that the expert

147
00:10:49,480 --> 00:10:54,520
we call it a pro on the other side can have visibility into my environment and what I'm

148
00:10:54,520 --> 00:10:55,920
looking at.

149
00:10:55,920 --> 00:11:01,520
And so sometimes one of the main use case, main question that they have is can we measure

150
00:11:01,520 --> 00:11:02,520
things?

151
00:11:02,520 --> 00:11:06,400
We want to be able to measure something so that if we send, if something is broken and

152
00:11:06,400 --> 00:11:10,320
we have to send something over, you know, you know, we know what size it should be.

153
00:11:10,320 --> 00:11:12,040
How can you make measure and more accurate?

154
00:11:12,040 --> 00:11:14,800
And it's really hard to do that on an image.

155
00:11:14,800 --> 00:11:19,520
So one thing that we do is being able to build a 3D mesh of the environment.

156
00:11:19,520 --> 00:11:24,880
And so once you have a 3D mesh, then the pro can just say from point A to point B, what

157
00:11:24,880 --> 00:11:27,400
is the distance?

158
00:11:27,400 --> 00:11:32,280
And so things that are important is measuring like the geometric data, but also the texture

159
00:11:32,280 --> 00:11:35,880
data because once you have texture, it gives you context.

160
00:11:35,880 --> 00:11:38,480
And so I was, yeah.

161
00:11:38,480 --> 00:11:46,040
The meshes that you build or are working with are these based solely on 2D data or like

162
00:11:46,040 --> 00:11:50,680
the iPad now has a lidar sensor in it, it's got to make the job easier.

163
00:11:50,680 --> 00:11:51,680
Yeah, definitely.

164
00:11:51,680 --> 00:11:56,360
That has been, yeah, that's, that's, I look forward to more devices have interesting.

165
00:11:56,360 --> 00:12:01,040
So we definitely have a framework that we build in house for doing meshing from only to

166
00:12:01,040 --> 00:12:02,040
the images.

167
00:12:02,040 --> 00:12:03,040
Okay.

168
00:12:03,040 --> 00:12:07,000
But that wouldn't be needed as accurate as using depth sensing, definitely.

169
00:12:07,000 --> 00:12:12,840
So we have different solutions, depending on the capability of the device that the customer

170
00:12:12,840 --> 00:12:13,840
is using.

171
00:12:13,840 --> 00:12:14,840
Okay.

172
00:12:14,840 --> 00:12:15,840
Yeah.

173
00:12:15,840 --> 00:12:20,840
And it's building the 3D mesh from the 2D image.

174
00:12:20,840 --> 00:12:25,320
Talk a little bit about, you know, is that a largely solved problem?

175
00:12:25,320 --> 00:12:26,960
Is it still like really hard?

176
00:12:26,960 --> 00:12:32,360
Like where is it on the spectrum of, you know, computer vision, AR, types of challenges?

177
00:12:32,360 --> 00:12:37,000
Yeah, it's still, it's still really hard, so it's a solved problem.

178
00:12:37,000 --> 00:12:43,120
As solved as it can be, what people are not doing is adding AI for predicting depth, but

179
00:12:43,120 --> 00:12:47,920
I see some challenges around using predicted depth for meshing because you need temporal

180
00:12:47,920 --> 00:12:48,920
coherence.

181
00:12:48,920 --> 00:12:52,800
So as you go from A to 3D, you need things to be kind of correct.

182
00:12:52,800 --> 00:12:56,320
You need a depth to be current across the different frames.

183
00:12:56,320 --> 00:13:00,440
So that's still a, that's still some of the people are looking at.

184
00:13:00,440 --> 00:13:03,800
So for the vast majority, multiple are using motion stereo.

185
00:13:03,800 --> 00:13:10,880
So you look at consecutive frames and you are trying to find the values that are consistent

186
00:13:10,880 --> 00:13:12,720
across multiple frames.

187
00:13:12,720 --> 00:13:18,560
And so that is going to be harder if you have frames that don't have much texture, depending

188
00:13:18,560 --> 00:13:22,120
on the change of lighting, that still would be tricky.

189
00:13:22,120 --> 00:13:28,000
So people are now looking towards like stereo systems or, you know, devices that have

190
00:13:28,000 --> 00:13:29,480
their same thing.

191
00:13:29,480 --> 00:13:35,480
So there's not much work going into, you know, meshing from 2D images.

192
00:13:35,480 --> 00:13:39,080
There's not that much work going in there because there will always be some limitations

193
00:13:39,080 --> 00:13:42,080
because of just the nature of the, of the problem.

194
00:13:42,080 --> 00:13:46,160
And the stereo use case that you're describing is that assuming multiple cameras?

195
00:13:46,160 --> 00:13:47,160
Yeah.

196
00:13:47,160 --> 00:13:52,440
So they, one of the, some of the techniques that people are using from 2D images is like

197
00:13:52,440 --> 00:13:58,640
using motion stereo, so you kind of look at two images from the same camera and if they

198
00:13:58,640 --> 00:14:01,040
are close enough, you can simulate stereo.

199
00:14:01,040 --> 00:14:04,920
And obviously you also have like phones that have left and right images.

200
00:14:04,920 --> 00:14:09,400
In that sense, you can just like make the problem easier because you don't have to try to

201
00:14:09,400 --> 00:14:14,480
rectify, you know, images that are happening at different time frames, time stamps.

202
00:14:14,480 --> 00:14:19,360
You, I'm sorry, I interrupted you, you're going through these use cases that you see and

203
00:14:19,360 --> 00:14:20,960
one of them is measuring things.

204
00:14:20,960 --> 00:14:21,960
Yeah, measuring things.

205
00:14:21,960 --> 00:14:28,360
Yeah, I'm just making the point that for measuring things, meshing and texture are very important.

206
00:14:28,360 --> 00:14:34,560
So part of my talk was, how do you attach texture to a mesh that is changing all the time

207
00:14:34,560 --> 00:14:39,520
and how do you do that in a way that's real time so that it doesn't interrupt the customer

208
00:14:39,520 --> 00:14:40,520
experience.

209
00:14:40,520 --> 00:14:45,560
So that was one of the big points I talk about in the presentation.

210
00:14:45,560 --> 00:14:46,560
Okay.

211
00:14:46,560 --> 00:14:47,560
Yeah.

212
00:14:47,560 --> 00:14:48,560
It was drilling to that.

213
00:14:48,560 --> 00:14:52,560
Is that something that your research is focusing on at a stream?

214
00:14:52,560 --> 00:14:53,560
Yeah.

215
00:14:53,560 --> 00:14:56,840
So that's one thing that you are actively working on is.

216
00:14:56,840 --> 00:15:01,560
So a lot of prior work, if you look at some of the mesh reconstruction frameworks out

217
00:15:01,560 --> 00:15:05,920
there, they will give you a color per vertex.

218
00:15:05,920 --> 00:15:09,800
So they don't really have, so the colors kind of look really washed out and blurry.

219
00:15:09,800 --> 00:15:16,040
And so it's kind of hard to like make out the edges of, you know, a countertop in the

220
00:15:16,040 --> 00:15:17,040
mesh.

221
00:15:17,040 --> 00:15:22,800
And so we see texture as a big part of, you know, making that easier, making is easier

222
00:15:22,800 --> 00:15:27,840
for people to be able to like make out the context and the edges.

223
00:15:27,840 --> 00:15:33,120
And so it becomes really tricky to like most of the time this, this is happening.

224
00:15:33,120 --> 00:15:36,480
The testing process happens after the meshing.

225
00:15:36,480 --> 00:15:40,920
We are trying to do that in real time as the mesh gets updated all the time.

226
00:15:40,920 --> 00:15:44,400
So that's, that is what makes it very challenging.

227
00:15:44,400 --> 00:15:46,880
And that's the research problem that you are working on.

228
00:15:46,880 --> 00:15:51,680
And so the textures that we're talking about also like at some kind of texture estimate

229
00:15:51,680 --> 00:15:53,600
or something per vertex.

230
00:15:53,600 --> 00:15:54,600
Yeah.

231
00:15:54,600 --> 00:15:59,280
So you have, you have like a mesh that you obtain from, it doesn't matter where you got

232
00:15:59,280 --> 00:16:00,280
it from.

233
00:16:00,280 --> 00:16:04,640
And you have some video and you're trying to kind of, and you know where your camera is.

234
00:16:04,640 --> 00:16:09,880
So I mean, time you can say, I'm at this point looking into this direction, how can I use

235
00:16:09,880 --> 00:16:14,080
that information and apply that to my mesh.

236
00:16:14,080 --> 00:16:17,800
And the camera is moving all the time, the video has blur.

237
00:16:17,800 --> 00:16:24,480
So how do you then like kind of try to make a mesh that looks good, at least like with

238
00:16:24,480 --> 00:16:26,160
no artifacts.

239
00:16:26,160 --> 00:16:30,320
So that's, yeah, that's a problem that you look at.

240
00:16:30,320 --> 00:16:37,400
And is the end application in trying to apply the colors and textures to the mesh?

241
00:16:37,400 --> 00:16:43,000
Are you essentially trying to create a, like a 3D model of whatever the camera is seeing

242
00:16:43,000 --> 00:16:44,480
someplace else?

243
00:16:44,480 --> 00:16:49,200
Yes, because yeah, not only you are doing that, you are trying to, so it, this is remote

244
00:16:49,200 --> 00:16:50,200
AR, right?

245
00:16:50,200 --> 00:16:54,320
So this could be shared with one person, multiple people who are looking at the same scene and

246
00:16:54,320 --> 00:16:59,640
they actually can like, they can insert 3D elements and things like that.

247
00:16:59,640 --> 00:17:03,360
So not only you are trying to do this, meshing this texturing, you are also trying to sync

248
00:17:03,360 --> 00:17:05,560
it across devices.

249
00:17:05,560 --> 00:17:10,520
So there's a lot of networking challenges that come into place when you start doing these

250
00:17:10,520 --> 00:17:11,520
things.

251
00:17:11,520 --> 00:17:19,280
So I'm wondering why I need the mesh to be very accurate if I've got the video stream.

252
00:17:19,280 --> 00:17:23,400
So because it's really hard to do measurements in a video stream because the camera is always

253
00:17:23,400 --> 00:17:24,400
moving.

254
00:17:24,400 --> 00:17:29,280
So if you're a professional on the other end, you pretty much have to say, oh, like, can

255
00:17:29,280 --> 00:17:33,200
you just be still for like the next 20 seconds?

256
00:17:33,200 --> 00:17:38,040
Well, I take this measurement and the measurement would be accurate because you might be clicking

257
00:17:38,040 --> 00:17:46,760
at some place, but actually, it's not, it's the pixel or the vertex data at that location

258
00:17:46,760 --> 00:17:47,760
is not that correct.

259
00:17:47,760 --> 00:17:52,040
You actually look at something behind, let's say behind the fridge instead of like on the

260
00:17:52,040 --> 00:17:53,040
edge of the fridge.

261
00:17:53,040 --> 00:17:56,560
And so when you have 3D information, it gives you just more context.

262
00:17:56,560 --> 00:18:03,000
You can rotate the mesh and then make sure that you're actually clicking the right end

263
00:18:03,000 --> 00:18:04,000
point.

264
00:18:04,000 --> 00:18:08,960
And so then having the color and the texture information associated with the mesh allows

265
00:18:08,960 --> 00:18:15,320
the person who's remote to make sure that they're clicking on the right things, exactly,

266
00:18:15,320 --> 00:18:16,320
exactly.

267
00:18:16,320 --> 00:18:18,800
So they're the experts so that they know how to take this measurement.

268
00:18:18,800 --> 00:18:22,040
Usually they have to come into the home to do this.

269
00:18:22,040 --> 00:18:26,280
And in the home, they are in 3D because they're in the location.

270
00:18:26,280 --> 00:18:30,920
So how can we kind of replicate that experience in a remote setting where they're not physically

271
00:18:30,920 --> 00:18:37,400
there, so that's where like 3D information is so, so important.

272
00:18:37,400 --> 00:18:41,560
And if you fast forward like in a setting where you have headsets, then obviously you can

273
00:18:41,560 --> 00:18:46,040
put on your headset and you can actually walk around the environment and feel like you

274
00:18:46,040 --> 00:18:48,080
are in the physical location.

275
00:18:48,080 --> 00:18:53,800
And again, with the color and texture, you want, certainly with the headset, you want

276
00:18:53,800 --> 00:18:58,600
to have a situation where you're kind of walking around navigating the world and the

277
00:18:58,600 --> 00:19:02,440
video might not be there anymore or the video might be going in in a different direction

278
00:19:02,440 --> 00:19:03,440
or exactly.

279
00:19:03,440 --> 00:19:04,440
Exactly.

280
00:19:04,440 --> 00:19:05,440
Yeah.

281
00:19:05,440 --> 00:19:06,440
Okay.

282
00:19:06,440 --> 00:19:07,440
Cool.

283
00:19:07,440 --> 00:19:16,240
So texture as a big element of measurement was one of the points that you went into.

284
00:19:16,240 --> 00:19:19,320
What were some of the other cases you covered?

285
00:19:19,320 --> 00:19:26,320
So one thing that we don't see more shared computer vision conference, like even ARV are,

286
00:19:26,320 --> 00:19:33,720
you know, applications, it's just been able to know is extract metadata from the environment.

287
00:19:33,720 --> 00:19:38,440
So more precisely, if you are looking at see a washing machine and a place looking to

288
00:19:38,440 --> 00:19:42,960
fix this washing machine, it's very important for them to know what is the serial number,

289
00:19:42,960 --> 00:19:44,920
what model are we looking at.

290
00:19:44,920 --> 00:19:48,920
So that's that's like the bread and bread of customer support.

291
00:19:48,920 --> 00:19:54,760
And how do we, and then the customer really don't know this information or had you kind

292
00:19:54,760 --> 00:20:00,560
of like they are focused on the fixing part of it, not necessarily on the data collection

293
00:20:00,560 --> 00:20:02,080
part of the process.

294
00:20:02,080 --> 00:20:07,720
So one of the computer vision problems that you have is how do you look at a device and

295
00:20:07,720 --> 00:20:16,200
be able to extract information about model number, serial number, especially from labels.

296
00:20:16,200 --> 00:20:20,400
So if some of these devices, they make it easy for you to have, they have labels.

297
00:20:20,400 --> 00:20:26,200
And then can you look at text and know what text is relevant and what text is associated

298
00:20:26,200 --> 00:20:29,560
to a serial number or to a model number.

299
00:20:29,560 --> 00:20:35,720
So that's the metadata extraction part of what we do.

300
00:20:35,720 --> 00:20:41,680
Having repaired a bunch of home appliances or at least attempted to, you know, and

301
00:20:41,680 --> 00:20:45,160
seeing a lot of these labels, they have all kinds of numbers on them and trying to figure

302
00:20:45,160 --> 00:20:47,400
out which ones are the right ones.

303
00:20:47,400 --> 00:20:55,600
It sounds similar to looking at some other kind of structured or semi structured text,

304
00:20:55,600 --> 00:20:59,960
like a document, like an invoice and trying to figure out what's the invoice number

305
00:20:59,960 --> 00:21:02,840
and what are the other items and that kind of thing.

306
00:21:02,840 --> 00:21:03,840
Yeah, yeah.

307
00:21:03,840 --> 00:21:05,440
And it can be so hard, right?

308
00:21:05,440 --> 00:21:08,680
The orientation is probably going to be weird and a perspective is probably going to

309
00:21:08,680 --> 00:21:09,680
be weird.

310
00:21:09,680 --> 00:21:10,680
Yeah.

311
00:21:10,680 --> 00:21:12,080
The lighting is probably going to be bad.

312
00:21:12,080 --> 00:21:13,080
Definitely.

313
00:21:13,080 --> 00:21:15,920
Like we do a lot with like skewed angles, like depending on where you are looking from,

314
00:21:15,920 --> 00:21:18,000
it can be very distorted.

315
00:21:18,000 --> 00:21:21,400
Adding to the fact that like manufacturers have very different standards.

316
00:21:21,400 --> 00:21:25,240
So you can't have like a rule-based system that says, oh, like we'll always be looking

317
00:21:25,240 --> 00:21:29,600
at, you know, this area of the label.

318
00:21:29,600 --> 00:21:37,000
So AI, this is where like AI really comes, comes handy because so yeah, we just basically

319
00:21:37,000 --> 00:21:40,320
look at it as a two-part problem.

320
00:21:40,320 --> 00:21:42,040
First is an OCR problem.

321
00:21:42,040 --> 00:21:47,320
So how can I just extract text only data from this image?

322
00:21:47,320 --> 00:21:51,880
And then it's, you know, which part are relevant to what we care about.

323
00:21:51,880 --> 00:21:56,680
And so that becomes a classification problem based on the textual data, based on ways

324
00:21:56,680 --> 00:22:01,720
located, train over a lot of labels and a lot of a lot of data.

325
00:22:01,720 --> 00:22:06,720
We can now kind of look at the label and tell the custom, the pro that this is a serial

326
00:22:06,720 --> 00:22:07,720
number.

327
00:22:07,720 --> 00:22:11,200
This is the model number of these appliance.

328
00:22:11,200 --> 00:22:13,200
What are the OCR part of that?

329
00:22:13,200 --> 00:22:19,520
Are you kind of taking the image as is and pulling the letters out?

330
00:22:19,520 --> 00:22:26,000
You know, however they are, are you trying to reorient that kind of as a, you know, ortho-rectification

331
00:22:26,000 --> 00:22:29,200
process so that it's straight?

332
00:22:29,200 --> 00:22:34,760
So we, so we leverage a lot of like the existing frameworks for OCR because OCR is just about

333
00:22:34,760 --> 00:22:37,920
taking some image, getting text out of it.

334
00:22:37,920 --> 00:22:44,360
So we can do, you can do a lot of, like a lot of data augmentation that happens in this,

335
00:22:44,360 --> 00:22:48,520
so these type of models have done a lot of data pre-processing to get ready.

336
00:22:48,520 --> 00:22:53,400
So they're very good at, you know, at, at extracts and text, it's not straight.

337
00:22:53,400 --> 00:22:54,400
Yeah.

338
00:22:54,400 --> 00:22:55,400
Yeah.

339
00:22:55,400 --> 00:22:57,440
So we don't have to worry about that part that part.

340
00:22:57,440 --> 00:23:01,680
So the biggest, so in this case, the biggest effort is like building a model that is

341
00:23:01,680 --> 00:23:07,280
going to do the, the classification right because obviously like OCR just gives you

342
00:23:07,280 --> 00:23:12,520
random text and so, and in the case of Cian, these numbers, they might occur in different

343
00:23:12,520 --> 00:23:15,760
places, might be like some of the tops, some of the buttons.

344
00:23:15,760 --> 00:23:20,720
And then the OCR might give you groups, but those groups could be like very disjointed.

345
00:23:20,720 --> 00:23:24,720
So how do you combine this information into something that makes sense?

346
00:23:24,720 --> 00:23:32,160
That's where our focus is on, less on the OCR, more like making sense of the OCR output.

347
00:23:32,160 --> 00:23:39,080
And are you using as a feature in that part of the problem, the actual text that might

348
00:23:39,080 --> 00:23:44,520
say, you know, model no or serial no, or are you kind of ignoring that and just looking

349
00:23:44,520 --> 00:23:48,400
at trying to do it based on the number itself?

350
00:23:48,400 --> 00:23:50,640
So we do it based on the whole text.

351
00:23:50,640 --> 00:23:55,040
So from the whole text, we would basically classify, we say, okay, we kept by the Cian

352
00:23:55,040 --> 00:24:00,520
number, what bits of this text represent the Cian number.

353
00:24:00,520 --> 00:24:03,600
And we do that by training a model that does that.

354
00:24:03,600 --> 00:24:09,520
So implicitly, the model is using, is using all of this like, you know, implicit information

355
00:24:09,520 --> 00:24:16,880
around like where this is a like Cian number on top, behind before and so on.

356
00:24:16,880 --> 00:24:20,720
We don't explicitly do it, but I think it's implicitly different.

357
00:24:20,720 --> 00:24:24,000
It's a fun problem.

358
00:24:24,000 --> 00:24:27,400
There were some other use cases that you walked through?

359
00:24:27,400 --> 00:24:28,400
Oh, yeah.

360
00:24:28,400 --> 00:24:32,960
Well, my favorite is always, so when you talk, so going back to what I was seeing around

361
00:24:32,960 --> 00:24:40,000
like blending, what's real, what's virtual and having like this immersive environment,

362
00:24:40,000 --> 00:24:47,360
one of the key things that I'm very excited about is walk through like tutorials in AR.

363
00:24:47,360 --> 00:24:53,680
So you get a new appliance, you want to know how to get started, we can have an AR tutorial

364
00:24:53,680 --> 00:24:59,520
experience around that machine to tell you how to like use it, like it's broken and then

365
00:24:59,520 --> 00:25:04,880
you can have a troubleshooting walk through around that appliance.

366
00:25:04,880 --> 00:25:09,000
And so for me, that's like one of the most fun use case because it gives the power back

367
00:25:09,000 --> 00:25:10,800
to the customer.

368
00:25:10,800 --> 00:25:16,080
And and one of the key problem in being able to do that is being able to identify the

369
00:25:16,080 --> 00:25:21,720
six-duff pose of whatever appliance you are looking at or whatever object you're looking

370
00:25:21,720 --> 00:25:28,200
at, we need to be able to know what is translation, rotation, so we can then impose data like

371
00:25:28,200 --> 00:25:30,800
virtual data around it.

372
00:25:30,800 --> 00:25:36,440
So I talk a bit about how we do six-duff pose estimation for objects like that.

373
00:25:36,440 --> 00:25:40,680
And elaborate on on what that means because I saw that in a presentation, you know, you

374
00:25:40,680 --> 00:25:45,640
get right as 60 and it's like 60, what it was, the six.

375
00:25:45,640 --> 00:25:50,440
Six-duff, well, six-duff, a six degrees of freedom.

376
00:25:50,440 --> 00:25:57,880
So three for translation, three for rotation, so that makes six variables that you have

377
00:25:57,880 --> 00:26:03,600
to estimate in order to like get a post-transform for where your object is respect to your camera.

378
00:26:03,600 --> 00:26:06,520
Oh, got it, got it, got it, got it.

379
00:26:06,520 --> 00:26:12,000
So you've got your you're located in an object in 3D space and it's got some rotation and

380
00:26:12,000 --> 00:26:14,480
that engine is next position.

381
00:26:14,480 --> 00:26:15,480
Yeah.

382
00:26:15,480 --> 00:26:16,480
Okay.

383
00:26:16,480 --> 00:26:23,040
So at any point in time, we are trying to figure out what are the six values so that you can

384
00:26:23,040 --> 00:26:28,560
build a transform from that and then you can use it to impose your digital content around

385
00:26:28,560 --> 00:26:29,560
it.

386
00:26:29,560 --> 00:26:30,560
Okay.

387
00:26:30,560 --> 00:26:31,560
Yeah.

388
00:26:31,560 --> 00:26:37,640
So maybe what are the challenging parts of that problem?

389
00:26:37,640 --> 00:26:44,280
And it's one of, even today, I think for the past, as far as I can remember, people have

390
00:26:44,280 --> 00:26:47,840
always been working on post-destimation.

391
00:26:47,840 --> 00:26:53,200
Even DCI-CDPR, there are so many papers around that because it's a very hard problem to solve.

392
00:26:53,200 --> 00:26:57,000
For us, it's even harder because you are actually not looking at the image, you are looking at

393
00:26:57,000 --> 00:26:58,160
a video.

394
00:26:58,160 --> 00:27:03,800
So your camera is changing and you expect that post to be correct across multiple frames.

395
00:27:03,800 --> 00:27:07,760
So for us, we use a lot of, we use a combination of techniques.

396
00:27:07,760 --> 00:27:16,880
So at DCI-CDPR, you see a lot of work around pure MLB techniques where you can just give

397
00:27:16,880 --> 00:27:22,520
an image to a model and it will tell you what are those six values or it will tell you

398
00:27:22,520 --> 00:27:24,360
what are the key points.

399
00:27:24,360 --> 00:27:33,160
So if you have the key points of your object, you can recover the post-information.

400
00:27:33,160 --> 00:27:37,960
So what we saw with those technologies that are not nearly accurate enough, like typically

401
00:27:37,960 --> 00:27:44,800
you have like a 10 degrees margin of error, which is for us not good enough because we are

402
00:27:44,800 --> 00:27:50,840
going to be superimposing content on top of this in AR so that we need like super accurate

403
00:27:50,840 --> 00:27:52,440
post.

404
00:27:52,440 --> 00:27:59,880
So what we end up doing is we have a combination of both like getting some MLB techniques to

405
00:27:59,880 --> 00:28:04,680
identify key points or even just asking users to select those key points.

406
00:28:04,680 --> 00:28:10,000
And then we focus on like from that initial post, how can we refine it further to really

407
00:28:10,000 --> 00:28:14,160
fit the edges of what is currently in the image.

408
00:28:14,160 --> 00:28:20,400
And so these are called region based approaches where it's looking at what is a background, what

409
00:28:20,400 --> 00:28:25,760
is the foreground, and how can we find a post that best differentiate between the two.

410
00:28:25,760 --> 00:28:30,080
So those are very accurate for this type of use case.

411
00:28:30,080 --> 00:28:31,080
Okay.

412
00:28:31,080 --> 00:28:37,960
It sounds like in this case a big part of the way you improve your results is by kind of

413
00:28:37,960 --> 00:28:41,480
manipulating the user interface so that you can get more information with that.

414
00:28:41,480 --> 00:28:42,480
Exactly.

415
00:28:42,480 --> 00:28:46,200
Overly burdensome on the part of the person is using it.

416
00:28:46,200 --> 00:28:47,200
Yeah.

417
00:28:47,200 --> 00:28:51,000
So that's so we try because accuracy is so important for us.

418
00:28:51,000 --> 00:28:56,840
And if we need some user input, we need to leverage some user input to get to that level

419
00:28:56,840 --> 00:28:57,840
of accuracy.

420
00:28:57,840 --> 00:29:00,360
And that's something that we are happy to do.

421
00:29:00,360 --> 00:29:05,840
And in a sense, it also helps us collect more data so that we can also improve the accuracy

422
00:29:05,840 --> 00:29:06,840
of our models.

423
00:29:06,840 --> 00:29:12,800
Have you seen many products out on the market that are kind of putting this kind of AR in

424
00:29:12,800 --> 00:29:14,040
the hands of the consumer?

425
00:29:14,040 --> 00:29:19,000
Like you mentioned this training use case for washing machine or something.

426
00:29:19,000 --> 00:29:20,240
Is anyone doing that?

427
00:29:20,240 --> 00:29:24,320
I haven't seen anyone doing that.

428
00:29:24,320 --> 00:29:30,240
So I think like streaming is in a very good position because we have like an existing

429
00:29:30,240 --> 00:29:34,760
customer base who are actually asking for these kind of features because they see that

430
00:29:34,760 --> 00:29:35,760
as a big need.

431
00:29:35,760 --> 00:29:40,200
And so that's kind of driving the problems you are working on.

432
00:29:40,200 --> 00:29:43,560
So I think I haven't seen it out there yet.

433
00:29:43,560 --> 00:29:47,080
So maybe people are doing working on that like behind the scenes.

434
00:29:47,080 --> 00:29:53,080
I was just going to say, I've seen a lot of demos of it in a kind of industrial type

435
00:29:53,080 --> 00:30:02,920
of use case like at one of the Microsoft conferences a year ago, the CIO, Sachin

436
00:30:02,920 --> 00:30:11,360
Adela demonstrated a kind of remote repair, you know, a worker goes into a telecom closet,

437
00:30:11,360 --> 00:30:14,920
you know, out in the middle of nowhere and they're trying to figure out how to, you know,

438
00:30:14,920 --> 00:30:17,880
get the 5G working again or something.

439
00:30:17,880 --> 00:30:25,640
There's a, you know, they're supported by someone who's pushing screens, overlay screens

440
00:30:25,640 --> 00:30:29,120
on top of what they're seeing telling you a lot of fixed stuff.

441
00:30:29,120 --> 00:30:35,360
The closest I've seen to it in something, consumery is I don't know how it's kind of

442
00:30:35,360 --> 00:30:38,880
prosumer slash low end enterprise.

443
00:30:38,880 --> 00:30:42,000
And it's actual, I think it's real as opposed to a demo.

444
00:30:42,000 --> 00:30:50,720
But there's a networking company ubiquity networks that makes these again, kind of prosumer

445
00:30:50,720 --> 00:30:56,760
networking gear and their latest line of switches in some of their promotional materials.

446
00:30:56,760 --> 00:31:04,120
They're showing you kind of holding your phone over the, these switches and they're

447
00:31:04,120 --> 00:31:07,840
showing you like what's connected to what in the different ports.

448
00:31:07,840 --> 00:31:08,840
Yes.

449
00:31:08,840 --> 00:31:13,720
But that's probably the only example I've seen of that actually in the wild.

450
00:31:13,720 --> 00:31:14,720
Yeah.

451
00:31:14,720 --> 00:31:15,720
It's a pretty cool.

452
00:31:15,720 --> 00:31:16,720
Yeah.

453
00:31:16,720 --> 00:31:17,720
Definitely.

454
00:31:17,720 --> 00:31:18,720
Yeah.

455
00:31:18,720 --> 00:31:24,280
And I've seen obviously like AI Kit has some demos around like being able to, if you

456
00:31:24,280 --> 00:31:29,520
have like a Lego set and then you, you're able to kind of recognize that and then in

457
00:31:29,520 --> 00:31:33,120
people some 3D content around that.

458
00:31:33,120 --> 00:31:41,240
So there's definitely been some use some demos around this type of use cases.

459
00:31:41,240 --> 00:31:45,200
So when we mentioned Microsoft, I was just thinking, oh, it has a depth sensor.

460
00:31:45,200 --> 00:31:47,200
That would make our life easier.

461
00:31:47,200 --> 00:31:48,200
Yes.

462
00:31:48,200 --> 00:31:49,200
Yes.

463
00:31:49,200 --> 00:31:50,200
Yes.

464
00:31:50,200 --> 00:31:51,200
Yes.

465
00:31:51,200 --> 00:31:57,240
That's why I can't wait for more their sensing to come to phones, but assuming just a phone.

466
00:31:57,240 --> 00:32:01,840
And so you're only able to use things that are commonly available on phones.

467
00:32:01,840 --> 00:32:02,840
Yeah.

468
00:32:02,840 --> 00:32:03,840
Yeah.

469
00:32:03,840 --> 00:32:08,360
So definitely the world where what is going to happen is that like on phones that don't

470
00:32:08,360 --> 00:32:13,160
have the sensing, we have these techniques on phones that do have extra capabilities, then

471
00:32:13,160 --> 00:32:15,520
we can offer.

472
00:32:15,520 --> 00:32:23,280
We can use that data that's sensor data to even push further the accuracy of our methods.

473
00:32:23,280 --> 00:32:26,640
So the more devices of those like the better.

474
00:32:26,640 --> 00:32:30,800
What are some of the other big challenges that you see in the space?

475
00:32:30,800 --> 00:32:31,800
In our space.

476
00:32:31,800 --> 00:32:32,800
Okay.

477
00:32:32,800 --> 00:32:33,800
So one of the big.

478
00:32:33,800 --> 00:32:41,160
So at the end of representation towards the end, I mentioned spatial AI and the yeah, and

479
00:32:41,160 --> 00:32:43,240
what it meant for us.

480
00:32:43,240 --> 00:32:48,960
And for us, it just means that going beyond trying to understand these tasks at the granular

481
00:32:48,960 --> 00:32:51,840
level, but like look at a big picture.

482
00:32:51,840 --> 00:32:56,520
So at the end of the day, what you want to know is in a video, in a collaboration between

483
00:32:56,520 --> 00:32:58,720
two people, what is the problem?

484
00:32:58,720 --> 00:32:59,720
What is the solution?

485
00:32:59,720 --> 00:33:03,880
We're talking about, and how do you capture this information in a way that you can then

486
00:33:03,880 --> 00:33:06,560
reproduce it somewhere else?

487
00:33:06,560 --> 00:33:09,960
And so yeah, you have this idea of both chat boards.

488
00:33:09,960 --> 00:33:14,680
You have this everywhere, like in the banking systems where you have an issue, and then they

489
00:33:14,680 --> 00:33:18,480
can just kind of based on your question, they will suggest you something.

490
00:33:18,480 --> 00:33:23,360
And if they can't fix that, then it will escalate to a human being.

491
00:33:23,360 --> 00:33:29,400
And so I take that AI and I add special AI on top of that, the sense that now this AI

492
00:33:29,400 --> 00:33:35,440
both actually they understand, they can look at what you're looking at, and they can make

493
00:33:35,440 --> 00:33:36,880
sense of it.

494
00:33:36,880 --> 00:33:44,720
So ultimately, can we create agents that are able to solve, like to help you solve these

495
00:33:44,720 --> 00:33:49,280
issues that you're needing to actually talk to a human being?

496
00:33:49,280 --> 00:33:56,120
That's kind of like the big moonshot problem that you are looking at, beyond this individual

497
00:33:56,120 --> 00:33:57,560
computer vision test.

498
00:33:57,560 --> 00:33:59,560
Yeah.

499
00:33:59,560 --> 00:34:07,480
Something as an example there, the, you know, the cliched scene we see in the movies where

500
00:34:07,480 --> 00:34:11,840
the bomb squad comes in and defuse the bomb, you know, someone's holding their phone

501
00:34:11,840 --> 00:34:16,120
over it in the system, as opposed to some remote expert saying, oh, it's definitely the

502
00:34:16,120 --> 00:34:17,120
yellow cable.

503
00:34:17,120 --> 00:34:18,120
Clip the yellow cable.

504
00:34:18,120 --> 00:34:19,120
Exactly.

505
00:34:19,120 --> 00:34:20,120
Exactly.

506
00:34:20,120 --> 00:34:21,120
Yeah.

507
00:34:21,120 --> 00:34:23,120
The bit I'd be right.

508
00:34:23,120 --> 00:34:24,120
Yeah.

509
00:34:24,120 --> 00:34:25,120
The bit I'd be right.

510
00:34:25,120 --> 00:34:28,120
Yeah.

511
00:34:28,120 --> 00:34:31,800
But yeah, that's what some of the things that we are thinking of now is how do we start

512
00:34:31,800 --> 00:34:37,600
getting there by maybe like starting by making sense of the video data, like can we start

513
00:34:37,600 --> 00:34:42,960
clustering videos by, you know, the functionality.

514
00:34:42,960 --> 00:34:48,680
So not just similarities in a sense, yeah, they have similar scenes, but they're trying

515
00:34:48,680 --> 00:34:51,120
to solve similar problems.

516
00:34:51,120 --> 00:34:56,400
So how can you capture that out of a video and be able to start creating these clusters,

517
00:34:56,400 --> 00:35:01,360
which can be very useful for just like training for a company to understand, like what are

518
00:35:01,360 --> 00:35:07,720
the most common issues and capture, you know, what are the solutions to these issues so

519
00:35:07,720 --> 00:35:11,240
that they can replicate that in other environments.

520
00:35:11,240 --> 00:35:12,240
Mm-hmm.

521
00:35:12,240 --> 00:35:18,840
Sounds a little bit like you're describing kind of going back to expert systems types of

522
00:35:18,840 --> 00:35:24,440
approaches for, you know, but coupling that with information that we've pulled using

523
00:35:24,440 --> 00:35:29,800
modern AI techniques, you know, neural networks and the like, am I hearing that correctly?

524
00:35:29,800 --> 00:35:30,800
Yeah.

525
00:35:30,800 --> 00:35:32,800
Special expert systems.

526
00:35:32,800 --> 00:35:35,000
Special expert systems.

527
00:35:35,000 --> 00:35:40,400
The our mission at stream is making the world expertise more accessible.

528
00:35:40,400 --> 00:35:41,400
Okay.

529
00:35:41,400 --> 00:35:46,240
And so how do you open access to like, you know, to everyone.

530
00:35:46,240 --> 00:35:49,320
So that's that's part of how we get to that vision.

531
00:35:49,320 --> 00:35:50,320
Okay.

532
00:35:50,320 --> 00:35:51,320
Yeah.

533
00:35:51,320 --> 00:35:56,880
Do you follow the computer vision space more broadly or is it what do you find interesting

534
00:35:56,880 --> 00:35:59,040
at CVPR this time around?

535
00:35:59,040 --> 00:36:00,040
Oh, man.

536
00:36:00,040 --> 00:36:06,720
Like obviously I'm always kind of looking at more like problems that we are already looking

537
00:36:06,720 --> 00:36:07,720
at.

538
00:36:07,720 --> 00:36:15,800
So one of the work that I read recently was normal assisted death estimation.

539
00:36:15,800 --> 00:36:19,520
And so in the sense, you're not looking at one image to create to predict death, you're

540
00:36:19,520 --> 00:36:25,160
actually looking at a couple of images, which for me is really interesting because then

541
00:36:25,160 --> 00:36:29,840
you just get as far information and you make sure that whatever your outputting is consistent

542
00:36:29,840 --> 00:36:32,040
across multiple views.

543
00:36:32,040 --> 00:36:39,880
And so for me, that's like the stereo problem that you were just using AI, which is good

544
00:36:39,880 --> 00:36:45,520
because you can now scale across like different type of scenes and you can be robustness

545
00:36:45,520 --> 00:36:50,880
to like letting issues and so on, which you can't do with like traditional methods.

546
00:36:50,880 --> 00:36:52,880
So that's that's very interesting.

547
00:36:52,880 --> 00:36:57,080
I'm still like reading through, there's a lot of content at CVPR so I'm still working

548
00:36:57,080 --> 00:37:05,440
my way through everything that was presented, but definitely that's one of them.

549
00:37:05,440 --> 00:37:09,760
And just in a general sense, I'm very excited about machine learning with no without needing

550
00:37:09,760 --> 00:37:16,280
global training data, I think that's a big thing people are still looking at that.

551
00:37:16,280 --> 00:37:22,240
So I'm less interested in being able to like generate photo realistic images, there's

552
00:37:22,240 --> 00:37:28,160
a lot of focus on that, but I really think of like machine learning and computer vision.

553
00:37:28,160 --> 00:37:33,480
And you look at all of these models, they need tons and tons of data to train them.

554
00:37:33,480 --> 00:37:40,480
So there's definitely a new line, some line of work around how do you basically do self-training

555
00:37:40,480 --> 00:37:46,800
systems, how do you do that without needing like a bunch of data, data, so that's something

556
00:37:46,800 --> 00:37:50,320
that I'm very interested about.

557
00:37:50,320 --> 00:37:51,320
Nice.

558
00:37:51,320 --> 00:37:56,400
The workshop that you presented at, is there something that you've been participating

559
00:37:56,400 --> 00:37:59,920
in for a long time or is it new at CVPR?

560
00:37:59,920 --> 00:38:03,160
So this is the second year, the workshop is taking place.

561
00:38:03,160 --> 00:38:10,440
So last year I was attending and we were presenting this scene generation from images work that

562
00:38:10,440 --> 00:38:11,440
I mentioned earlier.

563
00:38:11,440 --> 00:38:18,760
We presented that at the workshop and I was invited for this year keynote, for one of

564
00:38:18,760 --> 00:38:19,760
the keynote this year.

565
00:38:19,760 --> 00:38:25,360
So I was definitely like, I was glad to have that from the talk about our work.

566
00:38:25,360 --> 00:38:26,360
Yeah.

567
00:38:26,360 --> 00:38:28,720
It's a great workshop.

568
00:38:28,720 --> 00:38:32,880
I kind of encourage everyone to go and check the other speakers because there was a lot

569
00:38:32,880 --> 00:38:38,480
of work around just like the vision of AR VR, what's stopping us from, you know, from

570
00:38:38,480 --> 00:38:45,160
everyone doing conferences, you know, through VR, like, what are we not doing that today?

571
00:38:45,160 --> 00:38:46,160
And things like that.

572
00:38:46,160 --> 00:38:48,120
So that was the thing you also.

573
00:38:48,120 --> 00:38:49,120
Interesting.

574
00:38:49,120 --> 00:38:50,120
Yeah.

575
00:38:50,120 --> 00:38:57,000
I did organize the conference a few years ago and we had a presenter on AR VR and kind

576
00:38:57,000 --> 00:39:01,680
of some of the implications for machine learning, but it was super early.

577
00:39:01,680 --> 00:39:06,400
Like, there was nothing really happening.

578
00:39:06,400 --> 00:39:11,480
And so it's exciting for me to hear that, you know, the field is advancing and, you

579
00:39:11,480 --> 00:39:18,880
know, there's a particularly, and is it, is it fair to say that AR in particular because

580
00:39:18,880 --> 00:39:23,000
the scenes need to be understood more.

581
00:39:23,000 --> 00:39:27,520
There's more implications for machine learning than VR, which tends to be more kind of

582
00:39:27,520 --> 00:39:28,520
generative and scripted.

583
00:39:28,520 --> 00:39:29,520
Yeah.

584
00:39:29,520 --> 00:39:30,520
Yeah.

585
00:39:30,520 --> 00:39:31,520
Definitely.

586
00:39:31,520 --> 00:39:37,360
So in a lot of these tasks in augmented reality, we just, we need machine learning because

587
00:39:37,360 --> 00:39:42,000
no one is going to, there's no time to sit down and try and do like, like, mind reality,

588
00:39:42,000 --> 00:39:45,360
take a real scene and then create a digital.

589
00:39:45,360 --> 00:39:49,520
So if you're going to do like physics interact, if you're going to interact with the environment,

590
00:39:49,520 --> 00:39:52,680
then you need to understand that environment and that's computer vision.

591
00:39:52,680 --> 00:39:57,640
That's what computer is all about, taking reality and making sense of it.

592
00:39:57,640 --> 00:40:01,960
So it's very crucial to augmenting your reality.

593
00:40:01,960 --> 00:40:05,800
Well, Flora, thanks so much for taking the time to share what you're up to.

594
00:40:05,800 --> 00:40:06,800
Very cool.

595
00:40:06,800 --> 00:40:07,800
Thank you.

596
00:40:07,800 --> 00:40:10,000
Always happy to talk about these things.

597
00:40:10,000 --> 00:40:11,000
Yeah.

598
00:40:11,000 --> 00:40:12,000
Thanks.

599
00:40:12,000 --> 00:40:13,000
Nice.

600
00:40:13,000 --> 00:40:14,000
Thank you.

601
00:40:14,000 --> 00:40:19,840
All right, everyone, that's our show for today.

602
00:40:19,840 --> 00:40:25,640
For more information on today's show, visit twomolai.com slash shows.

603
00:40:25,640 --> 00:40:29,240
As always, thanks so much for listening and catch you next time.

