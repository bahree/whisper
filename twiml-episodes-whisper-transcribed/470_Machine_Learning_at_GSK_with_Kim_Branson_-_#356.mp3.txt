All right, everyone. I am here with Kim Branson. Kim is senior vice president and global head
of the artificial intelligence and machine learning at GSK. Kim, welcome to the Twomo AI podcast.
Thanks, Sam. Good to be here. Looking forward to our conversation. Let's jump right in. You come
to GSK by way of Genentech, which kind of this traditional or this historical Silicon Valley
challenger. Before that, you spent a lot of time in Silicon Valley and now you're at a large
pharma. Tell us about that journey and how it came to be. Sure. So, I guess my background,
as you know, I did my PhD in machine learning, really looking at
trying machine learning to actually drug discovery. So, small molecule drug discovery and design
was what I started off a bit. And then says, I'm dating myself, but like, you know, in 2003,
around then, and where we're, you know, the cutting edge techniques with things like support
back to machines and random forests and things like that. But doing a lot of work on doing simulation,
a physics simulation, things like that, or small molecules, spying to proteins and all the things
around with that. From there, I ended up doing, you know, a very sort of academic thing of spending
time at Stanford and others. There's a lot of sudden earlier work on machine learning applied
there into graph convolution networks. That sort of stuff was happening. And a lot of my time
was spent actually in, you know, I spent some time at Vertix Pharmaceuticals, but then a favorite
time, so I've been start up and so things like, you know, was involved in early search startup,
there was acquired by Twitter, another company doing large scale medical records aggregation
and differential privacy machine learning on that sort of stuff. And although, like all things,
90% of your time is building all the stuff to do ETLs, extract tables out of PDFs and all those
wonderful things. And that was acquired by Apple. And then a lot of large scale machine learning
on claims, other types of data, so all about like predicting like what's the probability of
predicting disease X at time T given given someone's past medical history. So you could intervene
early and things like that. And from there, you know, many friends at Genentech, I've always been
involved in computational chemistry. So joined Genentech and really was, I guess, really recruited
from Genentech to come to GSK, which, you know, it's one of the things that wasn't really on my radar
as a place to be. And I had some friends there on, you know, I'd known of the company for a while,
but they sort of committed me to come in and I realized that actually it's something very different
happening here. So they put it in a new head of R&D, so how Baron had joined and, you know, how it
was worked at Calico, with definitely Colin people like that. And I was sort of like, well,
there's a guy who really, you know, actually understands what machine learning looks like and what
it requires to be done properly almost. And sort of, you know, really explained to me that like,
it's going to be such a core part of GSK, right, that like they're really serious about it. It's not
just a company like I want to build a 10 person team or double and kind of half resource you,
but it was going to be such a fundamental thing. So that's what sort of led me here and has
led me to create the create the group and then scale as we do now. And give us some context for
GSK. What's the core business and where does ML and AI fit in? Yeah, sure. So we're obviously a
you know, pharmaceutical company, you know, we make, you know, medicines and vaccines across a
bright range of things. And so GSK is an old company, right? It's been about 300 years, right? So
all these companies, every now and then, they go through these sort of revolutions and they,
you know, they sort of turn themselves inside out and reposition themselves. And, you know,
what was really apparent is that we have this increasing body of sort of genetic data. So these
are these large genetic databases. So where we have lots of people, we cost the sequencing has gone
down. So, you know, members, I guess 20 years, we have the human genome, one, one, one, one genome,
now we can do lots, lots of people, right? So you can see sequence lots of people you know about
their medical histories. And you have, you can basically do it, answer a question like, you know,
he's a hero bunch of people that got a disease, hero bunch of people like Brooklyn that don't get
the disease. And ask a question, well, what's different in the genetics, right? The idea is that
the genetics kind of points at a clue as to what you might want to do medicine for, how, what's
involved in that disease. So a really increasing amount of just data being generated in the genetics
side of things. And the other side of things is happening is really these technology for
function genomics, right? So maybe the first wave you can think of as molecular biology, right?
Restriction enzymes, we have passwords, we can like, you know, do genetic engineering and
make a cell make a protein. The next, this is really the continuation of the evolution of those
those tools is now with CRISPR and talons and those sorts of things where you can actually perturb
a specific gene, right? We can turn it up, we'll turn it down in a particular cell type almost
at an almost sort of single cell level. So you now you've got this other set of technologies where
you can start to like generate huge amounts of data at scale. And what it turns out with is that
we're just biology can measure so much more now. There's just this massive amount of measurement
and it's multimodal. So it's, you know, looking at RNA-C kind of single cell levels that's looking
at messenger RNA made as you're doing these edits. You can do cellular imaging, you can do proteomics,
you've got all, you know, all the omics as we call them, you're always explosion of data. And so
really you need machine learning in the middle of it to sort of make sense of the data,
but also even to help you kind of make sense of all the literature you've gotten into plan
the sort of next experiments. And that's in the discovery phase. So it's it's really core,
we have this, this three-pronged strategy that's sort of this, the, you know, genetics, right?
And function dynamics on the side and the AIML in the middle to sort of integrate together
to help us really, you know, find the time that's in design better medicines.
And so is it, is it accurate to think of genetics as the kind of the data source there,
what's happening in the gene genomics as a control point, the way you influence what's happening,
and then AI is telling you what influence to exert based on what you, you know, the patterns
you see. Yeah, so I think the way to think about is, genetics gives us, so this genetic databases
give us a clue of what to start looking at, right? And we know it's really important because we've
done studies, and others have done this as well, we're showing that if you've got a medicine that
works on a target, so remember genes, we call it messenger RNA, messenger RNA,
from proteins, proteins is the thing that does the things in ourselves. And the proteins are,
what we would call it, it's a type of the thing we want to modulate, typically, and that might
would be with a small molecule, things like clinical, things like penicillin or aspirin,
or it could be something like an antibody, right? Which usually extracellular, not usually inside
cells, then it's sort of blocks, blocks something. And so the genetics is giving us the hint of
what to look for, but it doesn't, it's not the whole story, you still need to actually kind of,
it gives you a clue, but you need to sort of go and do further experiments and understand like,
well, you know, is this the correct target, or is it something else that's in this pathway that's
operating in? That's the function genomics coming in, where the function genomics allows us to
basically, what happens when we turn this particular gene off, like lower the level of that protein,
does that look like it has the right effect, right? And it's sort of mimicking the effect of
making a medicines, we don't have to make a medicine to work out whether it works, we can use
that kind of thing to inform it. So it sort of starts to put these things together and
all these experiments now, because the cost of measurement has gone down, because we can take a
single cell and make a change in a single cell, right? And then do RNA sequencing, like look at it,
all the changes in messenger RNA on those single cell levels, it generates a whole bunch of data
that's at a large scale. And that's where we sort of bring machine learning in,
maybe I can illustrate the point, probably the one of the most, one of the key problem we work on
is that in these large genetic databases, we really only want to know what to do with about
probably 15 to 20% of what we call the variance, and a variance is where, you know, your DNA sequence
will differ from mine, right? Maybe I get the disease and you don't, and I've got a particular
mutation, right? If that mutation falls into what we call the open reading frame of a gene, or the
bit that encodes for the protein, that gets translated, mRNA encodes the protein, we know what
affects the protein, and we can go and look at that protein and work out what's happening, you know,
is it not fold, is it fold, but it's missing, it's not as active, that kind of thing.
A whole bunch of them fall outside of that, right? They forms of the regulatory regions of DNA,
and you can imagine there's a whole bunch of control structures in DNA saying what's
the turn on what proteins and what conditions. And so we spend a lot of time in building machine
learning models, really to understand what genes those variants are regulating, right? The ones
that are in the control area, right? It's not always like the closest gene, it can they can have
quite long range effects, it might be different proteins, right? And so one thing that if we can
sort of understand what things are regulating, it gives us a whole lot of sort of potential
targets to go and look for. So that's the sort of thing we use machine learning on, right? So we
use it on discovering kind of, you know, things to make medicines against, so better targets,
and the genetically valid, so we know they're more like to work, and then actually maybe things
on the closer to the clinical side of the world, right? So I've got a medicine, how best to use
a who's going to respond, respond, how would we measure the response and things like that?
And that's things like computational pathology or other types of things in the clinical domain.
One of the things that I heard in your explanation, this is maybe a little bit of a tangent, was
suggesting that, you know, I think of, you know, variants as, you know, one of the genes gets,
you know, flipped from a A to a G or something like that, right? So you've got these four,
and one of them gets flipped. But I heard in your description that, you know, that's a oversimplification,
and maybe it's not just, you know, which gene is encoded. Am I hearing that correctly?
Yeah, so, you know, our cells are sort of a, you know, they're a network. We have lots of
different proteins to be made, and they, you know, they communicate to other proteins and
form up various functions. And sometimes it's literally the amount of the protein, right,
is important, right? And so, you know, there are classic diseases, and you can think of these,
what we call a rare genetic disorder, where you've got a single mutation, right? And it makes
the protein functionalist, you know, a canonical example might be, I mean, you think of hemoglobin,
the sickle cell, or you can think of factor 10A deficiencies, you don't make it effective factor
10A, you know, you need to put that protein back. Some things are about like the level of the
proteins it is, it can influence the, the behavior. Some things are about, some of these mutations
means that maybe the protein doesn't get made at all. Something that means the protein gets made,
but it doesn't, it's not as stable, so it gets turned over rapidly, so you just don't have as much.
And sometimes you might have a mutation that makes the protein always on, so it's always
conforming its fate, so it's not regulated anymore, right? That's another thing, and because it's
not regulated, it's driving the behavior other pathways, and it's leads to, you know, abhorrent
function, which leads to pathology and disease, right? And so, it's not just as simple as like,
I've got a mutant, right, you know, what does the mutant, this mutant tells me I need to make a
drive against that, it's actually what's the effect of the mutant? And this is the kind of the
thing before I was talking about that kind of the variant gene problem, as we phrase it.
Okay. Another missing piece is that kind of the gene-to-function problem, right? And that's the
other, that's another key thing. So you've, you've got these data sources that come from genetics
and genomics, and you're applying AI, ultimately trying to develop new drugs, new interventions.
Can you, you talk about some of the specific use cases or problems?
Yeah, so I think that, you know, there's a lot of things, first of all, thinking about,
obviously, you know, taking your clues from genomics and things to come up with those targets,
right? And then you have to think about, well, what's the effect of what we're just talking about
like that mutation on that target, right? So there might be, is it more stable or not?
And then it might be, well, I think about, how do I, you know, what a thing is to take the,
the target and that sort of silly context and know that you've kind of made something better or not,
right? Like, how do I know that I've actually found a good target that's moving, you know,
it's going to, it's going to become a good medicine in people. And that's where actually, so we do,
all of the things we build, all the models we build actually have sort of a large sort of
experimental feedback loop, right? And it's actually, we, for example, that variant to gene model
that has sort of an experimental feedback loop where we're actually doing what we call
experiments as code. So we're asking the model what it needs and it's really sort of adaptive
sampling under uncertainty constraints. So, you know, rather than having data being generated by
some other process at GSK and I'm trying to build a model of that where I might have, you know,
900 examples a week or something I'm very good at, right? And I'd be like, I really like more
examples of things I'm not good at. We actually sort of use a lot of sort of automated, like,
like biology. So this is biology. I'm with robot robotic automation to generate data and things
that sort of feed back into these models. And it's the model that becomes the tool that helps us
solve the problem, right? So we can use the, we can use that, um, that model we've built to help
us map more of those variants of those genes. But then we still need to understand that gene
to function for. And again, there we use automated experimentation. But in this case, we're doing
things with these various cellular models. So they can be what we call induced
peripheral cells, right? These IPSC cells. So these like stem-like cells from patients that have
a disease, patients that don't have a disease. And then we actually sort of want to work out,
you know, how, what we want to basically take the disease tissue and make it look more like the
normal tissue. And when we say look like it could be measured by a bunch of different ways,
it could be measured by looking at imaging data, it could be measured looking at gene expression
pattern or protein or some kind of functional consequence. And typically these assays are
complicated. You can't do, you know, with CRISPR and things like that. Sometimes you can just do
what we call a genome-wide screen. I'll just do all the things, right? All 20,000 genes.
These ones are so complex that you can't do that. And you sort of need to do an adaptive
experimentation thing. So you can take your clues, you start with from genetics and from the literature,
for example. And you sort of see the model for that. And then the model sort of makes an experiment,
right? And we do everything as a sequential learning product kind of problem. We make,
we do an experiment. We perturb a few genes. We look at how it moves things. We feed that data back
here and we say, okay, what have we learned? Based on that, my next best experiment is to do,
is to do this other thing. So then I make another mutation. I do another sort of round of like
interventions on that cellar system and then sort of iterate and anything about it's kind of like
an optimization problem. I'm trying to find the best target to move it to that, right? And so
and you want to get there with the fewest number of steps, right? And try and find the best things.
And at the same time, why you're trying to make it look like, you know, it's
affecting the disease. So you have to, like your ground truth is at cellular models. You also do
other things like making sure that like, you know, there are certain proteins in the body that you
can't touch, right? That like have toxicity associated with it, right? So a classic one is in,
you know, cardio toxicity, right? Like it's no good making something into your rA drug if it's
going to give you cardio toxic, right? So the certain things that we know we can't hit this,
there's toxicity from what we call on target toxic. If I hit this protein, but something bad happens
and then there are other sorts of toxicology, we can also at the same time have an AI
system that's learning to learn, which targets a toxic, which targets not to touch based on
prior data and things like that, which things are, which targets are easier to make a small molecule
not. It is a multi objective optimization because I can come up with targets that are really great,
right? And we sit around, well, we know no idea how to make a selective medicine against that,
right? So an example of one of those things is a protein involved in cancer called K-RAS.
You know, it took people years to come up with a selective K-RAS inhibitor. It was a
always a great target, but it just was really what we call intractable. So it's this optimization
of finding something that moves your, your model of the disease in the right area. It's kind of
tractable. It's not toxic, right? And then we can put forward. So that's that sort of the thing
there. So again, we use machine learning in that sense to help design those experiments that
carry that out. So can we, can we maybe take a step back. Have you talked through a specific
concrete example of a project, whether you're talking about the cancer one you just mentioned and
you know, what the data sources are, what the, the valuation criteria are and then talk through
how this sequential learning idea plays out in some concrete context. Sure, let's, let's,
let's talk about the variant to gene one, because I think that's, that's something that everybody
has more of a sense of that now, especially. So what we do in that model is we have some genetic
variants from, from standard GWAS analysis, these genetic wide association studies. And these
are saying these variants in this region of DNA, important in the role of the disease,
but we don't know which gene it is. It could be gene A, it could be gene B, it could be gene C.
And so what we actually do is the, the system that looks at that, it treats the whole thing sort
of a bit as a ranking problem, right? So top level model is, is like a ranking model, right? And
there are a whole bunch of different features that feed into that. So think of it as equivalent
to web search, you're saying for this, for this disease and this variant, what is the gene, right?
And you're coming up with your ranked list of genes, right? And you'd want your top ranked
list gene to be first, you first on the page, right? It'd be like the causal gene. So that system
actually has a bunch of different models, right? They're feed into that. So one of these models
are things that are like sort of these stacked encoder models that feature as of raw DNA. So look
at the raw DNA sequence and then they predict like which, where a transcription factor and which type
may bind, whether that sequence of DNA is what we call open or close, which is when it's packed up
and open or closed chromatin. There are other features that talk about whether those, these
particular genes are expressed or not in that, in a particular tissue type, because not all genes
are turned on in all different tissue types, right? You know, cardiac cells are different from
neuronal cells or different from skin cells. There are other features that come out of, like, knowledge
graphs are sort of like these node embeddings. And I can talk about how we have a pretty large
knowledge graph we use behind things. All these different types of models and there are some of
them are neural networks and are in right, some of them are different types of things. I will sort
of features that go into this other model. And it's again a neural network type model. But again,
it's supervised in the form that we say, okay, here's a variant and here what we think is the gold
standard gene for that variant is, right? And then we go away and you learn how to weight those
particular features, right? Just much like you would train everything else. The challenge we have,
right, is there isn't a massive amount of gold, canonical, you know, variant to gene features,
because I just told you that we only know what to do with 15% of them. So then we have to do the
experiment part and experiment is where we bring in the function genomics. So what we actually do
is take cell type, depending on what we're doing, it could be different types of cell types,
but you can say it's a primary T cell from a human donor. We do the edit and then we actually
sequence those cells. We look at the mRNA levels and we say, okay, we know what it was before
and we know what was afterwards. It was the differential gene expression. And you say, I think
this variant affects this gene. So then we go and look at that gene and look at the change in
gene expression, right? And if we get it right, you know, that gene expression falls, right?
And only that particular gene, right? And that becomes a training data. So then that kind of feeds
back in, you know, we rebuild the model and off we run again. And so what it means is that
the different teams that run those different sort of submodels, you know, they can, they also
have different data sources that they will bring in and some of them are generated from external
data, some of them internal data to build their kind of their feature factories that feed into this
thing. But that's how we train the whole model. And it's, it's a really interesting scenario because
probably I would say 45% of the time, right? A simple model, right? Which is like the variant
affects the closest gene. We'll get you there and it'd be right there. The problem is is that the
rest of the time that model doesn't work. And it's not the closest gene. It could be something
quite far away, right? And one third of the time from doing this experiment, it's really, really
far away. And so not what we expected. So we've been running this learning loop, which basically,
like, build some model in general, it's a trained data at the same time. And as a result, the overall
model that maps variants of genes gets better. And so we track that over time. So when we started
off, you know, we were mapping like, they were like 15% of the unexplained things in the UK by
when we could map them up to 24%. And they were at 40%. So we know what to do now with 40% of
our genetic variants in this database, right? And that gives us a whole bunch of new potential
targets to go and explore with some of the other systems I talked about.
Got it. So is the, again, sequential learning loop? And in some ways, you describe it and it
sounds like this, you know, automated thing that's kind of continuing to iterate over time,
performing an optimization across, you know, all of the experiments you're doing. In other
ways, it sounds like your applying machine learning, it's given you some, you know, some features,
some signals. And then it, you know, goes into a scientist brain that determines, you know,
what the next step is. How close does that loop? Yeah. So the tools I'm talking about, the models
we build, you know, they're obviously scientists involved in running the, you know, the experiments
as code, right? Sort of sort of like, you know, tending to the robots, depending on how complicated
experiment and the throughput we're doing, involved in that. It's really where the, where the
human science is getting involved is sort of the output of this sort of thing. You know,
another experiment we have is involved in discovery of an air of cancer drugs, right? And it's,
it's a concept, something called synthetic lethality. And all that means is basically all cells,
most, both biology we have redundant pathways are really important things, right? Tumor cells
grow really rapidly and they tend to sometimes like, because they divide so rapidly, they can tend
to like break and only have one functional copy of it or something. And there's a, you know,
and so what we know then is if we can identify which things are likely to break and I can make a
drug against that sort of thing, because it doesn't have a backup anymore, it's selectively kills
the tumour cell over your normal, your normal tissue, right? So a GSK drug like niraporib,
right? It's a, it's a, it's a class of drug called a pop inhibitor, but pop inhibitors are involved
in DNA repair, right? So basically if you stop the DNA repair, it then basically, you know,
acts to sort of kill the cancer cell. So what we actually do is have another system and again,
this one basically looks and tries to come up with what we call synthetic lethal pairs, right?
If you see this mutation, then you can target this other particular gene, right? And so we have a
commons of the things we know tumour cells like, you know, mutations they get and say well,
what things pair with that? So again, we do experiments, we knock that thing out, we turn it
out, we turn it down, we look at this effect on viability in a whole bunch of different tumour cell
lines, right? But then the output of that doesn't automatically become like this is the target
go away and make it anybody against it and make a small molecule against it. That's where we
interface with our experimental colleagues, because there's a whole lot of, these are all narrow
purpose ML systems we're building, right? So there's a whole lot of of data and things like that
and things that they bring into bed to think about to work out like how all that happens and
different experiments that they will then go and design to really, it's really a hypothesis
that's suggested by this machine learning algorithm, for example. So, you know, all the things we do,
you know, to surface the information to work with our colleagues and this sort of thing,
and really sometimes the production of the ML model is a lot more automated, but then the use
of the model is where the human scientists, right? These are tools for the scientists, right? We cannot
encode all the background knowledge of biology and things in the way we want. And also,
the scientific literature is really messy, right? Not everything in it is correct, right? So there's
certainly a role for domain expertise in that. And did I hear you earlier reference work that you've
done to apply machine learning to mind the scientific literature itself? That's right. So,
one of the ways you think about if you're doing sequential learning and you're running all these
big learning loops is like, you know, humans, we've been doing medicine for a long time, right? We
know a lot about biology and medicine and things like that. It would be foolish to start from a,
you know, a clean slate and have to learn all that and we'll take a lot more samples. So,
there are ways to think about that. Like, how do I have structured priors if you're a Bayesian
and things like that? So, we have another group who does, you know, one of the great advances you
see obviously is in NLP, right? So, we have all these journal articles that are either in, you know,
open source like on the bio archive or, you know, Elsevere or PubMed etc. Right? There's lots of
those sources and there's also in, there's also data sets published as well. And so, we have a,
a group that sort of builds a, an NLP type model and it's based on like birth type architectures
again, we're seeing encoders sort of appear everywhere. And what that does is that, that sort of
pulls out things, right? So, it entities, right? So, it's entity in relationship extraction. So,
we put what we call a semantic triple, which is really a thing, a type of relation and another thing,
or a subject, predicate object pairs, right? The predicates we care about are a limited set of
things. And luckily, the scientific literature isn't, has free form as the rest of the thing,
people writing things, right? So, you didn't say A has function in B, right? X does Y, X does not
do Y, right? And where X is and Y could be genes, diseases, small molecules, you name it,
when there's a set of things where we're interested. So, we mine all that out, we run all the sort of
thing over the literature, we pull out all those semantic triples, and we stick that into a really
big store. So, we have a graph that's like 500 billion nodes, right? It's huge.
I mean, you were referring to earlier. Yeah, yeah. So, we don't use that whole thing, what we do is
we pull out things. So, maybe I'm interested particularly in just genes, diseases, and you know,
protein, protein products, which is genes and things like that. And I can pull out those subgraphs.
And we might do some link prediction on that. So, actually, it's not known, but we're pretty sure
that X does do Y, or maybe there's some weak evidence for it. You know, you can apply again,
you're applying ML algorithms on top of that to build that knowledge base. And then we actually
use, usually, node embedding. There's a different way to think about, how do I represent that data
into my algorithm? I don't, it doesn't have to learn that, like, gene X does gene Y, we've really
told it that this happens, right? So, how do you represent that structure knowledge that you're
confident in? And this means that we can be kind of more sample efficient, right? You view all
ML algorithms as sort of information engines. We can be more sample efficient with the data we're
doing. So, we learn the things we don't know, right? Rather than relearn the things we do know,
it is a key. So, that's another big area. And once you've built these knowledge graphs,
you know, there's lots of other uses you can put them to, rather than just like machine learning
group, right? And finally, I just can say, oh, you know, what's new about my protein? Okay,
here's all the facts about my protein. Like, does X do what? I've got a hypothesis that X is involved
in Y, right? No one holds the scientific literature in their head anymore, right? It's, you know,
it's too complicated. So, you can go on query that. But the interesting thing is, like, you know,
said before, a big 300-year-old company, like, there are things that have been in Gone and GSK
that people have forgotten, right? Like, is that right? You know, so, like, you actually can
mind your own data and go, oh, wow, we did experiment about that. Or we thought we thought, you know,
is it interesting protein someone's got, by the way, you know, 20 years ago, we worked on this,
someone worked in a related thing and they found a molecule that affects it. It wasn't what they
were looking for at the time, but we've got it on a shelf somewhere. So that kind of thing becomes
like, you know, the brain of GSK. And because, you know, it's not just scientific papers, it's also
the data sets aside with papers, you can start putting whole data sets where people are doing these
big experiments at scale and industrial scale into these knowledge graphs as well. So, there are
lots of experiments where people are doing screens for a particular functional things like that,
and they come up with lists of genes that are known to be involved in things. You can import
that knowledge as well into the knowledge graph. So, it becomes a sort of, like, growing reference
space to use. One thing I'm curious about is you think of kind of how your team operates against
the quadrant of kind of innovating on the biology and innovating on the machine learning. I'm curious
where you find them and where you want them to be. You know, this space is moving so quickly,
oftentimes you may have to innovate on the machine learning to make it work for your application.
Is that the case here? Yeah, so I think that we have, in general, we don't work on very many
things as a group. So we're about a 120-person research group, and we're quite globally distributed,
I've been in San Francisco, I have people in team members in Boston, Philly, London,
Tel Aviv, Heidelberg, Switzerland, right? There's where we're kind of everywhere.
But a lot of things we work on, like, there isn't a solution, right? There isn't a variant
to gene off the shelf, piece of software algorithm, right? Because the data's not there,
you've got to build the whole thing. But there are cases where we can borrow things,
and there are cases where we have to do research. So we do a lot of research into causal
machine learning, because obviously we want to come up with things that are causal for the disease.
So like, if I, you know, and what I would say have a small level of clinical hysteresis,
and that all that means is basically, I small change in this particular, like, your drug
against this with thing like, I don't have to knock it down 100%, if I knock it down 10%,
has a larger effect on the cause of disease. That's the easier medicine to make. There's something
like, well, if you take this down to 99% of its level in the body, then you might see a clinical
effect. That's probably not a good candidate for medicine. So we do a lot of work on like
causal, reconstruction of causal data from network data and things like that.
And there are some areas where, you know, you might start on off with just taking things that
have worked really effectively. So computational pathologies is a great example. So pathology is
when you have, you might have seen those, those ready purple slides of a tumor and things like
that or a biopsy or things like that. And so typically they were like, you know, they were
their human toxic ears and stained slides, and they looked at biohuman pathologists,
who looked at things and those things like old stage one, two or three cancer, for example. We know
some information in the image, higher than number, worse it is, right? Now, what happened was,
you know, we got digitization happening. So we got people started to scan these slides,
a high resolution, right? And these are big images, right? These like four terabyte images,
okay? Gigapixel images. And then the other side, we've got confidence and units and things like
that sort of sitting around. So the natural thing was like, well, I'll just take a unit. I'll take
a confnet and I'll see what I can, what I can do. And I might want to segment things. I might count
the number of types of cells in the slide, rather than having a pathologist go through and do that,
right? I might want to say, well, what's the tumor stroma ratio? Like so, when you take a tissue
block, you might have the tumors growing here and there's normal tissue around it, right? How we
give that area, for example, what are the characteristics? And so it started off with people doing those
types of things, right? And those technologies work. But then almost everywhere, when you go, we start
ask more advanced questions, you innovate on my methodology, right? So some of the things we do now are,
well, can I predict the genetic status of a tumor just from the image alone, right? And you ask
pathologists, they're like, well, could you tell me whether this tumor has this particular mutation
from looking at it? They're like, it's crazy. There's no way I can do that on the human, right? And
you think, okay, well, but you can actually build models. And we've done this, they can actually predict
like the genetic status of the tumor. So there are subtle microchanges in environment and stained
density and things like that due to like the changes in the biological processes, right? A human
eye can't be, can't obviously be sensitive to train that, but you can sometimes do be retrospective
and see what features has the model learn. But suddenly you're taking the comb nets and these
types of things and res nets, sort of frequently. And then you're starting to push, push them in
into different areas and you start to do, start to tinker with them again. And then you're finding
different architecture. So we're again, we're seeing that, we see that also in, you know,
cellular imaging as well, where we use the same types of things. We're looking at cells and how
they're phenotypes have changed, what they look like pictureally, as they change when we give them
a drive or don't treat them the drive, for example. So it doesn't take much before you've taken some
off the self-technology, but typically you're, you're starting with an architecture or something
else like that that you will then adapt to a use case. So that big, you know, bearing to gene
algorithm, right? You know, well, I cast as a ranking problem. There's been lots of machine learning
research into ranking problems for a long time, right? There's lots of self-tooling and things
like that and ways to think about things. So we, those are things we start with, right? We bring in.
And, you know, but there are some things where it's, it's wholly new algorithms and architectures
and the things that, you know, we were sort of having to invent as well. And did your team publish
in those areas? Yeah. So, yeah, I mean, so we, we publish, you know, we publish all our code
and our work and it's kind of really important you do that because you're talking before about,
like, how do you find people, right? So I think what's happened is there's a lot of people that
have realized that, you know, there's now lots of this data appearing in biology, right? I mean,
since post-COVID-19, it's really interested in human health, right? And you want to find an
environment where you have the computation resources and people look to do that. But nobody wants to
join somewhere and then connect vanish into a black hole, right? And we use, the models you just
talked about, like, you know, we're only essentially able to, I would apply ResNet, some
to computation mythology and start with that because it's out there in the literature and the domain.
And that's why AI is so fast, is that free exchange of ideas and test sets and study out benchmarks
and we level those things as well? So, you know, we will, we publish that code, right? Because it's
the data that's really important, right? So, you know, we will publish, if it's a model that we've
built, we've got a code and there's a public data set and we can build a model, we would also
publish that model build public data, right? Because it's the sort of the data GSK's
generate rate and allows us to build a model at much higher quality, right? That's our kind of strategic
advantage, right? So, it's, again, it's all about the data and then that's similar to other industries,
right? You know, Facebook publishes lots of really cool graph algorithms and things like that
that don't give you their social graph, right? Data. Similar for us. But it also means that we can contribute
to the community. We're running a challenge, I think, at ICLR this year as well on gene discovery
and causal discovery from networks and things like that. I think we've got two or three papers
in Europe this year out of the group as well. So, yeah, we publish and, you know, in both in
the conference proceedings and in other scientific literature as well. It's very important for us.
Nice, nice. Do you think much about tooling and infrastructure platforms? I have read that you're,
I've read about your AI hub a bit. Yeah, the answer is yes, but yeah, absolutely. So, you know,
there's a few, so, the infrastructure, I mean, there's one thing I learned from doing start.
It's like, you know, the, you start building infrastructure now on the next best way to start
implementing infrastructures tomorrow because it allows you to scale and if you suddenly have
infrastructure problems, it's really difficult to solve once you're in that phase. In the AI team,
we have a whole group that's really an AML platform organization and they build all the kind of
tooling and infrastructure for us to kind of, you know, deal with data containers and running
things and scanning algorithms and other kind of those aspects. And it's about not only just,
you know, GPUs, computing things, you know, we do a lot of pie talk extensively,
is our, is our preferred platform of choice. But some of it even also comes down to the things
we're looking at, we end up needing kind of like novel compute, right? So, we've had a strategic
partnership with a company called Cerribris, which you may have, some of you that you may be familiar
with, right? Cerribris have one of these companies they built like, you know, a really
really amazing piece of hardware. And so we use Cerribris for a particular type of problem where
we're building these encoder models on, on DNA. Now, what's interesting is these encoder models
is we want to have a really, really large window size, right? And so you get to this, it's really
challenging to build model parallel and data parallel kind of algorithms at the sort of scale. And
the data sets we're passing over are really, really large as well, right? It's for these
genomic data sets. So that was a really interesting problem that the Cerribris system is like,
it's got massive throughput. It could be a really big model because of the scale of the chip
products and the latency between that and the memory was really large. So, you know, we started
working with them. We've got us, we have a CS1 system. We'll have our CS2 soon. And, you know,
that becomes a strategic thing that we can start to build new algorithms and play new things on
and actually build models for. And then, you know, so the, so the, the compute is really important.
For us, it's a, it's key to be unconstrained by, by computer, right? To be like, you know, to think
of like, you know, what's the best way to solve the problem. You know, we usually constrain by data,
like the data I love to have, right? And so this is also why, you know, we also work with Nvidia, where
we're pushing the bounds of CUDA. So we have an a strategic ground arrangement with Nvidia, where
we have people on site in London, where, you know, we're making changes to low-level, you know,
LibDNN and things like that or working with them on those types of things. So, you know, how do we,
how do we, how do we make it easier for us to do it focus on like, you know, only one problem,
which is the science problem rather than two problems, like the science problem and engineering
problem, right? Because the, the challenge we face are hard enough, right? So, so that's, that's a key,
is a key component for us. And, you know, the other thing is like, we want to think about how
many iteration cycles a machine learning person can do per day, right? I don't want to be someone
sitting there like, I've got an idea, I've kicked it off. Well, I guess that'll be done in two
days time. Yeah, I'll sit here and read a thing. I want to be able to like look at something,
have an idea or have a bunch of ideas, kick them off and then actually get the results back
that afternoon and think about it and then, you know, run on to other sets. That's also really key
for us is to be, you know, so as we grow, we've needed to add, you know, every every every new AML
high requires, you know, a bunch of A 100s or whatever we need, like added to the stack, right? It's a cost.
Yeah, you mentioned earlier, you reference a feature factory. So you're developing these features
and you reference a feature factory, is that a concept or an idea or is that a physical thing,
like a feature store? So you can imagine for, you know, if I've got, you know, that section of DNA
and I've got my very antenna, right? Those, there are different models that can like tell you different
things about it. So analogy with the web page, I would have the title of the web page, the links to
it, the text and those links, you know, the content of the web page, the word count, author, the date,
those are all features about the website, right? And, you know, that you have tools and code that
could pull those things out and represent that and a featureisation to some kind of AML model.
Similar in this case, there are, we look at that whole stretch of DNA and the disease that's
in the similar context and there are algorithms. In this case, there are models themselves
that work on how to featureize that, to represent that to that whole ranking algorithm.
So we term those sort of things as a feature factory, right? So the ones that look at the raw DNA
sequence and will say, well, this is open and closed chromatin, the other one will say, well,
this gene isn't on in this cell type, right? And it might imagine the model when it's trying
to, when it had ranked the importance of those things to give you, you know, it's candidates
of genes might say, well, well, this gene isn't even on in this cell type that's involved in this
disease. So this is probably a low unlikely thing, right? This one's in closed chromatin, that's
unlikely. This one's in open chromatin, it's involved in disease and things like that. So it's
becomes a good candidate. So it learns how to rank all these different things and how to combine
them. And so it's, it's not a, it's not a sort of a physical thing, but it is a sort of like,
it's a featureisation type aspect, right? So it's basically a featureisation by other
sort of submodels themselves. Yeah, when you think about all of the, the various algorithms
and tools and things that kind of factor into and enable what you're doing and look forward,
what are the areas that, either you need the most innovation happening or you're excited
because you see the innovation happening, you know, whether, you know, we're talking about algorithms
or tooling infrastructure, that kind of thing. Yeah, I think one of the things that I'm deeply
interested in is robustness and reliability constraints, right? And, and this plays into a,
you know, a debate with, you know, in regulation and things like that, is that as we start to build
sort of probabilistic reasoning systems and imagine, let's take the pathology example where I have images
and things like that. And we've, maybe, you know, we've designed it well, so we've made sure we've
got like, you know, bunch of people different backgrounds, you know, we've got a lot of,
lots of training day, we'd have underrepresented groups, we've done the best case to do that,
that's very important. And we built a model and the model has good performance characteristics,
you know, maybe 80% of the time it's correct and it predicts someone's got to, as a, you know,
it gets a genetic status right, so we know then who to sequence or not, for example, I'm just
using a hypothetical scenario. How do we know how that model behaves when, you know, what's the
episode image for like a, you know, for a pathology thing like it's like, maybe the area is out of
focus, right, or it's got a pen mark, or it isn't enough tumistrum. How do we know that this model
fails gracefully? How can we define as its balance of operation and things like that? And, you know,
for some other methods and things, it's easier to define and construct that, you know, for a new
network, it's harder to know how, you know, some of those changes result in, you know, the decision
boundary actually happening. And so knowing that you've trained a model that's, um, for some
of these scenarios, you would train off some, you know, some performance for robustness characteristics,
right? But it is hard to know how those robustness and reliability characteristics happen. So,
we're doing a lot of, we do a lot of research in that area and we have various groups we interact
with and PhD students, we sponsor. But that's a really active area that's, you know, it's not just,
you know, our organization, I have to particularly, it's, it's across the industry in various things,
where people want to know, um, know those, those sorts of aspects and that's where you get into
monitoring other sorts of things. But for us, it's, it's all about, um, knowing how I can measure,
that I found a good robust solution and it's not brittle, right? The small changes that
import don't lead to large changes in the activation. Um, that's, that's one key area. Uh, I think for
us, the, you know, looking at, um, simpler transformer architectures that could lead to the same
kind of performance is another really key thing is we can train them faster and things like that.
So I understand a little bit of that sort of, you know, just the, you know, um, model parameters
based performance trade-offs and sorts of sorts of things, you know, um, where you think, well,
maybe there is a simpler architecture that can do just as well. Uh, that's, that's a, a common
error research. And, you know, I've been a lot of it actually comes down to really, uh, biology is
all about, um, low, low and high dimensionality, right? And, uh, sort of biases and time series,
right? So a lot of our data, if you think about it, that idea where I've edited my variant in,
I'm looking to see which genes change, right? Well, I can measure that six hours after I've made
my edit 12 hours or 24 hours and the whole thing is changing over time, right? And so actually
begins to start to integrate those, those, those temple dimensions. So this is where we have a lot
of time series data and we can generate that in biology. And this is where I think that's another
area that, of key research and probably the, the final one is really sort of multimodal, um,
and end to end, and, and learning multimodal. So they're the classic examples. I have some
cells in a dish. I'm taking images of them and I might pull them out and do RNA seek. And I
wasn't at my time series, that's why they're in for good measure. And I want to start to look at
that and I want to build a model that can classify when a perturbation has made a cell look like the
wild type cell, the healthy tissue, and made its gene expression look like that, right? And I'd
like to go to end to end learn that right now we typically learn the gene expression model,
the cellular imaging component together. We'd maybe take the top two layers of those sorts of
things and you throw that into another model learns to integrate them. We don't propagate the
error back down through all of that, just because the complexity of the thing. But that's an area where
I think that, you know, could be really useful. And, you know, typically, you know, you mean, the
convolution size, right? That's that you could use attention on that. You can have these
dilated and flexible convolutions where you could adapt that because it might not, you know,
picking the one that looked good for that, it could be a very specific one that could work better
for that problem, right? When you want to, you know, so that that's a that's a massive error
research for us. And because we have that in medicine, right? We have, um, you know, your biopsy,
your pathology biopsy might be done once, right? When you're diagnosed. But we can do clinical
imaging every seven or eight weeks. It's like a CT scan or MRI, for example. But I might do your,
your blood work, right? I can sample it. You take a file of blood and I might look for circulating
tumor DNA, right? It's the things like grail or, or free gnome or those sorts of things, um,
garden health, right? Those sorts of assays we're looking at literally DNA in the blood that's
come from the tumor cells, right? And look sequencing that. And, and that could be done. They could,
uh, they were done at different timescales, but they're all multimodal things about that
particular patient and you're trying to integrate all those together to say, what's your particular
outcome going to be? Are you responding to this therapy? Where are you going to go? When, what,
what therapy should we give you next, for example? Mm hmm. And to some degree, that brings us back
to compute because the scale required to integrate all this together is significant. Yeah, it's
computing data, right? Because, um, you can, we can measure so many things, right? But we're
measuring, pre-offent, often what you see in biology is we can measure more data on things that
are wrestling, wrestling and people. So I can learn lots of data on a, on cells in a dish, right?
But cells in a dish aren't going to give me the effect of like, you know, a whole person, right?
I can't ever get information about whole organ failure from single cell culture of hepatocytes.
But I mean, we're starting to see more complex innovations in biology. So things like organoids
and things like that where they start to have more of the complexity and you see, and where we
end up finding machine learning is actually building a bridging model from the, the thing that we can
perturb a measure at scale and then how well does that correlate to, you know, to humans where we
can only really like, we can't perturb humans. We can treat humans if we, if we've got a really
good thing, we can measure things about us, right? So, um, that's sort of another area.
Um, that's one of the things that we were actually doing with this sort of this King's College
collaboration that was recently in the press where what we'll be doing is, and is, as we're taking,
um, tumor samples from, from patients, right? And we can culture their tumor, and it's not,
and it's not organoid. So it's the tumor, but it's passed their immune system and it's actually,
quickly, it's that immune system combines from that particular patient. And then we can start to
see how that responds, right? With various drugs, with influence, and we can measure very things
about that and look at that over time. And the idea there is to sort of build a model of,
you know, how best to treat that patient, one of the characteristics. So we even want to,
what is the risk? Lung cancer, for example, you might resect it. You're hoping that you've
got it all, and there's no secondary metastasis, but there are some people that will see a higher
rate of secondary metastasis than others, right? So could you, how can you identify that, for example?
So it's just a really interesting interplay between the development of experimental
biological techniques, the ability to generate data at scale, right? And the ability to build
models to kind of connect them back to humans. You mentioned robustness as being important,
and before that, you talked a little bit about explainability. I'm curious how you think about that,
and, and how you approach machine learning problems with those concerns in mind. Do you,
you know, drive for performance and then back off to the explainability requirements,
is it the other way around? Is there some kind of hybrid? It's, it's a really interesting
debate, because a lot of the times, you know, I think people kind of use interpretability
or these types of things as a proxy for I don't trust or understand sufficiently the engineering
validation, right? And so, you know, I mean, I had the question, I was like, okay, I can give you a
very, what was the simple model we like? I'm like, oh, I like a logistic regression with like six
parameters. I'm like, okay, if I give you a logistic regression with six parameters, right?
And maybe I only allow like, you know, positive non-zero coefficients, right? It's a huge number
of functional forms can be known, but most people can't look at it in their head and really
understand how that works, it makes the decision, right? Or have been said to the threshold. So,
and we use technology and systems everywhere day to day without knowing how they work, right?
Like everyone in the lab, where it comes down to is actually where you want reliability constraints
and how it performs, right? And that's that's the sort of the trade-off, but there's also there's
a trade-off between when do we really need to have secondary checks and things like that? We're
making really big decisions, right? You know, you know, avionics for flying my plane or maybe doing
I'm going about to diagnose someone with something else, you know, I really need to, how do I,
how do I have a functional sources data? How to make sure it's robust? And there's sort of things
in the discovery phrase where do we need to have some in explains to a human scientist? These are
the key features and why we think this, this cell type is more like this cell type and others and
this, this target's pushing it in the right direction versus and there's a trade-off between like,
well, we're asking the machine to do tasks that a human can't. We're putting in so much data,
and we're going to take those results and then check them in other ways, right? There, I would rather
harness the full power of machine learning and not hamstring the system by saying, well, okay,
here's a saliency map and having to us, I agree with that or not, or the functional form. So,
depending what we're doing, it's a trade-off, but what we always care about is making sure we build
a robust reliable model. So, like, understanding how you're assessing it, right? How you measure
the performance? How do you understand them? Like, you know, you haven't somehow an information
leakage and those sorts of things come through. So, it's attention, but where you do find things is where
you have to make sure that you, you know, if you're doing, you're placing what someone would be
doing manually, right? So, there's a whole thing of like, well, is this thing better than me? How do I
know it's working? How is that quality aspect? And usually, once you want to say is like, look,
I'm here to automate the boring, right? So, you can actually do and go to high-level science,
then spend your time, you know, looking and analyzing this, right? And also giving them sort of an
audit trail, they can go back and look at the data that went into the system and maybe look at it
from a self or diagnosis tools, right, on the model's performance as well, you know? So, things like
is the input vector within a vector space that's well bounded by the training set and the test set?
What does the error manifold look over that thing? Is it uniform or is there a spiky regions where
it, you know, because we, a lot of our performance measures are global measures or model performance,
but you could easily take the input vectors, you can title them on a 2D plane, you can work out the
error function over those things, right? And you can say, oh, wow, overall, it's a pretty good model,
but like, you know, small molecules that look like this, this thing's lousy at, right? So, having
uncertainty bounds, those sorts of things, they go a long way to actually putting these things in
production. And, you know, another thing is that it's really important not just to have any model
over spit out a number, anything we put in production has to give you both a number and a confidence
bound and also has to, it also has to refuse to return a value. It's saying, I don't have that.
This is so far out what I've seen, boss, I have no idea. We can collect those, we can log those,
maybe we've got enough of them, we can build another model and over time almost every model becomes
a cascade function. It's like, well, this is a global model for this one, this model for this,
maybe eventually we can unify them again, but that's really important. Those are all the sort of
the functional things because it's honestly, it's quick to make a model and once you give people
a tool and it's very quick for them to use it, but it's the opportunity cost of the downstream
decisions that they can make with it, right? They decide to do experiment A and not experiment B,
for example. So, we think a lot about those sorts of things. And, you know, when you start off,
it's always, I want to know how it works, what's the model thinking and things like that.
But, you know, a lot of those theories of mind are not really truly how the model's thinking,
even if we do distillation, right, and things like that. That's not really it is, it's sort of a
hack on those things. Getting models to predict or describe their confidence, that's an active
research area itself. Does that requirement that models, your model spit that out? Does that put
a limit on the types of architectures you use, or is it itself kind of an area where you have to
question reliability and trust of that confidence element? You know, we do a lot of work on that
internally. I think, I guess, Francesco Ferrin has got some pretty good papers coming out on
that. Well, he's published that we've done on that sort of aspect. We try and have some general
genetic methods we can bolt onto any architecture. So, you can kind of separate the problem a little
bit. Some architectures also give you, you can give you at the same time estimates of that,
depending on what you're doing. But we also, depending where we are in development and where we're
using it, you can, you might have great or less requirements for that kind of aspect of things,
right? So, there's a bit of a, certainly once you're making big important decisions and you're
putting things in production and it's going into other processes, you definitely need to have
those aspects. But I wouldn't say every model of good at GSK by every group will always have
those characteristics. For us, where we're making these big things that have, you know, big downstream
consequences and decisions, you need to have that sort of place. And I think that, you know,
I think just in the community globally, people are realizing that, right? You know, and that's also
about sort of monitoring things in production, right? You know, of checking things. And, you know,
there's plenty of examples of models going rogue and, you know, that kind of thing. And so,
for us, it's a key thing. But we, depending on the stage, is how stringent we are on those
requirements for it. Maybe kind of one more direction to briefly explore, kind of zooming out.
The, can you speak a little bit about the, you know, kind of building an organization like yours
in the context of a large, large company, large pharmaceutical company, you know, transformation
implications, organizational receptivity to probabilistic models. That kind of thing is it,
you know, it's a, it's a research organization or a scientific organization at its core. So,
do you, you know, not have the resistance that, you know, places have or...
Yeah, I mean, I wouldn't have come here if it wasn't for having, like, you know,
how is the head of R&D? Because you've got someone who really gets it. And you know, you
got to go, all right, by, by himself, like you said, engineer as well. It's really important to
have an organization that, because it's core to the strategy, like people like, oh, we're going
to have AIML. We're going to do it. And sort of when I came in, I'm like, look, we get, like,
the normal you hire and do people who doesn't work anymore, right? I want any of you people,
and I'm going to give an offer in a few days, like, like, three days. We're going to do it.
We're going to use hacker rack. We have all these different things we bring people in, and we're
not going to do the usual process, so you, they get offered in three months time or three,
you know, a month, because they've got other places to be, right? We want to show them that we're
not this giant, usified organization that we can move fast and do things, right? And, you know,
even to the way we're working, you know, we're going to use Slack. We're going to be distributed
everywhere, we're going to go with talent ears, you know, we use a laptop and a thing we can work
that way. You know, post COVID, I think the rest of the organizations caught up to that. So we came
in and like, you know, did things in a very, very different way, right? And like from the way we
interviewed HR hiring, but also, you know, we always max like this HPC requirements are we're going
to do things and like, and so we built like a whole new process to do things, right? That are
the way we work, right? We work in two big sprints. We have these different types of things.
What's interesting is actually seeing the wider organization kind of been given permission to
think and innovate and like some of them picking up some of those tech techniques and things like
that and the methods of managing science, right? Because I first looked at my biological colleagues,
they're like, you don't stand, but it doesn't work that way. You can't plan things out for like this
could take longer or shorter. I'm like, well, you know, computer science doesn't work that way either,
right? Like we actually like, you know, it's, I love to think that like we know the thing and we
write down all the steps and we just do it. That's not how it works, right? So it's always a garden
of walking paths. So doing that, stopping every two weeks, going what work, what did work? Okay,
now we'll do this. Let's have a really good way of planning and working across really complex
teams as we do. So, you know, we broaden a lot of that culture. You know, there are always people
that are true believers, right? They're like, oh my gosh, ML and AI could do everything. You're like,
well, slow down, you know, let's just talk about what we're doing here. And then there are people
that are super skeptical, right? That are like, well, you know, how is this anything different from
before? You know, this stuff or to the, no, you'll never place a human science, this human
ingenuity. You know, I've got a better way of picking tech. I can synthesize this on my head.
And maybe they really do have an alpha value. Maybe they really do have that. But sometimes a lot of
I'm like, yeah, I think you've got a bit of biorecal bias there as well. So it's attention to trade off.
And like all these things, we'll work out where these, some of these tools, based on the
technology maturity stack, where they're best used and where we're too early, right? Or where we don't
have enough data or the right data, that sort of thing. But it is, it's been fun and it's been fun
just sort of, you know, as the organizations grow and more people have come in. And I think what
people have realized is that, you know, if you're interested in doing machine learning in bio and
those sorts of things that come is like GSK actually have lots of compute. So if people that you,
like, join me with the MIT is like, now I've got more compute and more data than I ever had, right?
And I don't have to write grants and spend all my time doing these sorts of things. And if you
get it right, you've got a whole machine that will translate to an impact, right? To patients,
to really do those sorts of things. And that's, that's really important to a lot of people as well,
is that is that connection to the part of the whole thing, rather than, okay, I'm an academic,
I built some good idea. Now I have to make a start up into the whole thing, and this can take
like so long for my work to get out there and actually influence the world. So those are all
good things. You know, there are certain pluses and minuses to doing things in large corporations
to smaller ones, right? A small start up, we can raise capital, you know, we're all in the same room
or, you know, place that we know what we're doing, we're all in charge of scale, we don't have to
have all this overhead big things. Large corporation takes some time to turn the ship, but once you
can focus that whole thing on something, man, you can really drive, drive on it. So different skills,
you know, certainly the largest corporation sort of thing I've sort of worked in. And but
the organization has to want to do it. I think is, is the less than I've learned. Like, if they're not
really into it, or the senior leadership aren't really into it, or a large fraction, it's not
a core strategy. It's a very difficult thing, right? And different companies are in different stages,
right? Some of them are externalizing it, but we decided to build a really large in-house team.
What are the, what are the couple three top three things that keep you up at night? Like, what are
you most worried about in your role? The first thing is being able to generate the data at the
right kind of cadence, right? So one of the great, you know, if you're in different domains, right,
you can, depending what you're doing, you can get, you know, high frequency lots of new data generated
quickly. For what we have to do, you know, for example, if you think of like reinforcement learning,
right? You know, I've got a simulator of the game or things like that. I get many, many samples
that can run lots of experiments, right? I'm in BioLand, right? Like, I don't get, you know, 12
million data points, I get like 300 data points every four to six weeks, and they cost. And by
the way, it costs us a lot of money to generate the data points. So then, you know, you start to ask
this question of like, you know, what's my information gain? What's my model performance gain per data
point for time? Where am I? Am I linear? Am I a plateauing? Like, you know, how many cycles do I need
to run? I can run 12 cycles a year. I can get so many data points. Is that enough? Right? So it's
all about, for me, a lot of it is about a, can I'm, you know, am I ever going to be in a generator
enough data to solve this problem? We're going to have the right data. And another thing is the
cost of the experiment. But, you know, people will ask like, well, how much data do you need to build
this model? I'm like, I don't really know yet. You know, I need more, but then we'll start to see
a trend. But, you know, or am I collecting the right data? So it's really about those learning
cycles and those sorts of aspects. I'm, I'm really, the other thing that sort of keeps me up
enough is thinking about the best ways and the ways that we have other data sources and things
that we generate data, there's lots of historical data and GSK that we pull it together and use it
in the right fashion, right? And, you know, that we remove, it's really important to, you know, there's
a patient data about individual people, right? But when we run a trial and we do things with people,
they're contributing to medical research. And if you talk to a lot of people with the trials,
they're like, you say, well, we ran your trial and we put it into a box and we, the medicine
was successful or not, right? And like no one else can touch that data, right? I'm like, that's
criminal, right? They contributed medicine. There's other things we can learn about it. So what I
concern about is when we generate data and we're doing things as an organization, how do we make
stackable data sources to build this like a long-weigh children compass, right? An individual medicine
for a particular, for say, roomside arthritis, it may fail, right? Which is terrible for patients
and terrible for us. I drug didn't work. We didn't work as well as we hoped. But the question is
what do we learn from it? And how do we build data sets that have the same common longitudinal
characteristics, you know, maybe a common course, I can join them up together and I can build this
longitudinal corpus of data. And so a lot of time when people are doing an experimental lab,
they do an experiment and then, you know, they'll analyze that for that particular use case
and they lose the metadata or it's lost and things like that. So I have to tell people,
like, you know, you know, build data for future use so you can use it again and also like,
you know, collect those other data points at additional marginal cost, right? Like they're
really useful. So those are the things that really keep me up. You know, you know, the final
thing is really about like, from building models in pathology, we're starting to do things that are
really like, you know, doing patient prognosis, doing prediction, doing sorts of things, saying
these persons like their benefit from this medicine or not, as we started going to learn that,
it's like, we have to be right. And the other thing is we also, we make medicines for everybody.
The challenge we have is if we're using lots of prior information or I'm relying on a system
where, you know, I can get digitized medical records or pathology can be uploaded, we can raise
a risk of building really great AAR advances, but only work for people that are in, you know,
countries that have the data infrastructure and things like that. So we think a lot about, you know,
what are the data, what's the data culture? We have culture in the vaccines and medicine.
What's the equivalent of that? You know, we don't want to build these great advances and say,
oh, I'm sorry, you know, that's, you know, it's going to take five to 10 years for another country
to be able to have access to it, right? Those are my top three things. Well, Kim, thanks so much
for joining us and taking the time to share a bit about what you're working on and how you think
about the problems in your space. Thanks, Sam. It's been a lot of fun. I'm a big fan of the
cobs. Cheers. Thank you so much. Thanks so much.
