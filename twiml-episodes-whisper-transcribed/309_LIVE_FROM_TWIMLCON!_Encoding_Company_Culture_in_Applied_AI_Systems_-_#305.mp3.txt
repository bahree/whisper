The conversation you're about to hear was recorded live at Twomokon AI
platforms. For more coverage of Twomokon, visit Twomokon.com-news or follow us
on Twitter at Twomokon AI. But first, a word from our sponsor.
Thanks to our friends at SIGOPT for being a founding sponsor of Twomokon AI
platforms. SIGOPT then bites you to watch CEO Scott Clark's upcoming webinar
outlining the critical capabilities customers prioritize when building
machine learning platforms. He'll draw on experiences working with algorithmic
trading firms that represent over 300 billion dollars in assets under
management and enterprises with over 500 billion in market capitalization to
summarize these trade-offs. Head over to twomolai.com-slash-sIGOPT to register.
Our next guest is Deepak Agrawal. Deepak is vice-president of engineering at
LinkedIn and he's particularly passionate about the connection between the
organization's investment in machine learning and AI and the value that it
creates. And we're going to explore that in our chat.
Let's get started by, I think everybody knows LinkedIn. It's not something
that we need to spend a lot of time explaining. How many of you don't have an
account on LinkedIn? Okay, everybody, market saturation.
But let's maybe get started by talking about some other ways that LinkedIn is
using machine learning. Yeah, so we often say that LinkedIn machine learning is
like oxygen, right? So everything we do has machine learning built
inside it. Like if you go to LinkedIn, the first thing you want to do when you
join LinkedIn is get connected to people who can help you. Now, how do we do
that? We recommend you people that you can connect to. That's all powered
through machine learning. Once you get connected to people, then you start
consuming content that they produce on your news feed. And you know, there is
information overload on the feed. You can see so many content. So what kind of
content do you want to see? And when do you want to see? Like, for instance, if
you're looking for a job, you want to see job recommendations. But if you're
not looking for a job, if you're very happy and you want to learn more deep
learning and if you are connected to Andrew and he publishes something, then you
want to see that on your feed. So you need algorithms to scale this process.
Again, that's all powered through machine learning. If you are a market
here and you want to target the right audience, the entire advertising
ecosystem, we all know works through machine learning. If you're recommending
jobs that you can, that, you know, even if you're not looking for a job, we
still recommend you jobs because there is always a better opportunity out
there for all of you. That's all through machine learning. If you're a recruiter
and trying to source candidate, it's all through machine learning. If you're a
salesperson trying to close a deal, who are the decision makers? How do you
reach to a decision maker? So everything we do on LinkedIn, product, whatever
you see on the app, it's all powered through machine learning. And you know,
finally, you know, this is something that goes behind the scene. We have to keep
the site safe, right? There are, you know, there are a lot of bad actors out
there producing content that should not even reach you. You know, there are
people who create fake profiles. I mean, I've seen a lot of fake profiles of
famous people. And that's not a good thing, right? So just to keep the ecosystem
clean, that's again, machine learning plays a very important role in that as well.
So everything we do at LinkedIn is powered through machine learning. In fact,
when we create a new product idea, in addition to product managers,
engineering managers, designers, we also have a machine learning force
and sitting there, right? When we are designing the product, because we believe
that's the right way to do things. I mean, you know, the UI that you create,
if that can actually build some important feedback loop, that can play a big
difference. I mean, Andrew was talking about collecting label data. Well,
how do you actually ensure that you collect the right label data? We have to
actually start working on it at the design phase. It's too late if you don't
pay attention to it at that stage. Then you have to build very complicated
model that essentially do guessing, right? So you don't have to guess if you can
actually get the right data. And so that's why it's very important to start that
process from the very beginning of machine learning process of product
process. One of the things that has always fascinated me in my conversations with
folks at LinkedIn is we think of LinkedIn relative to a more traditional
enterprise as kind of a digital native company born on the web.
You know, the product is web. But in a lot of ways, the company has evolved
similarly. You know, it's initial investments in machine learning and
what the way it's supporting machine learning today are very different. Can
you talk a little bit about the journey at LinkedIn and how
ML and AI has evolved over the years? Yeah, so LinkedIn was always a data
first company, right? Like if you all remember, the word data science was
coined by DJ Patil at LinkedIn. So we were always very savvy about data. We
knew our businesses all about the data, the unique data we possess, right? So
we were always doing data science. We were always doing data product
innovation. We also started doing machine learning very early on. Like in 2007,
the first machine learning product, real product was the people recommendations,
right? So in those days, we would compute, we will have simple machine learning
models, of course. You know, so we'll have a simple model, the features of
these models. There were a handful of features, but they're very carefully
tweaked based on intuition. Once we have that model, then productionizing these
models at that scale was still very difficult. So we will actually build
Hadoop systems that will do the ranking and scoring offline, right? So because
online was not very well developed. And then we'll run these processes
every day, right? So search was another system that we actually developed
very early on that used machine learning. Fast forward 2012, we got more
sophisticated, right? So the first sophistication we added in terms of
machine learning was in our advertising system, right? So the advertising
system, we most of our other recommended systems are based on simple
collaborative filtering idea at those times. You know, people who bought this,
also bought this. But then advertising was the first place where we added a lot
of sophistication. We added, we build near real-time systems, we build online
systems that can score things at runtime, more complex models,
and you know, encouraged by the success we got there, we then
attacked the feed problem, the news feed problem. For those of you who have been
using LinkedIn for a long time, I'm sure most of you will tell me today the
news feed is much better than what it was five years ago. That's all due to
machine learning, right? I mean, so a lot of work happened to kind of add
sophistication to the news feed algorithm. And once we got success in these
two big applications, then we started thinking of how do we generalize it across
the board, right? Why, why just advertising? Why is this news feed? Why can't we build
a platform that can actually generalize it to everywhere? And that's what we
have been doing for the last few years. And so we have a program at LinkedIn
called ProML, Productive Machine Learning. And again, I think a lot of
companies have a platform. But I mean, one unique thing about our platform is,
you know, we are building a platform with a very strong opinion, right? So
you can build a machine learning platform that can cater to a lot of
tail users, right? So if you're a cloud company, you're going to build a
machine learning platform that can cater to the needs of a diverse set of
customers. That's not our goal, right? We know that our ROI is going to come from
a few big applications. And the platform we have built is really
suitable more for that, right? So large scale recommender systems,
large scale search systems, large scale classification problems. These are the
problems that we face. And our platform is really geared towards that, right? So
we also know that we have reached a point where without adding more sophistication
to our systems, you're not going to get the ROI that we used to get.
So I give you an example, like, you know, two years ago,
for our job recommendations system, we revamped the model. We kind of moved
away from a simple linear model to something more complex, involving deep learning.
And, you know, involving something that is a homegrown technology called
generalized mixed model. So I mean, I'm not going to technical detail, but these
are very high dimensional model. We had a model with GMM.
Yeah, exactly. This is the old technique in statistics, known since the 70s.
And in statistics, it was applied to application that 500 patients.
Now, you know, those 500 patients become half a billion patients. And then suddenly
the explosion in the number of parameters in complexity increases a lot, right?
So we applied that and, you know, we found a 30% improvement in result. That was stunning.
And so we were all very happy. But for the next six months, nothing happened.
And then like, it was very surprising. Like, okay, well,
did the engineers all go to Hawaii or what's happening? Like, why is nothing moving?
And what we realized is when we introduced that complexity,
the tooling did not keep pace with that, right? So it became very hard for the subsequent
engineers to kind of iterate on this model, because we didn't build the appropriate tooling that
will enable them to kind of iterate. So that was the realization.
And I know this is not going to work as we actually start introducing more sophistication,
the industrial process will only work if the engineers are still productive.
And in order to improve productivity, when you add more complexity, especially for such large
skill distributed systems, if you really want them to run efficiently, if you want them to run
in a reliable fashion, you have to make sure that the tooling and infrastructure can keep pace
with the innovation that we are doing. And so that was really the impetus of this project that
we kind of run called ProML. And we actually run it very rigorously. We are not just building
platform components. We actually measured the success of that. So every week, we measured the
number of successful experiments we have run. So there are a lot of experiments, our engineers run,
but we only track the number of successful experiments, right? Because otherwise you can start
cheating, right? Like someone can just go parameters sweep on the grid and say, okay, I did a
parameter sweep of 100 different values. And so I ran 100 experiments. I don't care. I mean,
you know, you can run 100 experiments or two experiments. Did how many of them succeeded, right? So
that's our metric. And we have seen like more than 30% improvement in the number of successful
experiment that we run on the site after introducing this program. We've still not done. There is still
a long way to go, but you know, this has been really useful for us. It has kind of also bought
the teams together. So earlier, you know, if you don't have a standardized way of doing things,
no matter how hard you try, the culture in the feed team would be different from the culture in
the jobs team, right? And that's not good, right? I mean, we don't want to create different
cultures in the same company. In fact, we want, given that we are a centralized organization,
we want people to flow from one area to the other, right? So you did your tour of duty on the
feed and you should just go to the jobs team and learn about the jobs product. And you should be
productive in a day, right? And that is only possible if you standardize things. And so this project
has also helped us to standardize things and not kind of deviate too much. That's a really
interesting observation. I talked to a lot of people who put the idea of culture against the idea
of technology in a sense. You know, technology is not most important in its culture, but you're
talking about the relationship between the two and technology supports creating the culture
in an important way. Yes, so in engineering, the process, which is called CICD, right? Continuous
integration, continuous development. If you look at different companies, they use different tools.
And to me, that process, the tools you use is actually a reflection of your culture, right? So the way
you do machine learning really reflects the culture you're trying to build. Like for instance,
there are many companies who actually build standardized tooling just for model creation.
Once you've created the model, the deployment can happen in very different ways.
Yeah, I'm not saying that's good or bad, but that kind of tells you the culture of the company,
right? We don't do that. We just want to standardize the entire end-to-end machine learning
culture. And then, you know, if you work at LinkedIn, that's really the culture that we have,
right? Because we believe that's the best way for us to increase the ROI of machine learning.
To increase ROI, there are several components. You have to be cost effective. How do you become cost
effective? You have to be efficient. You have to be productive. And you also have to innovate,
right? Because if you don't innovate, then no matter how productive you are, you're going to run
experiment that will only give you marginal returns after a while. So the innovation has to continue,
because innovation is not easy. We all know that. If you don't innovate for the next six
months, you certainly would not stumble upon a breakthrough that's going to completely change
the game, right? So you have to continuously innovate. And you cannot give your engineers time
to innovate if they are not productive. If they take too much time to do their job, when are they
going to get the time to innovate, right? So innovation, productivity, efficiency, and those are three.
So innovation, productivity, efficiency, like these things, they are all entangled. They all have
to be attacked together. You cannot just say, oh, I'm not going to worry about productivity. I'm
just going to worry about innovation. It doesn't work that way, right? Because, you know, the time that
an engineer has and, you know, the kind of investment you can make in the program is fixed, right?
You have a fixed budget problem and you have to really solve these three components in the best
possible way, depending on your business needs. Yeah. So we talked about platform abstractly thus far.
Can you give us an overview of ProML and the kind of major components, what's in place now,
the direction you're heading with it, et cetera? Yeah. So our aspirational goal is the entire
end-to-end machine learning process should become completely automated, right? Like once a scientist
understands the business problem, they understand the kind of models they need to build,
to solve that machine learning problem. From there on, everything else should become automated.
And obviously that's very easy to say, we all know it's not very difficult to do. I don't think
anyone has solved that, but that's the aspiration at least. So just like any other end-to-end machine
learning platform, we have a model creation process, right? So how do you create models very easily?
How do you compose different kinds of ideas together? Like if someone wants to take XGBoost and
GLM makes and neural network and try it out, like it should be very easy to try that. So model
creation, data preparation, you know, that's one major component. Once you've built that,
then how do you deploy these models in production? Once you've deployed the model in production,
then how do you make sure that you're not babysitting the model, right? We don't want our scientists
to become babysitters, right? Because that's not a good way to run a machine learning program.
So the maintenance of these models, once they're in production, should be almost automated.
And what do I mean by that? If the model is running in production, you still have to retrain
the model because the world around you is changing every day. It's not a stationary process.
So every day, the data should flow in the directory automatically. If for whatever reason,
the data does not flow, there should be an alert and the person who's running the model should
kind of know about it and then they should have a fall-off graceful degradation. If everything
is going well, then the data should come in and you should be automatically building systems that
can retrain the model and deploy it in production without any issues, right? And so this should not
require any human intervention, right? The rest of the things, model creation and all that stuff,
that's where the scientists need to spend the time. So one unique feature that we have added
to this entire ProML, which may be distinct from other is we created this notion of a feature marketplace.
Right? So we all know in order to create machine learning models, features are the most important thing.
And so we don't want the scientists to be starting from the scratch, right? So there is a marketplace
where we have actually created a lot of features for you, right? Think of them as prefabrication,
prefabricated features, right? For instance, based on all the activity of users on the site,
we know who has a strong job intent. We know who clicked on an ad in the last two. So these are
features that are all available to you. Based on your profile, we know what are your standardized
title and all that. So if you want to build a machine learning task, you can just grab these features
from the feature marketplace using a consistent API, we call it frame. And that makes it super easy
for you to start building a model right from the very word go. And you will actually reach 80%
there, right? So that's very easy. Now, if the additional 10% improvement can add a lot of
business ROI, then you better spend more time on that because otherwise the first cut should be
something that you should be able to do very quickly. So I alluded to this earlier, but
and you did as well. LinkedIn was very early on in this space. You made initial investments based
on your technology landscape at the time, very Hadoop centric, but that's needed to evolve
over the years. Can you talk about that evolution and the challenges that it presented the way
you managed it? Yeah, so that was not an easy journey. So I think when we started the advertising
problem, for instance, we wanted to do very large scale modeling as I told you, but Hadoop is not
very amenable to that kind of computation. So we actually did algorithmic innovations to do
those distributed computations. So for instance, the first version of model fitting algorithm we
launched on the site was based on ADMM, right? So ADMM can actually help you do computation in pieces
and then you can patch it all together by using a simple computer. And that was very amenable to
Hadoop and that's how we started. We got the ball rolling and we saw enormous ROI system and
then Spark came along. We are one of the very early adopters of Spark even when it was not
very stable. And so we believed that Spark is going to solve a lot of our problems. So we invested
very early in Spark. We worked in fact with folks in MLlib. In fact, a couple of the folks who actually
joined MLlib were at LinkedIn. So we had a great relationship with them and we then really
focused on its Spark first-class citizen in our ecosystem. That helped us a lot to scale these
algorithms. And then finally TensorFlow came in, right? And so, you know, when with the deep learning,
again, TensorFlow, TensorFlow in those days had a lot of great things, but it was not, it didn't have
everything that would help us deploy it into it. So we had to build things like Tony and other
things that we have open source. So that is that spin really our journey. And I think all along,
we had very strong support from the business to do what we really needed to because the ROI
was always there. So that is the key lesson for me, right? Whenever you're building a machine
learning program, you know, you know, when you're going to your executives, just show them the money
and then everything else becomes much easier. If you can't prove the ROI, then it becomes hard.
So be the money. Yeah. You can do a lot of research, but the program should act tangible and
direct value to the business. And if you can ensure that balance, I think you are in great shape.
Then you get all the support you need. Because otherwise, you know, going to your executive team
in the early days of deep learning and telling them, okay, you need to help us get a GPU cluster.
It's going to cost $20 million. It's not a very easy argument to make a way. You have to wait
for a while before they actually buy into that. But you know, if you if you have a track record
of delivering, then these things work out much much much more easily. We alluded to this and my
conversation with Andrew earlier, you have to manage that portfolio and expectations very carefully.
Right. You know, what you can deliver in your term, what the overall vision is. How have you done
that? Yeah, I think you're just just described my job. But yeah, so I mean, that's what a portfolio
manager would do. Right. So I mean, I have this philosophy, as I said, like, you know, so there are
three components to your portfolio. One is the core investment. So this is something that you do to
add value to your business on a consistent basis. Like is machine learning able to improve
the number of the engagement that members have on our side? Is machine learning able to help us
create more revenue? Help our customers have a better experience? Is machine learning able to
keep our sites safe and clean? Right. So these are core investments. We cannot for not to do those.
Right. So that's a bulk of the investment. Like I would say that's 60%. The other 30% is more strategic
investment. So we know that deep learning is going to help us in the next six months. And after
all the model iteration that our engineers are doing with ProML, you know, even if they can do a
lot of iteration after a while, these methods are going to only give marginal gains. In fact, you
know, it will, it will actually put us in a lot of trouble because there will be so many experiments
queued up. And the amount of experimental budget you have is still constant. You cannot run
hundreds of experiments on the side, even if you can, because the experimental budget is fixed.
Right. So you have to invest in strategic initiatives. So that in six months, you get something new,
which is going to give you a big gain. Right. And so the experiments you're running will actually
still produce a lot of ROI. So that's the strategic bucket. And the 10% is more venture bed. Right.
So new product ideas, things like, okay, how can we do deep reinforcement learning and production
is it in the future? How do we build chat boards that can change the entire way users interact
with each other on LinkedIn? These are all venture beds and ideas. So that's roughly the portfolio
we tried to maintain like 70, 20, 10. And so far it has worked pretty, pretty well. We also do
other things like to encourage grassroots innovation. We have something called the ideas program.
So every quarter, anyone in the organization can actually submit an idea they want to work on.
And then there is a committee who actually looks at all the ideas and we'll select the top 10 ideas.
And those ideas get funded through our normal resource pool. So this is just to make sure that
everyone, it's not always top down. The ideas are not always top down. And some of the ideas,
some of the best ideas we have seen actually come from that grassroots innovation.
Our organization is also very energized. They know that we are in the innovation business here.
This is how we actually produce ROI. So it's also important to create that culture of innovation.
And you also organize hackathons so that folks can actually experiment with new product ideas
or prototypes and so on and so forth. So it's really that culture that is very important I think
that helps you create that energy and keep doing that innovation. And then we have an awesome
infrastructure team that actually partners with us to make sure that we are productive and we are
efficient. Okay. You mentioned earlier that you're very deliberate about measuring the experimental
velocity of teams at LinkedIn. Can you dig into that a little bit deeper? Yeah. So just as we
have a centralized AI organization at LinkedIn, we also have a central experimentation platform.
Yeah. Right. So every AB test, you run, we document that, we log that data.
And based on those AB experiments, we have readouts of things that succeeded in production.
So we kind of aggregate all of that information and make sure that we kind of have a dashboard
and we look at it every week and we measure things. Just as we will measure any business metric,
just as you will, you know, anyone's just as in your business metric, there are seasonality,
right? Like in the summer, people take vacation and we see a drop. If I see a very big drop,
then that's an alarm bell, like what is happening. And then I realize, oh, while everyone is attending
a conference, so okay, well, that's why we didn't have an hour experience. So we are very deliberate
and we are very particular about that. We in fact, enhanced it now, right? So now we started with
the total number of successful experiments. Now I'm asking even the teams to label their experiments.
So is it a large t-shirt or medium-sized t-shirt or a small t-shirt, right? And it's very interesting
how teams kind of put those labels and then we discuss about it. Like, you know, you can give a
hard time to your team. You know, you're a good way. I'm a good manager. So why do you think this
is a large experiment? And you know, it's okay. Do you think you can't think bigger than this?
And so it's actually a nice thing for you to even get a sense of the culture that you have in
your organization. Like, who thinks what is big? What is medium? What is large? And over time,
you can actually create this culture where you are actually asking folks to run fewer numbers of
larger experiment because it is great to say we can run a lot of experiment where every experiment
has a cost associated with it. And I don't mean infrastructure cost only. I mean opportunity cost.
Right? If someone is running an experiment, they're going to wait. Then for two, after two days,
they're going to look at the results. And then they're going to move on to the next idea.
So this is a hidden, economists call it the hidden cost, right? So there is a cost to every experiment
you run. And the best way for an organization to become really effective is to run a large number
of high value experiment, not a large number of experiments. That is not all enough, right? I mean,
because in many cases, I've seen people run parameter sweep experiments. I mean, those experiments
should be all automated. So you have to actually have forensics on the experiments as well.
Once you become a large organization, the next step is to kind of analyze, have telemetry on your
experiments and then figure out what experiments can be automated and encourage your engineers to
work on the big things rather than on things that can be automated. You mentioned in opening things
up that LinkedIn was very early in adopting data science. And in that period of time, you know,
once that term caught on, you know, everyone who worked in this space was a data scientist.
Since then, the roles have evolved. We've got machine learning engineers, we've got platform
teams. Yeah, what does that evolution look like at LinkedIn and where do things stand now?
Yeah, so that evolution, like in any other place, is still evolving, like we all know. I mean,
to me, there are four pillars of AI or data science, whatever you call it. And that's how, actually,
five machine learning computer science, of course, that's the key. Then statistics,
optimization and economics and systems engineering, right? So these are really the five pillars. And
when people from these five disciplines come together and work together to solve hard
problem, that is what creates the magic, right? Now, I think labels are labels that keep changing
over time. I don't think at LinkedIn, we pay too much attention to the labels. We just make sure
that these five disciplines can always come together and work. And then, you know, once, so that's
the technology side, but technology side is also not enough to create awesome machine learning
solutions, like for instance, the most important thing in a machine learning problem is to define
the problem. What are you trying to solve? And you cannot define the problem unless you interface
very closely with the domain experts, you know? So you have to work very closely with the product
team. And in this day and age, you have to worry about security, you have to worry about legal. So
you have to interface with the legal team, right? So it takes a village to get AI, right? And so,
I think that's how we think about AI at LinkedIn. And in order to actually make sure that everyone
understands the basic of AI at LinkedIn, we have actually created what we call the AI Academy.
And so it has three levels of courses. The AI 100 is general awareness. In fact, I also
teach there, we're passionate about teaching, right? So this is like a two-day course. We have our
best people kind of teach to everyone in the company what AI is all about. And it starts from
data, right? I mean, you cannot be AI first company without being a data first company.
Because at least today, we don't know how to do AI without data. Maybe in the future,
we may create a new theory that can help us do AI without data. But right now,
we all know we need data to do AI. So data first to AI first. And that education is a very
important component. Then we have AI 200. So if you are an engineer who has taken Andrew's course,
but now want to get your hands dirty, you can actually get your hands dirty through AI.
200. And AI 300 is an internship program. You can be an intern in the AI organization. Someone
is going to work very closely with you. And you will be actually deploying things in production,
right? So the education and this collaboration and tooling, this is really how we run AI at LinkedIn.
And we always say, it takes a village to get AI, right? And we have seen that first time.
You know, if you try to create an AI program in a company in a silo, it would never work.
You just have to do it together. It's too big to be solved by one discipline is my opinion.
So maybe to start to close out, I'd love to hear your vision for where ML is going at LinkedIn.
And maybe even more broadly in the industry.
Yeah, so I think we have reached a point where deep learning is really helping us a lot.
And you know, we are able to solve some very traditional problems that were very hard to solve,
in a very spectacular way. But I still think Andrew was also talking about that.
Machine learning is still not very accessible broadly, right? Only a few companies
that have the talent and that have the culture that they've built in this space for a long time,
they are the ones who are really reaping a lot of benefit now. So how do we make machine learning
more accessible broadly so that everyone can benefit from that? I think that's a very challenging
problem. It's not easy, right? Like, you know, if you don't even understand how to manage data
to think that one fine day you'll just wake up and run a deep learning model and we'll all start
working automatically, that's a pipe thing. That doesn't happen, right? So getting there would be a
very big task for us. Once we get there, then I think the computational cost is also something
that we all have to think about. So GPUs are not just costly, they are also bad for the environment.
Right? And so, you know, we don't want a planet where we are actually doing so much compute
without being careful of what we're doing to our environment and to our planet, right? So
if you want to be cost effective, you have to figure out ways to kind of do the computation
in a more efficient way. I think that I believe it's going to become a very important
topic for all of us. And finally, responsible AI, right? And again, I don't want to steal
thunder. There is a panel on this later at the conference, but doing AI in an ethical way,
doing AI in a responsible way, making sure the privacy of individuals and everyone else is kind
of a dear to and we're not having data breaches and all that stuff, right? So that's again,
a very important topic for all of us because there's no point in creating a very powerful technology
if it works against the humans, right? I mean, we need to create technology that helps us
become better, right? And so this is again another big area for all of us to think about in the
future. It's not about where this will go. It is all about where we want to take it to, right?
So this is one thing where I don't want to make a prediction. I want to kind of appeal to all of
you that we all think about it very deeply and make sure that we are all using the technology
in a way that is going to help the human race, not hurt the human race. Awesome. Well, Deepak,
thanks so much for joining us. Thank you. All right, everyone. I hope you enjoyed our show
straight from the main stage at TwomoCon AI Platforms. For more information about today's show,
visit TwomoAI.com. And for more TwomoCon coverage, visit TwomoCon.com slash news.
Thanks so much for listening and catch you next time.
