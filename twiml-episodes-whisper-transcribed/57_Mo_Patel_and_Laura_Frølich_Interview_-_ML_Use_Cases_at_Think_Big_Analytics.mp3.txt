Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This show you are about to hear as part of a series of shows recorded in San Francisco
at the Artificial Intelligence Conference, which was hosted by our friends at O'Reilly
in Intel Nirvana.
In addition to their support for the event itself, Intel Nirvana is also our sponsor for this
series of podcasts from the event.
A huge thanks to them for their continued support of this show.
Make sure you check out my interview with Navine Rao, VP and GM of Intel's AI products
group, and Scott Appland, director of Intel's developer network, which you can find at
Twimbleai.com slash talk slash 51.
At the AI conference, Intel Nirvana announced DevCloud, a cloud hosted hardware and software
platform for learning, sandboxing and accelerating the development of AI solutions.
The DevCloud will be available to 200,000 developers, researchers, academics and startups
via the Intel Nirvana AI Academy this month.
For more information on the DevCloud or the AI Academy, visit intelnervana.com slash
DevCloud.
Our first multi-person interview.
I speak with Mo Patel, practice director of AI and deep learning and Laura Forelich,
data scientist of Think Big Analytics.
Mo and Laura joined me at the AI conference after their session on training vision models
with public transportation data sets.
We talked about this and a bunch of other interesting use cases they worked on involving
image analysis and deep learning, including an assisted driving system.
We also talked through the practical challenges faced when working on real machine learning
problems by feature detection, data augmentation and training data.
All right, everyone.
I am here at the AI conference, and I am with a couple of guests this time.
I am with Laura Forelich and Mo Patel with Think Big Analytics.
In fact, Mo and I, we had an opportunity to meet at a conference a while back, and you
kind of came up to me and introduced yourself as a listener of the podcast, which I think
that was maybe the first time that ever happened to me, and I was like, excited out of my
mind.
Yeah, I remember you were actually talking to the people that chain the deep learning
framework, and I was like, I recognize that voice, you know, it's a very recognizable
voice.
And we had an interesting conversation about the industrial AI stuff that I was working
on at the time, and some of the work that you were doing there.
So then when we saw that you and Laura were doing a presentation here at this conference
in San Francisco, I thought, oh, we got to get you on the show.
So welcome.
Thank you.
Thank you.
This is actually the first time I'm doing an interview with two guests, so to be interesting
to just how the kind of traffic management works.
But why don't we start by having Laura introduce yourself?
OK, so currently I'm a data scientist working with Think Big Analytics, as you just mentioned.
So I work on all types of projects.
Whenever a customer or company has a lot of data that they want to gain insight from
to solve some use case, I can help them out.
It doesn't have to be deep learning, like any sort of method that tries to reveal relevant
patterns using some method that makes sense, given the use case and the data at hand,
we'll go with that.
Before joining Think Big, I spent half a year in a research group where they investigated
something called non-specific effects of vaccines, which is basically vaccines turn
out to affect the immune system in a general way, not just protecting against the targeted
disease.
So very interesting research.
That I did a PhD at the Technical University of Denmark using various machine learning techniques
to analyze brain activity data.
Oh, wow.
Yeah.
So that's sort of my background.
OK.
Enemote?
Yeah, I am currently the practice director for AI with Think Big Analytics, mostly looking
at America's customers.
And part of that is probably not as working on projects as much, but doing more of the
proof-of-concept type work, so taking some of the most advanced things that are going
out there and see if we can apply them to our clients' problems, so part of that.
So there's a hands-on portion of it, but then there's also the dreaded power-pointing
of things, character-version of that highly technical stuff, into things that people don't
understand, which is a kind of a fascinating part of it, because I really love that trying
to lower the barriers, because there's a lot of hyperion, I try to lower the barriers
so that people can understand that this is not terminated or it's actually just math,
right?
So that's kind of my day-to-day, I really like doing that, and my background is I come
from, if you look at data science, machine learning type things, I come from more of
the computer science side, compared to people who come from statistics or maybe from some
sciences, the hard sciences, or the years software engineering, into transitioning into more
math type software engineering, and then into analytics, and yeah, oh, nice, nice.
And so the two of you did a talk yesterday, it was actually a tutorial, yeah, what was
the tutorial about?
The tutorial yesterday was on image analysis using deep learning methods, in brief.
So we had both the general introduction, making sure that everyone was on the same level,
agreed on what is an image, what are pixels, what sorts of values are we dealing with,
and so on and so forth like, and also going into what sorts of problems can we talk about
in the image analysis, and then we went into more detail on one particular topic, object
detection, and zoomed back out a bit in the end.
So that's how I sort of see the whole framing of the talk, I don't know if you have anything
to them.
Yeah, absolutely, I think, but just kind of making sure that people are aware of the computer
vision basics, and then diving into something that is fairly cutting edge, object detection,
and a lot of applications out there, and not only the theoretical part, but also in a notebook
style kind of layout saying, hey, this is how you can actually do it yourself as well,
right?
I felt that was very compelling, and then towards the end, talked about some of the challenges
around training models, you know, it's like, we make it sound so easy, but to do it for
real data, there are many challenges, and so we talked about that and we can go into
that detail if you want.
Yeah, what are some of the challenges?
Yeah, absolutely, you know, for example, there was a paper that came out or the weekend,
how this team trained image net data set in 24 minutes, right, and, and, I mean, you
look a bit of a controversy.
Yeah, yeah, it was, of course, yeah, because, you know, it's like, well, they used that
headline was Alex Net, not ResNet, which is like more of the current state of the art,
and, and what kind of hardware you use, and all sorts of different things, but that is
exactly the type of thing, right, is that what are all the different parameters, like
batch sizes and, and different processors and, and multi GPU, multi server, because most
of the things you see the examples of tutorials, it's like, just run this code, right?
But if you in production, even you have like a million images, and you need to make sure
that this will train over like days and not weeks, you know, you may have to scale it,
and how do you do the scaling, and those are all the challenges that's kind of like more
engineering style challenges, but then there's also challenges around annotation, right?
It's like, well, this is supervised learning, you have to annotate the data, and that could
mean anything from, it's like, you know, simple classification, kind of easy, so to speak,
right?
But there's a picture, and there's a label, right?
Now you, for object detection, there could be bounding boxes, so you draw squares around
the objects like our balls, and, and, you know, like umbrella, and, and things like that,
to make sure, and then label it that this is what the object is, and then the, even something
more advanced, which is kind of drawing the polygons around the objects itself, which
is the segmentation, kind of like the holy grail of, of, toward getting towards being able
to do object detection.
Many challenges in, and of course, you can try to do it internally or externally, we
actually, for a project, we actually built a segmentation tool that allowed people to
go ahead and draw boxes around cars, and pedestrians, and things like that.
So, you know, as much as we talk about the deep learning parts, right, there's all sorts
of many data engineering, data prep, data cleaning, things that were, have been around,
and also the traditional data science for a while, right?
So very much of, of, of, challenge.
And so that's that a bit with the annotation tool that we built internally.
We have that tool, and we were using it internally, but our team was just not large enough to
annotate enough images quickly enough, so we had to both use that and go to an external
company to help, get help from them to annotate all the images, and that was a lot of back and
forth with them just defining their requirements.
What sorts of things do we want labeled?
How do we want them labeled, like, what's the smallest size of object that we require
labels on?
What do we do if it's partially obscured, like, if there's a car blocking another car?
Those sorts of things, because you may have difficulties if you have a really small object,
and you label it, then during the training phase, you'll be punished.
The model will be punished if you don't detect that object, but it may actually not be
very interesting to detect that object at test time, because small image, small objects
are far away, so you may want to focus on objects that are closer.
One way to handle that would be to put an extra label and small objects, saying, difficult,
in that way you might handle such objects differently from normally labeled objects by not punishing
the model if it doesn't detect them, but also not punishing the model if it does detect
them.
Yeah, I've had some interesting conversations with folks that specialize in labeling data
for folks, and it really opened my eyes to just this process that you're describing.
If you think of, hey, just label my data, but if you're talking about images, all different
kinds of ways that you can do it, and they have direct impact on the types of, you know,
not just the types of models that you're creating in their performance, but the cost of the
labeling process.
Yeah, exactly.
So, this company that we worked with had this quite elaborate pricing scheme, I never
really looked at the details of it, but if you increase the number of classes, you would
sort of get an extra cost to the first classes as well.
So you really had to consider, like, more maybe can we sort of have this external company
to part of the labeling and then do some further post processing in our own tool, like
a way.
We needed a machine learning model to optimize the processing for vendor tool.
Yeah, exactly.
And, you know, there are some interesting projects around that, you know, there's that
snorkel project from the Don Lab at Stanford, they're trying to do some stuff around kind
of, you know, around the training data and building, building more training data, which
just reminded me, another big one is a data augmentation, which is another thing that,
you know, if your data doesn't have, I example, I gave, is that having the road data for when
it's foggy versus when it's not, right?
And what if we never capture the fog based, which is, it should be hard to do in San Francisco,
but, you know, what it's so, so those are all the type of things that, so luckily, you
know, at least what's great is that a lot of money, much of this knowledge has been encoded
now where they're just in chaos, there's just a function for data augmentation, sure,
maybe you could add your own, but there is that state of the art, like might as well just
use that when you're training process, but these are all the things and it's like when
you think about simply, it's like, yeah, just take the images, run it through a deep learning
model and outcomes your, your trained model after you do all the deep learning things,
like generalization and, and, and loss optimization, all those things, but then there's all these
other things that you have to kind of worry about, yeah.
And regarding data augmentation, even though people have made tools that you can sort of
just use, you of course have to think about all of the data augmentation steps, do they
make sense?
For traffic scenes, do you really want to flip your images horizontally to teach the
model that an upside down car is also a car?
Well, maybe you do, but it depends on your use case, right?
Right.
That would depend on your use case.
Yeah.
I mentioned action movies, you know?
Yeah.
I'm going to get those upside down cars, yeah.
And so these are, these are just some of the issues that you run into in the training phase
of building and deploying a deep learning model.
And then there's the whole, you know, how do you actually get this out into the wall to
do inference?
Like, are there, you know, best practices, tips, tools of the trade or, you know, tricks
that you've come across for that?
Yeah.
I mean, actually, as much as, as much knowledge there's out there about the training aspects
of it, there's actually not as much.
And something that like even even traditional machine learning in production, I think we
always, we heard this thing about data science and like, there's not a, you know, there's
a lot of talk about it, but the in production is still not, you know, you could do a survey
and you probably find that only maybe off all the people who actually say they do data science
only 25 to 30% maybe actually put it into production.
And then once you get to deep learning, that could, that number could start even going
lower.
And I think that's just because as you were saying that there are, there is a whole different
set of challenges when you're trying to put models into production, right?
So number one, where are you going to put it?
Are you going to put it on some beefy servers in a data center?
Then maybe you have lesser problems because you could take those big, gigabyte size models
and, and stupid, Democrats, containers and kind of a lot of the traditional way of apps
are served.
But then you start talking about, well, we're going to do mobile.
Well, that's, you know, that becomes another challenge.
How do you compress the models, maybe quantization, you know, lowering the floating point and
things like that?
So, you know, that's another, and somebody actually asked this question, they're like,
I was like, can I take this and put it into a card to, you know, do this?
And we're like, there is a lot more that would go into it before you be able to do that.
So the training is definitely challenging.
But being able to serve the models at scale, brings in kind of all your traditional DevOps
and data ops, kind of bringing it all in, model management, version control of the models
and data lineage, traceability, everything that we've been discussing for any other data
science type things.
Those are all bring, they're back on the table, right?
And how complex those can be.
Are there emerging or accepted tools or open source projects for doing that kind of thing?
I know that, you know, some of the folks, some of the vendors that focus on, you know,
machine learning platforms, they've got some of that stuff built in or integrated into
their tool set.
But it's all like, you know, within that tool set, are there, you know, is there a kind
of open source model management framework, for example, that's kind of emerging as a
standard or does it have to be kind of custom cut for an individual use case?
At least I haven't come across.
I know we built one internally, yeah, from just a lot of our projects, right?
Because we saw this need of model management and, you know, it's internally called Tink
Deep, you know, for managing models, you know, it's maybe, maybe we'll become a
open source, you know, you know, how these things go, you know, I think big we have tried
to make things open source, for example, the Kylo Framework for data lakes, you know,
so this could be another path on that roadmap, but, you know, there's a lot of polishing
that needs to be done before it gets.
So folks should reach out to you if they want to get their hands on this open source.
Yeah, possibly, possibly, I'm not the operator, so I can definitely put in touch.
But I have not come across, there are definitely some projects.
Once again, the formally amp lab, the UCB rise now, right, and then, and then Stanford
Don, right?
They, they have, I can't remember the names of the projects of my top of my head, but
I've seen that in, that's in their program agenda for being able to serve large scale machine
learning models and you can consider deep learning into that, into that.
And I will, you know, I'm not, not do a commercial plug, but that's actually one of the things
that we are also focused on because when we look at things, it's like, yeah, training
is, is there, but we work with traditional customers, enterprise customers who want to
put all that stuff into production because all the investment, investment they make on
AI or deep learning, data science is useless unless they actually put the models into production,
right?
So building the tooling around that, monitoring the models and all of those things are
interpreglability.
It's a huge part.
All of those being able to kind of have one stop shop for that is very attractive.
So there's commercial aspects to it.
Interesting, interesting.
So we, before we got started, we mentioned a few use cases that you would be able to
kind of talk to us about and walk us through and the first one is one that you worked on
Laura as a traffic project that you worked on with an automotive parts manufacturer and
it sounds like this was in some ways an inspiration for the session, the tutorial that you did
here.
Can you tell us about them?
So this project was for an assisted driving system.
So that self-driving at all, we didn't want to go there, but just to help each other
essentially.
So the idea was that you have a car that's connected to the system and it's driving along
like doing whatever it's doing, going where it's going and it has a camera recording whatever
it passes.
If it happens to pass a stop car that's on the road, it might be a good thing to be able
to detect that, to tell other cars, hey, look out.
If you're in this lane, you might want to change lanes already now, so you don't sort of
get a surprise when you get to that stop car.
You could even have that indicate this an accident or just congestion.
So stop cars on the road is definitely something that you might want to tell other cars about.
So the project was about trying to detect this and there are a lot of steps to this.
First, a video is of course a lot of images that come in one at a time.
So the first step was trying to look at, okay, so how well can we detect the objects
and trying to compare the various object detection methods that were out there, see how fast
are they, how accurate are they, all of these things and also try to get an idea of how
might we be able to improve them.
Then subsequently you have to be able to tell is this part of the picture part of the
road?
Or is it not part of the road?
Because if it's not part of the road, if it's parking, you don't care so much whether
there are stopped cars in that car's in there, so that's another thing.
Thirdly, you'll want to be able to tell is the car moving.
So one thing is detecting the cars in each image.
Another thing is seeing, so this is the same car that I just saw in the previous image.
And then estimating the distance, the difference in distance from when you saw it last, trying
to estimate its speed.
So this comes into object tracking, which is synced to from the research that we were
able to do to be still quite immature compared to object detection.
So far, the detection there are already a lot of methods, a lot of research out there.
For object tracking, it seemed to be less solved.
So we spent some time looking into that.
And so in the end, we had a demo that was able to detect most cars and sort of give an
estimate of is it moving, how fast is it moving?
And that was basically where we stopped.
Okay.
And were you exclusively trying to detect cars or other obstacles in the road?
So we did want to detect other objects in the start of the practice as well, turned out
that there were some difficulties with that because in traffic data sets, you tend to have
a lot of cars.
Right?
You don't have as many people.
You don't have as many bikes.
You don't have as many buses.
So your training data has a high imbalance.
And that just makes it more difficult to learn those other classes that are not cars.
So in the end, we ended up focusing more just the car part of it and not so much on detecting
all the other classes that we were initially interested in.
Okay.
Interesting.
So you mentioned the object detection methods that you came across.
Can you kind of summarize the state of object detection and what are the main methods and
what you found as you compared them one to the other?
So the methods that we looked into most were the ones that were quite recent at the time
of the project, which was taking the beginning of this year.
So at that time, there were a lot of what's referred to as single shot methods.
So you have two versions of one called YoLiLook1s, YoLo.
There's one called single shot multi-box detector.
And what these methods have in common is that they just look at the image once.
So you'll take the image that you're trying to analyze and pass it through the model
once to extract features.
And then based on those features, you'll predict the coordinates of the bounding box as
well as the class of the object within the bounding box.
And you'll do that for a bunch of predefined boxes on the image.
So before like seeing the image, you'll already have to find a large number of what's called
prior boxes or default boxes, their various names.
So for one particular method, it's around 7,300 prior boxes.
For each of these boxes, you give a prediction of the class that it might contain and a
correction to the predefined coordinates to match the object better.
And that's sort of the general theme for these for these single shot methods.
Okay.
Is that they generally they'll predefined some large set of boxes and then detect objects
relative to those boxes?
Yeah.
Exactly.
Okay.
Interesting.
And then on the object tracking side, what was the method that you found and how did you,
what was your experience with it, how did it perform?
So we ended up using a heuristic approach.
So basically looking at the detected objects and a number of frames, I think we ended up
with Iran looking back at the past 20 frames.
Basically trying to match the current car that we're looking at to the car in the previous
images that matches the color of the best and has the closest Euclidean distance and those
sorts of heuristics that turned out to work pretty well.
Okay.
And you mentioned kind of doing a scan of the literature.
Was that one of the methods that you found and the researcher, were you not able to get
any of that stuff to work while?
Yeah, exactly.
So the methods that we found and tried out, we spent quite a while trying to get them
to work and didn't really work out.
So in the end, we just thought, okay, let's try heuristically see how it works.
And it ended up working pretty well.
So then in the interest of time, we went with that.
And of course, we're hoping to get some of the more advanced methods to work in the future.
But the heuristic approach turned out to really give quite good results.
Which is also an important lesson for folks that actually have problems to solve.
Yeah.
Yeah.
So I was observing that project mostly and of course, as you were just saying, for
an outsider, I was like, oh, this is clearly, you need to use the Eurusing Conventional
Networks, but then use recurrent on top of that because you're trying to track something
and then maybe do the predictions for future frames and things of that sort.
And that was the kind of the common, and actually, of course, that's what I searched for.
There is somebody who was working on a recurrent yolo, right?
We did try that, actually.
That was what it means we tried, and yeah, it didn't work, and that's what we mean.
That maybe, you know, this is something that will, as more people, experiment and it will
start evolving.
And because object tracking is another great area for computer vision.
That's a big challenge for folks that are implementing this stuff like in real use cases
is that, you know, I've heard it described and repeated it as kind of overfitting on
a data set, right?
Like, hey, this works great for MSNet, right?
But, you know, for another data set, it doesn't.
It's hard to reproduce the results.
And that's actually another issue that we ran into.
So we took this pre-trained model that someone had under GitHub repo just to see how it worked
and it didn't detect anything.
So this was a sanitation model, and we wanted it to detect roads, and it just didn't detect
anything.
And, of course, in our images, we had a lot of roads because this was a traffic data
set.
And we were like, why is this, and one of the team members wrote the author, and he was
like, oh, well, that's because it didn't do data augmentation, because I wanted to improve
the performance as much as possible to the data set that I'm submitting this entry to.
So this was a competition, and he was just optimizing to these lighting conditions and so
on that the data set had.
So when we retrained the model with data augmentation, it was actually able to detect
a lot more road, even we didn't add any data, we just used data augmentation.
And what is data augmentation doing in this process?
So I think some of the parameters that we tuned in this case was suggesting the brightness
of the training images.
So like we had the original training images that this guy had trained on, but then we added
the same images, but with different levels of brightness and other things to the training
set.
So essentially to deal with things like glare, nighttime, twilight, and all sorts of other
lighting type situations, right?
So just do a bunch of different transformations on the image to adjust the brightness by
a few plus minus a few stops, or maybe apply some cool Instagram filters.
There you go.
Actually, so what's great, once again, I love this field and everything people are doing
because there is a paper around this topic of data augmentation and best practices.
And much of has been codified in Keras and some of the other computer vision libraries.
So you don't have to do imagination.
It's like, let's just apply this best-in-class data augmentation and then we'll just see
if it works.
Of course, if it doesn't, then you may have to go inside and tweak some things.
Yeah.
Okay.
Awesome.
Anything else on that project?
I can think of right now.
Yeah.
I think another thing from just my observations was that, you know, everybody has their local
perspective, right?
And we have a kind of a U.S.-based, U.S. centric viewpoint on traffic and driving.
And I think there's just a different set of things in countries, right?
And one actually, one good example of that is traffic science, right?
You know, like a stop sign is, you know, the concept is somewhat universal, but it is
different in other countries.
So these are the type of things that you heard or account for if you're trying to do more
like sign detection or other types of things.
Yeah.
Excellent.
Excellent.
So you mentioned a project called Lost and Found that was for a logistics company?
Yeah.
Yeah.
And that one, you know, as much as this kind of object detection type things, that one
had a little bit of a different type of challenge.
And that is that it's the type of thing that you were trying to find, right?
So it's like, imagine your shipping packages and then, you know, somehow the package loses
it's tracking label drips off or box break.
And then so this company finds the item and that's like, okay, what is this?
Because a lot of times it's not really clearly.
Or you as a customer will call and say, I'm missing like, you know, a brown pair of shoes,
you know, but this isn't this, you know, and somebody's manually going through all the
stuff to find it, right?
And just, yeah, it's a very tedious process.
And so how do you build a system that could give you like, you know, based on some description?
And I know Google image search is this great, but, you know, we don't have, you know,
it's not available.
It's not available as API even, you know, Google is not giving it to everybody.
And so there's some cloud APIs, but still not this, the point I'm trying to make is that
the universe of items you could lose is infinite, right?
It's not like a 10 class, 90 class, 9000 class.
It's like infinite.
So doing object detection model that will say, oh, we'll just be, we'll just detect the
label of the item and we'll just be able to find it.
So the approach was a little bit of a combination of transfer learning and feature matching as
opposed to like feature detection, feature engineering, feature learning.
So you know, one thing that customers do when they ask the question is they actually
could provide a photo from the internet or they may actually have the item.
And you know, this all sounds like probably somewhat easy for commercial items, right?
But if you think about industrial items or niche things or collectibles and stuff like
that, that's where you really start getting into the problems finding these items.
Certainly, you're not going to have them just sitting around in your training data.
Yeah, yeah.
The way to kind of tackle this problem, it's like an image search problem, right?
And previously, previous generation techniques were doing, you know, a lot of the handcrafted
features, there are algorithms like SIFT, Surf and Orb and some of these previous generation
feature detection algorithms, I believe if you open SIFT, yeah, SIFT, SURF and ORB, these
are all acronyms for feature detection in previous generation computer vision.
Matter of fact, I think, and you know, of course, I don't know the details, but when I look
at it, the Amazon app, when you open up the visual search, I think they're not using
deep learning.
They're using that previous generation because of the dots it's showing.
And based on just my knowledge, that's exactly how it works.
So I don't know if somebody, maybe somebody from Amazon can verify or maybe not, they probably
don't want to.
But yeah, so we were like, well, we don't, those, one of them I forget which one is actually
proprietary, so we can't use it, you know, without getting a license and all sorts of challenges
like that.
And we have deep learning now.
So being able to extract using some of the straight up state of the art models such
as ResNet or Inception, and you know, it's really thinking about this problem.
It's like, well, what are those models doing, right?
You have high level, high dimensional data images.
And the point of generalization is to come up with low level features that you can generalize
so that it will work on not just that data on much wider data.
So well, why don't we just try to take our target and the source and essentially extract
those low level features and create a feature store, right?
So the next time you have a search for something, you say, okay, let me, it's like, it's almost
like fingerprint matching type, if that's one way to think about it, right?
It's like creating the fingerprint of the search.
And then we have the entire database of fingerprints and you just say, okay, find the one that's
the closest match.
And we didn't have to get it 100% right because there was a human in the loop that somebody
would say, okay, all right, I can tell between these five images that this is the one,
right?
Find your features a priori or where the, the training process.
Yeah, we just use, we didn't do any training.
We just use the pre-trained model, for example, ResNet or, or inception.
We removed some of the, some of the classification layers because we weren't interested in the
classification, right?
Because if you look at the learning model, that's what the lower layers are doing.
It's eventually, after the features are, you get to the low level, you then do a, you
do your classic classification.
And then, so we said, we're not interested in that.
Let's freeze the model at certain level and then then see what the features are coming
out of that.
Store those and then let's do the same thing for the search candidate.
And then the object tracking, there's a lot of similarities as far as the Euclidean distance
type thing where you're trying to make sure that it's the same car from the previous image
to this image.
And then, so once you extract the features, you try to do a distance and see how close,
closely they match.
And that's essentially what you're trying to do is that you, in this case, we were doing
a cosine distance between both of the features, feature vectors, or the kind of the customers
feature vector and then the entire database of images, objects that we have to find the
closest ones.
So that's kind of like, you know, in that cell, that's how that problem was solved.
On that one, did you determine where you were going to freeze your network based on experimentation
or was it more kind of intuitive, you know, this is where the classification starts, so
we're just going to stop here.
Yeah, so yeah, if you ever kind of dissect one of these models, it's all, you can read
all the layers.
And yeah, you can tell which layers is when the kind of the feature extraction stops and
the classification starts typically around kind of densely connecting and softmax classifier.
Those are the kind of the later, later stages.
So once you start kind of stop your convolution layers, you know, that's when you can, that's
when you can stop.
And yeah, we, you know, we, so we try to experiment with like, like till the very end or somewhere
in the middle, you know, because these models are also, the state of the art have 50 layers,
you know, like many layers.
So it's like, you know, because we were not really interested in doing the classification.
So at what level of feature, raw features that, that our, our search works in a good performance,
yeah, right?
Because once again, like if you get to like, really low level, then our search results
will be like all over the place, right?
So we want to capture at some, some good level.
So they're not a, not scientific way, it's very much was a trial and error to come up with
where, where it was giving us the best results.
So I was actually wondering just now, did you use the features from many layers at one,
so did you pick just one layer and take the features from, from that?
Yeah, after, after several layers, so like 25 or 30 layers, we said, okay, now let's see
what the output of the images, you know, the feature vector and said, now this is the
one we want to use.
Yeah, I'm, I'm thinking inspired by these object detection methods that sense to use features
from different layers to capture both large and small objects in the amic.
But I guess it's not so important if you know that you have one object in the amic, and
you're just trying to find that one object, right?
Yeah, typically the, so, so what I didn't mention is that this company, when they get the
item, they take photos of it, right, multiple angles and everything.
So typically that's the only thing that's in there, once again, all classic problems still
are there, like data cleaning and, and background removal and all sorts of things like that, yeah.
Yeah, yeah, interesting, interesting.
And you mentioned one more, which was a fraud detection app for a bank.
Yeah, yeah, and, and did you want to talk about, I can also talk about it, you know, if
you could start to talk, yeah, so I, I wasn't on the topic, I, I know about the project,
yeah, I, I just felt like, because you, it's in Denmark, right?
And, and, yeah, is this, I think more details can be found, because it's the video off
this project, there was a talk at the O'Reilly conference in New York in June, and it's available
online, so those who are interested into this technique, I think what's really neat about
this is that in the previous two cases, we talked about computer vision, right, which
is kind of the, the cool factor, right, because everybody is excited about being, being able
to do things with images, right? But now this is, here comes like fraud detection, which
is, you know, through the common person, it's like, who cares, right?
As long as my career card works, and nobody, you know, anybody who defros me, but banks,
of course, are very much concerned with that, right?
So, so in this case, you're like, well, I mean, so of course, fraud detection is not new,
right?
People have been doing this for decades, all sorts of things like human created, curated
rules, right? It's like you swipe a card in San Francisco, and then you swipe it in, you
know, Bombay, right? It's like, obviously, there is a problem, right? And so, you know,
it's, you know, things like that, you can always have those handcrafted rules, but, you
know, those can become like, you have 20,000 rules, and it's like, you know, it becomes
a real cumbersome process, you start applying some traditional machine learning aspects to
it, so you do a lot of feature engineering on the data to say, okay, these are the, these
are the features that contribute to fraud, and we should flag for that, right? And then
we're like, well, what are the ways you can improve upon this? Now, we have deep learning,
feature learning, are there ways to actually improve upon the model? And so, you know, we
said, well, let's try a couple of things. So, one of the things, and this is a classic
by the object tracking example as well, like, so it's like, well, what is the first thing
I would do? And most people with transactional data or sequential data, the go to technique
is using some kind of a curl model, right? Where LSTM or something like that, right? So,
that you have that recent history, and then you can be, you should be able to tell, oh,
this is fraudulent, or it's about to be fraudulent. So, we tried that, and actually, there's the
model result, if I recall correctly, there's a chart in that talk, which shows you all the
different approaches. I think it was on par with the traditional machine learning, so not
really, not promising, but then, like, well, creative way of, like, well, what if we could
apply some of the vision-based techniques? So, it's, you know, vision, there are certain
properties that are necessary for a conditional model to work, shift very invariant statistics
and locally curated values. Those are the two things that famously, Yamakun tweeted
out a few months ago, which I remember very well, right? I'm not coming from research, right?
And those things really made a lot of sense to me, it's like, okay, all right, so those are
the properties you need, because traditionally, the type of data we have, even in credit
credit transaction, it's just some kind of a time series, you know, you have, you have
amount and location and all sorts of things like that, but not like a picture, right? And,
you know, this technique is detailed in, if you, you know, even five minutes of that talk,
there is a couple of slides on it, which will, this, like, visually, it's very intuitive,
is to come up with a feature map of the transactions, using some history. So, then idea is to create,
like, a visual image of the recent transactions, and then feed it through the convolutional
neural network to see if you could detect fraud. When you think about this, and this is something
that I would, I would ponder others to think about, right, is that think about this as,
as how we would do something like that, right? And one example I always talk about is that
people who sit in, like, operations controls, right? And they're looking at those 50 screens,
right? Or even traders when they look at, like, all these charts and all these things, right?
You know, they're visually also trying to look for some changes in signals that will allow,
say, oh, you know, I need to take some action. And the idea is very similar here, right? Is that
if we could convert, I mean, there's data behind those charts, right? If we could somehow create
the, and we just, like, we'll create charts out of the data, and then feed it through the,
through the convolutional neural network, and then tell it what abnormal looks like, right?
And then now I'll ask it to flag it. So, it's super interesting. I think those traders would be
a lot less effective if they were looking at a spreadsheet. Yeah, well, and we know, we know that,
right? It's like, yeah, it's like, I imagine if I gave you, like, a trend chart versus, like,
all the data in a table, right? How quickly could you make a decision, right? Yeah, so
put like that, it's super intuitive that a neural net would, you know, vision-focused
neural net would be effective in solving these kinds of problems. Yeah, yeah. And, you know,
it's a, it's a, it's a great, I love talking about this because it's a great example of,
of applying some of these kind of what the research there and the progress that we are making
in computer vision towards the problem that you were to think is in that realm, right? And you
can talk about, I mean, you know, and when you start thinking about this way, there are many more
things that open up, like, you know, anomaly detection and anything that you can, essentially,
you can visualize, like, if you had like a heat map, right? So, yeah, we go up shore, we can generate
a heat map of a lot of data, right? I think the big trick over there is that you have to be consistent
in the way you feed the data because remember the properties of the common neural networks
require that it's like, think of it, it's, it is an image. If you start shifting around the bits,
the image will stop making sense, right? And that's what we are really banking on, that it is
the image that could, that makes sense to us and we are trying to find those patterns. So,
it is very important that you don't use another technique the next time around. You keep the feature
map same for the, even throughout the production cycle, right? Not, not change it around, not change
the sequence of transactions and things like that. Okay. Yeah, it's detailed much more in that talk
if anybody's interested. We'll try to track down the line. Yeah. Yeah. Awesome. Well, these are all
really, really interesting use cases. Thank you both for taking the time to kind of talk through
them. Is there anything else that you wanted to mention? Either of you. Nothing comes to mind right
now. Probably something built in like half an hour? Yeah, of course, of course. I'll take Sam's
role and I'll ask you, what has your impression been in this, I guess, day or half or so far at the
conference? Well, I have enjoyed it personally. I think there are a lot of really interesting
talks and some that I would have liked to go to but that were just filled up by the time I got there.
There was one on what to do if you don't have a lot of data because there are some ways around
that I gather and something that a lot of people are starting to talk about as well as the use of
unsupervised learning. I don't remember the details but I ran into this really interesting
paper where they started so they wanted to train some kind of image analysis, object detection,
something method. I don't recall the details. The insight that they had was that well, how do
children learn to associate shapes that sort of belong together? So if a child sees a cat the first
time, it may not know that the legs and the head and sort of everything goes together into being a cat.
But then when the child sees the cat move a lot, it sort of realizes, okay, so this is the cat
and these parts belong together. So this is one object. So this is essentially learning how to
detect a whole object. And there is some research that shows that this is indeed how children learn
to recognize shapes or people who regain sight. Like there is actually some research showing that
this is how it might work for humans. So based on that insight, what these authors did was just
show videos to a deep learning model and it did learn to recognize objects in this way.
And then once, of course, once it learned how to recognize objects, then it's easier for it to
learn that, okay, so this object is a cat. This object is a house. So that was a really cool
application of using unsupervised learning to gain momentum. Yeah, like a little bit of a roundabout
way of going around labeling aspect. Yeah, it's pretty neat. And I think that's a, it's great,
you know, some of these research moving forward around, you know, because we've been talking about
big data related, you know, machine learning, right, deep learning. But there is a lot of data
class and balance is a huge, huge thing, right, just that there's some places where there's
just not enough things to detect, you know, how do you handle those things? Yeah, so it's very
fascinating. And I'll say that for me today, it was really fascinating, the keynote for Andrew
Ing, right, kind of very non-traditional, right? You know, he wrote out the white words and I would
recommend a lot of people to check out that because I really felt that, of course, you know, he's
well-informed, suspected, and things like that. But the things that he highlighted, I really felt
they really hit home because, you know, you know, you know, of course, people give great
keynotes and I think they're good. But I think he really said, these are the real things that you
need to do, you need to worry about or look at that are very relevant versus let's just try to
write, you know, increase the hype curve further and further, right? I thought that was very,
you know, that's something that I know I will kind of reference several times watching the video
because he really brought some of the key points out there. Awesome. Awesome. Great. Well,
thanks both of you. Thank you for having us come. Yeah, thank you. You know, long time listener
and finally hearing my voice, you know, it's going to be weird. Thank you so much for having us.
Awesome. Enjoy the rest of the conference. Thank you.
All right, everyone. That's our show for today. Thanks so much for listening and of course,
for your ongoing feedback and support. For more information on Mo and Laura or any of the other
topics covered in this episode, head on over to twimlai.com slash talk slash 54.
For the rest of this series, head over to twimlai.com slash AI SF 2017. And please, please, please,
send us any questions or comments that you may have for us or our guests, be a Twitter,
at twimlai or at Sam Charington or leave a comment on the show notes page. There are a ton of great
conferences coming up through the end of the year to stay up to date on which events will be attending
and hopefully to meet us there, check out our new events page at twimlai.com slash events
twimlai.com slash events. Thanks again for listening and catch you next time.
