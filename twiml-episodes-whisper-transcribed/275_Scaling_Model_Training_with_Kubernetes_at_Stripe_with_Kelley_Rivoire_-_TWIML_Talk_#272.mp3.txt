Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.
And last week we kicked off the second volume of our listener favorite AI platform series,
sharing more stories of teams working to scale and industrialize data science and machine
learning at their companies.
We've been teasing that there's more to come and today I am super excited to announce
the launch of our inaugural conference, Twimblecon AI platforms.
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices
necessary to scale the delivery of machine learning and AI in the enterprise.
Now you know Twimble for bringing you dynamic practical conversations via the podcast and
we're creating our Twimblecon events to build on that tradition.
The event will feature two full days of community oriented discussions, live podcast interviews
and practical presentations by great presenters sharing concrete examples from their own experiences.
By creating a space where data science, machine learning, platform engineering and ML ops
practitioners and leaders can share, learn and connect, the event aspires to help see
the development of an informed and sustainable community of technologists that is well equipped
to meet the current and future needs of their organizations.
Some of the topics that we plan to cover include overcoming the barriers to getting machine
learning and deep learning models into production, how to apply ML ops and DevOps to your machine
learning workflow, experiences and lessons learned in delivering platform and infrastructure
support for data management, experiment management and model deployment.
The latest approaches, platforms and tools for accelerating and scaling the delivery
of ML and DL and the enterprise, platform deployment stories from leading companies like
Google, Facebook, Airbnb, as well as traditional enterprises like Comcast and Shell, an organizational
and cultural best practices for success.
The two day event will be held on October 1st and 2nd in San Francisco and I would really
love to meet you there.
EarlyBurt Registration is open today at twimblecon.com and we're offering the first 10 listeners
who register the amazing opportunity to get their ticket for 75% off using the discount
code TwimbleFirst.
Again, the conference site is twimblecon.com and the code is TwimbleFirst.
I am really grateful to our friends over at Sigopt who stepped up to support this project
in a big way.
In addition to supporting our AI Platforms podcast series and next ebook, they've made
a huge commitment to this community by signing on as the first founding sponsor for the event.
App Software is used by enterprise teams to standardize and scale machine learning experimentation
and optimization across any combination of modeling frameworks, libraries, computing
infrastructure and environment.
Teams like Two Sigma, who will hear from later in this podcast series, rely on Sigopt Software
to realize better modeling results much faster than previously possible.
Of course, to fully grasp its potential, it's best to try it yourself and this is why
Sigopt is offering you an exclusive opportunity to try their product on some of your toughest
modeling problems for free.
To learn about and take advantage of this offer, visit twimblei.com slash Sigopt.
And now on to the show.
All right, everyone.
I'm on the line with Kelly Revoir.
Kelly is an engineering manager at Stripe working on machine learning infrastructure.
Kelly, welcome to this week in machine learning and AI.
Thanks for having me.
I'm really excited to chat.
We got in touch with you kind of occasioned by a talk you're giving at strata, which
is actually happening as we speak.
I'm not physically in SF for it this time, but your talk, which is going to be later today,
is on scaling model training from flexible training APIs to resource management with
Kubernetes, and of course, machine learning infrastructure and AI platforms is a very popular
topic here on the podcast.
And so I'm looking forward to digging into the way Stripe is platforming its machine learning,
processes and operations.
But before we do that, I'd love to hear a little bit about your background and how you
got started working in this space.
Yeah, sounds great.
Maybe I'll say a little bit about what I do now and then kind of work backwards from
that.
So right now, I'm an engineering manager at Stripe, and I work with our data infrastructure
group, which is seven teams kind of at the lowest level things like our production databases
or things like Elasticsearch clusters and then kind of working up through like batch
and streaming platforms, core, like ETL, data pipelines and libraries, and also machine learning
infrastructure.
I've been at Stripe for very close to six years now from when the company was about 50
people and have basically worked on a bunch of different things in sort of like risk,
data and machine learning, both as an engineer and engineering manager and also initially
more on kind of like the application side and then over time moving over to the infrastructure
side.
By training, I'm like a kind of research scientist person, so I studied physics and electrical
engineering in school, did my PhD at Stanford working on nanopotonics and then did a short
postdoc at HP labs on nanopotonics.
Is that a nanopotonics?
Yeah.
I think you had like I get an odds count on recently, which is not too far away, so maybe
that gives you a little bit of an idea.
And then yeah, it was at HP labs for a year, so working on sort of similar things and
also some 3D imaging.
And I guess I like to call what I did, although I don't know that anyone else calls it that sort
of like full stack science where like you have an idea and then you do some theory or modeling
or simulation and then you use that to design a device and then you actually go in the
clean room and like make the device and then you actually go in the optics lab and like
you know shoot a bunch of lasers at your device and measure it and then you sort of like
process the data and compare it to your theory and simulation.
And I was like I found like kind of the two ends the most, like sort of the magical moment
where like you know the data that you collected like matches what you thought was going to
happen from your modeling.
And I kind of decided that I wanted to do more of that and a little less of like fabrication
or material science.
And I was kind of sitting in Silicon Valley and started looking around and like stripe
was super exciting in terms of its mission, like having interesting data and just like having
amazing people.
Awesome, awesome, stripe sounds really interesting but shooting lasers at stuff also sounds really,
really cool.
Yeah, people get really excited when you tell them that.
So that was fun for a while.
Maybe tell us a little bit about stripes, kind of machine learning journey from an infrastructure
perspective.
You know how did it, it sounds like you're doing a bunch of interesting things both from
a training perspective, from a data management perspective, inference, but how did it evolve?
Yeah, I think one thing that's interesting about machine learning at stripe, like I think
a lot of places you talk to machine learning kind of like started out as being for some,
some kind of like offline analytics more like you know internal business questions like
maybe like you're trying to calculate long term value of your users.
And we do stuff like that now, but we actually started like our kind of core uses have always
been very much on kind of the production side, like our kind of most business critical
and first machine learning, machine learning use cases were things like scoring transactions
in the charge flow to evaluate whether they're fraudulent or not.
We're doing kind of like internal risk management of like making sure our users are selling
things that we can support from our terms of service or that they're kind of like good
users that we want to support.
And so we started out from having kind of a lot of these more like production requirements
of it needs to be this fast and it needs to be this reliable and I think our machine
learning platform kind of like evolved from that side, where you know initially we had
kind of like one machine learning team and then even just having a couple of applications
we started seeing like here are some commonalities like everyone needs to be able to score models
or you know even like having some notion of shared features could be really valuable
across just a couple of applications.
And then as we split our machine learning team one piece of that became machine learning
infrastructure, which we've developed since then and you know it's really important
for that team to work both with the teams doing the business applications, which now
include a bunch of other things in our user facing products like radar and billing as
well as internally and also you know it's important for the machine learning infrastructure
to build on the rest of your data infrastructure and really the rest of all of your infrastructure
and we've worked really closely with like our orchestration team on you know as you said
and chatting about my talk like getting training to run on Kubernetes.
Yeah man that's maybe an interesting place to start.
You kind of alluded to the interfaces between machine learning infrastructure as a team
and you know data infrastructure and you know just infrastructure how do they how do they
connect you know maybe even organizationally and how do they tend to work with them up
with one another.
For example you know in you know training on Kubernetes you know where is the line
between what the ML infrastructure team is doing and you know what it's requiring of some
you know broader technology infrastructure group.
Yeah I think the Kubernetes case is really interesting and it's one that's been super
successful for us so I guess maybe like a year or two ago we we didn't really focused
on the kind of scoring like real-time inference part of models because that's the hardest
and we'd sort of left people on their own it's like well you figure out how to train a
model and then you know if you manage to do that we'll help you score it and we realized
that that wasn't like great right so we started thinking you know what can we do and at first
we built some CLI tools to kind of like wrap the Python people were doing but then we wanted
to kind of do more so eventually we built an API and then a big hassle had been the resource
management and we just kind of wanted to like abstract that all the way and as it happened
at that time our orchestration team had gotten like really interested in Kubernetes and I think
they wrote a blog post like maybe a year and a half ago they had kind of just moved our first
application to Kubernetes which was some of our con jobs that we use in our financial infrastructure
and so we ended up collaborating this was kind of like a great next step of a second application
they could work on and you know we had some details we had to work out we had to figure out like
how do we package up all of our Python code and to you know some Docker file we can deploy and
it was really useful to be able to work with them on that but I think we have found really
good interfaces and working with them where you know we wrote a client for the communities API but
it's like anytime we need help or anytime there's management of the Kubernetes cluster they take
care of all of that so it's kind of given us this flexibility where we can define different
instance and resource types and swap them out really easily if we need CPUs or GPUs or we need
to like expand the cluster but we as machine learning infrastructure kind of like don't have to
deal with managing Kubernetes or updating it we have this amazing team of people who are like
totally focused on that restraint so your talk at strata was focused on this area
what was kind of the flow of your talk what were the main points that you are that you're
planning to go through with the audience there yeah great question so we we kind of think about
this in two pieces and you know maybe that's because that's how we actually did it so one piece
was the resource management that I talked about was you know getting getting things to
around on Kubernetes that was actually kind of like the second piece for us the first piece was
figuring out sort of like how should the user interact with things and like where should we give
them flexibility and where should we constrain things and so we ended up building what we call
internally rail yard which is like a model training API and it goes with there's sort of two
pieces there's like what you put in the API request and then there's what we call a workflow
and the API request is a little bit more constrained like you have to say you're meta data for whose
training so we can track it you have to tell us like where your data is like how you're doing things
like hold out just kind of basic things that you'll always need but then we have this workflow piece
that people can write like kind of like whatever Python they want as long as they define a
train method in it that will hand us back like the fitted model and we definitely have found that
like initially we were very focused on binary classifiers for things like fraud but people have done
things like word embeddings we have people doing time series forecasting we're using like
things like psychic learn actually views fast text pytorch profit so this has worked pretty
well in terms of like providing enough flexibility that people can do things that we actually didn't
anticipate originally but it's constrained enough that we can run it and sort of track what's going
on and and give them what they need and be able to automate the things we need to automate do you
think of your users as more kind of the data science type of user or machine learning engineer type
of user or is there a mix of those two types of backgrounds yeah it's a mix which has been really
interesting and I think back to what I said earlier like because we initially focused on
these kind of critical production use cases we started out where the teams users were really
pretty much all machine learning engineers and very highly skilled machine learning engineers
like people who are excellent programmers and you know they know stats in ML and they're
kind of like the unicorns to hire and over time we've been able to broaden that and I think
having things like you know this tooling has made that possible like in our user survey right
after we first shipped even just the kind of like API workflow piece and we were actually just
like running it on some boxes aside car process we hadn't even done Kubernetes yet but a lot of the
feedback we got was like oh this new person started on my team and I just like pointed them to the
directory where the workflows are and I like didn't have to think about how to split all these
things out because like you know you just kind of pointed me in the right direction and I could
point them in the right direction so I think that having having these kind of like common ways of
doing things has been a way to broaden our user set and as our data science team which is more
internally focused has grown they've been able to kind of like start picking up increasingly
large pieces of what we built for the ML engineers as well and we've been like excited to see
that and work with them and so the interface then is kind of Python code and our is the platform
containerizing that code or is the user expected to do it or is it integrated into some kind of
workflow like they check it in and then it becomes available you know to the platform via
check-in or CICD type of process yeah so we still have the experimental flow where people can
like kind of try things out but when you're ready to productionize your workflow basically what you
do is you get your code reviewed you merge it we use we ended up using google subpar library because
it works really well with basil which we use for a lot of our build tooling what are the those two
yeah so subpar is a google library that helps us like package Python code into like a self-contained
executable both the source code and any dependencies like if you're running PyTorch and you need some
CUDA stuff okay and it works kind of out of the box with basil which is the open source version
of Google's build system which we have started to use that stripe a few years ago and have expanded
since it's really nice for like speed reproducibility and working with multiple languages
so this is where our ML info team kind of worked with our orchestration team to figure out the
details here to be able to kind of like package up all this Python code and have it so that
basically almost like a service deploy you can kind of like have it turn into a Docker image that
you can deploy to like Amazon's ECR and then Kubernetes will kind of like know how to pull that down
and be able to run it so the ML engineer or the data scientist doesn't really have to think about
any of that it just kind of works as part of the you know you get your PR emerged and you
deploy something if you need to change the workflow but earlier on in the process when you're
experimenting the currency is a you know some Python code what kind of tooling have you built up
around experiment management and automatically tracking various experiment parameters or hyper
parameters hyper parameter optimization that kind of thing are you doing all that or is that
all on the user to do yeah that's a really good question so one of the things that we added
in our API for training as we found it was really useful to have this like custom prams field
especially because we eventually people ended up and you know we have some shared services
to support this like sort of a retraining service that can automate your training requests
okay and so one of the things that people from the beginning use the custom
programs for was hyper parameter optimization optimization we are kind of working toward building
that out as a first-class thing like we now have like evaluation workflows that can be integrated
with all of this as well and that's kind of like the first step you need for hyper parameter
optimization if you want to do it as a service like what are you optimizing if you don't know what
metrics are looking at right and so that's something we hope to do like over the next you know
three to six months is to make that like a little bit more of first-class support and you mentioned
this this directory of workflows elaborate on that a little bit yeah so one of the nice things is
you know when you're writing your workflow if you put it in the right place then our our
scholars service really are we'll know where to find it but one of the side benefits has also
just been that there is one place where people's workflows are and so that that's been kind of
like a nice place for people to get started and see like you know what models are other people
using or like what preprocessing or kind of what other things are they doing or what what types
of parameters like estimator parameters are they looking at changing to just kind of you know have
that be like a little bit more available to our users are internal users and the workflow
element of this is it is it graph-based is it something like airflow how's that implemented
yeah so in this case by workflow all I mean is just like Python code that you know you give it
like we're actually really hard our API passes to it like what are your features or what are your
labels and then you are Python code returns like here is the fitted pipeline or model and like usually
something like the evaluation data set that we can pass back we have had so we've people have
kind of built us and users like interesting things on top of having a training API
okay so some of our users built out actually the folks working on radar for our product built out
like an auto retraining service that we've since kind of taken over and generalized where they
schedule like nightly retraining of all the tens and hundreds of models and you know that's
integrated to be able to even like if the evaluation looks better like potentially automatically
deploy them we do also have people who have put like training models via our service into like
air flow decks if they have you know some some slightly more complicated set of things that they
want to run so we're definitely seeing that as well and you've mentioned radar a couple of times
is that a product that's right we're an internal project yeah radar is our like user facing fraud
product it runs on all of our machine learning infrastructure and you know every charge that
goes through stripe within usually a hundred milliseconds or so we've kind of like done a bunch
of real-time feature generation and evaluated like kind of all of the models that are appropriate
and in addition to sort of the machine learning piece there's also a product piece for it
where users can get more visibility into what our ml has done they can kind of like write their own
rules and like set block thresholds on them and there's there's sort of like a manual review
functionality so they're kind of some more product pieces that are complimentary to the underlying
machine learning just trying to complete the picture here you've got these workflows which are
essentially Python they expose a train entry point and you you mentioned this directory of
workflows is that like a directory like on a server somewhere with just like dot py files or is
that are they do you require that they be versioned and are you kind of managing those versions
yeah so that that's just like actually like in a code basically so that's like yeah the workflows
live together in code as part of kind of our training API it's like when you submit here's my
training request which has you know here's my data here's my metadata this is the workflow
I want you to run we give you back a job ID which then you can check the status of you can check
the result the result will have things in it like what was the getcha and so that's like something
that we can track as well got it so you're submitting the job with the code itself as opposed to
a getcha so I guess it depends a little bit which workflow you're running through like in the case
where you're running on Kubernetes you've merged your code to master and then we kind of
package up all this code and deploy the Docker image and then from there you can kind of make
requests to our service which will run the job on Kubernetes so at that point your code it's
you know whatever's on master for the workflow plus whatever you've put in the request
let's talk about where the the data comes from for training and what kind of
you know platform support your offering folks yeah that's a really interesting question
kind of within the framework of like what do you need for a like really RDPI request we support
two different types of data sources one is more for experimentation which is like you can kind of
tell us how to make this equal to query the data warehouse and that's kind of nice for experimentation
but not so nice for production what pretty much everyone uses for production is the other data
source we support which is parquet from s3 so it's like you tell us you know where to find that
and what your future names are and usually that's generated by our features framework that we call
semblance which is basically like a DSL that helps you know gives you a lot of ways to write complex
features like think have things like counters be able to do things like joins do a lot of transformations
and then you know the ML infrastructure team figures out like how to run that code in batch if you
are doing training or like there's a way to run it in real time basically and kind of like a
Kafka consumer setup but you only have to write your code feature code like once
is it the user that's only writing a feature code once are you going after kind of sharing
features across the user based to what extent are you seeing shared features yeah that's like a
really excellent question um yeah so the user writes their code once and like also I think having
a framework similar to the training workflows where people can see what other people have done
has been really powerful um so we do have people who are like definitely kind of sharing features
across applications and there's there's a little bit of a tradeoff like it's like a huge amount
of leverage if you don't have to rewrite some complicated business logic um you do have to manage
a little bit of making sure that um you know everything is versioned and that you're paying attention
to like not deprecate something someone else is using and that you're not like just like changing
a definition in place that you are kind of like creating a new version every time you are changing
something right so there's a little bit more management there and hopefully over time we can
improve our tooling around that but I think it's you know even even since before we had a feature
framework like being able to kind of share some of that stuff has been like hugely valuable for us
is the features framework is that a set of APIs or is that uh kind of a runtime uh thing like
what what exactly is it yeah there's kind of two pieces so um which is basically sort of what you
said like you know one is more like the API uh um like what are what are the things we you know
let users express and one thing we tried to do there is actually constrain not a little bit
so we like you have to use events for everything and we don't really let you express notions of time
so you kind of can't mess up that time machine of like what was the state of the features
at some time in the past where you want to be training your model we kind of like take care of that
for you um so that's kind of one piece and then yeah we kind of compile that into like an AST
and then we use that to essentially write like a compiler to be able to run it on different
backends um and then we can kind of like you know write tests and try and check um at the framework
level that that things are going to be as close as possible to the same across those different
backends so back end could be um something for training where you're going to materialize like
what was the value of the features at each point in time in the past that you want as inputs to
training your model um or another back end could be like I mentioned we have kind of this
Kafka consumer based back end that we use like for example um for radar to be able to like evaluate
these features like as a charge is happening uh and so to what extent you find that limitation of
everything being event based gets in the way of what folks want to do yeah that that's a really
good question to um it's definitely was originally a little bit of a paradigm shift for people
they're like oh I just want to use this thing from the database right um but we found that actually
it's worked out pretty well and that especially when you have users who are ML engineers like they
do really understand the value of like why you want to have things be event based and like the
sort of gotchas that that helps prevent um because I think everyone has their story about how you
were just looking something up in the database but then you know the value changed uh and you didn't
realize it so it's kind of like you're leaking future information into your training data and
then your model is not going to do as well as you thought it did so like I think moving to a more
event based world and I mean I think in general shape has also kind of been doing more streaming work
and um more having like good support also as as uh at the infrastructure level with Kafka has been
really helpful with that and so does that mean that the models that they're building need to be
aware of kind of this streaming paradigm during training uh or do they get a static data set
to train? Yeah so basically um you can kind of use our future's framework to just generate like
par k and s3 that has materialized like all of the information you want of what was the value of
each of the features that you want at all the points in time that you want and then yeah your input
to the training API is like please use this par k from s3 um we could make it a little more seamless
than that but that's worked pretty well and par k is just like a serialized like a file format
yeah it's pretty efficient you know I think it's used in a lot of kind of big data uses um you can
also do things like predicate pushdown and we have like a way in the training API to kind of
specify some filters there um to just kind of like save save some effort uh use a predicate pushdown
yeah so if you know you only need certain columns or something like you know you can you can load it
a little bit more efficiently and not have to carry around a lot of extra data got it okay the other
interesting thing that you talked about in the context of the this event base framework is the
whole um you know time machine is the way you said it kind of alluding to the the point in time
correctness of uh you know feature snapshot can you elaborate a little bit on um did you
did you start there or did you evolve to that that seems to be in my conversations kind of uh
I don't know maybe you'd like one of the the cutting edges or bleeding edges that people are
trying to deal with as they scale up these um these data management systems for features
yeah for this particular project um in this version we started there straight previously
had kind of looked at something a little bit related a couple years before um and in a lot of ways
we kind of learned from that so we ended up with something that was more more powerful and sort
of solved some of these issues at the platform level um we did you know at that point we had been
running machine learning applications in production for a few years so I think everyone has
their horror stories right of like all the things that can go wrong um especially kind of
out of correctness level and like everyone has their story about like re-implementing features
in different languages which we we did for a while too and kind of like all the things that can
go wrong there so um yeah I think we we really tried to learn from both like what are all the
things we'd seen go well or go wrong in individual applications and then also from kind of like
our previous attempts um at some of this type of thing like what what was good and you know what
could still be better and out of kerosene what do you use for data warehouse and are there multiple
or is it is there just one um we've used a combination of redshift and presto um over the past
couple of years um yeah they have a little bit of sort of like different abilities and strengths
and those are those are things that people like to use to experiment with machine learning although
like you know we generally don't use them in our production flows because we kind of prefer the
event based model and so is the event based model parallel or orthogonal to to redshift or
presto is there or is it a front end to either of these two systems yeah I guess we have we
actually have a front end that we've built for redshift and presto um you know separately from
from machine learning that's really nice and lets people like um you know to the extent they have
permissions to do so like explore tables or put annotations on tables um we haven't integrated
our in general I would say we could do some work on our UIs for for ML stuff we've definitely
focused more on the back end and in front API side although we do have some things like our
auto retraining service has a UI where you can see like um what's the status of my job like was
it you know did it finish um did it produce a model that was better than the previous model
I think I'm just trying to wrap my head around the event based model here you know as an example
of a question that's coming to mind uh in an event based world are you regenerating the features
you know every time and if you've got you know some complex feature that involves a lot of
transformation or you have the backfill of ton of data like what does that even mean in an event
based world where I think of like you have events and they go away uh is there some kind of store
for all that that isn't redshift or presto um well whenever we say event you know we're publishing
something to Kafka and then we're archiving it to s3 okay then that persists like you know as long
as we want it to um in some cases basically forever um and so that is available we do do end up
doing a decent amount of backfilling of kind of like you know you define the transform features you
want but then you need you know you need to run that back over all the data you'll need for your
training set that's something that we've actually done a lot of from the beginning partly because of
our applications like when you're looking at fraud um you know the way you find out if you were
right or not is that like in some time period usually within 90 days but sometimes longer than that
the cardholder decides um whether they're going to dispute something as fraudulent or not um and
that's compared to like you know if you're doing ads or trying to get clicks like you kind of get
the result right away um and we you know so I think we've always like been interested in kind of
like being able to backfill so that is you know you can log things forward but then it's like you'll
probably have to wait a little bit of time before you have enough of a data set that you can train
on it cool so we talked about the data uh side of things we talked about training and experiments
how about inference yeah that's that's a really great question and that's that's kind of like the
first thing that we built infrastructure support for um at first a decent number of years ago like
I think even before things like TensorFlow were really popular and so we have um like our own
scholar service that we use to do our production real-time inference um and you know we started out
especially because we have like mostly transactional data we don't know a lot of things like images
at least as our most critical applications at this point um a lot of our early models and even
still today like most of our production models are kind of like tree based models like initially
things like random forest and now things more like xg boost and so you know we've kind of like um
um we have the serialization for that built in to our training workflows and um we've optimized
that to run pretty efficiently in our scholar inference service and then we've built some kind of
nice layers on top of that um for things like model composition kind of what we call meta models where
you know you can kind of like take your machine learning model and um kind of like almost like
within the model sort of compose something like add a threshold to it um or like for radar we train
you know some array of like in some cases user specific models along with like maybe more of
some global models and so you can kind of incorporate in the framework of a model um doing that
dispatch for your kind of like if it matches these conditions that score with these models otherwise
score with this model and like here's how you combine it um and then the way that interfaces with
your application is that each application has uh what we call a tag and basically the tag points
to the model identifier which is kind of like immutable and then whenever you have a new model
you're ready to ship you just like update what does that tag point to um and then you know
print in production you're just saying like score the model for this tag I think that's pretty
similar to like you know if you read about uber's Michelangelo and things like that sometimes
we're like oh we all came up with the same thing yeah I think that like a lot of people have kind
of come up with some of these these ways of doing things that just kind of make sense yeah it
also sounds a little bit like uh some of what uh seldom is trying to capture in the kubernetes
environment um uh which I guess brings me to is the inference running in kubernetes or is that
a separate infrastructure it's not right now but I think that's mostly like a matter of time
and prioritization um like the first thing we move to kubernetes was uh the training piece because
the workflow management piece was so powerful or sorry the resource management piece was so powerful
like being able to swap out CPU GPU high memory we've moved some of our the sort of real-time
feature evaluation to kubernetes which has um been really great and made it like a lot less
toil to kind of deploy new feature versions at some point we will probably also move the
inference service to kubernetes we just kind of haven't gotten there yet because it is still some
work to do that and is the uh the inferences happening on AWS as well and are you using kind of
standard CPU instances or are you doing anything fancy there yeah so um we ran on cloud for pretty
much everything and um definitely use a lot of AWS for the real-time inference of the most sensitive
like production use cases um we're definitely mostly using um CPU and we've done a lot of optimization
work um so that has worked pretty well for us um I think we do have some folks who've kind of
experimented a little bit with like hourly or batch scoring um using some other things so
I think that's something that we're definitely thinking about as we have more people productionizing
kind of like more complex types of models where you know we might want something different
you mentioned a lot of optimization that you've done is that uh on a model by model basis or are
there uh platform uh things that you've done that help optimize across the various models that
you're deploying uh for inference yeah definitely a lot of things at the platform level like I think
the first models that we ever ever scored in our inference service were serialized with YAML
and they were like really huge and um they caused a lot of garbage when we tried to load them
and so like we did some work there for kind of tree-based models um to be able to load things
from disk to memory really quickly and like not producing much garbage um so that that kind of
thing are things that we did especially kind of like in the earlier days what are you using for
querying the models are you doing rest or grpc or uh something altogether different
yeah we use rest right now I think grpc is like something that we're interested in um but we haven't
done yet is all of your all of the inference done via um kind of via rest and like kind of
microservice style or do you also do uh more I guess embedded types of uh inference for like where
you need have super low latency requirements does rest kind of meet the need across the application
portfolio yeah um even for our most critical applications like shield things have worked pretty
out one other thing our orchestration team has done that's worked really well for us is um
migrating a lot of things to envoy um so we've seen some some things where like we didn't understand
why there was some delay like in what we measured for how long things trick versus like what it
took to the user there's just kind of one away as we move to envoy and what is envoy
envoy is like a service service networking mesh that was developed by lift um and is kind of like
an open source open source library and so it handles a lot of it can handle a lot of things like
service to service communication the inference environment is it doing absent of Kubernetes all the
things that you'd expect Kubernetes to do in terms of like auto scaling and um you know load
balancing across the different service instances or is that stuff all done um statically um we take
care of the routing um ourselves and we also at this point have kind of like charted our inference
service so not all models are stored on every host so that you know we don't need hosts with like
infinite memory and so that we take care of ourselves um the scaling we is not fully automated
at this point we do we have kind of like quality of service so we have like multiple kind of clusters
of machines and we tear a little bit by like you know how sensitive your application is and what
you need from it um so that we can be a little bit more relaxed with people who are developing and
want to test and not have that like potentially have any impact on more critical applications um but we
haven't done like totally automated scaling that's something we kind of still look at a little bit
ourselves uh so if you were kind of just starting down this journey uh without having done all the
the things that that you've done it's right where do you think you would start if you just
you know you're you're at an organization that's kind of increasingly invested in or investing in
machine learning and you know needs to try to you know gain some efficiencies. Yeah I mean I
think if you're just starting out like it's good to think about like what are your requirements
rate um and you know if you're just trying to iterate quickly it's like do the simplistic
thing possible right so you know if you can do things in batch like great do things in batch um
I think a lot of there are a lot of both open source libraries as well as manage solutions um like
on all the different cloud providers so I think you know I don't know you know if you're only one
person then I think that those could make a lot of sense also for people starting out because I
think one of the interesting things with machine learning applications is that um it takes a little
bit of work like usually there's sort of this threshold of like your modeling has to be good enough
for this to be like a useful thing for you to do like for fried detection that's like if we can't
catch any fraud with our models then like you know we probably shouldn't have like a fraud detection
product um so I think it is useful to kind of have like a quick iteration cycle to find out like
is this a viable thing that you even want to pursue and if you have an infrastructure team they
can kind of like help um lower the bar for that but I think there are other ways to do that
especially as you know there's been like this Cambrian explosion in the ecosystem of different
open source platforms as well as different managed solutions. Yeah how do you how do you think
in an organization knows when they should have an infrastructure team?
ML in particular. Yeah I think that's a really interesting question um I guess uh in our case I
think um you know the person who originally founded the machine learning infrastructure team
um had worked in this area before at Twitter and kind of had a sense of like this is going to be
a thing that we're really going to want to invest in given how important it is for a business
and also that if you don't kind of like dedicate some folks to it it's easy for them to kind of
get sucked up in other things like if you just have data infrastructure that's undifferentiated
so I think it's a really interesting question there probably is this business piece right of like
what are your ML applications like how critical are they to your business and like how difficult
are your infrastructure requirements for them as well I think a lot of companies develop their
ML infrastructure like starting out with things like making the notebook experience really great
because they want to support like a lot of data scientists who are doing a lot of analysis
and so that's like a little bit of a different arc from from the one that we've been on and I think
that's like actually a pretty business dependent thing. Okay awesome awesome well Kelly thanks so
much for taking the time to chat with me about this really interesting uh story and I've enjoyed
learning about it. Cool um thanks so much for chatting really enjoyed it. All right everyone that's our show
for today for more information about today's guest or to follow along with AI platform volume 2
visit twimmelai.com slash AI platforms 2 make sure you visit twimmelcon.com for more information
order register for twimmelcon AI platforms thanks again to sig up for their sponsorship of this
series to check out what they're up to and take advantage of their exclusive offer for twimmel
listeners visit twimmelai.com slash sig opt as always thanks so much for listening and catch you next
time
