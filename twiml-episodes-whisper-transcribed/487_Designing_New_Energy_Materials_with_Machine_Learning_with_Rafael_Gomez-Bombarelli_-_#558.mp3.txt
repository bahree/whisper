All right, everyone. I am here with Rafa Gomez-Bambarali. Rafa is an assistant professor at MIT in the Department of Material Science and Engineering.
Rafa, welcome to the Twomol AI podcast.
Thanks so much. It's great to have you on the show and I'm looking forward to our discussion.
We will be digging into a topic that of course is near and dear to your heart.
But one that you also presented recently at the SIGAP summit and that talk in particular was called designing new energy materials with machine learning.
But before we jump into that topic, I'd love to have you share a little bit about your background and how you came to work at the intersection of ML and materials.
Yeah, thanks very much. So I was trained a chemist. I did my undergrad studies back in my home country in Spain.
And I started working and doing research in the lab. I broke all the glassworm and it was decided that I have to have to be a theorist because I just couldn't be in a lab.
I did simulations. So I was using, you know, the loss of physics and a technique called the city functional theory to simulate what a material would do before you have to make it.
And after that, I came over to the States. I was at Harvard for a couple of years with working with Alanis.
Learning how to do, you know, 100,000 simulations instead of one, right? So stepping our own big data.
And at that point, we realized, you know, this was 2015 and we realized that we didn't have physics problems anymore.
We just had data problems. I would have this Oracle that could tell us whether a material was good or not.
And we just had to navigate this high dimensional design spaces, right? And since then, and after I prefaced it in industry and coming to MIT, this has been what we tried to do, right?
How do we interface this disability to predict with physics, right? Very rigorous.
It can extrapolate just expensive and the ability of machine learning, you know, you be used as a surrogate for the physics or as a way to invent new materials.
And that's what I've been doing for the last few years.
You mentioned density function of theory. What's that all about?
It's a simulation technique based on quantum mechanics.
So it's very popular. It's as accurate some of the most expensive simulation techniques, but, you know, tractable. So it scales for the computational complexity people out there.
It goes with N cube. So the size of your problem that the cost you pay is the cube of the size of your problem when we get to quantum computers.
It will be obsolete. But in the meantime, it's kind of the best trade off between accuracy and speed.
And by the way, it was this beautiful D-Mine paper in applying machine learning to the operation of DFT just a few weeks back on science.
I think another exciting place for the community to be interfacing machine learning and physics.
You mentioned that DFT scales with the cube of the size of the problem.
What characterizes the size of the problem or more broadly, how do you formulate the types of problems that you're trying to solve for simulation and for DFT?
The size of the problem is actually the number of basis functions one uses to represent electrons. But at the end of the day is the number of electrons in your system, right? And the number of electors in your systems is how many atoms.
So we're an atom crowd. We tried the nature of a problem atom by atom.
I think that typically we're limited to asking questions of systems with tens to 200 of atoms.
And maybe let's take a step back and have you take us deeper into the specific problems that you're trying to solve. Ultimately, you're trying to design new materials. Is that right?
Exactly. And there is fundamentally two ways in which we think about designing no materials.
One is what we call virtual screening, which is enumerating a long list of, you know, anything that could potentially work based on intuition, maybe things we extract from patents or maybe, you know, we knock on our collaborators door and doesn't to throw some molecules for us.
And we end up with a list, a closest space of candidates we want to evaluate. Right. And in this case, we can evaluate them with experiments or we can evaluate them with simulations.
Experiments are extremely expensive simulations are, you know, less expensive.
And then name of the game is how do we build these proxy functions that are as good as these oracles as this simulation or as this experiment, but are much more much cheaper.
Right. And as robust as possible, so we can apply them over all our design space consistently and in a trustworthy fashion. Right. And then we set up active learning loops where we go hand for the data that the model really needs based on uncertainty.
So we can quantify the uncertainty that this model has on its own predictions and find those points that, you know, the model is not very certain about maybe good, maybe bad. It doesn't really know. So we go add them to the pool and set up this active learning loops to screen over these finite spaces.
And the other way to think about design, which is sort of flipping the problem, right, rather than being given a list and having to walk one by one over every member of that list. And if it's good or bad for the property we want is using inverse design techniques where we flip this function.
So having a function that goes from structure to property, which is the way one typically thinks about materials, we try to make the reverse function a function that goes from the property you want to any structure any material that could realize that and this connects with the machine learning techniques such as generative models.
And the two precisely that right they invent new members of a distribution. People do this with celebrity faces or text and we do it with crystals and molecules.
Got it got it to be clear machine learning plays a role in both of these directions in the virtual screening you're using machine learning to essentially you take the place of experiments to create a proxy or set of proxy functions that you can apply to the candidate materials instead of actually put that putting them through an experimental process to validate them.
And the inverse design it's trying to predict the card predict the materials that will ultimately deliver these properties you want exactly so it's two sides of the same coin but the course what the models need to do is very different.
And there is a unique set of challenges in each way and because predicting properties from a structure so screening is an older task is a more established way of thinking the tools that are fairly established and people have spent quite a bit of time in the last few years thinking about representation learning for materials right this is this the key here.
You know text has its own representations and machine learning models have been made that are really good for reading text images are their own representation and very good architectures have been made for images for pixels.
And then the nature of the materials we're looking at is atoms right this goes back to what what is the ultimate bit of information in our system and his atoms and is how they're connected to one another.
So there's been a lot of work in making architecture for atoms and molecules and materials and graphical networks are the key winners so the chemistry and the materials community have been big contributors to the fundamental algorithm development in graphical networks because it's the workhorse that we need to read matter right this is our representation to read matter and it has all the right.
So there's a lot of different environments that we need to put matter into an algorithm but for writing it's very different writing matter turns out to be really hard and there is still open questions about what is the best architecture to write to write graphs right it's okay to write sequences but it's not as easy there is there's a number of
differences that are hard to control for in writing matter so we're very excited about that second part given these two approaches is your work focused on inverse design or is it shifting to inverse design do you see
as kind of this historical you know workhorse of the field but you know it's it's dated or will they coexist and you actively working on both.
In terms of screenings of for instance we just had a very exciting paper where we went back to a class of materials called zeolite the nanopolose materials yeah zeolite.
It's actually your cut litter is this is what that's called the sound that you use for for your cut that's a natural class of zeolites right it's a material that's out there that people can extract from the earth and we can also make synthetic ones that are more sophisticated but they're all in the same class of nanopolose materials materials have these small pores that fit and the thing you want
you're using catalysis they're using oil and gas for instance and they're using biomass conversion and sustainable catalysis and and we've moved the needle in that field by doing a lot of screenings so we set up our forward functions and just executed hundreds of thousands and millions of simulations at a scale that was you know enabled by new technology
that's a very much forward problem right so a place where we just went through a long list and and we were able to invent a new material that seems really good to for for a card exhaust for clean up the the fumes coming out of cars or from.
I thought you said cat exhaust for a second no that's good no no this is this for diesel.
So diesel engines need to need some treatment on the flue gas that that this material could be very good for and so that was that was you know state of the art but going through a long list but of course now we're okay what else is out there so typically in our experience a before we can formulate the inverse problem which
you really will need to have formulated the forward problems I will you know it's unlikely we're going to be able to invent out of the box before we master what's inside the box.
So we see the more as a sequence as a one two of exploring what's the bounded universe that people can think of and then going to sort of the unbounded universe of what people haven't thought of yet.
And so it's it's less because the techniques that you apply in the inverse design phase rely on specific artifacts that you created in the forward phase but
rather more about just the understanding that you've developed in going through that forward phase or is it both.
I think it's a little bit of both because you know just the data for the for the inverse model will need to have come from from forward model or from the same data and enable a forward model.
So the architectures and like I said the architectures for forward problems are very well developed you know in this field graph neural networks are really accepted as a good surrogate for almost anything you could simulate.
You could train up a graph neural network to replicate and that will be trustworthy and consistent.
In venting no matter dreaming up no matter it's actually hard there is a lot of subtleties that we haven't really been able to embed into algorithms in a transwardly fashion yet.
And we had recent work for instance with peptides in the therapeutic space inventing new peptides to help deliver new drugs into the cell to the place where they need to go.
And this was a place you know where inversely has worked nicely and because it's not too hard of a space to take steps on right so imagine inventing a new phase I requires an algorithm to figure out that you know you need to have two eyes that need to be at the same height and the nose and two years.
So there is there's a bunch of underlying rules about how to invent in each space and the small molecular space where many of new drugs are.
It has turned out to be really really hard and computer scientists you know there is there is sessions at every computer science a machine learning conference now about how do we invent new drugs with machine learning because it turned out it was harder than than we originally thought.
You mentioned that in the inverse design problem there are in variances that make that difficult can you elaborate on on those in variances.
I'm going to plug here my good colleague Tess Smith who started here this year at MIT who is you know the real expert about how to handle symmetry and and equivalent nor networks which has been a key part of representation learning like I said going in into the forward models.
As I said materials are at the end of the day 3D arrangement of atoms so we typically will have we be asking of the model to write a 3D arrangement 3D points cloud which is by necessity translationally and rotational invariant does not too hard.
But it's also permutationally invariant it doesn't matter which atom you call one which atom you call 27 right nature doesn't have little tax attached to each atom saying that they're one or two but most machine learning algorithms.
That right sequences anything to write as a sequence will have some canonical ordering right there is on order your readout algorithm is going to give.
And it turns out right that then we need to somehow figure out how to take into account this permutation invariance over over atom indexing and it has been hard is there is no immediate answer you know the the easy answer is to do data augmentation and just throwing the in factorial permutations into the your training data and then your model will you know the brute force to learn how to do permutation invariance just by example.
But of course it's you know it's factorially bad and so there is you know architectures and maybe growing graphs are of course graph course graining right where you constrain you keep a little bit of information such that the connectivity is between this this course grain particles span a basis to grow the rest of the of the architecture of the molecular structure.
So there is a number of sort of up and coming solutions but there is no immediate way to deal with the permutation invariance on the on the generative models.
So what degree are the generative models also graphical models or they do you do you lose the benefit of graphs when you're working in the generative domain.
Exactly. So in principle this is a place where materials and molecules diverge slightly typically in materials is not immediately obvious what the graph is who's connected to whom is a little bit more arbitrary when people you know and when you're looking at a steel there is no clear chemical bonds in a way that it immediately can be interpreted as a graph.
So it's a little bit more on the point cloud and there was this this beautiful paper just a few weeks ago from a txia who's just moving to Microsoft it's anybody out there.
And that knows tian is a shout out where he is creating three the materials through gradient fields so he's moving around the atoms and finding the places where they work best in three dimensions.
And the molecules are typically seen as graphs so typically what solid materials are not graph or not seen as graph molecules are typically seen as graphs meaning that you have you know the chemist when a chemist goes to the whiteboard I don't have any molecules on my whiteboard but when a chemist goes to a whiteboard they can't draw a molecular graph.
So in any case writing out graphs turns out to be really hard anyway because a same that have the same problem there is still a permutation invariant.
But the architecture that we have to deal with with this variance somehow so either you know molecules and materials and solids are slightly different in terms of how graph of 3D they are.
And of the day they both suffer from this complexity of permutation invariance so it's a general problem in creating the generative models for inverse design are you using the same types of models that you'd use for celebrity faces,
kind of thing is it just that the data the training data is different or are there tweaks to the models and architecture that you use for this particular problem, how dramatically different are they from you know what you might use in other generative domains.
So the first generation of generative models from a few years back they mostly co-opted text based generative models for the reason that there is this hacky way to represent molecules as a string.
So there is a notation that has been used in the late 80s where a molecule can be written as a text string.
It's a permutationally complex problem because there is you know n factorial ways to order the atom so it hasn't solved the permutation invariance at all.
But it's very easy to move over to text based tools so you know back in I think 2016 we use variational auto encoders based on an inspired by a bowman paper on continuous and generating sequences from a continuous space and folks have made next character prediction so making a molecule as a text string writing one letter at a time just you know the model looks back at what it's written so far and keeps going.
And then the natural is next step was to apply transformers to this representation so we've co-opted and text based models for generation but we've been operating on a you know imperfect representation this string is not what a molecule.
And so we've we've hacked the representation in order to leverage nlp and then there's been some progress on using a more than 80 representation at writeout but like we say we deal with the challenge of writing graphs and there are examples for instance there is rational or being the block based models for instance the the language of chemistry typically involves substractors that are discrete and well understood and they're always the same.
And the analogy could be writing text letter by letter or word by word right so there is there is if you're going to write a molecule you could write it atom by atom or you could write a functional group by functional group and that's how chemists have sort of them for a long time so it has made sense for folks to make this hierarchical models that and rather than trying to write every atom every every individual atom in the molecule
have the ability to write the whole sets of atoms collectively as well I want to draw a benzene ring right now driving to draw explicitly the ten atoms that make a benzene ring so it's it's combinatorically better right because you don't need to compound all these right choices in a probabilistic manner.
And so you've mentioned the various ways to co up the text based generative models have there been any efforts to do the same with the more visual generative models not for writing molecules so really there hasn't been an obvious connection typically the resolution of pictures people are interested in
100 by 100 pixels right there is tens of thousands hundreds of thousands of decisions to be made on a pixel by pixel basis and maybe you use a can for hyper resolution compared with a VA for the underlying or a flow model that you know there are a little bit more rare.
But at the end of the day we need to write you know I said tens to hundreds right we're going to have to write hundreds of tokens for a molecule so really the tools that that operate for images have haven't really been applied to writing molecules there is there is this interesting problem that has come up in the next in the last few I would say months, which is how do we use a machine vision to reading hundred and molecules.
We know when a professor goes and on the white board or for some chemists was writing in the lab notebook in you know 10 years ago.
This compound is really helpful for headaches right and that's in a paper notebook somewhere.
How do you set up a algorithm that will look at that page at that hundred and molecule and transform it into a computer readable representation like this strings I was talking about like that for something.
The machine learning and the machine vision tools have been applied are just been applied these days in chemistry which is you know read hundred and molecules from from folks's notebooks so that that's a state of the art place where you know segmentation and all these tools are being taken over to chemistry and someone's collected a data set of handwritten lab notebooks.
Exactly. You know I think most there was this Kaggle not that long ago set up by a chemical company if I recall correctly precisely for this right they realize they have all these data in the notebooks.
So they've labeled a fraction and they fast the community to make the algorithms to go you know label everything else.
It's one of those instances where it's worth to buy the bullet and just annotate your own your own training data.
And this is the one problem we don't have with our physics is that we always have this oracle to go to writing and this is a part of the part where bootstrapping our own data has been so satisfying in the type of research we do.
So the thing is that we don't need to go to this mechanical to get annotations right so with simulations we know you know what it's a finite cost we know how much effort it takes to get new data and it doesn't involve any operator time to get more data.
That with regards to the simulation and data augmentation that you you know that's one way to get around the permutation invariance but then you have this kind of factorial effect is that the factorial effect on the computer on the simulator or is it does it show up elsewhere.
That would show up in the number of rearranged versions of the same data you need to show to the algorithm right in in order to be confident that an algorithm has learned that it doesn't matter which order the atoms are up right if you need to teach you know that you know one to 16.
It doesn't matter what the order is in principle in order to be certain you need to show the model the 16 factorial combination right so you take your own training data and rearrange it such that the same molecule is represented by a different indexing.
And again people do this in machine vision all the time with the invariance is that the model should hold right that's what folks you know rotate zoom a little bit of noise so this this type of approach is the analogy in chemistry would be to rearrange the order of the atoms such that the model learns to be.
When you're doing the simulation are you are these kind of hand coded simulations that you're building typically are using simulation packages or tools.
When we go and do you know a hundred thousand simulations of some process are we typically rely on these very well established codes and these very well established algorithms for a number of reasons one is that you know just like Facebook one percent incidents is still tens of millions of problems.
We need a lot of lines in the present success of what we do right we need 99.9% success of something of that scale so we need this very robust codes and this very robust algorithms that are time tested.
And the second question is also around the social aspects of you know credibility if we're going to invest and choose one level of theory one simulation package one choice to build machinery on top right we're going to make for instance we made a data sets using that produce 30 million unique molecular structures and we put it out there for folks to have a gold standard for molecular structure.
And it needs to be something folks would accept if you know if it's something we just you know concocted at home and then we go out there and try to convince folks that it was really the best possible physics we could have.
There is a social aspect to in making sure that we're using time tested well accepted underlying ground truths to then build the machine learning upon.
And to go a little bit deeper in terms of the relationship between the simulation and the modeling can you talk about how the simulation results drive the modeling effort yes there is there is a you know there is this long line that goes from all physics and all data science to all data science or all machine learning and no physics.
So this is what we try to you know exploit or find good opportunities to make a difference.
So for instance and there is a simulation technique that's called course graining which consists of you know instead of simulating your system at full resolution and keeping track of where every atom is you decide that you know some atoms we're going to move collectively it doesn't really matter how they vibrate and oscillate with respect to one another.
And you just need to track them collectively right so that's very effective simulation technique it allows you to be faster for for a number of reasons why there is fewer particles to track.
But also when you propagate your system forward in time you can you can integrate for longer time steps so you can only take snapshots of your system at longer times because there aren't really any fast motions anymore because they were suppressed by by.
Coalescing this this particles into beats and so that's a place where we found machine learning was really helpful because there is a number of decisions to be made there is two decisions to be made in course graining the first one is which atoms should go being the same beat right and sometimes it's obvious but sometimes it's you know it's not immediately clear what the best low dimension of representation is and then machine learning has been great.
And learning low dimensional representation so this is a problem where we use auto encoders to take all atom systems and learn reduce dimensionality representations that then we do physics on right so we just.
We learn what they and loss of physics are that drive the evolution of the course grain system right and the evolution the core grain system will still ball and the what's called Hamiltonian dynamics and so we need to learn what the energy is what the relationship is between the velocities and the energies of of this course grain particles and that again that's a supervised task where we make the course grain system reproduce the behavior.
Of the all atom system right so this is a place where we replace two hard problems deciding what to compress into which beats and what is the interaction potential in the course grain representation both of which are sort of painful and not easily solved by classical techniques.
Using an auto encoder using a data driven approach and that learn both the optimal compression and also the optimal interaction potential and just in these last few weeks we've also made the last step of the tool which is generative which is to go get full resolution out of the course grain system right so there is some information that we threw away when we compress our system into the course grain representation.
And and then we can use a fully generative approach based on an equivalent yeah an equivalent strategy to up resolve right like like folks do with images and when they do a super resolution this would be an another was chemical super resolution well we start with these beats that eight up the full result atoms.
And stochastically and following the right statistics recover the full atom representation of the system right and this will allow us to simulate proteins faster not just you know alpha fall has rather solved the static structure of proteins right but the training data and the task that alpha fall souls is the structure of proteins at cryogenic temperatures not the.
So this is a type of technique that would allow one to simulate the dynamics of proteins and much lower computational cost I think what i'm hearing is that there's across your your work there's this.
So the relationship between how simulation is used and how machine learning is used and when you talk about simulation you're typically talking about it in the context of like it is informing the physics understanding like it is the thing that you're using to to understand how the molecules are behaving.
It's kind of forward or reverse approach and I think part of what what drove the last question and maybe I ask it more directly is are there any ways in which you're using simulation that is more like what you might think of in reinforcement learning where the simulation is the training or as part of the training of the model.
So I agree definitely with your with your first point right the way we we're thinking about we think about physics you know as the.
It gives us this really transverted rule right we need zero data once we have a lot there is there is you know it will hold up.
It's universal right we get some universality that is really appealing because it means that we can be a lot more scarce and as part on the data side because we know you know there's some underlying structure.
In the form of laws that we know how to follow and I'm trying to think examples of reinforcement learning so folks have used reinforcement learning to write molecules and I think the most compelling examples are to do reinforcement learning over what you would do in the lab.
So the operations now rather than writing the molecular say assembly of atoms and who knows you know how one would go put the atoms together in the lab is to use reinforcement learning over the chemical operations that you would do in a lab you know like get this precursor this precursor for the together at this temperature and we make this other intermediate and I will put together with that precursor.
And this is a place where you know it's the trees are wide they're very very many options right there's millions of molecules so there's millions of precursors you could use at any time so it's a very wide tree but it's not very deep there is only you know if a synthesis requires more than six or seven steps it's probably not very good.
So it's slightly different from the way folks typically apply reinforcement learning where they have sort of these really deep decision problems but the number of options are not even time are not that broad so that there is some subtlety about how to transfer those tools but in writing molecules there is some you know inventing new molecules and how to make them at the same time there's been some exciting developments in applying reinforcement learning that's simulating in the in the machine learning meaning what this molecule would do.
As you build it up one of the topics that you covered it at the sigop summit is the way you incorporated hyper parameter optimization optimization broadly in this process can you speak a little bit to that yeah this is something that I think has happened in every every project and has happened again since the summit where there is some.
You know there's some tasks some way we're trying to think about about the interplay of of physics and machine learning where we want to invent something right we want to make a model that you know samples from the Boltzmann distribution instead of having to do a metropolis Hastings Monte Carlo we just want to have you know draw good draws all the time and then drawing lots of bad draws so we make train a machine learning model and you know the first few weeks the students things are not quite working the gradients are not flowing back or but at some point the first model is trained.
The first model is trained we go we run it and it sucks it doesn't do a good job it works but it doesn't work well and every student is like oh my traffic like I was to do all this time you know now you go do hyper parameter optimization and just plug it in and you will get this the set of you know define the widths the heights you know the stopping but there is all these choices that are out there that we don't have we don't want to have to worry about.
You know nine out of ten times after you know a few tens of calls to the sea of interface and tens of experiments turns out things work and this has happened many times where.
Do you think that's is that the way that folks historically think about HBO like I think that you know often folks think about it from the perspective of it's kind of the icing on the cake like you need something that works pretty well
and it'll get you an incremental little boost but what I'm hearing you say is that it takes something that kind of sort of works and actually makes it solid like it helps you bake the cake as opposed to the icing to top off that analogy.
Yes exactly and it might have to do with the fact that these are sort of new tasks either the niche tasks that we don't know we inventing the task and the architecture at the same time right there hasn't been a competition for the last 10 years for folks trying to make the best Boltzmann sampler of atomic configuration right so then we're sort of trying to define the nature of the task and how to score how good we are at it.
And the architecture does it at the same time plus most of the task we're thinking about have some history on the physics side right so then the baselines there there are you know this is.
Pretty easy baseline which is just do what everybody has done for a long time meaning that you know we take our first step and typically we're below the traditional physics space approach you know we try to do our machine learning version and his words than what people's intuition had accumulated to over over the last decade right and then the thing that gets us over the line is HP also in our case I understand we're coming from but in our case it's typically more.
Typically more of a discrete change you know like it this is worse than what people were doing before to then be you know better than what people were doing before by by combining was just the architecture but the right choice of decisions in the hyper parameter i'm maybe inferring a bit or maybe I should ask I would think in the domain that you're working in the.
It doesn't necessarily help you to get you know some unicorn combination of hyper parameters that works on a very specific set of data you can talk about the relationship between HBO and the stability of the models that you're looking to produce and able to produce.
This is this is the Achilles heel of a lot of the machine learning applications to chemistry is that chemical space has turned out to be notoriously hard to interpolate on.
It has proven again and again that you know both for the nature molecules are you know tricky beasts and there are cliffs in you know small changes in chemical structure will resolving large changes in property just because that's the nature of the problem and that's fine.
There is also something that has proven to be really hard to learn this typically machine learning models over over chemistry and other materials really struggle with with domain transfer really struggle to to transfer to other related tasks for instance pre training over language models it's you know everything needs to be pre trained even pre training on a different language helps you want you know you train a model on English and it will do better at French.
And that's not intuitive right and we have found that that is not the case in chemistry we found that it's typically you know you train on toxicity and then you try to predict you know and solubility there is no connection between the two tasks at all.
On very very very little sometimes it's counterproductive it's one of those things that has has proven really hard so in terms of hyper problem and optimization helps when combined with the we setting up the right test architecture right so you should be testing on far away well defined you know different packets of chemistry because it's it's very easy to trick yourself into you know over estimating the performance of your model.
You know when you show it related chemistry in training test you you get over confident and then when you go try on actual different molecules that that you haven't thought of yet things break down so this a place where the combination of the right train test strategies and hyper parameter optimization can help but on the representation learning side there is still work to do in making this transferable machine learning models over over more.
Can you elaborate a bit more on the train and the test and the distance between these chemistry regimes and maybe give a concrete example from your experience.
You know typically the first thing is to do you know giving a bunch of data is to do random splits you train on 80% validate your choice of hyper parameter on another 10% and keep 10% on the bank until the very end just to make sure that evaluate the performance in a realistic unseen data.
And this typically works reasonably well in many tasks but over chemistry we have found to be we have found that models will transfer well to chemisties that are really nearby to the training set so for instance we just had a paper hardware we compared and.
The color of a molecule in a solvent right so I put this this molecule in water is straight I put it in alcohol it's blue I put it in Tolwin it's going to be green so if we split randomly the combinations randomly we might end up with the same molecule in train and test just in different solvent and we do amazing right if we've seen the molecule in water we're going to be amazing at the same what it does in Tolwin then we can split in just in terms of the chemistry so if the molecule is in test in train it cannot be in test regardless of the same.
Test regardless of the solvent right we do I still find because we're letting very we're letting similar molecules leak between train and test but then we do what's called scuffle splitting which means if these molecules belong into the same general group if they have if they share a lot of attributes then they need to be together in train or together in test right is this scalp scuffle splitting is of the highest year.
And then things suddenly break down and get really hard so in terms of performance I would say we were less than one color in the rainbow away like if you think about about the accuracy of color you can think you know the quality is like one hop over the rainbow that's we were better than that right so we said yellow it was yellow maybe was orange but it wasn't definitely blue.
And once you go to scuffle splits and you really forced the model to learn how to do new unseen classes of chemistry then we're talking maybe two colors on the rainbow I think things get really hard on this was our experience with color but it's the same with toxicity with potency with all the properties of fox carrying medicinal chemistry and it's just hard to quantify.
How your model will do when you show it new chemistry but of course we want the models to invent new chemistry I wouldn't want to just decorate molecules we knew already we want them to tell us something new and exciting and that continues to be a challenge for the community.
And so this to what degree does this broadly change the way you think about generalizability in your domain a lot it's this ongoing discussion what do we do for instance there is there is another class of molecules of models we use.
They call interatomic potential that I to replicate the expensive DFT right so if I move a molecule what's the energy price of you know distorting stretching vibrating the molecule so the ft is good at that but it's just too expensive and I want to simulate molecules for a long long time so we can achieve you know outstanding performance for a given molecule in its own distortions we can achieve outstanding performance there is this the error of experiments when we're going to do that.
The error of experiments when people compare theory to experiments the uncertainty of the experiments is higher than our ability to replicate the theory right so so we nailed on the theory with machine learning to the point that we just don't know if the experiments agree or not because experiments have no easier than our ability to replicate theory.
It's called chemical accuracy so the general models are indistinguishable from theory for a single molecule but now you try to mix in multiple molecules at the same time and the error goes up so it's really hard to again let's call this this conformational space so just moving pieces of the same molecule.
You know 0.1 kilocalories per mole that's the units we use kilocalories per mole we do you know less than one one is chemical accuracy less than one is your machine learning is as good as your underlying theory that you're trying to replicate.
Now you try to mix in new chemistries and suddenly things shoot up especially if they're unseen chemistries and I'll sing for me is a molecule they get 10 times worse they get 25 50 times worse.
And so we it's it's hard because it's hard to quantify similarity to we don't really have and then there is people like Heather Kuleck here at MIT who has done working quantifying.
How does my machine learning model see similarity between these two molecules in a way that I can use it and but it's it's a very much open question I don't have a good answer about how to make transferable models over chemistry.
Last work you described makes me think of some kind of molecular embedding space and I've got to imagine that's been a lot of thoughts been been put into that is that is that working yet.
So there's a button and so the graph neural networks creates a vectorial embedding for molecules and they were great job at that and then you can take you know your distance may be Cartesian you do a cosine distance over this embedding and that correlates relatively well with the arrow in the ball is it can be better than that.
But there's a there's a but there you're saying that the.
Similarity is very difficult in this space and imagining this goes back to these cliffs that we're talking about like the distance measures are with are they like not linear something like that and they just fall off a cliff.
Exactly so there's there's just two things one is is this very no linear nature of this the manifold right it is very no linear nature of the other sponsor face meaning that you're going to get surprised it is tiny tweak over structure turns out to be a tremendous tweak over property and the embeddings may struggle to capture that.
And then the second part is that this is all property specific so we've made a distance for this property but because there aren't really large data sets with all the properties at the same time that cover all of chemical space.
We have the same problem again of transfer ability right okay we've managed to make one distance at the rewards and correlates with the role in this task but again the distance and the embedding similarity for toxicity is going to be very different from embedding similarity for solubility.
We're back to the lack of transfer ability across tasks so even if you call in transfer ability across chemical space to some degree you're still struggling with chemical transfer ability across tasks.
It's not similarity of two molecules in some absolute physical context it's relative to some property that you built your whole you built your whole space around your whole representation around and those are non overlapping or non sufficiently overlapping when you want to look at broad sets of properties.
Indeed and we go back to the pre-training because one could try well you know we could pre-training in an unsupervised way you know with graphs people do and you know you delete some notes and you ask the model to put them back right is this graph completion pre-training or language there is analogous strategies.
But they don't really help with chemistry so the embeddings that you take that you learned from from doing and supervised completion type of tasks are not really helpful for downstream supervised tasks or you know there might be one percent for for this and that but it hasn't dominated and solved our challenges in the same way as it has in other domains.
So yeah you're one thing we haven't really talked about is you talked about the this forward problem reverse problem and how in the inverse design problem you are starting from these desired properties of the material and trying to come up with the materials and we haven't talked about kind of the feasibility or realize ability constraints.
So beyond the desired properties is that something that do you have the luxury of hey we're in academia we don't have to think about that at all.
Definitely not definitely to the point that you know there is a broad spectrum of answers there so some folks have found out that it's so hard to get things made that what they should be doing now is robots to make molecules and there is a big push in the community.
Okay let's go to the next step and you know computerize the creation of molecules in the lab.
Right so let's make robots a big this up and mix it up.
And here I need to plug my colleague Conor Coli among others one of the folks has been the charge in making machine learning models that write how you should make the molecules.
This is the reinforcement learning over over steps in the lab that I was describing right that's the idea right so you do not only your steps your actions for for the machine learning generative model mimic the actions you should take in the lab right so if something can be written by the model it can be made in the lab.
So that's another strategy for materials is a little bit better because materials are typically made at high temperatures and you know with this sort of you've seen casting right you you melt the iron and you hit it with a hammer so typically we're we're and more than you invite thermodynamics mostly.
You know just saying if if this is the most stable combination of these elements I could make that's typically how you decide if a material can be made right if if this is where you know tantalum that you don't want to be.
And this is the lowest energy they could have then I know I can put them in a in a crystal I can melt it and I'll get I get the right.
I remember from that materials classes all these hysteresis effects where the actual the the properties are non linear with you know heat and temperature and how things are made.
Yes that's that's kinetics right and so exactly that's that's the typically so for instance right that you would need to hit more than you ideally would have right to make sure that your system really melts and then you can slowly cool it.
There are no one says in terms of the execution on on how to make them in terms of which process parameters and then there is you know El Salvadori here at MIT.
Who does a natural language processing over chemical recipes for materials as we've got this robot that reads papers and tells you you know what you should use a singer this at a thousand degrees or just a center of twelve thousand twelve hundred degrees depending on on the four size you want right so.
Those are thinking about that but typically the binary decision of this is makeable this is not makeable in materials.
It's energy and we can get energy from physics and we can get sorrogates for energy with machine learning so we're typically fairly confident whether we have support or not in terms of is this just accessible or not.
And again this is a place where molecules are harder or you know less less tractable because yeah because energy you know if we were talking about energy everything would be CO2 right like if it was just energy oxygen would oxidize the scene and or will you world will be with the carbon dioxide and water.
There is a lot more nuance about meta stability and energy helps there are energetic arguments to be made but there is a lot of you know execution practicality arguments that are based on the way we make molecules today you know and that's that's the challenge right that.
The molecules that wouldn't seem make about twenty years ago are standard now right so how do you how do you embed that into into a machine learning model right the rules of what's makeable are very practical right they're based on the technology is available to us at a given time.
Well maybe I'm asking you back the question of just that you just asked how do you pull that into a machine learning model and in particular are those constraints that you bake into the model itself or are they things that you.
Can work with at the HPO level where the you know sig opt in this case is keeping those constraints in mind as it's trying to find a trying to navigate the hyper parameter space for you.
I don't think we have a good answer for that so with in the past we've added this sort of you know chemical beauty or chemical complexity and descriptors and then made them part of our objective function so we want some something that you know makes accurate.
Performing inaccessible molecules and we try to navigate the Pareto line of getting the best possible combination of all these things at the same time so that's one way to think about it and the other way is is to make them the algorithm itself the architecture that you're using.
Meaning what's what's makeable in the lab and the steps and the operations that you take are what's makeable in the lab, but of course, as you just said that's less than what's makeable in general because there might be this one step that no one has made in the lab yet.
At the moment you know somebody makes this new catalyst or this new coupling and the paper gets out then there's going to be this new way of making it so it's there's no open answer these days.
And I think folks are taking so any and all approaches at the same time awesome awesome well Rafa thanks so much for taking the time to share a bit about what you're working on very cool stuff and I really enjoyed chatting about it.
Thanks so much.
Thank you.
