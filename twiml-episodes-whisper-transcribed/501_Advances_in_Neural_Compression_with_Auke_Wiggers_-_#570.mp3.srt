1
00:00:00,000 --> 00:00:11,440
All right, everyone. Welcome to another episode of the Tumel AI podcast. I am, of course,

2
00:00:11,440 --> 00:00:17,680
your host Sam Charrington. And today I'm joined by Al Kay Wickers. Al Kay is a research scientist

3
00:00:17,680 --> 00:00:22,880
at Qualcomm. Before we get going, be sure to take a moment to hit the subscribe button wherever

4
00:00:22,880 --> 00:00:28,480
you're listening to today's show. Al Kay, thanks for joining me and welcome to the podcast.

5
00:00:28,480 --> 00:00:34,400
Today we'll be digging into some of the work you and your colleagues are presenting at ICLR.

6
00:00:35,360 --> 00:00:40,160
And especially your work on transformer-based transform coding. But before we do that,

7
00:00:40,160 --> 00:00:44,240
I'd love to have you share a little bit about your background and how you came to work in

8
00:00:44,240 --> 00:00:49,440
machine learning. Thanks a lot, Sam. Thanks for having me. So I started out in machine learning,

9
00:00:49,440 --> 00:00:54,800
I think, around the time that I studied at UVA. So there was an AI course at UVA,

10
00:00:54,800 --> 00:01:01,200
which I joined in 2012. And a couple of co-students, who were a year higher than me,

11
00:01:02,000 --> 00:01:07,280
were quite business-oriented. And already during their masters, they were founding small companies.

12
00:01:07,280 --> 00:01:13,440
And eventually they guided me into their startup, which was called Cypher. And I believe you've

13
00:01:13,440 --> 00:01:17,680
actually interviewed some of the original founders. So that's Max Welling, who's a professor

14
00:01:18,480 --> 00:01:24,000
currently at Microsoft. And some of the other original founders are Stella Qualcomm, Taco Cohen,

15
00:01:24,000 --> 00:01:29,680
who has worked a lot on a group of variant networks and time in Blancofort. And so they kind of

16
00:01:30,480 --> 00:01:35,280
talked me into joining their company. And I guess the rest is history, because this startup was

17
00:01:35,280 --> 00:01:40,080
acquired after a couple of years by Qualcomm. And I've been at Qualcomm ever since, for the past four

18
00:01:40,080 --> 00:01:45,360
and a half years or so, having worked on concepts like reinforcement learning, for autonomous

19
00:01:45,360 --> 00:01:50,080
driving. And in the past two and a half to three years, mostly on neural data compression,

20
00:01:50,080 --> 00:01:57,920
using generative models. Well, let's dig into that topic. Talk a bit broadly about your research into

21
00:01:59,280 --> 00:02:04,720
the compression side of things, and we'll dig deeper. So I think neural data compression is

22
00:02:04,720 --> 00:02:10,800
relatively new. I think that the seminal works came out around 2016, 2017. But it's quite a

23
00:02:10,800 --> 00:02:15,280
beautiful application of generative modeling. Because most likelihood models, in the end, what they

24
00:02:15,280 --> 00:02:22,640
give you is exactly what the name advertises, a likelihood for a data point. And information theory

25
00:02:22,640 --> 00:02:29,280
tells us that you can then compress that data point with a certain number of bits. And what these

26
00:02:29,280 --> 00:02:34,800
generative models can be used for, and that's how we were applying them, is to estimate likelihood

27
00:02:34,800 --> 00:02:40,720
of incoming data points. So you could imagine that's images or audio or video. And using entry

28
00:02:40,720 --> 00:02:47,920
coding, then squeeze out any redundancy, where this likelihood model tells you exactly how many

29
00:02:47,920 --> 00:02:52,960
bits you're going to need in order to compress that data. So what we've done in the past couple of

30
00:02:52,960 --> 00:02:58,720
years is mainly do this, the trained big generative models in sort of likelihood modeling tasks.

31
00:02:58,720 --> 00:03:02,240
But of course, with the end goal that will eventually use these in practical setting.

32
00:03:02,880 --> 00:03:09,840
So whereas in typical generative models, you could imagine that big hierarchical VIEs,

33
00:03:09,840 --> 00:03:15,040
they train in a fully continuous way in the latent space is continuous. So a big difference in

34
00:03:15,040 --> 00:03:20,480
neural data compression is that eventually you have to quantize this. You have to move to integer

35
00:03:20,480 --> 00:03:27,040
representations instead of floating points in order to compress in a lossless way. So a large part

36
00:03:27,040 --> 00:03:30,800
of the work we've done in the last three years is training these big generative models with a

37
00:03:30,800 --> 00:03:35,840
quantized latent space. Whereas the general field of generative modeling, I would say is mostly

38
00:03:35,840 --> 00:03:41,040
focused on continuous latent space models. Often when we're talking about quantization, it's

39
00:03:42,400 --> 00:03:50,400
the focus of the conversation is around efficiency and performance. In this case, we're talking

40
00:03:50,400 --> 00:03:56,880
about just the fundamental nature of how you're trying to use the output of the networks for

41
00:03:56,880 --> 00:04:04,160
communication. The interplay between the quantization from an efficiency perspective and

42
00:04:04,160 --> 00:04:09,760
the way you're using it in compression. Yeah, thanks for pointing it out. It's actually a good

43
00:04:09,760 --> 00:04:14,560
distinction. So in our case, the only thing we need to quantize in theory as long as we don't

44
00:04:14,560 --> 00:04:20,400
deploy these networks is the latent space. And the reason that we compress this is that this latent

45
00:04:20,400 --> 00:04:25,920
variable is actually the compressed representation of the data. So as soon as you want to transmit this

46
00:04:25,920 --> 00:04:32,960
to let's say different hardware, the only way you can do this in a truly lossless way is if this

47
00:04:32,960 --> 00:04:37,600
compressed representation is quantized because of things like floating point precision, for example.

48
00:04:38,240 --> 00:04:44,160
Now, the quantization and efficiency angle comes into play when you think about deploying these

49
00:04:44,880 --> 00:04:50,080
generative codecs. So of course, when we move to device, we would likely want these models to run

50
00:04:50,080 --> 00:04:55,600
in fixed point instead of in floating point. But most of the research that we've done so far,

51
00:04:56,800 --> 00:05:01,040
leading up to academic publications, for example, has been on floating point models,

52
00:05:01,040 --> 00:05:06,160
where the latent space is quantized. And then as soon as we go to prototypes where we deploy these

53
00:05:06,160 --> 00:05:11,200
to the device, that's when we'll quantize the entire model. So the way it's activations and so on.

54
00:05:11,200 --> 00:05:16,880
And that's when efficiency becomes most important. So we've been talking broadly about neural compression

55
00:05:16,880 --> 00:05:21,760
thus far and you're working that area, but you've got a particular interest on the video side.

56
00:05:21,760 --> 00:05:27,120
Can you talk a little bit about how the work you've done extends to video?

57
00:05:27,120 --> 00:05:33,760
So video compression poses a couple of challenges that image compression, for example, does not.

58
00:05:35,520 --> 00:05:40,960
And that's mainly about the subjective experience. So as a human observer,

59
00:05:40,960 --> 00:05:45,040
you would want your video to look consistent over time and you would want the motions to be consistent

60
00:05:45,040 --> 00:05:52,080
and so on. And also about exploiting more redundancies. So when you work on image compression,

61
00:05:52,080 --> 00:05:56,960
if you want to exploit redundancies, what you'll mainly watch out for are things that are similar

62
00:05:56,960 --> 00:06:02,320
across the image, you know, patterns that appear in different parts of it. Whereas for video,

63
00:06:02,320 --> 00:06:07,520
there's also the temporal redundancy. Most of the time when you look at subsequent frames,

64
00:06:07,520 --> 00:06:12,080
they will actually be very close to each other, especially for background of a video, for example.

65
00:06:12,640 --> 00:06:17,200
And you can exploit this by making sure that any previously transmitted information,

66
00:06:17,200 --> 00:06:23,360
you don't encode it again, so to speak. The other part, so this subjective quality part,

67
00:06:24,160 --> 00:06:28,880
makes it a more challenging task and I think more interesting, therefore, than image compression.

68
00:06:30,240 --> 00:06:36,880
As it's very noticeable if you have a codec that produces frames that are inconsistent over time.

69
00:06:36,880 --> 00:06:42,080
Many years, you're just watching it and it's jerky and just visually unsettling.

70
00:06:42,080 --> 00:06:48,240
Yeah, exactly. Or, you know, speckle noise and those kind of things. So a lot of our research

71
00:06:48,240 --> 00:06:53,280
is aimed at exploiting this temporal redundancy apart from concepts that we borrow from image

72
00:06:53,280 --> 00:06:58,320
compression, of course. And more recently, if you've also started looking more into how to

73
00:06:58,320 --> 00:07:05,760
improve the perceptual quality when you're using this video codec. So that could go as far as

74
00:07:05,760 --> 00:07:12,080
as GAN-based compression. So when you're really hallucinating parts of the reconstructions that are not

75
00:07:12,080 --> 00:07:19,920
truly in the bitstream, but also things like region of interest-based coding. So for example,

76
00:07:19,920 --> 00:07:24,400
now that you and I are talking, the background is actually not the most important part of the video.

77
00:07:24,400 --> 00:07:30,160
You will want the face to be accurate and maybe text that appears in the video. So paying more

78
00:07:30,160 --> 00:07:34,240
attention to that and spending more bits on that is another way to improve perceptual quality.

79
00:07:34,240 --> 00:07:41,600
What's the relationship between the research you're doing on the neural compression side and the

80
00:07:42,480 --> 00:07:49,520
historical research that's that's done into compression pre-neural networks, right? We

81
00:07:51,280 --> 00:07:55,280
you know, figured out a lot of these same things, right? The background is not moving. We don't have

82
00:07:55,280 --> 00:08:06,720
to spend as much bandwidth on that. Do you are you pulling kind of insights from the evolution

83
00:08:06,720 --> 00:08:12,080
of that prior work or is the neural setting so different that you're coming up with new tricks

84
00:08:12,080 --> 00:08:19,280
all the time? I wish the latter were always true. Of course, we're boring heavily from domain

85
00:08:19,280 --> 00:08:25,760
expertise that's been that's been created over many years, maybe 40, 50 years even. So

86
00:08:27,200 --> 00:08:33,840
handcrafted codex or standard codex, they apply a lot of domain knowledge that also goes for

87
00:08:33,840 --> 00:08:40,240
neural codex and what you see most often in especially video coding nowadays is that certain concepts

88
00:08:40,240 --> 00:08:46,800
are being pulled in like this motion composition that you mentioned. Of course, it's logical to

89
00:08:46,800 --> 00:08:52,800
use previous information, already decoded information on the receiver end. So we're boring a lot

90
00:08:52,800 --> 00:09:00,480
of those concepts from traditional coding and at the same time, many of the operations used in

91
00:09:00,480 --> 00:09:07,280
handcrafted codex can be replaced in a sometimes more efficient way and by efficient, I mean,

92
00:09:07,280 --> 00:09:10,800
from a rate distortion point of view, not necessarily from a computational point of view,

93
00:09:10,800 --> 00:09:19,680
then what the handcrafted codex do? What are the key benchmark tasks in video compression?

94
00:09:19,680 --> 00:09:27,120
So there are a couple of video data sets that are commonly used for evaluation of any video codec

95
00:09:27,120 --> 00:09:34,320
really and these have been established by this standards community. The trick is you can't

96
00:09:34,320 --> 00:09:39,760
just use any video. You want it to be raw video as high quality as possible, no compression apply

97
00:09:39,760 --> 00:09:46,240
to it and somewhat counterintuitively because this is pretty expensive to obtain and store.

98
00:09:47,280 --> 00:09:51,840
What most people train on nowadays is not raw video but actually already compressed video,

99
00:09:53,280 --> 00:09:57,680
maybe down sampled or augmented in some way to get rid of compression artifacts.

100
00:09:58,560 --> 00:10:04,880
But there are a few common raw video data sets that are commonly used for neural video coding

101
00:10:04,880 --> 00:10:11,520
benchmarking. When you're using these raw video data sets and then you want to apply the

102
00:10:12,720 --> 00:10:16,080
compressions in the real world, say with mobile phone

103
00:10:18,960 --> 00:10:24,720
information or camera information, do you have domain adaptation issues where

104
00:10:26,160 --> 00:10:30,560
you have to adapt the work? Yeah, that's an interesting question.

105
00:10:30,560 --> 00:10:37,200
And mostly because there's no big cross domain benchmark, so it's hard to really judge the

106
00:10:37,200 --> 00:10:42,640
impact until you deploy. We did notice this in a few cases and we have some work addressing this

107
00:10:42,640 --> 00:10:48,640
exact issue. So we noticed for example, when we pre-trained our codex on some video data set,

108
00:10:48,640 --> 00:10:54,880
we test it on some benchmark data set, then we get a certain score. But now if we fine tune on

109
00:10:54,880 --> 00:11:00,560
a subset of that new domain and we test it on some whole that set, we actually see quite a big

110
00:11:00,560 --> 00:11:07,680
performance increase. So these neural codex, they are generalizing beyond what they're trained on

111
00:11:08,400 --> 00:11:12,240
and hopefully your training data set is so diverse that you can handle many cases.

112
00:11:12,800 --> 00:11:17,600
But there is room for domain adaptation. And one work of my colleague,

113
00:11:18,480 --> 00:11:23,280
this is actually aimed at exactly this issue. So the solution that they come up with

114
00:11:23,280 --> 00:11:31,200
is to overfit on the data to compress and then transmit the parameter update, so the model update

115
00:11:31,200 --> 00:11:36,400
along with the bit stream. So what you get is a sort of customized codec for every data point,

116
00:11:36,400 --> 00:11:40,640
where the starting point is some global model that hopefully generalizes to many data points.

117
00:11:40,640 --> 00:11:46,080
But by altering the model in just a tiny bit, you can gain quite a bit of rate distortion performance.

118
00:11:46,080 --> 00:11:55,040
And another question on the data set given that a lot of the performance improvements that

119
00:11:55,040 --> 00:12:02,480
you're trying to achieve are perceptual. How what's the evaluation process? How do you evaluate

120
00:12:02,480 --> 00:12:07,360
performance in that environment? I think the best thing you can always do in perceptual quality

121
00:12:07,360 --> 00:12:12,880
evaluation is user studies. So actually having people, actual people look at two videos and then

122
00:12:12,880 --> 00:12:19,760
judge which one of these two is better by some criteria. And we have some tools to perform

123
00:12:19,760 --> 00:12:25,200
user studies. Of course, user studies are quite expensive. So what we typically use until that time

124
00:12:25,200 --> 00:12:30,560
are proxy metrics. So there are metrics developed by Netflix, for example, called VMAF,

125
00:12:30,560 --> 00:12:34,960
which measures perceptual quality based on some statistics that they observed in their

126
00:12:34,960 --> 00:12:41,200
own testing environment. There are many perceptual metrics that we borrow from the GAN literature,

127
00:12:41,200 --> 00:12:46,800
such as Frichet Inception Distance. So these are not perfect. They were actually not really

128
00:12:46,800 --> 00:12:51,440
intended for this use case. So Frichet Inception Distance, for example, uses an Inception Network

129
00:12:51,440 --> 00:12:57,120
pre-trained on ImageNet to extract some of its inputs. So it's not a perfect fit to video,

130
00:12:57,120 --> 00:13:03,040
but it's a decent proxy to gauge image quality before you finally do that expensive user study.

131
00:13:03,040 --> 00:13:12,240
So one of the big things that has been happening in this space broadly, computer vision is the

132
00:13:12,240 --> 00:13:18,880
introduction of transformers. How has that impacted the work you're doing around compression?

133
00:13:19,840 --> 00:13:26,880
So there are actually a few works on transformer based compression on images and one of which

134
00:13:26,880 --> 00:13:32,720
and is the one that we are talking about today, of course, the work by my colleague, Janau and Taco,

135
00:13:32,720 --> 00:13:39,200
on transformer based video compression. And I think the main intuition behind this work is that

136
00:13:39,840 --> 00:13:44,560
we know that these confolutions that we're using may not be the perfect building block or basic

137
00:13:44,560 --> 00:13:51,840
operator. And we've seen that a lot of the work improving image codecs is aimed at improving

138
00:13:51,840 --> 00:13:57,520
the likelihood model. So the model that you eventually use to compress these quantized latents

139
00:13:57,520 --> 00:14:03,520
by modeling the distribution. And not a lot of it was focused on changing the the transform as

140
00:14:03,520 --> 00:14:08,720
it's called. So the operation that takes the data and then transforms it into this compressed

141
00:14:08,720 --> 00:14:15,120
representation. And what Janau and Janau did and found out was that you can use some of these

142
00:14:15,120 --> 00:14:21,120
vision transformer building blocks in order to create a better transform. And somewhat counterintuitively,

143
00:14:21,120 --> 00:14:24,560
because a lot of the transformer and vision transformer models are actually computationally

144
00:14:24,560 --> 00:14:31,440
very expensive, they could do so with less computation than with the convolutional models that we've

145
00:14:31,440 --> 00:14:36,880
been using up until then. So when I noticed this, there was of course quite quite a nice insight.

146
00:14:37,440 --> 00:14:43,920
And we've been using these vision transformer based building blocks in our transform

147
00:14:43,920 --> 00:14:49,760
architecture ever since. I don't think we've gone into much detail in the podcast about

148
00:14:49,760 --> 00:14:56,880
vision transformers, fit and swim and things like that. Can you provide an overview of how

149
00:14:57,440 --> 00:15:03,600
the transformer architectures being applied to vision oriented tests? I think the main intuition

150
00:15:03,600 --> 00:15:12,400
behind most modern vision transformers is to treat the image as a collection of patches in a

151
00:15:12,400 --> 00:15:18,800
sort of similar way that language transformers may treat language as a collection of

152
00:15:18,800 --> 00:15:26,160
sentences, words, tokens. So the typical pipeline is something along these lines. We have an image

153
00:15:26,160 --> 00:15:34,080
to encode, let's say. And we would extract patches from that image and bet these in a certain way.

154
00:15:34,640 --> 00:15:41,760
And then we just act as if these are the tokens that are being produced by language transformers as

155
00:15:41,760 --> 00:15:48,480
well. And we apply these transformer blocks on the resulting vision tokens. Of course, vision

156
00:15:48,480 --> 00:15:52,800
transformers were originally proposed for tasks like detection and classification,

157
00:15:54,000 --> 00:15:59,600
whereas we're using them in a compression setting. So I guess a big difference between how vision

158
00:15:59,600 --> 00:16:04,800
transformers were used in classification and detection and what we're doing is that eventually we

159
00:16:04,800 --> 00:16:10,880
have to also generate a dense reconstruction as opposed to single classification. Can you talk a

160
00:16:10,880 --> 00:16:17,520
little bit more about the research that you mentioned with Taco and his co-author? Like I said,

161
00:16:17,520 --> 00:16:23,680
the key idea behind this is to use this swing transformer, which is a type of vision transformer

162
00:16:23,680 --> 00:16:28,560
that is more memory efficient than the original originally proposed vision transformer.

163
00:16:29,760 --> 00:16:35,840
So the vision transformer, as it was proposed, it uses global self-attention and therefore

164
00:16:36,400 --> 00:16:42,480
it's memory usage scales quadratically with the size of the image. Now we most of the time are

165
00:16:42,480 --> 00:16:47,600
encoding pretty large images and video. So of course, that doesn't really work well. What swing

166
00:16:47,600 --> 00:16:54,320
transformers propose is to instead use local windows in which you compute local self-attention

167
00:16:54,320 --> 00:16:59,520
and then aggregate these windows in a actually quite similar manner to how the early convolutional

168
00:16:59,520 --> 00:17:05,680
architectures aggregate information. So you start off with very small windows in which you aggregate

169
00:17:05,680 --> 00:17:11,680
almost pixel information and then at higher levels of the hierarchy you aggregate whatever came

170
00:17:11,680 --> 00:17:16,880
out of those smaller windows. So many inductive biases similar to the convolutional architectures

171
00:17:17,520 --> 00:17:22,880
and because this attention, which is the most expensive operation, memory-wise is only computed

172
00:17:22,880 --> 00:17:28,480
locally, it scales linearly with the memory usage scales linearly with the size of the image.

173
00:17:29,360 --> 00:17:36,560
So, young you now figured this is a good fit for compression as well since we often use very large

174
00:17:36,560 --> 00:17:44,160
image and video and what they did was take a hyper prior architecture, which is one of the seminal

175
00:17:44,160 --> 00:17:51,360
image compression architectures. I think originally proposed by Google in 2018 and replace some of

176
00:17:51,360 --> 00:17:57,760
the operators, especially the convolution and transpose convolution by their swing transformer

177
00:17:57,760 --> 00:18:05,120
equivalent in order to see by making a relatively simple change, can we show that these swing

178
00:18:05,120 --> 00:18:09,280
transformers are better suited for extracting information than convolutions are.

179
00:18:10,480 --> 00:18:15,520
Was it relatively plug-and-play or were there some tricks that had to be involved to get

180
00:18:15,520 --> 00:18:20,800
it all to work? I mean, there always are. There always are. Some tuning required.

181
00:18:22,320 --> 00:18:28,720
But what's nice is that the swing codec work was open source and then this is work by

182
00:18:28,720 --> 00:18:36,000
Microsoft Research Asia and so starting off actually was relatively simple. But yeah, it's largely

183
00:18:36,000 --> 00:18:43,520
take the hyper prior architecture and modify it, tune this well and it turns out you get a similar

184
00:18:43,520 --> 00:18:48,640
compute architecture that is much more efficient from a rate distortion point of view. So much

185
00:18:48,640 --> 00:18:54,640
better performance in your head. One of the key elements of the transformer-based

186
00:18:54,640 --> 00:19:01,840
transform coding paper is this idea of visualizing the effective receptive fields.

187
00:19:01,840 --> 00:19:06,480
Can you talk a little bit about what that means and where it comes into play?

188
00:19:06,480 --> 00:19:12,400
So of course, knowing that you can obtain better performance using these transformer blocks is

189
00:19:12,400 --> 00:19:15,680
not enough. We also want to know why this is so that you could potentially make use of it.

190
00:19:16,640 --> 00:19:21,280
So what Young and Jeno did was visualize this effective receptive field and how you could

191
00:19:21,280 --> 00:19:27,440
kind of view this is asking your network for a certain feature what inputs do I need to change

192
00:19:27,440 --> 00:19:32,480
and in what way in order to maximize or minimize this feature. So you can just use the property

193
00:19:32,480 --> 00:19:37,040
that is network is fully differentiable and compute a gradient with respect to the input image.

194
00:19:37,760 --> 00:19:44,080
And so what they found was that for this convolutional models, if you look at their use across

195
00:19:44,080 --> 00:19:49,360
different compression tasks, the effective receptive field size was largely the same. So it was

196
00:19:49,360 --> 00:19:56,720
something like 30 by 30 pixels, influence a local feature. But if they look at the results,

197
00:19:56,720 --> 00:20:01,520
the same results for the swing transformer-based models, they noticed that in contrast

198
00:20:02,240 --> 00:20:07,440
for tasks where the model only has to look at a single image, the effective receptive field

199
00:20:07,440 --> 00:20:12,240
was very small, indicating that you only need a bit of local information in order to compress

200
00:20:12,240 --> 00:20:18,080
efficiently. But when they applied it to a task where two images needed to be compared,

201
00:20:18,080 --> 00:20:23,840
so a motion estimation task for this motion compensation that we talked about for video,

202
00:20:24,960 --> 00:20:29,600
they noticed that the effective receptive field was much larger. And this makes sense from an

203
00:20:29,600 --> 00:20:34,880
intuitive point of view. Because if you want to determine how across two frames a certain motion

204
00:20:34,880 --> 00:20:39,120
appears, you would want to look at a lot of context. You wouldn't want to look just at a very tiny

205
00:20:39,120 --> 00:20:47,600
area. So this kind of hints that these swing models are able to better determine how much

206
00:20:47,600 --> 00:20:52,000
information they should take into account from a local area in order to make their decisions.

207
00:20:52,000 --> 00:20:56,960
When I hear the description of the effective receptive field, it makes me think a little bit about

208
00:20:58,800 --> 00:21:03,360
ideas like lime where you're kind of creating some perturbations and input and you're trying to

209
00:21:03,360 --> 00:21:11,440
see how they flow throughout a network. Is it a similar idea? It sounds sort of similar. Yeah,

210
00:21:11,440 --> 00:21:17,600
I think one major application, but that really is a blast from the past, is this deep-dream style

211
00:21:19,120 --> 00:21:24,160
image generation where you're kind of asking your network, okay, how should I change

212
00:21:24,160 --> 00:21:28,080
this input in order to make it resemble a dog or a cat or some concept.

213
00:21:29,360 --> 00:21:32,880
But of course, similar techniques have been used in many different settings.

214
00:21:33,760 --> 00:21:39,680
There was also some work in this paper that looked at the kind of characterizing the latent space.

215
00:21:39,680 --> 00:21:45,280
Can you talk a little bit about what that showed? So, young and young, I'll perform a couple of

216
00:21:45,280 --> 00:21:51,440
analyses and some of which were aimed at seeing how these swings, compression models,

217
00:21:51,440 --> 00:21:56,720
utilize the latent space and whether they utilize it in a better way than the convolutional counterparts.

218
00:21:57,600 --> 00:22:04,560
And one of the ways in which you can show this is by looking at how many bits are being transmitted

219
00:22:04,560 --> 00:22:10,000
through every latent channel. So, the latent space has a certain number of channels and it also

220
00:22:10,000 --> 00:22:14,960
has some spatial dimensions. And for each channel, you can count how many bits are going to be

221
00:22:14,960 --> 00:22:20,640
spent in order to transmit the information in this channel. And when you order these channels by

222
00:22:20,640 --> 00:22:26,880
the number of bits, you can kind of construct a progressive decoding scheme. You know, you only

223
00:22:26,880 --> 00:22:31,120
transmit the first channel, you get some of the key information, maybe you get the shape tried,

224
00:22:31,120 --> 00:22:37,120
and you transmit the second channel, you get some of the details right, and so on. And it turned

225
00:22:37,120 --> 00:22:44,400
out that these swing models were able to distribute the information more evenly than their

226
00:22:44,400 --> 00:22:51,600
convolutional counterparts. So, for progressive decoding style schemes, this is potentially interesting

227
00:22:51,600 --> 00:22:56,640
because it means that you can opt to, for example, just transmit the first half your guaranteed

228
00:22:56,640 --> 00:23:03,040
to get some information across. But it's also interesting in case of imperfect channels. So,

229
00:23:03,040 --> 00:23:07,920
if some information is lost, let's say, could you resample these channels, or could you still

230
00:23:07,920 --> 00:23:13,440
decode something that looks like what you meant to decode? And so, they have some experiments

231
00:23:13,440 --> 00:23:19,840
in the paper as well, where they mask certain parts of the latent channels and show that the

232
00:23:19,840 --> 00:23:24,640
swing transformer is more robust to this than the convolutional compression models.

233
00:23:24,640 --> 00:23:28,960
I'm resulting in cleaner reconstructions, even if some of the information is missing.

234
00:23:28,960 --> 00:23:35,840
It's occurring to me that as you've been using the word channel here, I've been thinking of channels

235
00:23:35,840 --> 00:23:41,600
like in an image sense, like color channels and things like that. But you're not necessarily

236
00:23:41,600 --> 00:23:48,240
using it in that sense here. I guess it's similar if you look at input images as a tensor.

237
00:23:48,240 --> 00:23:53,760
Yeah, sorry. For me, whenever I think of images, I always see four-dimensional tensors,

238
00:23:53,760 --> 00:23:58,320
batch channel height width. And for this latent space, it's actually quite similar. There's

239
00:23:58,320 --> 00:24:02,960
also a batch dimension, a channel dimension. And in this case, it happens to be a large number of

240
00:24:02,960 --> 00:24:08,560
channels because we choose it to be. But from a tensor perspective, it's the same thing.

241
00:24:09,280 --> 00:24:15,280
Okay. But they don't necessarily correlate to a particular physical property, like a color

242
00:24:15,280 --> 00:24:24,240
or something like that. They don't know. What we hope is that these transforms, the learned encoder

243
00:24:24,240 --> 00:24:28,960
and decoder, that they're able to map to some space where all of these redundancies are being

244
00:24:28,960 --> 00:24:34,720
squeezed out. But we have no idea what that space means inherently. We can visualize it. And

245
00:24:34,720 --> 00:24:40,320
there actually are some visualizations in the paper as well, where you can see some correlation

246
00:24:40,320 --> 00:24:45,200
to the original input and some spatial patterns appearing and so on. But there are no physical

247
00:24:45,200 --> 00:24:50,320
properties that you can tie it to now. It sounds like it's not quite as clean as some of the early

248
00:24:50,320 --> 00:24:56,880
work looking at layers of CNNs and seeing the, you know, textures, you know, shapes deep and then

249
00:24:56,880 --> 00:25:02,080
textures and things like that. Yeah, I think for some channels, you can't trace it back to some

250
00:25:02,080 --> 00:25:08,160
properties. But there's no guarantee. And what we've seen is that it sometimes is highly dependent

251
00:25:08,160 --> 00:25:12,480
on the input image as well. So with you pass an image through it and you see some response and

252
00:25:12,480 --> 00:25:17,440
you think, hey, great, I found one that corresponds to grass. And then the next image goes through it.

253
00:25:17,440 --> 00:25:23,840
It also responds and it's not a grass image. So sadly, we can't really tie it to any one property now.

254
00:25:24,880 --> 00:25:33,120
To what extent does this work then kind of fit into some of the other things that you're doing?

255
00:25:33,120 --> 00:25:42,400
Like we alluded to some of the work on kind of intra frame for video and there's some other stuff

256
00:25:42,400 --> 00:25:47,360
that you've done that we haven't talked about. P frame and B frame codex and I guess the

257
00:25:47,360 --> 00:25:54,560
transformer base to a new foundation piece that you then can kind of plug in all the other

258
00:25:54,560 --> 00:25:59,760
things that you're doing for video compression around. Yeah, I would say so. I would say so.

259
00:25:59,760 --> 00:26:04,560
I think our team is working on on many directions simultaneously and that hopefully are complementary.

260
00:26:05,120 --> 00:26:10,800
So like you mentioned, we have some work on on different schemes for neural codex.

261
00:26:10,800 --> 00:26:17,200
And this work really is aimed at replacing some foundational building block for any codec.

262
00:26:17,200 --> 00:26:21,360
So what John and Jena did was not apply this just to image compression, but also did some

263
00:26:21,360 --> 00:26:28,640
feasibility studies that showed that it helped promise in in the video setting. And in this case,

264
00:26:28,640 --> 00:26:35,200
it was only aimed at how do you call it? Not streaming, but a real-time video.

265
00:26:35,760 --> 00:26:39,920
So in low the lay setting, it's referred to in the compression setting.

266
00:26:41,840 --> 00:26:47,040
But there's no reason that this could not scale to the streaming use case as well.

267
00:26:47,040 --> 00:26:54,160
And what are some of the future directions that you're looking at in terms of building on top of

268
00:26:54,160 --> 00:26:58,880
this? So I think like you mentioned scaling it to some of these other codex that that we've

269
00:26:58,880 --> 00:27:03,520
been building. The codex that are well suited for the streaming case, for example.

270
00:27:05,040 --> 00:27:09,920
Seeing whether we can reduce computational complexity further. And as you may know,

271
00:27:09,920 --> 00:27:15,040
we've been demoing some of our earlier codex throughout the past year. And of course,

272
00:27:15,040 --> 00:27:19,200
it's interesting to consider whether we can actually make a feasible prototype out of this.

273
00:27:19,200 --> 00:27:24,480
Something that would be runnable on the vice. On that note, what kind of results did you see

274
00:27:25,680 --> 00:27:35,040
from a computational perspective with this type of coding? The convolutional ones are about

275
00:27:35,040 --> 00:27:41,600
equally expensive in terms of max. So multiply and accumulate operations. So it depends on how

276
00:27:41,600 --> 00:27:47,280
you measure compute. From that point of view, swing transformer models are somewhat counterintuitively

277
00:27:47,280 --> 00:27:56,160
more rate distortion per Mac efficient. So you get much better performance for the same amount of

278
00:27:56,160 --> 00:28:03,840
Macs. But there's a small caveat. The attention operation is memory hungry. Because of the

279
00:28:03,840 --> 00:28:08,560
soft Mac operation, even these swing transformers, which only compute the attention in this local

280
00:28:08,560 --> 00:28:14,320
window, there's still a big hit to the total memory. And this is noticeable. And young and

281
00:28:14,320 --> 00:28:20,800
you know, also included some comparisons on things like peak memory usage, which showed that the

282
00:28:20,800 --> 00:28:27,600
convolutional model is still the least expensive one there. So that is one of the papers that you and

283
00:28:27,600 --> 00:28:35,120
your colleagues are presenting at ICOR. There are a couple of others that I'd love to hear a

284
00:28:35,120 --> 00:28:40,240
little bit about. One of those is the confess paper. Can you share a little bit about that paper?

285
00:28:40,240 --> 00:28:49,200
So they're using contrastive learning in a cross domain future setting. So what this means is,

286
00:28:49,200 --> 00:28:54,960
for example, you were to train a model on ImageNet and you want to use this in a setting where x

287
00:28:54,960 --> 00:29:01,600
rays are being used. So it really is a big jump from domain to domain. And the method data

288
00:29:01,600 --> 00:29:07,360
files sort of combines three steps in order to facilitate this domain shift and enable it.

289
00:29:07,360 --> 00:29:12,880
They do use a self-supervised pre-training in order to create a certain set of features.

290
00:29:13,680 --> 00:29:20,880
Then they use a feature selection scheme where they sort of train a mask that is fit for a particular

291
00:29:20,880 --> 00:29:27,760
target domain. And they perform fine tuning at the end. And it turns out that this three step

292
00:29:27,760 --> 00:29:32,320
procedure of which the self-supervised pre-training is probably the most important one.

293
00:29:32,320 --> 00:29:39,520
You can easily transfer from domain to domain without overfitting on the very few test samples

294
00:29:39,520 --> 00:29:44,160
that you may have from the new target domain. So this is especially important when you think about

295
00:29:44,160 --> 00:29:50,320
use cases like personalization. Let's say you've trained a big model on a huge data set for

296
00:29:50,320 --> 00:29:54,080
many different users. And now you want to apply this to just your use case.

297
00:29:55,360 --> 00:30:00,800
Then this fuchsia learning becomes all the more important, likely because your data is expensive to

298
00:30:00,800 --> 00:30:06,480
obtain. And how is the self-supervised, how is self-supervision built into this?

299
00:30:06,480 --> 00:30:12,320
So the first step of this process is mainly about learning a good representation.

300
00:30:13,120 --> 00:30:21,760
And so contrastive methods have generally been used in vision in order to build a certain set of

301
00:30:21,760 --> 00:30:30,640
features that could be used for many different tasks. So for many semi-supervised settings,

302
00:30:30,640 --> 00:30:36,640
for example, it turns out that self-supervised pre-training on large unlabeled data sets

303
00:30:36,640 --> 00:30:41,920
are a great way to kickstart the actually quite difficult semi-supervised process.

304
00:30:42,800 --> 00:30:48,800
So it's kind of a way to use large unlabeled data sets through self-supervision

305
00:30:49,440 --> 00:30:53,280
so that you don't have to go through the motion of obtaining this expensive labeled data set

306
00:30:53,280 --> 00:31:00,320
beforehand. Another paper we wanted to talk a little bit about is the steerable CNN's paper.

307
00:31:00,320 --> 00:31:02,640
What does that mean? What's a steerable CNN?

308
00:31:02,640 --> 00:31:08,560
I would say the best way to introduce it is to talk about these group-equivariant nets that

309
00:31:08,560 --> 00:31:12,960
you've probably talked about with Taka Ko and some time ago. Normal convolutional models,

310
00:31:12,960 --> 00:31:19,040
they're equivalent to translations. So that means you shift the input, the output of the operator

311
00:31:19,040 --> 00:31:26,800
shifts with it. And what group-equivariant nets are generally aiming to do is be

312
00:31:26,800 --> 00:31:32,320
equivariant to different symmetry groups as well. Not just translation, but for example,

313
00:31:32,320 --> 00:31:39,760
also reflection, flipping. I mean, you've talked to Taka about this. So steerable CNN's are one

314
00:31:39,760 --> 00:31:49,040
of the most general ways to accomplish this. And what my colleague Gabriela has done in this work

315
00:31:49,600 --> 00:31:55,600
is a theoretical analysis of the space of these steerable filters and kind of

316
00:31:55,600 --> 00:32:02,320
come up with the permiss duration for the space. And what this will do is, if you're interested

317
00:32:02,320 --> 00:32:08,960
in building a network that's equivariant to some certain symmetry group, let's say one that's

318
00:32:10,000 --> 00:32:16,080
solely aimed at reflections and 90 degree rotations. And what you would have to do before

319
00:32:16,080 --> 00:32:21,360
Gabriela's work is kind of work out an architecture and a method that could allow you to

320
00:32:21,360 --> 00:32:28,240
to satisfy the constraints of that symmetry group. But using your work, you sort of have a general

321
00:32:29,040 --> 00:32:36,000
almost automatic procedure for for deriving the steerable filters. And what's nice about it

322
00:32:36,000 --> 00:32:41,040
is that the code is open source. So anyone could use this and kind of kickstart

323
00:32:42,320 --> 00:32:46,080
building steerable CNN for their symmetry group of interest.

324
00:32:46,080 --> 00:32:52,640
Got it. So you have a particular type of symmetry that is expressing your problem that you want

325
00:32:52,640 --> 00:33:01,760
to exploit. And previously you'd have to kind of handcraft a method for taking advantage of that.

326
00:33:01,760 --> 00:33:11,040
And this is, it doesn't sound like it's a general mechanism, but rather a procedure that you would

327
00:33:11,040 --> 00:33:16,320
follow that leads to a mechanism that works for your specific use cases. Is that the right way to

328
00:33:16,320 --> 00:33:21,520
think about it? Yeah, I think so. I think so. Yeah, like you mentioned, most of the works on steerable

329
00:33:21,520 --> 00:33:30,160
CNNs so far have attacked a specific group. So for example, working on on globes, when you think

330
00:33:30,160 --> 00:33:35,760
about things like weather data, of course, they're not actually 2D. They operate on around the

331
00:33:35,760 --> 00:33:40,560
globe or on the earth. And so it's desirable to be accurate variant across that sort of space as

332
00:33:40,560 --> 00:33:48,400
well. And, and so what's previously been done is you kind of handcraft for this specific use case

333
00:33:49,600 --> 00:33:54,240
a set of steerable filters. Awesome. Well, I'd love to have you share a little bit about what you're

334
00:33:54,240 --> 00:34:01,200
most excited about looking forward in, in the field that that you're focused on the neuro compression

335
00:34:01,200 --> 00:34:08,720
and neuro video compression. What's exciting that you see down the pike? I think one excite or

336
00:34:08,720 --> 00:34:14,320
direction that I'm particularly excited about is the whole perceptual quality direction. I really

337
00:34:14,320 --> 00:34:21,280
believe that this is where neural codex shine, not just gang-based codex, but really any codec that's

338
00:34:21,280 --> 00:34:28,640
aimed at improving perceptual quality. But for example, with the the advent of diffusion models,

339
00:34:28,640 --> 00:34:32,800
a different type of diffusion probabilistic models, a different type of generative model. Again,

340
00:34:33,440 --> 00:34:38,320
very good at generating high perceptual quality details, but not exactly straightforward for

341
00:34:38,320 --> 00:34:44,000
using in the compression setting. We know that neural networks are able to generate these sort of

342
00:34:44,000 --> 00:34:50,160
plausible details and trying to make use of that and trying to exploit that is something that

343
00:34:50,160 --> 00:34:56,080
I'm particularly excited by. And at the same time, I'm also excited about bringing down the compute

344
00:34:56,080 --> 00:35:02,720
and actually making practical codex, things that you or I could run on a mobile phone and watch videos

345
00:35:02,720 --> 00:35:09,760
with, because that really moves it from the academic setting to the tangible setting.

346
00:35:11,920 --> 00:35:18,560
When you talk about the increasing the perceptual quality and incorporating different pieces

347
00:35:18,560 --> 00:35:26,720
like GANs, is it more often that you're taking elements of these different types of models and

348
00:35:26,720 --> 00:35:39,760
building them into one end-to-end train thing? Or are there other cases where you've got

349
00:35:39,760 --> 00:35:44,800
higher level components that you're bringing together into more of a system type of an approach

350
00:35:44,800 --> 00:35:51,440
to solving the problem? There sure are hybrid approaches. I think the most elegant way is

351
00:35:51,440 --> 00:35:56,960
always this end-to-end approach where you have an encoder that's a network, you have a prior model

352
00:35:56,960 --> 00:36:04,240
that's also a network and a decoder is a network and there are some, I think, really well-made work

353
00:36:04,240 --> 00:36:11,520
from the Google Perception Group, for example, on an end-to-end image codec, which I believe they call

354
00:36:11,520 --> 00:36:16,960
high-fidelity, generative image compression. In this case, the decoder is a conditional GAN

355
00:36:16,960 --> 00:36:22,480
mapping from this latent space to image space, but the encoder is a network and the

356
00:36:22,480 --> 00:36:28,320
priors and network as well. You can train this entire thing end-to-end by mixing a few lost functions.

357
00:36:28,320 --> 00:36:33,360
I think it's quite an elegant way, but it's not the only way and there definitely is something to

358
00:36:33,360 --> 00:36:39,760
be gained from also looking at domain expertise in particular when you think about bringing down

359
00:36:39,760 --> 00:36:46,640
complexity and making sure that you don't need a huge neural network in order to kind of rebuild

360
00:36:46,640 --> 00:36:52,480
or learn what other people have already figured out for you. Right, right, and does that? What are the,

361
00:36:52,480 --> 00:37:03,600
are there clear lines from, kind of, performance and efficiency perspective? In other words,

362
00:37:03,600 --> 00:37:12,720
like is end-to-end always more computationally intense, more efficient, or does it depend on the

363
00:37:12,720 --> 00:37:21,520
specific set of architecture and implementation? It's hard to predict where this will go. I mean,

364
00:37:21,520 --> 00:37:26,880
at the moment, most of these end-to-end architectures, they are computationally fairly expensive,

365
00:37:26,880 --> 00:37:33,200
because they're kind of replacing many components that make use of inductive biases,

366
00:37:33,200 --> 00:37:38,880
and those inductive biases, especially when you think about standard codex, they're often

367
00:37:38,880 --> 00:37:44,160
also aimed at bringing down the complexity, not just at obtaining the best rate distortion performance.

368
00:37:44,880 --> 00:37:54,240
So some innovations in more traditional methods, they may have been chosen over others,

369
00:37:54,240 --> 00:37:58,080
not necessarily because they are bringing out the very best rate distortion performance,

370
00:37:58,080 --> 00:38:03,200
but because they have good rate distortion performance, but they're also efficiently implemented

371
00:38:03,200 --> 00:38:10,240
on hardware. So with neural codex, especially end-to-end codex, given that it's a fairly recent field,

372
00:38:11,040 --> 00:38:16,320
a lot of the focus has been on making models that scale well and making use of the compute we have,

373
00:38:16,880 --> 00:38:20,640
and recently there's been a lot more attention towards making practical codex as well.

374
00:38:21,280 --> 00:38:26,320
So for example, with some of our demos in the last year, and recently, David Minnan,

375
00:38:26,320 --> 00:38:33,760
from the same Google perception team gave a keynote at ISIP, in which he also had a call to action.

376
00:38:33,760 --> 00:38:39,120
Look at models that are computationally less expensive, but still obtain good rate distortion

377
00:38:39,120 --> 00:38:44,480
performance. So I hope that answers your question. I think now end-to-end is still computationally

378
00:38:44,480 --> 00:38:51,280
fairly expensive, but we've shown that it's doable. You can do on-device decoding, but there is

379
00:38:51,280 --> 00:38:57,840
a change in the works, I would say. Awesome. Well, OK, it was wonderful chatting with you and

380
00:38:57,840 --> 00:39:05,360
learning a bit about all the things that you and your team are working on, and best of luck with

381
00:39:05,360 --> 00:39:22,080
your presentations at the conference. Thanks, and thanks a lot for having me.

