WEBVTT

00:00.000 --> 00:05.200
All right, everyone, welcome to our AI Trends 2023 series.

00:05.200 --> 00:07.140
Each year, we invite friends of the show

00:07.140 --> 00:10.200
to join us to recap key developments of the year

00:10.200 --> 00:12.080
and anticipate future advancement

00:12.080 --> 00:15.480
in the most interesting subfields in AI.

00:15.480 --> 00:17.680
And today, we're joined by Samir Singh.

00:17.680 --> 00:19.600
Samir is an associate professor

00:19.600 --> 00:22.360
in the Department of Computer Science at UC Irvine

00:22.360 --> 00:24.160
and a fellow at the Allen Institute

00:24.160 --> 00:27.000
for Artificial Intelligence for AI2.

00:27.000 --> 00:29.960
To talk through some of the key research developments

00:29.960 --> 00:32.000
in NLP.

00:32.000 --> 00:34.320
Of course, before we get going,

00:34.320 --> 00:35.960
take a moment to hit that subscribe button

00:35.960 --> 00:38.080
wherever you're listening to today's show.

00:38.080 --> 00:41.960
And you can also follow us on TikTok and Instagram

00:41.960 --> 00:46.640
at Twimble AI for highlights from every episode.

00:46.640 --> 00:48.800
All right, let's jump in, Samir.

00:48.800 --> 00:53.480
Welcome back to the podcast and our Trends series.

00:53.480 --> 00:54.800
Yeah, thank you for having me, Sam.

00:54.800 --> 00:56.560
It's great to be back.

00:56.560 --> 00:58.320
It's super excited to have you back.

00:58.320 --> 01:00.560
We were joking a little bit before we got rolling

01:00.560 --> 01:04.160
that we picked big years to have you on.

01:04.160 --> 01:09.160
The last one was our 2020 right in the wake of GPT-3,

01:10.560 --> 01:12.840
a big year, and of course,

01:12.840 --> 01:15.000
this has been a huge year for NLP

01:15.000 --> 01:18.560
with the relatively recent release of chat GPT.

01:18.560 --> 01:20.760
Yeah, it's always kind of crazy

01:20.760 --> 01:23.640
when you have these big changes happening in the year

01:23.640 --> 01:26.760
where there is research still going on in parallel

01:26.760 --> 01:28.800
and people are exploring research questions

01:28.800 --> 01:32.280
and a lot of them either become obsolete

01:32.280 --> 01:34.760
or have to be revisited and things like that

01:34.760 --> 01:35.680
in the middle of the year.

01:35.680 --> 01:37.760
And in this year, especially,

01:37.760 --> 01:39.760
it was much closer to the end of the year.

01:39.760 --> 01:41.200
So looking back at the year,

01:41.200 --> 01:43.760
it's always the first thing to think about the trajectory

01:43.760 --> 01:47.440
and what ideas will still be for a citizen, what do you want?

01:47.440 --> 01:50.320
Yeah, yeah, as a great point, chat GPT

01:50.320 --> 01:53.120
happened right at the end of the year.

01:53.120 --> 01:55.640
Do you think we'd have this same sense

01:55.640 --> 01:57.600
that this was a huge year in NLP

01:57.600 --> 02:02.320
if it wasn't for that late year release of chat GPT?

02:02.320 --> 02:03.160
Oh, I definitely.

02:03.160 --> 02:06.280
I think this year has been really impressive.

02:06.280 --> 02:10.520
I would say even bigger, even if you take out chat GPT,

02:11.520 --> 02:13.960
overall, this year has been really big for NLP,

02:13.960 --> 02:16.880
even compared to the ERGPT-3 came out.

02:16.880 --> 02:18.480
So yeah, I think there have been,

02:19.480 --> 02:22.840
I feel like it took us a while to come in terms

02:22.840 --> 02:25.200
with what these large language models are capable of

02:25.200 --> 02:28.880
or what they clearly fail at and what they are good at

02:28.880 --> 02:32.080
and try to build better two-wing around it,

02:32.080 --> 02:33.840
build better support systems around it.

02:33.840 --> 02:35.520
And so yeah, I think this year has been good,

02:35.520 --> 02:38.440
even if you don't take it out on chat GPT, yeah.

02:38.440 --> 02:39.280
Awesome.

02:39.280 --> 02:41.640
Well, we're going to dig into chat GPT

02:41.640 --> 02:43.360
in a fair amount of detail,

02:43.360 --> 02:46.040
as well as some of the other advances you just hinted at.

02:46.040 --> 02:48.720
But before we do, I'd love to have you take a few minutes

02:48.720 --> 02:52.120
to just introduce yourself to our audience

02:52.120 --> 02:55.680
with a focus on kind of your research focus

02:55.680 --> 02:57.240
and what your interests are.

02:57.240 --> 02:58.080
Cool, yeah.

02:58.080 --> 03:01.600
So I've been working in NLP for a long time now,

03:01.600 --> 03:04.960
but my focus has mostly been looking at

03:04.960 --> 03:07.080
when these language models or machine learning

03:07.080 --> 03:09.400
in general gets interface with real users,

03:09.400 --> 03:12.640
what are the needs that sort of are there.

03:12.640 --> 03:14.880
So a lot of my work has been in explanations

03:14.880 --> 03:18.280
and interpretability, but also in robustness,

03:18.280 --> 03:20.080
both from an adversarial perspective,

03:20.080 --> 03:23.840
but also from out of domain generalization perspective.

03:23.840 --> 03:26.360
And also in terms of evaluation,

03:26.360 --> 03:28.720
how do we know whether the models are doing well,

03:28.720 --> 03:30.160
how well are they doing?

03:30.160 --> 03:32.320
And in general, be able to understand

03:32.320 --> 03:34.320
and predict when the models would work

03:34.320 --> 03:36.360
and when the models would work.

03:36.360 --> 03:37.240
Mm-hmm.

03:37.240 --> 03:43.080
And I'm imagining that the advent of large language models

03:43.080 --> 03:45.880
and the kind of the dominance of that approach

03:45.880 --> 03:50.880
to NLP modeling is, well, it certainly changed the tools

03:51.240 --> 03:52.920
and the approach that you take.

03:52.920 --> 03:55.320
Has it changed kind of the fundamental way

03:55.320 --> 03:56.680
that you approach the problem?

03:56.680 --> 03:58.160
To some degree, yes and no,

03:58.160 --> 04:01.160
I think it has made a lot of my work obsolete

04:01.160 --> 04:04.440
in the sense that we were doing a really good job

04:04.440 --> 04:07.400
of finding fundamental faults

04:07.400 --> 04:08.920
in a lot of these language models

04:08.920 --> 04:11.000
and turned out a lot of them go away

04:11.000 --> 04:14.200
when you have a lot more data or a lot larger size.

04:14.200 --> 04:16.680
And so the specific observations and insights

04:16.680 --> 04:19.680
we had, not all of them have persisted.

04:19.680 --> 04:22.600
But the other differentiation we had in our work

04:22.600 --> 04:25.720
was always being somewhat model agnostic

04:25.720 --> 04:30.000
or try to use a black box approach to the model

04:30.000 --> 04:32.480
rather than looking inside what's going on.

04:32.480 --> 04:34.920
And that is something that you can use

04:34.920 --> 04:37.680
in this world of only access to API.

04:37.680 --> 04:40.080
A lot of those tools can still work out.

04:40.080 --> 04:44.520
So yeah, so it's been a mix, but it's been exciting

04:44.520 --> 04:46.880
to sort of continue to do that.

04:46.880 --> 04:51.880
Well, you've identified some themes that from your purview

04:53.000 --> 04:56.320
have been some of the key topic areas

04:56.320 --> 04:59.880
and research that have emerged in the field

04:59.880 --> 05:04.280
over the past year, let's start there.

05:04.280 --> 05:08.840
And maybe before we dive into any of the individual items,

05:08.840 --> 05:13.560
what's your take on 2022 broadly

05:14.560 --> 05:19.560
and some of the areas that you are most excited about

05:20.160 --> 05:21.000
in the year?

05:21.000 --> 05:22.640
Yeah, so I think broadly speaking

05:22.640 --> 05:25.720
and we'll delve deeper into a bunch of these topics.

05:25.720 --> 05:30.160
But broadly speaking, I think the importance of data

05:30.160 --> 05:33.240
and the importance of looking at what might be

05:33.240 --> 05:35.800
in the pre-chaining data has sort of brought up

05:35.800 --> 05:39.360
back into the focus in a way that I feel earlier years,

05:39.360 --> 05:42.080
we were a lot more agnostic of what the model was being

05:42.080 --> 05:46.000
trained on and just more data was better and a thing.

05:46.000 --> 05:49.560
This year it's been a lot more sort of thinking

05:49.560 --> 05:51.920
about what goes in the models.

05:51.920 --> 05:55.080
And also thinking of ways to use the models

05:55.080 --> 05:59.240
not just by simply prompting it with a simple thing,

05:59.240 --> 06:00.880
but trying to get it to reason,

06:00.880 --> 06:03.840
trying to get it to break down the problem into pieces

06:03.840 --> 06:07.920
and try to evaluate how much the language models can do that.

06:07.920 --> 06:09.840
And that I think is key when you start thinking

06:09.840 --> 06:14.360
about taking language models to more higher level decision

06:14.360 --> 06:16.240
making or higher level reason.

06:16.240 --> 06:17.080
Awesome, awesome.

06:17.080 --> 06:20.240
What's the first area you'd like to dig into?

06:20.240 --> 06:24.280
So let's actually start with a chain of thought prompting.

06:24.280 --> 06:27.240
This is work coming out of Google

06:27.240 --> 06:29.680
that came out earlier this year.

06:29.680 --> 06:33.120
And I guess the easiest way to summarize is

06:33.120 --> 06:36.160
to say let's think step by step.

06:36.160 --> 06:39.600
The idea here is to have the model not just

06:39.600 --> 06:42.800
generate the answer directly, but try

06:42.800 --> 06:45.040
to have it go through the reasoning process

06:45.040 --> 06:47.680
and then arrive at the answer.

06:47.680 --> 06:54.040
This ended up being quite a strong like a quite an effective

06:54.040 --> 06:57.320
method to get the model to do a lot of things, especially

06:57.320 --> 06:59.240
when it comes to mathematical reasoning

06:59.240 --> 07:01.520
and sort of where you can break down the problems

07:01.520 --> 07:03.320
into a bunch of different steps.

07:03.320 --> 07:06.800
Chain of thought prompting did extremely well

07:06.800 --> 07:09.280
compared to what we had before.

07:09.280 --> 07:11.240
And part of the difference, I guess,

07:11.240 --> 07:14.280
was you're not just prompting with questions and answers,

07:14.280 --> 07:17.200
but you're also prompting with something

07:17.200 --> 07:18.840
that is much more detailed.

07:18.840 --> 07:21.520
So the prompt itself has a bunch of examples

07:21.520 --> 07:23.120
of breaking the reasoning down.

07:23.120 --> 07:25.120
And then you have the model being

07:25.120 --> 07:28.000
able to walk through that reasoning and get to the answer.

07:28.000 --> 07:34.520
And in that work is the idea that the user of the model

07:34.520 --> 07:37.560
should break the prompts down into more detail

07:37.560 --> 07:42.400
or that the model should learn how to kind of show its work

07:42.400 --> 07:47.320
and give in a course screen prompts, break it, break

07:47.320 --> 07:48.640
the prompt down itself.

07:48.640 --> 07:51.840
Yeah, so I think the initial paper focused on the user

07:51.840 --> 07:54.760
providing a few examples of this big down.

07:54.760 --> 07:56.480
So if you're saying, like, you know,

07:56.480 --> 07:59.000
here's a mathematical word problem.

07:59.000 --> 08:00.440
You have two op apples.

08:00.440 --> 08:03.120
And then somebody gives you double of that.

08:03.120 --> 08:04.760
How many apples do you have?

08:04.760 --> 08:07.160
Breaking it down into a double means times two

08:07.160 --> 08:08.280
and two times two is four.

08:08.280 --> 08:10.160
This is a very simple example.

08:10.160 --> 08:12.400
But this kind of giving an example or two

08:12.400 --> 08:18.600
of breaking this down can be quite powerful for that.

08:18.600 --> 08:20.600
Especially I think one of the key insights here

08:20.600 --> 08:24.920
and we can talk about other papers that sort of showed

08:24.920 --> 08:28.400
later things, but this is a very emergent property

08:28.400 --> 08:31.720
that seems to exist for really large language models.

08:31.720 --> 08:34.120
And as if you have smaller language models,

08:34.120 --> 08:36.920
it's kind of difficult to get them to do this kind of piece.

08:36.920 --> 08:39.640
And so that's also been exciting to see.

08:39.640 --> 08:41.520
Did the results there?

08:41.520 --> 08:42.880
Did you find them surprising?

08:42.880 --> 08:47.480
Were they counterintuitive that that would work?

08:47.480 --> 08:52.000
I think how well they worked were, I think

08:52.000 --> 08:54.800
it surprised everyone because it's a very simple idea

08:54.800 --> 08:56.600
to just break it down a little bit.

08:56.600 --> 08:58.920
Everybody kind of assumed that the transformers

08:58.920 --> 09:01.080
are sort of either doing this internally

09:01.080 --> 09:03.400
or completely not doing this internally, right?

09:03.400 --> 09:07.440
And by showing you that if you actually write

09:07.440 --> 09:10.560
out a bunch of examples, these transformers models

09:10.560 --> 09:13.560
are able to do this to the extent that they are

09:13.560 --> 09:17.840
was quite surprising and the gains were quite impressive.

09:17.840 --> 09:20.040
Can you talk a little bit about the evaluation

09:22.040 --> 09:23.280
of that method?

09:23.280 --> 09:25.200
Yeah, so the evaluation was mostly focused

09:25.200 --> 09:27.560
on mathematical world problems.

09:27.560 --> 09:30.720
So there's this GSM in, GSM 8 dataset,

09:30.720 --> 09:35.640
and then there's this A-A-W-Q-S MOPS, I guess, dataset as well.

09:35.640 --> 09:38.080
These are mathematical world problems.

09:38.080 --> 09:41.400
And this first evaluation was mostly looking at

09:41.400 --> 09:45.120
how well you can do a reason through some of those.

09:45.120 --> 09:48.440
And yeah, it was much, much better

09:48.440 --> 09:51.880
than anything that we had before.

09:51.880 --> 09:53.960
And then there were, they had some evaluations

09:53.960 --> 09:55.640
on symbolic reasoning as well.

09:55.640 --> 10:01.120
So if you give them sort of tasks which have, like,

10:03.720 --> 10:06.520
you know, like, let have a bunch of,

10:07.640 --> 10:10.280
so like finding a character inside a long string.

10:10.280 --> 10:13.800
Like what is the fifth character or something like that?

10:13.800 --> 10:15.680
You can break it down into a bunch of steps

10:15.680 --> 10:17.760
and if you give it a few examples, it can do it.

10:17.760 --> 10:19.440
If you don't give it a few examples

10:19.440 --> 10:22.160
of how to break it down the models are very bad.

10:22.160 --> 10:25.800
And have you seen any work that looks to extend this

10:25.800 --> 10:30.120
beyond the kind of math and symbolic domain?

10:30.120 --> 10:34.840
So beyond, I'll talk a little bit about some of related ideas

10:34.840 --> 10:37.760
and sort of question answering a little bit later.

10:37.760 --> 10:42.040
But there is one work that is related that I like.

10:42.040 --> 10:45.440
This is called algorithmic prompting.

10:45.440 --> 10:49.040
And this is stuff that came out of Google brain as well.

10:49.040 --> 10:50.200
So, you know, a lot of the,

10:50.200 --> 10:51.800
this stuff is coming out of Google brain

10:51.800 --> 10:54.520
because you need really large language models

10:54.520 --> 10:58.840
to be able to work with this or even bigger than GPD3, for example.

10:58.840 --> 11:02.000
So in this algorithmic prompting paper,

11:02.000 --> 11:04.600
this was kind of interesting where they had,

11:04.600 --> 11:07.160
essentially the same idea as Jane of Thought,

11:07.160 --> 11:10.000
except that they go really detailed

11:10.000 --> 11:12.840
into what those reasoning steps would be.

11:12.840 --> 11:17.280
So they mostly focus on, you know, things that can be

11:17.280 --> 11:19.720
described more as an algorithm rather than us

11:19.720 --> 11:22.400
just breaking it into a few pieces.

11:22.400 --> 11:23.960
So you can say things like,

11:23.960 --> 11:28.880
if I had to add 12 plus 24, right?

11:28.880 --> 11:30.320
How would you do that?

11:30.320 --> 11:32.400
They literally break it down into digits.

11:32.400 --> 11:36.200
Oh, you take the ones place that's two in one case four

11:36.200 --> 11:39.000
and the other, you add them up, you get six,

11:39.000 --> 11:40.480
there is no carry.

11:40.480 --> 11:41.640
Okay, that's that's one.

11:41.640 --> 11:43.040
This is the second step is looking.

11:43.040 --> 11:45.200
Now look at the next 10th place.

11:45.200 --> 11:47.680
It's one and two, add them up three.

11:47.680 --> 11:49.680
Look at the carry or the carry is zero.

11:49.680 --> 11:51.880
So it's just three and then 36, right?

11:51.880 --> 11:54.560
So all of this, this very detailed breakdown,

11:54.560 --> 11:56.560
which looked like extremely detailed.

11:57.440 --> 12:00.200
But what was really impressive to me about that paper

12:00.200 --> 12:03.560
is they showed that you can give examples

12:03.560 --> 12:06.480
of really low digit operations.

12:06.480 --> 12:08.800
So like maybe two or three digit operations

12:08.800 --> 12:10.280
when you're talking about addition

12:10.280 --> 12:13.480
or multiplication, any of these things.

12:13.480 --> 12:15.720
But at test time, you can firstly,

12:15.720 --> 12:18.120
even on two or three digit stuff,

12:18.120 --> 12:21.400
it was much, much accurate compared to regular chain of thought.

12:22.360 --> 12:26.680
Like, you know, 20% going from 80% of for chain of thought

12:26.680 --> 12:28.240
to something that's 100%.

12:28.240 --> 12:29.880
I'm kind of making up numbers.

12:29.880 --> 12:33.720
And this is relative to asking for the model

12:33.720 --> 12:37.160
to solve the same problem without any intermediate steps.

12:37.160 --> 12:39.840
No, so without any steps is even worse, right?

12:39.840 --> 12:42.360
So this is asking the model to,

12:42.360 --> 12:44.160
so like 12 plus 24,

12:44.160 --> 12:46.080
I don't know exactly what the chain of thought would be,

12:46.080 --> 12:47.960
but it would be something that would be

12:47.960 --> 12:51.040
at a higher granularity, let's just say, right?

12:51.040 --> 12:54.840
And so when you go, when you give this detail from

12:54.840 --> 12:58.280
the models are more accurate, which is not so surprising.

12:58.280 --> 13:01.160
But what was surprising was that they kept increasing

13:01.160 --> 13:05.600
the size of the number at the test time.

13:05.600 --> 13:08.320
So started adding more and more digits

13:08.320 --> 13:12.480
and even up to 18 digit numbers,

13:12.480 --> 13:15.680
the model is able to do these operations

13:15.680 --> 13:17.560
much, much more accurately.

13:17.560 --> 13:19.240
Because even though the problems were only

13:19.240 --> 13:23.960
on two or three digits, sort of numbers, right?

13:23.960 --> 13:27.600
And so does this, does this type of work?

13:28.800 --> 13:31.520
Answer definitively whether, you know,

13:31.520 --> 13:35.480
this is already happening inside the model versus

13:35.480 --> 13:38.640
there's some other effects like, in a sense,

13:38.640 --> 13:42.360
it's really counterintuitive that it would work at all.

13:42.360 --> 13:45.600
Like, you know, there's no registers inside the model

13:45.600 --> 13:47.400
that are tracking digits, you know,

13:47.400 --> 13:49.160
the ones place and attend the place.

13:49.160 --> 13:51.040
Like, why should that work?

13:51.040 --> 13:53.720
Yeah, so I think there are, there are,

13:53.720 --> 13:55.320
people are still trying to come to terms

13:55.320 --> 13:57.200
with white end of thought reasoning works.

13:57.200 --> 13:58.840
Is there something in the pre-gaining data?

13:58.840 --> 14:00.200
Is there something in the model?

14:00.200 --> 14:02.800
And there's been some interesting work there.

14:02.800 --> 14:04.840
But no, I think the tricky thing here

14:04.840 --> 14:07.560
is you're making all of these things explicit.

14:07.560 --> 14:12.560
So you're not relying on the model to keep these bits

14:12.560 --> 14:15.280
somewhere latent in its sort of memory, right?

14:15.280 --> 14:16.560
Like you're making it explicit

14:16.560 --> 14:18.640
and of course it's attending to all of that.

14:18.640 --> 14:22.200
And so the chances of it sort of going away

14:22.200 --> 14:24.840
into a wrong place is much lower.

14:24.840 --> 14:28.360
So, you know, scratch pad and a bunch of other papers

14:28.360 --> 14:31.600
had similar ideas of like, hey, let's give some model,

14:31.600 --> 14:33.960
some space to think about things, right?

14:33.960 --> 14:38.400
So it's possible that this is just letting the model

14:38.400 --> 14:39.880
actually think things through.

14:39.880 --> 14:41.640
So it's somehow more computation

14:41.640 --> 14:43.600
that the model is getting.

14:43.600 --> 14:45.560
And then there've been some papers showing that, yeah,

14:45.560 --> 14:48.040
that might be the difference, the fact that you're

14:48.040 --> 14:50.680
generating a single number, but you're letting,

14:50.680 --> 14:53.080
not just asking the model to give it one shot,

14:53.080 --> 14:54.480
but letting it think about it.

14:56.280 --> 14:58.360
And it's not so much the fact that you're giving

14:58.360 --> 15:00.280
these examples breakdowns that helps.

15:00.280 --> 15:02.600
But I think, you know, as many of these things,

15:02.600 --> 15:04.400
I'm sure the answer is complicated

15:04.400 --> 15:06.680
and it's some combination of things.

15:06.680 --> 15:09.640
The last thing you said almost sounds like

15:09.640 --> 15:11.600
the kind of multitask argument.

15:11.600 --> 15:14.480
It's not that, you know, the specific other thing

15:14.480 --> 15:18.240
that you're asking the model to do matters,

15:18.240 --> 15:20.400
but that you're asking it to do another thing

15:20.400 --> 15:24.120
and that kind of, you know, on the traditional side,

15:24.120 --> 15:26.440
like has some kind of regularization effect

15:26.440 --> 15:29.400
or some kind of effect that causes your results

15:29.400 --> 15:31.960
to be better just by overloading the model

15:31.960 --> 15:32.920
a little bit.

15:32.920 --> 15:34.400
Yeah, yeah, exactly, right?

15:34.400 --> 15:36.680
So you're letting, in some sense,

15:36.680 --> 15:41.040
you have more activations, you have more slated states,

15:41.040 --> 15:44.560
you just have giving model and more things to do.

15:44.560 --> 15:48.880
And so it has space to explore through more reasoning.

15:48.880 --> 15:51.280
So maybe that's one explanation for why this kind of stuff

15:51.280 --> 15:53.760
works, but yeah.

15:53.760 --> 15:54.640
Amazing, amazing.

15:54.640 --> 15:57.600
And I should have mentioned earlier on,

15:57.600 --> 15:58.920
but I will mention it now.

15:58.920 --> 16:00.800
All of the papers that we're referring

16:00.800 --> 16:03.400
to will be available on the show notes page,

16:03.400 --> 16:06.800
so folks can check them out.

16:06.800 --> 16:08.880
So the next thing that you had on your list

16:08.880 --> 16:10.840
was decomposed reasoning.

16:10.840 --> 16:13.080
It sounds like it's in a similar vein.

16:13.080 --> 16:15.680
Yeah, so I think this is, that's why I kind of put them

16:15.680 --> 16:17.480
together, but I think fundamentally

16:17.480 --> 16:20.280
this is a very different approach to the same idea.

16:20.280 --> 16:22.680
So yes, I think terminology is something

16:22.680 --> 16:24.600
that the field is going to be revisiting

16:24.600 --> 16:26.680
and decomposed using is kind of something

16:26.680 --> 16:27.880
that I give up with.

16:27.880 --> 16:29.800
I don't even know if people use it.

16:29.800 --> 16:31.280
But the idea here is that there have been

16:31.280 --> 16:32.560
a bunch of papers here, and I'm just

16:32.560 --> 16:35.520
going to sort of put here on through some of them.

16:35.520 --> 16:38.840
But the idea here is that you shouldn't rely on the language

16:38.840 --> 16:41.400
model alone to do the whole task.

16:41.400 --> 16:44.640
So suppose I give it a mathematical word problem,

16:44.640 --> 16:46.920
or if I give it a question answering problem,

16:46.920 --> 16:49.120
that's a lot complicated.

16:49.120 --> 16:52.960
I shouldn't rely on the model and its parameters

16:52.960 --> 16:54.760
to be able to carry everything out.

16:54.760 --> 16:57.240
Maybe the model needs to use a calculator.

16:57.240 --> 17:00.560
Maybe the model needs to do a web search.

17:00.560 --> 17:03.720
Maybe the model needs to even write a small Python script

17:03.720 --> 17:07.680
and actually run it to get the answer that I want.

17:07.680 --> 17:12.560
And so there's this whole idea of language models

17:12.560 --> 17:15.280
getting what you need, but not just relying on its own

17:15.280 --> 17:18.040
parameters, but breaking down your problem

17:18.040 --> 17:20.480
and figuring out, oh, I need to call something else.

17:20.480 --> 17:23.360
And this is what I'm going to do to call it.

17:23.360 --> 17:25.800
It's an idea that sort of claim came out

17:25.800 --> 17:28.080
post chain of thoughts sort of middle of the year,

17:28.080 --> 17:30.120
but they've been a bunch of papers all the way

17:30.120 --> 17:35.000
to the end of the year that have been doing a lot of this.

17:35.000 --> 17:37.760
So yeah, it's kind of been exciting.

17:37.760 --> 17:41.280
A lot of them have been on the QA side of things.

17:41.280 --> 17:44.640
So the two I'll mention is success at prompting that came

17:44.640 --> 17:47.400
out of my group, but there's also decomposed prompting

17:47.400 --> 17:49.040
that came out of AI2.

17:49.040 --> 17:50.640
And the idea behind both of these

17:50.640 --> 17:53.280
was to take a complex question, break it down

17:53.280 --> 17:58.400
into simpler ones, and then have the language model

17:58.400 --> 18:01.320
sort of call another language model

18:01.320 --> 18:05.160
that is answering each of these simple questions.

18:05.160 --> 18:07.480
So if a simple question is a mathematical operation,

18:07.480 --> 18:08.880
then you would use a calculator.

18:08.880 --> 18:13.080
If a simple question is a very simple lookup question,

18:13.080 --> 18:16.360
then you would use something that is like a squad style

18:16.360 --> 18:18.240
question answering system, things like that, right?

18:18.240 --> 18:21.880
So being able to take what the user wants

18:21.880 --> 18:24.360
and breaking down into pieces and then

18:24.360 --> 18:27.560
composing the answers together to give you the actual answer,

18:27.560 --> 18:29.080
this is the answer.

18:29.080 --> 18:31.600
Can you talk a little bit in a little bit more detail

18:31.600 --> 18:33.760
the difference between successive prompting

18:33.760 --> 18:35.160
and decomposed prompting?

18:35.160 --> 18:37.760
How did the settings for those differ?

18:37.760 --> 18:40.320
They came out pretty much around the same time.

18:40.320 --> 18:43.840
So it's difficult to sort of, and they sort of appeared

18:43.840 --> 18:46.280
at the same conference as well.

18:46.280 --> 18:48.720
I think, yeah, so I think some of it

18:48.720 --> 18:51.800
depended on sort of which data set they use.

18:51.800 --> 18:55.600
So decomposed prompting used specifically multi-hop data

18:55.600 --> 18:58.720
sets and sort of trying to decompose it that way.

18:58.720 --> 19:01.600
Successive prompting focused a little bit more

19:01.600 --> 19:04.480
on calculations and symbolic operations as well.

19:04.480 --> 19:06.960
So yeah, I would say the difference is between them.

19:06.960 --> 19:09.280
But kind of same idea, different data sets,

19:09.280 --> 19:10.760
slightly different tooling.

19:10.760 --> 19:12.360
Right, right, yeah.

19:12.360 --> 19:15.040
And we'll see, in some of these cases,

19:15.040 --> 19:17.360
other pairs of papers also that are very similar

19:17.360 --> 19:21.440
that came out around the same time because that's where we are.

19:21.440 --> 19:23.400
How about tool augmented?

19:23.400 --> 19:24.920
So tool augmented stuff.

19:24.920 --> 19:29.480
So there was a paper coming out of Google, I believe,

19:29.480 --> 19:32.080
called tool augmented language models.

19:32.080 --> 19:33.560
So down is a paper.

19:33.560 --> 19:37.320
And this is one of the papers that was essentially showing

19:37.320 --> 19:41.000
that you can have, instead of just calling a calculator

19:41.000 --> 19:43.520
explicitly or just having a fixed set of things,

19:43.520 --> 19:48.080
you can create a description of API is that the language model

19:48.080 --> 19:49.360
has access to.

19:49.360 --> 19:53.080
And have the language model itself generate

19:53.080 --> 19:57.120
example calls to that API when it's doing an output, right?

19:57.120 --> 20:01.480
So if I want to say like, hey, GPT-3 or whatever,

20:01.480 --> 20:04.560
how hot is it going to get today, right?

20:04.560 --> 20:08.240
Or how hot is it going to get today in Irvine?

20:08.240 --> 20:09.880
The language model is going to say, okay,

20:09.880 --> 20:13.720
this is a question about the weather in Irvine.

20:13.720 --> 20:16.440
So I'm going to compose an API called

20:16.440 --> 20:18.720
to a weather service.

20:18.720 --> 20:21.720
That's going to say, what's the weather in Irvine?

20:21.720 --> 20:24.520
And then it'll return some JSON object that says,

20:24.520 --> 20:26.400
oh, the high is this, low is this,

20:26.400 --> 20:28.320
probability of rain is this.

20:28.320 --> 20:30.680
And then the language model will kick in again

20:30.680 --> 20:33.720
and take that output and say, oh, it's going to be

20:33.720 --> 20:37.360
pretty hot today, as since it is Southern California.

20:37.360 --> 20:39.240
And yeah, you know, something.

20:39.240 --> 20:41.280
Seems like this research is heading in the direction

20:41.280 --> 20:44.880
of how would you kind of rebuild Siri or, you know,

20:44.880 --> 20:47.200
Alexa or something like that with LLMs.

20:47.200 --> 20:48.040
Yes, yeah.

20:48.040 --> 20:50.720
And I think this is one of the key sort of advantages

20:50.720 --> 20:53.680
of these language models is not that they can do

20:53.680 --> 20:55.600
additions and subtractions internally.

20:55.600 --> 20:58.080
Like I think that's interesting from an intellectual point

20:58.080 --> 20:58.680
of view.

20:58.680 --> 21:00.280
But when you're making actual products,

21:00.280 --> 21:02.840
you want this language model to,

21:02.840 --> 21:05.520
language is a way to interface with things

21:05.520 --> 21:07.600
that are external to you, right?

21:07.600 --> 21:10.320
So the language models should take in the user queries,

21:10.320 --> 21:13.600
but also be the interface to other things outside

21:13.600 --> 21:15.600
and be able to query it.

21:15.600 --> 21:17.560
I think we will talk a little bit about that later.

21:17.560 --> 21:21.240
But one of the reasons I like this is you can also somehow

21:21.240 --> 21:24.120
now attribute the answer that you're getting,

21:24.120 --> 21:26.920
not to some internal parameter in the language model,

21:26.920 --> 21:29.840
but to say, look, this is the API call I made.

21:29.840 --> 21:31.320
And this is the answer I got.

21:31.320 --> 21:33.960
And now that's what that's the answer I gave you.

21:33.960 --> 21:36.240
So in some sense, it becomes a little bit more

21:36.240 --> 21:37.960
after you attribute it.

21:37.960 --> 21:43.200
The idea of the language model writing a program

21:43.200 --> 21:47.480
to figure out the answer to a question

21:47.480 --> 21:48.880
is a fascinating one.

21:48.880 --> 21:53.880
And it almost feels like if anything around LLM

21:53.880 --> 21:57.680
is going to be the path to AGI, it's like it's that.

21:57.680 --> 22:01.200
What was your reaction to that research?

22:01.200 --> 22:03.880
Yeah, I think it seems quite like to me

22:03.880 --> 22:05.280
from a practical point of view,

22:05.280 --> 22:09.160
it seems quite exciting.

22:09.160 --> 22:10.640
From a code generation point of view,

22:10.640 --> 22:12.600
and things like that, it's useful as well.

22:12.600 --> 22:15.760
But the nice thing about the code writing code

22:15.760 --> 22:17.600
is that it's unambiguous.

22:17.600 --> 22:20.600
So it's making some calls to an external database.

22:20.600 --> 22:24.000
If I want to update the language model or update

22:24.000 --> 22:27.960
this whole system, I can just update my knowledge directly.

22:27.960 --> 22:29.560
The knowledge is external somehow

22:29.560 --> 22:31.880
to the parameterization of the language model.

22:31.880 --> 22:34.000
That makes it super convenient to delete things,

22:34.000 --> 22:38.440
or to add things, or to get attributions, and all these things.

22:38.440 --> 22:41.960
And the interface to that data source

22:41.960 --> 22:45.600
is always programs, either it's like a simple API call

22:45.600 --> 22:47.400
or a more complex one.

22:47.400 --> 22:49.360
And I think I really like this idea

22:49.360 --> 22:52.040
because it allows the language models

22:52.040 --> 22:54.040
to do things that it should be doing,

22:54.040 --> 22:56.920
which is to understand language, or let's not call it understand,

22:56.920 --> 23:02.760
would be able to parse language, be able to sort of transform it.

23:02.760 --> 23:05.840
But it doesn't necessarily have to know

23:05.840 --> 23:09.400
the temperature of Irvine every day, or things like that.

23:09.400 --> 23:11.080
That's not something I necessarily want,

23:11.080 --> 23:13.400
but I mean, this is like this model.

23:13.400 --> 23:14.880
Yeah.

23:14.880 --> 23:22.560
So just very subtly in there, you kind of addressed

23:22.560 --> 23:25.600
another big conversation that's happening in the community

23:25.600 --> 23:29.680
now in this idea of do language models understand.

23:29.680 --> 23:31.440
You call this decomposed reasoning.

23:31.440 --> 23:33.880
The thing is writing programs that kind of requires

23:33.880 --> 23:35.040
some kind of reasoning.

23:35.040 --> 23:39.720
Like, what's your take on these broader questions

23:39.720 --> 23:46.000
about reasoning and understanding in LLMs?

23:46.000 --> 23:47.760
Or would you like to defer that?

23:47.760 --> 23:52.480
Is there a natural point later for us to talk about that?

23:52.480 --> 23:55.120
Come back to it a little bit later.

23:55.120 --> 23:57.400
Maybe even in the next section or sort of,

23:57.400 --> 24:00.320
we are trying to sort of question what reasoning is

24:00.320 --> 24:03.560
and trying to evaluate that in some sense.

24:03.560 --> 24:05.320
But yeah.

24:05.320 --> 24:07.800
The semantic argument around understanding,

24:07.800 --> 24:11.560
like that's not that interesting, but like how a language model

24:11.560 --> 24:16.280
can reason, and the extent to which it's reasoning

24:16.280 --> 24:22.040
versus like cutting, pasting at some level beyond

24:22.040 --> 24:23.840
at an impressive, in an impressive way,

24:23.840 --> 24:25.840
like that's kind of really interesting.

24:25.840 --> 24:26.720
Yeah, definitely.

24:26.720 --> 24:30.880
So I would say, and then even pushing it a little bit further,

24:30.880 --> 24:33.160
like what are the consequences of the fact

24:33.160 --> 24:35.960
that it is cutting, pasting versus its reasoning, right?

24:35.960 --> 24:39.200
So how should we calibrate what things

24:39.200 --> 24:41.200
these should be deployed for and what things

24:41.200 --> 24:43.920
this should not be deployed for based on the situations?

24:43.920 --> 24:46.680
Those are the kind of things that I'm really, really interested.

24:46.680 --> 24:48.760
Awesome, awesome, awesome.

24:48.760 --> 24:50.040
Is that your next section?

24:50.040 --> 24:52.440
Yes, and that sort of ties in very well

24:52.440 --> 24:56.000
with what I think is exciting next, which is,

24:56.000 --> 24:59.360
I'm going to call it sort of understanding

24:59.360 --> 25:02.360
the relationship between the data, the pre-training data,

25:02.360 --> 25:04.720
and the output of the model.

25:04.720 --> 25:08.160
And I feel like there is, again, a few different threads here,

25:08.160 --> 25:09.960
but there is one that came out of my group

25:09.960 --> 25:15.200
that I think is a simple idea that really sort of captures

25:15.200 --> 25:17.520
exactly what you said, the cutting, pasting versus

25:17.520 --> 25:19.080
a reasoning thing.

25:19.080 --> 25:21.400
So this paper is called impact of pre-training term

25:21.400 --> 25:24.440
frequencies on few short reasoning.

25:24.440 --> 25:27.000
And the idea here is we were looking only

25:27.000 --> 25:28.600
at numerical reasoning right now.

25:28.600 --> 25:32.440
So we started looking at all of these examples of,

25:32.440 --> 25:36.480
oh, a GPT-3 can do addition and multiplication

25:36.480 --> 25:37.720
and things like that.

25:37.720 --> 25:39.640
And we started looking at the instances

25:39.640 --> 25:43.280
and turns out that it doesn't always do it, right?

25:43.280 --> 25:44.680
It's not 100% at those.

25:44.680 --> 25:49.160
It's 80% or 90% or whatever the number is.

25:49.160 --> 25:53.000
So we started looking at, OK, what differentiates the one

25:53.000 --> 25:57.240
that gets correct and do things that it doesn't get corrected.

25:57.240 --> 26:01.640
So for example, we saw that if you ask it, what is 24 times

26:01.640 --> 26:04.200
18, the model gets it right.

26:04.200 --> 26:05.840
It says 432.

26:05.840 --> 26:11.360
If you say, what is 23 times 18, the model gets it wrong.

26:11.360 --> 26:13.280
So 24 times 18 is correct.

26:13.280 --> 26:15.560
23 times 18 is not correct.

26:15.560 --> 26:17.920
Is this random, like what's going on here?

26:17.920 --> 26:19.960
And did you just enter up there?

26:19.960 --> 26:24.680
Did you find that consistent across invocations?

26:24.680 --> 26:27.480
I've run into that kind of thing.

26:27.480 --> 26:28.680
We've all run into that kind of thing

26:28.680 --> 26:30.600
playing with chat GPT and other things.

26:30.600 --> 26:34.080
And sometimes it gets certain things consistently wrong.

26:34.080 --> 26:36.040
Other times, it gets the thing wrong.

26:36.040 --> 26:37.560
Sometimes and not wrong.

26:37.560 --> 26:39.720
Other times, I get to random seed kind of thing

26:39.720 --> 26:41.720
or something else going on in the model.

26:41.720 --> 26:43.240
Did you explore that at all?

26:43.240 --> 26:44.600
Yeah, we definitely saw that.

26:44.600 --> 26:47.720
So it's both like if you're doing few short prompting,

26:47.720 --> 26:50.320
which examples you put in the prompt would sometimes

26:50.320 --> 26:52.920
change to the output or how you phrase it.

26:52.920 --> 26:56.960
Like you do you say, what is 24 times 18 or what is 24x18?

26:56.960 --> 27:00.560
You know, things like that definitely made a difference.

27:00.560 --> 27:04.240
But even after averaging these things out,

27:04.240 --> 27:07.600
we saw that 24 times 18 was in general more accurate

27:07.600 --> 27:09.880
than 23 times 18.

27:09.880 --> 27:13.440
And even more than that, we did even further analysis.

27:13.440 --> 27:17.280
And it turns out that all of our instances

27:17.280 --> 27:21.680
that involved 24, the model was much more accurate on

27:21.680 --> 27:24.600
than all of the instances that involved 23.

27:24.600 --> 27:28.480
Well, so we decided to do this for everything

27:28.480 --> 27:31.600
from 0 to 100.

27:31.600 --> 27:33.360
So all two data numbers, essentially,

27:33.360 --> 27:34.960
single and two data numbers.

27:34.960 --> 27:36.320
And no, it's a whole spectrum.

27:36.320 --> 27:38.920
And we didn't see a clear reason why

27:38.920 --> 27:42.800
some things are low accuracy, some things are high accuracy.

27:42.800 --> 27:44.920
And so then what we decided to do,

27:44.920 --> 27:48.440
this is the part that I think I quite excited about.

27:48.440 --> 27:50.720
We started to count how many times

27:50.720 --> 27:52.640
do each number, each of these numbers

27:52.640 --> 27:55.320
appear in the pre-getting data.

27:55.320 --> 27:59.720
And turns out, and you can see the plot in the figure.

27:59.720 --> 28:03.320
If you plot the log of the frequency of these terms,

28:03.320 --> 28:05.400
and now how accurate the models are,

28:05.400 --> 28:09.840
it is pretty much exactly like a.

28:09.840 --> 28:11.560
Which is intuitive.

28:11.560 --> 28:14.560
The model does better on things that it sees a lot of, right?

28:14.560 --> 28:16.040
Yes, yeah.

28:16.040 --> 28:20.080
But yeah, so it's also expected yet disappointing

28:20.080 --> 28:23.800
because you don't want it to be such a nice strong curve,

28:23.800 --> 28:25.040
like you want it to do.

28:25.040 --> 28:27.520
Like if it's doing mathematical reasoning,

28:27.520 --> 28:30.000
it should know that 23 is one less than 24,

28:30.000 --> 28:31.440
and all of these things, right?

28:31.440 --> 28:34.960
So I think it's one of these things

28:34.960 --> 28:38.360
where it was expected that the model would be better

28:38.360 --> 28:40.000
on things it has seen before.

28:40.000 --> 28:42.000
But you also, at the same time,

28:42.000 --> 28:44.400
hold this thing of like, oh, it is able to reason,

28:44.400 --> 28:45.520
it is able to do these things,

28:45.520 --> 28:48.920
and it's kind of difficult to resolve both of those, right?

28:48.920 --> 28:50.920
So this was one example.

28:50.920 --> 28:53.000
I think we are barely scratching the surface,

28:53.000 --> 28:54.480
but this was an example of paper

28:54.480 --> 28:58.840
that sort of started looking at some of these pre-training

28:58.840 --> 29:01.520
statistics, or not just single term frequencies,

29:01.520 --> 29:03.760
but diagram frequencies and things like that,

29:03.760 --> 29:07.840
and show that the model is quite sensitive

29:07.840 --> 29:09.920
to what these things should be, right?

29:09.920 --> 29:11.920
And I don't want to sort of make a claim

29:11.920 --> 29:13.920
that there is cutting, wasting, going on,

29:13.920 --> 29:15.960
or any of these things.

29:15.960 --> 29:18.640
But this effect is so strong that at least

29:18.640 --> 29:20.240
when we think about reasoning,

29:20.240 --> 29:22.880
and when we are evaluating reasoning

29:22.880 --> 29:24.680
in these language models,

29:24.680 --> 29:27.760
we should be taking this effect into account.

29:28.760 --> 29:31.080
And this may be a side note.

29:31.080 --> 29:34.680
It looks like the model that you evaluated with GPTJ,

29:34.680 --> 29:40.000
and clearly that's a model.

29:40.000 --> 29:41.760
So an open source model that you had access

29:41.760 --> 29:45.880
to the pre-training data kind of asks questions about,

29:45.880 --> 29:48.960
how do you get the same kind of insight

29:48.960 --> 29:52.440
into these models that are behind APIs?

29:52.440 --> 29:54.520
Yeah, so I think the question is,

29:54.520 --> 29:58.720
I kind of don't mind that models are behind APIs

29:58.720 --> 30:00.120
to some degree that's commercially

30:00.120 --> 30:01.680
that that kind of makes sense.

30:01.680 --> 30:04.000
I feel a little bit disappointing

30:04.000 --> 30:07.320
that the training data also is behind

30:07.320 --> 30:08.440
sort of close wall, right?

30:08.440 --> 30:11.360
So I know that there is a lot in the training data,

30:11.360 --> 30:13.280
but if we want to be able to understand

30:13.280 --> 30:16.160
why GPTJ works, or why GPTJ works,

30:16.160 --> 30:19.720
or even generally, when do language models work,

30:19.720 --> 30:21.120
when are they safe to deploy?

30:21.120 --> 30:22.680
All of these get a question.

30:22.680 --> 30:24.920
I think it's okay if the language model

30:24.920 --> 30:26.920
we only have a black box access to,

30:26.920 --> 30:29.680
but it would be good to have access to the training data.

30:29.680 --> 30:31.480
It would be good to have access to a bunch

30:31.480 --> 30:33.400
of these other things that can help us

30:33.400 --> 30:35.600
sort of do simple kind of analysis like this,

30:35.600 --> 30:37.760
and maybe more complex ones.

30:37.760 --> 30:41.240
And actually be able to sort of decide

30:41.240 --> 30:42.520
what to do with the model, okay?

30:42.520 --> 30:45.880
So I think this whole direction of trying to understand

30:45.880 --> 30:48.320
what's in the pre-training data, I think is key

30:48.320 --> 30:51.200
and then something that will persist

30:51.200 --> 30:52.760
for the next couple of years.

30:52.760 --> 30:57.040
Do you think we have the right tools to do that at scale?

30:57.040 --> 31:00.480
I'm imagining that was not an easy task

31:00.480 --> 31:03.480
to do just for simple mathematical problems.

31:03.480 --> 31:08.480
That's true, but training GPT-3 is also not a simple problem

31:09.400 --> 31:10.880
and people have solved it, right?

31:10.880 --> 31:15.720
So I think the tooling is something that everybody right now

31:15.720 --> 31:19.320
is sort of excited about building tools

31:19.320 --> 31:21.960
that actually give information and insights

31:21.960 --> 31:23.000
into these language models.

31:23.000 --> 31:25.760
And I think even at the idea,

31:25.760 --> 31:28.320
we are at sort of early stages of trying to do these things

31:28.320 --> 31:31.680
of building some tooling that can support this kind of analysis.

31:31.680 --> 31:34.040
But you know, if the data set is available,

31:34.040 --> 31:35.800
I think people will do amazing things.

31:35.800 --> 31:39.240
And I thought this would be impossible

31:39.240 --> 31:40.720
and it seemed like crazy.

31:40.720 --> 31:43.360
Like, hey, this is almost a terabyte of text.

31:43.360 --> 31:45.760
Like how can you do anything with that?

31:45.760 --> 31:49.760
And it was not trivial, but it was easier than impossible.

31:50.640 --> 31:53.440
How do you identify,

31:54.520 --> 31:56.760
you know, so you identified some behavior,

31:58.360 --> 32:00.200
the relationship between accuracy

32:00.200 --> 32:04.400
and frequency in the training data.

32:07.240 --> 32:11.840
How do you identify what that is a consequence of?

32:11.840 --> 32:15.600
Meaning is it specific to the way GPT-J was trained?

32:15.600 --> 32:18.840
Is it all transformer-based language models?

32:18.840 --> 32:23.040
Is it, you know, maybe something about that particular data set?

32:23.040 --> 32:27.680
Like, have you, are you able to say that it is

32:27.680 --> 32:32.400
a broad characteristic of LLMs in general

32:32.400 --> 32:35.160
based on the work that you've done thus far?

32:35.160 --> 32:38.840
That's a little bit difficult to sort of.

32:38.840 --> 32:40.960
Yeah, that's a little bit difficult to measure,

32:40.960 --> 32:43.840
partly because we don't have data set available

32:43.840 --> 32:45.640
for too many models, right?

32:45.640 --> 32:49.440
So at least we tried the whole slew of the Luther models

32:49.440 --> 32:50.880
that were trained of the same data set

32:50.880 --> 32:56.040
and we saw similar effects on different model sizes, essentially.

32:56.040 --> 32:59.560
And yeah, as data sets become pre-training data sets

32:59.560 --> 33:03.240
become more standard, it's fairly trivial to sort of extend this stuff.

33:03.240 --> 33:07.280
Since this paper, we also have sort of an online demo

33:07.280 --> 33:09.120
where we have a bunch of more tasks

33:09.120 --> 33:11.880
that try to go beyond mathematical reasoning.

33:11.880 --> 33:14.960
It's a little bit difficult to sort of even define

33:14.960 --> 33:16.760
what these sort of terms are

33:16.760 --> 33:19.680
and what you should be computing frequency of.

33:19.680 --> 33:22.440
But yeah, I think we should be able to do this stuff

33:22.440 --> 33:24.960
for other tasks and for other models.

33:24.960 --> 33:29.160
And to me, I think this is somehow a consequence

33:29.160 --> 33:33.320
of a language modeling loss that encourages us in some sense, right?

33:33.320 --> 33:38.200
So yes, the model has seen more and it'll be more accurate,

33:38.200 --> 33:40.320
but even the ones that it has seen less,

33:40.320 --> 33:42.520
like it has still seen billions of times.

33:42.520 --> 33:45.440
So there is no reason for it to be wrong on it,

33:45.440 --> 33:48.800
except for the fact that the language modeling loss would sort of

33:48.800 --> 33:51.480
want you to be more right on the ones that have seen more.

33:51.480 --> 33:52.320
Mm-hmm.

33:54.400 --> 33:58.600
Yeah, another paper identified out of Yav Goldberg's group.

33:58.600 --> 33:59.440
Oh, yeah.

33:59.440 --> 34:02.640
So this is work led by, I think I'll quickly talk about this.

34:02.640 --> 34:07.280
So this had a similar sort of intuition

34:07.280 --> 34:09.760
for trying to look at things in the data

34:09.760 --> 34:14.920
and trying to figure out why the model has certain biases

34:14.920 --> 34:16.960
or has certain errors.

34:16.960 --> 34:18.840
And this was sort of a little bit more

34:18.840 --> 34:23.840
on trying to identify when two entities are related, right?

34:23.840 --> 34:27.360
So if you say where was Barack Obama born,

34:27.360 --> 34:32.320
the model tends to say Chicago or in some sense,

34:32.320 --> 34:37.240
it can say Washington and depending on how you praise it.

34:37.240 --> 34:39.680
And like, why does it give the wrong answer?

34:39.680 --> 34:41.440
It's kind of a question.

34:41.440 --> 34:45.440
Why does it not say Hawaii or something?

34:45.440 --> 34:49.600
And I think to be able to answer this question,

34:49.600 --> 34:51.480
you have to go back to the pre-training data

34:51.480 --> 34:53.880
and try to see like, okay, what did it even see?

34:53.880 --> 34:56.360
So what I like about this paper is it kind of tries

34:56.360 --> 34:59.480
to build use causality tools

34:59.480 --> 35:01.080
and builds a whole causal graph

35:01.080 --> 35:05.000
for where these kind of predictions might have come from

35:05.000 --> 35:07.320
and then tries to estimate all of the edges

35:07.320 --> 35:08.440
in those causality graphs

35:08.440 --> 35:10.240
and tries to do some causal inference

35:10.240 --> 35:13.160
to sort of attribute it to specific

35:13.160 --> 35:16.280
statistics of the big data.

35:16.280 --> 35:21.280
So in this causal graph would each individual document

35:21.760 --> 35:25.600
in the pre-training data be an intervention of sorts?

35:25.600 --> 35:29.000
So they sort of worked, they worked at the level of,

35:29.000 --> 35:32.120
I guess, triples or something like that, right?

35:32.120 --> 35:35.760
So let's say you see Obama in Chicago

35:35.760 --> 35:37.360
being a senator there or something, right?

35:37.360 --> 35:40.240
So this is kind of a triple.

35:40.240 --> 35:43.240
And so they work on statistics of those triples

35:43.240 --> 35:45.800
of the pre-training data to sort of make it tractable

35:45.800 --> 35:49.080
and make it sort of allow this inference to work.

35:49.080 --> 35:52.040
But in applying the causality machinery

35:52.040 --> 35:56.040
like are each of those interventions relative to

35:58.160 --> 36:03.160
some prior relationship between the things the triples?

36:03.360 --> 36:05.560
Yeah, so there is the true relationship

36:05.560 --> 36:06.680
between these triples

36:06.680 --> 36:08.800
and then there is the observed relationship

36:08.800 --> 36:13.000
between these triples and how many of these things,

36:13.000 --> 36:16.520
how many times it appeared in the pre-training data.

36:16.520 --> 36:19.320
And so the idea would be when you're doing it

36:19.320 --> 36:22.360
over many different entities and many different relations,

36:23.520 --> 36:28.280
do you, so those kind of become your whole data set

36:28.280 --> 36:29.120
in some sense.

36:29.120 --> 36:32.080
So Obama has appeared with Chicago,

36:32.080 --> 36:34.640
but Hillary Clinton has appeared elsewhere

36:34.640 --> 36:36.320
and on all of these things.

36:36.320 --> 36:41.320
And then together, which of these relations

36:41.760 --> 36:45.600
seem to affect a specific prediction the most?

36:45.600 --> 36:46.600
That counts.

36:46.600 --> 36:47.600
Awesome, awesome.

36:48.560 --> 36:52.640
Kind of continuing on in the data theme,

36:52.640 --> 36:57.640
there's been a ton of work looking at the need for clean data.

36:58.120 --> 37:00.240
I think maybe one of the most surprising things for me

37:00.240 --> 37:03.160
is like the return of supervision

37:03.160 --> 37:05.560
at the scale of LLMs.

37:05.560 --> 37:08.040
Talk a little bit about this category.

37:08.040 --> 37:10.840
Yeah, so this was somehow the most surprising category

37:10.840 --> 37:12.720
for me for this year.

37:12.720 --> 37:15.360
I will say that like after GBT's

37:15.360 --> 37:17.880
it came out and at the end of last year,

37:17.880 --> 37:21.120
everybody was kind of excited about language models,

37:21.120 --> 37:24.120
but the solutions for what's next always seem to be like,

37:24.120 --> 37:27.040
hey, let's get more data and let's get larger models

37:27.040 --> 37:28.720
and let's train, train longer.

37:28.720 --> 37:31.960
And those are still sort of useful things nobody's denying.

37:31.960 --> 37:33.560
But this year has shown that like,

37:33.560 --> 37:35.600
okay, you can actually do a lot

37:35.600 --> 37:39.160
if you're a little bit careful about your data, right?

37:39.160 --> 37:42.240
And maybe if you'll start cleaning up your data

37:42.240 --> 37:45.400
and try to think a little bit about where the data,

37:45.400 --> 37:47.080
your pre-gaining data should come from,

37:47.080 --> 37:49.560
your pre-gaining data itself, you know,

37:49.560 --> 37:51.600
that could be quite interesting.

37:51.600 --> 37:55.160
So when you think of like RLHF as an example,

37:55.160 --> 37:57.480
do you think of that as fundamentally

37:57.480 --> 37:59.840
just cleaning up your data, being more careful

37:59.840 --> 38:01.920
about your data as opposed to?

38:01.920 --> 38:04.920
Yeah, so no, I think I was thinking more

38:04.920 --> 38:07.640
what happened with the loom language model

38:07.640 --> 38:11.920
which was trained on sort of a lot more

38:11.920 --> 38:14.400
thoughtful process of gathering the data set

38:14.400 --> 38:16.440
because partly because they documented it

38:16.440 --> 38:18.840
and we know what sort of they went through.

38:18.840 --> 38:22.880
But now like RLHF and those kind of things,

38:22.880 --> 38:26.840
I think our examples of showing that the language models

38:26.840 --> 38:29.480
are not quite ready for use case

38:29.480 --> 38:32.760
just based on pre-training on sort of large data

38:32.760 --> 38:34.720
that has been gathered.

38:34.720 --> 38:37.720
You need to read, like you can call it like,

38:37.720 --> 38:40.440
hey, cleaning up the data, but I think of it as like

38:40.440 --> 38:44.480
maybe reinforcing some of the nice signals in the data

38:44.480 --> 38:46.160
by having these examples.

38:46.160 --> 38:48.360
Or in some sense, you know, people have been fine-tuning

38:48.360 --> 38:51.280
on these sort of supervised data as well.

38:51.280 --> 38:55.760
And the gains that you get from RLHF

38:55.760 --> 38:58.840
have become extremely evident this year, right?

38:58.840 --> 39:01.680
So somehow that has become the secret source

39:01.680 --> 39:04.720
of opening eye in all of these companies

39:04.720 --> 39:08.320
that want to have really strong language models

39:08.320 --> 39:11.960
rather than scale and just draw a pre-training.

39:13.080 --> 39:15.200
And for completeness, we've talked a little bit

39:15.200 --> 39:17.800
about RLHF on the show, but what's,

39:17.800 --> 39:20.160
how do you think about it as a researcher?

39:20.160 --> 39:21.320
I think it's quite exciting.

39:21.320 --> 39:23.680
I think it sort of addresses a lot of

39:23.680 --> 39:25.640
my concerns with language models.

39:25.640 --> 39:29.760
I don't think pre-training data can be trusted, right?

39:29.760 --> 39:31.800
And you shouldn't just train something

39:31.800 --> 39:34.760
and expect the model to have clean output

39:34.760 --> 39:38.400
or have, you know, your values

39:38.400 --> 39:40.040
and any of these kind of things,

39:41.040 --> 39:43.480
whatever that means is the context of large language models.

39:43.480 --> 39:46.800
But essentially, if you want real users

39:46.800 --> 39:49.840
to be interfacing with language models,

39:49.840 --> 39:54.160
you need to make sure that there is some sort of check.

39:54.160 --> 39:57.320
And RLHF is not a solution, like a full solution,

39:57.320 --> 40:00.960
but at least there is a way to sort of say, okay,

40:00.960 --> 40:02.480
this is the actual task.

40:02.480 --> 40:05.240
Your actual task is to be interfacing with humans,

40:05.240 --> 40:08.080
not just regurgitating what you've seen

40:08.080 --> 40:09.800
in the pre-training corpus, right?

40:09.800 --> 40:13.960
And so that intuition sort of is captured by this RLHF.

40:13.960 --> 40:16.840
And do you, do you remember offhand any of the,

40:16.840 --> 40:19.200
if they were even published the stats

40:19.200 --> 40:22.040
in terms of the number of prompts,

40:22.040 --> 40:23.960
like human generated prompts that were used

40:23.960 --> 40:27.640
in chat GPT or in struct GPT?

40:27.640 --> 40:29.600
Yeah, I don't think they were published

40:29.600 --> 40:30.440
as far as I know.

40:30.440 --> 40:33.560
Yeah, I don't remember exactly what they are.

40:33.560 --> 40:35.760
I think until GPT had the documentation

40:35.760 --> 40:37.560
of sort of how they were gathered,

40:37.560 --> 40:39.880
but the size was like, you know,

40:39.880 --> 40:42.480
how many of them were sort of generation does,

40:42.480 --> 40:45.360
was just classification does, things like that.

40:45.360 --> 40:48.960
But I don't think the exact dataset is good.

40:48.960 --> 40:52.400
Do you, do you have a guess

40:52.400 --> 40:55.640
as to like the relative cost of, you know,

40:55.640 --> 40:57.560
gender of collecting the human feedback

40:57.560 --> 41:00.280
relative to the cost of training the models?

41:00.280 --> 41:03.840
Oh, the rate of cost of training the more it was like,

41:03.840 --> 41:05.120
I think it's much cheaper.

41:05.120 --> 41:08.360
Order of magnitude or is it like much, much, much cheaper?

41:08.360 --> 41:10.040
Because we always say like, you know,

41:10.040 --> 41:11.680
collecting the data label data

41:11.680 --> 41:13.440
is the most expensive part of machine learning.

41:13.440 --> 41:16.720
Is that still true at the scale of LLMs?

41:17.680 --> 41:20.840
Or is it that RLHF is like extremely efficient

41:20.840 --> 41:23.520
and you just need a little bit of guidance

41:23.520 --> 41:27.280
on top of the, you know, the pre-training data?

41:27.280 --> 41:29.560
I feel the true answer is somewhere in between.

41:29.560 --> 41:32.080
So I don't think it's like, it's nowhere

41:32.080 --> 41:34.560
very little data, like I think you need a lot of data

41:34.560 --> 41:35.680
to be able to do it,

41:35.680 --> 41:37.000
but I don't think it comes close,

41:37.000 --> 41:38.960
at least the way these are trained right now,

41:38.960 --> 41:42.040
I don't think it comes close to sort of training

41:42.040 --> 41:42.920
the model itself, right?

41:42.920 --> 41:45.920
So, but like when you think about, you know,

41:45.920 --> 41:49.920
charge GPT, it's been released publicly

41:49.920 --> 41:51.720
and a lot of people are using it.

41:51.720 --> 41:54.120
A lot of that data is gonna go into,

41:54.120 --> 41:56.920
in some form, back into the model and improve it.

41:56.920 --> 42:00.960
So was that expensive to collect in some sense

42:00.960 --> 42:03.880
because they had to run charge GPT, but, you know,

42:03.880 --> 42:06.040
they'll probably pay some managers to clean that up,

42:06.040 --> 42:09.000
but I don't think that's gonna prepare to actual training.

42:09.000 --> 42:13.400
It's also a really interesting example

42:13.400 --> 42:15.720
of like bootstrapping, like there's a certain amount

42:15.720 --> 42:18.120
that they collected themselves, you know,

42:18.120 --> 42:22.200
the instruction GPT work, and then they, you know,

42:22.200 --> 42:23.520
created something that was good enough

42:23.520 --> 42:25.200
to set loose in the world,

42:25.200 --> 42:26.960
and now they've got this virtual cycle

42:26.960 --> 42:29.440
where I'm imagining it's a lot cheaper

42:29.440 --> 42:31.440
for some annotator to clean up,

42:31.440 --> 42:33.960
what, you know, millions of people are creating

42:33.960 --> 42:36.880
then for them to create that themselves.

42:36.880 --> 42:39.400
And I think like, I think this year has also shown,

42:39.400 --> 42:41.040
maybe even to people at OpenAI,

42:41.040 --> 42:43.240
that the value of these things, right?

42:43.240 --> 42:45.080
Like when they released GPT-3,

42:45.080 --> 42:47.080
they probably didn't realize how valuable it would be,

42:47.080 --> 42:49.960
and then they sort of collected data released in start GPT,

42:49.960 --> 42:51.840
and yeah, on their benchmarks, it was good,

42:51.840 --> 42:53.480
but when people started using it,

42:53.480 --> 42:55.280
you realize how much better it is.

42:55.280 --> 42:56.960
I think similarly with chat GPT,

42:56.960 --> 42:59.160
they probably knew how good it was,

42:59.160 --> 43:00.640
but they probably didn't realize

43:00.640 --> 43:03.360
how good it actually is, right?

43:03.360 --> 43:07.480
And I think this idea of human feedback,

43:07.480 --> 43:10.440
being a secret source that is proprietary,

43:10.440 --> 43:17.440
I think, will continue to be a bigger piece in the future.

43:17.760 --> 43:19.880
Talk a little bit about Roots.

43:19.880 --> 43:22.280
Yeah, so the Roots is this nice data set

43:22.280 --> 43:25.000
that was gathered by the big science group,

43:25.000 --> 43:27.000
and I've been following the big science group,

43:27.000 --> 43:31.320
and a bunch of interesting things there.

43:31.320 --> 43:33.960
I guess I'll jump in to refer to the interview

43:33.960 --> 43:35.360
that I did with Thomas Wolf,

43:35.360 --> 43:38.640
that I don't think Roots came up explicitly,

43:38.640 --> 43:41.000
but we talked about that work,

43:41.000 --> 43:43.040
and that eventually resulted in Bloom,

43:43.040 --> 43:45.080
which we'll talk about a little bit more as well.

43:45.080 --> 43:48.440
Yeah, so Roots, I like because I think I really like

43:48.440 --> 43:50.680
what Luther have done with the pilot dataset,

43:50.680 --> 43:52.360
by releasing the dataset that was used

43:52.360 --> 43:54.320
to train all the GPTJ models,

43:54.320 --> 43:57.040
and I think the big science group sort of took their intuition

43:57.040 --> 43:58.600
and sort of went further with it,

43:58.600 --> 44:01.200
where they have a really well-documented,

44:02.320 --> 44:03.880
and not just well-documented,

44:03.880 --> 44:06.920
I would say a very thoughtful process

44:06.920 --> 44:08.960
of gathering this dataset.

44:08.960 --> 44:12.600
It's multi-lingual over many, many different languages,

44:12.600 --> 44:15.560
they've been careful about sort of listing which sources

44:15.560 --> 44:19.240
they want to even crawl in the first place before.

44:19.240 --> 44:21.760
So it's not like a post hot cleanup of the data,

44:21.760 --> 44:24.600
it's very sort of thinking about it.

44:24.600 --> 44:26.680
They gathered a dataset that is huge,

44:26.680 --> 44:29.200
and we talked a little bit about this data,

44:29.200 --> 44:31.560
also the hugging phase has sort of built tools

44:31.560 --> 44:34.760
on top of it to be able to quickly search it,

44:34.760 --> 44:37.560
to see what's in it, and stuff like that.

44:37.560 --> 44:41.640
And I kind of like that approach to life language models.

44:41.640 --> 44:45.920
So I think getting the right dataset

44:45.920 --> 44:47.840
is crucial for these language models,

44:47.840 --> 44:50.760
and doing this documentation and stuff

44:50.760 --> 44:53.120
is good for in the long term.

44:53.120 --> 44:57.200
So your next category is decoding only.

44:57.200 --> 44:59.760
Talk a little bit about what that means.

44:59.760 --> 45:04.520
Yeah, so this is a theme that I like about some of the work

45:04.520 --> 45:06.400
that has come out here.

45:06.400 --> 45:09.520
And partly it's because we have these language models,

45:09.520 --> 45:13.120
where we have this black box of interface to them.

45:13.120 --> 45:15.400
And a lot of it is just prompting,

45:15.400 --> 45:17.240
so changing things on the input side

45:17.240 --> 45:19.240
to see what the model generates,

45:19.240 --> 45:21.840
and the only thing most people are changing

45:21.840 --> 45:23.240
on the output side is like, oh,

45:23.240 --> 45:24.960
we let's change the temperature a little bit,

45:24.960 --> 45:27.560
and we get part of different things.

45:27.560 --> 45:29.280
But there has been a bunch of work looking at,

45:29.280 --> 45:31.040
okay, let's not just do that.

45:31.040 --> 45:34.000
Let's actually think about what's happening

45:34.000 --> 45:37.920
in the output of the model during decoding of the text.

45:37.920 --> 45:40.080
And maybe we can do smart things there

45:41.160 --> 45:44.720
that actually sort of change the output considerably, right?

45:44.720 --> 45:49.160
So some of these sort of came out sort of late last year.

45:49.160 --> 45:52.400
So there was this work on Newtius sampling,

45:52.400 --> 45:53.600
that's a little bit older,

45:53.600 --> 45:56.720
but then there was this stuff on sort of constrained decoding

45:56.720 --> 46:00.400
as well, where the constrained decoding paper

46:00.400 --> 46:02.680
came out of semantic machines.

46:02.680 --> 46:06.080
They showed that you can have,

46:06.080 --> 46:08.560
suppose you want to want the language model

46:08.560 --> 46:10.240
to generate programs, right?

46:10.240 --> 46:13.120
So the programs come with a certain grammar, right?

46:13.120 --> 46:16.080
Like there is a syntax that they need to follow.

46:16.080 --> 46:20.480
So you could actually constrain the output of the language model

46:20.480 --> 46:22.720
as it's generating token by token

46:22.720 --> 46:27.000
to sort of adhere to that syntax in some sense, right?

46:27.920 --> 46:29.640
And just by doing this constraint,

46:29.640 --> 46:31.120
you can get, firstly, obviously,

46:31.120 --> 46:34.360
you will get programs that are syntactically correct,

46:34.360 --> 46:37.880
but you can actually get the right things out of the model.

46:37.880 --> 46:42.880
And so there have been a lot of sort of works looking at

46:44.440 --> 46:48.960
how can we decode by having some constraints on the decoding.

46:49.760 --> 46:51.680
So one of the papers that came out this year,

46:51.680 --> 46:56.680
that we've got the best paper award as well,

46:56.680 --> 47:01.680
is called Neural Logic ASTAR, ASTAR-esque decoding.

47:02.040 --> 47:05.080
And the idea here is that instead of just doing

47:05.080 --> 47:07.440
left to right decoding where you're being greedy

47:07.440 --> 47:09.960
or where you're being doing some kind of beam search

47:09.960 --> 47:12.040
or sampling or any of these process,

47:12.040 --> 47:15.840
why don't you actually use some of the computer science ideas

47:15.840 --> 47:19.160
that we have like ASTAR search

47:19.160 --> 47:22.240
and try to find the best possible decoding.

47:24.160 --> 47:25.600
And then when you're doing this kind of thing,

47:25.600 --> 47:27.360
you can also think about constraints

47:27.360 --> 47:29.360
that you might want to put on the decoding.

47:29.360 --> 47:32.280
So you want to say, look, I want the decoding

47:32.280 --> 47:34.880
to have these three words in it, right?

47:35.920 --> 47:38.680
Like, hey, you're generating a recipe,

47:38.680 --> 47:41.640
make sure that it has these five ingredients, right?

47:41.640 --> 47:43.440
Somewhere in the generated text.

47:44.360 --> 47:47.920
You can also flip it around, hey, generate whatever text

47:47.920 --> 47:51.240
you generate, make sure it doesn't have these specific words,

47:51.240 --> 47:52.760
like that.

47:52.760 --> 47:56.520
And this paper sort of uses ASTAR

47:56.520 --> 48:00.280
during decoding to generate that text,

48:00.280 --> 48:04.040
that sort of, you know, your constraints are satisfied.

48:05.080 --> 48:06.240
And this paper showed that, yeah,

48:06.240 --> 48:07.360
once you do that properly,

48:07.360 --> 48:10.880
you can actually do a lot of the tasks much better

48:10.880 --> 48:14.200
just by controlling decoding rather than changing much

48:14.200 --> 48:15.400
on the inputs.

48:15.400 --> 48:17.080
It seems like this is another example

48:17.080 --> 48:22.400
where it's predicated on having open access

48:22.400 --> 48:23.760
to the model internals,

48:23.760 --> 48:27.840
and you potentially lose a lot if you don't.

48:27.840 --> 48:30.240
Yeah, I think so from what I understand,

48:30.240 --> 48:32.320
you can still do these kind of things

48:32.320 --> 48:35.160
with GPT-3 to some degree.

48:35.160 --> 48:37.400
I think what you need, you can, okay,

48:37.400 --> 48:40.920
so you can do this with black box model

48:40.920 --> 48:42.960
as long as you get the probabilities

48:42.960 --> 48:45.680
of all of the tokens at every step, right?

48:45.680 --> 48:49.080
So I don't think GPT actually does that, right?

48:49.080 --> 48:51.080
But, you know, you could imagine an API

48:51.080 --> 48:52.600
that says, okay, the next,

48:52.600 --> 48:54.920
here's the distribution over all of the tokens.

48:56.240 --> 49:00.360
And you should be still be able to do these kind of items.

49:00.360 --> 49:03.600
So, you know, some of the concerns is like,

49:03.600 --> 49:05.560
if you want decoding to be fast,

49:05.560 --> 49:08.160
then it's difficult to use some of these ideas.

49:08.160 --> 49:11.440
The A-star-1 specificities is a lot slower,

49:12.400 --> 49:14.440
but it's able to satisfy your constraints.

49:14.440 --> 49:18.640
So it can be where you're okay to trade off some time,

49:18.640 --> 49:21.320
but let the model take more time

49:21.320 --> 49:23.880
in making sure the output is seen

49:23.880 --> 49:25.640
and satisfy your constraints,

49:25.640 --> 49:27.200
it could be really cool.

49:28.680 --> 49:30.800
And now, yeah, often you see,

49:30.800 --> 49:34.760
hey, we applied one method, A-star, in this case,

49:34.760 --> 49:37.320
let's go back to the computer science toolkit

49:37.320 --> 49:38.520
and apply everything else.

49:38.520 --> 49:40.400
Have we seen that here?

49:40.400 --> 49:41.240
Not yet.

49:41.240 --> 49:43.840
I think it came out late enough in the year.

49:43.840 --> 49:46.280
But I guess it came out sort of early in the year,

49:46.280 --> 49:48.480
but yeah, we haven't seen that much yet

49:48.480 --> 49:49.800
because, but I think, yeah,

49:49.800 --> 49:52.040
that's the kind of thing that will happen next.

49:52.040 --> 49:54.640
It's like, okay, now this is, yeah,

49:54.640 --> 49:57.040
this is attracting a whole different kind of thinking

49:57.040 --> 49:59.840
where people were not thinking about decoding at all,

49:59.840 --> 50:01.880
and now they will be in this light,

50:01.880 --> 50:04.680
which is all this kind of good paper.

50:04.680 --> 50:05.520
Awesome, awesome.

50:05.520 --> 50:09.800
Well, those are great themes to kind of reflect on

50:09.800 --> 50:14.760
as we think about the past year and LP research.

50:14.760 --> 50:16.640
Our next category is to talk about

50:16.640 --> 50:20.560
some of the new tools and open source projects

50:20.560 --> 50:23.320
that we saw in the year.

50:23.320 --> 50:26.400
We've already talked a little bit about data sets,

50:26.400 --> 50:28.000
which is kind of related.

50:29.440 --> 50:32.960
But I think the first thing you have here is OPT.

50:32.960 --> 50:34.680
That's all about OPT.

50:34.680 --> 50:39.200
So yeah, I think OPT came out fairly early in this year.

50:40.000 --> 50:42.280
And I think it kind of surprised everyone

50:42.280 --> 50:46.920
because the sort of looking back at last year,

50:46.920 --> 50:49.840
there weren't that many open source reproductions

50:49.840 --> 50:51.440
of large sites, right?

50:51.440 --> 50:54.960
So I think Luther AI was sort of leading it.

50:54.960 --> 50:57.760
GPTJ was six billion and, you know,

50:57.760 --> 51:00.200
they were sort of growing it slowly and slowly

51:00.200 --> 51:02.880
and they had got to 20 billion parameters.

51:03.800 --> 51:06.640
And then OPT sort of came into the scene

51:06.640 --> 51:09.840
and they, yeah, there were a bunch of nice things.

51:09.840 --> 51:12.600
They documented a lot of their whole training process

51:12.600 --> 51:15.560
in a log work with sort of all kinds of insights

51:15.560 --> 51:17.360
about what training a log.

51:18.400 --> 51:21.080
Yes, it was released better, right?

51:21.080 --> 51:24.360
And that was also not to say too much against better,

51:24.360 --> 51:27.200
but it was also surprising that reproducibility

51:27.200 --> 51:30.720
and open source seemed to be key aspect of OPT as well.

51:30.720 --> 51:33.360
So that was kind of nice.

51:33.360 --> 51:37.560
And they also released a lot of models and, you know,

51:37.560 --> 51:42.560
like all different sizes, including 175 billion,

51:42.880 --> 51:46.760
which hadn't been available at all.

51:46.760 --> 51:50.640
And even right now, I think it's probably the most useful model

51:50.640 --> 51:53.160
if you want to do stuff with 175 billion

51:53.160 --> 51:55.720
is to use the OPT model, right?

51:55.720 --> 52:00.720
So I think the idea of documenting the whole training data

52:01.360 --> 52:04.240
gathering process, documenting the whole training

52:04.240 --> 52:05.840
of the model process,

52:05.840 --> 52:10.200
and then releasing all of these models available

52:10.200 --> 52:14.200
for research, I think has helped the research community a lot.

52:14.200 --> 52:16.400
And I expect that if there are people

52:16.400 --> 52:19.600
who want to build models and potentially find you

52:19.600 --> 52:22.520
in language models and do all of these things,

52:22.520 --> 52:24.720
the OPT would be a pretty big source.

52:24.720 --> 52:27.520
Have you seen much in terms of benchmarking it

52:27.520 --> 52:29.720
against GPT-3?

52:29.720 --> 52:32.680
Yes, so I think people have been benchmarking it.

52:32.680 --> 52:36.040
And I think it performs reasonably well.

52:36.040 --> 52:37.960
The tricky thing is, of course,

52:37.960 --> 52:41.520
there is instruct GPT, which is, you know,

52:41.520 --> 52:44.160
when you call it GPT-3 on the API right now,

52:44.160 --> 52:47.080
it's often defaults to the instruct one.

52:47.080 --> 52:49.320
And that one is a lot more difficult to beat,

52:49.320 --> 52:52.000
but for all of the purposes, I think of it as like,

52:52.000 --> 52:55.440
yeah, OPT-8 is basically same as GPT-3.

52:55.440 --> 53:00.440
We talked a little bit about the big science project

53:00.440 --> 53:04.240
and one of its outputs and other is Bloom.

53:04.240 --> 53:07.320
What did you, what was your take on Bloom?

53:07.320 --> 53:12.080
Bloom was, again, a really big data model.

53:12.080 --> 53:14.680
That was, I think, 180 billion parameters.

53:14.680 --> 53:16.880
So similar sizes, GPT-3,

53:16.880 --> 53:19.680
believes to become a daily open source.

53:19.680 --> 53:22.480
It's like we talked about completely well-documented

53:22.480 --> 53:25.560
data process and sort of training process.

53:25.560 --> 53:29.920
Combined with the fact that this was done by a group of people

53:29.920 --> 53:33.920
it's kind of just volunteering their time to do so.

53:33.920 --> 53:38.160
And then being able to reproduce to a large degree,

53:38.160 --> 53:41.720
what OpenAI has done was quite amazing, right?

53:41.720 --> 53:44.760
And then sort of, again, like both OPTs,

53:44.760 --> 53:46.960
releasing all of these things is kind of a sign

53:46.960 --> 53:49.640
for other big tech companies to say like,

53:49.640 --> 53:52.800
hey, you can do this because we've done this kind of thing.

53:52.800 --> 53:57.640
But Bloom has shown is that a bunch of people enthusiastic

53:57.640 --> 54:01.600
and excited folks that are enterprising can actually do things

54:01.600 --> 54:06.480
that maybe even a year or two ago would have seemed impossible.

54:06.480 --> 54:14.280
It may have been in our trends conversation

54:14.280 --> 54:20.800
from last year or maybe it was prior.

54:20.800 --> 54:23.520
But in these kinds of conversations,

54:23.520 --> 54:26.040
there was a point in time where we were lamenting

54:26.040 --> 54:31.040
the loss on the part of the individual academic researcher

54:31.400 --> 54:34.800
to contribute to fundamental model research

54:34.800 --> 54:37.440
because of the resources that were required.

54:37.440 --> 54:41.800
And to hugging face and the big sciences team,

54:41.800 --> 54:46.240
like they showed that not necessarily not so fast, right?

54:46.240 --> 54:48.000
Right, right, exactly.

54:48.000 --> 54:50.120
And the other thing I like about this,

54:50.120 --> 54:53.000
the blue method is and the corpus that came with it,

54:53.000 --> 54:56.240
they were also focused on being a lot more inclusive

54:56.240 --> 54:58.960
in terms of having a global perspective.

54:58.960 --> 55:02.040
So they were trying to cover many, many different languages.

55:02.040 --> 55:04.920
Very principled in the way they pulled the data together.

55:04.920 --> 55:06.560
Yeah, yeah.

55:06.560 --> 55:07.920
And also multilingual in a way

55:07.920 --> 55:10.160
that none of the existing models have been.

55:10.160 --> 55:12.600
So yeah, it's quite quite exciting.

55:12.600 --> 55:14.880
And so like conceptually,

55:14.880 --> 55:19.680
this is a great example of how one model

55:19.680 --> 55:24.560
at 175 billion primers and another model,

55:24.560 --> 55:26.360
you know, the same number of primers

55:26.360 --> 55:28.760
could be very different, you know,

55:28.760 --> 55:30.640
at least in the data that they were trained on

55:30.640 --> 55:33.920
and you would expect that to result in

55:33.920 --> 55:37.120
very different results using the model.

55:37.120 --> 55:40.280
To what extent have we characterized that?

55:40.280 --> 55:42.240
Like at that scale of data,

55:42.240 --> 55:43.440
it's still a lot of data,

55:43.440 --> 55:46.800
still a lot of like raw internet data.

55:46.800 --> 55:49.200
Does it all kind of fall out in the wash

55:49.200 --> 55:51.360
and all their efforts at being principled,

55:51.360 --> 55:54.480
you know, kind of just get lost?

55:54.480 --> 55:57.760
Or do we know how to compare that?

55:57.760 --> 55:59.760
Yeah, so there have been a bunch of benchmarks

55:59.760 --> 56:01.320
and including in their papers,

56:01.320 --> 56:02.720
but in general also.

56:02.720 --> 56:05.720
And that's where sort of the, you know,

56:05.720 --> 56:07.040
don't hold me to this,

56:07.040 --> 56:11.040
but I would say like Bloom is not the go-to language model

56:11.040 --> 56:14.280
for people if they want to do English things, right?

56:14.280 --> 56:17.280
All right, so I think maybe some of the trade-offs

56:17.280 --> 56:19.040
that made in collecting the data

56:19.040 --> 56:21.680
or even just having all languages

56:22.720 --> 56:24.840
result in the model that's definitely really good

56:24.840 --> 56:26.600
for multilingual things.

56:27.600 --> 56:30.840
But that's not what our benchmarks have been designed for

56:30.840 --> 56:32.200
unfortunately.

56:32.200 --> 56:34.240
And so if you just look at the benchmarks,

56:34.240 --> 56:36.520
like, you know, which are traditionally designed

56:36.520 --> 56:41.360
for English Bloom, I don't think quite is at par

56:41.360 --> 56:46.360
with OPD or GPDC and definitely not with Instruct.

56:46.360 --> 56:51.280
And when I mention benchmark, you know,

56:51.280 --> 56:54.160
there's that aspect of kind of applying

56:54.160 --> 56:56.280
the traditional performance benchmarks,

56:56.280 --> 57:00.640
you know, for LLMs to bloom and comparing their results

57:00.640 --> 57:02.160
to the others.

57:02.160 --> 57:04.200
But I'm also curious about.

57:06.200 --> 57:09.320
How we characterize like qualitative differences

57:09.320 --> 57:12.640
between the way Bloom responds

57:12.640 --> 57:15.800
and the way GPT responds.

57:15.800 --> 57:19.800
For example, you know, in terms of,

57:21.000 --> 57:23.880
you know, like the kind of fairness considerations

57:23.880 --> 57:27.960
or that kind of thing,

57:27.960 --> 57:30.560
or, you know, are there qualitative differences

57:30.560 --> 57:34.000
in the kinds of responses that you get

57:34.000 --> 57:37.400
that aren't picked up by the traditional benchmarks

57:37.400 --> 57:40.880
or are the traditional benchmarks like so, you know,

57:40.880 --> 57:43.720
expansive at this point, we've kind of

57:43.720 --> 57:46.520
characterized a lot of that stuff explicitly.

57:46.520 --> 57:48.800
Yeah, again, I think the answer is somewhere in between.

57:48.800 --> 57:50.920
So I don't know if people have thoroughly compared

57:50.920 --> 57:53.480
the due to see like it was the level of toxicity

57:53.480 --> 57:55.840
and people like that, you know, like,

57:55.840 --> 57:57.080
I think when OPD came out,

57:57.080 --> 57:59.640
they did a lot of disanalysis in their paper of like,

57:59.640 --> 58:03.040
hey, how toxic is their model, how safe is their model?

58:03.040 --> 58:05.320
And they realized that yeah, in some things,

58:05.320 --> 58:10.080
they were worse off than some of the existing models.

58:10.080 --> 58:14.320
But I think with loom, specifically,

58:14.320 --> 58:16.360
I don't know off the top of my head,

58:16.360 --> 58:18.960
how it sort of compared in terms of these,

58:18.960 --> 58:22.320
these other sort of other aspects.

58:22.320 --> 58:23.320
Okay.

58:23.320 --> 58:25.720
Talk about the inverse scaling competition.

58:25.720 --> 58:28.600
Yeah, so this was a pretty nice thing that gave out

58:28.600 --> 58:30.640
and I think I suppose it's still going on

58:30.640 --> 58:32.120
even though the submissions are down.

58:32.120 --> 58:33.880
So I'm kind of hoping to see

58:33.880 --> 58:36.480
what the actual effect of this was.

58:36.480 --> 58:38.720
But this was sort of introduced sort of

58:38.720 --> 58:40.400
in the middle of the year.

58:40.400 --> 58:44.560
And the idea here is the thinking of things

58:44.560 --> 58:47.320
like what sort of scaling laws was showing, right?

58:47.320 --> 58:50.000
Like when you scale up your models,

58:50.000 --> 58:52.520
performance goes up for everything.

58:52.520 --> 58:53.920
And that's kind of exciting to see.

58:53.920 --> 58:57.240
But it also tells us that okay,

58:57.240 --> 58:59.840
there are many, many things that just the models

58:59.840 --> 59:02.280
would just get better on as time goes by

59:02.280 --> 59:05.720
because they'll get bigger, they'll have more data set.

59:05.720 --> 59:08.280
The inverse scaling was this intuition to see,

59:08.280 --> 59:11.320
okay, what are, can we characterize the phenomenas

59:11.320 --> 59:15.880
that don't have the same trend, right?

59:15.880 --> 59:20.040
So other aspects, you know, you created a data set

59:20.040 --> 59:21.840
which is something everybody will agree

59:21.840 --> 59:23.320
is a reasonable data set.

59:23.320 --> 59:26.120
But when you give them to larger models,

59:27.400 --> 59:29.080
they actually get worse.

59:30.560 --> 59:33.400
And so this, this price and this competition

59:33.400 --> 59:37.040
is an effort to identify what those,

59:37.040 --> 59:39.720
what those tasks would be.

59:39.720 --> 59:43.160
And sort of the better your inverse scaling is.

59:43.160 --> 59:46.960
So the worse, the bigger models are on the data set

59:46.960 --> 59:49.560
that you've contributed, the more likely you are

59:49.560 --> 59:51.960
to build this competition.

59:51.960 --> 59:53.400
And so yeah, they've had the submissions

59:53.400 --> 59:55.600
and they're kind of evaluating them as opposed

59:55.600 --> 59:57.920
and they haven't quite announced it.

59:57.920 --> 01:00:01.680
But I think a lot of the stuff on, you know,

01:00:01.680 --> 01:00:06.240
a lot of the interesting things could come out of this effort.

01:00:06.240 --> 01:00:09.680
So one thing I could imagine is sort of deeper levels

01:00:09.680 --> 01:00:14.680
of misinformation, whether the model is relying so much

01:00:15.040 --> 01:00:18.040
on what it has seen in its training data.

01:00:18.040 --> 01:00:19.440
Now let's not call it misinformation,

01:00:19.440 --> 01:00:23.360
just not being able to update its information in some sense,

01:00:23.360 --> 01:00:24.200
right?

01:00:24.200 --> 01:00:26.800
So these large language models have memorized so much

01:00:26.800 --> 01:00:30.440
about the pre-telling data that they kind of reject

01:00:30.440 --> 01:00:32.800
evidence against that, right?

01:00:32.800 --> 01:00:35.080
Maybe if they're smaller, there's less memorization

01:00:35.080 --> 01:00:37.240
and more generalization.

01:00:37.240 --> 01:00:39.000
But I think it could be pretty exciting to see

01:00:39.000 --> 01:00:42.200
what are those things that actually get worse with scale.

01:00:42.200 --> 01:00:44.960
I think it's quite an interesting question.

01:00:44.960 --> 01:00:48.360
And next up, you have the Galactica,

01:00:48.360 --> 01:00:49.920
can we call it a debacle?

01:00:49.920 --> 01:00:50.920
Okay.

01:00:50.920 --> 01:00:54.800
So Galactica is this L alone that meta released

01:00:54.800 --> 01:00:59.800
that was tuned to generate scientific and research text.

01:00:59.800 --> 01:01:02.440
And research text.

01:01:02.440 --> 01:01:07.240
And was it even up for three days?

01:01:07.240 --> 01:01:10.080
It got pulled down pretty quickly, right?

01:01:10.080 --> 01:01:12.840
Yeah, I think maybe a little bit more than that,

01:01:12.840 --> 01:01:14.880
but yeah, they're about, yeah.

01:01:14.880 --> 01:01:19.880
And I think to me, it's a story about how not

01:01:20.960 --> 01:01:24.200
anything in terms of what the Galactica team did itself,

01:01:24.200 --> 01:01:26.400
like I think the model training it,

01:01:26.400 --> 01:01:29.520
everything was the right thing to be doing.

01:01:30.760 --> 01:01:34.200
The tricky thing was just how it was pitched

01:01:34.200 --> 01:01:39.200
and how, you know, there was just not clear caveats

01:01:40.760 --> 01:01:44.240
about what this model is capable of doing

01:01:44.240 --> 01:01:46.800
and what it's not capable of doing,

01:01:46.800 --> 01:01:49.600
that led to such a backlash, right?

01:01:49.600 --> 01:01:54.600
So I think it was a language model training

01:01:54.600 --> 01:01:57.680
like language model training on a lot of science papers.

01:01:57.680 --> 01:01:59.320
So it's going to produce papers

01:01:59.320 --> 01:02:01.680
that look like scientific text.

01:02:01.680 --> 01:02:04.280
I think that was an expected thing,

01:02:05.200 --> 01:02:08.800
but again, the backlash it got and stuff like that.

01:02:08.800 --> 01:02:10.160
Essentially, it tells everyone,

01:02:10.160 --> 01:02:15.160
and I hope the message is not to demo language models anymore,

01:02:15.680 --> 01:02:17.800
but I think the message should be how to make sure

01:02:17.800 --> 01:02:21.600
that you're not hyping things up more than they should be.

01:02:21.600 --> 01:02:26.320
If you reflect on chat GPT, which came not very long after

01:02:26.320 --> 01:02:31.320
Galactica and the launches of those respective products,

01:02:34.440 --> 01:02:36.880
are there clear, is there a clear like,

01:02:36.880 --> 01:02:38.280
do don't do list?

01:02:38.280 --> 01:02:42.200
So I will say that chat GPT itself was also not,

01:02:42.200 --> 01:02:46.080
you know, not completely without hype attached to it,

01:02:46.080 --> 01:02:48.640
even sort of how they, right?

01:02:48.640 --> 01:02:51.160
Somehow they managed it, it was a lot of hype.

01:02:51.160 --> 01:02:52.440
Right, right, right.

01:02:52.440 --> 01:02:55.280
I will say that they were fairly clear about the fact

01:02:55.280 --> 01:02:57.000
that like, hey, don't trust, you know,

01:02:57.000 --> 01:02:58.600
maybe they could have been clearer,

01:02:58.600 --> 01:03:01.920
but like don't trust the factual stuff and things like that.

01:03:01.920 --> 01:03:04.200
Like it's not a lookup engine.

01:03:04.200 --> 01:03:06.720
I think they kind of could have done a lot more of that,

01:03:06.720 --> 01:03:09.880
but you know, they at least had some caveats.

01:03:09.880 --> 01:03:12.880
But more than that, they part of their RLHF stuff

01:03:12.880 --> 01:03:16.520
was to make sure that the model is not producing,

01:03:16.520 --> 01:03:19.840
at least obviously sexist, don't say.

01:03:19.840 --> 01:03:23.000
Yeah, there was a lot, and maybe we're jumping into chat GPT,

01:03:23.000 --> 01:03:25.200
which actually is the next thing we're gonna talk about,

01:03:25.200 --> 01:03:27.880
but there was definitely a lot of,

01:03:27.880 --> 01:03:31.600
especially early on, things that it just would not a pine on.

01:03:31.600 --> 01:03:34.640
Like yeah, no, you're not gonna sucker me in and go in there.

01:03:34.640 --> 01:03:35.760
Right, right, right, yeah.

01:03:35.760 --> 01:03:38.680
And I think when you're building something that's public

01:03:38.680 --> 01:03:42.400
facing that you're selling as a tool, as a product,

01:03:42.400 --> 01:03:44.360
that is necessary, right?

01:03:44.360 --> 01:03:48.800
Like I don't think you should be doing otherwise.

01:03:48.800 --> 01:03:52.360
Galactica should not have been a public facing tool

01:03:52.360 --> 01:03:55.800
for every scientist to start using to write their papers.

01:03:55.800 --> 01:03:57.600
It should be a language model, right?

01:03:57.600 --> 01:04:00.680
And then what the product is or what the tool is,

01:04:00.680 --> 01:04:04.120
is a gap that other people can help fill in, right?

01:04:04.120 --> 01:04:06.640
So that was sort of the missing piece

01:04:06.640 --> 01:04:10.080
when I think about chat GPT versus galactica.

01:04:10.080 --> 01:04:13.880
It's like, yeah, chat GPT has some of the caveats

01:04:13.880 --> 01:04:16.160
about what it's doing.

01:04:16.160 --> 01:04:17.520
Has some of the caveats about oh,

01:04:17.520 --> 01:04:20.280
it's a language model, not a product to some degree.

01:04:21.280 --> 01:04:24.400
And galactica was missing it, right?

01:04:24.400 --> 01:04:26.040
So, yeah.

01:04:26.040 --> 01:04:27.560
Now, we're there.

01:04:29.400 --> 01:04:31.480
We were talking about open source.

01:04:31.480 --> 01:04:34.440
Next up is kind of commercial developments.

01:04:35.400 --> 01:04:37.240
Top of that list is chat GPT.

01:04:37.240 --> 01:04:40.040
Right, yeah, let's talk about it.

01:04:40.040 --> 01:04:42.200
It's, you said early on that,

01:04:42.200 --> 01:04:45.920
hey, even without chat GPT, you know, this was a huge year

01:04:45.920 --> 01:04:47.880
that's clearly not to say that chat GPT

01:04:47.880 --> 01:04:50.960
wasn't a huge contribution to the year.

01:04:50.960 --> 01:04:53.680
I mean, certainly one of the things

01:04:53.680 --> 01:04:58.680
that I found most interesting was the degree to which

01:05:01.000 --> 01:05:04.800
it kind of broke out of the MLA echo chamber

01:05:04.800 --> 01:05:07.560
to, you know, just talking to random friends

01:05:07.560 --> 01:05:10.080
and are like, hey, have you tried this chat GPT thing?

01:05:10.080 --> 01:05:11.160
Well, yeah, I have.

01:05:11.160 --> 01:05:15.480
Yeah, so that that's been the, I guess, the most surprising

01:05:15.480 --> 01:05:17.680
and in some sense, the longest,

01:05:17.680 --> 01:05:20.880
longest term impact for chat GPT is going to be the fact

01:05:20.880 --> 01:05:23.440
that it made it commoditize.

01:05:23.440 --> 01:05:27.040
It made it mainstream in a way that nothing before it had,

01:05:27.040 --> 01:05:28.040
right?

01:05:28.040 --> 01:05:29.120
And whether it deserved it or not,

01:05:29.120 --> 01:05:32.320
what the actual innovations are and all of these things

01:05:32.320 --> 01:05:34.160
is a different question, right?

01:05:34.160 --> 01:05:37.080
Like it is clearly, even for research,

01:05:37.080 --> 01:05:40.680
point of view, qualitatively better than GPT's tree.

01:05:40.680 --> 01:05:44.400
Whether it met some threshold for becoming the big thing

01:05:44.400 --> 01:05:47.160
that it did, it's sort of difficult to sort of

01:05:47.160 --> 01:05:50.080
in hindsight try to evaluate that.

01:05:50.080 --> 01:05:54.400
But I think it was, yeah, it is definitely something

01:05:54.400 --> 01:05:59.400
that became mainstream and MVP is talking about it.

01:05:59.840 --> 01:06:02.760
There is still a question in my mind whether that's a good

01:06:02.760 --> 01:06:05.760
thing or not in the long run.

01:06:05.760 --> 01:06:08.360
Because, you know, we can talk about some of the problems

01:06:08.360 --> 01:06:11.080
with chat GPT, the biggest one being,

01:06:11.080 --> 01:06:13.240
we know it's a language model.

01:06:13.240 --> 01:06:15.280
Like, to some degree, we've been figuring out

01:06:15.280 --> 01:06:17.400
last couple of years what these things are capable of

01:06:17.400 --> 01:06:18.760
and what these things are not.

01:06:18.760 --> 01:06:20.920
And I can say that in like a couple of minutes,

01:06:20.920 --> 01:06:23.800
come up with tons of examples where it would fail.

01:06:25.080 --> 01:06:27.720
That's not quite the case when you sort of start putting it

01:06:27.720 --> 01:06:28.800
out in the public.

01:06:28.800 --> 01:06:31.760
So, most people don't know what a language model is.

01:06:31.760 --> 01:06:33.280
And I have played around with,

01:06:33.280 --> 01:06:35.680
you know, I've got a bunch of my family to try it

01:06:35.680 --> 01:06:36.720
and things like that.

01:06:36.720 --> 01:06:39.080
And the biggest thing I've had,

01:06:39.080 --> 01:06:41.680
the biggest difficulty I've had conveying to them

01:06:41.680 --> 01:06:45.200
is the fact that it's not looking up anything

01:06:45.200 --> 01:06:46.640
when you ask it something, right?

01:06:46.640 --> 01:06:51.360
Like, that is a conceptual jump that is very, very

01:06:51.360 --> 01:06:53.600
difficult for people to get over.

01:06:54.480 --> 01:06:55.480
Yeah.

01:06:55.480 --> 01:06:57.600
And so people, like, yeah, of course,

01:06:57.600 --> 01:07:00.280
it should know about these things because it happened

01:07:00.280 --> 01:07:02.240
yesterday and for such a big news items.

01:07:02.240 --> 01:07:03.680
Like, why would it not know?

01:07:03.680 --> 01:07:06.680
And now, like, no, actually, it doesn't know anything

01:07:06.680 --> 01:07:07.800
beyond a certain time.

01:07:07.800 --> 01:07:11.440
And even saying it knows anything from then

01:07:11.440 --> 01:07:13.240
is a little bit difficult, right?

01:07:14.400 --> 01:07:18.240
So I think the best analogy that I've, you know,

01:07:18.240 --> 01:07:19.600
this applies to my research as well,

01:07:19.600 --> 01:07:22.400
but the best analogy I've had in trying to explain

01:07:22.400 --> 01:07:26.640
people what chat GPT does is do not think of it

01:07:26.640 --> 01:07:28.920
as a stochastic pattern or anything like that.

01:07:28.920 --> 01:07:31.200
But if you have to think in terms of animals,

01:07:31.200 --> 01:07:33.280
think of it as like a chameleon,

01:07:33.280 --> 01:07:36.560
like, it's trying to sort of fit in

01:07:36.560 --> 01:07:39.360
to a bunch of humans, right?

01:07:39.360 --> 01:07:43.240
And it's trying to just write things that will make it pass

01:07:43.240 --> 01:07:46.680
as if it sort of knows all of those things, right?

01:07:46.680 --> 01:07:51.680
I was in a Twitter exchange about I'd ask chat GPT

01:07:52.520 --> 01:07:57.520
to explain RLHF and it came up with this acronym

01:07:57.520 --> 01:08:02.160
it came up with this acronym that was like,

01:08:04.680 --> 01:08:07.000
oh, I forget it and it was really funny.

01:08:07.000 --> 01:08:10.680
It was like something leaderboard, you know,

01:08:10.680 --> 01:08:13.080
humans, human something.

01:08:13.080 --> 01:08:16.320
It was like, it was so far off.

01:08:16.320 --> 01:08:18.640
Interestingly enough, I'd asked it about,

01:08:18.640 --> 01:08:21.400
I'd had conversations, you know, interactions with it

01:08:21.400 --> 01:08:24.080
about RLHF and then knew what it was.

01:08:24.080 --> 01:08:26.760
Like to your point, it's about kind of where it sits

01:08:26.760 --> 01:08:29.160
in the context of the prompt.

01:08:29.160 --> 01:08:30.720
And I just kind of posted, you know,

01:08:30.720 --> 01:08:35.120
is it trolling me or is it just trying to BS me?

01:08:35.120 --> 01:08:38.720
And one of the responses that I got that, you know,

01:08:38.720 --> 01:08:40.360
and reflecting on it was like really insightful,

01:08:40.360 --> 01:08:43.040
like it's always trying to BS you.

01:08:43.040 --> 01:08:45.720
That's all it's doing is trying to BS you

01:08:45.720 --> 01:08:48.960
to produce some text that you will think is reasonable.

01:08:48.960 --> 01:08:53.960
And, you know, to its credit, a lot of times it's right

01:08:54.080 --> 01:08:56.040
but that's all that's trying to do.

01:08:56.040 --> 01:08:57.040
Right, right, yeah.

01:08:57.040 --> 01:09:00.080
And especially when it comes to factual stuff

01:09:00.080 --> 01:09:04.400
or even like, you know, it is a very useful bullshitter

01:09:04.400 --> 01:09:05.360
in some sense, right?

01:09:05.360 --> 01:09:08.560
So because when it's right or when it's partially right,

01:09:08.560 --> 01:09:12.560
that's still useful because, you know, it is what it is.

01:09:12.560 --> 01:09:14.880
But that when you put that label,

01:09:14.880 --> 01:09:17.240
like if they had sold it as like,

01:09:17.240 --> 01:09:21.360
hey, we have built a really good bullshitter, right?

01:09:21.360 --> 01:09:23.160
Like and it sold that as a product,

01:09:23.160 --> 01:09:24.520
then people would know, okay,

01:09:24.520 --> 01:09:26.000
not to use it for a bunch of tasks

01:09:26.000 --> 01:09:28.920
that they're currently thinking of using it, right?

01:09:28.920 --> 01:09:30.440
And so, yeah.

01:09:30.440 --> 01:09:34.000
So that's the sort of divide that in messaging

01:09:34.000 --> 01:09:38.080
that somehow researchers and NLP folks know,

01:09:38.080 --> 01:09:39.400
oh, yeah, language model loss.

01:09:39.400 --> 01:09:43.000
Obviously all that is doing is blah, blah, blah, right?

01:09:43.000 --> 01:09:45.480
And yes, RLHF can help to some degree,

01:09:45.480 --> 01:09:47.800
but clearly it's not going to be able

01:09:47.800 --> 01:09:51.840
to do these bunch of other things all the time.

01:09:51.840 --> 01:09:56.400
And that kind of thing is missing from general public,

01:09:56.400 --> 01:09:59.400
but also how a lot of people are planning to use it

01:09:59.400 --> 01:10:00.560
for example, right?

01:10:00.560 --> 01:10:03.320
So I think that aspect is the part

01:10:03.320 --> 01:10:06.800
that we need to think a little bit more about.

01:10:06.800 --> 01:10:10.560
You've got Palm and Arpa Flan down.

01:10:12.040 --> 01:10:15.040
You know, tell me a little bit more about your take there

01:10:15.040 --> 01:10:16.680
because I hear of them, you know,

01:10:16.680 --> 01:10:20.400
in a vague research context

01:10:20.400 --> 01:10:22.840
because no one really has access to these,

01:10:22.840 --> 01:10:27.760
but Google as much more so than, you know,

01:10:27.760 --> 01:10:30.800
something that is huge from a commercial perspective.

01:10:30.800 --> 01:10:33.680
Is this a prediction or is this a reflection?

01:10:33.680 --> 01:10:37.720
Yeah, so no, I think of this as a palm

01:10:37.720 --> 01:10:40.800
was a huge commercial development this year.

01:10:40.800 --> 01:10:44.280
Like Google built this really, really large model.

01:10:44.280 --> 01:10:47.000
Now there are, obviously they haven't released it, right?

01:10:47.000 --> 01:10:48.680
So what's the ideal situation?

01:10:48.680 --> 01:10:50.400
They completely release it open source.

01:10:50.400 --> 01:10:51.680
Everybody gets access to it.

01:10:51.680 --> 01:10:53.360
That's not gonna happen.

01:10:53.360 --> 01:10:55.760
Another possible thing is they put an API on it

01:10:55.760 --> 01:10:58.760
and charge people from a Google perspective

01:10:58.760 --> 01:11:00.320
that doesn't make sense, right?

01:11:00.320 --> 01:11:02.160
So it is something that they've built.

01:11:02.160 --> 01:11:03.680
It's valuable internally.

01:11:03.680 --> 01:11:07.040
I'm sure it sort of has, you know,

01:11:07.040 --> 01:11:09.160
there are reasons not to make it public,

01:11:09.160 --> 01:11:12.960
but it also has a lot of research insights

01:11:12.960 --> 01:11:16.120
because nobody else has such a big language model

01:11:16.120 --> 01:11:18.040
trained in a similar way.

01:11:18.040 --> 01:11:20.640
And I guess I want to give them props

01:11:20.640 --> 01:11:23.640
for at least publishing and evaluating

01:11:23.640 --> 01:11:26.680
and doing things like that with palm.

01:11:28.160 --> 01:11:31.720
Because it is doing, it is oversized that

01:11:31.720 --> 01:11:35.840
we will not see for maybe another year or two

01:11:35.840 --> 01:11:38.280
to be sort of publicly available,

01:11:38.280 --> 01:11:41.000
but yet we get to hear about some insights,

01:11:41.000 --> 01:11:43.680
what to expect, what are the emergence behaviors

01:11:45.000 --> 01:11:46.560
coming out of those language models, right?

01:11:46.560 --> 01:11:49.000
So yeah, it would be ideal if we could audit it

01:11:49.000 --> 01:11:51.200
and all of us and I could contribute

01:11:51.200 --> 01:11:52.840
in finding out what the problems are

01:11:52.840 --> 01:11:54.960
and when it works and it doesn't work.

01:11:54.960 --> 01:11:58.200
But given that, I think they did a good job.

01:11:58.200 --> 01:12:01.000
Specifically, what I will say is that that size

01:12:01.000 --> 01:12:04.000
has brought up a bunch of capabilities

01:12:04.000 --> 01:12:05.800
like the whole chain of thought thing

01:12:05.800 --> 01:12:08.240
that we talked about at the beginning.

01:12:08.240 --> 01:12:11.440
That somehow became possible at that size,

01:12:11.440 --> 01:12:14.880
but wasn't possible at other size, right?

01:12:14.880 --> 01:12:19.880
So that's why that research is also all coming out of Google

01:12:20.280 --> 01:12:25.600
because it applies mostly to, you know,

01:12:25.600 --> 01:12:27.880
to the language, to the LLM's of that size.

01:12:27.880 --> 01:12:30.880
Palm is 540 billion parameters?

01:12:30.880 --> 01:12:33.280
Something, yeah, five, four years, yeah, yeah.

01:12:33.280 --> 01:12:35.840
So I think, you know, they have access to it

01:12:35.840 --> 01:12:38.360
and they can produce a string of papers

01:12:38.360 --> 01:12:40.600
and yes, nobody else can write those papers,

01:12:40.600 --> 01:12:44.480
but from a consumer of research as well as producer, right?

01:12:44.480 --> 01:12:47.920
So from my consumer side, I love to read research

01:12:47.920 --> 01:12:50.560
and I'm glad that they're writing those papers

01:12:50.560 --> 01:12:53.040
because there's a lot of interesting stuff

01:12:53.040 --> 01:12:54.160
in all of the papers.

01:12:54.160 --> 01:12:55.840
So yeah, there's a whole string of papers

01:12:55.840 --> 01:12:59.160
that I would recommend and I can point you to them offline.

01:12:59.160 --> 01:13:01.400
But yeah, there's stuff that, you know,

01:13:01.400 --> 01:13:04.840
we'll see happen publicly next year

01:13:04.840 --> 01:13:06.160
or maybe another year after that

01:13:06.160 --> 01:13:08.840
when those models become so much better.

01:13:08.840 --> 01:13:13.840
So yeah, no, I think that's been kind of key.

01:13:13.840 --> 01:13:18.200
So I'd say for that commercial, but not commercialized.

01:13:18.200 --> 01:13:21.200
Yes, right, right, right, yeah, yeah, yeah.

01:13:21.200 --> 01:13:23.040
I'll soon to be commercialized, I'm sure,

01:13:23.040 --> 01:13:24.600
but maybe not for I could do.

01:13:24.600 --> 01:13:26.600
Yeah, awesome.

01:13:26.600 --> 01:13:31.600
Next up kind of the intersection between search and LLM's.

01:13:31.600 --> 01:13:32.800
What do you see in there?

01:13:32.800 --> 01:13:36.040
Yeah, so I think that's been kind of an interesting,

01:13:36.040 --> 01:13:38.440
it's been a commercial development again,

01:13:38.440 --> 01:13:40.040
questionable to some degree,

01:13:40.040 --> 01:13:42.040
but because I don't think the research is,

01:13:42.040 --> 01:13:44.000
and these models are quite up to stuff,

01:13:44.000 --> 01:13:48.560
but this somewhat coincided with ChatGPT.

01:13:49.760 --> 01:13:52.760
Well, ChatGPT certainly raised a ton of questions about,

01:13:52.760 --> 01:13:54.360
hey, is this a Google killer?

01:13:54.360 --> 01:13:56.000
Right, right, right, right.

01:13:56.000 --> 01:13:57.240
Yes, exactly.

01:13:57.240 --> 01:13:58.360
And along the same time,

01:13:58.360 --> 01:14:02.120
there were at least three search engines that I know of.

01:14:02.120 --> 01:14:05.920
There was publicity.ai that I don't think existed before.

01:14:07.040 --> 01:14:09.880
What the product they came up with,

01:14:09.880 --> 01:14:12.720
which is a search engine, which sort of gathers

01:14:12.720 --> 01:14:15.520
all of the results from a typical search engine,

01:14:15.520 --> 01:14:18.680
but then uses GPT-3 like models

01:14:18.680 --> 01:14:21.600
to summarize the content of those links

01:14:21.600 --> 01:14:25.520
and produces a paragraph that actually answers your query.

01:14:25.520 --> 01:14:28.520
U.com is again a search engine that has been around for a while,

01:14:28.520 --> 01:14:32.320
but they brought this whole Chat aspect to their search

01:14:32.320 --> 01:14:34.920
where you're sort of chatting

01:14:34.920 --> 01:14:37.000
and trying to come up with an answer.

01:14:37.000 --> 01:14:40.240
And again, it's sort of not just showing you a bunch of links,

01:14:40.240 --> 01:14:43.800
but composing information into text.

01:14:43.800 --> 01:14:45.600
That's Richard Social's company

01:14:45.600 --> 01:14:48.960
and we'll drop a link to my interview with him

01:14:48.960 --> 01:14:50.440
in the show notes as well.

01:14:50.440 --> 01:14:51.840
Okay, cool, yeah, yeah.

01:14:51.840 --> 01:14:56.840
And Niva is another, it's a private search company.

01:14:58.080 --> 01:15:00.760
It's a startup that also has an AI agent

01:15:00.760 --> 01:15:02.960
that you can talk to with and things like that, right?

01:15:02.960 --> 01:15:04.840
So I haven't played around with all of them.

01:15:04.840 --> 01:15:07.280
I've played around with them a little bit.

01:15:07.280 --> 01:15:11.200
And again, it's very easy to find problems

01:15:11.200 --> 01:15:15.600
and sort of realize that, okay, these language models are,

01:15:15.600 --> 01:15:17.080
you know, this interface is great

01:15:17.080 --> 01:15:19.440
and it would be great to get the right paragraph

01:15:19.440 --> 01:15:20.960
if it could get there,

01:15:20.960 --> 01:15:23.440
but oftentimes it don't quite work

01:15:23.440 --> 01:15:25.640
because of sort of fundamental issues with language models,

01:15:25.640 --> 01:15:28.240
but I think from a commercial development,

01:15:28.240 --> 01:15:31.600
I'm pretty excited about what search would look like in the future

01:15:31.600 --> 01:15:36.600
and where language models would fit into that for much.

01:15:36.600 --> 01:15:40.720
Yeah, one of my thought experiments

01:15:40.720 --> 01:15:44.240
with this in the context of chat GPT,

01:15:44.240 --> 01:15:45.760
I'm not that it was particularly deep,

01:15:45.760 --> 01:15:50.760
but like there was this early meme,

01:15:50.760 --> 01:15:52.440
you know, along the lines of,

01:15:52.440 --> 01:15:55.640
hey, Google searches crap, now it's all ads,

01:15:57.000 --> 01:16:00.400
chat GPT, you know, I love this interface,

01:16:00.400 --> 01:16:02.080
you know, it's gonna kill Google.

01:16:02.080 --> 01:16:07.160
And so I asked chat GPT to basically build response

01:16:07.160 --> 01:16:08.760
with an ad in it.

01:16:08.760 --> 01:16:11.000
It works, it can do it.

01:16:11.000 --> 01:16:13.080
I wouldn't be so sure that your, you know,

01:16:13.080 --> 01:16:15.840
LLM based search won't have any ads.

01:16:15.840 --> 01:16:17.640
Right, right, yeah.

01:16:17.640 --> 01:16:18.800
Yeah, no, I think, yeah,

01:16:19.880 --> 01:16:23.360
where the ads would come in and how subtle the ads

01:16:23.360 --> 01:16:25.240
will be once you throw in language model into it.

01:16:25.240 --> 01:16:28.280
Yeah, that's kind of interesting question.

01:16:28.280 --> 01:16:31.440
And I guess next up on your list of commercial developments

01:16:31.440 --> 01:16:35.320
is what I might call the LLMing of all the things.

01:16:35.320 --> 01:16:36.640
Yeah, so I think, you know,

01:16:36.640 --> 01:16:40.920
it's been two years or so since GPT three came out.

01:16:41.800 --> 01:16:45.320
And it's the question of like, okay,

01:16:45.320 --> 01:16:48.200
where is the world changing products

01:16:48.200 --> 01:16:50.640
that are using GPT when it came out,

01:16:50.640 --> 01:16:52.600
hey, it was gonna change everything,

01:16:52.600 --> 01:16:54.080
has it changed everything?

01:16:54.080 --> 01:16:57.600
And I would say like for the most part, no.

01:16:57.600 --> 01:17:01.160
The products that did seem to show some promise

01:17:01.160 --> 01:17:03.800
and some of these are ones that will appear in the future

01:17:03.800 --> 01:17:06.360
but have been kind of semi-announced.

01:17:06.360 --> 01:17:08.440
It's the notion of writing assistantships, right?

01:17:08.440 --> 01:17:12.600
So I think notion, notion AI is the one I think about

01:17:12.600 --> 01:17:15.160
where a lot of people, it's a mainstream product,

01:17:15.160 --> 01:17:16.880
anybody can use notion.

01:17:16.880 --> 01:17:19.920
And notion has this GPT three thing built in

01:17:19.920 --> 01:17:22.840
where it can write to do lists for you

01:17:22.840 --> 01:17:23.880
and things like that.

01:17:23.880 --> 01:17:28.520
So I think that is a pretty strong first version

01:17:28.520 --> 01:17:31.400
of GPT three as a commercial product

01:17:31.400 --> 01:17:34.120
that anybody can use that I'm quite excited about.

01:17:34.120 --> 01:17:38.160
I feel like the timing there is very chat GPT influence.

01:17:38.160 --> 01:17:40.320
Obviously, they've been working on it, you know,

01:17:40.320 --> 01:17:43.520
they saw it when GPT three came out

01:17:43.520 --> 01:17:47.520
but I think they made it available right after chat GPT

01:17:47.520 --> 01:17:52.280
and a lot of these, you know,

01:17:52.280 --> 01:17:54.600
Jasper's been around for a while

01:17:54.600 --> 01:17:59.200
but there's a lot of new kind of writing assistant

01:18:00.280 --> 01:18:02.080
types of things and it just does seem like

01:18:02.080 --> 01:18:04.440
there's a step function increase in kind of energy

01:18:04.440 --> 01:18:09.440
in the space of using LLAMs since chat GPT.

01:18:11.080 --> 01:18:13.160
Even though they're all based on GPT three

01:18:13.160 --> 01:18:15.680
which has been around for two years, right?

01:18:15.680 --> 01:18:19.720
Yeah, so I don't know exactly why that thing seemed

01:18:19.720 --> 01:18:20.600
to align well, right?

01:18:20.600 --> 01:18:22.680
So it's like, yeah, GPT three was announced

01:18:22.680 --> 01:18:24.880
but it was a while before the API was rolled down

01:18:24.880 --> 01:18:27.640
to everybody and you know, and maybe after that

01:18:27.640 --> 01:18:29.920
it takes a while to make the business case for these things.

01:18:29.920 --> 01:18:33.440
So yeah, maybe it's just timing of why it worked

01:18:33.440 --> 01:18:36.200
or there were people like already kind of working on it

01:18:36.200 --> 01:18:38.400
in a sort of on the side and they were like,

01:18:38.400 --> 01:18:41.640
hey, now we gotta sort of write this wave

01:18:41.640 --> 01:18:43.200
and sort of introduce things, right?

01:18:43.200 --> 01:18:45.160
So I don't know exactly what that looks like

01:18:45.160 --> 01:18:48.400
but yeah, no, I think the fact that it aligns

01:18:48.400 --> 01:18:51.160
also gets a lot more excitement and people know,

01:18:51.160 --> 01:18:54.360
okay, chat GPT is something I've played around with.

01:18:54.360 --> 01:18:57.280
This is now a chat GPT that's working on something

01:18:57.280 --> 01:18:59.720
that I do and there's a lot of value in that.

01:18:59.720 --> 01:19:04.720
Am I detecting an underlying pessimism maybe

01:19:06.000 --> 01:19:10.920
about like, you know, kind of, you know,

01:19:10.920 --> 01:19:13.000
where's the flying car that I was promised

01:19:13.000 --> 01:19:15.200
all I have is this GPT three thing?

01:19:15.200 --> 01:19:16.640
Well, it's not so much the pessimism

01:19:16.640 --> 01:19:20.120
because when I saw GPT three, it became evident to me

01:19:20.120 --> 01:19:22.160
that this is a great language model

01:19:22.160 --> 01:19:26.080
but it's not clear as it is how it can be made

01:19:26.080 --> 01:19:27.960
into a product, right?

01:19:27.960 --> 01:19:29.840
But it still came with a lot of hype

01:19:29.840 --> 01:19:31.840
and yeah, it can generate a bunch of things

01:19:31.840 --> 01:19:34.920
but we quite haven't quite seen

01:19:34.920 --> 01:19:36.840
what the product version of those look like.

01:19:36.840 --> 01:19:40.240
I think the language models are extremely powerful

01:19:40.240 --> 01:19:41.920
not just as language models

01:19:41.920 --> 01:19:45.120
but they can be converted into products.

01:19:45.120 --> 01:19:47.920
I don't quite feel like we are at a stage

01:19:47.920 --> 01:19:50.000
where it's just going to be through prompting

01:19:50.000 --> 01:19:53.080
and, you know, let's just tweak it a little bit.

01:19:53.080 --> 01:19:54.680
I think there are a bunch of products

01:19:54.680 --> 01:19:57.120
that will come out of just by doing that

01:19:57.120 --> 01:19:58.720
but there's a whole slew of product

01:19:58.720 --> 01:20:01.920
where the language models need to know a lot more

01:20:01.920 --> 01:20:04.920
about the context where they're gonna be

01:20:05.720 --> 01:20:09.920
to be able to be effective, yeah, effective tools.

01:20:10.920 --> 01:20:14.120
And you mentioned Microsoft, what do you have in mind there?

01:20:14.120 --> 01:20:16.520
Yeah, this was sort of a news that came out recently

01:20:16.520 --> 01:20:20.280
where they're trying to have a bigger stake in OpenAI

01:20:20.280 --> 01:20:23.880
but also just generally thinking of having OpenAI

01:20:23.880 --> 01:20:28.120
like tools available in Word, available in PowerPoint

01:20:28.120 --> 01:20:30.520
and all of these things, they don't have it yet

01:20:30.520 --> 01:20:34.000
but I think those kind of things are just some of the comments.

01:20:34.000 --> 01:20:38.120
Do you think a chat GPT-based Bing is a Google killer?

01:20:38.120 --> 01:20:41.360
Oh, I don't think with that branding

01:20:41.360 --> 01:20:46.360
they would have to call it something else or at this point, yeah.

01:20:46.680 --> 01:20:49.320
I mean, that seemed to be the suggestion, right?

01:20:49.320 --> 01:20:54.520
Chat GPT comes out, they're gonna take a big stake

01:20:54.520 --> 01:20:58.800
and it was mentioned, if not in the official announcement,

01:20:58.800 --> 01:21:02.520
it seemed to be the conjecture that it was gonna be

01:21:02.520 --> 01:21:07.520
some tie up with Bing explicitly to target search, right?

01:21:07.520 --> 01:21:11.640
I think there needs to be a lot more fundamental work

01:21:11.640 --> 01:21:13.480
and we can talk about this in the future predictions

01:21:13.480 --> 01:21:15.640
but there needs to be a lot more fundamental work

01:21:15.640 --> 01:21:20.080
before we sort of are able to kill search

01:21:20.080 --> 01:21:21.760
just by putting a language model, right?

01:21:21.760 --> 01:21:25.400
Like I think that gap is not as simple as replacing something

01:21:25.400 --> 01:21:28.040
or just augmenting existing search.

01:21:28.040 --> 01:21:30.760
I think you would have to think about what kind of things

01:21:30.760 --> 01:21:33.000
can language models actually do

01:21:33.000 --> 01:21:36.520
and you still want to rely on sources and things like that

01:21:36.520 --> 01:21:40.880
but yeah, so I think it's going to happen at some point

01:21:40.880 --> 01:21:44.240
but it's going to be like search as a,

01:21:44.240 --> 01:21:46.920
it won't be replacing search because it'll be a different thing,

01:21:46.920 --> 01:21:49.320
right? Like it'll be, it's not gonna be search

01:21:49.320 --> 01:21:50.880
because not in the way we think about search.

01:21:50.880 --> 01:21:52.560
It literally means, yeah.

01:21:52.560 --> 01:21:53.560
Exactly.

01:21:53.560 --> 01:21:56.240
It'll be question-answering or it'll be something else, right?

01:21:56.240 --> 01:21:58.960
Like it'll be a helper or whatever,

01:21:58.960 --> 01:22:01.640
but searches may not be anything.

01:22:01.640 --> 01:22:04.880
Well, one quick thing before we jump into predictions,

01:22:04.880 --> 01:22:09.560
you kind of reflected on your top use case for the year

01:22:09.560 --> 01:22:12.920
and that was code pilot.

01:22:12.920 --> 01:22:15.840
Tell me a little bit more about how you thinking about that.

01:22:15.840 --> 01:22:18.480
I think co-pilot came out probably not exactly

01:22:18.480 --> 01:22:21.720
in this calendar, but I feel like it got a lot more

01:22:21.720 --> 01:22:25.960
adoption this year and started becoming part of the tools

01:22:25.960 --> 01:22:28.680
where people are coding and firstly,

01:22:28.680 --> 01:22:30.080
I started using co-pilot this year,

01:22:30.080 --> 01:22:33.240
so I'm gonna put it in top use case this year.

01:22:33.240 --> 01:22:36.520
And I will say before notion.ai,

01:22:36.520 --> 01:22:39.280
co-pilot was probably the only use

01:22:39.280 --> 01:22:43.160
of large language models that I saw anywhere.

01:22:43.160 --> 01:22:45.840
So from that point of view, it was interesting

01:22:45.840 --> 01:22:47.800
that you could use to came out and there's nothing,

01:22:47.800 --> 01:22:50.600
nothing that can fill co-pilot.

01:22:50.600 --> 01:22:52.480
But from a use case point of view,

01:22:52.480 --> 01:22:54.680
it has been incredibly useful, right?

01:22:54.680 --> 01:22:58.120
So I've been able to, you know,

01:22:58.120 --> 01:23:02.200
do things that has made me a lot more effective as a coder,

01:23:02.200 --> 01:23:04.240
not that I code much, but when I do,

01:23:04.240 --> 01:23:08.520
I want to do a lot and co-pilot has let me sort of do that.

01:23:08.520 --> 01:23:13.040
And that's been amazing, I feel the right combination

01:23:13.040 --> 01:23:15.960
of having a nice user interface,

01:23:15.960 --> 01:23:19.480
having the right data that is trained on

01:23:19.480 --> 01:23:22.280
to be able to sort of really help people

01:23:22.280 --> 01:23:23.480
in what they want to do.

01:23:23.480 --> 01:23:25.400
Now of course, co-pilot has issues,

01:23:25.400 --> 01:23:30.400
it's producing, you know, code that can be dangerous,

01:23:30.400 --> 01:23:33.080
that can be buggy, and of course,

01:23:33.080 --> 01:23:37.400
there are the questions of copyright and plagiarism exactly.

01:23:37.400 --> 01:23:41.400
So I feel like I hope those things will get resolved,

01:23:41.400 --> 01:23:44.480
but those are again, when you start using a language model,

01:23:44.480 --> 01:23:47.920
these are the issues that you have to solve.

01:23:47.920 --> 01:23:50.840
And then I'm glad that co-pilot is bringing all of these things

01:23:50.840 --> 01:23:54.320
into the discussion by having, by it being out there.

01:23:55.320 --> 01:23:57.120
Yeah, I've had the same experience with it.

01:23:57.120 --> 01:24:02.120
I think I've shared this on social or in the podcast

01:24:03.080 --> 01:24:04.520
in a conversation.

01:24:04.520 --> 01:24:09.440
I saw all the co-pilot demos played around with it

01:24:09.440 --> 01:24:11.640
with kind of the toy problem things,

01:24:11.640 --> 01:24:14.920
but I don't do a lot of coding necessarily,

01:24:14.920 --> 01:24:17.440
but I do tend to binge on coding everyone's in law.

01:24:17.440 --> 01:24:19.920
Like, and usually like that end of year holiday thing,

01:24:19.920 --> 01:24:22.920
I'll have some projects, and I did that this year

01:24:22.920 --> 01:24:25.600
and used co-pilot, it was amazing.

01:24:25.600 --> 01:24:29.680
Like the productivity, you can, it helps,

01:24:29.680 --> 01:24:33.600
the productivity helps create for you, attacking,

01:24:35.160 --> 01:24:37.280
a new problem with new tools,

01:24:37.280 --> 01:24:40.120
without the context switching of going to Google

01:24:40.120 --> 01:24:42.720
and Stack Overflow, like it's incredible.

01:24:42.720 --> 01:24:44.600
I'm a total believer.

01:24:44.600 --> 01:24:47.880
Yeah, and I think that exactly is the kind of thing

01:24:47.880 --> 01:24:50.560
I expect language models to be useful for.

01:24:50.560 --> 01:24:51.920
They are not going to, and you know,

01:24:51.920 --> 01:24:53.640
which at GPD going back a little bit,

01:24:53.640 --> 01:24:56.120
people are talking about, hey, if people are going to lose jobs

01:24:56.120 --> 01:24:57.920
and it's going to change everything,

01:24:57.920 --> 01:25:01.200
and you know, we'll replace XYZ with Giat GPD.

01:25:01.200 --> 01:25:04.760
And I don't quite see that happening,

01:25:04.760 --> 01:25:07.760
but I do expect a lot of people in many different areas

01:25:07.760 --> 01:25:11.440
becoming a lot more productive because of Giat GPD.

01:25:11.440 --> 01:25:13.120
And co-pilot is, you know,

01:25:13.120 --> 01:25:15.000
is an example of how language models

01:25:15.000 --> 01:25:19.120
can make you a lot more productive without replacing,

01:25:19.120 --> 01:25:21.320
I don't think it's replacing specific programmers,

01:25:21.320 --> 01:25:23.680
it's just making, allowing them to do a lot more.

01:25:24.680 --> 01:25:27.440
And that I think is the best use of technology.

01:25:27.440 --> 01:25:28.280
Awesome, awesome.

01:25:28.280 --> 01:25:29.760
Well, let's jump into predictions.

01:25:31.480 --> 01:25:35.520
What are you most excited about kind of looking

01:25:35.520 --> 01:25:37.080
into your crystal wall?

01:25:37.080 --> 01:25:40.840
So I think the Giat GPD is the one that sort of,

01:25:40.840 --> 01:25:42.800
everybody knew language models,

01:25:42.800 --> 01:25:45.520
they just trained on data and making predictions.

01:25:45.520 --> 01:25:49.680
What Giat GPD really did was remind everyone, like,

01:25:49.680 --> 01:25:52.600
okay, even if the language modeling part is

01:25:52.600 --> 01:25:54.080
quote unquote, salt, right?

01:25:54.080 --> 01:25:56.920
Even if you get a really, really large language model,

01:25:56.920 --> 01:26:00.320
that doesn't mean you're done, right?

01:26:00.320 --> 01:26:03.440
And I think one of the biggest aspects of that was

01:26:04.400 --> 01:26:08.760
making sure that what you're generating is not just BS,

01:26:08.760 --> 01:26:12.400
it's somehow valid, somehow the truth,

01:26:12.400 --> 01:26:16.400
somehow something that you can cite and rely on, right?

01:26:16.400 --> 01:26:20.080
They definitely signed a light on how challenging that is.

01:26:20.080 --> 01:26:20.920
Right.

01:26:20.920 --> 01:26:21.760
Exactly.

01:26:21.760 --> 01:26:23.760
So I don't think this is going to be a prediction

01:26:23.760 --> 01:26:27.600
necessarily for 2023, maybe 2023 is when we'll start

01:26:27.600 --> 01:26:29.680
seeing the first attempts at this,

01:26:29.680 --> 01:26:33.440
but being able to generate text that's,

01:26:33.440 --> 01:26:35.040
that's not have misinformation,

01:26:35.040 --> 01:26:38.320
that differentiates factual from, you know,

01:26:38.320 --> 01:26:43.240
creative hallucinations that is able to cite its sources

01:26:43.240 --> 01:26:44.600
and sort of point to like,

01:26:44.600 --> 01:26:48.280
hello, this is the piece of paragraph that I'm based on,

01:26:48.280 --> 01:26:50.280
which I'm generating a piece of text.

01:26:50.280 --> 01:26:54.600
I think those things are needed and it's probably going

01:26:54.600 --> 01:26:58.400
to be the next aspect of language models

01:26:58.400 --> 01:27:01.680
that's going to be a big topic of research.

01:27:01.680 --> 01:27:04.240
Do you have a sense for where,

01:27:04.240 --> 01:27:09.240
how we get there is it kind of applying the same tools,

01:27:09.240 --> 01:27:13.640
RLHF, for example, attacking this specific problem,

01:27:13.640 --> 01:27:16.280
or do you think is, you know,

01:27:16.280 --> 01:27:18.440
we don't have the tools and it's going to need to be

01:27:18.440 --> 01:27:22.320
kind of new invention that gets us there.

01:27:22.320 --> 01:27:24.920
I think it's going to have to be new inventions

01:27:24.920 --> 01:27:27.520
and I want to sort of think of it as not just,

01:27:27.520 --> 01:27:31.320
you know, how do we attribute it to specific pieces of text,

01:27:31.320 --> 01:27:34.720
but I kind of think of it as like being able to use

01:27:34.720 --> 01:27:38.320
other tools, being able to use other things

01:27:38.320 --> 01:27:40.720
available to the language model

01:27:40.720 --> 01:27:42.400
when it's being trained as well, right?

01:27:42.400 --> 01:27:47.200
So it should not rely on memorizing facts to any degree.

01:27:47.200 --> 01:27:51.160
It should just rely on using existing tools,

01:27:51.160 --> 01:27:54.520
including search, including maybe calculations,

01:27:54.520 --> 01:27:59.160
maybe even Python, interpreter, whatever else it needs to do,

01:27:59.160 --> 01:28:02.160
but still be able to do the language modeling task, right?

01:28:02.160 --> 01:28:04.800
So I think there is some combination of being able

01:28:04.800 --> 01:28:08.360
to refer to external stuff and still do language modeling

01:28:08.360 --> 01:28:10.840
that we quite haven't quite extracted

01:28:10.840 --> 01:28:15.840
and that would be something that I think will come into picture.

01:28:16.000 --> 01:28:19.840
I'll give you an example of how sort of some people

01:28:19.840 --> 01:28:20.840
have been thinking about it.

01:28:20.840 --> 01:28:24.880
There's this whole idea of retrieval-based language modeling

01:28:24.880 --> 01:28:27.440
where you're still generating the text

01:28:27.440 --> 01:28:30.480
token by token, but you're always retrieving

01:28:30.480 --> 01:28:33.080
some set of documents and you're conditioning on them

01:28:33.080 --> 01:28:35.440
when you're generating each token.

01:28:35.440 --> 01:28:40.000
That's sort of one step towards what I'm talking about,

01:28:40.000 --> 01:28:43.440
where at least you're trying to look at retrieved documents

01:28:43.440 --> 01:28:46.400
when you're generating, but that doesn't guarantee

01:28:46.400 --> 01:28:49.600
what you're generating is actually based on.

01:28:49.600 --> 01:28:54.600
So you just spoke earlier about the decomposed reasoning.

01:28:55.080 --> 01:29:00.080
Is this prediction that those ideas become more real

01:29:01.080 --> 01:29:03.680
in some way in 2324?

01:29:03.680 --> 01:29:10.680
Is it that what we're doing with a trained model

01:29:10.280 --> 01:29:13.160
to kind of get decomposed reasoning,

01:29:13.160 --> 01:29:16.640
we're gonna push even deeper into the fundamental creation

01:29:16.640 --> 01:29:19.480
of the model like at train time and other things?

01:29:19.480 --> 01:29:21.800
Yeah, so more of the latter, right?

01:29:21.800 --> 01:29:23.720
So right now we are expecting the model

01:29:23.720 --> 01:29:25.600
to be able to do decouples reasoning,

01:29:25.600 --> 01:29:28.760
but we only do it at test time in some sense, right?

01:29:28.760 --> 01:29:31.080
Let's actually try to start thinking,

01:29:31.080 --> 01:29:33.000
putting that stuff during training, right?

01:29:33.000 --> 01:29:36.160
So like, again, I don't want to make this analogy too much,

01:29:36.160 --> 01:29:37.760
but when you think about,

01:29:37.760 --> 01:29:41.120
when you're training a human on how to do things,

01:29:41.120 --> 01:29:44.480
you don't just give it pairs of input and output,

01:29:44.480 --> 01:29:47.560
you give it a little bit more of a decomposition

01:29:47.560 --> 01:29:50.720
and then based on that, they're able to do what they do.

01:29:50.720 --> 01:29:53.320
If you want them to use the Python interpreter,

01:29:53.320 --> 01:29:56.720
you don't just expect them to finish it on their own,

01:29:56.720 --> 01:29:58.960
they can use the interpreter when needed, get a thing, right?

01:29:58.960 --> 01:30:01.600
So I just think of language models as,

01:30:01.600 --> 01:30:03.920
yeah, maybe they're still doing the language modeling task,

01:30:03.920 --> 01:30:06.480
but they have access to a bunch of other tools.

01:30:07.480 --> 01:30:09.800
And maybe this is more far-fetched than 2023,

01:30:09.800 --> 01:30:11.720
but I think in the long run,

01:30:11.720 --> 01:30:15.040
you want a system that's able to do those things.

01:30:15.040 --> 01:30:17.560
You got your next prediction is around diffusion models.

01:30:17.560 --> 01:30:20.520
It's kind of surprising that that term hasn't come up yet so far.

01:30:20.520 --> 01:30:22.960
Yeah, I guess it is surprising,

01:30:22.960 --> 01:30:24.520
but also in NLP in general,

01:30:24.520 --> 01:30:27.080
I feel like we are barely scratching the surface

01:30:27.080 --> 01:30:30.480
of what diffusion models can do.

01:30:30.480 --> 01:30:32.160
So, yeah, I think,

01:30:32.160 --> 01:30:33.960
clearly in the image generation space,

01:30:33.960 --> 01:30:37.080
we've seen a lot of progress with diffusion models

01:30:37.080 --> 01:30:40.760
and we've seen some in NLP, but not enough.

01:30:40.760 --> 01:30:44.160
I guess what I find attractive about diffusion model

01:30:44.160 --> 01:30:47.360
is that it's trying to generate more than just

01:30:47.360 --> 01:30:49.360
a single thing at a point point, right?

01:30:49.360 --> 01:30:52.480
So when diffusion models are applied to text,

01:30:52.480 --> 01:30:54.400
the way it would look like is not just

01:30:54.400 --> 01:30:56.440
producing one token at a time,

01:30:56.440 --> 01:30:58.960
it will try to produce a whole sentence

01:30:58.960 --> 01:31:02.320
or whatever we decide is the right guarantee.

01:31:02.320 --> 01:31:05.840
And that idea of a model that is trained

01:31:05.840 --> 01:31:08.320
not to do one token at a time,

01:31:08.320 --> 01:31:11.800
but to do something bigger really appeals to me

01:31:11.800 --> 01:31:13.720
because I feel like a lot of the issues

01:31:13.720 --> 01:31:16.200
we talk about with language models

01:31:16.200 --> 01:31:18.320
fundamentally come from the fact that

01:31:18.320 --> 01:31:20.800
it's trained to do one token at a time

01:31:20.800 --> 01:31:24.280
and sort of, and that's kind of the loss, right?

01:31:24.280 --> 01:31:29.280
So if we can have the model be trained to generate more

01:31:29.360 --> 01:31:31.120
and then give it a loss,

01:31:31.120 --> 01:31:33.080
I think that's fundamentally interesting

01:31:33.080 --> 01:31:35.800
in the previous model, so provide one way of doing it.

01:31:35.800 --> 01:31:40.800
Do you, would you kind of visualize this

01:31:40.800 --> 01:31:45.880
as a model like in a first iteration,

01:31:45.880 --> 01:31:48.360
spitting out bullshit and then successively

01:31:48.360 --> 01:31:50.800
like iterating towards truth?

01:31:50.800 --> 01:31:53.560
I guess that one way that this could play out.

01:31:53.560 --> 01:31:57.920
Yes, well, I mean, probably not,

01:31:57.920 --> 01:32:01.280
probably it's gonna be somewhere in the latent space.

01:32:01.280 --> 01:32:04.560
But I think the way I think about it is

01:32:04.560 --> 01:32:08.120
like if we were doing this token by token thing

01:32:08.120 --> 01:32:10.520
for images, it just wouldn't make sense, right?

01:32:10.520 --> 01:32:14.120
Like, pretty certainly would produce the images

01:32:14.120 --> 01:32:18.040
that we see coming out of stable diffusion and then yeah.

01:32:18.040 --> 01:32:20.040
Or even what it's going to learn

01:32:20.040 --> 01:32:21.400
is going to be something different,

01:32:21.400 --> 01:32:24.080
what it's going to learn is given the image

01:32:24.080 --> 01:32:25.520
that I have seen so far,

01:32:25.520 --> 01:32:29.040
let me predict the next pixel or the next piece, right?

01:32:29.040 --> 01:32:32.360
That somehow feels like a fundamentally different task

01:32:32.360 --> 01:32:34.960
than being able to generate an image fully, right?

01:32:35.960 --> 01:32:40.200
And so I feel like thinking about the same idea

01:32:40.200 --> 01:32:41.880
for text just kind of makes sense,

01:32:41.880 --> 01:32:44.880
like you write the summary in one shot

01:32:44.880 --> 01:32:48.040
and realize how wrong it is,

01:32:48.040 --> 01:32:50.440
feels like there's something fundamentally different

01:32:50.440 --> 01:32:53.120
than hey, you got a bunch of tokens correct,

01:32:53.120 --> 01:32:54.320
but you also got a bunch.

01:32:54.320 --> 01:32:56.320
And in some sense, there are some analogies

01:32:56.320 --> 01:32:59.920
to our LHF and using PPO for training,

01:32:59.920 --> 01:33:04.160
for example, where you try to make sure it's fluent

01:33:04.160 --> 01:33:05.200
and things like that.

01:33:05.200 --> 01:33:08.240
These are all losses designed on not just token

01:33:08.240 --> 01:33:10.800
by token basis, but something that's longer.

01:33:10.800 --> 01:33:13.440
And so we've known how useful they've been.

01:33:13.440 --> 01:33:15.480
So I feel like there may be something

01:33:15.480 --> 01:33:18.160
in taking that idea and applying it to appreciating

01:33:18.160 --> 01:33:21.080
something that's interesting, interesting.

01:33:21.080 --> 01:33:23.280
I expect a lot of people will be wanting

01:33:23.280 --> 01:33:25.120
to figure out how to do that.

01:33:25.120 --> 01:33:26.200
Great.

01:33:26.200 --> 01:33:29.240
And online updates to models.

01:33:29.240 --> 01:33:32.080
Yeah, so I think one of the problems with language models,

01:33:32.080 --> 01:33:34.280
so let's keep aside the grand vision

01:33:34.280 --> 01:33:36.880
of how language models will use search

01:33:36.880 --> 01:33:38.120
and all of these other things.

01:33:38.120 --> 01:33:40.320
But one of the fundamental problems with language models

01:33:40.320 --> 01:33:43.640
is that the word changes, but they don't.

01:33:44.480 --> 01:33:47.120
And this seems to be a fundamental sort of issue

01:33:47.120 --> 01:33:49.480
with language models, right?

01:33:49.480 --> 01:33:53.680
So I think thinking about how we can update

01:33:53.680 --> 01:33:59.160
language models every month or every week or every day,

01:33:59.160 --> 01:34:03.320
I think is an interesting problem to be thinking about

01:34:03.320 --> 01:34:04.880
and becomes increasingly relevant,

01:34:04.880 --> 01:34:08.240
where Bert doesn't know anything about COVID,

01:34:08.240 --> 01:34:10.800
so it's not useful for a bunch of applications,

01:34:10.800 --> 01:34:12.960
even though otherwise fundamentally,

01:34:12.960 --> 01:34:15.160
there's nothing wrong with it, right?

01:34:15.160 --> 01:34:19.640
That kind of stuff is just not fun.

01:34:19.640 --> 01:34:22.840
And I think there would be research on trying to fix that.

01:34:22.840 --> 01:34:27.720
What's the current, not necessarily state of the art,

01:34:27.720 --> 01:34:32.280
but the current approach for doing this at the scale

01:34:32.280 --> 01:34:35.320
of a GPT-3, is it collect more data

01:34:35.320 --> 01:34:36.440
and retrain from scratch?

01:34:36.440 --> 01:34:42.400
Or how did they approximate or approach

01:34:42.400 --> 01:34:46.320
some kind of incremental training ability, if at all?

01:34:46.320 --> 01:34:51.560
Yeah, so there hasn't been that much work on that front,

01:34:51.560 --> 01:34:56.320
I would say, this is something that needs a lot more attention.

01:34:56.320 --> 01:35:00.160
Yeah, but I think there'd be parameter efficient training

01:35:00.160 --> 01:35:05.160
on how can we slightly improve the change the model,

01:35:08.800 --> 01:35:10.120
but not completely change it.

01:35:10.120 --> 01:35:12.200
Find the set of parameters that we should update,

01:35:12.200 --> 01:35:14.840
so that it's not updating the whole parameters,

01:35:14.840 --> 01:35:16.520
but updating a little bit of it,

01:35:16.520 --> 01:35:19.000
things like that I feel are around,

01:35:19.000 --> 01:35:20.920
but it needs a lot more work.

01:35:20.920 --> 01:35:23.920
You kind of, one way to think about the fundamental problem

01:35:23.920 --> 01:35:25.360
is with the transformer,

01:35:25.360 --> 01:35:28.560
it's not like a layered architecture, like a CNN,

01:35:28.560 --> 01:35:32.320
where you can just chop off the N layers and retrain

01:35:32.320 --> 01:35:36.200
from that point, it's just a much more complex

01:35:36.200 --> 01:35:37.320
and interconnected model,

01:35:37.320 --> 01:35:42.040
so that kind of incremental updating doesn't work.

01:35:42.040 --> 01:35:44.720
Not so easily, yeah, I think there's been some work

01:35:44.720 --> 01:35:48.640
on sort of taking like one percent of the parameters

01:35:48.640 --> 01:35:50.320
sort of spread over the transformer

01:35:50.320 --> 01:35:52.800
and updating them with new text,

01:35:52.800 --> 01:35:55.520
but I think solving this problem

01:35:55.520 --> 01:35:59.520
is going to be something that needs to happen pretty quickly.

01:36:00.560 --> 01:36:03.040
And so to be clear, taking a step back,

01:36:03.040 --> 01:36:06.640
like this is all the looking forward section,

01:36:06.640 --> 01:36:10.640
those three things, kind of misinformation

01:36:10.640 --> 01:36:12.440
and attributable generations,

01:36:12.440 --> 01:36:14.200
diffusion models, and online updates

01:36:14.200 --> 01:36:16.200
for specifically in your category

01:36:16.200 --> 01:36:20.640
of the greatest, most exciting opportunities in the field.

01:36:21.520 --> 01:36:26.040
Areas where we're likely to see a lot of research attention

01:36:27.520 --> 01:36:30.640
and possibly some really interesting results

01:36:30.640 --> 01:36:32.400
coming up in the next year or two.

01:36:32.400 --> 01:36:35.320
And also sort of fundamental problems

01:36:35.320 --> 01:36:37.840
that need to be addressed by language models.

01:36:37.840 --> 01:36:42.600
And so that brings us to your top three predictions

01:36:42.600 --> 01:36:46.920
for the field's proper, what do you see there?

01:36:46.920 --> 01:36:49.760
Yeah, so I think, and maybe some of it is little

01:36:49.760 --> 01:36:51.800
with a disappointment as well.

01:36:51.800 --> 01:36:55.520
So the first one here is multiple modalities.

01:36:55.520 --> 01:36:57.000
I think there's been a lot of exciting work,

01:36:57.000 --> 01:36:59.160
so I don't want to sort of think that away.

01:36:59.160 --> 01:37:03.680
But to me, after GBT3 came out and then you saw a clip

01:37:03.680 --> 01:37:08.680
when Dali and Whisper and now there's video models

01:37:08.720 --> 01:37:11.600
and things like that, to me fundamentally,

01:37:11.600 --> 01:37:16.200
I don't understand technically why they're not the same model,

01:37:16.200 --> 01:37:17.840
but it's still a little bit disappointing

01:37:17.840 --> 01:37:19.400
that they're not the same model.

01:37:19.400 --> 01:37:22.600
It's like, why is there not the same model

01:37:22.600 --> 01:37:25.240
that change over the same data?

01:37:25.240 --> 01:37:29.600
GBT3 is trained on, but also on the Lyon dataset

01:37:29.600 --> 01:37:33.400
that does all the images and text and audio

01:37:33.400 --> 01:37:35.280
and video and stuff like that, right?

01:37:35.280 --> 01:37:39.320
And I think this is a sort of near future prediction

01:37:39.320 --> 01:37:42.800
is that we are going to see ways for pre-gaining models

01:37:42.800 --> 01:37:45.480
that cuts across multiple modalities.

01:37:45.480 --> 01:37:47.160
And I think clip was a good example,

01:37:47.160 --> 01:37:49.120
sort of early example of what you can do

01:37:49.120 --> 01:37:51.040
when you have a lot of text and images.

01:37:51.040 --> 01:37:53.960
But I think it still didn't have access

01:37:53.960 --> 01:37:57.320
to a lot of text-only data.

01:37:57.320 --> 01:38:00.800
And I want a model that can do chat GBT-like things,

01:38:00.800 --> 01:38:05.800
but also generate images for me and maybe read them out

01:38:06.000 --> 01:38:06.760
and things like that, right?

01:38:06.760 --> 01:38:11.760
So I feel like multiple modalities is an exciting sort of

01:38:12.400 --> 01:38:14.520
kind of an opportunity, but definitely something

01:38:14.520 --> 01:38:15.800
that's going to happen soon.

01:38:17.040 --> 01:38:19.120
When I first heard you describe this,

01:38:19.120 --> 01:38:22.040
I thought, well, multi-modal, that was the big thing

01:38:22.040 --> 01:38:25.920
we were talking about in these trends, conversations last year,

01:38:25.920 --> 01:38:27.840
but you're going a level deeper.

01:38:27.840 --> 01:38:31.960
You don't want multi-modal use cases or outputs.

01:38:31.960 --> 01:38:36.240
You want a single architecture to do multi-modal things.

01:38:36.240 --> 01:38:37.440
That's what I want.

01:38:37.440 --> 01:38:41.000
My prediction is going to be a little bit more grounded,

01:38:41.000 --> 01:38:46.000
so to say, but yeah, like video, for example,

01:38:46.080 --> 01:38:47.040
is a more concrete one.

01:38:47.040 --> 01:38:50.120
Like text to video, we've seen some initial versions

01:38:50.120 --> 01:38:54.000
of those that's probably where a lot of initial stuff would go in.

01:38:54.000 --> 01:38:58.400
But I've been really excited about sort of the mind dojo

01:38:58.400 --> 01:39:00.840
world of like playing with text and Minecraft

01:39:00.840 --> 01:39:03.160
and having an agent that can do a bunch of things

01:39:03.160 --> 01:39:04.160
in Minecraft.

01:39:04.160 --> 01:39:07.640
I feel like there are things that modules can learn

01:39:07.640 --> 01:39:11.280
from images, even for language modeling,

01:39:11.280 --> 01:39:14.480
it would benefit to see a lot of images in some sense.

01:39:14.480 --> 01:39:16.440
There are just a bunch of things in images

01:39:16.440 --> 01:39:18.280
that we never talk about in text.

01:39:18.280 --> 01:39:27.160
So from an AI agent, I think it's useful to think about something

01:39:27.160 --> 01:39:28.280
that has access to everything.

01:39:28.280 --> 01:39:31.160
But yeah, more concretely, we're just going to be pushing them

01:39:31.160 --> 01:39:32.120
sort of fair-wise.

01:39:32.120 --> 01:39:34.320
Like, yeah, it's going to be audio and images.

01:39:34.320 --> 01:39:37.800
And there's going to be a bunch of other pairs that will happen first.

01:39:37.800 --> 01:39:41.640
But eventually, I think having multi-actual multiple

01:39:41.640 --> 01:39:48.240
modalities, not just greater than one, but altities would be exciting.

01:39:48.240 --> 01:39:48.760
Awesome, awesome.

01:39:48.760 --> 01:39:49.920
Next up.

01:39:49.920 --> 01:39:54.840
Next, I'm kind of excited about better training and better

01:39:54.840 --> 01:39:57.640
inference and better in the sense of being

01:39:57.640 --> 01:39:59.560
more computationally efficient.

01:39:59.560 --> 01:40:03.240
I think this is an exciting work that a bunch of people

01:40:03.240 --> 01:40:04.120
are already doing.

01:40:04.120 --> 01:40:07.080
But I think this is just going to become increasingly

01:40:07.080 --> 01:40:10.080
important from a sustainability point of view,

01:40:10.080 --> 01:40:14.480
but also from university surviving and doing interesting things.

01:40:14.480 --> 01:40:17.800
And small companies are contributing to research.

01:40:17.800 --> 01:40:20.480
I think it's important to be able to train these models,

01:40:20.480 --> 01:40:23.040
to be able to run these models.

01:40:23.040 --> 01:40:25.240
And there's going to be a lot of research

01:40:25.240 --> 01:40:28.600
in trying to do those things.

01:40:28.600 --> 01:40:32.200
And you've got a few examples that we'll link to in the show notes.

01:40:32.200 --> 01:40:35.600
Any anything that you want to point out?

01:40:35.600 --> 01:40:39.400
Yeah, so let me mention two that I saw recently.

01:40:39.400 --> 01:40:43.280
One of them is this paper called Cramming.

01:40:43.280 --> 01:40:48.840
And the idea here is to think about the scaling laws paper,

01:40:48.840 --> 01:40:51.280
like, hey, what can you do when the models get larger

01:40:51.280 --> 01:40:52.520
and stuff like that?

01:40:52.520 --> 01:40:55.440
The Cramming paper sort of turns it on his head

01:40:55.440 --> 01:41:00.000
and decides, OK, what if I have just one GPU for one day?

01:41:00.000 --> 01:41:02.960
What's the most I can do with that?

01:41:02.960 --> 01:41:05.120
And it's a very different question,

01:41:05.120 --> 01:41:08.720
but it somehow is a lot more relevant to many more people,

01:41:08.720 --> 01:41:12.640
because a lot more people have a single GPU for a single day.

01:41:12.640 --> 01:41:15.600
And they show that you can get almost sort of bird level

01:41:15.600 --> 01:41:17.680
performance if you make the right choices

01:41:17.680 --> 01:41:21.400
and this sort of detail what those choices might look like.

01:41:21.400 --> 01:41:24.680
It's a paper, but I think I like that idea of like, hey,

01:41:24.680 --> 01:41:28.280
what if we were scrappy about training these models?

01:41:28.280 --> 01:41:29.320
How far can we get?

01:41:29.320 --> 01:41:31.480
I think that's a very interesting question

01:41:31.480 --> 01:41:35.600
that Google and OpenAI is not going to be asking,

01:41:35.600 --> 01:41:38.280
but might be relevant for a lot of other research.

01:41:38.280 --> 01:41:40.160
So the other one I want to talk about

01:41:40.160 --> 01:41:43.960
is this Petals work that came out of the big science thing.

01:41:43.960 --> 01:41:45.480
I haven't read too much about this,

01:41:45.480 --> 01:41:48.120
but it seems like a really interesting idea

01:41:48.120 --> 01:41:52.160
of the problem of running really large language models.

01:41:52.160 --> 01:41:56.120
So even if OPT releases 175 billion model,

01:41:56.120 --> 01:41:57.920
how do you actually run it?

01:41:57.920 --> 01:41:59.680
It doesn't really help most people,

01:41:59.680 --> 01:42:03.480
even if you have a big cluster, it's kind of difficult to run it.

01:42:03.480 --> 01:42:06.240
So what this Petals does is they're

01:42:06.240 --> 01:42:09.320
building this framework for using the ideas

01:42:09.320 --> 01:42:13.920
behind BitTorrent of sort of distributed computing

01:42:13.920 --> 01:42:15.600
and bringing it to language models.

01:42:15.600 --> 01:42:17.200
So like, hey, you should be able to run

01:42:17.200 --> 01:42:20.600
these 100 billion size language models,

01:42:20.600 --> 01:42:24.080
distribute it over a bunch of commodities

01:42:24.080 --> 01:42:27.560
of consumer computers.

01:42:27.560 --> 01:42:29.360
So yeah, I think this is an interesting idea.

01:42:29.360 --> 01:42:33.000
I haven't played around with it to see how far you can push it.

01:42:33.000 --> 01:42:37.000
There's partly, you need a bunch of people also running Petals.

01:42:37.000 --> 01:42:40.360
But once we get there, I think that could be a pretty exciting way

01:42:40.360 --> 01:42:42.040
to run language models.

01:42:42.040 --> 01:42:44.160
Interesting, interesting.

01:42:44.160 --> 01:42:50.400
So your third prediction is editing and revising models.

01:42:50.400 --> 01:42:51.600
What do you mean there?

01:42:51.600 --> 01:42:55.520
So these are these family of models that

01:42:55.520 --> 01:42:59.480
are not so much interested in generating text,

01:42:59.480 --> 01:43:03.600
but taking existing text and editing it.

01:43:03.600 --> 01:43:06.400
And I think this is a very interesting idea

01:43:06.400 --> 01:43:08.480
that can become increasingly important.

01:43:08.480 --> 01:43:11.880
And in some sense, this could be the way

01:43:11.880 --> 01:43:14.880
you fix language model output, but actually,

01:43:14.880 --> 01:43:18.560
is to have another model that takes the output of the language model

01:43:18.560 --> 01:43:20.920
and fixes it.

01:43:20.920 --> 01:43:25.000
So some of the work here, there was a paper out of Julias.

01:43:25.000 --> 01:43:30.840
A group from YouTube now that sort of looked at summarization.

01:43:30.840 --> 01:43:33.400
And there are systems that generate summaries.

01:43:33.400 --> 01:43:36.240
How can you take the generated summaries and edit it

01:43:36.240 --> 01:43:39.720
to correct all the factual mistakes it has made?

01:43:39.720 --> 01:43:43.640
And editing is somehow much, let's not say definitely

01:43:43.640 --> 01:43:45.880
a simpler problem, but it's a, in some sense,

01:43:45.880 --> 01:43:49.320
it could be a simpler problem than writing the whole summary

01:43:49.320 --> 01:43:51.720
from scratch, especially when you do the writing,

01:43:51.720 --> 01:43:53.640
you do left to right generation.

01:43:53.640 --> 01:43:55.960
So you can't go back and revisit something

01:43:55.960 --> 01:43:57.200
that you've done before.

01:43:57.200 --> 01:44:00.680
With these editing models, they have the whole picture

01:44:00.680 --> 01:44:02.680
to some degree, and all they have to do

01:44:02.680 --> 01:44:06.800
is fix it so that the picture is consistent.

01:44:06.800 --> 01:44:11.880
And so this idea seems like potentially simpler than generation.

01:44:11.880 --> 01:44:15.000
So you could generate something, and maybe this

01:44:15.000 --> 01:44:16.640
is also attached to diffusion models,

01:44:16.640 --> 01:44:19.480
where you write something that's maybe not so correct,

01:44:19.480 --> 01:44:21.960
but you revise it, and it becomes better.

01:44:21.960 --> 01:44:24.640
So there is a bunch of work along these directions

01:44:24.640 --> 01:44:27.000
that came out essentially this year,

01:44:27.000 --> 01:44:29.440
and maybe second half of this year.

01:44:29.440 --> 01:44:33.200
Some of it early on, that tries to gather data sets

01:44:33.200 --> 01:44:36.920
where you have edits, or try to maybe even generate

01:44:36.920 --> 01:44:40.080
data sets where you have edits, and create these models

01:44:40.080 --> 01:44:43.680
that are able to fix those edits in some sense.

01:44:43.680 --> 01:44:45.520
And so the prediction specifically

01:44:45.520 --> 01:44:50.720
is that teams will build on this and produce models

01:44:50.720 --> 01:44:55.520
that can actually kind of deliver on the ability

01:44:55.520 --> 01:44:59.040
to do editing and revising.

01:44:59.040 --> 01:45:02.320
And I think this could be, for example,

01:45:02.320 --> 01:45:05.640
there'll be an editing model that can fix bias issues.

01:45:05.640 --> 01:45:08.240
There'll be an editing model that fixes toxicity.

01:45:08.240 --> 01:45:11.520
There'll be an editing model that fixes factuality.

01:45:11.520 --> 01:45:14.800
And these editing models can make web searches

01:45:14.800 --> 01:45:18.200
and sort of take that information and edit the output.

01:45:18.200 --> 01:45:23.520
So I could imagine that this could be a practical way

01:45:23.520 --> 01:45:26.120
of solving many of the issues.

01:45:26.120 --> 01:45:29.240
It is a really interesting idea that,

01:45:29.240 --> 01:45:31.120
I don't know if it's like a separation of concerns

01:45:31.120 --> 01:45:34.000
or something like the language model

01:45:34.000 --> 01:45:36.400
doesn't necessarily need to do everything

01:45:36.400 --> 01:45:38.840
if we can compensate.

01:45:38.840 --> 01:45:41.400
So in a way, it's like decomposition as well.

01:45:41.400 --> 01:45:45.640
Like, let it generate, and if the way

01:45:45.640 --> 01:45:49.360
to get something that's not toxic that's accurate

01:45:49.360 --> 01:45:53.640
is to have another type of model support it.

01:45:53.640 --> 01:45:54.160
Great.

01:45:54.160 --> 01:45:56.800
Yeah, I think that's right.

01:45:56.800 --> 01:45:59.680
And for at least for summarization

01:45:59.680 --> 01:46:02.120
and things where it's supposed to be factual and stuff

01:46:02.120 --> 01:46:05.680
like that, I could see it sort of addressing those problems.

01:46:05.680 --> 01:46:08.120
Of course, if it's generating a long text

01:46:08.120 --> 01:46:12.480
and there are longer range sort of consistency issues

01:46:12.480 --> 01:46:14.360
and stuff like that, it might be a little bit difficult

01:46:14.360 --> 01:46:17.400
for editing models to come into feature there.

01:46:17.400 --> 01:46:19.400
What I like about editing is also,

01:46:19.400 --> 01:46:22.160
it's something that we can imagine not only working

01:46:22.160 --> 01:46:25.520
on language model output, but working on a human output

01:46:25.520 --> 01:46:28.760
or a text that's been written with the writing assistant

01:46:28.760 --> 01:46:29.760
and things like that, right?

01:46:29.760 --> 01:46:33.000
You can still go back and do a post-processing editing step

01:46:33.000 --> 01:46:35.560
to polish it up and I think that would be great as well.

01:46:35.560 --> 01:46:41.440
So our last category in the NLP predictions

01:46:41.440 --> 01:46:46.200
is top people's companies, organizations, teams

01:46:46.200 --> 01:46:49.480
to watch in the field 2023.

01:46:49.480 --> 01:46:54.480
Of course, the caveat of you're not any emissions here

01:46:57.200 --> 01:47:00.040
are not just like the work of any particular team,

01:47:00.040 --> 01:47:02.920
but who's got your mind share

01:47:02.920 --> 01:47:07.240
and who are you expecting to see interesting things

01:47:07.240 --> 01:47:10.000
from in the upcoming year?

01:47:10.000 --> 01:47:12.960
Yeah, so this has been a little bit difficult

01:47:12.960 --> 01:47:15.360
question I think every year, but one thing I will say

01:47:15.360 --> 01:47:17.280
and this is maybe the most obvious answer

01:47:17.280 --> 01:47:21.920
is to keep an eye on OpenAI and what they're up to, right?

01:47:21.920 --> 01:47:24.920
I think once they do something,

01:47:24.920 --> 01:47:26.280
people always come back and say,

01:47:26.280 --> 01:47:28.320
look, what they've done is not so exciting,

01:47:28.320 --> 01:47:31.560
oh, they only scale it up or oh, they only did this

01:47:31.560 --> 01:47:32.880
and this new thing.

01:47:32.880 --> 01:47:36.080
But the fact is that they are the first ones to do it,

01:47:36.080 --> 01:47:38.960
that the first ones to bring it out, make it available

01:47:38.960 --> 01:47:41.960
and that is, and get people excited

01:47:41.960 --> 01:47:45.800
about language models in a way that they weren't before,

01:47:45.800 --> 01:47:49.920
so that happened with GPT-2, GPT-3 and JGPD

01:47:49.920 --> 01:47:53.320
and I'm sure GPT-4 will have the same thing.

01:47:53.320 --> 01:47:56.560
I'm sure retroactively we will all talk about

01:47:56.560 --> 01:47:58.440
what the problems with GPT-4 are

01:47:58.440 --> 01:48:02.520
and how it's incrementally only training on more data

01:48:02.520 --> 01:48:05.640
or has more parameters or whatever it is,

01:48:05.640 --> 01:48:07.920
but I think qualitatively it'll bring something

01:48:07.920 --> 01:48:10.680
interesting to the table and I'm really curious

01:48:10.680 --> 01:48:14.480
about what that next interesting thing is going to be.

01:48:15.480 --> 01:48:20.320
Do you think the general predictions

01:48:20.320 --> 01:48:22.240
that are kind of floating around,

01:48:22.240 --> 01:48:27.240
basically spring and 100 trillion parameters?

01:48:30.360 --> 01:48:32.080
Is your money on those?

01:48:32.080 --> 01:48:36.920
I mean, to sort of have a completely different perspective,

01:48:36.920 --> 01:48:39.840
I think this is also a nice model

01:48:39.840 --> 01:48:41.960
that came out, this nice paper that came out

01:48:41.960 --> 01:48:44.800
a little earlier called the Chinchilla paper.

01:48:44.800 --> 01:48:47.000
This was a paper that showed that these models

01:48:47.000 --> 01:48:51.440
are extremely under-trained and they are data hungry.

01:48:51.440 --> 01:48:55.800
So one version of GPT-4 could be potentially

01:48:55.800 --> 01:48:57.480
not even a different architecture,

01:48:57.480 --> 01:49:00.200
not even more parameters, like exactly,

01:49:00.200 --> 01:49:05.200
let's keep it 175 billion and let's just somehow

01:49:05.200 --> 01:49:07.760
get 10 times the data if you can potentially get

01:49:07.760 --> 01:49:09.720
that spot somewhere, right?

01:49:09.720 --> 01:49:10.920
I could totally use it.

01:49:10.920 --> 01:49:14.920
But then everybody that shared the image

01:49:14.920 --> 01:49:17.680
with the little dot and the big dot would be totally wrong.

01:49:17.680 --> 01:49:21.880
Well, yeah, they'll just sort of replace that with data

01:49:21.880 --> 01:49:24.160
and it might still be true, right?

01:49:24.160 --> 01:49:28.880
For those not on Twitter, that is dominated LLM Twitter

01:49:28.880 --> 01:49:30.920
over the past couple of days.

01:49:30.920 --> 01:49:32.160
Is there even,

01:49:32.160 --> 01:49:36.440
I think that when GPT-3 came out,

01:49:36.440 --> 01:49:41.360
the kind of colloquial articulation

01:49:41.360 --> 01:49:44.320
of what they did was like train this language model

01:49:44.320 --> 01:49:45.640
on the entire internet.

01:49:45.640 --> 01:49:49.120
Like, is there 10X more data to train on?

01:49:49.120 --> 01:49:51.440
Yeah, I don't know how much they've trained on

01:49:51.440 --> 01:49:52.440
and how much there is.

01:49:52.440 --> 01:49:55.240
I mean, there's definitely 10X more data.

01:49:55.240 --> 01:49:58.960
There is a lot of stuff on the proprietary, right?

01:49:58.960 --> 01:50:00.080
proprietary.

01:50:00.080 --> 01:50:01.760
Maybe even proprietary, right?

01:50:01.760 --> 01:50:05.800
Like, transcribe a bunch of videos and audio and books

01:50:05.800 --> 01:50:07.120
and I guess, yeah.

01:50:07.120 --> 01:50:08.800
They do have that whisper model

01:50:08.800 --> 01:50:12.840
that that's really good transcribing.

01:50:12.840 --> 01:50:13.880
So they could use that.

01:50:13.880 --> 01:50:15.600
They didn't create that for no reason.

01:50:15.600 --> 01:50:17.000
Right, yeah.

01:50:17.000 --> 01:50:19.800
They also can go into scientific papers

01:50:19.800 --> 01:50:22.320
and like, I don't think the 48 million

01:50:22.320 --> 01:50:24.880
that I'm papers that Galactica was trained on

01:50:26.240 --> 01:50:27.920
was something GPT-3 was trained on.

01:50:27.920 --> 01:50:29.880
And I think that is a pretty valuable resource.

01:50:29.880 --> 01:50:31.800
That Galactica paper also showed that even

01:50:31.800 --> 01:50:33.720
on mathematical reasoning and things like that,

01:50:33.720 --> 01:50:35.640
they were actually better.

01:50:35.640 --> 01:50:37.680
So these scientific papers may be useful

01:50:37.680 --> 01:50:39.880
for a bunch of other things than that we don't realize.

01:50:39.880 --> 01:50:43.080
So yeah, I think where that data comes from is unclear to me,

01:50:43.080 --> 01:50:46.320
but it's clear that more data is somehow

01:50:46.320 --> 01:50:49.240
maybe even more interesting than more parameters.

01:50:49.240 --> 01:50:54.240
And more data could include more RLHF style things, right?

01:50:54.240 --> 01:50:56.200
Like, I don't know what to open it, I guess.

01:50:56.200 --> 01:50:57.200
Okay.

01:50:57.200 --> 01:51:00.720
The other company too, I would say again,

01:51:00.720 --> 01:51:03.680
continue taking a look at is hugging face.

01:51:03.680 --> 01:51:08.320
I've been constantly sort of amazed by how much

01:51:08.320 --> 01:51:09.600
they've been doing.

01:51:09.600 --> 01:51:13.480
One of the sort of key insights is like E-MNB,

01:51:13.480 --> 01:51:15.480
which is the stock conference in NLP

01:51:15.480 --> 01:51:18.480
has this demo track where they highlight sort

01:51:18.480 --> 01:51:22.760
of not research papers but products of demos

01:51:22.760 --> 01:51:24.400
that are relevant for research.

01:51:24.400 --> 01:51:27.040
And for the last three years, I think,

01:51:27.040 --> 01:51:31.840
at E-MNB, hugging face has got the best demo paper award.

01:51:31.840 --> 01:51:36.000
And that's that kind of thing sort of shows

01:51:36.000 --> 01:51:38.040
how they've been doing very different things,

01:51:38.040 --> 01:51:41.320
but also doing things that are impactful and interesting.

01:51:41.320 --> 01:51:43.880
So the two, I want to highlight this year is,

01:51:43.880 --> 01:51:45.600
again, they've done many, many things,

01:51:45.600 --> 01:51:50.080
but the one I want to highlight is the evaluate system

01:51:50.080 --> 01:51:53.680
where they had this whole evaluation framework

01:51:53.680 --> 01:51:56.920
for reproducing evaluations and evaluating models

01:51:56.920 --> 01:51:59.080
and making all of this stuff really easy.

01:51:59.080 --> 01:52:00.800
So you can introduce a new metric,

01:52:00.800 --> 01:52:03.840
evaluated in thousands of models, and so that.

01:52:03.840 --> 01:52:05.760
Make it really easy to compare models,

01:52:05.760 --> 01:52:09.360
make it really easy to reproduce papers.

01:52:09.360 --> 01:52:11.960
And I think that that's a really valuable service

01:52:11.960 --> 01:52:13.960
to do research.

01:52:13.960 --> 01:52:16.800
And the other one that I sort of we also started with this

01:52:16.800 --> 01:52:20.200
of like, hey, what's happening inside the pre-training data?

01:52:20.200 --> 01:52:23.640
One of the tools they have is this roots search tool

01:52:23.640 --> 01:52:26.640
that takes the roots pre-training data,

01:52:26.640 --> 01:52:29.520
but allows you to search it and find all kinds of things

01:52:29.520 --> 01:52:31.440
that are happening inside that pre-training data.

01:52:31.440 --> 01:52:33.600
So if you have a specific prediction,

01:52:33.600 --> 01:52:35.320
then you want to be like, hey, is there anything

01:52:35.320 --> 01:52:38.200
in the training data that looked exactly like this?

01:52:38.200 --> 01:52:41.040
You can do that search and get some results.

01:52:41.040 --> 01:52:45.240
So I think they are just being pretty creative

01:52:45.240 --> 01:52:48.880
and thoughtful about what is useful and building tools.

01:52:48.880 --> 01:52:50.320
And that's been exciting.

01:52:50.320 --> 01:52:53.520
And the last one that I'll bring up, and this is something

01:52:53.520 --> 01:52:56.000
that was on top of my head this week

01:52:56.000 --> 01:52:57.600
with it can change.

01:52:57.600 --> 01:53:00.760
It's a group called Ott.

01:53:00.760 --> 01:53:03.080
It's o-u-g-h-d.

01:53:03.080 --> 01:53:07.760
I believe it's Ott.org, it's a website.

01:53:07.760 --> 01:53:12.360
And this is sort of a research nonprofit.

01:53:12.360 --> 01:53:18.160
And they've been doing sort of interesting things

01:53:18.160 --> 01:53:20.040
related to sort of building tools.

01:53:20.040 --> 01:53:22.840
So they have this tool called Primer.

01:53:22.840 --> 01:53:26.440
And this is going back to decomposed reasoning.

01:53:26.440 --> 01:53:29.280
This tool called Primer, sort of you can give it a question

01:53:29.280 --> 01:53:31.280
and it tries to come up with an answer.

01:53:31.280 --> 01:53:33.600
But in the process of coming up with an answer,

01:53:33.600 --> 01:53:37.840
it can do a web search or it can write a small program

01:53:37.840 --> 01:53:39.280
and it can do all of these things.

01:53:39.280 --> 01:53:41.320
And they're very sort of nice tool

01:53:41.320 --> 01:53:44.280
to be able to visualize what the decompositions are

01:53:44.280 --> 01:53:46.480
and what sort of things are being done.

01:53:46.480 --> 01:53:50.920
So it's a really interesting use case of language models.

01:53:50.920 --> 01:53:55.040
And then they also have another tool called Elisit,

01:53:55.040 --> 01:53:58.280
which is in some sense, it's a little bit like Galactica,

01:53:58.280 --> 01:54:02.880
but it's not so much interested in generating papers for you,

01:54:02.880 --> 01:54:05.840
but helping you do research for your paper.

01:54:05.840 --> 01:54:08.440
So you have a specific question.

01:54:08.440 --> 01:54:11.200
It's going to find a bunch of relevant papers,

01:54:11.200 --> 01:54:13.480
take out snippets from those papers,

01:54:13.480 --> 01:54:16.160
and be able to do that.

01:54:16.160 --> 01:54:19.680
So I don't know, they've had a bunch of tools

01:54:19.680 --> 01:54:22.640
that when I'm looking at decomposed reasoning,

01:54:22.640 --> 01:54:25.040
it comes up and I'm looking at, OK,

01:54:25.040 --> 01:54:27.280
assist, select, assist, it sort of comes up.

01:54:27.280 --> 01:54:29.320
And so it's been interesting to see you

01:54:29.320 --> 01:54:31.120
and I'm curious what those will be next.

01:54:31.120 --> 01:54:33.040
I'm really curious about that.

01:54:33.040 --> 01:54:36.760
And I'm going to look into that in more detail.

01:54:36.760 --> 01:54:39.680
Awesome, awesome.

01:54:39.680 --> 01:54:42.120
Well, I think we are done.

01:54:42.120 --> 01:54:43.160
Like, you've been a champ.

01:54:43.160 --> 01:54:46.440
This has been awesome.

01:54:46.440 --> 01:54:48.600
It's been fun, yeah.

01:54:48.600 --> 01:54:53.040
Yeah, now, I mean, you rose to the occasion

01:54:53.040 --> 01:54:57.920
of kind of capturing an amazing year in NLP, for sure.

01:54:57.920 --> 01:55:01.680
So thanks so much for joining us.

01:55:01.680 --> 01:55:02.800
Yeah, thanks for inviting me.

01:55:02.800 --> 01:55:05.640
I think the time sort of justifies

01:55:05.640 --> 01:55:09.080
how much this year had in NLP this year.

01:55:09.080 --> 01:55:12.040
And I'm really curious to see where NLP is going to go.

01:55:12.040 --> 01:55:15.640
I will mention that ChatGPD came out right before,

01:55:15.640 --> 01:55:17.680
or I think maybe even during Eurips.

01:55:17.680 --> 01:55:19.000
So I attended Eurips.

01:55:19.000 --> 01:55:21.840
And I saw the first-hand experience

01:55:21.840 --> 01:55:24.080
of the whole machine learning community there.

01:55:24.080 --> 01:55:27.080
Then I flew to Abu Dhabi to attend the NLP.

01:55:27.080 --> 01:55:30.960
And that's where I saw the reaction of the whole NLP community.

01:55:30.960 --> 01:55:34.960
And it's been interesting to see sort of how the reactions

01:55:34.960 --> 01:55:38.960
have sort of spanned both optimism and excitement,

01:55:38.960 --> 01:55:41.160
which is kind of where I am, like to see like,

01:55:41.160 --> 01:55:44.160
hey, what can we bear with this stuff?

01:55:44.160 --> 01:55:47.240
To pessimism where they're like, oh,

01:55:47.240 --> 01:55:49.760
it doesn't really, yeah, it's not going to change anything.

01:55:49.760 --> 01:55:52.440
It's just a bigger language model.

01:55:52.440 --> 01:55:54.720
All the way to essentially, I want to say

01:55:54.720 --> 01:55:57.400
some form of denial, where it's like,

01:55:57.400 --> 01:56:01.680
look, it's behind a proprietary closed-off system.

01:56:01.680 --> 01:56:05.280
And therefore, it doesn't matter to research.

01:56:05.280 --> 01:56:08.480
And that's definitely not the day I agree with.

01:56:08.480 --> 01:56:10.480
So yeah, it's been exciting.

01:56:10.480 --> 01:56:13.160
And there's also a fourth, which maybe is less so.

01:56:13.160 --> 01:56:15.920
And I don't know, maybe less so in the research community

01:56:15.920 --> 01:56:20.440
than in the general sphere, which is fear of the implications

01:56:20.440 --> 01:56:23.440
of it.

01:56:23.440 --> 01:56:25.960
Did you find out less so on the research side?

01:56:25.960 --> 01:56:29.480
I guess less so, definitely less so on the, yeah.

01:56:29.480 --> 01:56:32.240
Because I think we've been, there is

01:56:32.240 --> 01:56:35.080
a little bit of fear becoming a little bit more obvious.

01:56:35.080 --> 01:56:37.960
But I think the community, because of a lot of people

01:56:37.960 --> 01:56:39.440
who've been sort of pointing out problems

01:56:39.440 --> 01:56:41.440
in the bit of life, like the models for a while,

01:56:41.440 --> 01:56:45.320
we are kind of, we know what not to.

01:56:45.320 --> 01:56:47.840
As a community, we should know what not to do.

01:56:47.840 --> 01:56:49.760
But it is a little bit scary, where

01:56:49.760 --> 01:56:51.320
people are using it for things that,

01:56:51.320 --> 01:56:53.960
clearly, at the onset, should be like,

01:56:53.960 --> 01:56:55.600
hey, why are you doing this straight?

01:56:55.600 --> 01:56:57.560
Yeah, yeah, yeah.

01:56:57.560 --> 01:56:58.600
Awesome.

01:56:58.600 --> 01:57:01.600
Well, once again, Tamir, thanks so much.

01:57:01.600 --> 01:57:03.400
Really great session and conversation.

01:57:03.400 --> 01:57:06.360
And I appreciate all the work you put into prepping for it.

01:57:06.360 --> 01:57:16.360
Yeah, thank you so much, it's fun.

