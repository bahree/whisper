Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington, a bit about the show you're about to hear.
This show is part of a series that I'm really excited about in part because I've been
working to bring it to you for quite a while now.
The focus of this series is a sampling of the really interesting work being done over
at OpenAI, the independent AI research lab founded by Elon Musk, Sam Altman, and others.
A few quick announcements before we dive into the show.
In a few weeks we'll be holding our last Twimble online meetup of the year.
On Wednesday, December 13th, please join us and bring your thoughts on the top machine
learning and AI stories of 2017 for our discussion segment.
For our main presentation, formal Twimble Talk guest, Bruno Gensalves, we'll be discussing
the paper, understanding deep learning requires rethinking generalization, by Shi Juan
Zhang from MIT and Google Brain and others.
You can find more details and register at twimlai.com slash meetup.
Also, we need to build out our 2018 presentation schedule for the meetup.
So if you'd like to present your own work or your favorite third-party paper, please
reach out to us via email at teamattwimlai.com or ping us on social media and let us know.
If you receive my newsletter, you already know this, but Twimble is growing and we're
looking for an energetic and passionate community manager to help managing grow programs
like the podcast and meetup and some other exciting things we've got in store for 2018.
This is a full-time role that can be done remotely.
If you're interested in learning more, reach out to me for additional details.
I should mention that if you don't already get my newsletter, you are really missing
out and you should visit twimlai.com slash newsletter to sign up.
In this episode, I'm joined by Greg Brockman, OpenAI co-founder and CTO.
Greg and I touch on a bunch of topics in this show.
We start with the founding and goals of OpenAI before diving into a discussion on artificial
general intelligence, what it means to achieve it, and how we go about doing so safely and
without bias.
We also touch on how to massively scale neural networks in their training and the evolution
of computational frameworks for AI.
This conversation is not only informative and nerd alert-worthy, but we cover some very
important topics, so please take it all in, enjoy, and send along your feedback.
A quick note before we jump in, support for this OpenAI series is brought to you by our
friends at NVIDIA, a company which is also a supporter of OpenAI itself.
If you're listening to this podcast, you already know about NVIDIA and all the great things
they're doing to support advancements in AI research and practice.
What you may not know is that the company has a significant presence at the NIPPS conference
going on next week in Long Beach, California, including four accepted papers.
To learn more about the NVIDIA presence at NIPPS, head on over to twimlai.com slash NVIDIA
and be sure to visit them at the conference.
Of course, I'll be at NIPPS as well, and I'd love to meet you if you'll be there, so
please reach out if you will.
And now on to the show.
Alright everyone, I am on the line with Greg Brockman.
Greg is a co-founder and CTO of OpenAI.
Greg, welcome to this week in Machine Learning and AI.
Thank you for having me.
Awesome.
Hey, why don't we get started as is the tradition here on the podcast with you telling us a little
bit about your background and how you got involved in AI?
Sure thing.
So, I got into programming relatively late, so after high school, I took a year off and
went abroad, was working on the chemistry textbook, and I sent it off to one of my friends
who had done something similar in math, and you were back saying there's only one problem
with this, which is that you don't have a PhD, so no one's going to publish it, so you
can either self publish, or you can make a website, try to promote things that way.
I was like, well, I guess I'll figure out how to make a website, and so I went online
and taught myself how to code and build a little sample table sorting widget, and thought
that was cool, and I built something bigger and bigger and never really looked back
at the chemistry.
And one of the really things that really captivated me was the idea of being able to write code
that could understand things that I could not, because the way that you write code that
you build systems, you think hard about a problem, you understand it, you write it down
in this very obscure way that we call a program, and suddenly anyone can get the benefit of
what you just did, and if there was a way to amplify that and to have programs that could
do things that I didn't have to even understand myself, then suddenly the set of problems
you could solve, you're so much broader.
And how'd you get from building a website to that?
Yeah, yeah.
So, I read Turing's 1950 paper, computer machinery and intelligence, and when you read it and
it's talking about the Turing test, and that he has this picture of, by the year 2000, you'll
be able to build this child machine that will learn just like a human child, and he will
get full intelligence.
And this was in 2009 that I was reading this paper, and it's like, where is that machine?
Why is it anyone built it?
Nice.
And so, throughout college, so I ended up doing a bunch of different startups, and ended
up transferring, so I started out Harvard and I was there for a year and a half for going
to MIT.
I was there for a semester and a half before leaving to go work on Stripe, where I was
the CTO for five years, and built that from four people to 250 employees, it's now around
a thousand or so.
And kind of, for me, the goal has always been to work on AI.
It was just a question of when and the right way of doing it.
And while I was working in the startup world, if you read hacker news and you look at what's
what's happening, you see all these articles and deep learning does X, deep learning for
Y, and the thing that was very unclear to me from the outside was substance or hype.
And I'd actually done some similar investigation on Bitcoin, from the outside, Bitcoin similarly
is something where there's a lot of people talking about it as substance or hype and had
really done a deep dive and kind of concluded that this was 2014 vid, that it was kind of
people weren't really focused on the right things, that if people were focused on just
kind of speculation, not about building products, delivering value, you know, still might
be the case that Bitcoin will succeed, but I had a very different observation when
it came to deep learning and AI and what was happening.
And I realized that a lot of my smartest friends from college were now in the field and that
things were starting to work in a very real way and solve tasks that you just couldn't
have solved another way. So the fact that things are actually happening, you can actually
build systems, the kind of real world application and that it's also still very much at the
very beginning of the S curve. For me, it was very clear that the moment is now and I was
talking to a bunch of people in the field, I was talking to Sam Altman and he put together
this dinner with Elon Musk and Ilya Setskiver and some others and the focus of this dinner
was clear that things are happening, things are moving very quickly. How can we best have
a positive impact? How can we help ensure that this plays out in the best possible way?
Because AI is just going to be the most transformative technology that humans ever create and just
having some, you know, any kind of contribution to making that play out better is the most
worthwhile thing that I can imagine. And the conclusion was that it seems like it's not
too late. It's not impossible to build a lab with a lot of the strongest researchers in
the field, especially if you focus it on this goal of this technology. It's not enough
just to build it. You also need to think about how do you make sure it actually benefits
everyone? And so we had that as our hypothesis and you know, I at the time had left striped
a couple months earlier and said, well, I'm going to go full time on trying to make this happen.
So we put together a team and in December of 2015, launch at NIPS announced that we existed
and since then have been working on going from zero to pushing the envelope of what is
possible in this field. Yeah, yeah. And for those who aren't familiar with OpenAI as an
organization, you know, what does it look like today in terms of the number of researchers
and what's the model? Like you see folks that are affiliated with OpenAI that are affiliated
at other places as well? How things played out, you know, since then?
Yeah, so we tend to think of ourselves as cherry picking the best parts of academia and
the best parts of industry towards a very focused goal. And OpenAI at the end of the day
is a serious play to build general intelligence and make sure that it plays out well for society
plays out in a safe way. And to do that, we don't know how to build something like general
intelligence today. So you need to do fundamental research, you need to push the elements of
what's possible, but it's also the case that field has really transitioned from being
an individual sport to being a team sport. Really just this year, and maybe in 2016, you
were able to start using large clusters of machines much more productively in 2012. Google
had the Katner on, which is 16,000 cores, but we're able to be surpassed by two grad
students on the GPU and a 2GPUs. And today, that's a very different story where you have
people training images of 10 minutes on 1024 GPUs. And so when you look at something like
our Dota project, which we'll talk about in a bit, which really require this team of
people with engineering backgrounds, with research backgrounds, all coming together towards
a shared goal. And so the way that we structure ourselves is that we have a few different
teams internally. So we have a robotics team, we have a Dota team, we have a few other
teams. And people, we have the full mix of skill sets that are required to accomplish
a goal within those teams. We also have infrastructure team that is more of a horizontal team that
supports the work of all these different teams and accelerates the work of those teams.
And one thing that we're increasingly seeing is collaborations amongst the teams, which
ends up being a really powerful thing, where we can take code from Dota and use it for
robotics. And it really accelerates what's possible there.
And can you speak a little bit to the, you know, the open aspect of open AI? It's, you
know, clearly something that was important to you in setting out on this journey. But
at the same time, there's been some critique of the level of openness at open AI. And there's
still some things that, you know, I think you're, you know, clearly publishing a lot of research,
but folks have asked, you know, are you publishing, could you be doing more in terms of publishing
data sets and things like that? You know, how do you think of that?
I think that's a great question. And I think that that's one misconception that people
have had since the beginning of open AI is I think people have put kind of a narrative
that's very different from what we're trying to do. And I think it's, it's a really good
thing to ask about. So the goal of open AI is to ensure that the world post general intelligence
is good for humans. And along the way, it's really important as, as a organization that,
that we both, like one thing that we think about a lot is what we can do to both accelerate
our organization, but then also things that we can do to help accelerate the field and
things that we can do to deliver value to the world generally. And the, the last one ends
up playing out on multiple time scales. So there's very short time scale work that you
can do. Like, for example, today we, we published, we released a few more algorithms in our
baselines project where we have, we've done high quality implementations of all the standard
reinforcement learning algorithms. So now, rather than people having to, and basically
end up with a bad baseline, that then they say, oh, my new method is better than this one,
you can just take the work that we've done to, to well tune all of these baselines and
use that. And so that's a short timeline delivering a value. The thing that we really
are trying to do is a much longer timeline, right? It's really about building an organization
that can be at the forefront of this research and to actually be able to steer how it plays
on society. And so to do that, it's not as simple as just take the work that you do in
real time and toss it over the fence or you know, put it up on GitHub. I think it requires
something much more thoughtful. And so I guess a lot of how we think about it is that
opening I will be a success if you fast forward five, 10 years or two, whenever you're, whenever
the, the appropriate checkpoint is, and you look back and you say that the amount of
value that we delivered over the long timeframe was the max that we could have. And some
of that I think is in the short run, it means that you don't necessarily publish or release
code to the maximum extent that we possibly could. That is always made with the choice
of because we think that we're going to be able to better deliver value over the long
run. And specifically, meaning, you know, they're clearly publishing code has a cost
to it, has resources that are associated with it, but even more so, you know, once you
publish it, there's some, you know, rightly or wrongly expectation of maintaining that
over time that also has a cost and all of that, those accumulated costs, you know, potentially
slow the organization down or kind of, you know, require ongoing resources that specifically
the, the thinking or is that just part of it? So that is, that is a real one for sure,
right? And again, like one weird thing about this field is that the technologies that are
being developed are ones that are very desired by big companies, right? Yeah. Look at the
Googles and the Facebooks and therefore, massive amounts of resources in this. And so
they actually have an impact as a nonprofit, which, you know, we're resourced, but it's
very different from the level of resourcing that you would see at one of those companies
that you really have to answer the question, what is it that I'm going to do that is the
differential impact? How can I make the most, get the most bang for my buck, make sure
that sort of the differential impact of this organization existing is as large as possible?
And so that means that having a large open source project that lots of people are using
is very, very valuable, but there are lots of other people who would do the same thing
and where it's sort of, it's much more, you look at TensorFlow, right, that that's something
that's great for Google because lots of people are using their tools, means that when
they hire people that they're using the same platform. And so the incentive exists for
the big companies to do that. And I think the thing that that we view as unique to us is
really thinking about this AGI problem and thinking about how do you make sure that when
it comes to, you know, there's kind of two problems that are really core to AGI. The
first is, well, you're going to be building this really powerful system, right? You should
imagine you basically are going to train like the goal, you know, general intelligence,
like how should you even think about that? What even is it? It's going to be a system,
I think, for the purposes of this conversation, we should define it as a system which can
perform any economically valuable task as well as a human. And so if you can build that,
first of all, to train it is going to require a lot more compute than to actually run it.
You know, let's say that you're going to need a bunch of agents that each one of those
agents is going to run much faster than real time in order to train. And so you should
kind of think of whatever system you're going to build, well, there's going to be this massive
data center. It's just going to be sitting idle while you're running your single AGI. And
so really, you're going to have the ability to run lots of them from day one. And so you
should kind of think of the thing you're going to build as this organization of the most
competent person you've ever met that are all working together in concert towards a shared
goal with no ego. And so, you know, it's going to be a pretty powerful system. And, you
know, today we have lots of, we have lots of companies that are organizations of people
and are able to accomplish pretty, pretty wild things. And I think if you can do build
the kind of system, I just described that it's really hard to see what the limits of
those of that's going to be. And so the first thing you have to ask is, is it going to
do what we want at all? And this is what is referred to as, you know, technical safety
and a problem that that we work on. So one one step towards the solving the technical
safety side that we've done is this human future feedback project and collaboration
of the mind. And the idea is that the core problem on technical safety is that you need
to be able to specify goals. The AI somehow needs to listen to you. It needs to reflect
human values. It needs to, you know, sort of do what humans want in some pretty deep way.
So there needs to be humans in the training process somewhere. And the human feedback project
that we worked on is this first step in this direction where a human labeler is shown
two videos of a behavior. And they just click on which behavior is more like the one
that they want. And we're able to show that with 500 bits of feedback that you're able
to train an AI to view some back flips. So that's step one. This kind of work is something
we think is really important. And it's a little bit taboo to talk about the field, right?
The idea that, yeah, you could build these systems. They're going to do anything crazy.
And I totally understand why, right? I think that there's several motivations for that.
One is that the field has gone through these these hype cycles of booms and busts and
winters and that to really think through the, well, what if this all succeeds? What
if it works is something where if you're just going to go through another cycle of that,
then there's really no point. And there's a second thing, which is that I think people
really reason from what the computers that they can do, right? You take your Pascal GPU
and sure you can train a great image classifier, but are you going to be able to train anything
better than that? And the answer is, well, not really. But there's really two things
that are changing that I think people don't see that are going to really accelerate the
kinds of models that we can run. The first one of these I alluded to earlier, which
is the fact that you can now use data center scale in order to get better performance.
And again, this is 2012, basically two GPUs. You get severely diminishing returns beyond
that. That's say, to the art 2014, you could do a GPU training and that seemed great.
And just this year, Facebook did 256 GPUs, ImageNet one hour, and someone else just did 10,
24 GPUs, ImageNet 15 minutes. And so you get this massive scaling of the kinds of models
that we can run, the kinds of systems we can train. The second one, which is the acceleration
of the neural net hardware. And if you look at everything up to 2016, it's actually very
smoothly Moore's Law, even though it's not driven by the same factors as Moore's Law,
which is pretty remarkable. So if you look, you can actually look in, there's this diagram
from Ray Kurzweil's book where the data cuts off at 1998. And he just fits a double exponential
to it and predicts exactly, basically, exactly correctly, the 2016 Pascal GPU is right on
the curve that's predicted from this data just cutting off at 1998. And so very smooth
Moore's Law all the way through. But this year's something weird happened. This year,
we ended up with an order of magnitude increase in the number of flaps available for running
neural nets. You know, these numbers are all public. So there's the Volta up at my head.
So 2016 Pascal GPU is like 20 Teraflops. The Volta came out this year is more like 90
Teraflops. And Google TPU 2.0 is more like 180 Teraflops. And so there's really this
explosion this year. And the thing is, we really expect this kind of acceleration of the
compute available for neural nets in a small compact package to continue to accelerate
much faster than Moore's Law. And it's a very simple reason. The reason is that you
can, the neural networks are very, very parallel. And just like the brain is this big network
of, you know, just a bunch of tiny little cores, they're all talking to each other. And
there's some learning rule and there's some propagation rule. And the way that we've
always designed hardware is much more for serial execution. No one's really had this incentive
to design this massively parallel hardware before. And so there's a lot of low hanging
free. You don't need to invent any novel technology. You can just use, you don't have to rely
on transistors getting smaller in order to get these speedups. And so if you combine these
two things, data center scale with faster neural network accelerators, suddenly the compute
available for any of our models is going to really skyrocket. And so that's kind of
the perspective that we have is that things are going to be different as a result of the
hardware coming online. Timelines are always tricky to predict exactly. But I think that
we're going to see even just next year, I think if you just look at what we could do
a year ago and what you can look at right now with respect to image generation with respect
to voice generation, I think in 2018, as long as we continue to see the compute coming
online and the way that we expect that we should be able to have perfect video generation,
we should be able to have basically perfect speech synthesis as well. And this kind of
this kind of acceleration in terms of the capabilities is going to be really tied to, well, we have
all these ideas, these models, but we need the compute in order to run them. And as long
as we get that compute that I expect to continue to see the capabilities increase and lock
stuff, that's thing number one that's really important. Thing number two that is extremely
important is the question of, okay, so let's say you build an AGI, it does what humans want
to reflect humans values. So who's values, right? And who are the people who get to specify
what this AI should want? And that's a much harder problem, right? The first one is a technical
problem. That sounds like a thing where, you know, if it, you know, as these systems
play out, we're very good at solving technical problems. If you can actually build this kind
of very powerful system, like there's good reason to believe that we put in the effort
you should also be able to make it safe and solve that technical side. It's not easy,
but you have to really want it. You have to really try to solve that problem, but it seems
like a thing that is solvable. The thing that's much harder is this non-technical problem
of who owns it and who specifies the goals. And that that is something that is also very
core to open AI and how we think about the value that we're delivering. And that we really
want this technology to be something that is not just benefiting one corporation, one person,
even one small subset of people. We really want this to be something that is benefiting
the world. And we have ideas around the right way for that to play out. But I guess when
it comes to how do we think about open, that's exactly how we think is solving those two
problems. If we can do that, then that is the most important thing any of us could imagine
doing. On that latter point in terms of whose values are those things that you have ideas
about but haven't turned into projects that are you doing? Are there public projects that
you've been working on that speak to that second item?
Yeah, so it's something we spend a lot of time thinking about. I think that we haven't
yet done public speaking about our thoughts there, but a lot of what we've been spending
time doing has been building relationships with a lot of people in the field, a lot of
people in governments, a lot of people just in various positions who I think will end
up being influential with respect to how this technology plays out. And this kind of,
I think it really is trying to lay the groundwork for where we hope things to go.
One of the themes that kept coming up as you were speaking was this notion of like a timeline
or time frame for AGI. Do you have one that you kind of manage to or is there general
agreement within open AI and the community as to, you know, when we think AGI is going
to happen or even the time scale and maybe has some context for this. I don't think I've
told this story on the podcast before maybe I have, but relatively recently I was with
some fellow entrepreneurs talking about we're just kind of catching up and someone pushed
me on, hey, so, you know, on this AI safety issue, they didn't use those words, but like
are you a Mark Zuckerberg or are you an Elon Musk? You know, and I tend to answer that
question like, well, you know, it's kind of in the middle, I think, you know, there's
a lot of sensationalism, but he kept pressing, pressing, pressing for me to answer. And
one of the things that occurred for me in thinking about this was that, you know, if I think
about who Elon Musk is, his time frame is probably way longer than mine, right? You know,
the guys like building rocket ships, he's thinking long term, right? And I tend to answer
that question in terms of, you know, I think people really over, over blow what, you know,
is likely to happen in 10 years, right? And so I wonder, you know, with that as context,
like how do you think about the world, you Greg and, and open AI more generally in terms
of the time frame for worrying about and thinking about these kinds of issues?
Yep. Timeline is a really interesting and hard question. It is the hardest question.
I think this is true for any technology. If you look at the invention of flight, people,
all the experts in the field right up until flight was created, we're saying flight is
for the birds, the Newton had proved that everything air flight would never happen. And then
you have the right brothers during their flight just a few months later. If you look at
kind of any transformative technology, it is really the case that it's hard to distinguish
exactly when it will happen. I think that there's something very inherent to this because
if people knew, oh, okay, here's a timeline to it, then you would just focus more, work
harder, and accelerate that timeline. And so they would turn out to be inaccurate. And
I think this, yeah, the way that we really think about it. So I think, I think Elias Dukowski
had a good blog post where he talked about something that he did and he was listening
to a bunch of AI experts saying, AGI is very, very far away. And he went up and he asked
people, okay, tell me, what is the least impressive accomplishment that you're very confident
is not going to happen in the next two years. And people really didn't have a good answer.
And how can it be that you both have thought very, very deeply about, okay, like it's going
to take this long, it's going to take exactly this long. Here's where we're going to deliver
it. And also don't have a, okay, here's something that I'm willing to bet. This is the least
impressive thing that just we're not going to do this timeline. And I think what's really
going on, I agree with the conclusion that he has there, is that people don't really have
a good concept of it, right? People don't really have that in general, people end up picking
with their gut rather than through having like really rational, here's exactly the factors
that are going to enable it. And here's why we're not going to be able to do it in the
near term. Besides the fact that, well, I look at what my, you know, I look at my, my
dumb AI agent where I'm trying to get this thing to even be able to tell a cat from a dog
and can you imagine trying to build something as smart as me, like, you know, it's just
this disconnect. And so the way that we think about it is that we certainly know that
some things are going to be changing. And I think that specifically the hardware is
going to change in a way that people are not expecting right now. And it is faster
than more so is not something that people price into there. They're internal sense of
what's happening. And there's a question of how far does that take you on what timeline
does it take you there? And the thing that's important to us as an organization is that
regardless of what the timeline ends up being, that we are able to have the influence that
we want, that we're able to ensure that this thing plays ends up playing out well. And
the second part to that is that, well, you can also say, so you don't know when the super
transformative stuff is going to happen. But you can say something about what is going
to happen in the near term, what is going to happen over the next five years. And again,
it's very clear we're going to be able to do synthesis of perfect videos. You know,
you look at what's the 2020 presidential campaign going to look like when you're able
to generate the kinds of videos that we already see we can, we're very, very close to being
able to do. There are a bunch of technologies. Robotics is a perfect example where today
there's been some, you know, there's been results of learning on robots. But that none of
the roboticists are impressed because all of the tasks that people can accomplish are
worse than what the roboticists can already do. And so if you talk to roboticists, they
say, okay, you know, come back calmly, we need to do something I couldn't do in the 70s.
And that's going to change. And the moment that you change that, the moment that you have
your first learning based result that blows away what was possible without learning, I think
there will be a sea change. Like we've seen this in the number of other disciplines we
saw it with vision pre 2012. You go to the big vision conference and there's like one
neural net paper for lucky. And now there's like, basically everything is neural net's
vision. Like I don't think anyone even remembers that it was, it was different. And I think
that on robotics that it's pretty clear that, you know, humans aren't any smarter. We're
not getting any better at, I think, through these problems and being able to program all
of the rules for exactly whatever about you do and how to react. And I would say that
fact that you're going to have learning come in and really change what is capable, what
robots are capable of. I think it's going to be massively impactful. And so that's how
we think about it is that the big goal of AGI is something where you can't know, you certainly
can't know that it's close, but I also don't know that you can know that it's super far.
And that we also know that there's going to be transformative applications in the
mere term. And so for us, the mandate for us, the way that we operate is stay on the
cutting edge, make sure that we're pushing forward and always be asking, how can we ensure
that our integral over time of value delivered is as large as possible?
That was a non, I'm not giving a timeline answer. Very, very well stated. But so you
did give two examples of applications where you think we'll see transformative short-term
things happening. One is audio and video generation and the other is robotics. Are there specific
examples of kind of leading indicators or examples that are leading indicators that kind
of give you the confidence that those two specific things, for example, will change pretty
dramatically, pretty quickly?
Yeah, so I guess on the robotics front, so we worked on robotics. And this is our goal
is to be able to change and to really unlock robotics through learning methods. And
it's actually interesting because the way that we think about it, the way that we work
on robotics is that we are geared towards trying to build general technologies rather than
trying to maximize robotic capabilities and that that steers the set of projects that
we're going to work on at once or are going to pick. But I think one thing that we're
really excited about is if we succeed, we stay focused on AGI, but can also enable
robotics to really kick off.
What's an example of those two things in opposition to one another?
One perfect example is that I think that there are so many really positive applications
that we're going to see in robotics over upcoming years. Like, elderly care robots are
perfect example, right? That's something that I think is going to deliver value to a lot
of people. It's going to really be transformative to a number of people's lives, but it's also
not necessarily something that we're going to work on ourselves. The kind of thing that
we want to do is to build the underlying technology that will allow that application to happen,
but stay focused on pushing forward on new applications rather than productizing.
Got it. So is that different than where basic research as opposed to applied research
shows that is there another nuance to that?
Yeah, so I'd say that we're halfway in between because when I think basic research,
and I guess it might depend per field, but when I hear basic research, I think of the
individual sport type research, right, of people kind of off on their own, thinking
deep thoughts and coming back when they have something that seems cool.
And that for us, that we really try to take results and push them to the limits of scale.
And with our Dota system, that's exactly what we did where rather than just show that,
hey, here's some system that can kind of work on, you know, in some toy way, actually
work on a really hard task. And that I think the distinguishes it from applied research
is that we focus, so solving Dota is clearly not going to be something that is going to
be transformative for many people's lives and transform it to a subset of people, but
not in the kind of direct impact that one would have in a more applied setting.
Yeah, one of the things that I saw recently that, you know, is it a bit of an example of
what you're suggesting will happen to videos. The Nvidia recently published some work
using GANs to generate these synthetic celebrity faces. Did you see that one?
Yeah, absolutely. That was incredible. That was incredible. Yeah, I was going to bring
that up as another leading indicator of this kind of thing.
Yeah, and so and we've already seen like, there's some other research. I forget if it was related
to GANs or another approach where you're able to, you know, given kind of static photographs,
you're able to kind of create three-dimensional and, you know, change the expression on the
photographs. All of these, like the pieces are all in place, or near in place to create these
perfect synthetic videos, although the full end-to-end thing isn't quite there yet.
Yeah, and you know, you need to get the full end-to-end thing in place.
What's that? Compute. And that's it. Is that your, it's just compute?
For that particular problem, I think that our ideas are really proving out,
and if we were able to run at larger scale, that we'd have a really good time,
and I think that it is, I think, really important to also drill into this story around compute,
because it's not as simple as just you take the code that someone already wrote,
and you just run it on more GPUs, and it's magically going to solve the problem.
But it's much more that it's like compute in this field is just like particle accelerators in physics.
If you don't have the particle accelerator, you're just not going to discover the secrets of the
universe, right? You're not going to have the breakthrough. If you have the particle accelerator,
it's not just that you just, you know, somehow like the physicist is not useful, and just,
you know, just translating ideas into experiments. It's that you now have this tool that
fundamentally allows you to achieve the result that you were looking for, and that's really where
we are on video generation, is that we have ideas that are clearly like in the right space,
and maybe we need some additional tricks, maybe we need to do some additional tuning,
but if we're able to run at much larger scale than we are right now, then we can actually try
out these ideas that we have. And I think that the converse is also true, that is that if,
for whatever reason, we were to freeze the level of compute that is available for run these models,
that progress would really slow down. Yeah, your earlier point about you kind of hinted at this
a couple of times in the conversation, but you know, I think one of the things that contributes
to our ability to, you know, predict, well, a couple of things, I think, contribute to our ability
to, or the difficulty we have predicting when AGI happens is, I don't know that we've like
clearly, maybe I should phrase this as a question, like how well defined do you think it even means
to have achieved AGI? Like, is it absolute, or is there like a minimum viable AGI product
that would suffice? Yeah, I think this is a good question. I kind of think of whatever I think of
the question of how do you define AGI? What does AGI? What will AGI look like? I always think
a little bit of, are you, have you heard of bike shedding as a term? Yep, absolutely. Yeah, so it
always, you should explain it though. You should explain bike shedding. So the idea behind bike
shedding is, so let's say that you're designing a nuclear reactor. So what you'll do is you'll
bring in these, you know, experts and kind of, the experts will tell you things and honestly, like,
if they tell you, like, you got to do it this way and like, this is really important,
you'll probably trust them and say, okay, you do it, like, you've got a lot of experience in this,
this is great, like, go off and run with it. So when it comes to, okay, we're also going to have
this bike shed outside and what color should it be? Everyone's going to have an opinion, right?
Everyone feels like they are an equally qualified expert to talk about bike shed colors. And I think
that with intelligence, there's something similar here where we all have our own conception of
intelligence, what it's like, what's hard, what it is that we do, what's going on in our heads.
And so I think that the question of, okay, well, this system that you've built does this,
but it doesn't do that. What is AGI going to look like? How hard is it? When is it going to arrive?
I think these things end up being approached, kind of like the bike shed where everyone has their
daily experience and kind of fit that to, you know, I think in one thing that is true is that
no one is truly an expert in AGI, right? We haven't built it yet. And so anyone who is claiming that
I've got this special knowledge, it's a little hard to take that at face value, right? You can't
go to university and say like, well, I got my AGI undergraduate degree. And built five of them
in the course of getting it. That's right. That's right. That's right. And so I think that, you know,
the bike shedding term is like, I think kind of a negative one usually, but I view it in this
almost positive way where we have such like intelligence is just so fundamental to us and who we are
in this notion of what it even means to be human, that everyone has thought about this, you know,
thousands of years ago people were speculating about what it would be to build a mind and what goes
on inside of our own heads. And so I think it's actually kind of this marvelous thing that people
care so much, but the flip side ends up being that you almost have this philosophical debate that
becomes very irreducible. And so the way that I think about it is that to the extent we're just
going to try to resolve philosophical questions that have been standing for 2,000 years,
we are probably out of luck except to the extent that our technical progress informs us. And so,
for example, we now have a much better sense of what it's going to be to build a mind than Aristotle
would have. So it's not going to be some big rule-based system. It's not going to be most of the
things that you might have expected. It's going to be a big statistical system. It's going to run
this massive parallel fashion on a bunch of cores and, you know, you can kind of describe things
like that. Is it going to be majors multiplies and taking gradients? Well, that's a different question,
right? And we certainly have not resolved that yet. But I think that the question then of,
okay, so what is an AGI? A lot of how I like to frame that conversation is to kind of sidestep
the deep philosophical questions of do you need to have something that's conscious? Do you need to
have something that kind of fulfills other notions of intelligence and really just focus on
what can it do? Can you build a system that is able to accomplish any economically valuable
tasks that you put in front of it? I think that is something where you can tell, right? I think
that you can tell if another way of reason about this is if you took a human and you wanted to
figure out is this person, again, real intelligence? Is that something that you think you could test?
And this is, you know, we certainly spend a lot of time trying to assess various people's
skills and capabilities on various different axes and, you know, you can almost think of it as
deciding if you built an AGI as giving it a bunch of different job interviews and seeing if you
want to hire it. And I think that there's that this kind of framing of there are deep philosophical
questions. But at the end of the day, you can think about it instead in terms of
very functional, what is the system capable of? And the latter is something we're able to do,
the former is something that is fundamentally very hard. I also think that this framing really
raises a second point, which is, well, is this, you know, it's a very utilitarian kind of view
of the kind of system that we're talking about, the kind of things that we might might want to build.
And why should we want to build something like that at all? You know, if like the really
opens this Pandora's box of what does it mean to be human and what is the value of humans,
how do we make sure that humans have have meaning and really a place in the resulting world?
And that is, I think, the hardest problem. And that is something that is, you know, something
that's very core to open AI and how we think about this technology is that it's pretty clear
that, like I think, indisputable, that in a short term, that companies are pouring in tons and
tons of resources in order to make advances in AI, which is different from AGI. I think that the
amount of resources going to that is smaller, is more focused. But I think that as it feels closer
to people, as people feel that while I look at all this progress in AI, it feels like, you know,
kind of my internal neural net is telling me that this could actually happen. And then you start
thinking through the economic value that would be delivered by that system and how important it
could be for the next company or why company, I think that that will change. And then I think that
the question is not so much about accelerating the timeline to AI, but it's really about ensuring
that this technology plays out in a way that isn't just one company gets on the spoils. But
it's really about humanity is ultimately the winner. Right. You know, it may turn out, we may
get thrown a curveball here and it may turn out that the technologies and techniques that allow
us to create AGI are totally orthogonal to the ones that, you know, we've created in the process
of trying to create AI. But, you know, from where we sit now, it certainly seems like, you know,
to the point of all the pieces that we discussed that go into creating these videos, like they're all
kind of right in line with the kinds of problems we would expect to have to solve in order to get to
an AGI. So all of that huge investment that is, you know, profit driven, if we can say, you know,
on the part of the many of the companies, most of the companies that are investing in those technologies
are, you know, maybe accidentally pushed us closer to this AGI. Yep. Yeah. And it's actually
pretty interesting that there's this classic mantra in the field that as soon as you're able to do
it, it isn't AI anymore. And people said this about chess. Right. The chess is the most important
thing. And that only, you know, some people are solving the thing. Exactly. And it turned out,
all of that stuff, once it happened, people are like, ah, that's not AI. I think that this is dead.
I think that this way of people reacting to things we're able to do is now different. And you will
get, AlphaGo, you will get Dota. And for these systems, there really is something going on in them
that is very akin to intuition is much deeper than simply performing some big search. And being
very dumb and making up for that dumbness with just having your massive brain. And you think about
the image generation that came from NVIDIA. And that's something where humans can't even sit down
and start to think about how you could write the rules for it. And so I think this is a very
encouraging thing. And I think that there is a, there's, there's kind of this, this piece to it,
which is what's really going on right now is that if you look at the problem of trying to recognize
a cat or dog in an image, trying to recognize objects and images, that the space of image is
the super complicated, very high dimensional space. This is a high dimensional manifold. It's,
there's this fundamental complexity in that domain. And so for the human to write down all the
rules for that would be a pretty massive undertaking. And so what we've built is that we have these
systems which are able to absorb the complexity of the domain and able to kind of figure themselves
around and that you've got this neural network that's got these millions of parameters. And that's
just not something that exists in the natural world. It's not something that we're used to.
And it's able to reconfigure itself and to, to really absorb all of the inherent,
that inherent complexity. And I think that the, a building to do that is what really distinguishes
this learning revolution from AI previously. And now it might turn out that there are limits to
what we can do with our learning algorithm. But it's also kind of crazy that the learning algorithm
we use backpropagation is developed in 1986. How can it be that this algorithm and really
neural nets and some ways date back to even maybe the 60s, maybe the 40s depending on how you count
that these very simple, very obvious ideas that you couldn't run on your, you know, your particle
accelerators if you, if you will, you didn't have the parts of accelerators to run the experiments.
But these simple ideas turned out to be so powerful. And I think there's something really
fundamental there that I can't decide between two different explanations. One is that intelligence
is fundamentally simple that there's a, you know, I can, I can kind of back explain some explanation
of well, if you had something that was complicated, then it would have a very large prior. And so
you're kind of making this prior. And so yeah, you shouldn't expect it to be very general. The more
depressing version of this is that well, maybe we're just really bad at making anything complicated work.
But if it's the first, and I think there's a lot of evidence that really indicates that it is the
first, then I think that's very encouraging that the simple ideas, if you implement them correctly,
the mathematics, if the mathematics works, right, if the math kind of points in the right direction,
if you implement it correctly, you scale it up massively, then you're going to be able to get
things that, these will happen that you weren't expecting. And one thing that's really weird to
me about the kind of progress that I see is that I've seen on repeated occasions algorithms that
work better at large scale than their designers expected, right? And so we've seen this with algorithms
here where talk to the person who invented it and they say, oh, no, that's not going to work for
XYZ reason. And then we scaled up really large and actually works really well. And I think that this
is going to, this is, again, for me as an engineer is totally contrary to experience. For me as an
engineer, it's, you really only get, if you're lucky, the kind of performance that the person was
intending and as soon as your 10x scale, 100x scale, good luck. Everything starts to break, right?
Totally. Totally broken. Is there more to that than just more data and more data, you know, fixing
more problems in terms of, or more data basically covering for our lack of sophistication in the
algorithms themselves? Yeah, so it's something like that. Though I would phrase it a little
differently, which is that like I think that the algorithms that we have are fundamentally capable
of absorbing all the compute and data you can throw at them. And the data question is also an
interesting one because the thing that people are used to is supervised learning where you have this
big static data set that encapsulates your world knowledge. But where things are really shifting
is towards more of the reinforcement learning paradigm. And if you think about it, that's where you
want to be, right? You want to have an environment that you're interacting with that you're able to
change. You have this dynamic feedback loop going on. And there you suddenly have upgraded your
environment. Like you can think of your big set of images as just a static environment. And now
you've upgraded to this very dynamic world. And there suddenly you're, you sort of are able to get
infinite data, or at least you can, you can spend a lot of compute to get a lot of data.
If it's a video game like Dota, you can run this on many, many cores. If it's a robotic simulator,
you again can spend a bunch of compute there. If it's a real world, you're in for a little bit
of a harder time. And so maybe you do something like Google did with having a big arm farm.
Maybe you do something else. And I think we really want to end up is that we want to end up
in a place where the limiting factor is the amount of compute that we can throw at these models.
And where we can have a massive generative models that have absorbed a lot of world knowledge
that you're able to do things inside of that. And we can't run those models yet today. We're at the
very, you know, sort of, we're at the very nascent edge of what I expect. We're going to be able to
do with generative models and with this kind of approach. Model-based RL is kind of the
the term, the term of art that a lot of people use. But I think that in upcoming years we will be
able to see lots of progress based on these ideas of scale up, use algorithms that can absorb all
the compute and that that can make up for lack of data, that can make up for lack of everything else.
And what specifically does model-based RL refer to relative to just RL?
Yeah. So the idea with model-based RL is that if you have a, it may be learned or maybe not
learned in some way, model of the environment that you query and you can kind of explore with
them. It's kind of like you as a human if you, you know, picture your house and picture walking
around your house and you can kind of plan things out. You can see like, oh, if I, you know,
do this, this thing will happen and then you don't actually have to go and spend the very expensive
time of walking through your house. And that kind of thing, you can see it's very powerful to have
this, this ability to plan and explore an imagination rather than the real environment. But again,
it's all very nascent. It doesn't really work right now. And I think that it really cannot work
until we have the faster computers online. One of the things you said at the very beginning of the
interviews kind of stuck with me is, is interesting. And that is this idea that ultimately to train
an AGI, it's going to require massive amounts of compute. But then once we train it, like the actual,
you know, inference, letting that AGI, you know, be generally intelligent is going to require
much less compute. And, you know, it strikes me that there are some interesting questions there.
Like, what do we do with all that compute? You know, you address some of it in terms of,
well, you kind of phrase it as, you know, maybe the thing that we are doing is we are running
multiple instances of this AGI thing in parallel, right? So we're taking advantage of all that
compute that we had to create to train it by, you know, running a bunch of these things in parallel.
But it also kind of makes me wonder if maybe the AGI doesn't need to be all that general,
if we're, you know, ultimately segmenting, you know, the problem space up in the end anyway.
Does that question make sense? What is that? Do you see where I'm going with that?
Not entirely. I guess there are two questions here. I guess one, you know, are there other
implications of of this idea that you propose that, you know, we're going to have, we're going to
have to build up this massive compute capability to train the AGI. And then, you know, once we've
trained it, we need that compute capability less. Like, what are all the implications of that?
That's one question. And then question number two is, you know, if ultimately, you know,
what we end up doing is running a bunch of parallel intelligences, you know, do they all need
to be general anyway? Can we have a bunch of a cluster of intelligences that, you know,
are really good at thing X, a cluster of intelligence that are good at thing Y, you know, scale that out.
And that is what we, that is what ultimately we start to think of as general intelligence.
We just have a bunch of these less general intelligences.
Yeah, it makes a lot of sense. So I said on the first one, well, so one, one thing that I think
is worth thinking about is when you actually build a computer system that is autonomously generating
huge amounts of revenue or value, there suddenly becomes this big incentive to make more such
computer systems. Like today, if you have a big pile of money, you want to turn it into more money,
well, you start a company or you invest in a company and you hire a bunch of people and those
people produce economic value towards some goal and that it kind of continues the cycle. Whereas
if you have a computer that is just as good as a human worker, well, then you have a big pile of
money, you should build a big data center and there's going to be this big incentive to kind of
dot the world with with data center. So I think that's one perspective on what happens on a
computer front. I think it is possibly the case that you can take your big training data center
and use all of that compute to run a single AI much faster. And so rather than imagine if you
had a Einstein in Silicon that you're now able to run a thousand X real time or a million X real time,
I mean, pretty good, right? You know, this person sitting around thinking about physics and
thinking about you got someone in there thinking about medicine and how to cure diseases, you got
someone in thinking about how should we build rockets to go to the stars and all sorts of things
like that. Like that would be a pretty valuable, pretty good. It's not guaranteed that we'll be
able to use all of that compute usefully in a single AI, but I think that at the very least being
able to run parallel copies of these of these AI is something that we should expect. And then there's
a question of, well, what would that be good for? And I guess when I think about these, I always
try to make analogies to things that are in our experience today. And so in our experience today,
why do we ever want to have a group of more than one human doing something? You know, it's like
building companies and the tasks are hard and that you have different people that specialize in
different skills and all of those things are things that we should expect would transfer to the
systems of the future. And so I think it'll be very valuable. By the way, so the idea of
a computer system that autonomously produces value where all the interesting stuff is done by
the computers and the humans just kind of stick around and clean out the fans is something that
exists today. Sounds pretty dystopic, but if you look at Bitcoin mines, that is exactly what they
are. And there's a good article recently with a bunch of pictures from Chinese Bitcoin mines,
which I recommend looking at if you want to think about kind of the more cyberpunk, this topic
version of this stuff. And so again, there are a lot of hazards here with the technology that we're
talking about building. And again, the weirdest thing for me is the fact that it's so that people
don't talk about this in a serious way and that I think that the, for most technologies, when
you're building them, you think about what happens if we really succeed. And I think that for
partially historic reasons, partially for this, this reason that we all feel our own sense of
how far off the AGI is and how hard it's going to be and how impossible this team and imagine
building it that really seriously think me through what happens if it works is something that
is a bit to do. So that's thing one. And then question two, can you remember your question two was?
I think question two was, you know, ultimately, do we need AGI at all if the deployment model,
if you will, ends up being to, or the scalability model ends up being to segment our workload into
a bunch of separate things. You know, does a collection of, you know, more specialized intelligences,
you know, become the thing that we initially come to see as a general intelligence?
Yeah. So I think that's an open question or that's a possibility. The way that I think about it
is, I guess, again, back to the idea of we have organizations of humans that can accomplish goals
that humans individually cannot. And so it might well be that even though you want to put specialization,
I would certainly expect that you'll end up with specialization towards specific tasks. I think
that I would expect that a general AI would also have these very hyper-trained narrow AI modules
within it. And you absolutely should do that. And you know, like one thing I think is kind of
interesting about today's AI systems is if you look at something like the Neural Turing machine,
you know, you basically spend, it was a big model, you spent a lot of compute, a lot of data,
a lot of training time in order to learn how to do, how to do various tasks. For example,
one of the tasks from the original paper is to learn to sort. And pretty cool, right? This system
learns how to sort in, you know, it's kind of learned this program. But when you really think about it,
it's like, I could do the same thing at Python on my core in like two seconds. And so at the end
of the day, if you have a specific task you're trying to solve, you can hyper-optimize for that
and do a lot better from a phase efficiency standpoint than this very general thing. I think
something similar happens with humans where we have, like when you have to sit and think about
something when you're not a master of it, and you're trying to really reason how it works
versus when you've practiced a bunch and it's in your muscle memory, right? It's kind of like,
this has gone to the much more efficient hot path. And I think that we'll certainly see analogs
to this sort of behavior. Fair enough. We're at the top of the hour, we're beyond the top of the
hour actually. And we haven't touched on the thing that I expected us to spend a bunch of time
on, which is the Dota 2 project, but we covered a lot of really interesting ground in terms of
AGI and what that means and what we should be thinking about. You know what I'm thinking we should
do is maybe, you know, call this a part one and find some time to get together again to do part
two where we dive into the work that you've done on Dota. Sounds good. All right. Perfect.
Now this was a lot of fun. Really appreciate it. Yeah, same here. Greg, thank you so much.
All right, everyone. That's our show for today. Thanks so much for listening and for your continued
feedback and support. For more information on Greg or any of the topics covered in this episode,
head on over to twimlai.com slash talk slash 74. To follow along with our open AI series,
visit twimlai.com slash open AI. Of course, you can send along your feedback or questions via
Twitter to add Twimlai or at Sam Charrington or leave a comment right on the show notes page.
Thanks once again to Nvidia for their support of this series. To learn more about what they're
doing at nips, visit twimlai.com slash Nvidia. And of course, thanks once again to you for listening
and catch you next time.
