Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Tom Sumowski, data scientist at Urban, the parent company of Urban
Outfitters, Anthropology and other consumer fashion brands.
Tom and I caught up recently to discuss his project Exploring Custom Vision Services
for Automated Fashion Product Attribution.
We started our discussion with the definition of the product attribution problem in retail
and fashion and the challenges it offers to data scientists.
We then looked at the approach Tom and his team took to building custom attribution models
and the results of their evaluation of various custom vision APIs for this purpose, with
a focus on the Roblox and Lessons he and his team encountered along the way.
Today's show is sponsored by our friends at Pegasystems.
During a world, the company's annual digital transformation conference, which will be held
at the MGM Grand in Las Vegas from June 2nd to 5th, is just a couple of months away now.
I'll be attending the event as I did last year and will once again be presenting.
In addition to hearing from me, the event is a great opportunity for you to learn how AI
is being applied to the customer experience at real Pegasystems.
As a Twimble listener, you can use promo code Twimble19 for $200 off of your registration.
Again, that code is Twimble19.
It's as easy as that.
Hope to see you there.
And now on to the show.
All right, everyone.
I am on the line with Tom Sumowski.
Tom is a data scientist with Urban, the parent company of Urban Outfitters, Anthropology
and other fashion brands.
Tom, welcome to this week in Machine Learning and AI.
Thanks for having me, Sam.
So Tom was recently part of the team that worked on some work around product attribution.
And just yesterday, their blog post about this project went live following them, open
sourcing some code.
And we'll talk all about that.
But before we do, Tom, why don't you tell us a little bit about your background and
how you got started working in data science and machine learning?
Sure.
By academic background, I have a bachelor's and master's in electrical engineering.
From there, I went to Lockheed Martin, which is a defense contractor, working in their
corporate research lab for many years.
And in there, the focus was primarily on wireless systems signal processing.
So I started off primarily with embedded systems or embedded platform development for the
area of electronic warfare.
So that involves anything to do with telecommunication.
So these days, everything sort of talks to each other, phones, radios, you name it.
Our goal was to sort of understand that communication spectrum and inferred activities that are
going on.
So started off in kind of building signal geolocation applications, kind of working at a very low level.
Later on, the shift moved towards a theme of cognitive in the 2012 timeframe for advanced
technologies in that space.
So what they sort of defense meant in terms of cognitive was adding intelligence to those
applications that typically were very sort of rule based and driven based on strict policies.
So in other words, they were looking to apply machine learning sort of in the earlier era
of that space.
So this is sort of pre-deep learning.
So on the job, I more or less learned how to apply machine learning for mainly pattern
recognition of different signal patterns and making decisions on how you should act based
on that information.
So for that, it was a lot of experimentation, a lot of learning with online courses like
Coursera.
And since we are a research lab, it was building essentially prototypes for proof of concepts
to the military.
And the goal there was to get the users of these systems.
So what they're called electronic warfare officers, the people that are running these
systems in operations to essentially trust the prototypes enough that they can field
and deploy them.
So the focus across that entire time for these types of systems were trust, reliability,
stability, and making sure that it's providing a lot of value to that warfare officer and
not sort of inundate in them with too much information.
So that was sort of the machine learning side.
More recently, I guess since the 2013-2014 time frame, that's when we as an organization
started getting interested in deep learning with some of the really early performance enhancements
that were shown from that.
So that's where I kind of dove pretty deep into trying to apply deep learning to the spectrum.
So when I say spectrum, going back to communications, it's a signal.
So we basically turn what we hear, say over the error in the cell phone signals into what's
called a spectrogram and analyze that spectrogram and try and identify patterns for how they're
behaving, how they're adapting, and kind of isolate different communication patterns.
So that was pretty much my time at Lockheed Martin.
About a year ago, actually for several years, I was kind of following the trends in machine
learning and deep learning.
And more recently, I guess what's called data science, following podcasts like this, blogs,
etc.
And really got an interest in how some of this machine learning technology was being
fielded or deployed in other industries.
So I was working more in a prototype space.
We did have to build actual products and test them.
But it wasn't really sort of at that level, say scaling out to hundreds of thousands of
users or surfacing it across a different, unique customer base.
So I started getting more of an interest in how it was being used in different industries
and through a colleague, they connected me with Urban Outfitters, which recently stood
up a data science and advanced analytics organization.
And I've been there for about a year.
Well that strikes me as a pretty significant shift from a defense contractor to kind
of a hit retailer.
Yeah, yeah.
Well, I mean, as a natural transition to move from working with electronic warfare officers
to trying to sell more dresses or match customers with the best dress that they believe that
they should be wearing.
Yeah, it does sound strange, but what's really funny is when I kind of started with the
job and kind of really understood the landscape, the underlying technologies that are being
used for both are very similar when it comes down to the approaches or the math or the deep
learning models, machine learning models.
It's just using it in a different way.
And I kind of landed on that earlier on in my career when my originally my focus was
on strict sort of wireless system signal processing and moving into more machine learning and
researching some of the math behind there.
Because when you kind of have to decode a message over the air coming from your phone that has
an estimation that's going on in the background and the underlying math that's done there is
very similar to what may be done in various machine learning domains.
So there's sort of a lot of overlap when you get down to that level.
And similarly for analyzing some of the spectrograms or the spectrum at Lockheed Martin, you can
look at that almost as an image and you're doing image processing on it similar to how you
would take a picture of a model and address and analyze the characteristics of that dress.
So there's sort of very similar space, definitely a completely different domain and they both
sort of have unique challenges.
My background is electrical engineering as well.
My graduate work was on the networking side like stochastic modeling of microcells and things
like that.
So I'm very, very curious about some of the work that you did in your past life and looking
at applying deep learning to the frequency domain and things like that.
But we're not going to talk about that now.
We're going to talk about selling dresses.
So you put together a couple of our two part blog series on this project in which you
are using deep learning and in particular computer vision and in particular custom vision
APIs to do what you called automated fashion product attribution.
So what's the business problem there?
What is fashion, what is product attribution and why is it important for urban outfitters?
Yeah, sure.
So in terms of product attribution, we'll start with like take a picture of a dress on
any sort of e-commerce website.
On that dress there are certain attributes about that particular product.
So for dresses that may be characteristics such as the sleeve length, it could be the
neckline.
So is it a deep V, is it a crew neck, the color, the pattern, the length of the dress, how
far it kind of goes down the leg, the fabric composition, the list sort of goes on there.
And those attributes sort of provide a textual metadata or description about the essence
of that product.
Traditionally, the descriptions may be fairly short based on limitations of being able
to code in those attributes if you're going to be doing thousands of them a week or thousands
of them a month.
So what we were interested in is are there ways we can sort of one automate the process
of attributing our products the way we currently attribute them.
And then two, are there ways that we can sort of augment the existing attributes set
that our merchants, buyers and web product team use today to kind of enhance the description
of those products.
Because the products themselves in terms of representation, you have the image itself,
you have the attributes, you have the copy or the textual description and the title.
So all of those are sort of good indicators of what that particular product is and how
it can be categorized.
The attributes themselves are used in various different ways across the business and this
is probably common across a lot of the e-commerce domain.
So one way is it's used for navigation filters for the customer online when they're shopping.
So how do you filter on obviously things like size and color but also some of those other
attributes I described earlier.
So if somebody is shopping for a cocktail party that may be different than how they shop
for business attire and it'd be that's helpful to have attributes or coding that kind of
tailors one to the other so that you can filter your site to give them the best sort of online
experience as possible.
Another way is search so you can treat those attributes as keywords and if you can identify
some type of correlation or distance between those keywords that's a good way of enhancing
the search beyond just strict keyword look up if it can sort of identify those associations.
That's more on the front side for customer facing but the business uses it, uses the attributes
in other different ways on the back end so think planning and forecasting.
So how do we know as a company how many floral prints, midi dresses do we want to buy for
next summer or how many even at the top level how many coats versus pants versus blouses
need to be purchased season by season.
So that kind of goes into the planning and forecasting side of trying to identify and
kind of cluster these different products based on some of these attributes and if things
are either misattributed or insufficiently attributed it can heavily sway the direction
that some of those forecasts or even retrospective analysis of prior performance goes.
What's the history of product attribution at a place like Urban Outfit as you just kind
of relying on the data that comes from the manufacturers and you haven't done much
with images previously or is there kind of historical work that's been done to try to do
this.
Sure.
So my understanding is it's sort of done in a couple different stages.
The first is during what we're calling the buying process or even the planning process.
So that's when a product is purchase orders put out for it.
It has attributes like you said perhaps from the manufacturer.
We sometimes get samples from that product that we can sort of confirm those attributes
before they get coded into the system on the front side of things.
Then the order goes through and several weeks may go by.
We get the product in house.
We decide to list that product onto the website.
That's when the web team comes in and they may sort of enhance or update some of those
core attributes to Taylor to the current trends, current season or the current creative elements
that are on the website to showcase some of those products.
So on the front end it may be very simple just sort of colors, patterns, sizes, brands,
et cetera.
And then when it gets closer to customer facing website side, that's when it may be
enriched with some of those examples telling you earlier like black dresses that may be
good for a cocktail event.
Historically that has been done very, it is a manual process.
And it's important that at least at some point in that chain does get reviewed by an actual
person because they're sort of so important into describing that product itself.
Long term, the goal is to sort of, again automate some of that, but still have somebody
always in the loop to confirm or if the system identifies errors to manually kind of override
those errors.
So we've also investigated in different vendors that provide this capability as well.
And kind of experimented on and off and to be honest, urban outfiters as a whole does
work with dozens of different vendors.
So at various times we may be trying out different attributes feature sets that come from
different vendors and using it in different contexts.
To date though, the primary force is sort of manually driven by the merchants, the buyers,
and the web product team.
Okay.
And so faced with this challenge of trying to interpret visual images and pull out these
different attributes, there are lots of ways that you could tackle this problem with machine
learning, what you did was turn to some of the cloud-based APIs that are available.
Can you talk a little bit about your motivation there and the various considerations?
Sure.
So we actually started with building experimental in-house models using primarily like Keras built
on like TensorFlow, sometimes PyTorch, and kind of tweaking existing models, retraining
them, fine-tuning them, or I think what's called essentially transfer learning, to I reclassify
those images based on whatever labels we have.
So we did play around with that a little to start.
That's of course coming in.
That's the first thing that I was interested in doing, very excited to get my hand on some
of the data that we had in-house, because we do have a large collection of hundreds of
thousands of products with some labels on them.
So why not straightening data to play with?
That was one nice thing moving over to this industry, and Urban Outfitters from Lockheed
Martin is Lockheed Martin in a research lab, since we kind of were working on essentially
prototypes and on more of the advanced side.
We were usually trying to generate our data, or it was also very expensive to acquire data,
particularly setting up receivers and things to collect some of those signals.
So being able to kind of walk in on day one and have the ability to tap into some of
the rich product imagery and descriptions was very nice.
At the same time, that was also a unique engineering exercise for me, like having to go through
that amount of data, and sort of in an efficient manner to kind of get it into a point where
it can train, as with many machine learning or deep learning applications, like 90% of
the work.
And that's where some of this kind of came in.
We started seeing some of these machine auto-ML type of solutions come into market.
We actually worked with Google on their alpha release of their auto-ML, so that was our
first entrance into it, since they're sort of we interact with them pretty often.
And from my perspective, or I should say our data science teams perspective, our team
is fairly small.
It's only a couple of people right now in the core data science group, and maybe a dozen
total across data science, advanced analytics in the organization we work.
So we have a lot of projects that we're interested in doing, and really it comes down to what's
the most effective use of our time.
And I was sort of attracted to at least trying out some of these auto-ML solutions to see
how far can we get with sort of this automated solution?
And is it good enough to use in production and satisfy the majority of our needs?
It may not be 100% perfect, it may not beat the state of the art, but does it move the
needle enough to warrant sort of going to a managed service versus building it in
house, and that allows to focus on efforts that say use those attributes rather than having
to generate those attributes, if that makes sense.
When you were experimenting with the building this in-house with Keras and TensorFlow and
the like, did you get far enough that as you started to explore the hosted services,
you had something to compare to or did you run into, were you able to build in-house models
to perform the tasks that you were trying to perform?
Yeah, I'd say we actually started building the in-house models in the initiative kind
of started in parallel with trying some of the managed services as well as in-house models.
And in terms of like what a model like this looks like, the inputs would be the image
and which in this case, the image for urban outfitters, if you go on one of our websites,
is a model wearing a product, sometimes with a clear colored background, sometimes with
a very rich atmospheric background.
So some of our photography we shoot on site with, say, photos of some of the models wearing
the products in a city or urban environment or in a desert environment.
So there's some of that scenery is in there in the background as well.
So we were just trying feeding the image in raw into one of these models and then the
output would be to start a single attribute for that model.
And that would be, say, a sleeve length and that sleeve length would have various values
that you're trying to predict.
Is it sleeveless?
Is it short-sleeved?
Is it three-quarters?
Is it long-sleeved?
From there we kind of built out a bank of those models.
So one that would say do sleeve length, one that does neckline, one that does the dress
length, one that does dress color, et cetera.
And what we realized is when we're building out those models, both sort of in-house and
with some of these services is that it really, that some attributes are really easy to identify
like sleeve length, maybe one.
Others are far more subtle, like the occasion of a dress, like, in my occasion we mean
under what occasion would you want to wear a dress like this.
We've also tried different types of products, so like tops and bottoms and doing the same
thing with those attributes.
So the challenge, I'd say the challenge across the two was sort of the same in that some
attributes were far more challenging to discern the individual values between the others.
And another challenge we ran into with these images is the complexity of the images required
to require sometimes a level of segmentation in order to get the best results.
So if we had just photo flats meaning it's just the product laid on a flat white background,
then that takes out any background, it takes out any model, and it takes out any distortions
of the product based on the models pose.
But most of the time actually all the time we have these more creative images, which are
great for providing context to the customer and how they can sort of wear this and under
what occasions, but the challenge from the data side is how do you, if we're interested
in just the essence of the product itself, it's hard to isolate the characteristics of
the product from the background.
So that's one thing that we realized really early on in both the in-house models and these
managed services is the importance of isolating the product.
And the same goes with if you have multiple products in a single photo.
So if somebody's wearing a blouse, jeans, a hat, and some shoes, then the other products
if you don't sort of isolate it can sort of mix in and bias the models that you're creating.
That was probably the biggest ones that we ran into.
So you started working with these custom vision services.
Maybe walk us through kind of your methodology for trying out these various services.
How did you even, like you didn't, did you use all of the ones that were available at
the time or did you create a short list?
How did you even choose which ones to look at?
Yeah, we, so we isolated down, we wanted to narrow down to about five or six to keep
this scope reasonable.
And from there we just chose the ones that sort of showed up the most and started with
some of the heavy hitters.
So think like Google IBM, so to summarize a list that we tried, there was Google, AutoML,
that's currently in beta.
There's Salesforce Einstein Vision, which is driven by the Salesforce IBM Watson, had a
vision solution.
There was a company called Clarify, which is a smaller business, but they were started
by a fairly prominent figure in deep learning and have been growing since 2013 issue.
I want to say, yeah, that's Matt Zeeler who's been interviewed on the show before.
Yes.
And then there was also Microsoft Azure has their custom vision model.
So we kind of stuck with the bigger ones that we found and then since Clarify had some
income and see and experience in there, we added that as well.
More recently after the conference presentation that I gave at rework deep learning London
in September, we signed up for the Fast AI version three course and that started in October
and November timeframe.
And we were just curious how well would these models work just applying less than one, less
than two, a Fast AI and how they would sort of compare to some of those.
So that one came in just because that had a lot of momentum.
They kind of had some close to out of box capabilities.
But we were looking for ones that seemed fairly prominent, had various capabilities such
as batch uploads, being able to automatically serve a model afterwards in some scalable
manner if we wanted to run this in production.
Something that had reasonable APIs or interfaces so that we can query it essentially as a
service and some level of support.
So there's certainly a lot of different people, a lot of players in this game.
But those are just sort of the ones that we landed that we thought were representative
of the current landscape.
As you explore these different services, you know, so what degree did you find that the
service itself is commoditized, meaning there's not a lot of difference in performance, there's
not a lot of difference in features, or are there, you know, fairly significant differences
from one to the next, again, performance features, usability, I think is something you
looked at as well.
Yeah, we broke it up in the two high level dimensions.
One was performance, which includes your standard metrics like classification accuracy,
AUC, and then usability, which includes all of the other factors that one may consider
when using this, not from an academic perspective, but more from a business perspective.
So that includes things like cost, the overall user interface in interacting with it, how
easy is it to upload data, how easy is it to serve data, is it quick to serve or perform
inference, how long does it take to train the models, how long does it take to tweak the
models, and we tried to look at it both from a data scientist's perspective, as well as
from a, anybody who is not necessarily an ML machine learning practitioner, so think
like a traditional software developer or an analyst, and the reason we did that is because
that's really where the target audience is for these, these at least automated machine
learning products, the larger space are those people that have data, they're able to acquire
labels, they're interested in building these models, but they don't necessarily want
to take several different courses in machine learning and identify how to build one out.
So on that front, we did have sort of an analyst slash intern kind of using these tools
from that perspective, and then other more experienced data scientists used the tool
too, and kind of collected our thoughts on some of the challenges we identified.
We also tried to diversify the data sets that we're operating on, so we had one data
set that we generated that was, we're calling it the Urban Outfitters Dress Data Set, which
is just a collection of about 15 or 5500 dresses, and labeled very simply with just what's
the dress pattern in terms of floral, solid stripes or not addressed, we're not addressed
as something indicating it's very obviously not addressed like shoes.
The goal there was just to have something that was kind of constrained and isolated.
Not necessarily the actual data we'd want to use in real-time operations, but something
that was controlled so that we can compare all the services against each other.
So that was the one we had in-house, but we also used three public benchmark data sets.
So we used C4-10 MNIST and fashion MNIST. So C4-10, I think they're 32 by 32 objects or
images of different objects. MNIST is the digits, data set, and fashion MNIST is sort
of the equivalent of MNIST, but using very simple looking fashion apparel, like shirts,
pants, dresses, etc. So that way we sort of had a mix of different types of images going
into this and we weren't sort of biasing towards the one dress data set that we had in-house.
Did you get interesting information by doing C4-10 MNIST?
One of the challenges I think in this industry is we talk about kind of being over-fitted
on data sets like MNIST and C4-10. I'd expect all of these services to do pretty well on
these. Was that the case or no?
Yeah, if I recall, they all did pretty well. None of them quite got to the point of the
true best scoring public benchmark. That's mainly because these algorithms that the services
are using usually fall into from what I've seen one or one of two categories. The first
one is transfer learning. So that's where you've taken already trained model, very deep
trained model, cut off the head of it, have your own labels on the end and just retrain
that sort of final layer or possibly tweak some of the layers in between. Then the other
one, so that's the majority of them. Google Cloud, I think, was the one that stood out as
different in that they used something called an architecture search or neural architecture
search using something called NASNET that they published shortly before releasing AutoML.
In that case, it's trying to find different configurations of the underlying neural network
architecture that fit that data best. In the same with the in-house models, we were basically
doing a in-house transfer learning on ResNet models. In all of those, you're already fit
to or you're not training necessarily from scratch and training for hours with a really
deep model. You're not going to hit those best performing numbers that you see in terms
of the benchmarks out there, but they all get reasonably well. So if I recall, there
may be fractions of a percent or a few percent often some of those data sets, and they all
sort of performed very similarly. In fact, some of them, if I recall, may have even performed,
we found they performed slightly better than we expected. It's mainly because that data
set is, like you said, either A, already incorporated into some of those models that's already
learned or B, essentially too small or too simple to be able to take a very large technique
like neural architecture search and find in the best solution for. So, but across the
board, for all those and even our internal dresses data set, performance as a whole was
not a differentiator among any of the services that we found because most of them fell essentially
within what one may deem the standard error for that data set. So like plus or minus a fraction
or a couple percent. So if you're trying to eke out the absolute best performance for your
use case, that may be very important for for our applications in sort of e-commerce or kind
of enhancing these attributes for our internal teams. A fraction of percent is not going to make
a huge difference. So it really came to that usability side that was more important for us.
So that was one interesting finding is that there wasn't really any one major leader in terms
of absolute performance. So it comes to all those other factors. This past summer, I had an
intern. It was my daughter, actually, and she did a similar project working on or working with
the speech to text API. So we had a bunch of podcasts. We had a bunch of transcripts and the
idea was, could we build an automated pipeline to kind of transcribe our podcasts and using
some of these similar kinds of APIs and ultimately maybe do some interesting things to help it
perform better on kind of domain specific words. So this project started out with her looking
at a bunch of APIs. And one of the things that we found pretty early on, like the initial plan
was, we've got these APIs. We can throw a bunch of podcasts or audio samples at them and
just kind of see how they do. But we found that each of the models had a bunch of like kind of,
you know, they have kind of knobs, configuration parameters like, you know, Google supported three
different models, telephone, video and default. They had different sample rates and things like
that. All of these things are specific to audio. And the result of all that was that, you know,
we certainly could not kind of compare these, you know, very easily using their default settings.
But even when we tweaked the individual settings of a different model, we never really had a strong
sense that we were able to compare apples to apples because, you know, we were kind of custom-fitting
parameters for each of these runs. Is there a similar dynamic on the vision side? And if so,
how did you address that? Yeah. So I think for on the vision side, the training and the, sort of,
building the model itself is completely automated. So there's not much you can really do in terms of
how to tweak knobs there. You can sort of iteratively update the model by providing new
images or relabeling images as you go. But we did see that there were sort of knobs
in terms of how you interact with the model after it's been trained. So not quite a knob,
but a feature. One of the services offers human labeling for mislabeled or unlabeled images
so that you can very rapidly kind of update your data set without having to do it from scratch.
I think that one was Google. Clarify stands out as the one that was most quickly able to kind of
through the interface, click on an image, relabel that image if it happened to be a false
positive false negative and click retrain and get a response in a matter of like seconds.
So that in that case, that's a sort of knob that's useful if you're rapidly interacting with it
and you have a really messy data set that you're trying to use the service, not just to build models,
but sort of to iterate you working with the model to iterate on that data set to kind of refine
both the model as well as the quality of your the labels in your data. So we found more less
on knobs in terms of the performance because it was a fully managed machine learning, so like
machine learning as a service, but more on the usability side. And at last point that you are
mentioning is that this feature that you are describing is this feature where you're able to
identify mislabeled or misclassified images in a given run and kind of automatically tag those
to get put into your training set for future runs. Yeah, yeah, that's that's the one there. Okay,
cool. You know, and part of your analysis, you talked about this challenge that you ran into
around data poisoning. Can you elaborate on that a little bit? Yes, so this goes to sort of the
things that one should be careful with when using these types of services. The you don't want
to look at these services as just a turnkey solution where you just give it the data. It's going
to give you the perfect model and you don't really have to do much after that, particularly if you're
sort of a manager that needs to plan time in deploying these type of products and allocating
resources to it. So one interesting one we came into was if you what we have is for each
product, there could be several different photos for it. So there could be a front-facing photo
of a dress, a rear-facing side, different angles, zoomed in photos of the product. And the one
example I think really resonated was a photo of a model wearing a dress and we, sorry, it was
addressed with roughly four or five photos of that model wearing it with different angles and
zooms. And then each one of those photos you could potentially treat as an independent sample.
So each one of them is marked as a, in this case we, I think we're a classifying dress length,
so is either a mini dress, mini dress or maxi dress, which is the different lengths. And if each
one of those five images are treated independently, then if you use an arbitrary train validation
test data split routine, then it's possible that that same exact product can end up both
in your training, validation and test sets. And what that means is when you go to predict
on say one of those test data, an image of that exact product was in training and then it happened
to get spilled into test because of the way you split it, you could have trained to identify that
that was a mini dress and then pass in a photo of that dress into the training side that is say
a zoomed in picture of that dress of say just the top sides that's really accentuating say the
neckline and maybe the sleeves. So you may not even see the bottom of that dress in that test photo,
but the model will very accurately predict that it is a mini,
light bottom length dress, which is not even in the photo. And the reason for that is because
the model saw photos of that exact image under different angles in the training set.
Right. So in that last statement, you said you'd pass in this view of the image into the the
training side. I think you met the inference side. And so you're giving your passing your
your trained model, cropped picture of the dress from which it could not reasonably predict the
length of the dress and finding that it performs well, suggesting that it's picking up on it
memorized basically that this dress is a mini dress. Yeah. And it may be for many other factors
other than actually looking at features that indicate the length of the dress. So in that particular
image, it had a nice clear blue background. The pattern of that dress was like a floral
purple kind of themed dress that may have been unique to that dress out of the entire data set.
So it may have just memorized that purple floral pattern is associated with an arbitrary labeled
a mini length dress that we gave it. So one has to take care of that sort of if you're,
if you just take, if we were to just walk in and take 50,000 photos with the labels on them and
just allow it to do a random split, then they're going to kind of go all over the place. You need
to kind of make sure that you cluster your products so that you're only feeding in
products to training and independent products to validation or test. So that's just kind of one
example we ran into. There's some other subtle ones that kind of go back to the point of segmentation.
So there was one interesting example where the model was wearing a, I think it was a red and black
striped dress and it kept ringing up as solid. And this was across both our in-house models and
the managed service models. And we're like, why does this keep showing up as solid? It is clearly
a striped dress. It looks a lot like the other striped dress that we stuck in there. So then we
started poking around at it by sort of arbitrarily cropping the image in different areas and feeding
in the cropped images to both the cloud services as well as our own classifier. And we realized that
as we move the cropped image more and closer and closer to the hat that the model was wearing,
which was a black solid hat, the more it was ringing up as solid. And then the more that we cropped
that hat out and just focused just on the dress, it was stripes. So that's a case where for whatever
reason it was tying that particular black solid hat. And that was the primary feature it was
using to classify pattern instead of the dress itself. So it can be a little tricky to kind of like
tame these models based on your input data. You have to be kind of be careful with that input
data. And that goes back to what I was mentioning earlier where you can't just treat as a turnkey
solution because that means in order to do that, you need to have a very clean data set. And in
order to have a very well, in practice, at least I have never run into a perfectly clean data set
in outside of academia. And even then they can be dirty sometimes. So that's sort of a cautionary
tale to like really kind of inspect the outputs of these models. And that's actually where a lot of
the effort that these managed services went into is the user interface after the model's built.
How do you surface all of the false positives, false negatives? How do you categorize and cluster
the different attributes that you're using your model to train on? And that way a user can
without machine learning experience inspect and say, hey, something's fishy is there.
So those are all great examples of things that people need to watch out for when they're working
on with these products from a data perspective where there are other categories of kind of gotchas
or did it all kind of boil down to data management data quality? I think it often boils down to the
source of the data itself. There was an issue of sort of not necessarily an issue, but one thing
to keep in mind is when do you think you're sort of getting to diminishing returns when tweaking
and modifying these models? So if you hit sort of 92%, there's no real way of knowing whether or not
you can get up to 98% without constantly tweaking and tweaking the model. And the same goes with
sort of academic data sets. And these services allow you to, as I mentioned earlier, kind of
relable mislabeled images, hit retrain and see the results again. But depending on the pricing
structure, that can all come out of cost, whether it be a retraining cost or adding in additional
images. So it's kind of tough to, as you're using these. And the same goes when you're kind of building
these models offline or in-house in isolation. How do you know when to add more data to improve the
model? How do you know when you need to improve the quality of the data? And then how do you know
when it just sort of you hit? You hit the best you could do in a reasonable amount of time is one
that was a little tricky. It's still data related, but it's more on the modeling side.
So you've got in the blog post a couple of really interesting tables comparing the
both usability and performance of the different services. And we'll link to the two blog posts
in the show notes. Folks can go there to look at the detail. But the performance comparison
table struck me as really interesting. In particular, the homegrown solution based on the Fast AI
library performed very well state of the yard or at least let's say exhibited the best performance
of all of the things that you compared on the public data sets, but not on your own urban outfit
or dresses data set. That is where Google outperformed the other solutions. And now to be fair,
the difference between Fast AI on your full data set and Google was very small. But I'm wondering
if you have any kind of intuition as to what's happening here. Yeah, unfortunately that one's
it's kind of tough to tease out exactly why Google kind of eaked out in performance over
Fast AI because like you said, it was if I recall like a less than a percent or fraction of a
percent. And for that data set, if I recall, there was in the test set 500 or so samples. So that could
be just a few different images that happened to do slightly better in Google versus Fast AI. So
factoring in the sample size of the test set, to me, that's almost sort of in the noise. The
difference between those even though I think I probably highlighted them explicitly in that blog post
there. So I kind of look at that as not necessarily that Google is performing better than Fast AI
or that the other services are completely behind. But just it's from an from a operational
standpoint, it's sort of in the noise like they all performs very similarly. Yeah, yeah.
And so what was the what was kind of the the key takeaway here in terms of, you know, when you
need to a tool to solve these kinds of problems for the business, where are you going to look?
So we haven't actually decided to use any of the managed services. It was more of a experiment to
to kind of see what was out there. Currently, our team is still kind of focused on using say
Jupyter notebooks. We have been dabbling with the Fast AI solution as well. It's that's that was
using like the less than one. So it was very quick to get started with it. And it also that
library provides some visualization capability that's little easier than say, just manually rolling
images through like MAT plotlib. But what we found, I wouldn't necessarily say we're forever staying
away from these particular services. We may not even use them necessarily for a fully deployed
model. But even just using it as a quick hand wavy benchmark of uploading some data, seeing how
well these things, like if you have a fresh data set and you just want to see how it runs,
running it through one of these services, getting in an interface so you can at least visualize
some of the performance. And do it all roughly if you data set up reasonably well, roughly less than
an hour. I mean, the training takes often less than five minutes surfaces you results pretty quickly.
It's a nice way of very quickly getting a feel for what's in the realm of possible for these
models, even for somebody who is an ML practitioner. So it kind of gives a nice sense of comfort that,
yes, this is a tractable problem. You could get 80 plus percent in performance. And whether or not
you kind of stick with that service to use it for deployment and productionize it. Or you just
kind of use that as insight that your homegrown models may perform well. I found it much easier
to kind of get up and running with that than wrangling the data in a custom in-house solution.
Well, we've talked about the Fast AI library a few times. And I will add in a mention, a plug,
you know, just as you found, you can do a lot going through the first couple of lessons of that
Fast AI course using their library. And it is a fan favorite of folks in the Twimmel community. We've
got a community that folks can find at twimmelai.com slash meetup, the Twimmel online meetup. And we've
brought three cohorts of folks through the Fast AI course. The videos of our study groups are
available on YouTube. And we're about to start, at least at the time of the recording of this
group going through the part two course. I imagine with enough demand, we'll bring another group
through the part one course again. But for anyone who's listening to this and wants to learn how to
build their own state of the art vision models, the Fast AI course is a great place to start. And
the Twimmel online meetup is a great place to get support in doing that. With that said, Tom,
thanks so much for taking the time to share your work on this. Super interesting. And I appreciate
you coming on the show. Yeah, thank you for having me.
All right, everyone. That's our show for today. For more information on Tom or any of the topics
covering in this episode, visit twimmelai.com slash talk slash 247. As always, thanks so much for
listening and catch you next time.
