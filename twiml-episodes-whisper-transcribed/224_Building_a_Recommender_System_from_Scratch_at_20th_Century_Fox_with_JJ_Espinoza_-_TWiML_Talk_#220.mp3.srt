1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,480
I'm your host Sam Charrington. Today we're joined by JJ Espinoza, former director of data

4
00:00:34,480 --> 00:00:40,560
science at 20th Century Fox. In this talk, we start out with the discussion of JJ's transition

5
00:00:40,560 --> 00:00:45,400
from econometrician to data scientist and then dig into his and his team's experience

6
00:00:45,400 --> 00:00:50,040
building and deploying a content recommendation system from the ground up.

7
00:00:50,040 --> 00:00:54,640
In our conversation, we explore the design of a couple of key components of their system,

8
00:00:54,640 --> 00:00:58,720
the first of which processes movie scripts to make recommendations about which movies

9
00:00:58,720 --> 00:01:03,760
the studio should make and the second process is trailers to determine which should be

10
00:01:03,760 --> 00:01:09,600
recommended to users. We discussed the challenges they've encountered fielding this systems,

11
00:01:09,600 --> 00:01:13,680
some of the tools that were used along the way and a few of the upcoming projects that

12
00:01:13,680 --> 00:01:27,800
could be layered on top of the platform that they've built and now on to the show.

13
00:01:27,800 --> 00:01:45,920
Next week in machine learning and AI, thank you Sam for having me absolutely. So why don't we get started by having you share a little bit about your background, how did you get started working in data science and in particular, how did you get started working in data science and the context of a movie studio.

14
00:01:45,920 --> 00:01:56,560
Yes, I mean, it's interesting. My background is in economics and mathematics. I got a bachelor's degrees from UCLA and then went on to earn a master's degree in econometrics, focusing on statistics,

15
00:01:56,560 --> 00:02:07,680
a little over a decade ago, and originally I started working in economic consulting and in Los Angeles, the biggest industry that's here and some of the largest headquarters that are here are in entertainment.

16
00:02:07,680 --> 00:02:17,520
So I started working in consumer research and building econometric models for the Walt Disney company for marketing ROI and science.

17
00:02:17,520 --> 00:02:29,880
So I started working at the Walt Disney company then went on to NBC Universal, I was director of this analytics and now at 20th century Fox, I was brought on by one of my former bosses at Disney to work on data science.

18
00:02:29,880 --> 00:02:39,600
So it's been interesting. I've worked in different aspects originally started out with the marketing piece. It was still focused on marketing mainly and it's evolved over time.

19
00:02:39,600 --> 00:02:53,600
Now I talked to a lot of folks to come into data science from different fields, physics is often cited. I don't know, I don't think I've talked a ton of economists and econometracists if that's the right word.

20
00:02:53,600 --> 00:02:59,040
How's the transition been from econometrics to data science?

21
00:02:59,040 --> 00:03:20,000
Yeah, great question. So it's official. It's pronounced econometrician econometrician. So I would say and for any of the listeners out there, if you really want to see a very thorough comparison and and also accessible comparison, I would look at Google's chief economist's name is Halvarian.

22
00:03:20,000 --> 00:03:42,160
He has a PowerPoint out there that basically describes the intersection between and the differences and what machine learning can learn from economics and vice versa. So economics is mainly focused on establishing causal inferences and highly explainable policy, relevant recommendations essentially.

23
00:03:42,160 --> 00:03:59,600
So a lot of the techniques are regression analysis, linear regression analysis, logistic regression. There are some things that kind of look like neural networks, for example, instrumental variable regressions where you have some regressions connected to others, so some inputs feeding other inputs.

24
00:03:59,600 --> 00:04:13,600
And in time series analysis, obviously, there's a lot of overlap with the econometric methods that are used. A lot of things like Arima look very similar to recurrent neural networks and things like that. So my interest in economics is more from the policy side.

25
00:04:13,600 --> 00:04:42,240
And one of the original applications in terms of a business of economics and econometrics is actually marketing mix modeling, which is the, you know, how economists would allocate budgets, but now in the digital world that the data has become much more granular and much more detailed, the predictive power of machine learning artificial intelligence really is a good skill set. I would say to have for for economists to focus more on the predictive side of the analysis.

26
00:04:42,240 --> 00:05:02,400
You mentioned how variant I haven't been keeping up with his work, but I came across some of his work quite a while ago. He's he kind of the, you know, the father of the whole idea of a network effect, if you will.

27
00:05:02,400 --> 00:05:26,880
Yeah, he's definitely had he definitely had some some discussion around that and that that is that is a portion of of an economic analysis, and it's big in the internet now. So economists have been analyzing network effects since probably since the telephone, you know, the usefulness of networks with network product. So he's worked on that and he's he worked on search a lot at Google.

28
00:05:26,880 --> 00:05:40,080
And so you are working on data science at 20th century Fox and tell us a little bit about the types of projects that you work on there.

29
00:05:40,080 --> 00:05:50,480
Yes, so we are focusing mainly on recommendation systems. I was the second data scientist at the movie studio, my far bus at Disney was the first.

30
00:05:50,480 --> 00:06:09,280
So initially the work involved collecting a lot of consumer data as you know, many people are not aware, but movie studios are not technically allowed to own movie theaters. That's from a Supreme Court case in 1948. So a lot of the consumer data that we get from ticket sales. We have to purchase.

31
00:06:09,280 --> 00:06:23,280
So our two main projects involve recommendation systems that look at movie scripts and one that looks at movie trailers and the first one that started was the movie script movie trailer data.

32
00:06:23,280 --> 00:06:38,280
And it's they're both coming at the same problem bit in different ways and there's a lot of objectives around these projects, but it starts from, you know, helping people decide, should we make this movie or not.

33
00:06:38,280 --> 00:06:49,280
Down to who should see this trailer, you know, based on what they've seen before in the type of content and narratives and story that tend to appeal to them. Okay.

34
00:06:49,280 --> 00:07:00,280
And so you you're you're building these recommendation systems and it turns out that you're also pretty active in publishing papers.

35
00:07:00,280 --> 00:07:19,280
Several papers up on archive as well as some blog posts that you've published about these these types of systems, you know, typically think of a movie studio as a big publisher of research, you know, tell me a little bit about the motivations there.

36
00:07:19,280 --> 00:07:31,280
Yeah, definitely. So one of the things because this is these two products are incredibly innovative. No other studio really has systems like these at the level that we have.

37
00:07:31,280 --> 00:07:40,280
We figured it'd be good to share some of the basic scientific knowledge and outcomes from our models in these research papers.

38
00:07:40,280 --> 00:07:49,280
In a way that could be accessible for the broader industry because as you may be aware, there's a lot of changes in our industry that are happening from technology.

39
00:07:49,280 --> 00:08:05,280
And it's it's good just to have that out there, you know, at a fundamental level without giving away, you know, too much of the secret sauce for the studios to learn from because I think there's there's a shift happening and it's good if we all kind of had some some of this knowledge out there.

40
00:08:05,280 --> 00:08:16,280
Well, maybe let's jump in a little bit deeper into what you're working on. You mentioned that the data that you're building these recommendation systems comes from third parties.

41
00:08:16,280 --> 00:08:25,280
How do you kind of design a pipeline for, you know, getting all this data in and processing it.

42
00:08:25,280 --> 00:08:43,280
So yes, our pipeline is is was designed on Google Cloud technology and what we we used several different tools. One of the ones we use is airflow to essentially orchestrate the data in a lot of the information that we have are related to mobile phone user IDs.

43
00:08:43,280 --> 00:09:02,280
So there is some encryption that needs to happen and some of the third parties what they do is they encrypt the mobile phone IDs and help us match all of these different data sets and then it comes into our database system and then it goes through multiple layers of processes because we have information both about movie trailers.

44
00:09:02,280 --> 00:09:14,280
We have information on the users. We also have to collect videos online videos and extract images from from the videos and create embeddings from the movie scripts.

45
00:09:14,280 --> 00:09:20,280
So all these things are orchestrated on a daily and weekly basis through airflow.

46
00:09:20,280 --> 00:09:35,280
We do have some things that are running in in cron jobs and things like that. Again, like I said, this is this was something that was done probably in nine months. So it's fairly automated given the time crunch and we're fairly proud of what it's able to do.

47
00:09:35,280 --> 00:09:39,280
In the end, the report generation uses Google data studio.

48
00:09:39,280 --> 00:10:02,280
So once the jobs are kicked off and the data is processed through the system, the reports are generated data studio, the cloud storage locations that we use to believe is we use Google cloud storage and we use big query for some of the reports and storing and all of that is housed automatically some of the data that we have is coming from AWS.

49
00:10:02,280 --> 00:10:25,280
So that needs to be orchestrated 20th century Fox is a AWS shop. So some some of the data comes through there and you know we need to build some some jobs to ingest that in out of curiosity as an AWS shop what motivated you to build the data pipeline for these projects on the Google platform.

50
00:10:25,280 --> 00:10:36,280
Yeah, so so the main reason was, you know, we had we had two people and we, you know, in the beginning, myself, my boss and you know, that's up to team of five.

51
00:10:36,280 --> 00:10:50,280
And from a learning curve perspective, Google cloud doesn't have a steep of a learning curve as AWS does. It's much easier to set up some of these systems now having said that.

52
00:10:50,280 --> 00:11:02,280
It isn't as configurable as AWS and there are some, you know, they don't have all the products and services, but because we wanted to get up, get something up running fairly quickly.

53
00:11:02,280 --> 00:11:13,280
Google cloud just streamlined, they do a lot of the back engineering on their end. So it just felt like a quicker thing to do. So one reason was the efficiency of getting some products out.

54
00:11:13,280 --> 00:11:31,280
And two was, since we're working in marketing data, a lot of the DCM logs, which is called basically double click. These are the YouTube ads and search ads and all the things that are, you know, that the major companies, you know, especially 20th century Fox used to analyze and collect their marketing data.

55
00:11:31,280 --> 00:11:40,280
It's already connected to Google. So a lot of the pipelines to ingest that marketing data were right there. So we were able to, you know, build efficiency that way too.

56
00:11:40,280 --> 00:11:52,280
Earlier you mentioned the scripts and the videos and I, when you first mentioned that I envision these as totally separate systems, but it sounds like you're bringing them into the same pipeline.

57
00:11:52,280 --> 00:12:02,280
What's the relationship between the scripts and the videos and the goals that you're that you've ultimately have for this recommendation system?

58
00:12:02,280 --> 00:12:20,280
Definitely. So you can imagine the final, the final stage or the final data set basically being a merge of customer data embeddings from a deep learning model of the video content, which is essentially a numerical representation of what the videos contain.

59
00:12:20,280 --> 00:12:41,280
As well as embedding embeddings of the movie script and then we also have a lot of other metadata. So think of one customer saying, hey, this one customer saw this movie, this movie had this budget had these actors, by the way, it also has these and this is what the text and the video of the marketing that they saw looks like numerically.

60
00:12:41,280 --> 00:12:59,280
And then just having millions and millions of rows of that customer data and understanding what movies they saw, what was advertised to them. So that's why we're really combining the computer vision, natural language processing because all of the trailers, one of the most important pieces of marketing for movie studio.

61
00:12:59,280 --> 00:13:13,280
It doesn't have all the information needed, a script is pretty useful as well. It essentially outlines the entire movie line by line and gives us an additional layer of information.

62
00:13:13,280 --> 00:13:31,280
In both of these cases, a script as well as a trailer, a script is kind of a representation of a couple of hours worth of movie, a trailer is a minute and a half, I suppose, of video content.

63
00:13:31,280 --> 00:13:49,280
Are you creating a single static embedding space for an entire script and an entire trailer, or are you in the case of a trailer like going frame by frame or something like that, or creating a sequence of embedding spaces.

64
00:13:49,280 --> 00:14:08,280
Yeah, we're using sequences, especially for the trailer analysis. So some of the corner wrote uses open CV to and essentially what we do is we download all the trailers, all the trailers that come out into into our database, and we extract one frame per second from that trailer.

65
00:14:08,280 --> 00:14:21,280
And then we use Google's YouTube 8M model, which is a pre-trained model that you could find on GitHub that essentially is modeled off of YouTube, which is perfect because we're using YouTube data.

66
00:14:21,280 --> 00:14:42,280
So then we feed it image by image into the pre-trained YouTube 8M model, and it creates an embedding layer back to us. So you can think of a trailer as, you know, it'll have a row every second, imagine a CSV file, imagine a row or JSON, whatever, you know, one row per second that contains an embedding.

67
00:14:42,280 --> 00:14:59,280
So that's been really useful because the latest research paper that we've worked on really looked at the temporal aspects of the trailer because marketing is especially providing YouTube ads is really important to kind of get people's attention right in the beginning and seeing them through.

68
00:14:59,280 --> 00:15:14,280
So understanding whether a car chase or romantic kiss in the first five or 10 seconds captured people, when do people drop off, it was really important to, you know, have that frame by frame analysis and in our computer vision model.

69
00:15:14,280 --> 00:15:34,280
Now, for the natural language processing, that one is one embedding for the entire script. We found that we tried different things and we found that just at the level of the script, it's hard to really predict consumer behavior because you don't have actors, you don't have directors, you really don't have much.

70
00:15:34,280 --> 00:15:44,280
But when you have the trailer, you have something a little bit more tangible and that that increased to predictive power. So we really focused on making the image processing much more granular.

71
00:15:44,280 --> 00:16:12,280
I've heard anecdotes that with some of the streaming services, they will create, I don't know if they go as far as creating trailers on demand based on what they think you like, but certainly they have multiple versions of the given videos cover graphics, for example, and they'll kind of swap those out depending on what they think your preferences are.

72
00:16:12,280 --> 00:16:17,280
Is that kind of personalization, what you're ultimately trying to drive for?

73
00:16:17,280 --> 00:16:30,280
Yeah, we're doing some of that. Right now, the recommendations are being served to executives and creative people to help in, for example, modifying the trailer.

74
00:16:30,280 --> 00:16:43,280
One of our, one of our, one of our one one good success and insights that we had actually impacted the commercial for Red Sparrow, they came out on the Super Bowl. We saw that certain content, certain aspects of the content.

75
00:16:43,280 --> 00:16:55,280
We're resonating with certain people in our database and that went up to the president of the studio through a report and essentially that the actual content that was played on the Super Bowl was different.

76
00:16:55,280 --> 00:17:08,280
So that was our first initial look into, you know, changing some of the content based on, on these analysis, but we also have the ability for executives and writers to modify what we call the synopsis of the script live.

77
00:17:08,280 --> 00:17:15,280
So we have a very, you know, we have a Google, Google sheet up that has all of the synopsis.

78
00:17:15,280 --> 00:17:27,280
It's our quick and dirty UI and essentially what people can do is when they're thinking about how to position a movie or advertise a movie, the synopsis is important because it's a summary.

79
00:17:27,280 --> 00:17:44,280
So they're able to essentially emphasize different parts of the movie, even before it can be made and emphasize either the romantic aspects in these synopsis and we have jobs that run every week and essentially tell people, hey, we ran the analysis on your latest synopsis.

80
00:17:44,280 --> 00:17:52,280
And here are, you know, the attendance probability for different people, different audiences based on how you change it.

81
00:17:52,280 --> 00:18:00,280
Here are other comparable movies, here are geographic level insights based on the changes to the script or synopsis that you made.

82
00:18:00,280 --> 00:18:06,280
And that's really interesting for them because then they get to see, they get to test different hypotheses.

83
00:18:06,280 --> 00:18:23,280
So a lot of this is, right now there isn't like a product, for example, there isn't, there isn't a user interface. Our users are the executives who can play around with synopsis and trailers and then feed them into our system and see what the, how the audience changes based on their modifications.

84
00:18:23,280 --> 00:18:39,280
You've got all these features from the images, from the trailers, you've got the embeddings from the scripts and you've got all this information about the user, as well as it sounds like metadata about the films.

85
00:18:39,280 --> 00:18:53,280
Do you kind of concatenate all of this and build a model based on all of this concatenated information or is there some more sophisticated way that you're combining all of this information?

86
00:18:53,280 --> 00:19:00,280
Yeah, definitely. So the combined, we've tried different ways of combining the information in the model at first.

87
00:19:00,280 --> 00:19:20,280
We try to widen deep model because a lot of the audio and video features essentially benefited from deep learning, but a lot of the other film specific and user specific data would benefit from something a little bit more linear.

88
00:19:20,280 --> 00:19:39,280
But we didn't see that much lift into the analysis from doing that, so we're essentially taking movie vectors and user vectors or characteristics and building a pretty deep neural network to extract the analysis.

89
00:19:39,280 --> 00:19:51,280
But the script analysis, a lot of that is done through word to vac. So word to vac is used as part of the embedding process. And as I mentioned, the YouTube 8M project is taking the frame by frame features.

90
00:19:51,280 --> 00:19:59,280
And that's how we're really able to get, you know, 75 80% accuracy on what people will watch.

91
00:19:59,280 --> 00:20:19,280
And you've mentioned YouTube 8M a couple of times. What specifically is it doing? Great. So YouTube 8M is a project that you could find on GitHub and essentially what what YouTube did was it build a deep learning neural network that can classify its videos on YouTube.

92
00:20:19,280 --> 00:20:39,280
So part of that project on GitHub essentially is it has parts of the trained model that you could download and essentially point to other videos and images to create an embedding of what basically a representation of the video.

93
00:20:39,280 --> 00:20:54,280
And that worked perfect for us. Normally we build, we build the embedding from training the model, but because it's, you know, it's YouTube's classification system for the videos and we're using YouTube videos mainly trailers.

94
00:20:54,280 --> 00:20:58,280
It worked well and it helped us get things to speed to market.

95
00:20:58,280 --> 00:21:16,280
So let me make sure I understand this. They've got a data set of videos. How do you use that in conjunction with with your sequences to create an embedding or you creating the embedding just on their data set and then using that embedding in the context of your your images.

96
00:21:16,280 --> 00:21:36,280
Yeah, no, we're essentially taking the last portion of their neural network. So the we're using their model essentially. So we download their model and then we take the last layer of the deep neural network before the classification portion, which is just basically a series of numbers from the neurons.

97
00:21:36,280 --> 00:21:54,280
And then that becomes like the numerical inputs for that image or and frame or video into our inputs. They have, they have a folder called feature extractor and they essentially, you know, they called it the inception model.

98
00:21:54,280 --> 00:22:09,280
You can basically download it's 25 megabytes. It's based off an array. It does things, you know, like PC a matrix analysis that uses tensor flow all of that essentially so that you can basically extract tensor flow records from videos and images.

99
00:22:09,280 --> 00:22:18,280
And that's that was a simple way for us to convert the video data into numerical analysis that our deep learning models could could train off of.

100
00:22:18,280 --> 00:22:29,280
Okay, got it. So they offer this YouTube 8M as a data set, but that sounds like they also offer a model that you can fine tune with your own data, which is what you did.

101
00:22:29,280 --> 00:22:37,280
That's correct. They have a data set and then they have a model that they built on top of it and and the code is available on on GitHub.

102
00:22:37,280 --> 00:22:49,280
Okay, great. And so you know, when you kind of think back to building this data pipeline and building the model, what were some of the biggest challenges that you had to overcome.

103
00:22:49,280 --> 00:22:58,280
Right. So in this, some of the biggest challenges were I, this was my first time actually working at scale in the cloud.

104
00:22:58,280 --> 00:23:11,280
I did work a little bit on AWS, but prior to this, most of my experience at previous entertainment companies and economic consulting involved just physical hardware servers.

105
00:23:11,280 --> 00:23:22,280
So I play around a bit query a bit. I play around with some of these different tools, but just having to do it for the first time was fairly interesting learning airflow.

106
00:23:22,280 --> 00:23:37,280
I didn't really know air flow at the time. We hired several data engineer software engineers and machine learning experts, some of them, which are our mentioned in the paper, you know, like Matt Nickens and Avitalion, as well as Andy and Miguel Campo.

107
00:23:37,280 --> 00:23:50,280
And he was who was actually actually P who had these engineering know how so once they came on board onto the team, they were able to do things a little bit more, I would say a little bit smoother because they had experience with it.

108
00:23:50,280 --> 00:24:00,280
So for the first couple of months, it was just kind of getting things up and running, understanding how things worked, getting the Python client applications and also security was a big one.

109
00:24:00,280 --> 00:24:14,280
The good thing about Google Cloud is that the security is is fairly abstracted away, which is that another reason why it was good for someone with less cloud experience to to work on it. So that's in general, some of the challenges.

110
00:24:14,280 --> 00:24:33,280
So, you know, some of the more specific challenges were, for example, you know, some legal challenges in terms of just like being able to download the trailers from YouTube, you know, installing things like open CV to, if you've never done it, I've spoken, we had training at Google in residents for a month.

111
00:24:33,280 --> 00:24:53,280
And I spoke with several computer scientists there and people who were training with them, they said it's a pretty difficult package to install. If you don't know what you're doing, there's several different steps. So that was challenging. So even once it was installed, just being able to have it run on other computers required quite quite a bit of work.

112
00:24:53,280 --> 00:25:10,280
We have some executable shell scripts now that can, you know, basically install all the software we need on any virtual machine or data block data proc cluster, which is like an EMR, you know, instance in AWS, but that part took a week.

113
00:25:10,280 --> 00:25:26,280
And you know, you're talking about taking about a week to install import, you know, CV to, and that was at Google headquarters. That was with people that had master's degrees and computer science on the team, you know, so, and me being kind of a novice to the date engineering part of it.

114
00:25:26,280 --> 00:25:41,280
It was, it was, it was a real challenge, but, you know, in about a week, we got it up and running. I actually had someone come up to me at the training at Google and basically say, wow, you got open CV running. We tried it for two weeks at my company and we quit.

115
00:25:41,280 --> 00:25:50,280
So along the lines of that story, I'll actually tell you, once I had open CV up and running.

116
00:25:50,280 --> 00:25:59,280
I did a fatal mistake, which was I, I pressed like remove everything in Linux. I think it's like Rm star.

117
00:25:59,280 --> 00:26:00,280
Oh, gosh.

118
00:26:00,280 --> 00:26:01,280
Yeah.

119
00:26:01,280 --> 00:26:12,280
So the entire computer is like, so I had to basically under, you know, reverse, basically recollect the packages and the data and compilable C code.

120
00:26:12,280 --> 00:26:29,280
So I remember spending one night just, you know, learning like some sort of forensic Linux methodologies to extract data. So the key is if Python normally doesn't compile, but if it compiles and something is deleted, you can, you can extract some of that in Linux.

121
00:26:29,280 --> 00:26:36,280
It's a long story. It's a long story. So it took about a week to get it set up. It was then I deleted accidentally. I felt like hiding in a hole.

122
00:26:36,280 --> 00:26:45,280
Then I learned some forensic Linux techniques to extract some of the parts of the code that were compiled and see when you open that open the Linux terminal.

123
00:26:45,280 --> 00:26:51,280
Like it was getting eaten away, you know, as things get saved and saved on to disk and resaved on to disk, but they were still there.

124
00:26:51,280 --> 00:26:55,280
So luckily, it took me just an extra day to set that up. So that was the challenge.

125
00:26:55,280 --> 00:26:58,280
That sounds like a lesson you will never forget.

126
00:26:58,280 --> 00:27:00,280
I never will forget that.

127
00:27:00,280 --> 00:27:06,280
How do you measure the performance of these, this system?

128
00:27:06,280 --> 00:27:13,280
Right. So we want, we have several dashboards that are up and what we want to do is we basically want to take.

129
00:27:13,280 --> 00:27:21,280
First of all, we have during training, we have a holdout data set as any good data scientists would have.

130
00:27:21,280 --> 00:27:31,280
We have dashboards that are published internally to our executives. Basically, it's a scorecard that tells you, hey, here's what the model predicted and here's what actually is happening.

131
00:27:31,280 --> 00:27:42,280
That's really at the tail end of the modeling process. While the model is training, we have automated systems that keep track of precision, recall accuracy.

132
00:27:42,280 --> 00:27:58,280
And save that inside of a dashboard inside of data studio. So after the model gets retrained, we can see, you know, if the accuracy has changed and we're using cloud ML engine and Google Cloud, which is great because it automatically creates versions of previous models.

133
00:27:58,280 --> 00:28:10,280
So one thing we could do is if we see something's off, we can compare current model that has a decline or an increase in accuracy to previous models that we've had.

134
00:28:10,280 --> 00:28:23,280
All of that is basically systematized and engineered into the process. And before model is actually published into what we call production, which is essentially our ability to push these dashboards up to the organization.

135
00:28:23,280 --> 00:28:42,280
And that analysis is ran. We have automated emails as well, you know, generate through Python that look through the logs of a lot of these TensorFlow cloud ML jobs and parse through the accuracy, parse the things to basically alert us when something is up so that we don't always have to go check the dashboards all the time.

136
00:28:42,280 --> 00:28:52,280
It's still good practice. We were trained once a week. So it's still good practice. We go in and check everything before before something something goes off. And luckily, it's been fairly steady.

137
00:28:52,280 --> 00:29:04,280
You know, this being a model that you're not pushing in some kind of live production environment, but rather your mainly generating reports that these executives can use to make decisions.

138
00:29:04,280 --> 00:29:17,280
How tight is the feedback loop between I would imagine it can be quite long if someone's making a decision that results in a new trailer being created.

139
00:29:17,280 --> 00:29:23,280
And then you have to push that trailer out and then collect some data on how it's impacting users.

140
00:29:23,280 --> 00:29:32,280
Does that create challenges for you and tying the, you know, and just being certain about causality?

141
00:29:32,280 --> 00:29:45,280
No, yeah, it definitely does. And it's one of the reasons why we're looking to activate, meaning we are looking to take these insights and immediately start buying ad campaigns through Google that's a project we're working on now.

142
00:29:45,280 --> 00:29:54,280
Because we've seen that the models people tend to trust, you know, some of the output of the models, other parts they have questions about. So we still have to talk through it.

143
00:29:54,280 --> 00:30:00,280
But I would say the turnaround time for feedback and evaluation and pivoting it's about a week.

144
00:30:00,280 --> 00:30:09,280
And week, it could be faster, but most of the time the way that movie trailers work is that come out on, you know, they'll come out during the week near the end of the week.

145
00:30:09,280 --> 00:30:16,280
People watch them over the weekend. They get served these ads and people's viewing behavior on YouTube is fairly predictable.

146
00:30:16,280 --> 00:30:28,280
But yeah, to your point, we'd love it if, you know, this project that we're working on, we call it activation because that's what they call it in ad tech, which is essentially taking these insights from the model and taking actual users IDs.

147
00:30:28,280 --> 00:30:35,280
And pushing the YouTube ads directly to them as opposed to, hey, look, here's kind of what's working. Here's what isn't working.

148
00:30:35,280 --> 00:30:43,280
You know, normally what we do is we contact our media agency, which is an agency that purchases ads for us and give them some of these insights as well.

149
00:30:43,280 --> 00:30:51,280
But our goal is to move towards more of a programmatic ad buying strategy with these insights.

150
00:30:51,280 --> 00:31:07,280
Reflection on kind of the modeling part of the process itself, are there ways that you're hoping to evolve the models and the way that you're building the models you mentioned, kind of doing more of this time series aspect of it.

151
00:31:07,280 --> 00:31:10,280
Are there other other thoughts that you're pursuing?

152
00:31:10,280 --> 00:31:27,280
Yeah, definitely. So images are fairly important. But one of the great things that that that we have access to is also audio and audio, especially in ads, some people have the audio turned off.

153
00:31:27,280 --> 00:31:34,280
A lot of the auto play video features on, you know, social media sites may have the audio turned off.

154
00:31:34,280 --> 00:31:46,280
Or, you know, a lot of times it's on. So in addition to the audio, we're looking into, sorry, in addition to the video, we're looking into doing these types of analysis for the audio of movies.

155
00:31:46,280 --> 00:31:57,280
And I think that I'll add another dimension. We're essentially trying to, you know, get the written word and get what people see and help understand what people, what people hear when they're seeing a movie.

156
00:31:57,280 --> 00:32:08,280
To kind of help that can kind of underscore things like music. And we had about a one week meeting with Google's magenta team and they specialize in artificial intelligence.

157
00:32:08,280 --> 00:32:17,280
And they gave us some insights into how potentially we can leverage the sound that's coming into here. So that's one, that's one area of research that we're actively pursuing.

158
00:32:17,280 --> 00:32:33,280
The second one that we looked at earlier in the year, but I found a bit of difficulty is in trying to combine, for example, two different scripts or two different movies.

159
00:32:33,280 --> 00:32:51,280
Generating text is difficult. You know, you can find similarity metrics between two pieces of text. But, you know, what if you could combine predator and Deadpool and have it write a script for you? I mean, it's, I mean, it's something, I don't know what that would look like.

160
00:32:51,280 --> 00:32:53,280
What does that even mean?

161
00:32:53,280 --> 00:33:05,280
You know what I mean? So that's the thing like mathematically the space and dimensions where these embeddings live. Once you, it's hard to translate it back out to an actual readable text and format.

162
00:33:05,280 --> 00:33:15,280
But it's something that that we've looked at. So we have, we have a lot of different ideas towards the end of the year, we've been just mainly systematizing and automating a lot of the processes that we have.

163
00:33:15,280 --> 00:33:23,280
So as you, as some of you, some of you guys may be aware, we're being acquired by the Walt Disney company. I think they'll have some interest in this technology.

164
00:33:23,280 --> 00:33:30,280
So we've mainly been doing is, you know, showing up the security, showing up the how automated this is documenting everything.

165
00:33:30,280 --> 00:33:35,280
And, you know, keeping some, some of these ideas on hold for a bit until the Q one of this year.

166
00:33:35,280 --> 00:33:49,280
And then that way we'll, we'll explore and pitch some of these other, some of these other features that could be added to the modeling data sets and different types of predictive algorithms that we can use to enhance our current work.

167
00:33:49,280 --> 00:34:09,280
Fantastic. Well, I refer to this earlier, but your team is published some blog posts on this on the Google Cloud blog, as well as several papers detailing different aspects of this ongoing project.

168
00:34:09,280 --> 00:34:19,280
And we'll be sure to link to those in the show notes. But I just wanted to thank you for taking the time to walk through some of this with us. It's really interesting work.

169
00:34:19,280 --> 00:34:25,280
Thank you, Sam. It was, it was great chatting with you. Thanks, JJ.

170
00:34:25,280 --> 00:34:33,280
All right, everyone. That's our show for today for more information on JJ or any of the topics covered in this episode.

171
00:34:33,280 --> 00:34:50,280
Visit twimmelai.com slash talk slash 220. If this talk peaked your interest, you should also check out twimmeltalk201 where Lee Maynassri of Comcast breaks down how she and her team led the rebuild of the Comcast Xfinity X1 recommender platform.

172
00:34:50,280 --> 00:35:05,280
As always, thanks so much for listening and catch you next time.

