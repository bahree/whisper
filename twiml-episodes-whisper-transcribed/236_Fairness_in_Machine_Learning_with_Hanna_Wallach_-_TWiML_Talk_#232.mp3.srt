1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,480
I'm your host Sam Charrington.

4
00:00:31,480 --> 00:00:35,600
Today we're excited to continue the AI for the Benefit of Society series that we've

5
00:00:35,600 --> 00:00:38,400
partnered with Microsoft to bring to you.

6
00:00:38,400 --> 00:00:43,160
Today we're joined by Hannah Wallach, principal researcher at Microsoft Research.

7
00:00:43,160 --> 00:00:48,120
Hannah and I really dig into how bias and the lack of interpretability and transparency

8
00:00:48,120 --> 00:00:50,560
show up across machine learning.

9
00:00:50,560 --> 00:00:55,400
We discuss the role that human biases, even those that are inadvertent, play intaining

10
00:00:55,400 --> 00:01:01,400
data, whether deployment of fair ML algorithms can actually be achieved in practice, and

11
00:01:01,400 --> 00:01:02,920
much more.

12
00:01:02,920 --> 00:01:07,640
Along the way Hannah points us to a ton of papers and resources to further explore the topic

13
00:01:07,640 --> 00:01:09,480
of fairness in ML.

14
00:01:09,480 --> 00:01:13,080
You'll definitely want to check out the show notes page for this episode, which you'll

15
00:01:13,080 --> 00:01:19,200
find at twimbleai.com slash talk slash 232.

16
00:01:19,200 --> 00:01:23,640
Before diving in, I'd like to thank Microsoft for their support of the show and their sponsorship

17
00:01:23,640 --> 00:01:25,640
of this series.

18
00:01:25,640 --> 00:01:30,020
Microsoft is committed to ensuring the responsible development and use of AI and is empowering

19
00:01:30,020 --> 00:01:35,320
people around the world with this intelligent technology to help solve previously intractable

20
00:01:35,320 --> 00:01:41,760
societal challenges, spanning sustainability, accessibility, and humanitarian action.

21
00:01:41,760 --> 00:01:45,080
Learn more about their plan at Microsoft.ai.

22
00:01:45,080 --> 00:01:49,840
Enjoy.

23
00:01:49,840 --> 00:01:50,840
All right, everyone.

24
00:01:50,840 --> 00:01:52,680
I am on the line with Hannah Wallach.

25
00:01:52,680 --> 00:01:57,600
Hannah is a principal researcher at Microsoft Research in New York City.

26
00:01:57,600 --> 00:02:00,480
Hannah, welcome to this week in machine learning and AI.

27
00:02:00,480 --> 00:02:01,480
Thanks, Sam.

28
00:02:01,480 --> 00:02:04,480
It's really awesome to be here.

29
00:02:04,480 --> 00:02:11,680
It is a pleasure to have you on the show, and I'm really looking forward to this conversation.

30
00:02:11,680 --> 00:02:19,240
You are clearly very well known in the machine learning and AI space.

31
00:02:19,240 --> 00:02:24,800
Last year, you were the program chair at one of the largest conferences in the field

32
00:02:24,800 --> 00:02:25,800
NURPS.

33
00:02:25,800 --> 00:02:29,280
And in 2019, you'll be its general chair.

34
00:02:29,280 --> 00:02:34,760
But for those who don't know about your background, tell us a little bit about how you got involved

35
00:02:34,760 --> 00:02:37,400
and started in ML and AI.

36
00:02:37,400 --> 00:02:38,400
Sure.

37
00:02:38,400 --> 00:02:40,120
Absolutely.

38
00:02:40,120 --> 00:02:45,760
So I am a machine learning researcher by training, as you might expect.

39
00:02:45,760 --> 00:02:50,520
And I've been doing machine learning for about 17 years now.

40
00:02:50,520 --> 00:02:57,400
So since way before this stuff was even remotely fashionable or popular or cool or whatever

41
00:02:57,400 --> 00:02:59,640
it is nowadays.

42
00:02:59,640 --> 00:03:05,160
And in that time, we've really seen machine learning change a lot.

43
00:03:05,160 --> 00:03:10,240
It's sort of gone from this weirdo academic discipline, only of interest to nerds like me

44
00:03:10,240 --> 00:03:16,240
to something that's so mainstream that it's on billboards, it's in TV shows, and so on

45
00:03:16,240 --> 00:03:17,240
and so forth.

46
00:03:17,240 --> 00:03:22,080
And it's been pretty incredible to see that shift over that time.

47
00:03:22,080 --> 00:03:25,240
I got into machine learning sort of by accident.

48
00:03:25,240 --> 00:03:27,680
I think that's often what happens.

49
00:03:27,680 --> 00:03:35,080
I had taken some undergrad classes on information theory and stuff like that.

50
00:03:35,080 --> 00:03:39,840
And that to be really interesting, but thought that I was probably going to go into human

51
00:03:39,840 --> 00:03:42,160
computer interaction research.

52
00:03:42,160 --> 00:03:48,480
But through a research assistantship, during the summer between my undergrad degree and

53
00:03:48,480 --> 00:03:53,520
my master's degree, I ended up discovering machine learning and was completely blown away

54
00:03:53,520 --> 00:03:58,280
by it and realized that this is what I wanted to do.

55
00:03:58,280 --> 00:04:04,760
I've been focusing on machine learning in various different forms since then.

56
00:04:04,760 --> 00:04:11,960
My PhD was specifically on Bayesian latent variable methods, typically for analyzing text

57
00:04:11,960 --> 00:04:17,000
and documents, so topic models, that kind of thing.

58
00:04:17,000 --> 00:04:23,840
But during my PhD, I really began to realize that I'm not particularly interested in analyzing

59
00:04:23,840 --> 00:04:26,920
documents for the sake of analyzing documents.

60
00:04:26,920 --> 00:04:32,680
I'm interested in analyzing documents because humans write documents to communicate with

61
00:04:32,680 --> 00:04:33,840
one another.

62
00:04:33,840 --> 00:04:39,920
And it's really that underlying social process that I'm most interested in.

63
00:04:39,920 --> 00:04:47,160
So then during my postdoc, I started to shift direction from primarily looking at text

64
00:04:47,160 --> 00:04:53,440
and documents to thinking really about those social processes.

65
00:04:53,440 --> 00:04:59,480
So not just what are people saying, but also who's interacting with whom.

66
00:04:59,480 --> 00:05:06,480
And thinking about machine learning methods for analyzing the structure and content of social

67
00:05:06,480 --> 00:05:10,520
processes in combination.

68
00:05:10,520 --> 00:05:17,360
I then dove into this much more when I got a faculty job because I was hired as part

69
00:05:17,360 --> 00:05:23,200
of UMass Amherst's Computational Social Science Initiative.

70
00:05:23,200 --> 00:05:29,520
So at that point, I started focusing really in depth on this idea of using machine learning

71
00:05:29,520 --> 00:05:32,160
to study society.

72
00:05:32,160 --> 00:05:37,640
And I established collaborations with a number of different social scientists, focusing on

73
00:05:37,640 --> 00:05:40,000
a number of different topics.

74
00:05:40,000 --> 00:05:45,520
Over the years, I've mostly ended up working with political scientists.

75
00:05:45,520 --> 00:05:51,440
And often study questions relating to government transparency and still looking at this sort

76
00:05:51,440 --> 00:05:58,400
of whole idea of a social process consists of individuals or groups of individuals interacting

77
00:05:58,400 --> 00:06:04,800
with one another, information that might be used in or arising from these interactions.

78
00:06:04,800 --> 00:06:07,960
And then the fact that these things might change over time.

79
00:06:07,960 --> 00:06:14,520
And I often use one of these or two of these modalities, so structure, content, or dynamics,

80
00:06:14,520 --> 00:06:18,880
to learn about one or more of the other ones as well.

81
00:06:18,880 --> 00:06:25,880
Because I continued to work in this space, I started to think more not just about how

82
00:06:25,880 --> 00:06:32,320
we can use machine learning to study society, but the fact that machine learning is becoming

83
00:06:32,320 --> 00:06:36,160
much more prevalent within society.

84
00:06:36,160 --> 00:06:42,280
And about four years ago, I started really thinking more about these issues of fairness,

85
00:06:42,280 --> 00:06:45,760
accountability, transparency, and ethics.

86
00:06:45,760 --> 00:06:49,640
And it was a pretty natural fit for me to start moving in this direction.

87
00:06:49,640 --> 00:06:54,720
Not only was I already thinking about questions to do with people, but I've done a lot of

88
00:06:54,720 --> 00:06:58,960
diversity and inclusion work in my non-research life.

89
00:06:58,960 --> 00:07:02,720
So I'm one of the co-founders of the Women in Machine Learning workshop.

90
00:07:02,720 --> 00:07:08,560
I also co-founded two organizations to get more women involved in free and open-source

91
00:07:08,560 --> 00:07:10,440
software development.

92
00:07:10,440 --> 00:07:16,640
So issues related to fairness and stuff like that are really something that I tend to

93
00:07:16,640 --> 00:07:19,960
think about a lot in general.

94
00:07:19,960 --> 00:07:24,840
So I ended up making sort of this shift a little bit in my research focus, and that's

95
00:07:24,840 --> 00:07:32,280
not to say that I don't still work on things to do with core computational social science,

96
00:07:32,280 --> 00:07:39,120
but increasingly my research is focusing on the ways that machine learning impacts society.

97
00:07:39,120 --> 00:07:43,600
So fairness, accountability, transparency, and ethics.

98
00:07:43,600 --> 00:07:49,680
And we will certainly dive deep into those topics, but before we do, you've mentioned a couple

99
00:07:49,680 --> 00:07:52,360
of times the term computational social science.

100
00:07:52,360 --> 00:07:55,600
That's not a term that I've heard before, I don't believe.

101
00:07:55,600 --> 00:08:04,200
Can you, is that, I guess I'm curious how established that is as a field, or is it something

102
00:08:04,200 --> 00:08:09,280
that is specific to that institution that you are working at?

103
00:08:09,280 --> 00:08:10,280
Sure.

104
00:08:10,280 --> 00:08:19,800
So this is really a discipline that started emerging in maybe sort of 2009, 2008, that

105
00:08:19,800 --> 00:08:21,600
kind of time.

106
00:08:21,600 --> 00:08:29,760
By 2010, which is when I was hired at UMass, it really was sort of its own little emerging

107
00:08:29,760 --> 00:08:37,400
field with a bunch of different computer scientists and social scientists really committed to pushing

108
00:08:37,400 --> 00:08:39,480
this forward as a discipline.

109
00:08:39,480 --> 00:08:44,680
And the basic idea, of course, is that, you know, social scientists study society and social

110
00:08:44,680 --> 00:08:51,280
processes, and they've been doing this for decades, but often using qualitative methods.

111
00:08:51,280 --> 00:08:58,960
But of course, as more of society moves towards digitized interaction methods and online

112
00:08:58,960 --> 00:09:05,080
platforms and other kinds of things like that, we're beginning to see much more of this

113
00:09:05,080 --> 00:09:07,040
sort of digital data.

114
00:09:07,040 --> 00:09:11,440
At the same time, we've seen this massive increase, as I said, in the popularity of machine

115
00:09:11,440 --> 00:09:16,920
learning and machine learning methods that are really suitable for analyzing data about

116
00:09:16,920 --> 00:09:19,920
social processes and society.

117
00:09:19,920 --> 00:09:25,640
So computational social science is really this sort of emerging discipline at the intersection

118
00:09:25,640 --> 00:09:30,440
of computer science, the social sciences and statistics as well.

119
00:09:30,440 --> 00:09:36,200
And the real goal is to develop and use computational and statistical methods, so machine learning

120
00:09:36,200 --> 00:09:43,720
methods, for example, to understand society, social processes, and answer questions that

121
00:09:43,720 --> 00:09:47,680
are substantively interesting to social scientists.

122
00:09:47,680 --> 00:09:53,960
At this point, there are people at a number of different institutions focusing on computational

123
00:09:53,960 --> 00:09:54,960
social science.

124
00:09:54,960 --> 00:10:02,120
So yes, of course, UMass, as I mentioned before, but also Northwestern, Northeastern, University

125
00:10:02,120 --> 00:10:03,120
of Washington.

126
00:10:03,120 --> 00:10:07,640
In fact, we've been doing this for years, and of course, Microsoft Research is no exception

127
00:10:07,640 --> 00:10:08,960
in this regard.

128
00:10:08,960 --> 00:10:14,880
Part of the reason why I joined Microsoft Research was that we have a truly exceptional group

129
00:10:14,880 --> 00:10:20,240
of researchers in computational social science here, and that was really very appealing to

130
00:10:20,240 --> 00:10:21,240
me.

131
00:10:21,240 --> 00:10:23,440
Oh, awesome, awesome.

132
00:10:23,440 --> 00:10:32,320
So you talked about your transition to focusing on fairness, fairness accountability, transparency,

133
00:10:32,320 --> 00:10:37,200
and ethics in machine learning and AI.

134
00:10:37,200 --> 00:10:43,920
Can you talk a little bit about what those terms mean to you and your broader research?

135
00:10:43,920 --> 00:10:46,240
Yeah, absolutely.

136
00:10:46,240 --> 00:10:53,800
So I think the bulk of my own research in that sort of broad umbrella falls within two categories.

137
00:10:53,800 --> 00:11:00,840
So the first is fairness, and the second is what I would sort of describe as interpretability

138
00:11:00,840 --> 00:11:02,720
of machine learning.

139
00:11:02,720 --> 00:11:12,080
And so in that fairness bucket, really much of my research is focused on studying the ways

140
00:11:12,080 --> 00:11:19,520
in which machine learning can inadvertently harm or disadvantage groups of people or

141
00:11:19,520 --> 00:11:25,200
individual people in various different usually unintended ways.

142
00:11:25,200 --> 00:11:30,600
And I'm interested in understanding not only why this occurs, but what we can do to mitigate

143
00:11:30,600 --> 00:11:36,200
it, and what we can do to really develop fairer machine learning systems.

144
00:11:36,200 --> 00:11:44,600
The systems that don't inadvertently harm individuals or groups of people in the intelligibility

145
00:11:44,600 --> 00:11:45,600
bucket.

146
00:11:45,600 --> 00:11:53,760
So there, I'm really interested in how we can make machine learning methods that are

147
00:11:53,760 --> 00:11:59,200
interpretable to humans in different roles for particular purposes.

148
00:11:59,200 --> 00:12:06,000
And there's been a lot of research in this area over the past few years, focusing on oftentimes

149
00:12:06,000 --> 00:12:11,480
developing simple machine learning models that can be easily understood by humans simply

150
00:12:11,480 --> 00:12:18,160
by exposing their internals, and also on developing methods that can generate explanations

151
00:12:18,160 --> 00:12:23,840
for either entire models or the predictions of models, and those models might be potentially

152
00:12:23,840 --> 00:12:25,800
very complex.

153
00:12:25,800 --> 00:12:31,480
My own work typically focuses really more on the human side of intelligibility.

154
00:12:31,480 --> 00:12:38,640
So what is it that might make a system intelligible or interpretable to a human trying to carry

155
00:12:38,640 --> 00:12:40,440
out some particular task?

156
00:12:40,440 --> 00:12:45,880
And I do a lot of human subjects experiments to really try and understand some of those

157
00:12:45,880 --> 00:12:51,360
questions with a variety of different folks here at Microsoft Research.

158
00:12:51,360 --> 00:13:00,920
On the topic of fairness and avoiding inadvertent harm, there are a lot of examples that I think

159
00:13:00,920 --> 00:13:09,040
many of our audience would be familiar with the pro-publica work into the use of machine

160
00:13:09,040 --> 00:13:18,200
learning systems in the justice process and others are there examples that come to mind

161
00:13:18,200 --> 00:13:24,480
for you that are maybe less well-known, but that illustrate for you the importance of

162
00:13:24,480 --> 00:13:25,480
that type of work?

163
00:13:25,480 --> 00:13:26,480
Yeah.

164
00:13:26,480 --> 00:13:33,480
So when I typically think about this space, I tend to think about this in terms of the

165
00:13:33,480 --> 00:13:36,520
types of different harms that can occur.

166
00:13:36,520 --> 00:13:42,600
And I have some work with Aaron Shapiro, Solon Brocus, and Kate Crawford, on the different

167
00:13:42,600 --> 00:13:44,720
types of harms that can occur.

168
00:13:44,720 --> 00:13:49,720
And Kate Crawford actually did a fantastic job of talking about this work in her invited

169
00:13:49,720 --> 00:13:54,480
talk at the Nureps Conference in 2017.

170
00:13:54,480 --> 00:14:00,000
So to give you some concrete examples, so many of the examples that people are most familiar

171
00:14:00,000 --> 00:14:06,000
with are these scenarios, as you mentioned, where machine learning systems are being used

172
00:14:06,000 --> 00:14:12,440
to allocate or withhold resources, opportunities, or information.

173
00:14:12,440 --> 00:14:21,080
And so one example would be of the compass recidivism prediction system being used to make

174
00:14:21,080 --> 00:14:24,880
decisions about whether people should be released on bail.

175
00:14:24,880 --> 00:14:35,760
Another example would be from a new story that happened in November where Amazon revealed

176
00:14:35,760 --> 00:14:44,040
that it had abandoned a automated hiring tool because of fears that the tool would reinforce

177
00:14:44,040 --> 00:14:47,440
existing gender imbalances in the workplace.

178
00:14:47,440 --> 00:14:52,400
So there you're looking at these existing gender imbalances and seeing that this tool is

179
00:14:52,400 --> 00:14:58,520
perhaps withholding opportunities from women in the tech industry in an undesirable way.

180
00:14:58,520 --> 00:15:02,320
And there was a lot of coverage about this very sensible decision that Amazon made to

181
00:15:02,320 --> 00:15:04,880
abandon that tool.

182
00:15:04,880 --> 00:15:13,240
Some other examples would be more related to quality of service issues, even when no resources

183
00:15:13,240 --> 00:15:17,240
or opportunities are being allocated or withheld.

184
00:15:17,240 --> 00:15:22,440
So a great example there would be the work that Joy, Wall and Weenie and Timnick Gebrou

185
00:15:22,440 --> 00:15:32,080
did focusing on the ways that commercial gender classification systems might perform less

186
00:15:32,080 --> 00:15:36,520
well, so less accurate, for certain groups of people.

187
00:15:36,520 --> 00:15:41,040
As another example, you might think of let's say speech recognition systems, and you can

188
00:15:41,040 --> 00:15:46,160
imagine systems that work really well for people with certain types of accents or for

189
00:15:46,160 --> 00:15:50,240
people with voices at certain pitches, but less well for other people.

190
00:15:50,240 --> 00:15:56,040
Certainly for me, I'm British and I have a list and I know that oftentimes speech recognition

191
00:15:56,040 --> 00:16:00,240
systems don't do a great job of understanding what I'm saying.

192
00:16:00,240 --> 00:16:04,280
This is much less of an issue nowadays, but you know, five or so years ago, this was

193
00:16:04,280 --> 00:16:07,720
really frustrating for me.

194
00:16:07,720 --> 00:16:12,000
Some other examples are things like stereotyping.

195
00:16:12,000 --> 00:16:17,840
So here, the most famous example of stereotyping and machine learning is Latanya Sweeney's work

196
00:16:17,840 --> 00:16:26,720
from 2013, where she showed that advertisements that were being shown on web searches for different

197
00:16:26,720 --> 00:16:33,560
people's names would more typically be advertisements that reinforce stereotypes about

198
00:16:33,560 --> 00:16:39,440
black criminality when people searched for sort of black sounding names than when people

199
00:16:39,440 --> 00:16:42,800
searched for stereotypically white sounding names.

200
00:16:42,800 --> 00:16:48,760
So there, the issue is this sort of reinforcement of these negative stereotypes within society

201
00:16:48,760 --> 00:16:53,440
by the placement of particular ads for particular different types of searches.

202
00:16:53,440 --> 00:16:58,920
So another example of stereotyping and machine learning would be the work done by Joanna

203
00:16:58,920 --> 00:17:04,960
Brason and others at Princeton University on stereotypes and word embeddings.

204
00:17:04,960 --> 00:17:10,080
And there's also been some similar work done by my colleague Adam Kali here at Microsoft

205
00:17:10,080 --> 00:17:11,080
Research.

206
00:17:11,080 --> 00:17:16,960
And both of these groups of researchers showed that if you train word embedding methods,

207
00:17:16,960 --> 00:17:23,400
so things like word to veck that try and identify a low-dimensional embedding for word types

208
00:17:23,400 --> 00:17:29,000
based on the surrounding words that are typically used in conjunction with them in sentences,

209
00:17:29,000 --> 00:17:34,520
you end up seeing that these word embeddings reinforce existing gender stereotypes.

210
00:17:34,520 --> 00:17:42,360
For example, so the word man ends up being embedded much closer to programmer and similarly

211
00:17:42,360 --> 00:17:48,840
women women ends up being embedded much closer to homemaker than vice versa.

212
00:17:48,840 --> 00:17:51,920
So there would be another kind of example.

213
00:17:51,920 --> 00:17:56,840
And then we see other kinds of examples of unfairness at harms within machine learning

214
00:17:56,840 --> 00:17:57,840
as well.

215
00:17:57,840 --> 00:18:05,200
So for example, over an underrepresentation, so Matthew K and some others at the University

216
00:18:05,200 --> 00:18:10,720
of Washington have this really nice paper where they show that for professions with an equal

217
00:18:10,720 --> 00:18:17,400
or higher percentage of men than women, the image search results are much more heavily

218
00:18:17,400 --> 00:18:21,600
skewed towards images of men than reality.

219
00:18:21,600 --> 00:18:23,800
And so that would be another kind of example.

220
00:18:23,800 --> 00:18:28,600
And what you'll see from all of these examples that I've mentioned is that the effect

221
00:18:28,600 --> 00:18:35,000
of really wide range of systems and types of machine learning applications and the types

222
00:18:35,000 --> 00:18:40,880
of harms or unfairness that might occur are also pretty wide ranging as well, going from

223
00:18:40,880 --> 00:18:46,960
yes, sure, allocation or withholding of resources, opportunities of information, but moving

224
00:18:46,960 --> 00:18:52,080
beyond that to stereotyping and representation and so on.

225
00:18:52,080 --> 00:18:57,320
So often when thinking about fairness and bias and machine learning and the types of harm

226
00:18:57,320 --> 00:19:06,000
that can come about when unfair systems are developed, the kind of all roads lead back

227
00:19:06,000 --> 00:19:14,640
to the data itself and the biases that are inherent in that data given that machine

228
00:19:14,640 --> 00:19:23,880
learning and AI is so dependent on data and often much of the data that we have is bias.

229
00:19:23,880 --> 00:19:28,520
What can we do about that and what are the kinds of things that your research is exploring

230
00:19:28,520 --> 00:19:30,960
to help us address these issues?

231
00:19:30,960 --> 00:19:31,960
Absolutely.

232
00:19:31,960 --> 00:19:32,960
Yes.

233
00:19:32,960 --> 00:19:37,360
So you've hit on a really important point there, which is that in a lot of the sort of public

234
00:19:37,360 --> 00:19:42,960
discourse about fairness and machine learning, you have people making comments about algorithms

235
00:19:42,960 --> 00:19:49,200
being unfair or algorithms being biased and really I think this misses some of the most

236
00:19:49,200 --> 00:19:54,000
fundamental points about why this is such a challenging landscape and so I want to

237
00:19:54,000 --> 00:19:58,440
just emphasize a couple of those here in response to your question.

238
00:19:58,440 --> 00:20:05,920
So the first thing is that machine learning is all about taking data, finding patterns

239
00:20:05,920 --> 00:20:13,320
in that data and then often training systems to mimic the decisions that are represented

240
00:20:13,320 --> 00:20:19,480
within that data and of course we know that the society we live in is not fair.

241
00:20:19,480 --> 00:20:20,480
It is biased.

242
00:20:20,480 --> 00:20:25,600
There are structural disadvantages and discrimination all over the place.

243
00:20:25,600 --> 00:20:31,680
So it's pretty inevitable that if you take data from a society like that and then train

244
00:20:31,680 --> 00:20:38,280
machine learning systems to find patterns expressed in that data and to mimic the decisions

245
00:20:38,280 --> 00:20:45,280
made within that society, you will necessarily reproduce those structural disadvantages that

246
00:20:45,280 --> 00:20:48,160
bias, that discrimination and so on.

247
00:20:48,160 --> 00:20:52,400
So you're absolutely right that a lot of this does indeed come from data.

248
00:20:52,400 --> 00:20:59,560
But the other point that I want to make is that it's not just from data and it's not from

249
00:20:59,560 --> 00:21:05,480
algorithms per se, the issue is really as I see it and as my colleagues here at Microsoft

250
00:21:05,480 --> 00:21:12,160
Research see it, the issue is really about people and people's decisions at every point

251
00:21:12,160 --> 00:21:15,080
in that machine learning life cycle.

252
00:21:15,080 --> 00:21:21,720
So I've done some work on this with a number of people here at Microsoft.

253
00:21:21,720 --> 00:21:28,320
Most recently I put together actually a tutorial on machine learning and fairness in collaboration

254
00:21:28,320 --> 00:21:34,000
with my colleague, Jen Wartman Vaughn and the way we really think about this is that you

255
00:21:34,000 --> 00:21:39,400
have to prioritize fairness at every stage of that machine learning life cycle.

256
00:21:39,400 --> 00:21:42,360
You can't think about it as an afterthought.

257
00:21:42,360 --> 00:21:49,480
And the reason why is that decisions that we make at every stage can fundamentally impact

258
00:21:49,480 --> 00:21:53,480
whether or not a system treats people fairly.

259
00:21:53,480 --> 00:21:58,480
And so I think it's really important when we're thinking about fairness in machine learning

260
00:21:58,480 --> 00:22:03,720
to not just sort of make general statements about algorithms being unfair or systems being

261
00:22:03,720 --> 00:22:10,240
unfair, but really to go back to those particular points and think about how unfairness can

262
00:22:10,240 --> 00:22:13,880
kind of creep in at any one of those stages.

263
00:22:13,880 --> 00:22:17,600
And that might be as early as the task definition stage.

264
00:22:17,600 --> 00:22:21,640
So when you're sitting down to develop some machine learning system, it's really important

265
00:22:21,640 --> 00:22:28,640
to ask the question of who does this take power from and who does this give power to?

266
00:22:28,640 --> 00:22:33,400
And the answers to that question often reveal a lot about whether or not that technology

267
00:22:33,400 --> 00:22:36,800
should even be built in this first place.

268
00:22:36,800 --> 00:22:40,880
Sometimes the answer to addressing fairness in machine learning is simply no, we should

269
00:22:40,880 --> 00:22:43,760
not be building that technology.

270
00:22:43,760 --> 00:22:48,000
But there are all kinds of other decisions and assumptions at other points in that machine

271
00:22:48,000 --> 00:22:50,240
learning life cycle as well.

272
00:22:50,240 --> 00:22:57,040
So the way we typically like to think about it is that a machine learning model or method

273
00:22:57,040 --> 00:23:01,160
is effectively an abstraction of the world.

274
00:23:01,160 --> 00:23:06,000
And in making that abstraction, you necessarily have to make a bunch of assumptions about

275
00:23:06,000 --> 00:23:07,000
the world.

276
00:23:07,000 --> 00:23:11,520
And some of these assumptions will be more or less justified.

277
00:23:11,520 --> 00:23:16,080
Some of these assumptions will be better fits for the reality than others.

278
00:23:16,080 --> 00:23:21,640
But if you're not thinking really carefully about what those assumptions are when you're

279
00:23:21,640 --> 00:23:27,000
developing your machine learning system, this is one of the most obvious places that you

280
00:23:27,000 --> 00:23:32,640
can inadvertently end up introducing bias or unfairness.

281
00:23:32,640 --> 00:23:34,840
Can you give us some concrete examples there?

282
00:23:34,840 --> 00:23:37,160
Yeah, absolutely.

283
00:23:37,160 --> 00:23:44,360
One common example of this form would be stuff to do with teacher evaluation.

284
00:23:44,360 --> 00:23:49,160
So there have been a couple of sort of high profile lawsuits about this kind of thing.

285
00:23:49,160 --> 00:23:52,000
But I think it illustrates the point nicely.

286
00:23:52,000 --> 00:23:57,560
So it's common for teachers to be evaluated based on a number of different factors, but

287
00:23:57,560 --> 00:24:01,200
including their students test scores.

288
00:24:01,200 --> 00:24:06,720
And indeed, many of the methods that have been developed to analyze teacher quality using

289
00:24:06,720 --> 00:24:13,160
sort of machine learning systems have really focused predominantly on students test scores.

290
00:24:13,160 --> 00:24:19,080
But this assumes that students test scores are, in fact, an accurate predictor of teacher

291
00:24:19,080 --> 00:24:20,240
quality.

292
00:24:20,240 --> 00:24:22,600
And this isn't actually always the case.

293
00:24:22,600 --> 00:24:26,760
A good teacher should obviously do more than test prep.

294
00:24:26,760 --> 00:24:32,560
And so any system that really looks just at test scores when trying to predict teacher

295
00:24:32,560 --> 00:24:38,080
quality is going to do a bad job of capturing these other properties.

296
00:24:38,080 --> 00:24:40,560
So that would be one example.

297
00:24:40,560 --> 00:24:43,920
Another example involves predictive policing.

298
00:24:43,920 --> 00:24:49,680
So a predictive policing system might make predictions about where crimes will be committed

299
00:24:49,680 --> 00:24:52,400
based on historic arrest data.

300
00:24:52,400 --> 00:24:58,160
But an implicit assumption here is that the number of arrests in an area is an accurate

301
00:24:58,160 --> 00:25:00,640
proxy for the amount of crime.

302
00:25:00,640 --> 00:25:05,800
And it doesn't take into account the fact that policing practices can be racially biased

303
00:25:05,800 --> 00:25:11,480
or there might be historic over policing in less affluent neighborhoods.

304
00:25:11,480 --> 00:25:14,080
I'll give you another example as well.

305
00:25:14,080 --> 00:25:21,640
So many machine learning methods work by defining some objective function and then learning

306
00:25:21,640 --> 00:25:27,560
the parameters of the model so as to optimize that objective function.

307
00:25:27,560 --> 00:25:32,520
And so for example, if you define an objective function in the context of let's say a search

308
00:25:32,520 --> 00:25:41,400
engine that prioritizes user clicks, you may end up with search results that don't necessarily

309
00:25:41,400 --> 00:25:44,240
reflect what you want them to.

310
00:25:44,240 --> 00:25:49,880
And this is because users may click on certain types of search results over other search

311
00:25:49,880 --> 00:25:50,880
results.

312
00:25:50,880 --> 00:25:57,320
And that might not be reflective of what you want to be showing when you show users a

313
00:25:57,320 --> 00:25:59,400
page of search results.

314
00:25:59,400 --> 00:26:05,240
So as a concrete example, many search engines, if you search for the word boy, you see

315
00:26:05,240 --> 00:26:08,520
a bunch of pictures of male children.

316
00:26:08,520 --> 00:26:15,360
But if you search for the word girl, you see a bunch of pictures of grown ups, women.

317
00:26:15,360 --> 00:26:18,320
And these are pretty different to each other.

318
00:26:18,320 --> 00:26:23,280
And this probably comes from the fact that search engines typically optimize for clicks

319
00:26:23,280 --> 00:26:25,360
among other metrics.

320
00:26:25,360 --> 00:26:30,280
And this really shows how hard it can be to even address these kinds of fairness issues.

321
00:26:30,280 --> 00:26:35,760
Because in different circumstances, the word girl may be referring to a child or a woman

322
00:26:35,760 --> 00:26:39,000
and uses search for this term with different intentions.

323
00:26:39,000 --> 00:26:43,480
And in this particular example, as you can probably imagine, one of these intentions

324
00:26:43,480 --> 00:26:46,840
might be more prevalent than the other.

325
00:26:46,840 --> 00:26:54,200
You've identified lots of opportunities for pitfalls in the process of fielding systems

326
00:26:54,200 --> 00:27:00,960
going all the way back to just the way you define your system and state your intentions

327
00:27:00,960 --> 00:27:13,720
and formulate the problem that you're going after, beyond simply being mindful of the

328
00:27:13,720 --> 00:27:17,800
potential for bias and unfairness.

329
00:27:17,800 --> 00:27:22,040
And just saying simply, I realize that that's not simple.

330
00:27:22,040 --> 00:27:24,840
It's work to be mindful of this.

331
00:27:24,840 --> 00:27:31,280
But beyond that, what does your research offer in terms of how to overcome these kinds

332
00:27:31,280 --> 00:27:32,280
of issues?

333
00:27:32,280 --> 00:27:35,960
Yeah, this is a really good question.

334
00:27:35,960 --> 00:27:41,400
And it's a question that I get a lot from people is, what can we actually do in practice?

335
00:27:41,400 --> 00:27:46,240
And there are a number of things that can be done in practice, not all of them are easy

336
00:27:46,240 --> 00:27:48,360
things to do, as you say.

337
00:27:48,360 --> 00:27:54,200
So one of the most important things is that issues relating to fairness in machine learning

338
00:27:54,200 --> 00:28:00,360
are fundamentally sociotechnical and they're not going to be addressed by computer scientists

339
00:28:00,360 --> 00:28:02,480
or developers alone.

340
00:28:02,480 --> 00:28:08,840
It's really important to involve a range of diverse stakeholders in these conversations

341
00:28:08,840 --> 00:28:14,440
when we're developing machine learning systems so that we have a bunch of different perspectives

342
00:28:14,440 --> 00:28:15,680
represented.

343
00:28:15,680 --> 00:28:21,360
So moving beyond just involving computer scientists and developers on teams, it's really important

344
00:28:21,360 --> 00:28:28,000
that we involve social scientists, lawyers, policymakers, end users, people who are going

345
00:28:28,000 --> 00:28:33,560
to be affected or impacted by these systems down the line and so on and so forth.

346
00:28:33,560 --> 00:28:37,160
And that's one really concrete thing you can do.

347
00:28:37,160 --> 00:28:43,480
There's a project that came out of the University of Washington called the Diverse Voices Project.

348
00:28:43,480 --> 00:28:50,600
And it provides a way of getting feedback from stakeholders on tech policy documents and

349
00:28:50,600 --> 00:28:51,600
it's really good.

350
00:28:51,600 --> 00:28:55,280
They have a great how-to guide that I definitely recommend checking out.

351
00:28:55,280 --> 00:29:00,280
But many of the things that they recommend doing there, you can also think about when

352
00:29:00,280 --> 00:29:06,280
you're trying to get feedback from stakeholders on, let's say, the definition of a machine

353
00:29:06,280 --> 00:29:07,280
learning system.

354
00:29:07,280 --> 00:29:12,600
So that task definition stage, and some of these could even potentially be expanded to

355
00:29:12,600 --> 00:29:16,840
consider other stages of that machine learning pipeline as well.

356
00:29:16,840 --> 00:29:21,600
So there are a number of things that you can do at every single stage of the machine learning

357
00:29:21,600 --> 00:29:22,600
pipeline.

358
00:29:22,600 --> 00:29:27,840
And in fact, this this tutorial that I mentioned earlier that I worked on with my colleague

359
00:29:27,840 --> 00:29:33,080
Jen Warman-Von actually has guidelines for every single step of the pipeline.

360
00:29:33,080 --> 00:29:37,720
But to give you examples, here are some things for instance that you can do when you're selecting

361
00:29:37,720 --> 00:29:39,400
a data source.

362
00:29:39,400 --> 00:29:45,840
So for example, it's really important to think critically before even collecting any data.

363
00:29:45,840 --> 00:29:49,240
It's often very tempting to say, oh, there's already some data set that I can probably

364
00:29:49,240 --> 00:29:54,920
repurpose for this, but it's really important to take that step back and before immediately

365
00:29:54,920 --> 00:30:01,080
acting based on availability to actually think about whether that data source is appropriate

366
00:30:01,080 --> 00:30:04,120
for the task you want to use it for.

367
00:30:04,120 --> 00:30:06,200
And there's a number of reasons why it might not be.

368
00:30:06,200 --> 00:30:10,600
It could be to do with biases in the data source selection process.

369
00:30:10,600 --> 00:30:15,600
There might be societal biases present in the data source itself.

370
00:30:15,600 --> 00:30:19,400
It might be that the data source doesn't match the deployment context.

371
00:30:19,400 --> 00:30:24,040
That's a really important one that people really should be taking into account.

372
00:30:24,040 --> 00:30:27,400
Where are you thinking about deploying your machine learning system?

373
00:30:27,400 --> 00:30:33,720
And does the data you have availability for training and development match that context?

374
00:30:33,720 --> 00:30:37,080
Because another example is still related to data.

375
00:30:37,080 --> 00:30:42,520
It's really important to think about biases in the technology used to collect data.

376
00:30:42,520 --> 00:30:49,040
So as an example here, there was an app released in the city of Boston back in 2011.

377
00:30:49,040 --> 00:30:51,040
I think it was called Street Bump.

378
00:30:51,040 --> 00:30:57,760
And the way it worked is it used iPhone data and specifically sort of the positional movement

379
00:30:57,760 --> 00:31:03,280
of iPhones as people were driving around to gather data on where there were potholes

380
00:31:03,280 --> 00:31:05,680
that should be repaired by the city.

381
00:31:05,680 --> 00:31:10,000
But pretty quickly, the city of Boston figured out that this actually wasn't a great way to

382
00:31:10,000 --> 00:31:11,960
get that kind of data.

383
00:31:11,960 --> 00:31:18,120
Because back in 2011, the people who had iPhones were typically quite affluent and only lived

384
00:31:18,120 --> 00:31:19,960
in certain neighborhoods.

385
00:31:19,960 --> 00:31:25,360
So that would be an example of thinking carefully about the technology even used to collect data.

386
00:31:25,360 --> 00:31:30,520
It's also really important to make sure that there's sufficient representation of different

387
00:31:30,520 --> 00:31:37,240
subpopulations who might be ultimately using or affected by your machine learning system

388
00:31:37,240 --> 00:31:43,080
to make sure that you really do have good representation overall.

389
00:31:43,080 --> 00:31:47,200
Moving on to things like the model, there's a number of different things that you can do

390
00:31:47,200 --> 00:31:49,520
there, for instance, as well.

391
00:31:49,520 --> 00:31:55,440
So in the case of a model, I mentioned a bit about assumptions being really important.

392
00:31:55,440 --> 00:32:00,720
It's great to really clearly define all of your assumptions about the model, and then

393
00:32:00,720 --> 00:32:07,760
to question whether there might be any explicit or implicit biases present in those assumptions.

394
00:32:07,760 --> 00:32:12,120
That's a really important thing to do when you're thinking about choosing any particular

395
00:32:12,120 --> 00:32:14,720
model or model structure.

396
00:32:14,720 --> 00:32:22,960
You could even, in some scenarios, include some quantitative notion of parity, for instance,

397
00:32:22,960 --> 00:32:27,640
in your model objective function as well, and there've been a number of academic papers

398
00:32:27,640 --> 00:32:33,280
that take that approach in the literature over the past few years.

399
00:32:33,280 --> 00:32:35,880
Can you give an example of that last point?

400
00:32:35,880 --> 00:32:37,720
Yeah, sure.

401
00:32:37,720 --> 00:32:42,920
So imagine you have some kind of a machine learning classifier that's going to make decisions

402
00:32:42,920 --> 00:32:50,520
of the form, let's say, no loan, hire, no hire, bail, no bail, and so on.

403
00:32:50,520 --> 00:32:55,640
The way we normally develop these classifiers is to take a bunch of labeled data, so data

404
00:32:55,640 --> 00:33:02,360
points labeled with, let's say, loan, no loan, and then we train a model, a machine learning

405
00:33:02,360 --> 00:33:09,320
model, a classifier, to optimize accuracy on that training data.

406
00:33:09,320 --> 00:33:14,840
So you end up setting the parameters of that model such that it does a good job of accurately

407
00:33:14,840 --> 00:33:19,120
predicting those labels from the training data.

408
00:33:19,120 --> 00:33:26,800
So the objective function that's typically used is one that considers usually only accuracy.

409
00:33:26,800 --> 00:33:33,600
But something else you can do is define some quantitative definition of fairness, some

410
00:33:33,600 --> 00:33:41,440
quantitative fairness metric, and then try to simultaneously optimize both of these objectives,

411
00:33:41,440 --> 00:33:46,120
so classifier accuracy and whatever your chosen fairness metric is.

412
00:33:46,120 --> 00:33:50,160
And there's a number of these different quantitative metrics that have been proposed

413
00:33:50,160 --> 00:33:51,160
out there.

414
00:33:51,160 --> 00:33:56,000
They're all typically looking at sort of parity across groups of some sort.

415
00:33:56,000 --> 00:33:59,760
So I think it's really important to remember that even though these are often referred

416
00:33:59,760 --> 00:34:05,640
to as fairness metrics, they're really parity metrics, and then neglect many of the really

417
00:34:05,640 --> 00:34:11,960
important other aspects of fairness, like justice and due process and so on and so forth.

418
00:34:11,960 --> 00:34:17,520
But it is absolutely possible to take these parity metrics and to incorporate them into

419
00:34:17,520 --> 00:34:23,720
the objective function of say a classifier and then to try and prioritize satisfying and

420
00:34:23,720 --> 00:34:29,800
optimizing that fairness metric at the same time as optimizing classifier accuracy.

421
00:34:29,800 --> 00:34:34,240
There have been a number of papers that focus on this kind of approach.

422
00:34:34,240 --> 00:34:40,600
Many of them will focus on one particular type of classifier, so like SVMs or neural networks

423
00:34:40,600 --> 00:34:44,560
or something like that, and one particular fairness metric.

424
00:34:44,560 --> 00:34:48,400
And there are a bunch of standard fairness metrics that people like to look at.

425
00:34:48,400 --> 00:34:54,080
I actually have some work with some colleagues here at Microsoft where we have a slightly

426
00:34:54,080 --> 00:35:01,120
more general way of doing this that will work with many different types of classifiers

427
00:35:01,120 --> 00:35:03,960
and many different types of fairness metrics.

428
00:35:03,960 --> 00:35:08,680
So there's no reason to start again from scratch if you want to switch to a different

429
00:35:08,680 --> 00:35:11,760
classifier or a different fairness metric.

430
00:35:11,760 --> 00:35:17,440
We actually have some open source Python code available on GitHub that implements our approach.

431
00:35:17,440 --> 00:35:26,680
So you've talked about the idea that kind of people are fundamentally the root of the

432
00:35:26,680 --> 00:35:35,080
issue that these are societal issues that they're not going to be solved by technological

433
00:35:35,080 --> 00:35:39,880
advancements or processes alone.

434
00:35:39,880 --> 00:35:47,240
At the same time, there's been a ton of new research happening in this area by folks

435
00:35:47,240 --> 00:35:49,680
in your group and elsewhere.

436
00:35:49,680 --> 00:35:55,920
Does that lead to a mismatch between what's happening in academia and on the technical

437
00:35:55,920 --> 00:36:00,600
side with the way this stuff actually gets put into practice?

438
00:36:00,600 --> 00:36:03,720
That's an awesome question.

439
00:36:03,720 --> 00:36:10,240
The simple answer is yes, and this actually relates to one of my most recent research projects

440
00:36:10,240 --> 00:36:13,360
which I'm really, really excited about.

441
00:36:13,360 --> 00:36:20,400
So last summer, some of my colleagues and I, specifically Gen Warman Vaughan, Merodudeck

442
00:36:20,400 --> 00:36:27,800
and Hal Damay, along with our incredible intern Ken Holstein from CMU, conducted the first

443
00:36:27,800 --> 00:36:35,960
systematic investigation of industry practitioners' challenges and needs for support relating

444
00:36:35,960 --> 00:36:39,600
to developing fairer machine learning systems.

445
00:36:39,600 --> 00:36:44,920
And this work actually came about because we were thinking about ways of developing interfaces

446
00:36:44,920 --> 00:36:51,000
for that fair classification work that I mentioned a minute ago and through a number of conversations

447
00:36:51,000 --> 00:36:55,920
with people in different product groups here at Microsoft and people at other companies,

448
00:36:55,920 --> 00:37:02,280
we realized that these kinds of classification tasks, while they're incredibly well studied

449
00:37:02,280 --> 00:37:07,960
within the fairness in machine learning literature, are maybe less common than we had thought

450
00:37:07,960 --> 00:37:10,520
in practice within industry.

451
00:37:10,520 --> 00:37:14,480
And so that got us thinking about whether there might be actually a mismatch between the

452
00:37:14,480 --> 00:37:20,720
academic literature on fairness in machine learning and practitioner's actual needs.

453
00:37:20,720 --> 00:37:25,160
What we ended up doing was a super interesting research project.

454
00:37:25,160 --> 00:37:29,560
It was a pretty different style of research for me and for my colleagues.

455
00:37:29,560 --> 00:37:36,400
So I am a machine learning researcher, so is Gen, so is Hal and so is Merod, Ken, our

456
00:37:36,400 --> 00:37:38,880
intern is an HCI researcher.

457
00:37:38,880 --> 00:37:45,840
And what we ended up doing was this qualitative HCI work to really understand what it is that

458
00:37:45,840 --> 00:37:52,880
practitioners are facing in reality when they try and develop fairer machine learning systems.

459
00:37:52,880 --> 00:38:01,000
And to do this, we conducted semi-structured interviews with 35 people spanning 25 different

460
00:38:01,000 --> 00:38:04,280
teams in 10 different companies.

461
00:38:04,280 --> 00:38:09,800
And these people were in a number of different roles ranging from social scientists, data

462
00:38:09,800 --> 00:38:16,560
labeler, product manager, program manager to data scientist and researcher.

463
00:38:16,560 --> 00:38:21,080
And where possible we tried to interview multiple people from the same team in order

464
00:38:21,080 --> 00:38:27,240
to get a variety of perspectives on that team's challenges and needs for support.

465
00:38:27,240 --> 00:38:34,840
We then took our findings from these interviews and developed a survey which was then completed

466
00:38:34,840 --> 00:38:42,240
by another 267 industry practitioners, again in a variety of different companies and a variety

467
00:38:42,240 --> 00:38:43,480
of different roles.

468
00:38:43,480 --> 00:38:49,400
And what we found at a high level was that yes, there is a mismatch between the academic

469
00:38:49,400 --> 00:38:55,320
literature on fairness and machine learning and industry practitioners actual challenges

470
00:38:55,320 --> 00:38:58,040
and needs for support on the ground.

471
00:38:58,040 --> 00:39:04,920
So firstly, much of the machine learning literature on fairness focuses on classification

472
00:39:04,920 --> 00:39:07,920
and on supervised machine learning methods.

473
00:39:07,920 --> 00:39:14,320
In fact, what we found is that industry practitioners are grappling with fairness issues in a much

474
00:39:14,320 --> 00:39:20,040
wider range of applications beyond classification or prediction scenarios.

475
00:39:20,040 --> 00:39:25,000
And in fact, many times the systems they're dealing with involve these really rich, complex

476
00:39:25,000 --> 00:39:28,640
interactions between users and the system.

477
00:39:28,640 --> 00:39:35,560
So for example, chatbots or adaptive tutoring or personalized retail and so on and so forth.

478
00:39:35,560 --> 00:39:41,800
So as a result, they often struggle to use existing fairness research from the literature

479
00:39:41,800 --> 00:39:46,600
because the things that they're facing are much less amenable to these quantitative fairness

480
00:39:46,600 --> 00:39:47,600
metrics.

481
00:39:47,600 --> 00:39:54,000
And indeed, very few teams have fairness KPIs or automated tests that they can use within

482
00:39:54,000 --> 00:39:55,840
their domain.

483
00:39:55,840 --> 00:39:59,920
One of the other things that we found is that the machine learning literature typically

484
00:39:59,920 --> 00:40:06,520
assumes access to sensitive attributes like race or gender for the purpose of auditing

485
00:40:06,520 --> 00:40:08,320
systems for fairness.

486
00:40:08,320 --> 00:40:13,000
But in practice, many teams have no access to these kinds of attributes and certainly

487
00:40:13,000 --> 00:40:15,840
not at the level of individuals.

488
00:40:15,840 --> 00:40:21,760
So they express needs for support in detecting biases in unfairness with access only to

489
00:40:21,760 --> 00:40:26,200
coarse grained partial or indirect information.

490
00:40:26,200 --> 00:40:31,280
And this is something that we've seen much less focus on in the academic literature.

491
00:40:31,280 --> 00:40:37,600
That last point is an interesting one and one that I've brought up on the podcast previously

492
00:40:37,600 --> 00:40:45,040
and many of the places you might want to use an approach like that is forbidden from a

493
00:40:45,040 --> 00:40:50,160
regulatory perspective to use the information that you want to use in your classifier to

494
00:40:50,160 --> 00:40:54,360
achieve fairness in any part of the decisioning process.

495
00:40:54,360 --> 00:40:55,360
Exactly.

496
00:40:55,360 --> 00:41:01,080
This sets up this really difficult tension between doing the right thing in practice from

497
00:41:01,080 --> 00:41:05,360
a machine learning perspective and what is legally allowed.

498
00:41:05,360 --> 00:41:11,280
And I'm actually working on a paper at the moment with a lawyer, Zach Hanard, actually

499
00:41:11,280 --> 00:41:16,920
a law student, Zach Hanard at Stanford University, on exactly this issue, this challenge between

500
00:41:16,920 --> 00:41:21,480
what you want to do from a machine learning perspective and what you were required to do

501
00:41:21,480 --> 00:41:28,440
from a legal perspective based on humans and how humans behave and hundreds of years

502
00:41:28,440 --> 00:41:30,800
of law in that realm.

503
00:41:30,800 --> 00:41:36,040
It's really challenging and there is this complicated trade off there that we really need to

504
00:41:36,040 --> 00:41:37,760
be thinking about.

505
00:41:37,760 --> 00:41:45,040
It does make me wonder if techniques like or analogous to a differential privacy or something

506
00:41:45,040 --> 00:41:53,920
like that could be used to provide kind of a regulatory, acceptable way to access protected

507
00:41:53,920 --> 00:41:57,800
attributes so that they can be incorporated into algorithms like this.

508
00:41:57,800 --> 00:42:05,000
So there was some work on exactly this kind of topic so at the Fatem L workshop co-located

509
00:42:05,000 --> 00:42:07,680
with ICML last year.

510
00:42:07,680 --> 00:42:15,240
And this work was proposing the use of encryption and such like in order to collect and make

511
00:42:15,240 --> 00:42:20,600
available such information, but in a way that users would feel as if their privacy were

512
00:42:20,600 --> 00:42:26,440
being respected and so that people who wanted to use that information would be able to use

513
00:42:26,440 --> 00:42:30,640
it for purposes such as auditing.

514
00:42:30,640 --> 00:42:34,880
And I think that's a really promising approach, although there's obviously a bunch of non-trivial

515
00:42:34,880 --> 00:42:39,640
challenges involved in thinking about how you might make that a reality.

516
00:42:39,640 --> 00:42:44,600
It's a really complicated landscape, but definitely one that's worth thinking about.

517
00:42:44,600 --> 00:42:48,000
Was there a third area that you were about to mention?

518
00:42:48,000 --> 00:42:49,000
Yeah.

519
00:42:49,000 --> 00:42:55,320
So one of the main themes that we found in our work studying industry practitioners is

520
00:42:55,320 --> 00:43:02,920
a real mismatch between the focus on different points in the machine learning life cycle.

521
00:43:02,920 --> 00:43:09,680
So the machine learning literature typically assumes no agency over data collection.

522
00:43:09,680 --> 00:43:10,680
And this makes sense, right?

523
00:43:10,680 --> 00:43:14,560
If you're a machine learning academic, you typically work with standard data sets that

524
00:43:14,560 --> 00:43:18,480
have been collected and made available for years, you don't typically think about having

525
00:43:18,480 --> 00:43:21,560
agency over that data collection process.

526
00:43:21,560 --> 00:43:26,680
And of course, in industry, that's exactly where practitioners often do have the most

527
00:43:26,680 --> 00:43:27,680
control.

528
00:43:27,680 --> 00:43:31,880
They are in charge of that data collection or data curation process.

529
00:43:31,880 --> 00:43:37,240
And in contrast, they often have much less control over the methods or models themselves,

530
00:43:37,240 --> 00:43:42,720
which often are embedded within much bigger systems, so it's much harder to intervene

531
00:43:42,720 --> 00:43:48,000
from a perspective of fairness with the models than it is with the data.

532
00:43:48,000 --> 00:43:52,720
We found that really interesting, the sort of difference in emphasis between models versus

533
00:43:52,720 --> 00:43:55,680
data in these different groups of people.

534
00:43:55,680 --> 00:44:02,480
And of course, many practitioners voiced needs for support in figuring out how to leverage

535
00:44:02,480 --> 00:44:08,760
that sort of agency over data collection to create fairer data sets for use in developing

536
00:44:08,760 --> 00:44:10,520
their systems.

537
00:44:10,520 --> 00:44:14,000
So you mentioned the FET ML workshop.

538
00:44:14,000 --> 00:44:23,160
I'm wondering as we come to a close, if there are any resources, events, pointers, I'm sure

539
00:44:23,160 --> 00:44:26,440
there are tons of things that you'd love to point people at.

540
00:44:26,440 --> 00:44:32,880
But what are your top three or four things that you would suggest people take a look at

541
00:44:32,880 --> 00:44:40,280
as they're trying to wrap their heads around this area and how to either have an impact

542
00:44:40,280 --> 00:44:45,080
as a researcher or how to make good use of it as a practitioner?

543
00:44:45,080 --> 00:44:46,080
Yeah.

544
00:44:46,080 --> 00:44:47,080
Absolutely.

545
00:44:47,080 --> 00:44:51,840
So there are a number of different places with resources to learn more about this kind

546
00:44:51,840 --> 00:44:53,000
of stuff.

547
00:44:53,000 --> 00:44:57,800
So first, I've mentioned a couple of times this tutorial that I put together with Jen

548
00:44:57,800 --> 00:45:04,080
Wartman Vaughn, that will be available publicly online very soon.

549
00:45:04,080 --> 00:45:08,360
It is in fact being broadcast next week, so it should be up by the time this podcast

550
00:45:08,360 --> 00:45:09,560
goes live.

551
00:45:09,560 --> 00:45:14,600
So I would definitely recommend that people check that out to really get a sense of how

552
00:45:14,600 --> 00:45:19,280
we at Microsoft are thinking about fairness and machine learning.

553
00:45:19,280 --> 00:45:24,560
Then moving beyond that and thinking specifically on more of the academic literature, the FET ML

554
00:45:24,560 --> 00:45:30,320
workshop maintains a list of resources on the workshop website.

555
00:45:30,320 --> 00:45:36,160
And that's again, another really, really great place to look for things to read about

556
00:45:36,160 --> 00:45:38,240
this topic.

557
00:45:38,240 --> 00:45:45,920
The FET Star Conference is a relatively newly created conference on fairness, accountability

558
00:45:45,920 --> 00:45:52,360
and transparency, not just in machine learning, but across all of computer science and

559
00:45:52,360 --> 00:45:54,280
computational systems.

560
00:45:54,280 --> 00:45:58,600
And again, there, I recommend checking out the website to see the publications that were

561
00:45:58,600 --> 00:46:02,520
there last year, and also the publications that will be there this year.

562
00:46:02,520 --> 00:46:07,000
There's a number of really interesting papers that I haven't read yet, but I'm super excited

563
00:46:07,000 --> 00:46:10,120
to read being presented at this year's conference.

564
00:46:10,120 --> 00:46:15,920
That conference also has tutorials on a range of different subjects.

565
00:46:15,920 --> 00:46:20,360
And so it's also worth looking at the various different tutorials there.

566
00:46:20,360 --> 00:46:26,880
So at last year's conference, Ivan Narayanan presented this amazing tutorial on quantitative

567
00:46:26,880 --> 00:46:33,880
fairness metrics and why they're not a one-size-fits-all solution, why there are trade-offs between

568
00:46:33,880 --> 00:46:39,720
them, why you can't just sort of take one of these definitions, optimize for it and call

569
00:46:39,720 --> 00:46:40,720
it quits.

570
00:46:40,720 --> 00:46:44,320
And so I definitely recommend checking that out.

571
00:46:44,320 --> 00:46:49,800
Some other places that are worth looking for resources on this.

572
00:46:49,800 --> 00:46:56,640
The AINow Institute, which was co-founded by Kate Crawford, who's also here at Microsoft

573
00:46:56,640 --> 00:47:02,800
Research and Meredith Whitaker, who's also at Google, also has some incredibly awesome

574
00:47:02,800 --> 00:47:03,800
resources.

575
00:47:03,800 --> 00:47:08,200
They've put out a number of white papers and reports over the past couple of years that

576
00:47:08,200 --> 00:47:13,680
really get at the crux of why these are complicated socio-technical issues.

577
00:47:13,680 --> 00:47:19,120
And so I strongly recommend reading pretty much everything that they put out.

578
00:47:19,120 --> 00:47:25,560
I would also recommend checking out some of the material put out by Data and Society,

579
00:47:25,560 --> 00:47:31,200
which is also an organization here in New York, led by Dana Boyd.

580
00:47:31,200 --> 00:47:37,240
And they, too, have a number of really interesting things that you can read about these different

581
00:47:37,240 --> 00:47:38,240
topics.

582
00:47:38,240 --> 00:47:43,680
And then the final thing I want to emphasize is the partnership on AI, which was formed

583
00:47:43,680 --> 00:47:49,480
a couple of years ago by Microsoft and a bunch of other companies working in the space

584
00:47:49,480 --> 00:47:57,880
of AI, to really foster cross-company collaboration and moving forward in this space when thinking

585
00:47:57,880 --> 00:48:03,960
about these complicated societal issues that relate to AI and machine learning.

586
00:48:03,960 --> 00:48:08,160
And so the partnership has been really ramping up over the past couple of years.

587
00:48:08,160 --> 00:48:11,600
And they also have some good resources that are worth checking out.

588
00:48:11,600 --> 00:48:12,600
Oh, that's great.

589
00:48:12,600 --> 00:48:17,000
That is a great list that will keep us busy for a while.

590
00:48:17,000 --> 00:48:21,200
Hannah, thank you so much for taking the time to chat with us.

591
00:48:21,200 --> 00:48:24,000
It was really a great conversation and I appreciate it.

592
00:48:24,000 --> 00:48:25,000
No problem.

593
00:48:25,000 --> 00:48:26,000
Thank you for having me.

594
00:48:26,000 --> 00:48:28,160
This has been really great.

595
00:48:28,160 --> 00:48:29,160
Awesome.

596
00:48:29,160 --> 00:48:30,160
Thank you.

597
00:48:30,160 --> 00:48:35,200
All right, everyone, that's our show for today.

598
00:48:35,200 --> 00:48:40,840
For more information on Hannah or any of the topics covered in this show, visit twimmelai.com

599
00:48:40,840 --> 00:48:43,680
slash talk slash two, three, two.

600
00:48:43,680 --> 00:48:48,920
To follow along with the AI for the benefit of society series, visit twimmelai.com slash

601
00:48:48,920 --> 00:48:51,000
AI for society.

602
00:48:51,000 --> 00:48:58,000
As always, thanks so much for listening and catch you next time.

