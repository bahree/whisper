1
00:00:00,000 --> 00:00:19,440
All right. So once again, I am at NYU Future Labs with the AI Nexus companies. And this time,

2
00:00:19,440 --> 00:00:29,040
I'm with Muticia Nunda, the co-founder and CEO of Alpha Vertex. Say hi. Hi, thanks. Thanks for having

3
00:00:29,040 --> 00:00:38,240
me. So why don't we get started by having you tell us a little text and what you guys are up to?

4
00:00:38,240 --> 00:00:46,320
Sure. So Alpha Vertex is an innovative financial technology company that is using the next generation of

5
00:00:46,320 --> 00:00:55,280
AI tools to help support investing around the world. Practically what that means is that we develop

6
00:00:55,280 --> 00:01:00,560
predictive models that try to forecast the returns of stocks over multiple time horizons.

7
00:01:01,200 --> 00:01:08,400
And at present, we cover all major markets around the world. So we cover 30,000 stocks globally,

8
00:01:08,400 --> 00:01:15,280
which represent about 95% of the investable universe. Okay. And so presumably you came out of

9
00:01:15,280 --> 00:01:21,360
financial services prior to this? Yeah. So my background is in finance, I've spent over 17 years,

10
00:01:21,360 --> 00:01:29,040
I started out in investment banking, did private equity and ended up as the chief of staff of one

11
00:01:29,040 --> 00:01:33,760
of the largest proprietary market-making businesses in the world at a company called Sescohana International

12
00:01:33,760 --> 00:01:38,880
Group. And then after leaving that, I spent five years at Bloomberg as the head of strategy and

13
00:01:38,880 --> 00:01:44,160
business development for Bloomberg enterprise solutions. Okay. So I've kind of done my tour of duty

14
00:01:44,160 --> 00:01:48,480
on Wall Street and around Wall Street. You've got quite a few punches on your ticket.

15
00:01:48,480 --> 00:01:54,240
You know, it strikes me that of all the companies that I've talked to

16
00:01:55,600 --> 00:02:04,720
here at ANX this today, the trying to help people make better trades with financial data is

17
00:02:05,600 --> 00:02:14,560
there's nothing new about it. Right? That is a classical idea and lots of people have tried to apply,

18
00:02:14,560 --> 00:02:20,720
you know, we've been trying to apply analytics to this for many, many years. What is new and different

19
00:02:20,720 --> 00:02:26,800
about your approach to it? So I'll first just start by sort of saying you're absolutely right that

20
00:02:26,800 --> 00:02:33,520
it is not a new idea because it's not a new problem, but the present moment there's about 98% of

21
00:02:33,520 --> 00:02:39,760
all funds underperform their benchmark over 10 years. And the simple reason is that they rely on

22
00:02:39,760 --> 00:02:46,480
kind of traditional portfolio managers to act as stockpickers. And what happens there is that you

23
00:02:46,480 --> 00:02:52,640
have maybe 1% of all fund managers are kind of really good at making money, but then the 99%

24
00:02:53,280 --> 00:02:59,920
of the population ends up underperforming. And so you're subject to kind of the talent that you

25
00:02:59,920 --> 00:03:06,720
bring in. As financial markets get more and more complicated, you know, there's about 2.5 billion

26
00:03:06,720 --> 00:03:11,840
bytes of data nowadays that people kind of have to keep track of. It's increasingly difficult

27
00:03:11,840 --> 00:03:17,200
for a human being to sort of incorporate all of this information in a useful way into the investment

28
00:03:17,200 --> 00:03:22,960
strategy. So with the onset of, you know, innovations in machine learning and AI in general,

29
00:03:23,760 --> 00:03:30,000
you know, we're able to build models that can capture all of these pieces of information, much

30
00:03:30,000 --> 00:03:36,560
similar to the butterfly effect. So if you think about, we have AI models that sort of will see

31
00:03:36,560 --> 00:03:42,640
things that are happening in Asia and then model how they ripple through global markets to affect

32
00:03:42,640 --> 00:03:48,560
a stock like Apple or Google in the US. It's really, really challenging to be able to do that as a

33
00:03:48,560 --> 00:03:54,800
human being or to build a model that can capture that complexity. So that's sort of what we're doing

34
00:03:54,800 --> 00:04:00,800
that's completely different is we have these very sophisticated models that can kind of look out

35
00:04:00,800 --> 00:04:06,320
to 10 or, you know, 20 degrees of separation away and analyze how things that are going on very

36
00:04:06,320 --> 00:04:10,640
in very distant places can kind of ultimately come back to impact stocks.

37
00:04:15,120 --> 00:04:20,560
So that the, I mean, certainly the hedge funds have been trying to apply this type of stuff for

38
00:04:20,560 --> 00:04:27,440
a very long time as part of what you're doing, bringing the, you know, the quant type of technology

39
00:04:27,440 --> 00:04:33,920
that hedge funds have been doing to the traditional investors, the institutional side.

40
00:04:34,480 --> 00:04:39,680
Yeah, so we, you know, interestingly enough, the earliest adopters of our technology are obviously

41
00:04:39,680 --> 00:04:45,440
the quantitative of systematic hedge funds. They kind of understand how powerful this technology is

42
00:04:45,440 --> 00:04:51,200
and, you know, they employ data driven technology based investment strategies. And so this to them

43
00:04:51,200 --> 00:04:57,280
is a high value signal that is additive to maybe other things that they do on a proprietary basis.

44
00:04:58,160 --> 00:05:02,960
Ultimately, over the long run, what we want to do is make these tools kind of more broadly

45
00:05:02,960 --> 00:05:08,800
accessible to the 99% of the investment managers out there that don't have, you know,

46
00:05:08,800 --> 00:05:15,600
this technology driven investment platforms. So we're working on things like having, you know,

47
00:05:15,600 --> 00:05:21,360
kind of machine learning driven institutional class research and analytics facilities that,

48
00:05:21,360 --> 00:05:27,440
you know, can be accessed via natural language. So you don't have to have the technical capability

49
00:05:27,440 --> 00:05:33,680
of a data scientist or a statistician or a PhD person to be able to do the types of analysis

50
00:05:33,680 --> 00:05:40,080
that we're going to be developing. So I think, you know, in the near term, the earlier adopters

51
00:05:40,080 --> 00:05:44,640
are the more quantitative and hedge funds, but over the long run, we want to go after everybody

52
00:05:44,640 --> 00:05:52,240
in the space. Okay, so you guys are, it's not so much arming the folks that can't keep up with the

53
00:05:52,240 --> 00:06:03,840
quants is that you guys have developed some really cool stuff that the quants need to. Okay, and so, um, thinking about how one might

54
00:06:03,840 --> 00:06:13,200
try to build a machine learning AI, fly effect, that sounds super complex. Like how do you, what's the,

55
00:06:14,000 --> 00:06:19,920
what's the approach there? So it is actually a pretty huge problem. The first thing is you have to try

56
00:06:19,920 --> 00:06:24,880
to represent all the world's financial information in a way that makes sense. Oh, well, that's easy.

57
00:06:28,080 --> 00:06:34,640
So the approach that we've taken is we've modeled everything as a computational graph. And for people

58
00:06:34,640 --> 00:06:41,120
who aren't familiar with that is literally this multi-dimensional relationship map of how everything

59
00:06:41,120 --> 00:06:48,080
is interconnected to everything else. So you can describe things like Apple competes with Samsung,

60
00:06:48,080 --> 00:06:53,600
Apple is a customer of Samsung's and Apple is suing Samsung and all of these are relationships

61
00:06:53,600 --> 00:06:59,200
between these two nodes or these two entities. In a traditional model, you can't represent these

62
00:06:59,200 --> 00:07:05,680
multi-dimensional relationships across companies. And in our case, like in the case of where Apple

63
00:07:05,680 --> 00:07:12,160
is suing Samsung, Apple, the relationship has a direction. So Apple is instigating the legal

64
00:07:12,160 --> 00:07:17,600
lawsuit on Samsung and we can encode the value of the lawsuit, how long it's been going on.

65
00:07:17,600 --> 00:07:22,400
And then you can actually do computations off of all of these connections between companies.

66
00:07:23,440 --> 00:07:28,960
But to do this, you have to monitor all the world's information, not just financial information.

67
00:07:30,240 --> 00:07:36,000
So to be able to just do that, we monitor over a million pieces of information in a given day.

68
00:07:36,560 --> 00:07:42,960
And these are across all types of sources. So we look at structured data sets like financial

69
00:07:42,960 --> 00:07:49,760
market data, we look at unstructured data sets such as regulatory filings, would say the SEC,

70
00:07:49,760 --> 00:07:57,120
the FDA, whatever. What companies are saying in press releases, news articles, we read about 300,000

71
00:07:57,120 --> 00:08:03,760
sources of news in a given day. And one of the things we had to build early on was this ability for

72
00:08:03,760 --> 00:08:09,680
the machine to understand the news on its own, extract relationships that it thought were

73
00:08:09,680 --> 00:08:14,480
meaningful, and then also weigh the quality of that source of information. So not all things

74
00:08:14,480 --> 00:08:21,120
are really created equal. If I read something on a blog in Asia, it might be actually a really

75
00:08:21,120 --> 00:08:27,600
high quality source or it may not be. And so you have to make these autonomous machines

76
00:08:27,600 --> 00:08:31,440
have the capability to sort of weigh the quality of the source of that information.

77
00:08:32,240 --> 00:08:37,040
And once it's extracted and you're putting it into this computational graph,

78
00:08:37,040 --> 00:08:42,080
you know, it has to say that I think this is actually a really good fact. Or if it's not,

79
00:08:42,080 --> 00:08:47,520
then it says it's not a high quality fact. And I need to wait on confirming or additional sources

80
00:08:47,520 --> 00:08:52,560
of, you know, to confirm this relationship that I just found. And that's just the starting point,

81
00:08:52,560 --> 00:09:01,040
right? So we cover, I guess 80,000 publicly traded companies around the world. We monitor 15

82
00:09:01,040 --> 00:09:08,000
million private companies and we track about 200 million people that work at these companies as

83
00:09:08,000 --> 00:09:13,120
a starting point. And then from there, we try to figure out all their products, who they do

84
00:09:13,120 --> 00:09:17,040
business with, whether they're contracting with other companies or with the government,

85
00:09:17,920 --> 00:09:23,360
any kind of litigation that may be going on, who their investors are and what their investors

86
00:09:23,360 --> 00:09:27,280
are doing. So, you know, it's just spider web of knowledge that we have to build

87
00:09:27,280 --> 00:09:34,400
before we can actually do a computation. Right, right. To your, there's so much in there.

88
00:09:34,400 --> 00:09:43,840
Like, just the issue of trying to identify the newsworthiness, the trust level associated

89
00:09:43,840 --> 00:09:48,880
with a given site. I mean, that's Google, right? It's PageRank, right? Are you using something

90
00:09:48,880 --> 00:09:55,040
similar to PageRank in? Yes, but no. So, I mean, Google's an amazing

91
00:09:55,040 --> 00:10:01,200
piece of technology. But, you know, again, with machine learning and a lot of things that we do,

92
00:10:01,200 --> 00:10:07,840
the domain knowledge is extremely important. So, Google doesn't have PageRank for finance.

93
00:10:08,720 --> 00:10:15,280
Right. So, we kind of have to build that ourselves. The good thing is, I guess Google tries to

94
00:10:15,280 --> 00:10:21,040
index the world. We just need to index the financial world. So, it's a subset of what Google does.

95
00:10:21,040 --> 00:10:26,800
But, we can take a lot of the same concepts from what Google does and incorporate that into

96
00:10:26,800 --> 00:10:32,480
our own algorithms. One of the things that we've done is we apply like this machine learning

97
00:10:32,480 --> 00:10:37,040
approach to teaching things whether or not this is a high quality source. So, we can give

98
00:10:37,600 --> 00:10:42,320
the AI good examples of what high quality, you know, like this is the Wall Street Journal,

99
00:10:42,320 --> 00:10:46,400
this is a high quality source. These are the types of relationships of pieces of information

100
00:10:46,400 --> 00:10:52,080
that come from this source versus here as a, you know, like a known-in blog. And then the machine

101
00:10:52,080 --> 00:10:56,800
could kind of extrapolate from that. So, it's a supervised learning problem, as opposed to

102
00:10:57,520 --> 00:11:01,840
go out and index all of these financial sites and figure out which ones are high quality

103
00:11:02,640 --> 00:11:08,640
the way Google. Yeah, because Google basically has boiled the ocean ahead of time. And we kind of,

104
00:11:08,640 --> 00:11:13,920
when we come across new stuff, we make a determination at that point in time. Right, right.

105
00:11:13,920 --> 00:11:19,680
That's super interesting. Tell us about some of the other interesting kind of machine learning

106
00:11:19,680 --> 00:11:26,720
problems that you... Yeah. So, yeah. So, the other thing that is actually really, really hard to do

107
00:11:26,720 --> 00:11:32,400
is kind of twofold. The first one is entity resolution. So, you're reading all of this information

108
00:11:32,400 --> 00:11:38,160
about people, places, companies, whatever. The way things are written is not necessarily always

109
00:11:38,160 --> 00:11:42,800
clear. And so, you might be reading something about Apple, but it's the fruit and not the company,

110
00:11:42,800 --> 00:11:48,160
and you have to be able to do some biguate that. So, we do a lot of really cool stuff in the

111
00:11:48,160 --> 00:11:53,840
entity resolution space. And then we've started to do some really exciting things that were in the

112
00:11:53,840 --> 00:11:58,640
domain of intelligent services, but I think kind of the technology is leaking out now in the sense

113
00:11:58,640 --> 00:12:04,800
of something called record linkage. So, if you have these giant data sets that you can onboard or

114
00:12:04,800 --> 00:12:12,400
acquire or, you know, create yourself, what typically happens is that something may call Apple, Apple

115
00:12:12,400 --> 00:12:17,360
in one dataset. It may call it Apple Incina, a different one, or it might just be a reference to

116
00:12:17,360 --> 00:12:24,160
the CEO of Apple. So, you need to be able to link all of these data sets together. And basically,

117
00:12:24,160 --> 00:12:31,040
you don't have a clear key on which to join everything. So, you have to make these calculated sort

118
00:12:31,040 --> 00:12:35,120
of, you know, similar, you have to basically say, this thing is likely this other thing.

119
00:12:35,120 --> 00:12:40,400
This ambiguity. Correct. And connecting things based off of, you know, you say, is this in the

120
00:12:40,400 --> 00:12:47,200
same location? Or, you know, is it referencing a product of this company? So, you know, and then

121
00:12:47,200 --> 00:12:51,360
basically be able to say, well, actually it is talking about Apple, but it was actually talking

122
00:12:51,360 --> 00:12:58,160
about the iPhone. So, basically merging, like, say, the iPhone to the Apple company without ever

123
00:12:58,160 --> 00:13:03,520
having a necessary mention of Apple in both datasets. Right. So, the record linkage is actually

124
00:13:03,520 --> 00:13:08,720
really, really, really hard. We started to do a lot of that. And it's a really fascinating

125
00:13:08,720 --> 00:13:13,440
kind of area in the big data world. How do you attack that? What technology approaches are you

126
00:13:13,440 --> 00:13:20,880
applying to? So, obviously, the first one is we have to be able to within the data extract

127
00:13:20,880 --> 00:13:25,680
kind of attributes or features that we think are important for the links that we're trying to

128
00:13:25,680 --> 00:13:30,800
establish. So, what are we trying to like in the first place? Yes, company data. So, if we go

129
00:13:30,800 --> 00:13:38,240
and grab every patent in the US for companies, and that's just one giant data dump, linking that

130
00:13:38,240 --> 00:13:44,320
back to, well, all these patents are actually owned by these companies. You know, the US government

131
00:13:44,320 --> 00:13:50,000
patent office might not have the same names as, you know, the publicly traded companies themselves.

132
00:13:50,000 --> 00:13:55,520
Right. So, you have to look at things like, is the address the same? The people mentioned as

133
00:13:55,520 --> 00:14:01,200
the patent holders, employees of these companies. So, you're looking for bodies of evidence that

134
00:14:01,200 --> 00:14:06,320
suggests that these two things are kind of mapped together. And you have different algorithms that

135
00:14:06,320 --> 00:14:11,760
can kind of give you a score and a confidence measures to how closely two records are to each

136
00:14:11,760 --> 00:14:17,840
other. So, that's one. And then another one is a, I think, called relationship extraction. So,

137
00:14:17,840 --> 00:14:24,960
if you read a piece of news as a human being, it's very easy for you to sort of boil it down to

138
00:14:24,960 --> 00:14:31,360
what it means, right? So, you know, this company is buying another company. But in the real world,

139
00:14:31,360 --> 00:14:36,880
that might be written out over three paragraphs. And so, for a computer to sort of establish that

140
00:14:36,880 --> 00:14:44,320
NTTA is acquiring NTTB after reading a paragraph or two is actually a pretty challenging problem.

141
00:14:44,320 --> 00:14:48,880
Right. And so, yeah, there's a lot of really interesting stuff that we use there, but most of it is

142
00:14:48,880 --> 00:14:54,560
in the deep learning, you know, kind of kind of space. So, we're using a lot of TensorFlow models

143
00:14:54,560 --> 00:14:59,920
that, you know, to try to understand the representation of language, extract kind of entities,

144
00:14:59,920 --> 00:15:07,040
and then the relationships that we're looking for. So, today's end up looking like, you know,

145
00:15:07,040 --> 00:15:15,360
those single or individual, like super, you know, very deep neural networks or they many neural

146
00:15:15,360 --> 00:15:21,840
networks that are each serving their own purposes. Yeah. So, we would, we've realized is that

147
00:15:22,720 --> 00:15:29,360
we've had better results with building specialized machine learning models and neural networks

148
00:15:30,080 --> 00:15:36,000
as opposed to one kind of generalized AI or generalized network. And so, even with our

149
00:15:36,000 --> 00:15:41,680
relationship extraction, what we do is we have thousands of models that kind of work in Unison,

150
00:15:41,680 --> 00:15:47,200
and each one is highly specialized in one thing. And so, if you think about having a very wide

151
00:15:47,200 --> 00:15:53,760
funnel at the beginning where you say, this is a draw document, the first sort of set of models

152
00:15:53,760 --> 00:15:58,800
will basically say, this document is actually talking about this concept. And then that gets filtered

153
00:15:58,800 --> 00:16:03,680
into, okay, so within this concept, these are the types of relationships I should be looking for.

154
00:16:03,680 --> 00:16:09,200
And then have more sophisticated models kind of take that information and then say, okay,

155
00:16:09,200 --> 00:16:14,960
fine. So, this is about a corporation, and corporations can have M&A as a relationship. And so,

156
00:16:15,440 --> 00:16:21,760
I kind of, I'm seeing a very strong signal that this document in general is talking about M&A,

157
00:16:21,760 --> 00:16:26,560
let me give it to the most sophisticated guy to go and extract that very specific relationship.

158
00:16:27,200 --> 00:16:30,480
And by sophisticated guy, are we talking about a person or another model?

159
00:16:30,480 --> 00:16:37,760
Another model, sorry, yeah. And so, we have this kind of pipeline of AI models that kind of work

160
00:16:37,760 --> 00:16:43,840
together that start really broad and then end up kind of very high-professionalized at the end.

161
00:16:44,720 --> 00:16:48,400
And are you doing any human and a loop anywhere, or is it all?

162
00:16:48,400 --> 00:16:54,080
We do, but there's a bit of reinforcement learning that goes on, especially with the record

163
00:16:54,080 --> 00:17:01,200
linkage stuff. And then we obviously retrain our models fairly frequently. And so, we'll have

164
00:17:01,200 --> 00:17:08,080
the human and the loop on the retraining of the models. But on a given day, everything's kind

165
00:17:08,080 --> 00:17:15,920
of running autonomous. Yeah, super interesting. So, and what you talked about the identifying the

166
00:17:15,920 --> 00:17:20,320
relationships and how you have the, you know, you can have the three paragraphs, you get to,

167
00:17:20,320 --> 00:17:26,800
yeah, you know, that there was an acquisition that makes me think of like, you know, CFO speak,

168
00:17:26,800 --> 00:17:33,360
right? Do you have a model that can train that is trained like decipher when a CFO is saying

169
00:17:33,360 --> 00:17:38,080
that they're going to miss? Yeah, yeah. Actually, it's an area of interest to us. We haven't yet

170
00:17:38,080 --> 00:17:45,040
done any kind of hard core development there, but there is actually a huge community of users in

171
00:17:45,040 --> 00:17:50,560
the kind of, you know, hedge fund space that you understand tone, not even just the text, but like

172
00:17:50,560 --> 00:17:56,160
the tone. Yeah. If there was a video of the guy making the statement or he was darting around.

173
00:17:58,160 --> 00:18:02,720
There is a lot of kind of interest in that. We just haven't yet had a chance to spend the time in

174
00:18:02,720 --> 00:18:09,120
that space, but it's clearly something, you know, okay. And so, where are you guys in the process?

175
00:18:09,120 --> 00:18:16,320
Yes. So, we've launched a product about, I guess, less than two months now called Precog,

176
00:18:17,040 --> 00:18:23,120
which is our, like, it's a predictive service that basically tries to measure the butterfly effect

177
00:18:23,120 --> 00:18:28,880
and then produce short on forecasts for the returns of these 30,000 stocks around the world.

178
00:18:29,680 --> 00:18:34,400
That is currently in production and we have a number of clients that we service.

179
00:18:34,400 --> 00:18:43,920
So, meaning you give it, you know, a company and you probably have some unique company thing.

180
00:18:45,280 --> 00:18:51,680
Well, just going to database and tell you, you know, you give it maybe say 30 days and it'll tell

181
00:18:51,680 --> 00:18:57,280
you a projected stock price in 30 days, is it? Yes, a lot. That, yeah, it's along those lines. So,

182
00:18:57,280 --> 00:19:03,680
what we do today is we only produce predictions on specific time horizons. So, it would be like a

183
00:19:03,680 --> 00:19:08,320
one week forecast, two week forecast or one month forecast. Okay. But it would be kind of a

184
00:19:08,320 --> 00:19:14,000
rolling day forecast for those horizons. Okay. Today being Monday, we'll give you a forecast for

185
00:19:14,000 --> 00:19:19,120
the return of the stock, not the price, but the return of the stock by next month. 714 and 30 days.

186
00:19:19,120 --> 00:19:26,080
Okay. And that, you know, again, if you do this systematically over a broad universe of stocks

187
00:19:26,080 --> 00:19:29,840
and you sort of, you know, you're never going to get 100% accuracy, which is impossible,

188
00:19:29,840 --> 00:19:35,840
which is, you have this winning prediction, you know, like right now, we think we get anywhere

189
00:19:35,840 --> 00:19:46,080
from 60 to almost 70% accurate in depending on the horizon and the stocks themselves. But 60 to

190
00:19:46,080 --> 00:19:51,920
70 is kind of the average, but across 30,000 names, that is like Vegas odds, right? So, if you're

191
00:19:51,920 --> 00:19:59,360
the casino, you may lose a hand to one player, you know, one table, but across all players, you're

192
00:19:59,360 --> 00:20:04,880
systematically making money. And so, the customers that are using our predictions want them

193
00:20:04,880 --> 00:20:09,520
on a very large universe of stocks, and then they are obviously not betting the firm on one prediction

194
00:20:09,520 --> 00:20:13,360
on one day. They're making sure. This is one signal that they're using out of a portfolio

195
00:20:13,360 --> 00:20:21,280
of signals. So, you're obviously gaining, you're able to make a prediction at day zero, and then, you

196
00:20:21,280 --> 00:20:27,360
know, at days 17, 14 and 30, you're getting additional signals as to, you know, how accurately

197
00:20:27,360 --> 00:20:34,480
your model is performing. How do you then use that signal to retrain? And then how do you deal with,

198
00:20:35,760 --> 00:20:42,160
like attribution issues? You know, you were off by, you know, 50% or even plus minus, like,

199
00:20:42,160 --> 00:20:45,840
how, where in your model did you go wrong? Have you started to figure, look at that?

200
00:20:45,840 --> 00:20:51,520
Yeah, we do. One of the things that we've tried to stay away, shy away from just kind of with

201
00:20:51,520 --> 00:20:58,240
our models is that we've tried to avoid using deep neural nets and all of these types of things

202
00:20:58,240 --> 00:21:04,240
when we're doing our forecasting. And the reason being with these deep neural networks, you actually

203
00:21:04,240 --> 00:21:09,600
don't know kind of what is driving the model's decision to produce the outcome that it produces.

204
00:21:09,600 --> 00:21:16,160
So, the models that we generally rely on tend to have a way to sort of reverse engineer

205
00:21:16,160 --> 00:21:21,200
why it made the decision it made. So, more like decision trees or something else?

206
00:21:21,200 --> 00:21:28,400
It's never one thing. So, we use kind of a hybrid approach where we will use an ensemble of

207
00:21:28,400 --> 00:21:35,120
methods to come up with a single prediction. But most of them have an ability where we can actually

208
00:21:35,120 --> 00:21:39,200
query the model and ask it, what was the most important thing in the decision that you made?

209
00:21:39,200 --> 00:21:45,680
Okay. And then those, those, those weights or those factors are things that we can kind of just,

210
00:21:45,680 --> 00:21:51,920
you know, sanity check in the markets to see if it is what it was. So, you know, we make, say,

211
00:21:51,920 --> 00:21:57,520
for financial stocks, we think, you know, yield spreads and things of that nature should be important

212
00:21:57,520 --> 00:22:02,560
to financials. But there may be a group of financials that are aren't moving with respect to

213
00:22:02,560 --> 00:22:07,280
interest rates. So, the model might rely on that. But then, you know, in a month's time, we could

214
00:22:07,280 --> 00:22:11,760
go back and say, well, why were you way off? And they'll say, well, I over waited, you know,

215
00:22:12,480 --> 00:22:17,520
this moving interest rates. Okay. We will try to retrain the model to make sure it doesn't do that.

216
00:22:18,800 --> 00:22:26,800
The thing we try to do is we don't want to superfluously or just over engineer the models,

217
00:22:26,800 --> 00:22:32,480
because you can end up with overfitting. So, we're really careful about when we train the models

218
00:22:32,480 --> 00:22:37,520
and then what we give them to retrain, you know, and there's a lot of art and science that goes

219
00:22:37,520 --> 00:22:43,520
into kind of being like, you know, 60 percent is good enough or not given everything that we know.

220
00:22:45,680 --> 00:22:52,000
So, it sounds like going back to your funnel analogy at the top of the funnel, you're using a

221
00:22:52,000 --> 00:22:59,280
lot of deep learning to extract signal from various sources and then closer to the end of the funnel,

222
00:22:59,280 --> 00:23:04,240
you're using more using models that have greater explainability.

223
00:23:04,240 --> 00:23:09,920
Absolutely. That's great. Okay. Interesting. Anything else that you'd like to share about what

224
00:23:09,920 --> 00:23:15,120
you guys are up to? I mean, just kind of long term. One of the things, obviously, we're trying to

225
00:23:15,120 --> 00:23:20,720
build, like I said in the beginning, was the capability for then, you know, non-technical users to be

226
00:23:20,720 --> 00:23:29,520
able to sort of surface some of these insights or even, you know, ask the discover new facts that

227
00:23:29,520 --> 00:23:34,240
are kind of not obvious, right, within this knowledge base that we've built and then also be able

228
00:23:34,240 --> 00:23:38,800
to take advantage of some of these predictive models that we're building. So, what we're really

229
00:23:38,800 --> 00:23:44,480
trying to do here is develop, you know, like, an ability for the machine to understand human intent

230
00:23:45,440 --> 00:23:50,400
and then also the context in which the intent is being asked. So, for example, you sound fairly

231
00:23:50,400 --> 00:23:55,840
kind of familiar with the financial domain, but like my grandmother may have a similar question to you,

232
00:23:55,840 --> 00:24:00,960
but the response that the machine should give her versus you needs to be kind of adjusted for that.

233
00:24:00,960 --> 00:24:06,720
So, we're really looking to build on intelligent agent that would be able to work under different

234
00:24:06,720 --> 00:24:11,840
contexts, right? So, if you're dealing with another professional investor, kind of, you know,

235
00:24:11,840 --> 00:24:16,400
having a conversation at that level, if you're dealing with somebody looking for financial advice

236
00:24:16,400 --> 00:24:21,440
who's not that sophisticated, then you kind of want to boil down some of these things and just have,

237
00:24:21,440 --> 00:24:26,320
you know, them explained back to the user in a format that they can understand. Right. So,

238
00:24:26,320 --> 00:24:31,840
that ultimately sort of where we're driving towards. Okay. Now, working folks, learn more about

239
00:24:31,840 --> 00:24:38,560
what you guys are doing. So, on our website is a great place to start. They can also check out,

240
00:24:39,280 --> 00:24:44,080
at least for individual investors or people interested in sort of these signals that we're

241
00:24:44,080 --> 00:24:52,080
producing. They're available on Quantopian. So, they're free to use until you want to put them

242
00:24:52,080 --> 00:24:57,600
into a production strategy. Okay. And they are available for 500 of the most liquid names in the

243
00:24:57,600 --> 00:25:06,240
United States. Okay. And so, the company website is alphavertex.ai. Alphavertex.ai and Quantopian is

244
00:25:06,240 --> 00:25:14,240
the site that is available. Okay. And if folks want to get in touch with you, can they do it

245
00:25:14,240 --> 00:25:21,120
through one of those sites? Yeah, they can email me at infoatalphavertex.ai and we will be glad to,

246
00:25:21,760 --> 00:25:26,400
you know, reach out to them and just understand what their needs are. Awesome. Well, thanks so

247
00:25:26,400 --> 00:25:30,800
much for being on the show. I really learned a lot about, really learned a lot from what you guys

248
00:25:30,800 --> 00:25:40,800
are doing and it sounds really exciting. Yeah, it was a pleasure. Thank you so much. Great. Thank you.

