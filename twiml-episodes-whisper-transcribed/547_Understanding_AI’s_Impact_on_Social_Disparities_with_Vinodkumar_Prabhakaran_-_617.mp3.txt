All right, everyone. Welcome to another episode of the Twimal AI podcast. I am your host, Sam
Charrington. And today, I'm joined by Vinod Purpakran. Vinod is a senior research scientist
at Google Research Vinod. Welcome to the show. Thank you, Sam. Thank you so much for having
me here. I'm looking forward to digging into our conversation. We'll be talking about some
of your research, which looks at both the way that AI impacts social disparities, but also
how AI can be used to study social disparities. To get us started with that, I'd love to have
you share a little bit about your background and how you came to work in the field.
Absolutely. So, yeah, I come from a more traditional computer science background, like my PhDs
in computer science, where I worked on sort of more research at the intersection of
natural language processing and social sciences or society. So, that's where I kind of started.
So, I come from a more kind of technical background in that sense. But over time, I've sort of
got to this place where researching at the intersection of AI in general and society. And
where I kind of have two different research profiles. One is where I use AI or NLP
or machine learning technologies to sort of look at societal disparities, use them as tools to
look at societal disparities. And that's the kind of work that I was doing mostly in the PhD
work, as well as afterwards as a postdoc at Stanford. But afterwards, like in the last three or
four years, Google as a research scientist, I've been mostly looking at how social disparities
influence these tools or how these social disparities are captured, reflected and maybe even
propagated or amplified through these machine learning and natural language processing tools.
So, it's kind of like looking at both directions of this intersection.
And you referenced some of the work you were doing for your PhD. What are some examples of the
way that you used AI and NLP to look at social or societal disparities? Yeah. So, I mean,
some of the work was like looking at how workplace interactions, how social power manifest,
you know, like workplace interactions, like that was my PhD work. So, looking at email conversations
at a workplace and like looking at just by looking at a language used and the structure of
these conversations, can you tell who is the superior or who is the subordinates in that conversation?
Like how does power manifest in like how people interact with one another? So, that was my kind
of PhD research thesis topic. That sort of got me into this work, which is kind of I'm really excited
about I continue to be excited about the work is I don't actively work on it, but right now,
but that was where we used this NLP tools to look at social racial disparities in police
community interactions. This is work I did as a postdoc at Stanford with a bunch of amazing
researchers there in collaboration with social psychologists and linguists and computer scientists
where we had access to body camera videos of about one year worth of data from the Auckland
police department. So, it was in collaboration with the police department and we used the NLP tools
to sort of look through this lot of data. This usually this data is looked at as like, you know,
something goes wrong. It's looked at as evidence whereas we were kind of demonstrating that it could
be used as like data to learn from and like understand what is going wrong potentially in the
in the in the department or in that particular city. So, yeah, we used NLP tools to sort of look at
like things like the level of respect, the level of politeness, the way the conversations are
structured when police officers stop community members for traffic jobs, for instance.
So, presumably the body camera videos were transcribed and that's what you applied NLP tools to.
Yes. So, the first paper that came out of that work like we did look at the transcribed data like
we had them manually transcribed and looked at like signals for politeness and respect and so on.
But there's also work on looking at the audio signals like the prosody. I was not actively involved
in it but the the the kind of frequency and pitch and all those factors might be signals that
so this is a huge project with like, you know, so many people looking at interesting signals there.
I also had some work where we are actually looking at the structure of these conversations like
where police officers or when did the police officers like give the reason for the stuff,
right? Like when you stop someone, do you start by saying, hey, I'm stopping you for this
in this reason, where is your diverse license and registration versus basically
stopping someone and saying like, hey, give me your license and registration and then telling.
So, that sets the conversation in a very different kind of path and that also be empirically
we're able to analyze and look at these kind of differences in this kind of subtle ways,
subtle ways in which like conversations can be different and what how that can affect
like the later parts of the conversation. So, this is ongoing work like the researchers
at Stanford continue to work on this and I work on like I kind of wrapping up it's been like four
years now, but some of the work I'm still in in in in bolden. So, there's like some work that's
going to be coming out from that like in the near future, hopefully. But yeah, that's that's an
example of a place where these AI tools are used to sort of look at social disparities.
I'm curious how much of the social science side of your of this work you were
involved in or kind of got into or were you collaborating with social scientists and working
on the more machine learning technical sides of it? Yeah, I love this question because it's
something that that particular project was such a learning experience for me to sort of having
deeper collaborations with social scientists like to and also like having that deep respect for
each disciplines like you know engaging with the social scientists. It was a very close collaboration
to answer your question. It was a very close collaboration. We had like weekly meetings and like
I went to the you know it was not that it's a typical computer science project that I have
been part of like prior to that. You get this data from somewhere and then you go into your lab and
like you know go through munch through the numbers and like try to come up with like you know some
results and then like you know put it in a paper. But this was a very um involved collaboration
with social scientists but also with the police department and communities. I didn't actively
involve with and interact with the communities there say but like you know our collaborators did
have like community workshops and stuff. But I did go to the police you know precinct and like
you know talk to the police department, police chief and like sort of understood like you know
the context of this data and I even went on like right along to sort of like be part of to see
how these conversations happen in reality. That I mean that really changed or shapes the way I
kind of look at this data. It's not no longer just like ones and zeros like it actually
have has a lot more brings a lot more meaning to like the data that we are working with and
so yeah it was and to answer your question about like the disciplinary collaboration also like
it was a very as I said like we had weekly co-a meetings and we had like multiple papers I was
working with like social psychology PhD students and yeah and we were in another podcast recently
me and another social psych substance colleague of one of these papers about like how interdisciplinary
works like in these kind of collaborations right like said I think in that particular project like
the I think one thing that we both mentioned was that there was a deep respect for these disciplines
rather than like you know the computer scientist coming and like taking away like the you know
the data and like going and doing magic like it was not that we were very actively involved in
framing the research questions how we ask the research questions how we interpret the results
all of that was like in deep collaboration and so the more recent work that you're doing
how did the work that you were doing previously kind of take you to the more recent work?
I think the work that I was doing at Stanford especially in this particular context kind of got me
a lot more sort of aware of like a lot more sort of yeah aware of like the kind of the
justice angle of like these social science work like they know the disparities and like that
that human angle to it and that sort of like and it was around the time that there was conversations
around fairness and bias in machine learning models that was just like you know beginning in 2015
1617 period so I was in that like I just happened to be in that space where like I was already working
on understanding social disparities and this is like an important or like a really cool sort of
like way of like looking in the other direction how does this disparities get into or get captured
into these machine learning models and that's what like sort of like pivoted or like it wasn't
a really pivot because and I still kind of like have this I still work on like social science
see kind of flavored work but that realization is like how I go into the Google research
job that I'm where I am right now where kind of like the focus at that time was on sort of
understanding like this how these disparities get captured in machine learning models and that
was like the transition because I was already working at that space in the other direction
and since joining Google most of my Google work as I said have been on the other direction where
you know looking at how disparities in data kind of get captured in these models and
you know clearly one key place where some of those disparities will get introduced into
models is through the that human interface which is data labeling and your more recent work is
kind of focused on on that in particular can you introduce us to your paper your 2021 paper the
issues aggregating human labels what was the kind of the broad set of issues that you were exploring
there absolutely so I think yeah there's a lot of work within the machine learning and OP field
in the last few years looking at like various sources of biases very sources through which like
you know biases get creeped into the models it could be coming through data it could be coming
through the humans who are building these models and their perspectives or you know limited
perspectives about society but in this particular so I was always curious about how diverse
perspectives in data beat like you know data beat and like you know people who are building these
like models how does like diverse perspectives like get captured in these like pipeline right you
know there's no that's not always like this like one single answer I mean we live in a world of like
you know pluralistic world with like so many value systems if you think about like across the
world there's no one single answer for many of these like questions and ethics and fairness
what is fair for me may not be what is fair for like someone in a different cultural context and
having a different cultural history so that kind of motivated that motivates my current work like
largely most of my research currently isn't that kind of motivated from that question of like you
know how do we different perspectives like all disagreements between like what is right and wrong
gets captured in these kind of interventions so it's like even when like you know we intervene or
like making things fairer like how do we do that like in a more pluralistic way so that that's
where like this particular paper work on this particular paper that you referenced I came about
and so we had a paper in 2021 where we looked at this one core thing as you said data labeling
which becomes like a such a core thing for the machine learning pipeline where you actually
you know have human writers annotate like you know be it like images or text content like for like
whatever whatever social constantly whatever construct that you're trying to model in cases where like
you know whether labeling whether something is a cat or a dog you can see that like most people
would agree like you know there might be cases where like you know people disagree whether something
is a cat or not but like it's it's a it's a relatively more objective kind of task but when
you ask like somewhere is this piece of text offensive that's you're actually leaning on a lot of sort
of aspects of that individual humans like you know social cultural context and their lived
experiences shape how they feel or how they perceive something to be offensive and not and so
we looked at like a bunch of datasets which kind of capture or attempts to capture such sort of
subjective or relatively more subjective tasks such as like sentiment whether this is a
positive sentiment or a negative sentiment or offenseiveness like or hate speech or emotions
like whether this is like something expressing happiness or sadness or that sort of so
this like a we had around three datasets around seven or eight different like tasks like these
kind of like social constructs that were like labeled and we looked at how people's like different
annotators like perspectives like matched or not so the traditional way of dealing with like when
people disagree for for a long time like you know in machine learning traditionally how you deal
with it when people disagree is that you take a majority vote like you you have especially when you
collect data from the crowd from the crowd work kind of platforms like mechanical talk and all
that you it's like relatively cheap so you get like three annotations for all in or three or five
or ten depending on how much money you have and you basically get the more number of annotations
and then you take a majority vote to say that like oh this is the majority of people in our pool
agreed and stuff we basically were questioning like how does what does that mean for like
perspectives or people with specific social cultural backgrounds which maybe underrepresented
within the annotator pool so we had a paper like kind of like precursor paper first where we kind
of did this analysis on this eight different datasets or eight different tasks annotation tasks
like where we looked at like if you just take the majority vote and then compare or look at like how
does each individual annotator agree with this majority vote like because we also have like the
annotations that this individual voters gave like how many times like their vote really made it to
the majority right so we looked at like that or was equal to the majority right like we looked at that
for these tasks like that varied significantly across tasks and sometimes it is like all over the
place some some tasks like such as hate speech if I remember correctly was a lot more kind of like
you know most people agree and most people with the majority vote like agreed with like a most
people but like the things like disgust is an emotion like it had like very wide range of like
interpretations and like people disagreed on that or one thing that jumps out at me just hearing
you describe that setting is in some context you might want to you might be tempted to look at the
degree to which a particular annotator annotates along with the majority as like a measure of
quality but here you're pointing out that it's in fact you know maybe a measure of diversity or
different context or something like that absolutely I think predominantly machine learning community
have been using that quality kind of framing in doing the majority vote like the reason why they
do majority vote is to actually remove the noise right so I think it probably comes from the
place that you know machine learning researchers like you know I want to ensure the quality of
the data that they're working with and this is like comes across as like noise but there's
there's even there's two different things there's like in the context of an individual label
data point the you know taking the majority allows you to eliminate the noise but then
from the context of the kind of the labeling operations right you know often you're looking at the
label or over time and and you're trying to understand you know which of the labelers that are
best I've seen work that also looks at like this kind of agreement with majority as a way to like
you know assess the quality of the greater themselves right that's that's what I'm alluding to and
that this now the this additional angle that hey it's not just quality depending on the type of
question that we're asking it's also a measure of you know different degrees of diverse perspectives
yeah yeah and I think one I do want to note that like yeah it's it's possible that it could be
a unreliable reader but if that reader is like internally consistent with their their own annotators
like if they see similar things like again if they're like so then then they are actually bringing
in a different perspective like that is kind of like it's not that it is lower quality it is just
a different value system maybe is reflected in their annotations so that's what we were going
after right like so this is an ongoing project it's not just one paper would not answer all the
questions so in this particular case we were basically trying to tease a part or disentangle
these disentanglements and sort of understand what factors might be contributing to why people
are disagreeing right like is it just this person being unreliable just like you know randomly
selecting things or is there a systematicity to the way they disagree with the majority right
and one and one way in that paper that we looked at was looked at like there was one dataset where
we also had access to the social demographic characteristics of these annotators so we looked
at whether there is a difference in across like different genders this particular dataset had
only binary gender no I think there was only one non binary gender person so it wasn't like
big enough to like analyze all the different kind of gender categories but it was a you know
gender was one thing and then a political affiliation or ethnic ethnicity or race
and in this particular study we show that there was no difference in gender there's
in like men and women had like or male and female annotators had similar rates of sort of like
having rate rateers who disagreed with the majority with the in across annotations but it was
interesting to note that like when it came to race African-American writers annotators like
who identified as African-American in that dataset had significantly lower agreement with the
majority rating than white American this was the dataset collected in the US so white American
Asian-American Raiders so that is a problem that is a problem when a from a fairness perspective
when you take majority then you're actually sidelineing a particular perspective like potentially
there is like a you know perspective that is being sideline what was the particular
the specific dataset and task so this is a dataset for determining sentiment like positive negative
neutral sentiment leveling I do want to note though that this dataset was collected with the
intention to study by so this was not a case where like there was a dataset and someone just like
did majority vote and then moved on with like in the in an incorrect way this dataset had the
social demographic information in them precisely because they wanted to study these this is a
dataset built by Mark Diaz my colleague for his PhD thesis so he now works with me so that's how
this dataset had this information usually machine learning researchers when they collect data
for various reasons like do not collect the social demographic information because it's a much
harder to get that information there's all sorts of risk involved with it yeah so we were able
to see these these disagreements with the majority that differs across different social demographic
kind of groups which is even a bigger problem from a fairness perspective because this is like a
human or like a researcher's decision to sort of take the majority vote and by doing that you're
actually actively sidelineing certain perspectives like in your data labels so yeah that was the work
that you referenced and then we had a follow up paper that I can talk about where we kind of look
at like how we can sort of deal with that like how we can constructively sort of deal with this
like diverse perspectives so let's dig into the let's dig into that second paper you've identified
this challenge and the second paper wanted to propose some potential solutions yeah so what
we did there was basically rather than sort of trying to find this single ground truth for like
these kind of subjective tasks ahead of the time like you know ahead of training these models
at the data collection stage itself like taking the majority out vote rather than doing that
we built a sort of like an approach or like we built a kind of a mission learning pipeline where
we use a multi task approach where it's it's traditionally used for like you know you want to train
a model for like multiple tasks that are similar so use use the same mission learning network like
you know the network that you train like you use the same data but then you kind of like have this
final layers that are trained for specific tasks so that you have like a shared embedding or shared
kind of network for the most of the part and then like you have like this specific task specific
like parts of that network so we looked into whether we can actually model separate annotators
and their systematicity in which like they annotate like using this sort of like shared network
like a multi annotator model so that like you have all the data is being used for training like
you know this model but like this model does not output one label it outputs like okay if it was
this person that person would have said the label X or so you have like if you had like 10 different
labels a labelers like 10 different Raiders you actually are modeling these 10 different
lip perspectives in the output and then in the output time you can actually take the choice of like
if you still want to do majority voting like you can do the majority voting at the output like
you know the machine learning model gave like 10 outputs and you can just take the majority
vote there or you could say that oh we want to take the kind of outputs produced by these
subset of annotators or subset of like these predictions that models like annotators from a
particular background it this you know you can imagine a scenario where like you have annotators
from like different countries let's say there are three countries that we have annotators from
India, US and like say Germany and then when you are actually rolling out like products
into the societies like you may want to actually use the majority vote or like use the vote from
a particular region or like you know or the machine learning models predictions that
reflect that annotator those annotators to kind of to be used in that region right you know so
there are like so many different ways that you can be used you can use this approach so you
basically captures this like multitude of perspectives in the prediction pipeline rather than
sort of like suppressing it in the in the beginning of the pipeline it also gives you an additional
ability to sort of know when there is uncertainty like when there is disagreement like because if you
make a label as a bit majority labels if you call that okay this sentence is offensive
just based on like in a majority vote like in the beginning of the data collection itself
we would never know whether that was like you know agreed on by 10 people or like you know two people
right like and it was an unanimous decision or not whereas in this case like you have these
like multiple perspectives at the prediction time so you kind of know that oh multiple annotators
would have disagreed on this particular case so maybe the machine should not predict here
it should be like a human label or from that particular social context should like make a call
whether this particular piece of text is offensive or not so that's another way I could imagine
being used this being used is that like it gives you a handle on how much uncertainty is potentially
here how much diverse perspectives potentially is here for this given piece of text it's kind of
consistent with the general idea in dealing with neural networks to just give it all the data and
don't try to clean it up too much because you're not hiding what you think might be noise that
the network can actually find some signal in probably I mean that was definitely not the motivation
for our like as approaching it but I think yeah the current neural networks are powerful
in sort of like you know bringing in these signals like you know because prior to this sort of
multi-task architectures which is like a two or three years maybe a little bit longer old prior to
that like you would have had to like train if you have like five different like set you know annotators
you have to train like five different models like you there's no way that like they have a shared
network and they can take signals like even the smaller signals like from each of these
raters to be like you know model differently or separately that would have not been possible so
I think the current neural networks ability to sort of like pick up on these signals are definitely
contributing to being able to sort of capture this diversity without like affecting like sort of
like the end performance if that's what you're going for. And so is the the model that you're
creating based on the kind of the annotator signal is this then kind of use separately as part of
a downstream training task or are you kind of embedding it end-to-end in the ultimate task.
That's a great question. I think that there is potential to actually use this as kind of like a
lead-in for like other downstream tasks and so to take a step back like so this is ongoing work
like you know we have not sort of like rolled it out and like into any products or anything this is
like I work in research so this allows in in Google research that allows us to sort of like do
this foundational research without having it to be tied to any particular products. So in that
sense like we have not sort of like rolled it out yet on any of the products but like I think
I envision this both as like a potential kind of like first step to sort of like you know
tease apart these differences but it could also work as a sort of end-to-end scenario where like you
know I could imagine like an online platform using this model to sort of have multiple answers for
like multiple perspectives for sort of you know choosing whether something some particular content
needs to be removed or art for instance like it could it could help in the content moderation kind
of pipeline to sort of queue content for review appropriate like you know reprioritize content
and so I could see this being used in like that kind of production scenario as an end-to-end
when I say end-to-end like I often envision like a human label or a human human sort of like
intervention it's a human in the loop or machine in the loop kind of situation rather than sort of
like button click and like you kind of like you know get the label and like that's that decide
like whether something is offensive or not. So there's often a human in the loop is needed
and yeah in those settings like I could imagine this model being useful. How did you evaluate the
the performance of the model? Traditionally for this dataset like if you had built like a single
kind of label model like you would basically go and evaluate you know the typical machine learning
evaluation pipeline like you know you have like a test set and you basically look at like the accuracy
or precision recall and so on. So we evaluated first in terms of basically if you choose to just
still do the majority word like if you had taken the majority in the beginning versus like
this whole pipeline of like this multi multi task or multi annotator architecture which has like
these multiple perspectives and then you take the majority word and looking at like sort of
how well this does you know in that evaluation. So using the traditional pipeline to evaluate
it and in that case we were able to see that like the performance was almost the same or sometimes
in some data it's even better because better in the sense that like because it's it's modeling
each annotator kind of like individually and it allows it to sort of like look at internal
consistency of their labels better than when you actually merge them. So it doesn't happen across
board but like in in in in some tasks like it performed even better in that particular evaluation
strategy. But we also looked at sort of how well are we doing on calculating this uncertainty?
We looked at are we able to or the the sentences where the model said something is offensive and
like you know there was a lot of disagreement in the end like in the end predictions are multiple
there's more disagreement. We compared that with like how much did annotators for this particular
text disagreed with each other right? Like so we compared when the model has this high disagreement
is that also the cases where like the annotators originally disagreed and that turned out to be the
case there was a significant correlation between sentences where the annotators disagreed
and the model disagreed and that kind of and that was a lot more than traditional ways of calculating
uncertainty. So this is another kind of way we evaluated so that was not about model performance
because our focus is not getting like the model performance from like 72% to 74% like that's like
the typical sort of like the mission learning kind of research kind of objective and for good
reason like you know it's it is important to improve the performance but in this work we are
we are interested in like how well we are capturing diverse perspectives and are when we are
kept having these disagreements in the predictions are they reflecting disagreements in the actual
data and which turned out to be significantly correlated. So yeah has this research led you to
kind of a set of axioms that or that you would recommend to folks as they're kind of building
these labeling pipelines and you know wanting to create models that are as robust as possible
as fair as possible like how should they think about all these issues? Absolutely yeah I was
this like so this work is ongoing and there's like a lot more work that needs to be done and this
particular model that I proposed is not going to solve all the problems in this space and so we had
like laid out like a bunch of like sort of recommendations in the first paper actually that like
you know the traditional practice is that you take a majority vote and sort of like that's what
is in the data set most people when they collect labels from like multiple people they just take
the majority vote and that one label is the only thing that's even in the data set. So you know we
recommend or we argue that people should like when people collect data should sort of
release annotator level that sorry individual annotator level kind of labels so that like you know
you're not treating these annotators as interchangeable anymore also where possible like you know
sort of being able to collect social demographic information about annotators as much as possible
to do it responsibly being able to sort of like release that along with the data set would only
enrich that and sort of the downstream users of this data set could basically use that information
to account for any fairness failures or any of these kind of analysis which is very hard for us to
do because most data sets in our community do not have even annotator level labels or like social
demographic information. In addition we also argued in favor of sort of like documenting about
the recruitment selection and assignment processes that are followed for this annotation kind of
pipeline so that like it gives more information about like how these annotation labels like you know
what was the diversity of Raiders like you know what like social demographic kind of factors or like
groups were represented and what was underrepresented so those information is super useful
for the downstream user of the of these data sets. So on that note like I want to also give a call
out to the recent paper by my colleague Mark Diaz on sort of providing a sort of comprehensive
framework for communicating these things about annotator diversity and annotator these processes
through kind of a transparency artifact called crowd work sheets which capture these kind of
information and that just got published like a couple of months ago at the fairness and accountability
and transparency conference. So I encourage people to check that check out that work as well and I
also want to call out that like this. The two papers that I discussed at length is work led by
my intern Ida Davani who was at USC at that time last summer and so this is like almost a year
long of work after internship like she continued as a researcher student researcher with us
and she has now joined back as our team as a full-time researcher so this work is an ongoing
effort like and one of the core sort of themes of my research going forward. Awesome awesome
well Vinod thanks so much for joining us share a little bit about what you've been working on.
Absolutely this was a pleasure to chat about this research and it was lovely chatting
with you Sam. Same thanks Vinod.
