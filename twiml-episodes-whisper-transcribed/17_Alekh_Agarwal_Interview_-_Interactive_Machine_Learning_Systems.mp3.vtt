WEBVTT

00:00.000 --> 00:19.320
Hello and welcome to another episode of Twimble Talk, the podcast rye interview interesting

00:19.320 --> 00:24.240
people doing interesting things in machine learning and artificial intelligence.

00:24.240 --> 00:27.560
I'm your host Sam Charrington.

00:27.560 --> 00:32.280
Once again, thanks so much to everyone who sent in their favorite quote from last week's

00:32.280 --> 00:33.280
podcast.

00:33.280 --> 00:35.440
Your stickers are on the way.

00:35.440 --> 00:39.360
We've had a blast receiving and reviewing all of your quotes.

00:39.360 --> 00:43.280
Don't forget to send us your favorite quote from today's show as well because this contest

00:43.280 --> 00:46.600
will continue while our sticker supply lasts.

00:46.600 --> 00:51.960
You can do that via a comment on the show notes page or a comment or post on our Facebook,

00:51.960 --> 00:55.280
Twitter, YouTube or SoundCloud pages.

00:55.280 --> 00:59.320
If you've been listening to the podcast for a while, you know that I really enjoy hearing

00:59.320 --> 01:04.520
from listeners and I appreciate all of the comments and feedback I get from all of you.

01:04.520 --> 01:09.280
Now I haven't mentioned this in a while, but one of the most important ways you can provide

01:09.280 --> 01:12.600
feedback is through an iTunes review.

01:12.600 --> 01:17.760
According to our stats, the vast majority of our listeners come from iOS and iTunes

01:17.760 --> 01:23.720
and many of them, one can only suppose, find the podcast via the iTunes directory.

01:23.720 --> 01:26.640
That's why your reviews there are so important.

01:26.640 --> 01:30.640
The more and better they are, the more people will want to check out the podcast.

01:30.640 --> 01:34.160
So please take a moment to review the show on iTunes.

01:34.160 --> 01:38.480
We'll have our link to the iTunes page in the show notes so you can click right through

01:38.480 --> 01:43.760
from there and you don't need to be a regular iTunes user to leave a review.

01:43.760 --> 01:48.640
Of course, if iTunes just isn't your thing and you've got other ways you prefer to spread

01:48.640 --> 01:52.360
the word about the podcast, those are appreciated as well.

01:52.360 --> 01:55.600
That's so much for spreading the word.

01:55.600 --> 02:00.440
The interview you'll hear on today's show was recorded last fall at the O'Reilly AI

02:00.440 --> 02:03.160
conference in New York City.

02:03.160 --> 02:07.880
On the subject of that conference, it's returning to New York in June and this time around

02:07.880 --> 02:11.720
we'll be giving away passes to two lucky Twomo listeners.

02:11.720 --> 02:15.720
Stay tuned because that giveaway is coming soon.

02:15.720 --> 02:19.600
If you happen to be in New York City now, I want to call your attention to another event

02:19.600 --> 02:22.800
that'll be taking place, this one next week.

02:22.800 --> 02:29.600
That event is called the Future Labs AI Summit and it'll be held at NYU on Wednesday afternoon.

02:29.600 --> 02:35.240
Speakers at the event will include Facebook and NYU's Jan LaCoon and NYU's Gary Marcus.

02:35.240 --> 02:38.040
I'll drop a link to this one in the show notes.

02:38.040 --> 02:39.200
Check it out if you're in the city.

02:39.200 --> 02:41.200
It sounds like a great event.

02:41.200 --> 02:46.680
Finally, I'd like to take a minute to remind you to check out my upcoming event, the

02:46.680 --> 02:53.240
Future of Data Summit, which will be held May 15th and 16th in Las Vegas, Nevada.

02:53.240 --> 02:57.440
If you haven't already checked out the event, I really encourage you to take a look.

02:57.440 --> 03:01.240
The person I had in mind when I created this event is someone who's in a role where they're

03:01.240 --> 03:06.400
responsible for helping to chart an organization's data strategy, or someone who wants to be

03:06.400 --> 03:11.800
in that kind of role, or someone who needs to understand how all of this will come together.

03:11.800 --> 03:16.680
This isn't the place where we'll go deep on neural nets, pun intended, or the latest research

03:16.680 --> 03:17.680
paper.

03:17.680 --> 03:22.080
Rather, this is an interdisciplinary summit where you'll get to hear from and engage with

03:22.080 --> 03:26.960
experts presenting on various aspects of our data-centric future.

03:26.960 --> 03:31.920
You'll hear from a soft Iraqi, an expert in big data infrastructure at Intel, on the coming

03:31.920 --> 03:36.840
advances in hardware and what they'll allow us to do in machine learning AI analytics

03:36.840 --> 03:38.400
and more.

03:38.400 --> 03:43.880
You'll hear about how cloud, IoT, and big data shift the cybersecurity threat landscape,

03:43.880 --> 03:49.120
and how we can secure these systems from IBM's global executive security advisor, Diana

03:49.120 --> 03:50.120
Kelly.

03:50.120 --> 03:56.080
I'll be leading a discussion on data privacy and algorithmic ethics, with Accenture AI's

03:56.080 --> 04:01.000
Roman Shoudry and Ericsson's Jonathan King.

04:01.000 --> 04:05.760
Erics Samar, founder and CTO of Reconna, will talk about the role of AI in optimizing the

04:05.760 --> 04:09.680
enterprise data center, so-called AI ops.

04:09.680 --> 04:14.680
And Endeavor VR founder Amy Peck will talk about the emerging role of virtual and augmented

04:14.680 --> 04:17.000
reality in the enterprise.

04:17.000 --> 04:20.600
These are just a few of the speakers I've lined up for you and I'll be announcing more

04:20.600 --> 04:21.920
shortly.

04:21.920 --> 04:27.320
To learn more about the summit, visit twimmolai.com slash future of data.

04:27.320 --> 04:30.080
Now, about today's show.

04:30.080 --> 04:32.960
This week, my guest is Alec Agarwal.

04:32.960 --> 04:37.520
Alec is a researcher with Microsoft Research in New York City, where his work is focused

04:37.520 --> 04:40.000
on interactive machine learning.

04:40.000 --> 04:44.240
As I mentioned before, Alec and I recorded this show at the ORILE AI conference where

04:44.240 --> 04:50.720
he delivered a talk called interactive learning systems, why now and how.

04:50.720 --> 04:53.840
Interactive learning systems are different from traditional supervised machine learning

04:53.840 --> 04:58.120
systems and that they need to explore and learn from their environments.

04:58.120 --> 05:02.440
This is an exciting area of research and one that really interests me personally.

05:02.440 --> 05:06.640
In part because it incorporates concepts like active learning, reinforcement learning,

05:06.640 --> 05:08.920
contextual bandits and much more.

05:08.920 --> 05:12.760
If you're interested in this topic, when you're done with this show, you should listen

05:12.760 --> 05:17.160
to the show I did with Georgia Tech's Charles Isbell, if you haven't already.

05:17.160 --> 05:18.960
That was show number four.

05:18.960 --> 05:24.800
The notes for this show can be found at twimmolai.com slash talk slash 17.

05:24.800 --> 05:33.920
Now on to the show.

05:33.920 --> 05:34.920
Hey, everyone.

05:34.920 --> 05:42.320
I'm here with Alec Agarwal with Microsoft Research and we're here at day two of the ORILE AI

05:42.320 --> 05:43.320
conference.

05:43.320 --> 05:48.240
Alec just did a great presentation on interactive learning systems and he was kind

05:48.240 --> 05:52.880
enough to join us to talk a little bit about that presentation.

05:52.880 --> 05:55.760
Alec wanted to start off by introducing yourself.

05:55.760 --> 05:56.760
Yeah, sure.

05:56.760 --> 05:58.000
So I'm Alec Agarwal.

05:58.000 --> 06:02.840
I'm a researcher at Microsoft Research actually here in New York City.

06:02.840 --> 06:05.040
I've been here for four years prior to that.

06:05.040 --> 06:10.160
I was doing my PhD at UC Berkeley and my work really touches upon a lot of teams in AI but

06:10.160 --> 06:15.120
one that is particularly in machine learning but one that has particularly been off

06:15.120 --> 06:18.760
interest lately is what I call interactive machine learning.

06:18.760 --> 06:26.960
So think that thing about problems where the machine learning algorithm is not just learning

06:26.960 --> 06:32.560
from a static pool of data that was hand annotated and collected by somebody else but think about

06:32.560 --> 06:37.840
really how the algorithm has to interact within a larger system, within a larger environment

06:37.840 --> 06:44.040
to collect that data to gather learning cues and then incorporate learning cues into

06:44.040 --> 06:52.400
the model in order to improve over time and just leads to several kind of paradigms in machine

06:52.400 --> 06:58.520
learning, things like active learning, reinforcement learning or you know, subsets of reinforcement

06:58.520 --> 07:03.640
learning like contextual bandits and so on, all of which I worked on and were touched

07:03.640 --> 07:05.280
upon in the talk today.

07:05.280 --> 07:06.280
Great, great.

07:06.280 --> 07:12.000
So you mentioned the machine learning, learning from the environment and one of the ways you

07:12.000 --> 07:17.960
illustrated that in your talk was, you showed a demonstration of a Super Mario Brothers

07:17.960 --> 07:18.960
game.

07:18.960 --> 07:23.640
Can you talk about what you were intending to show with that demo and?

07:23.640 --> 07:24.640
Yeah.

07:24.640 --> 07:30.160
So in some sense, whenever we are thinking about interactive learning systems, right?

07:30.160 --> 07:35.680
So one of the question is, what are environments in which we can safely run these experiments

07:35.680 --> 07:41.480
in which we can have this are basically algorithmic agents interact with their environment or

07:41.480 --> 07:43.560
manipulate the environment in a safe manner.

07:43.560 --> 07:48.600
Of course, you know, the natural or maybe canonical embodiment even in everybody is thinking

07:48.600 --> 07:52.400
of such agents are robots, right?

07:52.400 --> 07:54.920
But robots are hard to program.

07:54.920 --> 07:59.960
It's hard to in fact, like control all of their sensors and actuators and it takes time,

07:59.960 --> 08:03.880
it takes resources even to get one.

08:03.880 --> 08:11.120
And so a lot of people have found designing agents for various sort of games, either computer

08:11.120 --> 08:13.680
games or even traditional board games, right?

08:13.680 --> 08:20.960
Like baggammon, chess, gold, now, and other such games as kind of more controlled environments

08:20.960 --> 08:28.120
in which we can still have this interaction either with another player or with the environment

08:28.120 --> 08:30.040
of the game.

08:30.040 --> 08:36.680
And we can run these experiments kind of over and over again with actually remarkably

08:36.680 --> 08:37.680
high throughput.

08:37.680 --> 08:42.200
And overclock these games, so it allows for very fast experimentation.

08:42.200 --> 08:51.720
And so that's kind of the reason why people have really gravitated towards using games

08:51.720 --> 08:52.720
in particular now.

08:52.720 --> 08:56.640
There is, you know, this Atari learning environment, which is basically using Atari games again

08:56.640 --> 09:00.840
as a platform to test interactive learning situations.

09:00.840 --> 09:05.560
And what I was trying to show in the particular Super Mario demo actually, so I should give

09:05.560 --> 09:08.480
a credit to Stefan Ross, whose work that video comes out of.

09:08.480 --> 09:16.520
So, you know, what they were trying to demonstrate was supervised learning is not adequate or a

09:16.520 --> 09:21.600
static pool of data is not adequate to do well in interactive learning problems because

09:21.600 --> 09:28.920
when you manipulate your environment, then if you act a certain way, you see situations

09:28.920 --> 09:32.800
that may or may not be present in your training data.

09:32.800 --> 09:39.560
So you know, if I know how to drive well, then I might not get into a car accident or at

09:39.560 --> 09:44.920
least not an obvious one, or I might not get stuck behind a slow driving car very often.

09:44.920 --> 09:49.480
And now, of course, you don't, if you don't see any data for how to recover from those

09:49.480 --> 09:53.680
errors, then, you know, your driving agent is in trouble.

09:53.680 --> 09:58.800
So that's the part I was trying to emphasize that when we try to do supervised learning

09:58.800 --> 10:04.720
in these interactive scenarios, then often our algorithms tend to make mistakes that we

10:04.720 --> 10:09.280
don't have in the training data and then they don't know how to recover from those.

10:09.280 --> 10:15.600
And so they get, you know, stuck in corners and they just fall maybe in pits and do all

10:15.600 --> 10:17.560
sorts of silly things that we just don't do.

10:17.560 --> 10:18.560
Right.

10:18.560 --> 10:22.560
So, walk us through how the approach is to interactive learning that you talked about

10:22.560 --> 10:23.560
addressed that problem.

10:23.560 --> 10:29.680
Yeah, so in some sense, here's an alternative we could think about, right?

10:29.680 --> 10:36.720
So let's say I was thinking about a conversational agent like a chatbot.

10:36.720 --> 10:37.720
So I have two options.

10:37.720 --> 10:42.480
I could look through many, many transcripts of, you know, how people have talked to

10:42.480 --> 10:47.240
each other, maybe in a controlled domain, even like a call center or something.

10:47.240 --> 10:50.480
And I can try to train an agent from this data.

10:50.480 --> 10:55.040
Again, this might have issues that maybe I said something that doesn't make sense to

10:55.040 --> 11:02.080
the user, maybe the respond in a way that a call center human agent would never encounter

11:02.080 --> 11:04.720
and I don't know how to respond now.

11:04.720 --> 11:09.760
Now imagine in this situation, instead of just being stuck and floundering, I could

11:09.760 --> 11:14.800
actually sort of fall back to a human agent and say, hey, I'm kind of uncertain about

11:14.800 --> 11:15.800
what to do right now.

11:15.800 --> 11:17.320
Can you bell me out, please?

11:17.320 --> 11:23.040
Right now I have learned how to recover from this mistake.

11:23.040 --> 11:27.120
So in future, even if I'm about to make this mistake, I'll be able to recover from it,

11:27.120 --> 11:28.120
right?

11:28.120 --> 11:36.440
And so there is this concept of learning in situations that I encounter that interactive

11:36.440 --> 11:40.120
learning algorithms usually embody, right?

11:40.120 --> 11:46.440
And another kind of thing that often comes up is just even, you know, these are like

11:46.440 --> 11:48.040
very ambitious tasks, of course, right?

11:48.040 --> 11:55.480
So let's think about something even which seems much more mundane to us, like, you know,

11:55.480 --> 12:00.840
ranking search results or recommending news articles, personalizing news articles.

12:00.840 --> 12:06.000
So maybe I have data of what users have been clicking on and I learned from this data

12:06.000 --> 12:11.880
and I think I have found something better to choose, right?

12:11.880 --> 12:13.080
How do I evaluate it?

12:13.080 --> 12:17.560
Well, if this other, these other sort of choices are never displayed to the users by my previous

12:17.560 --> 12:22.880
system, I have no data that can actually back that these choices are good, right?

12:22.880 --> 12:27.560
And so we only know the performance of the thing that the user actually clicked on.

12:27.560 --> 12:30.920
We don't have any information about how the other things would have performed.

12:30.920 --> 12:31.920
Exactly.

12:31.920 --> 12:36.120
And so we run into this problem that we don't even know how to evaluate a system that does

12:36.120 --> 12:40.760
something different from what we have in the data and basically interactive learning

12:40.760 --> 12:47.480
exactly tries to break this gap or tries to come up with techniques for essentially how

12:47.480 --> 12:49.280
do you have to kind of learn on the job?

12:49.280 --> 12:52.440
You have to learn on the fly in order to address these situations.

12:52.440 --> 12:59.320
No, it seems like you've talked about two different things.

12:59.320 --> 13:03.400
One is, you know, you've got your machine prediction.

13:03.400 --> 13:07.880
How do we kind of pop out of that loop and get input from the human and the other makes

13:07.880 --> 13:13.640
me think more of like multivariate testing, AB testing, that kind of explore exploit kind

13:13.640 --> 13:14.960
of scenarios.

13:14.960 --> 13:15.960
How are those related?

13:15.960 --> 13:16.960
Yeah.

13:16.960 --> 13:27.440
So in some sense, explore exploit is kind of one step further from this, you know, like

13:27.440 --> 13:31.960
evaluation problem that I'm mentioning.

13:31.960 --> 13:38.880
If you think about exploration, why do you explore, right, you explore because sometimes

13:38.880 --> 13:42.280
you'll, you'll try one thing, sometimes you'll try another thing.

13:42.280 --> 13:47.880
So let's kind of in a statistical sense, if we zoom out and think about what's happening

13:47.880 --> 13:51.040
in expectation, roughly, you're trying everything.

13:51.040 --> 13:54.520
And in expectation, at least sort of as a population level, you're getting feedback

13:54.520 --> 13:59.880
about everything, right, and what, as a result, that enables you to do is it enables you

13:59.880 --> 14:03.640
to evaluate every choice that you could have made, right?

14:03.640 --> 14:09.280
So implicit in explore exploit is this ability to evaluate in interactive settings.

14:09.280 --> 14:13.720
And so what I try to do is I, I mean, explore exploit kind of in my mind consists of two parts.

14:13.720 --> 14:18.560
There is this ability to evaluate, which I think is very crucial in its own because people

14:18.560 --> 14:22.520
currently do often evaluation by doing, like you're saying, A, B testing or multivariate

14:22.520 --> 14:26.760
testing, which is a horribly inefficient way of going about the task.

14:26.760 --> 14:30.160
And explore exploit basically gives you a much more data efficient way of doing the same

14:30.160 --> 14:31.160
thing.

14:31.160 --> 14:34.800
And then it lets you, in fact, do something much more, it lets you actually refine your

14:34.800 --> 14:40.000
models in real time and update them and, you know, do this sort of really online learning,

14:40.000 --> 14:41.000
which is, which is great.

14:41.000 --> 14:44.720
But even if you don't want to do that, just this evaluation is something you can already

14:44.720 --> 14:45.720
get a lot of value out of.

14:45.720 --> 14:50.480
So that's why I kind of try to present both the pieces in their own right.

14:50.480 --> 14:57.440
But in some sense, yes, explore exploit is the is the general sort of overarching solution

14:57.440 --> 15:02.160
that addresses basically both of the issues.

15:02.160 --> 15:06.960
And why is it, why is it in that being so much more data efficient than the traditional

15:06.960 --> 15:07.960
purchase?

15:07.960 --> 15:08.960
Right.

15:08.960 --> 15:12.920
So there are two two important hallmarks.

15:12.920 --> 15:17.040
So so let's, let's first start from a non contextual setting, right?

15:17.040 --> 15:20.480
So let's just say you're trying to basically find the most popular new story that works

15:20.480 --> 15:22.800
for everyone.

15:22.800 --> 15:29.480
Then the a B testing way, so to say of going about it would be, let's say you have 10,

15:29.480 --> 15:33.360
you give one 10th of traffic to each one of them, week later, you see which one did the

15:33.360 --> 15:34.360
best.

15:34.360 --> 15:35.360
Okay.

15:35.360 --> 15:40.560
The explore exploit we are doing is you say, okay, well, initially I'll start giving 10%

15:40.560 --> 15:42.160
traffic to each one.

15:42.160 --> 15:46.280
The moment something starts to look better than the other second, I dynamically adjust

15:46.280 --> 15:47.280
my traffic.

15:47.280 --> 15:48.280
Right.

15:48.280 --> 15:52.840
So everything is getting only enough traffic that I need to rule it out as being inferior,

15:52.840 --> 15:53.840
right?

15:53.840 --> 15:54.840
And so that's already data efficient.

15:54.840 --> 15:55.840
All right.

15:55.840 --> 16:01.440
Now let's think about personalization, about contextualization, right?

16:01.440 --> 16:07.560
So so you said you're from St. Louis, you're hopefully not the only user from St. Louis

16:07.560 --> 16:09.960
who visits MSN.

16:09.960 --> 16:18.360
So so you visit, I, I maybe give you a randomized choice of new story.

16:18.360 --> 16:22.800
And maybe another user who has more or less the same features as you visits, right?

16:22.800 --> 16:26.280
I maybe try a different new story for them.

16:26.280 --> 16:29.680
Now what I've done is, she let's back up.

16:29.680 --> 16:36.560
So so so usually for personalization, let's again contrast with it with an a B test.

16:36.560 --> 16:43.320
So maybe test would say, okay, so, you know, I'll, I'll run this experiment to on, on some

16:43.320 --> 16:47.960
percent of data to evaluate, be on some percent of data to evaluate a, right?

16:47.960 --> 16:50.880
So each data point is either going to a or to be, right?

16:50.880 --> 16:53.400
There is no sort of data sharing happening.

16:53.400 --> 16:59.800
How, when you have a large class of available options, available models that you might want

16:59.800 --> 17:00.800
to pick out of.

17:00.800 --> 17:01.800
Right.

17:01.800 --> 17:08.440
If you've got tons of news, one set of parameters for neural network defining one model.

17:08.440 --> 17:10.000
So you really have an infinite number of them.

17:10.000 --> 17:13.960
But let's just pretend for now, they're a billion, okay?

17:13.960 --> 17:20.400
So of course, for a given user, many of these models would make the same choice, right?

17:20.400 --> 17:24.800
So if you randomize the new story that you present to the user, then and you look the,

17:24.800 --> 17:29.360
look at the outcome, then suddenly you have information about all the different models

17:29.360 --> 17:35.720
that made the story you displayed that made the same recommendation to this user, right?

17:35.720 --> 17:40.360
And you can use, share this user's information across all of them.

17:40.360 --> 17:44.960
So you're effectively training a billion models in parallel?

17:44.960 --> 17:48.080
You're effectively evaluating a billion models you parallel, right?

17:48.080 --> 17:51.560
And then because you're evaluating them in parallel, you can just choose the best one

17:51.560 --> 17:52.560
at the end.

17:52.560 --> 17:53.560
Right.

17:53.560 --> 17:54.560
Right.

17:54.560 --> 17:55.560
Well, you have optimization, right?

17:55.560 --> 18:00.800
So it's this data sharing that's crucial and there is a certain sense, there's a precise

18:00.800 --> 18:05.280
mathematical sense in which you can prove that this is, as a result, if you think about

18:05.280 --> 18:09.520
doing the same evaluation of a billion things through A, B testing versus what we call

18:09.520 --> 18:17.160
multi-world testing, then you require exponentially fewer samples in our approach.

18:17.160 --> 18:24.120
And there is kind of a precise theory in papers backing up that claim.

18:24.120 --> 18:26.120
So that's kind of the crux.

18:26.120 --> 18:27.120
Okay.

18:27.120 --> 18:28.120
Okay.

18:28.120 --> 18:33.560
So we've touched on this throughout the discussion, but one of the big areas that you apply

18:33.560 --> 18:35.760
this on is impersonalization.

18:35.760 --> 18:39.720
Can you talk a little bit about the use case and some of the background there?

18:39.720 --> 18:40.720
Yeah.

18:40.720 --> 18:49.520
So, you know, I think really, I mean, you know, it's 2016, I think it's great that some

18:49.520 --> 18:54.560
of the things we use actually do adapt to our tastes over time, but it's a pity that more

18:54.560 --> 18:55.880
of them don't, right?

18:55.880 --> 19:00.960
And I feel like everything I do should learn about me, especially because they are collecting

19:00.960 --> 19:01.960
a ton of data about me.

19:01.960 --> 19:05.600
So, it might as well put it to some good.

19:05.600 --> 19:12.320
And basically, I think this is, personalization is the one that's most interesting to me also

19:12.320 --> 19:17.640
is because like I was describing, once you start thinking, if you're just trying to

19:17.640 --> 19:22.400
evaluate 10 things, fine, there is some difference between A, B testing and multiple testing, but

19:22.400 --> 19:28.240
it's really when you start thinking about creating personalized, and if you're only doing

19:28.240 --> 19:32.640
10 things, then in some sense, a very smart person might just look at them and by their

19:32.640 --> 19:35.520
gut feeling, pick out the best one, right?

19:35.520 --> 19:40.400
But once you start thinking about personalized models, then first of all, it doesn't scale

19:40.400 --> 19:41.800
for humans to do it.

19:41.800 --> 19:43.840
It doesn't scale for A, B testing to do it.

19:43.840 --> 19:50.240
You really need a different technology to do it, and that's why I think it's a very

19:50.240 --> 19:55.080
well-suited scenario to something like multiple testing and contextual bandits.

19:55.080 --> 20:00.840
And so, that's where we've found the most, also, excitement from, you know, we've

20:00.840 --> 20:07.960
talked to a broad range of product managers, and that's the aspect that usually receives

20:07.960 --> 20:11.840
the most appeal to them.

20:11.840 --> 20:14.160
And what kind of results are you seeing with that?

20:14.160 --> 20:18.640
So with, I mean, I have most substantive results with MSN.

20:18.640 --> 20:24.800
We have a lot of other kind of experiments going on, but I think MSN ones are the one

20:24.800 --> 20:29.760
I can speak of most authoritatively, and basically, they've reliably, so, you know, they

20:29.760 --> 20:34.760
have like this web page with many different A, which they kind of logically partition

20:34.760 --> 20:39.280
into many different areas, so they've kind of applied our system to many different

20:39.280 --> 20:46.280
areas now, and reliably in all of those, they found kind of with minimal to no tuning,

20:46.280 --> 20:54.320
they found always that they were getting actually improvement in most kind of user engagement

20:54.320 --> 20:56.960
metrics that they track.

20:56.960 --> 21:01.720
What was even more interesting, and this kind of really reflects the personalization aspect,

21:01.720 --> 21:02.720
right?

21:02.720 --> 21:07.600
So they had been running these experiments in the US market, and then the Olympics came

21:07.600 --> 21:10.880
around, and somebody had an idea, hey, let's try this in Brazil, right?

21:10.880 --> 21:17.520
So this Portuguese, we've not changed anything in the system, except they, you know, like

21:17.520 --> 21:23.000
the user browsing history, they had it for those users on the Portuguese articles, of course,

21:23.000 --> 21:27.240
and they did like a Portuguese topic model for, so just the feature extraction part was

21:27.240 --> 21:33.120
a little language specific, nothing about the machine learning changed, and we got, you

21:33.120 --> 21:38.560
know, double to triple digit improvement over the existing system, just deploying this

21:38.560 --> 21:45.760
out of the box, and they started running it on 100% traffic, so this, it really does work.

21:45.760 --> 21:50.600
Is that level of improvement on par with what you saw in English, or greater, or slightly

21:50.600 --> 21:51.600
less?

21:51.600 --> 21:57.200
You know, that's kind of difficult to judge very well, because also they have different

21:57.200 --> 22:02.680
amounts of data and different level of performance of the baseline system in different markets,

22:02.680 --> 22:07.640
so it's a bit hard to compare that.

22:07.640 --> 22:14.120
But I think the important thing for them was that it was really with very little customization

22:14.120 --> 22:21.040
things were robustly able to transfer from one market to another, right?

22:21.040 --> 22:28.760
And so this approach, you've talked a little bit about it being more data efficient,

22:28.760 --> 22:34.520
but how does that translate to the actual implementation and computational efficiency?

22:34.520 --> 22:39.520
Yeah, so, I mean, is that an issue for these kinds of problems at that scale, or?

22:39.520 --> 22:44.160
I mean, there, yes, you have to be mindful about computation, we are mindful about computation,

22:44.160 --> 22:49.560
we've tried to make these things as efficient as possible, so with MSN, for instance, we

22:49.560 --> 22:54.840
were running in, you know, the front end of their servers, and we had like five percent

22:54.840 --> 22:59.160
of show performance overhead on their system, which was deemed fine for the value, so there

22:59.160 --> 23:06.560
is obviously some performance that is lost, that is incurred, but I mean, we've implemented

23:06.560 --> 23:13.640
these algorithms very carefully, and a lot of the costs can be amortized essentially.

23:13.640 --> 23:18.760
So now the 5% running on the front end, that's for model evaluation, is there a training

23:18.760 --> 23:19.760
step?

23:19.760 --> 23:23.920
Yeah, so the nice thing is that training step is running asynchronously in the cloud,

23:23.920 --> 23:29.880
and that, I mean, that's entirely, again, I mean, currently, even at MSN, first of all,

23:29.880 --> 23:36.520
one thing that's nice about doing things online is, right, you're not really thinking about

23:36.520 --> 23:40.760
having to deal with a billion or a trillion records all at once, you're just streaming

23:40.760 --> 23:41.840
over them.

23:41.840 --> 23:47.480
So scales become nicer, but, but, so with MSN, we were still able to do all the training

23:47.480 --> 23:51.960
on one machine, in fact, in the background, but even if that's not the case, it's pretty

23:51.960 --> 23:55.960
easy to parallelize the training algorithms, and we support that.

23:55.960 --> 24:02.040
So, yeah, I mean, you can, and we've, the only thing you have to be careful about is, so

24:02.040 --> 24:07.960
we've made sure that, we try to keep the system very reproducible, because one of the frustrating

24:07.960 --> 24:11.440
things we've encountered over and over again is when something goes wrong in these complex

24:11.440 --> 24:16.280
systems, it's really hard to trace downward, with parallelization, sometimes it can be

24:16.280 --> 24:21.280
a little bit more tricky, because all of the order of events and so on, so we recommend

24:21.280 --> 24:25.320
as far as possible to avoid it, but it's definitely possible to do it.

24:25.320 --> 24:29.920
All right, if you can do it for MSN on one machine, then a lot of people will be able to get

24:29.920 --> 24:33.200
pretty far on a single machine.

24:33.200 --> 24:37.400
And you've published papers about this, like, what have someone wanted to, to try out this

24:37.400 --> 24:38.400
approach?

24:38.400 --> 24:40.400
Like, what's the best way to, for them to learn about it?

24:40.400 --> 24:50.000
Yeah, so there is a short URL, aka.ms-mwt, that website has a ton of resources on it, and

24:50.000 --> 24:58.000
it has both, you know, more, like, do it yourself, type, guide, type things, if you directly

24:58.000 --> 25:02.080
want to get your hands dirty, if you want to learn more about the science, it provides

25:02.080 --> 25:08.720
extensive, there's a very extensive white paper that provides links to you in more extensive

25:08.720 --> 25:14.080
research papers, and so on, so really, depending on how much detail you want, all of the resources

25:14.080 --> 25:15.680
are available on that website.

25:15.680 --> 25:16.680
Awesome, awesome.

25:16.680 --> 25:22.680
And the project itself is on GitHub, so, you know, all of the code is open source.

25:22.680 --> 25:26.280
You can play with the machine learning algorithms, the machine learning algorithms actually

25:26.280 --> 25:29.920
have been open source for several years now, so they've been, you know, tried out not

25:29.920 --> 25:33.720
just by us, but by others across the research community as well.

25:33.720 --> 25:36.960
And what are the algorithms based on?

25:36.960 --> 25:40.880
Like, what general classes of algorithms do these look like?

25:40.880 --> 25:48.800
So, I mean, so the way we do things is we take these interactive or, you know, contextual

25:48.800 --> 25:54.680
bandit learning problems, and we basically sort of what the learning algorithms look like,

25:54.680 --> 25:59.160
their massage and massage and massage the data, till you can essentially think of it as

25:59.160 --> 26:02.320
some sort of a multi-class classification problem.

26:02.320 --> 26:03.320
Okay.

26:03.320 --> 26:07.160
And so, now, go to party with your favorite multi-class classification algorithm.

26:07.160 --> 26:14.200
I mean, we implement our own for the complete pipeline, but, and we support a lot of different

26:14.200 --> 26:19.960
types of models, like, you know, linear models, shallow, feed-forward nets.

26:19.960 --> 26:23.480
We have, you know, matrix factorization type models.

26:23.480 --> 26:29.400
We have a whole variety of sort of very, very quick feature manipulations that are ingrained

26:29.400 --> 26:33.680
into the models that you can just do, so it's, so all of this is happening in a software

26:33.680 --> 26:38.320
called Wobbit, the all of the machine learning part, which has been around for several years

26:38.320 --> 26:43.960
and is one of the more performant software tools for machine learning out there.

26:43.960 --> 26:51.720
So, it's, it, it provides a lot of functionality, and if you want something that's not in it,

26:51.720 --> 26:57.920
then, you know, there are ways to plug into other machine learning libraries as well.

26:57.920 --> 27:04.640
And the, your use case there was on personalization, are there other use cases that you've seen

27:04.640 --> 27:06.840
to supply to?

27:06.840 --> 27:13.320
So, I mean, it depends on how far you want to extend personalization, the definition, right?

27:13.320 --> 27:20.720
So, one of the things we are currently trying to work on is, so, so we have users of Microsoft

27:20.720 --> 27:27.720
Bank and, you know, a lot of people in this country suffer from sleep disorders or just, you

27:27.720 --> 27:28.880
know, is the health band?

27:28.880 --> 27:29.880
Yes.

27:29.880 --> 27:34.600
Because of like stress or other reasons, they're just not sleeping well.

27:34.600 --> 27:40.240
And so, the band was for a while trying to give sleep insights, basically some, some

27:40.240 --> 27:47.400
recommendation to change your lifestyle in some small way that might help you sleep better.

27:47.400 --> 27:53.400
And so, for instance, we are trying to now do an experiment where we would choose which

27:53.400 --> 27:58.880
recommendations to show based on how the users sleep then respond to the recommendations

27:58.880 --> 27:59.880
or do this.

27:59.880 --> 28:05.880
So, I mean, I think of this as within the realm of personalization, but, you know, and,

28:05.880 --> 28:14.720
I mean, again, we haven't had conversations or more medical domain so far, but we are

28:14.720 --> 28:18.120
really hoping that we can get there in future.

28:18.120 --> 28:27.120
So, that's definitely one realm, other definitely, definitely a good chunk of the conversations

28:27.120 --> 28:32.520
we've had other than that are, I would say, around personalization of various sorts.

28:32.520 --> 28:37.360
But even like one of the interesting use cases, some of our actually, so some of the people

28:37.360 --> 28:42.280
in our research team are, so because of the system, we also have some systems researchers

28:42.280 --> 28:50.760
on the team, and, you know, system itself does a number of resource allocation choices

28:50.760 --> 28:57.800
and, you know, server is kind of distributing, doing load balancing and a lot of other resource

28:57.800 --> 28:59.200
allocation problems, right?

28:59.200 --> 29:05.320
So, what they've been curious is if they can apply even some of these techniques to core

29:05.320 --> 29:07.800
systems problems.

29:07.800 --> 29:13.400
We have some preliminary experiments with that, nothing I would call convincing yet, but

29:13.400 --> 29:17.920
it's actually pretty broadly applicable.

29:17.920 --> 29:23.960
And by core systems problems, are you thinking things like, you know, allocation of resources

29:23.960 --> 29:24.960
within a data center?

29:24.960 --> 29:30.720
Yeah, you can apply it at several different scales, so you can think about applying it

29:30.720 --> 29:35.640
at the level of a router, at the level of a NIC, at the level of an OS, level of data center

29:35.640 --> 29:38.920
or scale, do you learn in a data center, there are many different places you can think

29:38.920 --> 29:39.920
of it.

29:39.920 --> 29:46.560
And in some sense, I mean, a lot of these are basically currently working on top of hand-designed

29:46.560 --> 29:50.000
rules that some very smart people thought about the problem very carefully and designed

29:50.000 --> 29:51.000
it, right?

29:51.000 --> 29:55.520
But there's no reason why we can't make them more adaptive and more intelligent.

29:55.520 --> 29:59.840
So yeah, I think there is definitely a lot of potential there in like machine learning

29:59.840 --> 30:04.920
for systems type of area for these interactive learning situations.

30:04.920 --> 30:08.160
And so how would you kind of taking a step back to summarize?

30:08.160 --> 30:12.520
How would you characterize at the highest level, you know, if you've got a problem that

30:12.520 --> 30:15.560
looks like X, you know, this is, you know, a solution.

30:15.560 --> 30:16.560
Wonderful question.

30:16.560 --> 30:21.960
I actually do, when I start to talk to people who are interested, I usually give them a template

30:21.960 --> 30:24.800
and ask them if their problem fits into that template, right?

30:24.800 --> 30:30.200
So at the high level, there is this loop of, you observe the world, you take an action

30:30.200 --> 30:33.440
and you observe a reward, right?

30:33.440 --> 30:36.800
So it's important that you face this loop over and over again.

30:36.800 --> 30:41.800
One of the things you have to be careful about, for instance, is often when we start this

30:41.800 --> 30:45.840
conversation, people don't necessarily have a well-defined notion of a reward they can

30:45.840 --> 30:46.840
point to.

30:46.840 --> 30:47.960
And that's very important.

30:47.960 --> 30:52.760
If we don't define it well, the system will just learn some garbage, right?

30:52.760 --> 30:56.760
The other thing that's kind of important to think about is, like I said, contextual bandage

30:56.760 --> 31:02.920
problem, I was saying in the top, makes this assumption that when I take an action, it

31:02.920 --> 31:06.000
does not have influence on the next context I see, right?

31:06.000 --> 31:11.280
So something like a conversation just does not fit this, you know, what you say is not

31:11.280 --> 31:15.000
independent of what I said before.

31:15.000 --> 31:21.440
But something like recommendation systems is largely true.

31:21.440 --> 31:25.680
So that's something you have to keep in mind when you're thinking about how good a fit

31:25.680 --> 31:27.000
this might be to your problem.

31:27.000 --> 31:35.120
Now, that said, we have a lot of research expertise and research advances in also working

31:35.120 --> 31:42.080
out the situations where your actions modify the state is just that they're the software

31:42.080 --> 31:43.080
not quite there yet.

31:43.080 --> 31:47.480
I mean, we have some software, but it's not really a full-fledged system yet.

31:47.480 --> 31:48.480
Okay.

31:48.480 --> 31:49.480
Great.

31:49.480 --> 31:50.480
Great.

31:50.480 --> 31:55.760
Any other considerations or things that folks should know when they're thinking about

31:55.760 --> 31:57.360
this space?

31:57.360 --> 32:02.400
No, I would say, again, think, if you're thinking about these problems, think really hard

32:02.400 --> 32:03.400
about the reward.

32:03.400 --> 32:05.600
That's the one that usually goes wrong.

32:05.600 --> 32:11.440
And the other thing is, I think we've tried very carefully in the various materials we've

32:11.440 --> 32:17.520
prepared to outline all the usual things that go wrong because one of the things we find

32:17.520 --> 32:23.720
is even after we talk to people, they often fall back into those traps, so it's very important

32:23.720 --> 32:27.560
to think through those carefully and make sure you don't fall into them.

32:27.560 --> 32:29.720
And what are some of those traps?

32:29.720 --> 32:35.720
So a lot of those traps are, well, essentially, even, I mean, it's really tempting to say that

32:35.720 --> 32:39.520
I have some observational data collected from my system.

32:39.520 --> 32:42.000
Let's just do some machine learning with it.

32:42.000 --> 32:45.400
And this almost never works in a reliable manner.

32:45.400 --> 32:47.840
And there are various levels at which this manifests.

32:47.840 --> 32:52.040
So one thing you might want to do is, oh, I ran this experiment and actually things are

32:52.040 --> 32:53.040
working quite well.

32:53.040 --> 32:54.720
Now, why don't I turn off the experimentation?

32:54.720 --> 32:56.040
I turn off the randomization.

32:56.040 --> 32:57.040
No.

32:57.040 --> 33:09.480
I mean, you know, preferences change or just various, various subtle bugs arise just due

33:09.480 --> 33:11.800
to the way people are recording things.

33:11.800 --> 33:14.760
So of course, if you do everything with our system, then, you know, we've designed things

33:14.760 --> 33:21.160
in a way that they shouldn't arise, but often people want to use their own custom

33:21.160 --> 33:23.160
components for parts of things.

33:23.160 --> 33:28.920
So if you're thinking about doing that, it's really important that you look into the

33:28.920 --> 33:33.640
failure modes that we emphasize and make sure you don't fall into those.

33:33.640 --> 33:38.480
And the other thing is, yeah, even on top of our system, when you're building something,

33:38.480 --> 33:42.600
it's important to think about the reproducibility of everything, because that's the one thing

33:42.600 --> 33:46.120
we found really was key when even with MSN, right?

33:46.120 --> 33:52.480
It wasn't all sort of a bed of roses initially, there were quite a few hiccups and because

33:52.480 --> 33:56.240
we kept everything reproducible, we could quickly figure out where the problem was.

33:56.240 --> 33:57.240
Right.

33:57.240 --> 33:58.240
Right.

33:58.240 --> 33:59.240
Awesome.

33:59.240 --> 34:00.240
Awesome.

34:00.240 --> 34:01.240
But why don't you repeat that URL once more time?

34:01.240 --> 34:02.240
Yeah.

34:02.240 --> 34:04.240
It is aka.ms slash MWT.

34:04.240 --> 34:05.240
Okay.

34:05.240 --> 34:09.320
And can folks, if they've got questions, can they contact you through that URL?

34:09.320 --> 34:10.320
Yes.

34:10.320 --> 34:11.320
Absolutely.

34:11.320 --> 34:12.320
Awesome.

34:12.320 --> 34:13.320
Awesome.

34:13.320 --> 34:14.320
Well, thanks so much, Alex.

34:14.320 --> 34:15.320
Great to meet you and appreciate the talk.

34:15.320 --> 34:16.320
Yeah.

34:16.320 --> 34:17.320
I'm talking to you.

34:17.320 --> 34:18.320
Thanks.

34:18.320 --> 34:20.320
All right, everyone.

34:20.320 --> 34:22.440
That's our show for today.

34:22.440 --> 34:27.240
Once again, thanks so much for listening and for your continued support.

34:27.240 --> 34:31.040
Don't forget to share your favorite quotes for a twimmel sticker.

34:31.040 --> 34:32.040
These stickers are great.

34:32.040 --> 34:33.760
You're going to love them.

34:33.760 --> 34:38.560
You can share your favorite quote via the show notes page, via Twitter, via our Facebook

34:38.560 --> 34:42.480
page, or via a comment on YouTube or SoundCloud.

34:42.480 --> 34:45.920
And don't forget to hit that iTunes link and leave us a review.

34:45.920 --> 34:52.160
The notes for this show will be up on twimmelai.com slash talk slash 17, where you'll find links

34:52.160 --> 34:56.520
to Alex and the various resources mentioned in the show.

34:56.520 --> 35:12.640
Catch you next time.

