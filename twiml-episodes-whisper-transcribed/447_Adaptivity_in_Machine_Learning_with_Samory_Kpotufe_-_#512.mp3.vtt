WEBVTT

00:00.000 --> 00:19.160
All right, everyone. I am here with Samary Potoufet. Samary is an Associate Professor at Columbia University.

00:19.160 --> 00:26.080
Samary, welcome to the Twomol AI podcast. Thank you. Thank you, Sam. Let's get started by

00:26.080 --> 00:33.960
having you share a little bit about your background. You work in statistical machine learning. How

00:33.960 --> 00:45.600
did you get there? How did I get into statistical machine learning? Somehow, somehow, I

00:45.600 --> 00:56.400
started in math in undergrad in mathematics. At some point, I was doing mostly pure math in

00:56.400 --> 01:02.800
undergrad. At some point, I wanted to do something where I could see the applications of math,

01:02.800 --> 01:09.520
where I could see the impact of mathematics. I wasn't so sure, so I went off and worked for

01:09.520 --> 01:16.400
some time. Then, at one point, I was watching TV and so robots on TV and got interested in how

01:16.400 --> 01:22.880
that was done. So, I started applying for grad school and eventually I got into machine learning.

01:22.880 --> 01:27.280
I didn't know at the time that it was going to become a big field and now it's a big field. So,

01:27.280 --> 01:33.760
great. That's awesome. Tell us a little bit about your research interest. What do you focus on?

01:33.760 --> 01:42.960
Very generally, I mean, I'm interested mostly in machine learning itself. However, in statistical

01:42.960 --> 01:50.080
aspect of machine learning, and by statistical aspect, I mean questions such as how much resources

01:50.720 --> 01:59.120
are needed to achieve low error in classification, for instance. By resources, resources could be

01:59.120 --> 02:09.760
number of samples. It could also involve constraints on the problem. So, maybe we cannot always have

02:09.760 --> 02:16.080
labeled samples. Maybe we have to return, maybe the classifier has to return a classification

02:17.280 --> 02:23.280
fast. So, there are all these potential constraints in real world applications that I'm interested in.

02:23.280 --> 02:29.520
And given those constraints, what are the best, what is the best possible performance of classification

02:29.520 --> 02:37.920
procedures? So, at a high level, that's that's my general interest. But, but I'm interested in

02:37.920 --> 02:42.160
being able to say mathematically, being able to give guarantees, being able to say mathematically,

02:42.880 --> 02:47.120
this is the nature of the problems we are looking at and this is the nature of the best possible

02:47.120 --> 02:52.880
algorithms for these problems. Okay. Yeah, I was going to mention that, you know, we all care

02:52.880 --> 02:57.680
about the amount of data that's required. Yeah. But, there's a little bit of a different,

02:57.680 --> 03:04.400
you approach that question from the perspective of a theoretician as opposed to a practitioner.

03:04.400 --> 03:09.360
I'm wondering if you can comment on kind of the relationship between theory and practice. And

03:09.360 --> 03:17.600
you're the relationship for your work, you know, to practitioners. So, I'll say this, I feel like

03:17.600 --> 03:27.040
the theoreticians within machine learning should mostly be inspired by practice. And so,

03:27.040 --> 03:32.400
I try my best to be inspired by practice in the sense that I will look at practical problems.

03:32.880 --> 03:39.200
And as myself, what would be the key questions that a practitioner might need answered,

03:39.200 --> 03:44.080
and that the data might not reveal at once? Right. And so, I'll give you some examples of such

03:44.080 --> 03:56.000
questions. And can we use math to sort of understand these questions? And so, that's at a very high

03:56.000 --> 04:02.480
level, given to give an exact example, if someone thinks for instance of clustering, right?

04:03.040 --> 04:07.600
There are tons of different clustering procedures out there, tons of clustering algorithms out there.

04:07.600 --> 04:16.000
And the practitioner has data in front of them. And they might ask, which one of these clustering

04:16.000 --> 04:20.640
algorithms should I use for my particular application for my particular data? That's a question

04:20.640 --> 04:27.040
that is very hard to answer directly just from practice. Why? Because I'm taking clustering here,

04:27.040 --> 04:30.880
because it's hard to even test anything. You don't have label data. It's hard to test the

04:30.880 --> 04:38.000
quality of your clustering if you don't have ground truth. So, here you don't have ground truth. So,

04:38.000 --> 04:45.280
so the main thing you can rely on is modeling your problem and asking, on the these assumptions

04:45.280 --> 04:51.200
I can make on my data, what is the best possible procedure? And so, we try to answer those questions

04:51.200 --> 05:01.120
mathematically. I hope that goes at it and says it a bit for you. And maybe talk more concretely

05:01.120 --> 05:08.400
about the types of problems that you are trying to model. Do you think about them? Kind of very

05:08.400 --> 05:15.760
broadly at the level of clustering, classification, that kind of thing, or do you make more granular

05:15.760 --> 05:22.080
assumptions about, I guess, both the problems and the data and the constraints and other things?

05:22.080 --> 05:30.800
It depends on how far the particular question I'm interested in is. There are some questions where

05:30.800 --> 05:39.760
we are very much at a high level, where we are thinking about the basic task, such as classification,

05:39.760 --> 05:47.200
clustering, and such. And then there are problems that we understand quite well already. And in

05:47.200 --> 05:52.160
those cases, I might start thinking about specific algorithms and how to distinguish between

05:52.160 --> 06:01.200
specific algorithms. I hope I'm answering the question right. Maybe you want to make it a bit more

06:01.200 --> 06:09.200
precise. Should I make it? I think the way to make it a bit more concrete is for us to talk about

06:09.200 --> 06:17.760
specific problems that you are interested in and research and kind of drill into detail there.

06:17.760 --> 06:22.720
Let's start with a recent paper or something that you're excited about.

06:22.720 --> 06:28.720
Since we were talking about clustering, let me just give some concrete problems in clustering.

06:28.720 --> 06:37.680
Right. First, we all know at a high level how one might be fine clustering. The problem of

06:37.680 --> 06:43.840
clustering is, I believe that the data comes from different groups, different subgroups.

06:43.840 --> 06:52.880
I don't have labels. However, I'm hoping that at least geometrically in some space,

06:52.880 --> 06:59.040
I should be able to discover that the data groups well into so many groups. Right.

06:59.040 --> 07:06.160
Right. So the issue that comes up right away is what do we mean by the data groups well?

07:06.160 --> 07:12.960
We can say, for instance, that the data groups well in the sense that the data points belong

07:12.960 --> 07:19.440
into the same group are closer to each other than to other data points. And that gives a sense of

07:19.440 --> 07:24.480
they close their well. But the moment I say they are closer to each other, that means I'm

07:24.480 --> 07:29.760
making an assumption on a notion of distance. How far they should be from each other.

07:29.760 --> 07:33.840
And then the question comes up, what notion of distance is the right notion of distance

07:34.480 --> 07:41.120
here that I should be using. Right. So that all depends sort of on the downstream task.

07:41.920 --> 07:46.960
And so the theoretician might then start asking that question, what is the downstream task?

07:46.960 --> 07:53.520
And then what is the right notion of distance for this downstream task? Or we can go higher and say,

07:53.520 --> 07:58.240
how do we define clustering at all properly? Maybe this is not the right way to define clustering.

07:58.240 --> 08:01.200
Another way to define clustering is probably through densities.

08:02.960 --> 08:07.760
What I mean by that is, if I throw a sand on the floor,

08:07.760 --> 08:16.480
right, I might say that or it closes into the sand clustered into a few groups. And what do I mean

08:16.480 --> 08:23.200
by that? There is a different density of sand in different parts of the space, the floor here.

08:23.200 --> 08:29.520
Right. And so that gives me a different notion, a different notion of clustering, maybe what I'm

08:29.520 --> 08:38.160
trying to find in my data are regions of high density of points. Now that's a whole other, sorry.

08:38.160 --> 08:45.760
Yeah, you would think that the two ways of looking at this distance and density there,

08:46.720 --> 08:49.600
you know, they're measuring the same underlying phenomenon. There's a lot of

08:49.600 --> 08:57.520
there, but maybe one mathematical formulation or expression lends itself to the way you're

08:57.520 --> 09:04.000
trying to approach the problem. Yeah. Exactly. Exactly. Because that falls back also again into

09:04.000 --> 09:09.600
what do I even mean by the distance between points in the first formulation? In the first

09:09.600 --> 09:13.360
formulation, there are many different notions of distance that I might be looking at.

09:13.360 --> 09:21.840
Yeah. And in this formulation here, the notion of a density is a bit more of a robust notion

09:21.840 --> 09:30.000
in this other formulation. And then let's say that there is such a, that I agree with that notion

09:30.000 --> 09:35.120
that this is my notion, it's the regions of highest density. Right. Then there are tons of

09:35.120 --> 09:40.960
questions that come up. What level of density do I look at? If I look at a particular resolution,

09:40.960 --> 09:47.440
I might see particular clusters. If I zoom down, I might see even more clusters. So there are

09:47.440 --> 09:54.560
questions that come up right away as to what threshold and what threshold am I calling my notion

09:54.560 --> 10:04.400
of density? And can we answer that automatically with an algorithm? Can an algorithm discover that

10:04.400 --> 10:09.840
automatically? And in what situations, depending on the downstream task, what notion of

10:09.840 --> 10:15.680
closer ratio we'll be using? So the type of questions that come up there are, so first, you

10:15.680 --> 10:19.920
define a notion of clustering. Next, you ask the question, on the notion of clustering,

10:20.560 --> 10:30.000
is it possible at all to discover the clusters? To what level can I discover the clusters,

10:30.000 --> 10:37.040
meaning what error do I expect if I have so many data points? So here I'm assuming there is a

10:37.040 --> 10:40.800
ground truth, of course. And what error do I expect with respect to the ground truth,

10:41.600 --> 10:45.280
depending on the number of data points that I have? And then the next question

10:45.280 --> 10:54.000
becomes that of what I call adaptivity. Adaptivity of people might call it self-tuning and such.

10:54.000 --> 11:00.480
Every cluster in procedure comes up with all sort of tuning parameters. Here, an implicit

11:00.480 --> 11:06.240
tuning parameter that I threw in a second ago is if I talk about densities while density level,

11:06.240 --> 11:12.720
while level, what resolution? And that's sort of a tuning parameter. And so are there algorithms

11:12.720 --> 11:18.160
that can look at the data by themselves? And discover also the right tuning parameter.

11:18.160 --> 11:23.440
And I called, we call that, at least in statistics, it's called adaptivity. And machine learning

11:23.440 --> 11:31.680
people tend to call that auto-ML or self-tuning and such. And so, but those are the type of questions

11:31.680 --> 11:40.080
that me and other theoreticians ask, are these possible at all? Is this possible? And if it is

11:40.080 --> 11:45.920
possible, what are the family of procedures that achieve these goals?

11:47.200 --> 11:54.400
And so tying back to kind of this, you know, the problem of the practitioner as the touchstone

11:54.400 --> 12:02.240
or inspiration is the ultimate goal to be able to say, if your problem looks like this or if your

12:02.240 --> 12:08.800
data looks like this, then you want to use L1 norm, L2 norm, whatever, or, you know, this set of

12:08.800 --> 12:15.920
density metrics versus that set of density metrics. Exactly. So that would be one goal. That would be

12:15.920 --> 12:21.120
one way the theoretician can be useful to the practitioner, right, is to be able to say that

12:21.120 --> 12:27.920
for these types of problems, this is the algorithms that seem to be, or these are the decisions,

12:27.920 --> 12:36.960
the practical decisions that seem to be most appropriate. Yeah. Right. The eventual, the big goal,

12:36.960 --> 12:43.280
I think, for any machine learner, whether theoretician or non-territician, is to come up with data algorithm

12:43.280 --> 12:49.600
that is sort of almost a black box and can decide on its own which situation it is in by just looking

12:49.600 --> 12:54.320
at the data. And that falls back into what I was calling adaptivity. It self-tunes. At the end,

12:54.320 --> 12:59.840
it has nearly no tuning parameter because all its internal tuning, it does on its own.

12:59.840 --> 13:08.640
And identifies the setting or the situation in which it is and adjust itself. So it's a

13:08.640 --> 13:16.640
grand goal. We do know that very generally, it's not possible. But we also do know that we didn't

13:16.640 --> 13:24.880
turn on the certain restrictions on the universal problems to say it is possible. And in fact,

13:24.880 --> 13:30.320
to some extent, humans do it. But it's because sort of our universal problem is restricted.

13:33.120 --> 13:42.480
And these kind of adaptive algorithms is that something that you that, well, clearly,

13:42.480 --> 13:47.200
this is something that you're interested in and research are there. You know, specific examples

13:47.200 --> 13:58.240
of research that you've done that in this area? Yeah, yeah. So, for instance, I'll give a

13:58.240 --> 14:04.560
high-level example. Here, during my PhD and maybe a little bit after my PhD,

14:05.600 --> 14:10.800
here are some of the problems I was looking at. I was looking at simple questions

14:10.800 --> 14:14.720
having to do with nearest neighbor methods. These are the most basic procedures out there,

14:14.720 --> 14:21.600
right, nearest neighbor methods. We do know that nearest neighbor methods when the dimension

14:21.600 --> 14:29.200
of the data is very high, do quite poorly in general. And you can show it mathematically that

14:29.200 --> 14:33.280
their convergence rates, whether you're in classification or regression, that their convergence

14:33.280 --> 14:44.080
rate gets worse with dimension. And dramatically worse with dimension. In other words,

14:44.080 --> 14:53.360
how dramatic you need a sample size exponential in dimension in order to achieve decent convergence,

14:53.360 --> 15:00.080
right, at least in the worst case. So that's how we quantify that. So that's always been known,

15:00.080 --> 15:03.840
that dimension, and it's called the curse of dimension, that dimension is an issue.

15:03.840 --> 15:16.800
However, people then felt like maybe we can, if it so happens that the data lies, the data is

15:16.800 --> 15:25.680
very high-dimensional, but intrinsically is low-dimensional. So examples of that, one example

15:25.680 --> 15:34.320
that I like to give is consider a robotic arm with sensors on the robotic arm. It has a lot of

15:34.320 --> 15:40.160
sensors, so it's generating a very high-dimensional data set. However, the arm has few degrees of

15:40.160 --> 15:47.600
freedom. And so we expect that that data lies on the lower-dimensional surface in a very high-dimensional

15:47.600 --> 15:56.960
space. And so what people don't try to do was come up with procedures called manifold learning,

15:58.240 --> 16:05.280
procedures, which will try to discover that manifold, and then re-represented that in that

16:05.280 --> 16:11.680
sort of manifold space so that it's now lower-dimensional, and then try to run these algorithms

16:11.680 --> 16:17.360
such as nearest neighbors in this lower-dimensional space, where the hope is that it will perform much

16:17.360 --> 16:22.320
better here. So again, stepping back, we're starting with a very high-dimensional problem,

16:22.880 --> 16:29.040
but we know that in a lot of machine learning applications, even though the data appears

16:29.040 --> 16:35.200
high-dimensional, it's in fact often low-dimensional, we just need to discover that lower-dimensional,

16:35.200 --> 16:41.520
medium, and then run the data, run the algorithm in that new space, in that lower-dimensional

16:41.520 --> 16:50.480
space, right? So is manifold learning an example of dimensionality reduction?

16:50.480 --> 16:56.400
Yeah, exactly. It's an example of non-linear dimensionality reduction.

16:57.200 --> 17:00.480
So now, what you've done is you've increased the pipeline.

17:00.480 --> 17:07.440
So now you have your manifold learning procedure, which also has to make a decision, multiple

17:07.440 --> 17:15.440
internal decisions. What dimension is the manifold? Is it really a manifold? Is it really a smooth

17:15.440 --> 17:23.600
manifold, or is it just a bunch of subspaces of different dimension? It has a lot of internal

17:23.600 --> 17:29.600
decisions to make. So in the end, your entire pipeline becomes this manifold learning plus the

17:29.600 --> 17:34.720
eventual classification, right? And you increase the pipeline, you've added in a lot of

17:38.000 --> 17:44.000
tuning parameters. And so the question of adaptivity comes in. Is there just one algorithm

17:44.000 --> 17:50.240
that I can look at that automatically does as well as if I found the right manifold, the right

17:50.240 --> 17:58.160
dimension, everything? Okay. So that's where adaptivity comes in. What am I being adaptive to? I'm

17:58.160 --> 18:03.440
being adaptive to the structure of the data without knowing a priori the structure of the data.

18:03.440 --> 18:12.000
So one of my earlier work was trying to understand to what extent it is that existing procedures,

18:12.000 --> 18:16.960
such as nearest neighbor, already adapt to the structure of the data, to the intrinsic structure

18:16.960 --> 18:23.120
of the data without needing manifold learning, without needing the other ways of

18:23.120 --> 18:29.040
of re-representing data, reducing dimension in some ways, dictionary learning is another one.

18:29.040 --> 18:33.360
So without needing dictionary, dictionary learning applies in the cases of sparsity where we

18:33.360 --> 18:41.040
believe that the data is high dimensional, but it's very sparse. And so it so turns out that a

18:41.040 --> 18:48.720
lot of existing procedures are automatically adaptive to the structure of the data. And that came

18:48.720 --> 18:54.000
out mathematically by looking at the problem mathematically and then asking, oh, if we say now that

18:54.000 --> 18:58.320
my data that is very high dimensional lies very close to a low dimensional subspace,

19:01.040 --> 19:06.800
what convergence can we prove? And it so turns out that the convergence we could prove is

19:06.800 --> 19:11.440
then in terms of the lower dimension rather than in terms of the higher dimension.

19:11.440 --> 19:21.680
So just so I can replay that the you know these algorithms like nearest neighbor that we could

19:21.680 --> 19:26.320
throw you know tons of different types of problems at some high dimensional

19:29.600 --> 19:35.280
will have problems in the general case with high dimensional data, but if the data happens to be

19:35.280 --> 19:45.200
the data that has this low dimensional structure. Yeah, exactly. Then the performance of those

19:45.200 --> 19:53.760
out it sounds like the performance of that algorithm is going to be more akin to what we'd

19:53.760 --> 20:00.240
expected the dimension of the data was low dimensionality. That's exactly what I'm saying. Yeah.

20:00.240 --> 20:05.280
Yeah. Here all of a sudden it turns out that these algorithms and it's not just nearest neighbors.

20:05.280 --> 20:11.360
We are starting to understand that it's many other algorithms. It's it's particular classification

20:11.360 --> 20:19.760
trees. It recently people understand that even things such as neural net net adaptive to

20:19.760 --> 20:27.280
intrinsic dimension in in these ways things such as support vector machines being understood as

20:27.280 --> 20:32.000
adaptive in these ways, but at that time when I started working on these problems it was a bit

20:32.000 --> 20:37.440
unclear what was adaptive in these ways. And so happened that it was yeah. Go ahead. I think I

20:37.440 --> 20:45.120
got my I have my question from before. It's like okay. Is it two different things to say that

20:45.680 --> 20:54.320
the algorithms will perform better because the data has this inherent low dimensionality,

20:54.320 --> 21:08.480
this inherent structure versus the algorithms are adaptive. Is it is it is it is it some combination

21:08.480 --> 21:16.320
of the hyper parameters or or something. Oh, okay. Okay. I see. Yeah. That makes them adaptive

21:16.320 --> 21:24.960
under a certain set of constraints of the data or is okay. I see. I see that. No, no. This is a very

21:24.960 --> 21:33.280
good question. Let me put it this way. So I had two things in mind that I would explain the

21:33.280 --> 21:38.320
question. So first I was talking about the entire pipeline. If you were to do dimension reduction

21:38.320 --> 21:43.600
and all that you could you see right away that one question that comes up right away there is

21:43.600 --> 21:49.840
what dimension should I reduce to. Okay. Which is the parameter of the problem. Yep. Right.

21:50.720 --> 21:56.400
What sort of low dimensional structure do I have? Right. It's another hyperparameter of the problem.

21:57.600 --> 22:06.240
And I'm using the term adaptivity here loosely to say that I'm adapting to these various hyperparameters

22:06.240 --> 22:13.680
of the problem without knowing them a priori. Yep. Exactly in the sense that you just explained

22:13.680 --> 22:20.480
which is that if the data happens to be so structured then we do automatically better without knowing

22:20.480 --> 22:26.320
that a priori. Now there is a nuance. What does it mean to do automatically better? Nier's

22:26.320 --> 22:34.400
never methods. I know just in fact I know just one algorithm. It's a family of algorithms. And it's

22:34.400 --> 22:42.480
a family even in the simplest possible ways. So I can decide to run a nearest neighbor by saying

22:42.480 --> 22:48.240
I'm only going to use the nearest data points or I'll use the k nearest data points. The two

22:48.240 --> 22:54.320
nearest or three or four those become parameters of the problem. Right. So now there is another

22:54.320 --> 22:59.280
level of adaptivity which is how do I choose those parameters automatically of the problem.

23:00.080 --> 23:05.680
So it so turns out that if I'm say in regression depending on the loss that I'm using if I'm say

23:05.680 --> 23:15.840
in regression then regression is a hard enough problem that finding, finding tuning to the best

23:15.840 --> 23:22.400
possible parameter is not as hard. Does that sort of make sense? Find tuning to the best possible

23:22.400 --> 23:26.160
parameter is not as hard since the problem itself is hard and we cannot do so well anyway

23:27.600 --> 23:32.800
in a non-parametric regression. The complexity of the problem is self-masked. Exactly, exactly.

23:33.600 --> 23:40.400
And so here you can show that in fact you can just do cross validation on the number of neighbors

23:41.360 --> 23:47.600
and you'll get the best possible rate as if you knew what structure the data lie down.

23:47.600 --> 23:54.320
Right. Okay. In classification it's a bit more complicated. Classification is a much easier

23:54.320 --> 24:01.200
problem than regression and so it's a bit more complicated. Cross validation gets you half the way

24:02.480 --> 24:06.800
for hard classification problems but for super easy classification problems to get the best

24:06.800 --> 24:14.640
possible result you might need more refined, quote-unquote adaptive procedures to choose now the

24:14.640 --> 24:22.480
parameters of your algorithm and there are various ideas out there and those are the various things

24:22.480 --> 24:35.120
I'm interested in. So I think the initial point that you made that you're talking about the

24:35.120 --> 24:44.480
pipeline being adaptive as opposed to the algorithm and even talking about the algorithm

24:44.480 --> 24:48.320
in this way. Yeah. The reality of these things is that they are all the same.

24:48.320 --> 24:53.600
Right. Another of them are the pipeline. It's all the same because I can just make my algorithm

24:53.600 --> 24:59.520
more complicated and it becomes a whole internal pipeline. And in fact that's what I'll say this

24:59.520 --> 25:06.000
way that for me that's what neural networks are. It's just a whole pipeline of sub procedures.

25:06.000 --> 25:19.200
Yeah. Yeah. Interesting. Interesting. And you're saying also the kind of the process that we

25:19.200 --> 25:27.680
typically use in practice to optimize these pipeline slash algorithms is the adaptive part.

25:27.680 --> 25:33.280
And we know that that's why. Yeah. Yeah. It's the adaptive part. So they are really

25:33.280 --> 25:39.680
multiple. I use the term adaptivity here at least here fairly loosely. In the research paper

25:39.680 --> 25:44.640
I have to be very much more precise as to what I mean by adaptivity because yeah. But you're

25:44.640 --> 25:49.040
bringing exactly the type of questions why we need to make it more precise when we say adaptivity.

25:50.320 --> 25:55.920
There is the notion of adaptivity to various sub problems, families or problems.

25:55.920 --> 26:02.400
Right. And then there is also the side problem of our algorithms themselves.

26:03.840 --> 26:11.040
You can view every parameter of your algorithm as trying to address one particular sub problem.

26:12.480 --> 26:18.640
And that's how they are tied. Does that sort of make sense? The parameters of our procedures.

26:18.640 --> 26:24.480
An example would be helpful I think. Here all I'm saying is that if I fix the parameter of my

26:24.480 --> 26:30.240
procedures, if I just fix the parameters, there is always one problem on which it's going to do

26:30.240 --> 26:37.200
well. Right. Right. Okay. So in some sense it's easy to think about it this that way that the

26:37.200 --> 26:42.800
parameters or the various configurations of my algorithm are addressing as subfamily or problems.

26:44.560 --> 26:49.680
And as I'm trying to be adaptive to a whole hierarchy of families or problems, I need to also

26:49.680 --> 26:56.160
be tuning my algorithm. Right. Funding the right parameter for the right problem. And right. And

26:56.160 --> 27:00.880
so those are those interactions that we are carrying about that we try to understand. Got it. So

27:00.880 --> 27:09.280
you're saying a broken clock is right twice a day. Exactly. Yeah. And that's the perfect algorithm

27:09.280 --> 27:14.400
at that time. But what we're really carrying about is an algorithm that works well across

27:14.400 --> 27:20.640
problems. Right. Right. Right. And we do know that in very in general, it's not possible. In

27:20.640 --> 27:27.040
general, in the following sense, you probably've heard such things as people talk about the no-free

27:27.040 --> 27:32.320
large theorem. Yeah. And so we know that in general sense, it's not possible. But we do know that

27:32.320 --> 27:38.080
if we restrict the family of problems, it is possible to some extent. There are situations where

27:38.080 --> 27:42.080
adaptivity is not possible at all. And we might get into that when we talk about transfer

27:42.080 --> 27:48.960
learning, multitask learning and all that. So and where I hope at least what I'm trying to

27:50.400 --> 27:57.360
to bring out is the fact that adaptivity in some ways is what we are seeking in machine learning.

27:57.360 --> 28:02.080
We are seeking that black box procedure that looks at the data and just knows this is the type of

28:02.080 --> 28:07.040
that I'm dealing with. This is how I should self configure to do as well as possible on this data.

28:07.040 --> 28:15.520
Yeah. So yeah, let's move on to transfer learning. What are you looking at in that area?

28:17.040 --> 28:24.880
Yeah. So in that area, I feel like most theoretical questions are quite open.

28:25.440 --> 28:33.040
Right. So I don't know if I need to define transfer learning quickly here for, I mean, in fact,

28:33.040 --> 28:39.360
I should because there is one thing. People use the term transfer learning and domain adaptation,

28:39.360 --> 28:47.680
all the all these terms fairly loosely. And so when I'm using it, I need to be clear what I'm

28:47.680 --> 28:53.840
trying to refer to. Okay. I was going to say in general, I would expect our listeners to be

28:53.840 --> 28:58.560
familiar with the term. Of course, of course. Yeah. I'm sure that the listeners are familiar with

28:58.560 --> 29:04.240
the term, but the key point for me here is that when I use the term, I need to make clear this is

29:04.240 --> 29:10.640
what I'm trying to say, not some other thing that I might have in mind. Right. And so transfer

29:10.640 --> 29:17.120
learning, what is transfer learning for me? Generally, we have, I have data from a set of tasks.

29:17.760 --> 29:26.800
Sorry, I have data from either a set of tasks or from a single task. And that data gave me

29:26.800 --> 29:31.120
information about a particular problem. Maybe it's a classification problem of some sort. However,

29:33.120 --> 29:39.120
the problem on which I like to do well is a different problem. So mathematically, we tried to

29:39.120 --> 29:44.640
look at it as two different distributions that drew data from a particular distribution,

29:44.640 --> 29:50.320
but that distribution is not representative of the eventual distribution that's that I would like.

29:50.320 --> 29:57.200
Right. So that a different problem, but that could also be the same problem with different data.

29:57.200 --> 30:02.400
Exactly. That could be the same problem with different data. And that maybe the data has shifted

30:02.400 --> 30:08.240
somehow. And here is a simple case of the of the same problem. And in fact, that's where we

30:08.240 --> 30:12.560
starting. Let's look at first the case where it's the same problem essentially. Right.

30:12.560 --> 30:19.920
So I've given various talks on this on this so far. And I always start with the same

30:22.000 --> 30:27.920
motivating example. And the motivating example is Apple's Siri voice assistant. Right.

30:29.440 --> 30:36.560
And if Apple people are listening to me, I hope they understand why I'm saying this.

30:36.560 --> 30:41.360
Apple Siri still doesn't understand me that well at all because of an accent. And why?

30:41.360 --> 30:48.080
Because it was mostly trained on American English speakers at first. Right. And

30:52.080 --> 30:58.320
and it's doing much better now, but one has to think about what they had to do. They had to acquire

30:58.320 --> 31:04.560
a lot more representative data. So you could view the original task as that where they got

31:04.560 --> 31:11.520
data from a particular distribution, but it's a distribution of American accent. Right. And then if

31:11.520 --> 31:18.480
you try to deploy it, let's say in Scotland, it's a different distributions. It's the same problem.

31:19.040 --> 31:22.160
It's a different distribution of accent. You still have Americans there. You have some Scottish

31:22.160 --> 31:28.400
there. Here in the US, you have a few Scottish there in Scotland. You have a lot more. So it has

31:28.400 --> 31:37.280
shifted, but it's the same problem. So at this point, there are various proposals out there to

31:37.280 --> 31:43.040
try and solve this problem faster. But what are the key issues? The key issues will be, for the

31:43.040 --> 31:50.080
practitioner, how much more that I should acquire from let's me call the target desk, how much more

31:50.080 --> 31:56.400
represented that I should acquire. So that's a question of resources. Right. And then once,

31:56.400 --> 32:01.600
and here there is a cost, right, a financial cost, and we want to acquire as little as possible.

32:03.600 --> 32:10.160
So how little can I get by with? And that's really where the idea of transfer is coming in.

32:11.200 --> 32:16.160
What information can I transfer? And then there is the question of the how, how do I transfer

32:16.160 --> 32:21.440
such information? So what is the proper procedure? Once I have that additional data to transfer

32:21.440 --> 32:26.160
the information, or do I need that additional data at all? Right. So those are very, those are

32:26.160 --> 32:34.400
basic questions. Right. And those questions, we want to try and understand, we want to try and

32:34.400 --> 32:41.600
understand the principles behind them. The principles behind transfer. Yeah. Just to jump in, I

32:41.600 --> 32:48.560
love where this is going because I think, and maybe this is more practitioner than a theorist

32:48.560 --> 32:54.080
perspective. I think, you know, it's easy to kind of get, take a snapshot of, you know, where

32:54.080 --> 32:59.360
transfer learning is today, like you've got a neural network, you train it, you take, you know,

32:59.360 --> 33:05.120
the lower layers, and then you kind of retrain on your, your, your top layers for your specifically.

33:05.120 --> 33:09.120
For your new data, exactly. That's transfer learning. But I think you're suggesting that,

33:09.760 --> 33:14.720
actually, there's probably a lot of different things that one could think about doing to affect

33:14.720 --> 33:20.160
transfer and maybe some are better than others. Exactly. There are many more potential procedures

33:20.160 --> 33:25.520
there. Right. Here, for instance, you just describe one. And another could be, and actually,

33:25.520 --> 33:31.360
people do this, maybe I'm going to take the new data and keep, take my system data, put it all

33:31.360 --> 33:37.280
together, pretend it's from the same distribution and retrain everything. Right. That's another

33:37.280 --> 33:43.920
potential procedure. And so there are many procedures out there. And, but before, let's put it this

33:43.920 --> 33:48.800
way. Let's even say that your favorite procedure is the one that you just described. I take the,

33:50.320 --> 33:59.200
a layer of a neural network. This is still presuming that I knew how to pick the target data.

33:59.920 --> 34:07.600
I knew which, which amount of target data I should be using. Right. So there was a decision there,

34:07.600 --> 34:18.480
also. So, so another basic question is, is there even a way for me to know a priori or to discover

34:18.480 --> 34:28.560
over time what amount of target data is required is needed? In full of a sudden, I acquire 1000 data,

34:28.560 --> 34:35.680
1000 data points. And I'm not doing well. What does that tell me? Does that tell me that the,

34:35.680 --> 34:43.760
uh, I didn't acquire enough? Or not the right or I'm not doing the right, uh, transfer.

34:45.440 --> 34:50.480
Or not the right data or not the right data. Yeah. Right. Yeah. It's kind of getting to like active

34:50.480 --> 34:55.440
learning and exactly. So they get into active learning. It gets into into a different form of

34:55.440 --> 35:00.720
active learning, active learning on the transfer. What are the limitations of that? Uh, it gets into,

35:00.720 --> 35:11.120
uh, um, uh, it gets into understanding what's the right notion of distance or what's the,

35:11.120 --> 35:16.400
by distance here, I really just mean information. What's the notion, the right notion of information

35:17.520 --> 35:24.400
to sort of quantify the information my original data already had about the target application.

35:24.400 --> 35:30.080
To be able to say that it doesn't have enough information, so I need that much more.

35:31.280 --> 35:38.480
Mm-hmm. Representative data. I think what you're saying is if we had a way to know

35:39.760 --> 35:47.280
what it was about our initial training data that made it unique and special and most informative

35:47.280 --> 35:53.760
to the model that we've trained. Yeah. And what it was about the data that we need in order to

35:53.760 --> 35:58.480
have our model perform better on the transfer task, you know, there may be a shortcut

35:58.480 --> 36:03.360
than what we're exactly. Then we might start identifying the right data to pick. Yeah. Yeah.

36:03.360 --> 36:09.200
And the right amount to pick. Yeah. Right. And so some of my early theoretical work on this problem,

36:09.200 --> 36:14.720
I've only started on this problem a few years back. And some of the, the first questions there

36:14.720 --> 36:20.800
are that question. What is the right notion of information? The source has about the target.

36:20.800 --> 36:31.280
Mm-hmm. Right. And how do you begin to characterize that? So, so you cheat, right? That's the first

36:31.280 --> 36:36.880
thing you do. You go and you read a lot of papers because a lot of small people have thought about

36:36.880 --> 36:42.880
the problem. And you try to see, okay, what are the notions they've come up with and what are the

36:42.880 --> 36:52.080
limitations of such notions? Right. And then what's there still to be answered? And, and here,

36:52.080 --> 36:58.640
one of the things that I claim is that they are told, okay, this is the one thing that I'm still

36:58.640 --> 37:04.480
doing that everybody, every theoretician has done on this problem so far, which is model the problem

37:04.480 --> 37:08.880
as just probability distributions. I have a, I have data that I drew from a probability

37:08.880 --> 37:14.400
distribution. That's my source. And then there is another probability distribution, which is my target.

37:15.920 --> 37:22.480
And then there are tons of notion of information that relate to probability distributions. And also

37:22.480 --> 37:26.560
notions of distance between probability distributions. And then we can start there and start asking,

37:26.560 --> 37:32.160
do these notions of information or notion of distance, do they sort of capture the hardness of

37:32.160 --> 37:37.680
transfer? And I can say how hard transfer is, if I had a million data points from my source,

37:37.680 --> 37:45.520
and just only 1,000 from my target, how hard would transfer be? And those are the places people

37:45.520 --> 37:51.680
have started. And so we started looking at those notions and then saying, okay, a lot of the

37:51.680 --> 37:56.320
various notions that people have looked at are notions that were developing other areas for

37:56.320 --> 38:01.600
in probability theory or in information theory and such. But we're developed for other problems.

38:01.600 --> 38:10.240
And not necessarily for the exact problems we are looking at, classification on the transfer

38:10.240 --> 38:16.160
setting. And in which case we have to start looking at classification and then start asking,

38:16.160 --> 38:23.200
what makes it easy to transfer a classifier from one distribution to another distribution?

38:23.680 --> 38:29.440
And what's entering that? So then we can ask, as we bring in the question, as we refining the

38:29.440 --> 38:36.320
question, we can start seeing what's essential to the problem, right? So for instance,

38:36.320 --> 38:41.120
if I'm using neural networks, I can ask the question and I'll step away from neural networks,

38:41.120 --> 38:46.720
even I'll just say, I'm using a family of classifiers, right? I'm using a family of classifiers,

38:46.720 --> 38:50.960
then I can step back and ask, okay, when is transfer easy? I can say transfer is easy,

38:51.440 --> 38:57.200
if on my original data, my original data, my source has information about the target,

38:57.200 --> 39:06.640
if on a for the particular family of classifiers I'm using, if whenever a classifier in my family

39:06.640 --> 39:14.560
does very well in on the source, I expected to not be too far from the best classifier on the target.

39:15.360 --> 39:19.200
I expected not to do too bad on the target. That could be one simple notion.

39:19.200 --> 39:27.920
And that in itself start driving down a notion of distance between them, given my preferred

39:28.560 --> 39:33.840
set of classifiers. And now we're talking about distance between the classifiers or distance

39:33.840 --> 39:39.280
between the data sets. Distance between the data sets or in fact between the problems,

39:40.160 --> 39:45.440
because I'm viewing the data sets as being drawn from a distribution, which is now the problem

39:45.440 --> 39:53.280
that I have. And part of the problem is transferring using just my given family of algorithms.

39:53.280 --> 39:56.960
Maybe I'm using neural networks, maybe I'm using random forests, and that's my given family

39:56.960 --> 40:01.120
of algorithms. And that's the family of algorithms within which I would like to stick as the

40:01.120 --> 40:06.480
practitioner. From my family of algorithms, the problem of transfer might be different,

40:07.120 --> 40:12.000
or the hardness of transfer might be different if I were to switch to another set of algorithms.

40:12.000 --> 40:17.920
And I hope that that one should be intuitive to people. If I'm using neural net, transfer

40:17.920 --> 40:21.920
on the neural net might not be the same as transfer on the classification trace.

40:23.920 --> 40:33.280
And even though the data is the same. So it's questions of that type that we try to answer.

40:33.280 --> 40:39.120
And then from there, try to answer more refined questions. Once we start getting a sense of good

40:39.120 --> 40:46.240
notions, like meaning notions that are predictive of hard, hard transfers, then we can start asking

40:46.240 --> 40:51.680
other questions. Okay, now under these notions, we know how what's the best we could possibly do.

40:51.680 --> 40:55.040
And then we can start asking are there algorithms that can attain

40:57.360 --> 41:02.480
these rates that can attain these performance? And then are there adaptive algorithms?

41:02.480 --> 41:06.080
Are there algorithms that can attain these performance without any knowledge

41:06.080 --> 41:14.160
about the underlying problem parameters? How much information about the problem parameters do I

41:14.160 --> 41:19.520
need to give an algorithm before it can do as best as possible for the problem?

41:21.120 --> 41:23.520
And that's again where I fall back into adaptivity.

41:23.520 --> 41:30.800
Yeah, maybe just summarizes part. I think, you know, a lot of what we're seeing in this conversation

41:30.800 --> 41:38.240
is you hear often, you know, machine learning works. We just don't know how or why it works.

41:38.240 --> 41:42.000
And I think you're illustrating in the context of transfer learning,

41:42.720 --> 41:49.200
how a theoretician goes about trying to advance our understanding of how a particular thing.

41:49.200 --> 41:55.200
Exactly, exactly. And so, and I can tell you some of the simplest questions we,

41:55.200 --> 42:02.000
we are starting to address, take the simplest or most naive heuristic in transfer.

42:02.000 --> 42:06.080
I take both data sets, throw them together, pretend it's the same,

42:06.080 --> 42:10.720
and rerun my training out and retrain algorithms on this new data.

42:10.720 --> 42:11.840
How well does this do?

42:13.360 --> 42:16.720
And so, we can start answering those questions. Okay, it's nearly

42:18.960 --> 42:23.360
the best you can do in particular situations. And we can say what exact situations it often

42:23.360 --> 42:29.760
has to do with how noisy your data is. And if it's not so noisy, and whether your,

42:29.760 --> 42:36.480
whether the pattern, your best classifier is shared between source and target is the problem,

42:36.480 --> 42:41.360
the same how close are the problems. And so, on the various situations, we can say,

42:41.360 --> 42:44.800
okay, this particular set of algorithms will do as well as possible.

42:44.800 --> 42:52.240
In other cases, the algorithms that do well will be or adaptive will be closer to meta-learning

42:52.240 --> 42:58.000
type procedures. And so, so we can start addressing those questions. But again, the whole thing

42:58.000 --> 43:03.840
for me is step back completely and start asking the most basic questions about the problem.

43:03.840 --> 43:09.360
Yeah, yeah, yeah. One more area I'd like to explore. You've also been doing some work in

43:09.360 --> 43:12.320
unsupervised learning. Can you talk a little bit about that?

43:12.320 --> 43:18.560
Yeah, so, so it's mostly the, I mean, I've talked a bit about that so far with the

43:18.560 --> 43:23.680
clustering, although I didn't quite as specifically talk about my work in clustering.

43:24.880 --> 43:30.080
But, and then lately, we've been looking at some applications in IoT,

43:31.200 --> 43:38.960
in allied detection and such in IoT. So, so in clustering, mostly the type of questions I've

43:38.960 --> 43:45.200
looked at and I'm still looking at, there are two lines of questions. The first is what I

43:45.200 --> 43:51.280
alluded to earlier, which is, let's imagine that I define clustering as just notions of

43:52.800 --> 43:59.040
in terms of density levels, in terms of finding regions of high density in the data.

43:59.040 --> 44:07.280
To what extent can I do this? And can I do this also adaptively? Can I? And there there was a key

44:07.280 --> 44:13.680
question that we had, which was, there are tons of in density based clustering itself,

44:13.680 --> 44:16.720
there are tons of heuristics, I will call them heuristics and it's not a bad term,

44:17.920 --> 44:23.760
there are tons of heuristics that do extremely well in practice. And yet, when you try to prove any

44:23.760 --> 44:29.200
guarantee about these heuristics, it's really hard. You cannot show that day. And however,

44:29.200 --> 44:34.160
they do better than anything we can prove beautiful guarantees about. And so one of the questions

44:34.160 --> 44:41.520
there was, can we come up with a heuristic that does, that compete with existing heuristics?

44:41.520 --> 44:49.120
And yet can guarantee the recovery of clusters if we define clusters as regions of high density of

44:49.120 --> 45:01.040
the data. So it's that upper question there. Then, then lately, I was, I've been looking at

45:01.040 --> 45:12.560
more, I mean, it's coming up in more in the IoT domain, internal of things. And here,

45:13.280 --> 45:18.160
the type of questions that are coming up, I have to do with constraints on the clustering and

45:18.160 --> 45:26.480
alloy detection and all that. In IoT, we want to just monitor traffic, network traffic. And be

45:26.480 --> 45:35.200
able to quickly say, when there is an anomaly in the network traffic. And the anomaly could just be

45:35.200 --> 45:42.880
a new device was introduced into the network, into the house. Or we are observing a new modality

45:42.880 --> 45:52.160
of the device. And these devices could be anything. It could be a smart coffee maker,

45:52.160 --> 45:58.320
smart fridge, it could be a network, it could be a sensor in a city, et cetera.

45:58.320 --> 46:03.520
And applications might be, you know, anything from cybersecurity to performance,

46:03.520 --> 46:11.040
management. Exactly. Yeah, et cetera. And, and even detecting, yeah,

46:11.040 --> 46:15.120
cybersecurity goes into it, right? Detecting simply that a new device was introduced,

46:15.120 --> 46:21.120
and this device looks a lot like a camera. So somebody threw a camera onto my network.

46:21.120 --> 46:27.360
Right. So something like that. And, and from your, I guess you're getting to this, but your,

46:29.120 --> 46:37.360
your data sources like net flow traffic traffic. And we are assuming that I can only observe the

46:37.360 --> 46:45.120
pattern of a packet. I cannot read a packet in network packet. All I know is that so many

46:45.120 --> 46:50.640
packets are being sent per second to these particular addresses and all that. And so it's very unsupervised.

46:50.640 --> 46:55.520
I don't know. It's an unsupervised problem. I don't have any, any labels. And however,

46:55.520 --> 47:00.080
this is where constraints come in. Huge constraints come into the into this right away.

47:00.080 --> 47:06.640
And the constraints of the following form, we have to be able to run the detector, train and

47:06.640 --> 47:15.040
run the detector. Let's say on a router at home, on a very small device. We don't have a huge

47:15.040 --> 47:22.480
server to, to do this. So all of a sudden, a lot of our library detectors out there,

47:22.480 --> 47:30.720
let's, for instance, let's think about one-class SVM. It just don't run well in these settings.

47:31.360 --> 47:35.760
Why I'm taking one-class SVM? One-class SVM like support vector machines in general

47:37.120 --> 47:44.960
require a lot of computation. So require a lot of computation because they deal with

47:44.960 --> 47:55.360
this, this, this very large matrices that they have to, to, to play with. And so right away,

47:55.360 --> 48:01.680
the question that comes up, questions of how do we reduce the data in some ways, right?

48:01.680 --> 48:05.680
Or how do we reduce this? If you're familiar with support vector machines, they work on

48:05.680 --> 48:10.960
a so-called gram matrix, which is essentially a representation of interactions between

48:10.960 --> 48:15.840
data points, okay? But because the moment I talk about interactions between data points,

48:16.720 --> 48:22.880
if I have n data points, I'm talking about n squared operations already, right? And so

48:24.480 --> 48:30.320
we want to reduce this and reduce, reduce this quickly and to be able to run on a nano computer

48:30.320 --> 48:36.160
on Raspberry Pi or something like that, right? And so it's not only space savings, but it's also

48:36.160 --> 48:41.520
time savings. And so for me as a theoretician, the first question that comes up is,

48:44.080 --> 48:49.360
first of all, can I maintain performance of a clustering

48:51.040 --> 48:57.520
after reducing data? And what type of data reduction is useful here? There are various things

48:57.520 --> 49:04.640
that people have tried. Nitron is a method called Nitron sketching method, all sort of

49:04.640 --> 49:12.320
a sub-sampling method that reduce these data representations. And then the questions for me was,

49:12.320 --> 49:20.000
okay, can I modify this method and guarantee that let's say in the context of clustering or in

49:20.000 --> 49:26.640
the context of allied detection, the performance is essentially maintained and yet I've achieved

49:26.640 --> 49:33.600
my constraint on size. So it's questions of that type. And here it's very practical because I

49:33.600 --> 49:39.360
cannot just answer the question. I have to, in fact, implement it and get it working and eventually

49:39.360 --> 49:50.560
deploy it. And presumably is this work in progress or have you come up with another question?

49:50.560 --> 49:56.640
Oh, so we have a couple, yeah, we have a couple papers on it. And you probably heard of Johnson

49:56.640 --> 50:04.320
leader in Strauss dimension reduction and random projection, things like that. So part of that

50:04.320 --> 50:08.560
the work, the initial paper on this work was all theory was trying to understand, okay,

50:08.560 --> 50:16.480
all these problems out there, all these methods out there sketching, nice from, can we recast

50:16.480 --> 50:25.440
this method in different terms that we can understand better, right? So sketching doesn't look at

50:25.440 --> 50:32.720
right away like a random projection. And we're asking sketching, which is really just

50:32.720 --> 50:44.800
sub sampling of a matrix. Can we view it as a random projection in a very high dimensional

50:44.800 --> 50:48.400
space, in an infinite dimensional space? Why infinite dimensional space? Because that's exactly

50:48.400 --> 50:52.960
where OCSVM's and all that work. They work in an infinite dimensional space. And I'm doing this

50:52.960 --> 50:58.320
sketching on a finite matrix. Can I view that somehow as a projection in an infinite dimensional

50:58.320 --> 51:03.280
space? Why do I want to view it that way? Because I do understand projections. We understand what

51:03.280 --> 51:08.240
projections do and what they maintain and all that. So that was the first step in the work.

51:08.240 --> 51:12.960
And then the next step has been, okay, now that we know how it works, we know for what type of

51:12.960 --> 51:20.560
clusters we preserve the clustering. And now do we have that type of clusters in this IoT

51:20.560 --> 51:25.520
applications? It so turns out that we did a lot of data analysis and, okay, we do have that

51:25.520 --> 51:30.960
type of clusters often. And now we are implementing it. And we have a paper on archive trying to show

51:30.960 --> 51:37.200
that, okay, look at this running on a nano computer on the Raspberry Pi. It runs almost, it runs

51:38.000 --> 51:46.640
20, 30 times faster than the original OCSVM, but we have the same performance. And so, but it's

51:46.640 --> 51:53.520
understanding this type of things. Very cool. Very cool. There's maybe a bit of a

51:53.520 --> 52:00.960
digression before we close out, but I was thinking earlier when we were talking about clustering

52:00.960 --> 52:09.600
and density and like zooming in and out. And kind of seeing the data at different granularities,

52:09.600 --> 52:14.480
it reminded me a little bit of like fractals and like fractal theory. And I'm just have,

52:14.480 --> 52:21.440
does that come up in theory? Yeah, it does come up a lot. So, so for instance, you might,

52:22.240 --> 52:26.240
so when I was talking about dimension earlier and I was even when I was saying, okay,

52:27.280 --> 52:32.240
let's say data is very high dimensional, but it's very structured, right.

52:33.440 --> 52:38.000
There are tons of structures, tons of intrinsic structures out there. Manifold is just one,

52:38.000 --> 52:43.840
right? And Manifold is just one such structure. And so, a first thing in that line of research was to

52:43.840 --> 52:54.000
try and understand what notion of intrinsic dimension can we use to capture all these

52:54.800 --> 53:01.840
low dimensional structured at once rather than working on understanding each one of them separately.

53:01.840 --> 53:05.120
Is there a notion that captures them at once? And there are fractal notions of dimension that

53:05.120 --> 53:11.360
do capture these notions of intrinsic dimension at once. And then once you understand that you can

53:11.360 --> 53:16.800
then ask the question, okay, let me now say that my data is low dimensional in the sense that

53:16.800 --> 53:23.840
it's fractal dimension is small. How do nearest neighbors work? Okay. Right. So, so that's how we

53:23.840 --> 53:29.920
approach those those problems. We first have to sort of step back again and say, okay, what is the

53:29.920 --> 53:35.440
essential quantity we need to work with here? And and and fractals somehow have to do with

53:35.440 --> 53:42.400
is representability of data or the information in something in in in the data space. And so,

53:43.760 --> 53:50.240
so so yeah, so a lot of notions we work with have this sort of recursive structure to them.

53:50.240 --> 53:58.080
Awesome. Awesome. And one last thing, you're chairing a conference that conference is happening now,

53:58.080 --> 54:05.440
so I assume. Yeah. Yeah. Tell us about the conference. So, this is called conference on learning

54:05.440 --> 54:11.360
theory. And so, called has been sort of the flagship conference for machine learning theory.

54:12.800 --> 54:18.480
I don't know how many years now, 20, 30 years, something like that. And so yeah, so it's

54:18.480 --> 54:26.560
happening this year in what we all know, sort of unfortunate circumstances, the times we live in.

54:26.560 --> 54:32.080
And yeah, and this year, one of the things I'll say is that this year we have a really, really,

54:32.080 --> 54:40.400
really nice program. So, so let me put it this way. I feel that somehow at the same time we are

54:40.400 --> 54:50.640
in strange times and hard times for a lot of people, but people also tend to focus and solve

54:50.640 --> 54:57.520
hard problems around this sort of tough times. And I feel that reflecting in somehow in the program

54:57.520 --> 55:04.720
we have this year, a lot of beautiful problems were solved. And we were just odd at the type of results

55:04.720 --> 55:11.840
that to be presented next week at the conference. Yeah. Awesome. Awesome. Yeah. Well, we will include a

55:11.840 --> 55:19.040
link to the conference in the show notes as well as the archive link to the network traffic

55:19.040 --> 55:24.720
representation paper. Samry, so great to catch up with you. Thanks so much for taking the time.

55:25.280 --> 55:30.560
Yeah, thank you so much Sam for having me and thanks for all the very interesting questions

55:30.560 --> 55:58.720
and the very pointed questions. Awesome. Thank you.

