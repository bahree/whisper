1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,520
I'm your host Sam Charrington.

4
00:00:23,520 --> 00:00:28,720
Let me start by sending a huge thanks to everyone who listened to our podcast series from the

5
00:00:28,720 --> 00:00:32,160
O'Reilly AI Conference in San Francisco last week.

6
00:00:32,160 --> 00:00:34,720
Thanks so much for your feedback and comments.

7
00:00:34,720 --> 00:00:36,560
We're glad you enjoyed the podcast.

8
00:00:37,600 --> 00:00:42,480
From now through the end of the year, I'll be attending a bunch of events and we'll be releasing

9
00:00:42,480 --> 00:00:46,480
a ton of great interviews, so please keep those comments coming.

10
00:00:47,360 --> 00:00:54,000
Reach out to us via at Twimble AI on Twitter or Facebook or via the show notes page for any

11
00:00:54,000 --> 00:01:01,040
episode. In the event you missed our tweets on Friday the 13th, we've got a very special announcement

12
00:01:01,040 --> 00:01:06,800
for you. In a few weeks, we'll be back in New York for the NYU Future Labs AI Summit.

13
00:01:07,600 --> 00:01:12,640
As some of you may remember, we held our very first Twimble Happy Hour in New York City

14
00:01:12,640 --> 00:01:18,080
just a few months ago and it was great, which inspired us to go even bigger this time.

15
00:01:18,080 --> 00:01:24,880
And that is just what we did. We're excited to present the AI Apocalypse and Killer Robots

16
00:01:24,880 --> 00:01:29,680
Halloween Social, which will be held on Monday, October 30th in New York City.

17
00:01:29,680 --> 00:01:35,760
This will be both a fun and informative event and I'll be doing a live podcast taping and

18
00:01:35,760 --> 00:01:43,040
panel discussion on the myths and realities of extreme AI featuring Dr. Seth Baum, Executive

19
00:01:43,040 --> 00:01:49,440
Director of the Global Catastrophic Institute, Shane Hobel, Founder of Mountain Scout Survival

20
00:01:49,440 --> 00:01:56,400
School, and Charlie Oliver, Founder of Tech 2025. This discussion will be interactive,

21
00:01:56,400 --> 00:02:01,280
so we'll get to discuss your most pressing questions about extreme AI and we couldn't

22
00:02:01,280 --> 00:02:06,960
be more excited about this event. For ticket info and more details about the AI Apocalypse

23
00:02:06,960 --> 00:02:14,160
and Killer Robots Halloween Social, visit twimbleai.com slash Halloween. And for 25% off of all

24
00:02:14,160 --> 00:02:21,920
ticket types for the NYU Future Labs AI Summit, use code twimble25. I'd like to take a moment

25
00:02:21,920 --> 00:02:27,280
to tell you about our sponsor for this episode, Nexosis, and thank them for supporting this week's

26
00:02:27,280 --> 00:02:33,600
show. Nexosis is a company focused on providing easy access to machine learning. The Nexosis

27
00:02:33,600 --> 00:02:38,800
Machine Learning API meets developers where they're at, regardless of their mastery of data

28
00:02:38,800 --> 00:02:44,160
science, so they can start coding up predictive applications today in their preferred programming

29
00:02:44,160 --> 00:02:50,000
language. It's as simple as loading your data and selecting the type of problem you want to solve.

30
00:02:50,000 --> 00:02:55,200
Their automated platform trains and selects the best model fit for your data and then outputs

31
00:02:55,200 --> 00:03:00,880
predictions. Get your free API key and discover how to start leveraging machine learning in

32
00:03:00,880 --> 00:03:11,120
your next project at nexosis.com slash twimble. That's n-e-x-o-s-i-s dot com slash t-w-i-m-l.

33
00:03:11,920 --> 00:03:15,440
Head over, check them out, and be sure to let them know who sent you.

34
00:03:16,080 --> 00:03:22,160
One final reminder about the upcoming twimble online meetup. Yes, just a couple of days from now

35
00:03:22,160 --> 00:03:28,000
on Wednesday, October 18th. At 3 p.m. Pacific time, we'll be discussing the paper,

36
00:03:28,000 --> 00:03:35,280
Visual Attribute Transfer Through Deep Image Analogy, by Jing Li Yao and others from Microsoft Research.

37
00:03:36,240 --> 00:03:41,600
The discussion will be led by Duncan Stothers. To join the meetup or to catch up on what you missed

38
00:03:41,600 --> 00:03:49,120
from the first two meetups, visit twimbleai.com slash meetup. Okay, about today's show. A few

39
00:03:49,120 --> 00:03:55,360
weeks ago, I sat down with James Goosa, US Chief Data Scientist at Deloitte Consulting,

40
00:03:55,360 --> 00:04:01,040
to talk about human factors in machine intelligence. James was in San Francisco to give a talk

41
00:04:01,040 --> 00:04:08,080
at the O'Reilly AI Conference on why AI needs human center design. James and I had an amazing

42
00:04:08,080 --> 00:04:14,480
chat in which we explored the many reasons why the human element is so important in ML and AI,

43
00:04:15,040 --> 00:04:19,280
along with useful ways to build algorithms and models that reflect this human element while

44
00:04:19,280 --> 00:04:26,000
avoiding problems like groupthink and bias. This was a very interesting conversation. I enjoyed it

45
00:04:26,000 --> 00:04:30,880
a ton and I'm sure you will too. And now on to the show.

46
00:04:38,240 --> 00:04:43,200
All right, everyone. I am here at the AI conference in San Francisco and I'm here with James

47
00:04:43,200 --> 00:04:49,680
Goosa. And James is the US Chief Data Scientist with Deloitte Consulting and he's going to be

48
00:04:49,680 --> 00:04:56,000
speaking later today actually on a topic that you've heard me allude to here on the podcast,

49
00:04:56,000 --> 00:05:01,600
the number of times, human factors in artificial intelligence. And so I'm really looking forward

50
00:05:01,600 --> 00:05:05,760
to diving into this conversation with you. James, welcome to the podcast. Thank you very much.

51
00:05:05,760 --> 00:05:10,720
I'm happy to be here. Absolutely. So why don't we get started by having you tell us a little

52
00:05:10,720 --> 00:05:14,400
bit about your background? You want to hear my checkered past? I want to hear your checkered

53
00:05:14,400 --> 00:05:21,840
past. Yeah. Well, in fact, I saw in your bio that you've got a PhD in the philosophy of science.

54
00:05:21,840 --> 00:05:28,080
Yeah, I know it's a cliche. Yeah, I have a PhD in philosophy from University of Chicago. I'm a

55
00:05:28,080 --> 00:05:32,560
very intellectually curious person. Actually, when I when I entered philosophy, what I thought

56
00:05:32,560 --> 00:05:37,440
was going to study was artificial intelligence. Really? Yeah, I'm a very old person. So this is

57
00:05:37,440 --> 00:05:42,240
back in the early 90s. And back then, artificial intelligence was, you know, talked about, you know,

58
00:05:42,240 --> 00:05:46,080
a lot of people are connected to this and Jerry Fodorans and so on and so on. And I almost went to

59
00:05:46,080 --> 00:05:49,360
University of Pittsburgh, which had a lot of types of Carnegie Mellon University. And there's a very

60
00:05:49,360 --> 00:05:52,640
strong, there's like one of the strongest philosophy of science programs in the country is at PIP,

61
00:05:52,640 --> 00:05:56,400
right down the street from CMU. And I really thought I'm going to do artificial intelligence.

62
00:05:57,040 --> 00:06:00,720
Long story, I changed my mind and went to the University of Chicago and I got a PhD in philosophy,

63
00:06:00,720 --> 00:06:04,960
but I focused more in philosophy of physics and especially the way statistics is used in physics.

64
00:06:04,960 --> 00:06:10,480
Okay, this is all my way of saying I studied pre-unemployment. I always joke that philosophy is

65
00:06:10,480 --> 00:06:14,880
the Greek word that means pre-unemployment. So, you know, I did, it was, it was wonderful. It was

66
00:06:14,880 --> 00:06:17,760
some of the best years of my life. I love Chicago. I love the University of Chicago. I love

67
00:06:17,760 --> 00:06:21,840
what I studied. It was fabulous. I needed a way to make a living. And this is back in the early

68
00:06:21,840 --> 00:06:26,080
early 2000s, the late 90s. I was, you know, thinking through the various options. I thought,

69
00:06:26,080 --> 00:06:31,360
well, I'm a humanities guy, good at law school. I'm going to do that. You know, I'm, I'm doing

70
00:06:31,360 --> 00:06:35,520
kind of scientific stuff. I could go to Wall Street, right. I could do the whole options thing,

71
00:06:35,520 --> 00:06:40,080
right, which is very sexy back then. I didn't think I, it would culturally work for me. I just,

72
00:06:40,080 --> 00:06:43,200
I just didn't think I, I actually landed a job and went out for the interview. I just didn't

73
00:06:43,200 --> 00:06:46,320
think I'd enjoy it. So, I didn't do it. So, I went, I went for the fame and fortune and glam

74
00:06:46,320 --> 00:06:50,640
or becoming an actuary. Okay. And I didn't know what that meant back then. But I assumed that

75
00:06:50,640 --> 00:06:55,520
actual science meant data science. And it didn't, but now it kind of does. And there's a weird

76
00:06:55,520 --> 00:06:59,600
way in which actual is where the original data scientist, there's a weird way in which my first

77
00:06:59,600 --> 00:07:03,520
data science program or project, which I did at the All-State Research Center in Menlo Park,

78
00:07:03,520 --> 00:07:07,360
California, is a weird way in which that was actually artificial intelligence. I didn't think

79
00:07:07,360 --> 00:07:11,360
of it at the time, but I basically, you know, credit scores, you know, you could think of credit

80
00:07:11,360 --> 00:07:16,560
scores as sort of an early example of AI in a sense that Chris Hammond from Narrative Science

81
00:07:16,560 --> 00:07:21,200
in Northwestern talks about, which is that it's not so much AI from focusing in it from a,

82
00:07:21,200 --> 00:07:25,440
like, through a technical lens, but functionally it's AI because we used to have this whole

83
00:07:25,440 --> 00:07:30,720
profession called bank loan officers. And it was like a lot of people. That was their job.

84
00:07:30,720 --> 00:07:35,680
It was bank loan officers. And it turns out that went out the window when we used algorithms

85
00:07:35,680 --> 00:07:40,640
to make loan, to make lending decisions. And I can get into this. There are a lot of reasons

86
00:07:40,640 --> 00:07:44,720
why that makes a lot of sense. Some of it is on the data side. And some of it is on the human

87
00:07:44,720 --> 00:07:49,200
cognition side. So a lot of reasons why what aspect of this makes a lot of sense. Well, there's a

88
00:07:49,200 --> 00:07:53,840
lot of reasons why there's a lot of reasons why that was an early case where algorithms outperformed

89
00:07:53,840 --> 00:07:58,320
human judgment. And you know, and the use of algorithms kind of like, you know, kind of like a

90
00:07:58,320 --> 00:08:04,960
shrunk a certain part of the workforce, right? And so was this contested like that feels intuitively

91
00:08:04,960 --> 00:08:10,160
obvious to me that, you know, this is a fundamentally database decision. And if you can accurately

92
00:08:10,160 --> 00:08:16,720
characterize the, you know, a person situation, you know, in data, which we are able to do now,

93
00:08:17,280 --> 00:08:22,800
then algorithms are going to do a pretty good job of this over, you know, golf and relationships.

94
00:08:22,800 --> 00:08:27,200
And some of the things that we think of as the past lives of the loan officer. Absolutely.

95
00:08:27,200 --> 00:08:31,040
And I would say yes, but I think that's okay. I think this kind of gets a bonus, but it's always

96
00:08:31,040 --> 00:08:35,600
the interesting part. Absolutely. No, exactly. Exactly. No, this, and this is sort of what it kind of

97
00:08:35,600 --> 00:08:39,520
gets at one aspect of what I'm going to be talking about in my talk this afternoon, which is that

98
00:08:39,520 --> 00:08:45,440
unated judgment is notoriously unreliable when it comes to making judgments and decisions.

99
00:08:45,440 --> 00:08:49,680
And you know, if the listeners have read things like Daniel Kahneman thinking fast and slow,

100
00:08:49,680 --> 00:08:54,880
or nudge by Taylor and Sunstein, you know, or clued by Gary Marcus, the cognitive scientist,

101
00:08:54,880 --> 00:08:59,760
we realized that, you know, our brains evolved. They were optimized by evolution for a certain kind

102
00:08:59,760 --> 00:09:04,240
of environment, you know, outrunning predators in the sedentary, whatever it was. I'm also

103
00:09:04,240 --> 00:09:08,400
thinking predictably irrational by the area. There's another popularization of this whole thing,

104
00:09:08,400 --> 00:09:12,080
exactly. But you know, the real, the real pop classic, in my opinion, is thinking fast and slow

105
00:09:12,080 --> 00:09:15,920
by the economy. That should, that should be in every machine learners shelf, in my opinion. This

106
00:09:15,920 --> 00:09:20,400
is kind of like the compliments machine learning in my opinion. So yeah, I mean, those same sort of

107
00:09:20,400 --> 00:09:25,200
mental heuristics that service well in kind of everyday life, they don't service so well when we

108
00:09:25,200 --> 00:09:29,840
put on a suit and sit around a boardroom and try to decide, should I, you know, acquire this company,

109
00:09:29,840 --> 00:09:34,160
should I admit the student to university? How should I treat this patient? Does this person get

110
00:09:34,160 --> 00:09:39,360
the loan or not? There can be all sorts of biases creeping in. Okay. And then this is the theme

111
00:09:39,360 --> 00:09:44,000
of predictably irrational and thinking fast and slow. So, you know, Kahneman talks about, he calls

112
00:09:44,000 --> 00:09:50,720
the miracles and the flaws of any judgment. It's a paraphrase. The miracle is that most of the

113
00:09:50,720 --> 00:09:54,400
decisions we make in day to day life are what he would call thinking fast. You know, just like,

114
00:09:54,400 --> 00:09:58,640
you know, effortless, they come automatically. We kind of tell a story and the story kind of works.

115
00:09:59,360 --> 00:10:03,040
But, you know, in these kind of mission critical cases where it's more like, you know,

116
00:10:03,040 --> 00:10:06,960
is this person going to, you know, commit a crime or is this person going to pay back the loan

117
00:10:06,960 --> 00:10:11,440
or is this person going to crash his car or does this person have the disease? Not so well,

118
00:10:11,440 --> 00:10:16,640
we really need help from algorithms. But that's kind of like one, one half of the story. I think

119
00:10:16,640 --> 00:10:20,720
another one of the topics that's really that we've all known about for decades, but it's really

120
00:10:20,720 --> 00:10:24,960
becoming, it's coming to the fore is the need to kind of make sure we reflect societal values

121
00:10:24,960 --> 00:10:29,360
in these algorithms at the same time. So, when I was, it all stayed doing this. It's kind of an

122
00:10:29,360 --> 00:10:32,960
interesting story. I wasn't building a credit scoring algorithm for all say because they were

123
00:10:32,960 --> 00:10:37,200
underwriting loans. It's because they're selling insurance contracts. Turns out the credit is

124
00:10:37,200 --> 00:10:40,880
hugely predictive. Who's going to crash their car or have like a water, homeowners claim something

125
00:10:40,880 --> 00:10:44,400
then? We can get into that. It's a very interesting story about like, why is the data so predictive

126
00:10:44,400 --> 00:10:49,360
in that way? But we're also very interested in the legal doctrine of disparate impacts. So,

127
00:10:49,360 --> 00:10:54,080
even if we didn't put like a protected class in the algorithm, there could be an unintended

128
00:10:54,080 --> 00:10:57,920
consequence, you know, where the algorithm could have like systematically different scores

129
00:10:57,920 --> 00:11:03,120
for different groups of people like income or whatever it is. Or urban rule or race or gender

130
00:11:03,120 --> 00:11:08,720
or whatever it is. We don't want that, right? And it's like, you know, some, you know, and it's

131
00:11:08,720 --> 00:11:12,240
very interesting because like, you know, these same early conversations are now reflected in a

132
00:11:12,240 --> 00:11:16,640
larger scale in the world of AI, right? But early on, you know, I'd get into these conversations

133
00:11:16,640 --> 00:11:20,480
with other quants, you know, back then there were actuaries in English. Back then we called it

134
00:11:20,480 --> 00:11:23,920
machine learning to answer your question, right? They were data mining. That's what we called it back

135
00:11:23,920 --> 00:11:28,880
then. KDD was around back then so we called it data mining. But, you know, the actuaries would say,

136
00:11:28,880 --> 00:11:32,000
well, this is just ridiculous. I mean, we just want to come up with the actuarly fair price.

137
00:11:32,000 --> 00:11:36,400
Everybody should just like be charged insurance and reflects their risk. But that might be a

138
00:11:36,400 --> 00:11:40,800
limited perspective. There might be other perspectives that legitimately should constrain models.

139
00:11:40,800 --> 00:11:45,680
So what is the, you know, what is the way to kind of optimize models? Is it, you know,

140
00:11:45,680 --> 00:11:48,800
is it kind of a metric of we're going to like, you know, make this prediction with the greatest

141
00:11:48,800 --> 00:11:53,600
out of sample accuracy? Or is it subject to this that in the other constraint? And some of those

142
00:11:53,600 --> 00:11:58,640
constraints are societal in nature. Some of those constraints are what we were calling human factors

143
00:11:58,640 --> 00:12:04,720
in nature. So there are some cases where maybe it's really complex risk. Maybe it's, maybe I'm

144
00:12:04,720 --> 00:12:08,880
trying to underwrite a very complex loan or a very complex insurance contractor, a very complex

145
00:12:08,880 --> 00:12:14,560
medical diagnosis or a judge making a parole decision. We don't necessarily want to simply turn

146
00:12:14,560 --> 00:12:18,960
that over to a machine, right? We don't just want to automate it away and take humans out of the

147
00:12:18,960 --> 00:12:22,160
loop. But, you know, rather what we want to do is we want to kind of take the best of both worlds

148
00:12:22,160 --> 00:12:27,520
and say, well, the machines are good in one way, you know, they can weigh together 400 factors,

149
00:12:27,520 --> 00:12:31,440
you know, better than we can weigh together four. And by the way, do it the same way before

150
00:12:31,440 --> 00:12:35,200
lunch and after lunch versus after lunch, which we don't do. But at the same time, they don't have

151
00:12:35,200 --> 00:12:40,000
common sense. Humans have common sense. They understand ethics. They understand the strategic goals

152
00:12:40,000 --> 00:12:44,720
of the organization. They understand, you know, you know, societal values, legal constraints,

153
00:12:44,720 --> 00:12:50,320
whatever it is. You know, public relations, whatever it is. And so figuring out how to marry

154
00:12:50,320 --> 00:12:55,200
the best of both worlds. That's part of what I mean by design. You know what I mean? Yeah.

155
00:12:55,200 --> 00:13:02,960
You know, it's like one analogy I've made for a long time is that, yeah, these algorithms

156
00:13:02,960 --> 00:13:08,080
do replace humans to certain tasks. So I always talk about credit scores, like in an early example

157
00:13:08,080 --> 00:13:14,880
of artificial intelligence. For some types of loans or risks or medical diagnosis or whatever,

158
00:13:14,880 --> 00:13:18,560
you just kind of like turn the algorithm on, it'll make a smart decision. And it might be wrong

159
00:13:18,560 --> 00:13:22,240
part of the time, but, you know, maybe the losses are acceptable for whatever reason. Yeah.

160
00:13:22,240 --> 00:13:28,240
In other cases, you just can't do that. And so it's more like there's this art to somehow blending

161
00:13:28,240 --> 00:13:32,560
the machine indication with the human judgment. And that's always fascinated me. That's been

162
00:13:32,560 --> 00:13:36,960
more the late motif of my work at Deloitte since leaving Allstate. You know, at Allstate, we're doing

163
00:13:36,960 --> 00:13:40,720
a lot of, you know, big data, personal insurance. Allstate is the second or third largest

164
00:13:40,720 --> 00:13:44,320
insurance carrier in the country. So they have all this big data. And you can do a lot of this kind

165
00:13:44,320 --> 00:13:48,000
of like, well, we'll just have a, you know, this algorithm, it'll spit out a price. Boom,

166
00:13:48,000 --> 00:13:52,960
there's your price. It's automation. But when I joined Deloitte, there's a lot more work for

167
00:13:52,960 --> 00:13:59,120
smaller medium-sized companies or organizations that want to use data to make more complex

168
00:13:59,120 --> 00:14:03,840
decisions. And so this idea of blending algorithmic indications with human judgment became

169
00:14:03,840 --> 00:14:09,120
much more of an issue. I only came to appreciate this gradually as I was working Deloitte to start.

170
00:14:09,120 --> 00:14:13,200
I started off as a pure quant. I was just interested in the math and I still am. I love it.

171
00:14:13,600 --> 00:14:16,400
So that's the geeky side of me. But there's also this kind of side where like I'm just

172
00:14:16,400 --> 00:14:21,040
fascinated by the way it's used in organizations. I'm just interested in like you need organizational

173
00:14:21,040 --> 00:14:27,680
buy-in. You need to reflect domain knowledge and institutional knowledge in the data,

174
00:14:27,680 --> 00:14:32,000
in the design of the algorithm. You need to think upfront about how is the algorithm going to be

175
00:14:32,000 --> 00:14:36,400
used in the organization? Who's going to be using it? Who are stay-coulders? And if you get all

176
00:14:36,400 --> 00:14:41,040
those things wrong and if you don't plan for them upfront, I won't guarantee it, but I will

177
00:14:41,040 --> 00:14:45,840
bet money. You'll get a negative ROI on your analytics project. So that's why I become

178
00:14:45,840 --> 00:14:50,160
obsessed by this. And I think the exact same issues arise in now the room age of AI.

179
00:14:50,160 --> 00:14:55,600
Yeah. Yeah. One of the things that struck me as interesting in hearing you tell the story about

180
00:14:55,600 --> 00:15:05,360
the work you did at Allstate in particular was it sounded like you were very aware at that time

181
00:15:05,360 --> 00:15:12,320
about you know issues that you know I think of in many ways is like only now kind of

182
00:15:12,320 --> 00:15:17,840
finding contemporary voice. You're right. So you know bias and algorithmic bias. I know it's very

183
00:15:17,840 --> 00:15:26,080
fascinating. Just last night, just last night Pedro Domingo tweeted true fact algorithms

184
00:15:26,080 --> 00:15:33,680
cannot discriminate. And so I replied well how do we define algorithm? What are we talking

185
00:15:33,680 --> 00:15:42,080
about here? Exactly. And so in that lens it's like this is the new issue like we're just

186
00:15:42,080 --> 00:15:46,640
gearing up to fight this fight. Yeah. But it sounds like you were grappling with this way back what?

187
00:15:46,640 --> 00:15:52,720
And not just not just thinking about it, but the impression I'm getting from the way you

188
00:15:52,720 --> 00:15:56,400
described it was that the organization had a consciousness around it. Absolutely.

189
00:15:56,400 --> 00:16:00,400
It's to talk more about this. Yeah, yeah. No, no, it completely is. And I'd love to talk a little

190
00:16:00,400 --> 00:16:04,080
bit more about pro social uses of big data. I have kind of like a little mantra on that.

191
00:16:04,640 --> 00:16:08,880
No, I mean this is one of those things where you know I mean Ben Franklin had decided doing

192
00:16:08,880 --> 00:16:13,680
well by doing good. And I don't want to make any grand claims for my employees or anything

193
00:16:13,680 --> 00:16:20,160
like that. But it's in organizations in light and self-interest to think in a long term.

194
00:16:20,160 --> 00:16:25,120
Maybe in the short term you can like just throw out whatever model you want. But you know

195
00:16:25,120 --> 00:16:29,200
they're smart enough to realize that if there are these unintended consequences it'll come back

196
00:16:29,200 --> 00:16:33,360
to the bite them later on. It doesn't make any sense, right? So since all organizations are

197
00:16:33,360 --> 00:16:38,320
self-interested, does that mean that some are more enlightened than others? I think some are more

198
00:16:38,320 --> 00:16:42,080
enlightened. But I mean it's clear. I mean you know something else we could just be a tangent.

199
00:16:42,080 --> 00:16:46,080
We could talk about a group thing. I mean they're they're you know think about all the organizations

200
00:16:46,080 --> 00:16:52,160
you know or you know both private and public sector organizations that make catastrophically

201
00:16:52,160 --> 00:16:58,720
bad decisions. You know when even this is another interesting thing like our organizations people

202
00:16:58,720 --> 00:17:03,680
will know. There are people that comprise organizations, right? Organizations can act as if

203
00:17:03,680 --> 00:17:07,600
they're rational or not or they're enlightened or not. And sometimes what happens is you'll

204
00:17:07,600 --> 00:17:11,920
meet a lot of very well-meaning people in an organization but they kind of have to self-sensor.

205
00:17:11,920 --> 00:17:14,560
And even if they think there's something that's not quite right. They have to kind of be

206
00:17:14,560 --> 00:17:19,600
their self-sensor or maybe they just get into this habit of believing their elders or their

207
00:17:19,600 --> 00:17:22,720
superiors because of drinking the cool aid. Drinking the cool aid. Yeah I mean this is called

208
00:17:22,720 --> 00:17:25,600
group thing, right? You know it's like the opposite of collective intelligence which is what

209
00:17:25,600 --> 00:17:30,400
the data science should be all about. No so I mean so that was a tangent but I mean I think

210
00:17:30,400 --> 00:17:35,920
this recognition is it was it purely internal? Was it driven by regulatory framework? Yeah

211
00:17:35,920 --> 00:17:40,080
sort of fear. Yeah I don't I don't want to be granteos. I do have a sense. I don't want to be

212
00:17:40,080 --> 00:17:43,920
grandiose. Insurance is very heavily regulated. It's regulated at the state level in fact so it's

213
00:17:43,920 --> 00:17:47,920
not just one agency. It's 50 agencies in the United States and it's actually it's a rare case

214
00:17:47,920 --> 00:17:52,160
where it's actually more heavily regulated in the US than it is in Europe and so they absolutely

215
00:17:52,160 --> 00:17:56,720
in part in fact part of the reason why my employer wanted to build a statue of limitations

216
00:17:56,720 --> 00:18:00,640
applies here. I think one of the reasons they wanted to build this thing in house is that they

217
00:18:00,640 --> 00:18:06,720
actually wanted to have control over the details of the model. They want to be able to make sure

218
00:18:06,720 --> 00:18:11,120
that no we're getting this exactly right you know it maybe it's maybe you can't use medical

219
00:18:11,120 --> 00:18:15,040
bankruptcies in this state. Well we're not see we can prove it because this is our algorithm.

220
00:18:15,040 --> 00:18:18,560
Where's if we bought some black box off the shelf thing we're not sure we've reflected that

221
00:18:18,560 --> 00:18:24,000
regulation in the thing and of course regulations are an attempt to reflect societal values right

222
00:18:24,000 --> 00:18:28,480
so the ultimate thing is you want to reflect societal values in the algorithms and regulations

223
00:18:28,480 --> 00:18:31,600
are kind of a halfway house. I'm speaking philosophically from my perspective but I think that's

224
00:18:31,600 --> 00:18:36,720
what's going on here and so that's the game right I mean the companies want to make sure they're

225
00:18:36,720 --> 00:18:41,760
using algorithms to you know run their their processes more efficiently in the case of all state

226
00:18:41,760 --> 00:18:46,480
you know we want to you know it's the oldest game in the book you want to be able to come up with

227
00:18:46,480 --> 00:18:52,320
a more accurate price for a risk you know the logic of credit scoring and insurance is you know

228
00:18:52,320 --> 00:18:57,120
we all know that six-year-old male motorcycles are bad drivers right or probably risk I should

229
00:18:57,120 --> 00:19:01,600
say risk you've an average drivers perhaps right risk you then perhaps a middle-aged female

230
00:19:01,600 --> 00:19:06,480
station wagon driver perhaps but if you can find the six-year-old male motorcycle driver who's

231
00:19:06,480 --> 00:19:10,800
also present in the chess club subscribe to Martha Stewart Living magazine and has a good credit

232
00:19:10,800 --> 00:19:16,240
score he's probably good risk and if you can collect all those good credit score six-year-old male

233
00:19:16,240 --> 00:19:20,720
motorcycle drivers you can kind of give them a lower rate because they are actually better risks

234
00:19:20,720 --> 00:19:25,600
than might appear on the surface and that means the other companies who don't have credit score

235
00:19:25,600 --> 00:19:28,800
have to charge more for the six-year-old male motorcycle drivers and it's this kind of

236
00:19:28,800 --> 00:19:35,520
adverse selection spiral so that's that's the that's the kind of like economic logic for doing

237
00:19:35,520 --> 00:19:41,040
and this is why insurance is a very early adopter of big data data mining analytics but that's

238
00:19:41,040 --> 00:19:45,920
subject to a constraint I mean if you just did kind of crowdsourcing competition you know come

239
00:19:45,920 --> 00:19:51,040
up with the best segmentation thing right there'll be you know unless you've prepared the data

240
00:19:51,040 --> 00:19:56,000
yourself and unless you're very careful about auditing that algorithm you know you're not sure that

241
00:19:56,000 --> 00:20:00,880
that reflects these regulatory constraints which you know our reflexes are societal values or not

242
00:20:00,880 --> 00:20:04,000
so you know and other things crowdsourcing would be bad in this context I'm just saying you have

243
00:20:04,000 --> 00:20:07,280
to kind of take that into account you're optimizing more than one thing not just out of separate

244
00:20:07,280 --> 00:20:12,640
accuracy but these other things too and you know and I think there are a lot of companies that it's

245
00:20:12,640 --> 00:20:16,800
not just regulation they just want to do the right thing you know I mean like actually Richard

246
00:20:16,800 --> 00:20:20,640
Thaler who's one of my heroes he's a father of behavioral economics of the University of Chicago

247
00:20:20,640 --> 00:20:26,800
Business School he tweeted about I won't I won't say which company it is but it's an airline

248
00:20:26,800 --> 00:20:32,880
that kind of fixed its fees for flights out of Miami at a fairly low rate to help people

249
00:20:32,880 --> 00:20:38,480
escape the storm even though they could have done surge pricing Thaler would say that that

250
00:20:38,480 --> 00:20:42,560
just kind of goes against the grain of human psychology you know we have these things that Adam

251
00:20:42,560 --> 00:20:46,880
Smith called moral sentiments you know which we call ethics now it's like that just doesn't feel

252
00:20:46,880 --> 00:20:53,200
right so even though from a from a technical classical economics homo economic as rational profit

253
00:20:53,200 --> 00:20:59,120
maximizing perspective they should charge $4,000 for a flight to Atlanta from Miami but they didn't

254
00:20:59,760 --> 00:21:02,320
and failure saying it's because it's because they're thinking in the longer term

255
00:21:02,960 --> 00:21:06,640
in that case it wasn't regulation it was just like we're playing the long game

256
00:21:06,640 --> 00:21:10,720
and there are other companies that did jack up the prices right and that's really interesting

257
00:21:10,720 --> 00:21:15,840
actually because they they may be the case I don't know I'm speculating but that may have been

258
00:21:15,840 --> 00:21:20,720
unbridled algorithmic thinking it may be that like a pricing algorithm you know it's quite

259
00:21:20,720 --> 00:21:25,120
possible that some of these competitors did do surge pricing because it's kind of like the

260
00:21:25,120 --> 00:21:29,440
algorithm is just kind of calling the shots and that's what the algorithms usually did absolutely

261
00:21:29,440 --> 00:21:33,440
right and so this is a really nice case where it's it's sort of like parallel I think to the

262
00:21:33,440 --> 00:21:38,160
insurance case except it wasn't due to regulation it was just more due to like customizing value

263
00:21:38,160 --> 00:21:41,840
you don't want to alienate people right people just are going to remember things like this so it's

264
00:21:41,840 --> 00:21:46,720
like our customers but does the algorithm the algorithm would have to have a pretty long

265
00:21:47,680 --> 00:21:53,840
life cycle to pick up on that customer lifetime value yeah that's and that's the point

266
00:21:53,840 --> 00:21:58,480
that suggests that you know it's more likely than not there was human in a loop there precisely

267
00:21:58,480 --> 00:22:03,200
no that that's exactly the point yeah it's like you know we can kind of speculate about

268
00:22:03,200 --> 00:22:06,800
singularities we kind of speculate and have you know fun conversations about what are we going to

269
00:22:06,800 --> 00:22:11,520
reach out to visual general intelligence we have like a robot that can like use common sense and

270
00:22:11,520 --> 00:22:15,200
price it you know both to optimize things but also that's okay but that's not having any time

271
00:22:15,200 --> 00:22:18,880
soon right and we've got these are machine learning algorithms are essentially like statistical

272
00:22:18,880 --> 00:22:22,640
models you know on steroids basically deep learning models are like well just regression

273
00:22:22,640 --> 00:22:26,880
models on steroids that that create their own features right so that's that's what we got

274
00:22:26,880 --> 00:22:30,880
and that's great it's really really powerful but but as you're suggesting I think what it implies

275
00:22:30,880 --> 00:22:35,440
is that we want to have humans that have common sense reasoning to keep the models in check

276
00:22:35,920 --> 00:22:40,880
and so that what that implies is that the people that yeah and I'm going to quote an economist here

277
00:22:40,880 --> 00:22:45,040
and in John Kaye the people that understand that don't do too much of that on this podcast though

278
00:22:45,040 --> 00:22:47,920
well I really I'm just joking sorry

279
00:22:47,920 --> 00:22:52,080
what are your economists yeah that's a big turn on no John Kaye was my favorite I think he's

280
00:22:52,080 --> 00:22:56,320
retired he was my favorite columnist in the financial times he used to be an Oxford economist I

281
00:22:56,320 --> 00:23:00,800
think or London Business School was something but he was asked 10 years ago to diagnose

282
00:23:01,520 --> 00:23:05,840
somebody asking the question in the in the aftermath of the financial crisis why is it that all

283
00:23:05,840 --> 00:23:14,000
these models built by Harvard, Cambridge, MIT, Quance failed so badly and Kaye was so direct

284
00:23:14,000 --> 00:23:17,680
into the point of so elegant he said the problem was that the people that understood the math

285
00:23:17,680 --> 00:23:21,600
didn't understand the world the people that understood the world didn't understand the math and

286
00:23:21,600 --> 00:23:26,640
you know so I think that's yeah that's that's another kind of case of where we need to kind of like

287
00:23:26,640 --> 00:23:30,400
or I think I think I can't remember who said this but I heard a very nice quote of the day that

288
00:23:30,400 --> 00:23:34,800
a really good data scientist needs a kind of communication and empathy ability to be able to kind

289
00:23:34,800 --> 00:23:40,160
of talk to the people that understand the world not just to reflect their knowledge in the data

290
00:23:40,160 --> 00:23:46,960
but also to reflect just the kind of like strategic values the societal values the long true we don't

291
00:23:46,960 --> 00:23:51,440
want to alienate our customers along to all those kinds of things you know what I mean so interesting

292
00:23:51,440 --> 00:23:57,040
so this is all kind of background for your talk like how did you had you organized your talk

293
00:23:57,040 --> 00:24:02,160
did you have a list of human factors that an organization needs to consider or oh no it's

294
00:24:02,160 --> 00:24:08,320
nothing nothing that cutting right honestly nothing about me is like that in my background's

295
00:24:08,320 --> 00:24:11,680
philosophy right I always kind of go back to first principles and I'm just kind of I'm just

296
00:24:11,680 --> 00:24:15,920
really just thinking about what are algorithms good at you know why do we have algorithms what

297
00:24:15,920 --> 00:24:20,800
what you know what are their limitations what are ways of overcoming those limitations and yeah

298
00:24:20,800 --> 00:24:26,400
and I you know I do have I do have some ideas for you know where we can need to you know inject

299
00:24:26,400 --> 00:24:30,880
sort of like extra statistical or extra machine learning or extra computer you know beyond computer

300
00:24:30,880 --> 00:24:34,480
science principles into into what we're doing and so these are all examples that I've been

301
00:24:34,480 --> 00:24:39,280
giving though so the way I structure the talk should we get into that now or yeah the way I

302
00:24:39,280 --> 00:24:44,640
structure the talk is actually I'm quoting someone I know a little bit Chris Hammond at Northwestern

303
00:24:44,640 --> 00:24:48,400
University of Narrative Science he's somebody greatly admire actually that he he and I overlapped

304
00:24:48,400 --> 00:24:51,680
the University of Chicago's I was getting my PhD in philosophy when he was a computer science

305
00:24:51,680 --> 00:24:56,240
professor there so he's now at Northwestern doing really innovative stuff and he's also the chief

306
00:24:56,240 --> 00:25:01,280
science scientist which he's science officer of narrative science the natural language generation

307
00:25:01,280 --> 00:25:08,640
I mean this is really a nice way to think about AI we shouldn't think of AI in terms of the

308
00:25:08,640 --> 00:25:12,720
underlying technology we should really think about AI in terms of like what is its function what

309
00:25:12,720 --> 00:25:18,480
are we trying to achieve here oh well we're trying to automate this process we're trying humans are

310
00:25:18,480 --> 00:25:23,040
really bad at this they fall asleep in the wheel so let's have AI that drives for them or let's

311
00:25:23,040 --> 00:25:27,120
just have AI that kind of recognizes their face when they're getting drowsy like the effective

312
00:25:27,120 --> 00:25:32,400
software right like Rana Alkalubi then kind of nudges them maybe maybe turns the radio up or

313
00:25:32,400 --> 00:25:37,280
something right gives them a punch just like whatever it is I'm not trying to nudge or punch

314
00:25:37,280 --> 00:25:41,200
that's a goal and so some of these things can be done through robotic process automation

315
00:25:41,200 --> 00:25:46,160
which is not even data driven it's just kind of like logic some can be done through deep learning

316
00:25:46,160 --> 00:25:50,080
so yeah sure if you upload my photograph into Facebook it'll say that's a picture of George

317
00:25:50,080 --> 00:25:55,680
Clooney which is a pretty good guess right joke so that that's automation and but there's also

318
00:25:55,680 --> 00:25:59,680
the augmentation side of things which I want to talk about too so that that's that's kind of

319
00:25:59,680 --> 00:26:03,520
like the large structures start off with Chris Hammond talk about the fact that when we talk about AI

320
00:26:03,520 --> 00:26:07,120
it should be kind of like a functional thing not a tech first thing so it's not just about deep

321
00:26:07,120 --> 00:26:11,680
learning it's not just about machine learning it's really like it's really building computer

322
00:26:11,680 --> 00:26:16,000
algorithms that do things that were they to be done by humans they'd be considered intelligent

323
00:26:16,000 --> 00:26:20,320
right that's sort of like that's kind of Chris Hammond channeling John McCarthy at Dartmouth in

324
00:26:20,320 --> 00:26:26,400
1956 he's one of the founding fathers of AI very smart kind of like operational definition

325
00:26:26,400 --> 00:26:31,040
sure and I like it because it just it's a consultant because I'm really a consultant first

326
00:26:31,040 --> 00:26:34,000
I'm a consultant who happens to be a data scientist or then a data scientist who happens to

327
00:26:34,000 --> 00:26:37,760
work in a consulting firm and so as someone who really believes that he's a consultant I think

328
00:26:37,760 --> 00:26:41,680
that's just a really great way of thinking about it because I've seen you know the hype cycles

329
00:26:41,680 --> 00:26:47,600
come and go but over and over and over again I see that the the organizations and the the leaders

330
00:26:47,600 --> 00:26:53,360
who kind of take a tech first view of the stuff it tends to get a lot of attention and buzz early

331
00:26:53,360 --> 00:26:57,680
on but it doesn't really produce the value downstream whereas if you start with kind of like a problem

332
00:26:57,680 --> 00:27:02,720
centric view first and kind of reverse engineer from there well what do I want you're more

333
00:27:02,720 --> 00:27:07,120
likely to succeed and it's likely to be a more efficient elegant and frankly cost effective

334
00:27:07,120 --> 00:27:11,520
solution with less risk and in many cases a lot simpler and a lot simpler would have done if you

335
00:27:11,520 --> 00:27:16,960
were just following shiny object yes no exactly in fact you know again I'm old so I'll you know

336
00:27:16,960 --> 00:27:20,880
I'll I'll quote I'll do some more own quotes one of my someone who's sort of an informal mentor

337
00:27:20,880 --> 00:27:26,000
of mine at the University of Chicago was a very prominent Bayesian econometrician named Arnold

338
00:27:26,000 --> 00:27:31,760
Zelmer and he had a concept called sophisticated simplicity sophisticatedly simple the idea is

339
00:27:31,760 --> 00:27:37,360
that you start up with a simple model and if it works done if it doesn't work you just gradually

340
00:27:37,360 --> 00:27:42,320
add structure right until it does work and then you stop yeah you don't start with most complicated

341
00:27:42,320 --> 00:27:47,760
things it makes you seem like most smart or impressive or macho right and I think that an

342
00:27:47,760 --> 00:27:51,600
analogous comment can be made about our official intelligence and Chris was kind of making this point

343
00:27:51,600 --> 00:27:56,480
of the day in his tutorial which I which I just loved you know he said if you if all you need is

344
00:27:56,480 --> 00:28:01,520
robotic process automation do it what's what's the downside just do it you don't even need big data for

345
00:28:01,520 --> 00:28:06,000
that you just need like you know smart consultants and programmers and you'll just save a lot of

346
00:28:06,000 --> 00:28:10,560
money and there's very little downside risk let me ask you about you do a lot of you well you've

347
00:28:10,560 --> 00:28:15,520
brought up RV a couple of times oh yeah it's it's part of the family of AI yeah I'm just curious

348
00:28:15,520 --> 00:28:21,520
your perspective on this I jump to the question right the question is you know can you provide for

349
00:28:21,520 --> 00:28:31,200
me specific proof point examples where you know people are doing RPA that's you know that suggests

350
00:28:31,200 --> 00:28:36,800
that RPA is more than a rebranding of BPM you know I don't I probably shouldn't comment too much

351
00:28:36,800 --> 00:28:41,680
on that because I'm not like one of our RPA experts and maybe it is it's just like this is this is an

352
00:28:41,680 --> 00:28:46,400
idea that's been around for a long time it just makes eminent common sense I don't I don't really

353
00:28:46,400 --> 00:28:51,600
care what you call it so much but just the idea of taking processes we're just somebody's doing

354
00:28:51,600 --> 00:28:57,520
something that's just like routine and wrote boring spade work and if you can just get a macro

355
00:28:57,520 --> 00:29:02,880
or a script to do that why not do it that that's kind of like analogous zone zone viscous simplicity

356
00:29:02,880 --> 00:29:07,520
like a statistics thing if it's just like looking at the difference of two means is all you need

357
00:29:07,520 --> 00:29:12,320
you know looking to bootstrapping do it and a business context if all you need is to automate

358
00:29:12,320 --> 00:29:16,400
something that's really really wrote in simple and spade work do it you don't need machine learning

359
00:29:16,400 --> 00:29:22,480
for that you know and you know in kind of going up up that kind of food chain of complexity you

360
00:29:22,480 --> 00:29:26,400
just kind of want to start I guess what I was getting is you want to start with the problem

361
00:29:26,400 --> 00:29:31,840
and kind of back into the either technology or the data science and the machine learning whatever

362
00:29:31,840 --> 00:29:37,600
is a little solve the problem and so you mentioned automation and augmentation augmentation what

363
00:29:37,600 --> 00:29:41,360
is that mean for you and how are you seeing folks skin value there I've seen I've seen folks

364
00:29:41,360 --> 00:29:46,240
gain value from augmentation in my whole career I've been in Deloitte since 2001 and it's been one

365
00:29:46,240 --> 00:29:50,160
of the most common themes of what I've been doing we built algorithms that will automate things

366
00:29:50,880 --> 00:29:56,000
but very often you know we don't always work by augmentation are we talking about augmenting

367
00:29:56,000 --> 00:29:59,920
human intelligence okay that's exactly right augmentation or some other yeah yeah

368
00:29:59,920 --> 00:30:03,520
you know I'm talking about augmenting intelligence and that that vocabulary is somewhat new to me

369
00:30:03,520 --> 00:30:08,000
I haven't always described what we do in those terms but I like I like the vocabulary so yeah

370
00:30:08,000 --> 00:30:13,760
I mean you know we very often early on when we do our projects you know like for example suppose

371
00:30:13,760 --> 00:30:18,880
we're working for again with so we're working for an insurance company but say it's a commercial

372
00:30:18,880 --> 00:30:22,640
insurance company so instead of selling auto insurance they say for example they sell workers

373
00:30:22,640 --> 00:30:30,240
confidence now there are fewer businesses to insurer in the world than there are cars and businesses

374
00:30:30,240 --> 00:30:35,200
have fewer factors in common than cars do some are florists some are hipster coffee shops some are

375
00:30:35,200 --> 00:30:39,600
hospitals right and so you have like means you have fewer rows in your database new fewer columns

376
00:30:40,160 --> 00:30:43,760
and you but you're trying to do something similar to what you know my my first job was which is

377
00:30:43,760 --> 00:30:46,960
you're trying to come up with like a better price for the risk or an underrated decision should I

378
00:30:46,960 --> 00:30:50,720
sell this this person insurance or not should I sell your hipster coffee shop insurance or not

379
00:30:51,600 --> 00:30:56,960
and that's the case where like what we found just empirically you know through our data and through

380
00:30:56,960 --> 00:31:02,400
through you know blind test validation is that it would work pretty well in certain cases and

381
00:31:02,400 --> 00:31:06,160
that was an empirical question and it was it was partly empirical partly strategic you know it's

382
00:31:06,160 --> 00:31:09,120
like we'd have to work with the client to figure out what is the cutoff here where we're going

383
00:31:09,120 --> 00:31:13,200
to like straight through processes decisions whereas these other decisions really is going to simply

384
00:31:13,200 --> 00:31:17,920
give it to an underwriter maybe like rank order some things we'll try to explain the underwriter

385
00:31:17,920 --> 00:31:21,760
what's going on here you know we'll try to like train the underwriter ahead of time to understand

386
00:31:21,760 --> 00:31:26,160
the premises of the models and you know if and if we don't do that it's just not going to work

387
00:31:26,160 --> 00:31:30,240
so it's like a very simple example of you know what we were calling human factors earlier I don't

388
00:31:30,240 --> 00:31:34,160
know human factors is quite the right word but it's some some kind of like a either a human centered

389
00:31:34,160 --> 00:31:40,880
or an organization centered design we you know I began to use the analogy you know Mr. underwriter

390
00:31:40,880 --> 00:31:45,440
or Ms. Underwriter just you know imagine that your eyes are myopic and see you go to the doctor

391
00:31:45,440 --> 00:31:50,800
and you get a pair of glasses you can see better well you know Daniel Connemon and Danny O'Reilly

392
00:31:50,800 --> 00:31:55,200
and all these behavioral economists and psychologists teach us that our brains are myopic our brains

393
00:31:55,200 --> 00:32:00,400
have these you know bound biased heuristics that we use to make decisions so they we have blurring

394
00:32:00,400 --> 00:32:06,080
mental vision and so in these in these augmentation cases the algorithms are kind of like prostheses

395
00:32:06,080 --> 00:32:09,760
they're kind of like eyeglasses for the mind's eye they just help de-biased our cognition

396
00:32:10,240 --> 00:32:15,280
so kind of getting that equation right we sort of the art to our science and what fascinates me

397
00:32:15,280 --> 00:32:20,000
about it is that statistics is part of it but not all of it you know so in business we've always

398
00:32:20,000 --> 00:32:23,680
called this kind of change management so this kind of goes into the change management rubric

399
00:32:23,680 --> 00:32:28,240
frankly right now it's in art but I but I like to think that it can become more of a science

400
00:32:29,120 --> 00:32:34,400
so I call this the last mile problem you know we don't stop with an algorithmic output

401
00:32:34,400 --> 00:32:39,120
we stop with the decision in the case of automation the computer makes a decision it's saying

402
00:32:39,760 --> 00:32:43,440
you know I'm just going to send you this ad for these pair of shoes because I think you like these shoes

403
00:32:44,240 --> 00:32:49,520
the augmentation is more like you know I'm going to tell the doctor there's this probability

404
00:32:49,520 --> 00:32:54,160
this person has this rare disease but it's really the doctor's judgment call I'm just going to

405
00:32:54,160 --> 00:32:58,080
and I'm going to tell the doctor why the algorithm thinks this maybe I'll use an information retrieval

406
00:32:58,080 --> 00:33:03,920
system our IBM Watson to give some collateral information but I'm going to give this to the doctor

407
00:33:04,560 --> 00:33:10,160
this is one area where behavioral economics comes back again is that behavioral economics teaches us

408
00:33:10,160 --> 00:33:15,520
that simply giving people information doesn't always result in the optimal decision it's also the way

409
00:33:15,520 --> 00:33:19,760
you present information matters that we've learned this in the last 30 or 40 years this is the

410
00:33:19,760 --> 00:33:23,440
whole basis of the book nut which is only 10 years old so we've really come to appreciate this a

411
00:33:23,440 --> 00:33:28,080
lot more so the so behavioral economics is absolutely a you know one way of thinking of this

412
00:33:28,080 --> 00:33:33,680
quote human factors idea or human centered design idea you know so I feel like we've been sort of

413
00:33:33,680 --> 00:33:39,040
like muddling through perhaps all these years and it works right it's it's it's it's it's more

414
00:33:39,040 --> 00:33:42,640
when I say muddling through I mean it's more an art than a science it's it's something that we've

415
00:33:42,640 --> 00:33:46,880
done for a long time we've gotten better at over time we do it with our with our clients but I

416
00:33:46,880 --> 00:33:51,520
I'm intrigued with the idea that now that AI and machine learning is becoming such a

417
00:33:52,160 --> 00:33:58,960
a business as a sidal trend maybe there can be a new science emerging about this idea of human

418
00:33:58,960 --> 00:34:03,680
computer collaboration or human computer interaction can I give you one more example that sort of like

419
00:34:03,680 --> 00:34:08,320
yes this is not absolutely not a gym example it's not a deloitte example but I find it incredibly

420
00:34:08,320 --> 00:34:12,560
thought-provoking so it's more of a metaphor but it but I find it's a very thought-provoking metaphor

421
00:34:12,560 --> 00:34:16,480
and it's a very nice way to think about sort of the future of work to you know people being

422
00:34:16,480 --> 00:34:20,640
displaced algorithms and so on and forgive me if you've heard this if you heard the sort of

423
00:34:20,640 --> 00:34:26,320
about freestyle chess no I don't think so good thank you I like when people say no I only learned

424
00:34:26,320 --> 00:34:30,160
of a few years ago myself I actually read the article and I forgot it and then I reread it

425
00:34:30,800 --> 00:34:36,640
so I read an article in 2011 by Gary Kasparov the chess grandmaster who's published in the New

426
00:34:36,640 --> 00:34:42,000
York Review of Books in 2011 and he was talking about his own experiences being put out of work

427
00:34:42,000 --> 00:34:47,760
by IBM Deep Blue so this is a prequel to Watson right you know there's like there's a magazine

428
00:34:47,760 --> 00:34:52,400
cover cover called the brain's last stand you know the machine is vanquishing man right the chess

429
00:34:52,400 --> 00:34:56,560
master because that's identified with human intelligence and this is way back in 97 it's like

430
00:34:56,560 --> 00:35:02,400
20 years ago right I mean I thought it was like I just turned 12 I think kidding and but it turned

431
00:35:02,400 --> 00:35:07,600
out the story is a lot more interesting than that Kasparov actually invented a new game after he

432
00:35:07,600 --> 00:35:13,280
lost to Deep Blue called advanced chess an advanced chess would be instead of me playing you in chess

433
00:35:13,760 --> 00:35:17,760
it'd be Jim equipped with a laptop playing you equipped with a laptop and it turns out that the

434
00:35:17,760 --> 00:35:23,680
same skills that enabled Kasparov to be good at traditional chess he wasn't quite as good

435
00:35:23,680 --> 00:35:30,400
as freestyle chess or this advanced chess concept right anyway that's fast forward to the year 2005

436
00:35:30,400 --> 00:35:35,440
and I think a German website had an open game called freestyle chess which is anybody around the

437
00:35:35,440 --> 00:35:41,600
world can enter it can be you know Kasparov playing another grandmaster it can be Kasparov plus

438
00:35:41,600 --> 00:35:44,880
teaming up with a super computer playing another grandmaster teamed up with another super computer

439
00:35:44,880 --> 00:35:51,520
can be anything okay and turns out there's an upset victory the team that one was two amateur

440
00:35:51,520 --> 00:35:56,240
chess players from New Hampshire it's two young guys working with three ordinary laptops equipped

441
00:35:56,240 --> 00:36:00,960
with three different chess programs they won freestyle chess they beat the grandmasters and the

442
00:36:00,960 --> 00:36:05,120
supercomputers and the grandmasters working with the supercomputers supercomputers and Kasparov when he

443
00:36:05,120 --> 00:36:11,120
wrote about this in the New York review of books he said this is a leader called Kasparov's law it's

444
00:36:11,120 --> 00:36:15,360
a weak human plus an ordinary computer plus a better process of working together

445
00:36:15,360 --> 00:36:20,480
outperforms the grandmaster or the supercomputer or both plus an inferior process

446
00:36:21,520 --> 00:36:24,880
and when I'm going to present this this afternoon I'm going to circle the better process

447
00:36:24,880 --> 00:36:30,800
that's what we need and when I when I read that for the second time I'm a slow study you know

448
00:36:30,800 --> 00:36:35,360
when I read this all these years later I realized oh my god that better process of the chess player

449
00:36:35,360 --> 00:36:39,520
working with the computer that's just like what we would do in our consulting practice when we give

450
00:36:39,520 --> 00:36:44,560
like a doctor an underwriter an admissions officer a public sector case worker a list of cases

451
00:36:45,280 --> 00:36:50,400
saying here this will be bias your judgment but it's ultimately up to you and we're going to help

452
00:36:50,400 --> 00:36:54,000
you do this we're going to train you to do it we're going to train you to understand the algorithm

453
00:36:54,000 --> 00:36:57,440
we're going to try to train you to understand you know it's premises it's assumptions the

454
00:36:57,440 --> 00:37:01,840
data it's based on the variables in the model and that way if you know that the the model contains

455
00:37:01,840 --> 00:37:07,200
variables one through forty but you know factors forty one forty two and forty three and if you judge

456
00:37:07,200 --> 00:37:11,600
those to be really important and the algorithm doesn't know that then you you can override the algorithm

457
00:37:11,600 --> 00:37:14,880
and that's okay because you're using your brain you're not just you know using kind of

458
00:37:14,880 --> 00:37:19,600
thinking fast you're not using biased heuristics you're making up using metacognition and using

459
00:37:19,600 --> 00:37:24,160
intelligence to say yeah the computer is saying this but I've also got this common sense of this

460
00:37:24,160 --> 00:37:28,400
other I know these contextual factors I'm going to override it and do this other thing and I

461
00:37:28,400 --> 00:37:33,360
could be wrong but at least it's a principal decision yeah so when I talk about freestyle chess

462
00:37:33,360 --> 00:37:38,640
I'm not trying to make the claim that a human computer will always win chess that's not my point

463
00:37:38,640 --> 00:37:45,760
but the point is that it's a very nice metaphor for this idea that the computer can do things

464
00:37:45,760 --> 00:37:50,320
the humans aren't good at like look through this decision tree of you know you know all these

465
00:37:50,320 --> 00:37:54,560
possible moves and all the implications these moves downstream better than a human can but the

466
00:37:54,560 --> 00:37:59,280
human has other you know kinds of capabilities right it can kind of so it can cast for off comments

467
00:37:59,280 --> 00:38:05,040
when about these two guys who won freestyle chess he said they're insight into looking deeply at

468
00:38:05,040 --> 00:38:09,760
what the computers were indicating it really enabled them to kind of outperform the grand

469
00:38:09,760 --> 00:38:13,200
message in a fair process so they actually had an insight into how the algorithms worked and

470
00:38:13,200 --> 00:38:17,120
developed kind of like an intuitive spidey sense for when should I trust this recommendation versus

471
00:38:17,120 --> 00:38:22,400
that recommendation so I just find it like a very nice metaphor for a real world professional

472
00:38:22,400 --> 00:38:26,880
making a machine critical judgment under it's called judgment under uncertainty with the help

473
00:38:26,880 --> 00:38:34,800
of an algorithm yeah it strikes me that the process in yeah his characterization of this is it's

474
00:38:34,800 --> 00:38:40,720
kind of a lot of vocabulary that has a bunch of individual things under it right there it's like

475
00:38:40,720 --> 00:38:44,960
user interface there's you know the things that we might traditionally think of as a process like

476
00:38:44,960 --> 00:38:52,880
your right steps there's mentioned a bank of experiences to fall back on on how you know

477
00:38:53,600 --> 00:39:00,400
how have I been able to rely on the computer's advice historically have you done or seen any kind of

478
00:39:00,400 --> 00:39:08,800
work to kind of characterize this more more granularly yeah I mean you mean like exactly how do you

479
00:39:08,800 --> 00:39:12,320
pull off this better process you may like what are the steps involved with the principles involved

480
00:39:12,320 --> 00:39:18,160
and so I think ultimately the goal is like as a business if I can you know if I can pick apart the

481
00:39:18,160 --> 00:39:23,600
pieces of what you know process means in this battle that enable you know plus computer to

482
00:39:23,600 --> 00:39:28,800
you know to outperform you know expert then you know that kind of provides me a roadmap for while

483
00:39:28,800 --> 00:39:34,000
first I need to make sure that my data is you know displayed in a way that is comprehensible for the

484
00:39:34,000 --> 00:39:38,880
computer example you know then I need to make sure that I've got the tools available to interact

485
00:39:38,880 --> 00:39:44,560
with you know the systems it's I'm just I'm curious whether you know how evolved the thinking is

486
00:39:44,560 --> 00:39:49,120
there like I said I would like it to be more of a science than it is but I think what ends up

487
00:39:49,120 --> 00:39:53,760
happening is that it's it's it's a series of kind of like what are Simon called satisfying we

488
00:39:53,760 --> 00:39:59,200
make it you know maybe not optimal decisions but we make we kind of like you know look at the

489
00:39:59,200 --> 00:40:02,160
business context maybe never will be a science maybe it's always going to be like it'll always

490
00:40:02,160 --> 00:40:06,960
be like a devil in the details kind of thing you know so like you know if it's a metal if it's

491
00:40:06,960 --> 00:40:12,560
a medical case then the case is where you let the computer just make an automatic decision versus a

492
00:40:12,560 --> 00:40:17,920
human might be different depending on how many doctors are around you know if you're in a poor

493
00:40:17,920 --> 00:40:22,800
country you know it might not be optimal to have a doctor working with a computer but you know if

494
00:40:22,800 --> 00:40:28,160
the village has no doctors at all and you can just like take a picture of a wound you know do deep

495
00:40:28,160 --> 00:40:32,480
learning on and upload it into the cloud and it comes back with like you don't need stitches versus

496
00:40:32,480 --> 00:40:37,840
you do need stitches that's pretty good ideally we'd have a human in the loop you know what I mean

497
00:40:37,840 --> 00:40:41,280
but so that it's asking me like it is modeling through kind of thing like you know what what is

498
00:40:41,280 --> 00:40:46,880
the cut off you know you know in some cases like jurisprudence you know I think Daniel kind of

499
00:40:46,880 --> 00:40:50,400
wrote about this actually a few years ago he said the public would be shocked to hear that like

500
00:40:50,400 --> 00:40:54,400
an algorithm was making decisions without a judge I mean is it even is that even constitutional

501
00:40:54,400 --> 00:40:59,280
so that might be kind of like just like ground you know unavoidable reasons why you always need

502
00:40:59,280 --> 00:41:03,600
to have a human in the loop but I think that you know I think that we are kind of gradually getting

503
00:41:03,600 --> 00:41:08,480
better at this stuff I mean people are coming up with better algorithms for explaining models

504
00:41:08,480 --> 00:41:12,240
like so one of the I'm probably going to get his name wrong one of the speakers early in the

505
00:41:12,240 --> 00:41:16,960
conference Carlos Questren from University of Austin yeah yeah he came up with the Lyme algorithm

506
00:41:16,960 --> 00:41:21,520
right for kind of explaining why does a deep learning model classify what it classifies well

507
00:41:21,520 --> 00:41:26,640
conceptually and I hated to say conceptually it's much more sophisticated but you know we've

508
00:41:26,640 --> 00:41:32,560
always done analogous things with our work right I mean we would we would output not just a score

509
00:41:33,520 --> 00:41:39,600
saying you know the answer is 42 we'd say well what does 42 mean and why does the algorithm think

510
00:41:39,600 --> 00:41:45,120
it's 42 you know so you know so every single score is contextualized with a set of sort of

511
00:41:45,120 --> 00:41:50,160
like English language you know language so again primitive natural language generation but still

512
00:41:50,160 --> 00:41:53,520
nevertheless natural language generation so all these kinds of things we've been doing for a long

513
00:41:53,520 --> 00:41:57,920
time are getting refined you know so we've got better reason algorithms we've got natural

514
00:41:57,920 --> 00:42:02,080
Christmas natural language generation right we've got more advanced data visualization maybe

515
00:42:02,080 --> 00:42:06,000
we're going to come up with better apps so that you know people the emotional aspect of this

516
00:42:06,000 --> 00:42:09,920
important you know John Whalen was speaking yesterday about the emotional quality of this kind of

517
00:42:09,920 --> 00:42:14,480
stuff and he said you know a nice comment he made was that people will choose a personal digital

518
00:42:14,480 --> 00:42:20,320
assistant even if it's the less accurate if it's just more emotionally pleasing to work with

519
00:42:20,320 --> 00:42:24,400
and so even just getting that right is something and that's something that my practice you know

520
00:42:24,400 --> 00:42:29,120
probably could get a little bit better at yeah so these are all different aspects of the human

521
00:42:29,120 --> 00:42:34,480
factors right so it's like some of it is helping us think better but there's a lot of interesting

522
00:42:35,040 --> 00:42:40,080
kind of neuroscience around emotions and I think you know in the last 20 years

523
00:42:40,080 --> 00:42:45,200
so another you know kind of like headline that it's kind of new to a lot of people including me is

524
00:42:45,200 --> 00:42:49,360
that emotions are not kind of like the Mr. Spock versus Captain Kirk thing that we all think of

525
00:42:49,360 --> 00:42:54,560
because it's not like emotions are so like the noise it closes the static rationality it's more

526
00:42:54,560 --> 00:42:58,400
like emotions are sort of part and parcel of the fundamental yeah yeah it's a part and parcel

527
00:42:58,400 --> 00:43:02,560
a big part of what thinking fast that's right thinking slow I'm just messed that up no thinking

528
00:43:02,560 --> 00:43:06,560
fast and slow that's exactly right yeah and also like you know the the effective computing stuff that

529
00:43:06,560 --> 00:43:11,840
Rana L. Kalebi was was speaking about that kind of relates I think you know that that's effective

530
00:43:11,840 --> 00:43:16,080
computing well there's also effective neuroscience and like one of the findings of effective neuroscience

531
00:43:16,080 --> 00:43:21,440
is that healthy emotions are important to rational decision making they're they're not separate you

532
00:43:21,440 --> 00:43:25,520
know and so that's an interesting kind of lens through which to look at this too you know and

533
00:43:25,520 --> 00:43:28,720
again these are these are all these are all developing now so why such an exciting time to be

534
00:43:28,720 --> 00:43:35,280
working in this field so I think the general idea is that I think savvy people have always realized

535
00:43:35,280 --> 00:43:38,960
that when it comes to more complex decisions you don't want to just turn over to an algorithm

536
00:43:38,960 --> 00:43:43,360
sure we're surrounded by more big data now sure our algorithms are getting better sure our

537
00:43:43,360 --> 00:43:48,240
computing power is is getting cheaper and cheaper so sure there will be more and more decisions

538
00:43:48,240 --> 00:43:53,600
that can be automated but until we come up with this kind of singularity which you know

539
00:43:53,600 --> 00:43:57,920
right whatever you know it's not separate podcasts not on that horizon anytime soon yeah we're

540
00:43:57,920 --> 00:44:01,120
you know for a lot of decisions we're going to have kind of humans in the loop we're going to need to

541
00:44:01,120 --> 00:44:06,640
kind of have a science of augmentation this kind of the freestyle x idea so freestyle insurance

542
00:44:06,640 --> 00:44:11,200
underwriting freestyle medicine freestyle jurisprudence freestyle university admissions right it's

543
00:44:11,200 --> 00:44:15,680
the algorithms helping be by us the humans but the humans kind of keeping the algorithms in check

544
00:44:15,680 --> 00:44:19,600
and so getting getting that balance right is that's what I find fascinating that that's what kind

545
00:44:19,600 --> 00:44:24,960
of that's fantastic yeah well I'll mention since you mentioned Carlos Gastran and Ronald

546
00:44:24,960 --> 00:44:29,920
Koyubi I'll note for folks that are listening that both of them have been on the podcast before

547
00:44:29,920 --> 00:44:39,440
great taste until that Carlos at the very first AI conference in New York and Rana at the

548
00:44:40,160 --> 00:44:44,880
previous one in New York this is the third one and so folks can find that find those on the

549
00:44:44,880 --> 00:44:51,440
website both great conversations beautiful and I really enjoy this conversation likewise thank you

550
00:44:51,440 --> 00:45:02,800
so much thank you real pleasure absolutely all right everyone that's our show for today thank you

551
00:45:02,800 --> 00:45:10,240
so much for listening and of course for your ongoing feedback and support for more information on

552
00:45:10,240 --> 00:45:16,640
James or any of the other topics covered in this episode head on over to twomlai.com slash talk

553
00:45:16,640 --> 00:45:23,600
slash 56 and please please please remember to send any questions or comments that you have

554
00:45:23,600 --> 00:45:31,280
either for us or our guests via Twitter at twomlai or at sam charrington or just leave a comment

555
00:45:31,280 --> 00:45:38,160
right on the show notes page for more information about the Halloween social visit twomlai.com slash

556
00:45:38,160 --> 00:45:45,840
Halloween tickets are on sale right now and we do expect a sellout so get your tickets to register

557
00:45:45,840 --> 00:45:53,040
for Wednesday's meetup visit twomlai.com slash meetup thanks again for listening and catch you

558
00:45:53,040 --> 00:46:18,240
next time

