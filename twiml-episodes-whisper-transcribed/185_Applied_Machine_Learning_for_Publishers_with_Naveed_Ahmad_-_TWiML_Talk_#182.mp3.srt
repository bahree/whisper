1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,920
I'm your host, Sam Charrington.

4
00:00:31,920 --> 00:00:36,200
In today's show, we're joined by Navid Ahmad, Senior Director of Data Engineering and

5
00:00:36,200 --> 00:00:38,920
Machine Learning at Hearst Newspapers.

6
00:00:38,920 --> 00:00:43,760
A few months ago, Navid gave a talk at the Google Cloud Next Conference on how publishers

7
00:00:43,760 --> 00:00:46,640
can take advantage of machine learning.

8
00:00:46,640 --> 00:00:51,360
In this conversation, we dive into the role of ML at Hearst, including their motivations

9
00:00:51,360 --> 00:00:56,160
for implementing it and some of their early projects, the challenges of data acquisition

10
00:00:56,160 --> 00:01:00,880
within a large organization, and the benefits they enjoy from using Google's BigQuery as

11
00:01:00,880 --> 00:01:02,720
their data warehouse.

12
00:01:02,720 --> 00:01:03,720
Enjoy the show.

13
00:01:03,720 --> 00:01:04,720
All right, everyone.

14
00:01:04,720 --> 00:01:07,040
I am on the line with Navid Ahmad.

15
00:01:07,040 --> 00:01:12,480
Navid is Senior Director of Data Engineering and Machine Learning at Hearst.

16
00:01:12,480 --> 00:01:15,760
Navid, welcome to this weekend machine learning and AI.

17
00:01:15,760 --> 00:01:16,760
Thank you.

18
00:01:16,760 --> 00:01:18,360
Thank you for having me on your show.

19
00:01:18,360 --> 00:01:22,200
So you've been in the publishing industry for about 10 years now.

20
00:01:22,200 --> 00:01:26,960
Can you tell us a little bit about both your current role and some of the past things

21
00:01:26,960 --> 00:01:28,960
you've done in publishing?

22
00:01:28,960 --> 00:01:29,960
Sure.

23
00:01:29,960 --> 00:01:33,840
So I'm currently a Senior Director for Data Engineering and Machine Learning at Hearst,

24
00:01:33,840 --> 00:01:36,240
being here for about two years.

25
00:01:36,240 --> 00:01:44,240
My rule has been building the data warehouse, building personalization, and also doing

26
00:01:44,240 --> 00:01:48,960
predictive analysis using that data.

27
00:01:48,960 --> 00:01:54,120
And before Hearst, I was at New York Times.

28
00:01:54,120 --> 00:02:03,120
I worked in the subscription space where I'm working on the CRM system, and also some

29
00:02:03,120 --> 00:02:12,400
of the aspects of machine learning, like churn modeling and email content detection.

30
00:02:12,400 --> 00:02:19,640
And before that, I was in Thomson Reuters, where I also built their data and used a distribution

31
00:02:19,640 --> 00:02:26,960
platform, as well as part of their CMS team contributed to recommendation systems.

32
00:02:26,960 --> 00:02:30,960
And before that, I was mostly in telecommunication.

33
00:02:30,960 --> 00:02:38,120
And so you recently presented at the Google Cloud Next Conference on Applied Machine Learning

34
00:02:38,120 --> 00:02:41,600
for Publishers.

35
00:02:41,600 --> 00:02:47,960
Before we jump into that topic, maybe you should take a second to provide an overview of Hearst

36
00:02:47,960 --> 00:02:51,560
for those who aren't familiar with the company.

37
00:02:51,560 --> 00:02:52,560
First.

38
00:02:52,560 --> 00:02:56,680
So Hearst, I especially work in Hearst newspaper department.

39
00:02:56,680 --> 00:03:05,680
Hearst is a very large organization with more than 300 businesses, which includes magazines,

40
00:03:05,680 --> 00:03:15,240
investment, and television channel, and within even newspapers, there's about 40 plus websites,

41
00:03:15,240 --> 00:03:21,120
which includes names like San Francisco Chronicle and Houston Chronicle and Times Union.

42
00:03:21,120 --> 00:03:29,160
It is, and Hearst had quarters located here at 57th Street in New York.

43
00:03:29,160 --> 00:03:31,720
And that's some background about Hearst.

44
00:03:31,720 --> 00:03:38,360
So one of the things that you mentioned in describing your background and in our conversation

45
00:03:38,360 --> 00:03:47,720
before the interview started was the role of the data warehouse and enabling you to

46
00:03:47,720 --> 00:03:54,680
perform the types of machine learning that you want to be able to perform at Hearst.

47
00:03:54,680 --> 00:04:01,680
Can you talk a little bit about the data warehouse and the process for establishing it?

48
00:04:01,680 --> 00:04:02,680
Yeah.

49
00:04:02,680 --> 00:04:11,680
So one of the first tasks for me when I joined Hearst was to build a data warehouse, especially

50
00:04:11,680 --> 00:04:18,720
I used Google BigQuery as a tutorial data warehouse.

51
00:04:18,720 --> 00:04:28,320
And the idea was that we need all the data sets in one place to be able to look at what

52
00:04:28,320 --> 00:04:36,040
is the relationship between newsletter and web and subscriptions.

53
00:04:36,040 --> 00:04:43,280
And the first use case was to build business and tell it on like regular reporting on

54
00:04:43,280 --> 00:04:45,120
top of this data.

55
00:04:45,120 --> 00:04:52,320
And the same data can be used to, that's used to give data reports about current status

56
00:04:52,320 --> 00:04:55,440
can be used to do predictive modeling.

57
00:04:55,440 --> 00:05:05,440
So using BigQuery and basically we build our data sets include Google Analytics, be our

58
00:05:05,440 --> 00:05:12,160
content, our newsletters, so all sorts of data related to our business is in BigQuery.

59
00:05:12,160 --> 00:05:15,600
And this forms as a foundation for machine learning.

60
00:05:15,600 --> 00:05:20,800
So the data warehouse is kind of central to your ability to perform machine learning and

61
00:05:20,800 --> 00:05:26,560
analytics and it was one of the first things that you established at Hearst.

62
00:05:26,560 --> 00:05:33,840
Can you talk a little bit about any challenges that you experienced trying to centralize all

63
00:05:33,840 --> 00:05:38,080
of the data from these various sources?

64
00:05:38,080 --> 00:05:45,760
The challenges were that our data was sitting in different formats like before the warehouse,

65
00:05:45,760 --> 00:05:54,720
would get either data from individual systems or there's Excel worksheets going around.

66
00:05:54,720 --> 00:06:02,880
So really figure out the data sources and then building a platform for ETL, that was

67
00:06:02,880 --> 00:06:04,120
great challenge.

68
00:06:04,120 --> 00:06:11,680
Some of the bigger data sources like Google Analytics and DFP, they were easier to get into

69
00:06:11,680 --> 00:06:18,960
BigQuery but some of the ones we have to build specific code to ETL that into BigQuery.

70
00:06:18,960 --> 00:06:20,720
And what is DFP?

71
00:06:20,720 --> 00:06:22,400
Double click for publishers.

72
00:06:22,400 --> 00:06:26,400
Okay, so that's an advertising, your advertising system?

73
00:06:26,400 --> 00:06:31,760
Yeah, this is all the log data for each advertisement that impressions.

74
00:06:32,640 --> 00:06:37,040
And so the data warehouse has your analytics.

75
00:06:37,040 --> 00:06:43,280
So the click stream data for people that are visiting the site, it's got information about

76
00:06:43,280 --> 00:06:48,480
the advertising, interactions and maybe those click streams.

77
00:06:49,040 --> 00:06:51,280
Does it also contain content information?

78
00:06:51,280 --> 00:06:55,200
Yeah, so all our CMS content is in BigQuery.

79
00:06:55,200 --> 00:06:59,760
All our newsletter, our bird data, that's in BigQuery.

80
00:06:59,760 --> 00:07:03,200
And is that data replicated to BigQuery for

81
00:07:03,200 --> 00:07:10,160
analytics or is that it's native place and you're publishing directly to and from BigQuery?

82
00:07:10,160 --> 00:07:12,800
Yeah, it is replicated.

83
00:07:12,800 --> 00:07:18,400
Like the other, we have a separate CMS system, it has our own database.

84
00:07:18,400 --> 00:07:20,960
So we're replicating that BigQuery, you know.

85
00:07:20,960 --> 00:07:26,320
And these are very different types of data.

86
00:07:26,320 --> 00:07:31,200
What's the goal for pulling this all into a single place?

87
00:07:31,200 --> 00:07:38,000
Yeah, so each of these data, they have identifiers that link them up with other datasets.

88
00:07:38,000 --> 00:07:44,160
For example, a newsletter with their hashed email address.

89
00:07:44,160 --> 00:07:47,600
You can link it up with subscription data.

90
00:07:47,600 --> 00:07:53,760
And from subscription data, we can link it up to Google Analytics data.

91
00:07:53,760 --> 00:07:57,600
So the idea is to be able to connect all the aspects of a user

92
00:07:57,600 --> 00:08:01,840
using these identifiers.

93
00:08:01,840 --> 00:08:07,360
It sounds like the primary focus from an analytics, a predictive analytics,

94
00:08:07,360 --> 00:08:14,880
and a machine learning perspective is based on this link data, this user-centric link data.

95
00:08:14,880 --> 00:08:16,080
Yes, absolutely.

96
00:08:16,560 --> 00:08:23,520
And so maybe let's talk through some of the different things that you've done with the data

97
00:08:23,520 --> 00:08:29,040
from a machine learning perspective. What are the different challenges that you're trying to address?

98
00:08:30,080 --> 00:08:35,920
So we built a few different machine learning-based products and predictive models

99
00:08:35,920 --> 00:08:38,160
using this data we have in BigQuery.

100
00:08:38,800 --> 00:08:46,560
So number one, the one we are currently is a churn model, churn prediction.

101
00:08:46,560 --> 00:08:54,160
The churn prediction is that be able to predict how likely a subscriber is to cancel their subscription.

102
00:08:55,120 --> 00:09:02,080
And it's very relevant in the media space because it's a bit a challenging environment to keep

103
00:09:02,080 --> 00:09:08,480
subscribers. Since we have subscription data, how people, the different attributes of their usage,

104
00:09:08,480 --> 00:09:18,160
behavior, how many newsletters they signed up to, also customer service data associated with

105
00:09:18,160 --> 00:09:26,880
each subscriber. And the way it works is we take like a year-old worth of data and take their

106
00:09:26,880 --> 00:09:34,880
attributes of month and feed it into a machine learning model. And we know that in the next month,

107
00:09:34,880 --> 00:09:41,040
how many of those subscribers were canceled and versus sudden cancel. So this is a binary

108
00:09:41,040 --> 00:09:48,240
classification problem. And using six months of data, we have training data of subscribers who

109
00:09:48,240 --> 00:09:53,600
didn't cancel versus cancel and we can build a predictive model on it. So one of the things

110
00:09:53,600 --> 00:10:01,440
that we've done is since we're on BigQuery, we've used this new, the launch feature called BigQuery

111
00:10:01,440 --> 00:10:10,800
ML, which was released in Google next back in August. And the great thing about BigQuery ML is that I

112
00:10:10,800 --> 00:10:18,800
can do a training and prediction model right using SQL syntax, i.e., you don't have to write code

113
00:10:18,800 --> 00:10:27,440
or Python code. And it does some of the like machine learning goodies, like normalization of

114
00:10:27,440 --> 00:10:39,360
features and also fine-tuning the model. And one of the things that I emphasize that this enables

115
00:10:39,360 --> 00:10:45,120
people who don't even have machine learning background, like people in business intelligence or

116
00:10:45,120 --> 00:10:54,000
who have background in SQL can easily write machine learning code just by using SQL syntax.

117
00:10:54,000 --> 00:11:02,080
So we built out our first model on San Francisco. And we're just working on

118
00:11:02,800 --> 00:11:11,120
integrating with our marketing and CRM systems. And so this was one of the use cases

119
00:11:12,160 --> 00:11:18,080
for machine learning. Then we've also built one of the things that I've done over here's use.

120
00:11:18,080 --> 00:11:23,280
If I could jump in before you go to the next one, one of the things that really struck me

121
00:11:23,280 --> 00:11:33,440
at the Google next conference and not to turn this into a BigQuery commercial, but I was really

122
00:11:33,440 --> 00:11:39,840
surprised by the enthusiasm for BigQuery. People seem to really love that database. Can you

123
00:11:40,960 --> 00:11:47,360
like maybe net out for me why folks are excited about it and the BigQuery ML piece that they just

124
00:11:47,360 --> 00:11:55,440
announced? Yeah. So in general, BigQuery is a fully managed data warehouse. So you don't have to

125
00:11:55,440 --> 00:12:05,680
have a DBA to fine-tune or optimize the database. And it's based on Google's own internal

126
00:12:05,680 --> 00:12:11,520
Dremel technology. What I've heard is heavily Dremel technologies is heavily used inside of Google.

127
00:12:11,520 --> 00:12:20,240
And so they expose that as BigQuery. And their philosophy is it's full scan. Any data set

128
00:12:21,440 --> 00:12:29,280
basically spawns off machines or compute in the back end and is able to very quickly get your

129
00:12:29,280 --> 00:12:36,880
results. Especially if you have large data sets and terabytes or petabyte scale, it only takes a

130
00:12:36,880 --> 00:12:44,560
few minutes to run SQL. So it's very easy in terms of maintenance. It's based on SQL syntax.

131
00:12:46,320 --> 00:12:54,480
That's why it's very an attractive option for data mining. And BigQuery ML, so they built out

132
00:12:54,480 --> 00:13:01,520
machine learning on top of BigQuery, which even further aids very fast because it's using the same

133
00:13:01,520 --> 00:13:10,080
compute infrastructure. And it abstracts out the machine learning as an SQL SQL.

134
00:13:10,800 --> 00:13:18,000
So it becomes, I foresee, that it'll enable machine learning for a lot of people, especially

135
00:13:18,000 --> 00:13:23,760
who are on BigQuery. And so it's the idea then that you'll have like, you know, where you might

136
00:13:23,760 --> 00:13:29,360
have aggregators in SQL like, you know, average or max or something like that, you can apply

137
00:13:29,360 --> 00:13:35,840
some kind of model to or is it different? Yeah, it's absolutely. So just like these functions,

138
00:13:36,560 --> 00:13:44,480
you mentioned they've introduced a few different functions to be able to train and predict and even

139
00:13:44,480 --> 00:13:51,200
get machine learning and metrics like once the machine learning model is built to get like,

140
00:13:51,200 --> 00:13:57,600
for example, what's the AUC of this machine learning or precision and recall. So you can do all

141
00:13:57,600 --> 00:14:04,640
of those things just by a function called with a SQL syntax. Yeah, as compared to using a tool like

142
00:14:04,640 --> 00:14:12,320
scikit-learn or TensorFlow, the other advantage is you don't have to take the data outside of the

143
00:14:12,320 --> 00:14:18,560
system. You do everything in one place. Like if you were using scikit-learn or TensorFlow, there's

144
00:14:18,560 --> 00:14:25,440
this whole process of extracting the data, massaging into a format that machine learning framework

145
00:14:25,440 --> 00:14:30,880
can understand, build a machine learning model, pull that data back into your data warehouse.

146
00:14:30,880 --> 00:14:37,680
So this whole cycle just gets reduced because you're doing everything in line and BigQuery.

147
00:14:37,680 --> 00:14:44,000
You were about to go into another use case beyond the well, actually back on the on the churn.

148
00:14:45,760 --> 00:14:50,240
Yeah, I'm curious. This isn't necessarily a machine learning question. You know, I've talked

149
00:14:50,240 --> 00:14:58,000
about churn prediction in many cases across many different industries and I understand broadly

150
00:14:58,000 --> 00:15:05,440
how it's applicable. But I'm curious in the case of a publisher, you know, what specifically

151
00:15:05,440 --> 00:15:12,160
is Hurst going to do on the business side once it's able to predict that a user has a

152
00:15:12,160 --> 00:15:20,320
high likelihood to churn? Yeah, that's a good question. And we're working with marketing and people

153
00:15:20,320 --> 00:15:27,280
in the subscription business. What are the different ways? So some of the ways is that we,

154
00:15:27,280 --> 00:15:33,040
in our marketing platform, we were able to give messaging and be able to send emails.

155
00:15:34,800 --> 00:15:41,040
Typically a user who is not engaged a lot are some of the people. Those are some of the attributes

156
00:15:41,040 --> 00:15:46,000
that are indicative that a person is going to cancel their subscription.

157
00:15:46,960 --> 00:15:52,880
So the email to nudge somebody, hey, you know, this, did you know that there's a certain feature

158
00:15:52,880 --> 00:16:00,640
or educating our print subscribers that they have free digital benefits are some of the things

159
00:16:00,640 --> 00:16:08,720
that they can, we can employ on getting people to less or cancel their subscription.

160
00:16:08,720 --> 00:16:16,560
You built this on BigQuery ML using this new SQL-like interface. Can you talk about how

161
00:16:17,280 --> 00:16:26,000
that experience of building these models using a SQL type of interface different from

162
00:16:27,040 --> 00:16:32,480
traditional approaches? You mentioned Scikit-learn, like how were they different? And I guess

163
00:16:32,480 --> 00:16:39,840
I'm particularly curious about the different skill level involved, but also any differences in

164
00:16:39,840 --> 00:16:45,200
the way you need to manage the models or the way you productize them, those kinds of things.

165
00:16:46,160 --> 00:16:51,200
Yeah, that's a good question. So one thing is getting started with BQML is really fast.

166
00:16:52,080 --> 00:16:59,200
I actually built the first prototype within a day. I got a very basic churn model up and running.

167
00:16:59,200 --> 00:17:10,080
I've done this before in my previous job using Scikit-learn and the difficulty really was

168
00:17:10,080 --> 00:17:17,120
getting aggregating data from different sources. Like in my previous job, getting data from

169
00:17:17,120 --> 00:17:25,680
like an Oracle database and, you know, a Hadoop database. So seemed like that took most of the time

170
00:17:25,680 --> 00:17:31,840
getting data from different data sources. Typically, the machine learning technology piece is the

171
00:17:31,840 --> 00:17:40,720
easier part. And the more time it's spent on getting the data and then figuring out the right

172
00:17:40,720 --> 00:17:52,960
features to use in our machine learning model. And so using BigQuery ML, getting used to the

173
00:17:52,960 --> 00:18:00,160
SQL syntax is, again, anybody with no SQL can be trained to do it within, I think, half a day.

174
00:18:00,720 --> 00:18:06,880
I think the tricky part is, like, for someone new is to learn some of the machine learning concepts.

175
00:18:06,880 --> 00:18:13,840
For example, if you train a machine learning model, how do you measure that it's, you've succeeded,

176
00:18:13,840 --> 00:18:24,080
like basic concept like precision recall and accuracy and AUC area and the curve. So those are more

177
00:18:25,280 --> 00:18:33,200
where more time is spent, like figuring out the features and then doing measurement of the result.

178
00:18:34,080 --> 00:18:42,000
And I feel like that doesn't differ from versus using Scikit-learn or BigQuery ML. The feature

179
00:18:42,000 --> 00:18:51,920
engineering and the measurement. But getting used to BigQuery ML is very quick. You don't have

180
00:18:51,920 --> 00:19:01,520
to use any language. And then the cycle I was talking about that data export, train model, data

181
00:19:01,520 --> 00:19:08,960
import, that cycle gets really reduced. So the number of experiments that you can do using BigQuery

182
00:19:08,960 --> 00:19:13,520
ML, you can do a lot more experiments with this technology.

183
00:19:14,320 --> 00:19:20,720
And is part of that because presumably they're scaling out the training behind the scenes and

184
00:19:20,720 --> 00:19:26,720
it's just a lot faster or are there other aspects to reducing that cycle time?

185
00:19:27,280 --> 00:19:34,880
Yeah, A is that it's a lot faster because of the BigQuery technology. And second is the

186
00:19:34,880 --> 00:19:41,520
what I mentioned before exporting data for machine learning model outside of the system,

187
00:19:41,520 --> 00:19:47,520
training it and fetching that data back in. So that gets significantly reduced.

188
00:19:48,400 --> 00:19:52,720
And the third one is in our case, since our data was all sitting in one place,

189
00:19:54,480 --> 00:20:00,400
we didn't have to incur this ETL cost of getting data from different sources. Everything was

190
00:20:00,400 --> 00:20:07,520
sitting in BigQuery. What are some of the other types of machine learning projects that you've

191
00:20:07,520 --> 00:20:14,960
done at Hearst? Yeah. So another one was application of natural language processing.

192
00:20:15,840 --> 00:20:23,360
There's a case study published back in November. I think if you just Google Hearst and Google

193
00:20:23,360 --> 00:20:30,800
that probably the first link. So we've applied Google's natural language processing to all our content.

194
00:20:32,080 --> 00:20:38,320
So each hour tag basically we're using two features of natural language. One is classification of

195
00:20:38,320 --> 00:20:45,920
content. Putting this into broad categories like if content is about food or wine or is it about

196
00:20:45,920 --> 00:20:54,080
real estate? And then also a more detailed version of it that it tags entities and the content.

197
00:20:55,280 --> 00:21:02,080
And the entities could be proper nouns or common nouns with their metadata,

198
00:21:02,080 --> 00:21:10,400
with their how much salient they are to that article and also their Wikipedia link for their

199
00:21:10,400 --> 00:21:19,920
popular entities. So we did that for all our content from all our websites and that stored in

200
00:21:19,920 --> 00:21:28,480
our CMS as well as also replicated into BigQuery for their analysis. So some of the use cases

201
00:21:29,680 --> 00:21:38,480
on this NLP data is so we built out BI business intelligent reports. For example, if you want to

202
00:21:38,480 --> 00:21:48,880
see how is for example a particular personality trending over time. So using Google NLP data and

203
00:21:48,880 --> 00:21:56,800
Google Analytics data in our content data in BigQuery we can build out reports. And also if you want to

204
00:21:58,320 --> 00:22:06,720
see which content categories get more traffic versus the content that we publish. Like which

205
00:22:06,720 --> 00:22:15,280
categories should we be focusing on for publishers? So we build all sorts of reports using these

206
00:22:15,280 --> 00:22:23,520
three data sets. So this is one of the use cases for Google NLP. The other one is which is more

207
00:22:23,520 --> 00:22:31,520
interesting is that we also pass like when we render our ads on thing we've integrated that with

208
00:22:31,520 --> 00:22:38,560
double click for publishers. So whenever an ad is rendered we also pass the key value pair

209
00:22:40,240 --> 00:22:46,240
of which category that ad belongs to. The category of the content that ads being displayed.

210
00:22:46,240 --> 00:22:54,720
So we built over a month this builds a database where we can say show me all the ads that are

211
00:22:54,720 --> 00:23:03,920
displayed on for example Olympics content or ads on our food contents. So if there's a new advertiser

212
00:23:03,920 --> 00:23:10,640
that comes on board and he says that I want to run a campaign on let's say basketball or

213
00:23:10,640 --> 00:23:19,360
Olympics content. We build this capability using our tag tagging technology. So they can just

214
00:23:19,360 --> 00:23:26,080
specify a criteria and double click for publisher said specify for this campaign advertise this

215
00:23:26,080 --> 00:23:34,960
campaign on all the content which is related to let's say tennis or basketball. So that's another

216
00:23:34,960 --> 00:23:40,080
use case for natural language processing. I know you've done some work both with Google's

217
00:23:40,080 --> 00:23:49,360
AutoML NLP that allows you to kind of fine tune some of their models but the classification was

218
00:23:49,360 --> 00:23:56,400
there off the shelf NLP service that you that one if I'm not mistaken you're not able to train

219
00:23:56,400 --> 00:24:04,320
that using your own data is that right? Yeah it's a good question. So the AutoML for NLP which is

220
00:24:04,320 --> 00:24:12,560
another tool released recently it works on top of the classification. So they give a

221
00:24:13,600 --> 00:24:20,240
a taxonomy of about 700 categories by default but let's say if somebody wants to train

222
00:24:20,800 --> 00:24:27,280
their own category like for some unique or novel content they can use AutoML for

223
00:24:27,280 --> 00:24:36,640
a natural language to train their own custom categories. So and that works on top of their

224
00:24:37,280 --> 00:24:44,400
they existing categories. So if you make a web service call to this AutoML it not only returns

225
00:24:44,400 --> 00:24:51,040
their default categories but also the trained version as well. And so you found that for the

226
00:24:51,040 --> 00:24:59,920
types of articles that you were initially trying to categorize the were these 700 built-in

227
00:24:59,920 --> 00:25:08,240
categories were they sufficient were they was it you know what was the experience of trying to

228
00:25:08,240 --> 00:25:17,040
map your business to this pre-canned AI service? So most of most of the time it works very well.

229
00:25:17,040 --> 00:25:25,680
So some of the categories like the like for most of our use cases those categories were fine.

230
00:25:26,400 --> 00:25:32,720
There's a few categories which don't work well especially their sensitive content so they

231
00:25:32,720 --> 00:25:40,880
don't really split out. So any article about crime or you know some violence everything gets

232
00:25:40,880 --> 00:25:48,160
categorized into sensitive content. We would have ideally liked that to be split down into more

233
00:25:48,160 --> 00:25:55,360
granular but Google thinks that they're like some restrictions they just don't want to split that

234
00:25:55,360 --> 00:26:04,080
out. But one of the categories that we wanted that I did a POC was to detect evergreen content

235
00:26:04,080 --> 00:26:11,360
because we had labeled about several thousand articles with evergreen label. So whatever green

236
00:26:11,360 --> 00:26:19,760
means is content that has a life shelf of more than a few days or a few weeks. For example an article

237
00:26:19,760 --> 00:26:29,520
which is a review of a museum or an article about some real estate. So those articles have a longer

238
00:26:29,520 --> 00:26:36,320
shelf life and it's very important in the newspaper if you can detect that type of content because

239
00:26:36,320 --> 00:26:44,000
that kind of content can be used in recommendations. Even an article written two years ago can be

240
00:26:44,000 --> 00:26:52,640
reused which otherwise would just be sitting there and nobody reads that content. So I built using

241
00:26:52,640 --> 00:27:00,960
AutoML for text. I built a classifier which basically detects evergreen versus non-evergreen content

242
00:27:01,680 --> 00:27:09,280
and our editorial they helped us label this content. We asked all the different markets

243
00:27:10,000 --> 00:27:16,960
to take their content and label them with evergreen content. Initially I did a very quick prototype

244
00:27:16,960 --> 00:27:25,680
using TensorFlow and then I had a hunch that their Google is working on this AutoML feature for

245
00:27:25,680 --> 00:27:35,520
NLP and then once this feature came out doing a POC was really quick. Within a day I was able to train

246
00:27:36,800 --> 00:27:42,640
an AutoML model that can differentiate evergreen content. So really the hard part in this is to

247
00:27:42,640 --> 00:27:51,280
get the labeled data. And the interesting thing is once this model is trained I even try to

248
00:27:51,280 --> 00:27:58,160
on CNN and New York Times content it was able to differentiate evergreen and non-evergreen content.

249
00:27:59,840 --> 00:28:06,000
And right now we're incorporating that into one of our recommendation systems.

250
00:28:06,000 --> 00:28:12,560
Yeah one of the things that I noticed a couple of times in our conversation you're the senior director

251
00:28:12,560 --> 00:28:19,200
of data engineering in ML. But it sounds like on a couple of these POCs you just kind of

252
00:28:20,000 --> 00:28:24,640
played around and put some stuff together you make it sound really easy. It can be easy to

253
00:28:24,640 --> 00:28:29,680
experiment but then if you actually want to use this in a way that the business is going to depend

254
00:28:29,680 --> 00:28:36,320
on it there's at least traditional a lot more that needs to go into it in terms of engineering.

255
00:28:37,200 --> 00:28:42,880
I guess there's maybe several questions in here but you know part of it is like does the cloud

256
00:28:42,880 --> 00:28:49,360
change that dynamic in your perspective or you know are these projects that you built and then

257
00:28:49,360 --> 00:28:56,160
kind of threw them over the wall to you know some team that then had to maintain these projects.

258
00:28:56,160 --> 00:29:02,320
Are you a question specifically about auto ML or just about recommendations and

259
00:29:03,440 --> 00:29:10,400
the other stuff I've been talking about? Specifically the I think with term prediction and the auto

260
00:29:10,400 --> 00:29:16,400
ML the impression I got was that it almost sounded like you got bored one weekend and kind of

261
00:29:16,400 --> 00:29:20,320
work on these and kind of you know you came up with these models in a you know a day or so.

262
00:29:20,320 --> 00:29:28,240
Yeah so that they weren't done in a day they were sort of like the turn model we started

263
00:29:29,280 --> 00:29:35,680
when BQMO was an alpha state only available for certain customers but I think it was about

264
00:29:35,680 --> 00:29:44,880
May we start building it so and it took a couple of months like refining and fine tuning it

265
00:29:44,880 --> 00:29:53,120
and so and we're now we're working on another market building out doing this prediction for Houston.

266
00:29:55,120 --> 00:30:01,280
Auto ML it was in phase that we started off like last year we said we need to collect the data

267
00:30:01,280 --> 00:30:09,040
since we want to be able to do saying so one mini project was to figure out how to get this data

268
00:30:09,040 --> 00:30:16,960
and get it labeled and and then once we had it label it was just sitting there for a while till

269
00:30:18,240 --> 00:30:24,320
I did some prototyping work for we're using TensorFlow and then when auto ML came out

270
00:30:26,480 --> 00:30:31,200
it was since we had it did already had done the hard work on labeling that data it was

271
00:30:31,200 --> 00:30:40,080
very something very quick to do so I guess one piece of the project wasn't done like in in a weekend

272
00:30:40,080 --> 00:30:45,520
there were different phases and happen in different times and the other thing is in general the

273
00:30:45,520 --> 00:30:53,920
the newer features in cloud especially the the higher level API that's much more quicker to do

274
00:30:53,920 --> 00:31:00,320
prototyping and getting things out quicker and auto ML is a feature that's meant for

275
00:31:00,320 --> 00:31:08,240
people to do things much quicker like you don't have to learn TensorFlow to be able to do it it's

276
00:31:08,240 --> 00:31:16,640
just upload the data give it label data set and then it takes a few hours to train a model and start

277
00:31:16,640 --> 00:31:24,240
using it you've done some work on recommendation systems as well yeah so as I was talking so one of

278
00:31:24,240 --> 00:31:32,240
the use cases with the NLP data that's sitting in our BigQuery where data warehouses to build a

279
00:31:32,240 --> 00:31:39,760
recommendation system so the the idea is that if two piece of content if they have overlapping

280
00:31:40,400 --> 00:31:48,320
NLP entities they're highly likely to be related to each other so this is content to content

281
00:31:48,320 --> 00:31:54,320
recommendation and since we had this again since we had this in our BigQuery database

282
00:31:55,680 --> 00:32:04,560
we built out a recommendation system which actually works using a big SQL which takes entities

283
00:32:04,560 --> 00:32:11,920
from one set of articles and matches that with another one and corporate saliency score and

284
00:32:11,920 --> 00:32:18,720
so there's there's a whole bunch of rules that we built out in our SQL and produces a table of

285
00:32:18,720 --> 00:32:27,440
recommendations and that's fronted by a web service layer that our website San Francisco Chronicle

286
00:32:27,440 --> 00:32:33,680
is I think three websites that are using that content content recommendation and this was a

287
00:32:33,680 --> 00:32:45,680
release as part of San Francisco's user interface rewrite so a few months ago we released a revamp

288
00:32:45,680 --> 00:32:51,440
of SF Chronicle so this was project this content to content recommendation was part of

289
00:32:52,480 --> 00:32:59,280
that deployment so you've got this content in the in BigQuery and then you're using

290
00:32:59,280 --> 00:33:09,600
Google NLP to essentially you're adding fields for each of the entities that are recognized and

291
00:33:09,600 --> 00:33:19,600
then you're using BigQuery ML to generate a recommendation for from a given piece of content to

292
00:33:20,240 --> 00:33:25,760
other recommended pieces of content based on these shared entities that have been recognized by

293
00:33:25,760 --> 00:33:34,640
the NLP service yeah so and just a minor correction that this one doesn't use BQML this is a plain

294
00:33:34,640 --> 00:33:42,160
SQL running which basically is looking at the number of overlaps of entities between two contents

295
00:33:42,160 --> 00:33:48,000
got it got it so it's basically scoring of how like based on the overlap how

296
00:33:49,040 --> 00:33:54,800
relevant one content is to another one okay and then you said you're frontending that with

297
00:33:54,800 --> 00:34:02,560
a website are you serving the data directly up from BigQuery or do you push it out to some

298
00:34:03,520 --> 00:34:10,880
cache or database yeah so yeah so we'd push it out this computation is done on a periodic

299
00:34:10,880 --> 00:34:18,880
basis about every 15 minutes and that's about the frequency that we're getting your content so we

300
00:34:18,880 --> 00:34:26,320
build out a table in a postgres database and then that web service layer is using a memcache as

301
00:34:26,320 --> 00:34:33,040
well as this postgres database to serve these out and you've also done some video content recommendations

302
00:34:33,600 --> 00:34:41,760
yeah so it's so another project we've done is we've taken our video converted that to sound

303
00:34:41,760 --> 00:34:50,160
and then extracted text from that sound and then applied Google's NLP to extract entities from

304
00:34:50,160 --> 00:34:58,400
the sound so so our content has the NLP tags and our video using voice transcription

305
00:35:00,240 --> 00:35:06,960
and using that text to also extract entities we can recommend video to content based on this

306
00:35:06,960 --> 00:35:17,120
technology and so to transcribe the audio did you use the the Google speech to text API for that

307
00:35:17,120 --> 00:35:24,560
yeah yeah so Google has a Google sound API which given any sound it can convert that to text

308
00:35:25,200 --> 00:35:33,360
and what was your experience getting reliable results from that yeah so our like full objective

309
00:35:33,360 --> 00:35:41,600
was to also have captioning on the videos so we did most of the time it works very well the

310
00:35:42,800 --> 00:35:51,360
sound transcription but in certain cases it doesn't like it if like it there was some few

311
00:35:52,720 --> 00:35:59,120
cases when it didn't work very well so we haven't used this for captioning but we took this text

312
00:35:59,120 --> 00:36:07,840
and it seems to work well if we apply NLP on it if we just want to extract categories and tags

313
00:36:07,840 --> 00:36:14,800
from it for that purpose it works well and those tags are used for it eventually being able to

314
00:36:14,800 --> 00:36:24,880
recommend a contextual content along with the videos any final thoughts or words of wisdom that

315
00:36:24,880 --> 00:36:32,720
you shared as you were wrapping up your presentation on these use cases yeah so some of the advice I

316
00:36:32,720 --> 00:36:41,120
gave was that like some of the things that we want to do in the future is to use applied deep learning

317
00:36:41,120 --> 00:36:50,800
for recommendations there's a lot of research done in the past couple of years for application

318
00:36:50,800 --> 00:37:00,320
deep learning and recommendation we're actually doing some research and we want to launch very soon

319
00:37:00,320 --> 00:37:07,840
with a recommendation system that applies user deep learning so I'd recommend that we should use

320
00:37:07,840 --> 00:37:16,800
like everybody should look into deep learning and TensorFlow so it's really everything you don't

321
00:37:16,800 --> 00:37:26,080
have to build from scratch if a problem is already solved by higher level APIs such as the NLP

322
00:37:26,080 --> 00:37:35,120
image video APIs or BQML and AutoML we should be using that it's really a trade-off do you want to

323
00:37:35,120 --> 00:37:40,960
hire data scientists to rebuild this thing or do you want to just pay that for that service and apply

324
00:37:40,960 --> 00:37:48,800
it and only in cases where the problem is novel or the data set is novel is when you want to build your

325
00:37:48,800 --> 00:37:56,480
in-house machine learning model for example we want to be able to do recommendations using deep

326
00:37:56,480 --> 00:38:03,200
learning so this is something that we're building in-house and other things that we're exploring

327
00:38:03,200 --> 00:38:12,400
as more use cases for NLP we identify that we can use this with our newsletters to be able to

328
00:38:12,400 --> 00:38:20,720
generate newsletters automatically that's something that we're looking at too and also more

329
00:38:20,720 --> 00:38:29,040
uses for BigQuery ML just like churn we can do the converses do propensity modeling how life

330
00:38:29,040 --> 00:38:36,640
be heard what's the likelihood of a user to subscribe to our whenever websites so this is

331
00:38:36,640 --> 00:38:44,560
overall our future plans awesome well Naveed thanks so much for taking the time to share what you've

332
00:38:44,560 --> 00:38:49,680
been up to with us it's been really interesting and I appreciate it okay thank you very much

333
00:38:49,680 --> 00:38:59,200
all right everyone that's our show for today for more information on Naveed or any of the topics

334
00:38:59,200 --> 00:39:06,560
covered in this episode visit twimmelai.com slash talks slash 182 if you're a fan of the podcast

335
00:39:06,560 --> 00:39:11,600
we'd like to encourage you to visit your Apple or Google podcast app and leave us a five-star

336
00:39:11,600 --> 00:39:16,560
rating and review your reviews help inspire us to create more and better content and they help new

337
00:39:16,560 --> 00:39:25,120
listeners find the show as always thanks so much for listening and catch you next time

