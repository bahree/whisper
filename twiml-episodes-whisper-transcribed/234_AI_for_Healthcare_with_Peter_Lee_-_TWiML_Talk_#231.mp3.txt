Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, today we're excited to continue the AI for the benefit
of society series that we've partnered with Microsoft to bring you.
In this episode, we're joined by Peter Lee, corporate vice president and Microsoft
research responsible for the company's healthcare initiatives.
Peter and I met a few months ago at the Microsoft Ignite conference where he gave me some really
interesting takes on AI development in China.
We referenced those in the conversation and you can find more on that topic in the show
notes.
This conversation those centers on three impact areas that Peter sees for AI and healthcare,
namely diagnostics and therapeutics, tools and the future of precision medicine.
We dig into some examples in each area and Peter details the realities of applying machine
learning and some of the impediments to rapid scale.
Before diving in, I'd like to thank Microsoft for their support of the show and their sponsorship
of this series.
Microsoft is committed to ensuring the responsible development and use of AI and is empowering
people around the world with this intelligent technology to help solve previously intractable
societal challenges, spanning sustainability, accessibility and humanitarian action.
Learn more about their plan at Microsoft.ai.
All right, everyone, I am on the line with Peter Lee.
Peter is a corporate vice president at Microsoft, responsible for the company's healthcare
initiatives.
Peter, it is so great to speak with you again.
Welcome to this week in machine learning and AI.
Sam, it's great to be here.
So Peter, you gave a really interesting presentation to a group that I was at at Ignite about
what some of Microsoft was working on at Microsoft Research, as well as a really interesting
take on AI development in China.
They kind of peak my interest and we ended up sitting down to chat about that in a little
bit more detail.
And while we, I did cover that from my blog and newsletter and I'll be linking to it
in the show notes, we won't be diving into that today.
It was a really, really interesting take that I reflect on often.
And I think it's an interesting setup for diving into your background because you do
have a very interesting background and interesting perspective and set up responsibilities at Microsoft.
So on that note, can you share with our audience a little bit about your background?
Sure, Sam.
I'd love to do that.
I agree.
It is a little bit unusual, although I think the common thread throughout has been about
research and trying to bring research into the real world.
And so I am a computer scientist by training.
I was a professor of computer science at Carnegie Mellon for a long time, actually, for
24 years.
And at the end of my time there was the head of the computer science department.
And then I went to Washington, D.C. to serve at an agency called DARPA, which is the
Defense Advanced Research Projects Agency.
It's kind of the storied research agency that built the Saturn V booster technology, invented
the ARPANET, which became the Internet, developed robotics, lots and lots of other things.
And I learned a lot about bringing research to life there.
And then after a couple of years there, I was recruited to Microsoft and joined Microsoft
Research, started managing the mothership lab in Redmond in the headquarters in Redmond
and then a little bit later all of the U.S. research labs and then ultimately all of Microsoft's
13 labs around the world.
And right about that time, Steve Balmer announced his retirement, Satya Nadella, took over
as a CEO, Harry Schum, took over all of AI and research at Microsoft and became my boss.
And they asked me to start a new type of research organization that internally is called Next,
which stands for New Experiences and Technologies.
And we've been sort of trying to grow and incubate new research-powered businesses ever since,
and most recently in healthcare.
I think when I think about AI and healthcare, there's certainly a ton of ground to cover
there.
But I think one of the areas that gets a lot of attention of late is all the progress
that's being made around applying neural nets, CNNs in particular to imagery.
I want to read from your perspective, how do you tend to think about AI applied to
the healthcare space and where the big opportunities are?
Yeah, when I think about AI and healthcare, I'm really optimistic about the future.
Not that there aren't huge difficult problems and sometimes things always seem to grow slower
than you expect.
It's a little bit like watching grass grow.
It does grow and things do happen, but sometimes it's hard to see it.
But over the last 15 years, the thing that I think is underappreciated is the entire healthcare
industry has gone digital.
It was only 15 years ago that, for example, in the United States, less than 10% of physicians
were recording your health history in a digital electronic health record.
And now we're up over 95%.
And that's just an amazing transformation over 15 years.
And it's not like we don't still have problems, the data is siloed.
It's not in standard formats, there's all sorts of problems.
But the fact that it's gone digital just opens up huge, huge amounts of potential.
And so I kind of look at the potential for AI in three areas.
One is sort of the thing that you pointed at, which AI technologies that actually lead
to better diagnostics and therapeutics.
Things that actually advance medical science and medical technology.
A second area for AI is in the area of tools.
Tools that actually make doctors better at what they do, make them happier while they're
doing it, and also improve the experience for you and me as patients or consumers of healthcare.
And then the third area is in this wonderful future of precision medicine.
That's taking new sources of information, digital information, your genome, your proteome,
your immunome, data from your fitness wearables and so on, integrating all of that together
to give you a complete picture of what's going on with your body.
So those are sort of three broad areas, and they're all incredibly exciting right now.
When you think about the first two of those categories, better diagnostics and therapeutics
and tools, how do you distinguish them?
It strikes me that giving doctors a better way to analyze medical imagery, for example,
or to use that example again is a tool that they can use.
But when you say tools, what do you specifically mean?
Yeah, you're absolutely right.
There's an overlap, it's not like the boundaries between these things are all that hardened.
But if you think about one problem that doctors have today is, by some estimates in the
United States, doctors are spending 40 to 50% of their work days entering documentation,
entering notes that record what happened in their encounters with patients.
And that's sometimes called an encounter note.
That documentation is actually required now by various rules and regulations.
It's an incredible source of burnout, and in fact, I'm guessing you've had this experience.
Most people have you go to your doctor, I go to mine, and I like her very much.
But while I'm being examined by her, she's not looking at me, she's actually sitting
at a PC, typing in the encounter notes.
And the reason she's doing that is if she doesn't do it while she's examining me, she'll
have to do it for a couple of hours, maybe in the evening, taking time away from her
own family.
And that burden is credited or blamed for a rise in physician burnout.
While AI technologies today are rapidly approaching the point where an ambient intelligence
can just observe and listen to a doctor patient encounter and automate the vast majority of
the burden of that required clinical note-taking.
And so that's an example of a kind of technology that could, in a really material way, just
improve the lives and the work they said is faction of doctors and nurses.
And that's, I put that in a different category.
And technologies that actually give you more precise diagnosis of what's ailing you,
or ability to target therapies that might actually attack the very specific genetic makeup,
let's say, of the cancer that's inhabiting your body right now.
Got it.
Got it.
So maybe let's take each of these categories in turn.
I'd love to get a perspective from you on where you see the important developments coming
from, from a research perspective, and where you see the opportunities and where you see
things heading in each.
Sure.
Well, why don't we start with your example of imaging because computer vision based
on deep neural nets has just been progressing at this stunning rate.
And it seems like every week you see another company, another startup, or another university
research group showing off their latest advances in using deep neural net based computer vision
technologies to do various kinds of medical image diagnosis or segmentation.
And here at Microsoft, we've been working pretty hard on those as well.
We have this wonderful program based primarily in India that's been trained on the health records
and eye images of over 200,000 patients.
And that idea of taking all that data, you get the signal of which of those patients
have, let's say, suffered from, say, diabetic retinopathy or a progression of refractive
error leading to blindness.
And from that signal in the electronic health record coupled with the images, we are able
to train a computer vision based thing to make a prediction about whether a child whose
eye image has been taken is in danger of losing eyesight.
And that is in deployment right now in India.
And of course, for other parts of the world, like the United States and Europe, which are
more regulated, these things are in various states of clinical validation, as they can be
more broadly deployed.
Another example is a project that we have called Inner Eye that is trying to just reduce
the incredible kind of boring and mundane problem of just pixel-by-pixel outlining the parts
of your body that are tumor and should be attacked with a radiation beam, as opposed
to the healthy tissue.
And that problem of radiation therapy planning has to be done really perfectly, which is
why it's this sort of pixel-by-pixel process.
But there is maybe five or 15 minutes of real black magic that's drawing on all of the
intuition and experience and wisdom of a radiologist, and then two to three hours of complete
drudgery.
And much of that complete drudgery can just be eliminated with modern computer vision
technologies.
And so these things are really developing so rapidly and coming online.
And they tend not to replace completely what doctors and radiologists can do, because
there is always some judgment and intuition involved in these things.
But when done right, they can integrate into the workflow to really enable to kind of
liberate clinicians from a lot of drudgery and to reduce mistakes.
And I think one other thing that's sometimes not fully appreciated is you also, when you
get these tools, you can take these measurements over and over and over again.
When they become cheap, you can take them every day if necessary, which allows you to
track the progression of a disease or its treatment over time much more precisely.
And so these sorts of applications, I think, in medical imaging, I think are really promising.
One thing I, it's a hobby horse of mine before I pause is, you know, in 2015 here in Microsoft
research, we invented something called deep residual networks, which are now commonly
called ResNet.
And ResNet has become part of an industry standard and research standard in computer vision
using deep neural nets.
We ourselves have refrained from using ResNet for doing things like imaging of 3D images
for the purposes of radiation therapy planning, and there are various technical reasons for
that.
And so sometimes we have a mixture of being proud seeing the rest of the world use our
invention for interesting medical imaging, but we also sometimes get worried that people
don't quite understand the failure modes in these things.
But still, the progress has just been spectacular.
I mean, that's kind of an interesting prompt.
Maybe let's take a moment to explore the failure modes and why don't you, it sounds like
you don't advise folks to apply ResNet to the types of images that we tend to see in medical
imaging.
What's that about?
Yeah, it's not advising or warning people against it.
So if you think about, let's say, take the problem of radiation therapy planning, it's
a 3D problem.
You have a tumor that is a 3D mass in your body and you're trying to come up with a plan
for that radiation beam to attack ideally as much of that tumor while preserving as much
healthy tissue as possible.
And of course, your picture into that 3D tumor is as a series of two-dimensional slices,
at least with current medical imaging.
And so one very basic question is, as you examine slice by slice, that tumor with respect
to the healthy tissue, is each slice being properly and logically registered with the
next one.
And a simple or naive application of a convolutional neural network like a ResNet doesn't automatically
do that.
The other problem is it's unclear to what extent a bad training sample or set of training
samples will do to one of these deep neural nets.
And in fact, just in the last few weeks and months, there have been more and more interesting
academic research studies showing some interesting failure modes from a surprisingly small number
of bad training samples.
And so I think that these things are changing all the time, our algorithms and our algorithmic
understanding are improving all the time.
But at least within our research groups, we've taken pains to understand that this application
of computer vision isn't like others.
It's more in the realm of driverless cars where safety is of paramount concern and we
just have to have absolute certainty that we understand the possible failure modes of
these things.
Sometimes with just an authorship application of ResNet or any similar kind of deep neural
net algorithm, we and now more and more other researchers at universities are finding that
we don't yet fully understand the failure modes.
In some ways, there's an opportunity beyond kind of naive application of an algorithm
that performs very well on ImageNet.
Also for, so today you can get data sets that include kind of these 2D representations
of what are fundamentally 3D applications or 3D images and kind of apply the regular
2D algorithms to them and find interesting things.
But you're saying that there, A, we can do better and B, we may not even be doing the
right things in many cases because of these safety issues.
I'm wondering do you, on the first of those two points, the doing better, is there either
a standard approach that's better than ResNet for these 3D images that you've developed
at Microsoft or have seen otherwise or where are we in terms of taking advantage of the
3D nature of medical images and deep learning?
Yeah, it's a good question, you know, for our inner-eye project, which is really run by
a great set of researchers, based mostly in our Cambridge UK research lab and led by Antonio
Kriminesi and he's really one of the pre-eminent authorities in computer vision.
In fact, he, he led an effort some years ago to work out the 3D computer vision for
Connect and so he's really specialized in 3D.
And so the inner-eye project, which is really for us an effort to really understand completely
the workflow of radiation therapy planning, that system actually doesn't use residual
network.
What it does is it uses a kind of an architecture of layered, what a code decision for us.
And that gives not only some benefits in terms of more compact representations of the machine
learned models and therefore some performance improvements, but it allows us to kind of
capture a kind of logical registration of the images as they go slice by slice.
In other words, it's you're inferring not just the segmentation of each 2D image slice,
but you're actually trying to infer the voxel, the 3D voxel volume of these, of the tumor
that you're trying to attack.
And so, and then on top of that, there's a process involved when you're dealing with
medical technologies.
You don't just put it out there and start applying it on people.
You get it peer reviewed, you get it peer reviewed and in this case in computer science journals
and in medical journals.
And you go through a clinical validation.
And if you're in the United States, for example, through an FDA approval process.
And so for us, as we're learning about what does our cloud, what do our AI services, what
do our tools have to be in order to support this future of AI powered healthcare?
In their eyes, an example of us going end to end to try to build it all out and to understand
all of those components and to understand what has to be done to really do it right.
And it's been a great learning experience.
We're now in the process, not only of working with various companies who might want to integrate
this interi technology into their medical devices, but we're starting to now pull apart
the kind of bricks and mortar that we used in the technical architecture for interi
in order to expose those as APIs for other developers to use.
And so our intent is not to get into the radiation therapy business.
Our intent is not to get into radiology, but we do want our cloud and our AI services
and our algorithms to be a great place for any other company or any other startup or
innovator who wants to do that and ideally do it on our cloud using our tools.
So an interesting point in there, you mentioned that the decision for us that you've developed
to address this problem, you know, I guess we often think of there being this trade
off between factors like explainability or, you know, safety as you related that second
point and performance, which we think of as the neuron that is delivering the kind of
the ultimate in performance in many cases.
But in this case, your this decision for us algorithm is outperforming your, at least
your classic 2D resnets.
And I'm imagining also providing benefits in terms of explainability slash safety, is
that correct?
Well, I we feel very strongly that it provides benefits in terms of safety, explainability
is really another very interesting question and the problem.
And so there's a potential for greater explainability, you know, one of the lessons that
we learned when we were working on AI for sales intelligence.
And so we had really developed a tremendous amount of AI that would ingest large amounts
of data from the world as well as from customer relationship management databases, emails
and so on for our sales teams.
And use that through various AI algorithms to do things like synthesize new offers to
specific customers or to surface new prospective customers or to suggest new discount pricing
for specific customers.
And one of the things we learned is that, you know, no self-respecting sales executive
is going to offer 20% discount to a customer just because his algorithm says so.
You know, and, you know, typically doctors are probably similar.
That's right.
And so in that situation, we also moved away from, in that specific case, moved away from
the pure deep neural net architecture to having kind of layered architecture of Bayesian
graphical models.
And the reason for that was so that we could synthesize an explanation in plain English
of not only, you know, offer a 20% discount, but why?
And as we get into away from more kind of point solutions that are kind of machine learning
or AI powered to more of that digital assistant that is the companion to a clinician and gives
that clinician a second opinion or advice on the first opinion, those sorts of explanations
undoubtedly are going to become important, especially at the beginning when we're trying
to establish trust in these things.
And so, you know, as we've been experimenting even with the kind of ambient intelligence
to just listen in on a doctor patient encounter and try to automate a note, one thing we found
is that doctors will look at the synthesized note and not trust everything in it, because
they don't quite yet have the understanding of, you know, why did the note come out
this way?
And so it became important to provide tools so that when you, you know, say, click on a
specific entry in the note that it could be mapped back to a running transcript and
to the right spot in the running transcript that was recorded.
And so these sorts of things, I think, are part of that maybe the human computer interaction
or the human AI interaction that we're having to think about pretty hard as we try to integrate
these things into clinical workflow.
Before we move on beyond diagnostics and therapeutics, all of the examples that you gave fell
into the domain of computer vision, are there interesting things happening in diagnostics
beyond the kind of onslaught of these new computer vision based approaches?
Yeah, I think actually some of the most interesting things are not in computer vision.
And this maybe crosses over into the precision medicine thing.
One of the projects I'm so excited about is something that we're doing jointly with
Seattle biotech startup adaptive biotechnologies.
And so the setup is this, you know, if you take a small blood sample from your body in
that sample, in that one mill sample, you'll end up capturing on the order of one million
T cells.
T cells are one of the primary agents in your adaptive immune system.
And about two and a half years ago, there was a major scientific breakthrough that got
published that showed that the receptor, there's a receptor on the surface of your T cells.
And in that receptor, there's a small snippet of DNA.
And there was strong evidence two and a half years ago that that snippet of DNA completely
determines what pathogen or infectious disease agent or cancer that T cell has been programmed
to seek out and destroy.
And that paper was very interesting because it used a simple linear regression in order
to identify from a read of that little snippet of DNA on your T cell receptor, whether you
had cytomegalovirus or not.
And so it was really just an impressive paper and then just very recent.
Well, the thing that was interesting about adaptive biotechnologies is adaptive biotechnologies
was in the business of giving you a printout of that specific snippet of DNA in all the
T cell receptors in the blood sample.
So they had a business model that would help some cancer centers titrate the amount of
a specific chemotherapy you were getting based on a reading of the DNA.
And so that raised the question, would it be possible to take that printout of those
T cell receptor DNA sequences?
And in essence, think of that as a language and translate it into the language of antigens.
And then if you can do that, can you take those antigens and do a kind of topic identification
problem to figure out what infectious diseases, what cancers, and what autoimmune disorders,
your body is currently coping with right now?
And so it turned into this very interesting new business opportunity for adaptive biotechnologies
that if machine learning could be used to solve those two problems, then they would have
a technology that would be very similar to a universal diagnostic, a simple blood test
powered by machine learning that could do early diagnosis of any infectious disease, any
cancer, and any autoimmune disorder.
And so Microsoft found that interesting enough that we actually took an investment position
in adaptive biotechnologies and agreed to work with them on the machine learning.
And adaptive for their part agreed to build a bigger production pipeline in order to generate
training data to power the machine learning that we're developing at Microsoft.
What has transpired since then has been an amazing amount of progress where we've added
tremendous amount of sophistication actually using deep neural nets and started to feed
it with billions of points of training data.
And in fact, this year the production facility adaptive will be able to generate up to a trillion
points of training data.
And we're now targeting five specific diseases, a varying cancer, pancreatic cancer, type
one diabetes, celiac disease, and Lyme disease.
And so that's two cancers, two autoimmune disorders, and one infectious disease with the
same machine learning pipeline.
And it's still an experiment, but it kind of shows you the potential power of these advances
in immunology, in genomics, and AI all being bound together to give the possibility.
We know the science now is valid.
And if we can now build a technology that ties those things together, we get the potential
for a universal diagnostic, but as close a thing that we could imagine getting to the
Star Trek tricorder as anything.
Yeah, that was the thing that popped immediately to mind for me, the tricorder.
But that example I think captures for me really plainly both the promise of applying machine
learning and AI to this healthcare domain, but also maybe a little bit of the frustration
and thinking through collecting a trillion samples and you've got this pipeline, why does
it take so long?
And there's certainly regulatory and political types of reasons that maybe we'll get into.
But I'm wondering if you can elaborate on with that much training data and the science
in place and a pipeline in place.
What are the realities of applying machine learning in this type of context that impede
kind of rapid scale?
Like why just five diseases and not 25, for example?
Yeah, that's such a great question.
And it's human biology is just so complicated.
You know, let's say there are three ways maybe to take a cut at that.
If we just look at the very basic science, let's just consider the human genome.
Something that geneticists at several universities have taught me, which was really eye opening,
is if you look at the human genome and then look at all the possible variants, the number
of variants and the human genome that would still be considered, you know, homo sapiens
is just astronomically large.
And yet the total number of people on the planet is a relative that number is really tiny,
you know, only what's seven and a half billion people.
And in fact, if we had somehow DNA samples from every human that has ever existed, I think
most estimates say there are fewer than a hundred and six billion people that have ever existed
since Adam and Eve.
And so if we are using modern machine learning, which is basically looking at statistical patterns
and correlations, we have an immediate problem for a lot of basic problems in genomics, because
we basically don't have a source of enough training data, the complexity of human beings,
the complexity of cancer, the complexity, the genetic complexity of disease is just vastly
larger than the number of people that have ever existed.
Meaning relative to the possible combinations of genes, every human is, you know, I guess
it shouldn't be surprising that every human is unique, but even given, I mean, it's a
little counterintuitive.
You think there's only like these four letters that were thrown together to make all this
stuff out, right?
Yeah, it's, and so, you know, and so what that means is that yes, we will, and we have
been making, we meaning the scientific community and the technology community have been making
stunning advances and making really meaningful improvements for neonatal intensive care for
cancer treatments for immunology, but fundamentally, scientifically, we still need something beyond
just machine learning, and we really need something that gets into the basic biology.
And so that's kind of one reason why this is hard, and another reason is these are just
big problems.
In the project with adaptive biotechnologies, there are between 10 to the 15th and 10
to the 16th different T-cell receptors that your body can produce, and on the order of
maybe 10 to the 7th known antigens, and, and so imagine what we're trying to do is trying
to fill out a gigantic X-cell spreadsheet, you know, with 10 to the 16th columns and 10
to the 7th rows, and that's just a heck of a big table.
And so you end up needing a large monitoring data to discern enough structure, find enough
patterns in, in order to have a shot at filling in at least useful parts of that table.
The good news is, you know, everybody has T-cells, and so we can take blood samples from
anybody, from just ordinary healthy people, and then we can go to research laboratories
around the world that have stored the libraries of antigens, and start kind of correlating
those stored libraries of antigens against those what are called naive blood samples.
And that's exactly what adaptive biotechnologies is doing, in order to generate the very large
monitoring data.
So it's a little bit of a good news situation there, that we don't need to find thousands
or millions of sick people, we can generate the data from just ordinary samples, but
it's still a very large monitoring data that we need.
And then, you know, the third kind of way that I think about this is, you know, it gets
back to the safety issue.
You know, we do things a certain way, because ultimately medicine and medical science
is based on causal relationships, in other words, you know, we want to know that A causes
B, but what we typically get out of machine learning is just A is correlated with B, and
we get those inferences, and then it takes more work and more testing under controlled
circumstances to know that there's a causal relationship.
And so all three of those things kind of create, you know, challenges, it does take time,
but you know, it's, I think the good thing is, as the regulatory organizations like the
FDA have gotten smarter and smarter about what is machine learning, what is good for
or what are its limitations, that whole process has gotten, I think, faster and more efficient
over time.
And then, and then there's a second element, which is, of course, companies are in it
to make money at a minimum, even if they have purely humanitarian intentions, at a minimum,
they have to be sustained over time.
And so that means that insurance companies, Medicare, Medicaid, they have to be willing
to reimburse doctors and nurses when they actually use or prescribe these diagnostics
and therapeutics.
And so all of that takes time.
At least on the, the second of your three points in thinking about scaling, solving problems
like this, specifically training data, do you have a, a rule of thumb, a chart that says,
okay, you are one trillion training samples, we'll get us these five diseases, but we'll
need 10 trillion to get to 10 disease.
I realize that that's almost an ass and I question, and it's much more complex than that,
but do you, does it make sense at all to think of it like that?
And I think of, I guess, the impact of collecting training data and what the trajectory looks
like that over time, kind of like the way we thought of, you know, as we drive the cost
of sequencing down, the downstream effects that that'll have.
Yeah, well, when you find the answer to that question, please tell me.
You know, it's, so in my experience, I've seen this go two ways.
You know, one of the wonderful things about modern machine learning algorithms today is
that they're, they're far less susceptible to problems of overfitting.
They come very close to this, this wonderful property that the more data, the more better.
And, but it does happen that sometimes you hit a wall, you know, that you start to see
a trail off in improvement.
And, and so it, we really don't know, we're very, that kind of early results that we've
gotten with admittedly simpler diseases like CMV and then CMV is actually, you know, not
that interesting from a medical perspective, they give us tremendous hope.
And then other kind of internal more technical validations give us supreme confidence that
the basic science, the biological sciences will understand now.
But you know, once you start really attacking much more complex diseases, you know, like
any cancer, it's, it's really hard.
I would be unwilling personally to make a prediction about what will happen.
But you know, there is every reason today for optimism.
And, and I think that only unknown is, you know, whether there is a, whether we fall
off a cliff at some point and, and stop finding improvements, or, you know, if we're, you
know, if, if we're going to just get to a viable FDA approved diagnostic in the near
term, that will be constantly improving as more and more people are diagnosed.
So, so it could really go in either way.
And then, yeah, I'm, I'm really unable and actually unwilling to make a prediction about
which way will go.
But, but we are feeling pretty confident.
Incidentally, I should say, you know, last month, adaptive biotechnologies closed a deal
with Genentech for applications of this T solar receptor antigen map in the therapeutic space
in the area of cellular therapies for targeted cancer treatments.
And, and that deal has a value of over $2 billion.
So there's also some, you know, when you're dealing with kind of commercial relationships
like that, you know, there, you know, there's a tremendous amount of due diligence.
There's also, you know, these are big bets and a big pharma is, is accustomed to making
a large risky bets like this, but I think it's, it's another sign, at least leading scientists
at one of the larger pharmaceutical organizations is also increasingly confident that, that, that
we can fill out this map.
So we've talked about diagnostics, we've talked about precision medicine.
What do you see happening on the tooling side, both from the doctor's perspective as well
as the patient experience perspective?
Yeah, you know, one thing that it's a, it's a simple thing, but it's been surprising
how useful it has turned out to be.
We've been piloting chatbot technology, you know, that we call the Microsoft Healthbot.
And this has been sort of in a beta program with a few dozen healthcare organizations.
And what it does is it, we've sort of advanced our cognitive services for language processing,
natural language processing for conversational understanding, and the tooling to provide
a drag and drop interface so that ordinary people can program these chatbots, at least
for medical settings.
And then we've improved the models, the language models that they understand medical and healthcare
concepts and terms.
And so we've been surprised at the kinds of applications that that people use.
So one example is there are organizations that have made prescription bots.
So the idea is this, maybe you get a prescription from your doctor or from the hospital, you
go to the pharmacy, you get prescription filled, and then day or two later you get a message
from this intelligent chatbot just asking, you know, how's it going?
Yeah, have you had any, do you have any questions or have you had any issues with your medication?
And it invites you proactively to get into a conversation that gives the healthcare provider
tremendous insight into whether you're adhering to your prescription.
That's a huge problem.
Something like 35% of people actually don't follow through with their prescription medications.
And it's just there to answer questions, maybe you have some stomach upset, or some people
who are in a lot of medications hate having all those bottles and they put them all, you
know, dump all the pills into a baggie and then they can't remember which pills are which.
And so the health bot is able to converse with you and say, oh, well, why don't you point
your phone camera at this, at a bunch of pills and I'll remind you what they are and
it uses modern computer vision resonance actually to remind you what these pills are.
And so the kind of engagement that the healthcare providers get, the improvements in engagement
and the satisfaction that people like you and me have is really improved or just asking
simple benefits, questions or medical triage, various sorts, these kinds of ideas have been
surprisingly interesting and in fact, so surprising for us that later this week will be making
that product generally available for sale and so you'll be able to use the micro health
bot technology without any restrictions, well, except for payment of course.
And so that is something that has gone extremely well.
And that technology now is kind of being baked into more and more of I think of what
people will be seeing.
We have a collaboration hub application in Office 365 called Teams and Teams has been
this just wonderful technology for improving collaboration and all sorts of work by settings.
Well, we've made teams healthcare compliant and able to connect to electronic health record
systems and then by integrating great kind of collaboration intelligence tools to just
kind of parse records or know where to go to find certain bits of information or just
to be able to ask an intelligent agent that is part of your team, you know, did so and
so, check, you know, the sutures last night and be able to get a smart answer, whether
people are wake or not, you know, is there are all these little ways that I think AI can
be used in the workflow of healthcare delivery.
One of the things that is I think underappreciated about healthcare delivery today, especially
in acute care settings is it's a super collaborative environment.
Sometimes there can be as many as 20 people, you know, that are working together as a team
delivering care to multiple patients at a time.
And so how to keep that team of 20 people all on the same page and all coordinated is
getting to be a really difficult problem, typically done with, you know, posted notes and
you know, half erased whiteboards, now transitioning to pretty insecure consumer messaging apps,
but the idea of having real enterprise grade collaboration support with AI, I think
just can make all of that much better and then provide much more security and privacy
for people.
So a lot of these applications of AI end up being, you know, more less flashy than doing
some automatic radiation therapy planning on the medical image, but they really kind
of help people, you know, those people on the front lines of healthcare delivery do their
jobs better.
Yeah, I tend to find myself having a really kind of mixed feelings about conversational
applications, at least from the perspective of talking about them on the podcast, like
I think that they are, there's no question that conversational experiences and interfaces
will be a huge part of the way we interact with computers in the future and that there's
tons of work that needs to happen there because of the reasons that you mentioned, like
less flashy.
I wonder if there's still interesting research or at least my question to you is, are there
still interesting research challenges there or is it all, you know, do we have all the
pieces and it's just kind of rolling up the sleeves and, you know, building enterprise
software, which we know is hard and takes time?
Yeah, it's a good question.
It feels like research to me and some of the laboratories are, some of the problems
of anything feel a little difficult, honestly, you know, it's, so, you know, if we just
say take the problem of listening to a doctor patient conversation and from that understanding
what you go into the standard form of a clinical encounter note.
Here's a typical thing, there could be an exchange if, let's say Sam, you're my doctor
and I'm your patient and you might be asking me how I'm doing and I might complain about
you know, pain in my left knee hasn't gone away and what, you know, and we can have an
exchange about how that goes and ultimately what goes into the note by you is a note about
my continued lack of weight loss and that my, you know, being overweight is contributing
to the lack of healing with my knee problem that may or may not have been a part of our conversation.
And so while it's important that the weight loss elements be in that clinical note, in
fact, it might even mean revenue for that doctor because there may be a weight loss program
that gets prescribed and so on, that's important and it's important not to miss that.
But the human exchange here and the things that are implicit in those conversations led
along the fact that I'll say kneecap and you'll say Patela are things that are as close
to general artificial intelligence style problems as anything and so, you know, it's, and
look, we don't kid ourselves that we're anywhere close to solving those kinds of problems
but those are the kinds of problems we think about, even when we just look at the kind
of day-to-day minute-by-minute work that people do to deliver healthcare.
Right.
Right.
Here's another one that's interesting to really unlock the power of AI, what we would
want to do is to just open up huge databases to great researchers and innovators everywhere.
But of course, we need to do that without violating anyone's privacy and so there's one problem,
something called de-identification.
It would be great to be able to take a treasure trove of, let's say, electronic health records
and, quote unquote, de-identify it.
Well, some parts of those electronic health records are easy to do because there might
be a field called Social Security Number, another field called Name, another one called
Address and so on, so you can just scrub those out.
But large amounts of chronicle data involve just unstructured notes.
And to really have a deep understanding of what's in those notes and in order to scrub
those in a way that won't inadvertently build somebody's identity or their medical condition,
again, is something that, in the ultimate, ends up being a very general AI problem.
That's a great reframing of the way to think about this is like, yes, most chatbots are
boring because they're boring.
It's like, you know, the kind of the entity intent framework that, you know, most chatbots
are built on is kind of like table stakes relative to what we're really trying to do with
conversational experiences and that really requires a level of sophistication and our ability
to use and work with and manipulate natural language that is very much at the research
frontier now.
And that's why most current, you know, in production chatbots are kind of boring.
Yeah, we've taken a step forward of trying to think of these things almost in terms of
playing, you know, being able to play a game of 20 questions.
You know, one of the most inspiring applications of health bots that we dream about is in
matching people to clinical trials.
You know, at any point, there are thousands of clinical trials available and you can
go to a website called clinicaltrials.gov and there's a search bar there and you can
type in something like breast cancer.
And when you do that, you get this gigantic dump of every registered clinical trial going
on that might be pertinent to breast cancer.
And while that's useful, the problem with that is it's hard to know which ones of those,
you know, if you are, say, someone who's desperate to find clinical trial to enroll in because
you've run out of other viable options for whatever is ailing you, it's just almost impossible
to go through all of that technical information and try to understand this.
And so, you know, would it be possible to use an AI to read through all of that technical
information and then to synthesize what amounts to a game of 20 questions, something that
will converse with you and ask you questions in order to narrow down to just that one or
two or three clinical trials that might be a match for you.
And it's that kind of thing where it's not fully general conversation of the sort that
I think you and I were talking about just a minute ago, but it is slightly more structured
than that in order to help you more intelligently and more efficiently find the right medical
or healthcare solution for you.
And that kind of application is something that we're really putting a lot of kind of hard
and mind into, along with many others around the world.
And it's exciting that we're starting to see these things actually make it into clinical
use today.
And so, I kind of agree with you.
I roll my eyes sometimes at the overheated hype around intelligently and chatbots as
well, just like anybody else, but it's really getting somewhere in these more limited domains.
I think it also says why the interesting work in domains like this is going to be, you
know, it's not generic, right?
You're solving a specific problem and there's a lot of investment in kind of getting the
machine learning DA right for this particular problem as opposed to implementing a generic
framework.
That's right.
Awesome.
Well, Peter, thank you so much for taking the time to chat with me about the stuff you're
seeing and working on in the healthcare space.
A ton of really interesting examples in there and I'm looking forward to kind of following
all this work and digging deeper.
Thank you.
I didn't even talk about China once.
It's great.
Well, you mentioned ResNet a few times, kind of taunting me to dive into that conversation.
But our fur folks to the article and we'll put the link in the show notes.
Sounds great.
It was really a pleasure chatting.
All right, everyone.
That's our show for today for more information on Peter or any of the topics covered in
the show, visit twimmalai.com slash talk slash two, three, one.
To follow along with the AI for the benefit of society series, visit twimmalai.com slash
AI for society.
As always, thanks so much for listening and catch you next time.
