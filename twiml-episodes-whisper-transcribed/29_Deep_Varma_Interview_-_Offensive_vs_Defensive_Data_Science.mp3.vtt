WEBVTT

00:00.000 --> 00:15.380
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.380 --> 00:20.300
people doing interesting things in machine learning and artificial intelligence.

00:20.300 --> 00:23.700
I'm your host Sam Charrington.

00:23.700 --> 00:28.720
Last week was a big week for the podcast, I announced the first anniversary of the show.

00:28.720 --> 00:33.080
This week I want to start the show by thanking everyone who's participated in our first

00:33.080 --> 00:35.360
anniversary contest.

00:35.360 --> 00:40.360
We asked you to comment on the show notes page or post an iTunes review and wow did you

00:40.360 --> 00:41.360
deliver?

00:41.360 --> 00:45.360
Your stories have been personal, thoughtful and downright encouraging.

00:45.360 --> 00:50.720
I've got a couple that I'd like to share and really it's so hard just to pick a couple

00:50.720 --> 00:56.400
of these but first Andrew posted on the show notes page.

00:56.400 --> 01:00.880
I've used this podcast to maintain a pulse on current ML and AI.

01:00.880 --> 01:05.800
Big thank you for both helping me in my day to day but also getting me interested in ML

01:05.800 --> 01:08.320
in general when I first started listening.

01:08.320 --> 01:13.560
Heck, I've been listening to this podcast when I was a student, then when I was an intern

01:13.560 --> 01:18.440
and then when I was an entry level analyst and then when I was promoted to analyst and

01:18.440 --> 01:25.280
now as a senior analyst where I am now is in no small part thanks to this podcast.

01:25.280 --> 01:30.600
Wow, that's a ton of ground to have covered in just a year, Andrew, congrats.

01:30.600 --> 01:34.840
We are so proud to have been a small part of your success.

01:34.840 --> 01:37.960
This next one has a bit of a backstory.

01:37.960 --> 01:46.320
Bill B123 posted a five star review on iTunes titled sold on the deep interview format.

01:46.320 --> 01:51.160
Bill said new interview style format was not initially good.

01:51.160 --> 01:55.200
I gave it two stars initially later raised it to four stars.

01:55.200 --> 01:59.880
Now I'm sold great podcast with tons of insights and learning.

01:59.880 --> 02:02.960
Keep up the great work Sam.

02:02.960 --> 02:08.640
Now I remember when Bill's first review hit the decision to switch to the interview format

02:08.640 --> 02:14.520
was really tough for me, but I knew it was necessary for my efforts to be sustainable.

02:14.520 --> 02:17.360
Bill's initial two star review really, really hurt.

02:17.360 --> 02:22.920
I think at the time him or another user said the interviews were just a bit too fluffy

02:22.920 --> 02:25.960
and that I was like the Tim Ferriss of AI.

02:25.960 --> 02:30.040
I wasn't really sure what to do with that because I kind of like Tim Ferriss's podcast,

02:30.040 --> 02:35.440
but I really took that to heart and I was pumped when Bill raised his review to four stars.

02:35.440 --> 02:39.880
And now I'm super excited to see that I've earned Bill's fifth star.

02:39.880 --> 02:43.040
Thanks so much Bill.

02:43.040 --> 02:47.600
Starting from nothing I never imagined this podcast would begin to blossom into such an awesome

02:47.600 --> 02:49.200
community of users.

02:49.200 --> 02:56.080
I say begin because we still have so much ground to cover and we are truly just getting started.

02:56.080 --> 03:01.400
For those who have not yet had a chance to enter the contest, please visit twimmolai.com

03:01.400 --> 03:04.240
slash birthday for more info.

03:04.240 --> 03:10.680
Don't forget first prize gets a bronze pass to the O'Reilly AI conference this June,

03:10.680 --> 03:13.840
which is an $1,800 value.

03:13.840 --> 03:19.360
One prize gets a Google home powered by AI, of course, and everyone that participates

03:19.360 --> 03:22.240
gets a couple of Twimmolaptop stickers.

03:22.240 --> 03:27.120
The contest ends June 1st and winners will be announced on the second.

03:27.120 --> 03:32.200
If you've posted a review on iTunes to enter the contest, please reach out to us at team

03:32.200 --> 03:36.080
at twimmolai.com to let us know who you are.

03:36.080 --> 03:40.520
All right, this week on the show, my guest is Deep Varma.

03:40.520 --> 03:45.280
This President of Data Engineering at Real Estate Startup Trulia.

03:45.280 --> 03:50.480
Deep has run data engineering teams in Silicon Valley for well over a decade, and he's now

03:50.480 --> 03:56.000
responsible for the engineering efforts supporting Trulia's big data technology platform, which

03:56.000 --> 04:02.200
encompasses everything from data acquisition and management to data science and algorithms.

04:02.200 --> 04:07.200
In the show, we discuss all of that with an emphasis on Trulia's data engineering pipeline

04:07.200 --> 04:12.040
and their personalization platform, as well as how they use computer vision, deep learning,

04:12.040 --> 04:15.520
and natural language generation to deliver their product.

04:15.520 --> 04:20.720
Along the way, Deep offers great insights into what he calls offensive versus defensive

04:20.720 --> 04:26.440
data science and the difference between data driven decision making versus products.

04:26.440 --> 04:40.200
Another great interview, and I'm sure you'll enjoy it, and now on to the show.

04:40.200 --> 04:41.200
All right.

04:41.200 --> 04:42.200
Hey, everyone.

04:42.200 --> 04:43.680
I am on the line with Deep Varma.

04:43.680 --> 04:49.520
Deep is Vice President of Data Engineering with Trulia, and I'm excited to have Deep join

04:49.520 --> 04:50.520
us.

04:50.520 --> 04:52.000
Deep, how are you doing today?

04:52.000 --> 04:57.680
I'm doing great. It's not that hot. The California and San Francisco have been hot for the last

04:57.680 --> 05:03.120
few days, but seems like the fog is coming back, so I'm definitely doing amazing.

05:03.120 --> 05:04.440
How are you, Sam?

05:04.440 --> 05:05.440
Nice, nice.

05:05.440 --> 05:09.680
Well, I'm doing very well, and I'm really looking forward to our conversation and to learning

05:09.680 --> 05:14.960
a little bit more about how you guys use data at Trulia.

05:14.960 --> 05:20.680
Why don't we get started by having you talk a little bit about your background and how

05:20.680 --> 05:22.680
you got into working with data?

05:22.680 --> 05:23.680
Yeah.

05:23.680 --> 05:30.240
I think you asked me a great question, because when I go and I speak in some of the schools

05:30.240 --> 05:36.280
to help undergrads or the grads, those who are doing the management, and one of the guys

05:36.280 --> 05:41.680
I think few months back in Berkeley asked me the similar kind of a question that Deep

05:41.680 --> 05:50.080
why data, and I think Sam, it goes back to my reasoning, my mindset from the childhood

05:50.080 --> 05:57.360
where I was always looking into the reasons why this is this, why this is this, and when

05:57.360 --> 06:04.520
I get into my master's and computer science, I still remember there were old databases

06:04.520 --> 06:11.120
which some of, like you may know and you may not know, it's the DBs or the Fox base.

06:11.120 --> 06:18.240
Those were the early versions or the manifestations of the structured databases coming in, and

06:18.240 --> 06:25.680
I was always been very interested and then entering into my first job in IBM, it's where

06:25.680 --> 06:32.000
we are working on those XML directs change, how we are going to have the directs change

06:32.000 --> 06:37.320
between one entity and another entity, how the web services discovery locator are going

06:37.320 --> 06:39.560
to come into the picture.

06:39.560 --> 06:45.560
So early on the foundation was where I've been from the get go from my own personal desire

06:45.560 --> 06:51.800
to look into the answers as well as exposure to the early technologies.

06:51.800 --> 07:00.120
Get me into the databases and entering into then my journey where exploring why and what

07:00.120 --> 07:08.240
I realized is at the end of the day, we are always surrounded by data and the data doesn't

07:08.240 --> 07:14.200
mean that it has to be a textual data, the data is how we interact with each other when

07:14.200 --> 07:21.000
we are making phone calls to the people, when I'm searching for something, and that's

07:21.000 --> 07:30.200
I think it's 2001, 2002 time frame was started getting into my DNA that, you know, my God,

07:30.200 --> 07:37.320
you know, every day when I were I interact with anyone, anything I do, it's the data.

07:37.320 --> 07:40.160
And this is where I got into Yahoo.

07:40.160 --> 07:48.320
So then that was a step, you know, we're getting into Yahoo, helping advertisers and publishers,

07:48.320 --> 07:53.840
you know, try to render good quality ads to the consumers.

07:53.840 --> 08:00.120
This is all again, the platform is, you know, how you understand your consumers better.

08:00.120 --> 08:05.800
And then going into my startups and, you know, looking into the data again, where we are

08:05.800 --> 08:10.280
looking into, you know, how the data floating from one system to other system, what predictions

08:10.280 --> 08:11.800
we can do.

08:11.800 --> 08:18.080
So in nutshell, I will say it is, this is how I got into the data and I think is for me,

08:18.080 --> 08:23.440
my behavior, sometimes I'm at a point, Sam, I will tell you, when I go home, my wife

08:23.440 --> 08:28.840
have to remind me, honey, you're back home, don't think from a data point of view, just

08:28.840 --> 08:34.120
think, you know, you're back home, data reasoning is not going to work here.

08:34.120 --> 08:35.120
That's funny.

08:35.120 --> 08:36.120
That's funny.

08:36.120 --> 08:43.000
I did my, some of my grad school work on queuing theory and my wife is so tired of me

08:43.000 --> 08:48.880
analyzing lines and queuing scenarios and banks and grocery stores and trying to tell

08:48.880 --> 08:50.840
her which line she should be in.

08:50.840 --> 08:52.640
So I definitely relate to that.

08:52.640 --> 08:53.640
Yeah.

08:53.640 --> 08:55.880
I see you were in Yahoo back in the glory days.

08:55.880 --> 09:02.280
Oh, you trust me, you know, those were the glory days and I still miss those days because,

09:02.280 --> 09:07.640
you know, Yahoo was the center of attraction and the talent was huge there.

09:07.640 --> 09:16.400
So I worked there for four years and, you know, unfortunately, Yahoo is no longer Yahoo.

09:16.400 --> 09:18.600
But it was amazing.

09:18.600 --> 09:19.600
Absolutely.

09:19.600 --> 09:26.000
Were you involved in, did you use sedupe or were you involved in kind of the development

09:26.000 --> 09:28.000
and advancement of the dupe at that time?

09:28.000 --> 09:29.000
Yes.

09:29.000 --> 09:36.080
So when I went in, we were trying to, so Yahoo bought this company over here and we were

09:36.080 --> 09:44.520
trying to integrate Yahoo's like a platform back and platform to get the search keywords.

09:44.520 --> 09:51.600
So this is where Hadoop pipelines were integral part of the data flow between the systems

09:51.600 --> 09:57.920
that how we get the data from our Pasadena based company and then into our system.

09:57.920 --> 10:03.560
So yeah, I was not deeply like I was not part of the Hadoop ecosystem, but I was one of

10:03.560 --> 10:08.000
the consumers of the Hadoop system to get the data I'm floating around.

10:08.000 --> 10:09.000
Okay.

10:09.000 --> 10:10.000
Okay.

10:10.000 --> 10:15.920
And then now at, at Trulia, tell me a little bit about your role.

10:15.920 --> 10:22.640
It sounds like at least from LinkedIn that you've got a pretty broad set of responsibilities

10:22.640 --> 10:28.280
spanning everything from kind of your data platform, you know, I'm sure there's some Hadoop

10:28.280 --> 10:34.200
ecosystem, something in there somewhere to, you know, the data science and the applications

10:34.200 --> 10:35.200
that run on top of it.

10:35.200 --> 10:36.200
Is that right?

10:36.200 --> 10:37.200
That's fair.

10:37.200 --> 10:40.280
And let me walk you through first why Trulia?

10:40.280 --> 10:47.960
I think that's to me is the biggest piece which inspired me to join Trulia and do you mind

10:47.960 --> 10:50.480
Sam if I ask you?

10:50.480 --> 10:54.440
Do you rent a home or do you have your own home?

10:54.440 --> 10:55.440
Yes.

10:55.440 --> 10:56.440
Awesome.

10:56.440 --> 10:58.840
I'm pretty sure you're going to relate to this story.

10:58.840 --> 11:07.160
So this is way back in 1999 when we decided to buy our first home, you know, it is me and

11:07.160 --> 11:13.400
my wife, you know, the data was there, but the data was in storage like I have to go

11:13.400 --> 11:19.280
into police stations, I have to go into counties, I have to go into those areas to collect

11:19.280 --> 11:25.320
the data, we take this data, then me and my wife sit together, we go through the listings,

11:25.320 --> 11:30.080
we look into the neighborhood, we used to maintain our excel sheet, oh, let's look

11:30.080 --> 11:37.160
into this listing and it was an cumbersome process and it was an emotional journey for

11:37.160 --> 11:43.360
us to go through this exercise and it took us months to buy our first home and we did

11:43.360 --> 11:45.280
it finally, right?

11:45.280 --> 11:52.600
And that inspired me to join Trulia because, you know, when you think it's, how can we

11:52.600 --> 11:58.560
use this data when I, when I was in my early conversations with Trulia, that was the biggest

11:58.560 --> 12:04.000
thing for me is, you know, how I'm going to come and join Trulia and make an impact to

12:04.000 --> 12:10.040
build Trulia has more of a data driven product company and first thing in my mind was the

12:10.040 --> 12:17.200
use case of me 1999 buying a home that deep, can I make four millions of consumer that

12:17.200 --> 12:23.160
journey much more meaningful, much more enjoyable by using this data?

12:23.160 --> 12:28.280
So that's how my journey began with Trulia.

12:28.280 --> 12:35.080
Now just to go a little bit more detail into my role in Trulia, I think it's, you know,

12:35.080 --> 12:42.000
Trulia is, you know, our number one goal and this is where our founders, they saw a huge

12:42.000 --> 12:47.520
opportunity to change this marketplace by providing information and insights to our

12:47.520 --> 12:53.120
consumers to help them make the right decision and, you know, and make this journey home

12:53.120 --> 12:56.440
search journey easy and enjoyable.

12:56.440 --> 13:02.560
So, you know, with that mindset, with that goal, what our founder set in fourth, we,

13:02.560 --> 13:08.440
we continue my goal was to continue this and provide amazing experiences to our consumers

13:08.440 --> 13:15.640
and we are investing a lot in our personalization, big data, machine learning platforms to help

13:15.640 --> 13:22.720
consumers like me to, you know, find their perfect home in much more efficient and better

13:22.720 --> 13:25.720
way than I did, you know, years back.

13:25.720 --> 13:32.880
So that's in not show what role is and I'm happy to dive into more details about those

13:32.880 --> 13:35.800
technologies, what I'm talking about.

13:35.800 --> 13:42.400
Why don't we start with you talking a little bit about the data products and what are,

13:42.400 --> 13:49.680
from a consumer experience perspective, how do, how is, you know, machine learning

13:49.680 --> 13:54.360
in AI and the various data products that you create on your team?

13:54.360 --> 13:57.560
How is that surface to the truly a user?

13:57.560 --> 13:58.560
Yeah.

13:58.560 --> 14:05.960
So I think, you know, it's, first of all, data driven product companies, you know, Sam,

14:05.960 --> 14:12.160
there is a big mindset needs to happen and I'm just, you know, three years back when

14:12.160 --> 14:19.400
I joined Trulia, my philosophy was always being to transform and use this data more on their

14:19.400 --> 14:25.920
offensive strategy rather than defensive strategy and I'm going to go into answer your question

14:25.920 --> 14:29.840
but I just wanted to give you a little bit more details because, you know, when you think

14:29.840 --> 14:34.480
about the data driven companies, there are two aspects of the data driven company.

14:34.480 --> 14:39.120
One is the data driven decision making and other is the data driven product and the

14:39.120 --> 14:44.320
decision making is more your product analytics, you know, where you launch a feature and then

14:44.320 --> 14:48.280
you, oh, is this feature working or this feature is not working?

14:48.280 --> 14:55.400
But my one goal is to transform this data driven decision making more from a defensive to

14:55.400 --> 14:58.800
offensive by saying, is this feature going to work?

14:58.800 --> 15:02.960
So that's the one component and the second component, which is the discussion we are having

15:02.960 --> 15:10.320
today is around the data driven product company and this is where the way it surfaces

15:10.320 --> 15:15.520
to our consumers, our average consumer have no idea when they come to Trulia, when they

15:15.520 --> 15:23.080
engage with Trulia via mobile web app or our browser or desktop applications.

15:23.080 --> 15:25.880
They don't know that we use the data.

15:25.880 --> 15:30.240
It is basically pretty much embedded in their user experience.

15:30.240 --> 15:34.960
It is embedded when they explore, when they start their search journey.

15:34.960 --> 15:41.800
It is our responsibility to understand their behavior, what they're looking into and what

15:41.800 --> 15:49.160
we have done Sam is we have built an underlying personalization platform first and so think

15:49.160 --> 15:58.480
about that as our foundation and you know, this is where we have our consumers unique preferences,

15:58.480 --> 16:03.920
search criteria and you know, what they're looking into like deep is looking into quiet

16:03.920 --> 16:09.320
neighborhood, good school district in mission district of San Francisco, that's the personalization

16:09.320 --> 16:11.360
platform.

16:11.360 --> 16:17.080
On top of it, we have our machine learning pillars and there are many pillars what we have

16:17.080 --> 16:22.040
invested in machine learning, the first one is our computer vision and deep learning,

16:22.040 --> 16:28.920
the second one is our recommender engine, the third one is our user engagement models

16:28.920 --> 16:34.440
and the fourth one is our natural language processing or the natural language generation.

16:34.440 --> 16:40.960
So these are the machine learning pillars what we have and the, you know, and then we

16:40.960 --> 16:47.240
use all those pieces in tandem like machine learning pillars, personalization platform,

16:47.240 --> 16:54.160
all together to give that experience to our consumers when, when you come to our side

16:54.160 --> 16:59.200
and you look into, you know, the photos, when you look into the, when you receive an

16:59.200 --> 17:05.640
email from Julia, when you receive a push notification from Julia, all this is part

17:05.640 --> 17:12.000
of our machine learning technologies, which the goal is to engage our consumers and give

17:12.000 --> 17:20.320
them much more relevance experience during their stay with Julia and I will go definitely

17:20.320 --> 17:21.320
this conversation.

17:21.320 --> 17:24.920
I will give you more details around each and every component what we're going to talk.

17:24.920 --> 17:28.920
But if you have any question, I'm happy to ask address that first.

17:28.920 --> 17:29.920
Okay.

17:29.920 --> 17:39.080
There's, there's just so much in there to dig into, I like the, I like the distinction

17:39.080 --> 17:46.040
between the decision making versus the products and you mentioned specifically also this kind

17:46.040 --> 17:49.200
of dichotomy between offensive and defensive.

17:49.200 --> 17:51.000
Can you elaborate on that a bit more?

17:51.000 --> 17:52.000
Yeah.

17:52.000 --> 17:53.000
Yeah, definitely.

17:53.000 --> 18:01.720
I think is being in the Silicon Valley for close to two decades now and I have seen startup

18:01.720 --> 18:08.080
companies coming up and, you know, their focus is mostly around building the products.

18:08.080 --> 18:11.920
There are companies like Google's and Facebook's are definitely, you know, those who are more

18:11.920 --> 18:17.880
data driven, but I have seen early on when the company start building the products, their

18:17.880 --> 18:23.920
focus is never been the data side, they have couple of analytics who are staying behind

18:23.920 --> 18:24.920
the scene.

18:24.920 --> 18:27.240
Oh, should I change my pricing?

18:27.240 --> 18:30.840
Should I make my pricing for this consumer do this?

18:30.840 --> 18:36.640
It's always an after five, six years, if they have to, you know, raise more funds or

18:36.640 --> 18:41.560
they're about to go public or when they go public, now the mindset changes, oh, what is

18:41.560 --> 18:43.280
my differentiator?

18:43.280 --> 18:45.680
How I'm going to differentiate my product?

18:45.680 --> 18:48.080
How I'm going to bring my consumers back?

18:48.080 --> 18:51.040
How I'm going to engage my consumers?

18:51.040 --> 18:58.120
And this is where the offensive strategy comes into a picture that why not we have the

18:58.120 --> 19:02.640
companies start thinking about the data from the get go?

19:02.640 --> 19:04.040
What data you're collecting?

19:04.040 --> 19:05.520
How do you build?

19:05.520 --> 19:11.680
And I think that's a struggle and that struggle brings it to the point where companies have

19:11.680 --> 19:18.840
to go back and reinvest their resources, their millions of dollars to rebuild their architecture

19:18.840 --> 19:23.680
because if you think about the data, Sam, at the end of the day, this conversation what

19:23.680 --> 19:31.000
you and me are having is so much we are talking about, you know, artificial intelligence, machine

19:31.000 --> 19:36.720
learning, we are collecting this data and we are compacting into a podcast, but these

19:36.720 --> 19:38.040
are the signals, right?

19:38.040 --> 19:43.760
So there is a quality of the data, integrity of the data, how do we take all those things

19:43.760 --> 19:50.760
and bundle up so that the organizations are thinking about changing the direction from

19:50.760 --> 19:56.520
the get go rather than after the fact and after years thinking about.

19:56.520 --> 20:02.480
So that's the way, you know, I think and I think I wanted to differentiate between the

20:02.480 --> 20:06.680
two aspects which I talked earlier and I just want to make sure that both you and me

20:06.680 --> 20:07.680
are on the same page.

20:07.680 --> 20:13.760
One is the decision making and other is the product building, both of those facets requires

20:13.760 --> 20:22.400
the data, decision making is our amazing analytics teams rather than them working on the data

20:22.400 --> 20:24.440
and saying, is it working?

20:24.440 --> 20:28.080
I want to transform that to, is it going to work?

20:28.080 --> 20:32.280
That's the big differentiation and the product what we talked about, you know, that's the

20:32.280 --> 20:35.680
second facet of it, does it make sense so far?

20:35.680 --> 20:44.560
Yeah, no, it does make sense and, you know, I think the transformation that you describe

20:44.560 --> 20:50.080
is kind of going, you know, maybe it's a different cut at that defensive versus offensive

20:50.080 --> 20:58.720
and a lot in another way or put another way, you're trying to get teams to stop building,

20:58.720 --> 21:03.600
you know, rear view mirror analytics and start, you know, building analytics that predicts

21:03.600 --> 21:06.400
what's, you know, going to be happening in front of the windshield.

21:06.400 --> 21:09.800
You go, I think you nailed it better than me, right?

21:09.800 --> 21:13.200
It says the right way to explain it, right?

21:13.200 --> 21:18.960
Yeah, and I think that that is, you know, that transformation is something that's happening

21:18.960 --> 21:25.800
very broadly in, you know, industry, not just in technology companies, but also in enterprises

21:25.800 --> 21:32.160
and it's, you know, that need to look out the windshield and not be stuck, you know, reading

21:32.160 --> 21:38.280
reports that took weeks to create, that reflect the previous quarter and aren't really even

21:38.280 --> 21:39.280
relevant anymore.

21:39.280 --> 21:44.960
I think that's why, you know, enterprises are kind of grasping onto, you know, machine

21:44.960 --> 21:50.200
learning and AI-based solutions as a way to kind of give them that forward-looking view.

21:50.200 --> 21:55.200
Yeah, and I will add one more thing Sam here, so also, you know, when you think about

21:55.200 --> 22:01.400
any enterprise, any startup, any technology company, at the end of the day, you know, all

22:01.400 --> 22:06.280
the work is done by the people and you have the limited resources.

22:06.280 --> 22:11.360
And you know, you are building a product, rather build a product which is going to work

22:11.360 --> 22:12.360
in the marketplace.

22:12.360 --> 22:19.160
Like, no one has this magic wand to say this is going to work, but this, this front,

22:19.160 --> 22:24.320
you know, this offensive strategy or, you know, whatever way we want to say, it helps

22:24.320 --> 22:30.920
us to align our resources in the right direction, too, so that we can change the direction of

22:30.920 --> 22:36.960
our ship going in the true north rather than, you know, going the south direction and

22:36.960 --> 22:38.880
then bring it back.

22:38.880 --> 22:39.880
Right.

22:39.880 --> 22:40.880
Right.

22:40.880 --> 22:46.680
Before we dive into the platform that you built, you know, one thing does strike me is

22:46.680 --> 22:54.160
that, you know, perhaps more than some other companies truly is, you know, truly is product.

22:54.160 --> 22:56.720
This offering is data, right?

22:56.720 --> 23:01.640
I'm not making some assumptions, but I'm assuming that you're, you know, sourcing, you

23:01.640 --> 23:06.040
know, a bunch of different fees, and you even describe some of these, you know, you're

23:06.040 --> 23:11.920
MLS listings, your county, you know, data feeds, maybe pulling in from good schools and

23:11.920 --> 23:17.280
other sites that are producing aggregate data on, you know, schools and crime and all

23:17.280 --> 23:18.280
these things.

23:18.280 --> 23:25.080
Like, your fundamental data is so fundamental to the thing, the things that you do.

23:25.080 --> 23:29.080
Before you even get to how do you kind of, what have you built and what have you learned

23:29.080 --> 23:38.720
about aggregating all of this data and a little bit of a, a little bit of context for this.

23:38.720 --> 23:46.200
I often hear or, you know, I recently produced an event called the Future of Data Summit

23:46.200 --> 23:50.560
and we had speakers talking about, you know, different aspects of AI and several of them

23:50.560 --> 23:55.720
got up and said, you know, well, in order to do, you know, machine learning and AI, you

23:55.720 --> 23:58.760
have to have the data.

23:58.760 --> 24:03.760
And I think that's true, but, you know, it kind of glosses over the fact that sometimes

24:03.760 --> 24:07.800
you have to get the data, not just like, it's not just sitting there waiting to be explored.

24:07.800 --> 24:09.120
You have to go and find it.

24:09.120 --> 24:12.280
And it seems like a lot of what you did is, is going to find it.

24:12.280 --> 24:17.600
And so, you know, how did you, you know, to what extent does your team get involved in

24:17.600 --> 24:21.280
that and what's your platform for enabling that?

24:21.280 --> 24:22.280
Yeah.

24:22.280 --> 24:26.600
I'm so glad talking to you, right, because you are nailing down exactly the points where

24:26.600 --> 24:28.440
I'm passionate about.

24:28.440 --> 24:33.000
So when you, when you think about there are two pieces to the data and I'm going to make

24:33.000 --> 24:39.920
it very simple, first to start with, like when someone goes to Google and they search

24:39.920 --> 24:44.880
on a Google, the first thing what they're doing is they are giving the search engine

24:44.880 --> 24:48.560
on their intent, what I'm searching for.

24:48.560 --> 24:55.440
And then Google has this content, which is, they went ahead by crawlers and all those

24:55.440 --> 24:58.960
things by building the relevancy and all those things.

24:58.960 --> 25:07.760
So consumer gives the intent, Google has this massive databases of the content and then

25:07.760 --> 25:13.760
the magic in the middle, which takes the intent and content and matches up, which we call

25:13.760 --> 25:18.960
as a relevancy and give it to the consumer, where consumer feels happy about it, right?

25:18.960 --> 25:19.960
Right.

25:19.960 --> 25:24.200
Now in the same context, Julia also had the two parts of the data.

25:24.200 --> 25:28.880
One is the consumer for whom we are building this product.

25:28.880 --> 25:33.120
And then the content where we get this content from.

25:33.120 --> 25:37.080
And I think, you know, this is the listings as you talked about.

25:37.080 --> 25:40.400
This is the public records, which you talked about.

25:40.400 --> 25:45.520
Now, schools data, the crime data, you know, the commute data.

25:45.520 --> 25:55.080
I think it is, that's the difference between 1999 and 2017, where we have the technologies

25:55.080 --> 26:00.560
like real time messaging systems like Kafka, we have strong topologies or the streaming

26:00.560 --> 26:02.080
systems.

26:02.080 --> 26:08.520
We have those Hadoop or Spark technologies where we can make it easier to ingest those

26:08.520 --> 26:10.720
data into our system.

26:10.720 --> 26:16.680
So we have, and this data is pretty open, right, I have written on my blog, roughly we

26:16.680 --> 26:22.000
have 2.5 million active four cell listings on our system.

26:22.000 --> 26:28.680
So across US, you have agents, those who are working with consumers to sell their home,

26:28.680 --> 26:35.280
they enter this information into analysis, how this data comes in from analysis to our

26:35.280 --> 26:40.800
system, then you know, when you sell your home, when you buy your home, you pay your

26:40.800 --> 26:46.960
taxes, you have these assessments and the taxes, which are going into the counties, how

26:46.960 --> 26:48.720
we get this data.

26:48.720 --> 26:54.760
So I think my team involves at the end of the day, whatever you see on Julia's side,

26:54.760 --> 27:03.320
it is my team's responsibility to use technologies to bring this data in the raw form first.

27:03.320 --> 27:10.040
And then enrich this data, because when you think about, you know, you are MLS 1 and

27:10.040 --> 27:18.040
you will come and say, 1, 2, 3, 4, first street and you can spell first street as FIRSD

27:18.040 --> 27:21.360
or someone just come and write number 1 first street.

27:21.360 --> 27:28.840
So we have to have this magic in the middle to join all this data and say this data is

27:28.840 --> 27:30.960
for this property.

27:30.960 --> 27:36.680
And then once we know that, you know, the geolocation, when we had the address cleansing,

27:36.680 --> 27:43.760
address normalization, and then when we work on the enrichment piece, enrichment pieces

27:43.760 --> 27:50.160
for this listing, this is the historical information about the property, this is when

27:50.160 --> 27:57.160
the property was sold last time, this is the Texas information, this listing is 2 minutes

27:57.160 --> 28:01.080
away from the public transit system, this is the school.

28:01.080 --> 28:04.560
And then we go through this enrichment process.

28:04.560 --> 28:09.880
Once we had that enrichment process, it goes into our indexes, which is, you know, we

28:09.880 --> 28:17.320
use our solar technology and we have built, you know, our API layers on top of it, which

28:17.320 --> 28:25.040
can take up to 10 to 15,000 requests per second to serve our front end technologies like

28:25.040 --> 28:30.680
web apps and, you know, mobile web or whatever it is.

28:30.680 --> 28:35.440
So what I explained it to you on surface, it looks like a big process, which takes days

28:35.440 --> 28:41.240
and days and days, interestingly enough, when the listing hits the marketplace, by the

28:41.240 --> 28:47.840
time it goes from one point with the enrichment to the front end, it is less than 15 minutes

28:47.840 --> 28:49.560
where we show the data.

28:49.560 --> 28:56.480
So this is all because of the technologies what we have enables us to give this content

28:56.480 --> 28:58.520
to consumers faster.

28:58.520 --> 29:04.320
So I just talked about the content piece, which is, you know, all the data flowing around

29:04.320 --> 29:09.640
and I think most likely, when you're ready, we will jump into the intent piece, which

29:09.640 --> 29:13.520
is the personalization and all of us, but does it make sense so far?

29:13.520 --> 29:14.520
It does.

29:14.520 --> 29:17.720
It does and I still have tons of questions on that content side.

29:17.720 --> 29:28.960
Um, so thinking about the various ways that you likely get data, I'm imagining maybe

29:28.960 --> 29:34.200
three and I'm there probably many more, but I'm imagining, you know, some data is coming

29:34.200 --> 29:36.440
you via feeds.

29:36.440 --> 29:43.000
Maybe this is like the listing, some data is coming you, coming to you via streams and

29:43.000 --> 29:51.440
some data is coming to you via, um, in batches, um, like, can you characterize like how much

29:51.440 --> 29:55.760
of the data is each and are there cat, is there a category that I'm missing in, in this

29:55.760 --> 30:00.960
and, um, in, and then like where do you, you know, where do you land it?

30:00.960 --> 30:06.160
How much of, you know, what you're doing is, you know, real time, stream based kind

30:06.160 --> 30:08.480
of, uh, processing?

30:08.480 --> 30:14.760
Um, yeah, so I think it's, it depends upon the set of the data, like, so when you think

30:14.760 --> 30:19.960
about the listings, listings are majority of our listings are the stream based, which

30:19.960 --> 30:24.960
are real time, because you know, listings, here's the market, but then when you think about

30:24.960 --> 30:33.560
the public records, which is these assessments, Texas information, that is mostly the batch

30:33.560 --> 30:39.720
based, what we have, and then you have the school data, which is, you know, not, it's not

30:39.720 --> 30:42.800
changing on a daily basis, that's a feed based.

30:42.800 --> 30:47.160
Then you have a crime data, which is, you know, more the streaming thing.

30:47.160 --> 30:53.200
So I think it is, it depends upon the data set, what we, so we have the technologies where

30:53.200 --> 31:01.280
we define, if the data needs to be refreshed more frequently, we use the streaming technologies

31:01.280 --> 31:05.760
and otherwise, you know, we use batch based systems.

31:05.760 --> 31:11.640
We have invested in building our, some of the systems uses the Lambda technologies.

31:11.640 --> 31:16.200
So this is, you know, the real time plus the batch base on an ugly basis, we run the

31:16.200 --> 31:21.120
full Lambda and then make sure that there is, you know, the accuracy on the quality of

31:21.120 --> 31:24.160
the data being implemented.

31:24.160 --> 31:26.400
There are some places we also use Kapa.

31:26.400 --> 31:28.440
I don't know if you heard about Kapa.

31:28.440 --> 31:32.240
So Kapa is also, you know, the real time, but the batch base.

31:32.240 --> 31:36.760
So I think it's at the end of the day, my team have built those pipelines.

31:36.760 --> 31:44.560
Some pipeline uses, you know, the Kafka messages to strong typologies, the streaming technologies.

31:44.560 --> 31:50.600
Some places we have the spark where we need the data to be processed much more faster.

31:50.600 --> 31:58.400
So I think it is at the different data set, at the different refresh SLA and based on those

31:58.400 --> 32:04.840
refreshing SLAs, we tend to, you know, bring it to our systems.

32:04.840 --> 32:10.840
And before we move on, why don't you give us a brief overview of Lambda architectures

32:10.840 --> 32:13.920
and you mentioned Kapa as well.

32:13.920 --> 32:14.920
Yeah, yeah.

32:14.920 --> 32:19.520
And you mentioned strom as well, this is streaming and having come across those.

32:19.520 --> 32:20.520
Yeah.

32:20.520 --> 32:23.080
So I think let's start with the streaming, right?

32:23.080 --> 32:27.120
So I think what's happening is the streaming, the real time streaming and the processing.

32:27.120 --> 32:34.320
So, you know, when we have those messages, so think about if there are 2.5 million active

32:34.320 --> 32:41.560
listings are there across, you know, US, when they hit our system, they're coming into

32:41.560 --> 32:45.760
our messaging layer and there are different messaging technologies are there.

32:45.760 --> 32:49.480
We are using Kafka, Kinesis, mix of that.

32:49.480 --> 32:56.020
From there, we move into the streaming and, you know, the streaming can be spark or it

32:56.020 --> 32:57.920
can be a strom topology.

32:57.920 --> 33:02.720
There is a place where we use strom topologies because when the listing hits, remember

33:02.720 --> 33:08.360
early on I was telling you that we need to do geo cleanse address cleansing, address

33:08.360 --> 33:12.520
normalization and then, you know, the enrichment.

33:12.520 --> 33:20.080
So this is where the goals that we had the spouts and the goals of our strom topology where

33:20.080 --> 33:26.200
spouts are the when which is ingesting the data and then, you know, the goals are the

33:26.200 --> 33:32.680
one which are making the decision making, you know, let I have to perform step xyz.

33:32.680 --> 33:39.640
So the strom topology helps us in the real time, take the stream of the data, perform

33:39.640 --> 33:47.920
the enrichment, perform the cleansing and then go and persist it into our new messaging

33:47.920 --> 33:51.480
layer from the data can be sent over.

33:51.480 --> 33:54.600
So that's the strom topology.

33:54.600 --> 34:01.400
Lambda is, you know, Lambda is being there in the marketplace for long and what Lambda

34:01.400 --> 34:06.560
means is, you know, look at the on a daily basis when we are getting millions and millions

34:06.560 --> 34:11.880
of messages and, you know, it comes into our system.

34:11.880 --> 34:17.120
It is very important for us to maintain the accuracy and the quality of the data.

34:17.120 --> 34:25.760
So on a nightly basis, we rerun the whole data set what we have collected on this day

34:25.760 --> 34:32.240
just to make sure that if there are gaps, we fill those gaps using the Lambda architecture

34:32.240 --> 34:37.480
and which will give us the higher level of accuracy and the quality of the data coming

34:37.480 --> 34:39.400
into our system.

34:39.400 --> 34:46.200
So the only difference is in, in Lambda, you have to write your code base differently

34:46.200 --> 34:48.440
to consume the batch base.

34:48.440 --> 34:53.560
But when you move into the Kapa architecture, you don't need to write the separate code

34:53.560 --> 34:54.560
base.

34:54.560 --> 34:59.280
You can have the similar code base which is used during the real time streaming and you

34:59.280 --> 35:04.240
can use the same code base for your batch based typologies also.

35:04.240 --> 35:06.640
So Kapa enables you to do that.

35:06.640 --> 35:12.480
So that's how we use those three technologies which I just talked about.

35:12.480 --> 35:13.480
Okay.

35:13.480 --> 35:14.480
All right.

35:14.480 --> 35:15.480
Great.

35:15.480 --> 35:18.680
We'll include some links to these and the shun outs.

35:18.680 --> 35:23.960
I've come across Lambda architecture before, but Kapa architecture is new to me.

35:23.960 --> 35:29.240
Yeah, it is coming pretty new in the marketplace last couple of years.

35:29.240 --> 35:32.000
I see people using it move.

35:32.000 --> 35:33.000
Okay.

35:33.000 --> 35:34.000
Great.

35:34.000 --> 35:43.480
So you've ingested all of this data and you've used technologies like Lambda and Kapa architecture,

35:43.480 --> 35:49.640
it's Apache Storm and other technologies, Kafka, Q's and messaging and all these things

35:49.640 --> 35:53.840
to get all this data in to enrich it.

35:53.840 --> 35:58.360
And where do you, where do you land it?

35:58.360 --> 36:02.160
So if your question on the landing is, where do we persist it?

36:02.160 --> 36:03.160
Right.

36:03.160 --> 36:04.160
Yeah.

36:04.160 --> 36:07.160
So we persisted in our solar index.

36:07.160 --> 36:10.560
So solar is the search technology.

36:10.560 --> 36:16.600
And when you think about, you know, we have this millions and millions of rows coming in.

36:16.600 --> 36:24.480
Do we need to, if there are 100 attributes in a row, do we need to persist everything

36:24.480 --> 36:25.480
in the solar?

36:25.480 --> 36:31.400
So we basically, all the searchable things we stored in our solar and then the things

36:31.400 --> 36:37.480
which are not searchable, which are just an augmentation of the data can go into any

36:37.480 --> 36:44.000
of the no SQL databases or some places where we feel the data is much more structured and

36:44.000 --> 36:49.440
we don't need, we use the relational databases also where the volume is pretty small.

36:49.440 --> 36:53.880
So we use my SQL and that's how we persist.

36:53.880 --> 37:04.600
So we use, you know, solar, HBase, Redis, DynamoDB, MySQL and I'm pretty sure, you know,

37:04.600 --> 37:10.040
Aerospike is another one which we recently started using the key value pair systems.

37:10.040 --> 37:15.800
So we use a very wide variety of the databases here in Trulia.

37:15.800 --> 37:20.240
And again, it all boils down to the use cases where do we need to store what?

37:20.240 --> 37:21.240
Right.

37:21.240 --> 37:22.240
Right.

37:22.240 --> 37:27.040
So the individual teams that are working on, you know, given products that are surfaced

37:27.040 --> 37:30.640
through the site and they choose whatever data store makes the most sense for their use

37:30.640 --> 37:31.640
cases.

37:31.640 --> 37:32.640
Is that right?

37:32.640 --> 37:33.640
Yes.

37:33.640 --> 37:34.640
Yes and no.

37:34.640 --> 37:35.640
So right.

37:35.640 --> 37:42.760
So for example, you know, if we know the latency is a big thing for us, then storing that

37:42.760 --> 37:45.440
in HBase may not make sense.

37:45.440 --> 37:51.480
So people may decide to use, you know, Redis or they may go with the DynamoDB.

37:51.480 --> 37:58.000
So I think we, we have the some guidelines around like the biggest thing for us is build

37:58.000 --> 38:03.600
the databases and see the latency, right, because we have the API.

38:03.600 --> 38:10.760
We have abstracted all the data as an API layer on top of those systems so that when

38:10.760 --> 38:17.600
front and team comes and says, give me all the data for this listing, then this API goes

38:17.600 --> 38:22.960
across the different systems or the databases to bring the data, stitch it together.

38:22.960 --> 38:25.520
So latency plays a major role.

38:25.520 --> 38:31.280
But to some extent, what you were trying to say, yes, the decentralization of teams definitely

38:31.280 --> 38:37.080
enable us to have teams pick the technologies, what they want to pick.

38:37.080 --> 38:41.560
We don't put so many guidelines except for the latency as one of the prerequisite making

38:41.560 --> 38:44.040
sure that we pick the right technologies.

38:44.040 --> 38:45.040
Okay.

38:45.040 --> 38:46.040
Okay.

38:46.040 --> 38:48.400
All right, so then all that in place.

38:48.400 --> 38:54.840
Let's jump into the personalization platform and the stuff that you're doing on top of

38:54.840 --> 38:55.840
it.

38:55.840 --> 38:56.840
Here you go.

38:56.840 --> 38:57.840
That's the fun piece, right?

38:57.840 --> 38:58.840
It's a, right.

38:58.840 --> 39:01.040
I really love that piece, I think.

39:01.040 --> 39:07.040
Now we, what we talked about in last few minutes was mostly around the content, right?

39:07.040 --> 39:11.000
So now we need to start thinking from an intent point of use.

39:11.000 --> 39:18.560
When consumers, they come to Trulia, you know, when they are interacting with our website

39:18.560 --> 39:27.040
or mobile app or mobile web or email at any given moment of time, what we have seen,

39:27.040 --> 39:29.920
our consumers are generating those signals.

39:29.920 --> 39:36.120
And the signals are nothing but their intent, you know, deep is looking into, you know,

39:36.120 --> 39:43.440
a listing in no value of a San Francisco, which is in quite neighborhood.

39:43.440 --> 39:44.440
That's a signal.

39:44.440 --> 39:49.320
And then deep is looking into photos, you know, and what kind of the photos deep is looking

39:49.320 --> 39:50.600
into.

39:50.600 --> 39:56.960
So we have this stream of data flowing into our system, what signals and what we internally

39:56.960 --> 40:04.880
call those as an events and events are generated by consumer interacting with those product.

40:04.880 --> 40:11.440
So we basically take those events and our personalization platform and, you know, collect those

40:11.440 --> 40:12.440
events.

40:12.440 --> 40:17.200
The events are just think about, you know, if Sam goes to Trulia and you look into some

40:17.200 --> 40:22.600
site, you know, on an average, like when you look into a specific property, Sam is going

40:22.600 --> 40:29.200
to generate an average of 20 events, you know, within few minutes of your interaction.

40:29.200 --> 40:34.920
So we have this again, the real time messaging layer, which collects those signals and,

40:34.920 --> 40:41.000
you know, we have Trulia has millions of consumers, which are active on a monthly basis.

40:41.000 --> 40:46.000
So when they send those signals, we bring it to our Kafka layer.

40:46.000 --> 40:52.560
And from the Kafka layer, we basically brings it again, we use a streaming technologies

40:52.560 --> 40:58.360
like Spark or Strong again for the intent site of the technologies tag too.

40:58.360 --> 41:04.880
And this is where either we have the real time machine learning models in place or we have

41:04.880 --> 41:12.280
some aggregated systems where those signals are getting evaluated, right?

41:12.280 --> 41:20.680
Okay, we just see deep or we saw an anonymous consumer, we take all those data and then

41:20.680 --> 41:27.600
we persist it into our caching layer where that caching layer, which can be, you know,

41:27.600 --> 41:31.600
the edge base is our persistence layer for all the personalization platform, but then

41:31.600 --> 41:34.080
the caching here is the RERIS, what we have.

41:34.080 --> 41:35.080
Okay.

41:35.080 --> 41:41.040
So if Sam is pretty much active on our site, then Sam moves from edge based to RERIS.

41:41.040 --> 41:44.360
That's how we make because of the latency.

41:44.360 --> 41:52.680
So at the end, you know, this personalization platform stores, Sam's unique preferences,

41:52.680 --> 42:00.160
search criteria is, you know, Sam is looking into your Sam owns this two bedroom, three

42:00.160 --> 42:06.400
baths in St. Louis area in a quiet neighborhood, Sam is looking into this.

42:06.400 --> 42:12.600
So I think that's the personalization platform is a very foundational aspect, which drives

42:12.600 --> 42:15.320
rest of the other thing.

42:15.320 --> 42:18.960
Then on top of, I'm going to move over to machine learning systems.

42:18.960 --> 42:19.960
Is that fine now?

42:19.960 --> 42:20.960
Sure.

42:20.960 --> 42:21.960
Yeah.

42:21.960 --> 42:27.840
So when you think about this personalization platform is put into place, which is like an

42:27.840 --> 42:33.360
engine, which is working on a daily basis by itself, our first machine learning platform

42:33.360 --> 42:36.840
is computer vision and the deep learning.

42:36.840 --> 42:41.920
This is where we, we've been leading this industry in the computer vision and the deep

42:41.920 --> 42:47.080
learning for years, where, you know, computer vision, right, it's a system which we have

42:47.080 --> 42:53.920
built, where we have, you know, trained our systems, machines to look into photos and

42:53.920 --> 43:00.320
they can see, oh, I'm looking into a photo of, you know, the swimming pool or I'm looking

43:00.320 --> 43:04.440
into a photo of a kitchen, which has a granite countertop.

43:04.440 --> 43:10.320
So that's the computer vision, what we have implemented and then what we do is all those

43:10.320 --> 43:17.320
unique attributes, the data, which comes out of the system, powers our home page and in

43:17.320 --> 43:21.440
our home page, you will see what we call as collections.

43:21.440 --> 43:26.880
The collections are nothing but the group of properties, which we bring it together.

43:26.880 --> 43:33.400
So you may see collections like, you know, homes with swimming pools or home with remodder

43:33.400 --> 43:38.440
homes or homes with kitchen, granite countertop.

43:38.440 --> 43:46.120
So those are the collections, which we powers our home page and the more our consumers

43:46.120 --> 43:51.760
engage with these collections, the more inside we get into our consumers.

43:51.760 --> 43:56.480
So that's the one use case of our computer vision.

43:56.480 --> 44:02.840
The second use case of our computer vision is, you know, I'm pretty sure you, me and

44:02.840 --> 44:06.560
all the consumers when they start their home buying journey.

44:06.560 --> 44:11.920
The first thing they do is they come into site like Trulia and the search for neighborhood

44:11.920 --> 44:17.200
then they go to a listing and then they start looking into the photos of the home.

44:17.200 --> 44:19.440
That's how the journey starts.

44:19.440 --> 44:25.760
And if those photos are not engaging and if those photos are not telling story, consumers

44:25.760 --> 44:29.800
are going to lose their interest and they will keep moving into the second and third.

44:29.800 --> 44:37.760
So what we have done is we using a conventional neural networks, CNN models, we have invested

44:37.760 --> 44:43.760
in understanding, you know, the scene types of the photos, whether the photo is appropriate

44:43.760 --> 44:47.760
or not, like some, someone can just put a photo of a dog.

44:47.760 --> 44:52.440
So we say great, you know, we can see it is not a photo of a home, it is a photo of

44:52.440 --> 44:53.440
a dog.

44:53.440 --> 44:59.640
And then the quality of a photo is this photo is blur, is this photo is much more clear.

44:59.640 --> 45:06.040
So these three things, what we take out from our CNN models is the quality of a photo,

45:06.040 --> 45:14.280
appropriateness and the scene type, we score those things and then the highest performing

45:14.280 --> 45:17.600
photo, what we call as our hero image.

45:17.600 --> 45:23.680
So what we do is most attractive photo, when you start your journey, we put the most

45:23.680 --> 45:29.720
attractive photo for you first, so that your engagement becomes much more better with

45:29.720 --> 45:30.720
Trulia.

45:30.720 --> 45:36.440
So that's the second use case of our machine learning computer vision.

45:36.440 --> 45:41.280
And what we have seen by investing in those technologies, you know, there are double digit

45:41.280 --> 45:44.480
increase in inquiries for our listing.

45:44.480 --> 45:48.680
So that's the one piece, make sense over.

45:48.680 --> 45:57.080
And is the, is that lift based on, do you think primarily just getting the right listing

45:57.080 --> 46:03.160
in front of the, the right person who's likely to like it or is it, you know, getting rid

46:03.160 --> 46:10.040
of the, or kind of suppressing the listings that aren't, you know, good in general, you

46:10.040 --> 46:13.760
know, is there any one factor that drives the kind of results that you've seen?

46:13.760 --> 46:19.800
Yeah, so I think our relevancy is driven mostly by the consumer's behavior, what consumers

46:19.800 --> 46:22.000
are interested into.

46:22.000 --> 46:26.280
And so we basically just based on the consumer needs and this is where the personalization

46:26.280 --> 46:32.520
platform comes into a play to drive that computer vision on the serving side, what to

46:32.520 --> 46:34.560
serve to the consumer.

46:34.560 --> 46:35.560
Mm-hmm.

46:35.560 --> 46:36.560
Makes sense?

46:36.560 --> 46:37.560
So, yes.

46:37.560 --> 46:44.080
Two different users, you know, say my wife and I are kind of collaboratively shopping for

46:44.080 --> 46:48.320
a home as husbands and wives tend to do.

46:48.320 --> 46:52.560
You know, she might, when she goes to the site, she might see pool pictures first and I

46:52.560 --> 46:57.240
might see kitchen pictures first or what have you, depending on what our, what our interests

46:57.240 --> 46:58.240
are.

46:58.240 --> 47:03.040
And these aren't interests that we've explicitly shared with you their interests that

47:03.040 --> 47:07.520
you've derived from the various, you know, signals from watching the way we interact with

47:07.520 --> 47:08.520
the site.

47:08.520 --> 47:09.520
That's fair.

47:09.520 --> 47:14.600
And that's how basically the more you engage, the more we know about you, because if you

47:14.600 --> 47:17.920
come for the first time, we really don't know about you, right?

47:17.920 --> 47:23.320
It's basically we need to reach enough confidence level to serve you the right content.

47:23.320 --> 47:27.480
But yes, your assessment was pretty good.

47:27.480 --> 47:28.480
Mm-hmm.

47:28.480 --> 47:36.960
It's funny, I can't help, but the thing that I, as a, as someone who travels a lot and

47:36.960 --> 47:42.720
as a result uses Yelp a lot, I am always complaining about just how dumb the Yelp app is.

47:42.720 --> 47:48.280
And I don't think I've ever done it on the podcast before, but I wish they were doing

47:48.280 --> 47:49.480
more of what you're doing.

47:49.480 --> 47:54.600
When I land in a city, I pretty much like open up Yelp and like type in Thai or type

47:54.600 --> 47:58.480
in in every time trying to find a place to eat.

47:58.480 --> 48:02.480
And I always wonder like, why doesn't it just show me what it knows that I'm going to

48:02.480 --> 48:06.000
be looking for, what it should know that I'm going to be looking for.

48:06.000 --> 48:11.160
So maybe I'll use that rant as a segue into like what are the challenges that you've

48:11.160 --> 48:17.040
seen or what do you think, you know, what's the barrier to, you know, more companies, you

48:17.040 --> 48:23.080
know, having technology that enables them to better know and, and personalize to their

48:23.080 --> 48:24.080
customers.

48:24.080 --> 48:30.960
So just to understand, so your question is, what is the biggest barrier to investing in

48:30.960 --> 48:32.720
this kind of technologies?

48:32.720 --> 48:33.720
Yeah.

48:33.720 --> 48:34.720
Yeah.

48:34.720 --> 48:43.280
I think it is mostly around making sure, remember early on, we talked about the data driven

48:43.280 --> 48:51.720
product companies that how do you understand the strength or the data what you have with

48:51.720 --> 48:52.720
you?

48:52.720 --> 48:57.240
And I think it's, it's the first it needs to start from the top level, the commitment

48:57.240 --> 49:01.400
from the top level, that's the area we want to invest in.

49:01.400 --> 49:08.080
And the second thing Sam is rather than boiling the ocean, right, or let's solve all the problems

49:08.080 --> 49:15.680
in one go, pick the small use cases to evangelize within organization so that, you know, product

49:15.680 --> 49:21.240
people and the other stakeholders can bought into those concepts because, you know, AI or

49:21.240 --> 49:25.840
the machine learning is still in very infancy stage, you know, we have not reached the point

49:25.840 --> 49:27.560
where everyone understands.

49:27.560 --> 49:33.560
So my recommendation to the people is definitely, you know, bring an evangelist, build a small

49:33.560 --> 49:41.640
use cases, show the value prop, back it up with the data, build slowly and gradually build

49:41.640 --> 49:42.640
the tsunami.

49:42.640 --> 49:46.080
And when this tsunami is going to hit, then everyone is going to bought into this.

49:46.080 --> 49:49.160
So that's the way what I look into.

49:49.160 --> 49:54.280
Yeah, that's a great articulation of the process.

49:54.280 --> 50:00.800
So you guys have also done some writing on your engineering blog about how you use natural

50:00.800 --> 50:03.760
language processing and in particular natural language generation.

50:03.760 --> 50:05.920
Can you talk a little bit about that use case?

50:05.920 --> 50:06.920
Sure.

50:06.920 --> 50:11.680
So yeah, so I think, you know, thinking about, so it all starts again for us.

50:11.680 --> 50:15.920
We don't start from thinking about machine learning first.

50:15.920 --> 50:18.960
We always start from thinking about the consumer first.

50:18.960 --> 50:20.920
That's the number one goal.

50:20.920 --> 50:27.200
And what we started seeing, there are thousands and thousands of cities or neighborhoods across

50:27.200 --> 50:34.400
US when consumers come to our side, you know, they're looking for more information about

50:34.400 --> 50:35.400
this side.

50:35.400 --> 50:39.720
They're looking into more information about that neighborhood.

50:39.720 --> 50:45.240
And we said, great, now we, you know, how can we use the data, what we have to build

50:45.240 --> 50:52.120
the story and one way you can do is, you know, you can have the human beings as editors

50:52.120 --> 50:56.040
and let them write the stories about the cities and the neighborhoods.

50:56.040 --> 51:02.520
But when you go into this kind of a massive scale, there is no way that's going to work.

51:02.520 --> 51:07.360
And that's where we said, okay, great, now let's rely on the machine learning technologies

51:07.360 --> 51:09.240
to solve that problem.

51:09.240 --> 51:14.880
And this is where we leaned on our natural language generation system.

51:14.880 --> 51:21.240
So what we do is, we look into a location and we have built this feature extractor.

51:21.240 --> 51:26.560
The feature extractor look into, you know, what are the restaurants close by?

51:26.560 --> 51:29.720
What are the commute systems looks like?

51:29.720 --> 51:34.840
Is the price going up for that neighborhood, is it going down?

51:34.840 --> 51:37.040
So we extract the features.

51:37.040 --> 51:43.240
And then once those features comes out, what we have is a document planner, which looks

51:43.240 --> 51:47.200
into the features we have a document planner.

51:47.200 --> 51:53.480
But before we go into a description generator, we have built our content bank.

51:53.480 --> 52:01.480
So think about the content bank is where we build the sentences based on the features

52:01.480 --> 52:08.040
that if we say this, like this neighborhood is a Victorian style homes.

52:08.040 --> 52:14.680
So our content bank is going to have a sentence which will say Victorian, this neighborhood

52:14.680 --> 52:16.480
has Victorian homes.

52:16.480 --> 52:24.000
So we have this content bank and that content bank is built based on some of the crowdsourcing,

52:24.000 --> 52:28.800
which definitely we do, but we use our data mining and machine learning technologies to

52:28.800 --> 52:32.080
look into the data to build those content bank.

52:32.080 --> 52:38.760
So now think about the document planner coming out for a neighborhood or a city or a specific

52:38.760 --> 52:41.440
location, which has all the feature sets.

52:41.440 --> 52:46.680
We have a content bank and this is where then we use the description generator.

52:46.680 --> 52:52.720
So description generator, take the document planner, take the content bank bank and use

52:52.720 --> 52:58.920
the NLG to generate the content for that specific location.

52:58.920 --> 53:06.040
So that's how we use NLG and it's been going great on that front.

53:06.040 --> 53:09.880
So what I think I hear you saying, and this can be instructive to folks that want to

53:09.880 --> 53:19.760
use this, is that as opposed to trying the thoroughbunch of data to some natural language

53:19.760 --> 53:27.200
generation system and hoping for it to generate something that makes sense, you guys have

53:27.200 --> 53:34.200
broken up the problem and structured it in such a way that first year, you're identifying

53:34.200 --> 53:41.400
the, you call them features of a given neighborhood, maybe not features in the sense of a training

53:41.400 --> 53:46.320
of machine learning algorithm, but they're just attributes of a neighborhood and you kind

53:46.320 --> 53:54.480
of structure your descriptions so that you will highlight one or more of these attributes.

53:54.480 --> 54:00.080
And then the content bank, what I thought I heard was that you kind of have a set of

54:00.080 --> 54:05.200
templates or rough structures of the way you talk about different things.

54:05.200 --> 54:09.640
So you kind of have a template for how you talk about, you know, a neighborhood composition

54:09.640 --> 54:15.280
in terms of its architecture, maybe some templates for restaurants, things like that.

54:15.280 --> 54:21.040
And then, you know, all of that, you know, those attributes that you decided to highlight

54:21.040 --> 54:28.240
in a given description and these, this set of templates are utilized by this description

54:28.240 --> 54:33.640
generated to create something that, you know, sounds more human and is more readable and

54:33.640 --> 54:38.920
usable than, you know, what you might get if you just threw all the data against a neural

54:38.920 --> 54:39.920
net of some sort.

54:39.920 --> 54:40.920
Yeah.

54:40.920 --> 54:44.360
One clarification, the feature is a structure.

54:44.360 --> 54:50.640
It uses the data mining technologies to extract the attributes what you're talking about.

54:50.640 --> 54:53.040
So yes, it goes into a neighborhood.

54:53.040 --> 54:58.160
It uses the data mining in generates attribute and you're right spot on on the content bank

54:58.160 --> 54:59.160
in a simple form.

54:59.160 --> 55:04.240
You can think about the templates or you can think about, you know, which defines the much

55:04.240 --> 55:08.520
more vocabulary, which is easily understood by our consumers.

55:08.520 --> 55:09.520
Mm-hmm.

55:09.520 --> 55:10.520
That's great.

55:10.520 --> 55:11.520
That's great.

55:11.520 --> 55:16.200
Well, I know you're bumping up against a time constraint here.

55:16.200 --> 55:22.240
I think this is a great, you know, use case, we spent a lot more time on the, you know,

55:22.240 --> 55:26.920
data engineering, data acquisition side than we usually do on the podcast and, you know,

55:26.920 --> 55:31.240
I enjoyed geeking out a little bit on some of that stuff.

55:31.240 --> 55:35.080
But it sounds like you guys are doing really, really awesome things.

55:35.080 --> 55:38.800
And so thank you so much for being on the show and sharing them with us.

55:38.800 --> 55:39.800
Great.

55:39.800 --> 55:40.800
Thanks, Sam.

55:40.800 --> 55:41.800
All right.

55:41.800 --> 55:42.800
Thanks, Deep.

55:42.800 --> 55:43.800
Bye-bye.

55:43.800 --> 55:48.400
All right, everyone, that's our show for today.

55:48.400 --> 55:53.440
Once again, thanks so much for listening and for your continued support.

55:53.440 --> 55:59.280
Don't forget to leave your review or comment and there are one year anniversary listener

55:59.280 --> 56:01.480
appreciation content.

56:01.480 --> 56:06.960
The full details can be found at TwomoAI.com slash birthday.

56:06.960 --> 56:13.960
And of course, you can leave your questions and comments over on the show notes page at twomoai.com

56:13.960 --> 56:21.120
slash talk slash 2525 for your final links to deep and the various resources we mentioned

56:21.120 --> 56:22.640
in the show.

56:22.640 --> 56:39.800
Thanks so much for listening and catch you next time.

