1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,520
I'm your host Sam Charrington.

4
00:00:31,520 --> 00:00:36,400
I'd like to start out by thanking everyone who joined me last week at the Twimble AI Summit

5
00:00:36,400 --> 00:00:37,960
in Las Vegas.

6
00:00:37,960 --> 00:00:39,960
It was a great event.

7
00:00:39,960 --> 00:00:44,920
For a summary of the event and my key takeaways from each of the event sessions, sign up for

8
00:00:44,920 --> 00:00:49,840
my newsletter at twimbleai.com slash newsletter.

9
00:00:49,840 --> 00:00:54,160
I wrote about it right after returning from the event last week and when you sign up,

10
00:00:54,160 --> 00:00:58,720
you'll automatically get an email telling you how to get access to back issues.

11
00:00:58,720 --> 00:01:04,480
Again, that's twimbleai.com slash newsletter.

12
00:01:04,480 --> 00:01:06,440
Event season continues this week.

13
00:01:06,440 --> 00:01:11,120
Tomorrow I'm key noting at the Prepare AI event here in St. Louis and then making my

14
00:01:11,120 --> 00:01:15,840
way out to San Francisco for Figure H's Train AI Conference.

15
00:01:15,840 --> 00:01:21,320
The Train AI agenda looks awesome and I'll be on site all day podcasting so if you're

16
00:01:21,320 --> 00:01:25,400
in the Bay area, you should definitely plan to stop by.

17
00:01:25,400 --> 00:01:31,680
Of course, if you do, use the discount code Twimbleai for 30% off of registration.

18
00:01:31,680 --> 00:01:35,040
Be sure to give me a shout if you're planning to be around.

19
00:01:35,040 --> 00:01:41,720
In this episode, I'm joined by John Bohannon, Director of Science at AI Startup Primer.

20
00:01:41,720 --> 00:01:46,000
As you all may know, a few weeks ago, we released my interview with Google Legend, Jeff

21
00:01:46,000 --> 00:01:51,160
Dean, which by the way, you should definitely check out if you haven't already.

22
00:01:51,160 --> 00:01:56,040
Anyway, in that interview, Jeff mentions the recent explosion of machine learning papers

23
00:01:56,040 --> 00:02:00,760
on archive, which I responded to jokingly by asking whether Google had already developed

24
00:02:00,760 --> 00:02:04,680
the AI system to help them summarize and track all of them.

25
00:02:04,680 --> 00:02:08,720
While Jeff didn't have anything specific to offer, a listener reached out and let me

26
00:02:08,720 --> 00:02:12,920
know that John was in fact already working on this problem.

27
00:02:12,920 --> 00:02:18,120
In our conversation, John and I discuss his work on Primer Science, a tool that harvests

28
00:02:18,120 --> 00:02:24,000
content uploaded to archive, sorts it into natural topics using unsupervised learning,

29
00:02:24,000 --> 00:02:28,600
then gives relevant summaries of the activity happening in different innovation areas.

30
00:02:28,600 --> 00:02:32,520
We spend a good amount of time on the inner workings of Primer Science, including their

31
00:02:32,520 --> 00:02:36,920
data pipeline and some of the tools they use, how they determine ground truth for training

32
00:02:36,920 --> 00:02:41,720
their models, and the use of heuristics to supplement NLP in their processing.

33
00:02:41,720 --> 00:02:43,920
Alright, let's do it.

34
00:02:43,920 --> 00:02:52,200
Alright, everyone, I am on the line with John Bohannon.

35
00:02:52,200 --> 00:02:56,360
John is Director of Science at a startup called Primer.

36
00:02:56,360 --> 00:02:59,200
John, welcome to this week in machine learning and AI.

37
00:02:59,200 --> 00:03:00,200
Hey!

38
00:03:00,200 --> 00:03:01,840
So this conversation is an interesting one.

39
00:03:01,840 --> 00:03:08,440
They grew out of a listener response to a comment made in my recent interview with Jeff Dean.

40
00:03:08,440 --> 00:03:13,680
Jeff commented on the explosion of machine learning papers on archive, and I jokingly

41
00:03:13,680 --> 00:03:18,320
asked if Google had already developed the deep learning based summarization techniques

42
00:03:18,320 --> 00:03:20,080
to help us all keep up.

43
00:03:20,080 --> 00:03:24,080
And it turns out that one of your colleagues, John, reached out to let me know that you

44
00:03:24,080 --> 00:03:27,120
have been working on this and have built it.

45
00:03:27,120 --> 00:03:30,560
And I think just before we got started, you showed it to me and it's pretty cool.

46
00:03:30,560 --> 00:03:37,080
So here we are, but before we get into the details of that project, you've got an interesting

47
00:03:37,080 --> 00:03:40,840
background in molecular biology and data journalism.

48
00:03:40,840 --> 00:03:45,320
How did you find your way to AI?

49
00:03:45,320 --> 00:03:53,280
It's a long journey, but I think it started in computer camp when I was nine years old.

50
00:03:53,280 --> 00:03:56,040
So that's the kind of summer camp I went to.

51
00:03:56,040 --> 00:04:04,520
But yeah, as my studies progressed, I actually drifted away into biology in a PhD in molecular

52
00:04:04,520 --> 00:04:05,680
biology.

53
00:04:05,680 --> 00:04:12,160
And then before doing my next postdoc, I wanted to take a break and do something different.

54
00:04:12,160 --> 00:04:18,760
So I tried being a journalist, a science journalist, and fell in love with it and basically jumped

55
00:04:18,760 --> 00:04:24,040
off the academic track and became eventually a computational journalist, basically using

56
00:04:24,040 --> 00:04:31,120
data and code to find and tell stories that are impossible to tell otherwise.

57
00:04:31,120 --> 00:04:37,280
And a friend of mine named Sean Gorley, who did his PhD with me in England at the same

58
00:04:37,280 --> 00:04:38,280
time.

59
00:04:38,280 --> 00:04:40,920
I actually lived in the same house.

60
00:04:40,920 --> 00:04:44,640
Our fate eventually became intertwined again.

61
00:04:44,640 --> 00:04:51,280
I moved to the Bay area to do a visiting scholar stint at Berkeley and he's in San Francisco.

62
00:04:51,280 --> 00:04:54,920
He says, hey, John, I've got this startup called Primer.

63
00:04:54,920 --> 00:04:58,000
And you really should come by and check out what we're doing.

64
00:04:58,000 --> 00:05:04,120
I think you're going to find that the stuff we're working on really, really matches with

65
00:05:04,120 --> 00:05:06,040
the stuff you work on.

66
00:05:06,040 --> 00:05:11,040
And so eventually I had some time and I was like, okay, I'll pop over there for a week.

67
00:05:11,040 --> 00:05:18,960
And sure enough, within one day, it was clear that they were solving problems that I just

68
00:05:18,960 --> 00:05:25,000
find so hard and I wanted so badly to solve myself that, basically, if you can't be

69
00:05:25,000 --> 00:05:26,400
to join them.

70
00:05:26,400 --> 00:05:27,400
Nice.

71
00:05:27,400 --> 00:05:28,400
Nice.

72
00:05:28,400 --> 00:05:36,520
So maybe for context, you can tell us a little bit about what the company does and the

73
00:05:36,520 --> 00:05:40,040
kinds of problems that they're working on or you're working on.

74
00:05:40,040 --> 00:05:41,040
Yeah.

75
00:05:41,040 --> 00:05:48,200
So Primer at its core is an AI company that's trying to make machines that read and write.

76
00:05:48,200 --> 00:05:51,520
That's the fundamental problem that underlies all this.

77
00:05:51,520 --> 00:05:58,800
In terms of a business model, we, for example, automate a lot of the work that a junior analyst

78
00:05:58,800 --> 00:06:03,520
would do in, say, a bank or the intelligence community.

79
00:06:03,520 --> 00:06:06,280
Also, frankly, what a journalist does.

80
00:06:06,280 --> 00:06:11,560
I feel like I'm reverse engineering myself every day because a lot of what you have to

81
00:06:11,560 --> 00:06:12,560
do.

82
00:06:12,560 --> 00:06:19,560
It's also somewhat automating a lot of what you do, Sam, like all of our jobs, what we

83
00:06:19,560 --> 00:06:24,360
have in common is that we have to read a ton of stuff, often very technical stuff, and

84
00:06:24,360 --> 00:06:25,760
makes sense of it.

85
00:06:25,760 --> 00:06:27,520
And then tell stories.

86
00:06:27,520 --> 00:06:30,720
Like that is the fundamental unit of information.

87
00:06:30,720 --> 00:06:33,800
That's our data structure, a story.

88
00:06:33,800 --> 00:06:38,000
And that is really hard for computers to do.

89
00:06:38,000 --> 00:06:40,600
It's really hard for people to do.

90
00:06:40,600 --> 00:06:41,600
Exactly.

91
00:06:41,600 --> 00:06:45,080
Yeah, it's one of those things that's both, that's hard for everyone.

92
00:06:45,080 --> 00:06:53,400
So I think you're relatively new to this podcast, but those that have been around for a while

93
00:06:53,400 --> 00:06:59,440
from the beginning know that it started out as more of a news-oriented format as opposed

94
00:06:59,440 --> 00:07:01,920
to an interview format.

95
00:07:01,920 --> 00:07:08,160
And basically, my mission was to kind of summarize the most interesting AI and ML tidbits from

96
00:07:08,160 --> 00:07:09,880
the previous week's news.

97
00:07:09,880 --> 00:07:15,520
But that is super, super hard, especially with so much news happening all the time.

98
00:07:15,520 --> 00:07:25,640
It would take a ton of time to curate all of that information and digest it and turn it

99
00:07:25,640 --> 00:07:28,360
into stories as you're saying.

100
00:07:28,360 --> 00:07:29,360
Exactly.

101
00:07:29,360 --> 00:07:33,360
And so like you face several problems and what we're trying to do at Primer is break it down

102
00:07:33,360 --> 00:07:36,720
into reasonable problems that you can actually attack.

103
00:07:36,720 --> 00:07:40,880
So one is, for example, what's relevant?

104
00:07:40,880 --> 00:07:43,000
What are you telling a story about?

105
00:07:43,000 --> 00:07:47,240
It's not enough to just say, I want to tell a story about last week's AI research.

106
00:07:47,240 --> 00:07:50,720
It's like, okay, well, what documents are relevant?

107
00:07:50,720 --> 00:07:54,320
Even if you could get the papers, then it's like, well, where do you get all the conversations

108
00:07:54,320 --> 00:07:55,640
about those papers?

109
00:07:55,640 --> 00:07:58,040
How do you figure out what those papers were about?

110
00:07:58,040 --> 00:08:01,720
If there were a thousand papers published over the past several months and you wanted to

111
00:08:01,720 --> 00:08:06,400
tell a story of a thousand papers, I don't know how a human would do that.

112
00:08:06,400 --> 00:08:08,800
Well, actually, I can tell you, humans simply don't do that.

113
00:08:08,800 --> 00:08:11,600
What we do is we take shortcuts.

114
00:08:11,600 --> 00:08:12,960
We sort of fly blind.

115
00:08:12,960 --> 00:08:19,320
We grab the zeitgeist and that's kind of a random process.

116
00:08:19,320 --> 00:08:24,840
It's like, well, I overheard some conversations and this seems to be a hot topic.

117
00:08:24,840 --> 00:08:27,400
I'm going to decide and so I'm going to amplify it.

118
00:08:27,400 --> 00:08:33,560
And what you end up with are coherent stories, but they're not necessarily what actually

119
00:08:33,560 --> 00:08:35,840
was the most important thing that happened.

120
00:08:35,840 --> 00:08:40,920
It's just some strange sampling of the space of all things that happened and that's the

121
00:08:40,920 --> 00:08:41,920
best you can do.

122
00:08:41,920 --> 00:08:47,160
But what if you had a machine that could actually read everything and show you in some sense

123
00:08:47,160 --> 00:08:48,760
everything that happened?

124
00:08:48,760 --> 00:08:49,760
That's the goal.

125
00:08:49,760 --> 00:08:56,360
So you showed me a kind of a portal into research papers is the idea to provide that as

126
00:08:56,360 --> 00:09:01,760
a service or more of the platform that allows someone to create that thing.

127
00:09:01,760 --> 00:09:08,080
So we're in a pretty privileged position, we're privileged in the sense that we've already

128
00:09:08,080 --> 00:09:09,920
got some really big customers.

129
00:09:09,920 --> 00:09:19,120
So the federal government, Walmart, Singapore's sovereign trust, several others coming online

130
00:09:19,120 --> 00:09:24,520
soon, those are the relationships that actually pay the bills.

131
00:09:24,520 --> 00:09:29,400
And so we do things like if you have a portfolio manager who's trying to keep track of a ton

132
00:09:29,400 --> 00:09:37,000
of companies, that portfolio manager needs to stay on top of all the relevant developments

133
00:09:37,000 --> 00:09:40,720
in the space roughly defined by all those companies.

134
00:09:40,720 --> 00:09:48,560
All the news about them, maybe SEC filings, if you want to assess changes in risk profile,

135
00:09:48,560 --> 00:09:50,960
it's sort of an overwhelming task.

136
00:09:50,960 --> 00:09:57,720
And so primer basically superpowers those analysts by automating all the things that are really

137
00:09:57,720 --> 00:10:03,480
hard and tedious and time consuming, and it basically reduces the cost of curiosity.

138
00:10:03,480 --> 00:10:08,920
It allows those analysts to not spend half their day reading a million things just to find

139
00:10:08,920 --> 00:10:17,760
out what was worth reading, instead they can see summaries of 100 papers at once, get

140
00:10:17,760 --> 00:10:22,880
a sense of whether it's worth diving deeper or look at another batch of 100 papers.

141
00:10:22,880 --> 00:10:32,320
It also gives alerts with predefined conditions so that you don't lose a second if something

142
00:10:32,320 --> 00:10:37,440
that you know in retrospect is going to be a situation worth knowing about, you'll get

143
00:10:37,440 --> 00:10:38,760
a heads up.

144
00:10:38,760 --> 00:10:45,200
So meanwhile though, you can use the same machinery that does reading and writing and summarization

145
00:10:45,200 --> 00:10:49,360
to do things like the thing I sent you, like read all of archive.

146
00:10:49,360 --> 00:10:54,560
So we do have a business model for this system going forward.

147
00:10:54,560 --> 00:11:00,920
We're going to be developing it into products for, for example, the pharmaceutical industry.

148
00:11:00,920 --> 00:11:07,040
But for the time being, we just have this beautiful laboratory where we get to really push the

149
00:11:07,040 --> 00:11:09,960
edge of natural language processing.

150
00:11:09,960 --> 00:11:14,840
Tell us more about this archive project that you've built.

151
00:11:14,840 --> 00:11:20,480
Yeah, archive is a really good illustration of this problem that we all face of too much

152
00:11:20,480 --> 00:11:21,800
information.

153
00:11:21,800 --> 00:11:28,520
If you ever go to the archive website, you basically see a fire hose of research coming in.

154
00:11:28,520 --> 00:11:37,120
Archive is amazing because it is literally the place where research gets debuted.

155
00:11:37,120 --> 00:11:44,800
It's the first place you'll see a paper coming out from Google or Microsoft or MIT on

156
00:11:44,800 --> 00:11:51,080
the topics that are basically going to define the machine learning progress over the next

157
00:11:51,080 --> 00:11:52,600
10 years.

158
00:11:52,600 --> 00:11:59,440
In retrospect, you can look back and you can see the timeline of this amazing scientific

159
00:11:59,440 --> 00:12:01,960
revolution unfolding.

160
00:12:01,960 --> 00:12:04,440
But it's not at all human readable.

161
00:12:04,440 --> 00:12:11,040
Even if you are an expert, even if you have a PhD in machine learning, you just can't

162
00:12:11,040 --> 00:12:13,040
make sense of all of archive.

163
00:12:13,040 --> 00:12:18,520
You might be able to make sense of the papers in your own subdomain, but even there, it's

164
00:12:18,520 --> 00:12:19,520
tough.

165
00:12:19,520 --> 00:12:20,520
You've got to find them.

166
00:12:20,520 --> 00:12:23,400
Archive isn't designed for humans in a way.

167
00:12:23,400 --> 00:12:29,760
I mean, it is, but it's just not user-friendly.

168
00:12:29,760 --> 00:12:34,160
Primer science is a stab at making sense of that.

169
00:12:34,160 --> 00:12:40,920
Basically, it's a really hard problem that's well-scoped.

170
00:12:40,920 --> 00:12:49,240
What it does is it harvests all these papers and it does unsupervised learning on the content

171
00:12:49,240 --> 00:12:54,200
of the papers to try and figure out what are the topics that this naturally falls into.

172
00:12:54,200 --> 00:12:59,120
So within machine learning, for example, I'm just looking now at some of the latest.

173
00:12:59,120 --> 00:13:04,800
The system has discovered that there are not only image reconstruction papers, there's

174
00:13:04,800 --> 00:13:10,400
like 58 papers actually in this bag that are on that theme, but it has discovered that

175
00:13:10,400 --> 00:13:15,880
there's a whole bunch of research on traffic and temporal analysis.

176
00:13:15,880 --> 00:13:18,080
There's something on mathematical optimization.

177
00:13:18,080 --> 00:13:23,720
There's a whole bunch of papers about semantic segmentation.

178
00:13:23,720 --> 00:13:28,640
All of this is happening without an ontology or a knowledge base.

179
00:13:28,640 --> 00:13:35,600
You're going to have to have such a system if you want it to work on any corpus of papers.

180
00:13:35,600 --> 00:13:41,840
You could imagine building some super ontology that captures everything there is to know

181
00:13:41,840 --> 00:13:47,040
about science, but then it's going to be out of date next month.

182
00:13:47,040 --> 00:13:51,320
I wouldn't want to build that thing because maintaining it would be a nightmare.

183
00:13:51,320 --> 00:13:55,320
Instead, you need a system that does more or less what humans do on a smaller scale.

184
00:13:55,320 --> 00:13:59,600
What we do is we look at things and we just sort of eyeball it and say, oh, these are

185
00:13:59,600 --> 00:14:03,000
kind of about this and these are about that.

186
00:14:03,000 --> 00:14:07,200
You get a natural segmentation of the space.

187
00:14:07,200 --> 00:14:13,080
Within each of these topics, it does a time series analysis and it tries to figure out if

188
00:14:13,080 --> 00:14:19,960
I take all the news and the social media signal, all the tweets about this research as it

189
00:14:19,960 --> 00:14:28,880
was published and afterwards, all the commentary, all the real time online critique, sort of the

190
00:14:28,880 --> 00:14:35,680
peer review that's happening in real time out in the open, can I detect events?

191
00:14:35,680 --> 00:14:39,360
An event can be more than just the publication of a paper.

192
00:14:39,360 --> 00:14:47,600
It could be that, for example, a self-driving car crashes somewhere and suddenly the world

193
00:14:47,600 --> 00:14:55,960
is looking intensely at an issue related to what we do and don't know about these systems.

194
00:14:55,960 --> 00:14:59,000
Some of this research may get pulled into that.

195
00:14:59,000 --> 00:15:04,960
If you want to detect that real-world event, you need a system that can actually divide

196
00:15:04,960 --> 00:15:09,840
all those documents, all those tweets, all those things that are relevant to the same thing

197
00:15:09,840 --> 00:15:13,000
and figure out how to segment them in time.

198
00:15:13,000 --> 00:15:16,720
It does that too and it tries to figure out, essentially, what were the big events in

199
00:15:16,720 --> 00:15:17,720
this space?

200
00:15:17,720 --> 00:15:24,800
How was human attention in the world divided in relation to this corpus of papers?

201
00:15:24,800 --> 00:15:31,040
Then does some other cute tricks to make it useful to you as you dive into all of this

202
00:15:31,040 --> 00:15:32,520
information.

203
00:15:32,520 --> 00:15:36,600
It pulls out all the people and tries to tell you what it knows about them.

204
00:15:36,600 --> 00:15:41,000
Just based on the corpus, mind you, we're also developing a version of this that is building

205
00:15:41,000 --> 00:15:46,040
a knowledge base and actually learning about people as it reads the news and as papers are

206
00:15:46,040 --> 00:15:47,040
published.

207
00:15:47,040 --> 00:15:55,400
What I sent you this morning is just, essentially, out of the box, I don't know anything about

208
00:15:55,400 --> 00:16:00,280
the world, but I know this group of thousands of papers you sent me and this is what I can

209
00:16:00,280 --> 00:16:01,680
tell you about them.

210
00:16:01,680 --> 00:16:03,800
These are all the people.

211
00:16:03,800 --> 00:16:05,600
These are all the topics.

212
00:16:05,600 --> 00:16:10,360
These are the events that seem to all of this information seems to be pointing at out

213
00:16:10,360 --> 00:16:13,320
in the real world.

214
00:16:13,320 --> 00:16:19,920
One is, if you're finding the jargon really hard to understand, I've generated a dictionary

215
00:16:19,920 --> 00:16:25,160
for you that is kind of a magical dictionary where if you click on a technical term, it

216
00:16:25,160 --> 00:16:30,800
actually shows you who coined that term, how is it defined?

217
00:16:30,800 --> 00:16:34,960
Give me some context about how to use this kind of like a Oxford English dictionary on

218
00:16:34,960 --> 00:16:35,960
steroids.

219
00:16:35,960 --> 00:16:36,960
Nice.

220
00:16:36,960 --> 00:16:37,960
Nice.

221
00:16:37,960 --> 00:16:42,240
I'm finding this interview more challenging than most because as you're speaking, I've

222
00:16:42,240 --> 00:16:47,920
got the tool in the background and I keep seeing papers that look really interesting.

223
00:16:47,920 --> 00:16:51,560
It's working.

224
00:16:51,560 --> 00:16:54,320
Super, super distracting.

225
00:16:54,320 --> 00:17:00,400
Maybe can you tell us a little bit about the technology that's making it all happen?

226
00:17:00,400 --> 00:17:02,840
What does the stack look like?

227
00:17:02,840 --> 00:17:04,560
What does the pipeline look like?

228
00:17:04,560 --> 00:17:09,400
How are you approaching the unsupervised learning piece?

229
00:17:09,400 --> 00:17:15,040
It all begins with a gigantic elastic search index.

230
00:17:15,040 --> 00:17:21,800
I think if you talk to a lot of the people that you've interviewed even already about what's

231
00:17:21,800 --> 00:17:28,440
at the bottom of this whole stack, there's often some massive index of documents.

232
00:17:28,440 --> 00:17:35,640
We're ingesting the news and blogs and tweets and scientific papers every day and that's

233
00:17:35,640 --> 00:17:41,760
the starting point of this whole system and it has this growing corpus.

234
00:17:41,760 --> 00:17:48,600
If you query as we've done today on artificial intelligence, for example, the first thing

235
00:17:48,600 --> 00:17:56,360
it has to do is retrieve all the information that is relevant and then kicks off this pipeline

236
00:17:56,360 --> 00:18:03,680
where basically the first thing it does is it tries with unsupervised learning plus

237
00:18:03,680 --> 00:18:10,560
several other steps to divide all the information up into natural topics.

238
00:18:10,560 --> 00:18:18,200
Within each topic, it then tries to detect the events in the real world that any of these

239
00:18:18,200 --> 00:18:21,400
documents might be referring to.

240
00:18:21,400 --> 00:18:27,640
If you've got 100 documents that might be news documents and scientific papers and

241
00:18:27,640 --> 00:18:33,120
social media signal about all the above, you do a time series analysis on it and you try

242
00:18:33,120 --> 00:18:37,920
and figure out, are there real world events?

243
00:18:37,920 --> 00:18:39,280
It's trying to make an inference here.

244
00:18:39,280 --> 00:18:46,120
Are there real world events that all of this information is pointing at and describing?

245
00:18:46,120 --> 00:18:52,080
It looks at events basically from the perspective of news articles, is that right?

246
00:18:52,080 --> 00:19:00,840
The system you're looking at does, yeah, but you can imagine any document that has a meaningful

247
00:19:00,840 --> 00:19:07,680
publication timestamp and includes a description or commentary about something that happened

248
00:19:07,680 --> 00:19:09,040
in the real world.

249
00:19:09,040 --> 00:19:12,920
It could in principle be mapped to something called an event.

250
00:19:12,920 --> 00:19:19,520
The concept of an event is bigger than what a human intuitively would call event.

251
00:19:19,520 --> 00:19:25,840
It might actually be, for example, an explosion of discussion around an issue.

252
00:19:25,840 --> 00:19:31,760
For example, the Me Too movement is not just an event, it's made up of many events and

253
00:19:31,760 --> 00:19:36,200
some of these events might not even be something that could have been observed in one place

254
00:19:36,200 --> 00:19:41,960
at one time, but there is a natural segmentation of all the things happening in the world into

255
00:19:41,960 --> 00:19:44,560
something that we call events.

256
00:19:44,560 --> 00:19:47,960
So that's the theory behind this.

257
00:19:47,960 --> 00:19:58,240
Then if you click over to overview, sorry to distract you again, then it tries to tell

258
00:19:58,240 --> 00:19:59,240
you a story.

259
00:19:59,240 --> 00:20:01,880
So we've got many versions of this.

260
00:20:01,880 --> 00:20:08,920
What you're looking at is basically one of the earliest versions of this, but basically

261
00:20:08,920 --> 00:20:15,960
if you asked a machine to go and read thousands of things and you give it a budget of one

262
00:20:15,960 --> 00:20:21,920
page to tell you what it learned, this is starting to get at what you'd expect to come

263
00:20:21,920 --> 00:20:23,440
back.

264
00:20:23,440 --> 00:20:25,120
This is what you get.

265
00:20:25,120 --> 00:20:34,040
It's basically, and it's kind of like a technical report on these are things that I learned.

266
00:20:34,040 --> 00:20:35,040
These are the big events.

267
00:20:35,040 --> 00:20:36,040
These are the big papers.

268
00:20:36,040 --> 00:20:37,040
This is what's getting us attention.

269
00:20:37,040 --> 00:20:44,760
Oh, and then by the way, my topic analysis has revealed that there are some changes of

270
00:20:44,760 --> 00:20:52,440
the foot in artificial intelligence, and these are the things that seem to be trending upwards

271
00:20:52,440 --> 00:20:53,520
and are really interesting.

272
00:20:53,520 --> 00:20:59,200
And oh, by the way, I discovered there's this weird paper that seems to fall in this topic,

273
00:20:59,200 --> 00:21:03,520
but it's deeply connected to this other topic, and that's statistically strange.

274
00:21:03,520 --> 00:21:05,640
I need to tell you about it.

275
00:21:05,640 --> 00:21:11,560
And by the way, here's some people who seem to be getting a ton of attention, and here's

276
00:21:11,560 --> 00:21:17,240
another person who has collaborated with them on a high profile paper, and they've never

277
00:21:17,240 --> 00:21:18,640
worked together before.

278
00:21:18,640 --> 00:21:19,640
That's interesting.

279
00:21:19,640 --> 00:21:25,720
So you can see what's going on here is the system has a model of what humans find interesting.

280
00:21:25,720 --> 00:21:28,840
And of course, we humans at Primer built that in.

281
00:21:28,840 --> 00:21:32,000
There's a story logic that I don't realize this.

282
00:21:32,000 --> 00:21:34,120
You don't want a system to tell you everything it learned.

283
00:21:34,120 --> 00:21:36,640
It's just going to be another fire hose.

284
00:21:36,640 --> 00:21:39,000
You've made no progress.

285
00:21:39,000 --> 00:21:42,120
A one-to-one map of the world is not a useful map.

286
00:21:42,120 --> 00:21:47,280
So you need something that will compress the information and try and tell you a story.

287
00:21:47,280 --> 00:21:48,960
So that's what the system does.

288
00:21:48,960 --> 00:21:54,640
I think I interrupted you as you were about to start talking about the pipeline that you're

289
00:21:54,640 --> 00:21:58,040
sending some of this stuff through.

290
00:21:58,040 --> 00:22:04,360
And just going back to the beginning with archives, are you ingesting all of the archive

291
00:22:04,360 --> 00:22:07,920
papers or crawling that site?

292
00:22:07,920 --> 00:22:08,920
Yeah.

293
00:22:08,920 --> 00:22:16,400
So Paul Ginsberg, who founded and still runs archive, is a friend of mine from a good

294
00:22:16,400 --> 00:22:18,600
while back.

295
00:22:18,600 --> 00:22:21,040
And he uses Primer Science as well.

296
00:22:21,040 --> 00:22:25,200
I think actually he's the very first one I made a user account for.

297
00:22:25,200 --> 00:22:26,200
Oh wow.

298
00:22:26,200 --> 00:22:27,200
Yeah.

299
00:22:27,200 --> 00:22:34,080
And so he's really helped out over the past year, making sure that we have direct access.

300
00:22:34,080 --> 00:22:37,760
So we don't have to scrape the site.

301
00:22:37,760 --> 00:22:45,920
We basically just pull down the entire day's new papers on one go.

302
00:22:45,920 --> 00:22:52,440
And we do the same with news, except it arrives more or less in real time.

303
00:22:52,440 --> 00:22:59,880
So we have a real-time stream, more or less, of the news with maybe a 10 minute delay.

304
00:22:59,880 --> 00:23:06,440
And we've got a real-time stream of all the tweets that are relevant to the space.

305
00:23:06,440 --> 00:23:11,000
Yeah, those via commercial APIs of some sort.

306
00:23:11,000 --> 00:23:13,080
We get them directly from Twitter.

307
00:23:13,080 --> 00:23:14,080
Okay.

308
00:23:14,080 --> 00:23:16,360
So yeah, we have a day to deal with them.

309
00:23:16,360 --> 00:23:17,360
Okay.

310
00:23:17,360 --> 00:23:18,360
And the news?

311
00:23:18,360 --> 00:23:21,720
The news we actually have several sources of.

312
00:23:21,720 --> 00:23:24,040
One of the most convenient is Lexus Nexus.

313
00:23:24,040 --> 00:23:26,000
They have a service called Morover.

314
00:23:26,000 --> 00:23:29,840
You can actually purchase a fire hose of news.

315
00:23:29,840 --> 00:23:31,480
They do a really good job, actually.

316
00:23:31,480 --> 00:23:32,480
Oh wow.

317
00:23:32,480 --> 00:23:33,480
Okay.

318
00:23:33,480 --> 00:23:39,920
So you pull all that into your Elasticsearch index and maybe talk a little bit about some

319
00:23:39,920 --> 00:23:44,200
of the underlying NLP bits that are enabling all this.

320
00:23:44,200 --> 00:23:45,200
Yeah.

321
00:23:45,200 --> 00:23:50,080
So when you kick off a query, what's happening is you're making a lot of reading happening.

322
00:23:50,080 --> 00:23:57,240
So for example, if you take a look at the topics that have been generated, text and word

323
00:23:57,240 --> 00:24:03,440
embeddings, quantum, and all of those topic labels that is generated, it actually

324
00:24:03,440 --> 00:24:09,080
discovered and chose those from the content of the articles themselves.

325
00:24:09,080 --> 00:24:18,640
So the first step in any NLP task on documents is to tokenize the entire document.

326
00:24:18,640 --> 00:24:21,400
So are you familiar with tokenizing?

327
00:24:21,400 --> 00:24:22,400
Mm-hmm.

328
00:24:22,400 --> 00:24:23,400
Yeah.

329
00:24:23,400 --> 00:24:28,760
So you basically discover all the words and punctuation and you run an analysis that

330
00:24:28,760 --> 00:24:30,040
gets you the parts of speech.

331
00:24:30,040 --> 00:24:35,200
It's kind of like what you did in grade school when you made the sentence diagrams to try

332
00:24:35,200 --> 00:24:40,800
and make sense of all the different parts of what someone says.

333
00:24:40,800 --> 00:24:44,000
And then a whole bunch of things happen in parallel.

334
00:24:44,000 --> 00:24:51,360
Basically, there's some things that are useful if you give it a bag of words, so you can

335
00:24:51,360 --> 00:24:55,440
take an entire scientific paper or even a thousand scientific paper person.

336
00:24:55,440 --> 00:24:58,080
They turn into bags of words.

337
00:24:58,080 --> 00:25:06,200
And with that kind of analysis, you could, for example, discover the groups of words,

338
00:25:06,200 --> 00:25:13,440
the Ngrams, that basically best describe this space and you can generate a label.

339
00:25:13,440 --> 00:25:20,520
So if you go into any of those topics, it has decided to give that topic a name based

340
00:25:20,520 --> 00:25:25,000
on the language within the documents themselves within the topic.

341
00:25:25,000 --> 00:25:29,920
So I'm still amazed that it works, frankly.

342
00:25:29,920 --> 00:25:33,880
NLP is kind of magical.

343
00:25:33,880 --> 00:25:37,520
When something makes sense to a human, when there's a machine that didn't really understand

344
00:25:37,520 --> 00:25:42,200
it in the same way you did, it's kind of magical.

345
00:25:42,200 --> 00:25:50,560
Are you using kind of off-the-shelf NLP toolkits, NLTK-5000 stuff, or are you rolling your

346
00:25:50,560 --> 00:25:51,560
arms off?

347
00:25:51,560 --> 00:25:54,040
No, we started off that way.

348
00:25:54,040 --> 00:25:59,560
So we've been using this tool Spacey from the very beginning.

349
00:25:59,560 --> 00:26:03,120
It's free, it's open source, and it's really powerful.

350
00:26:03,120 --> 00:26:10,320
And it's really, what shocks me is that there are just two people at the heart of this project,

351
00:26:10,320 --> 00:26:14,720
a fellow named Hannibal and a gal named Enis, who live in Berlin.

352
00:26:14,720 --> 00:26:18,240
Not far from where I lived for a few years, and I've gotten to know them a little bit

353
00:26:18,240 --> 00:26:20,000
just recently.

354
00:26:20,000 --> 00:26:24,640
And it does the nuts and bolts NLP that you need.

355
00:26:24,640 --> 00:26:29,440
So it will tokenize, but it'll also discover named entities.

356
00:26:29,440 --> 00:26:34,440
It'll help you find the people and organizations, and so forth.

357
00:26:34,440 --> 00:26:35,600
But you need to train it.

358
00:26:35,600 --> 00:26:41,040
That's something that we have discovered is just probably like everyone else.

359
00:26:41,040 --> 00:26:45,240
It'll get you started, but then you need to solve your own problems.

360
00:26:45,240 --> 00:26:46,920
It's only a starting point.

361
00:26:46,920 --> 00:26:55,160
So for example, with the people and all the information that we can extract about them

362
00:26:55,160 --> 00:27:01,280
and tell you a story based on the people in this space, Spacey is one of the things

363
00:27:01,280 --> 00:27:07,040
that we use early in the pipeline, but then there's a ton of custom code that we had to

364
00:27:07,040 --> 00:27:15,160
build to basically get the kind of information that Spacey can't get to clean up the stuff

365
00:27:15,160 --> 00:27:22,160
that Spacey gets wrong to link it with all the other information we're extracting by

366
00:27:22,160 --> 00:27:23,960
other means.

367
00:27:23,960 --> 00:27:29,840
And it's a mixture of machine learning and good old fashioned regular expressions.

368
00:27:29,840 --> 00:27:36,000
What I find so fun about being at an AI startup is the goal here is not to generate research

369
00:27:36,000 --> 00:27:37,000
papers.

370
00:27:37,000 --> 00:27:41,640
The goal is to just solve problems really well by whatever means you can.

371
00:27:41,640 --> 00:27:43,960
So which I think is like the right motivation to have.

372
00:27:43,960 --> 00:27:44,960
Right.

373
00:27:44,960 --> 00:27:49,000
You're just motivated to publish cutting-edge papers.

374
00:27:49,000 --> 00:27:52,880
You don't care if it works.

375
00:27:52,880 --> 00:27:58,800
I went to this conference called NIPS, which is essentially where all this cutting-edge

376
00:27:58,800 --> 00:28:00,600
research is being debuted.

377
00:28:00,600 --> 00:28:05,240
And something that really struck me is half the stuff that people are bragging about

378
00:28:05,240 --> 00:28:08,160
doesn't even really practically work.

379
00:28:08,160 --> 00:28:11,560
Our works within such a narrowly constrained way.

380
00:28:11,560 --> 00:28:12,560
Exactly.

381
00:28:12,560 --> 00:28:14,560
With a point problem that'll work.

382
00:28:14,560 --> 00:28:16,680
It's computationally intractable or whatever.

383
00:28:16,680 --> 00:28:18,000
And that's fine.

384
00:28:18,000 --> 00:28:23,400
It's like that's the whole point is to debut tomorrow's technology, but it's frustrating

385
00:28:23,400 --> 00:28:27,480
when you're trying to build something.

386
00:28:27,480 --> 00:28:33,160
You get excited about some new idea and you chase it down, only to discover, oh, this

387
00:28:33,160 --> 00:28:34,680
actually never could have worked.

388
00:28:34,680 --> 00:28:35,680
Yeah.

389
00:28:35,680 --> 00:28:36,680
Yeah.

390
00:28:36,680 --> 00:28:37,680
I've had that experience.

391
00:28:37,680 --> 00:28:38,680
I've had that experience.

392
00:28:38,680 --> 00:28:42,480
I found a paper using primer science, of course.

393
00:28:42,480 --> 00:28:45,800
This is a pretty weird situation to have AI eating itself.

394
00:28:45,800 --> 00:28:50,160
We basically have an AI system that reads AI papers, which we then used to try and improve

395
00:28:50,160 --> 00:28:54,240
the AI that reads papers.

396
00:28:54,240 --> 00:29:00,480
But we came across a really exciting paper and fully replicated it and it just doesn't

397
00:29:00,480 --> 00:29:01,480
work.

398
00:29:01,480 --> 00:29:02,840
And that's okay.

399
00:29:02,840 --> 00:29:04,240
That's how it goes in this space.

400
00:29:04,240 --> 00:29:08,760
When you're right at the edge of knowledge, it's not all going to work.

401
00:29:08,760 --> 00:29:15,200
So we have this principle, a primer, of always trying to find the practical solution as

402
00:29:15,200 --> 00:29:16,720
quickly as possible.

403
00:29:16,720 --> 00:29:21,920
Don't get seduced by ideas that are sexy to talk about, but it's not actually solving

404
00:29:21,920 --> 00:29:22,920
your problem.

405
00:29:22,920 --> 00:29:23,920
Yeah.

406
00:29:23,920 --> 00:29:25,880
I should throw in a plug for my newsletter.

407
00:29:25,880 --> 00:29:33,360
I've recently written on this topic of reproducibility and both science and AI drawing off

408
00:29:33,360 --> 00:29:40,320
of a recent interview I did with Claire Galnick on this same topic.

409
00:29:40,320 --> 00:29:45,680
But I really appreciate you owning up to that broader pipeline.

410
00:29:45,680 --> 00:29:54,600
One of the questions I get a lot when talking with folks about their products or projects

411
00:29:54,600 --> 00:30:02,040
is people want to know like, okay, granted you've applied some great cutting edge machine

412
00:30:02,040 --> 00:30:06,640
learning AI stuff, but what else is there required to make it work?

413
00:30:06,640 --> 00:30:13,880
What are the, how much heuristics are kind of in and around these tools to actually make

414
00:30:13,880 --> 00:30:14,880
it work?

415
00:30:14,880 --> 00:30:22,240
So to hear you note that, yeah, you know, good old regular expressions are used liberally

416
00:30:22,240 --> 00:30:25,680
to make sure that this all works.

417
00:30:25,680 --> 00:30:29,040
I think it's important for, it's important to realize that.

418
00:30:29,040 --> 00:30:31,320
Oh, yeah, absolutely.

419
00:30:31,320 --> 00:30:38,760
I guarantee you, you go into some of the biggest, most cutting edge groups at giant tech

420
00:30:38,760 --> 00:30:39,760
companies.

421
00:30:39,760 --> 00:30:43,760
You think that they're doing some kind of pristine AI that you just press a button and

422
00:30:43,760 --> 00:30:46,000
it understands things.

423
00:30:46,000 --> 00:30:49,760
I guarantee you look under the hood and there's just a ton of regular expressions.

424
00:30:49,760 --> 00:30:54,600
Now, that's not to say that machine learning isn't the way forward.

425
00:30:54,600 --> 00:31:01,000
Like it totally is, but to make these things work on actual problems, it's still labor

426
00:31:01,000 --> 00:31:02,320
of love.

427
00:31:02,320 --> 00:31:03,920
So you're doing a lot with Spacey.

428
00:31:03,920 --> 00:31:12,320
Are you also, which I'm assuming is more traditional NLP technology approach?

429
00:31:12,320 --> 00:31:21,480
Are you also doing things with like word-to-vec and deep learning based approaches?

430
00:31:21,480 --> 00:31:23,040
Yeah.

431
00:31:23,040 --> 00:31:29,440
In particular, as we've expanded into other languages beyond English, Spacey is just

432
00:31:29,440 --> 00:31:36,600
not going to cut it when you want to make something that understands Russian and Chinese.

433
00:31:36,600 --> 00:31:45,440
So we've actually had to pretty much make a bunch of tools from scratch, but it relies

434
00:31:45,440 --> 00:31:54,680
on word vectors and word embeddings and where things get complicated is actually where

435
00:31:54,680 --> 00:31:58,760
you try and pull this all together.

436
00:31:58,760 --> 00:32:10,320
If you use deep learning to extract, for example, some pattern in a corpus of 10,000 documents,

437
00:32:10,320 --> 00:32:15,200
the harder thing, once you've extracted it, is knowing whether you're right and whether

438
00:32:15,200 --> 00:32:17,400
it's worth saying.

439
00:32:17,400 --> 00:32:25,760
I can find a bunch of patterns in text pretty easily, but the harder thing is assessing

440
00:32:25,760 --> 00:32:31,320
how confident am I that I've found something that I haven't just misextracted.

441
00:32:31,320 --> 00:32:32,880
It's not just a serious pattern.

442
00:32:32,880 --> 00:32:37,800
And then even harder than that, is it worth telling you, like, how do I square this with

443
00:32:37,800 --> 00:32:41,000
my model of what humans are interested in?

444
00:32:41,000 --> 00:32:42,000
Right.

445
00:32:42,000 --> 00:32:46,880
Where we're headed with this is basically a model of stories, which ultimately is a model

446
00:32:46,880 --> 00:32:48,280
of humans.

447
00:32:48,280 --> 00:32:49,280
Humans are storytellers.

448
00:32:49,280 --> 00:32:51,600
We've evolved to do this thing.

449
00:32:51,600 --> 00:32:53,080
We just take it for granted.

450
00:32:53,080 --> 00:32:58,320
What we're doing right now, this conversation, is incredibly high tech.

451
00:32:58,320 --> 00:33:02,400
You and I, in real time, are like gliding through a narrative that this is.

452
00:33:02,400 --> 00:33:04,400
Many years of technology evolution.

453
00:33:04,400 --> 00:33:05,400
It's amazing.

454
00:33:05,400 --> 00:33:06,840
Yeah, it's amazing.

455
00:33:06,840 --> 00:33:12,880
So I think this is actually the next frontier of AI, decoding what story is.

456
00:33:12,880 --> 00:33:13,880
Yeah.

457
00:33:13,880 --> 00:33:15,640
So what does that mean practically?

458
00:33:15,640 --> 00:33:18,560
How are you approaching that?

459
00:33:18,560 --> 00:33:27,200
Yeah, so here's a bite-sized example, if you make something that reads scientific papers

460
00:33:27,200 --> 00:33:34,880
and tries to tell you what you need to know about AI research last week, for example.

461
00:33:34,880 --> 00:33:39,080
It's not enough to just give you a dashboard of, here's the most shared paper.

462
00:33:39,080 --> 00:33:42,160
Here's the paper that got the most news.

463
00:33:42,160 --> 00:33:45,360
Here's the paper that currently has the most citations.

464
00:33:45,360 --> 00:33:49,040
That's not doing much heavy lifting for you.

465
00:33:49,040 --> 00:33:54,760
If you were to hire a thousand human analysts to just work for you, like imagine you had

466
00:33:54,760 --> 00:33:59,520
that luxury, what would you ask them to do?

467
00:33:59,520 --> 00:34:06,360
That's kind of the better guiding question and what sort of story would they tell you?

468
00:34:06,360 --> 00:34:07,360
What would the format be?

469
00:34:07,360 --> 00:34:10,800
I guarantee the humans wouldn't come back and give you a dashboard.

470
00:34:10,800 --> 00:34:19,640
They would say, okay, the big deal last week is that a self-driving car crashed and it's

471
00:34:19,640 --> 00:34:26,160
kicked off a huge discussion about quality control and where system errors are going to

472
00:34:26,160 --> 00:34:32,080
creep in and how you can make machine learning systems understandable from an engineering

473
00:34:32,080 --> 00:34:33,080
point of view.

474
00:34:33,080 --> 00:34:36,120
How are we going to deal with this emerging problem?

475
00:34:36,120 --> 00:34:40,760
The people who are weighing in on this are the following researchers in deep learning,

476
00:34:40,760 --> 00:34:45,720
but here's some other people who are very knowledgeable, but they're in a adjacent domain.

477
00:34:45,720 --> 00:34:49,720
We think this is really worth knowing, but meanwhile, by the way, we discovered a paper

478
00:34:49,720 --> 00:34:56,000
published by a couple of researchers that you've rarely heard of, but it's getting a lot

479
00:34:56,000 --> 00:35:01,840
of traction and it seems to be on a topic that is emerging and you're probably going to

480
00:35:01,840 --> 00:35:05,240
care about this.

481
00:35:05,240 --> 00:35:10,520
It's basically, it has to do with voice recognition and we know that that's an interesting topic,

482
00:35:10,520 --> 00:35:15,920
but the more interesting thing is that this researcher is really well known in a totally

483
00:35:15,920 --> 00:35:20,240
different field and is just like diving into this and that's unusual.

484
00:35:20,240 --> 00:35:21,240
So check it out.

485
00:35:21,240 --> 00:35:22,240
Here's the paper.

486
00:35:22,240 --> 00:35:25,600
I'm just going to go out on a limb here and say, you really should read this paper.

487
00:35:25,600 --> 00:35:36,480
By the way, here's basically a new concept that is creeping into the space and we haven't

488
00:35:36,480 --> 00:35:37,640
seen it before.

489
00:35:37,640 --> 00:35:43,120
This might be a fluke, but I think this is actually something that's worth knowing about.

490
00:35:43,120 --> 00:35:45,760
Here are five papers that you should read.

491
00:35:45,760 --> 00:35:48,760
I'm working within your budget here.

492
00:35:48,760 --> 00:35:50,080
That's what all the humans would do.

493
00:35:50,080 --> 00:35:56,360
It's basically the one-to-two-page presidential intelligence briefing.

494
00:35:56,360 --> 00:35:57,960
Ideally, that's what it would look like.

495
00:35:57,960 --> 00:36:04,200
A ton of research has gone into boiling things down to a very tight story and that's all

496
00:36:04,200 --> 00:36:06,640
you need to know.

497
00:36:06,640 --> 00:36:15,000
The idea then is that you've got some kind of generative model for creating these, basically

498
00:36:15,000 --> 00:36:18,840
you're briefing over and it has two steps.

499
00:36:18,840 --> 00:36:21,000
Like at least two steps.

500
00:36:21,000 --> 00:36:26,880
One is, what information can I find that's truly relevant, the raw ingredients of a story?

501
00:36:26,880 --> 00:36:30,720
And then the next step is, well, how can I synthesize this into an actual story?

502
00:36:30,720 --> 00:36:34,560
I have to do text generation, document planning.

503
00:36:34,560 --> 00:36:40,120
You give me a budget, a page, a paragraph, maybe you just want a bullet point and I'll

504
00:36:40,120 --> 00:36:41,120
work with it.

505
00:36:41,120 --> 00:36:46,320
I'll be able to express this as a story given that constraint.

506
00:36:46,320 --> 00:36:54,840
And so kind of going back to our earlier exchange about good old fashion heuristics, how to

507
00:36:54,840 --> 00:36:55,840
what degree?

508
00:36:55,840 --> 00:37:03,680
I haven't looked at compared one of these briefing pages versus another, but how much is

509
00:37:03,680 --> 00:37:10,480
generation and how much is more templates and things like that?

510
00:37:10,480 --> 00:37:20,000
Yeah, so the philosophy we followed is always start fast and doable, put another way.

511
00:37:20,000 --> 00:37:25,360
You always want to start with a model that you can fully understand yourself and implement

512
00:37:25,360 --> 00:37:29,200
quickly so that you have some baseline.

513
00:37:29,200 --> 00:37:30,200
Sure.

514
00:37:30,200 --> 00:37:37,520
Yeah, we've always started with, first can you do it yourself as a human, maybe even

515
00:37:37,520 --> 00:37:40,040
no computer involved.

516
00:37:40,040 --> 00:37:46,760
If you were to read 10 papers and try and say something intelligent about them, for example,

517
00:37:46,760 --> 00:37:54,160
tell me, tell me, for example, what, if you were to classify events, and I gave you a

518
00:37:54,160 --> 00:38:00,840
pile of papers and I said, how would you classify these events, kind of tags would you attach

519
00:38:00,840 --> 00:38:01,840
to them?

520
00:38:01,840 --> 00:38:08,520
Or if you were looking for a particular kind of event, could you divide papers into yes

521
00:38:08,520 --> 00:38:11,040
and no?

522
00:38:11,040 --> 00:38:14,680
Always start with yourself, you the engineer, can you yourself do it?

523
00:38:14,680 --> 00:38:18,040
Because if you can't, you're probably going to have a hard time teaching a computer

524
00:38:18,040 --> 00:38:19,040
do it.

525
00:38:19,040 --> 00:38:24,920
Then if you get some other humans, probably the person just two chairs away from you, if

526
00:38:24,920 --> 00:38:29,680
you can get someone else to do the same task independently and get the same ideally or

527
00:38:29,680 --> 00:38:32,800
a similar answer, okay, now you're in good shape.

528
00:38:32,800 --> 00:38:39,440
Only then do you start building a computational system to try and do this automatically.

529
00:38:39,440 --> 00:38:44,000
Your first stab at that should be something great forward, a set of regular expressions,

530
00:38:44,000 --> 00:38:46,000
heuristics.

531
00:38:46,000 --> 00:38:53,160
Can you actually find this yourself using rules that you yourself devise?

532
00:38:53,160 --> 00:38:58,800
Then the only way really to get beyond that, to really tackle increasing complexity, is

533
00:38:58,800 --> 00:39:01,280
to have something that will learn on its own.

534
00:39:01,280 --> 00:39:06,000
You'll never do that with regular expressions, you have to use machine learning to have

535
00:39:06,000 --> 00:39:11,120
a system find patterns itself in a changing world.

536
00:39:11,120 --> 00:39:17,160
So I think you're saying then that there's, you know, you're somewhere on the spectrum

537
00:39:17,160 --> 00:39:20,360
of templates and machine learning.

538
00:39:20,360 --> 00:39:21,760
Oh yeah, always.

539
00:39:21,760 --> 00:39:28,400
In fact, I think the best things out there are always somewhere in the middle.

540
00:39:28,400 --> 00:39:29,400
Right.

541
00:39:29,400 --> 00:39:30,400
I think by definition.

542
00:39:30,400 --> 00:39:34,480
Yeah, and essentially it becomes a race.

543
00:39:34,480 --> 00:39:42,880
Can we build something that can learn faster and output better, smarter content than the

544
00:39:42,880 --> 00:39:44,440
system we have?

545
00:39:44,440 --> 00:39:51,880
We had a little race actually recently to try and build an event classifier and a brilliant

546
00:39:51,880 --> 00:39:58,640
engineer named Leonard Appleton took a stab at just using regular expressions, no machine

547
00:39:58,640 --> 00:39:59,640
learning.

548
00:39:59,640 --> 00:40:05,360
And another brilliant engineer named Yash took on the task of solving the same problem

549
00:40:05,360 --> 00:40:11,000
using a really complicated machine learning graphical model.

550
00:40:11,000 --> 00:40:15,520
And sometimes John Henry wins the race.

551
00:40:15,520 --> 00:40:22,440
Frankly, Yash could not build a system, at least last I checked, that could do better

552
00:40:22,440 --> 00:40:29,360
than Leonard's massive, complicated, regular expression, heuristic engine.

553
00:40:29,360 --> 00:40:33,440
But you know, eventually, eventually machine learning will win.

554
00:40:33,440 --> 00:40:34,920
Like we all know that.

555
00:40:34,920 --> 00:40:35,920
Right.

556
00:40:35,920 --> 00:40:36,920
Right.

557
00:40:36,920 --> 00:40:39,960
But that's the beauty of a practical approach.

558
00:40:39,960 --> 00:40:45,200
When you're really driven by practical principles, you're willing to say, well, we've got

559
00:40:45,200 --> 00:40:49,120
a better solution that's actually simpler and easier to understand.

560
00:40:49,120 --> 00:40:51,160
Let's use that for now.

561
00:40:51,160 --> 00:40:52,480
Keep trying.

562
00:40:52,480 --> 00:40:57,400
But it's never long before a machine learning based system does better.

563
00:40:57,400 --> 00:41:00,080
It's just an incredibly powerful tool.

564
00:41:00,080 --> 00:41:06,840
When you're using machine learning for tasks like summarization where you referenced earlier,

565
00:41:06,840 --> 00:41:11,560
you know, first you do it, then you get someone else to do it and you compare them.

566
00:41:11,560 --> 00:41:17,000
You know, your summary of a given paper or a given paragraph is likely to be very different

567
00:41:17,000 --> 00:41:18,000
from mine.

568
00:41:18,000 --> 00:41:23,720
Like what do you, how do you find ground truth so that you can train learning models?

569
00:41:23,720 --> 00:41:29,280
Yeah, that you've really put your finger on the hardest problem.

570
00:41:29,280 --> 00:41:34,160
Stories by their nature can be told infinite ways.

571
00:41:34,160 --> 00:41:39,800
And there are some automated techniques that have been around for a decade.

572
00:41:39,800 --> 00:41:41,200
They have French color names.

573
00:41:41,200 --> 00:41:44,720
I don't know how that came about, but there's something called russian, something called

574
00:41:44,720 --> 00:41:46,080
blue.

575
00:41:46,080 --> 00:41:52,760
And what they do is they treat the output as bag of word problems.

576
00:41:52,760 --> 00:41:55,640
And they try and find out how much information overlap.

577
00:41:55,640 --> 00:42:00,200
There is between a human summary and a computer summary.

578
00:42:00,200 --> 00:42:04,440
As you can imagine, that's great if you're trying to measure whether you got it terribly

579
00:42:04,440 --> 00:42:05,440
wrong.

580
00:42:05,440 --> 00:42:06,440
Right.

581
00:42:06,440 --> 00:42:10,680
If we make two summaries and they have not going to do with each other, then they're probably

582
00:42:10,680 --> 00:42:13,000
they're probably not talking about the same thing.

583
00:42:13,000 --> 00:42:14,000
That may be.

584
00:42:14,000 --> 00:42:15,000
That's right.

585
00:42:15,000 --> 00:42:21,200
I mean, summarizing fiction, right, you could be we could be summarizing on two totally

586
00:42:21,200 --> 00:42:23,160
different levels and both be right.

587
00:42:23,160 --> 00:42:24,160
That's true.

588
00:42:24,160 --> 00:42:25,160
That's absolutely true.

589
00:42:25,160 --> 00:42:29,400
I mean, the same, I think the same holds true for news.

590
00:42:29,400 --> 00:42:35,160
I mean, you know, I, I'll let you continue, but that seems like a very, very rudimentary

591
00:42:35,160 --> 00:42:36,160
metric.

592
00:42:36,160 --> 00:42:41,360
Well, I mean, you'd be surprised then to learn that the latest, greatest papers in this

593
00:42:41,360 --> 00:42:45,840
field are still using those metrics because they're easy.

594
00:42:45,840 --> 00:42:49,760
You know, you can, it's, it's a one click measurement, right?

595
00:42:49,760 --> 00:42:56,320
But it, it, it really doesn't help when you want to assess a subtle output of a story

596
00:42:56,320 --> 00:42:59,400
that could be sliced and diced sort of infinite ways.

597
00:42:59,400 --> 00:43:01,120
Unfortunately, it becomes a capture.

598
00:43:01,120 --> 00:43:06,560
You need some human to read it and go, oh, yeah, that makes sense or that's crazy, right?

599
00:43:06,560 --> 00:43:13,480
But there are, there are some techniques you can use, so one is you can actually crowd

600
00:43:13,480 --> 00:43:21,760
source assessment of narrative, you can, you can give human annotators and scorers a

601
00:43:21,760 --> 00:43:23,360
system, a rigorous system.

602
00:43:23,360 --> 00:43:25,880
So like, you can measure the coherence.

603
00:43:25,880 --> 00:43:33,640
You can measure the, the sophistication, the, whether or not you've, you've really summarized

604
00:43:33,640 --> 00:43:36,040
the space well in various ways.

605
00:43:36,040 --> 00:43:39,960
Those, those sounds like they would require a fairly sophisticated crowdsource.

606
00:43:39,960 --> 00:43:40,960
Yeah.

607
00:43:40,960 --> 00:43:46,400
Yeah, so that's right, like the, the more technical and sophisticated this task becomes,

608
00:43:46,400 --> 00:43:49,000
the less you can rely on mechanical Turk.

609
00:43:49,000 --> 00:43:52,520
Uh, in fact, eventually you've got your own engineers doing this.

610
00:43:52,520 --> 00:43:55,800
So it's, it's definitely not scalable.

611
00:43:55,800 --> 00:44:02,800
Um, but, uh, there are, uh, there are some, some tricks that you can use.

612
00:44:02,800 --> 00:44:09,840
So for example, um, if I generate a bunch of summaries, uh, on a topic that I've already

613
00:44:09,840 --> 00:44:14,880
summarized, for example, if I have a Wikipedia article about it, I can at least find out

614
00:44:14,880 --> 00:44:20,120
if the most important entities in the narrative have been represented.

615
00:44:20,120 --> 00:44:25,720
And I can also, uh, turn the system around and do extraction on the summary.

616
00:44:25,720 --> 00:44:31,720
You can even, I will suggest to make a generative adversarial network that generates stories

617
00:44:31,720 --> 00:44:33,640
and critiques them.

618
00:44:33,640 --> 00:44:35,160
You can see where this is going.

619
00:44:35,160 --> 00:44:41,520
Uh, eventually you can, you can have a system that, uh, tries to check off all the boxes

620
00:44:41,520 --> 00:44:45,800
of what counts as a good story, like you've, you've talked about the most important entities

621
00:44:45,800 --> 00:44:50,440
and you've expressed their relationships, uh, you've come in under budget in terms of,

622
00:44:50,440 --> 00:44:56,920
uh, space on the age, um, but ultimately you're going to need a human to assess whether

623
00:44:56,920 --> 00:45:04,440
it's a well-written story until we can crack the code of text style transfer, um, where

624
00:45:04,440 --> 00:45:08,920
you can actually say, tell me the story and the style of a New York Times reporter or

625
00:45:08,920 --> 00:45:14,800
tell me the story in the, in the style of, uh, um, you know, a, uh, terse military briefing

626
00:45:14,800 --> 00:45:15,800
time.

627
00:45:15,800 --> 00:45:18,640
Send on my text in, uh, Hemingway style.

628
00:45:18,640 --> 00:45:19,640
Exactly.

629
00:45:19,640 --> 00:45:26,120
Until we can actually have, uh, networks that can both detect and reproduce, uh, narrative

630
00:45:26,120 --> 00:45:27,120
style.

631
00:45:27,120 --> 00:45:31,440
Um, I think we're, we're, for the time being stuck in a world where it's really hard

632
00:45:31,440 --> 00:45:34,560
to assess how well our systems are doing.

633
00:45:34,560 --> 00:45:40,240
Um, ultimately you want to hook this up to your, your users and, and either passively

634
00:45:40,240 --> 00:45:43,000
or actively harvest their feedback.

635
00:45:43,000 --> 00:45:47,040
So, um, the simplest version of this, of course, is A-B testing.

636
00:45:47,040 --> 00:45:52,720
If you write many versions of a summary, um, and you expose a large number of humans

637
00:45:52,720 --> 00:45:57,800
to A versus B, you can just find out, uh, what they think of it by, for example, whether

638
00:45:57,800 --> 00:45:59,840
they click through and read it.

639
00:45:59,840 --> 00:46:01,840
You can, you can also make it active.

640
00:46:01,840 --> 00:46:05,080
You can let users say, yeah, that was good or that was bad.

641
00:46:05,080 --> 00:46:11,520
We're going back to my, uh, Hemingway text summaries, uh, Google inbox presenting you

642
00:46:11,520 --> 00:46:16,880
three choices for how to summarize the response, the appropriate response to an email.

643
00:46:16,880 --> 00:46:18,360
Yep.

644
00:46:18,360 --> 00:46:19,680
And we've played with that as well.

645
00:46:19,680 --> 00:46:26,720
We, we generate alternative summaries, uh, to events, for example, um, it's, it's, it's

646
00:46:26,720 --> 00:46:32,160
a really powerful way of real time effortless quality checking.

647
00:46:32,160 --> 00:46:35,880
You don't, you don't want to have to sort of pause your whole engineering operation in

648
00:46:35,880 --> 00:46:38,680
order all the time, just to assess how well you're doing.

649
00:46:38,680 --> 00:46:40,920
You really want it to be sort of continual.

650
00:46:40,920 --> 00:46:45,880
You want to always be reading the output of your own, uh, computational systems.

651
00:46:45,880 --> 00:46:46,880
We call it dog fooding.

652
00:46:46,880 --> 00:46:47,880
Mm-hmm.

653
00:46:47,880 --> 00:46:49,840
If you, you got to be real time dog fooding.

654
00:46:49,840 --> 00:46:55,440
And so the nice thing about primary science is this thing that I'm building is we, uh,

655
00:46:55,440 --> 00:47:00,640
we use it to discover the research that is going to help us make it better.

656
00:47:00,640 --> 00:47:01,640
Right.

657
00:47:01,640 --> 00:47:06,160
And so if you keep on using the thing, you are your own quality assessor.

658
00:47:06,160 --> 00:47:07,160
That really helps.

659
00:47:07,160 --> 00:47:08,160
Right.

660
00:47:08,160 --> 00:47:09,160
Right.

661
00:47:09,160 --> 00:47:10,160
Right.

662
00:47:10,160 --> 00:47:11,160
But hard to scale.

663
00:47:11,160 --> 00:47:14,680
I, I wish I could clone myself in some way to, uh, assess sort of, uh, at a thousand

664
00:47:14,680 --> 00:47:15,680
X.

665
00:47:15,680 --> 00:47:22,760
Now one thing that I didn't see in what you've built it, it seems like it is, it does

666
00:47:22,760 --> 00:47:30,560
a really good job at kind of this meta characterization of archive and what's happening

667
00:47:30,560 --> 00:47:32,720
in, in different categories.

668
00:47:32,720 --> 00:47:37,400
But I didn't see it attempting to summarize individual papers, which is the thing that

669
00:47:37,400 --> 00:47:39,960
Jeff Dean and I were originally talking about.

670
00:47:39,960 --> 00:47:42,160
Is it trying to do that somewhere?

671
00:47:42,160 --> 00:47:43,520
Not in what you're looking at.

672
00:47:43,520 --> 00:47:47,040
We are, we are actually working on that, that summarization problem.

673
00:47:47,040 --> 00:47:48,040
Okay.

674
00:47:48,040 --> 00:47:49,040
Um, yeah.

675
00:47:49,040 --> 00:47:54,040
We've taken two strategies, uh, and they're kind of running, uh, in parallel.

676
00:47:54,040 --> 00:48:00,760
One is extractive summarization, where you, you, you, you're, the system is allowed to

677
00:48:00,760 --> 00:48:06,320
pull words and even whole sentences directly from the text and then kind of pull them together

678
00:48:06,320 --> 00:48:07,400
into a summary.

679
00:48:07,400 --> 00:48:12,200
That works extremely well when you have a large number of docs, like if you, if you have

680
00:48:12,200 --> 00:48:17,360
a hundred documents all on all about the same thing, uh, extractive summarizations really

681
00:48:17,360 --> 00:48:18,560
powerful.

682
00:48:18,560 --> 00:48:23,920
And really efficient, uh, and then the alternative is abstractive summarization, where the system

683
00:48:23,920 --> 00:48:28,640
is going to write its own words, often character by character, uh, out of thin air and it has

684
00:48:28,640 --> 00:48:29,640
a language model.

685
00:48:29,640 --> 00:48:35,360
So it reads all these things and it, it basically makes a prediction about, uh, what it should

686
00:48:35,360 --> 00:48:41,000
say next as it generates a summary, um, a, a really nice, uh, bit of progress in this

687
00:48:41,000 --> 00:48:45,800
field that we've been using is abstractive summarization with pointers.

688
00:48:45,800 --> 00:48:51,560
So the idea here is you also have a sense of your confidence about whether the word or

689
00:48:51,560 --> 00:48:58,080
phrase that you're putting, uh, into the summary at any given time is going to be a good choice.

690
00:48:58,080 --> 00:49:02,520
And if you're not so confident, you point back to the text and you grab the thing itself.

691
00:49:02,520 --> 00:49:09,760
So for example, if you had a sentence that said, um, uh, a, um, one of the most exciting

692
00:49:09,760 --> 00:49:15,640
areas of, uh, artificial intelligence these days is, um, generate, generative adversarial

693
00:49:15,640 --> 00:49:16,640
networks.

694
00:49:16,640 --> 00:49:20,640
Now, if, if, if generative adversarial networks, that phrase is something that you have

695
00:49:20,640 --> 00:49:25,960
an encountered or your model basically says, I'm, I'm not sure if that, if I can actually

696
00:49:25,960 --> 00:49:29,680
paraphrase that, then what you want to do is what a good human writer would do.

697
00:49:29,680 --> 00:49:31,960
You just go back and you grab that thing.

698
00:49:31,960 --> 00:49:40,720
So you can, you can summarize while also having some of the advantages of extractive.

699
00:49:40,720 --> 00:49:47,680
So summarizing basically around the, the entities that you aren't too sure about.

700
00:49:47,680 --> 00:49:48,680
Exactly.

701
00:49:48,680 --> 00:49:52,200
It basically becomes a sliding scale between abstractive and extractive.

702
00:49:52,200 --> 00:49:56,040
The more confident it gets, the more abstractive it gets, the more flexible it gets, which

703
00:49:56,040 --> 00:50:01,920
will allow you to summarize a single scientific paper, for example, uh, in a couple of sentences.

704
00:50:01,920 --> 00:50:07,520
Um, and if you're not so sure, then it slides over to extractive and it will just pull

705
00:50:07,520 --> 00:50:13,480
out the sentences that it deem and the phrases that it deems are the most central and informative.

706
00:50:13,480 --> 00:50:14,480
Interesting.

707
00:50:14,480 --> 00:50:16,400
It's a hard problem though.

708
00:50:16,400 --> 00:50:17,400
It's a really hard problem.

709
00:50:17,400 --> 00:50:21,680
Another thing, uh, that makes it hard when it comes to scientific papers is they already

710
00:50:21,680 --> 00:50:23,600
have their own summaries.

711
00:50:23,600 --> 00:50:28,800
They're called abstracts and you, and you'd, and you'd think that, oh, great, this, it,

712
00:50:28,800 --> 00:50:34,920
job done, but, uh, apps, as you know, abstracts themselves can be so riddled with jargon and

713
00:50:34,920 --> 00:50:38,920
references to arcane things that it's hardly a summary at all.

714
00:50:38,920 --> 00:50:42,760
It's really only a summary for the authors of the paper, right?

715
00:50:42,760 --> 00:50:43,760
Right.

716
00:50:43,760 --> 00:50:46,960
So you really need a summary of the summary, right?

717
00:50:46,960 --> 00:50:48,440
And that's what we're working on.

718
00:50:48,440 --> 00:50:52,040
We're finding that you really do need to power this with an ontology and a knowledge base

719
00:50:52,040 --> 00:50:53,040
though.

720
00:50:53,040 --> 00:50:54,440
A library on that.

721
00:50:54,440 --> 00:50:55,440
Okay.

722
00:50:55,440 --> 00:51:00,640
So let's take, for example, a problem that I'm just starting to work on.

723
00:51:00,640 --> 00:51:07,440
How do you, how do you summarize and make sense of, uh, pharmaceutical research papers?

724
00:51:07,440 --> 00:51:15,160
So, uh, there is an ontology that is available to everyone, uh, that basically the NIH, um,

725
00:51:15,160 --> 00:51:17,680
paid for called MASH.

726
00:51:17,680 --> 00:51:25,640
And, um, it's, it's, it's kind of like, uh, every, every jargon term in, in, uh, biochemistry

727
00:51:25,640 --> 00:51:31,560
and molecular biology, gene names and gene types, uh, all of that is captured in this very

728
00:51:31,560 --> 00:51:40,160
rich ontology that was hand-built by, no doubt, by, uh, un thanked graduate students.

729
00:51:40,160 --> 00:51:45,280
And something that's really nice about MASH is that it's actually a subset of wiki data.

730
00:51:45,280 --> 00:51:51,600
And wiki data is the database, uh, that stands behind wikipedia.

731
00:51:51,600 --> 00:51:56,840
Um, now I, I say that, uh, in an idealistic way because actually, in real, that's the way

732
00:51:56,840 --> 00:51:57,840
it was dreamed up.

733
00:51:57,840 --> 00:52:01,120
Oh, wiki data is going to basically be the database that powers wikipedia.

734
00:52:01,120 --> 00:52:03,960
But in fact, um, it's not there yet.

735
00:52:03,960 --> 00:52:11,440
Humans, humans, um, vastly prefer to, uh, update wikipedia with content and wiki data basically

736
00:52:11,440 --> 00:52:12,440
place catch up.

737
00:52:12,440 --> 00:52:19,600
Nonetheless, it is a huge powerful, uh, open source knowledge base, uh, and the MASH ontology

738
00:52:19,600 --> 00:52:21,640
is a subset of it.

739
00:52:21,640 --> 00:52:28,480
And so, um, if you want to summarize a scientific paper, just a single scientific paper, the

740
00:52:28,480 --> 00:52:31,400
first thing you need to do is, is make sense of it.

741
00:52:31,400 --> 00:52:35,960
You need to map all of those words, which to the computer or just, it could be random

742
00:52:35,960 --> 00:52:39,200
numbers for all they cares has no idea what it means.

743
00:52:39,200 --> 00:52:44,200
We need to map them to concepts and that's what systems like MASH were designed to help

744
00:52:44,200 --> 00:52:45,440
us do.

745
00:52:45,440 --> 00:52:52,000
So the idea of being instead of what you're doing in science, primer, uh, and doing this

746
00:52:52,000 --> 00:52:57,080
in a totally unsupervised manner, here you're using the additional information you're getting

747
00:52:57,080 --> 00:53:05,560
from the, uh, pre-existing ontology to help the machine make sense of the various documents

748
00:53:05,560 --> 00:53:06,920
and to paraphrase it.

749
00:53:06,920 --> 00:53:14,920
So like a good summary is something that doesn't just like say less, uh, it also says, just

750
00:53:14,920 --> 00:53:19,160
as much but in a, in a compressed way, right, right, you know, if I just tell you the beginning

751
00:53:19,160 --> 00:53:24,160
of a story, I haven't really compressed that story for you, um, I need to like give you

752
00:53:24,160 --> 00:53:29,040
the sense of the beginning, middle and end and compress that all down into three sentences.

753
00:53:29,040 --> 00:53:34,920
And, um, you're not going to be able to do that just using a, uh, the standard NLP techniques

754
00:53:34,920 --> 00:53:37,520
on a scientific paper, you're just not going to be able to do it.

755
00:53:37,520 --> 00:53:38,360
No way.

756
00:53:38,360 --> 00:53:39,360
Right.

757
00:53:39,360 --> 00:53:44,960
You have to, uh, map that out to an ontology and say, oh, you know, this long sentence describing

758
00:53:44,960 --> 00:53:50,480
this, uh, genetic pathway, I can boil that down to a single sentence that says, um, the

759
00:53:50,480 --> 00:53:54,760
genetic pathway X, you know, interesting.

760
00:53:54,760 --> 00:53:58,640
But yeah, you, you need a lot of tacit knowledge to be able to do that.

761
00:53:58,640 --> 00:54:01,200
So that's what we're working on.

762
00:54:01,200 --> 00:54:02,200
Awesome.

763
00:54:02,200 --> 00:54:03,400
Well, John, this has been super interesting.

764
00:54:03,400 --> 00:54:05,880
I really appreciate you taking the time.

765
00:54:05,880 --> 00:54:06,880
Thank you.

766
00:54:06,880 --> 00:54:09,440
Anything else you'd like to share with the audience?

767
00:54:09,440 --> 00:54:14,080
Oh, just, uh, that, uh, I'd like to make a prediction.

768
00:54:14,080 --> 00:54:15,080
Go ahead.

769
00:54:15,080 --> 00:54:25,680
Well, I predict that the kind of stuff we're working on is going to accelerate artificial

770
00:54:25,680 --> 00:54:27,760
intelligence research more than anything else.

771
00:54:27,760 --> 00:54:34,680
I think building AI that can read the latest research on AI and help the engineers who

772
00:54:34,680 --> 00:54:42,280
build it, build it faster is going to, uh, vastly accelerate the whole process.

773
00:54:42,280 --> 00:54:43,280
Awesome.

774
00:54:43,280 --> 00:54:51,520
Well, we will put your prediction on the blockchain and, uh, just to make sure we get all the

775
00:54:51,520 --> 00:54:52,520
jargon in.

776
00:54:52,520 --> 00:54:53,520
Uh, exactly.

777
00:54:53,520 --> 00:54:54,520
Then we'll do an ICU.

778
00:54:54,520 --> 00:54:59,520
Awesome, thanks so much, John.

779
00:54:59,520 --> 00:55:00,520
Thanks, Sam.

780
00:55:00,520 --> 00:55:08,600
All right, everyone, that's our show for today.

781
00:55:08,600 --> 00:55:13,560
For more information on John or any of the topics covered in this episode, head on over

782
00:55:13,560 --> 00:55:19,360
to twimmaleye.com slash talk slash one, three, six.

783
00:55:19,360 --> 00:55:26,360
Thanks so much for listening and catch you next time.

