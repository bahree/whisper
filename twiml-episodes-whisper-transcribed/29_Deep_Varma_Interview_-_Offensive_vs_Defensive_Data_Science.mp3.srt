1
00:00:00,000 --> 00:00:15,380
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,380 --> 00:00:20,300
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,300 --> 00:00:23,700
I'm your host Sam Charrington.

4
00:00:23,700 --> 00:00:28,720
Last week was a big week for the podcast, I announced the first anniversary of the show.

5
00:00:28,720 --> 00:00:33,080
This week I want to start the show by thanking everyone who's participated in our first

6
00:00:33,080 --> 00:00:35,360
anniversary contest.

7
00:00:35,360 --> 00:00:40,360
We asked you to comment on the show notes page or post an iTunes review and wow did you

8
00:00:40,360 --> 00:00:41,360
deliver?

9
00:00:41,360 --> 00:00:45,360
Your stories have been personal, thoughtful and downright encouraging.

10
00:00:45,360 --> 00:00:50,720
I've got a couple that I'd like to share and really it's so hard just to pick a couple

11
00:00:50,720 --> 00:00:56,400
of these but first Andrew posted on the show notes page.

12
00:00:56,400 --> 00:01:00,880
I've used this podcast to maintain a pulse on current ML and AI.

13
00:01:00,880 --> 00:01:05,800
Big thank you for both helping me in my day to day but also getting me interested in ML

14
00:01:05,800 --> 00:01:08,320
in general when I first started listening.

15
00:01:08,320 --> 00:01:13,560
Heck, I've been listening to this podcast when I was a student, then when I was an intern

16
00:01:13,560 --> 00:01:18,440
and then when I was an entry level analyst and then when I was promoted to analyst and

17
00:01:18,440 --> 00:01:25,280
now as a senior analyst where I am now is in no small part thanks to this podcast.

18
00:01:25,280 --> 00:01:30,600
Wow, that's a ton of ground to have covered in just a year, Andrew, congrats.

19
00:01:30,600 --> 00:01:34,840
We are so proud to have been a small part of your success.

20
00:01:34,840 --> 00:01:37,960
This next one has a bit of a backstory.

21
00:01:37,960 --> 00:01:46,320
Bill B123 posted a five star review on iTunes titled sold on the deep interview format.

22
00:01:46,320 --> 00:01:51,160
Bill said new interview style format was not initially good.

23
00:01:51,160 --> 00:01:55,200
I gave it two stars initially later raised it to four stars.

24
00:01:55,200 --> 00:01:59,880
Now I'm sold great podcast with tons of insights and learning.

25
00:01:59,880 --> 00:02:02,960
Keep up the great work Sam.

26
00:02:02,960 --> 00:02:08,640
Now I remember when Bill's first review hit the decision to switch to the interview format

27
00:02:08,640 --> 00:02:14,520
was really tough for me, but I knew it was necessary for my efforts to be sustainable.

28
00:02:14,520 --> 00:02:17,360
Bill's initial two star review really, really hurt.

29
00:02:17,360 --> 00:02:22,920
I think at the time him or another user said the interviews were just a bit too fluffy

30
00:02:22,920 --> 00:02:25,960
and that I was like the Tim Ferriss of AI.

31
00:02:25,960 --> 00:02:30,040
I wasn't really sure what to do with that because I kind of like Tim Ferriss's podcast,

32
00:02:30,040 --> 00:02:35,440
but I really took that to heart and I was pumped when Bill raised his review to four stars.

33
00:02:35,440 --> 00:02:39,880
And now I'm super excited to see that I've earned Bill's fifth star.

34
00:02:39,880 --> 00:02:43,040
Thanks so much Bill.

35
00:02:43,040 --> 00:02:47,600
Starting from nothing I never imagined this podcast would begin to blossom into such an awesome

36
00:02:47,600 --> 00:02:49,200
community of users.

37
00:02:49,200 --> 00:02:56,080
I say begin because we still have so much ground to cover and we are truly just getting started.

38
00:02:56,080 --> 00:03:01,400
For those who have not yet had a chance to enter the contest, please visit twimmolai.com

39
00:03:01,400 --> 00:03:04,240
slash birthday for more info.

40
00:03:04,240 --> 00:03:10,680
Don't forget first prize gets a bronze pass to the O'Reilly AI conference this June,

41
00:03:10,680 --> 00:03:13,840
which is an $1,800 value.

42
00:03:13,840 --> 00:03:19,360
One prize gets a Google home powered by AI, of course, and everyone that participates

43
00:03:19,360 --> 00:03:22,240
gets a couple of Twimmolaptop stickers.

44
00:03:22,240 --> 00:03:27,120
The contest ends June 1st and winners will be announced on the second.

45
00:03:27,120 --> 00:03:32,200
If you've posted a review on iTunes to enter the contest, please reach out to us at team

46
00:03:32,200 --> 00:03:36,080
at twimmolai.com to let us know who you are.

47
00:03:36,080 --> 00:03:40,520
All right, this week on the show, my guest is Deep Varma.

48
00:03:40,520 --> 00:03:45,280
This President of Data Engineering at Real Estate Startup Trulia.

49
00:03:45,280 --> 00:03:50,480
Deep has run data engineering teams in Silicon Valley for well over a decade, and he's now

50
00:03:50,480 --> 00:03:56,000
responsible for the engineering efforts supporting Trulia's big data technology platform, which

51
00:03:56,000 --> 00:04:02,200
encompasses everything from data acquisition and management to data science and algorithms.

52
00:04:02,200 --> 00:04:07,200
In the show, we discuss all of that with an emphasis on Trulia's data engineering pipeline

53
00:04:07,200 --> 00:04:12,040
and their personalization platform, as well as how they use computer vision, deep learning,

54
00:04:12,040 --> 00:04:15,520
and natural language generation to deliver their product.

55
00:04:15,520 --> 00:04:20,720
Along the way, Deep offers great insights into what he calls offensive versus defensive

56
00:04:20,720 --> 00:04:26,440
data science and the difference between data driven decision making versus products.

57
00:04:26,440 --> 00:04:40,200
Another great interview, and I'm sure you'll enjoy it, and now on to the show.

58
00:04:40,200 --> 00:04:41,200
All right.

59
00:04:41,200 --> 00:04:42,200
Hey, everyone.

60
00:04:42,200 --> 00:04:43,680
I am on the line with Deep Varma.

61
00:04:43,680 --> 00:04:49,520
Deep is Vice President of Data Engineering with Trulia, and I'm excited to have Deep join

62
00:04:49,520 --> 00:04:50,520
us.

63
00:04:50,520 --> 00:04:52,000
Deep, how are you doing today?

64
00:04:52,000 --> 00:04:57,680
I'm doing great. It's not that hot. The California and San Francisco have been hot for the last

65
00:04:57,680 --> 00:05:03,120
few days, but seems like the fog is coming back, so I'm definitely doing amazing.

66
00:05:03,120 --> 00:05:04,440
How are you, Sam?

67
00:05:04,440 --> 00:05:05,440
Nice, nice.

68
00:05:05,440 --> 00:05:09,680
Well, I'm doing very well, and I'm really looking forward to our conversation and to learning

69
00:05:09,680 --> 00:05:14,960
a little bit more about how you guys use data at Trulia.

70
00:05:14,960 --> 00:05:20,680
Why don't we get started by having you talk a little bit about your background and how

71
00:05:20,680 --> 00:05:22,680
you got into working with data?

72
00:05:22,680 --> 00:05:23,680
Yeah.

73
00:05:23,680 --> 00:05:30,240
I think you asked me a great question, because when I go and I speak in some of the schools

74
00:05:30,240 --> 00:05:36,280
to help undergrads or the grads, those who are doing the management, and one of the guys

75
00:05:36,280 --> 00:05:41,680
I think few months back in Berkeley asked me the similar kind of a question that Deep

76
00:05:41,680 --> 00:05:50,080
why data, and I think Sam, it goes back to my reasoning, my mindset from the childhood

77
00:05:50,080 --> 00:05:57,360
where I was always looking into the reasons why this is this, why this is this, and when

78
00:05:57,360 --> 00:06:04,520
I get into my master's and computer science, I still remember there were old databases

79
00:06:04,520 --> 00:06:11,120
which some of, like you may know and you may not know, it's the DBs or the Fox base.

80
00:06:11,120 --> 00:06:18,240
Those were the early versions or the manifestations of the structured databases coming in, and

81
00:06:18,240 --> 00:06:25,680
I was always been very interested and then entering into my first job in IBM, it's where

82
00:06:25,680 --> 00:06:32,000
we are working on those XML directs change, how we are going to have the directs change

83
00:06:32,000 --> 00:06:37,320
between one entity and another entity, how the web services discovery locator are going

84
00:06:37,320 --> 00:06:39,560
to come into the picture.

85
00:06:39,560 --> 00:06:45,560
So early on the foundation was where I've been from the get go from my own personal desire

86
00:06:45,560 --> 00:06:51,800
to look into the answers as well as exposure to the early technologies.

87
00:06:51,800 --> 00:07:00,120
Get me into the databases and entering into then my journey where exploring why and what

88
00:07:00,120 --> 00:07:08,240
I realized is at the end of the day, we are always surrounded by data and the data doesn't

89
00:07:08,240 --> 00:07:14,200
mean that it has to be a textual data, the data is how we interact with each other when

90
00:07:14,200 --> 00:07:21,000
we are making phone calls to the people, when I'm searching for something, and that's

91
00:07:21,000 --> 00:07:30,200
I think it's 2001, 2002 time frame was started getting into my DNA that, you know, my God,

92
00:07:30,200 --> 00:07:37,320
you know, every day when I were I interact with anyone, anything I do, it's the data.

93
00:07:37,320 --> 00:07:40,160
And this is where I got into Yahoo.

94
00:07:40,160 --> 00:07:48,320
So then that was a step, you know, we're getting into Yahoo, helping advertisers and publishers,

95
00:07:48,320 --> 00:07:53,840
you know, try to render good quality ads to the consumers.

96
00:07:53,840 --> 00:08:00,120
This is all again, the platform is, you know, how you understand your consumers better.

97
00:08:00,120 --> 00:08:05,800
And then going into my startups and, you know, looking into the data again, where we are

98
00:08:05,800 --> 00:08:10,280
looking into, you know, how the data floating from one system to other system, what predictions

99
00:08:10,280 --> 00:08:11,800
we can do.

100
00:08:11,800 --> 00:08:18,080
So in nutshell, I will say it is, this is how I got into the data and I think is for me,

101
00:08:18,080 --> 00:08:23,440
my behavior, sometimes I'm at a point, Sam, I will tell you, when I go home, my wife

102
00:08:23,440 --> 00:08:28,840
have to remind me, honey, you're back home, don't think from a data point of view, just

103
00:08:28,840 --> 00:08:34,120
think, you know, you're back home, data reasoning is not going to work here.

104
00:08:34,120 --> 00:08:35,120
That's funny.

105
00:08:35,120 --> 00:08:36,120
That's funny.

106
00:08:36,120 --> 00:08:43,000
I did my, some of my grad school work on queuing theory and my wife is so tired of me

107
00:08:43,000 --> 00:08:48,880
analyzing lines and queuing scenarios and banks and grocery stores and trying to tell

108
00:08:48,880 --> 00:08:50,840
her which line she should be in.

109
00:08:50,840 --> 00:08:52,640
So I definitely relate to that.

110
00:08:52,640 --> 00:08:53,640
Yeah.

111
00:08:53,640 --> 00:08:55,880
I see you were in Yahoo back in the glory days.

112
00:08:55,880 --> 00:09:02,280
Oh, you trust me, you know, those were the glory days and I still miss those days because,

113
00:09:02,280 --> 00:09:07,640
you know, Yahoo was the center of attraction and the talent was huge there.

114
00:09:07,640 --> 00:09:16,400
So I worked there for four years and, you know, unfortunately, Yahoo is no longer Yahoo.

115
00:09:16,400 --> 00:09:18,600
But it was amazing.

116
00:09:18,600 --> 00:09:19,600
Absolutely.

117
00:09:19,600 --> 00:09:26,000
Were you involved in, did you use sedupe or were you involved in kind of the development

118
00:09:26,000 --> 00:09:28,000
and advancement of the dupe at that time?

119
00:09:28,000 --> 00:09:29,000
Yes.

120
00:09:29,000 --> 00:09:36,080
So when I went in, we were trying to, so Yahoo bought this company over here and we were

121
00:09:36,080 --> 00:09:44,520
trying to integrate Yahoo's like a platform back and platform to get the search keywords.

122
00:09:44,520 --> 00:09:51,600
So this is where Hadoop pipelines were integral part of the data flow between the systems

123
00:09:51,600 --> 00:09:57,920
that how we get the data from our Pasadena based company and then into our system.

124
00:09:57,920 --> 00:10:03,560
So yeah, I was not deeply like I was not part of the Hadoop ecosystem, but I was one of

125
00:10:03,560 --> 00:10:08,000
the consumers of the Hadoop system to get the data I'm floating around.

126
00:10:08,000 --> 00:10:09,000
Okay.

127
00:10:09,000 --> 00:10:10,000
Okay.

128
00:10:10,000 --> 00:10:15,920
And then now at, at Trulia, tell me a little bit about your role.

129
00:10:15,920 --> 00:10:22,640
It sounds like at least from LinkedIn that you've got a pretty broad set of responsibilities

130
00:10:22,640 --> 00:10:28,280
spanning everything from kind of your data platform, you know, I'm sure there's some Hadoop

131
00:10:28,280 --> 00:10:34,200
ecosystem, something in there somewhere to, you know, the data science and the applications

132
00:10:34,200 --> 00:10:35,200
that run on top of it.

133
00:10:35,200 --> 00:10:36,200
Is that right?

134
00:10:36,200 --> 00:10:37,200
That's fair.

135
00:10:37,200 --> 00:10:40,280
And let me walk you through first why Trulia?

136
00:10:40,280 --> 00:10:47,960
I think that's to me is the biggest piece which inspired me to join Trulia and do you mind

137
00:10:47,960 --> 00:10:50,480
Sam if I ask you?

138
00:10:50,480 --> 00:10:54,440
Do you rent a home or do you have your own home?

139
00:10:54,440 --> 00:10:55,440
Yes.

140
00:10:55,440 --> 00:10:56,440
Awesome.

141
00:10:56,440 --> 00:10:58,840
I'm pretty sure you're going to relate to this story.

142
00:10:58,840 --> 00:11:07,160
So this is way back in 1999 when we decided to buy our first home, you know, it is me and

143
00:11:07,160 --> 00:11:13,400
my wife, you know, the data was there, but the data was in storage like I have to go

144
00:11:13,400 --> 00:11:19,280
into police stations, I have to go into counties, I have to go into those areas to collect

145
00:11:19,280 --> 00:11:25,320
the data, we take this data, then me and my wife sit together, we go through the listings,

146
00:11:25,320 --> 00:11:30,080
we look into the neighborhood, we used to maintain our excel sheet, oh, let's look

147
00:11:30,080 --> 00:11:37,160
into this listing and it was an cumbersome process and it was an emotional journey for

148
00:11:37,160 --> 00:11:43,360
us to go through this exercise and it took us months to buy our first home and we did

149
00:11:43,360 --> 00:11:45,280
it finally, right?

150
00:11:45,280 --> 00:11:52,600
And that inspired me to join Trulia because, you know, when you think it's, how can we

151
00:11:52,600 --> 00:11:58,560
use this data when I, when I was in my early conversations with Trulia, that was the biggest

152
00:11:58,560 --> 00:12:04,000
thing for me is, you know, how I'm going to come and join Trulia and make an impact to

153
00:12:04,000 --> 00:12:10,040
build Trulia has more of a data driven product company and first thing in my mind was the

154
00:12:10,040 --> 00:12:17,200
use case of me 1999 buying a home that deep, can I make four millions of consumer that

155
00:12:17,200 --> 00:12:23,160
journey much more meaningful, much more enjoyable by using this data?

156
00:12:23,160 --> 00:12:28,280
So that's how my journey began with Trulia.

157
00:12:28,280 --> 00:12:35,080
Now just to go a little bit more detail into my role in Trulia, I think it's, you know,

158
00:12:35,080 --> 00:12:42,000
Trulia is, you know, our number one goal and this is where our founders, they saw a huge

159
00:12:42,000 --> 00:12:47,520
opportunity to change this marketplace by providing information and insights to our

160
00:12:47,520 --> 00:12:53,120
consumers to help them make the right decision and, you know, and make this journey home

161
00:12:53,120 --> 00:12:56,440
search journey easy and enjoyable.

162
00:12:56,440 --> 00:13:02,560
So, you know, with that mindset, with that goal, what our founder set in fourth, we,

163
00:13:02,560 --> 00:13:08,440
we continue my goal was to continue this and provide amazing experiences to our consumers

164
00:13:08,440 --> 00:13:15,640
and we are investing a lot in our personalization, big data, machine learning platforms to help

165
00:13:15,640 --> 00:13:22,720
consumers like me to, you know, find their perfect home in much more efficient and better

166
00:13:22,720 --> 00:13:25,720
way than I did, you know, years back.

167
00:13:25,720 --> 00:13:32,880
So that's in not show what role is and I'm happy to dive into more details about those

168
00:13:32,880 --> 00:13:35,800
technologies, what I'm talking about.

169
00:13:35,800 --> 00:13:42,400
Why don't we start with you talking a little bit about the data products and what are,

170
00:13:42,400 --> 00:13:49,680
from a consumer experience perspective, how do, how is, you know, machine learning

171
00:13:49,680 --> 00:13:54,360
in AI and the various data products that you create on your team?

172
00:13:54,360 --> 00:13:57,560
How is that surface to the truly a user?

173
00:13:57,560 --> 00:13:58,560
Yeah.

174
00:13:58,560 --> 00:14:05,960
So I think, you know, it's, first of all, data driven product companies, you know, Sam,

175
00:14:05,960 --> 00:14:12,160
there is a big mindset needs to happen and I'm just, you know, three years back when

176
00:14:12,160 --> 00:14:19,400
I joined Trulia, my philosophy was always being to transform and use this data more on their

177
00:14:19,400 --> 00:14:25,920
offensive strategy rather than defensive strategy and I'm going to go into answer your question

178
00:14:25,920 --> 00:14:29,840
but I just wanted to give you a little bit more details because, you know, when you think

179
00:14:29,840 --> 00:14:34,480
about the data driven companies, there are two aspects of the data driven company.

180
00:14:34,480 --> 00:14:39,120
One is the data driven decision making and other is the data driven product and the

181
00:14:39,120 --> 00:14:44,320
decision making is more your product analytics, you know, where you launch a feature and then

182
00:14:44,320 --> 00:14:48,280
you, oh, is this feature working or this feature is not working?

183
00:14:48,280 --> 00:14:55,400
But my one goal is to transform this data driven decision making more from a defensive to

184
00:14:55,400 --> 00:14:58,800
offensive by saying, is this feature going to work?

185
00:14:58,800 --> 00:15:02,960
So that's the one component and the second component, which is the discussion we are having

186
00:15:02,960 --> 00:15:10,320
today is around the data driven product company and this is where the way it surfaces

187
00:15:10,320 --> 00:15:15,520
to our consumers, our average consumer have no idea when they come to Trulia, when they

188
00:15:15,520 --> 00:15:23,080
engage with Trulia via mobile web app or our browser or desktop applications.

189
00:15:23,080 --> 00:15:25,880
They don't know that we use the data.

190
00:15:25,880 --> 00:15:30,240
It is basically pretty much embedded in their user experience.

191
00:15:30,240 --> 00:15:34,960
It is embedded when they explore, when they start their search journey.

192
00:15:34,960 --> 00:15:41,800
It is our responsibility to understand their behavior, what they're looking into and what

193
00:15:41,800 --> 00:15:49,160
we have done Sam is we have built an underlying personalization platform first and so think

194
00:15:49,160 --> 00:15:58,480
about that as our foundation and you know, this is where we have our consumers unique preferences,

195
00:15:58,480 --> 00:16:03,920
search criteria and you know, what they're looking into like deep is looking into quiet

196
00:16:03,920 --> 00:16:09,320
neighborhood, good school district in mission district of San Francisco, that's the personalization

197
00:16:09,320 --> 00:16:11,360
platform.

198
00:16:11,360 --> 00:16:17,080
On top of it, we have our machine learning pillars and there are many pillars what we have

199
00:16:17,080 --> 00:16:22,040
invested in machine learning, the first one is our computer vision and deep learning,

200
00:16:22,040 --> 00:16:28,920
the second one is our recommender engine, the third one is our user engagement models

201
00:16:28,920 --> 00:16:34,440
and the fourth one is our natural language processing or the natural language generation.

202
00:16:34,440 --> 00:16:40,960
So these are the machine learning pillars what we have and the, you know, and then we

203
00:16:40,960 --> 00:16:47,240
use all those pieces in tandem like machine learning pillars, personalization platform,

204
00:16:47,240 --> 00:16:54,160
all together to give that experience to our consumers when, when you come to our side

205
00:16:54,160 --> 00:16:59,200
and you look into, you know, the photos, when you look into the, when you receive an

206
00:16:59,200 --> 00:17:05,640
email from Julia, when you receive a push notification from Julia, all this is part

207
00:17:05,640 --> 00:17:12,000
of our machine learning technologies, which the goal is to engage our consumers and give

208
00:17:12,000 --> 00:17:20,320
them much more relevance experience during their stay with Julia and I will go definitely

209
00:17:20,320 --> 00:17:21,320
this conversation.

210
00:17:21,320 --> 00:17:24,920
I will give you more details around each and every component what we're going to talk.

211
00:17:24,920 --> 00:17:28,920
But if you have any question, I'm happy to ask address that first.

212
00:17:28,920 --> 00:17:29,920
Okay.

213
00:17:29,920 --> 00:17:39,080
There's, there's just so much in there to dig into, I like the, I like the distinction

214
00:17:39,080 --> 00:17:46,040
between the decision making versus the products and you mentioned specifically also this kind

215
00:17:46,040 --> 00:17:49,200
of dichotomy between offensive and defensive.

216
00:17:49,200 --> 00:17:51,000
Can you elaborate on that a bit more?

217
00:17:51,000 --> 00:17:52,000
Yeah.

218
00:17:52,000 --> 00:17:53,000
Yeah, definitely.

219
00:17:53,000 --> 00:18:01,720
I think is being in the Silicon Valley for close to two decades now and I have seen startup

220
00:18:01,720 --> 00:18:08,080
companies coming up and, you know, their focus is mostly around building the products.

221
00:18:08,080 --> 00:18:11,920
There are companies like Google's and Facebook's are definitely, you know, those who are more

222
00:18:11,920 --> 00:18:17,880
data driven, but I have seen early on when the company start building the products, their

223
00:18:17,880 --> 00:18:23,920
focus is never been the data side, they have couple of analytics who are staying behind

224
00:18:23,920 --> 00:18:24,920
the scene.

225
00:18:24,920 --> 00:18:27,240
Oh, should I change my pricing?

226
00:18:27,240 --> 00:18:30,840
Should I make my pricing for this consumer do this?

227
00:18:30,840 --> 00:18:36,640
It's always an after five, six years, if they have to, you know, raise more funds or

228
00:18:36,640 --> 00:18:41,560
they're about to go public or when they go public, now the mindset changes, oh, what is

229
00:18:41,560 --> 00:18:43,280
my differentiator?

230
00:18:43,280 --> 00:18:45,680
How I'm going to differentiate my product?

231
00:18:45,680 --> 00:18:48,080
How I'm going to bring my consumers back?

232
00:18:48,080 --> 00:18:51,040
How I'm going to engage my consumers?

233
00:18:51,040 --> 00:18:58,120
And this is where the offensive strategy comes into a picture that why not we have the

234
00:18:58,120 --> 00:19:02,640
companies start thinking about the data from the get go?

235
00:19:02,640 --> 00:19:04,040
What data you're collecting?

236
00:19:04,040 --> 00:19:05,520
How do you build?

237
00:19:05,520 --> 00:19:11,680
And I think that's a struggle and that struggle brings it to the point where companies have

238
00:19:11,680 --> 00:19:18,840
to go back and reinvest their resources, their millions of dollars to rebuild their architecture

239
00:19:18,840 --> 00:19:23,680
because if you think about the data, Sam, at the end of the day, this conversation what

240
00:19:23,680 --> 00:19:31,000
you and me are having is so much we are talking about, you know, artificial intelligence, machine

241
00:19:31,000 --> 00:19:36,720
learning, we are collecting this data and we are compacting into a podcast, but these

242
00:19:36,720 --> 00:19:38,040
are the signals, right?

243
00:19:38,040 --> 00:19:43,760
So there is a quality of the data, integrity of the data, how do we take all those things

244
00:19:43,760 --> 00:19:50,760
and bundle up so that the organizations are thinking about changing the direction from

245
00:19:50,760 --> 00:19:56,520
the get go rather than after the fact and after years thinking about.

246
00:19:56,520 --> 00:20:02,480
So that's the way, you know, I think and I think I wanted to differentiate between the

247
00:20:02,480 --> 00:20:06,680
two aspects which I talked earlier and I just want to make sure that both you and me

248
00:20:06,680 --> 00:20:07,680
are on the same page.

249
00:20:07,680 --> 00:20:13,760
One is the decision making and other is the product building, both of those facets requires

250
00:20:13,760 --> 00:20:22,400
the data, decision making is our amazing analytics teams rather than them working on the data

251
00:20:22,400 --> 00:20:24,440
and saying, is it working?

252
00:20:24,440 --> 00:20:28,080
I want to transform that to, is it going to work?

253
00:20:28,080 --> 00:20:32,280
That's the big differentiation and the product what we talked about, you know, that's the

254
00:20:32,280 --> 00:20:35,680
second facet of it, does it make sense so far?

255
00:20:35,680 --> 00:20:44,560
Yeah, no, it does make sense and, you know, I think the transformation that you describe

256
00:20:44,560 --> 00:20:50,080
is kind of going, you know, maybe it's a different cut at that defensive versus offensive

257
00:20:50,080 --> 00:20:58,720
and a lot in another way or put another way, you're trying to get teams to stop building,

258
00:20:58,720 --> 00:21:03,600
you know, rear view mirror analytics and start, you know, building analytics that predicts

259
00:21:03,600 --> 00:21:06,400
what's, you know, going to be happening in front of the windshield.

260
00:21:06,400 --> 00:21:09,800
You go, I think you nailed it better than me, right?

261
00:21:09,800 --> 00:21:13,200
It says the right way to explain it, right?

262
00:21:13,200 --> 00:21:18,960
Yeah, and I think that that is, you know, that transformation is something that's happening

263
00:21:18,960 --> 00:21:25,800
very broadly in, you know, industry, not just in technology companies, but also in enterprises

264
00:21:25,800 --> 00:21:32,160
and it's, you know, that need to look out the windshield and not be stuck, you know, reading

265
00:21:32,160 --> 00:21:38,280
reports that took weeks to create, that reflect the previous quarter and aren't really even

266
00:21:38,280 --> 00:21:39,280
relevant anymore.

267
00:21:39,280 --> 00:21:44,960
I think that's why, you know, enterprises are kind of grasping onto, you know, machine

268
00:21:44,960 --> 00:21:50,200
learning and AI-based solutions as a way to kind of give them that forward-looking view.

269
00:21:50,200 --> 00:21:55,200
Yeah, and I will add one more thing Sam here, so also, you know, when you think about

270
00:21:55,200 --> 00:22:01,400
any enterprise, any startup, any technology company, at the end of the day, you know, all

271
00:22:01,400 --> 00:22:06,280
the work is done by the people and you have the limited resources.

272
00:22:06,280 --> 00:22:11,360
And you know, you are building a product, rather build a product which is going to work

273
00:22:11,360 --> 00:22:12,360
in the marketplace.

274
00:22:12,360 --> 00:22:19,160
Like, no one has this magic wand to say this is going to work, but this, this front,

275
00:22:19,160 --> 00:22:24,320
you know, this offensive strategy or, you know, whatever way we want to say, it helps

276
00:22:24,320 --> 00:22:30,920
us to align our resources in the right direction, too, so that we can change the direction of

277
00:22:30,920 --> 00:22:36,960
our ship going in the true north rather than, you know, going the south direction and

278
00:22:36,960 --> 00:22:38,880
then bring it back.

279
00:22:38,880 --> 00:22:39,880
Right.

280
00:22:39,880 --> 00:22:40,880
Right.

281
00:22:40,880 --> 00:22:46,680
Before we dive into the platform that you built, you know, one thing does strike me is

282
00:22:46,680 --> 00:22:54,160
that, you know, perhaps more than some other companies truly is, you know, truly is product.

283
00:22:54,160 --> 00:22:56,720
This offering is data, right?

284
00:22:56,720 --> 00:23:01,640
I'm not making some assumptions, but I'm assuming that you're, you know, sourcing, you

285
00:23:01,640 --> 00:23:06,040
know, a bunch of different fees, and you even describe some of these, you know, you're

286
00:23:06,040 --> 00:23:11,920
MLS listings, your county, you know, data feeds, maybe pulling in from good schools and

287
00:23:11,920 --> 00:23:17,280
other sites that are producing aggregate data on, you know, schools and crime and all

288
00:23:17,280 --> 00:23:18,280
these things.

289
00:23:18,280 --> 00:23:25,080
Like, your fundamental data is so fundamental to the thing, the things that you do.

290
00:23:25,080 --> 00:23:29,080
Before you even get to how do you kind of, what have you built and what have you learned

291
00:23:29,080 --> 00:23:38,720
about aggregating all of this data and a little bit of a, a little bit of context for this.

292
00:23:38,720 --> 00:23:46,200
I often hear or, you know, I recently produced an event called the Future of Data Summit

293
00:23:46,200 --> 00:23:50,560
and we had speakers talking about, you know, different aspects of AI and several of them

294
00:23:50,560 --> 00:23:55,720
got up and said, you know, well, in order to do, you know, machine learning and AI, you

295
00:23:55,720 --> 00:23:58,760
have to have the data.

296
00:23:58,760 --> 00:24:03,760
And I think that's true, but, you know, it kind of glosses over the fact that sometimes

297
00:24:03,760 --> 00:24:07,800
you have to get the data, not just like, it's not just sitting there waiting to be explored.

298
00:24:07,800 --> 00:24:09,120
You have to go and find it.

299
00:24:09,120 --> 00:24:12,280
And it seems like a lot of what you did is, is going to find it.

300
00:24:12,280 --> 00:24:17,600
And so, you know, how did you, you know, to what extent does your team get involved in

301
00:24:17,600 --> 00:24:21,280
that and what's your platform for enabling that?

302
00:24:21,280 --> 00:24:22,280
Yeah.

303
00:24:22,280 --> 00:24:26,600
I'm so glad talking to you, right, because you are nailing down exactly the points where

304
00:24:26,600 --> 00:24:28,440
I'm passionate about.

305
00:24:28,440 --> 00:24:33,000
So when you, when you think about there are two pieces to the data and I'm going to make

306
00:24:33,000 --> 00:24:39,920
it very simple, first to start with, like when someone goes to Google and they search

307
00:24:39,920 --> 00:24:44,880
on a Google, the first thing what they're doing is they are giving the search engine

308
00:24:44,880 --> 00:24:48,560
on their intent, what I'm searching for.

309
00:24:48,560 --> 00:24:55,440
And then Google has this content, which is, they went ahead by crawlers and all those

310
00:24:55,440 --> 00:24:58,960
things by building the relevancy and all those things.

311
00:24:58,960 --> 00:25:07,760
So consumer gives the intent, Google has this massive databases of the content and then

312
00:25:07,760 --> 00:25:13,760
the magic in the middle, which takes the intent and content and matches up, which we call

313
00:25:13,760 --> 00:25:18,960
as a relevancy and give it to the consumer, where consumer feels happy about it, right?

314
00:25:18,960 --> 00:25:19,960
Right.

315
00:25:19,960 --> 00:25:24,200
Now in the same context, Julia also had the two parts of the data.

316
00:25:24,200 --> 00:25:28,880
One is the consumer for whom we are building this product.

317
00:25:28,880 --> 00:25:33,120
And then the content where we get this content from.

318
00:25:33,120 --> 00:25:37,080
And I think, you know, this is the listings as you talked about.

319
00:25:37,080 --> 00:25:40,400
This is the public records, which you talked about.

320
00:25:40,400 --> 00:25:45,520
Now, schools data, the crime data, you know, the commute data.

321
00:25:45,520 --> 00:25:55,080
I think it is, that's the difference between 1999 and 2017, where we have the technologies

322
00:25:55,080 --> 00:26:00,560
like real time messaging systems like Kafka, we have strong topologies or the streaming

323
00:26:00,560 --> 00:26:02,080
systems.

324
00:26:02,080 --> 00:26:08,520
We have those Hadoop or Spark technologies where we can make it easier to ingest those

325
00:26:08,520 --> 00:26:10,720
data into our system.

326
00:26:10,720 --> 00:26:16,680
So we have, and this data is pretty open, right, I have written on my blog, roughly we

327
00:26:16,680 --> 00:26:22,000
have 2.5 million active four cell listings on our system.

328
00:26:22,000 --> 00:26:28,680
So across US, you have agents, those who are working with consumers to sell their home,

329
00:26:28,680 --> 00:26:35,280
they enter this information into analysis, how this data comes in from analysis to our

330
00:26:35,280 --> 00:26:40,800
system, then you know, when you sell your home, when you buy your home, you pay your

331
00:26:40,800 --> 00:26:46,960
taxes, you have these assessments and the taxes, which are going into the counties, how

332
00:26:46,960 --> 00:26:48,720
we get this data.

333
00:26:48,720 --> 00:26:54,760
So I think my team involves at the end of the day, whatever you see on Julia's side,

334
00:26:54,760 --> 00:27:03,320
it is my team's responsibility to use technologies to bring this data in the raw form first.

335
00:27:03,320 --> 00:27:10,040
And then enrich this data, because when you think about, you know, you are MLS 1 and

336
00:27:10,040 --> 00:27:18,040
you will come and say, 1, 2, 3, 4, first street and you can spell first street as FIRSD

337
00:27:18,040 --> 00:27:21,360
or someone just come and write number 1 first street.

338
00:27:21,360 --> 00:27:28,840
So we have to have this magic in the middle to join all this data and say this data is

339
00:27:28,840 --> 00:27:30,960
for this property.

340
00:27:30,960 --> 00:27:36,680
And then once we know that, you know, the geolocation, when we had the address cleansing,

341
00:27:36,680 --> 00:27:43,760
address normalization, and then when we work on the enrichment piece, enrichment pieces

342
00:27:43,760 --> 00:27:50,160
for this listing, this is the historical information about the property, this is when

343
00:27:50,160 --> 00:27:57,160
the property was sold last time, this is the Texas information, this listing is 2 minutes

344
00:27:57,160 --> 00:28:01,080
away from the public transit system, this is the school.

345
00:28:01,080 --> 00:28:04,560
And then we go through this enrichment process.

346
00:28:04,560 --> 00:28:09,880
Once we had that enrichment process, it goes into our indexes, which is, you know, we

347
00:28:09,880 --> 00:28:17,320
use our solar technology and we have built, you know, our API layers on top of it, which

348
00:28:17,320 --> 00:28:25,040
can take up to 10 to 15,000 requests per second to serve our front end technologies like

349
00:28:25,040 --> 00:28:30,680
web apps and, you know, mobile web or whatever it is.

350
00:28:30,680 --> 00:28:35,440
So what I explained it to you on surface, it looks like a big process, which takes days

351
00:28:35,440 --> 00:28:41,240
and days and days, interestingly enough, when the listing hits the marketplace, by the

352
00:28:41,240 --> 00:28:47,840
time it goes from one point with the enrichment to the front end, it is less than 15 minutes

353
00:28:47,840 --> 00:28:49,560
where we show the data.

354
00:28:49,560 --> 00:28:56,480
So this is all because of the technologies what we have enables us to give this content

355
00:28:56,480 --> 00:28:58,520
to consumers faster.

356
00:28:58,520 --> 00:29:04,320
So I just talked about the content piece, which is, you know, all the data flowing around

357
00:29:04,320 --> 00:29:09,640
and I think most likely, when you're ready, we will jump into the intent piece, which

358
00:29:09,640 --> 00:29:13,520
is the personalization and all of us, but does it make sense so far?

359
00:29:13,520 --> 00:29:14,520
It does.

360
00:29:14,520 --> 00:29:17,720
It does and I still have tons of questions on that content side.

361
00:29:17,720 --> 00:29:28,960
Um, so thinking about the various ways that you likely get data, I'm imagining maybe

362
00:29:28,960 --> 00:29:34,200
three and I'm there probably many more, but I'm imagining, you know, some data is coming

363
00:29:34,200 --> 00:29:36,440
you via feeds.

364
00:29:36,440 --> 00:29:43,000
Maybe this is like the listing, some data is coming you, coming to you via streams and

365
00:29:43,000 --> 00:29:51,440
some data is coming to you via, um, in batches, um, like, can you characterize like how much

366
00:29:51,440 --> 00:29:55,760
of the data is each and are there cat, is there a category that I'm missing in, in this

367
00:29:55,760 --> 00:30:00,960
and, um, in, and then like where do you, you know, where do you land it?

368
00:30:00,960 --> 00:30:06,160
How much of, you know, what you're doing is, you know, real time, stream based kind

369
00:30:06,160 --> 00:30:08,480
of, uh, processing?

370
00:30:08,480 --> 00:30:14,760
Um, yeah, so I think it's, it depends upon the set of the data, like, so when you think

371
00:30:14,760 --> 00:30:19,960
about the listings, listings are majority of our listings are the stream based, which

372
00:30:19,960 --> 00:30:24,960
are real time, because you know, listings, here's the market, but then when you think about

373
00:30:24,960 --> 00:30:33,560
the public records, which is these assessments, Texas information, that is mostly the batch

374
00:30:33,560 --> 00:30:39,720
based, what we have, and then you have the school data, which is, you know, not, it's not

375
00:30:39,720 --> 00:30:42,800
changing on a daily basis, that's a feed based.

376
00:30:42,800 --> 00:30:47,160
Then you have a crime data, which is, you know, more the streaming thing.

377
00:30:47,160 --> 00:30:53,200
So I think it is, it depends upon the data set, what we, so we have the technologies where

378
00:30:53,200 --> 00:31:01,280
we define, if the data needs to be refreshed more frequently, we use the streaming technologies

379
00:31:01,280 --> 00:31:05,760
and otherwise, you know, we use batch based systems.

380
00:31:05,760 --> 00:31:11,640
We have invested in building our, some of the systems uses the Lambda technologies.

381
00:31:11,640 --> 00:31:16,200
So this is, you know, the real time plus the batch base on an ugly basis, we run the

382
00:31:16,200 --> 00:31:21,120
full Lambda and then make sure that there is, you know, the accuracy on the quality of

383
00:31:21,120 --> 00:31:24,160
the data being implemented.

384
00:31:24,160 --> 00:31:26,400
There are some places we also use Kapa.

385
00:31:26,400 --> 00:31:28,440
I don't know if you heard about Kapa.

386
00:31:28,440 --> 00:31:32,240
So Kapa is also, you know, the real time, but the batch base.

387
00:31:32,240 --> 00:31:36,760
So I think it's at the end of the day, my team have built those pipelines.

388
00:31:36,760 --> 00:31:44,560
Some pipeline uses, you know, the Kafka messages to strong typologies, the streaming technologies.

389
00:31:44,560 --> 00:31:50,600
Some places we have the spark where we need the data to be processed much more faster.

390
00:31:50,600 --> 00:31:58,400
So I think it is at the different data set, at the different refresh SLA and based on those

391
00:31:58,400 --> 00:32:04,840
refreshing SLAs, we tend to, you know, bring it to our systems.

392
00:32:04,840 --> 00:32:10,840
And before we move on, why don't you give us a brief overview of Lambda architectures

393
00:32:10,840 --> 00:32:13,920
and you mentioned Kapa as well.

394
00:32:13,920 --> 00:32:14,920
Yeah, yeah.

395
00:32:14,920 --> 00:32:19,520
And you mentioned strom as well, this is streaming and having come across those.

396
00:32:19,520 --> 00:32:20,520
Yeah.

397
00:32:20,520 --> 00:32:23,080
So I think let's start with the streaming, right?

398
00:32:23,080 --> 00:32:27,120
So I think what's happening is the streaming, the real time streaming and the processing.

399
00:32:27,120 --> 00:32:34,320
So, you know, when we have those messages, so think about if there are 2.5 million active

400
00:32:34,320 --> 00:32:41,560
listings are there across, you know, US, when they hit our system, they're coming into

401
00:32:41,560 --> 00:32:45,760
our messaging layer and there are different messaging technologies are there.

402
00:32:45,760 --> 00:32:49,480
We are using Kafka, Kinesis, mix of that.

403
00:32:49,480 --> 00:32:56,020
From there, we move into the streaming and, you know, the streaming can be spark or it

404
00:32:56,020 --> 00:32:57,920
can be a strom topology.

405
00:32:57,920 --> 00:33:02,720
There is a place where we use strom topologies because when the listing hits, remember

406
00:33:02,720 --> 00:33:08,360
early on I was telling you that we need to do geo cleanse address cleansing, address

407
00:33:08,360 --> 00:33:12,520
normalization and then, you know, the enrichment.

408
00:33:12,520 --> 00:33:20,080
So this is where the goals that we had the spouts and the goals of our strom topology where

409
00:33:20,080 --> 00:33:26,200
spouts are the when which is ingesting the data and then, you know, the goals are the

410
00:33:26,200 --> 00:33:32,680
one which are making the decision making, you know, let I have to perform step xyz.

411
00:33:32,680 --> 00:33:39,640
So the strom topology helps us in the real time, take the stream of the data, perform

412
00:33:39,640 --> 00:33:47,920
the enrichment, perform the cleansing and then go and persist it into our new messaging

413
00:33:47,920 --> 00:33:51,480
layer from the data can be sent over.

414
00:33:51,480 --> 00:33:54,600
So that's the strom topology.

415
00:33:54,600 --> 00:34:01,400
Lambda is, you know, Lambda is being there in the marketplace for long and what Lambda

416
00:34:01,400 --> 00:34:06,560
means is, you know, look at the on a daily basis when we are getting millions and millions

417
00:34:06,560 --> 00:34:11,880
of messages and, you know, it comes into our system.

418
00:34:11,880 --> 00:34:17,120
It is very important for us to maintain the accuracy and the quality of the data.

419
00:34:17,120 --> 00:34:25,760
So on a nightly basis, we rerun the whole data set what we have collected on this day

420
00:34:25,760 --> 00:34:32,240
just to make sure that if there are gaps, we fill those gaps using the Lambda architecture

421
00:34:32,240 --> 00:34:37,480
and which will give us the higher level of accuracy and the quality of the data coming

422
00:34:37,480 --> 00:34:39,400
into our system.

423
00:34:39,400 --> 00:34:46,200
So the only difference is in, in Lambda, you have to write your code base differently

424
00:34:46,200 --> 00:34:48,440
to consume the batch base.

425
00:34:48,440 --> 00:34:53,560
But when you move into the Kapa architecture, you don't need to write the separate code

426
00:34:53,560 --> 00:34:54,560
base.

427
00:34:54,560 --> 00:34:59,280
You can have the similar code base which is used during the real time streaming and you

428
00:34:59,280 --> 00:35:04,240
can use the same code base for your batch based typologies also.

429
00:35:04,240 --> 00:35:06,640
So Kapa enables you to do that.

430
00:35:06,640 --> 00:35:12,480
So that's how we use those three technologies which I just talked about.

431
00:35:12,480 --> 00:35:13,480
Okay.

432
00:35:13,480 --> 00:35:14,480
All right.

433
00:35:14,480 --> 00:35:15,480
Great.

434
00:35:15,480 --> 00:35:18,680
We'll include some links to these and the shun outs.

435
00:35:18,680 --> 00:35:23,960
I've come across Lambda architecture before, but Kapa architecture is new to me.

436
00:35:23,960 --> 00:35:29,240
Yeah, it is coming pretty new in the marketplace last couple of years.

437
00:35:29,240 --> 00:35:32,000
I see people using it move.

438
00:35:32,000 --> 00:35:33,000
Okay.

439
00:35:33,000 --> 00:35:34,000
Great.

440
00:35:34,000 --> 00:35:43,480
So you've ingested all of this data and you've used technologies like Lambda and Kapa architecture,

441
00:35:43,480 --> 00:35:49,640
it's Apache Storm and other technologies, Kafka, Q's and messaging and all these things

442
00:35:49,640 --> 00:35:53,840
to get all this data in to enrich it.

443
00:35:53,840 --> 00:35:58,360
And where do you, where do you land it?

444
00:35:58,360 --> 00:36:02,160
So if your question on the landing is, where do we persist it?

445
00:36:02,160 --> 00:36:03,160
Right.

446
00:36:03,160 --> 00:36:04,160
Yeah.

447
00:36:04,160 --> 00:36:07,160
So we persisted in our solar index.

448
00:36:07,160 --> 00:36:10,560
So solar is the search technology.

449
00:36:10,560 --> 00:36:16,600
And when you think about, you know, we have this millions and millions of rows coming in.

450
00:36:16,600 --> 00:36:24,480
Do we need to, if there are 100 attributes in a row, do we need to persist everything

451
00:36:24,480 --> 00:36:25,480
in the solar?

452
00:36:25,480 --> 00:36:31,400
So we basically, all the searchable things we stored in our solar and then the things

453
00:36:31,400 --> 00:36:37,480
which are not searchable, which are just an augmentation of the data can go into any

454
00:36:37,480 --> 00:36:44,000
of the no SQL databases or some places where we feel the data is much more structured and

455
00:36:44,000 --> 00:36:49,440
we don't need, we use the relational databases also where the volume is pretty small.

456
00:36:49,440 --> 00:36:53,880
So we use my SQL and that's how we persist.

457
00:36:53,880 --> 00:37:04,600
So we use, you know, solar, HBase, Redis, DynamoDB, MySQL and I'm pretty sure, you know,

458
00:37:04,600 --> 00:37:10,040
Aerospike is another one which we recently started using the key value pair systems.

459
00:37:10,040 --> 00:37:15,800
So we use a very wide variety of the databases here in Trulia.

460
00:37:15,800 --> 00:37:20,240
And again, it all boils down to the use cases where do we need to store what?

461
00:37:20,240 --> 00:37:21,240
Right.

462
00:37:21,240 --> 00:37:22,240
Right.

463
00:37:22,240 --> 00:37:27,040
So the individual teams that are working on, you know, given products that are surfaced

464
00:37:27,040 --> 00:37:30,640
through the site and they choose whatever data store makes the most sense for their use

465
00:37:30,640 --> 00:37:31,640
cases.

466
00:37:31,640 --> 00:37:32,640
Is that right?

467
00:37:32,640 --> 00:37:33,640
Yes.

468
00:37:33,640 --> 00:37:34,640
Yes and no.

469
00:37:34,640 --> 00:37:35,640
So right.

470
00:37:35,640 --> 00:37:42,760
So for example, you know, if we know the latency is a big thing for us, then storing that

471
00:37:42,760 --> 00:37:45,440
in HBase may not make sense.

472
00:37:45,440 --> 00:37:51,480
So people may decide to use, you know, Redis or they may go with the DynamoDB.

473
00:37:51,480 --> 00:37:58,000
So I think we, we have the some guidelines around like the biggest thing for us is build

474
00:37:58,000 --> 00:38:03,600
the databases and see the latency, right, because we have the API.

475
00:38:03,600 --> 00:38:10,760
We have abstracted all the data as an API layer on top of those systems so that when

476
00:38:10,760 --> 00:38:17,600
front and team comes and says, give me all the data for this listing, then this API goes

477
00:38:17,600 --> 00:38:22,960
across the different systems or the databases to bring the data, stitch it together.

478
00:38:22,960 --> 00:38:25,520
So latency plays a major role.

479
00:38:25,520 --> 00:38:31,280
But to some extent, what you were trying to say, yes, the decentralization of teams definitely

480
00:38:31,280 --> 00:38:37,080
enable us to have teams pick the technologies, what they want to pick.

481
00:38:37,080 --> 00:38:41,560
We don't put so many guidelines except for the latency as one of the prerequisite making

482
00:38:41,560 --> 00:38:44,040
sure that we pick the right technologies.

483
00:38:44,040 --> 00:38:45,040
Okay.

484
00:38:45,040 --> 00:38:46,040
Okay.

485
00:38:46,040 --> 00:38:48,400
All right, so then all that in place.

486
00:38:48,400 --> 00:38:54,840
Let's jump into the personalization platform and the stuff that you're doing on top of

487
00:38:54,840 --> 00:38:55,840
it.

488
00:38:55,840 --> 00:38:56,840
Here you go.

489
00:38:56,840 --> 00:38:57,840
That's the fun piece, right?

490
00:38:57,840 --> 00:38:58,840
It's a, right.

491
00:38:58,840 --> 00:39:01,040
I really love that piece, I think.

492
00:39:01,040 --> 00:39:07,040
Now we, what we talked about in last few minutes was mostly around the content, right?

493
00:39:07,040 --> 00:39:11,000
So now we need to start thinking from an intent point of use.

494
00:39:11,000 --> 00:39:18,560
When consumers, they come to Trulia, you know, when they are interacting with our website

495
00:39:18,560 --> 00:39:27,040
or mobile app or mobile web or email at any given moment of time, what we have seen,

496
00:39:27,040 --> 00:39:29,920
our consumers are generating those signals.

497
00:39:29,920 --> 00:39:36,120
And the signals are nothing but their intent, you know, deep is looking into, you know,

498
00:39:36,120 --> 00:39:43,440
a listing in no value of a San Francisco, which is in quite neighborhood.

499
00:39:43,440 --> 00:39:44,440
That's a signal.

500
00:39:44,440 --> 00:39:49,320
And then deep is looking into photos, you know, and what kind of the photos deep is looking

501
00:39:49,320 --> 00:39:50,600
into.

502
00:39:50,600 --> 00:39:56,960
So we have this stream of data flowing into our system, what signals and what we internally

503
00:39:56,960 --> 00:40:04,880
call those as an events and events are generated by consumer interacting with those product.

504
00:40:04,880 --> 00:40:11,440
So we basically take those events and our personalization platform and, you know, collect those

505
00:40:11,440 --> 00:40:12,440
events.

506
00:40:12,440 --> 00:40:17,200
The events are just think about, you know, if Sam goes to Trulia and you look into some

507
00:40:17,200 --> 00:40:22,600
site, you know, on an average, like when you look into a specific property, Sam is going

508
00:40:22,600 --> 00:40:29,200
to generate an average of 20 events, you know, within few minutes of your interaction.

509
00:40:29,200 --> 00:40:34,920
So we have this again, the real time messaging layer, which collects those signals and,

510
00:40:34,920 --> 00:40:41,000
you know, we have Trulia has millions of consumers, which are active on a monthly basis.

511
00:40:41,000 --> 00:40:46,000
So when they send those signals, we bring it to our Kafka layer.

512
00:40:46,000 --> 00:40:52,560
And from the Kafka layer, we basically brings it again, we use a streaming technologies

513
00:40:52,560 --> 00:40:58,360
like Spark or Strong again for the intent site of the technologies tag too.

514
00:40:58,360 --> 00:41:04,880
And this is where either we have the real time machine learning models in place or we have

515
00:41:04,880 --> 00:41:12,280
some aggregated systems where those signals are getting evaluated, right?

516
00:41:12,280 --> 00:41:20,680
Okay, we just see deep or we saw an anonymous consumer, we take all those data and then

517
00:41:20,680 --> 00:41:27,600
we persist it into our caching layer where that caching layer, which can be, you know,

518
00:41:27,600 --> 00:41:31,600
the edge base is our persistence layer for all the personalization platform, but then

519
00:41:31,600 --> 00:41:34,080
the caching here is the RERIS, what we have.

520
00:41:34,080 --> 00:41:35,080
Okay.

521
00:41:35,080 --> 00:41:41,040
So if Sam is pretty much active on our site, then Sam moves from edge based to RERIS.

522
00:41:41,040 --> 00:41:44,360
That's how we make because of the latency.

523
00:41:44,360 --> 00:41:52,680
So at the end, you know, this personalization platform stores, Sam's unique preferences,

524
00:41:52,680 --> 00:42:00,160
search criteria is, you know, Sam is looking into your Sam owns this two bedroom, three

525
00:42:00,160 --> 00:42:06,400
baths in St. Louis area in a quiet neighborhood, Sam is looking into this.

526
00:42:06,400 --> 00:42:12,600
So I think that's the personalization platform is a very foundational aspect, which drives

527
00:42:12,600 --> 00:42:15,320
rest of the other thing.

528
00:42:15,320 --> 00:42:18,960
Then on top of, I'm going to move over to machine learning systems.

529
00:42:18,960 --> 00:42:19,960
Is that fine now?

530
00:42:19,960 --> 00:42:20,960
Sure.

531
00:42:20,960 --> 00:42:21,960
Yeah.

532
00:42:21,960 --> 00:42:27,840
So when you think about this personalization platform is put into place, which is like an

533
00:42:27,840 --> 00:42:33,360
engine, which is working on a daily basis by itself, our first machine learning platform

534
00:42:33,360 --> 00:42:36,840
is computer vision and the deep learning.

535
00:42:36,840 --> 00:42:41,920
This is where we, we've been leading this industry in the computer vision and the deep

536
00:42:41,920 --> 00:42:47,080
learning for years, where, you know, computer vision, right, it's a system which we have

537
00:42:47,080 --> 00:42:53,920
built, where we have, you know, trained our systems, machines to look into photos and

538
00:42:53,920 --> 00:43:00,320
they can see, oh, I'm looking into a photo of, you know, the swimming pool or I'm looking

539
00:43:00,320 --> 00:43:04,440
into a photo of a kitchen, which has a granite countertop.

540
00:43:04,440 --> 00:43:10,320
So that's the computer vision, what we have implemented and then what we do is all those

541
00:43:10,320 --> 00:43:17,320
unique attributes, the data, which comes out of the system, powers our home page and in

542
00:43:17,320 --> 00:43:21,440
our home page, you will see what we call as collections.

543
00:43:21,440 --> 00:43:26,880
The collections are nothing but the group of properties, which we bring it together.

544
00:43:26,880 --> 00:43:33,400
So you may see collections like, you know, homes with swimming pools or home with remodder

545
00:43:33,400 --> 00:43:38,440
homes or homes with kitchen, granite countertop.

546
00:43:38,440 --> 00:43:46,120
So those are the collections, which we powers our home page and the more our consumers

547
00:43:46,120 --> 00:43:51,760
engage with these collections, the more inside we get into our consumers.

548
00:43:51,760 --> 00:43:56,480
So that's the one use case of our computer vision.

549
00:43:56,480 --> 00:44:02,840
The second use case of our computer vision is, you know, I'm pretty sure you, me and

550
00:44:02,840 --> 00:44:06,560
all the consumers when they start their home buying journey.

551
00:44:06,560 --> 00:44:11,920
The first thing they do is they come into site like Trulia and the search for neighborhood

552
00:44:11,920 --> 00:44:17,200
then they go to a listing and then they start looking into the photos of the home.

553
00:44:17,200 --> 00:44:19,440
That's how the journey starts.

554
00:44:19,440 --> 00:44:25,760
And if those photos are not engaging and if those photos are not telling story, consumers

555
00:44:25,760 --> 00:44:29,800
are going to lose their interest and they will keep moving into the second and third.

556
00:44:29,800 --> 00:44:37,760
So what we have done is we using a conventional neural networks, CNN models, we have invested

557
00:44:37,760 --> 00:44:43,760
in understanding, you know, the scene types of the photos, whether the photo is appropriate

558
00:44:43,760 --> 00:44:47,760
or not, like some, someone can just put a photo of a dog.

559
00:44:47,760 --> 00:44:52,440
So we say great, you know, we can see it is not a photo of a home, it is a photo of

560
00:44:52,440 --> 00:44:53,440
a dog.

561
00:44:53,440 --> 00:44:59,640
And then the quality of a photo is this photo is blur, is this photo is much more clear.

562
00:44:59,640 --> 00:45:06,040
So these three things, what we take out from our CNN models is the quality of a photo,

563
00:45:06,040 --> 00:45:14,280
appropriateness and the scene type, we score those things and then the highest performing

564
00:45:14,280 --> 00:45:17,600
photo, what we call as our hero image.

565
00:45:17,600 --> 00:45:23,680
So what we do is most attractive photo, when you start your journey, we put the most

566
00:45:23,680 --> 00:45:29,720
attractive photo for you first, so that your engagement becomes much more better with

567
00:45:29,720 --> 00:45:30,720
Trulia.

568
00:45:30,720 --> 00:45:36,440
So that's the second use case of our machine learning computer vision.

569
00:45:36,440 --> 00:45:41,280
And what we have seen by investing in those technologies, you know, there are double digit

570
00:45:41,280 --> 00:45:44,480
increase in inquiries for our listing.

571
00:45:44,480 --> 00:45:48,680
So that's the one piece, make sense over.

572
00:45:48,680 --> 00:45:57,080
And is the, is that lift based on, do you think primarily just getting the right listing

573
00:45:57,080 --> 00:46:03,160
in front of the, the right person who's likely to like it or is it, you know, getting rid

574
00:46:03,160 --> 00:46:10,040
of the, or kind of suppressing the listings that aren't, you know, good in general, you

575
00:46:10,040 --> 00:46:13,760
know, is there any one factor that drives the kind of results that you've seen?

576
00:46:13,760 --> 00:46:19,800
Yeah, so I think our relevancy is driven mostly by the consumer's behavior, what consumers

577
00:46:19,800 --> 00:46:22,000
are interested into.

578
00:46:22,000 --> 00:46:26,280
And so we basically just based on the consumer needs and this is where the personalization

579
00:46:26,280 --> 00:46:32,520
platform comes into a play to drive that computer vision on the serving side, what to

580
00:46:32,520 --> 00:46:34,560
serve to the consumer.

581
00:46:34,560 --> 00:46:35,560
Mm-hmm.

582
00:46:35,560 --> 00:46:36,560
Makes sense?

583
00:46:36,560 --> 00:46:37,560
So, yes.

584
00:46:37,560 --> 00:46:44,080
Two different users, you know, say my wife and I are kind of collaboratively shopping for

585
00:46:44,080 --> 00:46:48,320
a home as husbands and wives tend to do.

586
00:46:48,320 --> 00:46:52,560
You know, she might, when she goes to the site, she might see pool pictures first and I

587
00:46:52,560 --> 00:46:57,240
might see kitchen pictures first or what have you, depending on what our, what our interests

588
00:46:57,240 --> 00:46:58,240
are.

589
00:46:58,240 --> 00:47:03,040
And these aren't interests that we've explicitly shared with you their interests that

590
00:47:03,040 --> 00:47:07,520
you've derived from the various, you know, signals from watching the way we interact with

591
00:47:07,520 --> 00:47:08,520
the site.

592
00:47:08,520 --> 00:47:09,520
That's fair.

593
00:47:09,520 --> 00:47:14,600
And that's how basically the more you engage, the more we know about you, because if you

594
00:47:14,600 --> 00:47:17,920
come for the first time, we really don't know about you, right?

595
00:47:17,920 --> 00:47:23,320
It's basically we need to reach enough confidence level to serve you the right content.

596
00:47:23,320 --> 00:47:27,480
But yes, your assessment was pretty good.

597
00:47:27,480 --> 00:47:28,480
Mm-hmm.

598
00:47:28,480 --> 00:47:36,960
It's funny, I can't help, but the thing that I, as a, as someone who travels a lot and

599
00:47:36,960 --> 00:47:42,720
as a result uses Yelp a lot, I am always complaining about just how dumb the Yelp app is.

600
00:47:42,720 --> 00:47:48,280
And I don't think I've ever done it on the podcast before, but I wish they were doing

601
00:47:48,280 --> 00:47:49,480
more of what you're doing.

602
00:47:49,480 --> 00:47:54,600
When I land in a city, I pretty much like open up Yelp and like type in Thai or type

603
00:47:54,600 --> 00:47:58,480
in in every time trying to find a place to eat.

604
00:47:58,480 --> 00:48:02,480
And I always wonder like, why doesn't it just show me what it knows that I'm going to

605
00:48:02,480 --> 00:48:06,000
be looking for, what it should know that I'm going to be looking for.

606
00:48:06,000 --> 00:48:11,160
So maybe I'll use that rant as a segue into like what are the challenges that you've

607
00:48:11,160 --> 00:48:17,040
seen or what do you think, you know, what's the barrier to, you know, more companies, you

608
00:48:17,040 --> 00:48:23,080
know, having technology that enables them to better know and, and personalize to their

609
00:48:23,080 --> 00:48:24,080
customers.

610
00:48:24,080 --> 00:48:30,960
So just to understand, so your question is, what is the biggest barrier to investing in

611
00:48:30,960 --> 00:48:32,720
this kind of technologies?

612
00:48:32,720 --> 00:48:33,720
Yeah.

613
00:48:33,720 --> 00:48:34,720
Yeah.

614
00:48:34,720 --> 00:48:43,280
I think it is mostly around making sure, remember early on, we talked about the data driven

615
00:48:43,280 --> 00:48:51,720
product companies that how do you understand the strength or the data what you have with

616
00:48:51,720 --> 00:48:52,720
you?

617
00:48:52,720 --> 00:48:57,240
And I think it's, it's the first it needs to start from the top level, the commitment

618
00:48:57,240 --> 00:49:01,400
from the top level, that's the area we want to invest in.

619
00:49:01,400 --> 00:49:08,080
And the second thing Sam is rather than boiling the ocean, right, or let's solve all the problems

620
00:49:08,080 --> 00:49:15,680
in one go, pick the small use cases to evangelize within organization so that, you know, product

621
00:49:15,680 --> 00:49:21,240
people and the other stakeholders can bought into those concepts because, you know, AI or

622
00:49:21,240 --> 00:49:25,840
the machine learning is still in very infancy stage, you know, we have not reached the point

623
00:49:25,840 --> 00:49:27,560
where everyone understands.

624
00:49:27,560 --> 00:49:33,560
So my recommendation to the people is definitely, you know, bring an evangelist, build a small

625
00:49:33,560 --> 00:49:41,640
use cases, show the value prop, back it up with the data, build slowly and gradually build

626
00:49:41,640 --> 00:49:42,640
the tsunami.

627
00:49:42,640 --> 00:49:46,080
And when this tsunami is going to hit, then everyone is going to bought into this.

628
00:49:46,080 --> 00:49:49,160
So that's the way what I look into.

629
00:49:49,160 --> 00:49:54,280
Yeah, that's a great articulation of the process.

630
00:49:54,280 --> 00:50:00,800
So you guys have also done some writing on your engineering blog about how you use natural

631
00:50:00,800 --> 00:50:03,760
language processing and in particular natural language generation.

632
00:50:03,760 --> 00:50:05,920
Can you talk a little bit about that use case?

633
00:50:05,920 --> 00:50:06,920
Sure.

634
00:50:06,920 --> 00:50:11,680
So yeah, so I think, you know, thinking about, so it all starts again for us.

635
00:50:11,680 --> 00:50:15,920
We don't start from thinking about machine learning first.

636
00:50:15,920 --> 00:50:18,960
We always start from thinking about the consumer first.

637
00:50:18,960 --> 00:50:20,920
That's the number one goal.

638
00:50:20,920 --> 00:50:27,200
And what we started seeing, there are thousands and thousands of cities or neighborhoods across

639
00:50:27,200 --> 00:50:34,400
US when consumers come to our side, you know, they're looking for more information about

640
00:50:34,400 --> 00:50:35,400
this side.

641
00:50:35,400 --> 00:50:39,720
They're looking into more information about that neighborhood.

642
00:50:39,720 --> 00:50:45,240
And we said, great, now we, you know, how can we use the data, what we have to build

643
00:50:45,240 --> 00:50:52,120
the story and one way you can do is, you know, you can have the human beings as editors

644
00:50:52,120 --> 00:50:56,040
and let them write the stories about the cities and the neighborhoods.

645
00:50:56,040 --> 00:51:02,520
But when you go into this kind of a massive scale, there is no way that's going to work.

646
00:51:02,520 --> 00:51:07,360
And that's where we said, okay, great, now let's rely on the machine learning technologies

647
00:51:07,360 --> 00:51:09,240
to solve that problem.

648
00:51:09,240 --> 00:51:14,880
And this is where we leaned on our natural language generation system.

649
00:51:14,880 --> 00:51:21,240
So what we do is, we look into a location and we have built this feature extractor.

650
00:51:21,240 --> 00:51:26,560
The feature extractor look into, you know, what are the restaurants close by?

651
00:51:26,560 --> 00:51:29,720
What are the commute systems looks like?

652
00:51:29,720 --> 00:51:34,840
Is the price going up for that neighborhood, is it going down?

653
00:51:34,840 --> 00:51:37,040
So we extract the features.

654
00:51:37,040 --> 00:51:43,240
And then once those features comes out, what we have is a document planner, which looks

655
00:51:43,240 --> 00:51:47,200
into the features we have a document planner.

656
00:51:47,200 --> 00:51:53,480
But before we go into a description generator, we have built our content bank.

657
00:51:53,480 --> 00:52:01,480
So think about the content bank is where we build the sentences based on the features

658
00:52:01,480 --> 00:52:08,040
that if we say this, like this neighborhood is a Victorian style homes.

659
00:52:08,040 --> 00:52:14,680
So our content bank is going to have a sentence which will say Victorian, this neighborhood

660
00:52:14,680 --> 00:52:16,480
has Victorian homes.

661
00:52:16,480 --> 00:52:24,000
So we have this content bank and that content bank is built based on some of the crowdsourcing,

662
00:52:24,000 --> 00:52:28,800
which definitely we do, but we use our data mining and machine learning technologies to

663
00:52:28,800 --> 00:52:32,080
look into the data to build those content bank.

664
00:52:32,080 --> 00:52:38,760
So now think about the document planner coming out for a neighborhood or a city or a specific

665
00:52:38,760 --> 00:52:41,440
location, which has all the feature sets.

666
00:52:41,440 --> 00:52:46,680
We have a content bank and this is where then we use the description generator.

667
00:52:46,680 --> 00:52:52,720
So description generator, take the document planner, take the content bank bank and use

668
00:52:52,720 --> 00:52:58,920
the NLG to generate the content for that specific location.

669
00:52:58,920 --> 00:53:06,040
So that's how we use NLG and it's been going great on that front.

670
00:53:06,040 --> 00:53:09,880
So what I think I hear you saying, and this can be instructive to folks that want to

671
00:53:09,880 --> 00:53:19,760
use this, is that as opposed to trying the thoroughbunch of data to some natural language

672
00:53:19,760 --> 00:53:27,200
generation system and hoping for it to generate something that makes sense, you guys have

673
00:53:27,200 --> 00:53:34,200
broken up the problem and structured it in such a way that first year, you're identifying

674
00:53:34,200 --> 00:53:41,400
the, you call them features of a given neighborhood, maybe not features in the sense of a training

675
00:53:41,400 --> 00:53:46,320
of machine learning algorithm, but they're just attributes of a neighborhood and you kind

676
00:53:46,320 --> 00:53:54,480
of structure your descriptions so that you will highlight one or more of these attributes.

677
00:53:54,480 --> 00:54:00,080
And then the content bank, what I thought I heard was that you kind of have a set of

678
00:54:00,080 --> 00:54:05,200
templates or rough structures of the way you talk about different things.

679
00:54:05,200 --> 00:54:09,640
So you kind of have a template for how you talk about, you know, a neighborhood composition

680
00:54:09,640 --> 00:54:15,280
in terms of its architecture, maybe some templates for restaurants, things like that.

681
00:54:15,280 --> 00:54:21,040
And then, you know, all of that, you know, those attributes that you decided to highlight

682
00:54:21,040 --> 00:54:28,240
in a given description and these, this set of templates are utilized by this description

683
00:54:28,240 --> 00:54:33,640
generated to create something that, you know, sounds more human and is more readable and

684
00:54:33,640 --> 00:54:38,920
usable than, you know, what you might get if you just threw all the data against a neural

685
00:54:38,920 --> 00:54:39,920
net of some sort.

686
00:54:39,920 --> 00:54:40,920
Yeah.

687
00:54:40,920 --> 00:54:44,360
One clarification, the feature is a structure.

688
00:54:44,360 --> 00:54:50,640
It uses the data mining technologies to extract the attributes what you're talking about.

689
00:54:50,640 --> 00:54:53,040
So yes, it goes into a neighborhood.

690
00:54:53,040 --> 00:54:58,160
It uses the data mining in generates attribute and you're right spot on on the content bank

691
00:54:58,160 --> 00:54:59,160
in a simple form.

692
00:54:59,160 --> 00:55:04,240
You can think about the templates or you can think about, you know, which defines the much

693
00:55:04,240 --> 00:55:08,520
more vocabulary, which is easily understood by our consumers.

694
00:55:08,520 --> 00:55:09,520
Mm-hmm.

695
00:55:09,520 --> 00:55:10,520
That's great.

696
00:55:10,520 --> 00:55:11,520
That's great.

697
00:55:11,520 --> 00:55:16,200
Well, I know you're bumping up against a time constraint here.

698
00:55:16,200 --> 00:55:22,240
I think this is a great, you know, use case, we spent a lot more time on the, you know,

699
00:55:22,240 --> 00:55:26,920
data engineering, data acquisition side than we usually do on the podcast and, you know,

700
00:55:26,920 --> 00:55:31,240
I enjoyed geeking out a little bit on some of that stuff.

701
00:55:31,240 --> 00:55:35,080
But it sounds like you guys are doing really, really awesome things.

702
00:55:35,080 --> 00:55:38,800
And so thank you so much for being on the show and sharing them with us.

703
00:55:38,800 --> 00:55:39,800
Great.

704
00:55:39,800 --> 00:55:40,800
Thanks, Sam.

705
00:55:40,800 --> 00:55:41,800
All right.

706
00:55:41,800 --> 00:55:42,800
Thanks, Deep.

707
00:55:42,800 --> 00:55:43,800
Bye-bye.

708
00:55:43,800 --> 00:55:48,400
All right, everyone, that's our show for today.

709
00:55:48,400 --> 00:55:53,440
Once again, thanks so much for listening and for your continued support.

710
00:55:53,440 --> 00:55:59,280
Don't forget to leave your review or comment and there are one year anniversary listener

711
00:55:59,280 --> 00:56:01,480
appreciation content.

712
00:56:01,480 --> 00:56:06,960
The full details can be found at TwomoAI.com slash birthday.

713
00:56:06,960 --> 00:56:13,960
And of course, you can leave your questions and comments over on the show notes page at twomoai.com

714
00:56:13,960 --> 00:56:21,120
slash talk slash 2525 for your final links to deep and the various resources we mentioned

715
00:56:21,120 --> 00:56:22,640
in the show.

716
00:56:22,640 --> 00:56:39,800
Thanks so much for listening and catch you next time.

