WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.560
I'm your host Sam Charrington.

00:31.560 --> 00:36.480
In this episode I'm joined by Karen Vajepet, a human computer interaction developer at

00:36.480 --> 00:37.920
Figure 8.

00:37.920 --> 00:42.200
In this interview, Karen shares some of what he's learned through his work developing applications

00:42.200 --> 00:47.360
for data collection and annotation at Figure 8 and earlier in his career.

00:47.360 --> 00:52.920
We explore techniques like data augmentation, domain adaptation, and active and transfer

00:52.920 --> 00:57.160
learning for enhancing and enriching training data sets.

00:57.160 --> 01:04.160
We also touch on the use of ImageNet and other public data sets for real world AI applications.

01:04.160 --> 01:08.360
If you like what you hear in this interview, Karen will be speaking at my AI Summit,

01:08.360 --> 01:13.680
which takes place April 30th and May 1st in Las Vegas, and I'll be joining Karen at

01:13.680 --> 01:17.120
the upcoming Figure 8 Train AI Conference.

01:17.120 --> 01:19.960
May 9th and 10th in San Francisco.

01:19.960 --> 01:25.000
You can learn more about the AI Summit at Twimbleai.com slash AI Summit.

01:25.000 --> 01:31.040
At Train AI, you'll find a host of amazing speakers like Gary Casparov, Andre Carpathy,

01:31.040 --> 01:32.960
Marty Hurst, and many more.

01:32.960 --> 01:36.880
And you'll have the opportunity to receive hands-on AI, machine learning, and deep learning

01:36.880 --> 01:42.840
training through real world case studies on practical machine learning applications.

01:42.840 --> 01:52.360
For more information on Train AI, head over to www.figure-8.com slash train-ai.

01:52.360 --> 01:57.680
And be sure to use the code Twimbleai for 30% off of registration.

01:57.680 --> 02:01.680
A huge thanks to Figure 8 for sponsoring this episode of the podcast.

02:01.680 --> 02:04.320
Alright, let's roll.

02:04.320 --> 02:10.240
Alright everyone, I am on the line with Karen Vajapay.

02:10.240 --> 02:14.080
Karen is a human computer interaction developer at Figure 8.

02:14.080 --> 02:18.720
And he'll also be speaking at my upcoming AI Summit on the topic of data collection

02:18.720 --> 02:22.760
management and annotation for machine learning and AI.

02:22.760 --> 02:27.200
And in addition, both Karen and I will be at Figure 8's upcoming Train AI Conference

02:27.200 --> 02:28.680
in May.

02:28.680 --> 02:31.240
Karen, welcome to this week in machine learning and AI.

02:31.240 --> 02:33.040
Hey Sam, thanks for having me.

02:33.040 --> 02:36.080
It is great to have you on the podcast.

02:36.080 --> 02:39.240
Why don't we get started by having you tell us a little bit about your background?

02:39.240 --> 02:41.000
Yeah, definitely.

02:41.000 --> 02:47.800
So I guess we can start, I did my undergrad in bioengineering at UCSD.

02:47.800 --> 02:52.600
During this time, I actually did some research with one of my professors.

02:52.600 --> 02:56.920
And in over the course of that research, I kind of realized that I didn't want to do

02:56.920 --> 03:00.840
bioengineering because it was not applied.

03:00.840 --> 03:05.720
So that's when I actually transitioned into computer science and then went for a master's.

03:05.720 --> 03:09.600
I did my master's in computer science at Cornell Tech.

03:09.600 --> 03:15.440
And it was at Cornell Tech where actually did my first machine learning and modern analytics

03:15.440 --> 03:20.920
courses where we actually learned how to use clustering and kind of linear regression

03:20.920 --> 03:24.160
techniques to actually do data analysis.

03:24.160 --> 03:29.400
And while I was there, I actually had a co-op with this startup called Appio where they

03:29.400 --> 03:37.280
gave myself and my teammate a whole bunch of mobile data on people entering and exiting

03:37.280 --> 03:38.280
vehicles.

03:38.280 --> 03:43.880
And they wanted to train a machine to actually do a detection of when people were entering

03:43.880 --> 03:48.560
and exiting vehicles based off of accelerometer data.

03:48.560 --> 03:55.880
So it was kind of here where I really understood that data drives a lot of these applications.

03:55.880 --> 04:01.120
After I did my master's, I went on to found a company called Oyster with a couple people

04:01.120 --> 04:03.480
that had met at Cornell.

04:03.480 --> 04:08.400
And here I was the chief product officer and I was in charge of basically developing

04:08.400 --> 04:15.320
a product that would allow our customers who were developing a talent marketplace where

04:15.320 --> 04:22.320
we could match PhDs and people in academia with jobs in industry.

04:22.320 --> 04:27.240
And the way that we decided to go about doing this was to actually develop a platform

04:27.240 --> 04:34.760
where people could give us their skill set in a very granular way.

04:34.760 --> 04:39.760
And then we could match these individuals with the companies based off of the type of

04:39.760 --> 04:43.400
skills that those companies were looking for.

04:43.400 --> 04:47.680
And while we were developing this product, we didn't really realize the type of data that

04:47.680 --> 04:54.080
we were collecting once we started, but over the course of a few months, we actually had

04:54.080 --> 04:58.320
about 10,000 PhDs that had signed up with our website.

04:58.320 --> 05:03.040
And they had given us each about on average of 15 skills each.

05:03.040 --> 05:10.280
So we had a database of about 150,000 skill sets that people had indicated to us.

05:10.280 --> 05:16.520
And what we decided to do was actually look into this data and try to understand how

05:16.520 --> 05:19.440
we could make the most out of it.

05:19.440 --> 05:23.840
And so if you look at LinkedIn, one of the things that they've actually been trying

05:23.840 --> 05:31.600
to do is develop a skill graph where they want to map skills across industries and understand

05:31.600 --> 05:35.080
what sort of jobs require, what sort of skills.

05:35.080 --> 05:40.080
And so what we actually realized that as we were collecting this data, it was very structured

05:40.080 --> 05:45.600
data, which was a key factor in allowing us to implement what we did.

05:45.600 --> 05:51.080
And we built out a machine learning tool that we could actually predict an individual's

05:51.080 --> 05:55.520
skill set based off of one or two skills that they would give us.

05:55.520 --> 06:02.640
And so this was really cool because we could start to build out kind of people information

06:02.640 --> 06:07.320
based off of one or two skills that someone might give us and understand what industry

06:07.320 --> 06:12.920
they would fall into and how well they may perform within this industry.

06:12.920 --> 06:17.880
So after about a year of working at Oyster, just like all businesses, we kind of needed

06:17.880 --> 06:23.480
to make some money and we weren't getting traction on the business side as much as we

06:23.480 --> 06:25.320
would have liked to.

06:25.320 --> 06:29.200
So I actually went on to a startup called Icentium.

06:29.200 --> 06:35.600
And Icentium is based in New York City and what they do is they've developed a deterministic

06:35.600 --> 06:42.480
natural language processing system to take tweets and assign scores to these tweets

06:42.480 --> 06:48.280
and then use this information to actually make predictions in the stock market.

06:48.280 --> 06:55.320
So what my job was at Icentium was to actually write the tools that would use this AI to

06:55.320 --> 06:58.480
make those predictions in the stock market.

06:58.480 --> 07:03.760
And myself being a bioengineering computer scientist, I had actually no information about

07:03.760 --> 07:08.960
the stock market or really any understanding of the way that it works.

07:08.960 --> 07:13.520
So what I did know is that data drives applications.

07:13.520 --> 07:19.720
And so what I did starting at Icentium is tried to understand first what is this natural

07:19.720 --> 07:26.360
language processing really doing and what they had built was a completely rule based system

07:26.360 --> 07:30.080
where there was actually no machine learning integrated into it.

07:30.080 --> 07:36.920
They actually had a team of PhDs who are writing rules based on both the English language

07:36.920 --> 07:43.920
as well as the financial industry that would assign scores to individual tweets.

07:43.920 --> 07:48.120
Then what we would do is we would actually take millions of these tweets and aggregate

07:48.120 --> 07:51.480
scores for given entities in the stock market.

07:51.480 --> 07:57.560
So say you want to predict the price of Apple, you might look at all the tweets about Apple

07:57.560 --> 08:04.200
over the last three months and determine what trends you are seeing in the sentiment

08:04.200 --> 08:08.720
and then make a prediction based off of what the crowd is saying.

08:08.720 --> 08:14.760
Now the one catch with this is that it wasn't necessarily always the tweets about Apple

08:14.760 --> 08:17.520
that were driving the stock price of Apple.

08:17.520 --> 08:23.960
So we actually had to look at tweets from different entities and across industries to really

08:23.960 --> 08:27.880
get the best predictions on these stocks.

08:27.880 --> 08:32.320
And so that was interesting to see that even though this data doesn't seem like it's

08:32.320 --> 08:38.400
correlated, when you actually look at it from a holistic point of view, there are actually

08:38.400 --> 08:44.720
key factors across industries that may be driving trends in different industries.

08:44.720 --> 08:46.280
So that was really cool.

08:46.280 --> 08:50.920
So after about a year and a half at Isentium, that's when I joined Figure 8.

08:50.920 --> 08:57.680
And now what I'm doing is I'm actually developing tools that allow companies to create data that

08:57.680 --> 09:02.280
they need to develop machine learning and AI applications.

09:02.280 --> 09:08.240
Looking back at your time at Isentium, do you think if they did it all over again?

09:08.240 --> 09:13.360
They'd use the same rule based approach to determining sentiment and all that kind of

09:13.360 --> 09:14.360
stuff?

09:14.360 --> 09:19.000
Or was there something specific to what they wanted to do that really required that approach?

09:19.000 --> 09:23.800
So I think there's pros and cons to both systems.

09:23.800 --> 09:30.640
So one of the pros really with a rule based system is that you're really writing all of

09:30.640 --> 09:35.120
the rules, you're telling the computer exactly what it should be doing.

09:35.120 --> 09:40.400
But the problem with that is that if you want it to be complete, you have to write every

09:40.400 --> 09:41.400
single rule.

09:41.400 --> 09:45.400
So if you're trying to make a natural language processing system, you essentially have

09:45.400 --> 09:51.600
to write every single rule in the English language for that computer to really understand

09:51.600 --> 09:52.960
English.

09:52.960 --> 09:58.240
And the benefit with machine learning is that you can actually take a whole bunch of

09:58.240 --> 10:02.680
existing data that may be labeled or not, right?

10:02.680 --> 10:07.160
And then use this data to train a system to have a rough understanding of what the English

10:07.160 --> 10:09.160
language might be.

10:09.160 --> 10:14.160
So I'd say depending on what your use case is, you might want to go for a machine learning

10:14.160 --> 10:19.680
approach if you just need good enough to kind of get you to the next phase.

10:19.680 --> 10:25.280
Whereas if you're kind of in a targeted space like Isentium was with finance, it kind of

10:25.280 --> 10:31.280
made more sense to build out a rule based system that could capture the intricacies of

10:31.280 --> 10:33.920
this industry.

10:33.920 --> 10:40.320
So your role now is on or your title at least is human computer interaction.

10:40.320 --> 10:49.080
When I think of HCI, I think of user interfaces and user device experiences, is that a big

10:49.080 --> 10:51.240
part of your role today?

10:51.240 --> 10:52.480
Yeah, that's correct.

10:52.480 --> 10:57.200
So I'm actually working on the machine learning team as an HCI developer.

10:57.200 --> 11:04.000
So my role is really around developing the applications that our contributors and

11:04.000 --> 11:10.440
our customers will use to kind of develop the data that they need to build into their

11:10.440 --> 11:13.960
machine learning and AI applications.

11:13.960 --> 11:21.280
And so a typical application that you'd work on would be something like a tool that someone's

11:21.280 --> 11:24.040
using to create bounding boxes or label data?

11:24.040 --> 11:25.040
Yeah, correct.

11:25.040 --> 11:31.840
So we have contributors all around the world and they're all using our platform to create

11:31.840 --> 11:33.400
this data for the customers.

11:33.400 --> 11:38.480
And so the tools that those contributors are using are very essential in creating this

11:38.480 --> 11:43.240
data because the customers, they want to get the most accurate data.

11:43.240 --> 11:48.760
And if these tools are really hard for the contributors to use, then they may get tired

11:48.760 --> 11:53.160
while they're doing the tasks or they just may not get the most accurate data.

11:53.160 --> 11:58.000
So it's actually a very crucial component to make sure that these applications that the

11:58.000 --> 12:03.840
contributors are using are very user friendly, especially because we're catering to people

12:03.840 --> 12:08.040
all around the world so you don't necessarily know what their background is or how much

12:08.040 --> 12:09.760
technical knowledge they have.

12:09.760 --> 12:14.000
So it has to be kind of something that they can just pick up and run with.

12:14.000 --> 12:21.600
So both in your prior experience at at Figure 8, you've come across a lot of AI implementations.

12:21.600 --> 12:27.000
What are some of the main data challenges that you see organizations running into when

12:27.000 --> 12:28.400
they're implementing AI?

12:28.400 --> 12:29.600
Yeah, definitely.

12:29.600 --> 12:36.120
So I think a big challenge that companies face at least from my experience is that people

12:36.120 --> 12:40.120
don't necessarily know how the best way to use their data.

12:40.120 --> 12:45.080
So they have a whole bunch of data, they don't necessarily know what it really means.

12:45.080 --> 12:52.320
So to give you an example, when we were at Icentium, I spoke about how you may not use just

12:52.320 --> 12:56.560
Apple sentiment to determine Apple's stock price.

12:56.560 --> 13:01.600
It sounds very simple, but it actually took us quite a while to learn that and the only

13:01.600 --> 13:08.880
that we really did was digging into the data and understanding how trends in this sentiment

13:08.880 --> 13:16.080
are changing over time and how sentiment across entities kind of affect each other.

13:16.080 --> 13:21.600
So when we were developing these models, we were actually start to play around with smoothing

13:21.600 --> 13:22.440
algorithms.

13:22.440 --> 13:27.640
So instead of just aggregating all the data for a given time period, we actually want to

13:27.640 --> 13:33.800
smooth that data to give us a better signal over time because especially in finance, you

13:33.800 --> 13:38.680
don't want to make a trade every two hours, you kind of want there to be some stability.

13:38.680 --> 13:42.640
So you could say, all right, I'm going to buy it here and I'm going to sell it X amount

13:42.640 --> 13:47.720
of time later as opposed to having buy sell, buy sell frequently.

13:47.720 --> 13:51.920
While that may be a use case in high frequency trading, that wasn't necessarily the use case

13:51.920 --> 13:56.080
that we're trying to cater to, especially when we're basing our predictions off of social

13:56.080 --> 14:02.240
media, which is kind of longer than a couple second turn over time.

14:02.240 --> 14:08.320
And so a really important factor of what we were developing was to really understand how

14:08.320 --> 14:13.680
the data can change over time and how we can apply smoothing methods and change the

14:13.680 --> 14:18.480
way that we were kind of shifting data over time.

14:18.480 --> 14:22.160
You've collected a bunch of data.

14:22.160 --> 14:30.120
You've started to do some analysis to understand what that data can tell you.

14:30.120 --> 14:39.960
But even the specifics of the given problem that you're trying to go after, it sounds

14:39.960 --> 14:45.760
like in some of your experiences that, or we see often that that changes over time,

14:45.760 --> 14:53.320
are there practices that an organization can follow that will allow it to be able to

14:53.320 --> 14:59.680
more quickly map its data to the use cases as they evolve?

14:59.680 --> 15:00.680
Yeah, for sure.

15:00.680 --> 15:06.280
So I think it's one of the important things is that an organization should kind of set

15:06.280 --> 15:11.800
a goal of understanding what they're trying to become and what they want to ultimately

15:11.800 --> 15:13.920
do with their data.

15:13.920 --> 15:19.480
And by really planting a goal, it doesn't necessarily mean that that's the only endpoint

15:19.480 --> 15:24.280
that you're going to hit, but it allows you to kind of have a frame of reference as you

15:24.280 --> 15:28.120
kind of develop your strategies and as you build out your AI.

15:28.120 --> 15:35.600
And so you could set a goal for two years from now of becoming an organization that does

15:35.600 --> 15:38.520
a certain type of data processing, right?

15:38.520 --> 15:43.560
And over those two years, you want to make sure that you're building your data sets and

15:43.560 --> 15:48.120
building your algorithms to kind of achieve that goal, right?

15:48.120 --> 15:53.120
And along the way, you'll certainly run into changes in your data or changes in behavior

15:53.120 --> 16:01.120
of customers that might alter your ultimate goal, but by having some sort of state that

16:01.120 --> 16:07.040
you're trying to achieve, that will really allow you to develop your tools to kind of hit

16:07.040 --> 16:08.040
that point.

16:08.040 --> 16:15.320
Do you have a framework for thinking about the different ways that organizations can collect

16:15.320 --> 16:21.560
data to allow them to power AI use cases?

16:21.560 --> 16:27.640
Certainly, so again, I think it really depends on the particular use case, right?

16:27.640 --> 16:32.560
So when you think about image labeling, there are many different ways that you can label

16:32.560 --> 16:33.560
an image.

16:33.560 --> 16:40.040
For example, you can have an image of, say, a meadow with some sheep on it, right?

16:40.040 --> 16:43.760
And you could just label this meadow with sheep.

16:43.760 --> 16:47.440
And so that's kind of some sort of structured data where you have an image and you have an

16:47.440 --> 16:52.280
associated caption, but you might want to get more granular, right?

16:52.280 --> 16:57.000
And you want to actually now put boxes around each of those sheep, right?

16:57.000 --> 17:02.320
And then label those individual boxes as sheep and maybe label tree and maybe label sun,

17:02.320 --> 17:03.320
right?

17:03.320 --> 17:09.800
So now you actually have more insight into the content of that picture, as well as the

17:09.800 --> 17:17.080
specific objects and what the computer could learn based off of those regions of interest.

17:17.080 --> 17:21.040
And then going further, you could do something like semantic segmentation where you're actually

17:21.040 --> 17:26.560
labeling every single pixel in the image with an associated label.

17:26.560 --> 17:29.640
So it'll really depend on the needs of your technology.

17:29.640 --> 17:34.880
And if you are just trying to give like a general categorization of an image, you don't necessarily

17:34.880 --> 17:38.440
need to get that pixel level segmentation.

17:38.440 --> 17:44.000
But if you're doing something like autonomous driving where this is critical situation that

17:44.000 --> 17:51.280
could mean life or death in certain situations, you want to make sure that your AI is really

17:51.280 --> 17:55.080
as knowledgeable as possible about its surroundings.

17:55.080 --> 18:03.800
Yeah, I'm wondering even before you get to labeling, if there's a set of things that organizations

18:03.800 --> 18:10.080
should be thinking about in terms of the collection, like if you, you know, some organizations

18:10.080 --> 18:15.760
will have a data set that they've collected just based on their interactions with their

18:15.760 --> 18:21.000
clients, you know, whether that's financial data or user-submitted content, but they

18:21.000 --> 18:29.480
may want to do something with machine learning around data that, you know, they haven't already

18:29.480 --> 18:31.240
collected.

18:31.240 --> 18:34.840
Are there, you know, have you run into that kind of situation where an organization needs

18:34.840 --> 18:40.280
additional data that they don't already have and how to organizations typically approach

18:40.280 --> 18:42.800
that particular challenge?

18:42.800 --> 18:49.440
Yeah, so kind of one of the problems that I've seen is you may have a lot of data.

18:49.440 --> 18:55.640
Say you're trying to do a surveillance camera and you want the surveillance camera to detect

18:55.640 --> 18:56.640
objects, right?

18:56.640 --> 18:59.760
So you want to see people, you want to see cars.

18:59.760 --> 19:04.720
And so what you'll go and do is say, okay, well, let's go online and we'll find a

19:04.720 --> 19:08.560
bunch of open source images of vehicles and people, right?

19:08.560 --> 19:14.080
And a lot of these will come up as portraits of people and maybe like dealerships will

19:14.080 --> 19:16.640
have a lot of pictures of their cars.

19:16.640 --> 19:21.960
And so you might take a lot of these images, you'll go and train a model and then realize

19:21.960 --> 19:28.160
that it's actually doing a very poor job of detecting these objects in surveillance video.

19:28.160 --> 19:33.160
And the reason for this is probably because when you look at a portrait and you look at

19:33.160 --> 19:37.560
a person in a surveillance camera, they look very different.

19:37.560 --> 19:43.120
And so us as humans might understand that yes, this is a human and this is a human, but

19:43.120 --> 19:47.160
the computer doesn't necessarily know the difference between the two, right?

19:47.160 --> 19:52.600
So they'll look at the portrait and say, okay, I understand a human to have a head that's

19:52.600 --> 19:58.000
this size relative to their body and they have arms, legs and you can see their facial

19:58.000 --> 19:59.000
features.

19:59.000 --> 20:03.120
Whereas on a surveillance camera, they may just appear as kind of like a spec that's moving

20:03.120 --> 20:04.680
around.

20:04.680 --> 20:09.840
So if that's the case, you're going to actually have to go and collect that type of footage.

20:09.840 --> 20:15.120
So you really want to make sure that the data that you're using to train your system is

20:15.120 --> 20:19.920
similar to the type of data that you're going to use to run through and get predictions

20:19.920 --> 20:21.240
out of it.

20:21.240 --> 20:28.200
There have been a number of attempts to try to do things like domain augmentation, to

20:28.200 --> 20:34.200
kind of massage existing data so that it looks more like the domain that you're trying

20:34.200 --> 20:36.520
to make predictions off of.

20:36.520 --> 20:38.920
Is that have you worked with that at all?

20:38.920 --> 20:39.920
Yeah.

20:39.920 --> 20:44.560
So you can definitely do like sort of data augmentation to enhance your data set.

20:44.560 --> 20:49.680
So for example, if you're trying to do a speech recognition system, right, you want this

20:49.680 --> 20:53.800
to be robust in many different situations.

20:53.800 --> 20:59.160
And if it's going to be deployed on a mobile device, there's going to be a lot of ambient

20:59.160 --> 21:00.160
noises.

21:00.160 --> 21:02.160
There's going to be a lot of static.

21:02.160 --> 21:08.760
So if you're collecting very clean data, say in a recording studio, you could use this

21:08.760 --> 21:14.160
data and they'll be great, but your machine may not be robust enough to actually understand

21:14.160 --> 21:17.160
what people are saying in these noisy situations.

21:17.160 --> 21:22.920
So in that case, you could actually take these clean data samples and simulate noise into

21:22.920 --> 21:23.920
those.

21:23.920 --> 21:29.320
So you can add in background noises or horns or cars and then use that data to train.

21:29.320 --> 21:34.120
So now you've kind of used the same data and you just expanded your data set through

21:34.120 --> 21:36.120
this augmentation.

21:36.120 --> 21:37.120
Yeah.

21:37.120 --> 21:43.120
And I've kind of mushed together two terms data augmentation and domain adaptation, but

21:43.120 --> 21:47.960
you kind of refer to both of them there.

21:47.960 --> 21:54.960
I guess one question I have is around, you know, when folks are trying to build, again,

21:54.960 --> 22:01.600
AI for commercial applications, we see a lot of the use of these public data sets in

22:01.600 --> 22:08.760
research, like ImageNet and others, how often do you see folks using those data sets

22:08.760 --> 22:11.200
in commercial applications?

22:11.200 --> 22:17.560
So these sort of data sets are actually really great starting points for your AI and

22:17.560 --> 22:23.360
technology because from these data sets, there's actually a lot of pre-trained models that

22:23.360 --> 22:26.240
you can just start out with.

22:26.240 --> 22:33.320
So generally, depending on your data, it may not be the perfect application for your organization.

22:33.320 --> 22:37.200
But by having this starting point, you can actually do things like transfer learning

22:37.200 --> 22:43.040
and active learning to really improve your models and kind of get the results that you're

22:43.040 --> 22:44.520
looking for.

22:44.520 --> 22:49.360
And the nice thing is that you don't have to go and pre-trained this model on millions

22:49.360 --> 22:51.160
of examples, right?

22:51.160 --> 22:56.160
If you're using ImageNet and something like a YOLO detector, you have this YOLO model

22:56.160 --> 23:01.120
that's pre-trained, that has millions of images that it's learned from.

23:01.120 --> 23:05.120
And you can basically run like a transfer learning and remove the last couple layers

23:05.120 --> 23:11.560
of that YOLO model and then retrain it with your specific data.

23:11.560 --> 23:17.520
So it's going to have all of the rules that it learned from the previous data set

23:17.520 --> 23:25.440
around general images and edges, edge detection kind of pixel level.

23:25.440 --> 23:31.200
And then when you do the transfer learning and really retrain those last couple layers,

23:31.200 --> 23:35.480
that's when your applications going to get the knowledge that it needs that's specific

23:35.480 --> 23:37.000
to your application.

23:37.000 --> 23:43.600
Okay, and for those that heard YOLO and aren't familiar with it, YOLO is you only look

23:43.600 --> 23:45.720
once, right, the object detector.

23:45.720 --> 23:47.720
Yeah, that's correct.

23:47.720 --> 23:54.360
And actually, we'll be doing an online meetup focusing on YOLO, not this one that's coming

23:54.360 --> 24:00.280
up tomorrow actually, but the May meetup will be all about YOLO.

24:00.280 --> 24:03.040
You mentioned transfer learning are there.

24:03.040 --> 24:09.000
Things that you find that folks really struggle with around transfer learning that isn't

24:09.000 --> 24:17.960
obvious from the things that you might find online or the kind of easily accessible write-ups.

24:17.960 --> 24:22.600
So I think again, a really big part of it is actually going to be the data that's going

24:22.600 --> 24:24.040
into this.

24:24.040 --> 24:30.520
So when you do transfer learning, you actually really do need the right data that's similar

24:30.520 --> 24:36.240
to the type of data that you're looking to predict to retrain this model.

24:36.240 --> 24:41.920
Because again, it has this knowledge of all the previously trained examples.

24:41.920 --> 24:46.800
But if those examples are different than the type of data that you're looking for, then

24:46.800 --> 24:48.520
it's not going to perform very well.

24:48.520 --> 24:56.640
So for example, if you're looking at detecting cancer in X-ray images, right, this is a very

24:56.640 --> 25:02.360
specific use case that there actually isn't a whole lot of data that's publicly available.

25:02.360 --> 25:06.720
But you could do something like take this YOLO model, do transfer learning.

25:06.720 --> 25:12.520
So maybe you want to remove the last two layers of your model, and then take all of the

25:12.520 --> 25:18.560
X-ray imagery that you may have collected over your work or that might be open source and

25:18.560 --> 25:21.240
use that to retrain this model.

25:21.240 --> 25:27.200
So that those last two layers can gain the knowledge that's specific to this use case.

25:27.200 --> 25:33.960
And then in that process, you'll ultimately develop a neural net that's suited to your

25:33.960 --> 25:35.560
use case.

25:35.560 --> 25:40.160
When you're working with customers that are building out these kinds of applications,

25:40.160 --> 25:44.960
and taking into account all of these types of factors, how to, one of the questions

25:44.960 --> 25:47.920
I hear all the time is, how much data am I going to need for this?

25:47.920 --> 25:53.440
How do you advise them and walk them through starting to understand what the requirements

25:53.440 --> 25:54.760
will be?

25:54.760 --> 25:59.680
So I think the best way to really go about that is just very iteratively, right?

25:59.680 --> 26:07.760
So if you have a thousand images of this X-ray data for cancer detection, then use those

26:07.760 --> 26:08.960
thousand, right?

26:08.960 --> 26:13.840
And once you've trained your model, you really get an understanding of how well that works

26:13.840 --> 26:14.920
or not.

26:14.920 --> 26:21.360
So you could use a thousand images and that could get you up to like an 85% accuracy.

26:21.360 --> 26:27.800
And maybe you don't need like 95, 99% because all you want this model to do is just run

26:27.800 --> 26:29.480
a general overview.

26:29.480 --> 26:34.880
And so if you can get a confidence level of a patient above 50%, then you know, okay,

26:34.880 --> 26:38.720
well, there's a, this person person has a 50% risk.

26:38.720 --> 26:41.560
So we're going to actually pay more attention to them.

26:41.560 --> 26:45.960
And you could maybe filter out a lot of false positives that way.

26:45.960 --> 26:55.240
We started off talking about some of the tooling aspects of data collection and data labeling.

26:55.240 --> 27:06.040
Do folks tend to use or reuse kind of off the shelf tools to label data sets or is there

27:06.040 --> 27:10.920
a big advantage in customizing the tools for a particular, the needs of a particular

27:10.920 --> 27:12.480
data set or application?

27:12.480 --> 27:14.480
What's the right way to think about that?

27:14.480 --> 27:20.520
So I think here at Figure 8, right, we're catering to a large number of customers that

27:20.520 --> 27:23.160
have very different use cases.

27:23.160 --> 27:27.680
So when we develop our tooling, we want to make sure that it's really applicable to a lot

27:27.680 --> 27:33.440
of those use cases so that our customers can really get the most benefit out of it.

27:33.440 --> 27:38.880
But there's also many organizations that have very, very specific use cases, right?

27:38.880 --> 27:45.160
So maybe what they're looking to do is label just the tires in cars, right?

27:45.160 --> 27:50.040
And so they may not need a whole bounding box tool or pixel segmentation, but they may

27:50.040 --> 27:55.040
develop their own tool that allows for this very specific use case.

27:55.040 --> 28:01.560
In those cases, you may want to have a specific tooling that works well with your application.

28:01.560 --> 28:07.640
But for the more generalized use cases, there are definitely a number of tools out there

28:07.640 --> 28:12.360
that can help you get up and running pretty quickly without having to develop your own

28:12.360 --> 28:14.480
stack from end to end.

28:14.480 --> 28:21.200
As an HCI focus developer, are there ways that you think about testing the interfaces

28:21.200 --> 28:26.120
and the tooling to determine how effective it is?

28:26.120 --> 28:28.000
You mentioned user fatigue earlier.

28:28.000 --> 28:34.960
How do you determine, how do you quantify user fatigue and try to test for it and test

28:34.960 --> 28:37.000
design around it?

28:37.000 --> 28:40.240
Well, we just kind of run it with people.

28:40.240 --> 28:45.920
Luckily for us, we have thousands and thousands of people that use our platform every day.

28:45.920 --> 28:51.560
So as we're developing these tools, we kind of just schedule sessions regularly so that

28:51.560 --> 28:55.840
we can work with them and we'll sit in like an hour long session and just say, here's

28:55.840 --> 28:56.840
the tool, right?

28:56.840 --> 29:01.000
And give them the tool without any instructions and see if they can use it.

29:01.000 --> 29:05.720
And inevitably they'll stumble on it and so we'll kind of give them some advice and

29:05.720 --> 29:09.760
give them some direction and then that will help them get through the task.

29:09.760 --> 29:15.360
And it's really through this user testing process that we actually make the biggest steps

29:15.360 --> 29:20.320
and improvements in our tools because these are the people that are going to be using it.

29:20.320 --> 29:23.920
And so we want to make sure that they actually know how to use it, right?

29:23.920 --> 29:29.360
So if we're sitting internally and kind of just developing these tools in a sandbox without

29:29.360 --> 29:35.240
actually running it with real people, then ultimately what we're going to give the contributors

29:35.240 --> 29:37.920
and the customers isn't going to be what they want, right?

29:37.920 --> 29:43.280
So kind of as we develop these tools, we try to make sure that as frequently as possible,

29:43.280 --> 29:50.360
we can run these tests with new features anytime there's updates and so and then get really

29:50.360 --> 29:55.440
qualitative and quantitative feedback on these tools that we're developing.

29:55.440 --> 30:01.640
Can you give us some examples of things that you've observed in these kinds of tests and

30:01.640 --> 30:08.400
how they've translated into new ways of thinking about or implementing the tooling?

30:08.400 --> 30:09.400
Definitely.

30:09.400 --> 30:13.600
So I think one big thing, right, is icons.

30:13.600 --> 30:18.800
So we see icons everywhere and there's a lot of things that we've started to understand

30:18.800 --> 30:21.640
just from having these across platforms.

30:21.640 --> 30:27.160
And so for example, when you see a trash can, you know that that's a delete, right?

30:27.160 --> 30:32.040
But in certain cases, you may have multiple things working on the screen that if you push

30:32.040 --> 30:34.960
this trash can, what is it really going to delete, right?

30:34.960 --> 30:37.000
Is it going to remove an annotation?

30:37.000 --> 30:39.520
Is it going to delete your image?

30:39.520 --> 30:44.400
So it's kind of things around this where you say, all right, well, do you understand these

30:44.400 --> 30:45.400
icons, right?

30:45.400 --> 30:50.320
And you'll kind of have them walk you through what they think each icon is going to do.

30:50.320 --> 30:55.640
And if three out of four people are saying this icon is supposed to do something that you're

30:55.640 --> 31:01.240
not intending it to do, then you know immediately that, okay, well, I need to change the icon

31:01.240 --> 31:06.600
or I need to make it more clear as to what is going to happen when they click on this.

31:06.600 --> 31:12.320
So it goes all the way down to those little details where you see people doing interactions

31:12.320 --> 31:14.880
that you didn't intend.

31:14.880 --> 31:19.080
And so you kind of understand that, okay, well, these people are trying to use this tool

31:19.080 --> 31:20.320
in a specific way.

31:20.320 --> 31:27.160
So I either try to cater it so that it allows them to do their interactions better or I make

31:27.160 --> 31:32.680
it more clear that what I'm intending to do should be done in a certain way.

31:32.680 --> 31:39.960
Is the implication from that example that a lot of the core things you observe are not

31:39.960 --> 31:49.600
specific to the ML and AI use cases and are more general UI, UX types of issues?

31:49.600 --> 31:50.600
Yeah.

31:50.600 --> 31:58.080
So really with the AI and ML, a lot of that has to do with the actual data itself, right?

31:58.080 --> 32:03.160
So if you're looking to put bounding boxes and images, right, the data that you need

32:03.160 --> 32:07.600
is kind of regions of interest in an image.

32:07.600 --> 32:12.840
So the way that you generate that data, that's actually going to be, can be done very differently,

32:12.840 --> 32:13.840
right?

32:13.840 --> 32:17.960
Like you can have a machine kind of predict these boxes and then just give those back

32:17.960 --> 32:22.840
to the customer, you can have people draw boxes around these images or the objects and

32:22.840 --> 32:25.440
the images and give that to the customer.

32:25.440 --> 32:30.480
And so there's different ways that you can accomplish or create the same sort of data

32:30.480 --> 32:32.840
that's used in AI and ML.

32:32.840 --> 32:38.120
So when we do a lot of this HCI testing, it's actually on a tool level where we see the

32:38.120 --> 32:42.480
people's interactions with the tools that are going to allow them to create the data that

32:42.480 --> 32:43.680
they need.

32:43.680 --> 32:48.480
And one of the initiatives that we've been working on over the last few months is to really

32:48.480 --> 32:52.440
integrate ML and AI into these tooling.

32:52.440 --> 33:00.640
And so an example of that is we have this bounding box tool where customers or contributors

33:00.640 --> 33:07.960
can go in and put boxes around different objects of interest based on the customer need.

33:07.960 --> 33:14.000
But what we want to do as figurate and as a platform for data is we want to make this

33:14.000 --> 33:19.000
interaction with the contributors and the tooling as easy as possible.

33:19.000 --> 33:23.920
And so part of that could actually be, instead of giving the contributor contributors an

33:23.920 --> 33:29.720
image and asking them to label all the objects, we could actually do a prediction on that

33:29.720 --> 33:35.160
image and display the image and the predictions to the contributors and tell them, hey, instead

33:35.160 --> 33:40.760
of drawing boxes around every single one, can you confirm which boxes are correct and

33:40.760 --> 33:42.600
which ones are wrong?

33:42.600 --> 33:46.960
So now instead of having to go through and really annotate every single object, they're

33:46.960 --> 33:54.080
kind of just saying yes, no, yes, no, making that interaction a lot easier and ultimately

33:54.080 --> 33:56.680
getting the customer better and cheaper data.

33:56.680 --> 33:59.840
And it sounds like you just described a capture system.

33:59.840 --> 34:00.840
Yeah.

34:00.840 --> 34:06.680
So capture actually does that and that's exactly their goal is really to use machine learning

34:06.680 --> 34:09.560
to develop their tools.

34:09.560 --> 34:15.640
What other things have you learned about the tooling process that have surprised you like

34:15.640 --> 34:20.920
once you got the tools into the hands of end users?

34:20.920 --> 34:27.760
So one of the major challenges that we've seen is we're building a completely web-based

34:27.760 --> 34:29.680
platform, right?

34:29.680 --> 34:34.240
So people aren't installing any software, they're kind of just going online and they're

34:34.240 --> 34:37.840
running these tools and again, they're all around the world.

34:37.840 --> 34:43.280
And so you actually don't really know what these users' environments are.

34:43.280 --> 34:48.080
So these contributors could be using any web browser, they could be on any sort of device,

34:48.080 --> 34:54.720
they could be on any bandwidth, internet connection and these tools have to be consistent across

34:54.720 --> 34:56.480
all of those things.

34:56.480 --> 35:03.080
So as a developer, I'm really trying to capture as many environments as possible, but you

35:03.080 --> 35:06.080
really can never get every single one.

35:06.080 --> 35:10.960
And that's where the user testing comes in and it's really important because we'll actually

35:10.960 --> 35:16.680
go and work with people and we have a number of BPO's around the world and we'll work

35:16.680 --> 35:22.280
with people in each of those to understand what their environment is, what sort of tooling

35:22.280 --> 35:27.120
or browsers they're using, what their internet speeds might be like, and then we'll kind

35:27.120 --> 35:32.400
of build our tools to cater to as many of these as possible.

35:32.400 --> 35:36.640
Earlier in the conversation you mentioned active learning, can you talk a little bit about

35:36.640 --> 35:40.160
how you use that to support this process?

35:40.160 --> 35:41.240
Yeah, definitely.

35:41.240 --> 35:48.240
So with active learning, the idea is that you have semi-supervised data.

35:48.240 --> 35:53.280
So you have some of your data may be labeled and some of the data isn't labeled, right?

35:53.280 --> 35:59.880
And so what you can do is use this already labeled data to train a rough deep learning

35:59.880 --> 36:01.240
model.

36:01.240 --> 36:07.320
And then once you have this rough model, you can start to run these unlabeled instances

36:07.320 --> 36:14.840
through your neural net and based on the confidence of each of these predictions, you may

36:14.840 --> 36:18.640
use some of these to retrain your model.

36:18.640 --> 36:24.200
So if you have a thousand images that are already labeled and they have whatever caption

36:24.200 --> 36:30.720
that you need, you can use these images, train a model and say you have 100,000 images

36:30.720 --> 36:36.960
that aren't labeled, you might take chunks of a thousand images and run them through

36:36.960 --> 36:40.520
this model and they each may have a confidence.

36:40.520 --> 36:46.720
So if you're trying to predict vehicles in images, they might come out with one image

36:46.720 --> 36:51.680
has a confidence of 20% that there's a vehicle and another one might have a confidence that

36:51.680 --> 36:55.760
there's 85% that there's a vehicle in the image.

36:55.760 --> 37:00.800
And so what you'll do is you'll take all of these images that got 85% confidence and

37:00.800 --> 37:05.520
you'll just assume that there's a car in there and you'll use that to go and retrain

37:05.520 --> 37:09.920
this model with, say, another thousand images.

37:09.920 --> 37:15.240
And then you'll iteratively do this as you improve your and continue to improve your

37:15.240 --> 37:17.040
deep learning models.

37:17.040 --> 37:22.240
Can you speak to the specific kinds of results you've seen with doing active learning for

37:22.240 --> 37:24.240
these types of models?

37:24.240 --> 37:34.160
Yeah, so for some of the like bounding box use cases, we'll see customers may have, they'll

37:34.160 --> 37:36.000
run jobs with contributors.

37:36.000 --> 37:42.560
So they'll have maybe like 100,000 images that these contributors have manually labeled.

37:42.560 --> 37:47.480
So they're confident that the labels in these images are accurate.

37:47.480 --> 37:50.120
And then they'll go and train this model.

37:50.120 --> 37:57.120
And when they try to run new images through the model may be producing like 60 or 70%

37:57.120 --> 38:04.360
accuracy of like false positives and actual accuracy of the boxes on the objects.

38:04.360 --> 38:09.800
And as you continue to build active learning and run through iterations of the data, you'll

38:09.800 --> 38:14.960
see this, this accuracy go up like 70 to 80, 85%.

38:14.960 --> 38:21.160
And it's, and it's really like an infinite loop that you're constantly training these cycles.

38:21.160 --> 38:22.160
Interesting.

38:22.160 --> 38:27.840
So we've talked about, we've covered a bunch of ground in terms of just the data collection

38:27.840 --> 38:30.600
and annotation process are there.

38:30.600 --> 38:35.040
And you think of things that we have not spent enough time on and that we should dig into

38:35.040 --> 38:36.040
a little bit?

38:36.040 --> 38:43.440
Well, I'd say another important aspect of AI and ML is really understanding when is the

38:43.440 --> 38:46.400
right time to use AI and ML.

38:46.400 --> 38:52.840
So there may be certain applications where you're trying to do this, but it isn't actually

38:52.840 --> 38:56.720
the right fit for your organization.

38:56.720 --> 39:01.720
So for example, something like 3D printing, right?

39:01.720 --> 39:07.840
A lot of the 3D printing that we see is really based off of specific models that people

39:07.840 --> 39:14.720
are building and these models may be custom for a given individual.

39:14.720 --> 39:20.800
And so it's really difficult to understand where you might integrate ML or AI into these

39:20.800 --> 39:22.640
organizations.

39:22.640 --> 39:27.960
And so you have to kind of take a step back and look at what is the ultimate goal of your

39:27.960 --> 39:34.040
organization and is ML and AI really going to help you achieve that goal.

39:34.040 --> 39:39.400
And if the answer is yes, then you definitely should then go in and dig into your data and

39:39.400 --> 39:44.920
understand really what is my data and how can I pull insights out of it that are going

39:44.920 --> 39:46.960
to help us achieve that goal.

39:46.960 --> 39:52.440
And if not, then there may be other applications that you should move towards that would help

39:52.440 --> 39:55.280
you really achieve your organization's goal.

39:55.280 --> 39:56.280
Awesome.

39:56.280 --> 39:59.320
Well, that definitely brings us full circle.

39:59.320 --> 40:02.080
Anything else that you'd like to leave the audience with?

40:02.080 --> 40:06.040
For anyone listening, please come check out Train AI.

40:06.040 --> 40:12.320
Our website is figure-8.com slash train-ai.

40:12.320 --> 40:17.080
You'll see a whole host of speakers talking about similar topics we chatted about today

40:17.080 --> 40:18.560
and more.

40:18.560 --> 40:23.920
There's also an executive briefing and a hands-on machine learning course on the first

40:23.920 --> 40:25.320
day May 9th.

40:25.320 --> 40:26.920
So come check it out.

40:26.920 --> 40:33.440
Yes, definitely check it out and be sure to use the code Twimble AI for 30% off of

40:33.440 --> 40:34.440
registration.

40:34.440 --> 40:35.440
Karen, thanks so much.

40:35.440 --> 40:37.680
It was great chatting with you.

40:37.680 --> 40:39.680
It was great speaking with you too, Sam.

40:39.680 --> 40:40.680
Thanks for having me.

40:40.680 --> 40:41.680
Thank you.

40:41.680 --> 40:45.520
All right, everyone.

40:45.520 --> 40:50.880
What's our show for today for more information on Karen or any of the topics covered in this

40:50.880 --> 40:51.880
episode?

40:51.880 --> 40:57.880
You'll find the show notes at twimbleai.com slash talk slash 130.

40:57.880 --> 41:02.440
Thanks again to figure-8 for their continued support of the podcast and their sponsorship

41:02.440 --> 41:04.280
of this episode.

41:04.280 --> 41:17.800
Thanks so much for listening and catch you next time.

