WEBVTT

00:00.000 --> 00:14.480
All right. Let's get this started. Hi, Dawa. Hi. So I'm talking to you from my home in San Francisco. Where are you?

00:14.480 --> 00:20.320
I'm in Palo Alto, not too far away from you actually. Also at your home. Also at my home, yes.

00:22.080 --> 00:28.000
The new office. The new office, yeah. So we do actually have an office hugging face Silicon Valley

00:28.000 --> 00:34.560
office in Palo Alto, not too far away from here, which we opened recently. But yeah, it's I'm still

00:34.560 --> 00:40.560
getting used to going to an actual office. I really like my home office. Yeah, it's kind of here

00:40.560 --> 00:49.200
to stay. So this is really exciting for me because for a number of reasons, one hugging face is one of

00:49.200 --> 00:55.760
the most interesting companies today. So especially in the machine learning space, but most especially in

00:55.760 --> 01:02.960
the natural language processing space, which is where I work. And yeah, I saw the tweet in January

01:02.960 --> 01:09.120
that you sent out announcing that you were the new head of research at hugging face. And I've

01:09.120 --> 01:14.240
been dying to talk to you ever since. And it's been a good six months. So you've had times to

01:14.240 --> 01:20.240
settle in, find your feet, get up to speed, actually maybe make an agenda and plan for yourself

01:20.240 --> 01:25.920
at hugging face. So it seems like a great time to catch up. And also a lot of the listeners of this

01:25.920 --> 01:35.440
podcast will have heard Tomas Wolf from Hugging Face, one of the founders. Is that right? How would

01:35.440 --> 01:42.880
you describe Tomas? He's one of the three co-founders. And he's our chief science officer.

01:42.880 --> 01:49.600
So many on this in listening to us right now will have heard Tomas interviewed by Sam three months

01:49.600 --> 01:57.120
ago. And so this is and he had a lot to say about research. And so it's a perfect time to dig deeper

01:57.120 --> 02:02.000
into some of the things that he got into. And also to just open up new territory, find out what's

02:02.000 --> 02:07.920
on your mind. How's that sound? Yeah, for sure. Yeah, I thought it was the podcast with Tom was

02:07.920 --> 02:12.640
really amazing. So if people haven't listened to that, I highly recommend people listen to that too.

02:12.640 --> 02:19.200
Yeah. You and I spoke briefly a week or two back. And I took some notes. And I want to give you the

02:19.200 --> 02:25.360
and the listeners kind of the menu of things that came to mind for me that we could touch on. So

02:26.000 --> 02:32.720
big themes I would love to know more about you as a human. And you and Hugging Face, I think a lot

02:32.720 --> 02:38.720
of people probably have a name recognition for Hugging Face, but probably don't know really what

02:38.720 --> 02:44.720
it is. So it'd be good to dig into that a little bit. And then the main dish of the course,

02:44.720 --> 02:51.600
let's dig into the future of NLP. Yeah, one thing I'd like to emphasize is that Hugging Face is

02:51.600 --> 02:59.040
no longer an NLP company per se. So we are doing a lot of very interesting work in computer vision

02:59.040 --> 03:05.040
and speech and other areas of AI. So I like to think of Hugging Face as an AI company.

03:05.040 --> 03:09.680
Yeah. And so that's a perfect seg. Let's dig into that. So Hugging Face used to be an NLP

03:09.680 --> 03:15.520
company. I think it's safe to say. And it's really been expanding. I looked on CrunchBase

03:15.520 --> 03:20.880
just to see what the basic stats are these days. It's like somewhere between a hundred and 200

03:20.880 --> 03:29.760
people, Series C, and based in New York officially, although quite remote now like the rest of us.

03:30.480 --> 03:38.720
Yeah. And so when you joined, it was already transitioning into something bigger than NLP.

03:38.720 --> 03:43.680
Yeah. What was your perception of Hugging Face? How would you have described it like before

03:43.680 --> 03:51.600
you joined and now that you've joined? Yeah. So I've always been impressed by Hugging Face and how

03:51.600 --> 03:56.160
it presents itself to the outside world. It's a very open and transparent organization

03:57.760 --> 04:04.000
where it really is about a community effort to democratize a lot of the tools that everybody

04:04.000 --> 04:10.800
uses. So from data sets to models, so Transformers Library, of course, also the Hub, which is really

04:10.800 --> 04:17.520
a crucial part of the AI ecosystem these days, I think. So I've just always been very impressed

04:17.520 --> 04:25.600
by it. And so that's why I chose to join this company. I think it really is a special

04:25.600 --> 04:30.480
special place and it really plays a special role in the community. So I don't think that a company

04:30.480 --> 04:36.400
like Google or Meta could play the same role that Hugging Face plays in this ecosystem.

04:37.200 --> 04:43.920
I agree. I agree. It's a pioneer with open source for sure. So something else that I really

04:43.920 --> 04:48.160
like about Hugging Face is how European it is and now actually very international. The people

04:48.160 --> 04:53.600
are just, they come from all over the place. Did you know any of the core Hugging Face people

04:53.600 --> 04:59.920
before you joined? Yeah. So I mean, I met Tom a few times before and I knew Victor and a bunch

04:59.920 --> 05:07.200
of others Victor San. So it's funny actually that you mentioned the Europeanness. So I'm a European

05:07.200 --> 05:12.560
as you can tell from my accent. I'm originally from Holland, but I live in California and I spent

05:12.560 --> 05:18.320
some time in the UK and in New York before I moved to California. But Tom, my boss, actually lives

05:18.320 --> 05:24.880
in Utrecht in the Netherlands, which is where I studied for my undergrad. So and Tom is not

05:24.880 --> 05:31.120
Dutch. But you didn't cross paths during your years in the Netherlands? No, no, no. I left

05:31.120 --> 05:36.000
Holland more than 10 years ago. So I don't think Tom's been living here for 10 years. So it's not

05:36.000 --> 05:41.120
a Dutch mafia. It's a coincidence. It's a French mafia if anything. So the founders are French.

05:44.320 --> 05:50.080
So in terms of the company at large, what I find fascinating is that we have people I think in

05:50.080 --> 05:55.200
Morgan 25 countries all over the world. So in the science team, we have people on the west coast,

05:55.200 --> 06:02.160
on the east coast of the US and in Canada and lots of different places in Europe and in South

06:02.160 --> 06:11.840
Korea. And Turkey as well. I have a friend based in Istanbul. Yeah. Let's see. What is your job?

06:11.840 --> 06:26.240
Good question. I wish I knew. So broadly speaking, I'm just trying to help the team realize this

06:26.240 --> 06:32.640
very ambitious vision that the founders have for the company and for the science team inside the

06:32.640 --> 06:40.880
company. So yeah, it's not really a well-defined role. I think it also kind of depends on what stage

06:40.880 --> 06:47.280
we're in in a given research project, for example. So I'm kind of discovering that as I go along.

06:47.280 --> 06:51.840
So the official title is Head of Research. That's right. And so then comes the question,

06:51.840 --> 06:56.880
what is research at Hugging Face? How is it different from research at a university or research

06:56.880 --> 07:00.960
at a big company like Facebook slash meta, which is where you came from before this?

07:01.680 --> 07:06.080
Yeah. So we're trying to go for a bit of a different model. I think if you want to compare it to

07:06.080 --> 07:11.760
to a single place, then maybe something like DeepMind or OpenAI is closer to what we're trying to do

07:11.760 --> 07:19.120
than meta. So yeah, as you mentioned, I've been at Fair for five years and it was a wonderful time.

07:20.480 --> 07:26.720
But one of the things that was difficult at Fair was that it's very bottom up, which in theory

07:26.720 --> 07:32.640
sounds really nice, but it makes it very difficult to do very big ambitious projects. So if you really

07:32.640 --> 07:39.440
want to create step change research artifacts, which is what we're trying to do, then you need to

07:39.440 --> 07:44.000
pull together big groups of people and then make sure that they're all aligned in realizing this

07:44.000 --> 07:51.760
vision. And in a bottom up research organization, that's very difficult to do. So what we're trying

07:51.760 --> 07:57.120
to do is find the optimal place between the bottom up approach that Fair and Google Brain and

07:57.120 --> 08:03.440
places like that have and the top down approach, which are DeepMind and OpenAI have, where they have

08:03.440 --> 08:08.960
a benevolent dictator like Demis or Ilya, basically telling people what to do and what the

08:08.960 --> 08:13.040
vision is. And we're trying to occupy the middle ground a little bit there and really try to use

08:13.040 --> 08:19.280
the things that make us special. So that's the ability to move fast, the ability to work with

08:19.280 --> 08:26.240
the community, like we've been doing with projects like big science, and to really to exploit the

08:26.240 --> 08:33.120
things that make us unique. What's the difference between big science, which is a project involving

08:33.120 --> 08:38.080
lots of external people, as many as a thousand or signed up from what I heard from Thomas,

08:39.360 --> 08:44.480
probably more like hundreds that are active participants on a daily basis, but that's big.

08:45.280 --> 08:51.520
And then the research team at Hugging Face, describe your actual, what would you call the

08:51.520 --> 08:58.640
actual research team at Hugging Face? Is it like 10 people, 20? So I think last count was 30,

08:58.640 --> 09:04.960
35 people actually. Okay, big group. Science is one of the projects we have going on. So I can tell

09:04.960 --> 09:10.400
you a bit about the other projects we have going on. So one of the advantages of being at Hugging

09:10.400 --> 09:14.560
Face is that it's a super transparent and open company. So I can just tell you everything that

09:14.560 --> 09:24.880
we're doing without feeling bad about it. So no secret sauce revealed. So we have a project around

09:25.600 --> 09:32.000
multimodal models. So multimodality, I think everyone agrees is very important for the future

09:32.000 --> 09:37.120
of AI. And when you say multimodality, for those listening in, you're referring to more than

09:37.120 --> 09:43.280
just text, more than just images, all kinds of sensory, what we would think of as sensory modalities

09:43.280 --> 09:47.680
or information modalities for humans, you're trying to capture that for models, but all at once.

09:48.320 --> 09:53.840
Yeah, all at once. So I think if you look at more recent multimodal work, it's very often just

09:53.840 --> 09:59.760
text and images, but there are all kinds of different modalities that you all might want to

09:59.760 --> 10:03.760
integrate into one single model. So how many modalities are you stuffing in?

10:05.040 --> 10:12.640
So right now, it's images, text, videos, and audio, because those are the main ones. And then once

10:12.640 --> 10:18.080
you have those, then you can start thinking about other specific modalities, maybe sort of submodalities,

10:18.080 --> 10:23.520
right? So it's unclear whether code as a modality is a part of text or if it's something else.

10:24.320 --> 10:29.440
So there's all kinds of interesting questions about what the modality really is. So my PhD thesis

10:29.440 --> 10:34.960
actually was about grounding meaning in perceptual modalities, where I also incorporated all

10:34.960 --> 10:41.600
factory semantics. So you can build a bag of chemical compounds model and build smell vectors,

10:41.600 --> 10:48.080
essentially, and do interesting things with that. So that's a long time ago, but yeah, there's

10:48.080 --> 10:53.120
a lot of potential there. What does the word grounded mean in this context? So let's use NLP.

10:53.120 --> 10:58.480
Let's use an example like you have a model that, you know, like GPT-3. So it's learned how to

10:58.480 --> 11:04.480
generate text. What does it mean for that model to be grounded? Yeah, so I was going to say,

11:04.480 --> 11:10.240
I think the word grounded isn't pretty well defined, but I'm a philosopher by training originally,

11:10.240 --> 11:15.200
so I would argue that most things are not well defined. But in my thesis, I make an explicit

11:15.200 --> 11:20.480
distinction between referential grounding and representational grounding. And so I think

11:20.480 --> 11:26.480
referential grounding is what people often think about with like referral data sets. So those

11:26.480 --> 11:31.440
exist in computer vision, for example, where you have to pick out the object. So when someone says

11:31.440 --> 11:36.320
banana, then you have to be able to point into image where the banana is. But I think the

11:36.320 --> 11:41.520
much more interesting type of grounding is representational grounding where you have a holistic

11:41.520 --> 11:46.560
meaning representation of a concept like elephant and you or violin, maybe it's a better example.

11:46.560 --> 11:52.160
And so you know the semantic meaning of violin, you can go to Wikipedia and look up what violin is,

11:52.160 --> 11:56.400
what that means. But you also have a visual representation of it and you know what it looks like,

11:56.400 --> 12:00.400
you know, what it sounds like, maybe you know what it smells like, what it feels like, what it's

12:00.400 --> 12:06.880
like to play it, all of these different modalities are a part of your overarching meaning representation

12:06.880 --> 12:12.240
of the concept of violin. And I think that is the much more interesting type of meaning representation.

12:12.240 --> 12:16.960
And so that's the meaning we should try to get into machines if we want them to be able to

12:16.960 --> 12:21.680
really understand humans. So what are the, what are some of the problems you see with today's

12:22.240 --> 12:29.520
models that reveal that they're insufficiently grounded? Yeah, so I don't know if we're sure

12:29.520 --> 12:33.840
that models are insufficiently grounded. I think that's still an empirical question,

12:33.840 --> 12:37.920
but my hunch and I think a lot of people in the field share that hunch is that you need to have

12:37.920 --> 12:43.040
some understanding of the world as humans perceive it if you really want to understand humans.

12:43.920 --> 12:51.040
And so there's a lot of communication that happens between humans that never really

12:51.760 --> 12:56.400
becomes explicit. So people call this common sense, for example. So the example I always use is

12:56.400 --> 13:01.520
coffee and what coffee smells like. Everybody knows what coffee smells like. So I never have to

13:01.520 --> 13:08.000
explain that to anyone. And so for that reason, I also just have no idea how to describe the smell of

13:08.000 --> 13:12.800
coffee. I don't know if you can try that or describe the smell of a banana in one sentence.

13:12.800 --> 13:17.040
Like you've never had to do that because you know that everybody knows what banana smell like.

13:17.040 --> 13:22.400
And if you could pull it off, we would call you a poet. Yeah, exactly. So I think you're

13:22.400 --> 13:27.520
totally right. So you have to fall back to associations then because there is no descriptive language

13:27.520 --> 13:33.360
for this sort of stuff. And I think this happens all over the place in natural language communication

13:33.360 --> 13:39.840
between humans. And that makes it very hard for machines to learn this stuff just from reading Wikipedia

13:39.840 --> 13:44.080
or whatever corpus they're trained on. It's funny. You're very much coming at this as a philosopher,

13:44.080 --> 13:49.520
I could see. There's another angle, which is where I'm coming from. So you know, I'm at a company

13:49.520 --> 13:55.840
that is on the applied side. So we're using NLP to try and solve problems. And where I see what

13:55.840 --> 14:00.560
seems to be the grounded problem is the model clearly, if you just poke a little bit, it clearly

14:00.560 --> 14:06.640
doesn't understand what it's talking about. You know, it'll say all the right things. And then

14:06.640 --> 14:12.960
it reveals that it actually has no common sense understanding of what coffee is. Because it'll say

14:12.960 --> 14:18.480
something that's a human would find crazy. Yeah, but so I think the word understanding,

14:18.480 --> 14:23.200
what does understanding even mean there? I mean, so I think what you're maybe talking about.

14:23.200 --> 14:27.360
And that's so I think there are two main things missing in our current paradigm. One is

14:28.240 --> 14:34.560
multimodal understanding of concepts. And the other is the intentionality with a T of language.

14:34.560 --> 14:40.400
So the fact that we use language with an intent to change the mental state of whoever we're

14:40.400 --> 14:47.360
talking to, right? So I'm using my voice now to change your brain essentially. And so that intent

14:47.360 --> 14:52.080
is is crucial for real meaning and real understanding. And it's something that doesn't exist

14:52.080 --> 14:59.920
in language models. Do you reckon that we have to give real agency to systems to achieve that?

14:59.920 --> 15:04.560
To like have them care about something? And maybe with reinforcement learning or other paradigms?

15:05.200 --> 15:11.840
I think so. Yeah. So I don't know if agency, I mean, I don't want to keep like going on the

15:11.840 --> 15:18.800
definitions, but so agency is also a bit unclear, I think. So it's more, yeah, you can model the

15:18.800 --> 15:25.280
intent of communication when you're trying to model human communication. You can try to model

15:25.280 --> 15:30.560
the intent as a part of the interaction. So you could think of so the two things I just talked

15:30.560 --> 15:36.080
about you could integrate them in language models pretty easily, right? So you could have a language

15:36.080 --> 15:41.280
model that also has a multimodal input. Maybe you can put it in an embodied environment where it

15:41.280 --> 15:48.160
can walk around. And then maybe you can have multiple of these language models walking around

15:48.160 --> 15:52.880
in that world and interacting with each other and other humans. So if you put all of that together,

15:52.880 --> 15:58.240
then I think you get something very close to how humans learn language. Is this where you think

15:58.240 --> 16:03.040
Hugging Face is headed? Is this one of the grand directions? This is definitely one of the grand

16:03.040 --> 16:07.200
directions. Yeah. So one of our projects is multimodal. As I said, another one is about

16:07.200 --> 16:12.800
embodied learning. Thomas also talked about this when he spoke on this podcast. Yeah, the way he

16:12.800 --> 16:18.320
described it was, maybe we need to teach models language more like we teach humans language,

16:18.320 --> 16:24.000
which is in the world trying to get things done. Exactly. Yeah. So and that's because we want

16:24.000 --> 16:29.840
the models to use the kind of language that's useful for interacting with humans. So people sort

16:29.840 --> 16:36.160
of gloss over it, but the reason we want to have natural language understanding and natural

16:36.160 --> 16:40.560
language generation capabilities in these models because we want them to interact with humans.

16:41.440 --> 16:46.800
And so I mean, one of the other things I've been pushing a lot for is a more holistic evaluation

16:46.800 --> 16:51.920
of these models where rather than just evaluating them on static test sets, we actually expose them

16:51.920 --> 16:56.160
to real humans and we see how well they do in that setting. And as you as you mentioned,

16:57.360 --> 17:00.800
those models very quickly break down if you try to actually do that.

17:00.800 --> 17:06.640
All right. So a different question. I was really curious. So I consider you a very multilingual

17:06.640 --> 17:11.680
person. I mean, all Dutch people are. If you've ever met a Dutch person, you've met multilingual

17:11.680 --> 17:19.120
people. And here you are in NLP and adjacent. You know, you're you're definitely expanding

17:19.120 --> 17:25.520
beyond that. But you would consider yourself an NLP practitioner. Yeah. I think so. Yeah, kind of.

17:25.520 --> 17:31.440
I mean, I've been branching out for a long time. So I would consider myself an AI person like

17:31.440 --> 17:36.160
so a lot of my work is multimodal, but this is language first. Yeah, language is my my main

17:37.520 --> 17:47.280
interest. How frustrating or bizarre has it felt to be a deeply multi-lingual person in like

17:47.280 --> 17:53.520
a time and science where it's just so English dominated, the research itself, the tools down to

17:53.520 --> 17:57.040
the very data that we're training these things on. And I'm asking this as an obvious

17:57.040 --> 18:02.640
seg to this really exciting, you know, project that's underway to perhaps create the first truly

18:02.640 --> 18:07.280
multilingual based language model as that's my understanding of the project. But I first wanted

18:07.280 --> 18:13.760
to hear just like you, Dawa, like as a deeply multilingual person, you know, like what does it feel

18:13.760 --> 18:19.280
like? What has it felt like to be in this weirdly accidentally English dominated space?

18:19.280 --> 18:24.000
Yeah, so that's a very interesting question, but I don't know if I'm the right person to ask it

18:24.000 --> 18:32.480
because I moved to the UK for my PhD and then I moved to the US and so most Dutch people speak

18:32.480 --> 18:40.640
pretty decent English, I think. So I think where the accessibility of language models and the

18:40.640 --> 18:45.200
multilinguality of language models where that really matters is for people who are

18:45.200 --> 18:52.960
monolingual and who don't speak English. So people who can't easily access this technology

18:52.960 --> 18:57.120
because it's limited only to English. But I think that doesn't really apply to

18:57.120 --> 19:01.840
most Dutch people because they go very easily switch over as you mentioned.

19:01.840 --> 19:05.920
But also like using these things to make sense of the world that's not written in English.

19:05.920 --> 19:10.400
Like I could tell you how hard it is because that's my day to day is like dealing with Chinese,

19:10.400 --> 19:17.280
Russian or other languages like the tools and the data is far, far weaker.

19:17.840 --> 19:23.680
Oh yeah, yeah for sure and I think there's also very interesting underlying questions there about

19:25.120 --> 19:32.880
the cultural differences that manifest themselves in languages. So English as a language is very

19:32.880 --> 19:39.040
explicit so you can be relatively low context in how you communicate. So you're just very explicit

19:39.040 --> 19:44.000
or you know some people would consider Americans relatively blunt. I think in how they communicate

19:44.000 --> 19:49.680
same for Dutch people anyway. But if you think about like Japanese language which is very sort of

19:49.680 --> 19:57.360
indirect and very different in a sense from English I think that also manifests itself in the culture.

19:57.360 --> 20:02.080
So maybe there are just things that you can really capture about Japanese culture because you have

20:02.080 --> 20:08.160
a specific type of language model. So tell us a bit about the ongoing experiment to make a truly

20:08.160 --> 20:13.520
multilingual model. Yeah so this is the big science model. It has a name now it's called Bloom

20:14.240 --> 20:20.320
which I think is a really nice name because the logo of big science has also always been a flower.

20:20.880 --> 20:27.440
So the flower is starting to bloom and so this language model it's as you said the first

20:27.440 --> 20:35.840
big multilingual language model and it is only a few weeks away from being done training so

20:35.840 --> 20:42.240
it's been very cool you can just follow it on Twitter. There's a regular Twitter update whereas

20:42.240 --> 20:48.720
like we're at like 87% or something now and so have you been playing with checkpoints?

20:49.520 --> 20:54.880
Yeah so there's something called the Bloom book where people have been able to just submit

20:54.880 --> 21:01.120
problems and then someone would run them and store their output somewhere for people to inspect

21:01.120 --> 21:05.680
and so we're releasing some checkpoints soon as well for people to talk to and then when the final

21:05.680 --> 21:10.560
model comes out it's also going to be released so that people can play with it themselves.

21:10.560 --> 21:16.880
Cool. Is it a basic text-to-text autogressive model? Same architecture as your typical big text-to-text

21:16.880 --> 21:24.560
models? Yeah basically yeah so it's I think by design that there hasn't been too much divergence

21:24.560 --> 21:30.400
from the sort of standard language model that people are used to but there are some nifty new things

21:30.400 --> 21:37.840
in there so it uses like a LMI for like how to do the token embeddings and things like that so there

21:37.840 --> 21:42.560
are a couple of nice different things in there but yeah the main architecture is exactly what you

21:42.560 --> 21:48.160
would expect. Let's dig into that. A lot of people on this call won't really even know what a token

21:48.160 --> 21:53.440
or a tokenizer is. I think this is a really neat part of NLP. It's just very much like the tools

21:53.440 --> 21:59.280
you use kind of talk but let's just like take a moment. Tell us what is a token, what is a tokenizer

21:59.280 --> 22:05.680
and then like how did you do it differently with this this big bloom model and why did you have to?

22:06.400 --> 22:12.400
Yeah so I'm not I'm not the the right person to really answer detailed questions about the tokenization

22:12.400 --> 22:19.440
of the language model but that so I can explain what what tokenization is so it's basically just how

22:19.440 --> 22:25.680
do you cut up your your text so you know a sentence consists of words so you could just cut it up

22:25.680 --> 22:34.960
in the white space and and just every word is a token but that is inefficient so what people have

22:34.960 --> 22:40.160
been doing is trying to chunk it up in smarter ways because then you'd have like a vocabulary of

22:40.160 --> 22:46.400
millions right and with multiple languages it could be huge. Yeah so especially if it's multilingual

22:46.400 --> 22:51.120
maybe you just don't see words often enough to really have a very good understanding of their

22:51.120 --> 22:56.960
meeting so a good representation of their meeting and so what you can do is you can chunk

22:57.760 --> 23:03.760
different segments of words together in smart ways so so this is BPE by parent coding and things

23:03.760 --> 23:11.680
like that and so there has been a working group in the big science workshop so it's like a one-year

23:11.680 --> 23:17.200
workshop is how we're thinking about it and so I think there are 40 50 different working groups

23:17.200 --> 23:22.080
and there was one working group working on tokenization they wrote a very nice survey paper

23:22.080 --> 23:27.680
they did a big analysis of what the right tokenization is and one of the things that they found

23:27.680 --> 23:33.040
I think also together with under like the main model working group is that these alibi

23:33.840 --> 23:40.240
positional embeddings that really help so this was just an empirical finding and and so so

23:40.240 --> 23:45.920
you know there's just a lot of this small research that went into this this whole endeavor.

23:45.920 --> 23:50.800
So why not just go all the way down to the individual character? Why mess with tokens at all?

23:50.800 --> 23:55.840
Yeah it's a good question I mean there are admin efforts in this direction or like

23:56.800 --> 24:02.800
so back in the days there were like character RNNs before Transformers and people were trying

24:02.800 --> 24:07.600
to get this to work it sort of worked but it didn't really really work. It was a great way to

24:07.600 --> 24:14.320
generate made-up silly words. Yeah yeah for sure yeah and so yeah I think there's also an

24:14.320 --> 24:19.840
interesting possibility there where we reduce everything to the byte level and so when you think

24:19.840 --> 24:26.800
about like Unicode or or UTF-8 or things like that like in theory every single character can just

24:26.800 --> 24:31.440
be be modeled at the byte level and then maybe that's the future and then maybe you could even like

24:32.640 --> 24:37.920
put images and audio and everything is just bytes and so basically you can just have a

24:37.920 --> 24:44.240
pre-trained byte level model so I think that's an interesting research direction and there's

24:44.240 --> 24:49.600
been some work on that but it so far it hasn't really proven to be better than just smart ways

24:49.600 --> 24:55.520
of tokenizing your data. So maybe the real explanation for it not working yet is that we haven't

24:56.080 --> 25:02.480
used enough data yet so maybe we just need even more data as always Ben. Thomas mentioned 800

25:02.480 --> 25:07.520
gigabytes. What does that actually translate to in terms of like how much of the internet did you

25:07.520 --> 25:15.200
grab for this? I understand you crowdsourced it. Yeah so it was crowdsourced with a big community

25:15.200 --> 25:22.800
of collaborators who were part of this big science effort and so it's not really a crawl so it's

25:22.800 --> 25:27.680
very hard to say like what percentage of the internet is this it really depends on the language

25:27.680 --> 25:33.920
and the folks who contributed the data for their own language. I think some of them also had

25:33.920 --> 25:41.760
different approaches so it's a very kind of targeted way of collecting data and that's one of the

25:41.760 --> 25:46.240
beauties of this big science effort. So I think there's a lot of emphasis on this bloom model

25:46.960 --> 25:52.720
but what's also very interesting about the overarching endeavor is that we have this data set which

25:52.720 --> 25:59.920
is really beautiful and curated by experts in those languages. It has a very interesting coverage

25:59.920 --> 26:07.040
of different languages geographically over the whole world. I don't know what the latest numbers

26:07.040 --> 26:13.840
know in the 40s or 50s I think 45. Wow so this is a huge collection of languages and it includes

26:13.840 --> 26:18.960
like low resource African languages and things like that so I think that's really great and so

26:18.960 --> 26:23.760
there's the data effort but then like the the legal side of this like how do you distribute the

26:23.760 --> 26:29.600
model the governance side of the data itself. All of these the super intriguing questions have just

26:29.600 --> 26:35.280
been explored by the community completely in the open so it's just fascinating for me I'm sort

26:35.280 --> 26:42.160
of an outsider right so just yeah so I mean me too in a way like this started about a year ago

26:42.160 --> 26:48.720
I think or more than that and so I've just been following it from the sidelines and I'm still

26:48.720 --> 26:53.040
kind of like not directly involved in it that much and it's just amazing to see.

26:53.040 --> 27:00.720
So okay now back to you so here you are six months into your new role at Hugging Face.

27:02.000 --> 27:09.040
Give us a sense of like what you thought your job would be when you started and now six months

27:09.040 --> 27:14.640
later like what has changed what's the newest thing that you've learned about yourself and Hugging

27:14.640 --> 27:19.040
Face and the mission you know like what gets you out of bed in the morning that's changed.

27:19.040 --> 27:29.840
Yeah no that's a very interesting question I mean I think the job has been what I expected sort of

27:29.840 --> 27:35.280
so I knew going into this that it's just an amazing team and like we really have some some

27:35.280 --> 27:41.520
brilliant researchers in this team so I was very excited about getting to work with those folks

27:41.520 --> 27:49.040
and so that's been really awesome. I think one thing that I maybe didn't really expect is

27:50.400 --> 27:55.520
when you're a company like Hugging Face and you're this distributed all across the globe

27:55.520 --> 28:00.320
you have to be very decentralized so a lot of the communication happens asynchronously

28:00.320 --> 28:07.360
on Slack in public channels which I think is great and so Hugging Face really has a unique culture

28:07.360 --> 28:13.280
that supports this way of working together but if you come from a different working culture like

28:13.280 --> 28:20.800
me coming from Mehta that is quite the transition to make and so especially you can't just go to

28:20.800 --> 28:26.400
a whiteboard with people. Yeah so everything is remote but it's not even just remote where you're

28:26.400 --> 28:33.680
both like talking to your computer or resume it's like it's remote also in time so one of the things

28:33.680 --> 28:39.840
I'm struggling with is just time zone I think so I'm in California right so I'm sort of trailing

28:39.840 --> 28:48.320
the world and and so when when I wake up or when my son wakes me up at around 7 a.m. then I check

28:48.320 --> 28:53.200
my phone and I have like a million Slack messages and emails and things to read through and then

28:53.200 --> 28:58.800
usually my meetings start at 8 a.m. because I need to make sure I can talk to the Europeans

28:58.800 --> 29:04.640
and then they stop working soon after that and so I'm always kind of like trailing in time

29:05.360 --> 29:10.320
which is which is not easy. So you didn't see that coming? I was not prepared for that yeah I'm

29:10.320 --> 29:15.120
I'm still still adjusting but I mean it's an interesting learning experience and it's just

29:15.120 --> 29:20.400
fascinating I think to see like where the world is going with remote work and so this is the

29:20.400 --> 29:26.000
future way I think in which a lot of companies are going to be doing this. So what's it like

29:26.000 --> 29:30.720
running and building and nurturing a research team at a startup? I think that's something that people

29:30.720 --> 29:35.360
will be really curious about. I think a lot of a lot of people will be familiar directly or

29:35.360 --> 29:41.040
indirectly with how a research group even 30 strong like you said at a university works you know

29:41.040 --> 29:46.320
you've got a PI and that PI's job is mostly to get grant money and then you've got the postdocs

29:46.320 --> 29:51.040
who actually run the show and then you've got like grad students who are ranging from miserable to

29:51.040 --> 29:56.400
pretty happy and then you've got like interns and undergrads. Does it have anything like that

29:56.400 --> 30:01.840
structure? Is it just a totally different beast? Yeah and it's very different so I am definitely not

30:01.840 --> 30:11.520
a PI so I'm more a facilitator I think or a coordinator and so we have a very flat non-hierarchical

30:11.520 --> 30:18.080
organization. We do have team leads so those those would be closer to PIs I think so we have a

30:18.080 --> 30:23.600
multimodal project and it has very clear team leads and you know things like that. So my role

30:24.800 --> 30:30.960
is it's more like a sort of serving leader where I just try to to help people the best way I

30:30.960 --> 30:35.680
possibly can and to make sure they don't have roadblocks and and that people are talking to

30:35.680 --> 30:40.480
each other and that I'm aware of what's going on and I try to connect people to the right people

30:40.480 --> 30:45.280
and connect ideas to the right ideas. So it sounds like pretty normal management actually.

30:45.280 --> 30:51.600
Yeah but yeah I guess you could say that but it's very different from normal management at the

30:51.600 --> 30:56.080
same time I think because of how decentralized the company is and because of all of the other

30:56.080 --> 31:00.320
things that are just very different from from a traditional management role it like a big tech

31:00.320 --> 31:04.960
company. Well and also the fact that there's like a thousand strong group of people outside the

31:04.960 --> 31:08.880
company that you actually have to work with and coordinate with. Yeah but that's just a big

31:08.880 --> 31:14.800
science project right so I think I mean you make an interesting point that one of the things that

31:14.800 --> 31:21.200
makes hugging phase so special is that the community plays such a big role in the company and that's

31:21.200 --> 31:27.120
not just big science right so like if you look at Transformers the library and the open source ecosystem

31:27.120 --> 31:32.320
and data sets and things like that that's a huge community and all of these people are also

31:32.320 --> 31:38.480
contributing actively to making these tools so awesome. Yeah no I remember the day we first

31:38.480 --> 31:45.680
started using your Transformers library at my company primer it was a revelation you just like I

31:45.680 --> 31:54.320
can't under it I can't say enough about how positive the open sourcing of Transformer language

31:54.320 --> 32:01.520
models was and I think hugging phase deserves most of the credit just like yeah. I think one of the

32:01.520 --> 32:07.600
the the reasons that Burke became so popular so quickly was because of the Transformers Library

32:07.600 --> 32:13.520
or the predecessor right so Pythorch pre-trained bird I remember I was at a workshop at this

32:13.520 --> 32:17.600
Santa Fe Institute they do these workshops where they invite a bunch of people and they talk

32:17.600 --> 32:24.160
about some stuff and Fernando Pereira was there the the Google director of research I think

32:24.960 --> 32:30.400
and he was saying like we have this thing coming out and it's going to like blow everything out

32:30.400 --> 32:34.560
of the water it's amazing it's going to revolutionize NLP and like I've heard people say that before

32:34.560 --> 32:42.000
and I never really believed it but in this case he was right so so birds yeah so it dropped like

32:42.000 --> 32:46.320
I think two weeks later or something and then so everyone wanted to play with it and being in

32:46.320 --> 32:51.680
fair obviously Pythorch was the preferred framework and and it took like I don't know like a week or

32:51.680 --> 32:56.960
two before there was this Pythorch pre-trained bird model that everyone was playing with so it's

32:56.960 --> 33:02.720
amazing and so I did some snooping your most cited paper at least according to Google scholar

33:02.720 --> 33:09.200
is this 2017 paper on sentence representations why I think that's so so notable is that that's

33:09.200 --> 33:18.080
like just on the before side of Bert so you know Bert comes out in October 2018 something like that

33:18.080 --> 33:23.600
and so like well a full year before that you were deep in NLP solving hard NLP problems

33:24.400 --> 33:30.160
do you remember how crazy it was when suddenly like on the other side of that line when we had

33:30.160 --> 33:35.840
language models all the things in NLP that were really hard and tedious and you needed so much

33:35.840 --> 33:43.360
data to even barely get some performance suddenly became kind of routine and fun and easy like

33:43.360 --> 33:48.800
I'm not hiding I'm not hiding the reality that like tons of stuff doesn't work and tons of stuff

33:48.800 --> 33:56.240
is still hard but the the things that are hard are new things largely yeah so I agree with with

33:56.240 --> 34:01.600
that but so so to me as a researcher it didn't feel like a very abrupt transition actually so

34:02.320 --> 34:07.040
I think that was much more the case for NLP practitioners that more applied people trying to

34:07.040 --> 34:13.760
use the tools yeah but so for me as a researcher I think like the transition was was actually very

34:13.760 --> 34:19.360
natural and so we were doing things with LSTMs and then okay transformers so LSTMs didn't really

34:19.360 --> 34:24.560
work so you needed attention and so there were so even in infrecent we were also experimenting

34:24.560 --> 34:29.040
with self-attention and things like that and then what the transformers paper did is it basically

34:29.040 --> 34:34.720
removed the recurrent so rather than having an LSTM we did it forward just a normal NLP feed

34:34.720 --> 34:40.720
forward network and so it turned out that attention on its own is actually okay right so

34:42.320 --> 34:48.160
from that it became natural to try to do this on just language modeling test so that's GPT

34:48.160 --> 34:52.000
and then if you can do language modeling why not do it bi-directionally because we were playing

34:52.000 --> 34:57.600
with bi-directional LSTMs all the time infrecent is a bi-directional LSTM so birth is just a bi-directional

34:57.600 --> 35:06.720
GPT so it all was very natural I think when it came up so it felt it felt naturally from the point

35:06.720 --> 35:11.440
of view of like no understanding the science but I can tell you from the point of view of people

35:11.440 --> 35:17.520
trying to solve pay solve problems that people will pay you money for no yeah for sure it changed

35:17.520 --> 35:24.960
everything what there is an aspect though scientifically that is new right I was delighted when

35:24.960 --> 35:30.320
this little cottage industry of Bertology suddenly kind of sprouted out of nowhere so here's the

35:30.320 --> 35:36.960
thing you know it struck me that deep learning used to be very much like a branch of mathematics

35:36.960 --> 35:43.040
right because it was part of statistics you know so like all of ML was just math and it felt like

35:43.040 --> 35:48.560
the math world and then suddenly here we are today with models that are so complicated they're more

35:48.560 --> 35:53.920
like biology artifacts we're like kind of prodding them and probing them and trying to understand

35:53.920 --> 36:01.040
things like how how the heck does Bert you know does it understand grammar to what extent does it

36:01.040 --> 36:06.160
do it differently than us suddenly it's feeling more like an empirical science and less like a

36:06.160 --> 36:13.520
branch of math yeah I'm not I'm not sure I'm happy with that actually I also think that that is this

36:13.520 --> 36:17.680
so yeah I have a couple of things to say about that actually so I think this cottage industry of

36:17.680 --> 36:26.000
Bertology is interesting because a few years before that we had a cottage industry in Wortevek

36:26.000 --> 36:30.800
right so Wortevek kind of blew everyone away and then there were a couple of ACLs in the

36:30.800 --> 36:37.120
EMNOPs where just everything was something to veck and it was all just trying to analyze what

36:37.120 --> 36:41.600
Wortevek really did and so I think that's just kind of the progression of science where you have

36:41.600 --> 36:46.720
a big breakthrough model and then there's some consolidation right in the sort of Thomas Kuhn

36:47.440 --> 36:52.240
paradigm shift so there's a real paradigm shifting artifact like Wortevek or Bert and then

36:52.240 --> 36:57.120
there's a lot of consolidation where people try to understand this better so I think that's just

36:57.120 --> 37:02.480
the natural progression and that's just going to continue happening but about Bert specifically

37:03.600 --> 37:09.840
so so we just don't have the correct mathematical tools I think to really understand what it's

37:09.840 --> 37:14.320
learning and so there are some efforts now from like Chris Ola and trying to understand better

37:14.320 --> 37:19.760
what transformers are really learning but so we have a very interesting paper called

37:19.760 --> 37:26.400
Mass Language Modeling and a distributional hypothesis order work does not matter much or something

37:26.400 --> 37:32.240
like that so what we basically show is that you shuffle if you shuffle a corpus and so all of the

37:32.240 --> 37:37.760
senses are not in the right order anymore and you train a Bert model on it it just does just

37:37.760 --> 37:43.600
as well as a regular Bert so you mentioned like does Bert learn grammar which seem like weirdly

37:43.600 --> 37:49.120
seems to suggest that it doesn't matter like that it's clearly doing something differently than

37:49.120 --> 37:54.480
humans do because if you imagine trying to learn language with shuffled language it'd be a nightmare

37:54.480 --> 38:04.480
exactly yeah so so I think yeah maybe we're also over yeah I don't know like oh thinking that

38:04.480 --> 38:08.160
Bert is better than it really is or these sorts of models that they're actually better than

38:08.160 --> 38:14.720
they really are and I think like when you think about GPT-3 and how much of a splash that made

38:14.720 --> 38:20.080
there's also this this element that people just have a natural tendency to anthropomorphize

38:20.080 --> 38:25.600
everything like you do this to like your robot vacuum cleaner and things that you give it a name

38:25.600 --> 38:30.800
and right so that in the the words of Daniel Dennett the philosopher you're ascribing intentionality

38:31.760 --> 38:37.840
and and so I think we do that all the time through everything and we do it especially to

38:37.840 --> 38:42.400
things that produce language because language is essentially the only thing we know that is really

38:42.400 --> 38:48.320
really human only and and so when something produces language we just go out there has to please

38:48.320 --> 38:54.160
something brilliant behind that but very often it's just a higher order distributional statistics

38:55.040 --> 39:01.680
it's just clever hans that's basically we we create these benchmark tests and we watch the

39:01.680 --> 39:07.360
performance on these tests going up up up up and we attribute you know a model like GPT-3

39:07.360 --> 39:11.600
on these language tests as getting truly more clever it truly has a deeper quote-unquote

39:11.600 --> 39:15.840
understanding of the task at hand but then you do these clever experiments like the one you

39:15.840 --> 39:21.440
described with scrambling and it reveals well surely actually it's just using distributional tricks

39:21.440 --> 39:26.560
yeah but so I don't know I think the jury still I wouldn't put a debt strongly I think the jury's

39:26.560 --> 39:34.160
still out so I definitely think that there's an evaluation crisis in NLP and I mean I've been

39:34.160 --> 39:40.320
doing a lot of work with lots of folks in trying to improve that through things like Dynabench where

39:40.320 --> 39:47.440
we do a different we try to rethink benchmarking essentially but I mean it's undeniable that

39:47.440 --> 39:55.120
progress in NLP has just been insane so if you look at like what GPT-3 can do compared to GPT-1

39:56.080 --> 40:03.120
or what we can do now with Dali too compared to I don't know the earlier text to image synthesis models

40:03.120 --> 40:08.080
it's just crazy how fast we're yeah so the progress is real but we should be careful to not

40:08.080 --> 40:13.280
not kind of over-interpret what we're seeing so there's still a lot of stuff that

40:13.840 --> 40:18.560
so there's a headline there's a headline going around just this week about

40:19.360 --> 40:26.080
researcher from Google essentially attributing sentience to the Lambda language model

40:27.600 --> 40:33.040
and I think that's really to your point like what we're talking about it's these things actually

40:33.040 --> 40:39.200
know how to work with language and we humans are language machines we're like completely geared

40:39.200 --> 40:45.600
towards understanding and transmitting receiving and sending information with language that is the

40:45.600 --> 40:51.680
most human information you know intentionality and understand the world around us getting stuff

40:51.680 --> 40:58.800
done and so when some mathematical object is doing it it's I feel it too I can't help it have

40:58.800 --> 41:03.680
you ever kind of like had that feeling that you you just like have to push away of like man I'm

41:03.680 --> 41:08.160
talking to this thing but what do you mean by a mathematical object I mean I think you can argue

41:08.160 --> 41:12.560
that your brain is also a mathematical object or at least you can write your brain

41:13.200 --> 41:18.000
you took the bait I was hoping you shake the bait this is like philosopher catnip

41:20.320 --> 41:26.480
well yeah I don't know so I think this it's very interesting because there's a very nice

41:26.480 --> 41:31.760
theory of consciousness that says that we take the intentional stance towards ourselves as

41:31.760 --> 41:37.280
rational agents and that's what consciousness is there's a strange loop as Douglas Hofstetter calls

41:37.280 --> 41:43.920
it so yeah maybe we're we're evolutionarily hardwired to take an intentional stance towards

41:43.920 --> 41:48.880
things and that's why we're so confused by the NLP progress we've been seeing does it also hint

41:48.880 --> 41:57.600
that a way to achieve artificial intelligence of a more AGI flavor oh yeah so I yeah again like

41:57.600 --> 42:03.360
I don't really know what AGI even means and I think it's very premature to to start thinking

42:03.360 --> 42:09.280
about it and so so we have a philosopher slash ethicist who joined having faced recently

42:09.280 --> 42:17.280
Jado Pestili so she has has made this point on Twitter too I think where there are real problems

42:17.280 --> 42:22.800
right now with the deployment of AI and and so when we're thinking about the applied ethics of

42:22.800 --> 42:28.960
these systems there are just real things we need to fix right now and they are much more

42:28.960 --> 42:34.400
salient and much more important right now than thinking about AGI and paperclip maximizers

42:34.400 --> 42:40.320
and things like that I agree I agree completely there's just I think actually the biggest problem

42:40.320 --> 42:46.320
to solve from an ethics point of view is these systems not working that well on narrow tasks and

42:46.320 --> 42:51.280
people over trusting them that's where a lot of harm can come from not from bias even that's

42:51.280 --> 42:55.840
another level of problem just like over trusting systems misunderstanding their limits

42:56.560 --> 43:01.760
yeah true but so there's a trade-off here too right so we shouldn't hype them up too much

43:01.760 --> 43:06.400
because people will just misunderstand what these systems are capable of but we also shouldn't

43:06.400 --> 43:12.400
underhyped them too much so Sam Bowman professor at NYU is a very nice paper where he talks about

43:12.400 --> 43:17.520
the dangers of underhyping so if we all just pretend that there is no progress at all then at

43:17.520 --> 43:22.880
some point we are going to be very surprised when AGI suddenly emerges and we have sent

43:22.880 --> 43:29.920
you into AI so we definitely should think about this stuff and so AI alignment is a very active

43:29.920 --> 43:35.360
research area and it's a very important research area but it's all about finding that balance I think

43:35.360 --> 43:44.000
sure okay lightning round most exciting things on the horizon for research in NLP that you're

43:44.000 --> 43:49.120
either working towards now or you'd like to go a little bit further than what you're you know

43:49.120 --> 43:54.800
just about to ship just about to publish yeah so I'm very excited in multi-modality obviously

43:55.680 --> 44:01.600
I think that there's a lot of interesting work coming out in semi-parametric models where you

44:01.600 --> 44:08.160
have retriever components and some sort of lightweight reader model on top of that retriever so there

44:08.160 --> 44:13.040
is a paper from DeepMind coming out a couple days it came out a couple days ago the idea and a

44:13.040 --> 44:19.680
nutshell there is it that these language models are basically frozen in time based on the data you

44:19.680 --> 44:24.960
give them and so we need some way to help them keep refreshing what they understand about the world

44:24.960 --> 44:30.400
oh yeah that's just one application I think it's much much more so so it's I think it's much

44:30.400 --> 44:36.720
more about how you learn different things so so as humans we have different kinds of memory we have

44:36.720 --> 44:43.360
a semantic memory and an episodic memory and so we can we also have a library and the internet where

44:43.360 --> 44:49.840
we can look up stuff right so we don't have to store all of it in our parameters our brain so I

44:49.840 --> 44:55.520
think if you do this with models too where you have a big index where you can invest a lot of heavy

44:55.520 --> 45:00.800
compute in having a very high quality index then you can have lots of lighter weight reader models

45:00.800 --> 45:05.920
on top of this so this is also going to have lots of repercussions I think for industry where if

45:05.920 --> 45:11.360
you're a company like Facebook you want to have a million classifiers from all of these different

45:11.360 --> 45:15.760
teams all trying to do cool stuff with their classifier if they have a big index that they can

45:15.760 --> 45:22.560
rely on then you kind of do the computing in a much more intelligent way I think so so it's about

45:22.560 --> 45:28.160
finding the mix between the retriever which will be a big big sort of language model and the

45:28.160 --> 45:32.800
reader model on top which will also be a big sort of language model and just for people who are

45:32.800 --> 45:39.200
unfamiliar with the current status quo you what we have right now is a basically just two kinds

45:39.200 --> 45:44.160
of of information storage you've got the model itself which has been pre-baked with just an

45:44.160 --> 45:49.920
understanding of language and whatever emerges from that just basically predicting missing words

45:49.920 --> 45:54.880
and then you've got what people usually call the prompt which is whatever you can cram into the

45:54.880 --> 45:59.600
attention window at inference time so you can actually put a whole conversation there you can put

45:59.600 --> 46:04.000
example problems you could do a lot of neat things in the prompt but it's pretty darn limited right

46:04.000 --> 46:09.840
it's it's aside from your pre-baked knowledge that's crystallized all these things can do is whatever

46:09.840 --> 46:14.480
you can cram into the prompt and what you're suggesting is hey maybe we could actually build a

46:14.480 --> 46:23.040
whole separate system where they could retrieve knowledge at game time yeah so so there are some

46:23.040 --> 46:27.760
interesting so I've been involved in a model called rag retrieval augmented generation and there's

46:27.760 --> 46:33.920
also realm from from Google and yeah so the basic ideas that you can so you have your your

46:33.920 --> 46:39.440
language model which would be parametric and then you have your K&N like a nearest neighbor search

46:39.440 --> 46:44.080
algorithm essentially which is non-parametric and if you put those two approaches together you get

46:44.080 --> 46:49.760
a semi-parametric model and I think there's there's a lot of potential applications for that

46:49.760 --> 46:54.800
down the line so that's one thing and then the other thing so I said multimodal semi-parametric

46:54.800 --> 46:59.040
and I think the other thing that's going to be interesting and there's a lot of attraction

46:59.040 --> 47:06.080
happening there now too is around data-centric AI so I'm still rooting for things like active learning

47:06.080 --> 47:12.240
becoming much more mainstream measuring our data much more carefully so so we have folks like

47:12.240 --> 47:17.760
make Mitchell in hugging face working on data measurement tools and things like that so really trying

47:17.760 --> 47:22.640
to understand much better what's really happening in our data and trying to do things to the data

47:22.640 --> 47:29.120
or curate the data in different ways so that we can have better models in yet yeah I I quickly before

47:29.760 --> 47:37.760
calling you I actually refreshed my knowledge of what this data tool looks like it's kind of like

47:37.760 --> 47:42.240
an x-ray for data sets and it's a really beautiful idea I'm surprised that no one you know you know

47:42.240 --> 47:46.560
what an idea is a good one when you're like why haven't we been doing this for ages it's just like

47:46.560 --> 47:52.720
all the automatic obvious things you can measure about a data set that that's composed of language

47:52.720 --> 47:56.880
like let's put that all in one toolkit and then you can keep adding to it and make it more and

47:56.880 --> 48:02.320
more sophisticated that's the basic idea right yeah that that's exactly it and and so I think that

48:02.320 --> 48:07.920
it's just nice that so this isn't in a space so it has a graphical user interface it just

48:07.920 --> 48:13.120
exists maybe in the longer term it will be a natural part of the hugging face hub where you can

48:13.120 --> 48:19.040
just upload any data set to the hub and start measuring what's actually in your data and you have

48:19.040 --> 48:24.480
a nice interface where you can just inspect it on the fly so we have have a lot of interesting

48:24.480 --> 48:30.400
things going on in the direction of evaluation and measurement so so I think what's really crucial

48:30.400 --> 48:35.120
when you think about the AI pipeline of the future is that you have raw data which you turn into

48:35.120 --> 48:39.760
data sets and those data sets you turn into models which you want to measure your data sets and

48:39.760 --> 48:43.360
your models and you want to understand what's in there and how well they perform and then you want

48:43.360 --> 48:49.680
to based on your measurement deployed the best model to production so for making predictions with

48:49.680 --> 48:55.600
your model and so so measurement is really absolutely crucial in all of the decisions that you're

48:55.600 --> 49:01.200
making there but measurement is also very difficult and and so that's something that we're we're

49:01.200 --> 49:06.240
trying to address so we have an evaluate library that came up a couple of weeks ago we have some

49:06.240 --> 49:11.200
very exciting announcements coming up soon I don't know when this podcast comes out it might

49:11.200 --> 49:17.200
already be out by that time but we're working on evaluation on the hub so that you can essentially

49:17.200 --> 49:22.320
evaluate any model on any data set using any metric just at the click of a button so you don't

49:22.320 --> 49:28.080
have to do any manual stuff there well surely people are going to miss having to go and copy-paste

49:28.080 --> 49:36.480
massive chunks of scikit-learn code into their Jupyter notebooks come on dour yeah yeah no I think

49:37.040 --> 49:44.320
like lower lowering the barrier to doing proper evaluation according to best practices I think

49:44.320 --> 49:48.720
that's that's a hugely impactful thing to do and that's something that hugging faces uniquely

49:48.720 --> 49:53.920
equipped to do so so that's one of the things I've been excited about also in the science team

49:53.920 --> 49:57.760
you mentioned active learning I'd love to dig into that a little bit that's something I've

49:57.760 --> 50:03.920
worked with myself and it's an enticing idea just for those listening at home who aren't familiar

50:04.800 --> 50:10.080
you know usually when you want to make a data set to train a model you the human have to select

50:10.080 --> 50:16.240
the samples of data from your raw data pool that you're going to gold label and you know train

50:16.240 --> 50:20.400
your model on the idea of active learning in a nutshell is let's put a model between you and the

50:20.400 --> 50:26.960
data sometimes the model you're training and it will decide which ones to put in front of you the

50:26.960 --> 50:33.040
human whose time is expensive and you know whose site is limited and make good choices and try

50:33.040 --> 50:38.320
and find the most instructive examples from the data to labels so that you get the most bang for

50:38.320 --> 50:42.880
buck because no one wants to spend their whole life labeling data in fact there's there's some

50:42.880 --> 50:47.600
problems that that's just prohibitive you literally just can't do it true positives are too rare

50:47.600 --> 50:54.480
etc etc so is there some like really exciting new developments in active learning I feel like

50:55.360 --> 50:59.520
like it's kind of like a done deal is there is there something new and exciting on the horizon you think

51:00.240 --> 51:09.920
yeah I'm not so I think it just has a lot of potential and so what you described is a specific

51:09.920 --> 51:14.320
kind of active learning I think where you have an acquisition function that scores examples and

51:14.320 --> 51:20.320
you just decide which example you want to label but I think there are extensions of these algorithms

51:20.320 --> 51:25.520
where you can think not about the labeling part but about the pre-training part so maybe I can

51:25.520 --> 51:31.280
pick parts of a large corpus that I should be pre-training on now because I know what downstream

51:31.280 --> 51:37.360
task I care about in the end and so so there's a mismatch currently between the pre-training phase where

51:37.360 --> 51:42.400
we just do language modeling or so causal or mess language modeling and then we fine tune it on

51:42.400 --> 51:46.560
something that might be very different from what we're training on so we're we don't really know

51:46.560 --> 51:51.680
what to train on so I think if we can connect the pre-training phase to what we know we are going

51:51.680 --> 51:57.840
to care about then you can do very interesting things and so the way to to do that selection so

51:57.840 --> 52:03.440
data selection is true in acquisition function type of things so that's why it's related to active

52:03.440 --> 52:08.480
learning another interesting thing that I've been working on as well with some folks is dynamic

52:08.480 --> 52:13.680
adversarial data collection where you have a model in the loop and and a human is trying to

52:13.680 --> 52:19.120
fool the model and so if you take the the model fooling examples or or so if you take all the

52:19.120 --> 52:23.920
examples including the ones that didn't fool the model but that are still kind of intended to

52:23.920 --> 52:29.120
try to probe the model for a weakness if you train on that data and you keep updating the model as

52:29.120 --> 52:35.040
you're doing the training so that's the dynamic component then you get a much better model out

52:35.040 --> 52:39.920
in the end so it's really like 10 percent better so we have a nice paper where we try to do this

52:39.920 --> 52:44.560
in the limit over like 20 rounds of natural language inference and you just get a much much

52:44.560 --> 52:49.360
better model out in the end so I think the future of data collection the way we think about it now

52:49.360 --> 52:55.040
in the field is going to be changed a little bit where everything is just always going to be

52:55.040 --> 53:00.160
with models in the loop and that maybe ties back to this long term vision of having language

53:00.160 --> 53:05.360
models interacting with each other in some environment but if you can have humans and models

53:05.360 --> 53:09.760
together interacting with each other and learning from each other and maybe trying to also kind

53:09.760 --> 53:15.200
of probe each other and and be on the decision boundaries of certain things then you can learn

53:15.200 --> 53:19.680
much more efficiently I think it sounds like you're describing education like a school

53:19.680 --> 53:26.960
exactly yeah yeah so yeah so so in terms of education one of the important things is also a

53:26.960 --> 53:33.440
curriculum right so I think one thing you could do with this pre-training like the active learning

53:33.440 --> 53:37.760
of pre-training and connecting that to fine tuning is you could try to have a smarter curriculum

53:37.760 --> 53:43.600
so your if your acquisition function changes over time essentially you're designing a curriculum

53:43.600 --> 53:48.800
or you're learning a curriculum on the fly that helps your language model be as good as possible

53:48.800 --> 53:55.120
on the downstream test as you might care about okay and final question at least that I can think of

53:55.840 --> 54:03.200
is something that Thomas mentioned that intrigued me which is he said that there seems to be

54:03.200 --> 54:09.680
something missing with NLP you know like we making these language models bigger and bigger and bigger

54:09.680 --> 54:16.480
and yes we're improving the data and we're getting more data centric but he gave the impression

54:16.480 --> 54:20.640
that he really believes that there's something fundamentally missing it's not just more data

54:20.640 --> 54:25.840
it's not just more text and you've hinted at least yourself with multimodality and interaction

54:25.840 --> 54:33.840
agency or whatever that means so yeah if you if you had to make a guess 10 years from now

54:35.280 --> 54:41.200
like what do you think the paradigm is going to be will we even talk about NLP I suspect NLP

54:41.200 --> 54:48.000
will be a historical footnote they'll just be AI right and text will be just one of the many

54:48.720 --> 54:56.320
crucial developmental raw material for for artificial intelligence yeah but I still think

54:56.320 --> 55:02.880
that text will just remain a dominant modality so even as it is with humans

55:02.880 --> 55:09.280
exactly right so so language really is very crucial so I don't think NLP itself is going

55:09.280 --> 55:16.080
away but I think yeah in order to get to real meaning all of these other fields are probably going

55:16.080 --> 55:24.560
to be subsumed into NLP when it comes to language understanding so yeah I don't know in 10 years

55:24.560 --> 55:31.520
or now I think we're going to have very very different models in a sense but I think a lot of the

55:31.520 --> 55:35.600
the building blocks that we have now are still going to exist in those models do you think it'll

55:35.600 --> 55:43.920
just be all eventually robotics either real world and or virtual world robots like taking in all

55:43.920 --> 55:50.400
the sensory information virtual robots I definitely buy but so I think for actual physical robots

55:51.440 --> 55:56.880
I think like learning from that doesn't really scale that well so I think one of the big problems

55:56.880 --> 56:01.680
in robotics is similar to real right how do you transfer from a simulation to a real environment

56:01.680 --> 56:07.680
and so as our simulations become reader and reader that problem is going to become smaller and

56:07.680 --> 56:13.920
smaller so I don't think we need physical embodiment in any real sense in order to get to meaning but

56:13.920 --> 56:18.560
we'll probably definitely need virtual embodiment where you have an environment where you can interact

56:18.560 --> 56:23.680
with each other but maybe that environment already exists right so it maybe that environment is

56:23.680 --> 56:29.920
just the internet or maybe that environment will come into existence very soon in the form of

56:29.920 --> 56:34.960
the metaverse or whatever you want to call it any final thoughts you want to share with tens of

56:34.960 --> 56:42.240
thousands of people I think so I think one of the things that I think is important and that's

56:42.240 --> 56:49.600
kind of what hugging face also stands for is just open source and open science and so if I were to

56:49.600 --> 56:55.840
give any parting thoughts I would encourage people to always embrace openness because that really

56:55.840 --> 57:01.360
is crucial to making progress but also making sure that the progress that we make doesn't end up

57:01.360 --> 57:07.120
in the wrong hands or go in the wrong direction so just for those listening at home Daoah where can

57:07.120 --> 57:14.320
we find out more about you and what you do yeah so I have a website it's daoakila.github.io it has

57:14.320 --> 57:21.040
a couple of links to relevant social media profiles also have a Twitter account so that's Daoakila

57:21.040 --> 57:28.400
my name so D-O-U-W-E-K-I-E-L-A and so I'm trying to be more active on Twitter I'm still working

57:28.400 --> 57:33.760
on that and I'm bohan and bot on Twitter and you'll see me probably asking follow up questions

57:34.400 --> 57:39.920
out in the open following your advice Daoah thank you so much this was this was a blast thank you

57:39.920 --> 57:51.200
thanks for having me

