Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Let me start this show by sending a huge thank you to everyone out there listening.
We've dropped a ton of great interviews over the past few weeks, but through your dedication,
we continue to see a growing outpouring of feedback, comments and shares with each release.
If you're a regular listener, but don't normally send in feedback, we would absolutely
love to hear from you.
So please head on over to Apple Podcasts or wherever you listen and leave us a review.
Of course, a 5 star review is appreciated, but what's most important is that your voice
is heard.
It lets us know what you like or what you feel we could improve on, and it lets those
looking for a new machine learning and AI podcast know that they should join the Twimble
community.
Speaking of community, the details of our next Twimble online meetup have been posted.
On Tuesday, November 14th, a 3pm Pacific time will be joined by Kevin T, who will be presenting
his paper, active preference learning for personalized portfolio construction.
If you're already registered for the meetup, you should have received an invitation with
all the details.
If you've yet to join the meetup, head on over to twimbleai.com slash meetup to do so.
We hope to see you there.
Now, as some of you may know, we spent a few days last week in New York City, hosted
by our great friends at NYU Future Labs.
About 6 months ago, we covered their inaugural AI Summit, which is an event they hosted
to showcase the startups in the first batch of their AI Nexus Lab program, as well as
the impressive AI talent in the New York City ecosystem.
Well, we were more than excited when we found out they would be having a second summit
so soon.
This time, we had the pleasure of interviewing the four startups of the second AI Nexus
Lab batch, Mount Cleverist, Bite.ai, Second Mind, and Bowtie Labs.
We also interviewed a bunch of the speakers from the event and will be sharing those discussions
over the upcoming weeks.
In this first episode of the series, you'll hear from Mount Cleverist, a startup started
by childhood friends, James Villarubia, and Bernie Pratt.
Mount Cleverist is an online service for teachers and students that can take any text via
the web and generate a quiz along with answers based on the content supplied.
To do this, they employ a pretty sophisticated natural language understanding pipeline,
which we discuss in this interview.
We also touch on the challenges they face in generating correct question answers, how
they fine tune their machine learning models to improve those answers over time and more.
And now on to the show.
Alright everyone, I am here at NYU Future Labs, meeting with
some of the AI Nexus Lab companies.
And the first company into the interrogation room is Mount Cleverist.
And I'm here with the CEO, James Villarubia, and the COO, Bernie Pratt.
Welcome to this week in machine learning and AI.
Thanks for having us, yeah.
So why don't we get started with an introduction to the two of you, your backgrounds, as well
as the company and what the company is up to?
Sure.
So my background is really as an engineering statistician.
I got an engineering degree from UVA, then a master's in public policy, mostly focused
on tech policy.
And what I found was the coolest, most interesting problems out there to solve were just engineering
ones.
They were political ones.
They're big picture policy stuff.
So I jumped into the Pentagon as a statistician and then briefly went to the White House and
then find the DOJ.
And then I got kind of tired of DC and got the startup bug and came up to here to New
York and helped run a cybersecurity company for a couple of years, until eventually this
idea that we had about Mount Cloverist finally kind of peaked the right person's interest.
And they said, go do that.
And we said, great, we've been trying to get it going for a long time, so we're really
excited.
Thanks, James.
And Bernie, how about you?
Sure.
So I moved up to New York right after college and we did some work in a startup in finance
where we were pulling data out of press releases to send to programmatic traders.
And this allowed us to get into machine learning and natural language processing early on in
my career.
Not as an engineer, but as a manager of engineers doing product work primarily.
From there, I bounced around into a few different places, again, in product positions, but
very, very data heavy, API heavy.
And James and I, we've actually been friends for about 25 years.
We met in kindergarten.
Oh, wow.
Yeah.
And so.
It's the first time I've heard that.
Photo evidence, too.
It used to be part of our pitch.
Nice.
We were friends for a very long time and had always wanted to work together after doing.
So we went to high school together as well and did some good work in a few number of
different areas.
But this was the chance for a school.
Yeah.
We were in ROTC together.
James was doing the band and the newspaper and I was a member of that and we were just
all over the place.
We had ones in Bangkok.
Bernie never went, but I don't, I don't, we don't talk about that.
I know.
Yeah.
Nice.
This is an opportunity for us to work together.
Yeah.
Finally.
Awesome.
Awesome.
And so Mt.
Cleverist.
I love the name.
It's actually rather clever.
What does the company do?
So we actually started the company kind of based off an idea.
We had seven, some seven years ago.
My parents are both teachers.
I actually still teach at UVA part-time, cybersecurity.
And what we realized is that growing up, I see my parents deal with kind of bad textbooks,
that kind of handed down to them from the administration that they had to kind of bend
to meet the needs of the classroom.
And we were iterating through ideas and that was the one that I said, no, like if we could
figure out a way to solve this, like this would be really valuable because my parents
just always were complaining about it.
So we didn't really quite know the approach yet and we didn't have necessarily the NLP
and machine learning skills to really tackle it yet.
But as the industry has grown and kind of our skills have grown, we said, okay, this is
something we can apply.
NLP, machine learning, neural nets, and solve this kind of selection problem.
So what Mount Clevrist does is we can take any content on the web, URL, piece of text,
and then generate quiz questions on the fly on that text in a matter of seconds.
And then the kind of the real advantage, though, is not just creating the questions.
But after that, we track every interaction with that quiz.
What students get wrong, what they get right, how long they take with each question.
What questions and answers teachers like and don't like, what they want to use, feed
that into a neural net that then improves the system, improves the quiz for the next person.
So it's actually really at the end of the day, it ends up being one big online textbook
that actually improves itself the more you use it.
Interesting.
Interesting.
And so I'm familiar with the, there's a ton of research happening around question answering,
which is, you know, you have a body of text, you kind of start with some questions and
then you use AI machine learning to try to answer those questions.
You've kind of inverted that and you're using AI to generate the questions, as well as
the answers and track the kind of track and adjust over time.
Is that as established a research area, the question asking piece?
Not so much.
I mean, a few years ago, there were very few papers, at least now there are some interesting
work being done, but again, nothing substantial because a lot of it has been driven by kind
of the question answering Alexa, kind of Siri approach to machine learning.
What the question that has, or that what has been tackled in machine learning is this idea
of taking a mass amount of text and figuring out a good question, come around it, kind
of the Watson model.
Okay, Jeopardy.
So being able to parse mass amounts of text and generate one question, maybe a very well-formed
question, but it's one question.
Our problem and the really kind of the heart of our tech is that we take a very limited
amount of text and be able to generate 100, 120 questions off of one piece of text, each
asking about something different.
Then the kind of extra level of difficulty is then great, you've got a question, you've
got an answer that you've drawn out.
How do I generate good, wrong answers?
How do I find a question or an answer that is a good distractor from the correct answer?
So I put a multiple choice question in front of a kid.
If it's so close to the right answer that it's confusing, that's bad.
If it's so far away that they would never guess it anyway, it's also bad.
So you've got to find the kind of that Goldilocks middle ground on kind of good distractors.
And that was really probably the heart of the NLP problem.
I'm imagining like a dial where you can kind of tune the question for difficulty or I
guess a big part of the neural net piece that you described is about kind of normalizing
the question answer set so that your kind of average ends up where you want it to be.
Is that a good way to think about it?
Yeah, so we can actually look at kind of the readability of the text or kind of industry
standard scoring mechanism to take a chunk of text and figure out what grade levels this
would be appropriate for.
So the origin text, we can actually kind of guess at what good, what these questions are
going to come out kind of referring to in terms of like is this a fifth grade text or
is this a seventh grade text.
We're actually working on some tech to be able to convert something that is maybe written
at a 12th grade level down to a ninth grade level and then kind of use questions that are
more appropriate for that.
That's kind of in a coming release.
But yeah, it's being able to correctly pick questions that are appropriate for grade
level and answers that are appropriate for a grade level is a whole layer of complexity
that a lot of other, you know, tech solutions don't really think about, but it's unique
to the education space.
Okay.
I think of educators as folks that kind of want to have a lot of control over their source
materials.
Like how does this approach land for them relative to what they're used to?
So we're kind of taking this problem in two different ways.
The first is the initial product allows a teacher to bring in whatever content they would
like.
So if they found a news article that is exciting and relates to their class or if they
have a page that they've been using and sending students to for years instead of having
that all based on paper in terms of the grading, we're actually pulling that in and tracking
that information.
On the other side, there's a search component that you kind of brought into it and we're
working towards it, that is the broader vision of having a collection, a universe of content
and then being able to rank and sort that.
So there's the create problem and then there's the ranking and sorting.
So we kind of split those intentionally.
The control issue, it's something we want to get into.
We know that it's a political battle in the lightest of terms because it's not true
politics, but yes, that is something that we see often.
So we're coming up with ways to either encourage behavior in a certain way to kind of let
go of that, including automatic randomization of questions or adding in questions from
a different source that are applicable to the source that was provided.
So we're actually swapping in new information and using that as part of the quiz for students.
So it's not a standardization, but it's instead a way for us to help determine the quality
of content as well as the students understand.
And so there's kind of a couple of different problems there.
There's also a distinction in education between formative assessment and somewhat of assessment.
And this idea that teachers get really kind of close hold on exams because they want to
be able to kind of assign really clear grades and hold those students accountable and make
sure they can kind of deal with parents that disagree with those grades.
They have to have kind of a clear, clear chain.
But what has been kind of lost is this idea of this formative assessment that I want
to have homework that a kid can try over and over again until they get it right because
I'm actually interested in them learning it, not just grading on their first guess.
So what Mount Clevver says really focuses on is kind of trying to get the teacher to
not think about the quiz that we are providing as summative, as yet you're going to get grades
and this is going to account to the report card.
But more of a, get good questions in front of the kid to make sure that they understand
the content and they can take that quiz as many times as possible until they really grasp
it or master it.
Okay.
Can you talk a little bit about the pipeline that you used to deliver this from a machine
learning perspective?
Yeah.
It's, it's funny because we get a lot of, we get a lot of questions about like, oh,
we can't, like, can't, can't just be done by a machine learning model off the shelf.
And yes and no, in the sense that, in the sense that we are using some kind of, you know,
some basic, you know, basic kind of industry tried and true NLP models and neural nets.
What are some of those?
I mean, we are leveraging a lot of NER for a name that's a recognition using some kind
of interesting, interesting math around word vectors, what do I call like the donut model
saying, okay, I want to find similar words, but then, like, go up and then remove the
word that are really similar to find those kind of, again, the Goldilocks distractors.
So those are kind of the approaches that we're doing in the neural nets.
We're trying lots of different things because that part of the product is still kind
of not nascent, but we're getting there, so we're still kind of refining that.
But the pipeline itself, actually, what's interesting about Mount Cleverist is that it's not
just one huge big model with a model, is that we have to build, you know, an NLP that
can generate, you know, dozens of types of questions, so each one has its own kind of
pipeline.
She's that into a unified data structure, but then we have to normalize questions against
questions, answers against answers, formats against formats.
So even within the ranking system, within one lesson, within one subject, we might have
a whole series of models, each, again, not necessarily huge, robust, crazy big models,
but each doing is a unique specific thing.
Live right on that.
What do you mean by normalizing questions against questions, and are we talking about in terms
of their difficulty and things like that?
Right.
So in terms of their difficulty, so imagine you're a teacher and you've created a lesson,
and you want to, like, we've generated, let's say, a hundred quiz questions.
Now some of those are not going to be great because the NLP isn't perfect.
No, we don't know many products that are perfect.
So we want that kind of human and the loop feedback.
So we are capturing, kind of, what, you know, the upvotes and downvotes similar to kind
of Reddit of what teachers like and don't like, but at the question at the answer level.
So we get that, just that base level, kind of, like, how many people have upvoted, how
many people have liked or disliked this particular piece.
But then we've got this kind of student performance data on the back end.
So okay, well, when I showed this question in this format, in this context, like, this
is how well a student did, given how long they took.
So normalizing that data means that I have to take every interaction with every question
or format or answer, and then trying to figure out, okay, even though this question has
been shown to, you know, 10 different students in 10 different ways, how do I judge its effectiveness?
Is this a good measure of learning on this particular topic?
So like, that's the normalization that we're talking about.
And is that, like, some big batch job that runs every X day or week or something, or
is it something that you just kind of trigger it's erratically?
So we've actually made kind of a goal of the product to not do things in batch.
Which was, it was a bold goal, it may slow this down a little bit.
But we actually wanted, we set the goal from the user experience levels that we wanted
the quiz to improve on a, like, per interaction basis.
So like, every time a kid answers a question, exactly, it re-jiggers the entire, re-, does
that mean-
So by the time the second student sees the quiz, it's actually improved and changed and
learned from the results of the first student.
And is that learning, does that mean that you are, like, retraining models in that whole
pipeline?
Or does it mean that-
Are there some set of heuristics that you're using to kind of massage weights or things
like that without having to retrain all your models?
A bit of both.
So we don't retrain the model every run through, though we do have some kind of tail-trained
models kind of worked in.
But most of it is capturing kind of the heuristics of the performance with those questions in
this format.
And capturing that and then feeding that back in as additional information into the model
as we then kind of randomize and select what we want to do and what we want to show to
the second student.
So it's the heuristics of the first student fed back into the same model, tail-trained
a couple pieces, and then kind of what you get, we're like, oh, tried this version of
this content, or this version of this question with these answers in this format instead
of the old one because that old version, it wasn't a great test, but this one might
be.
So it's a little bit of randomization, a little bit of kind of design of experiments,
constantly trying to say, okay, how, you know, reduce how many experiments or variations
do we need to run before we figure out what the best stuff is?
Okay.
And in terms of identifying the target content in the first place, is that are the educators,
you know, feeding URLs into the system to direct you, or are you doing some ML, German,
crawling, or something like that to figure out the interesting content out there?
At the moment, we're relying on the, what we consider the expert networks of teachers.
So we want the educators to be the one providing content that they have used in the past, and
then we'll be doing that ranking in order to help them either bubble up what is better,
or take advantage of the stuff that they've tried in true sources.
So a lot of the open educational resources movement from the Obama administration has been
very helpful in that.
And a lot of institutions, as well as a lot of individuals, are now providing content
openly available under Creative Commons license, or MIT license, whatever it might be, is
now available to us and not behind the paywall, not under the umbrella of one of the large
publishers or anything like that.
So it's taking advantage of that recent trend.
Okay.
Interesting.
What have been kind of the biggest challenges in pulling this all together?
Well, I think that the biggest challenge was designing that data structure.
So again, we have all of these different models, all looking at essentially the same data,
and how you store that, and how you store that in a way that can be kind of lightweight,
pulled in the moment, you know, in between quiz questions.
That was something that I think both of us had to kind of take a step back and say,
how do we store this data at scale, but as we're small, but also at scale, kind of building
towards this bigger architecture, being able to track what I like to call kind of the context
mapping, that it was this version of the question with these answers shown, like maybe there
are 10 wrong answers, but we only showed four, and it was a true false or a multiple choice
with the none of the above, not this.
So being able to capture all that context, and store that, and then correctly tease it
out, and then build a model against it.
Turning out what that included, and how to put all the pieces of data together.
That was interesting.
We have a model in the tool called a quinstance, which is a quiz instance, because we've got
to generate a quiz, and it might have teacher preferences at one given moment, but then
within that quiz, within the context, we need to have an individualized version for each
student that they experience at that moment in time.
It's like, how do we further that?
It's a quiz, but that's also a quiz.
So it's quinstance became our term, and it's been great.
But we have a lot of those little things that we have to figure out on the way that we
have to structure this and store this in a unique way that I think gives us an advantage
in the market that I think other people have really thought about data for education this
way yet.
And so for folks that are like, how did you approach that problem?
Did you just stumble upon the answer, or did you just try out everything that was out
there and see what worked, and like, what did you end up doing?
These are so-or-subject.
Oh man, flashbacks.
I actually, when we first started trying to build this, I was just like a, you know, a budding
web developer.
I actually built the whole tool without a lot of the neural nets I hadn't quite gotten
there yet.
But I built my own kind of parsing engine and NLP engine out of PHP, because it was the
only language I knew.
Right, so I had a lot of lessons learned.
Right, so it was a terrible, terrible idea.
I think, on the front end, it was like a Drupal 6.
It was.
Right, exactly.
Not out of these moments, but like, it was a learning experience.
But exactly.
And like, I got 10 times better as a developer, and it's like, it jumps under, I think,
a lot of my career just being, having to suffer through that.
So having a really good problem to chew on for a long time.
And then I eventually said, OK, well, no, Python is better for this.
There's Java libraries that I can incorporate.
I need to stand up microservices architectures.
Right now, we have, I think, 14 different microservices, each running even different parts
of the models, all talking to each other, leveraging the same data set, caching some parts
here and there.
So I mean, but I had to learn that kind of all on my own, starting with that poor, poor
Drupal site.
But yeah, that's, that's, is, is trial and error could have been more press.
It, it, it, it, it almost was, I thought it was a band when I found Lyravelle.
I was like, oh man, it's not Drupal, but, yeah.
And in terms of the data store, did you like, is it like some kind of document oriented
MongoDB type of thing or like a Cassandra or what direction did you go?
We actually, because we didn't really know what we were doing, and I think that's actually
a great use case for Mongo is that it's a kind of a turnkey solution and you can kind
of drop in whatever you want.
And then there are a lot of kind of lightweight ORMs on top of it.
So you can change the model and it doesn't break everything.
You just have to go back and kind of clean some stuff later.
So we've been doing Mongo for probably the last year as we kind of moved to a more production
level product, but we've now hit the wall or think, okay, the type, you know, the model
types have kind of settled.
We kind of know what our structure is.
We know what it needs to be.
We've thought it out.
We've been in the wild and see what happened.
So now we're actually moving to kind of a combination of things.
Probably some kind of Mongo or other, you know, no SQL database for a lot of the document
structure.
And then probably Cassandra for a lot of the, like the interactions, the, in the moment
interactions because it's ability to scale kind of linearly where everyone else kind
of stops at a certain certain point.
So yeah.
So we were, I'm really excited to move to Cassandra, but Cassandra, you know, also has some,
you know, issues around search, you got to, you got to figure out how to get searched
on.
So there's a lot of other, you know, different ways looking at, you know, postgres for
other certain features because you can index and search there.
So we're actually moving to a more complex model.
But again, each database has to fit a different part of the system.
And we now know how that system really needs to, like, be set up to, you know, run at
light speed.
Okay.
So given that you're doing kind of the interactive updating and all that kind of stuff, I'm assuming
that's not so PHP, is that like so far or it is almost like there's a little bit of
Python on the back end, some Java libraries that haven't like written, but kind of leveraging
some open source stuff.
But primarily the entire stack has written in JavaScript, leveraging Amazon Lambda and
then react to the front end.
So we have made this is a if, you know, the heavens opened up and, and to God said, yes,
please like go use this product and we had massive scale that we are set up to, you know,
I assume handle it.
Scale breaks everything.
But we knew that there was going to be so much compute that we wanted to make sure we
could kind of turn every little lever.
So all those microservices are, most of them are speaking, you know, Lambda to each other.
And it also means that the models are lightweight enough in some respects that we can offload
some of that model computation to the browser.
So we have this JavaScript written neural nets.
So I can actually load some of the early computation onto the computer for the teacher or the user
or the phone and then do the computation and just pass back the results and kind of offload
a good 20 to 30% of our load back onto the user.
So we're not doing it now, but we are set up to do that eventually.
Oh, wow.
And are you using any particular open source library to do the front end inference?
Oh, I want to do right that yourself.
No, that's a combination of synaptic, I think synaptic.js was the one reason right now.
And that is, there have been many variations.
I think the first one we used was called brain.
There's a lot of JavaScript libraries that are just coming out where they said, I want
to write JavaScript and I want to do neural nets.
And they can do some pretty cool things on the browser.
Not necessarily the big scale stuff that we want to do on some of our calculations, but
a lot of the smaller, like within a document, within a document, you could have known
document space, we can do that on the browser side and we're, you know, going to shift
to that model later.
Hmm.
Interesting.
I'm trying to think through the way I've envisioned your process.
You get these documents, you're doing a bunch of what I think of as backend processing
to come up with questions and answers and normalize them and all that kind of stuff.
What would you want to do on the front end that would require, you know, running the inference
locally or would take advantage of that if not required?
So the first thing you do kind of as a user is you drop in the URL, eventually it'll
be search term or URL, but right now we're focused on kind of new content capture.
You drop into URL and then immediately we go scrape that content from that URL and then
start parsing it.
And a lot of that parsing work right now we're handling the backend, but it can be handled
by the browser.
Oh, got it.
Right.
So that is actually a big portion of the compute because it's a lot of parsing.
But if that can be off offload instead of 10 people doing it all on one of our machines
or a series of Lambda functions, but pushing that to the browser, because that's kind of,
it's not proprietary stuff, it's not crazy, you know, complicated, but it just needs to
get done.
Someone needs to pull out, you know, named entities, someone needs to, you know, to
break, do sentence barrier detection, all that stuff can be handled, then we just want
the results.
And then we get to kind of the nitty gritty of the data on the backend.
It strikes me that if a lot of folks aren't doing that now, that's going to be kind of
a popular way to do cooperative compute.
It's almost like, you know, having your users, you know, mine Bitcoin for you before the
legitimate link.
Right.
The only other industry that I know is really doing that is Bitcoin and not, you know, like,
oh, like, my advertising window is mining Bitcoin for someone in the Ukraine, so, yeah.
But I was like, okay, well, how, like, when I read that, I was like, that's a really cool
architecture.
And like, we'd already been kind of thinking about this as like, oh, so like someone's
proving that this is doable.
So it was kind of a check in the box with, yes, this is maybe where architecture should
go.
And it strikes me that that could be a startup in and of itself, right?
So there's an architecture because there's all kinds of problems that you can run into
of like, you know, the process is stealing all the compute and usability issues and stuff
like that.
Yeah.
If we figure that out, then we can shift to maybe that'll be our second business once we
sell this.
Yeah.
Nice.
You know, given all that, all that technology, like, there's tons of folks, particularly
here in New York City, right, doing ed tech, what makes you different.
So we've been, you know, been doing this project for a long time.
So we've been watching the ed tech market when the booms and busts thing were on the third
boom right now that we've been paying attention to.
And we keep seeing investors getting burned and mostly because they keep investing kind
of this, what seems like a really cool new thing.
But in reality, it falls into kind of like two standard business models.
One we call it kind of a warehouse model, which is just collect as much information as possible
and then make it searchable.
But if you look at some of the products out there, you find that those, the search algorithms
are poor at best.
And they're not searching on like real performance data, just keywords.
So you end up getting like a big warehouse and you'll search for the War of 1812, for
example, you get 342 results.
I'm not speaking about any product, particularly.
And all of them are ranked 3.9 or four stars out of four stars.
So as a teacher, like, I tried to use that, I was like, this is useless for me.
Great.
So I'm like, am I going to open 342 PDFs and then read them and say, oh, I think this
want to work.
And even if I learned, oh, this one actually was effective.
I have no way to really transfer that knowledge back in and share that with my community.
So that was like one thing, okay, we can solve that part.
And the other business model is we call kind of the wall of the garden model, which is
more like the, you know, 1920s newspaper business model, right?
Whereas, you know, like EdTech companies are essentially taking investor money, paying
authors to write content and then putting it behind a paywall, just like a newspaper,
traditional media.
And they're saying, oh, no, no, no, you can't read it.
But I promise you.
It's the best.
Right.
So I was going to say it.
So you have this model where the kind of the incentives are misaligned, or is that it's
not in their interest to share what pieces of content they like and don't like, because
you kind of get like, oh, you buy this whole suite of stuff.
So yeah, they want to improve it, but they're not necessarily helping the teacher do that
and really enlisting the teacher's help, being honest about what is and is not working.
And if I ask, you know, if someone at the New York Times, if I think they're, their
company, or the newspapers better than the Washington Post, I know who they're going to say,
right?
I know what that answer is.
So getting real data and performance out of kind of this traditional model is just
hard.
So you've got this kind of high quality, you know, hard to produce, expensive to produce
new content, model wall garden, and they got warehouse, you know, high quantity and very
low quality.
And the more, you know, more stuff you get, the worse the user experience gets.
So we said, can we tie quality and quantity together?
Can we make it kind of a positive feedback loop?
And that's really where the AI and the machine learning, you know, kind of cuts in is that
we are capturing at scale enough information to kind of keep floating the best stuff up
to the top, but searching through it and capturing it in a way is not just traditional search.
That means we can kind of cut through the top and get quantity and quality at scale.
Have you found that a better way to rank and rank the way you present content is by kind
of bubbling up what you've seen with the interactions as opposed to asking for an explicit
star rating or things like that?
Is that the direction you're going with this?
What I would say is that we know that star rankings are not working.
So it's been tried.
We are capturing that data, but instead of just having someone vote on like, oh, like
I like this piece of content.
Instead, we can capture more interesting data, but like, I've got this piece of content
and there's a hundred questions in it.
I've seen tons of people who have strong opinions about the top 20.
So it's kind of the ranking and sorting algorithm is a bit more off of the Reddit idea
of how do you have these kind of layered nested pieces of information, how do you float all
of that and some kind of recursive loop of like, oh, now I know that this piece of content
is better than this one.
And I'm not just using the uploads and downloads on those top level pieces, but everything
inside.
And that's really what's enabled us to kind of take the next level in terms of ranking.
And then you combine that with performance data and now you've got something that, you
know, no one else is going to be like, all right, all right, that's a very cool story.
What's next?
So if you take what we're doing right now and you really tease out that there's teachers
using up the system, what we can do is we can then take the data that the teachers and
students have produced and bring that to institutions themselves and say, here's a view
of your school that you've never seen before, one that would benefit you in terms of looking
at how your students are actually learning as opposed to just the output grades like
where we're looking at are the students digging deeper into the content?
Are they actually mastering something as opposed to did they just memorize it and move on?
So looking at the next phase is using aggregated data of students and teachers at the school
level, at the district level, the local government level and that type of thing.
If you remember, we were talking about earlier about the being able to come normalize and
standardize questions against questions, answers against answers.
Well, there's nothing preventing us from using very similar models to rank students
against students and not to students against their classmates or against themselves, but
against every other student who's ever touched the system.
So what I think is really unique about the way that we're doing this is that even if students
see different pieces of content and take different quizzes, we can, you know, compare
them and say, okay, which student actually learned more is doing better?
And if you look at the way we are standardizing students for results today, kind of standardize
tests, it's shove a student in a room on a Saturday and put 300 questions in front of
an prey.
And you hope you get good data.
But with Mt. Cleveris, though, instead of that 300 questions, you can get 30,000 data
points per student per year over the course of a year.
And you can finally get to this as Bernie alluded to, the second order metrics of success
that are kind of the holy grails of the education policy of questions like, did the student become
more curious about what did they become more curious?
Did the student get, you know, did they learn how to learn over time?
And figuring out kind of where and what was, you know, what the most successful at?
That's the stuff that a lot of kind of this typical standardized test models can't get
to.
So we know that at scale, that's where we think kind of a lot of this data will be valuable
is that we can actually provide real, you know, real standardized testing data to schools
without them having to actually do any standardized testing.
It's just part of the product.
Awesome.
But James and Bernie, it was great getting to learn a little bit about Mt. Cleveris and
kind of explore how you pulled it all together.
Fascinating story.
I really appreciate you taking the time.
Thank you.
Yeah.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Bernie, James, Mt. Cleveris, or any of the topics covered in
this episode, head on over to twimlai.com slash talk slash 63.
To follow along with the NYU Future Labs AI Summit series, which will be piping to your
favorite pod catcher all week, visit twimlai slash nexus labs too.
Of course, you can send along your feedback or questions via Twitter to at twimlai or
at sam charrington or leave a comment right on the show notes page.
Thanks again to NYU Future Labs for their sponsorship of the show and the series.
And of course, thank you once again for listening and catch you next time.
