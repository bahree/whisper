1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,560
I'm your host Sam Charrington.

4
00:00:23,560 --> 00:00:28,120
This week on the podcast, we're featuring a series of conversations from the Nips conference

5
00:00:28,120 --> 00:00:30,440
in Long Beach, California.

6
00:00:30,440 --> 00:00:34,320
This was my first time at Nips and I had a great time there.

7
00:00:34,320 --> 00:00:37,560
I attended a bunch of talks and of course learned a ton.

8
00:00:37,560 --> 00:00:43,720
I organized an impromptu round table on building AI products and I met a bunch of wonderful

9
00:00:43,720 --> 00:00:47,840
people, including some former Twimble Talk guests.

10
00:00:47,840 --> 00:00:52,680
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

11
00:00:52,680 --> 00:00:59,360
take a second right now to subscribe to at twimblei.com slash newsletter.

12
00:00:59,360 --> 00:01:05,040
This week through the end of the year, we're running a special listener appreciation contest

13
00:01:05,040 --> 00:01:09,760
to celebrate hitting 1 million listens on the podcast and to thank you all for being

14
00:01:09,760 --> 00:01:11,720
so awesome.

15
00:01:11,720 --> 00:01:16,400
Tweet to us using the hashtag Twimble1Mill to enter.

16
00:01:16,400 --> 00:01:21,040
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

17
00:01:21,040 --> 00:01:23,040
other mystery prizes.

18
00:01:23,040 --> 00:01:29,840
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

19
00:01:29,840 --> 00:01:32,280
for the full rundown.

20
00:01:32,280 --> 00:01:36,640
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

21
00:01:36,640 --> 00:01:39,880
of this podcast and our Nips series.

22
00:01:39,880 --> 00:01:44,880
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

23
00:01:44,880 --> 00:01:50,600
sessions, their big news this time was the first public viewing of the Intel Nirvana

24
00:01:50,600 --> 00:01:54,400
neural network processor or NNP.

25
00:01:54,400 --> 00:01:59,320
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

26
00:01:59,320 --> 00:02:04,440
primitives while making the core hardware components as efficient as possible, giving

27
00:02:04,440 --> 00:02:09,920
neural network designers powerful tools for solving larger and more difficult problems

28
00:02:09,920 --> 00:02:14,560
while minimizing data movement and maximizing data reuse.

29
00:02:14,560 --> 00:02:20,400
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

30
00:02:20,400 --> 00:02:22,560
Nirvana.com.

31
00:02:22,560 --> 00:02:28,040
In this episode, I speak with Yael Niv, professor of neuroscience and psychology at Princeton

32
00:02:28,040 --> 00:02:29,200
University.

33
00:02:29,200 --> 00:02:34,240
Yael joined me after her invited talk on learning state representations.

34
00:02:34,240 --> 00:02:38,840
In this interview, Yael and I explore the relationship between neuroscience and machine

35
00:02:38,840 --> 00:02:39,840
learning.

36
00:02:39,840 --> 00:02:44,760
In particular, we discuss the importance of state representations in human learning,

37
00:02:44,760 --> 00:02:49,800
some of her experimental results in this area, and how a better understanding of representation

38
00:02:49,800 --> 00:02:54,640
learning can lead to insights into machine learning problems such as reinforcement and

39
00:02:54,640 --> 00:02:56,360
transfer learning.

40
00:02:56,360 --> 00:02:58,520
Did I mention this was a nerd alert show?

41
00:02:58,520 --> 00:03:01,800
I really enjoyed this interview and I know you will too.

42
00:03:01,800 --> 00:03:06,280
Be sure to send over any thoughts or feedback via the show notes page.

43
00:03:06,280 --> 00:03:08,760
And now on to the show.

44
00:03:08,760 --> 00:03:20,400
All right, everyone, we are here in Long Beach at the NIPPS conference and I've got the

45
00:03:20,400 --> 00:03:22,880
pleasure to be seated with Yael Niv.

46
00:03:22,880 --> 00:03:29,880
Yael is a professor of neuroscience and psychology at Princeton University and she delivered

47
00:03:29,880 --> 00:03:33,760
a talk this morning on learning state representations.

48
00:03:33,760 --> 00:03:37,600
And I'm really excited to have her here to talk to us about what she's working on.

49
00:03:37,600 --> 00:03:40,400
Yael, welcome to this weekend machine learning and AI.

50
00:03:40,400 --> 00:03:41,400
Thank you.

51
00:03:41,400 --> 00:03:42,400
Thanks for having me.

52
00:03:42,400 --> 00:03:43,400
Absolutely.

53
00:03:43,400 --> 00:03:44,400
Absolutely.

54
00:03:44,400 --> 00:03:47,360
So you are a neuroscientist, a neuropsychologist.

55
00:03:47,360 --> 00:03:50,240
You're here presenting on machine learning.

56
00:03:50,240 --> 00:03:52,600
You talked about reinforcement learning.

57
00:03:52,600 --> 00:03:54,320
How did all of this come about?

58
00:03:54,320 --> 00:03:58,440
How did you end up at the intersection of these two fields?

59
00:03:58,440 --> 00:04:01,840
So the funny thing is I didn't start as a neuroscientist.

60
00:04:01,840 --> 00:04:09,680
Well, I started as a computational neuroscientist, more interested in AI and in psychology, but

61
00:04:09,680 --> 00:04:14,360
not an experimentalist like I am today, I started as a theoretician.

62
00:04:14,360 --> 00:04:17,160
So my PhD was in reinforcement learning theory.

63
00:04:17,160 --> 00:04:19,960
My main advisor was in computer science.

64
00:04:19,960 --> 00:04:22,400
My secondary advisor was in psychology.

65
00:04:22,400 --> 00:04:26,680
She was, I asked her to be an advisor so that she would ground me in real data and I

66
00:04:26,680 --> 00:04:32,360
wouldn't be making models of things that I've made up rather than the real world.

67
00:04:32,360 --> 00:04:35,920
And my PhD was also completely theoretical.

68
00:04:35,920 --> 00:04:41,200
I was modeling animal behavior with reinforcement learning, submitting my papers to nips.

69
00:04:41,200 --> 00:04:43,120
That's getting rejected by nips.

70
00:04:43,120 --> 00:04:48,320
I got a best paper award finally when I paper was finally accepted at nips.

71
00:04:48,320 --> 00:04:50,160
So I was actually really excited now.

72
00:04:50,160 --> 00:04:52,640
I have drifted away.

73
00:04:52,640 --> 00:04:56,920
I've drifted more into psychology, more into neuroscience, more into doing experiments.

74
00:04:56,920 --> 00:05:02,640
I still do modeling, but it really is now half half with experiments.

75
00:05:02,640 --> 00:05:06,360
And so I haven't been coming to nips for the last few years and I was really excited

76
00:05:06,360 --> 00:05:14,120
to be invited to give a talk here because it felt like closing a circle from all those

77
00:05:14,120 --> 00:05:17,720
papers being rejected now.

78
00:05:17,720 --> 00:05:19,040
Now giving an invited talk.

79
00:05:19,040 --> 00:05:22,880
So I immediately said yes, very nice, very nice.

80
00:05:22,880 --> 00:05:25,920
What do you consider to be your home conference now?

81
00:05:25,920 --> 00:05:29,920
My whole conference now is reinforcement learning and decision making RLDM.

82
00:05:29,920 --> 00:05:32,880
I helped found that conference.

83
00:05:32,880 --> 00:05:35,360
And we have it every two years.

84
00:05:35,360 --> 00:05:39,440
There was one this past year, so the next one is in 2019.

85
00:05:39,440 --> 00:05:45,360
And it's basically a very interdisciplinary conference that tries to bring machine learning,

86
00:05:45,360 --> 00:05:52,520
AI, psychology, etiology, economics, anybody who's interested in reinforcement learning

87
00:05:52,520 --> 00:05:55,280
and decision making over time.

88
00:05:55,280 --> 00:05:57,360
And it's a growing conference last time.

89
00:05:57,360 --> 00:05:59,120
I think we had about 600 people.

90
00:05:59,120 --> 00:06:00,120
Oh wow.

91
00:06:00,120 --> 00:06:01,120
Wow.

92
00:06:01,120 --> 00:06:02,120
So that's my home base.

93
00:06:02,120 --> 00:06:03,120
Nice.

94
00:06:03,120 --> 00:06:04,120
Nice.

95
00:06:04,120 --> 00:06:07,240
And one of the things that I've observed here and this is the first time I've ever been

96
00:06:07,240 --> 00:06:12,760
at nips is that there seems to be of several kind of themes that have emerged for me.

97
00:06:12,760 --> 00:06:19,560
One of the strong themes is kind of integrative approaches and interdisciplinary approaches.

98
00:06:19,560 --> 00:06:24,280
Has nips always been like that or are we seeing more of that now than in the past?

99
00:06:24,280 --> 00:06:26,520
I think nips has always aspired to that.

100
00:06:26,520 --> 00:06:32,080
It waxes and wanes, so people have talked about, you know, the n in nips, is it sometimes

101
00:06:32,080 --> 00:06:33,080
more ips?

102
00:06:33,080 --> 00:06:34,080
Okay.

103
00:06:34,080 --> 00:06:39,320
And whereas the neuro, there have been, you know, worse years and better years, I used

104
00:06:39,320 --> 00:06:40,320
to come regularly.

105
00:06:40,320 --> 00:06:42,280
So I remember this.

106
00:06:42,280 --> 00:06:48,320
And there are also kind of fads and things that become more popular and less popular.

107
00:06:48,320 --> 00:06:52,200
I remember when I was a master student, someone coming back from nips and saying that it should

108
00:06:52,200 --> 00:06:57,440
have been called support vector machine conference that year because it was all support vector

109
00:06:57,440 --> 00:06:58,440
machines.

110
00:06:58,440 --> 00:06:59,440
Right.

111
00:06:59,440 --> 00:07:03,600
Now almost nobody talks about SVM and everybody talks about deep learning.

112
00:07:03,600 --> 00:07:08,640
So it kind of changes focus, but there's always been, I think, a sincere attempt to keep

113
00:07:08,640 --> 00:07:10,720
the neuroscience in the mix.

114
00:07:10,720 --> 00:07:16,680
And it's not super easy because I think it's very clear that neuroscience has a lot to

115
00:07:16,680 --> 00:07:21,320
learn from or not necessarily to learn, but to take from machine learning.

116
00:07:21,320 --> 00:07:26,800
So we use machine learning algorithms to analyze our data to think of computational processes

117
00:07:26,800 --> 00:07:27,800
in the brain.

118
00:07:27,800 --> 00:07:31,640
Really, I feel like when I come to nips, I'm coming is like a shopper.

119
00:07:31,640 --> 00:07:35,160
I want to see what's on the shelf now that I can use in my research.

120
00:07:35,160 --> 00:07:38,520
I don't think it's necessarily that way, the other way around.

121
00:07:38,520 --> 00:07:45,600
I think AI in particular has been inspired by neuroscience, but mostly, you know, takes

122
00:07:45,600 --> 00:07:49,960
quick inspiration and runs, runs off to somewhere else, and that's fine because their goal

123
00:07:49,960 --> 00:07:50,960
is different.

124
00:07:50,960 --> 00:07:52,600
My goal is to understand the brain.

125
00:07:52,600 --> 00:07:54,960
The goal of AI is completely different.

126
00:07:54,960 --> 00:07:56,960
It's almost aspirational inspiration.

127
00:07:56,960 --> 00:07:57,960
Yeah.

128
00:07:57,960 --> 00:08:03,400
I think basically AI needs neuroscience less that neuroscience needs machine learning.

129
00:08:03,400 --> 00:08:04,400
Interesting.

130
00:08:04,400 --> 00:08:08,280
So it's always been kind of a tough thing to keep the mix together.

131
00:08:08,280 --> 00:08:12,520
But in your talk, you, the focus of your talk was that mix, right?

132
00:08:12,520 --> 00:08:13,520
I tried.

133
00:08:13,520 --> 00:08:16,000
I tried my best.

134
00:08:16,000 --> 00:08:20,120
It's not the typical talk that I give because these days, I don't usually talk to this type

135
00:08:20,120 --> 00:08:21,120
of audience.

136
00:08:21,120 --> 00:08:26,320
So I thought, hard, you know, what do I have to sell to AI?

137
00:08:26,320 --> 00:08:28,360
What do I want AI to buy?

138
00:08:28,360 --> 00:08:33,880
And so I was posing this challenge of, you know, real, well, I don't know, call it

139
00:08:33,880 --> 00:08:39,000
real with a capital R, but intelligence that is more like human intelligence.

140
00:08:39,000 --> 00:08:44,480
So with, you know, playing chess or playing go at an expert level, that's a huge achievement

141
00:08:44,480 --> 00:08:48,520
because these are really hard tasks, but they're really hard for humans too.

142
00:08:48,520 --> 00:08:54,120
You can't just become a go expert tomorrow.

143
00:08:54,120 --> 00:08:56,920
But there are other things that we learn super easily.

144
00:08:56,920 --> 00:09:03,240
And those things are very hard for computers as well for AI, you know, maybe, maybe in

145
00:09:03,240 --> 00:09:05,240
some way as hard as playing go.

146
00:09:05,240 --> 00:09:10,280
And I think that's, that's a real challenge because those are, because the amount of flexibility

147
00:09:10,280 --> 00:09:15,760
that the human brain has, the extent to which we learn super quickly, and we can toggle

148
00:09:15,760 --> 00:09:21,560
tasks, do one task, and then a minute later do another and then return back and kind

149
00:09:21,560 --> 00:09:27,600
of toggle the representations of policies that all the machinery that's needed for these

150
00:09:27,600 --> 00:09:30,320
tasks is really, really impressive.

151
00:09:30,320 --> 00:09:33,320
And that's what I have not seen in any AI yet.

152
00:09:33,320 --> 00:09:34,320
Right.

153
00:09:34,320 --> 00:09:37,440
I thought that was a really interesting characterization of kind of the challenge of AI, or what you

154
00:09:37,440 --> 00:09:39,440
believe the challenge of AI should be.

155
00:09:39,440 --> 00:09:40,440
AI challenge.

156
00:09:40,440 --> 00:09:41,440
AI challenge.

157
00:09:41,440 --> 00:09:42,440
One of them.

158
00:09:42,440 --> 00:09:47,160
AI should be, you know, as opposed to kind of these, you know, grand challenges like

159
00:09:47,160 --> 00:09:55,120
AlphaGo or like the game of Go, you propose many simpler problems solved with much less

160
00:09:55,120 --> 00:09:57,120
data as a challenge.

161
00:09:57,120 --> 00:09:59,640
Is that the way did I characterize that right?

162
00:09:59,640 --> 00:10:01,480
I don't know if the problems are simpler.

163
00:10:01,480 --> 00:10:02,480
OK.

164
00:10:02,480 --> 00:10:06,960
So I gave the example in my talk of crossing the street and it seems like a simple problem

165
00:10:06,960 --> 00:10:14,600
because we already think about it as a simple situation, but really the stimuli, the auditory,

166
00:10:14,600 --> 00:10:19,840
visual world around you when you're doing a simple task of crossing the street is very

167
00:10:19,840 --> 00:10:21,440
complex.

168
00:10:21,440 --> 00:10:28,320
And you need, you know, sophisticated visual machinery in our brain to parse out the

169
00:10:28,320 --> 00:10:32,120
objects you need sophisticated auditory machinery, et cetera.

170
00:10:32,120 --> 00:10:37,760
But more than that, you need to know, even if I've parsed everything, most of it is still

171
00:10:37,760 --> 00:10:38,760
irrelevant for the task.

172
00:10:38,760 --> 00:10:40,080
I don't even need to parse it.

173
00:10:40,080 --> 00:10:43,680
I don't need to parse what are the stores on the other side of the street.

174
00:10:43,680 --> 00:10:48,520
I don't need to parse even the colors of the cars because that's irrelevant for the task.

175
00:10:48,520 --> 00:10:53,640
So whether my visual system does that or not is one question, but my learning system

176
00:10:53,640 --> 00:10:58,880
should definitely ignore those aspects, and that's really hard because there are potentially

177
00:10:58,880 --> 00:11:03,720
infinite combinations of things that I might need to ignore or pay attention to.

178
00:11:03,720 --> 00:11:08,280
There's everything that I see and then there's everything in my past because it might be

179
00:11:08,280 --> 00:11:12,080
that the time of day matter, unobservable things, not only in my past, the time of day

180
00:11:12,080 --> 00:11:17,280
matters, the city in matters, what I did yesterday matters, maybe not for crossing the street,

181
00:11:17,280 --> 00:11:23,160
but if I'm driving a car and navigating, like what I know about traffic from past learning.

182
00:11:23,160 --> 00:11:28,560
So potentially, infinite dimensions of the environment and have to narrow them down

183
00:11:28,560 --> 00:11:29,560
to like 345.

184
00:11:29,560 --> 00:11:35,560
And one of those examples of the kind of the state that goes into figuring that out was

185
00:11:35,560 --> 00:11:37,680
Washington DC versus New York?

186
00:11:37,680 --> 00:11:38,680
Yeah, yeah.

187
00:11:38,680 --> 00:11:43,880
The idea is that, you know, I could also say, you know, New York versus London, long

188
00:11:43,880 --> 00:11:48,880
for each versus whatever, the context really affects, let me take one step back.

189
00:11:48,880 --> 00:11:52,800
So on the one hand, we want to generalize broadly and that's why it's really important

190
00:11:52,800 --> 00:11:59,960
to ignore some things and kind of represent and learn about only others because if I'm

191
00:11:59,960 --> 00:12:05,560
really, you know, if my brain is so sophisticated that can take into account everything in my

192
00:12:05,560 --> 00:12:10,400
visual scene, that would seem to be optimal, but it's actually super suboptimal because

193
00:12:10,400 --> 00:12:16,080
it means that when I learned something new, I don't know what to generalize it to.

194
00:12:16,080 --> 00:12:20,760
Is it, is this true only in this intersection only when a red car is coming at me only

195
00:12:20,760 --> 00:12:26,200
when I'm in front of this shop and depending on the task, let's say crossing the street,

196
00:12:26,200 --> 00:12:28,640
what I've learned is probably much more general than that.

197
00:12:28,640 --> 00:12:34,100
I might learn that in Princeton, the minute you step down the curve, all the cars top

198
00:12:34,100 --> 00:12:38,960
and let you go no matter what, that's general to all of Princeton, not only to that place,

199
00:12:38,960 --> 00:12:40,120
but only to Princeton, right?

200
00:12:40,120 --> 00:12:43,080
I wouldn't want to learn that about Long Beach.

201
00:12:43,080 --> 00:12:48,800
So it's all about setting the boundaries of generalization and what I've realized from

202
00:12:48,800 --> 00:12:54,920
this work is that really when you think about real world experience, you never, ever see

203
00:12:54,920 --> 00:12:57,560
two situations that are exactly the same.

204
00:12:57,560 --> 00:13:01,280
Even if you're at that same intersection, it won't be exactly the same crossing the

205
00:13:01,280 --> 00:13:02,600
street tomorrow.

206
00:13:02,600 --> 00:13:08,880
So the fact that we can reuse any past experience, the fact that we can learn at all means that

207
00:13:08,880 --> 00:13:10,320
we have to generalize.

208
00:13:10,320 --> 00:13:15,480
And so the basic thing that I think about all the time is how do we determine the boundaries

209
00:13:15,480 --> 00:13:16,840
of the generalization?

210
00:13:16,840 --> 00:13:23,360
So I've come from reinforcement learning where we think a lot about how do we learn values

211
00:13:23,360 --> 00:13:25,800
and policies.

212
00:13:25,800 --> 00:13:28,520
But I'm thinking that problem is pretty much solved.

213
00:13:28,520 --> 00:13:30,600
I mean, we have a lot of algorithms for that.

214
00:13:30,600 --> 00:13:32,680
We know how it's done in the brain.

215
00:13:32,680 --> 00:13:34,240
The question is on how do I learn values?

216
00:13:34,240 --> 00:13:36,360
The question to what do I generalize this value?

217
00:13:36,360 --> 00:13:38,120
What's the value of what?

218
00:13:38,120 --> 00:13:40,360
What state, what situation?

219
00:13:40,360 --> 00:13:45,720
And I want to go back to you said that my challenge was to solve or the challenge that

220
00:13:45,720 --> 00:13:47,360
my problem was less data.

221
00:13:47,360 --> 00:13:49,840
To solve simple problems with less data.

222
00:13:49,840 --> 00:13:52,120
Let me try to phrase that a little bit differently.

223
00:13:52,120 --> 00:13:58,040
So there's this old story, I don't know if it's a myth or not, that I heard about nips,

224
00:13:58,040 --> 00:14:03,320
that basically, well, it happened at nips, it's not about nips.

225
00:14:03,320 --> 00:14:08,760
So what I heard is that in the 80s, kind of like in the first heyday of neural networks,

226
00:14:08,760 --> 00:14:15,680
now there's second, someone published a nips paper that showed how you can use a

227
00:14:15,680 --> 00:14:19,240
neural network to parallel park a truck.

228
00:14:19,240 --> 00:14:23,600
And you know, this is in the 80s, think of, you know, computing power in those days and

229
00:14:23,600 --> 00:14:24,600
stuff.

230
00:14:24,600 --> 00:14:27,440
This is a really hard challenge to parallel park a truck.

231
00:14:27,440 --> 00:14:34,520
And after all the hurrah and happiness, apparently, or as the story goes, the next year,

232
00:14:34,520 --> 00:14:41,960
someone else published a paper in nips showing that one neuron, so a perceptron, one simple

233
00:14:41,960 --> 00:14:44,280
computing unit, can parallel park a truck.

234
00:14:44,280 --> 00:14:50,120
If you give it the input of the obstacles in polar coordinates rather than Cartesian coordinates.

235
00:14:50,120 --> 00:14:55,360
So what I took from that, I heard this when I was a master student, is basically, if you

236
00:14:55,360 --> 00:14:59,400
ask a question just right, it's really easy to answer.

237
00:14:59,400 --> 00:15:04,680
And that's basically what I'm saying, the decision making could be kind of pair down

238
00:15:04,680 --> 00:15:08,400
to a yes, no question that a perceptron can answer.

239
00:15:08,400 --> 00:15:13,200
But that pairing down is the hard thing, like giving the input just right.

240
00:15:13,200 --> 00:15:17,520
And so that's kind of my whole enterprise, my whole research enterprise is how does the

241
00:15:17,520 --> 00:15:24,080
brain learn how to ask the questions, ask complex questions so that they're made simple.

242
00:15:24,080 --> 00:15:30,760
And you know, how can we use that for AI to take all these complex problems, real world

243
00:15:30,760 --> 00:15:35,960
navigation, et cetera, and make them into easy problems that a perceptron can answer.

244
00:15:35,960 --> 00:15:36,960
Interesting.

245
00:15:36,960 --> 00:15:42,560
Is there evidence in the brain that that process looks like a filtering process, or is

246
00:15:42,560 --> 00:15:49,720
it more like a blindness to certain irrelevant variables, or is it the way things are classified?

247
00:15:49,720 --> 00:15:54,800
It seems like there are potentially a number of ways that you can not consider variables.

248
00:15:54,800 --> 00:15:55,800
Yeah.

249
00:15:55,800 --> 00:15:59,880
So, the evidence that we've seen so far in the brain that I actually didn't talk about

250
00:15:59,880 --> 00:16:06,320
today, and some glad that you asked about this, is that attention processes are strongly

251
00:16:06,320 --> 00:16:09,400
involved in sub-selecting.

252
00:16:09,400 --> 00:16:14,120
So first of all, anatomically, I should say that the areas in the brain that we know are

253
00:16:14,120 --> 00:16:19,080
involved in this kind of reinforcement learning and decision making, get input, the small

254
00:16:19,080 --> 00:16:22,040
area in the middle of the brain, it's called the stratum.

255
00:16:22,040 --> 00:16:26,440
And the stratum gets input from the whole cortex.

256
00:16:26,440 --> 00:16:33,640
So all the brain all around, the sensory, motor, high level, emotional, everything, everything

257
00:16:33,640 --> 00:16:34,960
funnels in.

258
00:16:34,960 --> 00:16:38,960
But it funnels in with a huge dimensionality reduction, so about 1 to 10,000.

259
00:16:38,960 --> 00:16:42,880
So 10,000 inputs into one neuron on average.

260
00:16:42,880 --> 00:16:46,920
So from the start, we know that there has to be huge dimensionality reduction.

261
00:16:46,920 --> 00:16:51,600
But more than that, I'm talking here about dimensionality reduction that's not structural,

262
00:16:51,600 --> 00:16:54,600
that's kind of modulated task specific for one task.

263
00:16:54,600 --> 00:16:57,360
I want to know the colors of the cars for another task I don't.

264
00:16:57,360 --> 00:17:02,200
So the input has to really be modulated on the fly according to our goals.

265
00:17:02,200 --> 00:17:07,280
And what we've seen is that areas in the brain that are involved in attention, in switching

266
00:17:07,280 --> 00:17:12,400
attention, selecting what to attend to are involved in this process.

267
00:17:12,400 --> 00:17:16,880
And it's really interesting for me to think about attention in that way, because we often

268
00:17:16,880 --> 00:17:22,440
think about selective attention or limited attention as a bottleneck or limitation.

269
00:17:22,440 --> 00:17:27,040
We can't attend to everything, so unfortunately, we have to ignore most of the scene.

270
00:17:27,040 --> 00:17:31,480
And I'm thinking about it in a much more normative way, that it's optimal to not attend

271
00:17:31,480 --> 00:17:32,480
to everything.

272
00:17:32,480 --> 00:17:33,720
That's what allows us to function.

273
00:17:33,720 --> 00:17:35,720
It's what allows us to learn.

274
00:17:35,720 --> 00:17:36,720
Right.

275
00:17:36,720 --> 00:17:42,480
So if you attend just right, and you're solving a problem, rather than creating a problem.

276
00:17:42,480 --> 00:17:48,640
So one of the challenges is to understand how do we learn what to attend to.

277
00:17:48,640 --> 00:17:50,120
And we're working on that.

278
00:17:50,120 --> 00:17:51,280
It's not easy.

279
00:17:51,280 --> 00:17:53,680
We have tasks where we can measure people's attention.

280
00:17:53,680 --> 00:17:56,160
We have a multi-dimensional scenario.

281
00:17:56,160 --> 00:17:59,400
We tell people that only one dimension matters.

282
00:17:59,400 --> 00:18:05,000
So these dimensions could be color, shape, texture of different stimuli that could be other

283
00:18:05,000 --> 00:18:06,200
dimensions.

284
00:18:06,200 --> 00:18:09,400
And we tell them that only one determines reward.

285
00:18:09,400 --> 00:18:13,160
And we're trying to see how they learn from trial and error, what to attend to.

286
00:18:13,160 --> 00:18:18,840
And in essence, what we're trying to do is devise a figure out, reverse engineer the

287
00:18:18,840 --> 00:18:24,760
algorithm, the computational algorithm, by which feedback for our actions affects our

288
00:18:24,760 --> 00:18:27,520
representation, affect what we attend to.

289
00:18:27,520 --> 00:18:30,480
And we're kind of in the midst of that.

290
00:18:30,480 --> 00:18:32,000
It's a it's a call order.

291
00:18:32,000 --> 00:18:35,760
We've actually talked to, it turns out that there aren't machine learning algorithms for

292
00:18:35,760 --> 00:18:37,760
modeling those kinds of data.

293
00:18:37,760 --> 00:18:38,760
Hmm.

294
00:18:38,760 --> 00:18:43,240
Now here attention come up a lot in the context of like LSTM networks.

295
00:18:43,240 --> 00:18:49,760
Is it just kind of a word overlap, but not the same kind of mechanism or idea, or are

296
00:18:49,760 --> 00:18:52,120
there some parallels there?

297
00:18:52,120 --> 00:18:56,680
I'm not well enough first in these models to know how direct the parallels are.

298
00:18:56,680 --> 00:19:00,760
I can say that from a psychology point of view, a neuroscience point of view, attention

299
00:19:00,760 --> 00:19:04,120
is almost kind of a dirty word, because we don't know exactly what it is.

300
00:19:04,120 --> 00:19:05,960
Some people hate that word.

301
00:19:05,960 --> 00:19:13,120
A selective filter is really kind of, you know, operationalization of this idea.

302
00:19:13,120 --> 00:19:17,920
I think there's been very little work, both in neuroscience.

303
00:19:17,920 --> 00:19:22,520
And well, I don't know it recently in machine learning, so truth I haven't really followed

304
00:19:22,520 --> 00:19:23,520
it.

305
00:19:23,520 --> 00:19:28,840
But in neuroscience, there's been very little work on how attention is learned from experience

306
00:19:28,840 --> 00:19:30,640
rather than from instruction.

307
00:19:30,640 --> 00:19:35,440
And all the work on attention is you tell the subject what to attend to, and then you figure

308
00:19:35,440 --> 00:19:39,360
out how is the brain doing that, how are they're focusing, how much are they sensitive

309
00:19:39,360 --> 00:19:41,080
to distractors, etc.

310
00:19:41,080 --> 00:19:43,080
And my question is completely different.

311
00:19:43,080 --> 00:19:48,400
And the place where I said that there aren't any algorithms for is to try to, so what

312
00:19:48,400 --> 00:19:52,240
we want is to predict attention, and we want to test our models.

313
00:19:52,240 --> 00:19:54,920
We want to evaluate how well our models are doing.

314
00:19:54,920 --> 00:20:01,720
So with choice data, which a lot of our experiments, we have subjects choosing actions.

315
00:20:01,720 --> 00:20:08,400
There's a whole wide range of models for modeling choices and comparing between models and

316
00:20:08,400 --> 00:20:12,200
saying, you know, this model is better than that model in predicting choice.

317
00:20:12,200 --> 00:20:16,440
What is missing for me is models that predict attention and a way to compare those.

318
00:20:16,440 --> 00:20:19,840
And the difference here, I mean, this is a little bit going into the weeds.

319
00:20:19,840 --> 00:20:24,000
But if you think of attention as a quantity that sums up to one, if I attend more to one

320
00:20:24,000 --> 00:20:30,720
thing, I can't attend more to everything else, this is a quantity that lives on the simplex.

321
00:20:30,720 --> 00:20:36,880
And it's a whole different statistics and geometry and comparison world on the simplex.

322
00:20:36,880 --> 00:20:40,280
And people haven't really worked on that because of this constraint that everything adds

323
00:20:40,280 --> 00:20:41,280
up to one.

324
00:20:41,280 --> 00:20:44,080
It's much, it's, it's, you need a different math.

325
00:20:44,080 --> 00:20:45,080
Okay.

326
00:20:45,080 --> 00:20:49,120
We've found a little bit of this math of all places in geology.

327
00:20:49,120 --> 00:20:55,280
Apparently, in geology, geologists want to ask, is this rock the same as that rock by

328
00:20:55,280 --> 00:21:01,200
looking at its composition and saying, you know, it's 80% this material and 15% this material

329
00:21:01,200 --> 00:21:02,400
is at the same as that one.

330
00:21:02,400 --> 00:21:07,000
So they do these comparisons of percentages and there's a whole book act to the on this

331
00:21:07,000 --> 00:21:09,400
compositional models.

332
00:21:09,400 --> 00:21:11,400
So we're reading that now.

333
00:21:11,400 --> 00:21:12,400
Wow.

334
00:21:12,400 --> 00:21:15,400
That's where we're trying to find our new math.

335
00:21:15,400 --> 00:21:16,400
Okay.

336
00:21:16,400 --> 00:21:17,400
Interesting.

337
00:21:17,400 --> 00:21:21,880
I talked about, you mentioned reinforcement learning in our conversation as well as in

338
00:21:21,880 --> 00:21:23,040
your talk.

339
00:21:23,040 --> 00:21:28,560
But when you, when I hear you say it, it's almost like the context is you're talking about

340
00:21:28,560 --> 00:21:32,400
the machine learning reinforcement learning, but you're also talking about like biological

341
00:21:32,400 --> 00:21:33,400
reinforcement.

342
00:21:33,400 --> 00:21:34,400
Am I reading that right?

343
00:21:34,400 --> 00:21:35,400
Yeah.

344
00:21:35,400 --> 00:21:36,400
Yeah.

345
00:21:36,400 --> 00:21:39,760
So reinforcement learning and psychology is called classical conditioning and instrumental

346
00:21:39,760 --> 00:21:40,760
conditioning.

347
00:21:40,760 --> 00:21:41,760
Okay.

348
00:21:41,760 --> 00:21:46,120
So the classic, you know, Pavlov with a dog, salivating, et cetera, that is learning

349
00:21:46,120 --> 00:21:49,360
values for states as in reinforcement learning.

350
00:21:49,360 --> 00:21:54,720
And then instrumental conditioning rats, lever pressing lever in order to get food, that's

351
00:21:54,720 --> 00:21:59,640
basically learning a policy that maximizes values and rewards.

352
00:21:59,640 --> 00:22:07,440
So there is a very kind of direct isomorphism or translation between the computational reinforcement

353
00:22:07,440 --> 00:22:13,280
learning world of learning policies and values and predictions and the behavioral psychology

354
00:22:13,280 --> 00:22:18,800
world and it goes through neuroscience or it goes to neuroscience because we also know

355
00:22:18,800 --> 00:22:24,720
in the brain where these different signals are computed and and and there's a lot of

356
00:22:24,720 --> 00:22:30,160
evidence this goes back to the 80s, actually, the 80s, no, the 90s.

357
00:22:30,160 --> 00:22:37,080
I think 94 was the first paper really making these parallels in particular dopamine, which

358
00:22:37,080 --> 00:22:42,880
is a really important neuromodulator in the brain that's involved in everything from

359
00:22:42,880 --> 00:22:52,880
Parkinson's disease to schizophrenia, depression, gambling, any drug abuse dopamine is seen

360
00:22:52,880 --> 00:22:59,440
today as representing in the brain, prediction errors, a lot of reinforcement learning.

361
00:22:59,440 --> 00:23:07,720
So literally, yeah, yeah, I'm often amazed that people don't know this because in neuroscience

362
00:23:07,720 --> 00:23:14,280
this is so big, this is basically one of the biggest success stories of taking machine

363
00:23:14,280 --> 00:23:19,200
learning, taking computational models and translating them to neuroscience and behavior

364
00:23:19,200 --> 00:23:25,560
is this is the idea that dopamine calculates a prediction error.

365
00:23:25,560 --> 00:23:30,120
So how different is what I'm getting from the world from what I predicted?

366
00:23:30,120 --> 00:23:34,600
This is a key quantity for learning and reinforcement learning, every reinforcement learning algorithm

367
00:23:34,600 --> 00:23:36,880
relies on prediction errors.

368
00:23:36,880 --> 00:23:39,720
We know behaviorally that animals learn from prediction errors.

369
00:23:39,720 --> 00:23:43,840
We know normally that dopamine represents these prediction errors and broadcast them to

370
00:23:43,840 --> 00:23:45,360
the whole brain.

371
00:23:45,360 --> 00:23:50,880
So anywhere in the brain, what does it even mean for dopamine to represent these prediction

372
00:23:50,880 --> 00:23:55,840
errors, meaning the levels of dopamine correlate strongly with prediction errors?

373
00:23:55,840 --> 00:24:02,360
Yes, it means that when in a task, I can through a computational model say, you probably

374
00:24:02,360 --> 00:24:07,000
just experienced a positive prediction error, so you expected, let's say, three MNM's and

375
00:24:07,000 --> 00:24:08,000
you got five.

376
00:24:08,000 --> 00:24:12,240
So through the model, I can track your learning and save with all the experience that you've

377
00:24:12,240 --> 00:24:14,920
had so far, you should be expecting three MNM's.

378
00:24:14,920 --> 00:24:16,960
You just got five, so it's a positive prediction error.

379
00:24:16,960 --> 00:24:22,360
If I record it from your brain, I would see a kind of positive burst of dopamine.

380
00:24:22,360 --> 00:24:28,200
So above baseline dopamine, whereas if my model says you should predict three MNM's and

381
00:24:28,200 --> 00:24:32,280
you actually get one MNM and I record from your brain, I'll see a negative dip.

382
00:24:32,280 --> 00:24:38,320
So I'll see less dopamine than the baseline dopamine levels at that exact time.

383
00:24:38,320 --> 00:24:41,280
It's a phasic, it's a short lived signal.

384
00:24:41,280 --> 00:24:42,720
It's broadcast all over the brain.

385
00:24:42,720 --> 00:24:46,880
It basically says at every point in time, are things better than expected right now or

386
00:24:46,880 --> 00:24:49,240
are things worse than expected?

387
00:24:49,240 --> 00:24:52,040
And AlphaGo relies on prediction errors.

388
00:24:52,040 --> 00:24:55,240
So all of reinforcement learning relies on prediction errors.

389
00:24:55,240 --> 00:24:56,560
And so we see that in the brain.

390
00:24:56,560 --> 00:25:00,360
So really when I say reinforcement learning, I'm thinking about the brain as much as I'm

391
00:25:00,360 --> 00:25:02,760
thinking about the algorithm.

392
00:25:02,760 --> 00:25:03,760
That's fascinating.

393
00:25:03,760 --> 00:25:04,760
Yeah.

394
00:25:04,760 --> 00:25:11,840
It's really an amazing case of convergence of all these lines of research.

395
00:25:11,840 --> 00:25:16,640
And is this a new observation or did you say 80s for this?

396
00:25:16,640 --> 00:25:17,640
94.

397
00:25:17,640 --> 00:25:20,440
I got not 80s, so 94.

398
00:25:20,440 --> 00:25:26,200
So in the 80s and the late 80s, the idea was that dopamine represents reward.

399
00:25:26,200 --> 00:25:28,680
So it's the brain signal for reward.

400
00:25:28,680 --> 00:25:32,320
And people started looking for that signal by recording from dopamine neurons, from

401
00:25:32,320 --> 00:25:37,160
monkey brains, as monkeys were obtaining rewards in a task.

402
00:25:37,160 --> 00:25:41,600
And they were really confused because sometimes dopamine would respond to the reward.

403
00:25:41,600 --> 00:25:42,920
Sometimes it didn't.

404
00:25:42,920 --> 00:25:47,560
And there were all these abstracts in the society for neuroscience saying, you know, this

405
00:25:47,560 --> 00:25:52,760
is clearly not a reward signal, but we have no idea what it is.

406
00:25:52,760 --> 00:25:58,000
And then Reed Montague and Peter Diane, who were then postdocs with Terry Sinowski at

407
00:25:58,000 --> 00:26:03,760
the Salk, the Salk Institute, read these papers and they'd been reading about reinforcement

408
00:26:03,760 --> 00:26:04,760
learning.

409
00:26:04,760 --> 00:26:08,400
So Rich Sutton and DiBarto's work.

410
00:26:08,400 --> 00:26:10,160
And they basically put two and two together.

411
00:26:10,160 --> 00:26:11,160
Wow.

412
00:26:11,160 --> 00:26:16,960
The story is that one day Reed Montague went to Peter Diane's office and said, look at this.

413
00:26:16,960 --> 00:26:20,160
This looks just like a prediction error signal.

414
00:26:20,160 --> 00:26:25,360
And then they published a paper they started from, you know, the politics of these things

415
00:26:25,360 --> 00:26:26,360
are weird.

416
00:26:26,360 --> 00:26:33,760
They started from a paper in science about bees, not about monkey, about bees navigating

417
00:26:33,760 --> 00:26:39,080
with the idea that a ketopamine, which is the equivalent of dopamine signals in insects,

418
00:26:39,080 --> 00:26:41,000
a ketopamine represents a prediction error.

419
00:26:41,000 --> 00:26:42,000
Okay.

420
00:26:42,000 --> 00:26:46,360
And then later they published a paper about that same thing in monkeys a year later in

421
00:26:46,360 --> 00:26:47,360
96.

422
00:26:47,360 --> 00:26:51,960
So this, this first paper was 95, I think there was an abstract in 94.

423
00:26:51,960 --> 00:26:59,480
And then in 97, the famous paper is Schultz Diane and Montague, 1997, where they published

424
00:26:59,480 --> 00:27:03,600
basically the recording data together with the model and said, and this was published

425
00:27:03,600 --> 00:27:07,480
in science saying dopamine seems to be a prediction error.

426
00:27:07,480 --> 00:27:13,480
And this was, you know, this was the hypothesis with some data and since then it's been tested

427
00:27:13,480 --> 00:27:15,760
a million times over.

428
00:27:15,760 --> 00:27:19,560
And on the one hand, it looks like sometimes it's so amazing.

429
00:27:19,560 --> 00:27:23,480
It looks like, you know, dopamine neurons must have read the text book, like they do exactly

430
00:27:23,480 --> 00:27:25,480
what you expect.

431
00:27:25,480 --> 00:27:30,120
And really, you know, convoluted situations, you set things up so that it should be, you

432
00:27:30,120 --> 00:27:33,680
know, whatever, and it is exactly that.

433
00:27:33,680 --> 00:27:41,640
But, you know, with neuroscience, the deeper you dig the more unexplained gold you find.

434
00:27:41,640 --> 00:27:45,760
So a lot of it fits the theory, some of it doesn't.

435
00:27:45,760 --> 00:27:49,320
And that, some of it is not isoteric stuff that we can ignore.

436
00:27:49,320 --> 00:27:52,600
It's not just noise, it's persistent differences.

437
00:27:52,600 --> 00:27:57,080
So the model, this idea is kind of a simplified.

438
00:27:57,080 --> 00:28:01,920
So dopamine, I believe that dopamine definitely represents a prediction error, but that's

439
00:28:01,920 --> 00:28:04,880
not the end of the story, that's the beginning of the story.

440
00:28:04,880 --> 00:28:10,720
And there's a lot of work trying to understand exactly how timing is represented in this

441
00:28:10,720 --> 00:28:15,960
system, and are these prediction errors only for reward or any kind of prediction that's

442
00:28:15,960 --> 00:28:17,280
violated?

443
00:28:17,280 --> 00:28:20,160
And if it's any kind of prediction that's violated, how do you know what to learn?

444
00:28:20,160 --> 00:28:21,640
How do you know what prediction was violated?

445
00:28:21,640 --> 00:28:27,240
Because when it's reward, it's kind of easy, you just update your prediction of reward.

446
00:28:27,240 --> 00:28:34,240
If it's anything, it becomes, you basically need more information in order to learn.

447
00:28:34,240 --> 00:28:38,040
It's making me wonder are there other chemicals that respond similarly that?

448
00:28:38,040 --> 00:28:42,720
Yeah, there aren't other chemicals in the brain that look like a prediction error signal.

449
00:28:42,720 --> 00:28:46,600
There are four neuromodulators in the brain, so there are lots of chemicals in the brain,

450
00:28:46,600 --> 00:28:52,840
but neuromodulators are signals that rather than communicating neuron to neuron kind of

451
00:28:52,840 --> 00:28:58,400
like a phone call, they're more like a PA system, they like broadcast very, very widely.

452
00:28:58,400 --> 00:29:02,840
So there's dopamine, norpinephrine, acetylcholine, and serotonin.

453
00:29:02,840 --> 00:29:07,720
And people have basically been dying to know what these four do because it's like, you

454
00:29:07,720 --> 00:29:12,560
know, if you had four broadcast systems, those are four things that you can tell everybody,

455
00:29:12,560 --> 00:29:15,400
what would you say with those four?

456
00:29:15,400 --> 00:29:21,000
And it's clear that all four are super critical in the sense that disrupting any of these

457
00:29:21,000 --> 00:29:25,480
systems causes a whole host of problems, which makes sense if they, you know, if they broadcast

458
00:29:25,480 --> 00:29:28,480
everywhere, they must be doing something really important.

459
00:29:28,480 --> 00:29:29,480
So dopamine is the best?

460
00:29:29,480 --> 00:29:33,440
So far only dopamine has been implicated in the learning process, or is that too strong

461
00:29:33,440 --> 00:29:34,440
statement?

462
00:29:34,440 --> 00:29:39,320
Dopamine is really important, that frame learns all the time, dopamine is the best understood.

463
00:29:39,320 --> 00:29:45,680
It seems like the simplest of the stories, but the others have been implicated in learning

464
00:29:45,680 --> 00:29:50,800
as well, so norpinephrine has actually been implicated in the breadth of attention, controlling

465
00:29:50,800 --> 00:29:55,160
the breadth of attention, controlling the gain of the system, controlling what you do or

466
00:29:55,160 --> 00:29:58,720
how do you respond to unexpected changes?

467
00:29:58,720 --> 00:30:05,200
So one of the ideas in learning is that when the world changes in an unexpected way, you

468
00:30:05,200 --> 00:30:10,640
should be able to kind of reset, to not continue to carry on previous learning, but to start

469
00:30:10,640 --> 00:30:15,680
a new, to reset your values, to increase your learning rate to say, okay, like, you know,

470
00:30:15,680 --> 00:30:17,480
I take a break here from the past.

471
00:30:17,480 --> 00:30:19,880
So norpinephrine is implicated in that.

472
00:30:19,880 --> 00:30:25,520
Acetyl colon is also implicated in adjusting learning rates to the variability of the environment,

473
00:30:25,520 --> 00:30:31,480
so an environment that's more variable, basically, it's more noisy, so every bit of information

474
00:30:31,480 --> 00:30:36,280
should carry less weight, so you should have a lower, a smaller step size in learning,

475
00:30:36,280 --> 00:30:42,160
we'll call it a smaller learning rate, so acetyl colon is implicated in that.

476
00:30:42,160 --> 00:30:47,920
And serotonin has been kind of, serotonin is so complicated, there are 20 different

477
00:30:47,920 --> 00:30:52,400
kinds of receptors for serotonin, and they do all kinds of things, I mean, all of these

478
00:30:52,400 --> 00:30:56,760
are complicated, everything that I'm saying is a gross simplification, but yeah, that's

479
00:30:56,760 --> 00:31:02,000
what we pay our neuroscientists for, you know, we're trying to figure it out.

480
00:31:02,000 --> 00:31:07,240
So I don't know, we've kind of strayed into like a lot of background material, which has

481
00:31:07,240 --> 00:31:10,920
been amazing, but you're, you're talk, right?

482
00:31:10,920 --> 00:31:17,080
So you're, I won't even try to like summarize it in a sentence, I'll let you do that.

483
00:31:17,080 --> 00:31:22,640
But it was a, well, no, that's what I would do to introduce it, you can actually kind

484
00:31:22,640 --> 00:31:27,440
of walk us through the framework that you presented and some of the experimental results

485
00:31:27,440 --> 00:31:29,560
and things like that.

486
00:31:29,560 --> 00:31:37,880
So kind of broad strokes, you're, you're kind of applying a Bayesian inference to learning,

487
00:31:37,880 --> 00:31:43,480
and at least what I got out of the talk was trying to identify, you know, these latent

488
00:31:43,480 --> 00:31:49,680
variables that are present in the way we kind of perceive the world through experimentation

489
00:31:49,680 --> 00:31:54,280
and relate that back to machine learning or statistical models.

490
00:31:54,280 --> 00:31:55,280
Yeah.

491
00:31:55,280 --> 00:32:00,160
So this goes back to what we talked about in the beginning of the podcast today, which

492
00:32:00,160 --> 00:32:06,600
is how do we parse the world into these, how do we put boundaries on learning and say

493
00:32:06,600 --> 00:32:10,680
all of these experiences are similar enough, I'm going to learn from one to another, I'm

494
00:32:10,680 --> 00:32:15,040
going to learn from this street to that street, and these are different, this is London,

495
00:32:15,040 --> 00:32:19,480
the cars come from the other side of the road, I definitely, definitely not generalized

496
00:32:19,480 --> 00:32:20,840
between these.

497
00:32:20,840 --> 00:32:26,520
And so what my talk was about is trying to identify and what my research is about, strike

498
00:32:26,520 --> 00:32:31,600
to identify the computational algorithms for putting these boundaries in place.

499
00:32:31,600 --> 00:32:38,000
Because I think that's, this is a computational way of talking about the issue of how do we

500
00:32:38,000 --> 00:32:41,160
take a complex problem and make it into a simple one.

501
00:32:41,160 --> 00:32:46,400
So when I cluster experiences together and say all these are similar, I'm basically saying

502
00:32:46,400 --> 00:32:49,040
I'm going to ignore all their differences.

503
00:32:49,040 --> 00:32:52,800
So that's part of like learning what to ignore, I'm going to say these are essentially,

504
00:32:52,800 --> 00:32:56,560
you know, in reinforcement learning, we would say, this is state one, right, we just

505
00:32:56,560 --> 00:32:59,080
give them a label, all of these things are state one.

506
00:32:59,080 --> 00:33:05,160
So now I don't have to, you know, I don't have to analyze all their minute differences.

507
00:33:05,160 --> 00:33:11,160
So I'm trying to understand how the brain decides what is state one, how the brain does

508
00:33:11,160 --> 00:33:13,360
this clustering.

509
00:33:13,360 --> 00:33:17,560
And it's really, it's not that I'm trying to identify what are the relevant aspects

510
00:33:17,560 --> 00:33:21,080
for each task that's, that's less interesting for me.

511
00:33:21,080 --> 00:33:27,320
I'm trying to, to understand what is this general purpose algorithm that can take complex

512
00:33:27,320 --> 00:33:32,080
input, like we talked about before, it can have potentially infinite dimensions that

513
00:33:32,080 --> 00:33:33,280
are relevant.

514
00:33:33,280 --> 00:33:39,600
And can easily say based on inputs one, two, and three based on these three dimensions

515
00:33:39,600 --> 00:33:43,280
and none, none other, I'm going to call this state one.

516
00:33:43,280 --> 00:33:47,120
And those three dimensions in a different scenario say that that one is state two and

517
00:33:47,120 --> 00:33:48,120
not state one.

518
00:33:48,120 --> 00:33:49,120
So that's what I want to understand.

519
00:33:49,120 --> 00:33:54,920
And I think because this is a NP hard, well, I don't know, NP hard, I haven't proven

520
00:33:54,920 --> 00:33:55,920
it.

521
00:33:55,920 --> 00:33:56,920
It's a very hard problem.

522
00:33:56,920 --> 00:34:00,520
Let's say this way, you can't actually use Bayesian inference for this.

523
00:34:00,520 --> 00:34:05,520
I gave, in my talk, I talked about the Chinese restaurant process prior.

524
00:34:05,520 --> 00:34:12,320
So a way to start from a prior that says there are a few latent causes for all of the observations

525
00:34:12,320 --> 00:34:13,320
that we see.

526
00:34:13,320 --> 00:34:17,360
And I'm going to try to cluster observations according to similarity and each cluster

527
00:34:17,360 --> 00:34:18,760
I'll call it a latent cause.

528
00:34:18,760 --> 00:34:21,360
And that will be, you know, my state one and state two.

529
00:34:21,360 --> 00:34:22,360
Is it?

530
00:34:22,360 --> 00:34:28,040
I was wondering this actually, what's the backstory for Chinese restaurant problem or process?

531
00:34:28,040 --> 00:34:34,160
Well, that comes from Stanford, you know, people who live in Palo Alto and go to Chinese restaurants.

532
00:34:34,160 --> 00:34:41,120
The backstory for this culinary metaphor is the idea that, so imagine you have a really

533
00:34:41,120 --> 00:34:47,560
large Chinese restaurant, like an infinite Chinese restaurant, and you think of each table

534
00:34:47,560 --> 00:34:50,280
as a cluster, a latent cause.

535
00:34:50,280 --> 00:34:52,440
And each customer is an observation.

536
00:34:52,440 --> 00:34:56,400
And the observation, so customers come in and they tend to sit at tables where a lot

537
00:34:56,400 --> 00:34:57,920
of people are already sitting.

538
00:34:57,920 --> 00:35:02,200
And that means that we tend to ascribe everything to a few latent causes.

539
00:35:02,200 --> 00:35:06,280
So a few clusters, we say, and we don't want to use all the tables out there.

540
00:35:06,280 --> 00:35:09,600
But once in a while, someone sits at a new table.

541
00:35:09,600 --> 00:35:11,200
So there's infinite capacity.

542
00:35:11,200 --> 00:35:15,960
It's an infinitely sized Chinese restaurant, but people tend to sit together.

543
00:35:15,960 --> 00:35:21,960
This is, you know, just the metaphor for an infinite capacity mixture model.

544
00:35:21,960 --> 00:35:22,960
There are others.

545
00:35:22,960 --> 00:35:24,560
There's the Indian buffet process.

546
00:35:24,560 --> 00:35:31,320
Because we're the same people, another culinary metaphor, and it's kind of similar to that.

547
00:35:31,320 --> 00:35:33,120
It's the only way she got to keep going, right?

548
00:35:33,120 --> 00:35:34,120
I know, I know.

549
00:35:34,120 --> 00:35:40,040
So in the Indian buffet process, you choose a number of dishes, and again, you're choosing.

550
00:35:40,040 --> 00:35:44,680
You tend to choose dishes, it's like an infinite buffet, and you tend to choose dishes that

551
00:35:44,680 --> 00:35:46,520
people have already chosen.

552
00:35:46,520 --> 00:35:47,720
But you're choosing several.

553
00:35:47,720 --> 00:35:52,080
So now the idea is that every observation has several latent causes, not only one.

554
00:35:52,080 --> 00:35:56,440
So in the Chinese restaurant, you only sit at one table, and here you're taking several

555
00:35:56,440 --> 00:35:57,440
dishes.

556
00:35:57,440 --> 00:35:58,440
Where are we?

557
00:35:58,440 --> 00:36:03,720
So I was saying, okay, so this is a Bayesian process.

558
00:36:03,720 --> 00:36:09,280
But even when we apply it to a really simple experiments, we have to approximate.

559
00:36:09,280 --> 00:36:11,840
It's not tractable.

560
00:36:11,840 --> 00:36:14,080
And I think the brain does even better than that.

561
00:36:14,080 --> 00:36:21,080
I think the brain doesn't even try to approximate closely a optimal Bayesian solution.

562
00:36:21,080 --> 00:36:23,720
I think the brain does something that just works.

563
00:36:23,720 --> 00:36:25,240
It might not be optimal.

564
00:36:25,240 --> 00:36:33,400
It's susceptible to biases or decision-making is not always correct, but it's vastly simplified.

565
00:36:33,400 --> 00:36:38,320
I think for the brain, simple is more important than optimal.

566
00:36:38,320 --> 00:36:45,320
And that's because the biggest constraint is, in my view, on learning a decision-making

567
00:36:45,320 --> 00:36:51,960
in real life, in real life, people and animals is time.

568
00:36:51,960 --> 00:36:52,960
We have tons of neurons.

569
00:36:52,960 --> 00:36:55,880
It's not that we don't have enough computational machinery.

570
00:36:55,880 --> 00:36:58,200
What we don't have is enough experience.

571
00:36:58,200 --> 00:37:03,040
Any task that we need to do 30,000 times to be able to do it correctly, and 30,000 is

572
00:37:03,040 --> 00:37:05,160
small for AI, right?

573
00:37:05,160 --> 00:37:11,160
Like millions and millions of trials, even reinforcement learning algorithms, usually thousands

574
00:37:11,160 --> 00:37:13,400
of trials is normal.

575
00:37:13,400 --> 00:37:16,120
We don't need thousands of trials to learn almost anything.

576
00:37:16,120 --> 00:37:23,600
Yes, to be a world-class chess player or to be a perfect athlete of something.

577
00:37:23,600 --> 00:37:24,920
You need a lot.

578
00:37:24,920 --> 00:37:25,920
That's skill learning.

579
00:37:25,920 --> 00:37:31,280
To learn, to just solve a task way better than chance, to learn to survive and not get

580
00:37:31,280 --> 00:37:33,480
run over by a car.

581
00:37:33,480 --> 00:37:35,640
It's just a handful sometimes.

582
00:37:35,640 --> 00:37:40,360
And I think, so what I'm really interested in and what I was trying to give a flavor for

583
00:37:40,360 --> 00:37:43,920
in this talk was, how does the brain solve this?

584
00:37:43,920 --> 00:37:47,880
And what I showed, I didn't show the how.

585
00:37:47,880 --> 00:37:53,760
I said, you know, in, as we don't know, it's not that I have some secret that I'm not

586
00:37:53,760 --> 00:37:54,760
telling.

587
00:37:54,760 --> 00:38:00,040
Was the idea with bringing up the Bayesian inference piece to say, it sounds like it wasn't

588
00:38:00,040 --> 00:38:05,600
to say this is, you know, a useful model necessarily, but more, whatever it is, it's

589
00:38:05,600 --> 00:38:08,520
much simpler than this, and this is the simplest that we'd have.

590
00:38:08,520 --> 00:38:16,000
Yeah, it's a useful model because it inspired, it's in the way that in our work, it inspired

591
00:38:16,000 --> 00:38:17,440
our experiments that I showed.

592
00:38:17,440 --> 00:38:21,280
So it gave us the idea that similarity is key.

593
00:38:21,280 --> 00:38:25,760
So whether it's that exact algorithm or something else, it gave us the idea that similarity

594
00:38:25,760 --> 00:38:29,920
between different observations is going to be key to clustering them or not, it gave

595
00:38:29,920 --> 00:38:35,080
us the idea that the clustering is key that basically, if you think of the real world,

596
00:38:35,080 --> 00:38:40,600
not an experiment, information comes in all the time, there has to be this meta decision

597
00:38:40,600 --> 00:38:41,600
in the brain.

598
00:38:41,600 --> 00:38:44,080
Is this something I know about, or is this new?

599
00:38:44,080 --> 00:38:48,360
If it's something I know about, what do I know, let me retrieve it from memory, let me

600
00:38:48,360 --> 00:38:52,000
act on it, let me update what I know if I find that things are different.

601
00:38:52,000 --> 00:38:55,520
If it's something new, well, let's observe the world and see what to do or let's choose

602
00:38:55,520 --> 00:38:57,880
an action randomly and see what happens.

603
00:38:57,880 --> 00:39:02,120
So there's always this tension between old and new, and that's what the Chinese restaurant

604
00:39:02,120 --> 00:39:06,200
process basically embodies for us or gave us this idea because in the Chinese restaurant

605
00:39:06,200 --> 00:39:10,400
process, you're asking, is this an old latent cause or a new latent cause when you see

606
00:39:10,400 --> 00:39:11,400
an observation?

607
00:39:11,400 --> 00:39:15,960
So I think that that gave us really deep insight, even though I'm not committed to that specific

608
00:39:15,960 --> 00:39:19,880
algorithm, I think that idea is real.

609
00:39:19,880 --> 00:39:24,720
That's how our memory works, that's how we organize information in our brain, is it

610
00:39:24,720 --> 00:39:26,200
older, is it new?

611
00:39:26,200 --> 00:39:29,600
And so it just made us think of this, like, what is doing that?

612
00:39:29,600 --> 00:39:34,200
This is a cognitive, a new cognitive process if you want because nobody had talked about

613
00:39:34,200 --> 00:39:35,840
this process before.

614
00:39:35,840 --> 00:39:40,800
So that's why the algorithm was so powerful for us, even if it's not exactly that one.

615
00:39:40,800 --> 00:39:46,880
And so the experiments that I showed were trying to probe the general idea, is similarity

616
00:39:46,880 --> 00:39:52,120
key for clustering in humans to answer is yes, is it key for how we organize our memory

617
00:39:52,120 --> 00:39:53,840
the answer is yes.

618
00:39:53,840 --> 00:40:00,960
And then the third experiment that I showed was looking at where are these clusters stored

619
00:40:00,960 --> 00:40:01,960
in the brain?

620
00:40:01,960 --> 00:40:06,000
And this was not so much a yes or no question.

621
00:40:06,000 --> 00:40:09,440
It was really, now it was, can we be our opportunistic about this?

622
00:40:09,440 --> 00:40:14,320
So if there is this clustering, if these states are that are learned by the brain are represented

623
00:40:14,320 --> 00:40:18,640
somewhere that we can read them out, then we can start tracking what is the algorithm

624
00:40:18,640 --> 00:40:20,000
by which they're learned?

625
00:40:20,000 --> 00:40:24,560
Because I still haven't answered the main question that I started my whole, my lab and my research

626
00:40:24,560 --> 00:40:28,960
career with, which is how do we learn these say representation?

627
00:40:28,960 --> 00:40:31,040
So I'm, you know, there are clues on the way.

628
00:40:31,040 --> 00:40:32,960
So now I'm thinking about it in this clustering way.

629
00:40:32,960 --> 00:40:35,440
Closest fiction, we're busy for a while, but.

630
00:40:35,440 --> 00:40:36,440
Yeah.

631
00:40:36,440 --> 00:40:39,320
Thinking about it in this clustering way, now I know where to read them out in the brain.

632
00:40:39,320 --> 00:40:46,520
Now I need to give human participants tasks or animals tasks where they are learning states

633
00:40:46,520 --> 00:40:51,480
on the fly, I can read out what they're representing at each point in time and try to understand

634
00:40:51,480 --> 00:40:56,160
what is the algorithm that modifies these state representations over time.

635
00:40:56,160 --> 00:40:58,160
Is it this Chinese restaurant process?

636
00:40:58,160 --> 00:41:00,320
Is it an approximation to it?

637
00:41:00,320 --> 00:41:02,280
Is it something different?

638
00:41:02,280 --> 00:41:09,480
And the specific example that you gave, I forget the problem, but you had subjects in

639
00:41:09,480 --> 00:41:15,800
a MRI and you were able to read out images from the specific implicated section of the brain

640
00:41:15,800 --> 00:41:20,400
in the orbit of frontal cortex, the area above our eyes, that's a really important area.

641
00:41:20,400 --> 00:41:27,320
And then just use those images as inputs to a predictive model that was shown to be

642
00:41:27,320 --> 00:41:30,280
better than chance, is that the right interpretation?

643
00:41:30,280 --> 00:41:35,000
So what I was doing is I was showing, it wasn't really a predictive model, it was basically

644
00:41:35,000 --> 00:41:40,360
using a classifier, a corrector machine, impact, you know, shopping through algorithms

645
00:41:40,360 --> 00:41:49,280
and nips, a corrector machine that basically takes the activation patterns in a magnet

646
00:41:49,280 --> 00:41:54,400
that measures activation patterns in the brain online as people are performing a task

647
00:41:54,400 --> 00:41:55,400
I could measure.

648
00:41:55,400 --> 00:41:56,400
What was that task again?

649
00:41:56,400 --> 00:42:02,280
The task was, it's kind of, you know, it's a little bit complex on purpose, it's a task

650
00:42:02,280 --> 00:42:07,080
where you have to judge, you see faces and houses that are overlaid on each other, and

651
00:42:07,080 --> 00:42:11,560
you have to judge whether the face is older young or whether the house is older young and

652
00:42:11,560 --> 00:42:14,560
there is this rule about whether you're judging faces and houses and-

653
00:42:14,560 --> 00:42:16,560
The point people to that one is pretty-

654
00:42:16,560 --> 00:42:17,560
Exactly.

655
00:42:17,560 --> 00:42:22,760
And you're basically, you're performing this task over time by keeping in mind which state

656
00:42:22,760 --> 00:42:25,200
of 16 different states you're in.

657
00:42:25,200 --> 00:42:28,400
And what we showed is that you can look at the orbit frontal cortex at the activations

658
00:42:28,400 --> 00:42:36,200
in an orbit frontal cortex and from that activity alone you can classify whether a person

659
00:42:36,200 --> 00:42:40,040
is now in state one or state two or state three of these 16 states.

660
00:42:40,040 --> 00:42:44,200
And so what that tells us is that that's a place where you can read out this state representation

661
00:42:44,200 --> 00:42:49,560
even when the representation is, it involves unobservable information, it's inferred, it's

662
00:42:49,560 --> 00:42:51,400
an internal cluster.

663
00:42:51,400 --> 00:42:56,120
And further you did some work that showed that no other places in the brain are likely

664
00:42:56,120 --> 00:42:57,960
to be storing this state information.

665
00:42:57,960 --> 00:43:02,160
Well there were other areas that were likely to be but we found-

666
00:43:02,160 --> 00:43:07,760
So a state representation for a task, for reinforcement learning task is a kind of a,

667
00:43:07,760 --> 00:43:12,200
it's a specific entity, the state you want it to be Markov so you want to have all the

668
00:43:12,200 --> 00:43:17,320
necessary information in even from the past in your representation right now.

669
00:43:17,320 --> 00:43:22,280
And from what I said before about generalization, you want it to not have any extraneous information.

670
00:43:22,280 --> 00:43:24,280
So it has to be a very specific thing.

671
00:43:24,280 --> 00:43:29,800
And that specific, all the relevant things and nothing but them, we found only the orbit

672
00:43:29,800 --> 00:43:30,800
of frontal cortex.

673
00:43:30,800 --> 00:43:37,520
Other brain areas, I mean the orbit of frontal cortex is pretty much as far from sensory input

674
00:43:37,520 --> 00:43:43,520
as can be in the sense that it doesn't get any sense, well it does get, it gets input

675
00:43:43,520 --> 00:43:50,400
from all of sensory cortex but it's not doing its own primary sensory processing.

676
00:43:50,400 --> 00:43:54,320
So a lot of other areas are representing parts of the state, you know, contributing

677
00:43:54,320 --> 00:43:56,640
that potentially to the orbit of frontal cortex.

678
00:43:56,640 --> 00:44:00,440
But I see the orbit of frontal cortex is kind of the final place that says, okay, now

679
00:44:00,440 --> 00:44:05,760
right now for this task, this is, this is my state, building on everything else.

680
00:44:05,760 --> 00:44:09,960
And yeah, and it was the only place in the brain that we found that kind of representation.

681
00:44:09,960 --> 00:44:12,560
Well, we're running long time.

682
00:44:12,560 --> 00:44:18,480
I am hoping that the videos from nips are going to be made available as recordings and so

683
00:44:18,480 --> 00:44:19,480
folks.

684
00:44:19,480 --> 00:44:22,760
I really encourage folks to take a look at your talk because we've really only kind of

685
00:44:22,760 --> 00:44:23,760
skim the surface.

686
00:44:23,760 --> 00:44:27,520
There are some really great experimental examples that you provided that I think folks

687
00:44:27,520 --> 00:44:33,000
would find interesting, but sort of that any kind of wrap up thoughts or places that you

688
00:44:33,000 --> 00:44:38,160
would want to, you know, send people or places that they should start if they're interested

689
00:44:38,160 --> 00:44:40,080
in this field.

690
00:44:40,080 --> 00:44:43,080
Can I take this to a slightly different place for a wrap up?

691
00:44:43,080 --> 00:44:44,080
Sure.

692
00:44:44,080 --> 00:44:45,080
Sure.

693
00:44:45,080 --> 00:44:47,880
So, I'm going to put one of my activism hats on.

694
00:44:47,880 --> 00:44:48,880
Okay.

695
00:44:48,880 --> 00:44:53,920
So in my field in neuroscience, we have a website called Bias Watch Nero.

696
00:44:53,920 --> 00:44:54,920
Okay.

697
00:44:54,920 --> 00:45:00,880
Bias Watch Nero.com, I think, or dot org, I think dot com.

698
00:45:00,880 --> 00:45:07,600
And in that site, what it does is it tracks the gender composition of conferences.

699
00:45:07,600 --> 00:45:11,880
So basically, the idea is for every conference in the field of neuroscience, including computational

700
00:45:11,880 --> 00:45:17,920
neuroscience, on that site, there will be a post of how many of the invited speakers.

701
00:45:17,920 --> 00:45:20,440
This is not contribute to talks, but invited.

702
00:45:20,440 --> 00:45:23,120
How many of the invited speakers are women?

703
00:45:23,120 --> 00:45:24,120
How many are men?

704
00:45:24,120 --> 00:45:26,840
What is the base rate for that specific conference?

705
00:45:26,840 --> 00:45:33,560
So they have a way in Bias Watch Nero calculating in a transparent way that anybody can recreate

706
00:45:33,560 --> 00:45:34,880
for themselves.

707
00:45:34,880 --> 00:45:39,280
What is the base rate of female faculty in that subfield?

708
00:45:39,280 --> 00:45:43,080
And the idea is, it's scientists, we know what a Bias sample is, right?

709
00:45:43,080 --> 00:45:48,600
We don't want to give our algorithms a Bias sample of what they need to learn from.

710
00:45:48,600 --> 00:45:52,880
So why are we giving our audiences a Bias samples of all the great ideas out there in science?

711
00:45:52,880 --> 00:45:56,840
If there are 20 percent women in a field, yes, you know, it would be better if there were

712
00:45:56,840 --> 00:45:57,840
50 percent.

713
00:45:57,840 --> 00:45:58,840
But there are 20 percent.

714
00:45:58,840 --> 00:46:01,760
Why do we have only 5 percent in our conferences?

715
00:46:01,760 --> 00:46:04,080
Why are we missing out on these great ideas?

716
00:46:04,080 --> 00:46:08,280
And this website has made a huge difference, I think, in neuroscience.

717
00:46:08,280 --> 00:46:12,480
And just a couple of years, the Bias has really gone down.

718
00:46:12,480 --> 00:46:18,360
And I think it would be great if computer science started a similar thing.

719
00:46:18,360 --> 00:46:24,440
So Bias Watch CS or something of that sort, including machine learning, including AI, because

720
00:46:24,440 --> 00:46:33,560
I think really it's even more of a pill climb for women in computer science related fields.

721
00:46:33,560 --> 00:46:36,520
And you know, diversity of ideas is good for everybody.

722
00:46:36,520 --> 00:46:38,480
We all want all the best ideas out there.

723
00:46:38,480 --> 00:46:44,560
We all want to, you know, make our, we want to progress as fast as possible with knowledge.

724
00:46:44,560 --> 00:46:50,360
And so, yeah, so, so it's kind of little, you know, call for activism in computer science

725
00:46:50,360 --> 00:46:51,360
in this field.

726
00:46:51,360 --> 00:46:56,120
And I, I know the people who started Bias Watch Neural, and I'm happy to help anybody

727
00:46:56,120 --> 00:47:00,120
who wants to start Bias Watch computer science, so they can just contact me.

728
00:47:00,120 --> 00:47:01,120
And yeah.

729
00:47:01,120 --> 00:47:02,120
All right.

730
00:47:02,120 --> 00:47:03,120
Great.

731
00:47:03,120 --> 00:47:04,120
What's the best way for them to contact you?

732
00:47:04,120 --> 00:47:05,120
Yeah.

733
00:47:05,120 --> 00:47:06,120
At Princeton.edu.

734
00:47:06,120 --> 00:47:07,120
That is so simple.

735
00:47:07,120 --> 00:47:08,120
Yes.

736
00:47:08,120 --> 00:47:17,200
You can spell Yael, which is Y-A-E-L, and Princeton has an E after the C. So people forget

737
00:47:17,200 --> 00:47:18,200
that.

738
00:47:18,200 --> 00:47:21,480
So yes, Yael, Y-A-E-L at Princeton.edu.

739
00:47:21,480 --> 00:47:22,480
Yeah.

740
00:47:22,480 --> 00:47:23,480
Thank you so much.

741
00:47:23,480 --> 00:47:24,480
Well, yeah, thank you.

742
00:47:24,480 --> 00:47:25,480
Thank you.

743
00:47:25,480 --> 00:47:26,480
It was a great conversation.

744
00:47:26,480 --> 00:47:27,480
I really enjoyed it.

745
00:47:27,480 --> 00:47:28,480
Thank you.

746
00:47:28,480 --> 00:47:32,520
All right, everyone.

747
00:47:32,520 --> 00:47:34,680
That's our show for today.

748
00:47:34,680 --> 00:47:39,640
Thank you so much for listening and for your continued feedback and support.

749
00:47:39,640 --> 00:47:44,800
For more information on Yael or any of the topics covered in this episode, head on over

750
00:47:44,800 --> 00:47:49,160
to twimlai.com slash talk slash 92.

751
00:47:49,160 --> 00:47:56,080
To follow along with the NIP series, visit twimlai.com slash NIPS 2017.

752
00:47:56,080 --> 00:48:02,720
To enter our Twimlai 1 Mill contest, visit twimlai.com slash twimlai 1 Mill.

753
00:48:02,720 --> 00:48:08,480
Of course, we'd be delighted to hear from you either via a comment on the show notes page

754
00:48:08,480 --> 00:48:13,680
or via a tweet to at twimlai or at Sam Charrington.

755
00:48:13,680 --> 00:48:18,200
Thanks once again to Intel Nirvana for their sponsorship of this series.

756
00:48:18,200 --> 00:48:22,920
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in

757
00:48:22,920 --> 00:48:27,160
the AI arena, visit intelnervana.com.

758
00:48:27,160 --> 00:48:31,920
As I mentioned a few weeks back, this will be our final series of shows for the year.

759
00:48:31,920 --> 00:48:37,120
So take your time and take it all in and get caught up on any of the old pods you've been

760
00:48:37,120 --> 00:48:38,720
saving up.

761
00:48:38,720 --> 00:48:41,160
Happy holidays and happy new year.

762
00:48:41,160 --> 00:48:43,440
See you in 2018.

763
00:48:43,440 --> 00:49:11,200
And of course, thanks once again for listening and catch you next time.

