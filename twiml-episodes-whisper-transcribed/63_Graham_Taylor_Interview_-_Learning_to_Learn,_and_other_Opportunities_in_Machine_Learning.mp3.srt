1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,480
I'm your host Sam Charrington.

4
00:00:23,480 --> 00:00:27,400
What you're about to hear is the third of a series of shows recorded at the Georgian

5
00:00:27,400 --> 00:00:31,200
Partners Portfolio Conference last week in Toronto.

6
00:00:31,200 --> 00:00:36,040
My guess this time is Graham Taylor, professor of engineering at the University of Guelph

7
00:00:36,040 --> 00:00:39,040
who keynoteed day two of the conference.

8
00:00:39,040 --> 00:00:43,200
Graham leads the machine learning research group at Guelph and is affiliated with Toronto's

9
00:00:43,200 --> 00:00:47,520
recently formed Vector Institute for Artificial Intelligence.

10
00:00:47,520 --> 00:00:51,320
Graham and I discussed a number of the most important trends and challenges in artificial

11
00:00:51,320 --> 00:00:56,800
intelligence, including the move from predictive to creative systems, the rise of human in the

12
00:00:56,800 --> 00:01:02,720
loop AI and how modern AI is accelerating with our ability to teach computers how to learn

13
00:01:02,720 --> 00:01:04,040
to learn.

14
00:01:04,040 --> 00:01:08,200
Georgian Partners is a venture capital firm whose investment thesis is that certain tech

15
00:01:08,200 --> 00:01:14,160
trends change every aspect of a software business over time, including business goals, product

16
00:01:14,160 --> 00:01:19,640
plans, people in skills, technology platforms, pricing and packaging.

17
00:01:19,640 --> 00:01:24,160
Georgian invests in those companies best positioned to take advantage of these trends and then

18
00:01:24,160 --> 00:01:28,920
works closely with those companies to develop and execute the strategies necessary to make

19
00:01:28,920 --> 00:01:30,280
it happen.

20
00:01:30,280 --> 00:01:35,280
Applied AI is one of the trends they're investing in as our conversational business and security

21
00:01:35,280 --> 00:01:36,280
first.

22
00:01:36,280 --> 00:01:39,660
Georgian sponsored this series and we thank them for their support.

23
00:01:39,660 --> 00:01:45,400
To learn more about Georgian, visit twimmolai.com slash Georgian where you'll also be able to

24
00:01:45,400 --> 00:01:51,240
download white papers on their principles of applied AI and conversational business.

25
00:01:51,240 --> 00:01:55,720
Before we jump in, if you're in New York City on October 30th and 31st, we hope you'll

26
00:01:55,720 --> 00:02:00,120
join us at the NYU Future Labs AI Summit and Happy Hour.

27
00:02:00,120 --> 00:02:04,040
As you may remember, we attended the inaugural summit back in April.

28
00:02:04,040 --> 00:02:08,440
The fall event features more grade speakers including Karina Cortez head of research at

29
00:02:08,440 --> 00:02:14,320
Google New York, David Venturelli, science operations manager at NASA Ames Quantum AI

30
00:02:14,320 --> 00:02:19,480
Lab, and Dennis Mortensen, CEO and founder of startup x.ai.

31
00:02:19,480 --> 00:02:27,600
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off tickets, use the

32
00:02:27,600 --> 00:02:29,720
code twimmol25.

33
00:02:29,720 --> 00:02:36,120
For details on the Happy Hour, visit our events page at twimmolai.com slash events, and now

34
00:02:36,120 --> 00:02:38,120
onto the show.

35
00:02:38,120 --> 00:02:49,280
Alright everyone, I am here at the Georgian Partners portfolio conference and I've got

36
00:02:49,280 --> 00:02:51,880
the pleasure of being seated with Graham Taylor.

37
00:02:51,880 --> 00:02:57,840
Graham is a professor at the University of Guelph here in Canada, and he did a really

38
00:02:57,840 --> 00:03:04,120
interesting talk today on some of the challenges and opportunities associated with machine learning

39
00:03:04,120 --> 00:03:07,840
in AI and particularly around his research area and deep learning.

40
00:03:07,840 --> 00:03:11,040
And we're here to spend a little bit of time chatting about that.

41
00:03:11,040 --> 00:03:12,720
Graham, welcome to the podcast.

42
00:03:12,720 --> 00:03:13,880
Thanks for having me on the program.

43
00:03:13,880 --> 00:03:17,880
I told you this is my first time doing a podcast, so I'm really excited to being a consumer

44
00:03:17,880 --> 00:03:20,520
of podcasts to actually get back to the podcast community.

45
00:03:20,520 --> 00:03:21,520
So thanks for having me.

46
00:03:21,520 --> 00:03:23,640
Nice, absolutely, absolutely.

47
00:03:23,640 --> 00:03:27,800
Why don't we get started by having you tell us a little bit about your background and

48
00:03:27,800 --> 00:03:32,000
how you got involved in machine learning in AI and what you're up to nowadays?

49
00:03:32,000 --> 00:03:36,360
Sure, so currently I'm working as a professor at the University of Guelph.

50
00:03:36,360 --> 00:03:41,440
I'm also a member of the new Vector Institute for Artificial Intelligence, which has started

51
00:03:41,440 --> 00:03:45,400
up and getting ready to move in in November here in Toronto.

52
00:03:45,400 --> 00:03:52,280
I'm the academic director of a program called Next AI, which is a founder development program

53
00:03:52,280 --> 00:03:55,560
for startup specifically working on AI technologies.

54
00:03:55,560 --> 00:03:59,480
So I wear a number of different hats, but they're all focused on artificial intelligence

55
00:03:59,480 --> 00:04:01,000
and machine learning.

56
00:04:01,000 --> 00:04:04,760
So let me tell you a little bit about how I entered that space.

57
00:04:04,760 --> 00:04:10,400
I often get asked this question of how I got into AI, and fortunately I can point at

58
00:04:10,400 --> 00:04:14,680
one specific point in my life, which really convinced me.

59
00:04:14,680 --> 00:04:19,240
And that was an inspiring professor when I was an undergrad student at the University

60
00:04:19,240 --> 00:04:20,240
of Waterloo.

61
00:04:20,240 --> 00:04:21,640
So I had a course.

62
00:04:21,640 --> 00:04:27,040
I believe the course was called Machine Intelligence, and the way that course was set

63
00:04:27,040 --> 00:04:33,920
up was to actually encourage us all the students to write AI programs to play each other's

64
00:04:33,920 --> 00:04:36,280
AI programs in this game called Abelone.

65
00:04:36,280 --> 00:04:39,360
And it's amazing looking at this effectively.

66
00:04:39,360 --> 00:04:43,960
I would say it's been at least 15 years now since we did this.

67
00:04:43,960 --> 00:04:47,800
But with all the news last week, in terms of this new AlphaGo system by Google Deep

68
00:04:47,800 --> 00:04:52,720
Mind, playing the game of Go, and how it was trained entirely by self-play.

69
00:04:52,720 --> 00:04:55,120
This is exactly what we were trying to do on this assignment.

70
00:04:55,120 --> 00:05:00,480
It was an easier game, but it was really inspirational to build these agents, played against each

71
00:05:00,480 --> 00:05:05,480
other, and our team ended up winning the competition, made us really proud and excited and eager

72
00:05:05,480 --> 00:05:06,480
to do more work.

73
00:05:06,480 --> 00:05:07,480
Nice.

74
00:05:07,480 --> 00:05:09,480
Yeah, so that's what started all off, okay?

75
00:05:09,480 --> 00:05:12,440
And so what's your path been so far?

76
00:05:12,440 --> 00:05:18,800
So from that point, as an undergrad, I got so excited about the potential of AI.

77
00:05:18,800 --> 00:05:23,000
No, I wouldn't say at all I was sort of going to predict what would happen up to this

78
00:05:23,000 --> 00:05:25,640
time and how huge it's growing.

79
00:05:25,640 --> 00:05:27,560
But I just observed being a technical person.

80
00:05:27,560 --> 00:05:32,920
I was really excited about building those tools and wanting to learn more.

81
00:05:32,920 --> 00:05:34,440
So I went to the University of Toronto.

82
00:05:34,440 --> 00:05:36,920
I knew they had a good machine learning program.

83
00:05:36,920 --> 00:05:42,080
They had a number of faculty there who I was aware of, their work, lots of graduates,

84
00:05:42,080 --> 00:05:43,080
students.

85
00:05:43,080 --> 00:05:44,440
It seemed like a great place to be.

86
00:05:44,440 --> 00:05:47,800
It was not too far away from Waterloo and London, Ontario, where I grew up.

87
00:05:47,800 --> 00:05:48,800
Okay.

88
00:05:48,800 --> 00:05:50,800
It seemed like a natural choice.

89
00:05:50,800 --> 00:05:55,720
Now I had no idea how big that group would become and the influence that Toronto machine

90
00:05:55,720 --> 00:05:58,760
learning group would have on deep learning today.

91
00:05:58,760 --> 00:06:03,480
So some people asked me about this and I said, well, I know I kind of stumbled upon this

92
00:06:03,480 --> 00:06:04,480
group.

93
00:06:04,480 --> 00:06:08,400
It seemed the right place to be for all the right reasons, but it ended up being an amazing

94
00:06:08,400 --> 00:06:09,400
time to be there.

95
00:06:09,400 --> 00:06:15,600
This is for me 2004 to 2009, really the start of the deep learning movement.

96
00:06:15,600 --> 00:06:22,280
So a lot of the individuals that are leading the major industrial research labs or even

97
00:06:22,280 --> 00:06:28,360
the nonprofit efforts like OpenAI, for example, they were students in the group at the time.

98
00:06:28,360 --> 00:06:34,840
A lot of the key papers and key ideas that were published and disseminated at that time

99
00:06:34,840 --> 00:06:38,120
have gone on, right, to be sort of the foundations of A&M.

100
00:06:38,120 --> 00:06:41,760
And a lot of it came out of that group and the groups that we were having close collaboration

101
00:06:41,760 --> 00:06:46,600
with, including the Montreal, now they call it Montreal Institute for Learning Algorithms

102
00:06:46,600 --> 00:06:51,240
run by Yasha Bengeo, and then the group at NYU, which at that time was led by you on

103
00:06:51,240 --> 00:06:53,160
the can.

104
00:06:53,160 --> 00:06:56,240
And mentioning NYU, that's where I went immediately after PhD.

105
00:06:56,240 --> 00:06:59,920
We had a good relationship, working relationship between these three labs, Montreal, Toronto

106
00:06:59,920 --> 00:07:00,920
and NYU.

107
00:07:00,920 --> 00:07:05,680
I considered both as postdoc options, but ultimately decided to go to New York for a couple

108
00:07:05,680 --> 00:07:10,600
years and work there as a postdoc, but felt the pull to come home after that.

109
00:07:10,600 --> 00:07:15,920
It was really exciting working in New York with Jan with Rob Fergus, another professor named

110
00:07:15,920 --> 00:07:21,760
Chris Breggler, and that really actually got me working in computer vision more.

111
00:07:21,760 --> 00:07:28,000
And then I came back in 2011, and my heart was really pulling me not just towards coming

112
00:07:28,000 --> 00:07:31,320
back to Canada, but also towards an academic position.

113
00:07:31,320 --> 00:07:36,040
So I had the opportunity to join the faculty at Guelph in 2012, and that's when I started.

114
00:07:36,040 --> 00:07:37,040
It's been five years.

115
00:07:37,040 --> 00:07:38,040
Nice, nice.

116
00:07:38,040 --> 00:07:40,760
And you mentioned a bunch of names there, but you didn't mention that your advisor for

117
00:07:40,760 --> 00:07:42,280
your PhD was Jeff Hinton.

118
00:07:42,280 --> 00:07:43,280
I should, yeah.

119
00:07:43,280 --> 00:07:47,600
I didn't mention a bunch of names, but I didn't credit my PhD advisor, I'm sorry, Jeff,

120
00:07:47,600 --> 00:07:48,600
I'm sorry.

121
00:07:48,600 --> 00:07:53,680
But I've talked to so many people who he's, you know, impacted via advising and other

122
00:07:53,680 --> 00:07:54,680
ways, so.

123
00:07:54,680 --> 00:07:55,680
That's right.

124
00:07:55,680 --> 00:08:00,680
I was co-advised by Jeff and another individual named Sam Royce, who was really influential

125
00:08:00,680 --> 00:08:01,960
in machine learning.

126
00:08:01,960 --> 00:08:08,520
He passed away actually in 2009, right when I started at NYU.

127
00:08:08,520 --> 00:08:15,280
It was tragic to lose him, but I certainly wanted to note his influence on my PhD as well.

128
00:08:15,280 --> 00:08:20,400
It was really amazing having one very senior advisor, Jeff, who really experienced having

129
00:08:20,400 --> 00:08:24,880
worked in the field, but also someone quite junior Sam had started as a faculty, I think,

130
00:08:24,880 --> 00:08:28,160
around 2005 or so.

131
00:08:28,160 --> 00:08:32,960
And he was full of energy and also helped me along the way.

132
00:08:32,960 --> 00:08:33,960
Awesome.

133
00:08:33,960 --> 00:08:34,960
Awesome.

134
00:08:34,960 --> 00:08:35,960
So you did a talk here this morning.

135
00:08:35,960 --> 00:08:38,560
Tell us a little bit about what your talk was about.

136
00:08:38,560 --> 00:08:39,560
Sure.

137
00:08:39,560 --> 00:08:42,240
So I broke my talk up into three parts.

138
00:08:42,240 --> 00:08:45,840
The first part was just introducing myself and telling people a little bit about the work

139
00:08:45,840 --> 00:08:50,280
that we do in Guelph and the types of machine learning problems we're interested in.

140
00:08:50,280 --> 00:08:55,600
The second part of the talk was focused on the challenges and also the opportunities

141
00:08:55,600 --> 00:09:01,600
in AI, and that was more of a technical discussion of what was coming around the corner.

142
00:09:01,600 --> 00:09:06,320
And then the third part of the talk was about some of the barriers, some startups might

143
00:09:06,320 --> 00:09:07,320
be facing.

144
00:09:07,320 --> 00:09:10,800
We're here at the George and Partners portfolio conference, so there's many startups in

145
00:09:10,800 --> 00:09:11,800
the audience.

146
00:09:11,800 --> 00:09:17,000
I've done a lot of work with startups, and I tried to focus on, again, some of the barriers

147
00:09:17,000 --> 00:09:19,640
they might face building companies.

148
00:09:19,640 --> 00:09:24,480
So why don't we start with kind of a rundown of the challenges and opportunities as you

149
00:09:24,480 --> 00:09:25,480
see them?

150
00:09:25,480 --> 00:09:26,480
Sure.

151
00:09:26,480 --> 00:09:32,480
So I started by talking about the technological changes coming towards us, and I think the

152
00:09:32,480 --> 00:09:39,000
first one that I started with was the move from what I would call largely predictive systems

153
00:09:39,000 --> 00:09:40,000
to creative systems.

154
00:09:40,000 --> 00:09:44,680
So when I say predictive systems, I mean these systems that were used to interacting

155
00:09:44,680 --> 00:09:49,040
with on a daily basis, the systems that might give us a temperature forecast tomorrow,

156
00:09:49,040 --> 00:09:53,200
or we might get up our mapping application and would tell us an estimated time to get

157
00:09:53,200 --> 00:09:58,440
it from A to B, or the people on the financial side might be interested in forecasting the

158
00:09:58,440 --> 00:10:01,800
price of a particular financial instrument the next day.

159
00:10:01,800 --> 00:10:06,000
But those types of inputs, they're either a category or they're a number, they're pretty

160
00:10:06,000 --> 00:10:09,480
low dimensional, and there's usually a single right answer.

161
00:10:09,480 --> 00:10:13,280
And so when I talk about the movement towards creative systems, I'm talking about systems

162
00:10:13,280 --> 00:10:18,880
that produce high dimensional output, and where there's no single right answer.

163
00:10:18,880 --> 00:10:25,080
So examples of this sort of more on the creative side would be art creation or poetry or music.

164
00:10:25,080 --> 00:10:31,440
And while these are some of the more culturally flavored activities, which gets some attention,

165
00:10:31,440 --> 00:10:38,080
there's also some real commercial applications such as automatic email reply or conversational

166
00:10:38,080 --> 00:10:44,920
dialogue systems, or I showed a proposed design for a robot that's creating meals and serving

167
00:10:44,920 --> 00:10:46,440
to them to you every day.

168
00:10:46,440 --> 00:10:49,560
And so I talked about some of the challenges in building those kinds of systems.

169
00:10:49,560 --> 00:10:51,360
I thought that one was pretty interesting.

170
00:10:51,360 --> 00:10:55,480
Folks that listen to the podcast will be pretty familiar with the idea of generative

171
00:10:55,480 --> 00:11:02,600
networks and style transfer and creating all the efforts we've seen to create movie

172
00:11:02,600 --> 00:11:05,920
scripts and poetry and all this kind of stuff.

173
00:11:05,920 --> 00:11:11,280
And so the art example has been the front and center for me for a while.

174
00:11:11,280 --> 00:11:18,400
But then when you kind of describe the recipe creation, that's a totally different domain

175
00:11:18,400 --> 00:11:22,680
than one that we hear about all the time, maybe because there are a bunch of different disciplines

176
00:11:22,680 --> 00:11:29,040
that need to come together for us to really explore, you know, or fulfill that, the Jetsons

177
00:11:29,040 --> 00:11:30,040
vision.

178
00:11:30,040 --> 00:11:31,040
Totally.

179
00:11:31,040 --> 00:11:32,040
Yes.

180
00:11:32,040 --> 00:11:37,240
But we're, you know, quickly moving towards an area where a lot of opportunities and value

181
00:11:37,240 --> 00:11:43,280
exist around generative, using neural networks specifically in AI in general for generative

182
00:11:43,280 --> 00:11:44,800
purposes.

183
00:11:44,800 --> 00:11:48,560
What are the key challenges there in your mind in that transition?

184
00:11:48,560 --> 00:11:53,640
I'm particularly interested in this as an engineer working in engineering school.

185
00:11:53,640 --> 00:12:00,360
I build things and I think we're seeing design migrate over from purely human design

186
00:12:00,360 --> 00:12:06,080
to at least at this the next few years being machines and humans working together on design

187
00:12:06,080 --> 00:12:11,760
and building objects, whether they be recipes or they be parts for vehicles or aircraft.

188
00:12:11,760 --> 00:12:18,400
So I think the major challenges in working towards a more algorithmic design would be what

189
00:12:18,400 --> 00:12:25,160
I pointed out this morning, namely the fact that there's no single right answer for design.

190
00:12:25,160 --> 00:12:31,800
You have potentially infinite number of solutions to a problem or designs that would be acceptable.

191
00:12:31,800 --> 00:12:37,400
And this makes it very hard to come up with reward signals or what we would call objectives

192
00:12:37,400 --> 00:12:41,800
for machine learning systems depending on how they're trained, right?

193
00:12:41,800 --> 00:12:48,960
So for us, when we think about a simple task, like image classification, image goes in,

194
00:12:48,960 --> 00:12:50,360
category comes out.

195
00:12:50,360 --> 00:12:53,800
We compare it with the ground truth category, there's usually a single right answer.

196
00:12:53,800 --> 00:12:58,560
For a system going back to this recipe example, how do you sort of, how do you measure the

197
00:12:58,560 --> 00:13:02,040
output of the system that cooks you a meal? I mean, you can, I guess, get some sort of

198
00:13:02,040 --> 00:13:06,760
subjective judgment of the person eating the meal, but it's not really calibrated with

199
00:13:06,760 --> 00:13:07,920
other people.

200
00:13:07,920 --> 00:13:11,240
And it's also just that single reward, these are the types of rewards maybe that are being

201
00:13:11,240 --> 00:13:14,920
used in reinforcement learning systems, they're very weak signals as well.

202
00:13:14,920 --> 00:13:21,280
So it'd be nice to maybe find some sort of medium between this like weak subjective reward

203
00:13:21,280 --> 00:13:26,080
or the explicit guidance that works for supervised learning systems.

204
00:13:26,080 --> 00:13:32,920
The other path for a long way in that, against that challenge, like that particular approach.

205
00:13:32,920 --> 00:13:38,320
Yeah, I think we've seen, as you've mentioned, the listeners of the podcast are going to

206
00:13:38,320 --> 00:13:43,400
be familiar with examples, particularly on the visual side of generative systems.

207
00:13:43,400 --> 00:13:46,920
That's kind of where we're stuck right now is evaluating generative systems, coming

208
00:13:46,920 --> 00:13:51,800
up with quantitative metrics, one to evaluate them, but also maybe as a way of feeding

209
00:13:51,800 --> 00:13:56,040
this kind of quantitative metric back into the system to make them better, right?

210
00:13:56,040 --> 00:13:58,840
How do we improve the objectives to train them?

211
00:13:58,840 --> 00:14:03,320
So then we've also seen a lot of progress really recently on the reinforcement learning

212
00:14:03,320 --> 00:14:09,440
reward side, but we aren't really anywhere, I think, on the sort of merger of those two

213
00:14:09,440 --> 00:14:10,440
systems.

214
00:14:10,440 --> 00:14:12,120
I think there's a lot more to be done.

215
00:14:12,120 --> 00:14:18,840
And particularly, we've seen a lot of nice examples in the generative space of language,

216
00:14:18,840 --> 00:14:23,480
so machine translation systems, conversational dialogue systems, but I think we're still

217
00:14:23,480 --> 00:14:28,880
stuck in coming up with the right kinds of metrics, so do you have, say, an English sentence

218
00:14:28,880 --> 00:14:35,560
going in, a French translation coming out, again, there's so many possible valid translations.

219
00:14:35,560 --> 00:14:43,400
We're still, in most cases, stuck at measuring a single, maybe I would say a canonical example

220
00:14:43,400 --> 00:14:47,640
given in some data set with the output of the system rather than really considering

221
00:14:47,640 --> 00:14:51,560
the space of the potential answers it could give.

222
00:14:51,560 --> 00:14:52,880
Okay.

223
00:14:52,880 --> 00:14:59,640
One of the examples that you use was inbox by Google, which I also use and you went a little

224
00:14:59,640 --> 00:15:00,640
further.

225
00:15:00,640 --> 00:15:05,920
You have a percentage in your mind of the time that you use that for responses, not quite

226
00:15:05,920 --> 00:15:09,560
there yet, but I do use the responses every once in a while.

227
00:15:09,560 --> 00:15:14,600
But you also talked about, you talked about a bunch of concepts, you know, transfer

228
00:15:14,600 --> 00:15:19,400
learning, you know, meta learning, feshut learning, and one of the questions that I had

229
00:15:19,400 --> 00:15:24,480
as you were kind of going through this was, you know, what are the, kind of the mechanisms

230
00:15:24,480 --> 00:15:31,880
and approaches for using, you know, in a large scale system, the feedback that you provide

231
00:15:31,880 --> 00:15:38,560
by selecting, you know, one of these responses in conjunction with the, like, the broader model

232
00:15:38,560 --> 00:15:40,760
that's trained for everybody.

233
00:15:40,760 --> 00:15:44,440
And what is that, what's that problem called, what are the approaches, like, how far are

234
00:15:44,440 --> 00:15:49,240
we along in, you know, developing a body of thinking around that?

235
00:15:49,240 --> 00:15:50,240
Right.

236
00:15:50,240 --> 00:15:55,040
So I think I was referring to this Google inbox client and I'm a big fan of it, and the

237
00:15:55,040 --> 00:15:56,040
user of it.

238
00:15:56,040 --> 00:16:01,440
And like you said, some percentage of the time, the auto email reply feature, it's what

239
00:16:01,440 --> 00:16:04,280
I would call a human and a loop system.

240
00:16:04,280 --> 00:16:08,360
And what I was saying earlier this morning is essentially, I wouldn't want to necessarily

241
00:16:08,360 --> 00:16:11,400
hand over all my email to an automatic reply system.

242
00:16:11,400 --> 00:16:12,400
Sure.

243
00:16:12,400 --> 00:16:16,240
AI is not at that stage where I could stop writing emails and people would just interface

244
00:16:16,240 --> 00:16:22,680
with me through this, this agent, but it's working at the level where it can propose several

245
00:16:22,680 --> 00:16:25,600
candidate replies, and I can still execute judgment over there.

246
00:16:25,600 --> 00:16:27,600
I can decide not to send the email at all.

247
00:16:27,600 --> 00:16:31,600
I can decide to not accept the proposals and write an email myself.

248
00:16:31,600 --> 00:16:36,040
I'm still completely in control, but it's making me more efficient when it, once in a

249
00:16:36,040 --> 00:16:39,440
while, proposes something that I can just click on and it will send.

250
00:16:39,440 --> 00:16:42,840
So I would say this is a system, it's a human and a loop system.

251
00:16:42,840 --> 00:16:47,560
It's where I maintain the judgment over what goes out, and I see this as an effective paradigm

252
00:16:47,560 --> 00:16:51,400
of humans and machines working together over the near term.

253
00:16:51,400 --> 00:17:00,280
But I also really like this idea of the transition from sort of full human control over a particular

254
00:17:00,280 --> 00:17:07,960
task all the way to fully automated performance, but this gray area in between, and a company

255
00:17:07,960 --> 00:17:12,960
that I co-founded in Toronto named Kindred, they're actually exploring this for robotics,

256
00:17:12,960 --> 00:17:18,880
where essentially the company is teaching robots to perform tasks that are very difficult

257
00:17:18,880 --> 00:17:24,000
to automate by allowing a human operator to control one or more robots.

258
00:17:24,000 --> 00:17:28,000
So the robot will be autonomous, but when it gets into trouble, it can be taken over

259
00:17:28,000 --> 00:17:32,560
by a remote operator who sort of gets it out of that, whatever it's stuck to, and the

260
00:17:32,560 --> 00:17:38,480
hope is again to, if this happens enough times, the robot learns about the way that the

261
00:17:38,480 --> 00:17:43,240
human assisted in getting out of that particularly difficult situation so that it becomes more

262
00:17:43,240 --> 00:17:44,880
and more autonomous.

263
00:17:44,880 --> 00:17:51,280
So again, it's not 0% or 100% automation, we're sort of exploring that gray area in between.

264
00:17:51,280 --> 00:17:53,560
So I really like this paradigm.

265
00:17:53,560 --> 00:17:58,600
Because you're using inbox, it's presenting you these possible emails that you might want

266
00:17:58,600 --> 00:18:03,240
to respond with, it gives you three, you choose one.

267
00:18:03,240 --> 00:18:09,800
That's potentially augmenting the set of training, label training data that the system has.

268
00:18:09,800 --> 00:18:16,280
And one way for Google in particular, or someone building a system like this in general,

269
00:18:16,280 --> 00:18:23,360
is to kind of throw that all in and continuously update the model and produce better models

270
00:18:23,360 --> 00:18:26,120
that are trained on more data.

271
00:18:26,120 --> 00:18:33,960
It strikes me that another way for a system like this to operate is that there's a general

272
00:18:33,960 --> 00:18:39,360
component of the model, but then there's a subcomponent of the model that's personalized

273
00:18:39,360 --> 00:18:42,560
to me and the way I respond.

274
00:18:42,560 --> 00:18:46,040
And the question is really, is anyone doing that?

275
00:18:46,040 --> 00:18:47,600
Does that have a name?

276
00:18:47,600 --> 00:18:49,600
Are there architectures for that?

277
00:18:49,600 --> 00:18:51,680
Have you ever come across that?

278
00:18:51,680 --> 00:18:54,400
Well, I would say this fits into the idea of personalization.

279
00:18:54,400 --> 00:18:59,360
And I think it's important for a product like inbox to have some element of personalization.

280
00:18:59,360 --> 00:19:04,040
A colleague actually told me that he doesn't use inbox because it makes him sound like

281
00:19:04,040 --> 00:19:07,280
a California dude.

282
00:19:07,280 --> 00:19:11,760
He said it puts exclamation marks on everything he says and uses terms like awesome exclamation

283
00:19:11,760 --> 00:19:14,680
mark, which he wouldn't say himself.

284
00:19:14,680 --> 00:19:15,840
So interesting.

285
00:19:15,840 --> 00:19:21,360
And he also claims that when I send him emails, he can tell when it's coming from the

286
00:19:21,360 --> 00:19:23,880
Google inbox, got a reply system.

287
00:19:23,880 --> 00:19:28,080
So again, what would fix something like this and maybe make him an adopter is a system

288
00:19:28,080 --> 00:19:31,160
that would adapt to his own style of writing emails.

289
00:19:31,160 --> 00:19:38,320
So it also ties back to the bias element of the conversation in a more subtle way than

290
00:19:38,320 --> 00:19:39,880
we sometimes think about it.

291
00:19:39,880 --> 00:19:40,880
Exactly.

292
00:19:40,880 --> 00:19:42,200
So why is it making him sound like a California dude?

293
00:19:42,200 --> 00:19:46,320
Well, maybe it was a bunch of emails from people in California, right?

294
00:19:46,320 --> 00:19:48,280
So it certainly ties back to that.

295
00:19:48,280 --> 00:19:53,840
I think in terms of how to talk about this and you even raised the idea of having a model

296
00:19:53,840 --> 00:19:58,840
that's been built from a whole lot of data, sort of a master model, and then personalize

297
00:19:58,840 --> 00:20:01,400
models for each of the people and sort of adapting.

298
00:20:01,400 --> 00:20:03,120
I think we do see it.

299
00:20:03,120 --> 00:20:04,920
It's an instance of transfer, right?

300
00:20:04,920 --> 00:20:09,360
So I was talking today about how do you deal with these problems where you have a very limited

301
00:20:09,360 --> 00:20:13,800
amount of labeled data, and I said, well, it's very popular right now to train a model

302
00:20:13,800 --> 00:20:19,360
in a big generic data set and then cut off the top of it and then replace that with something

303
00:20:19,360 --> 00:20:23,480
more specific and then train on a very much smaller set of data.

304
00:20:23,480 --> 00:20:27,640
So you can take something like generic object recognition, a big image net style data

305
00:20:27,640 --> 00:20:32,720
data set and then tackle a task like bird species classification, which is fine grain,

306
00:20:32,720 --> 00:20:34,400
but you have much less data.

307
00:20:34,400 --> 00:20:39,720
But that works because in the big system, there are birds in it, right?

308
00:20:39,720 --> 00:20:44,600
The data set image net has birds, so you can learn about feathers and wings and colors

309
00:20:44,600 --> 00:20:47,120
of birds and beaks and those sorts of features.

310
00:20:47,120 --> 00:20:50,840
That works when there's good match between the two different domains.

311
00:20:50,840 --> 00:20:54,400
And so this is what you're saying in terms of personalization, it can be viewed that way

312
00:20:54,400 --> 00:20:55,400
as well.

313
00:20:55,400 --> 00:20:58,680
You have a large data set of a whole bunch of different speakers and you can learn a

314
00:20:58,680 --> 00:21:05,480
model on that, but then you want to transfer or adapt this system to a particular individual

315
00:21:05,480 --> 00:21:08,040
where you have a smaller subset of data.

316
00:21:08,040 --> 00:21:12,840
And it makes sense, you wouldn't want to necessarily have a model for each individual person

317
00:21:12,840 --> 00:21:17,760
that work in isolation because there's probably not enough data there to generalize well.

318
00:21:17,760 --> 00:21:21,880
In that case, you want to capitalize from the all of the email that Google is holding

319
00:21:21,880 --> 00:21:25,120
in its service with people using Gmail.

320
00:21:25,120 --> 00:21:26,120
Okay.

321
00:21:26,120 --> 00:21:27,120
Okay.

322
00:21:27,120 --> 00:21:29,720
So additional challenges that you were describing.

323
00:21:29,720 --> 00:21:30,720
Oh, yeah.

324
00:21:30,720 --> 00:21:36,160
So I think we had gotten, we'd really only gotten across the sort of the two opportunities

325
00:21:36,160 --> 00:21:37,160
coming across.

326
00:21:37,160 --> 00:21:40,320
I could move into challenges or I could tell you about another couple of things that are

327
00:21:40,320 --> 00:21:42,520
coming across as sort of trends.

328
00:21:42,520 --> 00:21:44,760
So maybe I'll try to finish those off.

329
00:21:44,760 --> 00:21:51,000
The two trends that I hadn't mentioned yet, one was this idea of moving from careful human

330
00:21:51,000 --> 00:21:53,880
construction to learning to learn.

331
00:21:53,880 --> 00:21:57,240
So right now, like these, the systems are like the output of the hard work of graduate

332
00:21:57,240 --> 00:22:02,240
students and the faculty members advise in them and researchers and practitioners.

333
00:22:02,240 --> 00:22:08,200
I mentioned about the migration from feature engineering to architecture engineering, right?

334
00:22:08,200 --> 00:22:12,240
The way people describe deep learning often is that, oh, it's the end of feature engineering.

335
00:22:12,240 --> 00:22:16,600
We no longer have domain experts who craft very specific features.

336
00:22:16,600 --> 00:22:18,360
We can learn all the features with deep learning.

337
00:22:18,360 --> 00:22:19,360
Right.

338
00:22:19,360 --> 00:22:23,600
And I saw you had a picture of Stephen Merritti's article on your slide, which I've talked

339
00:22:23,600 --> 00:22:25,800
about on the podcast a while ago.

340
00:22:25,800 --> 00:22:26,800
Okay.

341
00:22:26,800 --> 00:22:27,800
Fantastic.

342
00:22:27,800 --> 00:22:31,360
So I love that blog post and it really, I think it's totally accurate.

343
00:22:31,360 --> 00:22:33,960
It moves into the world of architecture engineering.

344
00:22:33,960 --> 00:22:39,200
And so one way of getting out of this is essentially having these metal learning style algorithms.

345
00:22:39,200 --> 00:22:43,920
I mentioned a specific example in our lab where we're dealing with multimodal data.

346
00:22:43,920 --> 00:22:49,560
So in this case, we might have video and audio and we had motion capture coming in.

347
00:22:49,560 --> 00:22:53,120
And so we're figuring out how to actually merge those different modalities.

348
00:22:53,120 --> 00:22:57,240
I mean, the nice thing about deep learning models is with multimodal learning, you have

349
00:22:57,240 --> 00:23:03,440
so many opportunities of how to extract representations from the different modalities and how many

350
00:23:03,440 --> 00:23:06,560
levels of representations you should go for each of those.

351
00:23:06,560 --> 00:23:10,560
And when they should be merged together and which modalities should be merged, but there's

352
00:23:10,560 --> 00:23:12,560
all these decisions to be made.

353
00:23:12,560 --> 00:23:16,320
And so you can either have a grad student like we had who we just really skilled at figuring

354
00:23:16,320 --> 00:23:20,040
this all out and spends a year working towards a competition.

355
00:23:20,040 --> 00:23:23,800
But ultimately, we'd like to hand that over to an algorithm that figures that out.

356
00:23:23,800 --> 00:23:25,200
And that's what we've done.

357
00:23:25,200 --> 00:23:28,520
So that's one instance of learning in architecture.

358
00:23:28,520 --> 00:23:32,480
I know Google Brain has been working on this with reinforcement learning.

359
00:23:32,480 --> 00:23:34,520
They worked on learning optimizers.

360
00:23:34,520 --> 00:23:39,240
They're now working on, and other people are also working on learning activation functions.

361
00:23:39,240 --> 00:23:44,040
So really like it's, yeah, hand it over to the algorithm and this metal learning is really

362
00:23:44,040 --> 00:23:45,040
exciting.

363
00:23:45,040 --> 00:23:49,000
There's some great work that was done at Twitter for you go right now, show Google

364
00:23:49,000 --> 00:23:53,960
over to Google Brain on learning an algorithm that's just good at few shot or one shot

365
00:23:53,960 --> 00:23:54,960
learning.

366
00:23:54,960 --> 00:23:56,400
Also an instance of metal learning.

367
00:23:56,400 --> 00:23:59,560
So there's, it's an exciting area.

368
00:23:59,560 --> 00:24:07,320
I think after Steven's article came out, I spent a long time trying to, through my interviews,

369
00:24:07,320 --> 00:24:14,080
trying to understand the process of architecting deep neural networks.

370
00:24:14,080 --> 00:24:18,760
And I guess it took me longer to, you know, retrospectively, it took me longer than

371
00:24:18,760 --> 00:24:22,960
it should have to figure out, you know, the gradient descent by graduate student there

372
00:24:22,960 --> 00:24:28,040
with a couple of different versions of this, and graduate student descent.

373
00:24:28,040 --> 00:24:33,560
And you know, to, at least, you know, it seemed like a year ago or so, like that was the

374
00:24:33,560 --> 00:24:34,560
state of the art.

375
00:24:34,560 --> 00:24:42,360
But since then, you know, you described a number of methods for kind of automating architecture.

376
00:24:42,360 --> 00:24:46,040
One of the ones that you mentioned was a Bayesian based approach and there were some

377
00:24:46,040 --> 00:24:47,040
others.

378
00:24:47,040 --> 00:24:50,240
Can you go into a little bit more detail on the various ones that you mentioned?

379
00:24:50,240 --> 00:24:54,720
Sure, so we explored a couple of different approaches in, in my lab for the multimodal

380
00:24:54,720 --> 00:24:56,360
learning problem.

381
00:24:56,360 --> 00:25:01,000
One approach is Bayesian optimization, which a lot of people in the deep learning field

382
00:25:01,000 --> 00:25:06,760
are familiar with from the point of view is doing model search or hyper parameter optimization.

383
00:25:06,760 --> 00:25:10,120
So these are the decisions that we all need to make about how many layers and how many

384
00:25:10,120 --> 00:25:14,760
units per layer and what kind of activation function and then on the, the learning algorithm

385
00:25:14,760 --> 00:25:17,080
side, how long do we train for?

386
00:25:17,080 --> 00:25:22,440
Should we use atom optimizer or should we use RMS prop or what should our regularization

387
00:25:22,440 --> 00:25:23,440
coefficients be?

388
00:25:23,440 --> 00:25:26,240
There's all these decisions and, and with deep learning, there's more of these decisions

389
00:25:26,240 --> 00:25:28,920
in classical machine learning models.

390
00:25:28,920 --> 00:25:34,320
So people have proposed Bayesian optimization as a, a suitable tool and it, it's actually

391
00:25:34,320 --> 00:25:37,960
been very successful in automating some of the hyper parameter search.

392
00:25:37,960 --> 00:25:43,600
So we, in, in our first example, we just viewed architecture as another hyper parameter

393
00:25:43,600 --> 00:25:48,680
and we proposed essentially a, a search space of potential architectures in which this

394
00:25:48,680 --> 00:25:51,000
modality fusion could happen.

395
00:25:51,000 --> 00:25:55,840
And then we had a, Bayesian optimization algorithm with a, the main technical achievement

396
00:25:55,840 --> 00:26:02,320
was a, a, a, a, a, a, a, a way of assessing similarity between different architectures.

397
00:26:02,320 --> 00:26:06,160
And that was a building block for the Bayesian optimizer to basically search over that

398
00:26:06,160 --> 00:26:08,560
space of potential fusion architectures.

399
00:26:08,560 --> 00:26:09,560
Okay.

400
00:26:09,560 --> 00:26:13,520
And it came up with one that would beat the graduate student descent method.

401
00:26:13,520 --> 00:26:18,400
It in about 30 or so proposals, report different architectures.

402
00:26:18,400 --> 00:26:19,400
Okay.

403
00:26:19,400 --> 00:26:22,920
The downside to that system is when I'm saying 30 different architectures, each of those

404
00:26:22,920 --> 00:26:25,280
had to be trained and evaluated.

405
00:26:25,280 --> 00:26:29,560
And then that result given to the Bayesian optimizer such that it could propose the next

406
00:26:29,560 --> 00:26:30,560
one.

407
00:26:30,560 --> 00:26:34,840
So it's this iterative method in which you're training full architectures to convergence,

408
00:26:34,840 --> 00:26:39,120
you're evaluating them, you're choosing another one going back and evaluating.

409
00:26:39,120 --> 00:26:40,560
And so it gets quite slow.

410
00:26:40,560 --> 00:26:47,320
We explored a second approach in which we do, we view architecture searches to CASTIC

411
00:26:47,320 --> 00:26:48,320
regularization.

412
00:26:48,320 --> 00:26:52,640
It's kind of a meaty thing to say, but it's what you see in methods like dropout where

413
00:26:52,640 --> 00:26:55,560
people just knock out activities randomly.

414
00:26:55,560 --> 00:26:59,200
In neural networks, there's also drop connect where people knock out weights.

415
00:26:59,200 --> 00:27:02,160
And this is done on an example by example basis.

416
00:27:02,160 --> 00:27:06,880
So every time you present a new example to the model, you knock out a different subset

417
00:27:06,880 --> 00:27:10,800
of hidden activities where you knock out a different subset of weights.

418
00:27:10,800 --> 00:27:15,280
So they call this CASTIC regularization and it's been shown to make networks generalize

419
00:27:15,280 --> 00:27:16,280
better.

420
00:27:16,280 --> 00:27:19,440
And it was very popular until some things like batch norm came along and people started

421
00:27:19,440 --> 00:27:20,440
working with that.

422
00:27:20,440 --> 00:27:23,240
But still, it's a general principle for us.

423
00:27:23,240 --> 00:27:29,680
We did this kind of block wise, knocking out certain weights inspired by an approach by

424
00:27:29,680 --> 00:27:32,520
a graduate student at CMU called block out.

425
00:27:32,520 --> 00:27:35,840
And what this student found with this block out is that if you knock out certain blocks

426
00:27:35,840 --> 00:27:40,800
of weights, this can give you very different architectural patterns made through this

427
00:27:40,800 --> 00:27:41,800
weight structure.

428
00:27:41,800 --> 00:27:46,800
So you can have sort of mergers of groups of hidden units or splits or you can just completely

429
00:27:46,800 --> 00:27:51,000
ignore certain features that are being discovered in the network.

430
00:27:51,000 --> 00:27:55,800
And we basically propose a modality aware version of this.

431
00:27:55,800 --> 00:28:02,080
So as this training, explore many different multimodal fusion architectures and then eventually

432
00:28:02,080 --> 00:28:04,600
converge to one that worked pretty well.

433
00:28:04,600 --> 00:28:07,600
So that ended up being more efficient than the basic optimization approach.

434
00:28:07,600 --> 00:28:08,600
Okay.

435
00:28:08,600 --> 00:28:09,600
Okay.

436
00:28:09,600 --> 00:28:10,600
That's pretty technical.

437
00:28:10,600 --> 00:28:11,600
No, that's great.

438
00:28:11,600 --> 00:28:12,600
That's great.

439
00:28:12,600 --> 00:28:13,600
That's great.

440
00:28:13,600 --> 00:28:15,600
And then there was another challenge.

441
00:28:15,600 --> 00:28:16,600
Yes.

442
00:28:16,600 --> 00:28:20,600
So I talked about the idea, which is both an opportunity and a challenge of explainability

443
00:28:20,600 --> 00:28:21,600
in AI.

444
00:28:21,600 --> 00:28:22,600
Right?

445
00:28:22,600 --> 00:28:25,600
And I don't know if you've talked much about explainability yet.

446
00:28:25,600 --> 00:28:27,000
It's a little bit about it.

447
00:28:27,000 --> 00:28:32,000
I don't think you mentioned it in your talk, but I did an interview with Carlos Guestrin

448
00:28:32,000 --> 00:28:37,040
who has a paper called Lime, which seeks to do explainability.

449
00:28:37,040 --> 00:28:43,200
I appreciated the, you were quoting someone else, I believe, and you took issue with, you

450
00:28:43,200 --> 00:28:47,840
know, we often talk about neural networks as black boxes.

451
00:28:47,840 --> 00:28:52,000
And I think you, you, well, you can, yeah, sure, I can talk about that.

452
00:28:52,000 --> 00:28:58,400
It was actually a quote by Cuyengan Cho, a researcher at NYU, and he came to Toronto last

453
00:28:58,400 --> 00:29:02,360
summer very graciously to be part of this next AI program for startups.

454
00:29:02,360 --> 00:29:07,200
So part of that program, we bring in world-class individuals like Cho, and they talk about

455
00:29:07,200 --> 00:29:09,280
various things he was doing in course on NLP.

456
00:29:09,280 --> 00:29:13,720
But he criticized people calling neural networks black boxes and said that they're actually

457
00:29:13,720 --> 00:29:15,200
white boxes.

458
00:29:15,200 --> 00:29:20,640
And that was kind of neat because this, or after my talk, Nikola Paprano talked about

459
00:29:20,640 --> 00:29:23,520
black box versus white box attacks.

460
00:29:23,520 --> 00:29:24,920
And it's the same concept here.

461
00:29:24,920 --> 00:29:29,360
And in some sense, you can open them up, you can look at their parameters, you just happen

462
00:29:29,360 --> 00:29:35,400
to have hundreds of millions of parameters most of the time, and they're just uninterpretable,

463
00:29:35,400 --> 00:29:36,400
right?

464
00:29:36,400 --> 00:29:42,760
So they're not, I mean, if you're accessing them through an online service, like in Paprano's

465
00:29:42,760 --> 00:29:46,680
work, they were trying to attack a method that had been deployed, I think one was on

466
00:29:46,680 --> 00:29:49,960
Metamine services, one was on Amazon, one was on Google.

467
00:29:49,960 --> 00:29:53,520
And if you're interacting through the predictions, yes, it's black box.

468
00:29:53,520 --> 00:29:56,880
But if you're the person evaluating the machine learning system, or maybe you're

469
00:29:56,880 --> 00:29:59,640
your model, it's black box, right?

470
00:29:59,640 --> 00:30:02,080
And generally, still uninterpretable.

471
00:30:02,080 --> 00:30:03,400
It's still uninterpretable.

472
00:30:03,400 --> 00:30:08,720
So that's why we're keen to move to more interpretable systems or explainable systems

473
00:30:08,720 --> 00:30:10,200
in certain setups.

474
00:30:10,200 --> 00:30:15,880
So we've looked at it sort of in the medical space, we've looked at it in the financial

475
00:30:15,880 --> 00:30:20,000
prediction space forecasting, and then we've looked at sort of the classical vision problems

476
00:30:20,000 --> 00:30:24,680
on the benchmark data sets that everybody else benchmarks on.

477
00:30:24,680 --> 00:30:29,120
And yeah, I guess it's like when you teach about software and you're talking about requirements

478
00:30:29,120 --> 00:30:33,680
gathering, and how much money you're going to spend on each stage of the software development

479
00:30:33,680 --> 00:30:39,080
lifecycle, the same thing, assess the risks, like some problems require more careful consideration

480
00:30:39,080 --> 00:30:40,480
of risks, others don't.

481
00:30:40,480 --> 00:30:46,360
And I'd say that the same thing about interpretability and explainability, I mean, let's see what

482
00:30:46,360 --> 00:30:50,920
the application is, who are the users, what are the risks involved.

483
00:30:50,920 --> 00:30:55,880
And in many cases, we likely want to make the system more explainable.

484
00:30:55,880 --> 00:30:57,680
Okay, then opportunities.

485
00:30:57,680 --> 00:31:00,840
Yeah, we've arrived to opportunities.

486
00:31:00,840 --> 00:31:05,120
Yeah, actually, we've gone, sorry, we've gone through a lot of opportunities and I'll

487
00:31:05,120 --> 00:31:06,960
move more into sort of barriers.

488
00:31:06,960 --> 00:31:09,200
Let's say that the last part was some of the barriers.

489
00:31:09,200 --> 00:31:15,600
And when I talked about barriers, it was, I'll just go quickly through them, data.

490
00:31:15,600 --> 00:31:17,800
The next one was talent.

491
00:31:17,800 --> 00:31:21,120
And then the third one I talked about was building trust.

492
00:31:21,120 --> 00:31:24,800
So we can maybe go in and dissect each of these.

493
00:31:24,800 --> 00:31:30,600
And first of all, for data, I think in deep learning, we've seen a tremendous number

494
00:31:30,600 --> 00:31:34,960
of really cool examples of deep learning working in practice, but I would argue that it's

495
00:31:34,960 --> 00:31:37,680
been done in fairly limited set of domains.

496
00:31:37,680 --> 00:31:44,720
The ones I mentioned were the big three vision, speech and audio processing, and then natural

497
00:31:44,720 --> 00:31:45,720
language.

498
00:31:45,720 --> 00:31:46,720
Right.

499
00:31:46,720 --> 00:31:53,440
And these are generally unstructured domains where there's a lot of data, label data in particular.

500
00:31:53,440 --> 00:31:58,840
These are the sorts of applications being pursued by the commercial internet giant threat.

501
00:31:58,840 --> 00:32:03,400
And actually, it's something I've had a struggle with in my lab just motivating some students

502
00:32:03,400 --> 00:32:08,560
to tackle other kinds of problems where benchmark data sets are not available.

503
00:32:08,560 --> 00:32:13,160
So I actually mentioned today, for example, some agricultural applications that we've

504
00:32:13,160 --> 00:32:18,320
worked on, but again, you're rewarded more as a researcher to conduct your experiments

505
00:32:18,320 --> 00:32:23,600
reasonably quickly, get your papers out, and compare to other people in the literature.

506
00:32:23,600 --> 00:32:27,640
And you know, so you download ImageNet, you propose a new architecture, you publish paper

507
00:32:27,640 --> 00:32:28,640
on it.

508
00:32:28,640 --> 00:32:33,360
At the end of the day, if you want to solve a problem that actually, a really important

509
00:32:33,360 --> 00:32:40,280
problem like growing food in an environmentally friendly way and sustainable way, and that

510
00:32:40,280 --> 00:32:45,360
gives decent yields for the farmers, and you want to explore, say, deep learning for

511
00:32:45,360 --> 00:32:51,160
remote sensing in agricultural fields, this involves a crazy amount of data collection.

512
00:32:51,160 --> 00:32:55,680
It takes a lot of work to get in the field, do those flights, say UAV flights, do the

513
00:32:55,680 --> 00:33:00,160
ground truthing, which involves actually collecting samples, say you're looking at soil properties

514
00:33:00,160 --> 00:33:02,960
or nitrogen properties of plants.

515
00:33:02,960 --> 00:33:06,760
So this might take a summer, it might take multiple growing seasons, and you actually

516
00:33:06,760 --> 00:33:11,640
don't see the effects of any interactions until the end of the growing season when you

517
00:33:11,640 --> 00:33:13,720
actually can measure neat yields.

518
00:33:13,720 --> 00:33:20,600
So this is not the same time frame of a lot of the experimentation that happens in machine

519
00:33:20,600 --> 00:33:21,600
learning.

520
00:33:21,600 --> 00:33:26,240
So again, going back to the AlphaGo example, the system is able to play two and a half

521
00:33:26,240 --> 00:33:31,720
million games against itself because it can carry out a game and get the reward in less

522
00:33:31,720 --> 00:33:37,600
than a second, in an agricultural situation you can't get a reward in less than a second,

523
00:33:37,600 --> 00:33:40,720
it's six months, right, or longer.

524
00:33:40,720 --> 00:33:45,120
So anyways, this is a bit of a ramble just saying that we're fairly limited in the ways

525
00:33:45,120 --> 00:33:51,040
that we're applying deep learning right now, and it's a lot about the data, how do you

526
00:33:51,040 --> 00:33:56,680
collect the data, where do you get the data, how hard is it to gather that process it?

527
00:33:56,680 --> 00:34:00,480
And so anyways, there's a quote by Christix and then I gave it at the end, which was

528
00:34:00,480 --> 00:34:04,800
data is really the key ingredient to AI because it's the missing ingredient.

529
00:34:04,800 --> 00:34:09,560
So we publish our algorithms, like there's great algorithms out there, they're available

530
00:34:09,560 --> 00:34:11,080
to people.

531
00:34:11,080 --> 00:34:17,400
Compute power has really grown and it's become cheaper, so we have access to great compute.

532
00:34:17,400 --> 00:34:21,720
So it's really the data that we don't have, that's the missing ingredient for these sorts

533
00:34:21,720 --> 00:34:23,240
of problems I'm talking about.

534
00:34:23,240 --> 00:34:27,760
And it's also for companies, it's the proprietary ingredient.

535
00:34:27,760 --> 00:34:33,360
So people aren't publishing data sets as quickly as they're publishing papers on algorithms.

536
00:34:33,360 --> 00:34:36,600
What's one of the challenges that I see is data.

537
00:34:36,600 --> 00:34:42,160
Well, if you can maybe quickly summarize the other two and leave us with any final thoughts

538
00:34:42,160 --> 00:34:43,160
as we wrap up.

539
00:34:43,160 --> 00:34:44,160
Sure.

540
00:34:44,160 --> 00:34:50,320
So in terms of the other two, one is talent, and I think this is the idea of companies

541
00:34:50,320 --> 00:34:54,400
face with, well, how are we going to fill these positions where we need really skilled

542
00:34:54,400 --> 00:34:59,760
people in machine learning, and whether, you know, questions around, oh, do we need PhDs,

543
00:34:59,760 --> 00:35:01,640
our masters graduate, it's good enough.

544
00:35:01,640 --> 00:35:06,400
Can we take somebody training in a different area and move them into machine learning field?

545
00:35:06,400 --> 00:35:10,600
I think there's a lot of amazing stuff going on here, particularly in, not just Canada,

546
00:35:10,600 --> 00:35:11,600
but Toronto.

547
00:35:11,600 --> 00:35:15,880
There's an announcement last week by the provincial government to fund Vector Institute

548
00:35:15,880 --> 00:35:22,680
with $30 million to work with the Ontario universities to develop professional graduate

549
00:35:22,680 --> 00:35:25,560
programs and AI and machine learning.

550
00:35:25,560 --> 00:35:29,800
And in five years, we're going to be looking at graduating 1,000 students per year.

551
00:35:29,800 --> 00:35:33,760
That will be the goal for steady state in this province.

552
00:35:33,760 --> 00:35:36,880
I think we're addressing that issue with talent right now.

553
00:35:36,880 --> 00:35:42,360
But as I mentioned today, we also have a lot of professors leaving academics, going to

554
00:35:42,360 --> 00:35:48,400
work at industry part time or full time, and we need to work on retaining those individuals.

555
00:35:48,400 --> 00:35:53,800
I think decisions like starting the government supporting AI initiatives, like Vector and

556
00:35:53,800 --> 00:36:00,960
Meela and Amy and Alberta, those are all working toward making this, making academics attractive

557
00:36:00,960 --> 00:36:06,760
in this field versus industry, but I think we need to do more to encourage the people

558
00:36:06,760 --> 00:36:12,440
staying in academics or moving into academics to continue to train the next generation.

559
00:36:12,440 --> 00:36:15,240
And then the final topic was on trust, right, building trust.

560
00:36:15,240 --> 00:36:18,760
And actually, we've already touched on a couple of those issues.

561
00:36:18,760 --> 00:36:21,320
Explain to you as one, bias and fairness.

562
00:36:21,320 --> 00:36:27,600
I tend to like the idea of using technology as actually as Nikola Papernos said in his

563
00:36:27,600 --> 00:36:33,360
talk following mine, these ideas are on differential, privacy preserving algorithms to increase

564
00:36:33,360 --> 00:36:38,880
people's trust in machine learning systems, rolling out technology like fair representations

565
00:36:38,880 --> 00:36:43,160
for removing bias from algorithms that make predictions.

566
00:36:43,160 --> 00:36:47,840
And so I think I feel pretty good about the future of AI, I guess I'll summarize it

567
00:36:47,840 --> 00:36:48,840
that.

568
00:36:48,840 --> 00:36:50,640
It's a great field to be working in.

569
00:36:50,640 --> 00:36:54,320
This Toronto area is a great place to be working on these technologies.

570
00:36:54,320 --> 00:36:56,360
There's a lot more to come.

571
00:36:56,360 --> 00:37:01,440
And I think in terms of the problems, I do see a diversification in the future of the types

572
00:37:01,440 --> 00:37:03,760
of tasks we're solving.

573
00:37:03,760 --> 00:37:07,600
And I think they're at least working with the startups in the next AI program.

574
00:37:07,600 --> 00:37:12,080
I also see a lot of interest, both from the companies building these technologies, but

575
00:37:12,080 --> 00:37:17,560
also from the investment side, and companies that are doing social good as well.

576
00:37:17,560 --> 00:37:22,720
So building both profitable companies, but also solving real important issues.

577
00:37:22,720 --> 00:37:23,720
Right.

578
00:37:23,720 --> 00:37:24,920
And so that's why I look forward to.

579
00:37:24,920 --> 00:37:25,920
Awesome.

580
00:37:25,920 --> 00:37:30,240
Well, Graham, thanks so much for taking the time to sit with me and share all that you

581
00:37:30,240 --> 00:37:33,720
shared about your kind of vision for this and how you see it.

582
00:37:33,720 --> 00:37:34,720
Thanks a lot to my pleasure.

583
00:37:34,720 --> 00:37:35,720
Great to meet you.

584
00:37:35,720 --> 00:37:36,720
Great.

585
00:37:36,720 --> 00:37:37,720
Thank you.

586
00:37:37,720 --> 00:37:43,120
All right, everyone, that's our show for today.

587
00:37:43,120 --> 00:37:47,840
Thanks so much for listening and for your continued feedback and support.

588
00:37:47,840 --> 00:37:52,720
For more information on Graham or any of the topics covered in this episode, head on over

589
00:37:52,720 --> 00:37:56,920
to twimlai.com slash talk slash 62.

590
00:37:56,920 --> 00:38:04,800
To follow along with the Georgian partner series, visit twimlai.com slash GPPC 2017.

591
00:38:04,800 --> 00:38:10,600
Of course, you can send along feedback or questions via Twitter, at twimlai, or at Sam

592
00:38:10,600 --> 00:38:14,600
Charrington, or leave a comment on the show notes page.

593
00:38:14,600 --> 00:38:18,080
Thanks once again to Georgian partners for their sponsorship of the show.

594
00:38:18,080 --> 00:38:23,160
Be sure to check out their white papers, which you can find by visiting twimlai.com slash

595
00:38:23,160 --> 00:38:24,160
Georgian.

596
00:38:24,160 --> 00:38:35,640
Thanks again for listening and catch you next time.

