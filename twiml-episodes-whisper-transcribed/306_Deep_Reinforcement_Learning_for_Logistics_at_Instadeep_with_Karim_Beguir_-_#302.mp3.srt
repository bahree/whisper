1
00:00:00,000 --> 00:00:04,960
We are just one week out from the premiere event focused on accelerating,

2
00:00:04,960 --> 00:00:09,000
automating, and scaling, machine learning, and AI in the enterprise.

3
00:00:09,000 --> 00:00:12,000
That's right, Twomokon AI Platforms.

4
00:00:13,000 --> 00:00:17,500
Over the past few weeks, we've shared a ton of details about the conference,

5
00:00:17,500 --> 00:00:22,000
including our live keynote interviews where I'll be onstage with Andruing

6
00:00:22,000 --> 00:00:25,000
and leaders from Uber, LinkedIn, and Cruise.

7
00:00:25,000 --> 00:00:30,000
Our Super Size 2 Track Agenda, complete with speakers from Levi Strauss, Zappos,

8
00:00:30,000 --> 00:00:33,000
Capital One, Airbnb, and many more.

9
00:00:33,000 --> 00:00:36,000
Our community experiences like our Day 2 Unconference,

10
00:00:36,000 --> 00:00:41,000
where attendees will propose and vote on topics to further explore in small group sessions.

11
00:00:41,000 --> 00:00:45,000
And the Twomokon Happy Hour, complete with food and drinks,

12
00:00:45,000 --> 00:00:50,000
interview, and photo booths, and AWS Deep Racer Contest, and much more.

13
00:00:50,000 --> 00:00:54,000
It's not too late to get your tickets, but you'll need to act fast

14
00:00:54,000 --> 00:00:56,000
if you don't want to miss out.

15
00:00:56,000 --> 00:01:00,000
Head over to twomokon.com now, and secure your ticket for what's shaping up

16
00:01:00,000 --> 00:01:02,000
to be an amazing event.

17
00:01:02,000 --> 00:01:05,000
And now, on to the show.

18
00:01:11,000 --> 00:01:12,000
All right, everyone.

19
00:01:12,000 --> 00:01:14,000
I am on the line with Kareem Baghear.

20
00:01:14,000 --> 00:01:18,000
Kareem is the co-founder and CEO of InstaDeep.

21
00:01:18,000 --> 00:01:21,000
Kareem, welcome to this week in machine learning and AI.

22
00:01:21,000 --> 00:01:24,000
Hi, Sam. Thank you for having me.

23
00:01:24,000 --> 00:01:27,000
You are very welcome to be on the show,

24
00:01:27,000 --> 00:01:30,000
and I'm looking forward to diving into the conversation.

25
00:01:30,000 --> 00:01:33,000
But before we get into our main topic,

26
00:01:33,000 --> 00:01:38,000
which will be focused on AI innovation in the field of logistics,

27
00:01:38,000 --> 00:01:40,000
I love to learn more about your background.

28
00:01:40,000 --> 00:01:44,000
You are in a, can we call it a unique position

29
00:01:44,000 --> 00:01:50,000
of having co-founded a deep learning company in Tunisia, North Africa.

30
00:01:50,000 --> 00:01:53,000
How did you get to that point?

31
00:01:53,000 --> 00:01:58,000
Yeah, actually, I grew up in southern Tunisia in a city

32
00:01:58,000 --> 00:02:00,000
which is very familiar to Star Wars lovers,

33
00:02:00,000 --> 00:02:02,000
because it's actually Tatooine.

34
00:02:02,000 --> 00:02:05,000
So it's the same name that George Lucas used

35
00:02:05,000 --> 00:02:09,000
for the first movie about Star Wars.

36
00:02:09,000 --> 00:02:11,000
So grew up in southern Tunisia,

37
00:02:11,000 --> 00:02:14,000
and my love of applied mathematics took me then

38
00:02:14,000 --> 00:02:17,000
to graduate education in France,

39
00:02:17,000 --> 00:02:20,000
called Polytechnic, and ultimately to the US

40
00:02:20,000 --> 00:02:24,000
at Courant Institute at NYU in the US.

41
00:02:24,000 --> 00:02:26,000
And, you know, in study,

42
00:02:26,000 --> 00:02:31,000
is a project that, you know, I co-founded with Zora

43
00:02:31,000 --> 00:02:35,000
that I met in Tunisia with the goal of, you know,

44
00:02:35,000 --> 00:02:39,000
basically proving that we can do, you know, advanced AI

45
00:02:39,000 --> 00:02:41,000
and deep learning in Africa.

46
00:02:41,000 --> 00:02:43,000
And it's been a crazy adventure for us.

47
00:02:43,000 --> 00:02:46,000
We started really with like two laptops and no funding.

48
00:02:46,000 --> 00:02:48,000
It was a pure bootstrap.

49
00:02:48,000 --> 00:02:50,000
And we're very excited to, you know,

50
00:02:50,000 --> 00:02:52,000
how much we've come since then.

51
00:02:52,000 --> 00:02:56,000
Awesome. And the company to this day has most of its employees

52
00:02:56,000 --> 00:02:58,000
in Africa, all in Tunisia,

53
00:02:58,000 --> 00:03:01,000
or are there other locations?

54
00:03:01,000 --> 00:03:05,000
So, yes, majority of our employees are in Africa,

55
00:03:05,000 --> 00:03:07,000
something like 70% of the company.

56
00:03:07,000 --> 00:03:10,000
And after the original Tunis office,

57
00:03:10,000 --> 00:03:14,000
we also have opened up setup shop in Lagos,

58
00:03:14,000 --> 00:03:17,000
in Nigeria, as well as Nairobi in Kenya,

59
00:03:17,000 --> 00:03:21,000
and looking forward to opening an office soon in South Africa.

60
00:03:21,000 --> 00:03:24,000
So, it's been a very, very interesting ride

61
00:03:24,000 --> 00:03:26,000
in terms of like discovering the potential

62
00:03:26,000 --> 00:03:28,000
of African talent.

63
00:03:28,000 --> 00:03:31,000
And we also have two offices in London and Paris as well.

64
00:03:31,000 --> 00:03:32,000
Okay.

65
00:03:32,000 --> 00:03:35,000
And so, what's the company's focus?

66
00:03:35,000 --> 00:03:39,000
So, the company is focused on basically building

67
00:03:39,000 --> 00:03:43,000
advanced decision-making systems for the enterprise.

68
00:03:43,000 --> 00:03:45,000
So, if you look at, you know,

69
00:03:45,000 --> 00:03:47,000
deep learning and uses of AI,

70
00:03:47,000 --> 00:03:49,000
historically, you know, most companies

71
00:03:49,000 --> 00:03:52,000
and most startups focus on pattern detection.

72
00:03:52,000 --> 00:03:56,000
So, advanced uses of either machine learning or deep learning.

73
00:03:56,000 --> 00:04:00,000
But at Instady, we actually believe that, you know,

74
00:04:00,000 --> 00:04:02,000
technology is around reinforcement learning

75
00:04:02,000 --> 00:04:05,000
and the like are coming now to a state of fusion

76
00:04:05,000 --> 00:04:09,000
that make them actually deployable for the enterprise.

77
00:04:09,000 --> 00:04:12,000
And we anticipate significant savings.

78
00:04:12,000 --> 00:04:15,000
So, we are focusing on helping large companies

79
00:04:15,000 --> 00:04:18,000
realize the promise of AI, which is, of course,

80
00:04:18,000 --> 00:04:20,000
easier said than done.

81
00:04:20,000 --> 00:04:23,000
So, our role is to help them, you know,

82
00:04:23,000 --> 00:04:25,000
seize the opportunity in AI,

83
00:04:25,000 --> 00:04:28,000
while at the same time, you know, as discussed,

84
00:04:28,000 --> 00:04:30,000
building up a new generation of talent

85
00:04:30,000 --> 00:04:32,000
and making people believe that, you know,

86
00:04:32,000 --> 00:04:33,000
actually their dreams can come true,

87
00:04:33,000 --> 00:04:35,000
they can be part of this.

88
00:04:35,000 --> 00:04:36,000
Nice.

89
00:04:36,000 --> 00:04:39,000
Nice. You mentioned reinforcement learning.

90
00:04:39,000 --> 00:04:42,000
Yeah, that comes up on the show quite a bit.

91
00:04:42,000 --> 00:04:47,000
And there are very, it can be a polarizing topic

92
00:04:47,000 --> 00:04:50,000
in the sense that there are folks that I talk to

93
00:04:50,000 --> 00:04:53,000
that are very excited about it.

94
00:04:53,000 --> 00:04:55,000
And the promise that it offers not just

95
00:04:55,000 --> 00:05:00,000
in these traditional RL applications like game playing,

96
00:05:00,000 --> 00:05:03,000
but for enterprise applications.

97
00:05:03,000 --> 00:05:05,000
And then there are other folks that say,

98
00:05:05,000 --> 00:05:07,000
it's way too complicated.

99
00:05:07,000 --> 00:05:11,000
We're nowhere near being able to put it to good use.

100
00:05:11,000 --> 00:05:14,000
It sounds like you're in this ladder camp.

101
00:05:14,000 --> 00:05:16,000
So, yeah, we believe, actually,

102
00:05:16,000 --> 00:05:18,000
that the technology is sufficiently mature now

103
00:05:18,000 --> 00:05:21,000
to get to practical applications.

104
00:05:21,000 --> 00:05:24,000
I mean, if you see historically, you know,

105
00:05:24,000 --> 00:05:26,000
DeepMind can start to become, like,

106
00:05:26,000 --> 00:05:30,000
a deep RL started to become something impressive

107
00:05:30,000 --> 00:05:33,000
with originally DeepMind and the progress they did

108
00:05:33,000 --> 00:05:35,000
with DQN on Atari Games and the like.

109
00:05:35,000 --> 00:05:38,000
But now with progress on multiple fronts,

110
00:05:38,000 --> 00:05:41,000
it's actually possible to deploy these technologies

111
00:05:41,000 --> 00:05:42,000
in real life.

112
00:05:42,000 --> 00:05:45,000
And, you know, we expect this to become

113
00:05:45,000 --> 00:05:47,000
more and more mainstream.

114
00:05:47,000 --> 00:05:51,000
So, you know, we actually doing work in multiple sectors,

115
00:05:51,000 --> 00:05:54,000
but in particular in logistics that confirms

116
00:05:54,000 --> 00:05:57,000
that these technologies can actually be deployed

117
00:05:57,000 --> 00:05:59,000
and can actually generate savings

118
00:05:59,000 --> 00:06:02,000
versus more classical technologies.

119
00:06:02,000 --> 00:06:05,000
Okay, and so we've referenced a couple of times

120
00:06:05,000 --> 00:06:07,000
the field of logistics,

121
00:06:07,000 --> 00:06:10,000
the connection being a presentation that you delivered

122
00:06:10,000 --> 00:06:14,000
at the recent Nvidia GTC conference

123
00:06:14,000 --> 00:06:17,000
on some of the work that you've seen

124
00:06:17,000 --> 00:06:20,000
and that you're doing on AI and logistics.

125
00:06:20,000 --> 00:06:24,000
Can you maybe frame up the problem for us?

126
00:06:24,000 --> 00:06:27,000
What are some of the specific challenges

127
00:06:27,000 --> 00:06:30,000
that enterprises are seeing in logistics

128
00:06:30,000 --> 00:06:34,000
and what are the gaps between, you know,

129
00:06:34,000 --> 00:06:36,000
logistics obviously has been a problem

130
00:06:36,000 --> 00:06:39,000
for many, many, many hundreds of years.

131
00:06:39,000 --> 00:06:42,000
And we've developed technology-based solutions

132
00:06:42,000 --> 00:06:45,000
to try to solve these problems.

133
00:06:45,000 --> 00:06:47,000
What are the gaps that are leading folks

134
00:06:47,000 --> 00:06:50,000
to look at deep learning

135
00:06:50,000 --> 00:06:54,000
and machine learning-based approaches?

136
00:06:54,000 --> 00:06:56,000
So, you know, on logistics,

137
00:06:56,000 --> 00:06:58,000
you have multiple problems

138
00:06:58,000 --> 00:07:02,000
that actually require making decisions

139
00:07:02,000 --> 00:07:04,000
in a complex environment

140
00:07:04,000 --> 00:07:07,000
where you have a very large number of choices.

141
00:07:07,000 --> 00:07:09,000
So, just a specific example,

142
00:07:09,000 --> 00:07:11,000
if you look at a right-sharing, for example,

143
00:07:11,000 --> 00:07:14,000
for a mobility company such as Uber or others,

144
00:07:14,000 --> 00:07:16,000
effectively you have a situation

145
00:07:16,000 --> 00:07:20,000
where you have hundreds of cabs in like a big city

146
00:07:20,000 --> 00:07:23,000
and at the same time maybe, you know,

147
00:07:23,000 --> 00:07:26,000
hundreds or thousands of riders requesting a ride

148
00:07:26,000 --> 00:07:28,000
and with a right-sharing allowed,

149
00:07:28,000 --> 00:07:30,000
basically you can onboard, let's say,

150
00:07:30,000 --> 00:07:32,000
three to four people in a given cab.

151
00:07:32,000 --> 00:07:34,000
This is a complex decision to make.

152
00:07:34,000 --> 00:07:38,000
Which cab should cater to which requests?

153
00:07:38,000 --> 00:07:41,000
It's not an easy decision.

154
00:07:41,000 --> 00:07:43,000
And this is typical of what we believe

155
00:07:43,000 --> 00:07:45,000
is the opportunity for, you know,

156
00:07:45,000 --> 00:07:48,000
deep reinforcement learning-based systems.

157
00:07:48,000 --> 00:07:50,000
So, you effectively have to look for a needle

158
00:07:50,000 --> 00:07:52,000
in a hay sack and, you know,

159
00:07:52,000 --> 00:07:54,000
one of the ways to do that is actually

160
00:07:54,000 --> 00:07:56,000
to train a modern AI system

161
00:07:56,000 --> 00:07:58,000
to look for good solutions.

162
00:07:58,000 --> 00:08:01,000
So, it really comes down to,

163
00:08:01,000 --> 00:08:03,000
you have a very large search space,

164
00:08:03,000 --> 00:08:06,000
sometimes bigger than the number of atoms in the universe.

165
00:08:06,000 --> 00:08:08,000
And you need to find a clever way

166
00:08:08,000 --> 00:08:10,000
to go through this search space.

167
00:08:10,000 --> 00:08:12,000
So, I'll tell you historically what enterprises

168
00:08:12,000 --> 00:08:15,000
have been doing to solve this problem,

169
00:08:15,000 --> 00:08:18,000
is, you know, if you look at operations research,

170
00:08:18,000 --> 00:08:22,000
OR, typically all the algorithms

171
00:08:22,000 --> 00:08:24,000
that are used in OR are clever ways

172
00:08:24,000 --> 00:08:27,000
to make that search space simpler

173
00:08:27,000 --> 00:08:30,000
and they are more amenable to, you know, algorithms.

174
00:08:30,000 --> 00:08:32,000
So, that is, you know,

175
00:08:32,000 --> 00:08:34,000
that works pretty well in practice,

176
00:08:34,000 --> 00:08:36,000
but it's far from being optimal.

177
00:08:36,000 --> 00:08:38,000
And typically, we're talking about problems

178
00:08:38,000 --> 00:08:41,000
that, you know, have a very large search space

179
00:08:41,000 --> 00:08:42,000
and can be NP-hard,

180
00:08:42,000 --> 00:08:45,000
so it means that there is not an easy solution

181
00:08:45,000 --> 00:08:47,000
to solve them.

182
00:08:47,000 --> 00:08:51,000
So, by making the search space smaller,

183
00:08:51,000 --> 00:08:53,000
you know, OR algorithms effectively

184
00:08:53,000 --> 00:08:55,000
are introducing biases.

185
00:08:55,000 --> 00:08:58,000
So, you know, they're built on heuristics,

186
00:08:58,000 --> 00:09:00,000
and those heuristics mean that a system

187
00:09:00,000 --> 00:09:02,000
that could learn from scratch

188
00:09:02,000 --> 00:09:08,000
could go beyond what is the state of the art at the moment.

189
00:09:08,000 --> 00:09:12,000
So, it's very similar to what happened in the game of chess

190
00:09:12,000 --> 00:09:14,000
before Alpha 0 came,

191
00:09:14,000 --> 00:09:16,000
so Alpha 0, the groundbreaking algorithm

192
00:09:16,000 --> 00:09:20,000
that DeepMind developed in late 2017,

193
00:09:20,000 --> 00:09:22,000
basically before Alpha 0,

194
00:09:22,000 --> 00:09:25,000
the best chess player was a system

195
00:09:25,000 --> 00:09:29,000
that had to, you know, to use lots of rules,

196
00:09:29,000 --> 00:09:31,000
whether coming from the programming side of things

197
00:09:31,000 --> 00:09:33,000
or coming from the chess side of things,

198
00:09:33,000 --> 00:09:35,000
but like there were hundreds of rules

199
00:09:35,000 --> 00:09:38,000
that were pretty much embedded into the program

200
00:09:38,000 --> 00:09:41,000
to make it do, you know, to make it competitive.

201
00:09:41,000 --> 00:09:44,000
Yet, Alpha 0 came and replaced all of that

202
00:09:44,000 --> 00:09:46,000
with just two simple concepts,

203
00:09:46,000 --> 00:09:48,000
searching and learning,

204
00:09:48,000 --> 00:09:50,000
and it was able to outperform, you know,

205
00:09:50,000 --> 00:09:53,000
stockfish this particular chess program

206
00:09:53,000 --> 00:09:55,000
in, you know, as little as four hours

207
00:09:55,000 --> 00:09:57,000
if you use distributed machine learning.

208
00:09:57,000 --> 00:09:59,000
So, I was watching that.

209
00:09:59,000 --> 00:10:01,000
I was actually at Nuribs for the first time

210
00:10:01,000 --> 00:10:04,000
when David Silver presented these groundbreaking results,

211
00:10:04,000 --> 00:10:06,000
and I was like, wow,

212
00:10:06,000 --> 00:10:08,000
this is actually very exciting,

213
00:10:08,000 --> 00:10:11,000
and it has applications that are beyond games.

214
00:10:11,000 --> 00:10:13,000
Like, think about, you know, as I said,

215
00:10:13,000 --> 00:10:15,000
the right-sharing example I gave you

216
00:10:15,000 --> 00:10:18,000
or, for example, you know, how you fit objects

217
00:10:18,000 --> 00:10:20,000
or packages in containers.

218
00:10:20,000 --> 00:10:22,000
All of those have this property

219
00:10:22,000 --> 00:10:25,000
that, you know, you need to look for the right solution

220
00:10:25,000 --> 00:10:28,000
in a very large amount of, you know,

221
00:10:28,000 --> 00:10:29,000
a very large space.

222
00:10:29,000 --> 00:10:31,000
And so, all of those are potentially

223
00:10:31,000 --> 00:10:33,000
amenable to this kind of solutions.

224
00:10:33,000 --> 00:10:34,000
And, you know, since then,

225
00:10:34,000 --> 00:10:35,000
we've been working very hard,

226
00:10:35,000 --> 00:10:38,000
I think, to make this promise a reality for our customers.

227
00:10:38,000 --> 00:10:41,000
When you're approaching a problem like

228
00:10:41,000 --> 00:10:43,000
the right-sharing problem,

229
00:10:43,000 --> 00:10:46,000
how do you transform, you know,

230
00:10:46,000 --> 00:10:48,000
this problem that, you know,

231
00:10:48,000 --> 00:10:49,000
kind of makes sense when you explain it

232
00:10:49,000 --> 00:10:53,000
into something that you can apply directly

233
00:10:53,000 --> 00:10:56,000
deep learning or RL2.

234
00:10:56,000 --> 00:11:00,000
So, effectively, you need to build up

235
00:11:00,000 --> 00:11:02,000
a model of the environment,

236
00:11:02,000 --> 00:11:04,000
first, which comes from, you know,

237
00:11:04,000 --> 00:11:05,000
the real world.

238
00:11:05,000 --> 00:11:09,000
So, for example, what we use at InstaDeep

239
00:11:09,000 --> 00:11:10,000
is open-street maps.

240
00:11:10,000 --> 00:11:13,000
So, we take, actually, the exact, you know,

241
00:11:13,000 --> 00:11:15,000
information on traffic,

242
00:11:15,000 --> 00:11:18,000
on the topology of the city from open-street map.

243
00:11:18,000 --> 00:11:21,000
And we transform the problem of,

244
00:11:21,000 --> 00:11:25,000
you know, going from point A to point B

245
00:11:25,000 --> 00:11:27,000
and selecting different, you know,

246
00:11:27,000 --> 00:11:32,000
passengers into a set of possible actions.

247
00:11:32,000 --> 00:11:34,000
And basically, you know,

248
00:11:34,000 --> 00:11:35,000
build it into an RL framework.

249
00:11:35,000 --> 00:11:37,000
That's exactly the same method

250
00:11:37,000 --> 00:11:38,000
that we would use, for example,

251
00:11:38,000 --> 00:11:40,000
on a problem like dinpacking.

252
00:11:40,000 --> 00:11:42,000
You have a certain volume.

253
00:11:42,000 --> 00:11:45,000
You have certain constants that you need to respect.

254
00:11:45,000 --> 00:11:48,000
And your goal is to find optimal configurations

255
00:11:48,000 --> 00:11:50,000
such that, you know,

256
00:11:50,000 --> 00:11:52,000
the total volume that you're using

257
00:11:52,000 --> 00:11:53,000
is the smallest possible,

258
00:11:53,000 --> 00:11:56,000
while respecting all the other constraints.

259
00:11:56,000 --> 00:11:59,000
So, basically, there is a significant work

260
00:11:59,000 --> 00:12:01,000
which is done on the environment.

261
00:12:01,000 --> 00:12:04,000
But the general principle remains the same.

262
00:12:04,000 --> 00:12:05,000
At the end of the day,

263
00:12:05,000 --> 00:12:08,000
you have, let's say,

264
00:12:08,000 --> 00:12:11,000
a reward function that comes out of this.

265
00:12:11,000 --> 00:12:13,000
Usually, on NP hard problems in logistics,

266
00:12:13,000 --> 00:12:15,000
you know, it's very easy to verify

267
00:12:15,000 --> 00:12:19,000
if a given solution is a good one or not.

268
00:12:19,000 --> 00:12:23,000
What is not so easy is to find what is a good solution.

269
00:12:23,000 --> 00:12:26,000
So, the problem of like scoring the result

270
00:12:26,000 --> 00:12:29,000
is usually not extremely difficult.

271
00:12:29,000 --> 00:12:31,000
You just need to build up the environment

272
00:12:31,000 --> 00:12:33,000
to make, you know, this possible

273
00:12:33,000 --> 00:12:36,000
and then find ways to train this escape.

274
00:12:36,000 --> 00:12:40,000
So, sticking with this right-sharing example

275
00:12:40,000 --> 00:12:43,000
is the assumption that for a given ride,

276
00:12:43,000 --> 00:12:46,000
you kind of start with a starting place

277
00:12:46,000 --> 00:12:50,000
and a destination may be determined by an initial rider

278
00:12:50,000 --> 00:12:53,000
and you're trying to figure out what other riders to pick up.

279
00:12:53,000 --> 00:12:55,000
Yes. Basically, you have to make decisions

280
00:12:55,000 --> 00:12:59,000
about which riders to take into the car.

281
00:12:59,000 --> 00:13:02,000
And, you know, the car can have two states

282
00:13:02,000 --> 00:13:05,000
like it could either have, you know, already people

283
00:13:05,000 --> 00:13:09,000
in the car and given destination where to go

284
00:13:09,000 --> 00:13:11,000
or it could be completely empty.

285
00:13:11,000 --> 00:13:15,000
So, you have to make a decision about who you want to onboard.

286
00:13:15,000 --> 00:13:18,000
And obviously, if you onboard a given person,

287
00:13:18,000 --> 00:13:21,000
a given client, then you have to take that person to destination.

288
00:13:21,000 --> 00:13:25,000
So, and you do this, you know, progressively, you know,

289
00:13:25,000 --> 00:13:28,000
given the capacity that you have left.

290
00:13:28,000 --> 00:13:30,000
In a situation like ride-sharing,

291
00:13:30,000 --> 00:13:33,000
a typical metric of, you know,

292
00:13:33,000 --> 00:13:36,000
that you're trying to optimize for

293
00:13:36,000 --> 00:13:40,000
is the total number, like the total delay

294
00:13:40,000 --> 00:13:43,000
for the passengers you're onboarding,

295
00:13:43,000 --> 00:13:45,000
meaning that, you know, for example,

296
00:13:45,000 --> 00:13:49,000
if an open street map tells you that you could deliver that person

297
00:13:49,000 --> 00:13:51,000
in five minutes,

298
00:13:51,000 --> 00:13:54,000
if you deliver that person in seven minutes,

299
00:13:54,000 --> 00:13:57,000
then it's a two-minute delay.

300
00:13:57,000 --> 00:14:01,000
And your goal is to, of course, optimize the total amount of delays

301
00:14:01,000 --> 00:14:03,000
to minimize it and have, you know,

302
00:14:03,000 --> 00:14:07,000
satisfied, basically, client base.

303
00:14:07,000 --> 00:14:09,000
How have you created a data set

304
00:14:09,000 --> 00:14:13,000
to use to train these models?

305
00:14:13,000 --> 00:14:17,000
Is it all synthetic or simulated data

306
00:14:17,000 --> 00:14:21,000
or is there some set of data

307
00:14:21,000 --> 00:14:25,000
that you're using to kind of bootstrap this process?

308
00:14:25,000 --> 00:14:29,000
So, initially, we started by working with, you know,

309
00:14:29,000 --> 00:14:31,000
New York City and Manhattan,

310
00:14:31,000 --> 00:14:34,000
because there is significant amount of data,

311
00:14:34,000 --> 00:14:36,000
which is available through the New York Taxi

312
00:14:36,000 --> 00:14:38,000
and Limousine Commission.

313
00:14:38,000 --> 00:14:41,000
So, that makes it, you know, interesting to start with.

314
00:14:41,000 --> 00:14:44,000
But the way the OR approach is effectively

315
00:14:44,000 --> 00:14:46,000
to use open street map

316
00:14:46,000 --> 00:14:49,000
to extract as much data as possible

317
00:14:49,000 --> 00:14:51,000
to model the environment of the city

318
00:14:51,000 --> 00:14:52,000
we're looking at,

319
00:14:52,000 --> 00:14:55,000
and open street map has also, you know, many cities available.

320
00:14:55,000 --> 00:14:58,000
It's actually, I believe, the tool that you were used

321
00:14:58,000 --> 00:15:01,000
when they started their operations.

322
00:15:01,000 --> 00:15:04,000
But when it comes to this problem,

323
00:15:04,000 --> 00:15:07,000
it's actually possible to train from zero data.

324
00:15:07,000 --> 00:15:09,000
So, it's very similar in principle

325
00:15:09,000 --> 00:15:12,000
to what DeepMind did with Alpha0.

326
00:15:12,000 --> 00:15:15,000
Zero means that there is zero data.

327
00:15:15,000 --> 00:15:19,000
What you're setting up is basically a full simulated environment

328
00:15:19,000 --> 00:15:22,000
of, you know, of the city.

329
00:15:22,000 --> 00:15:25,000
And you can learn by doing.

330
00:15:25,000 --> 00:15:29,000
So, the system will initially make random decisions

331
00:15:29,000 --> 00:15:32,000
and then progressively realize what is good

332
00:15:32,000 --> 00:15:35,000
and what is less good and learn from there.

333
00:15:35,000 --> 00:15:38,000
So, this is also what is interesting about this problem

334
00:15:38,000 --> 00:15:42,000
and also why, you know, relatively small scale start-up

335
00:15:42,000 --> 00:15:45,000
like in study today can tackle those problems.

336
00:15:45,000 --> 00:15:50,000
It's because, you know, have not having necessarily a lot of data

337
00:15:50,000 --> 00:15:54,000
is not a hurdle provided you can build a realistic environment.

338
00:15:54,000 --> 00:15:57,000
So, the data components is really about, you know,

339
00:15:57,000 --> 00:16:01,000
affecting your ability to build a realistic environment of the city.

340
00:16:01,000 --> 00:16:03,000
But if you find a way to do that,

341
00:16:03,000 --> 00:16:07,000
technically speaking, you actually do not need data.

342
00:16:07,000 --> 00:16:09,000
You've got this environment,

343
00:16:09,000 --> 00:16:14,000
beyond the open street maps, data.

344
00:16:14,000 --> 00:16:17,000
What's the kind of shape of this environment?

345
00:16:17,000 --> 00:16:20,000
Did you build kind of, feel like a custom simulator

346
00:16:20,000 --> 00:16:24,000
or is it, is it just the data?

347
00:16:24,000 --> 00:16:29,000
You know, and not a lot of kind of external kind of tooling

348
00:16:29,000 --> 00:16:32,000
or did you use an open source simulation platform

349
00:16:32,000 --> 00:16:34,000
that you plug the data into?

350
00:16:34,000 --> 00:16:37,000
What does it all look like?

351
00:16:37,000 --> 00:16:41,000
So, actually, we built our own, our own basically simulation

352
00:16:41,000 --> 00:16:44,000
and giant for this problem.

353
00:16:44,000 --> 00:16:46,000
And including visualization,

354
00:16:46,000 --> 00:16:50,000
we actually also have visualization teams at InstaDeep.

355
00:16:50,000 --> 00:16:54,000
So, effectively open street map is used to provide us with, you know,

356
00:16:54,000 --> 00:16:57,000
metrics about, you know, how much time, for example,

357
00:16:57,000 --> 00:17:01,000
it takes from going to point A to point B in a city.

358
00:17:01,000 --> 00:17:05,000
And basically different, different, like, intermediate points.

359
00:17:05,000 --> 00:17:09,000
For example, like, I give and write could have, like,

360
00:17:09,000 --> 00:17:12,000
50 intermediate points or 13 intermediate points, you know,

361
00:17:12,000 --> 00:17:15,000
that kind of order of range.

362
00:17:15,000 --> 00:17:18,000
But once you have that, effectively,

363
00:17:18,000 --> 00:17:21,000
everything else runs on our platform.

364
00:17:21,000 --> 00:17:23,000
So, we've built, you know, our own platform

365
00:17:23,000 --> 00:17:26,000
to tackle these type of problems.

366
00:17:26,000 --> 00:17:28,000
And this is, you know, one of the things

367
00:17:28,000 --> 00:17:30,000
that makes it exciting for us.

368
00:17:30,000 --> 00:17:34,000
Like, you know, it's something that we have full control on.

369
00:17:34,000 --> 00:17:38,000
So, we use basically, you know, Kubernetes and TensorFlow.

370
00:17:38,000 --> 00:17:41,000
And train on Nvidia DGX1.

371
00:17:41,000 --> 00:17:46,000
So, Nvidia super computers to be able to extract results

372
00:17:46,000 --> 00:17:48,000
and continue to learn.

373
00:17:48,000 --> 00:17:52,000
And one is also interesting about this type of, you know,

374
00:17:52,000 --> 00:17:56,000
solution is that when they are deployed in the real world,

375
00:17:56,000 --> 00:17:59,000
you have an ability to, you know, keep learning from the results

376
00:17:59,000 --> 00:18:01,000
you're getting as well.

377
00:18:01,000 --> 00:18:03,000
You're using DGX1s.

378
00:18:03,000 --> 00:18:05,000
Those aren't cheap.

379
00:18:05,000 --> 00:18:09,000
Because your end user need to have the DGX1

380
00:18:09,000 --> 00:18:13,000
to continue to fine tune a model.

381
00:18:13,000 --> 00:18:17,000
Or is that, you know, what you've got to build up these models

382
00:18:17,000 --> 00:18:21,000
and then you can push the models out for inference?

383
00:18:21,000 --> 00:18:24,000
So, these are, yeah, it's exactly like you said.

384
00:18:24,000 --> 00:18:26,000
We basically use those for training.

385
00:18:26,000 --> 00:18:28,000
But once a model is trained,

386
00:18:28,000 --> 00:18:31,000
it would simply be, you know, about inference

387
00:18:31,000 --> 00:18:34,000
and customer does not necessarily have to have, you know,

388
00:18:34,000 --> 00:18:36,000
have you weaponry as such.

389
00:18:36,000 --> 00:18:37,000
Exactly.

390
00:18:37,000 --> 00:18:38,000
Exactly.

391
00:18:38,000 --> 00:18:44,000
So, that kind of leads me to the question about generalization

392
00:18:44,000 --> 00:18:47,000
and transfer learning in a sense.

393
00:18:47,000 --> 00:18:51,000
You know, for example, if you train a model on,

394
00:18:51,000 --> 00:18:54,000
or when you train the model on New York City,

395
00:18:54,000 --> 00:18:58,000
can you apply that model to San Francisco?

396
00:18:58,000 --> 00:19:02,000
If you do need to then look at San Francisco,

397
00:19:02,000 --> 00:19:04,000
do you have to start from scratch?

398
00:19:04,000 --> 00:19:08,000
Or can you do kind of a traditional transfer learning type

399
00:19:08,000 --> 00:19:09,000
of fine tuning?

400
00:19:09,000 --> 00:19:13,000
Does that, is there an analogy of that kind of fine tuning

401
00:19:13,000 --> 00:19:14,000
and reinforcement learning?

402
00:19:14,000 --> 00:19:15,000
How does that all work?

403
00:19:15,000 --> 00:19:16,000
Absolutely.

404
00:19:16,000 --> 00:19:17,000
Absolutely.

405
00:19:17,000 --> 00:19:21,000
Actually, you know, transfer learning works up to an extent.

406
00:19:21,000 --> 00:19:25,000
So, you are much better indeed using a trained model,

407
00:19:25,000 --> 00:19:27,000
even though it's a different city,

408
00:19:27,000 --> 00:19:29,000
to get started in another,

409
00:19:29,000 --> 00:19:32,000
even though you're going to still require, you know,

410
00:19:32,000 --> 00:19:35,000
significant training dependent on how similar the topology

411
00:19:35,000 --> 00:19:36,000
of the city is.

412
00:19:36,000 --> 00:19:39,000
You know, if the topology of the city has really nothing to do,

413
00:19:39,000 --> 00:19:41,000
then of course, that's an issue.

414
00:19:41,000 --> 00:19:44,000
If, for example, to coming back to Manhattan,

415
00:19:44,000 --> 00:19:48,000
if you have another city which has a relatively speaking

416
00:19:48,000 --> 00:19:51,000
rectangular shape for its streets and avenues,

417
00:19:51,000 --> 00:19:53,000
then obviously, you know,

418
00:19:53,000 --> 00:19:56,000
what you have learned in Manhattan would be

419
00:19:56,000 --> 00:19:58,000
to a certain extent applicable.

420
00:19:58,000 --> 00:20:01,000
So, we've seen actually this type of transfer learning

421
00:20:01,000 --> 00:20:03,000
happening in different problems.

422
00:20:03,000 --> 00:20:06,000
I mean, this is the right sharing problem.

423
00:20:06,000 --> 00:20:08,000
When it comes to, for example, packing bins,

424
00:20:08,000 --> 00:20:09,000
it's the same.

425
00:20:09,000 --> 00:20:12,000
If you are packing bins in a certain configuration,

426
00:20:12,000 --> 00:20:14,000
let's say, for 30 items,

427
00:20:14,000 --> 00:20:19,000
what you learn can be directly redeployed to larger dimensions,

428
00:20:19,000 --> 00:20:22,000
like 200 items under like.

429
00:20:22,000 --> 00:20:24,000
Sometimes even without retraining.

430
00:20:24,000 --> 00:20:26,000
So, you know, you nailed it on it.

431
00:20:26,000 --> 00:20:30,000
It's actually, you know, there is the equivalent of transfer learning

432
00:20:30,000 --> 00:20:32,000
in this type of problems as well.

433
00:20:32,000 --> 00:20:34,000
Yeah, I guess when we think about transfer learning

434
00:20:34,000 --> 00:20:36,000
applied to computer vision, right?

435
00:20:36,000 --> 00:20:41,000
You're often training models on image net and you're,

436
00:20:41,000 --> 00:20:46,000
you're trying to teach the lower layers of the model,

437
00:20:46,000 --> 00:20:50,000
kind of primitives like textures and edges and things like that.

438
00:20:50,000 --> 00:20:53,000
Is that a very problem-specific thing?

439
00:20:53,000 --> 00:20:57,000
Or are there, you know, some equivalent of edges

440
00:20:57,000 --> 00:21:01,000
at the, you know, just broad logistics level

441
00:21:01,000 --> 00:21:05,000
such that you would get some transfer benefit going,

442
00:21:05,000 --> 00:21:10,000
taking a model from bin packing to a right sharing, for example.

443
00:21:10,000 --> 00:21:16,000
So, there wouldn't be lots of, you know, intersection between,

444
00:21:16,000 --> 00:21:19,000
let's say, taking a bin packing model to right sharing.

445
00:21:19,000 --> 00:21:23,000
But there would be between different bin packing problems

446
00:21:23,000 --> 00:21:26,000
or different right sharing problems.

447
00:21:26,000 --> 00:21:30,000
So, typically like to take the bin packing example, for example,

448
00:21:30,000 --> 00:21:34,000
like if you're dealing with a problem that has, you know,

449
00:21:34,000 --> 00:21:36,000
like multiple hundreds of items,

450
00:21:36,000 --> 00:21:41,000
this is actually a large combinatorially exploding problem.

451
00:21:41,000 --> 00:21:44,000
However, if you are already having a good performance

452
00:21:44,000 --> 00:21:47,000
with a smaller set of, you know, items,

453
00:21:47,000 --> 00:21:52,000
actually this set could be as small as 10 items,

454
00:21:52,000 --> 00:21:56,000
then indeed the system has figured out something about

455
00:21:56,000 --> 00:22:01,000
how do I fit packages together that can be redeployed

456
00:22:01,000 --> 00:22:02,000
on larger models.

457
00:22:02,000 --> 00:22:06,000
So, it's a very interesting topic, but like at InstaDeep,

458
00:22:06,000 --> 00:22:10,000
we've been able to take models that been trained on,

459
00:22:10,000 --> 00:22:15,000
let's say, 10 items and deploy them with almost no changes

460
00:22:15,000 --> 00:22:19,000
at all to larger sets of problems, like 50 and 100 items,

461
00:22:19,000 --> 00:22:22,000
and still getting, you know, a type of performance

462
00:22:22,000 --> 00:22:26,000
which is better than other OR type algorithms.

463
00:22:26,000 --> 00:22:29,000
So, indeed, you know, there is a transferability,

464
00:22:29,000 --> 00:22:34,000
but I would say it's transferability among certain domains.

465
00:22:34,000 --> 00:22:39,000
So, you know, it's things that still have some level of similarity

466
00:22:39,000 --> 00:22:40,000
between them.

467
00:22:40,000 --> 00:22:43,000
And within, for example, the domain of bin packing

468
00:22:43,000 --> 00:22:48,000
are there dependencies on kind of the,

469
00:22:48,000 --> 00:22:51,000
you clearly mentioned the number of items,

470
00:22:51,000 --> 00:22:55,000
but like the shape of the items or the shape of the containers,

471
00:22:55,000 --> 00:22:56,000
the bins?

472
00:22:56,000 --> 00:22:57,000
Yes, of course.

473
00:22:57,000 --> 00:22:58,000
Yes, of course.

474
00:22:58,000 --> 00:23:01,000
If you have, you know, shapes that are quite similar,

475
00:23:01,000 --> 00:23:05,000
you would get strong transfer learning features.

476
00:23:05,000 --> 00:23:09,000
For example, like if it's the same shape for the general volume,

477
00:23:09,000 --> 00:23:11,000
let's say it's a container,

478
00:23:11,000 --> 00:23:14,000
and you learn to solve the problem for 10 items

479
00:23:14,000 --> 00:23:18,000
and you're just adding up more items that are relatively similar

480
00:23:18,000 --> 00:23:20,000
to the items you already have.

481
00:23:20,000 --> 00:23:24,000
In certain cases, you would have very little extra training,

482
00:23:24,000 --> 00:23:28,000
meaning that starting with the train model on smaller items

483
00:23:28,000 --> 00:23:31,000
would still deliver to you an acceptable performance.

484
00:23:31,000 --> 00:23:33,000
And this is actually surprising.

485
00:23:33,000 --> 00:23:37,000
We were, when we started working on this with my team,

486
00:23:37,000 --> 00:23:40,000
we were not necessarily expecting this type of properties,

487
00:23:40,000 --> 00:23:42,000
and yet we got there.

488
00:23:42,000 --> 00:23:46,000
And these are some of the results we published in an article

489
00:23:46,000 --> 00:23:49,000
that the latest, the last no-rips,

490
00:23:49,000 --> 00:23:51,000
which is called rank rewards.

491
00:23:51,000 --> 00:23:55,000
We found ways to basically go beyond what had been done

492
00:23:55,000 --> 00:23:58,000
for Alpha Zero and the game of chess and go

493
00:23:58,000 --> 00:24:01,000
to apply this to real, to real concrete problems.

494
00:24:01,000 --> 00:24:03,000
And yes, we were surprised that, you know,

495
00:24:03,000 --> 00:24:06,000
the level of transfer, you know,

496
00:24:06,000 --> 00:24:09,000
transfer ability from one problem or another was actually pretty high.

497
00:24:09,000 --> 00:24:14,000
What can you say about the data efficiency

498
00:24:14,000 --> 00:24:18,000
or sample efficiency rather of RL

499
00:24:18,000 --> 00:24:21,000
for this particular type of problem?

500
00:24:21,000 --> 00:24:26,000
Do you, you know, how much, how many,

501
00:24:26,000 --> 00:24:28,000
how, I don't know what the,

502
00:24:28,000 --> 00:24:30,000
if you want to talk about in terms of time

503
00:24:30,000 --> 00:24:35,000
or compute cycles or, you know, runs or batches,

504
00:24:35,000 --> 00:24:40,000
but how complex is, you know, getting to a model

505
00:24:40,000 --> 00:24:44,000
that, you know, starts to perform well

506
00:24:44,000 --> 00:24:49,000
compared to some of the other things that we might apply RL2.

507
00:24:49,000 --> 00:24:52,000
So, I mean, there is definitely some complexity,

508
00:24:52,000 --> 00:24:55,000
but with, you know, modern equipment,

509
00:24:55,000 --> 00:24:59,000
you could get to results in a matter of, like,

510
00:24:59,000 --> 00:25:02,000
something like 24 hours or, you know, a couple of days.

511
00:25:02,000 --> 00:25:05,000
We're not talking about like super, super long jobs,

512
00:25:05,000 --> 00:25:07,000
but it's not a couple of hours either.

513
00:25:07,000 --> 00:25:11,000
So, to give you concrete examples on our Nvidia DGX one,

514
00:25:11,000 --> 00:25:14,000
we would typically train a model overnight

515
00:25:14,000 --> 00:25:16,000
and be satisfied with the results.

516
00:25:16,000 --> 00:25:19,000
So, it's not something unsurmountable, which means that, you know,

517
00:25:19,000 --> 00:25:22,000
if you're not Google or if you're not like a very large player,

518
00:25:22,000 --> 00:25:25,000
you would not be able to compete.

519
00:25:25,000 --> 00:25:30,000
Obviously, it's strongly dependent on the quality of your algorithm.

520
00:25:30,000 --> 00:25:33,000
We spend a lot of time, for example, trying to find ways to,

521
00:25:33,000 --> 00:25:38,000
we inject the concept of competitiveness,

522
00:25:38,000 --> 00:25:41,000
even though these are like one player games,

523
00:25:41,000 --> 00:25:43,000
you're just trying to solve a problem.

524
00:25:43,000 --> 00:25:46,000
You know, one of the secret, the secret source, for example,

525
00:25:46,000 --> 00:25:49,000
in Alpha Zero, is that you are playing against yourself,

526
00:25:49,000 --> 00:25:51,000
and so you're constantly, you know,

527
00:25:51,000 --> 00:25:54,000
it's a two-player game where you're constantly challenging yourself.

528
00:25:54,000 --> 00:25:58,000
So, when you're really bad, like you start with random parameters,

529
00:25:58,000 --> 00:26:01,000
it doesn't matter because the other person in front of you,

530
00:26:01,000 --> 00:26:04,000
which is, you know, a replica of yourself, is equally bad.

531
00:26:04,000 --> 00:26:06,000
What matters is to get slightly better.

532
00:26:06,000 --> 00:26:09,000
And it's the same thing when you get to, like, superhuman performance,

533
00:26:09,000 --> 00:26:12,000
you can still improve, because what matters, in a sense,

534
00:26:12,000 --> 00:26:15,000
is the relative improvement than the absolute improvement.

535
00:26:15,000 --> 00:26:18,000
So, we found ways to re-inject this type of principles

536
00:26:18,000 --> 00:26:21,000
of, you know, adversarial competitiveness into this problem,

537
00:26:21,000 --> 00:26:24,000
and you can get good results, as I said, in, you know,

538
00:26:24,000 --> 00:26:27,000
a recent, a decent amount of time.

539
00:26:27,000 --> 00:26:30,000
Interesting. Is that part of the work that you've published

540
00:26:30,000 --> 00:26:32,000
at the recent NURPS?

541
00:26:32,000 --> 00:26:35,000
Yes, absolutely. So, we have a paper called Rankewarts,

542
00:26:35,000 --> 00:26:39,000
which was accepted at the DPRL workshop at the recent NURPS,

543
00:26:39,000 --> 00:26:44,000
and it describes exactly the type of results and breakthroughs

544
00:26:44,000 --> 00:26:46,000
we are describing.

545
00:26:46,000 --> 00:26:51,000
We talked about how, historically, these types of OR problems

546
00:26:51,000 --> 00:26:54,000
are solved using heuristic methods.

547
00:26:54,000 --> 00:26:59,000
One of the trends that I see in deep learning is,

548
00:26:59,000 --> 00:27:03,000
kind of, there's a pendulum where we kind of started

549
00:27:03,000 --> 00:27:06,000
at these very model-based approaches to things,

550
00:27:06,000 --> 00:27:10,000
and we swung to, like, pure statistical and learned approaches.

551
00:27:10,000 --> 00:27:12,000
And now, there's a lot of effort happening

552
00:27:12,000 --> 00:27:16,000
to try to fuse model-based and heuristics-based approaches

553
00:27:16,000 --> 00:27:18,000
with statistical approaches.

554
00:27:18,000 --> 00:27:21,000
Is that something that you've looked at for these types of problems?

555
00:27:21,000 --> 00:27:24,000
Yes, absolutely. The general idea that you're going to inject

556
00:27:24,000 --> 00:27:28,000
learning ability into a process is a very powerful one.

557
00:27:28,000 --> 00:27:32,000
But, yes, in some cases, it actually can be pretty efficient

558
00:27:32,000 --> 00:27:34,000
to take and existing heuristic,

559
00:27:34,000 --> 00:27:37,000
but add learning ability into the mix.

560
00:27:37,000 --> 00:27:39,000
And it comes to a very natural idea.

561
00:27:39,000 --> 00:27:41,000
If you are a large company, for example,

562
00:27:41,000 --> 00:27:44,000
you have lots of, like, you have a fleet of trucks,

563
00:27:44,000 --> 00:27:46,000
you have lots of packages to deliver tomorrow,

564
00:27:46,000 --> 00:27:49,000
and some of these companies would do overnight runs,

565
00:27:49,000 --> 00:27:52,000
take the result, and use it to, you know,

566
00:27:52,000 --> 00:27:54,000
process their operations for the day.

567
00:27:54,000 --> 00:27:56,000
And the next day, they're doing the same.

568
00:27:56,000 --> 00:28:00,000
It's kind of a waste not to learn anything new.

569
00:28:00,000 --> 00:28:02,000
So, adding learning ability into the mix,

570
00:28:02,000 --> 00:28:04,000
and especially constant learning ability,

571
00:28:04,000 --> 00:28:08,000
as you get, you know, more experience through your operations,

572
00:28:08,000 --> 00:28:10,000
is a natural idea.

573
00:28:10,000 --> 00:28:14,000
So, there are two schools, one which is indeed saying,

574
00:28:14,000 --> 00:28:18,000
you could, you know, you could use existing heuristics

575
00:28:18,000 --> 00:28:22,000
and build learning on top, which, you know, which makes sense.

576
00:28:22,000 --> 00:28:25,000
But, increasingly, and especially with, you know,

577
00:28:25,000 --> 00:28:30,000
compute, compute power for machine learning,

578
00:28:30,000 --> 00:28:33,000
doubling every three months and a half or so.

579
00:28:33,000 --> 00:28:36,000
And this is a study by OpenAI.

580
00:28:36,000 --> 00:28:40,000
In certain cases, it becomes possible to learn from first principles,

581
00:28:40,000 --> 00:28:43,000
meaning that you do not necessarily need the heuristics.

582
00:28:43,000 --> 00:28:46,000
So, the honest answer is very problem-dependent.

583
00:28:46,000 --> 00:28:52,000
Some problems are still too big to be learned completely and to end.

584
00:28:52,000 --> 00:28:57,000
But some of them can be actually solved using this type of techniques.

585
00:28:57,000 --> 00:29:01,000
So, it sounds like your general perspective

586
00:29:01,000 --> 00:29:08,000
is that where you can use pure learning-based solution that's preferable?

587
00:29:08,000 --> 00:29:13,000
Yes, absolutely, because that removes significant sources of bias.

588
00:29:13,000 --> 00:29:18,000
And, you know, by learning through first principles and direct experience,

589
00:29:18,000 --> 00:29:20,000
you can get a lot done.

590
00:29:20,000 --> 00:29:23,000
There is an additional advantage, you know,

591
00:29:23,000 --> 00:29:27,000
sometimes a heuristics-based model would require specific constraints to be in place

592
00:29:27,000 --> 00:29:30,000
and might not tolerate certain number of constraints.

593
00:29:30,000 --> 00:29:34,000
When you're taking, like, a pure learning approach from first principles,

594
00:29:34,000 --> 00:29:37,000
you can actually set all the constraints,

595
00:29:37,000 --> 00:29:40,000
all the real world constraints that you care about.

596
00:29:40,000 --> 00:29:45,000
And this is also an advantage, means, like, if these are the real world constraints,

597
00:29:45,000 --> 00:29:50,000
and actually there are people in the real world running those operations with those constraints,

598
00:29:50,000 --> 00:29:52,000
you know, solutions exist.

599
00:29:52,000 --> 00:29:58,000
So, as a consequence, a model learning end to end could get to that point as well.

600
00:29:58,000 --> 00:30:07,000
You mentioned that one of the considerations is the some inherent bias in the heuristics-based models.

601
00:30:07,000 --> 00:30:10,000
You know, this might come from constraints or other factors,

602
00:30:10,000 --> 00:30:13,000
and that a learning-based approach overcomes those.

603
00:30:13,000 --> 00:30:16,000
Have you seen the flip side as well,

604
00:30:16,000 --> 00:30:22,000
where the learning you've seen bias introduced by the learning-based approach

605
00:30:22,000 --> 00:30:28,000
that disadvantages it relative to what an organization might want to do?

606
00:30:28,000 --> 00:30:32,000
So, I mean, in general, like, if the problem is well formulated,

607
00:30:32,000 --> 00:30:37,000
and, you know, the system will learn to crack that problem.

608
00:30:37,000 --> 00:30:43,000
So, yes, in cases where you see the system not doing what you expect,

609
00:30:43,000 --> 00:30:50,000
it really comes down to, there might have been some constraints that you forgot to include into the mix.

610
00:30:50,000 --> 00:30:57,000
But if you really, you know, give to the system all the constraints that he should care about,

611
00:30:57,000 --> 00:31:03,000
you know, the system ultimately will explore and learn to crack that problem with the tools you have.

612
00:31:03,000 --> 00:31:08,000
So, it's more a question of defining the right set of objectives,

613
00:31:08,000 --> 00:31:12,000
and what's the space of, you know, possible behavior?

614
00:31:12,000 --> 00:31:17,000
If you do this well, there is no reason why a system which has sufficient capacity

615
00:31:17,000 --> 00:31:21,000
would introduce significant biases.

616
00:31:21,000 --> 00:31:24,000
Just to give you an example in the game of chess as well,

617
00:31:24,000 --> 00:31:29,000
you know, there have been some recent publications, I believe the title of the book is Game Changer,

618
00:31:29,000 --> 00:31:34,000
where, you know, chess experts have looked at the behavior of Alpha Zero,

619
00:31:34,000 --> 00:31:37,000
and, you know, some of them said, you know, this is very interesting,

620
00:31:37,000 --> 00:31:40,000
because, you know, we believe this is very close to the truth,

621
00:31:40,000 --> 00:31:43,000
meaning that the system has explored so many configurations,

622
00:31:43,000 --> 00:31:46,000
and developed an intuition which is really based on facts,

623
00:31:46,000 --> 00:31:50,000
that, you know, some people would consider this, you know,

624
00:31:50,000 --> 00:31:56,000
including actually Kasparov as, you know, the best manifestation of truth in chess

625
00:31:56,000 --> 00:31:59,000
that is possible to achieve.

626
00:31:59,000 --> 00:32:03,000
I'm also wondering about, I imagine when you're talking to enterprises

627
00:32:03,000 --> 00:32:06,000
that are considering these types of approaches,

628
00:32:06,000 --> 00:32:12,000
there are concerns like explainability and even robustness.

629
00:32:12,000 --> 00:32:16,000
You know, when you've got a set of rules, the actions that those rules tell you

630
00:32:16,000 --> 00:32:21,000
to take are relatively predictable, whereas with a deep learning solution,

631
00:32:21,000 --> 00:32:28,000
I'm imagining, you know, there can be instabilities or your decision boundaries are very complex,

632
00:32:28,000 --> 00:32:35,000
and you can have the system-recommend actions that are, you know, maybe, you know,

633
00:32:35,000 --> 00:32:39,000
not actions that you would want to take in the real world,

634
00:32:39,000 --> 00:32:44,000
or these behaviors that you see coming out of systems like these?

635
00:32:44,000 --> 00:32:49,000
I mean, it's a fair point. I mean, explainability will always be, you know,

636
00:32:49,000 --> 00:32:52,000
a concern at some level when using deep learning,

637
00:32:52,000 --> 00:32:57,000
and even though there is a promising research going on in terms of, like,

638
00:32:57,000 --> 00:33:00,000
getting better ways to understand what the system is doing,

639
00:33:00,000 --> 00:33:04,000
and how we, how it can explain to humans what it is doing.

640
00:33:04,000 --> 00:33:10,000
But typically, we don't operate into, with customers for whom explainability

641
00:33:10,000 --> 00:33:15,000
would be extremely strong. So coming back to the examples we discussed,

642
00:33:15,000 --> 00:33:20,000
whether it's right-sharing or been-packing, ultimately, the customer will care about,

643
00:33:20,000 --> 00:33:23,000
you know, in the case of been-packing, how many containers he's using,

644
00:33:23,000 --> 00:33:27,000
provided all the constraints about how to pack, have been respected.

645
00:33:27,000 --> 00:33:31,000
Same thing with right-sharing, you know, the client is going to care about, you know,

646
00:33:31,000 --> 00:33:36,000
what's the, you know, average, for example, a number of, number of miles driven

647
00:33:36,000 --> 00:33:40,000
to deliver one customer and the number of customers delivered.

648
00:33:40,000 --> 00:33:47,000
In this case, it's okay if a few decisions are suboptimal, provided the total,

649
00:33:47,000 --> 00:33:52,000
you know, the average decision is better. So in a sense, you're not getting extremely

650
00:33:52,000 --> 00:33:57,000
penalized for taking a decision which locally would look suboptimal.

651
00:33:57,000 --> 00:34:01,000
So that is typically, you know, the kind of, you know, customers re-engaging

652
00:34:01,000 --> 00:34:06,000
and it's a thing studied for logistics, but I mean, to your point, absolutely.

653
00:34:06,000 --> 00:34:12,000
I mean, if you all have certain situations for which where the negative impact

654
00:34:12,000 --> 00:34:16,000
of a bad decision can be massive, I don't know, for example, you know,

655
00:34:16,000 --> 00:34:19,000
like in, you know, drug manufacturing or things like that,

656
00:34:19,000 --> 00:34:24,000
obviously, explainability could be of interest, but we haven't seen this,

657
00:34:24,000 --> 00:34:29,000
you know, with the type of problems we work on, because really about logistics

658
00:34:29,000 --> 00:34:33,000
and it's about optimizing well-known constraints and, you know,

659
00:34:33,000 --> 00:34:36,000
there's not something which, you know, like if you deliver, for example,

660
00:34:36,000 --> 00:34:42,000
a person and it has taken too long for that particular person, you know,

661
00:34:42,000 --> 00:34:46,000
it is a concern, it can be improved, but it's not a massive concern

662
00:34:46,000 --> 00:34:48,000
that it would stop all your operation.

663
00:34:48,000 --> 00:34:53,000
True, wouldn't stop all your operations, but as organizations mature

664
00:34:53,000 --> 00:34:59,000
in their use of these types of technologies, they may want to, you know,

665
00:34:59,000 --> 00:35:05,000
their perception of what's the right reward function evolves and matures,

666
00:35:05,000 --> 00:35:10,000
and at some point, you know, maybe they wouldn't want, you know,

667
00:35:10,000 --> 00:35:14,000
a customer to randomly sit in the car and be driven across town only,

668
00:35:14,000 --> 00:35:18,000
be driven back because that's what the machine said.

669
00:35:18,000 --> 00:35:23,000
Do you have a path to addressing that in this kind of approach,

670
00:35:23,000 --> 00:35:27,000
or are they totally antithetical to one another?

671
00:35:27,000 --> 00:35:33,000
You can, you can, you know, control this type of, like, spurious behavior.

672
00:35:33,000 --> 00:35:36,000
For example, like typically, like in the example you take,

673
00:35:36,000 --> 00:35:41,000
if somebody, if a car would take someone and really drive around town in crazy ways

674
00:35:41,000 --> 00:35:47,000
before delivering that person, you could actually put a very significant constraint

675
00:35:47,000 --> 00:35:51,000
on like in a penalty if you want on the outcome.

676
00:35:51,000 --> 00:35:56,000
So instead of having just, for example, like the total delay

677
00:35:56,000 --> 00:35:59,000
that this person experienced, you could say that, you know,

678
00:35:59,000 --> 00:36:02,000
beyond a certain point, which is statistically driven,

679
00:36:02,000 --> 00:36:05,000
and you can collect the statistics for a given city.

680
00:36:05,000 --> 00:36:08,000
Hey, if you go beyond that, there's really a significant problem.

681
00:36:08,000 --> 00:36:12,000
And as a consequence, let's say, you know, the penalty on the reward system

682
00:36:12,000 --> 00:36:14,000
would be very high.

683
00:36:14,000 --> 00:36:20,000
So that's one of the ways you can, you can actually remove spurious cases from your system.

684
00:36:20,000 --> 00:36:25,000
And when you're engaging with someone around these types of problems,

685
00:36:25,000 --> 00:36:31,000
do you find that there is very unique and specific problem definition

686
00:36:31,000 --> 00:36:36,000
that needs to happen and kind of building out these reward functions

687
00:36:36,000 --> 00:36:40,000
and penalties and things like that, or, you know,

688
00:36:40,000 --> 00:36:44,000
the most people just want a been packing solution that,

689
00:36:44,000 --> 00:36:48,000
for which you've already really defined the problem.

690
00:36:48,000 --> 00:36:53,000
So typically when we work with large corporates,

691
00:36:53,000 --> 00:36:58,000
those companies have already processes in place, including, you know,

692
00:36:58,000 --> 00:37:04,000
and they have a good sense of what type of constraints they are experiencing.

693
00:37:04,000 --> 00:37:08,000
So one of the cool things at INSADEEP is that we will adapt to their problem.

694
00:37:08,000 --> 00:37:11,000
Our process is pretty general.

695
00:37:11,000 --> 00:37:14,000
So we basically put ourselves in the customer's shoes.

696
00:37:14,000 --> 00:37:18,000
So we'll take the problem with, like, look at it the way they look at it.

697
00:37:18,000 --> 00:37:20,000
They tell us the constraints they have.

698
00:37:20,000 --> 00:37:23,000
And yes, those constraints could change from one customer to another

699
00:37:23,000 --> 00:37:27,000
because maybe they're not doing exactly the same type of operations

700
00:37:27,000 --> 00:37:30,000
or maybe because they have different priorities and preferences.

701
00:37:30,000 --> 00:37:34,000
So we'll incorporate their constraints and work from there.

702
00:37:34,000 --> 00:37:38,000
And as I said, you know, what's pretty cool in the way we look at things

703
00:37:38,000 --> 00:37:43,000
and these general systems is that they can actually learn from, you know,

704
00:37:43,000 --> 00:37:45,000
the constraints you give them.

705
00:37:45,000 --> 00:37:48,000
So you're not a prisoner of given constraints.

706
00:37:48,000 --> 00:37:54,000
So it's not a problem in practice to be looking at different customers

707
00:37:54,000 --> 00:37:58,000
and different constraints because the principles that are behind it

708
00:37:58,000 --> 00:38:01,000
and this is the thing which I'm most excited about,

709
00:38:01,000 --> 00:38:05,000
are very general. So no matter how you look, you know,

710
00:38:05,000 --> 00:38:09,000
what are your constraints and some of those could be real physical constraints

711
00:38:09,000 --> 00:38:11,000
and others could be preferences,

712
00:38:11,000 --> 00:38:14,000
there is always a way to solve your problem.

713
00:38:14,000 --> 00:38:18,000
Provided that it is actually a real world problem that has solutions

714
00:38:18,000 --> 00:38:21,000
and it's typically the case and you want to improve your solutions.

715
00:38:21,000 --> 00:38:24,000
And so we spent most of the time talking about reinforcement learning.

716
00:38:24,000 --> 00:38:28,000
Are there other approaches that you use for these types of problems

717
00:38:28,000 --> 00:38:33,000
or is that the primary area you're working with?

718
00:38:33,000 --> 00:38:37,000
So I mean, we also use like, you know, classical algorithms,

719
00:38:37,000 --> 00:38:40,000
but these are more for benchmarking.

720
00:38:40,000 --> 00:38:43,000
You know, I think the people who believe that, you know,

721
00:38:43,000 --> 00:38:46,000
the type of smart systems are there to stay.

722
00:38:46,000 --> 00:38:50,000
But when we say reinforcement learning just to be very clear

723
00:38:50,000 --> 00:38:53,000
and I know the audience is pretty technical.

724
00:38:53,000 --> 00:38:59,000
What we believe the most at in in in in study is what we believe in the most is

725
00:38:59,000 --> 00:39:02,000
the concept of having, you know,

726
00:39:02,000 --> 00:39:06,000
a neural net that's going to work with planning type algorithms.

727
00:39:06,000 --> 00:39:10,000
For each we look at a technology such as Monte Carlo research.

728
00:39:10,000 --> 00:39:14,000
It is, you know, in the secret source of, you know,

729
00:39:14,000 --> 00:39:17,000
what makes, for example, alpha zero or so good.

730
00:39:17,000 --> 00:39:20,000
So, you know, I have a great analogy for this.

731
00:39:20,000 --> 00:39:23,000
It's this idea and I presented this at GTC,

732
00:39:23,000 --> 00:39:27,000
which is this idea of thinking, thinking fast and thinking slow.

733
00:39:27,000 --> 00:39:32,000
So this is the same title as Daniel Kaneman's book, The Noble Price.

734
00:39:32,000 --> 00:39:35,000
It's this idea that with planning, you know,

735
00:39:35,000 --> 00:39:38,000
basically Monte Carlo research into this type of algorithms,

736
00:39:38,000 --> 00:39:41,000
you're going to be able to think slow,

737
00:39:41,000 --> 00:39:44,000
which means you're going to consider many parts

738
00:39:44,000 --> 00:39:47,000
and incorporate those parts of flat planning into your decision.

739
00:39:47,000 --> 00:39:50,000
And the thinking fast part is the neural net.

740
00:39:50,000 --> 00:39:53,000
So to be very clear, it's not just a rel in the sense,

741
00:39:53,000 --> 00:39:58,000
hey, I'm experiencing a reward and I'm kind of looking blindly for that reward.

742
00:39:58,000 --> 00:40:02,000
I'm actually trying to find, you know, interesting paths.

743
00:40:02,000 --> 00:40:06,000
So another way to look at it, which is an analogy I really like,

744
00:40:06,000 --> 00:40:10,000
is, you know, in deep RL, you're like in the dark in a room

745
00:40:10,000 --> 00:40:12,000
and you're trying to find the switch.

746
00:40:12,000 --> 00:40:15,000
And that is the sparse reward, for example, that you're looking at.

747
00:40:15,000 --> 00:40:18,000
But you're like kind of like looking blindly and trying to find the switch.

748
00:40:18,000 --> 00:40:23,000
You know, if you add planning and CTS type approaches,

749
00:40:23,000 --> 00:40:25,000
it's like you have a search light.

750
00:40:25,000 --> 00:40:26,000
It's not a perfect search light.

751
00:40:26,000 --> 00:40:28,000
So it's only covers a little bit of area.

752
00:40:28,000 --> 00:40:30,000
But at least you're not completely blind.

753
00:40:30,000 --> 00:40:32,000
And this will help you be more efficient, much faster.

754
00:40:32,000 --> 00:40:35,000
So these are the two tech that are used.

755
00:40:35,000 --> 00:40:37,000
And it makes a lot of sense.

756
00:40:37,000 --> 00:40:42,000
And in a funny way, it's actually interesting that Daniel Kaneman's book,

757
00:40:42,000 --> 00:40:45,000
by the way, which is a great book, talks about it

758
00:40:45,000 --> 00:40:48,000
from the angle of, you know, human neuroscience

759
00:40:48,000 --> 00:40:52,000
and human, you know, behavioral patterns.

760
00:40:52,000 --> 00:40:56,000
But there are some equivalents in the machine learning and AI world.

761
00:40:56,000 --> 00:40:58,000
So let me ask you this.

762
00:40:58,000 --> 00:41:03,000
We've spent quite a bit of time talking about this topic

763
00:41:03,000 --> 00:41:06,000
and what you presented at GTC.

764
00:41:06,000 --> 00:41:11,000
What were the key takeaways that you were hoping to leave your audience there with?

765
00:41:11,000 --> 00:41:15,000
So, yeah, for me, the key takeaway is that, you know,

766
00:41:15,000 --> 00:41:19,000
machine learning systems are progressively, you know,

767
00:41:19,000 --> 00:41:22,000
going to have an impact when it comes to making decisions

768
00:41:22,000 --> 00:41:23,000
in the real world.

769
00:41:23,000 --> 00:41:27,000
And, you know, while we have mostly seen, you know,

770
00:41:27,000 --> 00:41:29,000
those in the context of games,

771
00:41:29,000 --> 00:41:32,000
and those games have, you know, started like very simple games

772
00:41:32,000 --> 00:41:33,000
at Aristotle.

773
00:41:33,000 --> 00:41:35,000
And now it's more like Starcraft complex games.

774
00:41:35,000 --> 00:41:38,000
I think we are about to see those reads

775
00:41:38,000 --> 00:41:42,000
I think we are about to see those really spread out

776
00:41:42,000 --> 00:41:44,000
and make an impact in the real world.

777
00:41:44,000 --> 00:41:46,000
And this is due to, you know,

778
00:41:46,000 --> 00:41:49,000
a few key properties of these systems.

779
00:41:49,000 --> 00:41:52,000
First, these systems can learn end to end.

780
00:41:52,000 --> 00:41:56,000
So you can, you know, in certain cases go from pixels to behavior

781
00:41:56,000 --> 00:41:59,000
or from raw data to behavior, things like that.

782
00:41:59,000 --> 00:42:02,000
But these systems get better with scale.

783
00:42:02,000 --> 00:42:06,000
And we have reached a point in terms of maturity of, you know, hardware

784
00:42:06,000 --> 00:42:11,000
and also like, you know, algorithms that makes, you know,

785
00:42:11,000 --> 00:42:15,000
you know, deploying these systems at scale a real possibility.

786
00:42:15,000 --> 00:42:19,000
So I would, you know, to the users, you know,

787
00:42:19,000 --> 00:42:22,000
the listeners of the program, I would say that, you know,

788
00:42:22,000 --> 00:42:25,000
expect a lot of action on these in the real world

789
00:42:25,000 --> 00:42:27,000
and not in games anymore.

790
00:42:27,000 --> 00:42:32,000
And to a certain extent, if you are, you know, having a decision-making

791
00:42:32,000 --> 00:42:37,000
system that uses a compute and doesn't have learnability

792
00:42:37,000 --> 00:42:42,000
deeply embedded in it, that means that probably you will have to change,

793
00:42:42,000 --> 00:42:46,000
you know, the way you are operating to include, you know,

794
00:42:46,000 --> 00:42:49,000
AI first approaches and, you know, learnability.

795
00:42:49,000 --> 00:42:53,000
And that means that, you know, lots, lots of industries

796
00:42:53,000 --> 00:42:56,000
and particular logistics, last mile delivery,

797
00:42:56,000 --> 00:42:59,000
are going to be deeply impacted by those.

798
00:42:59,000 --> 00:43:02,000
Also, well, Karim, thank you so much for taking the time to chat with us.

799
00:43:02,000 --> 00:43:04,000
This is really interesting stuff.

800
00:43:04,000 --> 00:43:05,000
Thanks a lot.

801
00:43:05,000 --> 00:43:06,000
It was a pleasure.

802
00:43:06,000 --> 00:43:09,000
And, you know, thanks a lot for having me.

803
00:43:13,000 --> 00:43:14,000
All right, everyone.

804
00:43:14,000 --> 00:43:16,000
That's our show for today.

805
00:43:16,000 --> 00:43:18,000
For more information about this and every show,

806
00:43:18,000 --> 00:43:21,000
visit TwomoAI.com.

807
00:43:21,000 --> 00:43:24,000
Remember, just one week left to register for TwomoCon,

808
00:43:24,000 --> 00:43:25,000
AI platforms.

809
00:43:25,000 --> 00:43:28,000
So head over to TwomoCon.com now.

810
00:43:28,000 --> 00:43:32,000
Thanks so much for listening and catch you next time.

