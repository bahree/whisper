Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
While at the Nips conference back in December, I attended the Black and AI workshop and dinner
and had a chance to meet a bunch of amazing people.
This week on the show, we'll be highlighting some of the great work being done by folks
in this community.
In this episode, I'm joined by Mustafa Cise, research scientist at Facebook AI Research
Lab or Fair in Paris.
Mustafa's broad research interests include the security and safety of AI systems, and
we spend some time discussing his work on adversarial examples and systems that are robust
to adversarial attacks.
More broadly, we discuss the role of bias in datasets and explore his vision for models
that can identify these biases and adjust the way they train themselves in order to
avoid taking them on.
Before we dive in, we want to hear about your experiences with home and personal AI.
So we launched my AI video contest last week.
Please head on over to twimbleai.com slash myai and take a few minutes to submit your thoughts
on the role AI is playing in your home and personal life, your favorite example of home
or personal AI, the AI that you really want to see in your lifetime.
Or just where you see this all going.
The entries with the most likes will win great prizes, including a Cosmo and a Lighthouse,
both of which we discussed on our AI and consumer electronic series last week.
And now on to the show.
Alright everyone, I am on the line with Mustafa Cise.
Mustafa is a research scientist at Facebook AI Research Lab and we had the pleasure of
meeting at the recent nips event and particularly we hung out quite a bit at the black and AI,
both the workshop and the dinner.
Mustafa, it is great to reconnect with you.
How are you doing?
I'm great.
How about you?
Fantastic.
Fantastic.
So the tradition here is for me to give you an opportunity to introduce yourself to
the audience.
So why don't you tell us a little bit about your background and how you got interested
in AI?
Sure.
So I'm Mustafa.
I was born and raised in Senegal in West Africa.
If you have never been there, you should, it's a fantastic country, just a little bit of
ad.
Nice.
So I was born and raised there and I did most of my education there.
I went to the University Gaston Berger, where I studied mathematics and physics.
And it's during one of these courses that I really got interested into AI.
It was an algorithmic course where we had a project.
The task was to design an AI, basically, to solve the game of AOLA, which is a strategic
game, a bit like Checkers or chess, but very popular in West Africa.
And most of the systems that were designed by the students were based on a rule base.
If you do it, if then else rules, basically.
So I really wanted to design a system that learned from different games and learns to solve
the game, basically.
So that's how I got into AI.
I started watching videos on YouTube and teaching myself some of the basic algorithms.
Okay.
Awesome.
Awesome.
And so how did you make your way from there to your current role at Facebook AI research?
So, after that experience, I really enjoyed it.
And I kind of realized that this was what I wanted to do for the rest of my life.
I could contribute in this area.
So I decided to go abroad because there were now not an advanced course or degree in AI
in the university.
So I went to France to study for a master's.
So I studied, I spent the first year in Paris at the University of Paris in Marie Curie.
Then I spent the second year at the University of Montreal.
Then I came back to Paris to do a PhD there.
And then after I graduated from my PhD, I did a one-year postdoc and after that postdoc,
I came to Facebook AI research.
So it's been...
You're based in New York cities, all right?
Actually, I'm based in Paris even though I spent some time in New York sometimes.
Oh, okay.
But I'm based in Paris.
So we have a live in Paris with about 30 researchers and engineers.
Oh, nice.
Yeah.
Nice.
And so maybe we can start out by having you tell us a little bit about your research interests
and the kind of things you're working on nowadays.
Yeah.
So I'm interested in AI and machine learning at large.
But these days I spend most of my time working on trust in AI.
So by trust, I mean all the topics pertaining to safety and security in AI, fairness and
biases and also interpretability.
I think these are very important topics that are gaining some momentum now but that have
been overlooked for some time.
And it's very important that as a community, we focus on building systems that can be trusted
and that can be used broadly to say.
So that's what I spend most of my time working on right now.
Okay.
Can you maybe give me some examples of your recent research you've done in these areas?
Sure.
So in the area of security and safety, I have worked on understanding the topic of adversarial
examples.
So adversarial examples are examples that the model, the models of an AI system sees or
hears or depending on the modality, but that makes the model behave in a completely different
way that it should behave that is in a completely unexpected way.
So these are malicious examples and they tell us a lot about how the models that we are
building right now are kind of not very well understood.
So I have done some work with colleagues in understanding how these adversarial examples
arise and how to generate them for speech recognition or for semantic segmentation which
are very challenging tasks, but which have applications in personal assistance or in self-driving
cars.
And it's very important to be able to create these adversarial examples because that's
the first step in order to evaluate the robustness of an AI.
So then we moved on working on defenses against these adversarial examples.
We have also done some work in this area which have been recently published with some
colleagues here and another line of work is also this area of biases and fairness.
So recently we have shown in a study that the models basically the deep learning models
that you use to train to recognize images when you train them on a popular benchmark
data set which is imagined at.
They learn quite some biased decision making process and this is somehow unexpected because
most of the time the data set is considered as balance, but there are explanations for
these.
So we have observed some interesting biases ranging from racial biases for example and
all this is described in the paper.
Okay.
Yeah, I think I've talked about adversarial examples in my newsletter but I don't think
we've really dug into it in the podcast, at least not in a lot of detail.
So maybe let's start there and for folks who aren't familiar with that whole field,
do you have some favorite examples of adversarial examples?
Sure.
So imagine you have a system that is supposed to tell you what is in a picture.
So if you present the system with a picture it says, this is a dog or this is a cat etc.
So it is possible to inject to the picture that you're presenting to the image.
Some very structured noise that is imperceptible to the human eye but that will change the
decision of the system.
So you can have two seemingly identical pictures of dog of the same dog and for one of them
the system will say this is a dog and for the second one the system will say this is
a cat or this is a car or any other thing.
So this is kind of intriguing because these systems are very accurate normally and we tend
to compare them to humans because they are very accurate and when we measure their accuracy
we say hey, usually we say hey look the system is as accurate as a human but on the flip side
a human is very robust to these kind of perturbations like no matter how much perturbation you
add to an example if a human looks at two pictures that are similar she will be able
to say that this is a dog and this is a dog as well but if the system can be tricked
by these slight modifications that are imperceptible to the human eyes it means something.
It says a lot about the current understanding or the lack of understanding that we have
about the behavior of these models and it makes it an interesting topic to study.
So this is basically the high level explanation of adversarial examples.
By the way I gave an example speaking about topics or speaking about images, image recognition
but I think that's what most of us associate with adversarial examples with scenes a bunch
of these but it's not just images.
Absolutely.
So we have shown in a recent paper a generic method called Houdini that allows you to generate
adversarial examples not only for images but also for speech recognition task where you
can add some noise in an audio file such that humans cannot distinguish the two audio
files but a speech recognition system will be completely interpreted to audio files in
a completely different way.
So it's a very broad topic.
Oh wow.
So the example that comes to mind for me is if you can create an audio file that says
one thing but interprets the system as saying okay Google you could really mess with people's
their virtual assistants and things like that that they've got around their home.
Sure.
That's one one scenario you could imagine definitely.
That's one scenario you could imagine.
To what extent are the current adversarial examples that work in general like to what
extent is that model specific meaning is the work that's happening around adversarial
examples and defenses is it all explicitly impact specific types of models you know deep
neural networks or specific types of deep neural networks or specific architectures
of networks or is it a broader phenomenon that can apply more generally.
So I would say that this is a broad phenomenon that applies to different class of model
branching from deep neural networks of course people talk about it because it's the most
familiar family of functions right now but it also applies to separate vector machines
or to decision trees and all sorts of things.
That being said the kind of model that you that the specific type of model that you're
that you're using can have properties that make it less robust to adversarial examples.
This is true across the different family of models but depending on the conditioning
and the nice some nice properties that the model may have they may be more or less robust
to adversarial examples.
However for the defenses there are defenses that exploit the nature of the model sometimes
but there are also some defenses that are agnostic to the type of model that you consider.
For example we recently proposed in a paper defenses against adversarial examples that
are just based on transformations of the input in order to remove the noise that has
been injected to the input.
So this is typically completely agnostic to the type of model that you consider.
Okay.
These adversarial examples they you know they're maliciously created what's the process
generally for creating them.
So the process for creating an adversarial example is actually the inverse process for
training the model.
So when you train the model you show to the model an image of a dog and you measure how
well the model recognizes that this is a log.
This is a dog and you reinforce that decision if it is a positive one and or you you kind
of encourage the model to change its decision if the decision is not good.
So when you create an adversarial example you do the reverse process and the way you
do it is that we show an example of a dog to the model and when the model says yes this
is a dog then you calculate from the model the right direction in which you need to move
the example in the input space in order to change slowly the decision of the model which
means that your base on the decision of the model in order to know how to trick it.
So it's the opposite way of training the model.
So the subtlety here is that changing the decision of the model does not require a big
change in the input space.
In fact if you consider images at the pixel level that change is always almost imperceptible
by a human eye but it is surprisingly sufficient for flipping the decision of a model.
You know there are technologies in the case of images and audio as well where you can have
like these digital watermarks where you are doing similar things right you are placing
some kind of watermark I guess in that case it is not noise per se but you are altering
the image in a way that is imperceptible to the human observer but has some meaning to
the system.
I guess I'm just sharing that as kind of an example of how that process is.
It's very similar it's not the same thing but it's very similar that you manage to put
in the input some information that makes the model behave in some way but that is basically
imperceptible and you are definitely right pointing to the watermarks because it's very
similar in spirit.
And so there are a bunch of folks that are researching these adversarial examples.
What's the current research thrust is it trying to identify different ways of generating
them or have we all kind of is there you know only you know that general way that you
describe to create them and everyone is working on defense or what.
So there are different rules for generating adversarial examples but they basically exploit
the same information which is the gradient.
In one way or another it can be direct when you have access to the full information
about the model or it can be direct where you kind of train a substitute of a model and
generate the gradient from it and transfer the adversarial example to another model.
So there are a few different attacks that do exist but for now I have to say that attackers
have an edge because it's much more difficult to full a model than to protect it from adversarial
examples.
However in my opinion adversarial examples are not just interesting from a security point
of view.
Of course this is very important because it's critical when you want to use these powerful
and attractive deep learning models for example in certain types of applications.
But another aspect of the adversarial example is that they tell us something about the models
that we love and that we use on a daily basis.
They say that there are things that we do not understand because this behavior is
sort of pathological.
So they trigger interesting research questions beyond the security perspective but on the
very nature of these models and the learning algorithms that we use.
And we have also found with other colleagues that adversarial examples can be used for
other purposes and in our case it was to exhibit biases that the model may have learned
from the data which was quite interesting to see.
Can you elaborate on that and what the implications are?
When you have a model so basically machine learning models or you design a parametric
model you take some data you apply a learning rule and you train the model from the data.
So you can expect that if your model is trained to optimize for some criteria it will behave
very well if the learning algorithm is good, if you have sufficient data, if the model
that you have chosen is powerful enough that it will behave in some expected way, right?
But so many your input data is follows a similar distribution.
Absolutely, that's the back hypothesis, very good observation.
So the point here is that your model may have captured some regularities that are present
in the data but that you may not notice because when you test the model or when you evaluate
it you are not using the right criteria that could show you the broad scope of everything
your model have learned.
So I can give you an example.
So we have found that, for example, if you take a popular model like ResNet, residual networks
which are very popular in computer vision and you train them on ImageNet using all the
sophisticated learning algorithms then you will achieve some excellent performance as far
as accuracy is concerned but when you look deeply into why the model predicts what it
predicts you can see some very funny things.
So for example we found out that if you consider the class the category basketball.
So for some reason the model tend to consider that if you show an input with a black person
to the model it will predict that this is a picture whose corresponding category is
basketball and this is no matter whether it is a basketball or not doesn't matter.
And the reason is the model has picked up some biases in the data.
So that suggests this actually and it's true also for different sort of biases.
It almost makes me think that like all models over fit, it's just some of them over
fit too much but you know the other ones are overfitting in ways that you know might produce
acceptable results until you know we're throwing a curve ball and see something that we didn't
expect.
Absolutely.
This is a very good observation.
So many of the models which if they are not properly regularized or if the last function
the objective, the criteria that you're optimizing for does not take into account these problems
they may well learn different sort of biases from the data.
And these biases are numerous and diverse by the way.
So I always put it this way I think you are what you eat and for models for the models
the data sets they're trained on right now are just junk food.
So we should we should not expect them to behave differently.
If we don't endow the models with the ability to pick precisely the examples they should
learn from and what they should learn and what they should not learn.
Just the way we should we we do it with the kids for example where the kid takes something
and puts it in her mouth if it is something that a kid can eat you probably can let her
do but if it is not then you will stop her and say hey you cannot do this and as adult
as well sometimes we say oh I can eat this I cannot eat that because there is there is
a lot of fat here there is not enough there is a lot of sugar here etc.
So we should endow the models with the ability to do some self criticism and introspection
to say I should learn from this data set I should learn from this data point or I should
not learn from this data point etc.
So these are very fascinating questions I spend a lot of time thinking and working
on these days.
So I wanted to drill down into that last point you made about endowing the model with
this ability to kind of discriminate between good and bad data but before we do that we
kind of got to the bias by way of the adversarial examples and I think you were making a connection
there that I didn't fully catch and I want to make sure that if that is what you were
trying to do that we make that clear.
Sure so the reason why adversarial examples can be interesting to exhibit the biases
the model may have learned is actually very simple so when you learn a classifier basically
what you are trying to do is to find a decision boundary to draw a line that says that this
side of the line is the cat and the other side of the line is the dogs.
This is very simple and you can visualize it very well if the model is linear but if
the model is non-linear it is much more challenging.
And the way adversarial examples are built it is by taking a point of a specific class
and moving it slowly but in a very straightforward way to the other side of the decision boundary.
So not all examples are created equal.
If an example is close to the decision boundary in the latent space then you will not need
to move it a lot in order to take it to the other side of the decision boundary.
And if an example is far from the decision boundary you will have a hard time taking it
to the decision boundary to the other side of it meaning that you will have to add a
lot of noise, a significant amount of noise.
But the examples that are close to the decision boundary are those examples for which the
model have learned a representation that is not very robust when you consider the concept
they should belong to.
So if you take a picture of a dog and for which you can flip the decision of the model very
easily by adding very slightly very small amount of noise that means that the model is
not very sure about what this is and that is called criticism.
So this is a criticism for the model.
It's something that the model barely knows what it is but it's not very sure.
But if you take a picture of a dog and you add a lot of noise and still you have a hard
time flipping the decision of the model then the model is pretty confident what this is.
So and that is what the model considers as being prototypical.
So after learning from some data you can take another dataset, some test dataset and
and by using the adversary the procedure for generating adversarial examples you can
see how much noise you should add to some data points in order to change the decision
of the classifier and use that as a proxy of how prototypical an example is to the model.
So the examples that are prototypical to the model to what the model have learned about
a given concept will be hard to change their decision.
But for the others it will be easy to change their decision.
And if you look at the prototypical example they will tell you what what the model has
really learned what it is very confident about.
And that's that's actually what we used and in order to discover what what the model considered
really as being pictures of basketball.
And when we looked at it it was all pictures of basketball basically with just blood
persons inside and never white persons.
And when we looked at pictures of that or criticisms which is you know some pictures
that the model classifies pretty well but it's not very sure about it it's at the edge
of its knowledge then it's very it's pictures that are mostly populated with white persons.
And all this is basically validated on different categories and different classes.
Similarly we found that when the model considers us being prototypical offered an image belonging
to the category ping pong is basically a picture with an Asian in it.
So so so after when people see this kind of things they will say hey the model is is racist.
In fact a model cannot be racist because it's not intelligent the models that we build
are accurate but they are not intelligent.
And whatever they learn whatever they whatever behavior they exhibit they just learned
it from the data they were trained on.
So somehow these models are the mirror of what we are.
So they they just tell us something about the process we use to collect the data to
train the model and sometimes they even tell us something about those who collected that
data and those who built the model.
So so I will take this occasion to emphasize on something that is very important regarding
diversity in in our community.
It's actually critical and not just because it's fashionable or it's because it sounds cool.
It's critical that as a community we become more open and more diverse because the models
that we build and the data sets these models learn from if we want to have a broad impact
at the global level worldwide and build technology that is representative of the human
beings on this planet and not just a specific population which tends to be westerns and
North Americans it's important that we become more open and more diverse so that everybody
has the tools and and the the techniques to build the technology to solve its own problems.
Well everyone has the tools and techniques but also those organizations that are building
you know these types of systems you know have a more natural inclination to structure
their data sets for example so that they're more robust.
Actually I'm not even sure that everybody has the tools and techniques because let's
say let's consider the machine learning community for example.
So this community every year organizes conferences which the tool of the most important ones being
Nips and ICML and they are basically every year organized in a western country either in
Europe or in the US with some exceptions ICML has been organized previously in China for
example and last year in Australia which I consider being a western country as well.
So it is very difficult for people from other parts of the world to get into the community
just because the places where these gatherings happen are not easily accessible to them
and that's one thing right that's one thing and when you are when you are European or
American you may not see these things because you have the right passport that allows you
to go everywhere but when you are not then then you can see this you know I just give you
an example this year I had two papers accepted for publication at ICML it was in Australia
I could not go there because they did not grant me a visa.
For many people this is they don't even believe it when you say it but it's true and it's
just like the random the normal the routine for many people for many people so that's one
thing and a second thing is that even for those who manage to attend these conferences
if they are not established in a lab that is in one of these countries it can be very difficult
to do some research some you know interesting and important research because most of what we do
right now requires huge amount of computational resources which are not available in many countries
so we need to do something so you know there are various initiatives to make the community more
open and more inclusive some of which I can name are the women in machine learning and also the
black in AI which we organize this year we also have this initiative called Data Science Africa
which we organize every year it's a summer school where we teach machine learning to local
students chat with very popular so far so they're initiatives but I think we need much more
I also throw in a shout out to the deep learning in Dava which happened in South Africa for the
first time last year and they're planning a follow on this year as well yes yes that that also is
an is an interesting initiative that was very helpful in this direction as well interesting so
and I guess in that way all of the you know I think that that that kind of ties together all of
the various things that you work on that may seem like separate and distinct areas of research
they're not really separate they they fit in what I think as a person I should be doing so I
am committed to build an axiological artificial intelligence and by axiological I mean
an artificial intelligence that is aligned with the value of the society in which it operates
and and to be aligned with the value of the society in which you operate means that you were
aware of the biases that you should avoid but also you were aware of the utility that you can
have the problems that you may be you may solve and also the guarantees of level of safety and
security so that's that's the general scope of what gives me busy right now which brings us back
to that previous comment that I kind of put a bookmarker on and that was kind of the distinction
you made between us as the creators of the AI being aware of bias in data sets and factors like
that versus the models themselves being aware of these things and evolving through training
or design or other things to I guess the way you put it was be selective about the data they
consume can you elaborate on on that and the kind of work you're seeing happen there so
so what I said is that you are what you eat and the data the models are trained on right now is
basically junk food so what I meant is that we do not put a lot of effort into our cells first
looking into this data and seeing what the model what should be there and what should not be there
so for example if you collect the data that that serves to train a model which will be used
worldwide then you make sure that that data is representative of the population worldwide
that's that's that's the basic first step to do I get that part I thought you also maybe this is
where you're getting with the second step I thought you were also suggesting that you're also
exploring ways to teach the models yes to know yes that's that's that's where I'm coming so that's
so the first part is the data part itself actually there is the degree zero which which even comes
from before the the data part the zero the zero step step zero is to make sure that the people
who work on the data and on the model are diverse as possible because only that way they will be
able to notice these biases and of course the first step after that is to make sure that the
data itself is representative of the population but then after that which which a more little
bit more technical question is the models should be in doubt with the ability to select what they
should learn from and what they should not learn from and what what kind of features they should be
invariant of and and this is a this is a less explored territory because there is definitely
some work in the area of making the models invariant to gender or invariant to race etc but I think
we should we should in order to design models that are intelligent but that are free from the
biases we should just take inspiration from ourselves you know we do we humans do some do
introspection basically we when we see a book and we read the book we don't believe everything in
the book right we say this makes sense this doesn't make sense we criticize many of the things
and that's also part of learning but right now that learning is completely positively supervised
which means that when you show when you show an example to a model the the label it is associated
with is exactly what you want to learn from so we should we should end out the model to be able
to to be able to criticize the data it is learning from and to select what it should learn from
according to some criteria and what it should not learn from according to other criteria and
and to me this is an excited an exciting direction research direction which which has not been
much explored I have to say but but it's something that we should work more on are there any examples
of of this even very simple ones that have been explored in the research so recently I published
a paper with a colleague of mine where we try to do something like this we try to we try but but
for a model that is already learned we we try to design a method that allows an operator to take
the model and apply the algorithm to in order to in order to to exhibit the biases of that
that the devices that have been learned by the model but then one could one could imagine that
this algorithm is applied recursively to the model while the model is being trained and that's
where it gets interesting it's like again type of training something like that something like that
not not not necessarily again because in this way is I don't see it really as being invariant
something but but but really as being able to criticize itself and also to to decide which data
it should learn from and which which one it should not learn from so more in an active learning
fashion but it's a bit more complicated than that so so it's it's it's it's an interesting
and challenging research direction I'm very excited about what were the results of that
that paper how far did you get with it so what we thought is some of the results that I mentioned
before so we thought that if you take a state of your computer vision model image classification
model and train it on a most standard data you will see stuff appearing that are not expected
for example you will see that the model considers if you basically if you show it a picture of
Barack Obama it would classify as basketball and we have we have real examples like that in the
paper and if you show it a picture of the president of China it would classify it as ping pong
and and and this can be shocking but it's just it's just some biases that are present in the data
in fact there is something very interesting that we show that actually we found out which is
we we thought that this was due to the imbalance in the in the class basketball for example
which we thought that the class basketball ball only contained pictures with mostly black people
but when we I've assumed this whole time that that's what you're uh that that was what you were
saying actually that's not what happened so yeah so we we we we we we computed the statistics and
what we found out is that the class basketball basically contains as many pictures with white
people as pictures with black people so if you take a picture the probability that there is white
people is roughly the same as a as a as a probability that you see a black person in it but what
happened actually is that if you maybe black people are much you know less much more underrepresented
in your data center so they're more uniquely associated absolutely absolutely so it's this is
it's a mutual information problem so if you look at the if you look at just the first order
observations then you may not really see what's happening but if you look at the problem more
broadly then you see that it's exactly what you what you what you said so that uh even in the
data set black people are more uniquely associated with the class basketball which is a bit problematic
because uh again it say something about how the data set was collected and I'm sure that the
people who collected the data set they did put at the time a lot of care to make uh you know
everything uh as correct as possible but still you can see the type of uh biases that can
arise uh from the data set yeah and so so we're talking about algorithms that can learn to identify
these uh these issues and you mentioned the ping pong and basketball examples in that research
did your algorithm you know were these kind of handpicked examples or did your supervisory
algorithm find these examples within the first model yeah it's it's it's really not handpicked
actually I gave them I gave these examples because it's racial biases and people know what it means
but uh but we found other things for example when for the class traffic light we found out that the
model whenever there is in an image you a blue sky with a bar you know some some black bar in it
the the model will always predict traffic light just because doesn't matter whether there is
traffic light or not it just because in most pictures with traffic light you see the this you
know this middle this iron bar with in a blue sky or something so it's it's really you know
it's really a lot of different biases but I just I just give these two as examples because
people can relate with it and know exactly what that means can you envision like or or at least
I'm envisioning as you're describing this maybe um some kind of meta annotation process where
you know you you you initially annotate your data but then you run and you train a model and then
you kind of apply this algorithm to the model and it shows you all of these biases and then you
have this meta annotation step where folks are identifying whether these biases are valid or
something like that and using that to retrain your model do you envision something like that actually
I will take it one step further I will I will remove even the human in the second loop
if things are done if things are done properly ideally what I would like to see is that a model
learn from the data in an initial step and itself criticizes itself and says I should not have
learned this and that and then goes in a second round of learning and in a be more careful
what it learns from which data point etc and iteratively produces a final model that is at least
um less biased because you never can have a model that is completely free from all biases that's
just not possible um but you can mitigate it and and in my opinion that's something that we
definitely can do this question maybe getting further ahead of you know where you are with this
research but you know given and given this exact any of these examples really but this example of
the blue sky and the bar and the the traffic light the way we train these examples or these models
rather uh you know what what do you envision the mechanism being inside that training loop
that allows the model to even know you know what the thing is that it's supposed to be
uh paying attention to so there are two things here the first is the the learning rule that we're
using so currently most of these models are trained in a completely supervised way so we say hey
this is an image of a traffic light and then the model kind of learn to figure out these things
uh so I think that we should reduce the supervision that's that's one one thing and reducing the
supervision having a learning algorithm that needs less supervision will make definitely the
algorithm more robust because it will have access to much more data which is unsupervised and
it will the labeling process itself is very noisy in general so the the model may be exposed to
less noise and it it may less overfit so if the model is able to to leverage more of the unsupervised
data it it may end up less overfitting so that's one thing the second thing is that
the objective functions that the model is optimizing should be designed in a way
that makes the model avoid learning some biases and that's that's where the that's also one
change that needs to happen in the way we supervise the model so so so I just give you an example so
right now the way we trained if you take a any state of the other image that model it was trained
in examples that have pictures and label and the and the model has no idea how the labels are
related to each other it it ended up somehow figuring it out and I should not even say figuring it
out but it ended up you know producing representations yes representations where cats are
close to dogs etc but it's not really the case so your model can make completely stupid mistakes
so if a cat is misclassified as a dog it's it's a wrong decision but it's still fine because
semantically we know that they are closer to each other than a cat being misclassified as a plane
for example and and you know so far we have not managed to train the models to to really take
into account this decision in order to improve their accuracy and the type of errors they take
so there are a lot of attempts in the literature but in my opinion has not been very convincing
so there is a lot of things to explore in these directions yeah it's it sounds like
there's definitely a lot to explore and furthermore these you know the kind of the direction
that you're proposing is a fairly as dramatic too strong you know a dramatic shift from kind
of the way we train these models today well at some point we just need to to change paradigms and
that's that's how that's how research works I'm not claiming that what I'm saying is the right
thing to do I mean it's probably not but I feel that I observed that we're reaching the limits
of most of the the learning algorithms and the models that we're currently using can offer
and probably what we take us to the next level is not in the box in which we're thinking right now
so we should probably take a step back and and look at the the very principles of the learning
algorithms that we're using and think them in a way that can take us to the next level
awesome awesome well I think that sounds like a good place to wrap up Mustafa thank you so much
for taking the time to chat with us any final words that you'd like to add so thank you very much
for inviting me it has been a pleasure to discuss with you I just would like to mention a last
thing which is there is a lot of research currently in the area of bias and fairness and and
and all of these very interesting and fascinating topics but there is also one thing that in my
opinion is a bit overlooked in the community which is fairness and bias is not only in the models
that we design or the datasets the models learn from it starts with the problems that we consider
if we start by considering problems that are important for a specific population in the world
and just focus on solving these type of problems the end resource is the datasets that we'll be
using and the models that will be that will be learning from these datasets will obviously be biased
so I think as a community we should also open ourselves to consider problems that are of interest
at the global level and I will just give you an example to conclude where there is definitely
it's definitely an exciting direction all the work happening in self-driving cars etc
and a lot of resources is being poured into this and it's important for the for humanity in general
I think but right now this is important for a very very tiny percentage of the population of
this planet and there are challenging challenges out there where as a community we could have a huge
impact but we are still overlooking and we should open ourselves to that and that I just would like
to conclude by mentioning this but thank you very much for inviting me to this podcast I was
very happy to discuss with you absolutely thank you so much thank you all right everyone that's
our show for today remember we want to hear from you on AI in the home and in our personal lives
head on over to twimmalei.com slash my AI to share your thoughts for more information on Mustafa
or any of the topics covered in this episode head on over to twimmalei.com slash talk slash 108
thanks for listening and catch you next time
