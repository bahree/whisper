WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:37.720
I'm your host Sam Charrington.

00:37.720 --> 00:43.120
One of them mentioned that one of the things he misses is giveaways and he's right.

00:43.120 --> 00:47.000
It's been way too long since we've done a giveaway here on the show.

00:47.000 --> 00:53.000
So did he know though that we had one in the works that we're excited to kick off today.

00:53.000 --> 00:57.640
The O'Reilly Artificial Intelligence Conference is returning to New York in April and we

00:57.640 --> 01:02.760
have one free conference pass ready to give away to a lucky listener.

01:02.760 --> 01:10.160
To enter, first go to twimbleai.com slash AI and Y giveaway to access the entry form.

01:10.160 --> 01:14.120
Next, choose any or all of the nine ways to enter.

01:14.120 --> 01:18.400
The more entries you earn, the higher your chances to win.

01:18.400 --> 01:24.400
And finally for three bonus entries, answer the question at the bottom of the entry box.

01:24.400 --> 01:29.120
The giveaway ends soon so be sure to get your entries in while you can.

01:29.120 --> 01:33.120
Now about today's show.

01:33.120 --> 01:38.200
We're joined by Kevin Tran, PhD student in the Department of Chemical Engineering at Carnegie

01:38.200 --> 01:40.200
Mellon University.

01:40.200 --> 01:45.640
Kevin's research focuses on creating and using active learning workflows to perform density

01:45.640 --> 01:51.560
functional theory or DFT simulations, which are used to screen for new catalysts for a

01:51.560 --> 01:54.640
myriad of materials applications.

01:54.640 --> 01:59.760
In our conversation, we explore the challenges surrounding one such application, the creation

01:59.760 --> 02:05.520
of renewable energy fuel cells, which is discussed in his recent nature paper, active learning

02:05.520 --> 02:12.840
across intermetallics to guide discovery of electro catalysts for CO2 reduction and H2

02:12.840 --> 02:14.340
evolution.

02:14.340 --> 02:18.820
We dig into the role and need for good catalysts in this application, the role that quanta

02:18.820 --> 02:23.400
mechanics plays in finding them and how Kevin uses machine learning and optimization

02:23.400 --> 02:26.600
to predict electro catalyst performance.

02:26.600 --> 02:30.200
Okay, let's do it.

02:30.200 --> 02:37.320
All right, everyone, I am on the line with Kevin Tran. Kevin is a PhD student in the Department

02:37.320 --> 02:41.200
of Chemical Engineering at Carnegie Mellon University.

02:41.200 --> 02:43.960
Kevin, welcome to this week in machine learning and AI.

02:43.960 --> 02:46.160
Thank you for having me, Sam.

02:46.160 --> 02:51.720
So Kevin, you're in chemical engineering, you've practiced chemical engineering.

02:51.720 --> 02:55.920
How did you come to get involved in the application of machine learning?

02:55.920 --> 03:00.080
I kind of fell into it with the research that I'm doing.

03:00.080 --> 03:06.960
And so we are looking for new materials and we need an intelligent way to screen them.

03:06.960 --> 03:11.240
And machine learning is actually one of the tools that we found that apparently does

03:11.240 --> 03:12.240
that pretty well.

03:12.240 --> 03:16.520
You're using that in your graduate work, have you used machine learning prior to grad

03:16.520 --> 03:17.520
school?

03:17.520 --> 03:18.520
I have not actually.

03:18.520 --> 03:23.480
So everything that we're doing now is kind of learned on the spot, so to say.

03:23.480 --> 03:27.480
And so it's new for us and we start a lot of room for improvement, but so far it's

03:27.480 --> 03:28.480
working pretty well.

03:28.480 --> 03:29.480
Awesome.

03:29.480 --> 03:33.880
So tell us a little bit about the problem that you're trying to solve there.

03:33.880 --> 03:39.760
Yeah, so at a high level, it's really about sustainable energy, right?

03:39.760 --> 03:43.320
And that's actually the reason that I started doing this research.

03:43.320 --> 03:50.120
And so the idea is to help make what we call solar fuels and solar chemicals.

03:50.120 --> 03:54.080
So what that means is we take energy dry from the sun.

03:54.080 --> 03:56.800
So let's say solar cells.

03:56.800 --> 04:01.680
And one of the problems right now is that when we take solar cells and make electricity

04:01.680 --> 04:07.480
out of that, if we have a lot generated during the day and we can't use it, then it's kind

04:07.480 --> 04:08.480
of wasted, right?

04:08.480 --> 04:12.520
So there are a lot of people doing really nice research into figuring out how to store

04:12.520 --> 04:14.480
that energy.

04:14.480 --> 04:20.720
So the reason is one method, the method that we are interested in is storing it in chemical

04:20.720 --> 04:21.720
bonds.

04:21.720 --> 04:28.000
So what we would do is we would take something like carbon dioxide and water.

04:28.000 --> 04:32.240
Take that and the electricity we get from the sun and convert that into more valuable

04:32.240 --> 04:36.960
fuels or chemicals like methane or ethylene or hydrogen.

04:36.960 --> 04:41.800
And then in turn, we can use those for whatever application that we want.

04:41.800 --> 04:48.880
And so part of that problem to actually turn those chemicals into more valuable chemicals

04:48.880 --> 04:52.280
is that we need good catalyst to do so.

04:52.280 --> 04:58.600
And right now, our research focuses on finding good catalyst to do those reactions.

04:58.600 --> 05:03.880
And it involves a lot of computer simulations that we have to do intelligently.

05:03.880 --> 05:08.640
And so we're using machine learning to help us decide which simulations to do it early.

05:08.640 --> 05:15.080
OK, so to dig into that a little bit more, you're trying to store solar energy and chemical

05:15.080 --> 05:16.080
bonds.

05:16.080 --> 05:19.120
And some ways that sounds a little bit like what a battery is doing also.

05:19.120 --> 05:22.440
Yeah, it's fundamentally a chemical thing.

05:22.440 --> 05:29.640
Now, or you could, you know, while the sun is up, use the solar energy to kind of power

05:29.640 --> 05:37.960
some chemical reaction and that's kind of where your need for some kind of catalyst comes

05:37.960 --> 05:38.960
into play.

05:38.960 --> 05:40.720
Yeah, yeah, exactly.

05:40.720 --> 05:46.800
So the setup we have actually looks very similar to a battery, right, or the physical setups.

05:46.800 --> 05:54.400
And so if you think of a car battery, right, it has a solution and it has an anode and

05:54.400 --> 05:58.480
a cathode as in more or less two metal rods on either end.

05:58.480 --> 06:04.680
And it can either generate electricity or it can, you can put energy electricity into

06:04.680 --> 06:10.280
it to store the energy in more or less chemical form.

06:10.280 --> 06:16.640
And so our system, what it does is instead of storing the energy in the battery itself,

06:16.640 --> 06:23.120
it generates methane and fuels from carbon dioxide and the water that is in the battery itself

06:23.120 --> 06:26.200
or in this case, it's the electrical chemical cell.

06:26.200 --> 06:29.360
You just described it as an electrical chemical cell.

06:29.360 --> 06:30.360
Yes.

06:30.360 --> 06:36.720
And so it's not like you could use that energy to power some kind of machine to, you know,

06:36.720 --> 06:42.080
extract some other compound or fuel, but rather you're doing this all chemically.

06:42.080 --> 06:43.080
Yes.

06:43.080 --> 06:44.080
Yep.

06:44.080 --> 06:45.080
That's the idea.

06:45.080 --> 06:49.160
The issue there is in order to perform that reaction and a large scale that could help

06:49.160 --> 06:50.760
a lot of people.

06:50.760 --> 06:54.800
We needed to be fast and efficient and affordable.

06:54.800 --> 06:59.040
And the catalyst that we have right now can meet some of those criteria, but not a lot

06:59.040 --> 07:00.040
of them.

07:00.040 --> 07:04.760
And so we're looking for more materials that we can scale up to a commercial scale.

07:04.760 --> 07:06.720
And that's the problem we have right now.

07:06.720 --> 07:10.160
In order to do this, you need these catalysts.

07:10.160 --> 07:14.040
How do you measure the performance of one of these catalysts?

07:14.040 --> 07:15.040
Yes.

07:15.040 --> 07:20.760
So there are a lot of metrics for performance that experimentals usually look at, one of

07:20.760 --> 07:26.160
which we call activity, but the idea is how fast it can actually drive the reaction.

07:26.160 --> 07:34.720
So let's say one catalyst can transform one kilojoule of energy and it would take maybe

07:34.720 --> 07:37.920
a few seconds and another catalyst, it could take hours.

07:37.920 --> 07:42.600
So we want the catalyst that could take a few seconds to actually do that reaction.

07:42.600 --> 07:46.360
Another example is the efficiency.

07:46.360 --> 07:51.400
So just because you put one kilojoule of energy in does not mean you get one kilojoule out.

07:51.400 --> 07:52.920
It's often much less than that.

07:52.920 --> 07:59.240
And so certain catalysts are more efficient about transferring that energy than others.

07:59.240 --> 08:03.560
And there's often a balance between those two and even all the properties such as how

08:03.560 --> 08:05.640
expensive the material is, right?

08:05.640 --> 08:09.400
And so there's a lot of things that we plan to play with and look at.

08:09.400 --> 08:13.320
But the work we have right now, since we're just starting out, only looks at the first

08:13.320 --> 08:17.080
thing that I mentioned, which is how fast the reaction can go.

08:17.080 --> 08:20.280
What's the space of possible catalysts look like?

08:20.280 --> 08:21.680
It's a good question.

08:21.680 --> 08:27.320
So one of the issues that the field has right now is that in order to solve this problem,

08:27.320 --> 08:30.800
we're more or less looking at most of the periodic table, right?

08:30.800 --> 08:35.200
And so there are a lot of elements there, a lot of iterations that go through.

08:35.200 --> 08:40.120
And even if you choose, let's say, two metals, they stole the question of what ratio do

08:40.120 --> 08:42.400
you have between those two metals, right?

08:42.400 --> 08:45.160
What if you have three metals, what ratios do you have there?

08:45.160 --> 08:52.280
And so what we have is a really rough first pass of looking at two or three metal common

08:52.280 --> 08:58.240
issues, sometimes four, for about 30 different elements on the periodic table for select

08:58.240 --> 09:01.600
combinations of ratios between them.

09:01.600 --> 09:05.200
And so in that sense, it sounds kind of like a constrained optimization type of a problem.

09:05.200 --> 09:11.920
You're trying to figure out how to get the greatest yield from this set of materials that

09:11.920 --> 09:12.920
you're working with.

09:12.920 --> 09:14.560
Yeah, exactly.

09:14.560 --> 09:20.800
And it's kind of a difficult problem because the search space explodes geometrically as

09:20.800 --> 09:23.200
you add more materials, right?

09:23.200 --> 09:26.240
And more composition blends.

09:26.240 --> 09:30.800
And so what we have is even though it feels large and seems large to us, it's really the

09:30.800 --> 09:33.800
tip of the iceberg of what we could do, right?

09:33.800 --> 09:37.040
At what level are you modeling this?

09:37.040 --> 09:42.640
Like are you modeling this at and forgive my ignorance here if I'm not asking this correctly?

09:42.640 --> 09:48.920
And are you kind of when you're searching for high yield catalysts?

09:48.920 --> 09:55.480
Are you modeling atomic interactions, subatomic interactions and molecular interactions?

09:55.480 --> 09:59.320
Or could you do all of this, you know, what you need at the level of, you know, just kind

09:59.320 --> 10:01.880
of the things you might see on a periodic table?

10:01.880 --> 10:02.880
Yeah, yeah.

10:02.880 --> 10:09.320
So what we are doing specifically is modeling things at the atomic level.

10:09.320 --> 10:16.080
And so there's a type of theory in chemistry, it's called density functional theory.

10:16.080 --> 10:22.640
So what this does, it more or less uses quantum mechanics to take a set of atoms and predict

10:22.640 --> 10:25.480
the properties of those atoms.

10:25.480 --> 10:31.560
And using those properties, those are actually indicative of the performance of the catalyst.

10:31.560 --> 10:37.920
And so we take a catalyst, we perform destiny functional theory simulations really around

10:37.920 --> 10:39.560
how the atoms interact with each other.

10:39.560 --> 10:45.480
And that will tell us if that catalyst or those atoms really will perform well in electrochemical.

10:45.480 --> 10:46.480
So.

10:46.480 --> 10:50.280
And so how long do the simulations take to run?

10:50.280 --> 10:55.240
The issue here is that these simulations take anywhere from hours to days or the big

10:55.240 --> 10:58.120
ones sometimes even weeks to run a single one.

10:58.120 --> 10:59.120
Oh, wow.

10:59.120 --> 11:04.360
Can you give us a high level understanding of how these DFT simulations work?

11:04.360 --> 11:10.280
What are they doing to determine ultimately whether these catalysts are going to be high

11:10.280 --> 11:11.280
yield?

11:11.280 --> 11:12.280
Yeah, yeah.

11:12.280 --> 11:17.720
So to answer that, we can back up a second to talk about how the catalysts work.

11:17.720 --> 11:23.040
So let's take for example, we want to turn carbon dioxide into methane, right?

11:23.040 --> 11:27.120
So it's CO2 going to CH4.

11:27.120 --> 11:28.280
That doesn't happen in one step.

11:28.280 --> 11:36.400
That happens in a series of chemical reactions and we know from chemistry experts that the

11:36.400 --> 11:40.160
elementary reaction we call it, they're a smaller reaction that matters most to convert

11:40.160 --> 11:48.120
CO2 to methane is actually adding one hydrogen onto carbon monoxide.

11:48.120 --> 11:53.480
And so what that means is how strongly that carbon monoxide binds onto the catalyst is

11:53.480 --> 11:54.720
really important.

11:54.720 --> 12:00.080
So let's say carbon monoxide binds onto the catalyst very strongly.

12:00.080 --> 12:01.800
So it will react quickly.

12:01.800 --> 12:06.040
But once it's done reacting, it'll actually just stay there, never come off and you will

12:06.040 --> 12:08.320
never actually have any product.

12:08.320 --> 12:13.360
Conversely, if carbon monoxide binds to weekly, it will never go on to the catalyst in

12:13.360 --> 12:16.640
the first place and therefore it will never react.

12:16.640 --> 12:21.120
So there's kind of a Goldilocks effect where the catalyst needs to have a sweet spot of

12:21.120 --> 12:22.800
energy in the middle.

12:22.800 --> 12:25.360
So that's where density functional theory comes in.

12:25.360 --> 12:27.200
We call it DFT.

12:27.200 --> 12:33.320
And so given a configuration of atoms and how the carbon monoxide is sitting on a surface

12:33.320 --> 12:40.480
of a catalyst, DFT can tell us how strongly that carbon monoxide is sticking to the surface.

12:40.480 --> 12:43.240
And that is indicative of how well it's going to perform.

12:43.240 --> 12:50.280
Presumably, you're using a simulator because the laws that govern this are either too

12:50.280 --> 12:59.120
ill-defined to apply directly or are at too granular a level to apply directly.

12:59.120 --> 13:00.120
That's interesting.

13:00.120 --> 13:05.880
So when we call them simulations, we just say that because it's what it feels like.

13:05.880 --> 13:10.680
But what we're actually using is quantum mechanics and so we know the laws that govern how everything

13:10.680 --> 13:11.680
interacts.

13:11.680 --> 13:17.000
The reason that takes so long is because it's really just a math problem solving a lot

13:17.000 --> 13:20.080
of partial differential equations simultaneously.

13:20.080 --> 13:22.320
And that just takes a really long time.

13:22.320 --> 13:29.640
You recently published a paper on your work in this field in nature, nature catalysis.

13:29.640 --> 13:33.880
And you're using active learning to help solve this problem.

13:33.880 --> 13:36.320
Where does active learning come into play?

13:36.320 --> 13:37.320
Yeah.

13:37.320 --> 13:41.360
So we have a set of elements that we want to look at, right?

13:41.360 --> 13:45.880
And they can have different varying composition space as we thought about before.

13:45.880 --> 13:52.360
And we want to actually keep running simulations to keep finding new materials for experimental

13:52.360 --> 13:54.440
list to test out.

13:54.440 --> 13:59.000
So our whole workflow is designed around the idea of we want to continuously find new

13:59.000 --> 14:01.760
materials for other people to look at.

14:01.760 --> 14:06.560
So that's where the active part comes in because it feels almost in there, right?

14:06.560 --> 14:12.000
So the more simulations we have, the more data we have, and the more data we have, the

14:12.000 --> 14:17.520
more we get a better idea of how to what sites to perform next.

14:17.520 --> 14:24.160
So in this case, what we're doing is, let's say we start out with a small database of a

14:24.160 --> 14:26.400
few hundred simulations.

14:26.400 --> 14:32.240
We use machine learning on those as a training set.

14:32.240 --> 14:37.840
And from there we can predict how well the other catalyst that we have not simulated are,

14:37.840 --> 14:39.840
how well they will perform.

14:39.840 --> 14:44.960
And once we have an idea of that, we can pick the ones that we think will perform well

14:44.960 --> 14:50.400
as per the machine learning model, and actually just do the simulations, get more data,

14:50.400 --> 14:53.520
and then perform their questions again, and just continue that loop.

14:53.520 --> 14:57.760
And that's how we're using active learning in a sense to find new materials.

14:57.760 --> 15:03.560
You've got this database of materials that you've run these simulations on, and they're

15:03.560 --> 15:08.080
characterized by their different kind of material properties.

15:08.080 --> 15:12.240
Well, actually, maybe you can talk a little bit about how they're characterized.

15:12.240 --> 15:16.720
Are they, you know, is it strictly at the atomic level or are there other material

15:16.720 --> 15:21.480
properties that you're using to ultimately become features in your machine learning

15:21.480 --> 15:22.480
models?

15:22.480 --> 15:31.240
Yeah, so what we do is we look at the location of where the carbon monoxide is sitting

15:31.240 --> 15:32.800
on the surface.

15:32.800 --> 15:37.880
So if you can imagine how the atoms are set up, the carbon monoxide could be bonded to,

15:37.880 --> 15:44.080
let's say, one plinum atom on a surface, and that plinum atom could be bound to maybe

15:44.080 --> 15:46.000
eight other plinum atoms.

15:46.000 --> 15:54.080
So what we feed as features to our aggression model is how many atoms that the carbon monoxide

15:54.080 --> 15:55.080
is bound to?

15:55.080 --> 15:57.440
So in this case, it'll be one plinum.

15:57.440 --> 16:03.240
How many atoms that are in the next nearest neighbor or the next outer shell?

16:03.240 --> 16:06.120
So in this case, it'll be eight plinums.

16:06.120 --> 16:09.400
We also look at the identity, the chemical identity of those atoms.

16:09.400 --> 16:11.080
So in this case, it'll be platinum.

16:11.080 --> 16:16.600
If there's more than one metal, we consider, let's say, this plinum and aluminum.

16:16.600 --> 16:18.240
We consider the aluminum too.

16:18.240 --> 16:20.760
We also look at the chemical properties of those elements.

16:20.760 --> 16:26.880
So platinum and aluminum have what we call electronegativity, which is how much they

16:26.880 --> 16:28.760
like electrons.

16:28.760 --> 16:30.440
And we look at that.

16:30.440 --> 16:34.720
And we also look at what we call the atomic number.

16:34.720 --> 16:37.640
So where it is on the periodic table.

16:37.640 --> 16:44.880
And one of the interesting features that we have is the average binding energy of that

16:44.880 --> 16:47.160
absorbate on the material.

16:47.160 --> 16:52.960
So in this sense, let's say we already have a database of maybe 20 calculations of platinum.

16:52.960 --> 16:56.880
And we take an average of those binding energies.

16:56.880 --> 16:59.400
And that number is associated with platinum.

16:59.400 --> 17:03.200
It kind of gives a feel for how strongly it binds the palatine in general.

17:03.200 --> 17:07.200
We extend, we take that number and we feed it into our alloys.

17:07.200 --> 17:13.640
And so when we have a material that has aluminum platinum, it has a feel for how strongly

17:13.640 --> 17:15.440
it binds the platinum and aluminum.

17:15.440 --> 17:19.040
And it sort of averages those together in a way when we do the regression.

17:19.040 --> 17:20.040
Okay.

17:20.040 --> 17:24.520
And that's that number that you described earlier is wanting to have that Goldilocks effect.

17:24.520 --> 17:26.720
That one needs to be kind of just right.

17:26.720 --> 17:27.720
Exactly.

17:27.720 --> 17:32.680
And so you've got all of these as features that come out of the materials that you're

17:32.680 --> 17:42.040
using, the simulations themselves are those only used as your, your labels, your answers

17:42.040 --> 17:47.000
or are you also gaining information from those simulations that you're using as feature

17:47.000 --> 17:49.520
input?

17:49.520 --> 17:57.720
So the simulations that we're performing effectively give us the labels for our regression models.

17:57.720 --> 18:05.400
At the same time, the, so those labels are the binding energy of carbon dioxide on a

18:05.400 --> 18:12.800
particular site on a catalyst, a site being a location where the carbon dioxide could

18:12.800 --> 18:15.520
just attach onto the catalyst.

18:15.520 --> 18:20.720
I was mainly curious whether the simulation was just giving you that, you know, that answer

18:20.720 --> 18:27.280
that binding energy that you use as labels or if the simulation was also kind of illuminating

18:27.280 --> 18:35.440
other characteristics of the, the material pairs or combinations, you know, they're qualitatively

18:35.440 --> 18:40.320
or quantitatively that you could also use as, you know, input signal.

18:40.320 --> 18:41.320
Sure.

18:41.320 --> 18:44.520
So the simulations themselves really just give us the labels.

18:44.520 --> 18:50.600
If we want to look for more holistic information around what materials might work well, we actually

18:50.600 --> 18:53.320
just look at our database of simulations.

18:53.320 --> 18:59.040
So I guess for each individual one, we don't really clean much chemical information from

18:59.040 --> 19:05.880
that for from seeing patterns arise in how many simulations work well given a certain

19:05.880 --> 19:09.560
alloy, we clean information from there.

19:09.560 --> 19:16.560
You've got this regression model that essentially allows you to take unknown combinations of

19:16.560 --> 19:24.240
materials and guess what the binding energy will be without running through these really

19:24.240 --> 19:29.280
long simulations, is that that kind of the general goal there?

19:29.280 --> 19:30.280
Yeah.

19:30.280 --> 19:31.280
Yeah.

19:31.280 --> 19:32.280
Exactly.

19:32.280 --> 19:33.880
So that's what our surrogate model is doing, our machine learning model.

19:33.880 --> 19:34.880
Yep.

19:34.880 --> 19:41.400
And then you're using that to allow you to kind of constrain the search space for new elements.

19:41.400 --> 19:46.760
How do you tie that surrogate model into the active learning piece or the piece that you're

19:46.760 --> 19:49.360
using to constrain your search space?

19:49.360 --> 19:50.360
Yeah.

19:50.360 --> 19:57.600
So the very naive way to do this is to take the model, evaluate across our entire search

19:57.600 --> 20:04.000
space and pick the materials and the adoption sites that actually would work best.

20:04.000 --> 20:05.000
But there's the idea of.

20:05.000 --> 20:10.760
And if I can interrupt that, sure, should even be an improvement over running these simulations

20:10.760 --> 20:11.760
every time.

20:11.760 --> 20:12.760
Right?

20:12.760 --> 20:13.760
Yeah.

20:13.760 --> 20:16.840
It's an intelligent way of doing the simulations, right?

20:16.840 --> 20:21.480
So what people do in the current field is they use their intuition to decide what

20:21.480 --> 20:23.720
materials to simulate next.

20:23.720 --> 20:24.720
We're here.

20:24.720 --> 20:31.280
We're actually using machine learning predictions to make that decision instead of human intuition.

20:31.280 --> 20:36.400
How well do you trust the surrogate model when you run it and it tells you that something

20:36.400 --> 20:40.200
should have this kind of just right bonding energy?

20:40.200 --> 20:45.600
Do you trust it or do you use that to determine what you should actually simulate?

20:45.600 --> 20:47.240
So that's a really good question.

20:47.240 --> 20:51.320
At the current state of the model, I trust it with a grain of salt, right?

20:51.320 --> 20:52.920
Does that make sense?

20:52.920 --> 20:57.160
But what I really trust are the simulations.

20:57.160 --> 21:02.760
And so the whole point of our workflow isn't necessarily to make machine learning model

21:02.760 --> 21:05.200
that can predict everything perfectly.

21:05.200 --> 21:10.280
This is really to have the machine learning model decide what simulations to run next,

21:10.280 --> 21:13.640
which will build our database in an intelligent way.

21:13.640 --> 21:14.640
Right.

21:14.640 --> 21:15.640
Right.

21:15.640 --> 21:21.120
And so you were starting to walk through that process and how you applied the machine learning

21:21.120 --> 21:22.880
given your surrogate model.

21:22.880 --> 21:23.880
Yeah.

21:23.880 --> 21:24.880
Yeah.

21:24.880 --> 21:28.560
And so I think we discussed the naive way to do it, which is to use the model to find the

21:28.560 --> 21:31.880
best candidate and to just run that.

21:31.880 --> 21:36.880
But if you do that, you run in, there's the idea of exploration versus exploitation.

21:36.880 --> 21:38.320
I'm not sure if you're familiar with.

21:38.320 --> 21:39.320
Mm-hmm.

21:39.320 --> 21:40.760
Yeah, we talk about that quite a bit on the show.

21:40.760 --> 21:41.760
Yeah, yeah.

21:41.760 --> 21:43.160
And so that's pure exploitation.

21:43.160 --> 21:49.520
And so there's a decent amount of research into ways to balance the two.

21:49.520 --> 21:56.600
For our workflow, we had a quick and dirty way to try and balance this where we actually

21:56.600 --> 22:01.800
made a Gaussian distribution that is centered at our optimal point, right?

22:01.800 --> 22:07.920
So let's say our optimal point is maybe 0.1 for the binding energy.

22:07.920 --> 22:12.960
So we made a Gaussian distribution centered there with a certain standard deviation.

22:12.960 --> 22:19.640
And for our search space, we used that distribution to assign a probability of how of selecting

22:19.640 --> 22:22.680
a new candidate material to simulate.

22:22.680 --> 22:28.320
So let's say if something was at 0.1 or the model thought it was exactly at 0.1 or optimal,

22:28.320 --> 22:32.000
then it would assign the highest probability to picking that.

22:32.000 --> 22:35.720
And the further out you go from that space, the lower the probability gets.

22:35.720 --> 22:41.160
And so once we assign an array of probabilities to our possible search space, then we just

22:41.160 --> 22:43.000
picked them at random.

22:43.000 --> 22:47.720
So in a way, we would focus on the area that we're interested in.

22:47.720 --> 22:52.720
But still, in a way, stochastically choose other materials that we might not normally

22:52.720 --> 22:53.720
choose.

22:53.720 --> 22:58.880
Just retaining some of that explore characteristic.

22:58.880 --> 22:59.880
Exactly.

22:59.880 --> 23:00.880
Yeah.

23:00.880 --> 23:01.880
Okay.

23:01.880 --> 23:05.120
And then the active learning piece of this is what specifically?

23:05.120 --> 23:13.000
So the active learning piece is the process of iteratively deciding new points to simulate

23:13.000 --> 23:14.480
for us, right?

23:14.480 --> 23:17.160
And so we have this loop constantly going.

23:17.160 --> 23:24.080
So the active part is the fact that we're doing a regression and then a prediction and

23:24.080 --> 23:29.680
then a query or simulation, in our case, to get us more data, which yields another

23:29.680 --> 23:31.320
regression, et cetera, et cetera.

23:31.320 --> 23:35.160
So the active part is the iterative nature of our workflow.

23:35.160 --> 23:40.160
So you're running this kind of in serial.

23:40.160 --> 23:50.600
And then each time you produce this distribution of possible materials to simulate based on

23:50.600 --> 23:55.240
the tests that you've done previously, you pick one, you run that.

23:55.240 --> 24:01.240
And then you, based on the result, you put that in your distribution, you pick another

24:01.240 --> 24:09.280
one, you run that and you keep feeding these results back into your database, your distribution

24:09.280 --> 24:10.880
of things to try.

24:10.880 --> 24:11.880
Exactly.

24:11.880 --> 24:12.880
Exactly.

24:12.880 --> 24:13.880
Very cool.

24:13.880 --> 24:14.880
Very cool.

24:14.880 --> 24:20.520
And so how do you characterize the results that you saw with the paper?

24:20.520 --> 24:21.520
Yeah.

24:21.520 --> 24:25.240
And so that's an interesting question because we've been playing with around with different

24:25.240 --> 24:29.200
methods to actually analyze our database.

24:29.200 --> 24:35.280
So we, our main three methods that we thought of, we actually put inside the paper, one

24:35.280 --> 24:44.360
is to just make a list of materials that are within an acceptable range of performance

24:44.360 --> 24:45.560
and give that to experimentalists.

24:45.560 --> 24:48.360
And so we have that in the paper.

24:48.360 --> 24:55.360
Another way is to create a, almost a heat map, a two-dimensional heat map of what materials

24:55.360 --> 24:56.640
would work well.

24:56.640 --> 24:59.320
So on one axis would be one set of elements.

24:59.320 --> 25:04.600
So Luminon, Platinum, Gold, Copper, and the other axis is those same elements.

25:04.600 --> 25:09.680
And so each point would be a blend of those elements.

25:09.680 --> 25:13.640
So the grid would be maybe choose Copper and Luminon.

25:13.640 --> 25:22.480
And from there we can color code the grid according to the fraction of the binding sites that

25:22.480 --> 25:23.760
would work well.

25:23.760 --> 25:26.520
And so that was another way that we could look at it.

25:26.520 --> 25:32.440
And the third way is actually simply using Teasney to cluster all of the points that

25:32.440 --> 25:38.600
we've done simulations on, and then color code them by their energy and look for clusters

25:38.600 --> 25:44.240
that have the appropriate energy for us and look for the themes within those clusters

25:44.240 --> 25:47.560
and recommend that those materials be studied.

25:47.560 --> 25:54.280
And so you've got these three methods, how do you know that this method is helping you?

25:54.280 --> 26:02.400
Are you finding, are you able to, you know, ultimately is it the number of kind

26:02.400 --> 26:08.760
of candidate combinations that you're able to find per unit of your own time as a researcher

26:08.760 --> 26:12.200
or is there some other kind of fundamental measure here?

26:12.200 --> 26:14.600
Yeah, that's a good question.

26:14.600 --> 26:20.520
The two that I kind of lean towards are the number of candidates we find over time.

26:20.520 --> 26:23.640
So we actually have a plot of that in the paper as well.

26:23.640 --> 26:28.240
And over time we can see that number rising and when I was getting a weaning according

26:28.240 --> 26:32.080
to the things the way we modify our workflow.

26:32.080 --> 26:37.640
The other way is really, I guess the real way is to see if any of the candidates we have

26:37.640 --> 26:42.080
actually work and in an experimental setup.

26:42.080 --> 26:49.240
And so for that case, experimental validation, we do have collaborators that are testing

26:49.240 --> 26:55.280
some of these materials and we actually have another paper in review right now where

26:55.280 --> 27:01.880
our collaborators tested one of the things that we've suspected to be performing well.

27:01.880 --> 27:02.880
And it turns out it did.

27:02.880 --> 27:05.320
And so that worked out perfectly in our minds.

27:05.320 --> 27:07.640
And so we're really excited to get that paper out.

27:07.640 --> 27:11.160
And you might be seeing that in the next few months or so.

27:11.160 --> 27:12.160
Nice.

27:12.160 --> 27:15.720
And it doesn't sound like it would be enough to just demonstrate one.

27:15.720 --> 27:21.000
There's some notion of kind of demonstrating that this is a, you know, more efficient or

27:21.000 --> 27:27.120
more repeatable process than, you know, what folks usually do.

27:27.120 --> 27:34.560
And that goes back to have you have you produced enough candidates and had those go through

27:34.560 --> 27:39.520
the experimental process in order to be able to make claims around that.

27:39.520 --> 27:40.520
Yeah.

27:40.520 --> 27:44.920
So this is the candidates that we found so far.

27:44.920 --> 27:46.720
We found them relatively recently.

27:46.720 --> 27:52.920
So our, we are still in the process of testing them with experimental collaborators.

27:52.920 --> 27:58.240
One thing that does make us feel better about our workflow is that I'd say on the order

27:58.240 --> 28:03.760
of 40 to 60% of the candidates that we have found have already been shown in the literature

28:03.760 --> 28:10.200
to work well for what we're looking for, but ended up not being used for other reasons

28:10.200 --> 28:11.680
that we're not looking at right now.

28:11.680 --> 28:15.400
And so that kind of gives us a gut check to say that, hey, the things that our workflow

28:15.400 --> 28:18.880
are finding are things that people have already looked at.

28:18.880 --> 28:24.520
And so hopefully the things that we haven't experimentally tested yet might be good candidates

28:24.520 --> 28:25.520
too.

28:25.520 --> 28:26.520
Oh, that's right.

28:26.520 --> 28:32.880
Because you're only looking at one of many important categories that you need to, or

28:32.880 --> 28:37.840
properties that you need to consider to actually commercialize something.

28:37.840 --> 28:46.640
So your workflow is spinning out candidates that according to your criteria are good ones.

28:46.640 --> 28:51.440
But you know, may not, you know, may have already been proven to be not commercially viable

28:51.440 --> 28:53.480
for other reasons.

28:53.480 --> 28:54.480
Exactly.

28:54.480 --> 28:55.480
Exactly.

28:55.480 --> 28:58.720
And so that's where some of our research might be going in the future to start looking

28:58.720 --> 29:03.600
at those other properties as well so that we can really trim down our list and reduce

29:03.600 --> 29:07.880
the chances of giving a bad recommendation to an experimentalist.

29:07.880 --> 29:14.600
And so how do you envision scaling your workflow to these multiple criteria?

29:14.600 --> 29:18.280
So that's something that we're actually looking to now.

29:18.280 --> 29:20.360
And we're still hoping that out.

29:20.360 --> 29:28.120
But the rough idea is to find the Pareto front of multiple objectives and to try and balance

29:28.120 --> 29:31.440
where we want to be on that Pareto front, if you're familiar with that term.

29:31.440 --> 29:32.440
A library on that.

29:32.440 --> 29:34.680
How do you go about doing that practically?

29:34.680 --> 29:35.680
Yeah.

29:35.680 --> 29:42.160
So practically, we would first have to find another property that we'd be interested

29:42.160 --> 29:49.360
in, let's say stability of the catalyst or how long it'll stay there because if a catalyst

29:49.360 --> 29:53.840
operates for a few hours and then degrades into something else and stops working, then

29:53.840 --> 29:55.600
that's not a good catalyst, right?

29:55.600 --> 29:59.640
So that's one of the things that we're going to be looking into in the future.

29:59.640 --> 30:05.720
And our simulation, our simulations can actually get a handle on how stable something is.

30:05.720 --> 30:10.320
And so what we would do is we would have a metric for what we call activity or how fast

30:10.320 --> 30:13.440
the reaction goes and we would have a metric for stability.

30:13.440 --> 30:17.360
And for all the candidates that we want to look at, we could evaluate how well it performs

30:17.360 --> 30:19.080
in each.

30:19.080 --> 30:20.920
And I mean, there's no free lingerie.

30:20.920 --> 30:26.440
And so each candidate is probably going to perform well on one and maybe not for the other.

30:26.440 --> 30:36.040
And so what we call a Pareto front is where let's say you have a certain activity, right?

30:36.040 --> 30:40.040
And you find, let's say you want to have the ideal activity of point one.

30:40.040 --> 30:44.120
We can find the candidates that have that idea, that ideal activity, but also are the

30:44.120 --> 30:45.120
most stable.

30:45.120 --> 30:48.400
And so a lot of give us one answer for that particular activity.

30:48.400 --> 30:52.520
And then we can take a step to say, okay, let's look around materials that have an activity

30:52.520 --> 30:59.480
of point two a little further away and of that, find the subset that are the most stable.

30:59.480 --> 31:05.760
And then we can continue that across the spectrum of activities and find a list of materials

31:05.760 --> 31:10.360
that are active and generally more stable than other candidates.

31:10.360 --> 31:12.080
And we would still get a list from there.

31:12.080 --> 31:17.080
But using that, that we could down select our material set even further.

31:17.080 --> 31:21.520
What else do you plan on doing the kind of further this research line?

31:21.520 --> 31:22.520
Yeah.

31:22.520 --> 31:27.800
So other things we're looking at are more intelligent ways to select the next experiments

31:27.800 --> 31:29.760
or the next simulations.

31:29.760 --> 31:36.880
And so what I told you was the Gaussian selection.

31:36.880 --> 31:41.600
So right now we're actually having conversations with machine learning people at Carnegie Mellon

31:41.600 --> 31:49.880
University to figure out what type of algorithms might be best suited for our application.

31:49.880 --> 31:51.360
So there's that.

31:51.360 --> 31:55.880
And another way is to actually simply improve their regression methods that we're using

31:55.880 --> 32:01.320
right now to see if we can get better serial models that can be more intelligent about

32:01.320 --> 32:05.640
what this light didn't have a better prediction rate.

32:05.640 --> 32:12.840
With the first of those kind of an alternative to Gaussian selection is there some intuition

32:12.840 --> 32:19.000
that you're pursuing as to what methods might be better or what might the characteristics

32:19.000 --> 32:20.560
of better methods be.

32:20.560 --> 32:21.560
Yeah.

32:21.560 --> 32:24.880
So again, this is something that we're just starting to dabble in.

32:24.880 --> 32:29.880
So we haven't flushed the ideas out, but a lot of the active learning literature that

32:29.880 --> 32:36.120
we've seen so far is really centered around Bayesian prediction and processes.

32:36.120 --> 32:42.520
And so that's something that we might look into to model our system with and maybe start

32:42.520 --> 32:44.800
selecting new candidates with.

32:44.800 --> 32:53.320
And on the second direction that you mentioned, improved models, what are you thinking there

32:53.320 --> 32:58.320
or what directions are you looking at?

32:58.320 --> 32:59.320
Yeah.

32:59.320 --> 33:02.680
So that kind of has an interaction with other things we're looking at.

33:02.680 --> 33:09.120
Like I said, if we end up using Bayesian statistics to do the active learning, well then that

33:09.120 --> 33:11.960
means our regression is going to be Bayesian based.

33:11.960 --> 33:17.560
If we don't end up doing that, I've toyed with the idea of actually using neural networks

33:17.560 --> 33:19.280
to do these predictions.

33:19.280 --> 33:22.600
But there's a decent amount of overhead work that we need to go into that.

33:22.600 --> 33:25.960
So we're still thinking about that, not sure if we want to go that direction.

33:25.960 --> 33:31.360
But using neural networks is probably, if we choose to go that route, probably what

33:31.360 --> 33:32.360
we would do.

33:32.360 --> 33:33.360
Okay.

33:33.360 --> 33:39.920
And when you say overhead, what are you specifically speaking of computational or the

33:39.920 --> 33:45.680
effort that goes into just learning, you know, how to build out the models or what?

33:45.680 --> 33:53.400
I would say the effort going into building the models because the way in which we turn

33:53.400 --> 33:57.480
our system into features really matters a lot.

33:57.480 --> 34:01.840
And that's really very active area of research in the field right now.

34:01.840 --> 34:06.720
But it requires a lot of intuition, a lot of luck, and a lot of time to fit these models

34:06.720 --> 34:10.680
well, especially when we're looking at search spaces as large as ours.

34:10.680 --> 34:15.880
Well, Kevin, thanks so much for taking some time to share with us what you're working

34:15.880 --> 34:16.880
on.

34:16.880 --> 34:17.880
It's really cool stuff.

34:17.880 --> 34:18.880
Of course.

34:18.880 --> 34:19.880
You're welcome.

34:19.880 --> 34:24.960
All right, everyone, that's our show for today.

34:24.960 --> 34:31.000
For more information on Kevin or any of the topics covered in this show, visit twimmelai.com

34:31.000 --> 34:34.000
slash talk slash 238.

34:34.000 --> 34:41.760
Remember to enter our AI conference ticket giveaway at twimmelai.com slash AI and Y giveaway.

34:41.760 --> 35:10.280
And of course, as always, thanks so much for listening and catch you next time.

