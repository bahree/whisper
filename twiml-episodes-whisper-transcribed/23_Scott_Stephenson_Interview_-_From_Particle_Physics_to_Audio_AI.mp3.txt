Hello and welcome to another episode of Twymilthalk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Once again, thanks to everyone who sent in their favorite quote from a recent podcast.
Another batch of stickers got mailed out this week, and please keep those quotes coming.
We've had a blast getting your perspective from each talk.
If you're new to the Twymil family, you too can get a sticker.
Just send us a comment via the show notes page, tweet us at Twymilai or at Sam Charrington
or share your quote via a post on our Facebook page.
Before we jump into today's show, which I'm sure you're going to enjoy, I'd like to take
a moment to remind you all about my upcoming event, the Future of Data Summit, which will
be held May 15th and 16th in Las Vegas, Nevada.
I'm really looking forward to the summit, and I hope you can join me there.
You'll hear from industry leaders and technology users on how they're taking advantage of
emerging data-centric technologies like IoT, blockchain, deep learning, and more.
You'll learn a ton.
I'm continuing to add new speakers to the lineup, including Jennifer Prinkey from Walmart
Labs, who will be joining us to talk about what she calls data mixology.
Jennifer currently leads a team of data scientists and engineers working on improving the online
experience of the Walmart customer by integrating Walmart's stores data with e-commerce data.
She also manages the metrics and measurements team there, a group in charge of creating
metrics to measure the impact of all new features and algorithms, as well as solutions to
automatically control and manage all machine learning algorithms in production.
So we'll be talking about the importance of finding and fusing data to making machine
learning predictions in production and describing the evolving platform that Walmart's put
in place to facilitate all this.
Jennifer is just one of a whole lineup of great speakers I've got for the summit, and
the summit is just the first two days of the Interrupt ITX conference, which offers a
week full of great educational content for folks interested in data and analytics.
In addition to a dedicated data and analytics track at the conference, there will also be
an AI theater and demo showcase dedicated to the practical application of AI, deep learning
and machine learning for traditional enterprise tasks, such as dynamic pricing and insurance,
perfecting the retail customer experience, optimizing job costing, transportation expenses
and more.
Case studies will be presented by businesses employing successful AI strategies and will
provide strategic guidance and suggestions for attendees to utilize in their respective
businesses.
Accompanying the AI theater is the demo showcase, which will provide a hands-on lab demonstrating
real world AI technologies.
If you have any questions at all about the summit, don't hesitate to reach out to me.
And to learn more, visit twimmolai.com slash future of data.
The code I provide on that page, which is simply my last name, Charrington, provides
Twimmol listeners with a 20% discount when they register for Interrupt ITX.
And now about today's show.
I guess this week is Scott Stevenson, Scott is co-founder and CEO of DeepGram, which has
developed an AI-based platform for indexing and searching audio and video.
Scott and I cover a ton of interesting topics in this talk, including applying machine learning
techniques to particle physics, his time in a lab two miles below the surface of the
earth, applying neural networks and deep learning to audio, and the deep learning framework core
that his company is open sourced.
I know you're going to love this one.
And now on to the show.
Hey everyone, this is Sam Charrington and welcome to another episode of this week in Machine
Learning and AI, this week I've got Scott Stevenson on the line with me.
Scott is co-founder and CEO of Machine Learning and AI Startup DeepGram.
Scott, say hi.
Hi, thanks for having me, Sam.
It's great to have you on the show.
I think the best place to get started is to maybe have you spend a little bit of time
talking about your background because you come out of physics, right, not audio, speech,
and all that.
Yeah, absolutely.
We are, you know, DeepGram is an audio AI company, but our background or most of the
technical people in DeepGram, our background is in particle physics or at least some form
of, you know, deep physics.
And for me and my co-founder Noah Shetty, we both were doing particle physics before we
started DeepGram, which was searching for dark matter.
And this is about, you build an experiment that sits around, miles underground, essentially,
you know, like one mile, two miles underground, the deepest labs in the world are two miles
underground.
And we were, our experiment was in the deepest lab in the world in Western China.
It was called Panda X.
And we built this experiment over the course of four years with a lot of other people,
you know, with around two dozen other people.
And with the help of the Chinese government, and that's an interesting story.
But yeah, and the techniques that we learned in building this experiment, we've figured
out were like really applicable to audio.
And the reason is that physics is very, at least particle physics at the very hairy
edge of research is still analog.
And you still read, you still look for particles using analog detectors, they're called photomultiplier
tubes.
And the signals that you get out of these are just a waveform.
It looks a lot like an audio waveform, but it's at a much higher sampling frequency.
And those waves that are contained in the output of these photomultiplier tubes, the signals
that are in there tell you the signature of the particle that you're seeing.
So you might see a single photon, you might see a big splash of a muon in your detector.
But it's all contained in those waves.
And so what you have to do is extract that information from the photomultiplier tubes,
and then make a guess, you know, did you see dark matter or not.
And so we got very good at doing that.
And it's actually really interesting because the state of the art in particle physics is
essentially a lot of humans sitting down, figuring out how do you extract information from
these signals.
And you know, maybe you've heard of like the Higgs boson being discovered a few years
ago.
A lot of people, yeah.
And that was done by thousands of scientists sitting down and saying, hey, I really want
to find the Higgs boson.
How do I make cuts in my data?
How do I process my data to figure out, to make my signal to noise, you know, good enough
to find these particles.
And this is actually a lot of this hard manual labor of figuring out these cuts is done
by machine learning now.
So people will build boosted decision trees or kind of, that's not necessarily the realm
for neural networks, but boosted decision trees and other statistical machine learning
techniques.
People are figuring out how to sort of automate this.
But that's like the world that we came from essentially.
And we were on that, you know, we were in that area thinking, like, man, this sucks.
Let's automate this, you know, how do we automate it?
Because I feel like a machine already just going through, like, here's another plot.
Did this work well?
Did that not work well?
Like, what went on?
And it's like, this could all be done with a machine.
And so a lot of the people that are in deep-gram now, we were all sitting at University of
Michigan thinking, you know, how can we make machine learning, or how can we jam machine
learning into physics, particle physics, and then, like, extract something out of it.
And did you, how far did you get down that path?
Did you actually complete the jamming, or did you end up leaving and going off to start
deep-gram before you fully applied ML to the particle physics?
Yeah, so it's varied for the different people in deep-gram.
So I did finish my PhD, I finished about two years ago.
And Noah, who was my co-founder, went to a PhD program at Stanford, like, actually didn't
even start it.
He came out to the Bay Area, and we were working on deep-gram at the time, and we got a little
bit of funding, and he was like, you know what, let's not do this physics thing.
And anyway, but the couple other people in deep-gram either finished their PhD or, you know,
got a master's and left after that.
But we were definitely successful in sticking machine learning into a particle physics analysis
pipeline.
And in particular, for dark matter, what we did is reconstruct events in a 3D position,
reconstruct the 3D position of the events that are happening inside.
So the way that this works, I mean, it's like not to go too deep, but a particle detector
is just at least for dark matter.
It's just a tub of liquid, and it's a tub of cryogenic liquids, so it's very cold.
But, and it's put two miles underground, and it's under a shield, and it's all very,
you know, like, particular, but it's all, it's just a tub of liquid.
And that tub of liquid has a lot of atoms in it, and those atoms happen to be, in this
case, xenon.
And you choose xenon because it doesn't interact with other things, it's a noble gas.
So just like helium or neon or argon, it doesn't really interact, it doesn't form molecules,
it doesn't really have, like, a chemical decay to it, nothing like that.
But it's really big, and scientists surmise that dark matter particles, like really big
particles, essentially.
So you find the biggest non-reactive particle that you can, you put it into a tub, and
then that tub is just sitting there with tons of atoms, and it's just waiting for a dark
matter particle to hit it.
It's just sitting there, just, you know, that its sole purpose in life is to get hit by
a dark matter particle, and almost none of them have a dark matter particle hit it.
But that's what's happening, and those signals, that atom that got hit, as soon as it gets
hit, it flies through the other xenon atoms that are sitting around in this liquid, and it
creates a puff of light, basically, and stirs up some photons, essentially, and it's
like ten photons, you know, it's a very small number, but you have these detectors that
can detect the single photons, and those single photons are bouncing around inside your detector,
and they're finally getting picked up by these single pixels, essentially, and you need
to figure out, like, where was that event, right?
And so you're talking about, like, really low statistics pattern matching, and how do
you make some kind of algorithm that says, hey, this is what the pattern that I got, where
do I think this event was?
And that was really successful, and compared to what a human can come up with, essentially,
and the other is determining the energy of the particle, how big of a splash was it?
That's a little easier to do, but machine learning made it a little bit better.
And what about boosted decision trees lent themselves to application for this problem,
domain?
Yeah, so boosted decision trees, one thing, is that they're, like, fast to train.
So that was really good.
They also, when you have a low number of statistics, or at least a fairly low number of statistics,
they can still do pretty well, compared to, like, a neural network that would over train
or something.
So we were very hot at the time on doing boosted decision trees for that type of problem,
and I still think it's actually very good, and people are still using this, you know?
So yeah, it lent itself very well to doing that, the reconstruction, and it also lent itself
really well to doing a determination of whether it is a signal or background.
Was it a dark matter particle or not?
Right.
Because the signal that you see in this photo multiplier tube in the waveform, it actually
does look different.
It's like a little bit noisier, and it's a little fatter, and things like that.
But it's only kind of statistically noisier and statistically fatter, you know?
And so these machine learning algorithms can do a better job than are just standard hard
cuts.
A human can still do better than the machine learning algorithm.
You can look at all of these, but you don't want to look at billions of events.
Interesting.
Interesting.
Do you have a sense that there are maybe more that physics, people with a physics background
are somehow disproportionately represented in the machine learning and AI community?
I'm only working on a small data set here, sample size, but I recently interviewed Josh
Blum, who is an astrophysicist and did an AI start up.
Have you seen that at all?
Yeah, I do think so.
And I run into a lot of people that have a physics background.
And I think the reason that these two fields are kind of coming together is that AI is
kind of just information physics.
The way you try to solve a problem is exactly the same as how you would solve a particle
physics problem.
Like, get a ton of data, try to figure out what the trends are, try to see what the
outliers are.
Like, how do you actually tackle a problem?
It's identical.
I feel like I'm doing the exact same thing as physics, but I'm just doing it with voice
now.
Super interesting.
So before we get too far from it, he mentioned a lot of your work happening in China and
that there were some stories there.
How did that happen?
So connection.
Yeah, there was just an upstart experiment as I was starting graduate school.
This experiment was just sort of being circulated as a possibility.
It wasn't even, there wasn't really anything going on yet, but people were like, hey, there's
a spot underground in Western China, it's two miles underground, it would be the deepest
lab in the world and it's not a lab yet, but we're trying to petition the Chinese government
to turn it into a lab so that we can put a dark matter experiment there.
And I was like, I am so in, you know, this, whatever that is that you just said, let's
do it.
You know, and that's actually what it was.
I went to my, you know, my new advisor at the time, I was like, let's do it.
He's like, you know, this isn't really a thing yet.
And I'm like, I don't care.
Let's do this, right?
And it eventually turned into a thing.
It involved, China is an interesting place, so I'll say that.
But at first, there were failed talks to turn this into a lab and they went something like
this, oh, well, okay, to back up just a second.
Like why was this lab there anyway or why was this spot there?
They were building the world's tallest hydroelectric dam right next to it.
And they have all these tunneling machines and these mountains are made out of marble and
marble is really easy to tunnel through.
So they like Swiss cheese the mountain, you know, they just cut a whole bunch of holes
in the mountain and there just happen to be a tunnel that is two miles underground.
And this is where they were diverting water through.
So it's actually a new type of hydroelectric dam where they have a really tall dam part
and then they extract energy there.
And then they go where there is a river that would go around a mountain, like a long distance
around a mountain, instead they just tunnel through the mountain.
And so the mountain itself is now a secondary dam essentially.
Oh, wow.
Yeah, you can look this up.
It's the Jin Ping one and Jin Ping two dam in China and it was just completed like a
year or two ago.
But yeah, they, that's what they were doing.
And so we were like, hey, you know, this is a great spot to do it.
And we went there and said hello hydroelectric dam company.
We are like ten scientists and we'd love to put an experiment down here and they're
like, no.
And so we were like, hmm, how can we work this a little better?
And we were in, in kuhuts with Shanghai Jiao Tong University, which is, you know, one
of, a very well known technical university in China.
And we went to them and, well, we were in collaboration with them.
And so like along with them and the leadership there, they said, okay, we'll talk to some
people essentially.
And so the educational system in China has a ton, a ton of power.
And so they went to the hydro, the, the, like leaders of the education community went
to the hydroelectric dam company and said, hey, do you guys want to do this?
And they said, sure.
So they have some sway there.
It always helps to have the right people involved.
Yeah, absolutely.
And so, so that's how that all got started.
And from that, from that yes moment to having a lab to actually work in, it was less than
nine months.
They had to, you know, get dynamite blast out a lab, like, turn it into an actual thing
with cranes and, like, yellow railing.
So it looks like a James Bond layer, you know, they had to do all of that.
And they did it in, in less than nine months.
And we were in there building our experiment right after that.
And were you physically in China in this lab or virtually connected to it from Michigan?
So I, I would physically, I would physically go there, like, four months out of the year,
essentially.
Okay.
Yep.
And that was, again, like for me, I grew up in a really small town in Michigan.
And I kind of figured out that, like, the world exists, you know, outside of like a 38
hundred person city or city, I shouldn't say city, you know, tiny town.
And I, you know, was it, I went to, you know, undergrad in, in Missouri at a University
of Missouri, St. Louis.
And then I went to graduate school at University of Michigan.
And the world is just getting bigger to me, you know, and, and then now I'm like, in
China, halfway across the world, totally different people, totally different culture.
And I was loving every minute of it.
I thought it was great.
And it's awesome.
Yeah.
And there was, there, there was so much to learn from being over there.
But one of the biggest is that they have very few roadblocks, essentially.
If you want to get something done and you have the resources to do it, you will get it
done very quickly.
And so that's why we could start the experiment, well, you know, that's why they could build
a lab in nine months.
That's why they could build this dam in five years.
That's why we could start our experiment, like design the experiment, build the experiment,
put it in, run it, et cetera.
You, all of our data analysis, all in under four years, you know, because they, they were
just like so helpful in removing roadblocks.
So, so that was, that was really nice to see coming from the US.
That's awesome.
That's awesome.
You, uh, so you worked on this project, finished your PhD, um, connected with your, your
co-founder was also from, um, Michigan.
Yeah.
You was before.
Yeah.
Yeah, we were working on that experiment together, um, he's a decade younger than me.
So, so essentially, he came into University of Michigan at like 15 or 16 years old and
I was, you know, like an old graduate, old jaded graduate student, you know, at that
point.
And, uh, it started working in our lab and, you know, he is just very good and we hit
it off and would spend a lot of time together, um, outside of his ex too, like mining bitcoin
or building drones or whatever, you know, just kind of having fun and doing technical
stuff together.
Uh-huh.
Yeah.
Yeah.
Uh, and so you guys, uh, you guys started this company, the, you, you talked about how the
problems were similar, but how did the, what was the genesis for, you know, the idea behind
what you're doing at deep-gram?
Yeah.
So, so Noah definitely has to take credit for this, my co-founder.
So I was a little naysayer at first, um, where he, he wanted to start recording his life.
So he built a, a little device out of like an Intel Edison, um, or a Raspberry Pi, one
of these that it just had a battery, a Wi-Fi antenna, microphone, and a, you know, process
around it, essentially, and he set it up so that it would record 24-7 and basically every
minute make a new audio file and just sort of dump it into, um, storage.
And then whenever it came in contact with Wi-Fi, it would upload all of it.
Oh, wow.
So essentially, he was backing up his life, backing up his audio life at least, uh-huh.
And I was like, what are you going to do with all of that?
You know, you're, trust me, your life's not that interesting.
You know, in real time, you're not going to go back and listen to it again, right?
And he's like, yeah, whatever, right?
And so he recorded like two weeks of his life, you know, several hundred hours and, you
know, you're sitting there at the end of it thinking like, oh yeah, okay, now this is
a real problem.
How are we going to go back and find anything that we want to hear?
And so we started, we did the first thing that you would think of, um, hey, let's turn
this into text, you know, speech to text.
And, um, when that didn't work very well, because, uh, this, that's just like the state of
speech to text, essentially, where if you have a microphone that is not super high quality
and it's not right up in somebody's face and the person is not annunciating really well,
then you're going to get pretty bad word error rate, meaning, you know, it's not going
to be that accurate.
Right, right.
And, and so, so that's, that's a situation for like almost all of the world's audio.
It's not great audio, um, but in this case, all of the audio was like that.
And we're like, okay, this speech to text thing is not going to work.
Um, can we do something better?
Maybe, maybe somebody in the world out there like Google, hey, they know how to do search,
they know how to do audio.
Maybe they've made some kind of API or something that can do this audio search, right?
And so we started playing around and looking there and we just couldn't find anything.
And, uh, we found papers actually papers from like 2008, 2009, where Google wrote about
this type of thing like doing search and audio, um, their technique was to turn it into
text and then tried to search in it and it worked pretty well when it was on a reduced
domain.
So they did like, uh, political speeches and, uh, yeah, and they had fairly good results,
but, you know, when you try to generalize it, it doesn't work very well because everybody
speaks a little bit differently and political speeches are well recorded and, you know, everybody's
annunciating, you know, so it goes pretty well then.
But if you try to do this in a general sense, it doesn't work that well and we, we emailed
us or, um, you know, set up a Skype call, um, with one of the engineers on the paper and
said like, what are you guys doing now?
Because, um, you know, some years had passed, you know, surely guys are working on this
problem, right?
And they're like, no, no, no, we gave up on that.
We're not doing that.
Um, we're just trying to make our speech a text better.
Okay.
Um, we're like, huh, okay.
So it's still not very good.
That doesn't help us.
Um, so, so we started just working on ourselves.
We were like, okay, you know, we know signals, we know machine learning, we know how to deal
with lots and lots of data and, uh, let's see if we can make some kind of search engine
for audio.
And over the course of about four or five months, we went from, you know, very poor accuracy,
meaning like maybe 20% of the time you'll find what you're looking for to, uh, 80 or 90%
of the time finding what you're looking for.
So that's like going, you know, state of the arts, 20%, you're at 90%, you're feeling
pretty good.
Um, so, so that's, uh, what the genesis of Deepgram essentially, we're like, whoa,
this has a lot of applications.
And at the time, you know, we were coming out of academia and we're like, hey, students
can use this for lectures and whatever.
But when we start to think about it a lot more, we're like, you know what, the world, like,
the big audio source in the world, the huge data lake of audio in the world is like recorded
business calls.
So customer service calls and things like that, just call center stuff.
And they're all really low quality.
Nobody knows what to do with them, essentially, and we're like, man, like a Deepgram could
be a massive company, um, if we do this right, yeah.
Well, I think the other killer use case is, uh, I don't know if you're married or not,
but my wife and I always have this conversation where, oh, I just told you this or no, you
didn't say that or whatever.
If I had his device and recorded all of our audio and then could easily go back and prove
whether I said that or not, uh, there's got to be a huge market for that.
I love, I love this use case for Deepgram because yes, I feel the same way.
Actually, we've run across a lot of people though that are like, oh, no, this will make
things worse.
They probably would because my, uh, I think I probably think I'm right more than I actually
am much more than I actually am.
Yeah, exactly, but, um, but no, I think there's a regularizing effect there that if you
know that there's something, uh, recording maybe before you say, I know I said this.
You'll actually go back and check or something, you know, so I think it could work out.
Actually, we do, we do kind of think about this problem.
Um, I think that Mark is not really ready for it yet.
You know, if you just put a recorder on everybody and said, you know, yeah, now we're backing
up everybody's life and everything you say is recorded.
Uh, I don't think people right now are going to be super into that.
But it might come, um, and, and I kind of hope it does like, like for me, I wish I, I
had it and we do have these devices like laying around our office from those days, you
know, where I think, man, if I was just, you know, you know, I think, man, if I was just
wearing that all day every day, um, then that would be great.
Of course, they're very heavy.
So we don't do that now, but you could make it much smaller.
Yeah.
Probably a lot later than they were then.
Yep.
Um, interesting.
So you described the, the frustration that you were having with, uh, trying to do this
and the state of the art being turning the audio into text and then running a search
engine against that text is the implication then that you guys are not, uh, using text
as an intermediary.
Yes.
That is true.
So, um, we, we do build an index just like you would.
So, you know, this, this is sort of the terminology that you would say before you built
an index out of text, right?
Well, we're not building an index out of text anymore.
We're building an index, you know, like actually out of activations in a deep neural network,
you know, uh, so, so activations in a representation that's deep in a deep neural network.
Um, the best way to think of that though is that you're doing something kind of like searching
through phonemes.
So phonemes are like the, uh, the alphabet of speech, right?
Um, but they're kind of a human, um, they're, like humans have assigned to those.
Like those, these are the 40 phonemes or these are the 53 phonemes.
Like you don't have to do that.
You can just have it be defined by the data and, um, so that, that's what we do.
But if you look at the, if you look at what these match up to, it's very similar to phonemes.
So that, uh, statement that you quickly, uh, went by kind of blew my mind a little bit.
You're indexing, you're building an index out of activations deep in a neural network that
elaborate on that a little bit for us.
Yeah, absolutely.
So the, the output of a neural network is, is again, just an activation essentially, um,
and what you're doing is you're forcing it to like, in the case of a speech recognition,
you're forcing it to guess like, what is the word being said at this time, um, or, or
in the case of like doing some multi-class, um, uh, uh, network that's trying to guess,
you know, is this a handwritten zero or handwritten two or something like that that's trying
to guess that.
Yeah.
Right.
And so it, if you just sort of back up one layer from that, it's still pretty close
to, to that output, you know, it's just some like linear combination and that has been
stuck through an activation, um, to get to that output.
And so, so, you know, you can, you can think of, um, images where it, at the very top,
it depends on how you think of neural networks, but at the, at the, near the input of the,
of the neural network, um, there's, it's looking for edges or rower things or something
like that, right?
And then as you get deeper, it's looking more for like faces or trees or something like
that, right?
And the same thing is happening in audio and now it's looking for things that are kind
of word like or kind of like portions of a word like, right?
And, um, so, so those activations are what we are interested in.
And how deep are the networks that you're typically using and how do you know which of
those activations to tap into or capture and, uh, is that static or dynamic?
Does that change, uh, based on the utterances or how do you figure all that out?
Very, very great question, um, you can, so there's several ways to go about this problem.
You can just say literally let the data define this and, um, go to town.
Um, that, that does work pretty well.
Um, you can, you can sort of get a boost in accuracy, uh, a little bit of a boost in
accuracy.
If you, if you pin it at first, like when you're training, you might pin it to phonemes
or you might pin it to some, uh, larger subset and, uh, then, then remove that restriction.
So it's sort of seeded that it should have learned something like this.
And then now it sort of builds off that knowledge, um, so, so that's kind of how you can guide
the network, you know, you're pointing it in a direction that's approximately correct,
but then you relax that and allow it to pick the things that are, that are working best.
Um, the things that we're working on right now to make this even more accurate are instead
of, uh, a word target, uh, you are searching for a light, oh, sorry, not searching.
You're trying to force the network to guess like a topic or a part of speech, you know,
is it a noun or a verb or something like that?
And you do all of these things at the same time.
So your network is sort of branched out and it knows, you know, I'm, I'm trying to guess
words fine.
I'm trying to guess the topic.
I'm trying to guess all of these things.
And so when you, when you force that, um, uh, restriction on the network, then it, it
actually, it, you can control kind of what the activations are, are the information
that the activations are holding, you can kind of control that by the target.
Uh, so how, uh, can you say how deep your network's typically are, they, what you would
think of is, you know, very deep, you know, tens, hundreds, uh, or more layers or, are
they relatively shallow?
Yeah, we're in the tens, um, in the tens category.
And, um, yeah, we, so we, we have had a lot of success in that regime.
If you go deeper, you need, um, a lot more firepower on the computational side.
So, and you actually, and you need, um, more engineering on the, like, where do I put
my skips and where do I put, like, uh, everything like that, you know, um, architecture becomes
more complex.
Yeah, exactly.
And so, um, so we're, uh, a little, we're like an eight person team.
So we don't, uh, yeah, we, we, we tried to, um, minimize some of those, uh, engineering
bottlenecks as much as possible, um, and we've seen very good, oh, well, I guess, I guess
another way to say this is that if the state of the arts 20% and you're like in the 80 and
90% range with the networks that you have, you know, then, like, you could spend a lot
of time turning it into, like, 92% accurate, um, by making your, um, making your network
a lot deeper or something like that.
But we, yeah, we, we don't focus too much on that yet until we have the pressure, um,
to do that.
Okay.
Okay.
So with the network in the tens of layers, then it sounds like it's, you have a pretty
good sense based on your architecture of where, uh, you know, what layers, the phoneme
layer versus, I don't even know names for the, the other, um, we don't know names either.
Honestly.
Like this, this is, it would be really, um, this hasn't been explored as much as images,
you know, there, in, you know, there's sort of reasons for that, like, where are the data
sets to do this?
You know, um, also humans, I think, are not as interested, um, or I guess it's harder to,
to ingest audio than it is images, images, you know, you're like instantly see it.
You're also entertained by the images that you see that you see, but the, um, if you hear
audio that you're not super interested in, um, you're like, you, you, your eyes glaze over
right away, right?
And so labeling the audio data is expensive and time consuming.
And you can't just like click through a bunch of things and now you've labeled a ton
of images, you know, you can't just click through a bunch of things and label a bunch of
audio files, you know, like, that didn't work.
And so those, those are kind of the problems that audio faces, but also, uh, you can't
point to things and, um, you can't, you can't point to things and say, like that little
edge feature right there is important because you're hearing it, right?
It's, it's just not, we don't have that visualization capability.
And so, uh, it's a little more challenging in that regard.
So, so yeah, we don't have names for this stuff either, you know, um, there are, there
are linguists out there in the world that probably have some, some names, you know, uh, but
we just look at like a 2D FFT, um, you know, a spectrogram of the audio and say, like,
oh, I can sort of see what's going on there.
And you just kind of treat it like it's an image and think of it that way.
Okay.
Uh, so the other interesting, um, the interesting use case that occurred to me when I first
saw your stuff was, you know, I've got a bunch of audio in the form of podcasts, podcast
interviews.
And, uh, I'd love to find a way to make that more accessible.
And one idea that I've had for a while is, hey, it'd be great if I had a, a bot, like
a Facebook messenger bot or a chat bot or whatever, where, you know, you, you pull this thing
up and it's a, and you can ask it, hey, I want to find 20 episodes about convolutional
neural nets.
And it will, you know, look in its index and tell you, oh, check out, you know, episodes,
you know, 13 and five at times x, y and z.
It sounds like you're saying that your stuff could be a part of building that.
So what would the process be to, you know, to get to what I'm talking about?
Forget all the bot stuff, but, you know, just in terms of the voice search engine piece.
Yeah, that's exactly right.
The, um, the, the, the mentions is a part of it and topics is another part of it.
And that's exactly the type of problem that DeepGram solves.
So you have, uh, you have a lot of audio, right?
And you have, you have listeners that don't want to sit through, you know, 20 hours of
audio, trying to look for that one little tidbit, um, of information that they're, that
they're interested in that time.
And, um, that is what DeepGram solves.
And that is actually, um, kind of a, a, a really interesting interactive experience once
you finally do it, you know, so like when we first built DeepGram and we made, you know,
the search engine and actually found moments where we were like, whoa, there, I, this is exactly
what I was looking for.
You know, it's a totally, totally different experience than what you would feel if you
had tried to do this with speech detects.
And so, so yeah, we're, we're very into solving that type of problem.
And, and the way that you do it with DeepGram is we, um, we provide a, an API.
So you go and sign up at dgram.com and, um, create an account and there, you upload your
audio and you give us the query that you're looking for.
So if you're looking for convolutional neural nets or CNNs or, uh, convolution, you
know, put those keywords in, send it to the API and you'll get the results back of all
of the, uh, mentions that were in your audio.
And, uh, if you want to make that go even deeper to, it's, where it's more topic based
rather than keyword based, then you like sort of work with us, uh, for a custom API end
point to build a model that is more, um, tuned to the topics that you care about.
And that is essentially you guys working in a professional services or whatever capacity
needs, uh, build some, um, I don't know, what is that, what does that process look like
from your perspective?
What are you actually doing behind the scenes to make the, the topical stuff work?
Yeah.
Yeah.
Good, good question.
Um, the, so the general stuff kind of works right off the bat.
Hey, you, you want to search for a keyword, you know, we have a lot of data that we've
already trained on.
And even if your keyword isn't in there, we can find it because it's essentially a fuzzy
search.
Um, but if you want to do this topic modeling, uh, then what you do is give us labeled
data.
So in your case, it would be, um, I have, I have all these episodes and they were the
contents of these episodes, uh, you would have to know this, um, you know, yeah, we talked
about convolutional neural networks in this one, um, we talked about, you know, recurrent
neural networks in this one and boosted decision trees in this and, you know, we talked
about productizing in this one.
And, uh, you just put those simple labels in and, um, then you train a, um, model and
it doesn't have to be a deep neural network either at that point because you have such
a small amount of data, you probably do something like a, uh, boosted decision tree, um,
but you train this model to do that topic prediction based on the search in your, in your
audio.
And this is actually getting really into the weeds, but the way that it actually works is
we utilize the search for that topic modeling.
So our, our search is, is the best in the world is the most accurate, um, and, and if you
train a model with a target that says, here are these topics.
And I want you to be able to predict these topics and I want you to be able to generalize
to other audio files, then our, our model that we're building, whatever it is, but that
topic modeling, uh, model, it is going through and it's not doing any heuristics whatsoever.
It's sort of exhaustively searching all the possible phrases and saying, like, which
ones help and which ones don't?
And it's recursively eliminating the ones that don't help and, and it's arriving on things
to search for that are very good.
And it doesn't have to be a single search term.
It could be tensing, you know, tens search terms and it also might be the non-existence of
a term.
It's like, you know, if, if you're talking about these 10 things and this other one, then
it's some other topic, you know, and it, et cetera, and I don't even know how it works,
right?
It, that's, that's how it works.
And then in the end, you look at the output and you say, like, these are the things
it's saying search for and these are the things it's saying don't search for the area,
you know, you don't want these to exist.
And that's how the model works, you know, and it works extremely well.
So, um, so yeah, that's, that's sort of, you know, how the sausage is made.
And how much, how much data do you need in order for you to, to start getting, uh, interesting
results, both from the basic search and, uh, the topic modeling?
Or is the fact that you guys have already trained your model, like, well, let's start with
that question.
Um, is the fact that you guys have already trained the model on the data that you already
have?
Um, does that mean that I don't need to have some, uh, some specific level of data about
my domain in order to get good results?
Yeah.
In some cases, that, that's how it works where, you know, if you have sort of like a broad
topic, like, is this about sports or is this about politics, you know, um, then, then
that data is sort of already lying around and, and, and incorporated into a model.
But if you're talking about niche, niche markets, you know, um, then you probably want to
supply something and, uh, then we'll, you know, build a model on top of that.
But that doesn't, the data that you supply probably isn't the only training data that
we're using, you know, we're just, we're just supplying that.
It's essentially a form of transfer learning and then we're like changing the target of
the model, mm-hmm.
And so what am I supplying then?
Yeah.
You would supply like audio with some tags, um, so like here's an hour long audio and
here are the 20 things we talked about or here are the 10 questions I asked or something
like that.
Okay.
And, uh, the neural net can just figure out what's relevant and what is, uh, and kind of
map those to the tags.
Exactly.
And even given, uh, you know, I would imagine a fairly limited or what, how much, how much
audio and tags do I need to give the model for that to be useful for it, for it to be
useful to a, like a consumer that's searching for something, um, because they're fairly
error tolerant actually, right?
Like if you get, if you get one out of two wrong, that's not too bad as long as one of
those is okay, right?
As long as one of those is what you're actually looking for, so you probably need something
like, um, 50 or 100, um, different labeled files and you'll get a results that are similar
to that.
If you have around a thousand labeled files, then, um, then you get results that are
more like, you know, 90% accuracy.
And these are hour long files or shorter?
It, it kind of depends on what you're searching for, but, um, they're generally in like the
10 to hour long range.
Um, you'll need more files if it's only like a minute long.
Okay.
Yeah.
Oh, interesting, interesting.
And so that, so then what you guys are offering is a service, um, and it, that exposes an API,
but you also have done some work around, uh, an open source framework.
Um, let's talk about that for a little bit.
So, uh, why don't you walk us through what you've done there?
Yeah.
We're, we're extremely pumped about it.
It's called Kerr.
It's open source.
You can go contribute to it.
It's on GitHub, um, but the, the idea behind Kerr is that, uh, years ago, you know, around
a decade ago, GPUs came onto the scene for neural networks, um, data started to become
available and it became a really smart idea to start training neural networks on GPUs.
And the people doing it back in the day knew something about GPUs, it, you know, in order
to accomplish that task, you, you had to have some like domain expertise in order to pull
it off in a good way.
And, uh, Nvidia started to pick up on this and said, hey, uh, we're going to make CUDA,
a framework that allows like C developers to get a little handle into the GPU and be
able to train things that way, or be able to be able to use the GPU for matrix, um, multiplication
and things like that.
And so that's like another layer.
You have the bare hardware and then you have like CUDA and, uh, then you have, um, like,
uh, Brian Catanzaro, uh, making CUDNN so that it even works better for deep neural networks,
right?
But this is still all very, um, low level, uh, type stuff.
And then, uh, you know, outcome other frameworks like theano or, uh, cafe or, um, TensorFlow.
And those are another abstract layer on top.
They make it more human palatable, right?
Right.
Right.
Right.
They're, they're much more palatable for the, um, developer or computer scientist that,
you know, like really knows their stuff, right?
And, um, and that, and that's totally fine and they work very well.
Um, what we, what we have found out though at working internally, um, at deepgram is
that working in those frameworks still is very, it's very slow for us to, uh, to try to
do experiments, essentially, to try new model architectures and then, and then see the
result of because we are not, we're not necessarily training time limited where, uh, we would
really love to be training time limited.
We were engineering limited, you know, and so it's like the time that you're putting in
to sort of cook all of this up and make sure, do all the error checking and whatnot to
make sure it's like training the way it's supposed to.
We're like, man, why don't we just make some other framework on top?
Let's just stack another framework on top that is more abstract.
That sort of doesn't care about the back end and it, it doesn't care, you know, if you're
using theano or TensorFlow or whatever, you can just switch it with a flip with a flip
of the switch.
And, um, if, if something's faster, fine, whatever, but like all we really want to do is
describe our model.
We want to say, hey, the input is going to be this audio.
Then we're going to have a convolution, a 2D convolution and another con, a 2D convolution
with a batch norm.
And then we're going to have some, uh, a few recurrent, uh, layers.
Those are going to have batch norm too.
And then we're going to stick those out into, you know, a dense that then predicts every
time, time slice, which character is going to be there, which word is going to be there.
And so that's how we want to think of it.
We don't want to think about like Python code and like, how do we like put all of that
in there?
And so, so that's what, um, we start working on a deepgram is like, how do we do that?
So that we can multiply our engineering effort, essentially, you know, um, I just go in and
I change a few settings and, and then start running my network again and then go off
and do something else.
And that's, that's exactly what cur is.
It's a descriptive, deep learning framework.
And, um, you know, we have examples on how to do image classification, examples on how
to do speech recognition, um, with a network that's very similar to BIDU's deep speech
model and, uh, like language, uh, language modeling and things like that.
And it's all in this descriptive format, um, it's still not like, don't get me wrong.
Like deep learning is still not easy because, um, right, because there's a computational
problem there and there's a data problem there.
Like how do you get it into the format that you need?
You know, how do you collect the data?
How do you clean it?
Uh, everything like that.
But the model part, you know, once you have your data and once you're all set up, the
model part is now like so much easier for us using curve.
And so, so we thought, you know, um, this is kind of a competitive advantage, yeah, it
is, um, but we have gained so much knowledge by, um, using open source software and talking
with people very freely, sort of in the deep learning community that, you know, this
tool has been so valuable to us.
Let's just release it like we're, we're probably going to get more, you know, than if we
just kept it to ourselves, you know, and we actually have like, we, we released
the current framework and within like weeks, somebody had added multi GPU support, you
know, and I was like, whoa, oh wow, these people are serious, right?
And so, so yeah, and, and this is also another way to like, um, if you're, if you're an
AI company out there in the world and you want to hire engineers, you know, this is another
way to find good ones and they could be across the world.
And so, so it's like we're sort of, we want to, we want to be giving to the community.
We also just want to be, um, we want to be part of that conversation because I think we
have a lot to add, essentially.
And, um, I think that the deep learning community, there's so much demand to, um, there's
so much hype, first of all, um, but there's also, there's a ton of demand for on talent
and there's a ton of demand for the type of critical thinking you need in order to
solve these deep learning problems.
You don't have to be secretive.
You don't have to be like, this is our secret soft that whatever, no, everybody is like
talent limited, they're computationally limited, they're data limited, they're not like
good idea limited.
So, right.
Right.
So, yeah.
The, you mentioned it's, it's declarative.
That's one of the, the main things that's doing it.
Are you, have you created like a DSL to define your, neural net or is it a different, uh,
type of express differently?
Yeah.
So, okay, great, yeah, great question and the way that you interact with Kerr is you
pip install it and, um, so, so it's, you know, written Python and, uh, you can use it
as an API just as you would carous or something like that where, um, you know, you're programming
in Python and that's just how you use Kerr and that's totally fine.
But sort of the DNA of your model is contained in what we call a Kerr file and that is YAML
or JSON.
And so, so that, that contains like your hyper parameters, your model architecture, like
how you want your data to be supplied and things like that.
Um, and a lot of that is boilerplate that is already, um, out there in examples, essentially.
And so you just like use a Kerr file that somebody else has put out there in the world
and then just edit it a little bit in, for your purpose, essentially.
And so, yeah, that's how it operates, um, just basically YAML files and if you want to
do, um, like a deeper, uh, surgery, then you do it in Python using API.
Hmm.
And does it, uh, can you, uh, does it sit on top of any of the other frameworks or can
you, how do you leverage the work that's, um, being done on, you know, TensorFlow and
all the other frameworks out there?
Yeah, absolutely.
I should have said that earlier.
It does, um, it supports theano, it supports TensorFlow and it supports PyTorch.
And, um, maybe we'll support other backends, um, since, since, uh, like deep learning for
a J is, uh, is working its way into Keras now, uh, you know, we'll probably be supporting
them soon.
So, so yeah, that's, uh, that's, that's kind of how it goes, um, if, if you have a high
level API that would fit into something like Keras, um, then it'll fit into Kerr as well.
Okay.
Yeah.
So we've already done the legwork for those three, you know, TensorFlow, theano and PyTorch.
Awesome.
And you mentioned, uh, you mentioned this already, but it's not just for audio.
It's for images and basically anything that you're trying to use a deep neural net, uh,
with.
Absolutely.
We, all that we do at DeepGram is audio, that's true, um, but the net, but the, uh, Kerr,
you know, and the networks that you can train using Kerr are agnostic.
It doesn't matter.
You know, you can do sequences.
You can do audio, you can do, you know, text, audio, images, you know, it, cook it up
and you'll be able to do it.
Um, we just had a hackathon, um, last weekend and, uh, people were doing all sorts of things,
you know, uh, music, uh, editing, editing videos on the fly so that, you know, your cat will
look like a van go as it's, um, you know, crawling, crawling around and things like that,
you know, like the, and they're using, they're using Kerr to do these things.
So, so yeah, it's, um, it's, it's sort of agnostic, hmm, uh, that's very cool, very
cool.
Um, and so you mentioned, uh, the Baidu deep speech stuff, was that the, uh, was that
the inspiration for Kerr or to what extent do you, is your, uh, is the DeepGram work, um,
you know, your product based on that research?
Sure, so our, our, the models that we use, um, to, to build our indexes and to ingest
audio are extremely similar to the deep speech networks, um, you have a convolutional stack
and you have a recurrent stack and the target is, uh, characters or words, um, in the deep
speech cases characters, but, but nevertheless, the, the architecture is extremely similar.
Um, and so the networks that we supply like in Kerr, or sorry, the, uh, examples that
we supply in Kerr, um, are extremely similar to what we use, um, but, but we, we have tried
to make networks that people are already familiar with, like they can go read a paper
and figure out how it works, right?
Okay.
So yeah, and well, you know, we put that into the example file as well, saying like, hey,
if you want to read about the architecture, this is where it came from.
So that's, yeah, we, we're not trying to, um, confuse anybody about like how it works.
So, so we sort of stick to the things that you can go out and look at a paper for, um,
sorry.
It has not written, um, a paper on what we're doing yet, um, it's kind of in the works
always, um, but, you know, we, we really would love to, but, you know, that when you have,
when you have like businesses to, uh, you know, to take care of and, uh, you know, customers,
I guess, to take care of and, um, and only, you know, eight people, then, then you, that
kind of gets thrown by the wayside.
Right.
Right.
Any standout use cases for, uh, for this approach and deep-gram, anything that you're
seeing, uh, as, you know, kind of coming to the fore in terms of what people want to do
with it?
Absolutely.
Um, from, from the business side, um, there is, uh, fraud, fraud detection is a really big
one, uh, where people will call into financial services companies and try to, um, you know,
try to get money from them, essentially, try to take money out of your account or use,
you know, get a credit card sent to the wrong address or something like that so that they
can take advantage of it.
And there are sort of patterns, um, in this and you can, the, these companies have, you
know, uh, millions, hundreds of millions of calls every year.
And they're trying to find, they're trying to correlate these things and say, like, hey,
we know that fraud happened on these calls, et cetera.
Can you, like, help us find where that's happening, you know, like every day people are calling
in, trying to defraud us, can you at least give us an alert so that we can look at those
harder, you know, and, and that's just, that's, that's one of the channels, um, that,
that, that, that, like, provides a lot of value, uh, essentially to the, to the world, you
know, because if, if there's lower fraud, you know, then, uh, everything becomes less expensive
for everyone, essentially, um, but there's also like a quality assurance aspect to this
and, and compliance aspect, um, again, this is still in calls where, you know, are people
just saying the things they're supposed to say, you know, are they having good responses,
you know, do customers have, uh, nice interactions, um, and having, uh, the way the companies
deal with this now is they pay humans, um, to look at maybe anywhere from one to five
percent of the calls, and in, in general, this is outsourced where, uh, you send them,
you know, a random selection of your calls, and then you say, like, tell us what happened
in these calls and they'll report back with a rubric of maybe like 10 or 20 different
things and the quality will be pretty low. And that's like the only source of truth
for these companies about their customer interactions that happen through phone calls.
And so that's the type of thing that, you know, Teapgram is trying to help with. We, we
take like that QA data that you've sent out to have humans label will help you figure
out which of those labels are actually good. And then we'll build a model based on those
labels to predict all of your audio, essentially, uh, you know, to predict the, uh, contents
of all of your audio. And then you can take your QA team or your compliance team or whatever
they're doing. You still have hundreds of these people like listening to all these calls.
You point them in a new direction, you know, so they aren't doing the same like wrote
thing over and over. They're like actually using their brain to do things that humans are
really good at, which are like creative things like figure out, you know, a new way to, to
find fraud rather than just sort of listening and, uh, hoping that they detect it randomly.
You know, um, so we, this is, this is like, um, where Teapgram has a huge impact, I think,
or has the, uh, ability to have a huge impact is just automating that entire process. Um,
so for, for the consumer side, we are, um, we're not putting as much effort into making
our product like, like Google for the internet, but for audio. I wish we were like, actually
this is a product and somebody should build it using DeepGram. But, um, but, and we have
built demos of this, uh, where, where you just scrape a lot of YouTube videos and then
you're able to search it, um, using DeepGram's tech. Um, but, but I think, um, from a like
company health perspective, like, in other words, DeepGram not dying in two years, um, and
not having to raise like 200 million in the process of that death. Um, you know, it
could go a lot of ways, but, but essentially, um, I think that product is, is, is like available
and it's something to test. I think that, um, that just the greater human world right
now is not necessarily ready, ready for it at this moment. But, you know, in the next
couple of years, uh, that's what we're going to expect. We're going to expect that all
of the content in the world is searchable. That, you know, when I think of like a movie
quote or when I think of something that I listened to in a podcast or when I think
of some interview that I saw on YouTube and I think like, oh, yeah, they talked about
this and they talked about that. I should be able to search for it and I should be able
to find it and people will start demanding that soon. Um, but yeah, we haven't spent, we
did some, um, some market research on this, you know, um, to, to figure out is, is this
an area that we should spend our effort right now, but it just isn't yet. Um, maybe, maybe,
maybe we'll start doing that, you know, in the next couple of years, but, uh, it, it,
it's not our focus yet. Awesome. Awesome. Um, well, this has been a great conversation.
We, we talked about Kerr and that's open source and we'll put a link to the GitHub and the
show notes, anything else folks should know to, uh, look for or how to get in touch. Yeah,
you can, you can get in touch with deepgram at, uh, on Twitter at deepgram AI and it
where you can send a message to me, um, at Scott at deepgram.com. I am not shy about
throwing my email out there. So if you want, if you want to contact me, just, just go
ahead. Awesome. Awesome. Well, thanks so much. God, it's been great having you on the
show. Sam, I appreciate it. Thanks. All right, everyone. That's our show for today. During
this interview, you may have heard me mention my previous interview with Josh Blue, whose
company wise.io was acquired by GE to help them permeate machine learning and AI throughout
that company. If you haven't already listened to that show, which was number five, you should
because it was a great one. But even better, you should plan to attend the future of data
summit because Josh will be speaking there on building AI products and running them in
production. Really, you don't want to miss the summit. So check it out at twimlai.com.
And if you work on machine learning and AI for your company and you think you've got
an interesting story to share, don't hesitate to reach out. I'm finalizing the agenda
for the summit soon, but I'm always looking for interesting user stories. This podcast
is full of great quotes. Don't forget to share your favorites for one of our Twomo
stickers. You can share them via the show notes page via Twitter and via our Facebook
page. The notes for this show will be up on twimlai.com slash talk slash 19 where you'll
find links to Scott and the various resources mentioned in the show. Thanks so much for listening
and catch you next time.
