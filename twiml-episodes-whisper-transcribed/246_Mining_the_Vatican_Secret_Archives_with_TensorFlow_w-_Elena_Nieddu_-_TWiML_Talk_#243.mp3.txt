Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today's show continues our TensorFlow Developer Summit series, which I recorded at the event
on the Google campus just a few weeks back.
The summit was great, I met a bunch of Twimble Talk listeners and friends of the show and
got caught up on the new features released as part of the TensorFlow 2.0 Alpha.
This time around, we're joined by Elena Niedu, PhD student at Roma Trey University, who
presented her team's project in Kudice Ratsio at the summit.
In our conversation, Elena provides an overview of the project, which aims to annotate and
transcribe Vatican secret archive documents via machine learning.
We discussed the many challenges associated with transcribing this vast archive of handwritten
documents, including overcoming the high cost of data annotation.
I think you'll agree that her team's approach to that challenge was particularly creative.
If you're interested in TensorFlow in deep learning, you'll probably also be interested
in the awesome swag box that Google gave to summit attendees.
They included the new Coral Edge TPU device and the Spark Fund Edge development board,
as well as some other fun goodies.
And I'm excited to give you an opportunity to win one with our latest giveaway.
To enter, just visit twimmolai.com slash TF giveaway, and let us know what you would
do with this kit if you got your hands on it.
Of course, we owe a huge thanks to the TensorFlow team for helping us bring you this podcast
series and giveaway.
With all the great announcements coming out of the TensorFlow Dev Summit, including the
2.0 Alpha, you should definitely check out the latest and greatest over at TensorFlow.org,
where you can also download and start building with the framework.
And now on to the show.
All right, everyone.
I am here with Elena Nierdou.
Elena is a PhD student at Roma Kray University.
And we're together at the TensorFlow Developer Summit, where she gave a talk yesterday.
Elena, welcome to this week in machine learning and AI.
Thank you, Sam.
Awesome.
I'm really looking forward to diving into this conversation with you.
I had a chance to catch your presentation yesterday, which was about the project you worked
on.
Incodice Ratio.
Yes.
Incodice Ratio.
Incodice Ratio.
My Italian is very rusty.
It doesn't matter.
It's actually late in, so.
Okay.
So my Latin is not existing.
Non-existent at all.
We'll be diving into that, but to kind of get us started, I'd really love to hear about
your background and what got you interested in starting to work in machine learning and
AI.
I began working with machine learning exactly when I started working at the project, because
I have studied computer science at university, but I was more like operations oriented, systems
oriented, and very much I didn't think I would join a PhD or something like that.
But then, when I was searching for thesis for my master's thesis, then this project came
up.
It was very interesting to me, because since it's about applying machine learning to the
context of historical documents, and I had somewhat a background in classical studies
in high school, then it seemed to me like it was the perfect match for me, because it
joined two of my passions, two of my interests, and also it might have been, for me, the very
last chance, or so I thought, to learn something about machine learning, because then I suppose
I was supposed to go work somewhere, and then I just said to me, well, let's give it a
shot.
Okay.
Awesome.
Awesome.
So the project that you're working on, describe the project for us, what's the ultimate
challenge?
So we're used to think of big data as something that happens in the web.
So very large amounts of data we produce every day, it's told everyone of us has heard
at least one time in their life, but something that we sometimes fail to consider is that
there is lots of historical precious information that is stored in physical archives, like
in literal paper and something like that.
And in Rome and in the whole of Italy and the whole of the world really, there is lots
of huge archives that mostly have are still undiscovered because of their sites and because
they're physical.
They're not on a web.
And so the goal of the Encoli-Shirazio project is to make these archives accessible, and
in particular the Vatican, we're working on manuscripts inside of a Vatican secret archive.
Now, what?
I feel like if it's called the secret archive, I should be working on it.
No, this is, I'm glad you're saying this because this is a common misconception even with
Italians, because the secret in Vatican secret archive does not stand for confidential, but
it comes from the late in origin of the word, secretum, which does not mean confidential,
but the brother means private or separate, because the Vatican secret archive is a private
property of the Pope.
Okay.
So each Pope inherits the archive, that is the center of all the administration, of all
the archived administration of the church.
So the challenge is to do what with the documents in this archive?
Well, the leader of the project, Professor Marie Aldo, who is also my PhD advisor, he's
an expert in knowledge extraction from web sources.
And so the project precisely started because he was wondering whether we could apply the
same techniques to the domain of historical documents.
And so, yeah, the goal would be to build a knowledge base, a historical knowledge base
out of the documents, but the very first challenge you're facing when you want to do that is
that most of these documents aren't digitized, and if they are digitized, their digitized
images, their scans.
And for those that are printed, you can use traditional OCR and then it usually just works.
But then there is more ancient documents that pre-exist the invention of the printing
press, for example, and they are by necessity handwritten.
And so that's where the classical OCR fails, and you have to think of something more like
more smarter, I guess, or it's a greater challenge than simple OCR.
And so was this project already in existence and ongoing when you joined it or?
It was, it was starting, but there was, I was actually dragged into the project because
one friend of mine had already done his master thesis, and he was the very first person to
begin the code base of Encadyceratio, Chalandrea.
And he told me, look, this project is great, you should join it.
And so it was just starting, and then I followed up on his work, but yeah, it was about
two years ago now, and the project had started for like six months or so.
And so to provide some context on the size of the secret, but not secret archive, I've
seen a couple of comparisons, one sort of Panama Canal, the other to Mount Everest.
Yeah.
Who, what are those?
Well, that's, that's to give a hint of the scale, and that's like Professor Mallorino
who works at the Vatican Secret Archive.
He likes to say that if we were, if we were to take each shelving of a Vatican secret archive
and put it one next to the other, we could cover the, the length of the Panama Canal.
It's 85 kilometers of linear shelving.
So it's a very large collection, and it's a very, very varied collection, like you have
documents from, from China, from, and Asia in general, from, of course, from, he, from
Europe, from the Americas, and Africa, and so on.
And they spend centuries, like there is documents as old as the nymph century.
And up to this day, really, because the, the archive is, is still active, they are still
collecting documents that other institutions inside of the Holy See, don't need anymore.
Because that's how it works, or for example, suppose there is a chance series somewhere
in the world that belongs to the Holy See.
And when they have to archive their files, because they are not using them anymore, they
send that to the, they send them to the Vatican Secret Archive, so not only it's 85 kilometers,
it's growing.
Okay.
Interesting, interesting, and so are you, is this project focused on a particular subset
of the documents, or are you kind of randomly picking them, or, right now we're focusing
on medieval manuscripts, because, which sounds like a pretty broad subset.
Yes, it's still a pre-broad subset, but it allows for some consistency.
And in particular, we're working on a collection that is called the Vatican registers, and the
Vatican registers, they record every letter that the Pope sent in an official way.
So every time the Pope was to send a letter, of course he would write it, and before it
was sent, it was copied inside of the Vatican registers, so that there was a proof that
that letter was sent, and also people could read it if they needed to, because before there
was canon law, what the Pope actually said in letters was the law, act as law itself.
And so it was important to scholars, to law scholars, back then, like in the Middle Ages.
And that's the subset of documents we're working with, but our goal is not just to, not
just to transcribe that, but to create a methodology and to create a way that, so that if tomorrow
we were to change documents, that the documents we're focusing on, we could just reapply the
same techniques and get similar results.
So you said that you're, this project is really your first introduction to machine learning,
and also the project was just getting started as you joined it.
It sounds like a lot to figure out for some of the machine learning.
Maybe can you share a little bit about how you got started and how the project has evolved?
Well, as I mentioned, the project didn't start as necessarily as a machine learning project.
It started as a knowledge discovery project in general.
And in the beginning, transcription wasn't even the main focus.
Okay.
So we thought that since OCR is a solved problem, well, we will just OCR the documents and
then, and then we'll see.
And then we found out that that was just not the case.
And we needed something that was smarter that helped us really transcribe what was there
because it was not trivial at all.
And in the beginning then, so what we did was we went to experts in reading and ancient
writing, they're called polyographers.
And they, we asked them, like, do you think this is possible and they were like, no, no way.
I have studied years to do that, no way I can do that, can do that.
And then the second problem was that we, then so we were searching for how other people
solved our own problem because we didn't think we were the first to, you know, to find
this, this kind of issue.
And what we found out is that many systems that work also very well, but they require a
massive amount of annotations.
So for example, say you want to have a good transcription for a page, then you have to
provide like at least 1,000, 3,000, 10,000 lines carefully annotated.
And since this, this kind of writing can be very hard to read, even when polyographers
know of these tools, they get very discouraged because they say, okay, it's not cost-effective
for me.
Why do I have to give you like 1,000 lines that is months of my work, even years, if it's
very hard to do, just to have a transcription or a particular document, it's not scalable.
So the first thing we found out, it was that the main issue was scalability.
And so that's one of our greatest concerns.
So what is an example of one of these more classical techniques that requires that much
annotation?
And is it the same kind of annotation that we do on a machine learning type of a context?
Yes, but usually you can use unskilled workers to do that.
For example, you put work on Amazon Mechanical Turk, Crowdflower, you name it.
It's usually easy tasks, for example, the one that you're solving when you're doing
captures.
So like mark the boxes that have stop signs in it.
Or mark the boxes that have pedestrian crossing or something like that.
And then it's easy, and everybody can solve it because it's trivial or is there a card
in here?
Yes, no.
But when we're talking these kind of manuscripts, like I said, you need an expert.
An expert are expensive both in the literal sense.
That's time-wise because you need time to get it done.
And there is people who do that.
For example, there is the read project that you allow you.
If you send them in a annotation, they will train their own system and let you have the
transcription for following parts and then iteratively improve on that.
But even when we talked about that, for polyographers, they told us, yes, we know of that system,
but we just don't use it because sometimes it's not worth our time.
We will just do it ourselves if that's the amount of work we're supposed to do.
And I don't mean to say it's not useful because it can be useful, for example, for the
end goal of transcribing entire collections.
But you have to be invested in doing that, and more often than not, maybe historians
are focused on finding a particular occurrence and they don't want all the rest of the hassle.
And so in this project, when I'm imagining one of the big challenges is still annotations
and having some kind of labeled data set, how did you go about that?
I feel it's a challenge for many machine learning practitioners because, of course, when it's
about business data, maybe your industry and you already have that data and it's there
and you just have to use it.
But for many other fields and especially in the humanities, there is no such thing.
And so what we decided to do was not to annotate by line or by word, but to annotate single
characters because, of course, the size of the alphabet is way smaller than the size
of a vocabulary.
And also, it suffers way less of the skew of the distribution.
What I mean by that is that if you consider the whole 100 pages of text, it could be
latent, but it could also be English.
What you will find out is that there is a very few words that occur an enormous amount
of times.
And then most of the words, they just occur like 10 to 30 times.
And they are the same words you would be more interested in recognizing because maybe
there are verbs, they are names and so on.
Like you don't care about conjunctions or V and so on.
And that is why the systems require lots of data.
Because you need a good amount of each relevant example.
But letters on other hand are more distributed.
Of course, for example, vowel in Italian and Latin as well, vowels will occur more frequently
than consonants.
But still, it's something you can get over with.
And also that enabled us to create a system that was very much like the Capture Solving
system.
So we made our own custom crowdsourcing platform where the workers were shown an image
of a word and then on the upper side they were shown some positive examples of symbols
that they were supposed to recognize inside of that word.
Without even having knowledge of the symbol they were shown for.
And then they were asked to mark by clicking inside of the image.
The areas that corresponded to the example they were provided.
And one question that I had was were they annotating on a letter by letter basis or stroke
or subset of a letter basis?
I thought I saw something that suggested it was done at the stroke level and then you
were combining strokes to form letters.
Yes, of course, since we couldn't know before which parts of the word would correspond
to which letter because otherwise we would already have transcription by definition.
Then we use a system that can over segment a word in its strokes.
So for example, maybe you have an M and the system was segmented into a free sticks.
And so if workers were asked to mark an M they would just click on the free sticks and
then submit their answer.
A bit like what you do when you have an image segmented into small squares and you click
on the squares.
Pretty much the same principle, a bit more fine grade.
And so in this case you created this labeling interface and as opposed to using our mechanical
turkeys you use what resource.
So in Italy it's not that easy to access to mechanical turk or crowdsourcing platform
in general.
And also high school students have to work a certain amount of time in the last three
years of their high school.
And so they join projects where they are supposed to do works that are related to their kind
of studies.
For example, high school students like I was myself in classical studies, maybe they
will go to museum and help tourists or something like that or those that are more science-oriented.
They might go somewhere else or to laboratories or industry, something like that.
And so thanks to high school teacher Marika Shone, we were able to present our project,
our crowdsourcing platform as a working platform for high school students.
And also they were given a few lessons about image processing, machine learning and also
polyography.
And they were able to visit the university.
Sometimes they worked for us at the university at our labs.
And some but the good thing about that was that they could also work from home.
And so they were very flexible on when and how to work and that was an advantage that
also the professors, their teachers liked because sometimes to do the work part you have
to skip the school part.
And so of course teachers are not very happy about that.
And in that way they could like manage their own time and see a different way of working
in the morning.
So you have these crowdsourced annotations that you collected from the high school students.
How many or how was the volume of annotations?
The annotations there are more than 40,000 annotations over about 30 classes.
30 classes.
Okay, so the 30 classes are letters approximately.
Yes.
And so you 40,000 letter examples of letters.
Yes.
Not by character to each single, each we have about depending on, of course, depending
on distribution of the characters, but we have about 1,000, 1,500 examples per character.
So that's where the 40,000 comes from.
Often when you're working with crowdsourced data, you'll do things like you'll have
multiple workers annotate the same thing and do like forums or things like that.
Did you do anything like that or was there a higher level of trust that you established
with this group that allowed you to skip that kind of thing?
We experimented with different levels of like we just had a threshold.
So for example, we considered good things that were voted more than three times or five
times.
We experimented with different thresholds and or for example, we realized that some
characters were more challenging than others and we didn't want the students to spend too
much time on two easy characters, but just or maybe there were characters that occurred
less.
And so in the case of character that occurred less, maybe the threshold was lower or characters
that were very easy that we saw the student didn't get them wrong so often.
And in some other cases, we it was it was not as clear.
It was it was harder for them to to find them instead of a word and so on and so that
threshold was higher.
And so you collected this training data presumably to train some kind of model.
What kind of model did you train and how did you arrive at that model?
So we in the end settled for a convolutional neural network, which is pretty standard considering
image processing tasks, but we wanted to like build incrementally because we didn't
want to like we could have just downloaded one one big model.
But we knew our dataset wasn't that big and it wasn't as hard as image net, like recognizing
single characters is more like NIST than this image net.
But at the same time, we wanted something that really fitted our data.
So we just started simple, like very basic logistic regression.
And then it improved from that to see at each step how better we were doing and why.
And so you say at each step, meaning at each kind of iteration of the modeling step or?
Yes, we would go about a cycle of, for example, choosing, first of all, choosing a type
of a network of architecture in general.
And then seeing, seeing just as it was, seeing how it, how it did.
And then, for example, trying to change hyper parameters inside of that model.
And then we would try to make it more complex.
So for example, if it was a fit forward network, then we would just start easy one true layers.
And then, and then tune the hyper parameter inside of those.
And then when we were sure that we wouldn't get, we couldn't get better.
We would choose the best configuration and then maybe add the layer, remove a layer and
see, for example, because there is a, like, at some point, you won't improve anymore
because the model has limitations.
And so we did that for a certain number of times, so starting with the fit forward and
then moving to a convolutional.
So we're here at the TensorFlow event, presumably using TensorFlow to develop this.
What was that experience like?
Well, it could be challenging because there were like, there were so many things, so many
ways you could do the same thing.
And no one was necessarily better than the other.
And for a beginner that can be, like, a bit scary, but of course, I wasn't alone because
I was a beginner, but there was somebody who was not that also directed the whole process
of building the network that was Simones Cardapani, who is also a Google developer expert
in machine learning.
And so we were together in this and I was guided, like, I didn't do it all by myself.
Of course, I did my fair share of studying and of working on it.
So which APIs did you use for?
OK, so the thing is, for example, we were very torn apart between Keras and PureTensor
Flow.
Because we loved Keras for prototyping because it kept the code small and very readable,
understandable, much object-like, something you would like, you're familiar with, maybe,
more familiar than other approaches.
And but then at some point, for example, maybe TensorFlow had a new feature and Keras had
to implement it inside of it, and it was constantly lagging behind.
And so, well, before the integration, at least, or maybe they said, we're now supporting
a tensor board.
And then, for example, it was true, but it was true only for a very restricted set of
operations.
And so since we sort of wanted to have it all.
And so maybe sometimes we would just, for example, prototype fast in Keras.
And then when we were pretty sure about the model that it wouldn't change much anymore,
we would move to PureTensor Flow.
OK.
And was that was that process difficult or just tedious?
Well, at some point, TensorFlow introduces the high-level API layers.
And so it was pretty much straightforward because it was one-to-one, like syntax change
a little bit, but not so much.
And but of course, it was a hassle, like you constantly have to do back-and-forth.
And yeah, so the last model we trained, the one that we are using right now, but we're
trying to improve on.
But so far, it was implemented in layers and estimators.
It was actually implemented in layers estimators so that we could take full advantage of tensor
board.
And for example, at some point, we wanted to try the deep-dream technique to generate
images at each layer of the network to understand what was learned.
And that would, we just couldn't do that in Keras.
So they hid too much of the internals from you.
And so this model, you've got this CNN that's basically classifying these characters within
the words, within these documents.
What's the broader kind of pipeline that that fits into?
OK.
Of course, since we have this, we know we recognize that recognizing character is a limitation
because of course, then you have to provide the network with characters.
So there is a whole image processing step before the actual classification so that we can
provide the network with actual characters.
Or maybe not, because that's the point.
When you get to a word, it's pretty easy to segment into words, then you get to a word.
And then everything is written all together.
It's very like its cursive script.
So each word starts when each character starts when the other ends.
And it can be very tricky to understand where character starts and where character ends.
So we have a few old-school computer visual heuristics that help us cut these whole words
into sub-shapes, or as if they were puzzle pieces in a way.
And then we combine these puzzle pieces together.
We make the network classify them in isolation and in combination.
So the network has some confidence on those classification and can also have a special
class that's supposed to represent our wrong segmentation.
And so the network can both tell you, according to my knowledge, if this is an A or it can
tell you, no, that's not a character, drop it.
So that helps us prune the whole space of the possibility which otherwise would be very
large, because we're constantly combining adjacent pieces of the puzzle that are parts
of the word.
Is it like the segmentation is kind of like a sliding window kind of thing, or is it more
the algorithm is producing distinct segments, but it's noisy?
Yeah, more like the second one.
In the beginning we used a sliding window, but it produced way too much noise.
And then you're very dependent on, for example, the stride you're moving at.
And you may have noise from other characters in the background, in the sound of the sides.
So we have this series that basically computes the higher contour and the lower contour of
the characters.
And wherever there is a local minimum or a local maximum, because that's how the hand
writing goes.
That's interesting.
And then we're able to connect them and we create strokes from the characters.
So each segment could be either an entire character or a part of a character.
So then you have these, you kind of pass through the first step in this pipeline is passing
through the segmentation step.
Yes.
And then for each of these segments, you're passing it on to your CNN to tell you, what
are the 30 characters, is it, or is it not a character?
Exactly.
And then at this point, you still may have some ambiguity, because for example, there
is things you might not be able to solve, consider, oh well, this looks so much better if
I can draw, but okay, consider when you have, for example, one word we often use as
an example, let's try and make this, this, this imagination challenge, okay, it's unknown
A and N, O, now in this hand writing, the eyes have no dots on them, okay?
So even though, so you, you broke unknown into pieces with our segmentation.
And so probably what you will get is A and then a series of sticks, because they could
either be eyes or ends or amps, and then you get the O, but then again, for example, each
of the sticks of the ends could be either eyes, they could be all from the, from the perspective
of a character recognition without having further context, they could either be eyes, they
could be ends, they could be amps combined with eyes.
And all of these combinations are valid from the point of view of the optical recognition
of the visual features, but then again, of course, only one of them is correct, which is
the double N. And so to know that, you need more context and that context is the knowledge
of the leading language, so that is why even though we have classification, then we need
to rank, we could have ambiguity on some words. And so we have to rank them, and we rank
them according to a language model we built out of the latent text.
Okay.
So they, of course, language model picks the one that sounds more latent-ish.
From your classifier, you produce kind of a list of the top N candidates, and then you
pass that to your language model to basically re-order, re-rank them based on the semantics
or the context of the language.
Exactly.
And then that gives you your transcription, your transcribed word.
Yes.
It gives us, well, it gives us a ranking of transcriptions, and so, yeah, the metrics
we were using are not just correct or not correct, but also how good were they ranked?
On that note, how do you kind of evaluate the performance of the network?
So of course, we evaluated the performance of the network on the single character recognition,
but also we evaluated the performance of the pipeline as a whole on a word-at-word level.
So the network by itself has 94% average accuracy over 33 classes, and then, of course, since
there is ambiguity or maybe sometimes the segmentation can go wrong, or since it's a very
pipeline process at each step you could introduce error, or also the network might be wrong.
If the segmentation were perfect, so for example, if we didn't have that kind of ambiguity,
we could get up to 80%, 85% accuracy, and what we actually have is about 65% perfect transcription,
and then we get up to 80% if we consider minor spelling errors.
Okay.
For example, maybe E was mistaken for a C, or sometimes since convolutional neural networks
are terrible at counting, you have like a double C or a double S, and so double letters,
which occur pretty often, they might be conflated into one single error.
So it sees like two S, and then it just say one S, because it's twice the same symbol,
but it's just, you know, it's when you have an image of, you want the classic network
that tells you cat or dog, and it doesn't matter how many cats or dogs are in the picture.
It will not tell you it's not a cat if there is two cats.
So the language model is only really ranking the candidates that you give it.
It's not doing anything like, it's not like an embedding that's looking at possible
words around the ones that you give it that would be more likely, so as to like correct
for these single letter spelling errors.
Okay.
So far, we experimented with error correction, but only in the form of Viterbi, like
of what?
And I hit the mark of models, something like that.
And so using the Viterbi algorithm, but the problem we have with that is that sometimes
it produces like, it produces more candidate transcription and more noise.
Okay.
And so maybe we can get a few words corrected, but then we greatly, like our ranking gets
worse because there is so many things, there is more things that it has to consider.
And the more alternatives you produce, the more likely it is, they sound like latent.
And so that language model would put them like on top even though maybe they weren't
correct.
Okay.
So, you mentioned that the character classifier is performing it like 94%, the pipeline
at 64 or 80 depending on whether you're counting these spelling errors.
How does that compare to the payliographers and their performance?
Well, of course, you would have the ground truth we're testing against was written by
callographers.
So you have to assume that, like, they are 100%, that's a limit we do not expect, of
course, to do better.
Even though sometimes it actually occurred that maybe the polygrapher has made up very
small spelling error, especially when they were dealing with many sticks, because suppose
you have, like, UM and I, you and et cetera, that's very, very common sequences that can
occur, considering like numerals, it's like, it's all sticks and sometimes they get the
wrong count.
Okay.
And so they also may get wrong and sometimes it happens, but it's like, I guess they get
5% wrong maybe, at most, maybe to kind of wrap things up, what do you plan for the future
with this project, what are the next steps?
Okay.
So, of course, we have collected some more data, since then, but always at character level,
just it changed the way we did it, so that it enabled us to keep, to be aware of the context
characters we're put in.
And so one very first step I would like to go into is trying to create a system that both
performs segmentation and recognition.
So we have done recently a few tests and it turns out that this part of, you know, handmade
segmentation is one of the major bottlenecks we have.
So we, so right now we depend highly on how much good, how good we can do.
With the segmentation.
And so maybe to train a segmentator could be interesting and see, and see what happens,
maybe it doesn't work, but it's worth a try.
And then, of course, our end goal is to move to sequence models, because there is things
that otherwise we wouldn't be able to solve.
Like other DMs, RNNs, that kind of thing.
Exactly, or anyways, for example, models that use attention, I don't know, like the captioning
ones.
Okay.
To see the problem of transcription as one of, of, of captioning somehow.
Okay.
Interesting.
Yeah.
Yeah.
And but for that, of course, you, you need the line dataset and, and our, our, our focus,
you know, is on scalability and that's why we would like to try generative models to
help us improve our, the dataset that we have.
So, for example, we are able to reconstruct words from the annotation of the students.
They are a bit noisy, but they still look pretty good, like, they look like words.
And then, if we were able to feed, like, a generative model and to explore latent space
and create slightly different versions of the same words, that's so that we could augment
our dataset.
Okay.
And the character dataset.
Combining, actually combining the character dataset into creating words and then augmenting
those words.
Okay.
And so creating very, very realistic synthetic dataset.
So as, as always, you could get a polyographer to do that, but we want to involve them, just,
what we, like, in the, oh, as always, you could, like, you could ask an expert to do that,
but where our concern is scalability, so we want to be as unsupervised as possible
in this.
It is part of the way this could play out is you build the, you have the model that
does it at the character level and kind of gets good enough at that, that you can generate
some transcriptions and then you can use those to train the line level.
Exactly.
Exactly.
Okay.
Or maybe integrate, so maybe we generate some transcriptions and then so the polyographer
just have to say, okay, this is wrong, but this is right, and then a bit like translate
or do right now with Google Translate, so they give it to Google Translate.
It does, so and so translation, and then they just correct it and they go faster this
way.
Any advice for folks that are kind of embarking on projects like this, or just getting started
and...
Well, just don't be scared, because you can do it and it's, like, don't expect yourself
to be able to do that in one day, but eventually, like, going step by step and it's at some
point, like, if somebody asked me, where will you be one year from now, one year ago, and
I wouldn't answer, I would be here, so...
Right.
Awesome.
Awesome.
Well, I'd like to thank you so much.
Thank you.
Alright, everyone, that's our show for today.
For more information on Elena or any of the topics covered in this episode, visit twomolai.com
slash talk slash 243.
As always, thanks so much for listening and catch you next time.
