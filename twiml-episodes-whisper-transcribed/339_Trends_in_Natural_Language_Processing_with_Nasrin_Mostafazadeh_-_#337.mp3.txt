Hey everyone, hope you all had a wonderful holiday.
For the next few weeks we'll be running back the clock with our second annual AI Rewind
series.
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,
and other developments that made us splash in 2019 in key fields like machine learning,
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.
Be sure to follow along with the series at twomolai.com slash rewind 19.
As always, we'd love to hear your thoughts on this series, including anything we might
have missed.
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via
a comment on the show notes page you can find at twomolai.com.
Happy New Year, let's get into the show.
Alright everyone, welcome back to our AI Rewind 2019 series.
This episode will be covering NLP, and I've got the pleasure of being on the line with
Nasreen Mostafazadeh.
She is a senior AI research scientist at Elemental Cognition, Nasreen, welcome back to the twomolai
podcast.
Hi Sam, glad to be back, thanks for having me.
Definitely glad to be speaking with you again.
We last spoke back in August of 2018, when we spoke about contextual modeling for language
and vision, some of your research.
This time we'll be reviewing some of your thoughts on the most important papers and developments
more broadly in the field that you work in, natural language processing in 2019.
I'll have folks refer back to that previous episode for a little bit more about you and
your background and what you're working on, but to get this conversation started, why
don't we just start with your kind of broad take on 2019 in NLP?
What was the, you know, was it a big year for NLP?
Sure, so actually I think, yeah, I think 2019 was actually an exciting year for NLP,
where, you know, these sort of large pre-trained neural models have been stretched widely to
various different directions.
And, you know, slowly, but surely as a community, we've started to think about, like, the
problems they have, the weaknesses, the blindness bias they have.
So I can say this is sort of paradigm shift that we are seeing in NLP, you know, sort
of be art in 2020 now, kind of started, I can, you know, reflect back on the decade.
This paradigm should have started, you know, back in 2015 to 2016 or so.
And various NLP tasks could, you know, start to get tackled by a relatively, you know,
straightforward approach that you would just encode the input text.
It could be, you know, looked at as a sequence of words, sequence of characters, et cetera.
Then you use, like, attention to actually basically look back into the encoded representation
when you're trying to predict something for the task, which could be a sequence of
output tokens.
So back in the late 2017 or so, you know, Chris Manning, which is one of the paniers of
our field had this, basically, there was this code and belief from him that he's, he
believed in biosteums, he had a hegemony, which he believed that basically, no matter what
the task is out there, an LP task, if you try a biosteum at it and use attention to
attend back to the, basically input, input encoding of the input, you basically can achieve
state of the art.
Now, this referring back to the attention is all you need paper.
So attention is that only you need paper is more recent, so that was then the transform
is came to picture.
This has been LSTNs where it's still a thing, right?
That's amazing.
That how fast the field is moving in 2017 is still the, as I said, like the consensus
in an LP was that you can achieve achieved state of the art if you just throw a biosteum
at it with attention.
That was the recipe.
And back in that time, I remember like when I was like giving talks, I would conclude
that look, although that has been true for a bunch of, a host of different benchmarks,
it happens that for the test, it requires vast amounts of background knowledge, reasoning
and basically require establishing a long context, we can, we can not yet achieve a state
of the art or near human performance using these biosteum models.
So fast forward, just one year, in 2018, we had like LMO, this deep contextualized word
representation that basically started sort of this one more step forward of building these
large language models, which happened to be contextualized, so preaching on a very
large corpus and then fine tune on downstream test, which itself started meeting lots and
lots of different state of the arts and establishing, you know, brand new state of the arts.
And so the test that I had in mind when I was personally criticizing the fact that, oh,
look, by throwing biosteums with attention on a particular benchmark, you don't necessarily
issue a state of the art, for a conist's reasoning task, which is something that I personally
very passionate about and happens to be a minor line of research.
So the particular task was a story closed test, which I talked with you in the last
time I talked with you, specifically a story closed test, which is this task that given
a sequence of four sentences, which form a coherent story, very short story, the task is
to choose between two alternative endings to that story, which, you know, is designed
basically to evaluate a system's commonsense reasoning capabilities.
So what happened in 2017 is that in mid 2017 or so, the attention is all unique paper came
out, the transformer paper that you just mentioned a minute or two ago.
So that paper basically enabled a cascading effect of other very large pre-trained transformer
models that could actually establish the state of the art in various commonsense reasoning
tasks.
And being the GPT-1 paper, so the GPT-1 paper came out around, in 2018, which was, you
know, this, they called it like, generative pre-training model.
This was a very large language model that open AI folks have basically trained on a very
large diverse corpus and then fine tune on a small data sets.
And actually, the data set that they highlighted as to the place where they've made the most,
you know, amazingly, basically, progress happened to be a story close to the, you know, benchmark
that I really cared about.
So they had gotten, you know, notably they had gotten like around 86 or so percent accuracy
in which was exceedingly better than the previous numbers that people had reported on
the test set.
And so that really sort of changed my personal mind about verbure going with this.
I started believing in the fact that, oh, look, although these models may seem to be sort
of doing pattern recognition at the scale, which may not go hand in hand with doing reasoning
and connecting to that and all these sorts of things that we care about and label as commonsense
reasoning, if we, you know, do them in the right way or give these models enough chance
of being trained for, you know, on the right data sets, fine tune on right data sets, etc.
They are actually capable of doing knowledge transfer.
So I think that sort of set the ground up for us to move into 2019, verbure had more
and more of these very large pre-trained models that then you could basically fine tune
on various downstream tasks and establish a state of the art, no matter whether or not
they are from our very core and healthy tasks, like a task such as product speech tagging
or very, like, semantically oriented tasks such as the story costus itself, commonsense,
reasoning, etc.
So I think this has been the main exciting thing about 2019, where we could see that this
wasn't just the, you know, glimpse of how this wasn't just the one time thing that these
models could perform well, it continued into 2019.
And I think I'm actually excited about seeing verbure go with improving these, and, you
know, we will talk more about the downsides of these models, but yeah, I'm very excited
to see verbure are going with this paradigm shift into 2020.
Yeah, I chatted a little bit in one of my previous conversations in this series.
It was a conversation with Zach Lipton, in particular, about the role that these transformer
models have played in NLP, and his take was pretty interesting.
It was focused on the notion that the amount of compute that went into creating these
models, creates a huge barrier for, or sets a new kind of a new standard that creates
a huge barrier for folks that want to do future research on the model side, the amount
of compute required to develop models that can achieve state-of-the-art performance
is, you know, such a high bar, are you seeing that as well?
Absolutely, actually, one of the papers that I wanted to highlight, which I think I can
just highlight it now, is this very amazing work, I would say, that came out of UMass, this
is from Struvo et al, a CL 2019 paper called Energy and Policy Consideration for Deep Learning
in NLP.
So I would say that, yes, as I was saying, we've come a very long way in making advancements
in NLP, and through these very large pre-trained models that we are building, but they have
two major sort of policy, you know, like external implications, right?
One of them is the fact that these are, these require really expensive and extensive resources,
you know, millions of dollars are basically used, you know, in terms of like cloud, etc.,
for basically building these models, which is very much sort of unique and makes it
entitled to the top players in the field, such as like the, you know, large tech companies.
And I think that this sort of implies, as if AI research would tend to get privatized
and only accessible to the players in industry, with access to resources, which is of course
not fair, is not fair and it will have lots of other implications for the society as
whole and who will have access to these kinds of amazing outcomes of AI.
So I think that's definitely a major problem that we have.
And along with that, the reason I wanted to highlight this paper is that something else
that we have in Manpada, as much about art, environmental implications, basically, of
these large models that we are building, right?
So this paper that I referenced, Energy and Policy Concereship for Deep Learning, says
that although people keep talking about the fact that we are throwing, we need to throw
so much money at these models, which only a handful of players are capable of doing, we
also are basically increasing our carbon footprint.
So the tagline was actually, which got a lot of news coverage, was that training a single
AI model can emit as much carbon as five cars in their lifetime.
And I think that's pretty, you know, crazy, right?
I think that the number of reciting was something around like, you know, more than like half
a million pounds of carbon dioxide is emitted after just basically training one of these
large models that we were just talking about.
So I think that that's just a major consideration that we should take into account moving forward
as a field.
It's definitely huge and I would refer folks interested in learning more about that to
check out my interview from back in July of 2019 with MS Trubel, the author of the paper
that you're referring to.
So with that in mind, when you stepped back and thought about some of the more important
things or more interesting things to you in 2019, you divided that into a couple of key
trends.
Do you want to talk about the first of those?
Sure, absolutely. So the first theme that I wanted to highlight was interpretability,
ethics, fairness and bias in NLP.
So this also happened to be one of the traumatic paper tracks for our, you know, one of our
major conferences in 2019.
And I think the time is actually ripe for us as a community to start thinking about the,
you know, ethical implications of our work and basically, I think in beyond just making
scientific improvements, but also about what are we actually enabling.
So I think it's been really great in the shape learning in AI community as a whole that
in the past like three, four years or so.
A lot of players in the field are talking about ethics in AI, but the truth is that I think
it has been long overdue and we need to educate practitioners and scientists so much more
on the topic.
And I think it's been really a positive change that in our conferences at least, we've started
making particular tracks, particular themes, et cetera, for highlighting these particular
considerations and giving them the credit that they deserve.
Yeah, it seems like not long ago, the conversation in this area in NLP was rather more simplistic
than it is today.
You know, we would talk a lot about the kind of the word to VEC example, you know, and
several of those were popular.
But now it seems like the conversation is quite a bit more nuanced.
Is that something you would agree with?
Yes, absolutely.
The field is definitely maturing and as I said, the fact that we are establishing particular
tracks for just soliciting papers and submissions for these particular considerations is definitely
helping that movement.
And yeah, I think I can go ahead and tell you a little bit more about the particular
papers that I had in mind that I wanted to highlight.
So I think I will go ahead and talk about, so there are different angles right to this
problem.
Sort of ethics and fairness in AI and like de-biasing, basically, AI and hands-on-up
models is one end of things and then basically building explainable AI in NLP systems is the
other end of a spectrum because we want to have these systems be accountable towards
the predictions that they are making which goes hand in hand with sort of de-biasing them
or, you know, basically better societal use cases that they could have.
So I will start with the explanation one.
So explanation is this really overloaded term and, you know, today probably won't be
the day that you are going to cover what explanation means.
But I'm going to highlight one paper through which I'm going to mention a few other papers
that sort of started a debate in the field in NLP field this year about explanation.
So this paper is titled Attention is Not Explanation, it's a work that came out of North
Eastern University, published in NACCLE 2019, the authors were Valous and Jean.
So this paper, as the title suggests, is talking about attention and not being explanation.
And so what they're actually trying to highlight is the fact that you remember just a few
minutes ago, I was talking about this paradigm of encoding and then attending and then decoding
for doing multiple NLP tasks.
So attention has been used and often presented at least implicitly as this relative importance
of input kind of a measurement that we've had in the field.
Basically pretty much like a common citation for summarizing this commonly helped you by
Leah Alt has a 16 paper, it was that attention sort of provides an important way of explaining
the inner workings of neural models.
So it's like pretty much, it was pretty much established until this conversation was
started by this paper, that attention is something that you can count as explanation.
Again, I'm not going to argue or basically define what explanation means, but even loosely
there were enough people in the community to count it's attention as explanation.
And you know, for me, like as someone working in Communism's reasoning, caring about deep
natural language understanding, like basically going beyond what's explicit out there, et
cetera, I personally took so many issues with that leave because as you can imagine, there
are so many tasks where your answer or whatever the inner workings of your reasoning engine
is, of your reason, reason paradigm is it's not going to be anything explicit in the input
that you can even highlight as the attention waits.
So that's a major obviously shortcoming, but sitting data side even for tasks like say
a squad, et cetera, where you are actually going to attend literally to parts of the input
of text to provide the prediction is still like people were using that as their explanation.
So this work actually was critical of that premise, basically, it was claiming that it
has been unclear what is the relationship between attention waste and the model outputs.
And so they argue that if attention wants to be a faithful explanation for any models
prediction, it should have two particular characteristics.
One is that there should be a correlation between the inputs and outputs, which means that
the way that they quantify this is that attention waste should be correlated with measures
of feature importance that we have.
And then the second point that they make that they think for a faithful explanation should
be held to is the fact that the models explanation should be exclusive, meaning that if we change
the attention distribution dramatically, of course the prediction should also change.
So these are the two main basically points that they made and they went ahead and presented
actually various experiments for showing that for the first point that actually attention
waste are not correlated with measures of feature importance like the grading based ones.
And for the second one, they actually showed that even if you shuffle like randomly shuffle
the distribution of the attention rates, there are many cases where the predictions are
actually going to stay constant.
So they conclude that the standard attention modules do not provide any meaningful and systematic
explanations of basically the community should stop treating them as such.
The interesting thing that happened after this paper came out, and as I said, it was
accepted and published at NACCLE of an overmajor conference, was that there was a follow-up
paper to it which was titled Attention is Not to Not Explanation.
Yeah, so this was a work that came out of Georgia Tech, again like publishing ACL in
EMNLP 2019, sorry, that was arguing that this approach that the authors of the attention
is not explanation took had some problems, right, and then there was some back and forth
they actually encouraged the audience that if they're interested they can go and read
the blog post or respective blog post that they had basically arguing the different points
that they had.
But I think that the conclusion is that I think this whole thread was very healthy for the
community to start thinking about such presumptions that we make before digging deeper and basically
proving what we are counting as XYZ.
So I think that was a very interesting example of a good scientific contribution to the
community where we go back in time and look at what we assume to be true and just dig deeper.
And so in line with that I actually want to mention, so there has been lots of other
actually follow-up papers, I want to highlight one actually toolkit that came out of AI2
which is called Alan LK Interprete.
So this actually happened to get the best demo paper awarded our major, one of our major
conferences as well, which is a toolkit that makes it easy for different people to apply
and visualize actually such saliency maps for whatever model they're deploying.
I think this whole thread, as I was saying, was very good for reminding people that you
have to think about interpretability, you have to think about what you count as interpretable
and for the very least you should be able to visualize what is salient and how you can
do adversarial attacks towards different models.
And I think the kind of open source tooling that AI2 does is really helpful for enabling
individuals across like academia and industry to basically dig deeper and deliver on the
premise of interpretability.
Is there a quick way for you to summarize where the community ended up through this back
and forth and the subsequent papers on this issue of the relationship between attention
and explanation?
Yes, so I would say that this is just my personal view.
I told you that the fateful explanation that the authors of the original paper we're talking
had this was this twofold thing.
They were saying that there should be a correlation between inputs and outputs.
I think that the way that they had done it wasn't rigorous enough in the eyes of the, you
know, other paper, but the way that they had rebuttal actually to me again as someone
reading their argument made sense.
I think they did a good enough of a job with justifying why the correlation was in place.
But I do agree with the authors, the Georgia Tech authors that yes, explanation is this
very loosely defined term.
It's not clear what the original authors meant by explanation and maybe that's title of
attention is not explanation was too overloaded, right?
They could have specified what they mean by explanation.
I think, yes, so I would say that the community should stick to not calling attention,
explanation.
So it's one thing though that I actually agreed with the original authors paper.
Georgia authors point was that they had said that although that they would seem that that
title is overloaded, it's as saying that, you know, correlation is not causation.
It doesn't mean that correlation can never be causation, right?
There are, if you do your studies rigorously, et cetera, there are types of correlation
which are in the causation.
But you can still say that in a sense that, oh, look, be careful, don't count correlation
as causation.
So I think, yeah, so that's pretty much my overview of observing the back and forth.
And then the next paper that you identified is more on the kind of fairness and bias
and of the spectrum, which one was that?
So that paper was titled, what's in a name, reducing bias and values without access to
protected attributes.
So that was a paper that came out in, for example, our knackle 2019 and actually won the
best thematic paper award a knackle by Romanov at all, Umanas Loel, MSR, CMU, collaboration.
So the reason I wanted to highlight this paper is, well, first of all, it happened to have
been highlighted by the community before by getting the best paper award.
But second of all, they had a pretty amazingly simple approach and yet strong results, which
I think should be something that we do more and more so in our community.
So basically, this paper highlights the fact that, look, we are at this day and age deploying
lots and lots of AI systems that are automating decision-making in our daily lives.
And some of these decision-making scenarios are high stakes.
So for example, like we have applications of AI in criminal justice, we have it in overcruiting,
et cetera.
And having deploying bias models can basically yield very negative outcomes in people's daily
lives.
And we should be, like, as a community mindful, as like practitioners, again, as scientists,
the term should be really mindful about the such implications of the models that you're
building.
So, you know, as you were saying earlier, there are, there have been, like, these representational
biases, like, about award embedding and how, like, I don't know, if you do, like, the
classical analogy for Wurtubek, like, X's to Y, like, as, like, Z's to what, if you do
the analogy for, like, say, we are talking about recruiting, right?
You want to know what kind of jobs go with what kind of people.
So if you're an adjust man to computer programmer, it's, like, been cited a lot around that
it would save women as to homemaker, right?
And these are obviously very problematic when these kind of, these kinds of, like, representational
biases can turn into, like, a really harmful, allocative biases in downstream tasks.
So this paper is particularly trying to address how to mitigate these allocative harms that
come out of these, you know, bias representations.
Their tagline is pretty cool, actually, their tagline is five bias with bias, which is really
awesome.
Agline, they say that they basically want to leverage bias representations, like, word embeddings
basically, to the bias of classifier, so very simple idea, strong results.
So what did they do?
They actually based their study on a prior data set on occupation classification.
This is a data set of 400,000 or so public bias of biographies, short biographies of
different individuals that are aligned with the 28 different possible occupations that
they could have.
So you read, like, little paragraph of, like, you know, expires, you did this and this
and then there's a title of the occupation matched with.
So prior work had shown that bias exists in this task, which is, so in the way that when
you're trying to predict what is the occupation, it's supervised in terms of gender and race.
So the way that they're sort of measuring this bias is by the classification accuracy
gap that they are seeing.
So this is also, like, this was a prior work that came before this work by, you know,
the same sort of team of authories, very de-quantified this problem as the true positive
rate that existed, the true positive rate difference that exists between genders for this
particular downstream tasks.
So they have this, I really enjoyed reading this paper, they have this very nice graph
that they show that, for example, it's more accurate to predict the job of, I don't
know, like being a model for a female, that it is to predict the job of being a doctor,
physician for a female.
And the fact is that because the bias in the actual true positive, the population already
exists, it's sort of this compounding bias that happens at prediction time, which was also
supported by earlier work.
One interesting thing I want to mention is that you would think that if that maybe these
models, so imagine you're just building your most vanilla classifier, right, imagine
the biased model that I was saying, you just feed in the bios, you attend, et cetera,
you make a prediction, 28 categories, like labels that you're generating.
You would think that if you scrub the gender indicators from the bios, let's say like the,
you know, the proper nouns, the, you know, gender products, et cetera, maybe you will be
able to de-biased these models, meaning that you can shrink that true positive rate gap
that I was mentioning.
But they, this study in the prior work actually showed that scrubbing such explicit gender
indicators does no good, so no difference at all.
So the same accuracy, same TPR, like true positive rate gap, as with a model that uses
the explicit gender indicators, which goes to showing that the bias is source from elsewhere,
which is a very interesting kind of a realization that this work and the prior work have had.
So in order to overcome this problem, they, they have this very simple yet super effective
idea that they are saying that we want to use the embedding of names as the universal
proxies for race, gender, and presumably age.
So what they say is that look turns out in names of individuals, but just, you know, different
kind of proper nouns, the names and family names, we are already encoding lots and lots
of biases.
So they even like show like they prove in the paper show sort of how the gender and race
could be core highly correlated with these names that you cluster them.
So they go ahead and define this very simple way of sort of debiasing the classifier that
you build by discouraging the model to learn a correlation between the name embedding and
the predict, predicted label.
So get your any, you know, vanilla classifier, all you do is that you swap in your existing
objective function with this new objective function that now penalizes, basically penalizes
the model if there's a correlation between the name embedding and the predicted label.
So they show very like really strong results that by doing so, they can really minimize
the gap, the TPR gap that I was mentioning.
So that was their conclusion that this is achievable, basically moving forward.
If you are deploying map data models and industry in really high stake situation, this name
embedding has happened to be a good proxy for debiasing the model.
But they emphasize that the bias is not zero yet, so there is definitely further room
for improvement of such high stake, this predictive models in future.
So kind of when you think about the relationship between the explainability aspect of the first
paper that you mentioned and the bias fairness, you know, do you, you know, there are other
papers that are in different kind of points on this axis that are worth mentioning for
folks that want to dig in deeper.
No, nothing very particular in mind, I do think that again, these are kind of new developments
in the NLP community.
And I think, you know, there's this definitely strong like connection between building models
that can explain themselves and hence us being able to diagnose their bias towards their
predictions.
Nothing else that I can think of right now, honestly.
But yeah, there are actually conferences, outside of the NLP community, like the fat
conference, et cetera, that have lots of amazing work coming out of them.
In the, you know, it's the same area, maybe they take language as one of their tasks that
every now and then they report results on.
And I think that people should definitely check those conferences out.
But it sounds like an area that you expect to see more of in the future, but I guess we'll
get to predictions, let's not get ahead of ourselves.
And before we do that, kind of the next batch of papers that you identified go back to kind
of your initial take on 2019 and the role of these large pre-trained models, walk us
through the papers that you had in mind there.
Sure.
So I think it's a needless to say that this year has been the year of transfer learning
for NLP sort of continue, as I was saying, continuing on 2018, but more so maybe 2019, because
we saw a real world impact through this work, basically.
So I want to mainly highlight two main such models, one birth and one GPT-2, which I think
people have heard enough of, but I don't think that we can really end the year without
sort of mentioning them at least.
So as like I'm sure probably your audiences have heard a lot, branches despite directional
and encoder from transfer model by Google AI folks.
It came out actually in 2018, so kind of sort of maybe not 2019 paper, but it actually
got officially published in knackle 2019 and got the base paper worth there.
So I think, you know, whatever we can count it 2019 paper.
And this paper basically is just yet another large pre-trained language model, maybe the
only main difference that is notable is the fact that their training objective was different.
They had this training objective called MAST language model, but the main reason that
this paper got as much attention as it did was the fact that right after it came out
and throughout the ML community, it was achieving different states of the art for wide variety
of an LP task, ranging from question answering to national language inference, etc.
So I was just checking the end of day and to this day this paper has collected like 2,300
citations and counting and it's definitely, I think, by any stretch of imagination the
paper of the year in terms of the impact it has had.
And I think I don't want to spend like much time talking about like all the other models
that came out, like there's so many models that have, you know, been built sort of on top
of birds and being inspired by birds, or birds, etc.
But I think the main thing that I want to highlight is the fact that Bert got used a lot
in industry, basically at Disney and I just, there's so many, like, I was personally surprised
and you would see like these different startups that even their job posts, one of the requirements
that they've list is like, oh, like you have to have work with Bert and I'm like, what?
Like, what?
I mean, living and it makes like Bert becomes like a requirement for like people to recruit.
So this just goes to saying that there was this fear of missing out in the, you know, industry
for people who were not actually improving whatever underlying and healthy pipelines they
had through bird.
And as I said, it was due to the fact that there were so many positive signals from the,
you know, corresponding positive and progress in the other tests that everyone thought
that, oh, whatever XYZ test they're working on should also be one of them.
So anyways, that, that's one of the interesting, I would say, observations about the affected
Bert had in the community.
And the second is I think the main notable use case of even maybe NLP, but at least Bert
in the industry was the fact that Google, itself, Google search itself, I reported that
they have not incorporated Bert into their search engines.
This is pretty grand, right?
Like Google being one of the major tech companies search being the major of that company.
I think this is just really, congratulations to the authors of the Bert paper who, you
know, this is making real world impact.
And I think that as research scientists, a lot of us basically dream of being able to
make something in real world that actually serves a real problem.
So Google actually was, you know, cited the exciting in their blog post that they, like,
I don't know, like one out of 10 search queries are now improved by using Bert for both
re-ranking of the hits that you retrieve and you make a query.
And also generating those snippets that are, like, these little summaries of the pages
basically that are retrieved, which is, you know, pretty amazing to hear, honestly, as
just an LLP researcher. And I think that the main difference that they were citing was
the fact that now, because this is going from keyword search, which is, like, you know,
all the school, like, just search, like, information retrieval, et cetera, they are moving away
from that on towards natural language understanding for search.
They are capable of doing much more sophisticated query understanding, natural language understanding.
So, like, for example, they highlighted the fact that they are now understanding prepositions
much better than they used to. So, for example, queries such as, I don't know, like,
2019 Brazil traveler to USA needs a visa. They were saying that before Bert, they didn't
know that this means that, like, it should be a Brazilian traveling to US. But now with
Bert, they know that, like, what that to preposition actually means, and hence retrieve
better results, which is, you know, pretty, I would say, amazing outcome for the community
to see such an impact.
That's awesome. Yeah. I have definitely, well, I guess it goes without saying that I've
seen it all over the place as well. But you also mentioned GPT-2. Did you want to chat
about that as well? Absolutely. So, another thing that we cannot go
without talking about is GPT-2. GPT-2 definitely in the way that the whole, you know, release
of it was handled was one of the highlights of the year in AI, for sure, let alone NLP.
So, just, you know, for whoever that might not be familiar with GPT-2 was this another
larger scale pre-trained language model that basically came out of opening AI. The main
feature of it being the fact that it was large enough so their largest model was 1.5,
like, billion-providers. So, it was large enough and trained on good enough of it, you
know, sort of high-quality curated web scale data set that they were able to showcase
that they are generating really coherent outputs, paragraphs, and stories, you call it.
They also showed that through this particular very large language model that they've built,
they are able to do zero-shot generalization to other downstream tasks. So, not fine-tuning,
meaning you don't have it even a small scale particular training corpus to fine-tune the
model, just literally zero-shot right out of the box. You show that you can do, you know,
some level of, you know, you can compete with a state-of-the-art in some, you know, much less
than a state-of-the-art, but still makes some performance in machine translation, question
answer, and reading comprehension, summarization, et cetera. So, this was the work, but I would
say that the attention to this work, God, was not due to the performance necessarily,
but due to the way that it was released. So, what happened, and I'm sure, you know,
I know you guys have already covered this, so I'll just mention this quickly, that this
state-release process that they had in mind, where they basically cited that given how
amazing this work, this model is working, it's too basically dangerous for it to be released
to the public community for the potential of misuse. So, they held back, and they did
this stage-release process, they released the smallest model in February, 2019, then
in November, they finally reached a full 1.5 million-parameter model, but that whole process
sort of created this GPT-2 saga, of course, there were so many people that were kind of
outraged by the fact that, oh my goodness, this is open, and open AI, how could you not
release something that you have created, and then there were some proponents saying that
I don't know, and this is a good example of the community sort of thinking about the implications
of their work, et cetera. So, sitting data side, and I don't think it's the, as I said,
I know you guys have already debated this issue, but I think it was an interesting moment
for the NLP and AI research in general to have gone through this. But I wanted to mainly
talk about the text generation aspects. So, sitting beside the PR, and like the rights
or wrongs that the opening AI folks did, forward a way that they released this model. For
me, as a researcher myself, like having worked in text generation and still working on
it, I was really excited to get my hands on the largest spas model that they had given
them examples that they had in their paper, because the problem of doing cohere in national
language generation has been one of the longest-running problems in NLP community, for sure.
And I think anyone would be excited to know how far we've gone and tackling that problem.
So just looking back in time, again, like 2017 or so before any of these models were out,
I myself, like sort of characterized very viarbit language generation, saying that,
look, we have these at the time, this Verde RNN, LM, so Recurin Neural Network-based language
model is not transformed based language models. I would characterize them as being locally
coherent, meaning that they are very much capable of generating grammatical sentences,
but then generating logically sound paragraphs are still so longer than a sentence, and like
something like a story or narratives, which happens to be my area of research. They were
still super lacking. So just looking at the examples that they had in their paper made
me really excited to just try it out, right? So I was talking about story closed test,
very given like four sentences, a particular, very simplistic story generated basically
the ending, right? So I basically, after even the initial releases, the smaller models
put in them, but the latest and largest model, I personally tried out this, did you
see the GPT2 model for various such story closed test instances? So one, for example, that
I would want to highlight was just this very simple story that I'm going to build up on
top of. So the story is Lili was writing her scooter, a bike turn in front of her, she
tried to break abruptly, and she fell on the ground. And so that way that GPT2 continues,
as the story is, Lili fell into the lake. I dragged her out. She said that she could not
go down. I was desperately searching for another slant, blah, blah, blah, and you know, it
goes forever. So it goes to saying that the one of the major problems that I would characterize
about, like, neural language models for a national engagement generation back in 2017 was
that, here, as him is very good in hypothesizing why a model's generation is actually logically
sound, because if you have this way of projecting our own sense of meaning, right, from even
the most mindless generation. And I think what's happening here is just very much still in
line with that as well, that sure, you can hypothesize that probably Lili then fell into the
lake and then I dragged her out and she says she could not go down, et cetera. But it's
clear, as you know, the farther you go, and this was just one example, as I said, like,
I tried to sum so many more, the more you play with the model, the more you see that that
logically sound generation globally at, like, paragraph, little story, little, et cetera,
is still something that, as a community, we are lacking. And I think that there's a wide
range of, you know, generation tasks that you can work on and care about. And I guess
I would characterize this still to this day, I would say that generating logically sound
stories, narratives that make sense, a show common sense is, you know, one of the major
bottlenecks, I would say, of building an LP model that can work well, basically, effectively.
How would you characterize the differences between the smaller model and the full model that
was released later in the year in terms of storage generation? That's a very good question.
I, I never did a quantitative analysis, right? And that goes to one of the main other problems
we have in the area that I actually wanted to highlight, which is evaluation. These still
don't, like, as a community, we don't have a, a good way of evaluating generation. So
it's like we don't have a, they have automatically evaluating whether or not a system is generating
something sound, sort of meaning AI, judging AI, that's a major problem. So because of
that, it's been really hard to know, like, to sort of quantitatively measure progress.
So that's just a separate problem we have, which I'm hoping some, it should be something
actually that, as a community, we work harder on. And as you can imagine, right, the reason
we do industry courses as a multi-choice test set was so that it's evaluable, quickly,
systematically, easily, right? But it comes with a caveat of being basically gameable and
all the kinds of, you know, biases that we find about the data sets. So generation is
ideal, but then the flip side is we don't know how to evaluate generation. So setting
that aside qualitatively, and, you know, their proxies, we can use blue, et cetera, but
the fact that they don't correlate with human judgment is an issue that we just yet have
an address. But yeah, quantitatively looking at the results, there wasn't honestly that
much difference between the largest GPT-2 models that I've played with and the smallest,
but definitely a summary that people have to do more systematic evaluation.
Yeah, so one more thing I wanted to mention here, hopefully real quick, is about the fact
that so in the community after GPT-2 came out, there have been lots of back and forth in
different use cases that people have found. And, you know, there was even this, the interview
that was done with GPT-2, so many different angles that this whole line of research basically
has taken in the public eye, media coverage, et cetera. One thing that people, if you
people at least have rightfully pointed out is the fact that are we actually making real
progress towards national language understanding and national language generation through such
pieces of work? Are these models capable of building so-called mental models of the
world, right? So this is something that I'm personally extremely passionate about. And
like I'm hope, you know, at Elemental Commission, one of the pieces of work that we are hoping
to come out next year is exactly on this. So for the lily story that I was mentioning,
for example, like for any even like a child, human child reading that story, they would
know the causal chain of events that happened. They would know the emotional, you know, turbulence
that the character went through like, oh, she was like writing a scooter and then the
bike turn, oh, how was she feeling then? When she fell on the ground, oh, did she skin
her knee? How did she feel after being injured, et cetera? And we can basically build this
pretty consistent models of the world, mental models of the world, even as children read
a very short story. So I think that we are very far away from building an AI system that
can showcase such common implicit common sense, understanding of the world, even as a five-year-old
child would do. And I think there's enough evidence that the likes of GPT-2 are not
doing that, given the mistakes that they're making. And I think as a community, we should
focus on tackling such problems moving forward.
Yeah, I mean, this is probably a good time to note that the interview that I did with
David Faruji from Elemental Cognition, the title of that one was, are we being honest
about how difficult AI really is? Actually, turn out to be our number one, you know, most
popular show of 2019. Oh, wow. That's awesome. Yeah, yeah. But that was one of several that,
you know, spoke to kind of, you know, maybe a sobering perspective on the way we think
about AI and building models and what they're really capable of, what they, you know,
what we should be expecting out of them. And you're kind of echoing that same sentiment.
Exactly. Yes. Yes. So what's the next paper on your list?
So now that we kind of covered our bases with the main two pioneers or whatever we can
call them BERT and GPT-2s of the pre-train paradigm that we were living in 2019, I think
it's good to highlight one of the major advances that we could make through the likes of BERT,
et cetera, on a downstream task that was held as one of the feats of the year. So this
is a work that came out of AI-2 as well. It's called from F to A on the, on the New York
region science exam and overview of the RISTO project. So this is basically an accomplishment
that AI-2 had, which fills on top of the work that they've been doing for the past like
four or five years, at least, on tackling science exams. So, you know, what Lake Paul
Allen had this dream of doing, like building a digital RISTODL, and actually four years
ago or so, they made a challenge for the research community to come up with an AI system that
can be 10-8 greater in this standardized science test. So back then, to belief was that,
you know, we've, okay, we've built like IBM Watson, which is good at jeopardy. Can
we build a system that doesn't do jeopardy, but it's somewhat simpler, it just beats
an 8th grader. So, as I said, that was like one of the, like, one of Paul Allen's dreams,
but back in time, when they did this as a Kaggle contest on the best system that was submitted
got around like 60-something percent, which was far, far away from the human performance,
of course, or like a 8th grader performance to pass the test. So, fast forward, one of
the main advances, I would say, in 2019, that was made was that they, through using births,
so, both births and perverda, language models, they could boost their performance from
63 or something, I think, percent that they had achieved in 2016 to now 90.7% in 2019,
which was a passing score. So, this was pretty much of a feat in the field. For, you know,
various reasons, this, you know, like the choice of science exams is something that we
can debate, whether or not it's a good benchmark, but at least from the surface level, it seems
like, you know, science, such science questions require national linkage understanding, having
common sense knowledge, pretty broad common sense knowledge, knowing how the world works,
et cetera, and then reasoning capabilities, right? And also from like a more practical
standpoint, exams are accessible, measurable, right, the multi-choice exam, of course,
is something that you can quickly evaluate. So, it seems like a pretty compelling, these,
you know, the following seem like a pretty compelling reason to, to one account that is a good metric.
But, of course, as many even teachers would argue, standards, tastes like multiple choice tests
are not the best measure of intelligence. They are gameable, really like even children who
get good tests scores are not necessarily the most intelligent and learn the best in their classes.
So, there are those aspects, and I actually go at that AI2 folks have been pretty good with not
letting this get hyped up, right, out of their scope of what they would characterize, beyond their
scope of what they would characterize as their real outcome of this work. So, they even themselves
did some adversarial testing of the model. They showcase that if you add various other
multiple, like, what are some of their choices to this multiple choice instances that are like
likely to sort of be the answer, it's just this challenging answer. The model's performance
drops from 90%, 90 plus percent that it had thought into 60%. So, they have really even themselves
highlighted the fact that, look, this is, this is it. It's a narrow particular test set,
standard test set that this model is working well on. It doesn't mean true intelligence. Please
don't title this as, now we have an AI system that can be ties, coolers, etc. So, with that caveat,
aside from one more thing, by the way, they also mentioned that the real eighth graders
also answered the questions that include diagrams and they don't. So, that's another, you know,
point to take into account. But yeah, even all this, I think still, this is pretty amazing that
why basically introducing these large language models that do implicitly include lots of
word knowledge, lots of contextualized knowledge, lots of grammatical even knowledge,
you are able to boost your performance on such a test. Nice, nice. What's the next paper on your list?
So, the next paper is called right forward and wrong reasons. So, first of all, I love the
title. I think it surpasses so many things that it has gone wrong and could go wrong
in our, you know, without benchmarking and and open community. So, this this paper is sort of
for me an example from a host of different papers that have come out and are trying to show us
the blind spots of these models or all various ways that they are making the supposedly rights
predictions, but all for the wrong reasons. This actually, this kind of paradigm, not paradigm,
maybe a realization, I'd call it. And then on the community started a few years back and actually
to my knowledge, at least one of the first ones was on the very story close test test that we
ourselves did, very then we made it into a challenge. The top performing model actually had this
observation that turns out there in these biases in the way that the endings in the story
cost us are authored by our crowdsource workers. So, things like the fact that, oh, it turns out
that the wrong ending is, you know, often has like negative adjectives or it's like a longer
shorter, etc. So, these like synthetic sort of biases that are in the in our test sets which
we don't even realize. Now, remember us talking about how difficult it was to construct these
test examples without kind of various types of tells in them that, you know, would tip off
the model. Exactly. Exactly. It's very hard and I think we talked back then that you're even lucky
as, you know, just in the research community, we're lucky even to be catch these, right? God knows
which other benchmarks that we are using on a day to day basis have other implicit on like hidden
biases that we are not even aware of or is so hard to uncover. So, I think this is just, you know,
you thought other papers is 2019 paper, those outcomes like the story close test that was like 2017
actually. So, this is like two, three years after and still we are dealing with this problem.
So, this particular paper that was co-authored by folks from Johns Hopkins and Brown appeared in
ACL 2019, which to me is like just highlighting the growing movement in an LP community to move
beyond interpreting the test sets, you know, leaderboards as just pure achievements. But,
care more about analyzing what's actually the thing that these models are learning and hard to
perform in book. So, they actually, what this particular paper observes is that for the particular
task of MNLI, which is this multi-genre and natural language inference data set, they show that
there are superficial syntactic properties such as, like, whether or not the words in the sentence
that is going to be the, you know, on the prediction set overlaps with the one in the input.
So, like, pretty superficial, you kind of like go and scratch your head like, oh my god,
how come we are still doing this and we're having these problems after like three years of people
talking about this. But this is, you know, their reality. We've been, you know, sort of evaluating
our models on the benchmarks, which still have these hidden problems. And as I said, because
it's really hard, as you were discussing, it's really hard to uncover social biases.
So, what they did is that, and I think I can actually mention just once more, natural language
inference is the task where given a particular input, sentence, the system is, and another sentence,
you are supposed to classify whether or not the second sentence is an entailment or a contradiction,
or in some of these benchmarks neutral, meaning that it doesn't necessarily contradict or entail.
So, anyways, they did this analysis. They made this data set, sort of an adversarial data set,
Kant's called Kant's data set, where they actually curate these particular test instances,
which sort of uncover whether or not a particular model is using substantive heuristics.
For example, they have this heuristic, called lexical overlap, just pure lexical overlap.
The definition is that assume that a premise, which is the input sentence on the left hand side,
entails all the hypotheses constructed from the words in the premise.
So, if you, it can also, for example, the use is that, for example, if the premise is the doctor
was paid by the actor, you can hypothesize that the doctor paid the actor just because it has the
fully word lexical word overlap is going to be entailed by that sentence, but it is wrong, right?
So, the model that basically shouldn't say that that is an entailment, but if a model is biased
towards using such lexical overlap heuristics, it will wrongly classify that as correct and entailment.
So, what they do is that they show, show that actually, on this Kant's data set that they create,
that is actually true, that a lot of like a majority actually have the state-of-the-art models
on M and L.I. Data set, they're doing this very thing, that they were actually very inaccurate in
classifying such instances where the heuristic flip basically. So, they actually show that,
although that's the case, they show that if they augment these models and retrain them using
the Kant's data set, they can improve their performances. But the reason, as I said, the main reason
I wanted to highlight this paper is that still, 2019, we are dealing with the same problem
that we were dealing in 2017 of having models that are biased towards the intricacies of the
test sets and train sets that they're getting trained on, and that's something that we have to
keep working on moving forward. Yeah, I suspect that, you know, different versions of these problems
will keep us busy for quite some time, which actually leads us quite nicely into your predictions
for the field. Yes, absolutely. So, I think that, as I was just, you know, the way that we started
this whole conversation, I think they've come really along, Bay, in the past couple of years,
if not like the past decade, and tackling lots of low-hanging fruits in NLP using these
really amazing tools that we've built. But, you know, I think the papers that I had selected
kind of nicely highlight the problems we have as well, like the limitations and the kind of
weaknesses that these models tend to keep showing. And I think 2020 should be the year that we
start to get ambitious again, and think about how much, you know, harder kinds of problems
we can tackle moving forward, now that we have covered the basis sort of. So, actually this year,
2020, for the first time in the history of ACL conference, so ACL being a, you know, major
computational linguistic community conference, we have a special theme that asks the community
to write papers to reflect back on the progress of the field and what V as a community should be
focusing on moving forward. And I think that's pretty refreshing because it indicates that there
is this consensus that, look, from the outside, it feels like there's so many benchmarks that keep
getting beaten every month or so through these new other tools that come out bigger, better.
But where are we going with this? Are we actually defining truly what natural language
understanding means? Are we truly working on systems that show common sense, you know,
inferences of even a child? Are we actually building systems that can transfer the knowledge that
they have, what they learn from a test to another without really needing to get, you know,
retrained, et cetera? So, I think I would love for that to be how the, you know,
shift their focus in 2020. I think that we should focus on the things that we cannot do yet,
or have not basically have had the chance of doing because of having focused on the simpler
problems. I think one major issue we have with all these new things that we've built is that
still to this day in the industry, there are a lot of systems that use like old-school,
you know, rule-based models, pattern recognition, and the sense of just doing
reggae smashing, et cetera, because, you know, they know how they work, they know how to turn it off
and think the system, whatever makes a stupid mistake. But these, you know, very accurate actually
neural models, they often make stupid, stupid mistakes that we don't even know why, right? And
that's, I think, something that needs to be looked into, how can we build, sort of,
better controls over these highly accurate models to know where they could go wrong,
can we get guarantees, et cetera? And I think the more we move into areas at high stakes,
the more the need to do so. Do you think those controls look more like changes to the way
these models are trained or evaluated or lost functions or things like that, or more like hybrid
types of systems that incorporate elements of rules and elements of more modern NLP?
I think it could be either, right? I think what was very refreshing about the way that deep learning,
a revolutionized NLP in the past, a couple of years, is the fact that despite the mainstream,
there were many, you know, like, even if a smaller community, but there were folks who were still
doing research in the area and thinking beyond what the mainstream is dictating. And I think,
in order to make tremendous progress moving forward, we do need people who think differently.
We do need people who think they know, like, there's no way that deep learning is going to be the
silver bullet we have to think about a hybrid system. Or people who believe that, no, there's no way
that we can have, like, symbolic models incorporated into these neural models, and we have to just
fix the way that we do training in order to exhibit better generalization, better transfer of knowledge,
et cetera. So I think there's no way for me or anyone, honestly, to say which one is necessarily
going to thrive. I think the more people we have in the community caring about the right problems,
as opposed to the right approaches, that the higher chances of tackling these major remaining
problems in the area. What else do you foresee? So there are a couple of other things. I think that
the will start having more rigorous evaluations in place. I think that we would better know the
implications of establishing state-of-the-art on various benchmarks. As I was saying, there are
even environmental implications of all the sort of fact-planting that we do at this day and age.
And I think more people should think about those aspects of their work. Actually, there was
a work, another work called gray eye by UW people that they were encouraging the community to
also report the efficiency of their resource usage, along with the other classical metrics,
such as accuracy, et cetera. And they are reporting numbers. And I think those are really
interesting directions that the community could take. And potentially, we may no longer count
the best work of the year, the largest work of the year. Maybe we know that, oh, look,
this just had this really negative implication environmentally and whatever it wasn't fair. So we
can think beyond that, basically. And another thing is such a no-brainer. I think we are going
to start to work more and more on explainable models and interpretable models. So it's very
commonly the reason people care about explanation and interpretability is for the accountability issue,
for fairness issue, et cetera, which is really major. But the reason I personally am a big advocate
and I've been interested in working on this sort of past couple of years on V2. So even more broadly
at elemental cognition is the fact that explanation is this inherent capability of human beings,
right? Even a little child can explain the kinds of reasoning that they do. Of course,
we can argue, again, what is explanation. But I think building models that can be held accountable
towards the predictions they make and have ways of explaining it to an average human, which I would
argue should be through national language, is going to be something that we will see more and more
in 2020. And the probably, hopefully, last thing that I would mention is, of course, we have
to build causal models of the world that I was mentioning. We need to build systems that show
common sense, build systems that are able to basically build this causal map of the world, how
the events basically follow each other, how do we know this happens versus the other thing doesn't
happen. And what are the implications in terms of the emotions of characters who vary what,
et cetera. So I think these are really kinds of directions that the field should be taking moving
forward in the decade, not necessarily 2020. And I'm hoping, really, in the next eight,
nine years or so, we are going to say that finally, we have a system that can start to at least
show the basic common sense understanding of a five-year-old child. That's awesome. Awesome.
Nestering, thanks so much for taking the time to review your favorite papers of 2019 with us and
to talk through your predictions. At no doubt, it will be an exciting year in 2020 and NLP and
looking forward to keeping in touch on it. Yes, same here. Thank you so much, Sam,
looking forward to 2020. Thanks so much.
All right, everyone, that's our show for today. For more information on today's guest or for
links to any of the materials mentioned, check out twimmelai.com slash rewind19.
Be sure to leave us a five-star rating and a glowing review after you hit that subscribe
button on your favorite podcast catcher. Thanks so much for listening and catch you next time.
