WEBVTT

00:00.000 --> 00:15.960
All right, everyone. I am here with Alexander Richard. Alexander is a research

00:15.960 --> 00:20.920
scientist with Facebook reality labs. Alexander, welcome to the Twomol AI

00:20.920 --> 00:25.760
podcast. Hi, thanks for having me. Absolutely. Really looking forward to our

00:25.760 --> 00:30.080
conversation. Let's get started by having you share a little bit about your

00:30.080 --> 00:35.120
background. How'd you come to work in machine learning? That is basically just a

00:35.120 --> 00:39.280
chain of coincidences. I was never planning to do it. I was planning to study

00:39.280 --> 00:42.920
music, but you know, in order to have a good career. Yeah, but in order to have a

00:42.920 --> 00:46.400
good career, you know, you need to be extraordinary, which I am not, that's

00:46.400 --> 00:51.000
basic. So it was like, that was my backup plan. And like in the first semester at

00:51.000 --> 00:54.640
university, it was in Germany at university, where they had a big institute for

00:54.640 --> 00:58.240
speech recognition. And there was before smartphones were big, before there was

00:58.240 --> 01:02.000
Alexa, before there was Siri. And they had like this open house one day, and you

01:02.000 --> 01:05.960
know, free food, free drinks, as a poor student, of course, you go there. And they

01:05.960 --> 01:09.040
had this demo where you could talk into a microphone at the words that you said

01:09.040 --> 01:13.160
would just magically appear on the screen. And it was like, wow, I was fleshed by

01:13.160 --> 01:17.080
that. That was amazing. So that was probably the point when I decided I really

01:17.080 --> 01:20.680
want to go into machine learning and this exactly what I want to do. So I

01:20.680 --> 01:25.240
basically stayed at this institute until I got my masters for like four or five

01:25.240 --> 01:32.520
years and then changed to computer vision for the PhD and randomly ran to some

01:32.520 --> 01:36.840
guy I cited a lot because he was doing the PhD in the same topic that I did. And

01:36.840 --> 01:40.000
he was like, you know, I graduated, I'm at Facebook now. We're looking for interns.

01:40.000 --> 01:43.840
It's a slightly different field than what you're doing. But don't you want to

01:43.840 --> 01:48.200
join us? I'm never going to leave Europe or Germany. But well, for an internship,

01:48.200 --> 01:54.760
why not? So I came to Pittsburgh. I joined this lab and it was, again, an

01:54.760 --> 01:58.920
amazing experience, like the first time that I could really see that with my

01:58.920 --> 02:03.560
education, I can work on something that can potentially change the way we

02:03.560 --> 02:08.960
communicate and really have a big impact on people's lives. And there was so

02:08.960 --> 02:12.120
exciting that I completely made up my mind. And half a year later was like, okay,

02:12.120 --> 02:16.520
I graduate and I want to go back to Facebook. There was a great project, a great

02:16.520 --> 02:20.240
time and you work with great people. So that's how I ended up here. Super random.

02:20.240 --> 02:24.360
Awesome. Well, tell us a little bit about Facebook reality labs. I'm assuming

02:24.360 --> 02:30.080
there's some kind of CMU connection given. I mean, there are a lot of

02:30.080 --> 02:34.160
industry labs in Pittsburgh, probably due to CMU. Our director, yes,

02:34.160 --> 02:39.200
I used to be a CMU professor. Okay. I think that's the origin of it all. And

02:39.200 --> 02:45.400
that's also the origin how Facebook reality labs came to the city. Yeah, I mean,

02:45.400 --> 02:51.160
the lab is all about social talent presence, right? So our mission, so to

02:51.160 --> 02:57.120
say, is, well, if we would have succeeded already, then we wouldn't have this

02:57.120 --> 03:01.480
conversation over video conferencing, right? You wouldn't be a small rectangle in

03:01.480 --> 03:04.960
my big room. I wouldn't be a small rectangle in your big room. But we would

03:04.960 --> 03:08.080
both be wearing a virtual reality headset. And we would stand right next to each

03:08.080 --> 03:12.400
other and having this conversation in 3D in virtual reality in a shared space.

03:12.400 --> 03:16.320
And that is really the promise and the mission of this lab that we try to accomplish,

03:16.320 --> 03:20.560
which I believe is super exciting. And as I said, which is like this one thing that

03:20.560 --> 03:24.200
can really be a big leap forward in the way how we connect over the

03:24.200 --> 03:28.680
distance, which in my view, makes it so exciting to be part of that. Nice. And as

03:28.680 --> 03:34.600
the lab, what's the relationship between the Facebook reality lab and

03:34.600 --> 03:41.840
the Oculus? Yeah, I mean, it emerged from Oculus. It used to be Oculus research,

03:41.840 --> 03:46.360
actually, at the time when I interned there. And then transitioned into Facebook

03:46.360 --> 03:50.160
reality lab, which is now the big branch, which is looking into VR and AR

03:50.160 --> 03:56.960
at Facebook. Okay. Awesome. Awesome. So tell us a little bit about your

03:56.960 --> 04:02.680
research interests. We're going to be talking in particular about one of your

04:02.680 --> 04:07.960
papers, Neural Synthesis of Binaural Speech from Mano Audio, which was an

04:07.960 --> 04:13.920
ICLR best paper award winner. But you know, more broadly, I'd love to hear

04:13.920 --> 04:19.280
your general research interests. Right. I'm particularly interested in

04:19.280 --> 04:23.760
anything audio visual. And particularly, you know, in generative modeling from

04:23.760 --> 04:28.240
audio and visual input. And that is like, I believe one field that has been

04:28.240 --> 04:33.240
largely forgotten in early years of computer vision, that the visual modality

04:33.240 --> 04:36.240
always just gives you very limited information, particularly when you think

04:36.240 --> 04:40.080
about a webcam, where the resolution is low, or when you think about a virtual

04:40.080 --> 04:44.840
reality headset, where you just have very limited sensory data. And audio is

04:44.840 --> 04:48.400
like one of these modalities that gives you a lot of cues that can fill in

04:48.400 --> 04:52.440
the gaps. So it holds like a big promise on improving whatever we developed

04:52.440 --> 04:56.600
over the last years in computer vision. And really filling in what visual

04:56.600 --> 05:01.520
sensors in specific circumstances cannot give you. And I found that super

05:01.520 --> 05:04.880
exciting. So I was really starting to focus looking particularly onto how can

05:04.880 --> 05:09.200
we combine audio and vision in a generative modeling setting where we really

05:09.200 --> 05:13.600
want to generate something like realistic looking faces in the end. And you can

05:13.600 --> 05:18.200
imagine that this is difficult if in this specific application of virtual

05:18.200 --> 05:22.240
reality, if you wear a headset, your face is partly occluded, you just cannot

05:22.240 --> 05:27.240
observe anything. Your data is always lossy. And occluding audio is a much

05:27.240 --> 05:32.240
hard task or put in other words, it's much easier to get the complete audio

05:32.240 --> 05:35.240
information that is surrounding you than to get the complete visual

05:35.240 --> 05:41.240
information. Yeah, can you can you give an example that you think really

05:41.240 --> 05:48.240
illustrates the, you know, the ultimate power of combining audio and video?

05:48.240 --> 05:52.440
Yes, I think we should talk about two things there. One is clearly the

05:52.440 --> 05:57.040
sensing side. So what is the input, right? And let us take the example of a virtual

05:57.040 --> 06:00.440
reality headset. We might have a camera that is kind of facing your mouth, but

06:00.440 --> 06:04.440
it's very hard to really get good illumination of the interior of your mouth

06:04.440 --> 06:08.440
and have some accurate tongue modeling. It is very hard to get all the mouth

06:08.440 --> 06:12.040
all the mouthclosures correct. Like I have a beard, right? There is a camera

06:12.040 --> 06:17.440
facing towards me with a skewed angle. It is extremely hard to see every detail

06:17.440 --> 06:21.640
of my lips. But with audio, we get these details. I can't produce the P sound

06:21.640 --> 06:25.240
without closing my lips. I can't use the M sound without closing my lips.

06:25.240 --> 06:29.240
All these correlations are clearly some that we need to explore. That is like

06:29.240 --> 06:33.040
on the sensing side, but then we also work obviously on the on the output

06:33.040 --> 06:37.840
side where it comes to putting things into virtual reality. And we have

06:37.840 --> 06:40.840
this correlations between audio and vision there as well, right? I mean,

06:40.840 --> 06:44.240
I just have a single microphone here, but if we had multiple microphones

06:44.240 --> 06:47.640
and I would be snapping my finger, you should hear this from two different

06:47.640 --> 06:51.640
directions. And that needs to be some audio visual correspondence, which

06:51.640 --> 06:55.040
is frequently forgotten. It's freaking like, yeah, okay, we generate something

06:55.040 --> 06:59.040
where I am talking and I can do things with my hand, but we have mono audio

06:59.040 --> 07:02.240
and the systems. And that seems like leaving out half of the signal that

07:02.240 --> 07:09.040
is there. Yeah, I think, you know, thinking about video games that are

07:09.040 --> 07:14.440
kind of purely, you know, generated, you know, in many cases, like, you know,

07:14.440 --> 07:19.640
it's as much as much more like a movie production than like a generated

07:19.640 --> 07:24.040
something that a model generates. You know, I used to hearing kind of stereo

07:24.040 --> 07:30.440
sounds and, but I'm imagining that, you know, that's a lot more difficult

07:30.440 --> 07:37.040
when you're creating generative scenes from a model of some sort.

07:37.040 --> 07:41.840
Yeah, absolutely. I think one of the big problems here is that in video games

07:41.840 --> 07:44.840
it's usually fine if you have something that is just plausible, meaning

07:44.840 --> 07:48.040
the sound is coming from the right direction and kind of fits the environment

07:48.040 --> 07:51.640
that's all good. Same for the visual part, right? You have a synthetic

07:51.640 --> 07:55.440
video character that doesn't necessarily authentically look like you.

07:55.440 --> 08:00.440
So plausibility is fine in video games, but for the mission that we are pursuing

08:00.440 --> 08:03.640
plausibility is not enough. We really want to have accuracy. We really

08:03.640 --> 08:07.640
want to represent yourself in virtual reality that your loved ones, your

08:07.640 --> 08:10.840
friends, people who know you are close to you can really recognize you and

08:10.840 --> 08:14.640
recognize all the subtleties in your voice, in your facial experience.

08:14.640 --> 08:17.840
But all the expressions, this micro expressions that you do, right?

08:17.840 --> 08:21.040
That needs to be there if you want to have a real social experience.

08:21.040 --> 08:26.040
And that is the big difference to movies and games where you can just post

08:26.040 --> 08:28.640
process and just need to produce something that is plausible.

08:28.640 --> 08:36.040
Right. Right. Yeah. For a long time now, the kind of holy grill and ARVR is

08:36.040 --> 08:39.440
this idea of the uncanny valley, like there's something, there's just

08:39.440 --> 08:43.840
something wrong. I think for a long time it's been pretty basic things,

08:43.840 --> 08:49.640
you know, the resolution of the visor isn't high enough. I think you're

08:49.640 --> 08:54.640
pointing to, you know, maybe we've overcome some of those challenges.

08:54.640 --> 09:00.040
And now the uncanny valley frontier is a lot more subtle. Is that part of

09:00.040 --> 09:03.940
it? Absolutely. Absolutely. It is. Whenever we have social

09:03.940 --> 09:07.440
interaction, there is so much that we are trained as humans, right?

09:07.440 --> 09:11.440
So I have like decades of training talking to people. You have a lot of

09:11.440 --> 09:15.040
training talking to people and we really know how a social conversation

09:15.040 --> 09:18.840
between humans works and what these subtleties are and how to interpret

09:18.840 --> 09:22.240
them. Well, let's say our mind knows, but we do not know this actively.

09:22.240 --> 09:26.840
Yeah. So it's very hard to quantify. So how do we teach a machine to do

09:26.840 --> 09:30.440
this to do something we cannot even quantify ourselves? And that is the

09:30.440 --> 09:33.160
massive challenge now to go beyond the uncanny valley with the

09:33.160 --> 09:37.640
subtleties of communication. And our approach there is to argue that

09:37.640 --> 09:41.440
everything needs to be metric, meaning whatever we can measure in reality

09:41.440 --> 09:44.640
should be transferred like that into virtual reality. If we do that

09:44.640 --> 09:47.840
right, then automatically we will have all these subtle social cues and

09:47.840 --> 09:51.440
virtual reality as well. However, if we would resort to a solution that

09:51.440 --> 09:54.640
is only plausible, we might transfer something into virtual reality

09:54.640 --> 09:57.440
that substantially changes the meaning of what you are saying.

09:57.440 --> 10:00.840
Like, I don't know, it's my smile, a generally happy smile, or am I

10:00.840 --> 10:03.840
being sarcastic? This is very important for our conversation.

10:03.840 --> 10:07.040
But if we misinterpret that because we generate something that is

10:07.040 --> 10:11.440
just plausible, but not accurate, not metric, then we have a problem

10:11.440 --> 10:13.240
because we changed the meaning of conversation.

10:14.240 --> 10:18.640
I'd love to have you dig a little bit into that comment a bit more

10:18.640 --> 10:23.440
about metrics and having everything that you can measure in a

10:23.440 --> 10:27.440
physical world, be measurable in virtual reality. The thing that

10:28.640 --> 10:32.840
the thought that it prompts is that there are an infinite many things

10:32.840 --> 10:37.840
that we could measure in the physical world and to treat all of

10:37.840 --> 10:43.440
those as discrete metrics in the virtual world. And then, you

10:43.440 --> 10:46.840
know, try to train a model, for example, that's looking at, you

10:46.840 --> 10:52.240
know, so many metrics. You know, that's a stark contrast to the

10:52.240 --> 10:55.840
way things that the way things have generally been trending,

10:55.840 --> 11:00.440
which is, you know, let's just take pixels and try to, you know,

11:00.440 --> 11:04.440
focus on pixels and then the networks, if we can throw enough

11:04.440 --> 11:07.640
compute at them, they'll figure everything out. Can you elaborate on

11:07.640 --> 11:09.040
kind of the way you think about that?

11:09.040 --> 11:13.240
I mean, absolutely. So when we talk about training a network,

11:13.240 --> 11:16.640
we still want to optimize a metric loss. Like in standard L2 loss,

11:16.640 --> 11:19.240
what everyone else is doing in the vision community as well,

11:19.240 --> 11:21.640
when you have generative models, it's, you see that frequently

11:21.640 --> 11:24.240
when you have supervision, you optimize your L2 loss against

11:24.240 --> 11:29.040
your supervised signal. So we do not really try to reinvent that.

11:29.040 --> 11:31.640
But what we are saying, when we say we want to be metric,

11:31.640 --> 11:35.440
means our systems have to learn from the best possible measurements,

11:35.440 --> 11:38.640
meaning if I only give you data where you see like a snippet of

11:38.640 --> 11:43.440
my mouth and maybe images of my eyes and a mono audio signal,

11:43.440 --> 11:45.840
something we can maybe get from a headset, from a virtual reality

11:45.840 --> 11:50.040
headset, we cannot expect that we can make a realistic representation

11:50.040 --> 11:53.640
of that in virtual reality, if we do not have better measurements,

11:53.640 --> 11:57.040
meaning it all starts with accurate measurements, highly accurate

11:57.040 --> 11:59.640
measurements. So the research that we do is like you would come to

11:59.640 --> 12:03.240
Pittsburgh, you get a 3D scan of your face, and that is prior

12:03.240 --> 12:06.440
information that is accurate measurements about how your face moves.

12:06.440 --> 12:09.440
And then the next step is to take this metric information

12:09.440 --> 12:14.240
of this priors that the models have learned and take the VR headset

12:14.240 --> 12:18.240
input. And you know, we can take these parts that are visible and

12:18.240 --> 12:21.440
transfer them into virtual reality. But there are parts that are

12:21.440 --> 12:24.440
not visible that we cannot measure while you're wearing a headset.

12:24.440 --> 12:28.240
And for this, we need the strong priors. And the strong priors

12:28.240 --> 12:31.440
is what we get from very accurate measurements in the sense that

12:31.440 --> 12:35.840
we need a kind of set of people where we have high fidelity,

12:35.840 --> 12:39.440
3D scans of facial motion, facial expressions, so that we can

12:39.440 --> 12:42.440
fill in the gaps. So, you know, whenever information is not

12:42.440 --> 12:45.840
present, how can we get the closest to reality? Well, we need

12:45.840 --> 12:51.440
some kind of prior in whatever way. So you mentioned, you know,

12:51.440 --> 12:55.740
going to the lab and getting scanned the paper that we'll be

12:55.740 --> 12:59.740
talking about falls under the context of this broader effort

12:59.740 --> 13:02.840
called codec avatars. Is that what that is? That scanning

13:02.840 --> 13:06.440
process? And exactly. It's also a little bit more about the

13:06.440 --> 13:10.940
broader effort. Absolutely. The idea of codec avatars is that

13:10.940 --> 13:15.340
you can have an avatar of yourself in virtual reality that

13:15.340 --> 13:18.340
looks like you, sounds like you, moves like you, of course,

13:18.340 --> 13:21.440
not autonomously, but driven by you while you wear the headset

13:21.440 --> 13:26.340
with this lossy sensory input. And this whole problem looks like

13:26.340 --> 13:29.440
you were not talking about the forget the name of the apple

13:29.440 --> 13:34.640
avatars cartoons. We're talking. No, no, we are realistic

13:34.640 --> 13:38.940
avatars. Exactly. Ideally, indistinguishable from reality,

13:38.940 --> 13:41.540
of course, this is a very high bar that we said for ourselves

13:41.540 --> 13:44.340
there, but we are talking about photo realistic avatars.

13:44.940 --> 13:48.040
Because again, the same point that I made before, if we really

13:48.040 --> 13:50.640
want to transfer and transmit all these subtleties of

13:50.640 --> 13:53.940
communication, a comic style avatars just not enough. There is

13:53.940 --> 13:57.340
not this very personalized specific smirk that you might

13:57.340 --> 14:00.940
have when you smile and your very specific facial expression.

14:00.940 --> 14:04.440
So we need something that is photo realistic. And well, the

14:04.440 --> 14:07.140
way we approach this is essentially, well, it all starts with

14:07.140 --> 14:10.540
like accurate data measurements. So we need enough facial

14:10.540 --> 14:14.640
data to be able to make a high quality 3D reconstruction

14:14.640 --> 14:17.900
in VR. We usually talk codec avatars, since for like

14:17.900 --> 14:21.440
encoder decoder. So when we talk about the decoder side, that

14:21.440 --> 14:25.440
is what would generate your highly realistic 3D representation

14:25.440 --> 14:29.840
in virtual reality. And that is what we tried to learn from lots

14:29.840 --> 14:33.040
of data where we have lots of cameras capturing your face.

14:33.040 --> 14:36.440
And then we are able to reconstruct your face in 3D. Of course,

14:36.440 --> 14:38.740
while you are being captured, you cannot do every kind of

14:38.740 --> 14:42.340
expression that you can do in reality. But what we can ask you

14:42.340 --> 14:46.240
to do is some peak expressions, some example expressions. And

14:46.240 --> 14:49.340
we know that neural networks are magnificent interpolation

14:49.340 --> 14:51.840
machines. So we feed this data into neural networks. And then

14:51.840 --> 14:54.540
if you have some facial expression that falls in between two

14:54.540 --> 14:58.240
different peaks, we can generate a very accurate and faithful

14:58.240 --> 15:01.340
representation of that specific expression just from what

15:01.340 --> 15:03.940
the network has learned from the peak expressions that you

15:03.940 --> 15:08.340
can be doing. So this is the decoder side of codec avatars. On

15:08.340 --> 15:11.140
the encoder side, we are talking more about how can we take the

15:11.140 --> 15:14.040
sensory input from the headset, which is obviously lossy, we

15:14.040 --> 15:17.340
cannot place hundreds of cameras around here, right? How can

15:17.340 --> 15:21.540
we take this very limited data and map this to a representation

15:21.740 --> 15:25.240
where the decoder can interpret enough to reconstruct your

15:25.240 --> 15:29.440
face how it looks like. And as you can imagine, this is a very

15:29.440 --> 15:31.540
challenging problem that we have on the visual and on the

15:31.540 --> 15:35.740
audio side, because the data that we can collect for generating

15:35.740 --> 15:38.840
this 3D representation, well, we can have multiple cameras

15:38.840 --> 15:41.740
et cetera, of your face, but we can never have paired data

15:41.740 --> 15:44.740
with whenever you're wearing your headset, because your face

15:44.740 --> 15:47.440
is occluded by the headset, your sound might be different

15:47.440 --> 15:50.140
because you know, you're in a massive capture stage that is

15:50.340 --> 15:53.040
loud because there is a sea and stuff like that for visual

15:53.040 --> 15:55.340
captures, but then for audio captures, you might be in another

15:55.340 --> 15:58.540
room, I have high ceilings that might be reverberation. So all

15:58.540 --> 16:01.040
these differences in the domains that we have on the input

16:01.040 --> 16:03.640
side and on what we want to generate, this poses the big

16:03.640 --> 16:06.640
problem of how can we actually connect these two parts, the

16:06.640 --> 16:10.340
encoder and the decoder. And that is one of the big challenges

16:10.340 --> 16:14.140
that's probably a bit deep if we want to dig into the details

16:14.140 --> 16:16.940
here for codec avatars in general, but I'm happy to answer any

16:16.940 --> 16:20.140
questions in specific if you want to dig deeper on that.

16:21.740 --> 16:26.240
Since you're inviting us to elaborate a little bit on the

16:26.240 --> 16:30.440
challenges there, what makes the connection between these two

16:30.440 --> 16:33.740
the key source of challenge in this problem domain?

16:34.240 --> 16:38.440
Right. So what people have shown is that, you know, you have

16:38.440 --> 16:41.740
variational auto encoder, you have generative for zero networks.

16:41.840 --> 16:45.840
So if you learn a representation of data that you captured,

16:45.840 --> 16:49.340
we have shown that we can generate faithful and highly accurate

16:49.340 --> 16:52.240
representations of, for example, your face or even of sound,

16:52.240 --> 16:54.640
if you look at what's going on in the Texas Beach community,

16:54.840 --> 16:57.740
that is extremely realistic and works extremely well.

16:57.740 --> 17:02.040
What is a bit lacking is the question if you have a noisy

17:02.040 --> 17:05.040
input for audio that could be speech with like a lot of background

17:05.040 --> 17:08.240
noise with kids running around with car driving with an AC

17:08.240 --> 17:11.140
running for vision that is you wear the headset, you have

17:11.640 --> 17:15.040
difficult illumination, you have varying background because

17:15.040 --> 17:18.640
you move around. And the question is, how can we use this

17:18.640 --> 17:21.540
information to really condition our models that can generate

17:21.540 --> 17:25.240
this faithful representations and still match exactly what we

17:25.240 --> 17:31.240
have seen on the sensory input? And yeah, so one way we approach

17:31.240 --> 17:35.440
this is by essentially looking at the renders in VR.

17:35.440 --> 17:38.940
So if you think about the whole pipeline, you have headset

17:38.940 --> 17:43.540
inputs, we want to decode this to some numeric representation

17:43.540 --> 17:46.740
that can be transmitted from the transmitted to the receiver.

17:46.940 --> 17:49.940
And then on the receiver side, we want to have this big network

17:49.940 --> 17:52.640
that creates this code and generates your face out of that.

17:52.640 --> 17:55.540
The question is, how do we match the code? And yeah, for that

17:55.540 --> 17:59.340
we basically apply some standards, domain transfer techniques

17:59.340 --> 18:04.340
it's based on guns based on some other techniques, it is

18:04.340 --> 18:08.540
always difficult to evaluate this because a major problem is

18:08.540 --> 18:10.940
that we do not have correspondences. So how do we really

18:10.940 --> 18:14.440
know that what we represent is absolutely faithful and 100

18:14.440 --> 18:16.940
percent faithful. And yeah, that is where one of the big

18:16.940 --> 18:19.140
research challenges lies for the future.

18:19.140 --> 18:22.940
And is the idea that you've got the headset and the headset

18:22.940 --> 18:28.940
has cameras looking in at the eyes? And is there a, is there

18:28.940 --> 18:32.940
a research foundation that comes out of, I don't know where

18:32.940 --> 18:37.340
we'll come out of, you know, biology, psychology, neuropsychology

18:37.340 --> 18:41.440
that says that the eyes are robust enough to tell you

18:41.440 --> 18:43.440
everything that might be happening with the face?

18:44.140 --> 18:47.940
We try to stay away from these kind of priors because

18:47.940 --> 18:50.540
as I said, there are several things in communication that we

18:50.540 --> 18:53.140
just cannot quantify and even psychology on neuroscience

18:53.140 --> 18:55.840
cannot really tell us which parts of the face are important

18:55.840 --> 18:58.740
for which parts of nonverbal communication specifically.

18:58.940 --> 19:03.240
So what we try is really to get the most amount of data we

19:03.240 --> 19:06.740
can get from a headset, whatever restricts us in terms of

19:06.740 --> 19:10.940
hardware, but that is our approach to this and not relying on

19:10.940 --> 19:13.540
something where we introduce human knowledge, which in the

19:13.540 --> 19:16.940
end might be wrong or might be biased. And it's just not

19:16.940 --> 19:20.140
data driven anymore in that case, but there's an assumption

19:20.140 --> 19:24.740
that there is that relationship, the eyes, you know, we're

19:24.740 --> 19:27.340
going to look at the eyes and we're going to try to predict

19:27.340 --> 19:31.040
the face and with enough data, we hope that that relationship

19:31.040 --> 19:31.540
holds.

19:32.140 --> 19:34.740
Absolutely. I mean, if you just look at the anatomy of your

19:34.740 --> 19:36.940
face, you see that some parts of your face, just if more

19:36.940 --> 19:40.340
muscles can expose much more motion and that is like areas

19:40.340 --> 19:43.940
around the lips, which are certainly, you know, harder or like

19:43.940 --> 19:47.240
more important to actually observe than your ears, which barely

19:47.240 --> 19:50.040
move at all and which are very easy to just plug in from

19:50.040 --> 19:51.940
prior information that we have from you.

19:52.440 --> 19:59.040
Yeah. Yeah. Awesome. Awesome. So tell us about the specific

19:59.040 --> 20:02.620
paper, the neural synthesis of the neural speech from

20:02.620 --> 20:07.140
Mono audio. What's the kind of what's the setting and

20:07.140 --> 20:12.740
motivation? Yeah. Let's draw a quick line to computer vision

20:12.740 --> 20:16.940
or computer graphics. 3D neural rendering is big in that field

20:16.940 --> 20:20.040
for years and has led to massive improvements. And for some

20:20.040 --> 20:23.340
reason, audio was kind of lacking behind that, right? So when

20:23.340 --> 20:28.440
you look at like, how do you do specialist audio in computer

20:28.440 --> 20:32.040
games and movies, it is all breaking down to linear time and

20:32.040 --> 20:35.340
variant systems. You have some filters that you measure in an

20:35.340 --> 20:40.240
idealized anicoic setup. And then you just like stack a bunch

20:40.240 --> 20:42.940
of linear filters on top of each other and get some plausible

20:42.940 --> 20:48.240
reconstruction. And we wondered why this is the case and why we

20:48.240 --> 20:50.840
do not follow the same route that computer graphics is going

20:50.840 --> 20:54.240
where you have some 3D renderers that you can train and to end

20:54.240 --> 20:57.940
from data. Because that is ultimately what we want, right? If

20:57.940 --> 21:01.240
we want to represent reality, we do not want to have a linear

21:01.240 --> 21:04.040
function that is as close as possible. But we want to have

21:04.040 --> 21:06.540
data that we can measure and that we can optimize against.

21:06.540 --> 21:10.240
And that was basically the premise we started this work with.

21:10.240 --> 21:15.040
And it was kind of interesting to see, first of all, to see

21:15.040 --> 21:18.040
how much better traditional signal processing and audio

21:18.040 --> 21:22.540
processing is compared to how good traditional computer

21:22.540 --> 21:25.840
graphics methods were, right? So if you look at a traditional

21:25.840 --> 21:28.440
render of a computer graphics method, traditional face renders,

21:28.440 --> 21:31.740
you had facial models that were crude approximations and we're

21:31.740 --> 21:34.640
really looking on Kenny. And if you look at results like

21:34.640 --> 21:37.540
recent progress in neuro rendering, that looks super realistic.

21:37.840 --> 21:40.840
In audio, it seems that this gap was much smaller probably

21:40.840 --> 21:43.540
because audio is well understood on a mathematical way has

21:43.540 --> 21:47.140
less uncertainty in it inherently. But we were still figuring

21:47.140 --> 21:52.640
that there is there is the stack of linear transformations

21:52.640 --> 21:55.540
that you do in signal processing that can gradually introduce

21:55.540 --> 21:57.840
more and more errors. And you always make assumptions, you always

21:57.840 --> 22:01.240
make an assumption that your room behaves in a very specific

22:01.240 --> 22:04.040
way. You make assumptions that your ears behave in a very

22:04.040 --> 22:06.940
specific way, in the way, you know, how they modify sound.

22:07.240 --> 22:10.940
And that is not really true. Let me tell you about a funny

22:10.940 --> 22:14.340
experiment we did initially. We used by neural microphones

22:14.340 --> 22:16.840
that we stuck into the ears of a colleague of mine. And he was

22:16.840 --> 22:20.040
just walking around and recording sound. And I was putting on

22:20.040 --> 22:22.640
my headphone and listening to that recording. And because our

22:22.640 --> 22:25.940
two ear shapes were so different, to me, it always sounded as

22:25.940 --> 22:28.840
it was coming from behind my head. And that was clearly not

22:28.840 --> 22:32.140
the case in reality. So there is clearly something where this

22:32.140 --> 22:34.940
traditional signal processing based approaches reach their

22:34.940 --> 22:37.940
limits. And if we have a data driven approach, we can really

22:37.940 --> 22:41.540
learn something from evidence from data and do not have to make

22:41.540 --> 22:45.240
modeling assumptions. And we could show in this paper that we

22:45.240 --> 22:48.940
can really improve the quality of audio in this case. And I

22:48.940 --> 22:51.740
think it's a very exciting direction to think about 3D

22:51.740 --> 22:55.040
modeling of audio in the way that we think about 3D modeling

22:55.040 --> 23:02.740
of graphics. And so when you talk about improving the quality

23:02.740 --> 23:07.940
of audio, you know, drill down into the specific metric that

23:07.940 --> 23:09.340
you're looking at there.

23:10.140 --> 23:14.640
Yeah, also super difficult. We of course went went down the

23:14.640 --> 23:17.100
first way that you always do in computer vision. You

23:17.100 --> 23:19.540
optimize an else who lost on the raw waveform. Why shouldn't

23:19.540 --> 23:22.340
it work? If you reach zero loss, you are perfect. So it's all

23:22.340 --> 23:25.340
good. The problem is in reality, of course, you never reach a

23:25.340 --> 23:28.040
zero loss. And then the question is, what kind of errors do

23:28.040 --> 23:31.340
you have? And it seems that particularly in audio, this

23:31.340 --> 23:34.340
discrepancy between the value of your L2 loss and the

23:34.340 --> 23:37.640
perceptual quality of the sound that you generate is pretty

23:37.640 --> 23:42.140
massive. So you can easily imagine that more so than a video,

23:42.140 --> 23:45.140
you can easily imagine that you have like tiny deviations

23:45.140 --> 23:48.140
that just imagine you have a sine wave in audio. And you

23:48.140 --> 23:50.740
have like a tiny flickering, you always say pretty close, but

23:50.740 --> 23:53.240
you have like a high frequency signal that flickers. All of

23:53.240 --> 23:55.540
a sudden, you have the super annoying high frequency noise

23:55.540 --> 23:59.440
in your signal that we perceptually find really disturbing

23:59.440 --> 24:02.540
and can immediately say, well, this is not real. So that was

24:02.740 --> 24:06.140
really challenging. And we were diving deeper into this

24:06.140 --> 24:09.140
looking into the loss functions and how to train audio. And

24:09.140 --> 24:12.340
one of the observations is that the L2 loss has a very

24:12.340 --> 24:16.140
undesirable property audio is basically a composition of

24:16.640 --> 24:19.540
amplitude information and face information amplitude, meaning

24:19.540 --> 24:23.340
how high are the peaks of the sine waves and face information

24:23.340 --> 24:25.840
meaning, where does the sine wave start? How much is it

24:25.840 --> 24:29.340
shifted, right? And then we can add up potentially infinitely

24:29.340 --> 24:31.940
many sine waves. And this is the audio signal that you can

24:31.940 --> 24:35.440
hear, essentially a four year decomposition. And what we

24:35.440 --> 24:39.040
figured out is that the L2 loss, you can actually show the L

24:39.040 --> 24:42.240
2 loss optimizes the amplitudes aggressively in the

24:42.240 --> 24:45.440
beginning of the training process of a neural network, but

24:45.440 --> 24:47.940
completely neglects the face information. That means you

24:47.940 --> 24:51.140
try to fit amplitudes of signals where you shouldn't even

24:51.140 --> 24:53.940
fit the amplitude because if you just would shift the signals

24:53.940 --> 24:56.940
accordingly, it would solve all your problems. The L2 loss is

24:56.940 --> 25:01.140
not doing this inherently, it is keeping, it is trying to

25:01.140 --> 25:03.440
match the amplitudes, but it's not trying to match the face

25:03.440 --> 25:06.140
and it's not trying to match the face at a very late time

25:06.140 --> 25:11.240
in training, which turns out to be a problem. So the very simple

25:11.240 --> 25:14.240
solution on the loss side is to explicitly optimize for this

25:14.240 --> 25:18.440
face information. However, face information is highly

25:18.440 --> 25:21.740
difficult because as soon as you have noise and in every real

25:21.740 --> 25:24.240
life measurement, you do have noise as soon as you have

25:24.240 --> 25:29.640
noise, this is not really reliable data. So we went a step

25:29.640 --> 25:34.040
further and tried to implement something about noise in both

25:34.040 --> 25:37.340
the amplitude direction and the face direction. Yeah,

25:37.340 --> 25:40.640
absolutely. I mean, we tried to record audio in a room and

25:40.640 --> 25:43.040
we tried to get something that is as clean as possible, but

25:43.040 --> 25:45.740
you might have electrical noise because of your setup. You

25:45.740 --> 25:49.340
might have some airflow that is, I don't know, going into the

25:49.340 --> 25:52.340
microphones and that is that is air pressure, right, which is

25:52.340 --> 25:56.140
interpreted as sound. So all these subtleties really corrupt

25:56.140 --> 25:59.140
your data constantly when you have audio, and you have to

25:59.140 --> 26:04.240
account for that. So yeah, we kind of shot for a solution that

26:04.240 --> 26:08.040
is on the modeling side. And I believe that is something that

26:08.040 --> 26:10.840
has been highly successful in all of deep learning that you

26:10.840 --> 26:15.040
take some component of traditional processing and incorporated

26:15.040 --> 26:18.040
in your networks, the most prominent success stories, certainly

26:18.040 --> 26:20.140
the convolution operation, right, bringing convolution

26:20.140 --> 26:24.340
into your networks has boost computer vision. In audio

26:24.340 --> 26:27.640
dynamic time, warping is a very old traditional technique

26:27.640 --> 26:31.640
where you want to align signals in some way. And we essentially

26:31.640 --> 26:34.040
incorporate that as a neural network layer, so fully

26:34.040 --> 26:36.640
differentiable neural network layer to solve this face

26:36.640 --> 26:39.840
problem because this time warping allows you to shift

26:39.840 --> 26:43.040
components of the audio signal left or right depending on where

26:43.040 --> 26:46.040
you want to have them. And yeah, it turned out that once

26:46.040 --> 26:50.240
more incorporating one of these traditional methods can have

26:50.240 --> 26:53.040
a big impact on how well neural networks perform.

26:54.240 --> 27:00.440
And so in the problem that you've formulated, what are the

27:00.640 --> 27:04.240
the inputs is it, you know, clearly there's a mono audio

27:04.540 --> 27:09.740
and the output is going to be some stereo audio. But are

27:09.740 --> 27:15.440
you also, you know, you localized as their location inputs

27:15.440 --> 27:18.740
like how what are the very inputs? Yeah, let's let's talk

27:18.740 --> 27:20.640
about this very specific application where we want to

27:20.640 --> 27:23.440
generate by a neural sound, meaning that is spatialized where

27:23.440 --> 27:25.940
it can hear where it's coming from, but that also sounds as if

27:25.940 --> 27:28.340
you perceive it with your ears, you know, all the transformations

27:28.340 --> 27:31.240
that your ears do to the sound. The input clearly, as you said,

27:31.240 --> 27:34.940
is mono audio. I might have a microphone very close to my lips

27:35.140 --> 27:37.940
and I just speak into that and that is all I need as input

27:37.940 --> 27:41.140
signal, but I also need to know is the relative position

27:41.140 --> 27:43.640
between you and me. If we are in a virtual environment and you

27:43.640 --> 27:46.440
are standing two meters apart to the right of me, your

27:46.440 --> 27:48.840
voice will sound different as if you're like super close to my

27:48.840 --> 27:51.940
face and on the left of me. So that is like the second input.

27:51.940 --> 27:55.040
Where are we in the virtual space in relation to each other?

27:56.040 --> 27:59.040
And given this input, we want to modify your mono signal

27:59.440 --> 28:02.940
based on these positions so that you receive a stereo

28:02.940 --> 28:06.540
signal on your headphones that is, you know, simulating this

28:06.540 --> 28:08.840
effect of you hear it with your own ears, meaning the

28:08.840 --> 28:11.840
spatialization is correct and the transformations, the signal

28:11.840 --> 28:14.840
undergoes with your ears is correct as well. So that is

28:14.840 --> 28:18.540
essentially the problem setting of this particular problem.

28:19.240 --> 28:23.540
What we were restricting ourselves to is one set of ears

28:23.540 --> 28:26.540
because you can imagine ears are like fingerprints like

28:26.540 --> 28:30.640
everyone has a different ear shape and it's a really unsolved

28:30.640 --> 28:33.440
problem how to generalize to different ear shapes. So we use

28:33.440 --> 28:36.240
like a kind of medium ear shape, a generic ear shape that

28:36.240 --> 28:40.240
works kind of well for everyone. But apart from that, it's

28:40.240 --> 28:42.640
really this full setting of having mono signal and just the

28:42.640 --> 28:45.440
positions and getting a complete manner of signal synthesized

28:45.440 --> 28:46.240
on your headphones.

28:46.440 --> 28:52.240
Okay. And what is the the data set that you use for the

28:52.240 --> 28:55.140
modeling? How did you collect it? What challenges did you find

28:55.140 --> 28:55.440
there?

28:56.340 --> 29:00.240
Yeah, interesting question. Glad you asked for that. It was a

29:00.240 --> 29:04.840
long process to actually get a successful capture. We started

29:04.840 --> 29:09.340
trying something where you can buy mannequins with silicon

29:09.340 --> 29:13.240
ears that behave similar to you flesh of similar properties.

29:13.440 --> 29:17.040
And we started with that and placed just such a mannequin

29:17.040 --> 29:20.140
into a room and then had a tracking system that we can put

29:20.140 --> 29:22.540
markers on top, basically a motion tracking system,

29:22.540 --> 29:24.640
markers on top of the mannequin, markers on top of the

29:24.640 --> 29:27.440
participant who is walking around and talking to the mannequin.

29:27.640 --> 29:30.440
There were ears and microphones in the ears of the mannequin

29:30.640 --> 29:34.540
and we get sound so far so good. We have tracked everything

29:34.540 --> 29:36.940
so we get all the information we need, right? The position

29:36.940 --> 29:40.440
between mannequin and speaker and also the binarital sound.

29:42.440 --> 29:46.940
The initial round of this was completely useless and without

29:46.940 --> 29:50.040
any success. We had an AC running in that room and if you

29:50.040 --> 29:52.240
spend like, I don't know, two hours in the room, it gets

29:52.240 --> 29:56.040
hot at the AC turns on and off and on and off. And then you

29:56.040 --> 29:59.240
could hear footsteps of people and you could hear people

29:59.240 --> 30:02.140
breathing as they came too close to the microphone. So you

30:02.140 --> 30:04.640
can imagine all these things, all these lessons that you

30:04.640 --> 30:06.640
that you learned when you do a data collection that you never

30:06.640 --> 30:07.640
did before.

30:09.140 --> 30:12.640
We can look at that as some kind of regularization or

30:13.640 --> 30:15.640
the main adaptation or something.

30:15.640 --> 30:18.240
That is true. That is true. But with the premise that we want

30:18.240 --> 30:21.640
to learn metric information that we really want to get out

30:21.640 --> 30:24.340
with reality sounds like regularization is bad, right?

30:24.340 --> 30:27.040
Regularization is just like another prior that we impose

30:27.040 --> 30:30.240
another assumption that we make on the signal and ideally we

30:30.240 --> 30:32.340
want to learn from a signal that is as clean as possible.

30:32.340 --> 30:35.240
So it was really worth going all the way of making a room

30:35.240 --> 30:38.240
acoustically treated that it's silent enough and that you

30:38.240 --> 30:43.840
really just have the default room reverberations and these

30:43.840 --> 30:47.440
audio effects that you have when you talk in a room, but no

30:47.440 --> 30:48.440
noise floor anymore.

30:49.440 --> 30:53.040
We basically went down all this way, installed some

30:53.040 --> 30:56.240
acoustic paneling and improved the capture setup

30:56.240 --> 30:59.040
significantly, which in the end gave us successful data.

30:59.040 --> 31:03.040
And it should also be mentioned that like, if you compare

31:03.040 --> 31:06.040
to how this has been solved before, you need to build an

31:06.040 --> 31:08.040
unequake chamber and then you would record just the

31:08.040 --> 31:11.040
transformations that are here that your ear are doing in an

31:11.040 --> 31:13.040
unequake chamber because you do not have room reverberations

31:13.040 --> 31:14.040
there, right?

31:14.040 --> 31:17.040
So you just capture that as a bunch of fixed spatial

31:17.040 --> 31:19.040
positions and you have a model for that.

31:19.040 --> 31:22.040
And then you have another model where you measure room in

31:22.040 --> 31:24.040
pulse responses at different places in a room.

31:24.040 --> 31:26.040
And then again, you try to stack this together.

31:26.040 --> 31:28.040
That is what traditional processing is doing.

31:28.040 --> 31:32.040
And one of the big disadvantages there was you always measure

31:32.040 --> 31:34.040
as discrete spaces in time.

31:34.040 --> 31:37.040
And if you can imagine you walk around, this is how sound

31:37.040 --> 31:38.040
changes, right?

31:38.040 --> 31:39.040
Doppler effect.

31:39.040 --> 31:42.040
If I get closer to some sound source or closer to a

31:42.040 --> 31:46.040
microphone, the sound waves basically compress and you get

31:46.040 --> 31:48.040
like these, yeah, these motion effects.

31:48.040 --> 31:50.040
And you can never capture this in a static setup.

31:50.040 --> 31:53.040
So we were really shooting for something where we have this

31:53.040 --> 31:56.040
dynamic setup where we track a person in real time and we

31:56.040 --> 31:59.040
have moving trajectories of sound that we can model.

31:59.040 --> 32:03.040
And I think, yeah, that is probably one of the reasons that

32:03.040 --> 32:05.040
the system in the end turned out to work so well.

32:05.040 --> 32:08.040
That this is like the first time that you have this realistic

32:08.040 --> 32:11.040
scenario of moving sound sources.

32:11.040 --> 32:16.040
Now, I can even with that said, I can imagine a pretty broad

32:16.040 --> 32:18.040
spectrum of complexity.

32:18.040 --> 32:23.040
Like I can imagine looking at the relative position of the

32:23.040 --> 32:26.040
participant in the mannequin in 2D space.

32:26.040 --> 32:29.040
Can imagine then looking at them in 3D space.

32:29.040 --> 32:33.040
You know, I can imagine then, you know, extending that to

32:33.040 --> 32:37.040
include like radio direction for each other part.

32:37.040 --> 32:40.040
Like how did you manage that complexity factor?

32:40.040 --> 32:41.040
Absolutely.

32:41.040 --> 32:44.040
I mean, we tried the simplest thing which is restricting

32:44.040 --> 32:47.040
ourselves to a kind of donut of social interaction.

32:47.040 --> 32:50.040
People usually tend to feel uncomfortable if I can very

32:50.040 --> 32:53.040
close them and conversations.

32:53.040 --> 32:56.040
Or if I'm very far, I wouldn't have a conversation but I

32:56.040 --> 32:57.040
would move close efforts.

32:57.040 --> 33:00.040
So we restricted ourself to the circle around all like basically

33:00.040 --> 33:03.040
this donut around people where social interactions typically

33:03.040 --> 33:04.040
happen.

33:04.040 --> 33:07.040
So we had a restricted input space which can be helped.

33:07.040 --> 33:10.040
One thing that we completely misjudged is a mannequin is

33:10.040 --> 33:11.040
always standing still.

33:11.040 --> 33:14.040
If you have a user in virtual reality, putting on a virtual

33:14.040 --> 33:16.040
reality has seen something here and something.

33:16.040 --> 33:19.040
First thing that you do is look around in all the directions.

33:19.040 --> 33:22.040
So we tried to imply our first model and it was all

33:22.040 --> 33:25.040
breaking because this was all movement that we didn't

33:25.040 --> 33:28.040
see and that we didn't account for in this simplified setting.

33:28.040 --> 33:31.040
So we actually moved forward from this donut allowed

33:31.040 --> 33:33.040
near field captures where people get really close to the

33:33.040 --> 33:36.040
mannequin where people move around to talk from the top

33:36.040 --> 33:40.040
and from below into the ears of the mannequin to cover

33:40.040 --> 33:42.040
much denser spatial coverage.

33:42.040 --> 33:45.040
And then in the end, there are other effects that you have

33:45.040 --> 33:48.040
to have to think of not just positions but even speech

33:48.040 --> 33:49.040
direction.

33:49.040 --> 33:53.040
I have a head which is modifying the way I sound.

33:53.040 --> 33:56.040
So if I look in this direction, I sound different to you as

33:56.040 --> 33:58.040
if I look straight to your face.

33:58.040 --> 34:00.040
And these were like effects that we didn't model at all.

34:00.040 --> 34:03.040
Well, yeah, this is neglectable but it is honestly not.

34:03.040 --> 34:06.040
If you want to learn from the raw waveforms and that is such

34:06.040 --> 34:09.040
sensitive data, then this is not neglectable and you really

34:09.040 --> 34:13.040
need all these additional information that you can get.

34:13.040 --> 34:17.040
So talk a little bit about the method or approach

34:17.040 --> 34:20.040
that you came up with for solving the problem.

34:20.040 --> 34:21.040
Yeah, absolutely.

34:21.040 --> 34:22.040
I'd love to.

34:22.040 --> 34:28.040
So the massive advancement in deep learning for audio is

34:28.040 --> 34:32.040
certainly wave net, which was this 2016 paper by DeepMind.

34:32.040 --> 34:36.040
And it has ever since revolutionized text to speech and

34:36.040 --> 34:40.040
vocoding, meaning transferring mouse pectograms of frequency

34:40.040 --> 34:43.040
information into actual waveforms of speech.

34:43.040 --> 34:46.040
So this is the logical starting point whenever you do

34:46.040 --> 34:49.040
something where you have generative audio and you want to

34:49.040 --> 34:50.040
generate a raw waveform.

34:50.040 --> 34:53.040
And that is where we also started from a standard wave net.

34:53.040 --> 34:57.040
And we just used a conditioning on this position information

34:57.040 --> 34:59.040
between transmitter and receiver.

34:59.040 --> 35:00.040
We were hoping that this would do the job.

35:00.040 --> 35:03.040
This wouldn't be like a exciting research inside,

35:03.040 --> 35:05.040
but in terms of getting the job done that would have been

35:05.040 --> 35:06.040
amazing.

35:06.040 --> 35:09.040
But as you can expect, you get a lot of distortions.

35:09.040 --> 35:12.040
You get into a lot of problems for the reasons that

35:12.040 --> 35:15.040
I told you before that, you know, it is hard to match

35:15.040 --> 35:16.040
the face.

35:16.040 --> 35:19.040
Also, this original wave net architecture has been designed

35:19.040 --> 35:21.040
to produce something that is plausible, but not necessarily

35:21.040 --> 35:23.040
matrically accurate.

35:23.040 --> 35:25.040
So we were facing all these problems.

35:25.040 --> 35:27.040
And from there on, extending the architecture.

35:27.040 --> 35:31.040
One of the major components is that, well, if you have

35:31.040 --> 35:34.040
a BINORAL audio, this sound takes some time to travel

35:34.040 --> 35:36.040
from me to you.

35:36.040 --> 35:39.040
And we can clearly geometrically map that.

35:39.040 --> 35:41.040
But this is never correct because what does it mean

35:41.040 --> 35:42.040
geometrically?

35:42.040 --> 35:44.040
If I look in this direction, it travels in a slightly

35:44.040 --> 35:45.040
different way.

35:45.040 --> 35:47.040
If you look away, sound has to travel around your head.

35:47.040 --> 35:50.040
So just going by distances is not enough.

35:50.040 --> 35:53.040
But what we do want to do is having some warping that

35:53.040 --> 35:57.040
tells us, OK, we are at time T when I emit the sound.

35:57.040 --> 36:01.040
What is time T prime when this specific part of sound

36:01.040 --> 36:03.040
arrives at your ears?

36:03.040 --> 36:06.040
And this is what we implemented as a form of neural

36:06.040 --> 36:07.040
time warping.

36:07.040 --> 36:10.040
Basically taking dynamic time warping, this very traditional

36:10.040 --> 36:13.040
approach to align two time sequences, which is not

36:13.040 --> 36:16.040
differentiable and putting it into a differentiable setting

36:16.040 --> 36:19.040
while still maintaining physical properties.

36:19.040 --> 36:22.040
Meaning, sound has to be causal, right?

36:22.040 --> 36:24.040
I mean, if I say something, it cannot arrive at your ears

36:24.040 --> 36:26.040
before I actually said it.

36:26.040 --> 36:27.040
It has to be monotonous.

36:27.040 --> 36:29.040
So what I say first arrives at your ears first.

36:29.040 --> 36:32.040
Unless I move faster than speed of sound, which unfortunately

36:32.040 --> 36:33.040
I can't.

36:33.040 --> 36:35.040
And we have to incorporate these physical properties

36:35.040 --> 36:36.040
into the model.

36:36.040 --> 36:37.040
And this is what we essentially did.

36:37.040 --> 36:40.040
We designed this differentiable version of dynamic

36:40.040 --> 36:43.040
time warping as a neural network layer.

36:43.040 --> 36:46.040
And this enables us to maintain all the physical properties

36:46.040 --> 36:49.040
of sound, but at the same time still aligning

36:49.040 --> 36:51.040
exactly between the transmitter and the receiver.

36:51.040 --> 36:54.040
And from that point, if you have an exact alignment,

36:54.040 --> 36:57.040
standard deep learning approaches, convolutional

36:57.040 --> 37:01.040
time domain convolutional architectures, do a very good job

37:01.040 --> 37:03.040
in changing the amplitudes of the signal.

37:03.040 --> 37:06.040
But the face of the signal has already been accounted for

37:06.040 --> 37:08.040
to a large amount because you already

37:08.040 --> 37:11.040
want everything in the time domain on the right position.

37:11.040 --> 37:14.040
So you solve one of the major problems, one of the major

37:14.040 --> 37:17.040
issues that L2 lost a struggle with doing.

37:17.040 --> 37:21.040
And if you start from there, essentially, was a piece of cake.

37:21.040 --> 37:25.040
And so what assumptions did you need to make to make that

37:25.040 --> 37:28.040
differentiable?

37:28.040 --> 37:31.040
Not too many assumptions, actually, because you can formulate

37:31.040 --> 37:35.040
most of these, most of these components in a quite straightforward

37:35.040 --> 37:36.040
way.

37:36.040 --> 37:39.040
One assumption is clear that you do not move faster than

37:39.040 --> 37:41.040
speed of sound, but for any kind of human.

37:41.040 --> 37:42.040
It's reasonable.

37:42.040 --> 37:44.040
Opposition that is absolutely reasonable.

37:44.040 --> 37:46.040
Absolutely.

37:46.040 --> 37:50.040
One maybe slightly bigger limitation or assumption that

37:50.040 --> 37:53.040
we had to make in this whole architecture and this whole

37:53.040 --> 37:57.040
network is that the acoustic properties of your environment

37:57.040 --> 37:58.040
are the same.

37:58.040 --> 38:01.040
So what we cannot model is changing acoustic properties

38:01.040 --> 38:04.040
because, I don't know, you stand super close to a wall

38:04.040 --> 38:07.040
and you would have different diffractions.

38:07.040 --> 38:10.040
That would require you to really have a prior on rooms and to

38:10.040 --> 38:13.040
really have a good representation of the 3D scene you are in.

38:13.040 --> 38:16.040
So what the model learns is a kind of global representation

38:16.040 --> 38:18.040
of the room acoustics you are in.

38:18.040 --> 38:22.040
But it does not learn that you are standing in the corner of

38:22.040 --> 38:24.040
the room and it sounds significantly different.

38:24.040 --> 38:27.040
This is something we need to solve if we want to bring this

38:27.040 --> 38:30.040
technology into virtual reality and want to have a realistic

38:30.040 --> 38:31.040
impression.

38:31.040 --> 38:34.040
But yeah, in this initial work, this is something we had to

38:34.040 --> 38:37.040
abstract from and really focus on only like the global

38:37.040 --> 38:40.040
appearance of sound and getting the spatialization and

38:40.040 --> 38:42.040
banalization correct.

38:42.040 --> 38:47.040
I'm trying to remember if we've talked about this already

38:47.040 --> 38:52.040
in a slightly different context, but the relationship

38:52.040 --> 38:57.040
between the metrics that you're training on and like

38:57.040 --> 38:59.040
perceptual metrics.

38:59.040 --> 39:02.040
You know, from everything we've said thus far,

39:02.040 --> 39:05.040
you know, you've got a room that's kind of a fixed

39:05.040 --> 39:08.040
size and fixed geometry.

39:08.040 --> 39:12.040
You've got the mannequin ears and you've talked at length

39:12.040 --> 39:16.040
about how sensitive the models are to those specifics.

39:16.040 --> 39:19.040
It seems like you can do very well on your metrics,

39:19.040 --> 39:21.040
but still from a perception perspective.

39:21.040 --> 39:25.040
And maybe this is more generalization than from person

39:25.040 --> 39:29.040
to person than perception, still have issues.

39:29.040 --> 39:31.040
Can you talk a little bit about that?

39:31.040 --> 39:33.040
Yeah, that's a very good question.

39:33.040 --> 39:35.040
We need to make a distinction here where we have good

39:35.040 --> 39:37.040
generalization properties and where we don't.

39:37.040 --> 39:39.040
Because in the setup, you always have a transmitter and

39:39.040 --> 39:40.040
a receiver.

39:40.040 --> 39:42.040
That is fundamentally different from vision where we kind

39:42.040 --> 39:44.040
of assume if you render an image, we all see it in the same

39:44.040 --> 39:45.040
way.

39:45.040 --> 39:47.040
We do not care that maybe your eyes are slightly different

39:47.040 --> 39:48.040
than my eyes.

39:48.040 --> 39:50.040
For audio, we do care because our ears are fundamentally

39:50.040 --> 39:51.040
different.

39:51.040 --> 39:54.040
And we generalize really well on the transmitter side.

39:54.040 --> 39:57.040
Meaning it doesn't really matter which kind of person is

39:57.040 --> 39:58.040
speaking.

39:58.040 --> 40:00.040
We can train on a very small amount of people.

40:00.040 --> 40:01.040
A few dozen is enough.

40:01.040 --> 40:05.040
And generalize to any kind of arbitrary voice and get

40:05.040 --> 40:07.040
an accurate representation and reconstruction of this

40:07.040 --> 40:09.040
arbitrary transmitter voice.

40:09.040 --> 40:12.040
When it comes to the receiver, of course, the generalization

40:12.040 --> 40:15.040
is well, we have one fixed ear pair.

40:15.040 --> 40:18.040
So if you have an extraordinary ear shape,

40:18.040 --> 40:21.040
you might have something that is typically called front back

40:21.040 --> 40:23.040
confusion that you cannot tell if the sound is coming

40:23.040 --> 40:25.040
from the front or from the back of you.

40:25.040 --> 40:28.040
And these are issues we can really only solve if we can

40:28.040 --> 40:30.040
generalize towards ears.

40:30.040 --> 40:33.040
Which again gives us this close connection to codec

40:33.040 --> 40:37.040
avatars because if we do have a realistic representation

40:37.040 --> 40:40.040
of your head and of your ear shape of your, you know,

40:40.040 --> 40:43.040
of your head geometry in the end, what we hope is that we

40:43.040 --> 40:47.040
can personalize the receiver side based on this ear

40:47.040 --> 40:50.040
geometries that we know from codec avatars.

40:50.040 --> 40:54.040
And how about the room geometry?

40:54.040 --> 40:59.040
Does that generalize well across different types of

40:59.040 --> 41:00.040
environments?

41:00.040 --> 41:01.040
Yes.

41:01.040 --> 41:03.040
Quite frankly, no, it does not.

41:03.040 --> 41:07.040
If you record audio in this kind of setting,

41:07.040 --> 41:10.040
or you were a virtual reality headset, your microphone

41:10.040 --> 41:12.040
would be super close to your mouth.

41:12.040 --> 41:15.040
So you do not really pick up a lot of these room acoustics

41:15.040 --> 41:16.040
anyway.

41:16.040 --> 41:18.040
And the signal you have is almost unequally.

41:18.040 --> 41:21.040
It doesn't really have a lot of the information of the room

41:21.040 --> 41:24.040
you are in if it's very close to your mouth anyway.

41:24.040 --> 41:26.040
So from that point of view, that is fine.

41:26.040 --> 41:28.040
The question is, how can we, you know,

41:28.040 --> 41:31.040
transfer the system into different virtual rooms?

41:31.040 --> 41:35.040
At the moment, the way it sounds is the sound of the room

41:35.040 --> 41:37.040
where we collected the data.

41:37.040 --> 41:40.040
This might not necessarily be the only room in virtual reality

41:40.040 --> 41:41.040
you want to be.

41:41.040 --> 41:44.040
And honestly, you probably want to be in a variety of rooms,

41:44.040 --> 41:47.040
which means we have to put some work into disentangling

41:47.040 --> 41:49.040
this part, disentangling the binarialization part

41:49.040 --> 41:51.040
from the room acoustics part.

41:51.040 --> 41:54.040
And one way to go there is to have a better capture setup

41:54.040 --> 41:58.040
where the room where you record all this specialized audio data,

41:58.040 --> 42:01.040
all this binarial audio data is quasi-anicoic.

42:01.040 --> 42:04.040
So basically doesn't really have room responses at all.

42:04.040 --> 42:07.040
And if you have that, and then if you have at the other time,

42:07.040 --> 42:09.040
captures in multiple rooms where you can learn

42:09.040 --> 42:11.040
characteristics of the room responses,

42:11.040 --> 42:13.040
you can disentangle these two effects

42:13.040 --> 42:15.040
and model them sequentially.

42:15.040 --> 42:18.040
But again, in a data driven way, this is very hard

42:18.040 --> 42:19.040
because then the question is again,

42:19.040 --> 42:22.040
how do we get correspondences between the data we capture

42:22.040 --> 42:27.040
in rooms and the data we capture in this binarial capture stage?

42:27.040 --> 42:29.040
It's something we are looking into,

42:29.040 --> 42:33.040
but we do not have a solution to that yet.

42:33.040 --> 42:34.040
Okay.

42:34.040 --> 42:38.040
So we've talked about the setup.

42:38.040 --> 42:40.040
We've talked about the approach.

42:40.040 --> 42:44.040
We've talked about metrics and evaluation a bit.

42:44.040 --> 42:48.040
We've also talked about this big picture

42:48.040 --> 42:51.040
that you're trying to get to.

42:51.040 --> 42:54.040
What are the next steps from where you are

42:54.040 --> 42:56.040
to get you towards the bigger picture?

42:56.040 --> 42:59.040
Where does this research direction take you?

42:59.040 --> 43:01.040
Oh, yeah, absolutely.

43:01.040 --> 43:06.040
As I said initially, my interest is working on audio visual topics

43:06.040 --> 43:09.040
and this binarialization, the sound binarialization paper

43:09.040 --> 43:13.040
is basically an approach to do 3D rendering of sound.

43:13.040 --> 43:16.040
So where's the visual component here, right?

43:16.040 --> 43:19.040
And again, if we talk about codec avatars, encoders,

43:19.040 --> 43:21.040
and decoders, there are these two elements

43:21.040 --> 43:23.040
where we need to fuse the vision in.

43:23.040 --> 43:26.040
On the decoder side very clearly, we talked about ear shapes.

43:26.040 --> 43:28.040
So we need to make sure that the visual information

43:28.040 --> 43:32.040
can influence the way you perceive the sound based on your ear geometry,

43:32.040 --> 43:34.040
based on your hatch geometry.

43:34.040 --> 43:36.040
And on the side of the encoder data,

43:36.040 --> 43:41.040
we talked about how we kind of rely on a quasi-anechoic mono input, right?

43:41.040 --> 43:44.040
But what if you are in a noisy environment?

43:44.040 --> 43:46.040
What if there are kids running around?

43:46.040 --> 43:48.040
We don't want to binarialize that sound, right?

43:48.040 --> 43:50.040
If I'm in VR and your dog, your kids,

43:50.040 --> 43:52.040
whatever is making noise in the background,

43:52.040 --> 43:55.040
it's not present in the virtual environment here.

43:55.040 --> 43:57.040
And so we need to get rid of that.

43:57.040 --> 44:00.040
So that is like another big direction where we have to think about

44:00.040 --> 44:03.040
how can we take the sensory data from the headset,

44:03.040 --> 44:06.040
use the audio sensors, the microphone that we have,

44:06.040 --> 44:08.040
but also use the visual sensors to make sure

44:08.040 --> 44:11.040
that only what you actually say,

44:11.040 --> 44:13.040
only your actual speech is transferred into virtual reality.

44:13.040 --> 44:15.040
And we get rid of all of these other noises

44:15.040 --> 44:17.040
and all of these other background signals

44:17.040 --> 44:20.040
that we are not interested in, you know,

44:20.040 --> 44:22.040
transferring into virtual reality.

44:22.040 --> 44:26.040
Well, Alexander, thanks so much for taking the time to share with us

44:26.040 --> 44:28.040
a bit about what you're up to.

44:28.040 --> 44:30.040
Congrats on the best paper award.

44:30.040 --> 44:33.040
I guess I should ask you, you know, what is your...

44:33.040 --> 44:36.040
What do you think were the factors that

44:36.040 --> 44:43.040
led to the judges picking out this paper for that award?

44:43.040 --> 44:45.040
I mean, the judges are probably the best people

44:45.040 --> 44:48.040
to talk to for that question, but if you ask me,

44:48.040 --> 44:53.040
I believe it's probably this novelty of the field,

44:53.040 --> 44:55.040
you know, moving for audio,

44:55.040 --> 44:57.040
which is still predominantly signal processing

44:57.040 --> 44:59.040
and linear time invariant systems

44:59.040 --> 45:01.040
to what data-driven deep approaches,

45:01.040 --> 45:03.040
having something that is, you know,

45:03.040 --> 45:06.040
a 3D rendering equivalent for audio.

45:06.040 --> 45:08.040
And I think this is just like something

45:08.040 --> 45:09.040
that has not been done before.

45:09.040 --> 45:13.040
And it is probably kind of unclear why audio

45:13.040 --> 45:17.040
is like lagging behind the computer graphics community

45:17.040 --> 45:18.040
in that sense.

45:18.040 --> 45:20.040
And I believe that is probably the big impact of the paper

45:20.040 --> 45:23.040
that we are able to make this step into 3D audio rendering

45:23.040 --> 45:26.040
that is data-driven and deep.

45:26.040 --> 45:28.040
That's awesome. Well, congrats again on that.

45:28.040 --> 45:32.040
I'm looking forward to the follow-on, I think.

45:32.040 --> 45:35.040
A lot of interest in the community now

45:35.040 --> 45:38.040
about kind of multi-modal, multi-channel,

45:38.040 --> 45:41.040
combining audio, video, and other modalities.

45:41.040 --> 45:43.040
Oh, absolutely. Sure.

45:43.040 --> 45:46.040
Yeah, I mean, we are always happy to host interns

45:46.040 --> 45:49.040
and have people over who are interested in this work.

45:49.040 --> 45:53.040
So super happy if someone here is interested

45:53.040 --> 45:55.040
in this direction of audio-visual.

45:55.040 --> 45:57.040
We are always looking for interested.

45:57.040 --> 45:58.040
Mission reach out to you.

45:58.040 --> 46:00.040
Oh, of course.

46:00.040 --> 46:03.040
To our recruiters, whatever you want.

46:03.040 --> 46:04.040
No, it's always welcome.

46:04.040 --> 46:06.040
I feel it's a community that needs to grow.

46:06.040 --> 46:08.040
I feel the community really looking at this multi-modal

46:08.040 --> 46:11.040
problems, this audio vision problems, is too small.

46:11.040 --> 46:13.040
I skimmed over the nearest publications

46:13.040 --> 46:16.040
and there were like nine papers that had sound in that title,

46:16.040 --> 46:18.040
which seems to be too low for a conference

46:18.040 --> 46:19.040
of the size of the nearest.

46:19.040 --> 46:21.040
So I really feel it's a field that needs to grow.

46:21.040 --> 46:23.040
Yeah, absolutely.

46:23.040 --> 46:26.040
Well, once again, Alexander, thank you so much for joining us.

46:26.040 --> 46:30.040
Thank you for having me.

