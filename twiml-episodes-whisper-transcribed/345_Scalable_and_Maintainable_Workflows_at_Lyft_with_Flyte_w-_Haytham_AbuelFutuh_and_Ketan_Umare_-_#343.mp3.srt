1
00:00:00,000 --> 00:00:13,400
Welcome to the Twomo AI Podcast.

2
00:00:13,400 --> 00:00:15,920
I'm your host Sam Charrington.

3
00:00:15,920 --> 00:00:24,920
Hey everyone, producer Amari here.

4
00:00:24,920 --> 00:00:28,800
As we mentioned on the podcast last week, we've got some exciting new additions to our

5
00:00:28,800 --> 00:00:35,480
Twomo community programs, including the IBM AI Enterprise Workflow Specialization

6
00:00:35,480 --> 00:00:41,720
Study Group, which will be hosted by Sam, and the causal modeling in machine learning

7
00:00:41,720 --> 00:00:47,240
study group hosted by Robert Osa-Zua-Ness, who you heard from earlier this week on the

8
00:00:47,240 --> 00:00:48,840
topic of causality.

9
00:00:48,840 --> 00:00:54,080
Today, I want to remind you that Sam and Robert are hosting a course and study group overview

10
00:00:54,080 --> 00:00:58,960
session this Saturday, February 1st, at 8 a.m. pacific time.

11
00:00:58,960 --> 00:01:03,400
To get signed up for this course, or to learn more about the other upcoming and ongoing

12
00:01:03,400 --> 00:01:08,680
Twomo community programs, visit TwomoAI.com slash community.

13
00:01:08,680 --> 00:01:11,680
And now on to the show.

14
00:01:11,680 --> 00:01:21,640
Alright everyone, I am here with Heitem Abul Fatou and Katen Umare from Lift, Heitem

15
00:01:21,640 --> 00:01:24,520
and Katen, welcome to the TwomoAI podcast.

16
00:01:24,520 --> 00:01:25,520
Thank you.

17
00:01:25,520 --> 00:01:26,520
Thank you.

18
00:01:26,520 --> 00:01:29,880
So, we're here in once again, sunny San Diego.

19
00:01:29,880 --> 00:01:35,160
We got the two days of like horrible, miserable rainy weather, you know, earlier in this

20
00:01:35,160 --> 00:01:36,160
week.

21
00:01:36,160 --> 00:01:40,680
And we're here to talk about some of what you're doing in Lift, namely the flight project

22
00:01:40,680 --> 00:01:44,800
that you just presented on yesterday, it's open sourced.

23
00:01:44,800 --> 00:01:45,800
And it's open sourced.

24
00:01:45,800 --> 00:01:46,800
And it's open sourced.

25
00:01:46,800 --> 00:01:50,920
We're going to dig deep into this, but before we do, I'd love to have each of you kind

26
00:01:50,920 --> 00:01:54,520
of introduce yourself, share a little bit about your background, how you got to work

27
00:01:54,520 --> 00:01:58,120
on ML and Fra and, you know, what's your story?

28
00:01:58,120 --> 00:01:59,120
Talk to us.

29
00:01:59,120 --> 00:02:00,120
Katen?

30
00:02:00,120 --> 00:02:01,120
Yeah.

31
00:02:01,120 --> 00:02:02,440
Hey, my name is Katen.

32
00:02:02,440 --> 00:02:10,840
I lead the flight team and probably one of the founding person of the flight at Lift.

33
00:02:10,840 --> 00:02:18,240
So my background is I work across different industries from hedge funds to retail, to logistics,

34
00:02:18,240 --> 00:02:19,240
to cloud.

35
00:02:19,240 --> 00:02:25,920
I'm mapping and now finally back in sort of transportation area.

36
00:02:25,920 --> 00:02:32,160
And one of the things I've been interested in is just large scale data processing and

37
00:02:32,160 --> 00:02:40,200
solving business problems where data and computation comes together and machine learning is

38
00:02:40,200 --> 00:02:42,840
a place where, you know, that's really, really important.

39
00:02:42,840 --> 00:02:48,240
I started Lift in 2016, but I started on flight close to the end of 2016.

40
00:02:48,240 --> 00:02:52,840
And it was mostly like, you know, I started working on this team.

41
00:02:52,840 --> 00:02:58,320
We were trying to get ETAs, which are, I'll explain what an ETA is.

42
00:02:58,320 --> 00:03:04,000
So when you open up and lift up and you see, hey, three minutes to get your driver or if

43
00:03:04,000 --> 00:03:08,800
you are in a car and you see like it will take 15 minutes to reach the airport, sometimes

44
00:03:08,800 --> 00:03:10,320
it's accurate.

45
00:03:10,320 --> 00:03:14,600
And it's accurate mostly because of a ton of machine learning models that go in the

46
00:03:14,600 --> 00:03:19,720
background and including understanding how the road traffic is and understanding all

47
00:03:19,720 --> 00:03:25,560
kinds of things that are happening on in the current conditions on the road.

48
00:03:25,560 --> 00:03:33,880
So using, so I was leading the team and which I joined the team, there was another engineer

49
00:03:33,880 --> 00:03:34,880
on it.

50
00:03:34,880 --> 00:03:42,600
He used to run the models all on his laptop and he was like, how are you running this?

51
00:03:42,600 --> 00:03:48,200
And he's like, I have this script, I've just run this and it just runs and it figures

52
00:03:48,200 --> 00:03:49,200
out.

53
00:03:49,200 --> 00:03:52,480
And you know, then I run this other script and then I run this other script and I do that's

54
00:03:52,480 --> 00:03:53,480
crazy.

55
00:03:53,480 --> 00:03:54,480
Yeah.

56
00:03:54,480 --> 00:03:59,760
So, and we, I'm trying to decide whether to interrupt you and just like dive deep into

57
00:03:59,760 --> 00:04:00,760
that.

58
00:04:00,760 --> 00:04:01,760
That sounds crazy.

59
00:04:01,760 --> 00:04:02,760
It is crazy.

60
00:04:02,760 --> 00:04:08,440
Like running a model that's doing like live prediction of ETAs on a platform, like training

61
00:04:08,440 --> 00:04:12,480
training the model, training the model or collecting the data for the model aren't

62
00:04:12,480 --> 00:04:13,680
things like that, right?

63
00:04:13,680 --> 00:04:17,200
And still reproducibility issues and stuff like that.

64
00:04:17,200 --> 00:04:18,200
A lot of issues.

65
00:04:18,200 --> 00:04:19,200
Yeah.

66
00:04:19,200 --> 00:04:21,960
But you wouldn't be surprised how many times this happens in the industry, right?

67
00:04:21,960 --> 00:04:27,640
It's just like the, and this is actually how we lead into it because this is the current

68
00:04:27,640 --> 00:04:32,600
state of the infrastructure for machine learning and especially productionizing models is,

69
00:04:32,600 --> 00:04:33,600
it didn't exist.

70
00:04:33,600 --> 00:04:38,440
We didn't think about retraining these models at that time and quickly we wanted to retrain

71
00:04:38,440 --> 00:04:42,040
them and then I'm the laptop's not going to scale.

72
00:04:42,040 --> 00:04:46,400
Now, the other story that happened, so this is just leading into flight, right?

73
00:04:46,400 --> 00:04:50,640
And the other story that happened is there was a research scientist on my team.

74
00:04:50,640 --> 00:04:53,120
He created a model and that model is pretty cool.

75
00:04:53,120 --> 00:04:55,360
It served live for many years.

76
00:04:55,360 --> 00:05:01,000
But he left the company and the model went with him probably, so we lost it.

77
00:05:01,000 --> 00:05:06,520
Like we had no idea where the model was and as leader, they told me, hey, let's recreate

78
00:05:06,520 --> 00:05:11,680
this model and I, I don't know how to and I, we knew the algorithm.

79
00:05:11,680 --> 00:05:15,400
So we just rewrote it and we got everything done.

80
00:05:15,400 --> 00:05:18,320
It would not give the same results.

81
00:05:18,320 --> 00:05:23,280
And it, like it literally took us three months to get like to the same level of accuracy.

82
00:05:23,280 --> 00:05:24,280
Wow.

83
00:05:24,280 --> 00:05:29,800
And we were like, okay, so he had done all of this extra work that we really kind of lost.

84
00:05:29,800 --> 00:05:33,640
We didn't waste too much effort on it because we knew the algorithm, we knew some of the

85
00:05:33,640 --> 00:05:34,640
tricks.

86
00:05:34,640 --> 00:05:38,480
But still, it's like wasted effort in trying something out and then going and trying

87
00:05:38,480 --> 00:05:40,680
out the accuracy and you're like, oh, it's time to say.

88
00:05:40,680 --> 00:05:41,680
Yeah.

89
00:05:41,680 --> 00:05:42,680
Yeah.

90
00:05:42,680 --> 00:05:45,400
It's not like one person spending all the three months, but it's still like, it's like,

91
00:05:45,400 --> 00:05:46,880
it's wasted effort.

92
00:05:46,880 --> 00:05:51,320
So at that time, I decided that this needs to be, this needs to change.

93
00:05:51,320 --> 00:05:55,160
And delivering new models became slower and slower.

94
00:05:55,160 --> 00:05:57,720
So that was the birth of flight.

95
00:05:57,720 --> 00:06:00,040
At that point, we used to call it a bad name.

96
00:06:00,040 --> 00:06:07,800
I'm not even going to put it on the podcast, but I used to, and we wrote, like I wrote

97
00:06:07,800 --> 00:06:11,800
like a first draft proposal internally and everybody was like, you are crazy.

98
00:06:11,800 --> 00:06:14,640
This thing is not, never going to work.

99
00:06:14,640 --> 00:06:23,320
But somehow in like a couple months, we erected a V zero, V one of this thing.

100
00:06:23,320 --> 00:06:29,200
And we got a team to try it out and this team was also struggling a lot with delivering

101
00:06:29,200 --> 00:06:30,200
their models.

102
00:06:30,200 --> 00:06:36,120
And the intersection where flight really fits in is when you have a lot of data and you

103
00:06:36,120 --> 00:06:40,720
want to reproduce, reproduce your models again and again, like maybe every day, every week

104
00:06:40,720 --> 00:06:42,360
or every hour.

105
00:06:42,360 --> 00:06:47,320
And you want like the trace of what happened and the lineage between everything.

106
00:06:47,320 --> 00:06:49,400
And this team actually fit the bill.

107
00:06:49,400 --> 00:06:52,720
And they, for the first time, they were able to deliver a model in like six months.

108
00:06:52,720 --> 00:06:54,080
And this was a gigantic model.

109
00:06:54,080 --> 00:06:58,160
It affected the bottom line of lift and it was really, really meaningful.

110
00:06:58,160 --> 00:07:04,760
And it was not without a lot of, you know, stress and working hard through the night.

111
00:07:04,760 --> 00:07:10,280
But that was the starting of flight and that was in 2017.

112
00:07:10,280 --> 00:07:17,200
And then we didn't stop like the use of the company just skyrocketed.

113
00:07:17,200 --> 00:07:20,720
And we at that point, we are like, hey, we should open source this thing because it's

114
00:07:20,720 --> 00:07:26,640
such a big problem to solve that a small team at lift can probably never solve it on

115
00:07:26,640 --> 00:07:27,640
their own.

116
00:07:27,640 --> 00:07:31,680
And if you open sources, we should be able to work with the community here, more ideas

117
00:07:31,680 --> 00:07:33,600
and improve it all the time.

118
00:07:33,600 --> 00:07:42,480
So we actually rewrote everything from scratch, made it Kubernetes native, took, took like

119
00:07:42,480 --> 00:07:46,560
the primitives that we understood from looking at all the various use cases.

120
00:07:46,560 --> 00:07:53,680
And that's the amazing part of lift, like it's a rich ground of amazing use cases.

121
00:07:53,680 --> 00:07:58,440
And we used all of that and put like basically distilled that information into flight.

122
00:07:58,440 --> 00:08:01,600
And that's our first cut into the world.

123
00:08:01,600 --> 00:08:02,600
Hi, Tom.

124
00:08:02,600 --> 00:08:06,240
Tell us about your background and what you do at lift.

125
00:08:06,240 --> 00:08:07,240
Sure.

126
00:08:07,240 --> 00:08:08,240
Yes.

127
00:08:08,240 --> 00:08:10,040
So my name is Haytan Abouhtou.

128
00:08:10,040 --> 00:08:16,880
I have worked previously at Microsoft, Google, and I had a journey up and down the stack.

129
00:08:16,880 --> 00:08:24,040
I worked in like enterprise, create applications in low-level storage, like Azure storage.

130
00:08:24,040 --> 00:08:30,720
And I, you know, at some point, wanted to try out ML and I kind of found this sweet spot

131
00:08:30,720 --> 00:08:36,200
in ML Infra to fit the bill kind of thing for me.

132
00:08:36,200 --> 00:08:44,400
I joined lift two years ago, January, and at that time, it was that we're still stabilizing

133
00:08:44,400 --> 00:08:53,200
the prior incarnation of flight, the unnamed product.

134
00:08:53,200 --> 00:08:55,560
It was great and teams loved it.

135
00:08:55,560 --> 00:09:04,760
As Keeta was saying, at that point, I joined in the midst of this discussion about what

136
00:09:04,760 --> 00:09:07,840
do we do next.

137
00:09:07,840 --> 00:09:14,960
So I got to be part of the decision-bidening, going Kubernetes native, and all the very

138
00:09:14,960 --> 00:09:19,680
I see as critical design decisions.

139
00:09:19,680 --> 00:09:26,880
We took in flight to virtual decisions, all the, like going with a very strong type system

140
00:09:26,880 --> 00:09:32,440
and very strong language specifications through Protobov, like there is a lot of things

141
00:09:32,440 --> 00:09:39,080
we view, we are like very opinionated about in flight, and a lot of things we are not.

142
00:09:39,080 --> 00:09:43,520
We like explicitly decided to leave open.

143
00:09:43,520 --> 00:09:51,360
Based on the experience we had, we think we found a good path for where we give you the

144
00:09:51,360 --> 00:09:58,680
learnings, like enforce the learnings we have had before, in how we ask you to write

145
00:09:58,680 --> 00:10:04,000
your code or deliver your models or did the processing tasks or whatever.

146
00:10:04,000 --> 00:10:10,520
At the same time, leave it open for a variety of different workloads that can run the system.

147
00:10:10,520 --> 00:10:17,520
And there we are, I'm very proud with how the product turned out to be in the launch and

148
00:10:17,520 --> 00:10:21,000
the reception we have had during the conference.

149
00:10:21,000 --> 00:10:25,160
And just a shout out to the team, we wouldn't have been possible, because our like just crazy

150
00:10:25,160 --> 00:10:26,600
amounts of effort with the team.

151
00:10:26,600 --> 00:10:29,040
It's an amazing team at Live.

152
00:10:29,040 --> 00:10:34,800
And we are proud of all our users also at Live, just they have stayed with us through

153
00:10:34,800 --> 00:10:38,600
bad times, good times, and thank you for all the support.

154
00:10:38,600 --> 00:10:39,600
That's awesome.

155
00:10:39,600 --> 00:10:44,880
So, Kate, then you've given us a little bit of an overview of flight.

156
00:10:44,880 --> 00:10:51,680
Maybe take a step back, and what's the core value proposition that flight is offering

157
00:10:51,680 --> 00:10:58,680
and how do you mention that it's Kubernetes native, like how does it relate to Qflow?

158
00:10:58,680 --> 00:10:59,680
Yeah, for example.

159
00:10:59,680 --> 00:11:00,680
Good question.

160
00:11:00,680 --> 00:11:06,560
Let me start with the motivation of like, or what is it that we think is missing and

161
00:11:06,560 --> 00:11:08,480
what we were trying to address?

162
00:11:08,480 --> 00:11:12,840
One of the things, as I said, we started in 2017, so the landscape was very different

163
00:11:12,840 --> 00:11:13,840
at that point, right?

164
00:11:13,840 --> 00:11:16,000
So we evolved from that point.

165
00:11:16,000 --> 00:11:22,320
And this is a V2, even though actually I think this is the real V1, but this is a V2,

166
00:11:22,320 --> 00:11:26,560
so that means we went through a process of like actually making something and failing

167
00:11:26,560 --> 00:11:30,160
and then redoing it, that has a lot of learnings with it.

168
00:11:30,160 --> 00:11:35,160
So one of the learnings is that we feel that there is this artificial divide that's happening

169
00:11:35,160 --> 00:11:42,000
between ML and data, but actually they go hand in hand, like you, it's not that these

170
00:11:42,000 --> 00:11:47,240
companies have amazing data systems, they're not the Google's Facebooks or the Amazon's

171
00:11:47,240 --> 00:11:48,240
of the world, right?

172
00:11:48,240 --> 00:11:53,360
They are smaller companies, nimble, and they want, they are basically building their data

173
00:11:53,360 --> 00:11:55,120
stack too.

174
00:11:55,120 --> 00:12:02,960
So, and the other thing that we realize is there are teams cross-collaborate quite a bit.

175
00:12:02,960 --> 00:12:07,920
Learning models are built, let's say, by A team, but the team B probably provides the

176
00:12:07,920 --> 00:12:10,960
data that builds that machine learning model.

177
00:12:10,960 --> 00:12:19,240
And actually the fallacy of separating them is that many times in production, we use machine

178
00:12:19,240 --> 00:12:24,680
learning models to predict, and that creates data that becomes a fact in the fact tables

179
00:12:24,680 --> 00:12:26,760
in the data world.

180
00:12:26,760 --> 00:12:30,840
And then many times you use machine learning models to convert that fact to a dimension,

181
00:12:30,840 --> 00:12:31,840
which trains other models.

182
00:12:31,840 --> 00:12:36,920
So there is this cyclic nature that's happening, and this needs to be captured at that granularity

183
00:12:36,920 --> 00:12:42,840
of saying that there is data and processing and machine learning all interacting together.

184
00:12:42,840 --> 00:12:47,640
So, that was the motivation behind flight that we need a single tool and a platform that

185
00:12:47,640 --> 00:12:57,960
allows for collaborating, sharing, and MLOPS along with definite focus on orchestration,

186
00:12:57,960 --> 00:13:04,320
and that's why the core of flight is a workflow engine that actually runs all of these pipelines.

187
00:13:04,320 --> 00:13:08,520
But from the idea point of view, it was built for collaboration and sharing across the

188
00:13:08,520 --> 00:13:15,080
company various aspects, as well as processing and machine learning on the same tool.

189
00:13:15,080 --> 00:13:19,560
And so maybe to make that more concrete, we can kind of compare contrast to what Q flow

190
00:13:19,560 --> 00:13:20,880
is trying to do.

191
00:13:20,880 --> 00:13:21,880
Right.

192
00:13:21,880 --> 00:13:29,480
Are they orthogonal, are they complementary, does flight use Q flow?

193
00:13:29,480 --> 00:13:32,400
That's a great question.

194
00:13:32,400 --> 00:13:40,680
So Q flow started a while ago, started as one thing, initially they had just the TensorFlow

195
00:13:40,680 --> 00:13:42,800
operator.

196
00:13:42,800 --> 00:13:49,920
And as the product started maturing, it became not just a product, it became a collection

197
00:13:49,920 --> 00:13:57,720
of products under the name Q flow, Q flow serving, Q flow pipelines, TensorFlow became its

198
00:13:57,720 --> 00:14:01,520
own thing, you can use it from its own and so on and so forth.

199
00:14:01,520 --> 00:14:08,120
So we don't see the comparison between flight and Q flow as a collection of tools or products

200
00:14:08,120 --> 00:14:12,880
that the fear comparison I would say is between flight and Q flow pipelines, which is like

201
00:14:12,880 --> 00:14:21,920
one segment of Q flow that sits on its own, in a way, and underlying engine, the workflow

202
00:14:21,920 --> 00:14:26,280
engine under Q flow pipelines is not actually a reason why Google, it's an open source

203
00:14:26,280 --> 00:14:29,240
product, a different product, right.

204
00:14:29,240 --> 00:14:34,600
So it's also swapable that way, as it's, you can think of it as like a puzzle kind

205
00:14:34,600 --> 00:14:37,840
of thing and these are just a few pieces of that puzzle.

206
00:14:37,840 --> 00:14:44,000
And flight does things slightly differently for those few pieces that are comparable, if

207
00:14:44,000 --> 00:14:45,000
that makes sense.

208
00:14:45,000 --> 00:14:53,060
So then, like saying that flight might be a swapable alternative to Argo under Q flow

209
00:14:53,060 --> 00:14:54,060
pipelines.

210
00:14:54,060 --> 00:14:58,400
Actually, yeah, we might, it might be more vertical than that, but I'll give you an example

211
00:14:58,400 --> 00:15:04,840
like a Q flow that is PyTarge and TensorFlow to absolutely opinionated distributed or

212
00:15:04,840 --> 00:15:09,080
not distributed or deep learning frameworks, they both exist, right.

213
00:15:09,080 --> 00:15:13,280
And it should be the user's choice of what they want to use.

214
00:15:13,280 --> 00:15:19,160
And that's how I feel about how flight and let's take you flow pipelines work.

215
00:15:19,160 --> 00:15:24,960
They might, they could interrelate and that absolutely, it's the number best interest

216
00:15:24,960 --> 00:15:29,440
to make all of these tools play very well with each other, but they could be like completely

217
00:15:29,440 --> 00:15:32,640
vertically available as two alternatives.

218
00:15:32,640 --> 00:15:38,920
And you could use Q flow serving to serve your models, but build those models on flight.

219
00:15:38,920 --> 00:15:43,200
This flight offers a great abstraction on, on compute and it gives you big data with

220
00:15:43,200 --> 00:15:44,200
it, right.

221
00:15:44,200 --> 00:15:49,560
So that's essentially our differentiator where we think like Q flow pipelines doesn't even

222
00:15:49,560 --> 00:15:50,560
try to do that.

223
00:15:50,560 --> 00:15:55,040
And then the lineage and the cataloging that we do is further on built on top of it, which

224
00:15:55,040 --> 00:15:58,440
is also the other bit that we should talk about, I guess later.

225
00:15:58,440 --> 00:16:05,880
Well, you mentioned that one of the aspects of flight is that it's strongly typed, makes

226
00:16:05,880 --> 00:16:12,640
me think most immediately to like FB learner and kind of its approach to typing and you're

227
00:16:12,640 --> 00:16:16,400
nodding your heads, maybe they think, yeah, can you talk a little bit about that kind

228
00:16:16,400 --> 00:16:18,760
of design decision and implications?

229
00:16:18,760 --> 00:16:23,240
I can actually tell you the first time I saw the FB learner blog.

230
00:16:23,240 --> 00:16:29,960
It's uncanny, but it's also unbelievable at some level, so we looked at it and I shared

231
00:16:29,960 --> 00:16:30,960
it with the team.

232
00:16:30,960 --> 00:16:31,960
I'm like, did you see this?

233
00:16:31,960 --> 00:16:37,800
This looks exactly like R. And this was around the same, about 2017, 2016, that time.

234
00:16:37,800 --> 00:16:45,840
And we had the same like the annotation decorators in Python and I'm like, is it like they're

235
00:16:45,840 --> 00:16:52,120
people thinking about like us, but I think that yeah, that helped us, you know, they

236
00:16:52,120 --> 00:16:53,120
were earlier than us.

237
00:16:53,120 --> 00:17:01,760
I'm not saying that, but it's just that that kind of helped us believe like more in that

238
00:17:01,760 --> 00:17:11,200
we were probably on the right path, and so, yeah, so we, in the version one of flight,

239
00:17:11,200 --> 00:17:17,840
we did not have a very, we had a type system, but the type system only existed on the SDK,

240
00:17:17,840 --> 00:17:24,000
because we call it in the Python, where you, maybe take a step back for folks that, you

241
00:17:24,000 --> 00:17:28,760
know, here type system, or some only type, and there's no context for that.

242
00:17:28,760 --> 00:17:32,600
What does that mean from the perspective of the user experience for flight?

243
00:17:32,600 --> 00:17:39,720
Yeah, so let's tell you right, a function in any language, you, you have some inputs

244
00:17:39,720 --> 00:17:42,840
and outputs to that function.

245
00:17:42,840 --> 00:17:46,760
There are languages like Python, where you do specify inputs and outputs, but you don't

246
00:17:46,760 --> 00:17:49,400
know what those types are, barring the new typing.

247
00:17:49,400 --> 00:17:53,400
Whether they're strings, or integers, or fields, or whatever, or more complex things.

248
00:17:53,400 --> 00:17:55,400
Could be typing in there, and so on.

249
00:17:55,400 --> 00:17:59,440
But if you go to a language like Go, or Jow, or C++, it's a really strong type, you have

250
00:17:59,440 --> 00:18:05,520
to say this is an int, and this is a string, and that's the order and whatever, right?

251
00:18:05,520 --> 00:18:09,400
There are benefits of having types in the system, and that's why people love them to

252
00:18:09,400 --> 00:18:13,080
use them in programming languages, because you get compiled time safety.

253
00:18:13,080 --> 00:18:18,960
When you build the function, if you pass in an int, and try to use it like a string, you

254
00:18:18,960 --> 00:18:23,560
get an error at compile time, so you don't have to wait for it to run for 10 days, and

255
00:18:23,560 --> 00:18:29,040
then figure out, oh shit, but in Python, actually, there was a, there's more of a movement

256
00:18:29,040 --> 00:18:31,840
to add these kind of typing, compile time type safety.

257
00:18:31,840 --> 00:18:37,320
We think that that is the same thing that should be done for, why not do it for all functions.

258
00:18:37,320 --> 00:18:41,960
Like in our RPC systems today, like services, microservices, and people are using, we have

259
00:18:41,960 --> 00:18:47,400
types, and we have APIs, and people talk to them, and then they receive outputs.

260
00:18:47,400 --> 00:18:52,720
In flight, that's how we've designed, every single function is a task, we call them,

261
00:18:52,720 --> 00:18:58,400
and these have inputs and outputs, and by strong typing, I mean, these inputs and outputs

262
00:18:58,400 --> 00:19:02,920
have a known set of types, but what are the known set of types that you need to use with

263
00:19:02,920 --> 00:19:03,920
machine learning?

264
00:19:03,920 --> 00:19:08,040
So we had to come up with a type system that allows you to specify the various types of

265
00:19:08,040 --> 00:19:13,840
types that users use when they are pilling models, like an example could be a structured

266
00:19:13,840 --> 00:19:22,800
schema, which is like a row vector, or could be a tensor, could be a blob, which could

267
00:19:22,800 --> 00:19:28,240
be a serialized model, and it could be various formats, could be on X, could be, and those

268
00:19:28,240 --> 00:19:33,360
could be annotations on top of that blob saying that this is a serialized with the TensorFlow

269
00:19:33,360 --> 00:19:35,160
serialization format.

270
00:19:35,160 --> 00:19:40,240
So that's the type system that we are referring to.

271
00:19:40,240 --> 00:19:46,120
So when you declare a task, if it's a model building task, and you use joblib to serialize

272
00:19:46,120 --> 00:19:51,640
it, you can output a model that says it is joblib.dat, and then the next task that actually

273
00:19:51,640 --> 00:19:58,360
consumes it knows that this is a type joblib, so I can just load it into using joblib.

274
00:19:58,360 --> 00:20:01,960
And if you try to now put these two tasks together, they will work, but if you try to

275
00:20:01,960 --> 00:20:07,080
put a task that does not understand joblib, only uses TensorFlow, it will fail at compile

276
00:20:07,080 --> 00:20:08,440
time.

277
00:20:08,440 --> 00:20:13,640
And that's what we wanted to achieve in a pipeline, try to fail earlier if possible.

278
00:20:13,640 --> 00:20:20,200
And one of the things that that enables for FB Learner cases that they can take all of

279
00:20:20,200 --> 00:20:25,480
these tasks that are strongly typed, kind of created dependency graph, and then execute

280
00:20:25,480 --> 00:20:30,840
them in parallel, like as one completes spin off its descendants, are you doing some more

281
00:20:30,840 --> 00:20:31,840
things?

282
00:20:31,840 --> 00:20:32,840
Exactly.

283
00:20:32,840 --> 00:20:38,960
I just remember something you wanted to add to the timing system, so one of the decisions

284
00:20:38,960 --> 00:20:45,760
we made there was using Protobov to declare the, to specify the types, and then that might

285
00:20:45,760 --> 00:20:51,440
be one of the distinctions between the existing systems you might see, even like FB Learner

286
00:20:51,440 --> 00:20:53,560
and others, and flight.

287
00:20:53,560 --> 00:20:58,720
And that makes it not, like takes it one step further, it doesn't only allow you to put

288
00:20:58,720 --> 00:21:06,400
Python tasks together, because after you declare the task in whatever language you choose to,

289
00:21:06,400 --> 00:21:11,440
using our spec, you can, you know, let's say right go, function becomes one step in your

290
00:21:11,440 --> 00:21:16,840
graph, the next step might be a Python task in a different container or, you know, what

291
00:21:16,840 --> 00:21:17,840
you have you.

292
00:21:17,840 --> 00:21:22,960
Because at the end of the day, they would compile to this standard spec that has standard

293
00:21:22,960 --> 00:21:27,840
set of types that are compatible with, you know, our SDK in any language.

294
00:21:27,840 --> 00:21:33,680
And yes, I guess back to your question, it does allow us to, we once you declare the graph

295
00:21:33,680 --> 00:21:41,320
in this spec, we have a compiler that looks at the graph and figures out dependency graph,

296
00:21:41,320 --> 00:21:48,040
and we can go as parallel as we, as the system would want, you know, pairing the dependencies

297
00:21:48,040 --> 00:21:50,560
and all of that into accounts.

298
00:21:50,560 --> 00:21:51,560
Yeah.

299
00:21:51,560 --> 00:21:56,200
And there are other advantages, like what I think, I just wanted to add one more thing is

300
00:21:56,200 --> 00:22:01,680
the reason why we did this is because we saw use cases within Lyft, where users were

301
00:22:01,680 --> 00:22:07,840
writing models in Scala for spark processing, right, like they just want to do spark processing

302
00:22:07,840 --> 00:22:08,840
Scala.

303
00:22:08,840 --> 00:22:09,840
They don't want to write this in Python.

304
00:22:09,840 --> 00:22:14,400
They do sometimes, but sometimes they want to use Scala for higher performance.

305
00:22:14,400 --> 00:22:18,600
And we are like, how do we move the data between these two different languages?

306
00:22:18,600 --> 00:22:23,520
And so that's why we came up with, so we also using arrow underneath, this is an open

307
00:22:23,520 --> 00:22:25,320
search project.

308
00:22:25,320 --> 00:22:30,640
But like we wanted to create a layer on top of it, so that it's easy to construct these

309
00:22:30,640 --> 00:22:34,720
polyglot pipelines, if I may.

310
00:22:34,720 --> 00:22:41,440
So we're talking about stuff like type systems and protobuffs and arrow and not language

311
00:22:41,440 --> 00:22:48,360
that the typical data scientist is about a lot, yeah, is it, you know, have you built

312
00:22:48,360 --> 00:22:52,840
abstractions that make it more acceptable to data scientists or do you just have a different

313
00:22:52,840 --> 00:22:53,840
audience?

314
00:22:53,840 --> 00:22:57,040
I think that's a very good question.

315
00:22:57,040 --> 00:23:01,320
And actually sometimes we do debate amongst ourselves, what's our audience, right?

316
00:23:01,320 --> 00:23:11,720
But we do want to appear in the beginning, at least, to the savvy engineer like us who

317
00:23:11,720 --> 00:23:14,720
wants to get his hands a little dirty.

318
00:23:14,720 --> 00:23:19,200
But as we are progressing, we are creating layers on top of them.

319
00:23:19,200 --> 00:23:25,160
So think about it, we are like, we built the foundation and now building layers on top

320
00:23:25,160 --> 00:23:26,160
is much easier.

321
00:23:26,160 --> 00:23:30,000
So for example, like we're just about to merge full notebook support, so you can write

322
00:23:30,000 --> 00:23:31,320
tasks in notebooks.

323
00:23:31,320 --> 00:23:34,240
So like we're just going to add like support for Jupyter notebooks.

324
00:23:34,240 --> 00:23:39,080
The way and the way we are thinking about Jupyter notebooks is users like data scientists

325
00:23:39,080 --> 00:23:41,800
and researchers love to use Jupyter notebooks.

326
00:23:41,800 --> 00:23:47,600
They write code in Jupyter notebooks in a very different way than how engineers write

327
00:23:47,600 --> 00:23:49,520
code on like using IDs, right?

328
00:23:49,520 --> 00:23:50,520
It's very different.

329
00:23:50,520 --> 00:23:53,720
And we want to keep and preserve that semantic of writing code.

330
00:23:53,720 --> 00:23:57,520
So we actually found a project called Paper Mill, which is pretty cool.

331
00:23:57,520 --> 00:23:58,520
We like that.

332
00:23:58,520 --> 00:23:59,520
That's pretty cool.

333
00:23:59,520 --> 00:24:02,960
There's only out of Netflix now in a background independent company.

334
00:24:02,960 --> 00:24:03,960
Yes.

335
00:24:03,960 --> 00:24:07,400
And so we decided to adopt it.

336
00:24:07,400 --> 00:24:14,000
And we take that and we convert a notebook into a function into our system with the

337
00:24:14,000 --> 00:24:15,000
same inputs and output.

338
00:24:15,000 --> 00:24:17,680
We do the magic of like passing the inputs and outputs.

339
00:24:17,680 --> 00:24:20,840
You just write your code as if you're writing a regular notebook.

340
00:24:20,840 --> 00:24:26,960
You can try it out to whatever, drop it into a container, give it to flight.

341
00:24:26,960 --> 00:24:30,560
And then it'll take care and execute it and actually record the output notebook and

342
00:24:30,560 --> 00:24:31,720
store it for you.

343
00:24:31,720 --> 00:24:35,280
So you can go back in time and look at it even if you want to.

344
00:24:35,280 --> 00:24:44,040
So yes, our audience is was engineers in the beginning because we do have that sort of

345
00:24:44,040 --> 00:24:49,680
audience at lift, but we are, we also have like data scientists and research scientists

346
00:24:49,680 --> 00:24:54,280
using more and more and we are improving every day for them.

347
00:24:54,280 --> 00:24:58,240
So if you start today, you might look at the Python flight kit and see that it's a little

348
00:24:58,240 --> 00:25:06,000
more suited for people who write with IDs, but with the notebook introduction and our demo

349
00:25:06,000 --> 00:25:08,680
we showed like how to use everything with the notebook.

350
00:25:08,680 --> 00:25:11,040
We are going more towards research scientists.

351
00:25:11,040 --> 00:25:14,080
And they will not even see some of the things that we do, like for example, I'll give an

352
00:25:14,080 --> 00:25:19,440
example, in a task, you can return a data frame, a pandas data frame and we understand

353
00:25:19,440 --> 00:25:24,240
that a pandas data frame is actually the throw vector that we do underneath which is converted

354
00:25:24,240 --> 00:25:27,400
to a prototype and arrow and it's just sent through.

355
00:25:27,400 --> 00:25:28,680
You don't even have to think about it.

356
00:25:28,680 --> 00:25:34,000
You just work with pandas data frame and hopefully we are thinking we're not yet implemented

357
00:25:34,000 --> 00:25:37,600
this, but that goes into spark data frame and the other side.

358
00:25:37,600 --> 00:25:41,440
So we created the abstraction layer for that.

359
00:25:41,440 --> 00:25:43,200
Does that introduce a lot of latency?

360
00:25:43,200 --> 00:25:48,880
I was having a conversation with someone who was talking about reason why they don't

361
00:25:48,880 --> 00:25:54,200
use TF serving is because they're primarily doing inferencing on images and it requires

362
00:25:54,200 --> 00:25:58,360
that you have to put everything into a data frame and there's a bunch of latency that

363
00:25:58,360 --> 00:25:59,880
that introduces.

364
00:25:59,880 --> 00:26:07,280
Do you run into these kinds of issues where your abstraction hides kind of some nimbleness

365
00:26:07,280 --> 00:26:12,800
and what the underlying data format is and you're doing conversions underneath the covers?

366
00:26:12,800 --> 00:26:19,920
So actually, arrow in that case is an extremely interesting project, according to me, aim

367
00:26:19,920 --> 00:26:29,000
is to make zero copy abstractions from one format to pandas data frame, to spark data

368
00:26:29,000 --> 00:26:36,720
frames and I think the founder is, I think it's Wes, who the guy who has been following

369
00:26:36,720 --> 00:26:43,640
that project and I think that's more of that is required, where we just should stop wasting

370
00:26:43,640 --> 00:26:49,280
CPU cycles on transforming data from one format or one language to another and more use

371
00:26:49,280 --> 00:26:54,520
the zero copy abstractions that we can create and that just makes the open source ecosystem

372
00:26:54,520 --> 00:26:55,920
much nicer.

373
00:26:55,920 --> 00:27:00,360
And that's why we chose arrow, but you don't have to, for example, if you're emitting

374
00:27:00,360 --> 00:27:03,960
out images, you don't have to have a data frame for it in flight.

375
00:27:03,960 --> 00:27:08,880
You can just say, I am emitting a directory of million images and we will take the million

376
00:27:08,880 --> 00:27:13,600
images and upload them and download them onto the other machines and use them.

377
00:27:13,600 --> 00:27:18,560
So yeah, so there is, you can use data frames, but you don't have to, and that's where

378
00:27:18,560 --> 00:27:20,440
the type system comes in.

379
00:27:20,440 --> 00:27:23,040
It needs to be more granular then.

380
00:27:23,040 --> 00:27:24,040
Okay.

381
00:27:24,040 --> 00:27:28,680
So we've talked a bunch about the type system and the workflow that that enables and kind

382
00:27:28,680 --> 00:27:36,840
of some of the user experience in introducing flight you mentioned.

383
00:27:36,840 --> 00:27:44,480
This important idea of kind of connecting back to the data and, you know, enabling things

384
00:27:44,480 --> 00:27:49,400
like end-to-end data providence and this, you know, kind of loop that you pointed out

385
00:27:49,400 --> 00:27:54,880
where your inference actually is, you know, data for your next train or a future train

386
00:27:54,880 --> 00:27:59,280
at least, talk more about kind of how that works like I'm thinking, the thing that comes

387
00:27:59,280 --> 00:28:06,680
to mind as I'm hearing that is Airbnb has a project that's kind of an adjacent project

388
00:28:06,680 --> 00:28:13,520
to their big head platform that is focused on, like doing, you know, point in time feature,

389
00:28:13,520 --> 00:28:19,680
mapping and management and feature repository, that kind of thing, is that the kind of thing

390
00:28:19,680 --> 00:28:20,680
we're talking about?

391
00:28:20,680 --> 00:28:25,000
That could be one of the things, but it's not like exactly what we're talking about,

392
00:28:25,000 --> 00:28:30,640
but like I think your, it's feature service is, well, let's call it like a generic name,

393
00:28:30,640 --> 00:28:31,640
feature service.

394
00:28:31,640 --> 00:28:33,160
Features as a service, right?

395
00:28:33,160 --> 00:28:40,000
What is an implementation on top of this potentially that allows you to pass the, like,

396
00:28:40,000 --> 00:28:45,720
consume the data back into the model and also build and send it to feature service.

397
00:28:45,720 --> 00:28:53,440
What we are referring to is the causal dependencies between the compute and the actual production

398
00:28:53,440 --> 00:28:58,920
of that data and then further consumption of data by the next compute layer and production.

399
00:28:58,920 --> 00:29:01,840
So now let's take an example.

400
00:29:01,840 --> 00:29:09,240
You get a map from OSM, OpenTreatMaps, if we consume it and you build a graph, the road

401
00:29:09,240 --> 00:29:14,560
network out of it, then some other team actually analyzes the road network in real time that's

402
00:29:14,560 --> 00:29:20,080
happening and creates the traffic pattern that's that are happening at the moment on the

403
00:29:20,080 --> 00:29:25,760
road and annotates the road network with some speed profiles, that's what we call them.

404
00:29:25,760 --> 00:29:29,640
And then the third team actually consumes these two things and creates the final road network

405
00:29:29,640 --> 00:29:31,880
that's deployed to production.

406
00:29:31,880 --> 00:29:37,000
So when we did a prediction on ETA, we would want to know which version of the map did

407
00:29:37,000 --> 00:29:42,120
I use and what was the speed profile at that point in time and what were the traffic

408
00:29:42,120 --> 00:29:45,320
conditions that were led to that speed profile?

409
00:29:45,320 --> 00:29:47,080
That's the type of question that we want to answer.

410
00:29:47,080 --> 00:29:54,440
So to go to that, you need to have a full trace in the system of how the data was generated,

411
00:29:54,440 --> 00:29:59,200
when did it get generated, when did it get passed under the next bit and then so we call

412
00:29:59,200 --> 00:30:03,040
this typically lineage or prominence as you said.

413
00:30:03,040 --> 00:30:07,400
And the way we track this is our type system was the other reason why we had our type system

414
00:30:07,400 --> 00:30:16,200
is to have the central engine automatically publish all of this data as it's generated

415
00:30:16,200 --> 00:30:20,960
by every single task execution to a central service that actually just records a unique

416
00:30:20,960 --> 00:30:27,240
signature of the execution and what did it generate with the inputs and the outputs.

417
00:30:27,240 --> 00:30:32,000
So now you have a relationship between what got generated by what and now you can create

418
00:30:32,000 --> 00:30:37,480
a graph because you know the graph that ran like flight knows intrinsically that how what

419
00:30:37,480 --> 00:30:39,240
was the computation graph.

420
00:30:39,240 --> 00:30:46,480
So you can go in and create the causal dependency structure across these data sets.

421
00:30:46,480 --> 00:30:52,280
We do this today, the exposed thing that you get in open source is not the full, we don't

422
00:30:52,280 --> 00:30:56,440
show the dependency tree that will come out soon at some point, but we are using this

423
00:30:56,440 --> 00:31:06,920
for memorization. So if you recompute the same data set, let's say you took some data set

424
00:31:06,920 --> 00:31:12,080
and you transformed it and it produced an output.

425
00:31:12,080 --> 00:31:16,080
Some other user goes in and takes the same data set and transforms it and produces the

426
00:31:16,080 --> 00:31:20,400
same output potentially because the code really has not changed.

427
00:31:20,400 --> 00:31:23,080
In the past people were like we would spend money.

428
00:31:23,080 --> 00:31:31,720
So then the solution would be let's create a intermediate high table or whatever, right?

429
00:31:31,720 --> 00:31:32,720
Things like that.

430
00:31:32,720 --> 00:31:38,040
But that's not really what it was for many artifacts that are like images, right?

431
00:31:38,040 --> 00:31:41,480
If you did luminon sampling on some set of images and you're not going to store them

432
00:31:41,480 --> 00:31:42,480
in high.

433
00:31:42,480 --> 00:31:45,520
I think we can do that, but we don't.

434
00:31:45,520 --> 00:31:51,680
But what we do is because we can identify the compute process and the set of inputs that

435
00:31:51,680 --> 00:31:54,320
were given, we create a unique signature of that.

436
00:31:54,320 --> 00:31:58,040
And so next time when we observe that the same thing is being run, flight just smartly

437
00:31:58,040 --> 00:32:00,200
replaces it with the existing outputs.

438
00:32:00,200 --> 00:32:05,720
Now you have to tell flight that this process is reproducible because if it has side effects

439
00:32:05,720 --> 00:32:09,280
or it is like, you know, if you're using random number, the generator or something, that

440
00:32:09,280 --> 00:32:10,280
might not work.

441
00:32:10,280 --> 00:32:16,120
So we, you have to tell us, but if you tell us then we just go and replace any execution

442
00:32:16,120 --> 00:32:18,720
of that on the platform across the company.

443
00:32:18,720 --> 00:32:24,480
So that saves money and time because the iteration time now, like if you're running something

444
00:32:24,480 --> 00:32:30,320
that's 10 steps, like a 10 step pipeline and you fail on the 10th step, you just fix

445
00:32:30,320 --> 00:32:31,960
the 10th step and rerun it.

446
00:32:31,960 --> 00:32:32,960
Right.

447
00:32:32,960 --> 00:32:33,960
And the 9 steps just are cached.

448
00:32:33,960 --> 00:32:38,480
So you just automatically fright will fast forward you to the 10 step and go, go, let's

449
00:32:38,480 --> 00:32:40,280
run that guy again.

450
00:32:40,280 --> 00:32:42,720
And so this is what we call it, memorization.

451
00:32:42,720 --> 00:32:43,720
So flight.

452
00:32:43,720 --> 00:32:50,760
I think you've described it as kind of a workflow engine, it's not a data store or something

453
00:32:50,760 --> 00:32:55,000
that is like creating snapshots or anything, it's more like, you can think of it more

454
00:32:55,000 --> 00:33:01,200
like metadata and pointers to, you know, existing training data and interim data, transform

455
00:33:01,200 --> 00:33:03,160
data, output data, et cetera.

456
00:33:03,160 --> 00:33:04,160
Exactly.

457
00:33:04,160 --> 00:33:05,160
Yeah.

458
00:33:05,160 --> 00:33:10,240
And or presumably you're also kind of tracking model versions as they're trained.

459
00:33:10,240 --> 00:33:11,240
Yes.

460
00:33:11,240 --> 00:33:20,480
Yeah, we have, that's one of the things we are a bit opinionated about flight is every

461
00:33:20,480 --> 00:33:26,560
artifact in the system, all the data produced, all the definitions, all the tasks and workflows

462
00:33:26,560 --> 00:33:28,560
are immutable.

463
00:33:28,560 --> 00:33:34,680
So we have versions, strict versions, versioning scheme that you can use your own versioning

464
00:33:34,680 --> 00:33:42,760
scheme, but it has to be strict as in you can't mutate something and try to register it

465
00:33:42,760 --> 00:33:45,680
or produce it with the same version again.

466
00:33:45,680 --> 00:33:52,400
So we, when we produce any metadata about tasks, outputs or whatnot, they are always unique

467
00:33:52,400 --> 00:33:55,400
and like the signature is always unique.

468
00:33:55,400 --> 00:34:01,160
And that allows us throughout the system to always refer to very consistently to like

469
00:34:01,160 --> 00:34:06,120
executions, past executions in the history and the produced artifacts and produced models

470
00:34:06,120 --> 00:34:12,280
and, you know, anything that went through the system with very confidence that, you know,

471
00:34:12,280 --> 00:34:18,000
we know exactly which even like piece of code produced that.

472
00:34:18,000 --> 00:34:19,000
So yeah.

473
00:34:19,000 --> 00:34:20,000
Yeah.

474
00:34:20,000 --> 00:34:24,800
And you should be able to fully re-run, re-produce it, but it'll cause a new version.

475
00:34:24,800 --> 00:34:30,040
We don't even have one single update API in the entire code list, because you have like

476
00:34:30,040 --> 00:34:33,280
a functional system.

477
00:34:33,280 --> 00:34:38,440
What's the smallest kind of use case that you can envision someone using this for?

478
00:34:38,440 --> 00:34:45,280
Does it make sense and is it kind of approachable enough for kind of a, you know, a single person,

479
00:34:45,280 --> 00:34:48,880
a kind of clone or repo and like actually get some value out of it or do you need a team

480
00:34:48,880 --> 00:34:56,080
of, you know, 10 MLN for people or that's a great question, great question, yes.

481
00:34:56,080 --> 00:35:02,240
We, and I will say like our work there is not done, but we have put a lot of effort to

482
00:35:02,240 --> 00:35:11,880
make the first time experience and the maintenance for small projects, as you mentioned, very

483
00:35:11,880 --> 00:35:13,560
approachable.

484
00:35:13,560 --> 00:35:19,880
We have written, we have written docs that, like I would say maybe most of the time, like

485
00:35:19,880 --> 00:35:24,480
the efforts we put in the docs, we're in the docs that tell people how to get started.

486
00:35:24,480 --> 00:35:29,480
So it's like me that easy, even sometimes we went back to like our architecture and what

487
00:35:29,480 --> 00:35:33,120
not to try to, you know, make that easy when like we looked at the docs and it's like that's

488
00:35:33,120 --> 00:35:34,320
a lot of steps.

489
00:35:34,320 --> 00:35:35,320
This is not okay.

490
00:35:35,320 --> 00:35:38,320
Let's go back and redo things, right?

491
00:35:38,320 --> 00:35:39,680
So that's one part of it.

492
00:35:39,680 --> 00:35:45,680
I would say the other part I think is we have seen that in a lot of cases, people when

493
00:35:45,680 --> 00:35:52,240
they get started in developing a model or even doing some transformation or whatnot, they

494
00:35:52,240 --> 00:35:57,560
don't think initially, unless they have done that before, they don't think that they will

495
00:35:57,560 --> 00:36:00,000
need a workflow for it.

496
00:36:00,000 --> 00:36:05,640
They think you know, we'll spawn off a notebook and do my thing and be happy, right?

497
00:36:05,640 --> 00:36:09,640
Which usually is the case in the beginning, right?

498
00:36:09,640 --> 00:36:14,080
Until you know, somebody leaves the company or you started, like you want to go back to

499
00:36:14,080 --> 00:36:18,680
a model that worked, but you don't know which version of the code produced that and so

500
00:36:18,680 --> 00:36:24,120
on and then you start realizing the problems and the need for such systems.

501
00:36:24,120 --> 00:36:30,880
But by trying to make it as easy to get started as approachable, we hope that at one point

502
00:36:30,880 --> 00:36:38,520
it becomes like a standard given that you always start there and the system, like the friction

503
00:36:38,520 --> 00:36:47,560
between I have nothing to the first task or the first execution is almost not there.

504
00:36:47,560 --> 00:36:52,400
Like more people would get into the happy love doing this as the first step before I think

505
00:36:52,400 --> 00:36:57,720
your first piece of code and it sets you up for success, leader.

506
00:36:57,720 --> 00:37:02,000
We are basically saying that every company should grow and become great and you know, start

507
00:37:02,000 --> 00:37:07,640
with what we think is like the bare minimum and it will evolve with you, right?

508
00:37:07,640 --> 00:37:09,720
So it's an evolveable system.

509
00:37:09,720 --> 00:37:13,720
The other bit that I want to add to that is that I think that one of the reasons why we

510
00:37:13,720 --> 00:37:20,280
move to Kubernetes is to make that first case experience and for small company experience

511
00:37:20,280 --> 00:37:22,120
really, really good.

512
00:37:22,120 --> 00:37:23,720
Kubernetes is great to get.

513
00:37:23,720 --> 00:37:29,160
You can go to any of the clouds and get one Kubernetes cluster and we use customized,

514
00:37:29,160 --> 00:37:33,360
I don't know if you know of that project, but that's a pretty interesting project.

515
00:37:33,360 --> 00:37:40,400
All of flight is really one YAMO and you can say, CUP, CTO, apply minus F, that YAMO

516
00:37:40,400 --> 00:37:46,320
and boom, you get a flying cluster, including all of the things that we just talked about.

517
00:37:46,320 --> 00:37:47,320
Wow.

518
00:37:47,320 --> 00:37:48,320
Okay.

519
00:37:48,320 --> 00:37:52,480
Yeah, you can even do that on the Docker for this top or Kubernetes.

520
00:37:52,480 --> 00:37:56,720
Like on your machine, you don't even have to go to a CUP provider to get started and

521
00:37:56,720 --> 00:38:00,640
set it up even with menu for storage.

522
00:38:00,640 --> 00:38:06,160
So you will get the full experience, your manual tasks and once you're ready to take it

523
00:38:06,160 --> 00:38:15,600
to the next level, run on in AWS or GCP or whatnot, that you don't have to change how you

524
00:38:15,600 --> 00:38:18,480
were doing things or how you wrote your tasks.

525
00:38:18,480 --> 00:38:23,680
They will just seamlessly run on the bigger cloud within lift and we talked about this

526
00:38:23,680 --> 00:38:25,720
in the presentation yesterday.

527
00:38:25,720 --> 00:38:31,280
We run a multi cluster set of flights, a single cluster, but of course you will not start

528
00:38:31,280 --> 00:38:32,280
there.

529
00:38:32,280 --> 00:38:33,280
Right?

530
00:38:33,280 --> 00:38:34,280
And we did not start there.

531
00:38:34,280 --> 00:38:35,800
We started with single cluster.

532
00:38:35,800 --> 00:38:40,280
We soon outgrew that and started, you know, deploying multiple cluster to flight.

533
00:38:40,280 --> 00:38:42,560
And the user's multi cluster.

534
00:38:42,560 --> 00:38:48,040
What's the multi tenancy models like a single user has stuff running on multi clusters

535
00:38:48,040 --> 00:38:52,040
or you just have multi clusters and you have some users among cluster and some on another.

536
00:38:52,040 --> 00:38:56,400
We have multiple clusters and we for the user though, it's one cluster.

537
00:38:56,400 --> 00:39:02,640
So we abstract that entire thing behind the service and the way we spun off the work

538
00:39:02,640 --> 00:39:07,360
depends on the load of the system or priority classes or things like that.

539
00:39:07,360 --> 00:39:12,000
Is that all stuff that's happening in the open source or this way that you're operating

540
00:39:12,000 --> 00:39:13,000
it?

541
00:39:13,000 --> 00:39:14,000
No, it's all open source.

542
00:39:14,000 --> 00:39:15,000
Interesting.

543
00:39:15,000 --> 00:39:16,760
Yeah, you don't need to do this.

544
00:39:16,760 --> 00:39:20,280
You don't have to use multi cluster, but let's see your company grows further beyond

545
00:39:20,280 --> 00:39:21,280
that single cluster.

546
00:39:21,280 --> 00:39:22,680
Yes, flight will evolve with you.

547
00:39:22,680 --> 00:39:23,680
Yeah.

548
00:39:23,680 --> 00:39:24,680
Yeah.

549
00:39:24,680 --> 00:39:26,480
Yeah, we basically open source everything that we do at lift.

550
00:39:26,480 --> 00:39:27,480
Yeah.

551
00:39:27,480 --> 00:39:30,280
Some other thing that I just talked about like the lineage or whatever, it will come soon.

552
00:39:30,280 --> 00:39:31,280
Yeah.

553
00:39:31,280 --> 00:39:32,280
But most of the things.

554
00:39:32,280 --> 00:39:37,520
What's exciting about this for me is that there are many, many companies that are doing

555
00:39:37,520 --> 00:39:40,000
this kind of thing internally.

556
00:39:40,000 --> 00:39:46,480
We talked about Facebook, Airbnb, Uber, obviously with Michelangelo.

557
00:39:46,480 --> 00:39:54,840
Some have talked about open sourcing, but I'm not sure I can think of any name brand

558
00:39:54,840 --> 00:40:00,080
company that is open source their internal platform outside of like TFX if you consider

559
00:40:00,080 --> 00:40:02,560
that Google open sourcing their internal platform.

560
00:40:02,560 --> 00:40:03,560
Yeah.

561
00:40:03,560 --> 00:40:04,560
So this.

562
00:40:04,560 --> 00:40:06,840
The TFX doesn't come with an execution portion of it, right?

563
00:40:06,840 --> 00:40:08,800
It's just basically the library at the moment.

564
00:40:08,800 --> 00:40:09,800
Right.

565
00:40:09,800 --> 00:40:10,800
Right.

566
00:40:10,800 --> 00:40:11,800
Right.

567
00:40:11,800 --> 00:40:15,400
And so this is the library that does all the type stuff.

568
00:40:15,400 --> 00:40:21,720
There's the execution piece, workflow engine, you know, after the step after the CUBE

569
00:40:21,720 --> 00:40:25,920
CTL, like is it spinning up a web front end that I can see stuff?

570
00:40:25,920 --> 00:40:26,920
Yeah.

571
00:40:26,920 --> 00:40:27,920
Our web front end is pretty snappy too.

572
00:40:27,920 --> 00:40:30,640
We show like errors in the UI and graphs and things like that.

573
00:40:30,640 --> 00:40:35,360
You can click you get log links in the UI and like inputs and outputs and the artifacts

574
00:40:35,360 --> 00:40:36,360
produced.

575
00:40:36,360 --> 00:40:37,360
All of that is in the UI too.

576
00:40:37,360 --> 00:40:38,360
Yep.

577
00:40:38,360 --> 00:40:39,360
And then the CLI too, of course.

578
00:40:39,360 --> 00:40:43,840
And tell me a little bit about the like the process of open sourcing it.

579
00:40:43,840 --> 00:40:47,560
Like, was that the size of it all?

580
00:40:47,560 --> 00:40:48,560
Yeah.

581
00:40:48,560 --> 00:40:49,560
All right.

582
00:40:49,560 --> 00:40:59,720
So I had a baby five months ago, five months ago, and I had another baby in a flight.

583
00:40:59,720 --> 00:41:04,440
So they think having literally having two babies at the same time.

584
00:41:04,440 --> 00:41:11,280
It's a twin, a twin, and I have not slept almost for five months probably.

585
00:41:11,280 --> 00:41:14,320
And so as the team, like, been a fantastic job.

586
00:41:14,320 --> 00:41:18,080
So yeah, that's why we took this long, actually.

587
00:41:18,080 --> 00:41:21,640
We started the process last year.

588
00:41:21,640 --> 00:41:22,640
We could have been.

589
00:41:22,640 --> 00:41:24,640
I mean, let's start with why?

590
00:41:24,640 --> 00:41:25,640
Very good question.

591
00:41:25,640 --> 00:41:27,440
They have to like, why?

592
00:41:27,440 --> 00:41:29,280
And we did debate that a lot, didn't they?

593
00:41:29,280 --> 00:41:30,280
We did a lot, right?

594
00:41:30,280 --> 00:41:31,280
Yeah.

595
00:41:31,280 --> 00:41:32,280
Especially that, right?

596
00:41:32,280 --> 00:41:36,800
The previous system wasn't going to be open sourced, was sift only.

597
00:41:36,800 --> 00:41:41,960
So like, when this came, this, when the new, when you started deciding to redo it, I

598
00:41:41,960 --> 00:41:45,400
remember, even hit them giant and when I was one of the question, why do you want to open

599
00:41:45,400 --> 00:41:46,400
sourcing?

600
00:41:46,400 --> 00:41:52,600
And I think what happened is there was interest from outside was one of the reasons, I don't

601
00:41:52,600 --> 00:41:56,400
need to name, like who, where it was, but there was some interest.

602
00:41:56,400 --> 00:42:02,720
The other thing is we realized this is such a big, like, big problem to solve.

603
00:42:02,720 --> 00:42:09,320
And I said this in the, I think, before, but a small 9 or 10 people team cannot do this.

604
00:42:09,320 --> 00:42:16,120
It needs to be an industry wide effort, hopefully, if not, at least like a few, few tens

605
00:42:16,120 --> 00:42:19,160
of people working on this.

606
00:42:19,160 --> 00:42:26,640
And, and all with like, if we do set the right primitives, then we can let it evolve into

607
00:42:26,640 --> 00:42:29,080
the, into the piece that it needs to be, right?

608
00:42:29,080 --> 00:42:34,600
And that gets great stuff for life, like if we, we get, for example, we don't, we currently

609
00:42:34,600 --> 00:42:39,280
only have a Python SDK, but I know there are other companies are saying that we want

610
00:42:39,280 --> 00:42:43,640
to add a scala SDK for this and we're like, awesome, we will use it, right?

611
00:42:43,640 --> 00:42:48,440
There is demand for this at least, but we don't have the time to build this.

612
00:42:48,440 --> 00:42:53,840
So that's one, a definite biggest reason that get, basically, leverage.

613
00:42:53,840 --> 00:43:01,400
Second is lift open source on why it was a foundational technology for lift two.

614
00:43:01,400 --> 00:43:06,560
And it's been, it's been, you know, amazing success, right?

615
00:43:06,560 --> 00:43:08,560
We're in this community.

616
00:43:08,560 --> 00:43:13,400
So we're not saying we are going to be even 10% of that weight, that's just much.

617
00:43:13,400 --> 00:43:19,000
But from that we learned that people actually, like, it's a great hiring tool.

618
00:43:19,000 --> 00:43:25,040
You are not working on a technology in the company that's, that now you have to hire

619
00:43:25,040 --> 00:43:27,840
engineers and teach them.

620
00:43:27,840 --> 00:43:31,000
You are going to work on a technology that they have probably used at their previous jobs

621
00:43:31,000 --> 00:43:33,640
and that's great for lift.

622
00:43:33,640 --> 00:43:38,600
So, and, yeah, answer and just wait for the team potential, right?

623
00:43:38,600 --> 00:43:42,280
So, and I would like to just add to all of this.

624
00:43:42,280 --> 00:43:50,680
I think we, as we talked throughout the conversation today, there are a few things we strongly believe

625
00:43:50,680 --> 00:43:52,480
in.

626
00:43:52,480 --> 00:43:57,560
And we wanted to have that debate in the open, because I think it will not only influence

627
00:43:57,560 --> 00:44:01,880
like this product or similar products, it will influence like the entire ecosystem,

628
00:44:01,880 --> 00:44:07,000
how it, how it interacts with each other, how, how you do serving even after like the

629
00:44:07,000 --> 00:44:13,600
all of that, even if you don't build these pieces, the concepts, like the underlying concepts.

630
00:44:13,600 --> 00:44:19,200
And I think it, like, we, we see that it does bring something to the table that isn't there

631
00:44:19,200 --> 00:44:20,200
yet.

632
00:44:20,200 --> 00:44:25,280
So, we wanted to make sure that, you know, we have that conversation, like it, it will,

633
00:44:25,280 --> 00:44:32,800
we think, advance like the entire, like the email, link for a community overall, to hopefully

634
00:44:32,800 --> 00:44:34,640
a slightly better place.

635
00:44:34,640 --> 00:44:39,000
Well, hi, Thom Ketan, thanks so much for taking the time to chat with us.

636
00:44:39,000 --> 00:44:40,000
Thank you, Sam.

637
00:44:40,000 --> 00:44:41,000
Thanks for having us.

638
00:44:41,000 --> 00:44:42,000
Yeah.

639
00:44:42,000 --> 00:44:43,000
Yeah.

640
00:44:43,000 --> 00:44:44,000
All right, everyone.

641
00:44:44,000 --> 00:44:49,520
That's our show for today to learn more about today's guests or the topics mentioned

642
00:44:49,520 --> 00:44:54,400
in the interview, visit twomelai.com slash shows.

643
00:44:54,400 --> 00:44:58,880
For more information on either of our new study group offerings, causal modeling and machine

644
00:44:58,880 --> 00:45:07,200
learning or the IBM Enterprise AI workflow, visit twomelai.com slash learn 2020.

645
00:45:07,200 --> 00:45:11,960
Of course, if you like what you hear on the podcast, be sure to subscribe, rate, and review

646
00:45:11,960 --> 00:45:14,520
the show on your favorite pod catcher.

647
00:45:14,520 --> 00:45:31,680
Thanks so much for listening and catch you next time.

