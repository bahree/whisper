Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
You are invited to join us for the very first Twimblecon conference which will focus on the
tools, technologies and practices necessary to scale the delivery of machine learning
and AI in the enterprise.
The event will be held October 1st and 2nd in San Francisco and early bird registration
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.
All right everyone, I am on the line with Lawrence Watson.
Lawrence is a data scientist at Carbon Tracker.
Lawrence, welcome to this week in machine learning and AI.
Hi Sam, great to be on the show.
It is great to have you on the show.
Let's get started by exploring a bit of your background, you started out in physics,
made your way over to climate change policy and now you're working in data science in
that field.
Tell us a little bit more about that path for you.
Sure, so when I was small I always wanted to be a physicist.
I just love to ask why and to get down to the root of the question and when you got
to the fundamental laws of the universe, I thought there was far enough.
When I finished my degree, I really wanted to have a positive impact or to address some
of the issues that needed addressing in the world so I decided to try and work on clean
energy and energy policy.
I worked with a think tank that campaigned to reform the European carbon markets and that
was when I got started into programming.
They had a Python and Django stack in fact and I sort of took over looking after their
database, scraping the European Commission's website where they released all of the data
about who was buying and selling these emissions permits and then went from there really and
I've been at carbon tracker almost two years trying to convince the financial markets
about climate risk.
When you say trying to convince the financial markets about climate risk, what is that
intel?
Sure.
So we, carbon tracker popularized this idea of a carbon bubble that if the valuations
of fossil fuel companies that are some extent based on the amount of fossil fuels they're
going to sell in the future, if they're wrong in their predictions and those valuations
are in fact going to be constrained by climate change either by economic factors or by governments
saying you're not going to be allowed to continue in this way that has a financial implication.
So it's all been about quantifying the extent to which they're going to face those risks.
Practically what does that mean?
What is the carbon tracker product or service that works on what this qualification?
So we've done a lot of work on oil and gas and then I've been working on coal power.
For oil and gas we put all the oil projects on a cost curve.
So we say how much does it cost for the company X in this region to produce a barrel of oil?
And then we say how much oil could we burn in a climate compatible scenario?
And the projects that are above the line don't make it in.
And so we say, you know, some companies are better positioned for this energy transition
in the climate compatible way and some aren't.
And then we started doing the same for coal power, saying if there's going to be a coal
power phase out in a given country, how should that be done if it's done on the least cost
basis?
Which company has the highest cost units and should therefore be closed down first?
And that is about the market as well, because I'm saying close down, but it may be that
the market sends a signal such that the coal plant is no longer economically viable and
that phase out would happen in the same way.
And so who are the users of this intelligence, is it the policymaker or the companies themselves?
We think that we have some appeal to a lot of different audiences, definitely policymakers,
because they need to be aware of these forces that are going on in the end to transition
and make sure the policies are there to support it.
But investors certainly, because we think there's a lot of shareholder value at risk for
companies that aren't prepared for the changes that are happening.
We've seen that a lot in Europe in the utility sector, where European utilities lost a lot
of money because they didn't see renewables coming, essentially.
But also to campaigners and journalists as well.
And so where does data science fit into all of this?
So we take a lot of different databases from all sorts of different sources and try and
join them up.
But the project that we've done more recently was really about looking at places where
the databases weren't so good, where it's very hard to get publicly available data about
the kinds of operations that we're trying to find out about.
So trying to find coal production data, trying to find it out about how plants in India
have running their pollution technologies, all of which has impacts on how much they cost
to run and the profitability of the companies.
So it's about joining up different data sources, making them talk to each other and about
filling in the gaps, where we don't have good data.
So walk us through the process.
How did you go about pulling together all of these data sources?
So firstly, I have to give thanks to all of the data providers all around the world doing
a huge amount of good work in terms of making, I mean, we buy a lot of data sets, but
there are also some really good freely available public data sets, including coal swarm, which
have a global database about operating coal plants, which I know is a lot of work and
really a testament as well to open source, because they have a website where a wiki site
where people can upload information, so they aggregate them, do a lot of work themselves
to put it all together.
Of course, there's no uniform naming as often the case, so we have to do a lot of string
matching and patent matching in order to sync up where we think these projects are and
bring together these different qualities that we're looking for basically in terms of capacity
in terms of ownership, and these things are often some organization may be really interested
in what is there, but a different organization may be really interested in who owns it and
how that ownership structure has changed over time, so gluing those things together, and
in terms of how we do it, because carbon track has a lot of X city analysts, so we do a
lot of analysis in Excel, and so my first task in the beginning was to interface with
a lot of existing Excel databases and put them into SQL, which is usually what I would
use, and then just a lot of string matching, they're not terribly exciting at the base layer,
but essential for everything we did on top.
Okay, and are you doing some work with satellite imagery as well, is that right?
Yeah, yeah, so why don't I just jump in and tell you a little bit about that project?
And so what we wanted to find out was the capacity factor of coal plants, and the capacity
factor is simply how much of the time, more proportion of the time they're running compared
to their maximum theoretical maximum output, and that's a key part of the financial evaluation
of any plant, because the profits that they can make are a balance between the revenues
they can earn when they run, and the ongoing costs, and the fixed costs that they have
to pay every year in terms of maintenance costs, in terms of lifetime extensions, and
other fixed costs.
So working out how often these things are running is a really important part of our modeling.
Now for various parts of the world, that data is public, though sometimes it's a few years
out of date, so in the US and in Europe, you can find that data through the EPA and in Europe
through, it's called a NSOE, which is about electricity transparency, but the data is
out there, but for other parts of the world, particularly sort of Southeast Asia and across
Asia, that data was not available, or if it is available it's about four years out of date.
So we wanted to investigate whether we could use remote monitoring that is satellite imagery
to answer that question and work out how often these plants are running, and we thought
we had a good shot at it because at the same time, the proliferation of this, of competitors
in the satellite imagery space, and the availability of the data has just increased hugely in the
last few years.
Partly that's to do with these nano-sats and cube-sats, which are very small, but still
give reasonable quality outputs, and some companies like a planet who we worked with put up
a sort of over a hundred of the, I think 150 now, these cubes-sats, and I know a lot
of other providers, I think you're doing the same or certainly thinking of if they're
not already.
The goal then is to look at areas where you've got known coal production facilities and
try to predict their output, their capacity at any given point in time, and it sounds
like one of the things you're doing is you've got the, you've got some known facilities
and their output via the NSoE and EPN you're using these as labels to then train a model.
Exactly, that was our starting point that we would have all this label data, and thus
we would have a strong base to do some proper supervised learning.
And so without too much further scoping, we then set off to try and do them.
And possibly as with all things, perhaps we should have spent longer scoping out or thinking
about other kinds of solutions, but we just dove straight in and I started writing scripts
to interface with satellite providers, APIs, where you can query for images and things.
So the first step was to create GeoJson objects specifying the areas where it's called areas
of interest or AOIs, what's referred to by these satellite image providers.
So we had to make shape files or GeoJson files for each one of these coal parts that we
wanted to get the data.
Often we just drew a square, a sort of 1.5 kilometer squared size shape file around each
plant and sent that off to the image provider's API and said, give me all the images you
have in the last two years that meet a threshold of cloud cover of, say, less than 10% cloud
cover and let me download it.
And that was amazing in the first instance because that sort of capability, again, is very
new because usually or what has historically been the case was if you want to work with
satellite data, you get image tiles and the tiles are like the photograph, the image
that the satellite sensor takes and it covers a large area and it's a big file.
So if I wanted to do that, historically, I would have had to download sort of terabytes
of data and then sort of click it myself.
Thankfully, I didn't have to do that. I could create a labeled small, well, set of small
images where images were a few hundred kilobytes each or something because they had been pre-clipped
out.
I think a digital globe called these image chips.
And so reasonably quickly, we had a data set of around sort of 80,000 images for the
rest of the labelled.
So we were sort of ready to go relatively quickly though.
There were some challenges there in terms of making sure that the time zones were right
for the labels and the satellite images and sinking that up correctly.
That was a minor headache.
And I mean, another challenge that we saw straight away was that we didn't have any sort
of balance in terms of the training classes.
If we were going to look at whether plants were running very hard or they were running
somewhat or they were off, or if we just, as we ended up just looking at whether plants
were on or off, it was a very imbalanced class and part of that is because satellites
take images in the daytime when it and it plants also happen to run more in the daytime.
So there's no images in the night when plants were running less and more likely to be off.
So we had very imbalanced class, training classes straight away that we had to deal with
them, which we counted just by sampling the imbalanced class multiple times and then
doing lots of manipulations to it.
And that worked out okay, but not fantastic, I would say.
So that was one limitation that we saw straight away.
And were you primarily looking at this binary status, whether the plant was on or off as
opposed to trying to measure the plume area and try to determine the output in some way?
Initially, we absolutely wanted to be hopes, we had high hopes that our model would just
spit out a reasonable output number for each plant and that had to be calibrated on the
actual capacity, the size of each plant, which we had, we had that information.
But unfortunately, we tried to just do a make a regressor, but it didn't turn out terribly
well.
The results it gave didn't seem to reflect reality particularly.
And part of that, we found by doing some manual analysis, was just looking at the classification
that it was doing in terms of the actual pixels that it was counting up.
And this is where perhaps an early weakness that was, I think, I made a decision early
on to take visual images from the API, which is author-rectified sort of our RGB images
because visual images are what's needed, of course, to do an image processing machine
learning technique.
What's also available is 13 band analytic images, which contain a lot, a lot more information.
And we ended up using that to do the counting up rather than doing a visual classifier because
any kind of very light pixels, which could have been to do with a wish-lectivity of metal,
other kinds of sort of smoke plumes and things coming off the ground, that was all getting
counted up and sort of adding to the plants output, which wasn't a good reflection of
what was actually going on.
So fairly quickly, we decided that we weren't going to be able to get the output using this
method.
And so we ended up doing, on Google Earth Engine, we made an algorithm that would count
up the pixels in each image and just say, using the analytical images, because it could
do a spectral linear mixing of the information in those analytic image chips.
And that gave us a better estimate of an instantaneous output.
So our idea then was to combine these two different techniques to say, here's when we think
the plants are often, here's what we think is an average of the plants output over that
given time.
And by combining them, we got to a reasonable results in the end.
You mentioned earlier that a lot of your challenges could be traced back to kind of the way
you scope the problem or problem definition.
And this is one example of where you need it to pivot in the way you approach the problem
or are there others that we can talk for?
Yeah, certainly. I mean, one thing that became clear to us over time as we got more into
the problem was we started out really wanting to understand China and what's happening in
China.
And of course, from a climate change perspective, China is a huge global emitter.
And it has a huge amount of cold capacity.
It has sort of 1100 gigawatts of cold capacity.
So from a climate perspective, it's very important to know what's going on.
And historically, the data has been very poor and China has been criticized for publishing
statistics that perhaps don't really reflect what's going on.
And that sort of thing, or the data is only available years later.
And what we found as we started working more and more closely on this and talking to
more and more people who are closer to the ground was that more in recent times.
In fact, there is a real-time monitoring effort in five Chinese provinces.
So we were actually able to obtain very good high quality granular data for coal emissions
in some of these places, which was brilliant because we could then calibrate the predictions
we'd made for China in those regions.
But then, of course, to some extent reduce the need for the methodology we had developed.
So we were sort of happy pleased and frustrated at the same time with that.
But then, of course, that was only for five regions.
It was only a mandatory in certain places.
So for other places, of course, we can still use our methodology and be even more certain
or confident in the results that it gave us.
Did you find that the models that you created translated well from one region of the world
to another, or did they not?
Yeah, we did.
And I think the interesting variation, in fact, was to do with the ground state of the plants.
Oh, and yeah, here's another good challenge that I'd forgotten about.
Or maybe my mind tried to push it down into my surface.
So annoying.
So just to make it very clear about how we were trying to do this,
we were looking at the visual plumes, the smoke and water vapor that come out of the flu stack
and the cooling towers from a coal plant.
And we were testing and sort of sub-setting on various sort of samples to try and improve
our results.
And I forget how exactly we stumbled upon it, but as soon as we did, it became so obvious
that there are various kinds of cooling technologies that coal plants have.
There's what's called once through, which is if the plant is located in a body of water,
it'll suck in water from, say, a lake or a river, use that to cool its equipment and then
pass it out a slightly warm stream of water.
And there's also active and passive cooling towers, which is draft cooling towers, where
evaporation is used and produces these huge steam plants.
Now those two cooling types produce very different cooling plumes.
And the once through method, in fact, did not behave very well in our models at all.
So once we were able to slice that out and only examine plants with active and natural
draft cooling, we saw much better results.
So that was a real sort of step change that perhaps if we had had a deeper understanding
of the physical characteristics of coal plants at the beginning, we could have seen coming
as it happened, and we were able to infer it by looking at the results as we went along.
It also strikes me that, for example, when would make a big difference if there's no
wind, you're going to have this plume that kind of goes straight up and doesn't spread
horizontally in your image.
But if it's very windy, it'll kind of disperse in one direction, and your model would
need to be able to figure that out to perform more.
Absolutely, we did do a reasonable amount of investigation against environmental data.
And what really enabled us to do this was Google Earth Engine, which had a lot of our environmental
data layers that we could just bring in very easily.
It would have been much more difficult if we weren't using Google Earth Engine, I must
say.
There were other platforms off of similar things, but we were able to, so we were able
to use that to get per image a wind speed, a wind direction, temperature, atmospheric
pressure, and humidity.
And so we did some analysis against all of those variables to see if there was significant
correlation with plume size, and with wind, we did find if the wind isn't blowing the
plume goes straight up and doesn't disperse so much, which when you're looking over vertically.
And some satellites, in fact, do offer an angle offset so that you can try and do volumetric
estimations.
That was something that we'd left on the side and didn't end up trying to investigate.
That was sort of beyond the amount of time we had, but if the wind is very low, you
get the straight up plume.
If the wind is going somewhat, you get more of a dispersal, and if it's a very strong
wind, then you get more of a straight line, so actually the plume area looking from above
isn't as great as sort of medium wind. But overall, we didn't find that it was hugely
significant, saying with the other estimates, because we hoped that by bringing those variables
into the model, we were able to get a really good accuracy, but they didn't actually help
as much as we had hoped, unfortunately.
And I think that was when I talked about the scoping, I mean, we didn't know how hard
this problem was. I think it's the real truth of the matter.
When you just look at the noise and the variation of generation output compared to the plumes
emitted by a plant, you just find that there's a huge variation. It's just very noisy to
start with.
So the accuracy of what we could end up with was always going to be limited by that natural
variation. Whereas if, for example, we were just trying to quantify what kind of equipment
or what kind of cooling technologies, or say we were doing something different and just
looking at whether solar panels are built in a certain area, static stuff that it, rather
than time series data, that's a lot more, I think you can get a much finer degree of
accuracy than trying to get something dynamic like this.
Right.
So we lived and learned.
And so the availability of these image chips helped quite a bit, but did that totally alleviate
the data preparation challenges, or did you have other areas where data prep was really
a big challenge for you?
Doing everything together at the beginning was some challenge.
But beyond that, I mean that just the amount of cloud architecture and cloud solutions
that are being offered really does a lot of the work for you.
And for example, AWS now hosts a lot of archived satellite image data sets that anyone can
access, which we investigated.
We didn't end up using a lot of these satellite image providers now have their own platforms.
And they really try and take as much out of your hands as possible so you can really get
to the implementation phase as quickly as possible.
And it's entirely possible to do extremely rapid prototyping now with new kinds of image
classifiers on satellite data, whether that's through Google Earth Engine, which you can
just pull in all sorts of different sources, or through one of these, mostly I must say,
they're proprietary, but a lot of them have sort of quite low barriers, especially for
academic institutions, and a lot of them have not profit elements as well.
Of course, they have huge fixed costs because they put satellites into the air and they
need to make a return on it, but they are very keen for people to try new things and
to, well, they hope, make commercializable solutions.
For example, Planet has had a Kaggle Challenger while ago about classifying bits of the Amazon
rainforest to try and get people working on these different challenges.
And I know that there's a huge amount being done across this sort of NGO and energy, climate
change space on all sorts of things around water, around coal mining, forest cover, a forest
station, or all those sorts of questions, which can, as I say, I mean, we are a carbon
tracker.
And people that work with this project, it was only sort of maybe three of us.
And the fact that, you know, we could even pull together a sort of machine-learning project
with satellite images, I think, says a lot about how quickly you can get up and running
with modern tooling.
For the classifier that was looking to predict the status of the plants, what model did
you use?
Visual classifier, so it looks like a CNN.
Yeah, exactly.
It was a CNN.
We used Inception V3.
And we just wanted an image-pressing model that knew edges and knew its way around an
image.
We ended up stripping off some of the last final layers.
And I think we added a dense layer.
But, you know, we didn't have to do a huge amount of tweaking.
Maybe we should have done, but that was what we ended up doing, and it worked pretty
well.
Pretty okay, I would say.
You were evaluating your model performance based on, again, this kind of, these known US
and Europe plants, or did you end up with something different?
No, we just used the label data for the US and the EU.
And later, when we found out about this source in China, we were able to do some training
using the label China data as well.
And so we just looked at precision and recoil for the plants that were on and the plants
that were off.
And again, because of this very imbalanced data set, most of the images were of plants
with some sort of smoke vapor bloom.
We had very good accuracy or very good precision for plants that were on.
And then much worse, in fact, for plants that were off, which dragged the accuracy down
overall, which is quite understandable with a very balanced training set.
Head of curiosity, what's the typical kind of cycle time for these plants being on or
off?
Are they cycling several times a day?
Or is it, you know, they're cycling for, you know, days or weeks on end?
So there's a cost if you're a coal plant operator to go down to zero is if you have to start
up against what's called a cold start, and it's much easier to just lower your output.
So plants are sort of ramping their outputs up and down quite a lot.
But then they might turn off for, you know, a period of days.
But most of the time, it's more, more useful for them to try and keep running as much
as possible and to keep some inertia going.
And for the classifies point of view, that isn't very helpful because it's just saying
oh, it's still on, it's still on.
And that was why the results that we got were initially we thought we could do have a
really fine grain time series and be able to see, you know, every day or every time
we got an image whether it's on or off and draw a very nice time series chart.
And what we ended up deciding was that we could get reasonable accuracy over a time period
because we could take these averages.
But we weren't able to get, you know, to say in this week what's happening.
That was just a limitation that we found.
Though as plants in some areas, I know in the UK where the economic conditions are much
more competitive for coal and they're really struggling, they are trying to innovate which
does mean cycling more often because they'll be trying to make sure that they hit the spikes
in our prices and then just sort of lay low the rest of the time.
So I mean, in theory, or I mean, running these models hopefully will be able to identify
some of these changing operating characteristics or I mean certainly spotting plant down times
and things will be something that we would we would love to do in terms of just setting
this running and then automatically flagging when things are changing.
And did you define off with regard to some kind of threshold to account for this, you
know, cycling down very low or was it hard off was the only off that you were considering?
It's a good question because we came up with two things there, whether the pixel count
would give a value at all, but then there was sort of generic noise in the images.
And when you said before about how did the algorithm fare in different regions of the
world?
Of course, the image is a square around the power plant.
So it is getting some of the landscape, of course the landscape is differing all around
the world and we didn't know to what extent the CNN would pick up various bits of this
landscape and other parts of the coal plant and try and include those and including of
course there's some sighted on rivers and on coastal regions and things.
So we really wanted to try and delve into it to ensure that that wasn't happening and
it would be repeatable.
And so even when it was off there was a base level of reflectivity.
So in short, yes, there was some sort of threshold.
And so that's the classifier on the regressor side was that did you use the same sources
and so we and EPA for your training data or for your labels or did that come from someplace
else?
No, we started off with the regress and we did try to use exact same training data, but
we found that it just wouldn't converge, which was very frustrating.
So we switched that pretty quickly and we just thought this isn't going to, this isn't
going to produce good results.
You didn't get anywhere with the regressor at all or you switched to a different approach.
Yeah, we just switched out and and and said let's just do this with a pixel counting method
basically.
Ah, got it, got it.
So this wasn't a machine learning, there was no training data, just counting the pixels
kind of correlating that to an output based on, you know, understanding of the, you
know, the underlying principles precisely, precisely.
And, you know, so of course, you know, we like to use, love to use machine learning
where we can, but in this case, we just thought, okay, we just need to get a, we just need
to get some number outputs here and the simplest way looks to be best.
I mean, of course, a huge challenge in terms of doing the pixel counting is that, you
know, a, a white pixel and a smoke plume for a plant looks indistinguishable from, from
cloud cover, though there is some, some stuff about sort of height and, and some reflectance
and those sorts of things, but essentially for various parts of the world and in Southeast
Asia, certainly very difficult, you're never going to get optical images frequently enough
that aren't obscured by clouds.
The reason why you could set a limit like 10%, that doesn't mean that there's 10% clouds
evenly distributed.
It means in a, in an image, a large image tile, there will be some clouds somewhere.
And so you just hope that that doesn't intersect with the area of interest you're trying to
look at.
And most of the time it didn't, but some of the time it did.
So that was another source of, of error with, with cloud cover.
And there are some senses that are less affected by this.
And there's a whole lot of senses, including from the European Space Agency's Copernicus
platform that, that look at ozone, self-adarkside, nitrogen dioxide, and there's also some radar,
short synthetic aperture radar that will pick up on metals.
And that's very good for looking about where things are being built or if there's industrial
operations happening.
And I know there's some, some very exciting analytics companies that are, that are using
that approach to regularly check on, on industrial applications or economic activity automatically.
So they, their algorithms will just run every day and say, you know, his, his, what we
think, the economic activity in this sector is based on this, this radar data.
How long did you spend on this project?
All told, I think we, we, we got the data at around Christmas last year or that's when
I was writing the, the scripts to get the first training set.
And it took us, I mean, we did other projects in the interim, so it probably took us about
half, half, half two of our time for about nine months, something like that.
Maybe, maybe a little more, actually, you know, I shouldn't, I shouldn't really undersell
the amount of effort it does, but it was, it, you know, it does no one any favours to
do that.
And it was really to fill in the gaps in, in a, in a global coal economics platform that
we have just been developing.
We're going to launch, we're launching just at the start of December, which is going
to showcase the cost and the profitability for every coal plant worldwide, as well as
contrast it when renewables are going to be cheaper, or it'll be cheaper to build new renewables
than to even run existing coal plants.
So that's the, the data we wanted to make available.
And that was why we were doing this project such that we could fill in the gaps of places
that we, we really couldn't find any data at all.
So this project did feed into that portal for China, but over the year, we also tried
to develop a lot of local partners, and so it turned out that they're talking to people
and getting, getting local expertise in all parts of the world was also a very useful
way as well.
That's, is that machine learning?
I don't know.
Review machine at work.
Interesting, interesting.
And so are these models, are they kind of, in an ongoing production, production state feeding
into this portal, or have you, did you kind of do a survey and then collect the data and
then is the portal, was any more static data?
Yeah, it's, our aim is to update it every, every three months, while I had the ambition that
I was sort of productionized everything and just keep all the scripts running.
We didn't, ultimately didn't have the, the need to do that, you know, we're, we're
not from profits, so we're not selling, we don't have customers in that, in that sense.
So it never seemed like it was, it was, it was really necessary to do that.
However, because there is, it is really interesting for a lot of different groups to have this
information, and this is where in the, the not profit sector, you know, it's, it's, it's
very hard to get these sort of partnerships set up where someone builds it and everyone
else uses it, but I have some hopes that, that that will happen.
And there is a lot of very exciting information sharing and technical methodology sharing across
these, these different sort of technologies around the energy transition, and it's, it's
only going to improve over time.
So, so hopefully our work will, we'll be a part of that in some way.
It strikes me that this is, you know, just one really interesting use case around how
machine learning AI data science can be applied in the sphere of climate change, climate
change policy.
Are there others that jump out for you?
Yeah, well, absolutely.
I mean, the, the, the Paris climate agreement and the international agreements, one big
part of that is about countries, inventories, and, and holding every, every signatory to
those agreements to, to account in terms of what they say they're going to be doing.
And, and that means, and that's a whole industry in terms of monitoring compliance, monitoring
CO2 emissions, because what falls out straight away just, just on our coal work is, if you
know how often the plants are running and you know some other characteristics, you can
estimate how much CO2 is, is being emitted.
And there's a lot of satellites now that have, that are just for CO2 monitoring as well
as other air pollutants.
So using, using, using AI, you can, and, and data science, you can run automated algorithms
to just continuously measure these things.
And again, a lot of this data is, is available, it's just a question of gluing and all together.
So I do see a, a future where everyone or civil society is able to, to keep tabs on all
large polluters and, and nation states to say, you know, are you really doing, are you
staying true to the commitments you've made on, on climate?
Is this large polluter staying true to its commitments to, uh, run its air pollution scrubbers
or is it in fact, you know, just, just pumping out pollution into the atmosphere, um, and
thinking it can get away with it.
So I see that really starting to change in, in real time, and the other part of it is
making arguments about the speed of growth of, of the positive side of the, the speed of
uptake of renewables of solar deployment and also smarter, uh, predictive capabilities
in terms of power systems.
So being able to, uh, more accurately predict our solar and wind output, um, shifting
demand, uh, and smart grid, so smoothing out the peaks in NG demand, um, which reduces
the system cost for every on and, and further erodes the case for, for fossil fuels.
Um, so, uh, and across, I, I mainly look at the power sector, but there's, you know, power
also has a water stress and water impacts, which is, which is extremely important, that's
something that can very easily be seen and changes over time can very easily be seen,
um, from space or in automated ways, uh, deforestation can, can now be tracked again in,
sort of, near real time.
Um, so yeah, a huge amount of applications, and I think, um, the, the more, the more people
that, that we in the non-profit sector can sort of mobilise to, to walk in those, the
more data scientists we can, we can attract to, to come and work on those projects, the,
better able will be to, to address them.
Lawrence, thank you so much for taking the time to chat with me.
I really enjoyed it.
Right, Sam, thanks so much for having me on.
All right, everyone, that's our show for today.
For more information about today's show, visit twimmolai.com.
Be sure to visit twimmolcan.com for information or to register for Twimmolcan AI platforms.
Thanks again to C3 for their sponsorship of today's episode.
And to check out what they're up to, visit c3.ai.
As always, thanks so much for listening and catch you next time.
