1
00:00:00,000 --> 00:00:13,040
All right, everyone. I am here with Nestreen, most of the

2
00:00:13,040 --> 00:00:19,560
day. Nestreen is the co-founder of Vernique. Nestreen, welcome back to the

3
00:00:19,560 --> 00:00:20,880
Twonwall AI podcast.

4
00:00:21,160 --> 00:00:24,560
Thanks so much for having me again, Sam.

5
00:00:24,560 --> 00:00:30,880
For those that recognize the name, Nestreen is a long-time friend of the show and

6
00:00:30,880 --> 00:00:40,120
this is your third interview maybe. The last time we caught up was in January of

7
00:00:40,120 --> 00:00:47,080
2020 before so much in the world changed and we were talking about trends and

8
00:00:47,080 --> 00:00:52,480
natural language processing and today we'll of course be talking a little bit

9
00:00:52,480 --> 00:00:58,160
about that but we'll be focusing on some big changes in your world since we

10
00:00:58,160 --> 00:01:04,920
last spoke and starting with the company that you founded. So maybe catch us up

11
00:01:04,920 --> 00:01:09,160
a little bit on big changes in your world since the last time.

12
00:01:09,560 --> 00:01:15,080
Absolutely. Well, it's so interesting that the last time that I talked with you

13
00:01:15,080 --> 00:01:20,160
was January 2020. It was precisely third of January 2020 and the reason I

14
00:01:20,160 --> 00:01:26,480
remember this is that you may recall it was literally the day after U.S.

15
00:01:26,480 --> 00:01:31,720
basically went on the brink of having a war with Iran. So we were all extremely

16
00:01:31,720 --> 00:01:34,840
stressed. That was a night. I literally didn't sleep. We were all like checking

17
00:01:34,840 --> 00:01:40,680
our phones trying to see whether or not there will be literally a war and it

18
00:01:40,680 --> 00:01:47,200
was just a very bizarre time. So that's kind of how 2020 started for a bunch of

19
00:01:47,200 --> 00:01:52,400
us not even knowing where it's headed. So I'm even third of January was a very

20
00:01:52,400 --> 00:01:58,960
very weird year. So anyways since then that that very even actually played a

21
00:01:58,960 --> 00:02:05,720
major role in how the rest of my life went on from that point on. So

22
00:02:05,720 --> 00:02:11,080
basically diverse so many other things that happened like on eighth of January

23
00:02:11,080 --> 00:02:17,840
there was this plane that was downed with like 176 people on it. A lot of them

24
00:02:17,840 --> 00:02:22,600
Iranians some of whom I literally went to school with you know who could have

25
00:02:22,600 --> 00:02:27,000
been literally me. So after so much thinking I always you know I've been

26
00:02:27,000 --> 00:02:32,520
working in the startup scene for the last five years or so and I truly

27
00:02:32,520 --> 00:02:38,080
believe in wanting to make impact through the power of startups and through

28
00:02:38,080 --> 00:02:43,360
being able to laser focus of the particular fundamental problem in real

29
00:02:43,360 --> 00:02:49,400
world. So anyways I always wanted to do this and this whole series of events

30
00:02:49,400 --> 00:02:58,840
around like January and February 2020 made me kind of think how blessed I am as

31
00:02:58,840 --> 00:03:04,000
an individual and the kind of opportunities that I have and I'm not

32
00:03:04,000 --> 00:03:09,440
necessarily leveraging with older people that didn't even get to live to want

33
00:03:09,440 --> 00:03:14,080
to fulfill their dreams like it felt like it's time so I should just just do

34
00:03:14,080 --> 00:03:19,600
it and go for it. So anyways the process of starting learning was so many you

35
00:03:19,600 --> 00:03:23,120
know years of in progress basically but it was always about waiting for the

36
00:03:23,120 --> 00:03:29,040
right time for the right time. And then in February 2020 I was like that's it I'm

37
00:03:29,040 --> 00:03:34,080
gonna start it. And hilariously the first day basically official first day of

38
00:03:34,080 --> 00:03:42,720
Bernic is March 1st 2020. Not knowing what we are really up for so we had all

39
00:03:42,720 --> 00:03:48,920
these omit my co-founder and I've had a New York City. Yeah we had all these

40
00:03:48,920 --> 00:03:54,320
grand plans of starting this deep tech AI company in New York City with like

41
00:03:54,320 --> 00:03:59,120
you know a bunch of plans as to when we go fundraising, when we go hire our team

42
00:03:59,120 --> 00:04:07,920
and of course March 2020 the you know shutdowns started and then we in April

43
00:04:07,920 --> 00:04:14,560
so we realized that okay we are in the middle of a global pandemic so the rest

44
00:04:14,560 --> 00:04:21,680
is kind of history it's the timing was impeccable to say the least so we

45
00:04:21,680 --> 00:04:26,480
basically kind of put everything on hold for a couple of months just to see

46
00:04:26,480 --> 00:04:32,480
where the world is going like it wasn't necessarily clear that anyone wanted

47
00:04:32,480 --> 00:04:39,760
to even think about a brand new AI company while everyone was basically

48
00:04:39,760 --> 00:04:46,320
looking their booms. So yeah we just waited a couple of months and then around

49
00:04:46,320 --> 00:04:51,600
like late 2020 we were like okay let's just go for it so we started a fundraiser

50
00:04:51,600 --> 00:04:56,000
fundraising and in about a month or so we could like close around and then we

51
00:04:56,000 --> 00:05:01,440
started hiring and now we are based in New York City if you're in flat iron

52
00:05:01,440 --> 00:05:05,360
specifically it's like the dream neighborhood we're working in a physical

53
00:05:05,360 --> 00:05:12,800
office and this is our space and it's been just a really really great time to

54
00:05:12,800 --> 00:05:18,080
to want to do this honestly despite all the hurdles how hard it was to go

55
00:05:18,080 --> 00:05:22,240
through the pandemic to start a pandemic company basically I think it has

56
00:05:22,240 --> 00:05:26,480
helped us build all sorts of muscles that we never thought we can

57
00:05:26,480 --> 00:05:31,680
and hence we are really in a very good shape in terms of all the other

58
00:05:31,680 --> 00:05:35,760
hurdles that we have to overcome moving forward. That's awesome that's awesome

59
00:05:35,760 --> 00:05:45,280
congratulations on that. So the company is as yet stealth and not much is

60
00:05:45,280 --> 00:05:49,440
known publicly about what you're up to but you've promised that you're going to

61
00:05:49,440 --> 00:05:56,080
share a little bit of pull back the kimono a bit so to speak.

62
00:05:56,080 --> 00:06:00,560
What's the company up to? What are the challenges that you're hoping to take on?

63
00:06:00,560 --> 00:06:07,920
Absolutely so our mission is to enable anyone to make data informed decisions

64
00:06:07,920 --> 00:06:12,800
be for their personal matters or for their businesses without having

65
00:06:12,800 --> 00:06:17,600
any kind of technical background and what that means is that we are

66
00:06:17,600 --> 00:06:22,720
basically innovating in the human machine interfaces area where we want to

67
00:06:22,720 --> 00:06:28,560
replace hard to grasp things such as programming languages with other

68
00:06:28,560 --> 00:06:32,720
intuitive modalities of interaction including of course human natural

69
00:06:32,720 --> 00:06:36,480
language which you know given our backgrounds is abundantly clear but we are

70
00:06:36,480 --> 00:06:40,080
not just you know biased thinking that natural language is the

71
00:06:40,080 --> 00:06:45,200
answer to anything and everything in terms of intuitiveness we are thinking

72
00:06:45,200 --> 00:06:50,240
actively and working at simply towards other like modalities of interaction

73
00:06:50,240 --> 00:06:54,640
that would be really easy to use and intuitive in the given context.

74
00:06:54,640 --> 00:06:59,600
So basically this is like two degrees how much we usually share publicly but

75
00:06:59,600 --> 00:07:04,160
I love talking with you so I would love to start sharing a little bit more in

76
00:07:04,160 --> 00:07:09,360
terms of our vision for Verneak and what kind of a technology

77
00:07:09,360 --> 00:07:13,840
of your building so very deep tech company meaning that we are really

78
00:07:13,840 --> 00:07:17,680
we are overcoming various like scientific and engineering challenges

79
00:07:17,680 --> 00:07:23,120
that would ultimately enable us to build such a platform that I was talking

80
00:07:23,120 --> 00:07:26,800
for data informed decision making so what does data informed decision

81
00:07:26,800 --> 00:07:31,440
making means it means that there's all sorts of data out there they can come in

82
00:07:31,440 --> 00:07:36,080
any shape or form they can basically have any format they could be any small

83
00:07:36,080 --> 00:07:40,640
any large they could basically be anything and everything and

84
00:07:40,640 --> 00:07:45,520
when you want to put an interface on top of it and enable anyone to use it

85
00:07:45,520 --> 00:07:49,760
like let's say get and the little calls insights about it or like just ask

86
00:07:49,760 --> 00:07:53,600
any kind of questions that they can they have that can help them

87
00:07:53,600 --> 00:07:57,360
better their lives or their better their businesses that means that you have

88
00:07:57,360 --> 00:08:01,200
to have interoperability and that means that you have to be domain general

89
00:08:01,200 --> 00:08:05,200
we are not trying to build like something that will be just working in one

90
00:08:05,200 --> 00:08:08,160
sector we don't want to build something that is going to be just in a

91
00:08:08,160 --> 00:08:12,480
particular domain operation we want to build a truly domain general AI

92
00:08:12,480 --> 00:08:18,400
platform that can have interoperability across all sorts of data so it's a pretty

93
00:08:18,400 --> 00:08:23,200
grand mission in terms of the technology that it needs and we are actively

94
00:08:23,200 --> 00:08:27,760
working on all sorts of problems that in the academic sense has been

95
00:08:27,760 --> 00:08:32,560
you know an issue for the past couple of years as well so we are working

96
00:08:32,560 --> 00:08:37,680
on basically building for some foremost intuitive and easy to use

97
00:08:37,680 --> 00:08:41,520
interfaces including natural language building a domain general natural

98
00:08:41,520 --> 00:08:44,880
language interface in and of itself is very very challenging

99
00:08:44,880 --> 00:08:49,200
but mainly if you're trying to make it controllable we're trying to make

100
00:08:49,200 --> 00:08:53,200
sure to be assigned provenance as to where the data source comes from

101
00:08:53,200 --> 00:08:56,720
and what kind of data you're basically putting in the loop

102
00:08:56,720 --> 00:09:00,240
on top of it we're trying to make sure that these

103
00:09:00,240 --> 00:09:04,080
basically interfaces that we put are controllable

104
00:09:04,080 --> 00:09:08,000
meaning that we can have interaction with them and we can teach them

105
00:09:08,000 --> 00:09:11,440
or help them forget something that they've learned that was wrong

106
00:09:11,440 --> 00:09:15,760
you're also trying to make sure that these models are actually

107
00:09:15,760 --> 00:09:20,720
able to get all sorts of feedback and make better decisions down the road for

108
00:09:20,720 --> 00:09:25,440
the sake of the user so basically they should be able to not just

109
00:09:25,440 --> 00:09:30,000
base their decisions that they help the user make on one source but like just

110
00:09:30,000 --> 00:09:34,400
bring all the bits and pieces together and basically do reasoning

111
00:09:34,400 --> 00:09:38,960
but so many other characteristics that this kind of a fundamental AI

112
00:09:38,960 --> 00:09:42,240
platform of your building should have including the fact that it should be

113
00:09:42,240 --> 00:09:46,720
amenable to data privacy issues and so there's so many other things if you're

114
00:09:46,720 --> 00:09:51,520
actively working on but in terms of our lines of AI research we are

115
00:09:51,520 --> 00:09:55,840
of course working on conversational agents you're building basically dialogue

116
00:09:55,840 --> 00:09:58,560
systems that have these features that I was just

117
00:09:58,560 --> 00:10:04,320
outlining you're also working on like old sorts of zoning phenomena that are

118
00:10:04,320 --> 00:10:10,080
not data hungry that actually can help us go from it domain to another with

119
00:10:10,080 --> 00:10:13,920
like the least amount of time and the least amount of

120
00:10:13,920 --> 00:10:19,760
supervision of course but we also have problems on the data side itself so

121
00:10:19,760 --> 00:10:23,920
building a platform that has interoperability

122
00:10:23,920 --> 00:10:28,000
across the board requires like innovations in distributed systems

123
00:10:28,000 --> 00:10:32,000
requires innovations in building like runtime run time engines

124
00:10:32,000 --> 00:10:36,480
that can actually digest all sorts of data and understand it so

125
00:10:36,480 --> 00:10:41,520
it's not just AI research that is a problem that we are tackling but also

126
00:10:41,520 --> 00:10:46,480
you know other kind of computer science

127
00:10:46,480 --> 00:10:50,800
issues that we have to grapple with on a day-to-day basis and I will

128
00:10:50,800 --> 00:10:56,800
add that we are very mindful of having it like design

129
00:10:56,800 --> 00:11:00,960
elements and design driven thinking to be the front of the center of what we

130
00:11:00,960 --> 00:11:06,080
are building so that we don't kind of like repeat the mistakes that have been

131
00:11:06,080 --> 00:11:10,960
made in general in the texting of like bunch of technical people getting

132
00:11:10,960 --> 00:11:14,800
together thinking they have a intuitive solution and then turns out

133
00:11:14,800 --> 00:11:19,920
it's really not usable so that's that's our front and center we are really

134
00:11:19,920 --> 00:11:24,560
striving to build something that works for the masses as opposed to

135
00:11:24,560 --> 00:11:30,480
something that is just you know for the sake of pushing the boundaries of AI

136
00:11:30,480 --> 00:11:35,120
how big is the team so far? we were total of eight people we had like six or

137
00:11:35,120 --> 00:11:39,520
so interns also that loved us so we are very small we're

138
00:11:39,520 --> 00:11:44,960
hiring across the board we used to be just hiring on for like research team and

139
00:11:44,960 --> 00:11:49,760
the engineering team but now we're also hiring on the business side so we are

140
00:11:49,760 --> 00:11:54,800
hoping to grow to to over so more people as soon as possible we wanted it

141
00:11:54,800 --> 00:11:59,600
yesterday but hiring is the toughest one of the toughest things in

142
00:11:59,600 --> 00:12:05,760
my experience yeah I ask because the you've outlined a pretty broad

143
00:12:05,760 --> 00:12:11,360
set of areas not just that you want to build product in but that you need to

144
00:12:11,360 --> 00:12:17,520
do fundamental research in and that sounds like a ton for eight people

145
00:12:17,520 --> 00:12:24,480
oh my god how do you yeah before we get to kind of how you take that on

146
00:12:24,480 --> 00:12:30,320
maybe another way of coming out that is like how do you think about the

147
00:12:30,320 --> 00:12:36,960
MVP for the product like when I think of you know the simplest

148
00:12:36,960 --> 00:12:42,400
thing that you know kind of the simplest version of the the vision you

149
00:12:42,400 --> 00:12:47,280
outlined I think of something like a you know natural language query generator

150
00:12:47,280 --> 00:12:50,480
but those you know exist they're not without

151
00:12:50,480 --> 00:12:54,560
challenges like how do you think about the the MVP and how do you

152
00:12:54,560 --> 00:13:00,480
contrast with something like natural language query generation good question

153
00:13:00,480 --> 00:13:05,040
so I will just first inform us say something as a in like in

154
00:13:05,040 --> 00:13:12,240
parentheses we call MVP legum at Bernie so legum is this Swedish term I don't

155
00:13:12,240 --> 00:13:15,440
know if you're familiar with it that means just to write them out and just

156
00:13:15,440 --> 00:13:18,560
there's only like we take issues over the night we take

157
00:13:18,560 --> 00:13:25,280
issues on like minimum viable product often not be really minimum and not

158
00:13:25,280 --> 00:13:32,080
being really viable so anyway the question is what's just to write

159
00:13:32,080 --> 00:13:35,280
them on for us right just what we've been dealing with for the past

160
00:13:35,280 --> 00:13:40,480
like year now doing like R&D until we get to a point that we're

161
00:13:40,480 --> 00:13:43,760
comfortable with taking our product out so it's a very

162
00:13:43,760 --> 00:13:49,600
evasive complex question what the logon basically should be for our domain

163
00:13:49,600 --> 00:13:53,280
on the business side what we're doing is that we're very practical we are taking

164
00:13:53,280 --> 00:13:58,320
this technology one domain at a time we're like right now focusing on a

165
00:13:58,320 --> 00:14:02,800
particular domain for example that we're hoping to come out with in the next

166
00:14:02,800 --> 00:14:07,680
couple of months and in that particular domain then you have this

167
00:14:07,680 --> 00:14:11,280
you know flexibility of narrowing down the data that you're training on narrowing

168
00:14:11,280 --> 00:14:16,400
down basically the capabilities as well right so for example right now

169
00:14:16,400 --> 00:14:20,640
we we know that we want our system to be instantaneous and it is so we have

170
00:14:20,640 --> 00:14:24,080
like a threshold that we know okay if it's responding after one second

171
00:14:24,080 --> 00:14:28,880
it's like definitely a downer but if it is like we can just keep pushing the

172
00:14:28,880 --> 00:14:33,280
boundaries so there are such parameters that we are kind of tuning but

173
00:14:33,280 --> 00:14:38,320
anyways so in terms of the it taking it one domain at a time

174
00:14:38,320 --> 00:14:42,960
we are doing that but we have this rule in house that we are like if it is

175
00:14:42,960 --> 00:14:50,240
basically diverging more than 20% from the general domain general platform

176
00:14:50,240 --> 00:14:55,120
we are just working too much on one particular domain so basically we are

177
00:14:55,120 --> 00:15:02,720
making our logon or MVP to be an 80% of the same kind of code base same kind of

178
00:15:02,720 --> 00:15:07,840
technology that we would have and spending 20% of time in a particular

179
00:15:07,840 --> 00:15:12,560
domain without it generalizing out of it so it's it's been it's not easy I

180
00:15:12,560 --> 00:15:17,040
would say and I think I will have a more clear answer to that when we are out

181
00:15:17,040 --> 00:15:22,560
with our product I know what I said is a little vague as to what the MVP should

182
00:15:22,560 --> 00:15:25,680
be but there's so much to talk about in terms of how to

183
00:15:25,680 --> 00:15:30,560
curate the exact packaging of such a technology

184
00:15:30,560 --> 00:15:34,880
so that is still meaningful you can get enough

185
00:15:34,880 --> 00:15:40,640
signal back from the market and from the actual users and then keep iterating

186
00:15:40,640 --> 00:15:47,120
and is the example of some kind of natural language query is that

187
00:15:47,120 --> 00:15:55,920
directionally you know accurate accurate enough to give us some to provide some

188
00:15:55,920 --> 00:16:01,040
concrete grounding for the conversation you're going domain at a time some

189
00:16:01,040 --> 00:16:07,360
envisioning you pick a domain who knows what that domain is let's say

190
00:16:07,360 --> 00:16:13,200
contracts you know legal contracts right and so you know some lawyers want to

191
00:16:13,200 --> 00:16:17,440
do discovery and it's hard for them to find what they want and so

192
00:16:17,440 --> 00:16:21,200
you're essentially giving them a box to type in and you're doing smarter

193
00:16:21,200 --> 00:16:25,040
things with what they type in to get them to you know to turn that into a

194
00:16:25,040 --> 00:16:29,120
query that you can run against whatever systems they're trying to search

195
00:16:29,120 --> 00:16:33,840
against as an example but that's you know that's the unstructured text

196
00:16:33,840 --> 00:16:37,120
example you know there's the structure text example you're

197
00:16:37,120 --> 00:16:42,560
working with you know power companies and you're trying to help them manage

198
00:16:42,560 --> 00:16:46,320
their grids better or something and you give them a box to

199
00:16:46,320 --> 00:16:51,760
you know tell me the you know aggregate power across whatever whatever

200
00:16:51,760 --> 00:16:57,760
six regions I guess I'm you know the picture that

201
00:16:57,760 --> 00:17:02,880
is coming to mind is maybe you know

202
00:17:02,880 --> 00:17:10,000
AI or natural language interfaces for like business intelligence types of

203
00:17:10,000 --> 00:17:13,440
schools or problems yes that's that's very close

204
00:17:13,440 --> 00:17:18,160
that we can talk for hours and hours right about the state of the field like

205
00:17:18,160 --> 00:17:22,560
what are the relevant technologies how they're failing and why they're

206
00:17:22,560 --> 00:17:27,040
failing and why what you're doing is is really different but yes at the end of

207
00:17:27,040 --> 00:17:30,880
the day it's it resembles that so basically let me

208
00:17:30,880 --> 00:17:34,320
move you an example this is not what we are doing right now

209
00:17:34,320 --> 00:17:38,400
but one of the reasons we wanted to do what we are doing is that even as

210
00:17:38,400 --> 00:17:43,520
technologists it's so hard to make decisions based off of your data

211
00:17:43,520 --> 00:17:48,240
because it just requires so many bells and whistles so

212
00:17:48,240 --> 00:17:52,560
for example you just asked me right before this like what did you eat for breakfast

213
00:17:52,560 --> 00:17:56,400
right and I told you well I don't eat breakfast so as an individual I

214
00:17:56,400 --> 00:18:00,960
do intermittent fasting I have this app that I'm recording

215
00:18:00,960 --> 00:18:04,720
the number of hours that I'm not eating a day I've been just doing it

216
00:18:04,720 --> 00:18:08,480
abitually in the past couple of years then I have this other app for I'm

217
00:18:08,480 --> 00:18:13,040
recording what I'm eating every day and then just because I have some like

218
00:18:13,040 --> 00:18:16,800
underlying conditions that I have to be careful with whatever x, y and z

219
00:18:16,800 --> 00:18:21,680
that I'm like putting in my body basically and then I have this other app

220
00:18:21,680 --> 00:18:26,160
where I'm like basically recording my late I hop on a scale rate to scale

221
00:18:26,160 --> 00:18:29,600
every morning personally so there's this app that is capturing

222
00:18:29,600 --> 00:18:33,760
the data from my health on a day-to-day basis on my bait on a day-to-day

223
00:18:33,760 --> 00:18:37,440
basis so anyways as an individual like myself although I'm a

224
00:18:37,440 --> 00:18:41,680
technical person it's so hard for me to get the answer to a question like

225
00:18:41,680 --> 00:18:45,920
I don't know what is there any correlation between my weight loss and the

226
00:18:45,920 --> 00:18:48,960
number of hours that I'm intermittent fasting right

227
00:18:48,960 --> 00:18:53,200
and why is it hard because it requires me figuring out how to download the

228
00:18:53,200 --> 00:18:58,560
data from all of those apps if you even can if you even can exactly

229
00:18:58,560 --> 00:19:02,640
and then maybe I don't know going to a like opening up

230
00:19:02,640 --> 00:19:07,280
like a ipython book and trying to remember what is the correlation function or

231
00:19:07,280 --> 00:19:10,400
some sort and then trying to call it and then coming up with the

232
00:19:10,400 --> 00:19:13,440
answer right it just takes so much time and I am a

233
00:19:13,440 --> 00:19:18,240
technical person so imagine what the world looks like for other individuals and

234
00:19:18,240 --> 00:19:22,240
small businesses who have all these kinds of data and they are making wrong

235
00:19:22,240 --> 00:19:27,040
decisions on a day-to-day basis because of their lack of access to such

236
00:19:27,040 --> 00:19:30,960
easy-to-use interfaces so anyways we want to very much in line be what you

237
00:19:30,960 --> 00:19:34,880
said you're not like you know basically doing any of these things that I

238
00:19:34,880 --> 00:19:38,800
mentioned right now but in the long run for the company that's

239
00:19:38,800 --> 00:19:43,120
division we want it to be that we have this one

240
00:19:43,120 --> 00:19:47,040
interface that we can put on top of anything and everything and it can

241
00:19:47,040 --> 00:19:50,560
smartly navigate its way to find the right sources

242
00:19:50,560 --> 00:19:53,520
and then come back with the with their results

243
00:19:53,520 --> 00:19:57,520
but it definitely is basically like a natural language

244
00:19:57,520 --> 00:20:00,880
quarrying interface but as I mentioned we believe

245
00:20:00,880 --> 00:20:05,360
tremendous we believe in the tremendous value that other modalities of

246
00:20:05,360 --> 00:20:09,360
interaction bring into the scene and that's something that has been

247
00:20:09,360 --> 00:20:12,720
definitely neglected but on top of it you know I've worked on

248
00:20:12,720 --> 00:20:15,600
natural language understanding basically my whole

249
00:20:15,600 --> 00:20:20,800
like life like not even adult life like like whatever 15 years right there

250
00:20:20,800 --> 00:20:24,080
just no one knows how to build a domain general

251
00:20:24,080 --> 00:20:26,960
language interface like that we've had tremendous

252
00:20:26,960 --> 00:20:29,760
progress in the field in the past couple of years which is why

253
00:20:29,760 --> 00:20:34,640
likes of myself are motivated to want to finally take

254
00:20:34,640 --> 00:20:38,880
of you know this kind of technology to the market so that we can get

255
00:20:38,880 --> 00:20:43,040
you know clear signal as to the flaws right and the problems and the

256
00:20:43,040 --> 00:20:48,080
solutions of feedback into that basically academic AI world

257
00:20:48,080 --> 00:20:51,840
we could go down the rabbit hole of

258
00:20:51,840 --> 00:20:56,800
wearables and personal health tech and all that kind of stuff and maybe that's a

259
00:20:56,800 --> 00:21:01,280
a fourth conversation but for now I think the the thread that I want to pull a

260
00:21:01,280 --> 00:21:08,720
little bit on is the the state of AI research

261
00:21:08,720 --> 00:21:13,280
in the domains relevant to what you're trying to build

262
00:21:13,280 --> 00:21:16,720
because a lot of what you're I think your

263
00:21:16,720 --> 00:21:21,360
contention is is hey that you know the idea of a search box that

264
00:21:21,360 --> 00:21:25,040
you know lets you find things out right that's something we've been pursuing

265
00:21:25,040 --> 00:21:29,120
for a while but we've been failing and some of that is

266
00:21:29,120 --> 00:21:35,120
execution and you know or different execution and different vision

267
00:21:35,120 --> 00:21:41,280
but some of it is that the research or the technology just

268
00:21:41,280 --> 00:21:46,960
fundamentally isn't there and so maybe a next place to explore is

269
00:21:46,960 --> 00:21:51,920
you know where you see the gaps in the technology and how you're

270
00:21:51,920 --> 00:21:56,480
using you know that assessment to kind of prioritize the way

271
00:21:56,480 --> 00:22:00,960
you're approaching research at Verneak.

272
00:22:00,960 --> 00:22:04,720
Absolutely so I think this is very much actually

273
00:22:04,720 --> 00:22:08,800
continuation of the conversation we had in January 2020

274
00:22:08,800 --> 00:22:13,280
when you know I was reviewing basically the state of the field

275
00:22:13,280 --> 00:22:17,920
if you you may recall so my net was that we've come a very long way

276
00:22:17,920 --> 00:22:22,400
in natural language understanding and natural language processing in general

277
00:22:22,400 --> 00:22:27,600
in the past like now six years or so it's been tremendous

278
00:22:27,600 --> 00:22:31,840
how much progress we've made not on just down a stream past but also in real

279
00:22:31,840 --> 00:22:35,040
world products basically that have been impacted by these kind of

280
00:22:35,040 --> 00:22:41,440
technologies but I characterized like the flaws in a few ways that I think

281
00:22:41,440 --> 00:22:45,040
some of which are now kind of being

282
00:22:45,040 --> 00:22:48,560
addressed by the recent developments so like just to

283
00:22:48,560 --> 00:22:54,640
backtrack a little bit so it's kind of interesting so I started my personal

284
00:22:54,640 --> 00:22:58,560
work in natural language understanding

285
00:22:58,560 --> 00:23:03,520
because I was back in times is back in high school I used to work in robotics

286
00:23:03,520 --> 00:23:06,000
then I switched to natural language understanding and comments this

287
00:23:06,000 --> 00:23:10,960
reasoning in particular because I came across this motivating example that

288
00:23:10,960 --> 00:23:17,360
I saw in a random book that was like for an AI system for a machine to understand

289
00:23:17,360 --> 00:23:22,880
natural language it requires to put together lots of bits and pieces about

290
00:23:22,880 --> 00:23:26,640
the world that it is grounded in and hence it's a so-called AI

291
00:23:26,640 --> 00:23:30,000
complete problem so the motivating example was that

292
00:23:30,000 --> 00:23:33,680
the monkey ate the banana because it was hungry and

293
00:23:33,680 --> 00:23:38,080
is the it referring to the you know monkey or the banana

294
00:23:38,080 --> 00:23:42,080
basically was the the the riddle for the AI system

295
00:23:42,080 --> 00:23:47,040
so I literally literally started my life in AI in

296
00:23:47,040 --> 00:23:51,280
sorry natural language understanding for for that very same problem

297
00:23:51,280 --> 00:23:54,640
we're tackling that very same problem which I found fascinating

298
00:23:54,640 --> 00:23:59,120
and then fast forward these kind of problems were really not the focus for

299
00:23:59,120 --> 00:24:03,360
the field for a very long time until in 2015 and

300
00:24:03,360 --> 00:24:07,040
16 or so that these you know at the time it was like

301
00:24:07,040 --> 00:24:11,440
biolistiums that were suddenly starting to to to leave their mark on

302
00:24:11,440 --> 00:24:15,120
some comments this reasoning in natural language understanding benchmarks

303
00:24:15,120 --> 00:24:17,760
so much so that it was touted as the

304
00:24:17,760 --> 00:24:22,720
solution but then of course with the transformers in 2017 or so

305
00:24:22,720 --> 00:24:25,600
everything changed for the better so much so

306
00:24:25,600 --> 00:24:29,760
that it just kept kept going right we had all these other models and

307
00:24:29,760 --> 00:24:32,960
for me personally always I was on the camp of

308
00:24:32,960 --> 00:24:36,640
questioning but they're not any of this is real progress as someone that

309
00:24:36,640 --> 00:24:40,320
cared about comments and reasoning which is

310
00:24:40,320 --> 00:24:45,440
sort of historically been this line of thinking about reasoning which

311
00:24:45,440 --> 00:24:50,080
supposedly conflicts with like you know

312
00:24:50,080 --> 00:24:57,040
machine learning and stochastic and like look into to the data etc

313
00:24:57,040 --> 00:25:00,880
so anyways for me it was very natural to want to be biased against the

314
00:25:00,880 --> 00:25:05,760
progress so much so that in 2016 basically the work that I

315
00:25:05,760 --> 00:25:10,560
personally did was on building a the story close test benchmark

316
00:25:10,560 --> 00:25:14,080
which was basically trying to push these systems to showcase whether or not

317
00:25:14,080 --> 00:25:17,600
they have any sort of comments is reasoning by continuing this

318
00:25:17,600 --> 00:25:21,840
like a short story and basically finishing the story

319
00:25:21,840 --> 00:25:26,240
right in a right way that is like reasonable and logical

320
00:25:26,240 --> 00:25:31,280
so anyways then we then I did that work the intention was to

321
00:25:31,280 --> 00:25:35,760
assess whether or not these models of common sense and I wasn't that

322
00:25:35,760 --> 00:25:39,360
convinced that they do I wasn't that convinced that they have any reason in

323
00:25:39,360 --> 00:25:43,040
capabilities and then the transformers came of course GPT-1

324
00:25:43,040 --> 00:25:46,960
did a great job on story close test and then we were like oh is it just

325
00:25:46,960 --> 00:25:50,480
picking up on the intricacies of the data is it just

326
00:25:50,480 --> 00:25:54,400
basically learning the biases that exist in the data

327
00:25:54,400 --> 00:25:57,920
data sad not doing true national language understanding

328
00:25:57,920 --> 00:26:02,480
and then turned out that it maybe was to a degree but we even changed the test

329
00:26:02,480 --> 00:26:05,680
said to another version of story close test that was kind of

330
00:26:05,680 --> 00:26:13,680
debiased and then GPT-1 was basically the only model that could sustain its performance

331
00:26:13,680 --> 00:26:18,560
and anyways still I'm an skeptical let's say and I'm like okay I want to see

332
00:26:18,560 --> 00:26:22,880
deeper language understanding and deeper comments as reasoning

333
00:26:22,880 --> 00:26:29,440
and then GPT-3 came out basically this in 2020

334
00:26:29,440 --> 00:26:36,080
that was doing zero shot performance on story close test meaning using none of the

335
00:26:36,080 --> 00:26:40,560
training data at all and it was getting to 80 something percent performance

336
00:26:40,560 --> 00:26:44,400
which if you had told me in grad school I would not have

337
00:26:44,400 --> 00:26:49,440
taught as possible right in 2020 so these this is tremendous progress

338
00:26:49,440 --> 00:26:56,800
right I think it's really hard for us to try to

339
00:26:56,800 --> 00:27:00,400
sweep it under a rug that these models are not showing any

340
00:27:00,400 --> 00:27:05,360
fundamental language understanding and all they're doing is pattern recognition

341
00:27:05,360 --> 00:27:09,680
because one can even argue what it is that we do as

342
00:27:09,680 --> 00:27:13,680
understanding human beings and how much of it is recognizing patterns and

343
00:27:13,680 --> 00:27:18,400
the piers we have built on the world throughout our lifetime

344
00:27:18,400 --> 00:27:22,560
so anyways I think this is tremendous progress and I'll add one other note

345
00:27:22,560 --> 00:27:27,120
that since to me the progress of the field of natural

346
00:27:27,120 --> 00:27:30,720
language understanding has been so intertwined with my personal line of

347
00:27:30,720 --> 00:27:35,200
research as well so in 2020 I also did this really

348
00:27:35,200 --> 00:27:39,280
interesting work that I personally believe then with my great colleagues at

349
00:27:39,280 --> 00:27:43,520
elemental cognition called glucose that was basically

350
00:27:43,520 --> 00:27:49,680
about building these world models while you're reading a story so story

351
00:27:49,680 --> 00:27:54,240
closest was all about read for sentences predicting but now let's go

352
00:27:54,240 --> 00:27:59,120
maybe on that that's just not just predicting but also come up with these

353
00:27:59,120 --> 00:28:07,360
deep like a world model of the you know a person's kind of set of

354
00:28:07,360 --> 00:28:13,040
states and like events and their causal chains and like draw a coherent picture

355
00:28:13,040 --> 00:28:17,120
of the narrative that you're building let's make this as the new

356
00:28:17,120 --> 00:28:23,680
basically benchmark for evaluating whether or not a system is showing

357
00:28:23,680 --> 00:28:28,720
any sort of deep understanding so anyways this was we you know came up with this

358
00:28:28,720 --> 00:28:33,520
work and basically did all of it which you know we can delve deeper dive deeper

359
00:28:33,520 --> 00:28:38,720
into but it when it came out was like two or three months

360
00:28:38,720 --> 00:28:44,240
after the GPT-3 work and it's fascinating how GPT-3

361
00:28:44,240 --> 00:28:49,520
was doing better than GPT-2 on even in this particular basically task of ours

362
00:28:49,520 --> 00:28:54,000
and how far it has gone in showing that it has some sort of a world model so

363
00:28:54,000 --> 00:28:59,520
I personally think that whoever kind of claims that these models do not have

364
00:28:59,520 --> 00:29:04,560
any world model don't have any kind of a human like cognition don't have

365
00:29:04,560 --> 00:29:12,080
any deep understanding of language it's it's really incorrect to say the least

366
00:29:12,080 --> 00:29:16,480
of course these models are fundamentally flawed no question right we talked

367
00:29:16,480 --> 00:29:22,560
about this in the last session that these models are extremely biased

368
00:29:22,560 --> 00:29:28,960
they are really easy to get tricked which makes them in my opinion brittle

369
00:29:28,960 --> 00:29:33,840
you know how like it was the thing to call symbolic models back in time

370
00:29:33,840 --> 00:29:38,160
brittle I think these models are also quite brittle right it's easy to

371
00:29:38,160 --> 00:29:43,120
for them to get sidetracked and make really stupid mistakes although they

372
00:29:43,120 --> 00:29:48,320
work say well like often so these and you know of course these models are

373
00:29:48,320 --> 00:29:53,440
also really not controllable which as I mentioned is something that we are

374
00:29:53,440 --> 00:29:58,320
working actively at brain ink on so these are all flaws that they have

375
00:29:58,320 --> 00:30:02,800
no question but I think saying that these models do not have a world

376
00:30:02,800 --> 00:30:09,280
model do not have any understanding is is really not correct so those are the

377
00:30:09,280 --> 00:30:13,040
contentions of the stochastic parrots paper wasn't it?

378
00:30:13,040 --> 00:30:19,360
yes and I so it's it's we can talk for hours and hours about this as well so

379
00:30:19,360 --> 00:30:24,720
many things I personally think that look

380
00:30:24,720 --> 00:30:31,120
these these models are really great pattern recognizers

381
00:30:31,120 --> 00:30:35,840
and one can argue that recognizing patterns and then trying to

382
00:30:35,840 --> 00:30:40,800
stitch them together is not real understanding but I would

383
00:30:40,800 --> 00:30:45,760
refute that and I would say that look at the end of the day for me as a researcher

384
00:30:45,760 --> 00:30:49,520
old I could do throughout the past six years or so

385
00:30:49,520 --> 00:30:52,640
was to think about ways of evaluating

386
00:30:52,640 --> 00:30:58,720
and like NOU systems for deeper understanding and these models are proving

387
00:30:58,720 --> 00:31:01,600
consistently that they're making progress

388
00:31:01,600 --> 00:31:06,880
towards doing what constitutes as having understanding so we can of course

389
00:31:06,880 --> 00:31:11,840
argue what is a good benchmark I think benchmarking is one of the problems

390
00:31:11,840 --> 00:31:15,760
we've had for years and years we are making progress but

391
00:31:15,760 --> 00:31:21,200
for me one of the reasons that why I want to work in startups is to

392
00:31:21,200 --> 00:31:25,680
you know build something in real work that actually works for end users

393
00:31:25,680 --> 00:31:31,520
in the messy noisy real-world environments as opposed to our lab settings

394
00:31:31,520 --> 00:31:35,440
so that's definitely a problem we have that we have really narrow inherently

395
00:31:35,440 --> 00:31:41,440
narrow and bias benchmarks but setting data side I think that

396
00:31:41,440 --> 00:31:45,360
you know this is like a kind of theoretical right like

397
00:31:45,360 --> 00:31:49,280
there are people who don't believe in distributional semantics being the

398
00:31:49,280 --> 00:31:52,960
like expression kind of meaning that you can represent

399
00:31:52,960 --> 00:31:57,200
and believe that formal semantics and formal kind of meaning representation is

400
00:31:57,200 --> 00:32:01,280
the way to go which I would argue against I think having

401
00:32:01,280 --> 00:32:06,160
a way of representing meaning distributionally sort of representing a word

402
00:32:06,160 --> 00:32:13,200
by the context in which it just is often occurring at is

403
00:32:13,200 --> 00:32:17,360
is a viable representation of meaning so I think as at the end of the day

404
00:32:17,360 --> 00:32:20,640
if we have the right benchmarks for evaluating

405
00:32:20,640 --> 00:32:24,800
representation of meaning we have the right benchmarks for

406
00:32:24,800 --> 00:32:28,400
evaluating common sense reasoning and if these models pass

407
00:32:28,400 --> 00:32:32,720
past them that that tells you something and this is to take I had been

408
00:32:32,720 --> 00:32:38,240
GPT-3 work came out so many people were asking my opinion as to

409
00:32:38,240 --> 00:32:41,760
how I think about this because you know I was one of the

410
00:32:41,760 --> 00:32:46,080
proponents of less push-d systems for deeper understanding and like

411
00:32:46,080 --> 00:32:50,000
you know they're not they're really lacking it but the truth is we

412
00:32:50,000 --> 00:32:54,960
have made progress I would say that we need to move the

413
00:32:54,960 --> 00:33:01,520
basically the the bar you should you know raise the bar of course as to

414
00:33:01,520 --> 00:33:05,840
pushing these models to go further and further but they've definitely come

415
00:33:05,840 --> 00:33:10,160
very far already so I don't think that these models are

416
00:33:10,160 --> 00:33:14,080
parrots really it depends on the definition of course of

417
00:33:14,080 --> 00:33:18,000
a parrot but if it means that it's really just repeating

418
00:33:18,000 --> 00:33:23,520
without having any kind of an understanding I think that's

419
00:33:23,520 --> 00:33:29,040
that's not the case so going back to kind of the

420
00:33:29,040 --> 00:33:32,720
the state of research broadly and the gaps

421
00:33:32,720 --> 00:33:37,600
that need to be overcome for you to accomplish what you're trying to do at

422
00:33:37,600 --> 00:33:45,520
Vernique it sounds like you know the first one of those is

423
00:33:45,520 --> 00:33:49,600
or the first you know area you know of exploration is

424
00:33:49,600 --> 00:33:53,360
just understanding in general in order for you to do what you're trying to

425
00:33:53,360 --> 00:33:57,360
do you need a system to be able to understand to some degree or

426
00:33:57,360 --> 00:34:01,920
another or according to some you know metrics the

427
00:34:01,920 --> 00:34:06,000
what the user is trying to express in whatever box they're typing in or

428
00:34:06,000 --> 00:34:10,720
whatever interface they're using and it sounds like

429
00:34:10,720 --> 00:34:17,120
you're saying that models that you would lean on for that

430
00:34:17,120 --> 00:34:22,080
understanding are broken in lots of ways and

431
00:34:22,080 --> 00:34:25,440
that's part of what you need to research is how to fix

432
00:34:25,440 --> 00:34:29,120
the ways that they're broken you know bias and

433
00:34:29,120 --> 00:34:33,360
transparency explainability however you want to to put that

434
00:34:33,360 --> 00:34:38,000
but you are you have seen enough evidence of

435
00:34:38,000 --> 00:34:42,240
enough understanding for you to do what you're trying to do for them to be

436
00:34:42,240 --> 00:34:46,960
promising yes absolutely and honestly not even

437
00:34:46,960 --> 00:34:50,880
just talking about the particular technology that you're building at

438
00:34:50,880 --> 00:34:55,680
Vernique but generally for the field I think that

439
00:34:55,680 --> 00:34:58,640
well first and foremost I hope that so many people

440
00:34:58,640 --> 00:35:03,840
work on so many other directions of AI so that we really have diversity of

441
00:35:03,840 --> 00:35:06,800
thought we have diversity of thinking we have other

442
00:35:06,800 --> 00:35:14,560
models that may get to flourish if anything deep learning

443
00:35:14,560 --> 00:35:18,640
trend has taught us that by a couple of people not giving up on what they

444
00:35:18,640 --> 00:35:22,480
believe then they could prove us all wrong right and then

445
00:35:22,480 --> 00:35:26,320
like all the amazing progress that we're seeing is the

446
00:35:26,320 --> 00:35:30,400
fruits of that basically but anyways on our end in particular

447
00:35:30,400 --> 00:35:33,440
for the sake of natural language understanding yes I think these

448
00:35:33,440 --> 00:35:37,840
models are to have tremendous flaws but they've shown

449
00:35:37,840 --> 00:35:44,000
him enough evidence of being basically

450
00:35:44,000 --> 00:35:48,000
foundational to say the least so I know that this is also something

451
00:35:48,000 --> 00:35:52,080
pretty controversial right now like you know when

452
00:35:52,080 --> 00:35:55,600
Stanford start calling these this whole like transfer learning and

453
00:35:55,600 --> 00:36:00,160
pre-training and fine-tuning paradigm foundation models

454
00:36:00,160 --> 00:36:04,160
so many people started raising eyebrows as to okay fund foundation should be

455
00:36:04,160 --> 00:36:08,160
reliable foundations should be that you can poke in like

456
00:36:08,160 --> 00:36:11,040
just just change things which I completely agree

457
00:36:11,040 --> 00:36:16,720
but setting the kind of arguing over the terminology aside

458
00:36:16,720 --> 00:36:20,480
I think that these models have shown enough evidence that they can give us

459
00:36:20,480 --> 00:36:24,480
like lexical and word knowledge which I think are

460
00:36:24,480 --> 00:36:28,640
very foundational for for building natural language understanding and

461
00:36:28,640 --> 00:36:32,400
dialogue systems like you know I come from the school of

462
00:36:32,400 --> 00:36:35,840
thought of people who have spent their lifetime trying to build

463
00:36:35,840 --> 00:36:39,600
semantic parsers which is which has to do with these formal

464
00:36:39,600 --> 00:36:43,760
like semantics of representing each and every word

465
00:36:43,760 --> 00:36:47,440
in a way that like conforms to an ontology and then

466
00:36:47,440 --> 00:36:50,240
how you would go about like representing a verb and

467
00:36:50,240 --> 00:36:53,440
conjunction with something else and like connecting the dots and

468
00:36:53,440 --> 00:36:56,800
everything so that you can represent the meaning and then on top of it

469
00:36:56,800 --> 00:37:01,040
building a dialogue system that recognizes intents and like tracks and

470
00:37:01,040 --> 00:37:04,880
those planning and everything so I think that

471
00:37:04,880 --> 00:37:09,200
what these models are doing and you know they are even

472
00:37:09,200 --> 00:37:12,400
very promising in doing things in a multilingual sense

473
00:37:12,400 --> 00:37:16,960
but anyways these what these models are doing is that they're enabling us to

474
00:37:16,960 --> 00:37:20,960
really let go of some of these pipeline like

475
00:37:20,960 --> 00:37:24,960
things that we had in an LP and just not starting from scratch basically

476
00:37:24,960 --> 00:37:28,080
starting from a model that comes at some sort of a

477
00:37:28,080 --> 00:37:32,480
lexical and word knowledge and turns out comments this knowledge baked in

478
00:37:32,480 --> 00:37:37,440
inish you know I they have flaws as you mentioned and we've talked about

479
00:37:37,440 --> 00:37:41,040
the fact that they're not controllable the fact that they're not transparent

480
00:37:41,040 --> 00:37:44,480
and the fact that you can't let them like teach them

481
00:37:44,480 --> 00:37:48,320
intractively and let them forget about things that they're doing wrongly these

482
00:37:48,320 --> 00:37:53,920
are old problems that need to be fixed which may require its own paradigm shift

483
00:37:53,920 --> 00:37:57,040
could be architecturally could be on the data side

484
00:37:57,040 --> 00:38:02,400
but I think yes to answer your question in one final sentence

485
00:38:02,400 --> 00:38:05,600
these models have shown enough evidence that they could be used as

486
00:38:05,600 --> 00:38:08,480
foundations so that we don't start from scratch

487
00:38:08,480 --> 00:38:15,520
yeah yeah maybe in kind of keeping conscious of time

488
00:38:15,520 --> 00:38:18,720
maybe we can switch gears from talking about kind of the

489
00:38:18,720 --> 00:38:25,440
research broadly to the the research that you're pursuing

490
00:38:25,440 --> 00:38:31,120
and you know I think we can infer from there what you think is

491
00:38:31,120 --> 00:38:38,160
missing in the the research broadly so it sounds like some controllability of

492
00:38:38,160 --> 00:38:44,480
language models is one of the areas what are some of the other areas

493
00:38:44,480 --> 00:38:52,560
of research that you're digging into so yes controllability is one

494
00:38:52,560 --> 00:39:00,240
absolutely we are so our line of thinking is that we believe in

495
00:39:00,240 --> 00:39:04,000
I think I've seen people use this terminology so I'll try to repeat it

496
00:39:04,000 --> 00:39:09,600
a retrieval augmented generation so we want to make sure that we don't

497
00:39:09,600 --> 00:39:14,720
build you know like visci washi language models at the end of the day just

498
00:39:14,720 --> 00:39:18,240
you don't know where the information is coming from we want our models to be

499
00:39:18,240 --> 00:39:21,680
able to retrieve from existing data sources as I mentioned the

500
00:39:21,680 --> 00:39:24,320
soul about data and from decision making we want to

501
00:39:24,320 --> 00:39:27,840
you as a user to know where your information is coming from

502
00:39:27,840 --> 00:39:31,760
or if we are like actively like telling you what

503
00:39:31,760 --> 00:39:35,760
basically rethink we want you to know the source of it

504
00:39:35,760 --> 00:39:40,960
so we want to basically work on ways of retrieving what is out there

505
00:39:40,960 --> 00:39:45,200
with the resource and the provenance intact this is another thing that we're

506
00:39:45,200 --> 00:39:48,640
pursuing and in general generalization right we want to build a

507
00:39:48,640 --> 00:39:52,880
domain general platform how do you make it so that the models

508
00:39:52,880 --> 00:39:58,160
that you have are truly generalizable right this has been an ongoing line of

509
00:39:58,160 --> 00:40:02,400
work for in deep learning for many years but it's

510
00:40:02,400 --> 00:40:06,480
you know of course not solved I think last two years or so has been phenomenal

511
00:40:06,480 --> 00:40:12,960
in terms of how how much more flexible and generalizable these models are

512
00:40:12,960 --> 00:40:17,200
but it's still not you know there's a very very long way to go

513
00:40:17,200 --> 00:40:21,440
the other thing is a data array these models are extremely data hungry

514
00:40:21,440 --> 00:40:26,160
I love how there has been a lot of progress on kind of zero shot and in

515
00:40:26,160 --> 00:40:31,200
context learning kind of paradigm at paradigms but at the end of the day

516
00:40:31,200 --> 00:40:36,240
still the wherever you go to a new domain and you see this first of the

517
00:40:36,240 --> 00:40:40,320
first hand when you work in a real-world setting like in startups

518
00:40:40,320 --> 00:40:44,480
these models are truly data hungry right how can you make it so that these

519
00:40:44,480 --> 00:40:48,320
models are more sample efficient and they don't really need that much

520
00:40:48,320 --> 00:40:52,720
training data for adapting to a new domain and this is for us

521
00:40:52,720 --> 00:40:57,360
of utmost priority because we literally want to make a domain general model

522
00:40:57,360 --> 00:41:01,200
so how do we just go from a domain to another

523
00:41:01,200 --> 00:41:08,000
on the technology side without spending a lot of time just collecting data

524
00:41:08,000 --> 00:41:12,480
and then tuning models learning like new intricacies of that that

525
00:41:12,480 --> 00:41:20,000
existing domain and although I see a lot of value

526
00:41:20,000 --> 00:41:24,320
and someone like myself having spent time thinking about

527
00:41:24,320 --> 00:41:29,840
problems that like a pure deep learning person would not have had and the

528
00:41:29,840 --> 00:41:33,760
value that it kind of brings to building a real-world product so

529
00:41:33,760 --> 00:41:38,320
like we need to work on how to build conversational agents right it's not just

530
00:41:38,320 --> 00:41:42,000
about natural language understanding like one utterance at the time

531
00:41:42,000 --> 00:41:46,400
but it has a lot to do with word modeling really and

532
00:41:46,400 --> 00:41:51,440
kind of building a belief system right the old-school kind of dialog systems

533
00:41:51,440 --> 00:41:55,920
had this framework called BDI that was really cool so belief desire

534
00:41:55,920 --> 00:41:58,640
intention so when you have a full-fished

535
00:41:58,640 --> 00:42:03,600
conversation with someone you basically think about

536
00:42:03,600 --> 00:42:07,840
building models basically and you think about them about what the other

537
00:42:07,840 --> 00:42:12,080
party knows what they don't know what you know how you can help them reach

538
00:42:12,080 --> 00:42:16,720
to a certain goal and you plan right so the existing systems out there

539
00:42:16,720 --> 00:42:19,840
don't really do any planning right these are all

540
00:42:19,840 --> 00:42:24,560
kinds of things that need to to be worked on how do you basically

541
00:42:24,560 --> 00:42:28,480
and build a sustainable word model that includes the sets of belief desires

542
00:42:28,480 --> 00:42:32,800
and intentions and you dynamically basically

543
00:42:32,800 --> 00:42:38,720
monitor right what's happening and another thing is this is also very important

544
00:42:38,720 --> 00:42:42,400
is that we need to have memory right systems at the end of the day

545
00:42:42,400 --> 00:42:47,440
controllability comes from them being able to store what they've learned

546
00:42:47,440 --> 00:42:52,480
from the interactions separate from their actual

547
00:42:52,480 --> 00:42:56,080
like prior knowledge separate from their other kind of conditions so that they

548
00:42:56,080 --> 00:42:59,600
can make an important basically decision and these are also

549
00:42:59,600 --> 00:43:03,600
all kinds of things that are not in these so-called like foundation models

550
00:43:03,600 --> 00:43:08,880
right and need to be worked on and so given

551
00:43:08,880 --> 00:43:13,440
the again kind of the breadth of all of these things that you need to figure

552
00:43:13,440 --> 00:43:18,080
out in order to to build a product how do you

553
00:43:18,080 --> 00:43:23,920
how do you scope that down like you know each of those could be a you know a

554
00:43:23,920 --> 00:43:30,560
four-year PhD effort right or or many right how do you scope that down and

555
00:43:30,560 --> 00:43:35,280
connect that to you know your your mission as a startup founder

556
00:43:35,280 --> 00:43:38,400
to get a product out the door that meets a need

557
00:43:38,400 --> 00:43:44,800
yes so we are not of course I outlined our years of planning right

558
00:43:44,800 --> 00:43:48,240
outlined the kind of problems that we have to work on it doesn't mean that

559
00:43:48,240 --> 00:43:51,520
we're working on it at this moment or we need to work on it

560
00:43:51,520 --> 00:43:56,720
immediately so that just to answer you on the business side how we are

561
00:43:56,720 --> 00:44:02,560
practical but to be honest we do so last like year and half now has been

562
00:44:02,560 --> 00:44:07,840
truly the most rewarding part of my entire life literally in terms of

563
00:44:07,840 --> 00:44:12,320
just seeing firsthand how far you can go then you have no other choice but to

564
00:44:12,320 --> 00:44:16,960
innovate but to just keep working at what you need to and

565
00:44:16,960 --> 00:44:21,280
the truth is although you're very small and which is the nature of a lot of

566
00:44:21,280 --> 00:44:26,000
startups anyways there's so much you can do if you really

567
00:44:26,000 --> 00:44:30,560
you know gets scrappy and like know where to spend your time and

568
00:44:30,560 --> 00:44:34,320
effort so we have this thing and at very need that I don't know if you know

569
00:44:34,320 --> 00:44:37,760
what I'm gonna just mention so there used to be this meme going on

570
00:44:37,760 --> 00:44:42,000
that was the sketching of spider-man that was done in 10 seconds versus

571
00:44:42,000 --> 00:44:47,440
the sketching of it in 10 minutes by an actual artist so even if you are

572
00:44:47,440 --> 00:44:51,520
the master of your skill being art you can always produce something in

573
00:44:51,520 --> 00:44:55,520
10 seconds and you should be always able to to do it and then there is a

574
00:44:55,520 --> 00:44:58,640
10 minute version of course that would be your best work so that's the

575
00:44:58,640 --> 00:45:02,160
person we take at Vernon for anything and we say this to all of our

576
00:45:02,160 --> 00:45:06,480
employees and we even said it to all of our interns

577
00:45:06,480 --> 00:45:09,920
that look you have you don't have 10 minutes you have 10 seconds you have to

578
00:45:09,920 --> 00:45:14,400
have a first version of this go go for it you you've done it on the

579
00:45:14,400 --> 00:45:18,880
design side as well like we have an exceptional designer and then she

580
00:45:18,880 --> 00:45:22,560
started we were like you have two weeks you have to come up with a full

581
00:45:22,560 --> 00:45:27,520
pledge okay UI UX and just go for it and she really did it and this is a test

582
00:45:27,520 --> 00:45:32,080
we do even when we are doing interviews so anyways we we're not working on

583
00:45:32,080 --> 00:45:37,360
everything and anything at once a lot of what I outline or honor and in our kind

584
00:45:37,360 --> 00:45:41,920
of a longer term planning but I think we've gotten really good at

585
00:45:41,920 --> 00:45:47,520
trying to to do a 10 second version of everything just to to know that we

586
00:45:47,520 --> 00:45:52,000
have something in place you know if you were advising

587
00:45:52,000 --> 00:45:56,880
you know someone who was thinking about starting a deep tech type of

588
00:45:56,880 --> 00:46:01,360
start-up what are kind of the general principles that you would

589
00:46:01,360 --> 00:46:10,320
suggest or or put forth for translating you know going from this

590
00:46:10,320 --> 00:46:15,840
kind of gap in the research to product like what are the general ideas

591
00:46:15,840 --> 00:46:21,200
there to me honestly so there there there were so many reasons why I wanted

592
00:46:21,200 --> 00:46:25,040
to embed myself in this start-up scene as opposed to like other maybe

593
00:46:25,040 --> 00:46:30,640
industry research labs or even academia and I think the main driver

594
00:46:30,640 --> 00:46:34,640
is and should be the fact that you want to build an AI

595
00:46:34,640 --> 00:46:39,680
system that actually works so I think for anyone who is working on a

596
00:46:39,680 --> 00:46:44,160
fundamental like a research at a fundamental research setting

597
00:46:44,160 --> 00:46:47,680
they can see what kind of thing they're really passionate about and see how

598
00:46:47,680 --> 00:46:53,040
far it has gone in like you know lab settings and see if it makes sense for it

599
00:46:53,040 --> 00:46:58,160
to want to to be basically in an actual product so I don't know if I have

600
00:46:58,160 --> 00:47:03,040
necessarily principles but I can just talk a little bit more about

601
00:47:03,040 --> 00:47:07,040
my own frustration as to the progress that I couldn't make

602
00:47:07,040 --> 00:47:13,120
outside of this this kind of start-up world that may resonate with someone

603
00:47:13,120 --> 00:47:18,320
out there that we might see it themselves as well. I would say that so I

604
00:47:18,320 --> 00:47:22,800
spend about like a year and a half or so back when I was in grad school

605
00:47:22,800 --> 00:47:29,120
and industry research labs and to me at the end of the day

606
00:47:29,120 --> 00:47:34,080
publishing for the sake of publishing was not really satisfactory so as much

607
00:47:34,080 --> 00:47:38,160
as the we are all seeing the fruits of academic like scientific research

608
00:47:38,160 --> 00:47:42,160
right then sharing which we will you know do at Bernic as well as part of our

609
00:47:42,160 --> 00:47:46,000
I think duty right as researchers to contribute back

610
00:47:46,000 --> 00:47:51,440
like having it as the the main thing that drives you the main thing that you

611
00:47:51,440 --> 00:47:54,000
have to report on the main thing that you care about

612
00:47:54,000 --> 00:47:58,240
is very counterproductive to me right and I feel like okay I'm spending all of

613
00:47:58,240 --> 00:48:03,360
this time of mine on basically this this line of work

614
00:48:03,360 --> 00:48:06,560
where is it going right what kind of value am I bringing to the

615
00:48:06,560 --> 00:48:09,840
broader world so that's like one of the reasons I

616
00:48:09,840 --> 00:48:13,200
felt like personally the kind of research I was doing for the sake of

617
00:48:13,200 --> 00:48:17,760
publishing is not really a good like bar to have for my life

618
00:48:17,760 --> 00:48:21,920
I want to do it for the sake of making a progress but if you had something

619
00:48:21,920 --> 00:48:24,880
worthy of saying you should say it I think it's

620
00:48:24,880 --> 00:48:28,640
our you know duty as I mentioned to to contribute back

621
00:48:28,640 --> 00:48:34,000
so that's an argument maybe for having a product to focus

622
00:48:34,000 --> 00:48:39,680
your research and I think maybe put differently from principles

623
00:48:39,680 --> 00:48:43,840
trying to get at the kernel of like you have so much that you could possibly

624
00:48:43,840 --> 00:48:47,680
research like just how do you prioritize how do you

625
00:48:47,680 --> 00:48:52,160
lead or focus a research team that could go in lots of

626
00:48:52,160 --> 00:48:56,240
different directions like you can't be Bell Labs and just

627
00:48:56,240 --> 00:48:58,880
do a little bit of everything and kind of have it

628
00:48:58,880 --> 00:49:02,720
well I guess that's the the example that you were just talking about you know

629
00:49:02,720 --> 00:49:07,040
with a product in mind like how do you prioritize

630
00:49:07,040 --> 00:49:11,520
what you're going to spend your time on so I think actually so

631
00:49:11,520 --> 00:49:14,800
we should talk in a few months when we have the actual product and then I would

632
00:49:14,800 --> 00:49:18,720
love to be more specific about how we did that because I think that

633
00:49:18,720 --> 00:49:22,800
answers a lot of these questions so the truth is honestly having a

634
00:49:22,800 --> 00:49:26,080
product helps with narrowing down the focus so much we are

635
00:49:26,080 --> 00:49:30,720
we are a truly laser focus right now on a particular domain

636
00:49:30,720 --> 00:49:34,880
which drives a lot of our decisions and gives us a lot of insights as to

637
00:49:34,880 --> 00:49:39,280
what to prioritize and what not to so just talking about the issues that I

638
00:49:39,280 --> 00:49:43,440
was outlining that we have with the existing you know models which kind of

639
00:49:43,440 --> 00:49:46,720
applies to the internal and house models that we have as well

640
00:49:46,720 --> 00:49:51,600
like lack of control lack of transparency like not being able to for them to

641
00:49:51,600 --> 00:49:55,680
work fast enough right because your user has a certain level of

642
00:49:55,680 --> 00:49:58,800
expectation as to how quickly they should respond this completely

643
00:49:58,800 --> 00:50:02,640
change when you go to a new domain so that just dictates our

644
00:50:02,640 --> 00:50:08,960
basically priorities it's like we it's so funny about like

645
00:50:08,960 --> 00:50:12,400
year and a half ago or so when Omid and I we were sitting down to

646
00:50:12,400 --> 00:50:16,800
just outline these features that we want our AI platform to have

647
00:50:16,800 --> 00:50:20,400
we literally wrote down the list like it should be instantaneous in terms of

648
00:50:20,400 --> 00:50:23,840
response time we should have controllability it should be I don't know first

649
00:50:23,840 --> 00:50:27,840
start being like single turn and then multi turn and then it should be

650
00:50:27,840 --> 00:50:32,000
domain general from the get go blah blah and we literally on the notion

651
00:50:32,000 --> 00:50:34,560
page which is the software we use for knowledge

652
00:50:34,560 --> 00:50:38,880
management we have a priority assigned to them which is like high media

653
00:50:38,880 --> 00:50:43,600
blah and then it changes per domain so if I could share that with you that

654
00:50:43,600 --> 00:50:46,720
notion page you would see that we are literally doing it like the

655
00:50:46,720 --> 00:50:50,320
features that even we care about in terms of the

656
00:50:50,320 --> 00:50:55,440
the technology itself is very much dependent on the domain and then we keep

657
00:50:55,440 --> 00:50:59,200
going back and forth on them depending of what we're focusing on

658
00:50:59,200 --> 00:51:03,600
for a particular basically quarter got it got it got it

659
00:51:03,600 --> 00:51:10,640
so so summary there is you know if you're very focused on kind of product and

660
00:51:10,640 --> 00:51:14,640
features then that will tell you what you need to figure out to

661
00:51:14,640 --> 00:51:19,680
deliver those and in order to tell me more detail you'd have to talk about the

662
00:51:19,680 --> 00:51:25,120
specific features and research and that's going to come soon

663
00:51:25,120 --> 00:51:29,120
yes yeah because I think actually you would really like it

664
00:51:29,120 --> 00:51:32,960
for us we had this dilemma of what our first kind of

665
00:51:32,960 --> 00:51:36,800
sector would be first domain would be and probably will be even couple when we

666
00:51:36,800 --> 00:51:41,600
come up with it or regardless the the thing that is the closest to our

667
00:51:41,600 --> 00:51:45,440
hearts is the one that has a lot to do with some of the things that we were

668
00:51:45,440 --> 00:51:48,880
just discussing and it really applies to everyone's day-to-day

669
00:51:48,880 --> 00:51:52,880
decision-making and it's really exciting so that's another thing there's like

670
00:51:52,880 --> 00:51:57,280
this kind of hidden decision like a feature right when you're

671
00:51:57,280 --> 00:52:01,680
prioritizing this gigantic space in this gigantic space which has to do

672
00:52:01,680 --> 00:52:06,400
with your personal passion it really is right when you're especially

673
00:52:06,400 --> 00:52:11,680
so people call this product market fit but I think in our more or less

674
00:52:11,680 --> 00:52:17,120
called technology market fit that like founders like us you have a

675
00:52:17,120 --> 00:52:22,240
particular technology that could be fitted into any market but in any product

676
00:52:22,240 --> 00:52:26,080
and now you have this dilemma of even thinking about the technology being

677
00:52:26,080 --> 00:52:29,360
fitted into market not just to the product being fitted into markets it's

678
00:52:29,360 --> 00:52:35,920
really even a larger search of space for a founder to want to to navigate

679
00:52:35,920 --> 00:52:39,840
but yeah I think that personal passion and personal care is something that

680
00:52:39,840 --> 00:52:43,520
will play a role and has played a role then you know hopefully a couple

681
00:52:43,520 --> 00:52:48,960
months from now and be chattel happily spill all the beans

682
00:52:48,960 --> 00:52:54,320
awesome awesome well Nestreen it was wonderful catching up with you as always

683
00:52:54,320 --> 00:52:58,400
thanks so much for taking time and looking forward to next time

684
00:52:58,400 --> 00:53:02,560
absolutely pleasure with mine yeah looking forward to next time

685
00:53:02,560 --> 00:53:30,720
awesome thank you

