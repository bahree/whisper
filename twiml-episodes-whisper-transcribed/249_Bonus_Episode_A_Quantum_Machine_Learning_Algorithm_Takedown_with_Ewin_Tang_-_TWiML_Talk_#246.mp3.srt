1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,200
I'm your host Sam Charrington.

4
00:00:31,200 --> 00:00:36,120
Alright gang, in this special bonus episode of the podcast, I'm joined by Ewin Tang, a

5
00:00:36,120 --> 00:00:41,320
PhD student in the theoretical computer science group at the University of Washington.

6
00:00:41,320 --> 00:00:46,000
In our conversation, Ewin and I dig into her paper, A Quantum Inspired Classical Algorithm

7
00:00:46,000 --> 00:00:52,160
for Recommendation Systems, which took the Quantum Computing community by storm last summer.

8
00:00:52,160 --> 00:00:57,360
Now we haven't called out a nerd alert interview in quite a long time, but this interview inspired

9
00:00:57,360 --> 00:01:01,440
us to dust off that designation, so get your notepad ready.

10
00:01:01,440 --> 00:01:03,440
And now on to the show.

11
00:01:03,440 --> 00:01:09,360
Alright everyone, I am on the line with Ewin Tang, Ewin is a PhD student at the University

12
00:01:09,360 --> 00:01:12,960
of Washington in the theoretical computer science group.

13
00:01:12,960 --> 00:01:15,840
Ewin, welcome to this week in machine learning and AI.

14
00:01:15,840 --> 00:01:17,760
Hi thanks, thanks for having me.

15
00:01:17,760 --> 00:01:21,360
I'm really glad we finally got a chance to connect.

16
00:01:21,360 --> 00:01:30,360
I think I first reached out to you back in the summer when you came on my radar for your,

17
00:01:30,360 --> 00:01:37,240
I think it was your undergrad thesis around, was this like July or so?

18
00:01:37,240 --> 00:01:40,320
Yeah, that sounds about right, yeah.

19
00:01:40,320 --> 00:01:45,400
So I'm not going to explain it, I'm going to let you explain it, but before we do that,

20
00:01:45,400 --> 00:01:52,200
tell us a little bit about your background, what you're up to now, and how your interests

21
00:01:52,200 --> 00:01:56,480
in machine learning evolved.

22
00:01:56,480 --> 00:01:58,800
Right now I'm like a first year grad student.

23
00:01:58,800 --> 00:02:06,000
So last year, and the four years before that I was at the UT Austin University of Texas

24
00:02:06,000 --> 00:02:07,800
at Austin.

25
00:02:07,800 --> 00:02:15,440
And initially actually starting my undergrad, I was a math major, so I was like interested

26
00:02:15,440 --> 00:02:24,000
in doing some more much more theoretical stuff, but then I sort of realized how interesting

27
00:02:24,000 --> 00:02:28,960
computer science was and how a lot of it has a lot more tangible applications that seem

28
00:02:28,960 --> 00:02:32,080
really interesting.

29
00:02:32,080 --> 00:02:39,680
And that's what led me to take Scott Erinzen's course in quantum information theory, so

30
00:02:39,680 --> 00:02:43,360
quantum computing type stuff.

31
00:02:43,360 --> 00:02:48,400
And from there, I kind of just stumbled into the field of quantum machine learning.

32
00:02:48,400 --> 00:02:54,920
So I've done a little bit of research in like a machine learning type things or sort

33
00:02:54,920 --> 00:02:59,880
of like the theoretical side, but this sort of work within quantum machine learning was

34
00:02:59,880 --> 00:03:05,480
just brought up because I wanted to do a thesis and undergrad thesis advised by him, and

35
00:03:05,480 --> 00:03:08,240
then that was a problem that he suggested.

36
00:03:08,240 --> 00:03:14,040
And so that basically was, he sort of phrased it to me as like a problem of actually solving

37
00:03:14,040 --> 00:03:16,000
a, like a lower bound.

38
00:03:16,000 --> 00:03:20,920
So technically it, it wouldn't have been like machine learning per se if I actually got

39
00:03:20,920 --> 00:03:22,040
that result.

40
00:03:22,040 --> 00:03:27,040
But since the result I got was now algorithm, it's now I guess kind of machine learning.

41
00:03:27,040 --> 00:03:29,960
But yeah, there is at least started my interest in the field.

42
00:03:29,960 --> 00:03:33,920
I'm working on that a little bit here at UW, and I'm also working on some other things

43
00:03:33,920 --> 00:03:36,320
with my advisor, James Lee.

44
00:03:36,320 --> 00:03:44,000
And so the headlines I remember from back in the summer were like this major kind of

45
00:03:44,000 --> 00:03:51,160
quantum computing advance, which were, which I took to be that there were these this

46
00:03:51,160 --> 00:03:57,080
class of machine learning quantum machine learning algorithms that were to a large degree

47
00:03:57,080 --> 00:04:04,040
considered to be a big part of the justification for quantum computing, at least by some people.

48
00:04:04,040 --> 00:04:10,480
And your thesis basically showed how well you didn't really need quantum computers to

49
00:04:10,480 --> 00:04:11,480
do this.

50
00:04:11,480 --> 00:04:17,760
Is that, you know, how, how close is that to describing the thesis and can you kind of put it

51
00:04:17,760 --> 00:04:21,120
in your own words and maybe give us an overview of it?

52
00:04:21,120 --> 00:04:23,880
Yeah, that sounds, that sounds actually pretty accurate.

53
00:04:23,880 --> 00:04:32,000
So the general idea of my thesis is that there's this field of algorithms called like quantum

54
00:04:32,000 --> 00:04:35,520
algorithms, you know, that do that solve machine learning tasks.

55
00:04:35,520 --> 00:04:40,600
And usually this is like very basic tasks like linear algebra type things.

56
00:04:40,600 --> 00:04:49,320
And the reason people were so excited about them when they were like when they were released

57
00:04:49,320 --> 00:04:56,640
is that they produce exponential speed ups for these linear algebra problems.

58
00:04:56,640 --> 00:05:03,800
So basically, if you're familiar with quantum computing, the sort of the highest, the best

59
00:05:03,800 --> 00:05:06,720
type of speed up that you can get is an exponential speed up.

60
00:05:06,720 --> 00:05:13,360
And that's, for example, what we believe occurs for like factoring and like shows algorithm

61
00:05:13,360 --> 00:05:18,880
and the problems that quantum computing is known for solving.

62
00:05:18,880 --> 00:05:22,800
And so an exponential speed up would be great.

63
00:05:22,800 --> 00:05:30,280
But it kind of has some caveats in the sense that it, these algorithms assume some very

64
00:05:30,280 --> 00:05:33,600
strong assumptions about their input.

65
00:05:33,600 --> 00:05:42,840
Actually they say, okay, if you give me all of the data in a quantum state, so that means

66
00:05:42,840 --> 00:05:51,600
like if you give me everything in like superposition basically, then I can do it exponentially fast.

67
00:05:51,600 --> 00:05:56,080
Then I can, you know, do your machine learning incredibly fast.

68
00:05:56,080 --> 00:06:01,120
However, doing this is actually creating these quantum states is actually a hard task

69
00:06:01,120 --> 00:06:07,080
computationally. And so people didn't really were kind of skeptical about whether you could

70
00:06:07,080 --> 00:06:13,480
actually implement these things, you know, people weren't sure whether you could, whether

71
00:06:13,480 --> 00:06:18,880
even when the subroutines, when these like linear algebra routines are fast, that just

72
00:06:18,880 --> 00:06:22,480
getting to that point would take so much time that it wouldn't have been worth the effort

73
00:06:22,480 --> 00:06:23,480
to begin with.

74
00:06:23,480 --> 00:06:28,800
So that's the sort of conflict that's sort of motivating the work.

75
00:06:28,800 --> 00:06:34,920
So this, my thesis was about a particular algorithm.

76
00:06:34,920 --> 00:06:40,880
It's an algorithm by Jordan Karen Edison, a new form per cache called quantum recommendation

77
00:06:40,880 --> 00:06:42,600
systems.

78
00:06:42,600 --> 00:06:47,480
And it purports to like give an exponential speed up for the problem of like recommending

79
00:06:47,480 --> 00:06:52,920
products to users like, like what Netflix or Amazon do.

80
00:06:52,920 --> 00:06:58,560
And the interesting thing about that algorithm is that it actually has, it actually has

81
00:06:58,560 --> 00:07:00,400
relatively few assumptions.

82
00:07:00,400 --> 00:07:08,840
So it doesn't assume that you, that you, that you can just implement quantum states out

83
00:07:08,840 --> 00:07:09,840
of thin air.

84
00:07:09,840 --> 00:07:13,760
It gives like a protocol, it gives like a data structure for you to construct these

85
00:07:13,760 --> 00:07:14,760
things.

86
00:07:14,760 --> 00:07:20,000
And further, a lot of the previous quantum algorithms were sort of trying to solve problems

87
00:07:20,000 --> 00:07:24,120
that weren't being studied classically.

88
00:07:24,120 --> 00:07:30,000
So these previous quantum algorithms were just constructing problems that they believed

89
00:07:30,000 --> 00:07:35,680
could be solved faster with quantum computing, and then they justify their usefulness.

90
00:07:35,680 --> 00:07:39,720
But the nice thing about this algorithm is that it had been studied previously, and then

91
00:07:39,720 --> 00:07:47,120
people had not been able to get even close to as fast as what the quantum algorithm achieved.

92
00:07:47,120 --> 00:07:53,880
So basically this recommendation system algorithm is, was one of the best examples of quantum

93
00:07:53,880 --> 00:07:55,440
machine learning that we had.

94
00:07:55,440 --> 00:08:00,640
And it had an exponential, it was exponentially faster than all previous classical algorithms

95
00:08:00,640 --> 00:08:02,040
that we knew of.

96
00:08:02,040 --> 00:08:10,080
Is this recommendation system or algorithm that they proposed is it based on matrix factorization

97
00:08:10,080 --> 00:08:17,360
and is that why did they kind of inherit the exponential speed up of some matrix factorization

98
00:08:17,360 --> 00:08:22,040
algorithms, or is there something kind of fundamental to their approach that gave them

99
00:08:22,040 --> 00:08:24,280
that exponential speed up?

100
00:08:24,280 --> 00:08:25,280
Yeah, yeah.

101
00:08:25,280 --> 00:08:27,480
So it's basically like a matrix factorization.

102
00:08:27,480 --> 00:08:28,480
Yeah.

103
00:08:28,480 --> 00:08:37,080
It's essentially what happens is that you want, so your recommendation matrix is some

104
00:08:37,080 --> 00:08:43,320
like low rank matrix, basically that just the implication is that people base their preferences

105
00:08:43,320 --> 00:08:46,360
on some small number of attributes that something has.

106
00:08:46,360 --> 00:08:52,000
So maybe you judge an Amazon product by its popularity, its price, and then the

107
00:08:52,000 --> 00:08:54,280
number of stars or something.

108
00:08:54,280 --> 00:09:01,080
And so you have relatively few attributes which tends to lead to low rank matrices.

109
00:09:01,080 --> 00:09:08,080
And the general idea is that you want to factor this matrix, like compute the SVD, and

110
00:09:08,080 --> 00:09:12,960
this will separate your users into user classes, basically.

111
00:09:12,960 --> 00:09:18,680
So you can say, okay, these are like prototypical users, and then these are sort of recommendations

112
00:09:18,680 --> 00:09:20,560
for prototypical users.

113
00:09:20,560 --> 00:09:25,640
And thereby, if you give me a user, I can look at it and say, how similar is it to these

114
00:09:25,640 --> 00:09:29,360
users that I already know the recommendations for, and then compute the recommendations

115
00:09:29,360 --> 00:09:30,360
accordingly.

116
00:09:30,360 --> 00:09:34,600
And so this is basically what the quantum recommendation system does.

117
00:09:34,600 --> 00:09:41,800
It does all of this in like the quantum superposition world, like it doesn't actually explicitly write

118
00:09:41,800 --> 00:09:47,200
down any of these things, but it's able to do that without writing anything down, and

119
00:09:47,200 --> 00:09:51,480
so it can do it a lot faster than it would be to write anything down.

120
00:09:51,480 --> 00:09:55,920
So I'm curious, maybe we can hit pause on the thesis conversation.

121
00:09:55,920 --> 00:10:04,640
I had several interviews about quantum stuff, and to be honest, I don't really understand

122
00:10:04,640 --> 00:10:07,280
it still.

123
00:10:07,280 --> 00:10:10,680
And so that's probably the case for at least a few people in the audience as well.

124
00:10:10,680 --> 00:10:17,040
I'm curious, in your words, how do you think of this whole quantum computing stuff?

125
00:10:17,040 --> 00:10:19,920
How do you explain it to people?

126
00:10:19,920 --> 00:10:26,680
It's kind of hard because I'm not really that good at quantum computing intuition myself.

127
00:10:26,680 --> 00:10:35,040
So I think the way that I like to think about it at least when I'm like, at least the way

128
00:10:35,040 --> 00:10:43,120
that it's relevant to my research is that it's useful to think of quantum superposition.

129
00:10:43,120 --> 00:10:49,520
If you think of like saying a quantum superposition of a state, usually you can like pretty well

130
00:10:49,520 --> 00:10:55,560
approximate the meaning by replacing like quantum superposition with probability distribution.

131
00:10:55,560 --> 00:11:04,640
So like generally what I like to do is whenever I hear about like quantum computing is just

132
00:11:04,640 --> 00:11:10,920
about setting up these probability distributions in the right way, and these probability distributions

133
00:11:10,920 --> 00:11:16,120
are souped up so that you can have these like interference.

134
00:11:16,120 --> 00:11:19,480
So you can have probabilities cancel each other out.

135
00:11:19,480 --> 00:11:20,480
Yeah.

136
00:11:20,480 --> 00:11:28,400
And basically it seems like a lot of the like exponential type speed ups especially come

137
00:11:28,400 --> 00:11:31,760
from this type of like way of thinking about it.

138
00:11:31,760 --> 00:11:37,600
So roughly we can think of a computer that's made up of not buses and cells and gates

139
00:11:37,600 --> 00:11:43,040
that deal with ones and zeros but rather those similar kinds of things that deal with probability

140
00:11:43,040 --> 00:11:48,080
distributions and weird ones that interact in weird ways.

141
00:11:48,080 --> 00:11:49,080
Right.

142
00:11:49,080 --> 00:11:51,080
So yeah.

143
00:11:51,080 --> 00:12:01,600
If you think about like like, yeah, so it's useful a lot to think of like, okay, like a probability

144
00:12:01,600 --> 00:12:08,840
like a like a quantum superposition of zero and one is essentially just like a probability

145
00:12:08,840 --> 00:12:15,120
distribution like a coin flip picking zero or one each half the time.

146
00:12:15,120 --> 00:12:22,200
And obviously this fails sometimes because whenever you want to like do some cancellation,

147
00:12:22,200 --> 00:12:27,640
you might have a situation where you know technically it's okay, so you have a one

148
00:12:27,640 --> 00:12:33,200
fourth probability of one thing and then you add that to one quarter probability of the

149
00:12:33,200 --> 00:12:37,160
same thing and then you get zero somehow.

150
00:12:37,160 --> 00:12:47,400
But a lot of the techniques that are used in quantum in quantum computing are basically

151
00:12:47,400 --> 00:12:53,480
similar to what similar to their like classical analogs similar to what you would do to manipulate

152
00:12:53,480 --> 00:12:55,480
probability distributions.

153
00:12:55,480 --> 00:13:04,400
And so then back to your thesis, you, there's this recommendation algorithm based on matrix

154
00:13:04,400 --> 00:13:14,440
factorization and you started out by trying to identify what initially we believe that

155
00:13:14,440 --> 00:13:24,200
the the quantum algorithm that they provided that the quantum recommendation system algorithm.

156
00:13:24,200 --> 00:13:28,800
You could prove that it was exponentially faster than any classical algorithm.

157
00:13:28,800 --> 00:13:33,560
And so what this would do is basically say show, hey, we found an example where quantum

158
00:13:33,560 --> 00:13:37,360
can help exponentially in machine learning.

159
00:13:37,360 --> 00:13:41,520
And so this is like a, this is something that people had already believed.

160
00:13:41,520 --> 00:13:49,440
So there was an existing proof that this algorithm couldn't be done in classical computers.

161
00:13:49,440 --> 00:13:50,680
We wanted to show such a proof.

162
00:13:50,680 --> 00:13:51,680
Okay.

163
00:13:51,680 --> 00:13:52,680
Got it.

164
00:13:52,680 --> 00:13:59,640
We wanted to actually show that you could, you could not do the same thing classically.

165
00:13:59,640 --> 00:14:04,240
And people sort of already believed this because of how strong the results looked.

166
00:14:04,240 --> 00:14:08,760
And so this is, this would have been like the, I guess the final nail in the coffin.

167
00:14:08,760 --> 00:14:14,680
But the thing is that, well, I worked on it for a while and I kind of got nowhere.

168
00:14:14,680 --> 00:14:17,720
So like, this is for my undergrad thesis.

169
00:14:17,720 --> 00:14:22,800
I guess I had like, you know, nine ten months to dissolve it and like halfway through I had,

170
00:14:22,800 --> 00:14:24,800
you know, nothing.

171
00:14:24,800 --> 00:14:32,120
I didn't have any approach to allow around the things that I tried didn't seem to work.

172
00:14:32,120 --> 00:14:39,920
And gradually I came to realize that the reason the strategies didn't work was because

173
00:14:39,920 --> 00:14:44,760
the recommendation algorithm can actually be mimicked with a classical algorithm.

174
00:14:44,760 --> 00:14:46,920
What was it that told you that?

175
00:14:46,920 --> 00:14:49,520
There are a couple of factors that led me to realize this.

176
00:14:49,520 --> 00:14:56,760
So, so the first factor was basically that whenever I tried to prove some sort of lower

177
00:14:56,760 --> 00:15:02,240
bound, tried to prove that classical algorithms couldn't do the problem, I would run into

178
00:15:02,240 --> 00:15:04,240
some sort of roadblock.

179
00:15:04,240 --> 00:15:10,520
And then in analyzing what these roadblocks were further, you could basically, I could

180
00:15:10,520 --> 00:15:17,200
basically sort of figure out what were the parts that seemed that the classical algorithm

181
00:15:17,200 --> 00:15:18,200
could do.

182
00:15:18,200 --> 00:15:21,160
So, so it's kind of like a push and pull.

183
00:15:21,160 --> 00:15:25,520
If you want to design a classical algorithm, you have to try to do it without hitting any

184
00:15:25,520 --> 00:15:32,160
roadblocks that because the problems that you come up with are hard, whereas if you're

185
00:15:32,160 --> 00:15:36,600
trying to prove a lower bound, you're trying to show hardness, the roadblocks are actually

186
00:15:36,600 --> 00:15:42,320
things that the, that the, that a classical algorithm would find easy.

187
00:15:42,320 --> 00:15:49,840
Essentially, all of the roadblocks that I found were basically, were indications for

188
00:15:49,840 --> 00:15:54,920
what could be done for a, and a classical algorithm.

189
00:15:54,920 --> 00:16:00,440
And so, so that was one factor, basically just that research into lower bounds gives

190
00:16:00,440 --> 00:16:05,240
you some understanding of like how, where the hardness of the problem is and how you

191
00:16:05,240 --> 00:16:07,720
could avoid it.

192
00:16:07,720 --> 00:16:14,120
And the other factor was I, I early into the research I found this paper by says Alan

193
00:16:14,120 --> 00:16:20,440
Fries, Robbie canon and Santosh Vampala, it's a machine learning type algorithm, basically

194
00:16:20,440 --> 00:16:27,800
it does low rank matrix factorization and it does it in time independent of the input

195
00:16:27,800 --> 00:16:28,800
size.

196
00:16:28,800 --> 00:16:33,800
So, basically what that means is like no matter, no matter the dimension, it can do it

197
00:16:33,800 --> 00:16:36,520
just as fast.

198
00:16:36,520 --> 00:16:42,560
And it does this with some, some assumptions, so it is, namely it assumes that you can

199
00:16:42,560 --> 00:16:48,480
sample somehow from the entries of this matrix, this input matrix.

200
00:16:48,480 --> 00:16:54,000
And the interesting thing is that these input assumptions, these sampling assumptions that

201
00:16:54,000 --> 00:16:59,520
are crucial for the algorithm were also present for the recommendation system, the recommendation

202
00:16:59,520 --> 00:17:00,520
system model.

203
00:17:00,520 --> 00:17:07,840
So you recall that we don't get the matrix just to like read from like we don't get like,

204
00:17:07,840 --> 00:17:16,120
we don't just get like RAM access memory to a random access memory to the matrix, the

205
00:17:16,120 --> 00:17:19,720
matrix that contains like user product preferences.

206
00:17:19,720 --> 00:17:22,960
We also get it in a data structure.

207
00:17:22,960 --> 00:17:29,760
So in order for the quantum algorithm to be able to prepare the superposition that

208
00:17:29,760 --> 00:17:33,520
it wants, it designs a data structure.

209
00:17:33,520 --> 00:17:38,400
And this data structure classically also allows for you to perform the exact type of samples

210
00:17:38,400 --> 00:17:42,840
that you would need to do this fast classical algorithm.

211
00:17:42,840 --> 00:17:50,720
Now just from this algorithm, it's not obvious at all what really to do with it.

212
00:17:50,720 --> 00:17:56,520
It kind of shows that you could do it and then you could factor the matrix and stops there

213
00:17:56,520 --> 00:18:02,520
when in fact for the recommendation system problem, you need to do something with those,

214
00:18:02,520 --> 00:18:05,360
do something with those vectors that you find.

215
00:18:05,360 --> 00:18:10,360
And it's not obvious how you would do that, but it was an indication that said, okay,

216
00:18:10,360 --> 00:18:13,360
you can do some things in this model.

217
00:18:13,360 --> 00:18:17,280
Some of the tasks that you might have thought were hard classically.

218
00:18:17,280 --> 00:18:20,360
You might need to rethink those, those problems.

219
00:18:20,360 --> 00:18:29,080
So a big part of your thesis was coming up with this classical algorithm that was analogous

220
00:18:29,080 --> 00:18:32,360
to this quantum algorithm.

221
00:18:32,360 --> 00:18:40,000
You mentioned that you'd found some prior work that had similar constraints as the algorithm

222
00:18:40,000 --> 00:18:44,200
did in the quantum side.

223
00:18:44,200 --> 00:18:50,600
What did you need to do to get from the prior work to the algorithm that corresponded to

224
00:18:50,600 --> 00:18:52,120
the quantum side?

225
00:18:52,120 --> 00:18:53,920
So it's kind of interesting.

226
00:18:53,920 --> 00:18:59,760
I wasn't familiar with any of this, so this is like in the area of the radical recommendation

227
00:18:59,760 --> 00:19:00,760
system literature.

228
00:19:00,760 --> 00:19:05,520
There's this set of papers that basically talk about how to like theoretically model

229
00:19:05,520 --> 00:19:10,040
recommendation systems and do like that sort of analysis.

230
00:19:10,040 --> 00:19:16,720
So they use the name recommendation system instead of like, I think people use collaborative

231
00:19:16,720 --> 00:19:20,800
filtering more recently, is that that's on right?

232
00:19:20,800 --> 00:19:31,080
But yeah, so there were a set of papers that basically in like the early 2000s to late

233
00:19:31,080 --> 00:19:36,360
90s that basically laid out the entire model for recommendation systems.

234
00:19:36,360 --> 00:19:42,200
And this is the model that is used for my algorithm and for the quantum algorithm as

235
00:19:42,200 --> 00:19:43,200
well.

236
00:19:43,200 --> 00:19:47,360
And they actually came up with something.

237
00:19:47,360 --> 00:19:51,560
It's basically the same thing as what Frie's canon and been polited.

238
00:19:51,560 --> 00:19:57,200
They basically found this prior work and then sort of got stuck because what you have

239
00:19:57,200 --> 00:20:03,760
to do is just to lay out the groundwork.

240
00:20:03,760 --> 00:20:10,760
What we want to do is we want to give a sublinear algorithm for this recommendation system

241
00:20:10,760 --> 00:20:11,760
problem.

242
00:20:11,760 --> 00:20:19,160
And because it is a sublinear algorithm, this means that we can't read all of the matrix

243
00:20:19,160 --> 00:20:21,520
in all of the input in.

244
00:20:21,520 --> 00:20:28,600
And so if you imagine your input as being a set of preferences that users have for particular

245
00:20:28,600 --> 00:20:36,160
products, it even means that we can't list out all of the use that we have or list out

246
00:20:36,160 --> 00:20:37,320
all of the products we have.

247
00:20:37,320 --> 00:20:46,280
We don't have enough time to do that because we want to give like a logarithmic time algorithms.

248
00:20:46,280 --> 00:20:51,000
It's the time that we are allotted is so small that we can't do much.

249
00:20:51,000 --> 00:20:56,200
And the problem with that is that we're dealing with like a linear algebra problem.

250
00:20:56,200 --> 00:21:04,760
And so what this prior work outputs is it outputs the singular vectors of the input matrix.

251
00:21:04,760 --> 00:21:09,920
But it can't output the singular vectors because the vectors are the length of the matrix.

252
00:21:09,920 --> 00:21:11,680
And so you can't do that in enough time.

253
00:21:11,680 --> 00:21:12,680
Does that make sense?

254
00:21:12,680 --> 00:21:13,680
I think so.

255
00:21:13,680 --> 00:21:14,680
Yeah.

256
00:21:14,680 --> 00:21:15,680
Yeah.

257
00:21:15,680 --> 00:21:18,440
So you don't have enough time to output the whole vector.

258
00:21:18,440 --> 00:21:23,520
So you have to describe what the vector looks like in, you have to give what they call

259
00:21:23,520 --> 00:21:26,720
a succinct description.

260
00:21:26,720 --> 00:21:32,960
And so what they do is they can't say, here's the vector that we give as output because

261
00:21:32,960 --> 00:21:35,320
to write out all of that down it takes too long.

262
00:21:35,320 --> 00:21:43,880
And so what you say is you say, look at this set of users and then perform some linear

263
00:21:43,880 --> 00:21:53,400
combination or some like you do some vector addition or something on these users.

264
00:21:53,400 --> 00:21:55,720
And that's going to be your singular vector.

265
00:21:55,720 --> 00:22:00,720
So you've described the vector that you want to output based on the vectors that you were

266
00:22:00,720 --> 00:22:03,240
given.

267
00:22:03,240 --> 00:22:09,600
And that way you can avoid the whole process of giving out each entry one by one.

268
00:22:09,600 --> 00:22:15,520
But that's a problem if you actually want to use this result because now you can't

269
00:22:15,520 --> 00:22:16,680
actually read the vector.

270
00:22:16,680 --> 00:22:21,040
You have to deal with all of this stuff beforehand and that stuff takes a lot of time presumably,

271
00:22:21,040 --> 00:22:23,040
right?

272
00:22:23,040 --> 00:22:30,520
And essentially you run into this issue where you really want to write your vectors down

273
00:22:30,520 --> 00:22:34,000
but you can't because you don't have enough time.

274
00:22:34,000 --> 00:22:39,160
And the quantum algorithm sort of foregoes this issue by just having everything in a quantum

275
00:22:39,160 --> 00:22:40,160
state.

276
00:22:40,160 --> 00:22:44,880
And so in like if you have something in like a quantum superposition or something like

277
00:22:44,880 --> 00:22:50,880
that, you don't have to write anything down.

278
00:22:50,880 --> 00:22:56,640
So that was the main issue from going from the prior work to the recommendation system

279
00:22:56,640 --> 00:22:58,120
algorithm.

280
00:22:58,120 --> 00:23:04,280
And so what ended up happening, what I ended up doing is figuring out, okay, you can avoid

281
00:23:04,280 --> 00:23:11,240
this problem by looking at the quantum, like by taking quantum super, super positions.

282
00:23:11,240 --> 00:23:15,440
And so maybe you can actually do the same thing with the distribution.

283
00:23:15,440 --> 00:23:20,840
So you can avoid the problem of writing everything down by using a probability distribution.

284
00:23:20,840 --> 00:23:27,120
And what I mean by that is basically to say the following, using the fact that we can sample

285
00:23:27,120 --> 00:23:31,880
high-weight entries of basically anything that we want about the input matrix.

286
00:23:31,880 --> 00:23:41,040
So you know, basically I can get the user product preferences that matter most.

287
00:23:41,040 --> 00:23:52,880
I can sample randomly, so I can just pick a random, a bunch of random samples from the matrix,

288
00:23:52,880 --> 00:23:54,840
from the input matrix.

289
00:23:54,840 --> 00:24:00,360
And use that to give a decent approximation of the vectors that I want.

290
00:24:00,360 --> 00:24:05,600
What's interesting is that basically you can do this through the entire process.

291
00:24:05,600 --> 00:24:14,360
So basically once you have this succinct description, you can actually use that for sampling.

292
00:24:14,360 --> 00:24:18,720
So even though I haven't written anything down about the vector, I can actually still

293
00:24:18,720 --> 00:24:20,520
sample from it.

294
00:24:20,520 --> 00:24:23,360
It's basically not obvious, but you can actually do it.

295
00:24:23,360 --> 00:24:26,960
And so once you have these samples, you can sample from the high-weight entries of these

296
00:24:26,960 --> 00:24:27,960
vectors.

297
00:24:27,960 --> 00:24:29,360
You can go the rest of the way.

298
00:24:29,360 --> 00:24:32,000
You can do whatever you want to do.

299
00:24:32,000 --> 00:24:37,000
And for the recommendation system case, basically what you want to do is you want to perform

300
00:24:37,000 --> 00:24:38,320
some projection operation.

301
00:24:38,320 --> 00:24:46,440
You want to project your user vector onto these matrix singular vectors.

302
00:24:46,440 --> 00:24:51,440
And it turns out that you can do that using these sampling strategies.

303
00:24:51,440 --> 00:24:54,360
And that basically gives you the algorithm.

304
00:24:54,360 --> 00:24:56,280
You can do everything without writing anything down.

305
00:24:56,280 --> 00:24:59,360
And so you take poly logarithm at time.

306
00:24:59,360 --> 00:25:02,640
So you take a sublinear amount of time.

307
00:25:02,640 --> 00:25:06,640
The quantum algorithm is, you said invariant with the length.

308
00:25:06,640 --> 00:25:12,480
So it's like big of one kind of time for this recommendation system.

309
00:25:12,480 --> 00:25:16,600
And so you needed to do it in something sublinear.

310
00:25:16,600 --> 00:25:17,600
Right.

311
00:25:17,600 --> 00:25:24,080
So the quantum algorithm is like big of log n, something like that.

312
00:25:24,080 --> 00:25:29,200
So that's like logarithmic time, anything that's like that to some powers, like called

313
00:25:29,200 --> 00:25:30,600
poly logarithmic time.

314
00:25:30,600 --> 00:25:34,280
So what we achieve is we achieve poly logarithmic time.

315
00:25:34,280 --> 00:25:40,120
So we achieve something like log n squared or cubed or something like that.

316
00:25:40,120 --> 00:25:41,120
Okay.

317
00:25:41,120 --> 00:25:45,680
Initially we thought that the quantum algorithm could do log n and the classical algorithm

318
00:25:45,680 --> 00:25:48,520
could do only like O of n.

319
00:25:48,520 --> 00:25:52,320
And so I brought the time down from O of n to O of like log squared n.

320
00:25:52,320 --> 00:25:56,080
So it's like a like exponential improvement.

321
00:25:56,080 --> 00:26:04,560
And is this something that you envision would actually be used or deployed in some recommendation

322
00:26:04,560 --> 00:26:05,560
system?

323
00:26:05,560 --> 00:26:14,840
Or is it more the kind of theoretical implementations on, you know, the what it means for this quantum

324
00:26:14,840 --> 00:26:21,200
algorithm and the theoretical analysis of this classical algorithm?

325
00:26:21,200 --> 00:26:27,000
I mean, I certainly hope it could be useful in the future.

326
00:26:27,000 --> 00:26:34,920
So the thing is like because this algorithm wasn't really intended to be like, it was just

327
00:26:34,920 --> 00:26:40,720
something that I like hacked together and it like because because for my theoretical

328
00:26:40,720 --> 00:26:47,240
purposes, anything that's like even close to the quantum algorithm would have worked.

329
00:26:47,240 --> 00:26:53,600
But I didn't think that hard about trying to optimize everything about the algorithm.

330
00:26:53,600 --> 00:26:56,480
So I'm not sure if it would actually work in practice is something that I still want

331
00:26:56,480 --> 00:26:58,080
to try to figure out.

332
00:26:58,080 --> 00:27:03,280
But I do think that it would be possible for somebody to like clean up the algorithm

333
00:27:03,280 --> 00:27:07,480
and actually get a version that works a lot better and then maybe is competitive with

334
00:27:07,480 --> 00:27:09,000
other techniques.

335
00:27:09,000 --> 00:27:13,280
I think what's done in practice right now is sort of limited by the like the amount of

336
00:27:13,280 --> 00:27:16,480
time that these factorization techniques take.

337
00:27:16,480 --> 00:27:24,920
So I think if you look at like what recommendation systems do in practice, they have to like update

338
00:27:24,920 --> 00:27:25,920
regularly.

339
00:27:25,920 --> 00:27:32,640
So they have to perform the algorithm every week or every, I don't know, I don't know

340
00:27:32,640 --> 00:27:33,640
how often they do it.

341
00:27:33,640 --> 00:27:39,840
But because you have to take so much time out to actually just sit down and compute the

342
00:27:39,840 --> 00:27:44,960
things that you want to compute, it just that if you could speed up that problem, you

343
00:27:44,960 --> 00:27:51,320
could probably help a lot with the flow of the I guess recommendation system.

344
00:27:51,320 --> 00:27:56,760
I guess I guess I'm not the right person to ask about like the classical what it means

345
00:27:56,760 --> 00:28:00,360
for like, you know, practice.

346
00:28:00,360 --> 00:28:08,280
But I can at least say that for the quantum machine learning side of things, it's pretty

347
00:28:08,280 --> 00:28:12,920
interesting that you can actually just simulate all of these things with simulate all of these

348
00:28:12,920 --> 00:28:16,200
quantum algorithms with classical sampling techniques.

349
00:28:16,200 --> 00:28:23,840
And did you a big part of your algorithm is replacing knowing the input vector with the

350
00:28:23,840 --> 00:28:27,320
distribution of the input vector?

351
00:28:27,320 --> 00:28:33,480
Did you characterize at all how that replacement impacts the ultimate result or performance

352
00:28:33,480 --> 00:28:35,240
of the recommendation system?

353
00:28:35,240 --> 00:28:41,840
I guess my thinking is that when you replace the full vector with some distribution, that's

354
00:28:41,840 --> 00:28:49,120
essentially like injecting noise or adding noise to this recommendation system.

355
00:28:49,120 --> 00:28:50,120
Yeah.

356
00:28:50,120 --> 00:29:00,120
And I'm just wondering if you looked at all at what the impact of that noise is on the

357
00:29:00,120 --> 00:29:03,600
performance of the recommendation system theoretically?

358
00:29:03,600 --> 00:29:09,480
Actually, so the noise is quite bad from a theoretical perspective.

359
00:29:09,480 --> 00:29:16,680
So in fact, so the sampling algorithm that I mentioned, the classical sampling algorithm,

360
00:29:16,680 --> 00:29:20,920
the prior work was released in, I believe, 2007.

361
00:29:20,920 --> 00:29:27,160
And since then, there have been a lot of like improvements on this algorithm that basically

362
00:29:27,160 --> 00:29:33,480
say, okay, these sampling assumptions were too weak because they added too much, they

363
00:29:33,480 --> 00:29:38,960
sort of, we can basically do better just by assuming a little bit more.

364
00:29:38,960 --> 00:29:43,440
And so a lot of these more recent algorithms follow the line of, okay, let's assume that

365
00:29:43,440 --> 00:29:50,200
our matrix is a sparser, okay, let's assume that we're able to read our matrix with

366
00:29:50,200 --> 00:29:52,040
some number, some number of times.

367
00:29:52,040 --> 00:29:56,960
So like, we're basically streamed all the matrix entries and then we're supposed to

368
00:29:56,960 --> 00:29:59,360
develop an algorithm out of that.

369
00:29:59,360 --> 00:30:08,920
And so in these models, you can actually do, you can improve your error quite a bit.

370
00:30:08,920 --> 00:30:16,120
Um, it turns out that for the, it turns out that for the recommendation system problem,

371
00:30:16,120 --> 00:30:22,120
a lot of noise is okay, that's also kind of just because we have strong assumptions

372
00:30:22,120 --> 00:30:24,440
for how well, how good our data is.

373
00:30:24,440 --> 00:30:31,960
So even, even way back in like 2002, we had to assume theoretically that our data was

374
00:30:31,960 --> 00:30:35,840
really, really nice in order for us to give any recommendation algorithm.

375
00:30:35,840 --> 00:30:40,640
So we're still kind of operating off of some of these assumptions.

376
00:30:40,640 --> 00:30:46,600
So like if you, if you look at my paper, um, like there are a lot of really weird assumptions

377
00:30:46,600 --> 00:30:50,920
that I have to cover in like the first, like the first 10 pages is to like sort of set

378
00:30:50,920 --> 00:30:54,880
the groundwork and these are things that you could weaken presumably or that you would

379
00:30:54,880 --> 00:30:57,760
have to weaken in practice.

380
00:30:57,760 --> 00:31:00,760
And so I'm not sure how the noise would impact that.

381
00:31:00,760 --> 00:31:04,240
It's not something that it looked at, but it does impact it quite a bit.

382
00:31:04,240 --> 00:31:12,120
Do you have a sense for the ultimate impact on the quantum machine learning kind of community

383
00:31:12,120 --> 00:31:16,280
or our research based on the, the results that you came up with?

384
00:31:16,280 --> 00:31:19,960
So, so there have been a couple of developments since this, uh, this paper actually there.

385
00:31:19,960 --> 00:31:26,280
So since this algorithm was, okay, I call it like de-quantize, basically, I mean, like

386
00:31:26,280 --> 00:31:27,560
you took the quantum out of it, right?

387
00:31:27,560 --> 00:31:30,840
So you, you gave a classical algorithm corresponding to it.

388
00:31:30,840 --> 00:31:36,640
Um, there have been, uh, three more quantum machine learning algorithms that have been

389
00:31:36,640 --> 00:31:41,120
essentially like this, this kind of de-quantized, uh, that have been de-quantized.

390
00:31:41,120 --> 00:31:46,320
So I guess, so I guess to like, um, a bystander looks like quantum machine learning is like

391
00:31:46,320 --> 00:31:47,880
birding to the ground or something.

392
00:31:47,880 --> 00:31:53,760
Like, um, like all of these algorithms are going down one by one, but I think I've talked

393
00:31:53,760 --> 00:31:58,280
to like some quantum machine learning researchers who were like, uh, who basically said, yeah,

394
00:31:58,280 --> 00:32:01,920
um, we didn't really believe that these algorithms were that strong anyway.

395
00:32:01,920 --> 00:32:07,160
So it's, it's good that you're, uh, that you're, um, showing that, you know, like that

396
00:32:07,160 --> 00:32:14,120
these are, um, being de-quantized because, uh, essentially, there is a lot of hype in

397
00:32:14,120 --> 00:32:20,520
quantum machine learning and these types, like, these hyper, hype is spurred by, um, papers

398
00:32:20,520 --> 00:32:22,560
that claim a lot of exponential speedups.

399
00:32:22,560 --> 00:32:30,640
Um, and so, so basically, uh, these classical algorithms, there have been some classical

400
00:32:30,640 --> 00:32:36,880
algorithms that try to say, okay, these exponential speedups seem really hard to achieve in practice

401
00:32:36,880 --> 00:32:40,400
just because these are easy tasks classically anyways.

402
00:32:40,400 --> 00:32:46,360
So you have to, um, basically, you, you should, you, this should be like something stronger

403
00:32:46,360 --> 00:32:52,800
that you can do quantumly or, um, in order to actually get a exponential speedup that's,

404
00:32:52,800 --> 00:33:00,200
uh, I guess, stronger, uh, depending on what your model is, some exponential speedups

405
00:33:00,200 --> 00:33:03,200
are weaker and some are stronger, I guess.

406
00:33:03,200 --> 00:33:08,120
And so we're just, right now, I guess, uh, this, this work is sort of, serves to separate

407
00:33:08,120 --> 00:33:10,600
some of the wheat from the, the shaft, I guess.

408
00:33:10,600 --> 00:33:18,000
Are there currently existing strong use cases for the, these quantum algorithms or, or

409
00:33:18,000 --> 00:33:24,920
strong algorithms, or are they, these, you know, have we, uh, dequantized a bunch of

410
00:33:24,920 --> 00:33:30,720
them and, you know, do we need to now find the, the new strong cases?

411
00:33:30,720 --> 00:33:35,120
I think that question, like, the answer to that question definitely depends on like, it's

412
00:33:35,120 --> 00:33:37,960
like an ongoing topic of discussion.

413
00:33:37,960 --> 00:33:45,000
So I think, um, like, personally, I don't think that there's any quantum of human learning

414
00:33:45,000 --> 00:33:50,680
algorithm that has like a, like a strong theoretical backing for being good, um, for being like,

415
00:33:50,680 --> 00:33:55,240
forgiving exponential speedups, um, but I don't know, maybe that could change depending

416
00:33:55,240 --> 00:33:56,240
on the day.

417
00:33:56,240 --> 00:34:04,160
Uh, there is one algorithm, there, there, there's still hope and that hope is, uh, because

418
00:34:04,160 --> 00:34:11,920
a lot of the algorithms that remain are based on this procedure for matrix inversion.

419
00:34:11,920 --> 00:34:18,040
So you may have heard of this quantum algorithm by Harrow, Hasidim, and Lloyd, that inverts

420
00:34:18,040 --> 00:34:23,880
a, um, a matrix in time poly logarithmic in the dimension.

421
00:34:23,880 --> 00:34:29,760
And this algorithm is actually, it, it's what we call BQP complete, um, so this is like

422
00:34:29,760 --> 00:34:35,680
a complexity class that basically says, if you can de-quantize this algorithm, then you

423
00:34:35,680 --> 00:34:41,640
can de-quantize all of quantum computing, um, and so basically we believe it's hard, uh,

424
00:34:41,640 --> 00:34:46,320
we believe it actually does give exponential speedups in like certain circumstances.

425
00:34:46,320 --> 00:34:52,960
However, the thing is that, uh, the, the situations for which we can devise exponential

426
00:34:52,960 --> 00:34:54,800
speedups are very contrived.

427
00:34:54,800 --> 00:35:01,600
Um, so, you know, if your input data happens to be exactly this matrix, then you can get

428
00:35:01,600 --> 00:35:07,160
an exponential speedup, for example, um, but of course, in practice, you want to be able

429
00:35:07,160 --> 00:35:12,400
to apply this, uh, apply this to just any matrix that you have.

430
00:35:12,400 --> 00:35:18,040
And we know for a fact that, or essentially we, we, we can't do this for any arbitrary

431
00:35:18,040 --> 00:35:19,040
matrix.

432
00:35:19,040 --> 00:35:24,040
We can't get a speedup for any matrix, at least it doesn't seem that way right now.

433
00:35:24,040 --> 00:35:27,280
And so it's kind of like a, like a balancing act.

434
00:35:27,280 --> 00:35:34,640
So you have to try to find like the machine learning problem that's just hard enough to

435
00:35:34,640 --> 00:35:39,360
be out of reach for classical computers, but not too hard so that quantum computers can't

436
00:35:39,360 --> 00:35:40,360
solve them.

437
00:35:40,360 --> 00:35:42,480
So you need to be in like that sweet spot.

438
00:35:42,480 --> 00:35:47,440
And I guess the final thing is that even if we could find such a thing, the work would

439
00:35:47,440 --> 00:35:53,840
still be somewhat speculative because we don't actually know whether we can build quantum

440
00:35:53,840 --> 00:35:54,840
memory.

441
00:35:54,840 --> 00:35:59,360
Uh, so right now we have quantum computers and they can like do circuits and things like

442
00:35:59,360 --> 00:36:00,360
that.

443
00:36:00,360 --> 00:36:05,880
Um, but for a lot of these machine learning algorithms to work, we assume that we have

444
00:36:05,880 --> 00:36:11,720
some really strong, like, I guess, the RAM, they call it quantum RAM, Q RAM.

445
00:36:11,720 --> 00:36:16,440
Uh, we have some really strong RAM that holds all our data and that we can do some like,

446
00:36:16,440 --> 00:36:21,840
we can, we can query to and, um, construct states from quickly.

447
00:36:21,840 --> 00:36:27,120
And so this is, uh, additionally is something that we don't know whether it works in practice.

448
00:36:27,120 --> 00:36:31,560
So, so it works for classical computers, but we're not sure whether it works for quantum

449
00:36:31,560 --> 00:36:32,560
computers.

450
00:36:32,560 --> 00:36:36,280
And this is something that an experimentalist would have to confirm, I guess.

451
00:36:36,280 --> 00:36:40,800
It's all a little bit speculative, but it's, it's interesting to think about all the

452
00:36:40,800 --> 00:36:43,160
different scenarios that could happen, right?

453
00:36:43,160 --> 00:36:47,440
It sounds like it's, uh, you know, considering that you, that so many of these algorithms

454
00:36:47,440 --> 00:36:55,800
have been de-quantized in, uh, just a few months since your paper, uh, was published.

455
00:36:55,800 --> 00:36:59,880
It, it sounds like it's a very fast moving area.

456
00:36:59,880 --> 00:37:05,360
And there's, I didn't realize that the, that when we talk about these quantum algorithms

457
00:37:05,360 --> 00:37:09,240
and quantum computers, that, you know, there's this huge missing piece, which is this quantum

458
00:37:09,240 --> 00:37:10,240
RAM.

459
00:37:10,240 --> 00:37:13,800
I didn't realize that that wasn't part of what we, you know, when we talk about quantum

460
00:37:13,800 --> 00:37:17,600
computers, that that wasn't more of a complete package, but, uh, it sounds like there's

461
00:37:17,600 --> 00:37:20,240
still a ton of work to do in this space.

462
00:37:20,240 --> 00:37:21,240
Right.

463
00:37:21,240 --> 00:37:27,480
It's, it's interesting, actually, like, um, some quantum algorithms don't need memory, if

464
00:37:27,480 --> 00:37:28,480
that makes sense.

465
00:37:28,480 --> 00:37:33,400
So, so, for example, you can, you can, do, you can run short as algorithm without requiring

466
00:37:33,400 --> 00:37:40,080
memory because basically you can, for, for whatever number I give you, you can give

467
00:37:40,080 --> 00:37:44,760
me a circuit or a quantum circuit that basically simulates the RAM.

468
00:37:44,760 --> 00:37:49,320
So, I can construct, you can give me the input and I can construct the RAM quickly.

469
00:37:49,320 --> 00:37:54,400
So, let's say, like, you gave me a number four and you wanted me to construct it.

470
00:37:54,400 --> 00:38:00,080
Well, basically what I can do in the terms of like the, what, what, what the, what we

471
00:38:00,080 --> 00:38:06,240
care about for, like quantum algorithms, what you can do is just devise a circuit that

472
00:38:06,240 --> 00:38:07,560
just outputs four.

473
00:38:07,560 --> 00:38:11,840
It sounds like you're saying that we kind of cheat, right?

474
00:38:11,840 --> 00:38:19,760
Like we, if you, if you have some given number or a vector, you could show that there exists

475
00:38:19,760 --> 00:38:25,000
some quantum circuit, you know, that could serve as a RAM that will return this, that could

476
00:38:25,000 --> 00:38:30,840
store this number, but we don't really have a fully functioning system for getting that

477
00:38:30,840 --> 00:38:33,600
number into memory and then retrieving it back.

478
00:38:33,600 --> 00:38:34,600
Right.

479
00:38:34,600 --> 00:38:37,840
So, what I'm saying is, like, you can hard-coded it.

480
00:38:37,840 --> 00:38:38,840
Yeah.

481
00:38:38,840 --> 00:38:39,840
Yeah.

482
00:38:39,840 --> 00:38:40,840
Yeah.

483
00:38:40,840 --> 00:38:45,800
And the thing is that for algorithms that are faster than like O of N, so they're sublitting

484
00:38:45,800 --> 00:38:50,040
your algorithms, you can't hard-code something like that in because it takes too much time

485
00:38:50,040 --> 00:38:51,040
to read it.

486
00:38:51,040 --> 00:38:57,280
And so, you really need the RAM, whereas for other algorithms, you maybe can get away without

487
00:38:57,280 --> 00:38:58,280
using the RAM.

488
00:38:58,280 --> 00:38:59,280
Well, awesome.

489
00:38:59,280 --> 00:39:04,720
And even thanks so much for taking the time to walk me through what you've done and kind

490
00:39:04,720 --> 00:39:06,400
of the broader implications.

491
00:39:06,400 --> 00:39:10,680
It's, you know, it's, as we just said, it's an interesting space.

492
00:39:10,680 --> 00:39:15,240
And because of that, I keep, I'll keep kind of coming back to it and, you know, banging

493
00:39:15,240 --> 00:39:19,560
my head against it a little bit and see if anything manages to permeate.

494
00:39:19,560 --> 00:39:25,320
But certainly what you've done, you know, you got a lot of publicity for your results

495
00:39:25,320 --> 00:39:30,800
back in the summer and it's really interesting work and I appreciate you taking the time

496
00:39:30,800 --> 00:39:32,560
to walk us through it.

497
00:39:32,560 --> 00:39:33,560
Yeah.

498
00:39:33,560 --> 00:39:34,560
No problem.

499
00:39:34,560 --> 00:39:35,960
Thanks for having me.

500
00:39:35,960 --> 00:39:40,000
All right, everyone.

501
00:39:40,000 --> 00:39:41,640
That's our show for today.

502
00:39:41,640 --> 00:39:47,360
For more information on Ewin or any of the topics covered in this episode, visit twimmelai.com

503
00:39:47,360 --> 00:39:50,800
slash talk slash 246.

504
00:39:50,800 --> 00:39:57,800
As always, thanks so much for listening and catch you next time.

