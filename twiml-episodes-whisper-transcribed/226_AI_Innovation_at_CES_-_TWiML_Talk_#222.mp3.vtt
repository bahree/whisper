WEBVTT

00:00.000 --> 00:05.880
Hey, what's up everyone? Sam Charrington here, host of this week in Machine Learning

00:05.880 --> 00:11.360
and AI Podcast, bringing you a look at my recent trip to the Consumer Electronic Show

00:11.360 --> 00:13.000
and Las Vegas.

00:13.000 --> 00:18.280
CES is one of those things that's hard to fully comprehend without having seen, so I thought

00:18.280 --> 00:21.360
it'd be fun to give you a look at it from my vantage point.

00:21.360 --> 00:25.920
In this special YouTube exclusive, we're going to check out some of the interesting examples

00:25.920 --> 00:29.600
of Machine Learning and AI that I found at the event.

00:29.600 --> 00:35.200
As you'd expect, there were a ton of robots on display at CES this year, but this enormous

00:35.200 --> 00:40.600
combine harvester in the John Deere booth might be one of the biggest bots you'll ever

00:40.600 --> 00:41.600
see.

00:41.600 --> 00:46.200
I spent some time speaking with Dears Tavanga, Ciavora, learning about the autonomy and

00:46.200 --> 00:49.320
computer vision tech built into this bad boy.

00:49.320 --> 00:50.320
Check it out.

00:50.320 --> 00:55.600
I'm here at the John Deere booth with Tavanga, who is telling me that there is a long history

00:55.600 --> 01:00.600
of autonomy in these foreign vehicles, you said you've been doing it for how long?

01:00.600 --> 01:01.600
Certainly.

01:01.600 --> 01:07.200
John Deere has been doing a self-driving for over 15 years now, and we actually have the

01:07.200 --> 01:11.600
accuracy level with our equipment down to plus or minus 2.5 centimeters of accuracy

01:11.600 --> 01:14.840
for a piece of equipment of this size and scale.

01:14.840 --> 01:19.080
So, yeah, here at CES this year we're excited about telling that story, and also many of

01:19.080 --> 01:21.600
the other technologies we continue to evolve.

01:21.600 --> 01:25.920
Here at the stand, we've been talking through some of our advancements in machine learning

01:25.920 --> 01:32.160
and computer vision, and also looking at we're going next with artificial intelligence

01:32.160 --> 01:36.360
or recent purchase of blue river technologies, and really how John Deere is trying to help

01:36.360 --> 01:38.880
farmers continue to feed the world.

01:38.880 --> 01:43.600
And you mentioned that there's computer vision and work at work in this combine.

01:43.600 --> 01:44.600
So, what's a lot of that?

01:44.600 --> 01:47.880
So, one thing we're really excited about with the new S700 series, Combine, what you're

01:47.880 --> 01:48.880
seeing here.

01:48.880 --> 01:56.840
We have some camera technology that's now embedded into the key aspects of the Combine.

01:56.840 --> 02:00.000
So, what you're seeing here is the grain elevator, right?

02:00.000 --> 02:04.600
Think of the Combine as a factory on wheels that's going through that corn field harvesting,

02:04.600 --> 02:09.480
and what's coming out up here is actually the desired product, Dipper, a harvesting corn

02:09.480 --> 02:13.320
for example, there's going to be kernels coming up here, and what's coming out the back

02:13.320 --> 02:15.920
of the Combine is going to be compost, right?

02:15.920 --> 02:20.920
So, it's going to be chopped up pieces of stock leaves, and all of that, and stuff you don't

02:20.920 --> 02:21.920
want.

02:21.920 --> 02:25.160
Well, yes, actually farmers actually do want to keep that on the field because then it

02:25.160 --> 02:28.000
puts nutrients back into the soil, right?

02:28.000 --> 02:31.400
So, hey, they're here, they're just done for.

02:31.400 --> 02:37.560
So, what I was talking through earlier is, as that material is coming up, now our equipment

02:37.560 --> 02:44.360
is able to allow the operator of the equipment to also program, and in essence have the machine

02:44.360 --> 02:48.480
make those adjustments for them, which is an essence taken care of what is the farmer's

02:48.480 --> 02:49.480
payday, right?

02:49.480 --> 02:54.320
So, they've been investing quite a bit, and so that crop cycle, so it's making those adjustments

02:54.320 --> 02:56.520
for them on the fly.

02:56.520 --> 03:01.120
These days, the average farmer for the U.S. farmer is about 5,000 acres, and for these guys,

03:01.120 --> 03:06.080
it's a very narrow window of time, so there are times working, you know, 12 to 16 hour

03:06.080 --> 03:07.080
days.

03:07.080 --> 03:13.040
So, both through driving or self-driving equipment, through this computer vision and automation,

03:13.040 --> 03:16.240
this and our equipment today, it actually helps them do so more productively.

03:16.240 --> 03:21.920
Right, and with the computer vision in particular, the result is higher yield and less foreign

03:21.920 --> 03:22.920
matter in the...

03:22.920 --> 03:23.920
Certainly, certainly.

03:23.920 --> 03:26.560
So, you know, think about this way.

03:26.560 --> 03:30.560
As we mentioned earlier, as they're going through harvesting, that's the farmer's payday,

03:30.560 --> 03:31.560
right?

03:31.560 --> 03:35.280
So, we're trying to help them maintain the best quality grain that's collected, and

03:35.280 --> 03:39.280
also make sure that with those tailings or what's coming out of the back of the combine,

03:39.280 --> 03:42.040
they aren't losing any of that grain also.

03:42.040 --> 03:43.040
Awesome.

03:43.040 --> 03:44.040
Thank you.

03:44.040 --> 03:45.400
Pretty cool, right?

03:45.400 --> 03:49.640
Of course, there were also a ton of drones here at CES.

03:49.640 --> 03:53.960
While they're getting smarter and smarter each year, one startup is attempting to give

03:53.960 --> 03:56.280
drones superhero-like powers.

03:56.280 --> 03:57.280
All right, cool.

03:57.280 --> 03:59.280
I'm here with Leah with AstroAR.

03:59.280 --> 04:00.800
What's AstroAR up to?

04:00.800 --> 04:01.800
Oh, man.

04:01.800 --> 04:02.800
Okay.

04:02.800 --> 04:03.800
So, we build drones that stop bullets.

04:03.800 --> 04:06.840
We've developed a sensor array.

04:06.840 --> 04:09.080
We're performing sensor fusion directly on the drone.

04:09.080 --> 04:10.080
It's all edge.

04:10.080 --> 04:12.720
And Vidya Inception partner, that's how we're able to do this.

04:12.720 --> 04:16.520
That's how we're able to get the type of hardware necessary to run those machine learning

04:16.520 --> 04:23.600
algorithms directly on the drone, doing all of this edge millimeter wave radio frequency,

04:23.600 --> 04:28.600
combined with computer vision, and other sensors I can't really delve into.

04:28.600 --> 04:32.880
But I assure you the ability to detect the difference between a fake gun and a real gun,

04:32.880 --> 04:36.600
a loaded gun and an unloaded gun, we can read the writing on the facts in the shell casings

04:36.600 --> 04:37.600
loaded into the gun.

04:37.600 --> 04:38.600
Wow.

04:38.600 --> 04:42.880
And now when I hear a drone that stops bullets, I think of like Wonder Woman bracelets and

04:42.880 --> 04:46.840
like getting in front of the bullets in real time, that's not really what this is doing.

04:46.840 --> 04:48.160
Not exactly.

04:48.160 --> 04:53.160
Because the drone is able to identify the gun, the presence of a firearm in advance, we

04:53.160 --> 04:55.360
identify this as an anomaly, not a firearm.

04:55.360 --> 04:59.560
It's just an object of interest for which a police officer is made aware of.

04:59.560 --> 05:03.440
We're not trying to take anybody's guns, but you can't shoot people.

05:03.440 --> 05:06.920
The drone will take up a position that we refer to as guardian angel mode.

05:06.920 --> 05:09.200
We'll get ready to get involved.

05:09.200 --> 05:14.280
Now, it assesses threat based on the proximity of a hand to a gun, any hand, any gun.

05:14.280 --> 05:20.240
So yes, they are absolutely, they're absolutely capable of getting in the way from that vantage

05:20.240 --> 05:21.240
point.

05:21.240 --> 05:24.680
And also, there is a human reflex that we are exploiting in order to ensure that this

05:24.680 --> 05:25.680
works.

05:25.680 --> 05:33.080
Do you remember, do you remember how Sarah Connor, she's running at that security guy?

05:33.080 --> 05:37.840
He knows how dangerous she is and yet, when she tosses the keys to him, his hands come

05:37.840 --> 05:38.840
up.

05:38.840 --> 05:39.840
Yeah.

05:39.840 --> 05:40.840
Yeah.

05:40.840 --> 05:41.840
That is a human reflex.

05:41.840 --> 05:42.840
It is universal.

05:42.840 --> 05:43.840
The drones will rush you.

05:43.840 --> 05:48.080
They will make themselves the most interesting thing in your world for about the next 30

05:48.080 --> 05:49.080
seconds.

05:49.080 --> 05:50.600
They will be extremely difficult to ignore.

05:50.600 --> 05:52.440
It's like ignoring a flying blender.

05:52.440 --> 05:59.920
If you're familiar with the canine technique of bark and hold, it's a canine can be trained

05:59.920 --> 06:03.680
to rush someone, they'll jump up and down and bark.

06:03.680 --> 06:04.680
Yeah.

06:04.680 --> 06:05.680
Bark, bark, bark, bark.

06:05.680 --> 06:06.680
Yeah.

06:06.680 --> 06:10.040
You're getting a live pseudo live demonstration of bark and hold here.

06:10.040 --> 06:11.040
Yeah.

06:11.040 --> 06:12.040
I'm a bark and hold.

06:12.040 --> 06:14.880
It's what that's called, when police do it.

06:14.880 --> 06:18.000
And it's impossible to ignore that dog.

06:18.000 --> 06:19.160
Exactly the same principle.

06:19.160 --> 06:21.360
It is impossible to ignore those rotors.

06:21.360 --> 06:24.600
Those rotors are, it's loud, it's making noise.

06:24.600 --> 06:25.920
We have red and blue.

06:25.920 --> 06:27.360
You're absolutely right.

06:27.360 --> 06:28.360
Yeah.

06:28.360 --> 06:35.840
So you've got millimeter wave technology that sounds like it can see through walls and

06:35.840 --> 06:37.560
objects and determined.

06:37.560 --> 06:41.360
The sensors that we use are actually capable of detecting cancer over the air.

06:41.360 --> 06:43.600
We repurpose them for this.

06:43.600 --> 06:44.600
Okay.

06:44.600 --> 06:45.600
And so you're just.

06:45.600 --> 06:48.800
They originally designed to perform a, it's basically a handheld MRI.

06:48.800 --> 06:49.800
Okay.

06:49.800 --> 06:50.800
Yeah.

06:50.800 --> 06:51.800
And so what's the range?

06:51.800 --> 06:52.800
10 meters.

06:52.800 --> 06:53.800
Okay.

06:53.800 --> 06:54.800
It's about 30 feet.

06:54.800 --> 06:56.120
Now these work best in pairs.

06:56.120 --> 06:57.640
So about 60 feet.

06:57.640 --> 06:58.640
Yeah.

06:58.640 --> 06:59.640
Okay.

06:59.640 --> 07:05.640
And so it detects that there's a gun present and kind of takes a guardian angel position.

07:05.640 --> 07:10.640
And it also sounds like you're doing some kind of pose detection to deter, like pose detection.

07:10.640 --> 07:13.640
Do it to a certain extent, to a certain extent.

07:13.640 --> 07:14.640
Yes.

07:14.640 --> 07:15.640
This.

07:15.640 --> 07:16.640
It's probably.

07:16.640 --> 07:17.640
It's predicted by.

07:17.640 --> 07:18.640
Yeah.

07:18.640 --> 07:20.640
The city of hand to gun.

07:20.640 --> 07:21.640
Okay.

07:21.640 --> 07:28.640
That hand touches that gun and it's your program activates.

07:28.640 --> 07:31.640
The only thing getting shot is a drone.

07:31.640 --> 07:32.640
Yeah.

07:32.640 --> 07:33.640
Right.

07:33.640 --> 07:34.640
Yeah.

07:34.640 --> 07:35.640
That's this is the armor.

07:35.640 --> 07:36.640
Oh, this is one of the armors.

07:36.640 --> 07:37.640
This is the raw armor.

07:37.640 --> 07:38.640
Raw.

07:38.640 --> 07:44.640
The produce armor has a backing of ceramic or Kevlar or you know, it's built up with you.

07:44.640 --> 07:45.640
Build a proof of acrylic.

07:45.640 --> 07:46.640
Okay.

07:46.640 --> 07:47.640
It's a composite.

07:47.640 --> 07:48.640
It is a composite.

07:48.640 --> 07:53.640
And I mean, believe it or not, we actually have bulletproof panions.

07:53.640 --> 07:54.640
I know, right?

07:54.640 --> 07:55.640
I'm not even kidding.

07:55.640 --> 07:56.640
I'm not kidding.

07:56.640 --> 07:57.640
It works.

07:57.640 --> 07:58.640
What can I say?

07:58.640 --> 07:59.640
So I mentioned the.

07:59.640 --> 08:00.640
Come on over.

08:00.640 --> 08:01.640
Come on over.

08:01.640 --> 08:02.640
What's that?

08:02.640 --> 08:03.640
Move out of the way.

08:03.640 --> 08:04.640
Dude, you're blocking our.

08:04.640 --> 08:05.640
Yeah.

08:05.640 --> 08:06.640
I mentioned the trajectory thing.

08:06.640 --> 08:08.640
The pose detection.

08:08.640 --> 08:09.640
Yeah.

08:09.640 --> 08:10.640
Because you mentioned trajectory tracking.

08:10.640 --> 08:11.640
Yeah, trajectory tracking.

08:11.640 --> 08:12.640
The has has most.

08:12.640 --> 08:16.640
It's most focused on what is the gun doing in the hand.

08:16.640 --> 08:22.640
So it kind of estimates the departure angle of the bullet from the gun and gets in the way based on that.

08:22.640 --> 08:23.640
Yeah.

08:23.640 --> 08:24.640
I mean, you're going to.

08:24.640 --> 08:26.640
It's also trying to provoke you to aim at it.

08:26.640 --> 08:27.640
Okay.

08:27.640 --> 08:28.640
Yeah.

08:28.640 --> 08:31.640
So this is very much an active process that it is trying to get you to, hey, I'm over here.

08:31.640 --> 08:32.640
Hey, I'm over here.

08:32.640 --> 08:33.640
Pay attention to me.

08:33.640 --> 08:36.640
Um, for God's sakes, we make the armor shiny.

08:36.640 --> 08:37.640
Okay.

08:37.640 --> 08:40.640
And so it sounds like it's pretty, uh, inference heavy.

08:40.640 --> 08:44.640
You're not kind of sending information to a cloud and waiting for.

08:44.640 --> 08:46.640
We're not.

08:46.640 --> 08:47.640
We absolutely.

08:47.640 --> 08:49.640
If we tried to send this over the cloud, someone would get killed.

08:49.640 --> 08:51.640
So what are you using on device?

08:51.640 --> 08:53.640
Uh, no, we're like I said, we're an Nvidia inception startup.

08:53.640 --> 08:56.640
We are, uh, we're also arrow partners and we are.

08:56.640 --> 08:57.640
It's like a Jetson or.

08:57.640 --> 08:59.640
Uh, what's the Xavier?

08:59.640 --> 09:00.640
Okay.

09:00.640 --> 09:01.640
Yeah, we're using Xavier's.

09:01.640 --> 09:02.640
I know Xavier's.

09:02.640 --> 09:03.640
These are TX 2's.

09:03.640 --> 09:04.640
We're.

09:04.640 --> 09:05.640
We're not going to bring.

09:05.640 --> 09:06.640
We're not going to bring the Xavier's.

09:06.640 --> 09:08.640
Well, taking that down out the block.

09:08.640 --> 09:09.640
Sorry, guys.

09:09.640 --> 09:10.640
Um, yeah.

09:10.640 --> 09:12.640
And this is the model of the.

09:12.640 --> 09:14.640
Uh, yeah, that's what's going on.

09:14.640 --> 09:15.640
That's being too arrow.

09:15.640 --> 09:17.640
Let's throw that we use.

09:17.640 --> 09:19.640
As you can see, we also incorporate.

09:19.640 --> 09:21.640
These are, this is a D415.

09:21.640 --> 09:22.640
Um, that we've incorporated.

09:22.640 --> 09:24.640
We have an excellent relationship with Intel.

09:24.640 --> 09:26.640
An excellent relationship with Nvidia.

09:26.640 --> 09:28.640
An excellent relationship with arrow.

09:28.640 --> 09:31.640
Um, and we are producing some custom hardware, some custom

09:31.640 --> 09:33.640
silicon in order to achieve these purposes.

09:33.640 --> 09:36.640
So what are the key things you've had to learn from a machine learning

09:36.640 --> 09:39.640
and AI perspective or deep learning perspective?

09:39.640 --> 09:42.640
A lot easier and a lot more straightforward than people seem to think.

09:42.640 --> 09:43.640
Okay.

09:43.640 --> 09:44.640
Yeah.

09:44.640 --> 09:47.640
I mean, quite frankly, this has much.

09:47.640 --> 09:50.640
That and the FBI doesn't like it when I talk about this too much.

09:50.640 --> 09:54.640
I mean, I want to, I want to deep dive this.

09:54.640 --> 09:55.640
I love it.

09:55.640 --> 09:57.640
So I mean.

09:57.640 --> 09:58.640
But there's only so much I can say.

09:58.640 --> 09:59.640
Okay.

09:59.640 --> 10:00.640
Yeah.

10:00.640 --> 10:01.640
Got it.

10:01.640 --> 10:02.640
Well, very cool application.

10:02.640 --> 10:03.640
I hope this works out.

10:03.640 --> 10:04.640
Absolutely, absolutely.

10:04.640 --> 10:06.640
Oh, there's one last thing I would like to say.

10:06.640 --> 10:08.640
No, we do not weaponize the drones ever under any circumstances.

10:08.640 --> 10:10.640
We will not add pepper spray.

10:10.640 --> 10:11.640
We will not add a taser.

10:11.640 --> 10:13.640
We will not add anything that will make this into a weapon.

10:13.640 --> 10:15.640
It is a shield that is its job.

10:15.640 --> 10:17.640
Yes, you can defeat it with a t-shirt.

10:17.640 --> 10:18.640
You go right ahead.

10:18.640 --> 10:19.640
That's like messing with the cops canine.

10:19.640 --> 10:20.640
You see what happens.

10:20.640 --> 10:21.640
Yeah.

10:21.640 --> 10:25.640
Now, you wouldn't know it by the name, but CES continues to showcase

10:25.640 --> 10:29.640
more and more enterprise-oriented innovations each year.

10:29.640 --> 10:34.640
One enterprise robotics vendor, Omron, uses a ping pong playing robot

10:34.640 --> 10:39.640
that's actually not for sale to demonstrate their industrial chops.

10:39.640 --> 10:40.640
How did this work?

10:40.640 --> 10:41.640
So this is forpious.

10:41.640 --> 10:45.640
Our fifth generation AI equipped table tennis shooter

10:45.640 --> 10:48.640
and essentially it uses machine vision to monitor the player

10:48.640 --> 10:52.640
to watch the player's body language also how they swing the paddle

10:52.640 --> 10:54.640
and then also how the where the ball is.

10:54.640 --> 10:59.640
And use AI to compare that to how it was taught from an expert player.

10:59.640 --> 11:02.640
So it'll use that and actually try to teach the player to get better.

11:02.640 --> 11:06.640
So it uses the vision to analyze, you know, to see where the ball is.

11:06.640 --> 11:11.640
It uses the AI to analyze how the player is playing against the other

11:11.640 --> 11:12.640
and how to help them improve.

11:12.640 --> 11:16.640
And it also uses IT control to be able to get the robotics in the right place

11:16.640 --> 11:19.640
to be able to actually play and keep the rallies going.

11:19.640 --> 11:21.640
Okay, and how many cameras are involved?

11:21.640 --> 11:22.640
There's five cameras.

11:22.640 --> 11:23.640
Five.

11:23.640 --> 11:24.640
Yeah.

11:24.640 --> 11:25.640
Where are they located?

11:25.640 --> 11:26.640
So they're actually in the front.

11:26.640 --> 11:28.640
So if you look in front, they're all facing the player looking down

11:28.640 --> 11:31.640
and again watching the player, watching the paddle, watching the ball.

11:31.640 --> 11:34.640
And this isn't a product you sell.

11:34.640 --> 11:36.640
What's the, why do you show this here?

11:36.640 --> 11:39.640
Yeah, even though we do get offers to sell it,

11:39.640 --> 11:41.640
it is actually just a demonstration of our technology.

11:41.640 --> 11:45.640
We actually, we use the same technology in industrial automation,

11:45.640 --> 11:48.640
healthcare, in mobility, and also energy management.

11:48.640 --> 11:52.640
So this is, this is kind of fun, but those other places are what it does for a living.

11:52.640 --> 11:53.640
Okay.

11:53.640 --> 11:57.640
So here's an industrial automation application of some of the same technology you've seen before.

11:57.640 --> 12:00.640
If you look at the robot behind me, it's doing a pick and place application

12:00.640 --> 12:03.640
which is very typical in a factory automation setting.

12:03.640 --> 12:04.640
Kind of pick and place.

12:04.640 --> 12:07.640
So basically picking up something from one conveyor to one place

12:07.640 --> 12:09.640
and sorting it and putting it somewhere else.

12:09.640 --> 12:10.640
Okay.

12:10.640 --> 12:13.640
So it's a job that is not really good for humans to do because it's repetitive.

12:13.640 --> 12:17.640
So it's an example of being able to take an unsafe job

12:17.640 --> 12:19.640
and even be able to do it with a robot.

12:19.640 --> 12:22.640
And it's using the same technologies in terms of computer vision

12:22.640 --> 12:24.640
and path planning and the like.

12:24.640 --> 12:25.640
Right.

12:25.640 --> 12:26.640
Exactly.

12:26.640 --> 12:28.640
So if the, there is, there's vision similar to what's being used in portbius

12:28.640 --> 12:32.640
looking at the conveyor line to find and locate the items that are on the conveyor line.

12:32.640 --> 12:36.640
And then the same robot that's usually used for the paddle is also being used

12:36.640 --> 12:38.640
in a pick and place application here.

12:38.640 --> 12:41.640
If you were a fan of our AI in sports series last year,

12:41.640 --> 12:43.640
you would love CES.

12:43.640 --> 12:48.640
There were a bunch of really exciting sports tech applications on display there.

12:48.640 --> 12:52.640
One interesting one was this AI powered boxing trainer,

12:52.640 --> 12:57.640
bot boxer, which can be yours for just $20,000.

12:57.640 --> 12:58.640
This is Bob Boxer.

12:58.640 --> 13:00.640
It's an intelligent luching bag.

13:00.640 --> 13:01.640
It can move.

13:01.640 --> 13:03.640
But you can also see your shots.

13:03.640 --> 13:07.640
So it tries to attract multiple points to your body

13:07.640 --> 13:10.640
and tries to predict where the shot is going to come from.

13:10.640 --> 13:12.640
So it's using computer vision to do that?

13:12.640 --> 13:13.640
It's using computer vision.

13:13.640 --> 13:14.640
Yeah.

13:14.640 --> 13:17.640
And with time, it also learns your style.

13:17.640 --> 13:18.640
Okay.

13:18.640 --> 13:21.640
And so it can look at you and it can try and tease you.

13:21.640 --> 13:23.640
It can, you know, it can see.

13:23.640 --> 13:25.640
It can see you winding up a shot.

13:25.640 --> 13:28.640
Okay. It's going to predict where the shot is heading from.

13:28.640 --> 13:30.640
And how's it doing that?

13:30.640 --> 13:35.640
We're tracking the upper body angles

13:35.640 --> 13:37.640
and using that to predict what shot.

13:37.640 --> 13:39.640
We use multiple track points.

13:39.640 --> 13:42.640
You can see you doing this.

13:42.640 --> 13:44.640
Telegraphing a shot.

13:44.640 --> 13:48.640
And it also understands what you're trying to trick.

13:48.640 --> 13:50.640
Like, good boxes, good boxes.

13:50.640 --> 13:52.640
They strategize.

13:52.640 --> 13:55.640
They would come up with one angle and then they struck it.

13:55.640 --> 13:56.640
It's trying to get another.

13:56.640 --> 13:57.640
So, buy boxes.

13:57.640 --> 13:58.640
You can also understand these things.

13:58.640 --> 14:00.640
Center for different levels.

14:00.640 --> 14:01.640
You can try it.

14:01.640 --> 14:02.640
You can try it.

14:02.640 --> 14:04.640
You know, dodge the shot.

14:04.640 --> 14:08.640
But here, you know, you can, you can work on your,

14:08.640 --> 14:10.640
you can work on your boxing technique,

14:10.640 --> 14:15.640
but it's also an awesome upper body cardio workout technique.

14:15.640 --> 14:16.640
We have different modes.

14:16.640 --> 14:17.640
We have training modes.

14:17.640 --> 14:19.640
You can work on your different punches.

14:19.640 --> 14:22.640
We'll give you standard boxing combination.

14:22.640 --> 14:25.640
Combination of jab is across the hooks.

14:25.640 --> 14:27.640
And it'll build you a workout routine.

14:27.640 --> 14:28.640
Awesome.

14:28.640 --> 14:30.640
Yeah, thank you.

14:30.640 --> 14:33.640
One of the product categories I was most excited about at CES

14:33.640 --> 14:36.640
was health and fitness technology.

14:36.640 --> 14:39.640
Here, we speak with Frank McGillan of Quell,

14:39.640 --> 14:41.640
a wearable task with delivering chronic pain relief

14:41.640 --> 14:44.640
using AI and neuro technology.

14:44.640 --> 14:46.640
I'm here with Frank McGillan with Quell.

14:46.640 --> 14:47.640
What's Quell doing?

14:47.640 --> 14:49.640
So Quell is the first pain relief solution

14:49.640 --> 14:52.640
to integrate neuro technology plus AI

14:52.640 --> 14:55.640
to deliver personalized approach to treating chronic pain.

14:55.640 --> 14:57.640
All right, so let's break that apart.

14:57.640 --> 14:59.640
So neuro technology.

14:59.640 --> 15:00.640
Neuro technology.

15:00.640 --> 15:01.640
Yeah, yeah.

15:01.640 --> 15:04.640
What is the neuro technology aspect of the problem?

15:04.640 --> 15:07.640
So neuro technology is stimulating sensory nerves

15:07.640 --> 15:11.640
with high frequency, high power, electrical stimulation.

15:11.640 --> 15:14.640
And it triggers the body's natural pain blocking mechanism.

15:14.640 --> 15:15.640
Okay.

15:15.640 --> 15:17.640
After first generation product in 2015,

15:17.640 --> 15:21.640
and this year we brought AI to make this product even smarter

15:21.640 --> 15:23.640
and react better to the patient's needs.

15:23.640 --> 15:26.640
And so how specifically are you using AI?

15:26.640 --> 15:27.640
Yeah, well, we're tapped into.

15:27.640 --> 15:30.640
We have 70,000 users from our first generation product

15:30.640 --> 15:33.640
who volunteered their health profile information.

15:33.640 --> 15:35.640
So we're able to look at everything from,

15:35.640 --> 15:37.640
you know, where do they have pain?

15:37.640 --> 15:38.640
How long have they had pain?

15:38.640 --> 15:39.640
Other health conditions.

15:39.640 --> 15:42.640
It's body mass, age, all those factors.

15:42.640 --> 15:45.640
They were applied machine learning to that data,

15:45.640 --> 15:46.640
to look at clusters,

15:46.640 --> 15:49.640
to find similarities in clusters.

15:49.640 --> 15:51.640
And we've built that into a smart algorithm

15:51.640 --> 15:53.640
into the growl device.

15:53.640 --> 15:55.640
So when a new user gets quell,

15:55.640 --> 15:57.640
they're benefiting from all the learning we have

15:57.640 --> 15:59.640
from those 70,000 people.

15:59.640 --> 16:00.640
Okay.

16:00.640 --> 16:01.640
So what's an example of something

16:01.640 --> 16:03.640
that machine learning helps you do better?

16:03.640 --> 16:04.640
Yeah.

16:04.640 --> 16:08.640
So we found that, you know, certain phenotypes of pain sufferers

16:08.640 --> 16:12.640
that are normal calibration may misjudge

16:12.640 --> 16:14.640
what their dosage should be.

16:14.640 --> 16:17.640
They say that their dose should be lower or higher,

16:17.640 --> 16:19.640
for example, than what they really need.

16:19.640 --> 16:21.640
And my dosage, we're talking about the amount of electrical

16:21.640 --> 16:22.640
which is an elation.

16:22.640 --> 16:23.640
Yeah, exactly.

16:23.640 --> 16:24.640
Exactly.

16:24.640 --> 16:27.640
So in the key to getting quality relief

16:27.640 --> 16:29.640
is getting the dosage right.

16:29.640 --> 16:30.640
You know, if you think about it,

16:30.640 --> 16:31.640
if you're taking pain medication,

16:31.640 --> 16:34.640
if you're taking 10 milligrams and you need 100,

16:34.640 --> 16:35.640
you're not going to see relief.

16:35.640 --> 16:37.640
Well, the same thing with neuro stimulation

16:37.640 --> 16:39.640
or electrical therapy.

16:39.640 --> 16:41.640
So based on our smart algorithm,

16:41.640 --> 16:44.640
we're now actually factoring in, you know,

16:44.640 --> 16:46.640
hundreds of variables in addition to this

16:46.640 --> 16:49.640
nervous sensitivity test we do when you first use the product.

16:49.640 --> 16:50.640
Okay, awesome.

16:50.640 --> 16:53.640
And do you have, it looks like a band that you wear.

16:53.640 --> 16:55.640
You don't happen to have one on your knee.

16:55.640 --> 16:56.640
I actually do.

16:56.640 --> 16:58.640
So, you know, it's born under your leg here.

16:58.640 --> 16:59.640
Okay.

16:59.640 --> 17:01.640
And it's controlled through the app.

17:01.640 --> 17:04.640
So we have a swell app here which lets you

17:04.640 --> 17:10.640
help you manage the device.

17:10.640 --> 17:13.640
We provide a lot of tracking information.

17:13.640 --> 17:16.640
So we're not only collecting some of this profile data,

17:16.640 --> 17:20.640
but we're also collecting things like activity,

17:20.640 --> 17:22.640
gate, sleep.

17:22.640 --> 17:24.640
So we provide a lot of tracking information.

17:24.640 --> 17:27.640
And again, we're using this data not only to make the device smarter,

17:27.640 --> 17:30.640
but also helping improve in the real world

17:30.640 --> 17:32.640
how to achieve recovery pain relief.

17:32.640 --> 17:35.640
Okay. And are there other examples of where machine learning

17:35.640 --> 17:38.640
AI have come into play beyond the calibration?

17:38.640 --> 17:39.640
So that's our first step.

17:39.640 --> 17:40.640
Okay.

17:40.640 --> 17:42.640
You know, we're really, again, trying to make sure we benefit

17:42.640 --> 17:45.640
from all the data we've collected from our current users.

17:45.640 --> 17:49.640
Yeah, what we're looking at for next step is how to apply that,

17:49.640 --> 17:51.640
not only in that initial calibration,

17:51.640 --> 17:52.640
but on an ongoing basis.

17:52.640 --> 17:53.640
Okay.

17:53.640 --> 17:54.640
Awesome. Thanks so much, Frank.

17:54.640 --> 17:55.640
Cool.

17:55.640 --> 17:56.640
Thank you.

17:56.640 --> 17:58.640
Perhaps my least favorite category at CES

17:58.640 --> 18:00.640
was so-called smart cities.

18:00.640 --> 18:02.640
Not because they're not smart,

18:02.640 --> 18:05.640
but rather because too often it's just a thinly veiled euphemism

18:05.640 --> 18:07.640
for the surveillance state.

18:07.640 --> 18:10.640
That said, my conversation with Zemanthus was interesting

18:10.640 --> 18:13.640
because they claim to fix something that we all hate.

18:13.640 --> 18:15.640
Traffic.

18:15.640 --> 18:18.640
All right, cool. So I'm here with Alexandros from Zemanthus.

18:18.640 --> 18:20.640
What is Zemanthus up to?

18:20.640 --> 18:23.640
Zemanthus, we are predicting what will happen

18:23.640 --> 18:27.640
in your traffic route before you get there.

18:27.640 --> 18:30.640
So we can give you, for instance, information

18:30.640 --> 18:34.640
about an upcoming traffic jam, 30 minutes or one hour

18:34.640 --> 18:36.640
or two hours into the future.

18:36.640 --> 18:38.640
And the beauty of it is, you can avoid it.

18:38.640 --> 18:40.640
If you know it's going to happen.

18:40.640 --> 18:42.640
Now, in order to do this,

18:42.640 --> 18:43.640
you need some advanced mathematics.

18:43.640 --> 18:45.640
And this is where Xemanthus came in.

18:45.640 --> 18:48.640
We have a patented algorithm already

18:48.640 --> 18:53.640
that combines machine learning and stochastic modeling

18:53.640 --> 18:55.640
in order to produce this advanced forecasting

18:55.640 --> 18:58.640
of your future routes to work or home.

18:58.640 --> 19:01.640
So you can actually get there in time before anyone else.

19:01.640 --> 19:05.640
But of course it's not just for just private users.

19:05.640 --> 19:09.640
The municipality is gained because there is reduction in fuel.

19:09.640 --> 19:11.640
There is reduction in congestion.

19:11.640 --> 19:14.640
There is reduction in pollutants for the environment.

19:14.640 --> 19:18.640
So everybody gains by avoiding a congestion, for instance.

19:18.640 --> 19:22.640
So the idea is that you're combining statistical models

19:22.640 --> 19:25.640
with more model-based approaches.

19:25.640 --> 19:26.640
Exactly.

19:26.640 --> 19:29.640
So machine learning is, as you know,

19:29.640 --> 19:34.640
useful to predict patterns in the usual historic information.

19:34.640 --> 19:37.640
So machine learning finds out what is usually going to happen

19:37.640 --> 19:38.640
in traffic.

19:38.640 --> 19:41.640
But to figure out what will happen in the future,

19:41.640 --> 19:44.640
the patterns is not always very accurate

19:44.640 --> 19:46.640
because traffic is chaotic phenomena.

19:46.640 --> 19:49.640
So to figure out what happens in the future,

19:49.640 --> 19:51.640
we use our own stochastic algorithms.

19:51.640 --> 19:54.640
Which is like predicting what will happen in the stock market

19:54.640 --> 19:56.640
together with the machine learning.

19:56.640 --> 19:58.640
And those two, combined together,

19:58.640 --> 20:01.640
allow us to have amazing accuracy for future events

20:01.640 --> 20:03.640
before they occur.

20:03.640 --> 20:07.640
Okay. And so these are like your kind of jamming machine learning

20:07.640 --> 20:10.640
models and like Markov processes,

20:10.640 --> 20:13.640
MM1 cues and MMN cues and all that kind of stuff.

20:13.640 --> 20:16.640
So it is Markov modeling together with Monte Carlo

20:16.640 --> 20:17.640
for the simulations.

20:17.640 --> 20:18.640
Exactly.

20:18.640 --> 20:20.640
Okay. So primarily simulation-based.

20:20.640 --> 20:21.640
Okay.

20:21.640 --> 20:24.640
Because once you simulate based on the real data,

20:24.640 --> 20:28.640
what will happen, then you can start avoiding the problems.

20:28.640 --> 20:29.640
Okay.

20:29.640 --> 20:30.640
Because you know where they will occur.

20:30.640 --> 20:33.640
In that location of the road in 30 minutes,

20:33.640 --> 20:34.640
I'll have a traffic jam.

20:34.640 --> 20:35.640
I know what to do now.

20:35.640 --> 20:36.640
Okay.

20:36.640 --> 20:38.640
You've got some of the use cases here

20:38.640 --> 20:41.640
and you're also displaying some of the results you've seen.

20:41.640 --> 20:43.640
This is an example in Sweden.

20:43.640 --> 20:48.640
We got real data in blue versus green are predictions.

20:48.640 --> 20:50.640
Which is right on top of each other.

20:50.640 --> 20:51.640
Yeah.

20:51.640 --> 20:53.640
And this is over several days we can do that.

20:53.640 --> 20:54.640
Okay.

20:54.640 --> 20:57.640
An amazing thing, which is really important,

20:57.640 --> 20:59.640
is this graph in the bottom.

20:59.640 --> 21:02.640
So down here, you have the usual traffic jam,

21:02.640 --> 21:06.640
but then on the 23rd of November,

21:06.640 --> 21:08.640
there is no traffic congestion.

21:08.640 --> 21:10.640
Yet we predicted that.

21:10.640 --> 21:11.640
They won't be any.

21:11.640 --> 21:12.640
Okay.

21:12.640 --> 21:13.640
You know what happened on the 23rd of November?

21:13.640 --> 21:14.640
Looks like black Friday.

21:14.640 --> 21:15.640
It's a black Friday.

21:15.640 --> 21:16.640
Okay.

21:16.640 --> 21:18.640
And you know why this is amazing?

21:18.640 --> 21:19.640
If you go around,

21:19.640 --> 21:22.640
any machine learning algorithm will predict the pattern.

21:22.640 --> 21:24.640
Every, you know, five o'clock,

21:24.640 --> 21:25.640
you have a traffic jam.

21:25.640 --> 21:28.640
But they won't predict it's a black Friday.

21:28.640 --> 21:30.640
We predicted it was a black Friday.

21:30.640 --> 21:31.640
Not traffic jam.

21:31.640 --> 21:34.640
It seems like it would depend on what kind of features

21:34.640 --> 21:37.640
you're using, like if you're,

21:37.640 --> 21:42.640
if your feature engineering kind of tags historical data

21:42.640 --> 21:44.640
with, you know, events like back Friday

21:44.640 --> 21:46.640
would do a better job of predicting it.

21:46.640 --> 21:48.640
So what allowed your approach to do a better?

21:48.640 --> 21:50.640
A stochastic pattern, the algorithm is what allows that.

21:50.640 --> 21:53.640
Because it's not only looking at the patterns,

21:53.640 --> 21:54.640
which are real cool,

21:54.640 --> 21:57.640
but says what will happen in the future based

21:57.640 --> 21:59.640
on the current information.

21:59.640 --> 22:02.640
And then the stochastic algorithm takes over for the future.

22:02.640 --> 22:05.640
And never, ever hit the congestion

22:05.640 --> 22:06.640
because it was a black Friday.

22:06.640 --> 22:07.640
I don't know how it happened.

22:07.640 --> 22:09.640
Because I don't control the stochastic.

22:09.640 --> 22:10.640
It's a market process.

22:10.640 --> 22:11.640
It's a random thing.

22:11.640 --> 22:12.640
You will do its own thing.

22:12.640 --> 22:14.640
But the figure did it out on its own.

22:14.640 --> 22:15.640
That's why we saw this.

22:15.640 --> 22:17.640
It was an amazing result.

22:17.640 --> 22:21.640
And so the data sources that you've played around with

22:21.640 --> 22:23.640
are primarily traffic-based or there are others.

22:23.640 --> 22:25.640
So we use whatever is provided.

22:25.640 --> 22:28.640
In this case, we have cameras and video.

22:28.640 --> 22:29.640
So we use that.

22:29.640 --> 22:32.640
But we can use RFID to detect as you name it.

22:32.640 --> 22:34.640
Whatever we are provided, we use it.

22:34.640 --> 22:37.640
It can all go in as input in the algorithm.

22:37.640 --> 22:38.640
It doesn't care.

22:38.640 --> 22:42.640
And what was the origin of the technology?

22:42.640 --> 22:44.640
The origin is my own because I'm a mathematician.

22:44.640 --> 22:48.640
So I actually created my own stochastic model

22:48.640 --> 22:50.640
because I work in traffic.

22:50.640 --> 22:52.640
To simulate traffic.

22:52.640 --> 22:55.640
And because it's a chaotic phenomenon,

22:55.640 --> 22:56.640
it was research for me.

22:56.640 --> 22:59.640
But I saw the obvious advantage here.

22:59.640 --> 23:00.640
So I created a company.

23:00.640 --> 23:01.640
Okay.

23:01.640 --> 23:02.640
And so have you published on this at all?

23:02.640 --> 23:03.640
Yes.

23:03.640 --> 23:05.640
After we did the pattern, then we published it.

23:05.640 --> 23:08.640
Okay. And where can we find that?

23:08.640 --> 23:10.640
What's the name of the paper that folks should look at?

23:10.640 --> 23:11.640
There are several papers now.

23:11.640 --> 23:12.640
Okay.

23:12.640 --> 23:15.640
I've been giving different conferences,

23:15.640 --> 23:16.640
the information.

23:16.640 --> 23:18.640
So I've published a number of papers by now.

23:18.640 --> 23:21.640
Any particular one we can start out with?

23:21.640 --> 23:22.640
Yeah.

23:22.640 --> 23:24.640
Asymmetric simple exclusion processes.

23:24.640 --> 23:26.640
Asymmetric simple exclusion processes.

23:26.640 --> 23:27.640
For traffic dynamics, yes.

23:27.640 --> 23:28.640
Okay. Awesome.

23:28.640 --> 23:29.640
Yeah.

23:29.640 --> 23:30.640
Alexandra, thank you so much.

23:30.640 --> 23:33.640
Autonomous vehicles come in all different shapes, sizes,

23:33.640 --> 23:35.640
form factors at CES.

23:35.640 --> 23:37.640
And one of the most impactful applications

23:37.640 --> 23:40.640
targets people with limited mobility.

23:40.640 --> 23:44.640
Check out this clip for a look at a computer vision equipped wheelchair

23:44.640 --> 23:46.640
developed by HuBox Robotics,

23:46.640 --> 23:48.640
a startup partnering with Intel.

23:48.640 --> 23:51.640
You can see all my facial expressions

23:51.640 --> 23:53.640
and all the data I can capture.

23:53.640 --> 23:54.640
All the points.

23:54.640 --> 24:04.640
So this is what is running inside the Intel Nug.

24:04.640 --> 24:14.640
So in real time, you can capture facial expressions in a more precise way

24:14.640 --> 24:16.640
than any other technology you do.

24:16.640 --> 24:20.640
So which allows us to use the kids to stop the wheelchair,

24:20.640 --> 24:23.640
open mouth to go to one side,

24:23.640 --> 24:25.640
let me just turn them.

24:25.640 --> 24:26.640
Turn them off.

24:26.640 --> 24:29.640
Nice.

24:29.640 --> 24:32.640
And a kids to stop.

24:32.640 --> 24:33.640
Raise that eyebrow.

24:33.640 --> 24:34.640
Kids to stop.

24:34.640 --> 24:36.640
Open mouth.

24:36.640 --> 24:38.640
And kids to stop.

24:38.640 --> 24:40.640
And a quick smile.

24:40.640 --> 24:42.640
And move forward.

24:42.640 --> 24:43.640
And even a tongue out.

24:43.640 --> 24:45.640
Sometimes I can hold back.

24:45.640 --> 24:48.640
So you see the precision.

24:48.640 --> 24:52.640
You see the quality of the capture and facial expressions

24:52.640 --> 24:53.640
in the technology.

24:53.640 --> 24:55.640
So this is what we are delivering to the market

24:55.640 --> 24:58.640
using terrorism scammer and also the Intel Nug.

24:58.640 --> 25:01.640
It's our own work computer that I have here

25:01.640 --> 25:02.640
underneath the wheelchair.

25:02.640 --> 25:05.640
So we are processing everything locally

25:05.640 --> 25:08.640
to capture those facial expressions.

25:08.640 --> 25:10.640
So far we can capture 10 facial expressions.

25:10.640 --> 25:13.640
You can use five of them to control your wheelchair.

25:13.640 --> 25:40.640
And the other ones you can talk to Alexa by Amazon for example.

25:40.640 --> 25:51.640
So from AI perspective in this based on a supervised type of model

25:51.640 --> 25:53.640
did you train it on?

25:53.640 --> 25:55.640
It's already pre-trained.

25:55.640 --> 26:00.640
So what we did was we create a huge data set of 3D faces

26:00.640 --> 26:02.640
because we are using 3D technology.

26:02.640 --> 26:05.640
We pre-trained it to classify 10 facial expressions.

26:05.640 --> 26:07.640
I'll give you an example.

26:07.640 --> 26:12.640
We are able to classify a full smile, a half smile,

26:12.640 --> 26:14.640
and a shy smile for example.

26:14.640 --> 26:16.640
So this is huge, right?

26:16.640 --> 26:18.640
And using 3D technology we are able to do this

26:18.640 --> 26:20.640
regardless of the light condition.

26:20.640 --> 26:23.640
So what we did was we create two layers.

26:23.640 --> 26:24.640
This is 2TAC, right?

26:24.640 --> 26:25.640
But we create two layers.

26:25.640 --> 26:27.640
One, using machine learning.

26:27.640 --> 26:30.640
So we can classify those facial expressions.

26:30.640 --> 26:33.640
And the second layer, we use some eristics.

26:33.640 --> 26:37.640
So I have never met before, but if we smile right now,

26:37.640 --> 26:40.640
I can tell that you are smiling, right?

26:40.640 --> 26:43.640
So we just touched algorithm to get back there.

26:43.640 --> 26:44.640
So we have two layers.

26:44.640 --> 26:46.640
One machine learning.

26:46.640 --> 26:48.640
And now we have our eristics.

26:48.640 --> 26:52.640
So we start to optimize this technology

26:52.640 --> 26:55.640
using the intention of open Vino.

26:55.640 --> 26:57.640
So now we can open Vino.

26:57.640 --> 27:00.640
So now we can have a deep learning layer.

27:00.640 --> 27:04.640
So 3 layers, machine learning, eristics,

27:04.640 --> 27:05.640
and also deep learning.

27:05.640 --> 27:09.640
And we can do this all offline, all offline,

27:09.640 --> 27:12.640
just using the knock here underneath the wheelchair.

27:16.640 --> 27:19.640
So here I can perform some facial expressions.

27:19.640 --> 27:20.640
Just turn on.

27:20.640 --> 27:22.640
I can't remember the numbers.

27:22.640 --> 27:24.640
I can't remember, however,

27:24.640 --> 27:26.640
you know, the numbers are there.

27:26.640 --> 27:28.640
They think like a background.

27:28.640 --> 27:30.640
It's left on the ground.

27:30.640 --> 27:32.640
A pretty smile to move forward.

27:32.640 --> 27:34.640
It's right there.

27:34.640 --> 27:36.640
I can't remember the numbers.

27:36.640 --> 27:38.640
Raised eyebrow.

27:38.640 --> 27:40.640
I can't remember.

27:40.640 --> 27:42.640
And even a dongle.

27:42.640 --> 27:44.640
I can't move backwards.

27:44.640 --> 27:46.640
So they're just taking the...

27:46.640 --> 27:47.640
Cool.

27:47.640 --> 27:48.640
Thank you.

27:48.640 --> 27:51.640
All right, everyone, that's our show for today.

27:51.640 --> 27:53.640
Be sure to like this video

27:53.640 --> 27:55.640
and smash that subscribe button down below.

27:55.640 --> 27:58.640
For more information on any of the awesome companies

27:58.640 --> 28:01.640
or technologies you saw today,

28:01.640 --> 28:06.640
visit twimmelai.com slash talk slash 2-2-2.

28:06.640 --> 28:09.640
As always, thanks so much for watching

28:09.640 --> 28:19.640
and catch you next time.

