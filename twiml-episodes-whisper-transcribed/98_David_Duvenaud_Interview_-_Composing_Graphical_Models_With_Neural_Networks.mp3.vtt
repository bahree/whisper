WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.640
I'm your host Sam Charrington.

00:23.640 --> 00:30.720
Last week I spent some time at CES in Las Vegas exploring the vast sea of drones, cameras,

00:30.720 --> 00:37.160
paper thin TVs, robots, laundry folding closets and other smart devices.

00:37.160 --> 00:39.800
You name it, it was there.

00:39.800 --> 00:44.600
Of course, I was also able to sit down with some really interesting people working on some

00:44.600 --> 00:48.000
pretty cool AI-enabled products.

00:48.000 --> 00:52.000
Head on over to our YouTube channel to check out some behind-the-scenes footage from my

00:52.000 --> 00:55.480
interviews and other quicktakes from the show.

00:55.480 --> 01:01.000
And of course, be on the lookout for our AI and consumer electronic series right here

01:01.000 --> 01:03.800
on the podcast coming soon.

01:03.800 --> 01:10.240
A quick note, this is your final reminder about tomorrow's Twimble Online Meetup.

01:10.240 --> 01:15.320
At 3pm Pacific, we'll be joined by Microsoft Research's Timnett Gebru.

01:15.320 --> 01:19.760
Timnett will be joining us to discuss her paper using deep learning and Google Street

01:19.760 --> 01:24.560
View to estimate the demographic makeup of neighborhoods across the United States.

01:24.560 --> 01:28.200
And I'm especially looking forward to her going into more detail about the pipeline she

01:28.200 --> 01:33.840
used to identify 22 million cars and 50 million street-view images.

01:33.840 --> 01:37.760
As usual, we'll get the meetup kicked off with the discussion segment in which we'll

01:37.760 --> 01:43.080
be exploring your AI resolutions and predictions for 2018.

01:43.080 --> 01:47.960
For links to the paper, to register for the meetup or to check out previous meetups,

01:47.960 --> 01:51.560
visit twimbleai.com slash meetup.

01:51.560 --> 01:56.080
The show you're about to hear is part of a series of shows recorded at the rework,

01:56.080 --> 01:59.840
deep learning summit in Montreal, back in October.

01:59.840 --> 02:06.440
This was a great event, and in fact, their next event, the deep learning summit San Francisco

02:06.440 --> 02:11.400
is right around the corner on January 25th and 26th.

02:11.400 --> 02:15.800
The summit will feature more leading researchers and technologists like the ones you'll hear

02:15.800 --> 02:21.480
here on the show this week, including Ian Goodfellow of Google Brain and Daphne Kohler

02:21.480 --> 02:24.240
of Calico Labs and more.

02:24.240 --> 02:30.560
Definitely check it out and use the code Twimbleai for 20% off of registration.

02:30.560 --> 02:35.560
In this episode, we hear from David Duveno, Assistant Professor in the Computer Science

02:35.560 --> 02:39.480
and Statistics Department at the University of Toronto.

02:39.480 --> 02:43.880
David joined me after his talk at the deep learning summit on composing graphical models

02:43.880 --> 02:49.320
with neural networks for structured representations and fast inference.

02:49.320 --> 02:53.960
In our conversation, we discuss the generalized modeling and inference framework that David

02:53.960 --> 02:58.680
and his team have created, which combines the strengths of both probabilistic graphical

02:58.680 --> 03:02.000
models and deep learning methods.

03:02.000 --> 03:06.880
He gives us a walkthrough of his use case, which is to automatically segment and categorize

03:06.880 --> 03:12.640
mouse behavior from raw video, and we discuss how the framework is applied to this and other

03:12.640 --> 03:14.600
use cases.

03:14.600 --> 03:21.360
We also discuss some of the differences between the frequentist and Bayesian statistical approaches.

03:21.360 --> 03:25.440
I had a great time with this interview, and I think you will too.

03:25.440 --> 03:27.960
And now on to the show.

03:27.960 --> 03:39.480
Alright everyone, so I am here at the rework deep learning conference in Montreal, and

03:39.480 --> 03:45.240
I am with David Duveno, David's a professor at the University of Toronto, and he actually

03:45.240 --> 03:50.200
just got off stage, and I'm excited to be sitting here with him, or here with you.

03:50.200 --> 03:52.800
I should say David, welcome to the podcast.

03:52.800 --> 03:53.800
Thank you, Sam.

03:53.800 --> 03:58.200
So, tell me a little bit about your background and how you got interested in machine learning

03:58.200 --> 03:59.200
in AI.

03:59.200 --> 04:01.840
It sounds like you come a little bit from the statistics side of the world.

04:01.840 --> 04:04.840
No, I've actually been sort of maybe pushed into this.

04:04.840 --> 04:05.840
That's right, man.

04:05.840 --> 04:10.320
I'm half-appointed since that's the University of Toronto.

04:10.320 --> 04:11.320
And it's really nice.

04:11.320 --> 04:15.480
No, actually the way I got into this was actually reading JÃ¼rgen Spadeuber's web page when

04:15.480 --> 04:20.720
I was in undergrad, and he had, I think, you know, to his credit, he really was thinking

04:20.720 --> 04:26.600
about a lot of things that people, you know, later also found interesting, and he has

04:26.600 --> 04:29.160
a way of presenting his ideas as sort of like the last word.

04:29.160 --> 04:32.360
I mean, everyone sort of does, but at the time I didn't really know how to evaluate these

04:32.360 --> 04:35.360
things, and I thought, oh man, I need to go to grad school, because this guy is going

04:35.360 --> 04:37.600
to solve AI in like two years or so.

04:37.600 --> 04:38.600
So, yeah.

04:38.600 --> 04:42.680
I mean, of course, now I have a much broader view, and I feel like if you go to grad school

04:42.680 --> 04:43.680
in Switzerland?

04:43.680 --> 04:44.680
No, no.

04:44.680 --> 04:48.680
Although I certainly, like, actually went to grad school at the University of British Columbia.

04:48.680 --> 04:49.680
Okay.

04:49.680 --> 04:50.680
Yeah.

04:50.680 --> 04:53.320
Initially, all my friends went to grad school, and my first time I applied for scholarships

04:53.320 --> 04:56.240
I got rejected, and so I had to be chip on my shoulder, and I was like grad school

04:56.240 --> 04:57.240
is for jerks.

04:57.240 --> 05:02.160
But yeah, then eventually I applied again and got it, and yeah, I went to work with Kevin

05:02.160 --> 05:07.760
Murphy back when graphical models were still cool, and I did a bit of machine vision and

05:07.760 --> 05:08.760
things like that.

05:08.760 --> 05:09.760
Okay.

05:09.760 --> 05:10.760
Yeah.

05:10.760 --> 05:13.800
And then he actually set me up with an internship at Google.

05:13.800 --> 05:15.040
It wasn't called Google Brain Back then.

05:15.040 --> 05:19.200
It was actually the video content analysis team, but it was all the same people that ended

05:19.200 --> 05:20.200
up forming it.

05:20.200 --> 05:24.560
And we just sort of tried to get confidence to predict whether YouTube videos contained

05:24.560 --> 05:28.440
people dancing for this content, so that YouTube was running at the time.

05:28.440 --> 05:29.440
Interesting.

05:29.440 --> 05:30.440
Interesting.

05:30.440 --> 05:32.880
And so you're now at University of Toronto?

05:32.880 --> 05:33.880
Yes.

05:33.880 --> 05:35.360
How long have you been at the University?

05:35.360 --> 05:36.720
Just a year actually.

05:36.720 --> 05:37.720
Okay.

05:37.720 --> 05:42.040
And you said it, you've kind of been pushed into this joint appointment.

05:42.040 --> 05:43.040
Well, okay.

05:43.040 --> 05:44.040
Wow.

05:44.040 --> 05:45.040
I'm not.

05:45.040 --> 05:50.880
I mean, I started off being interested in machine learning, and then went and did my PhD on

05:50.880 --> 05:55.680
Beijing Non-Priorometrics, and I realized that sort of math was a missing component.

05:55.680 --> 06:00.600
You know, like everyone in machine learning has to be able to code and do math, but you

06:00.600 --> 06:03.040
really can't get away without knowing the math.

06:03.040 --> 06:06.520
And really a little bit of formalism can go a long way.

06:06.520 --> 06:12.000
And especially recently, I've been working on building gradient estimators for discrete

06:12.000 --> 06:13.160
latent variable models.

06:13.160 --> 06:17.840
And this is also shows up in reinforcement learning where having unbiased gradient estimators

06:17.840 --> 06:19.880
is actually really important.

06:19.880 --> 06:23.800
And unbiased and it sounds like this boring horrible thing that frequentes statisticians

06:23.800 --> 06:24.800
care about.

06:24.800 --> 06:28.920
But it actually now, I think, is going to turn out to be sort of a central thing that

06:28.920 --> 06:32.120
people are going to be worrying about over the next few years.

06:32.120 --> 06:39.080
And so you just did your talk, and your talk was on combining graphical models with deep

06:39.080 --> 06:40.080
learning models.

06:40.080 --> 06:41.080
Yeah.

06:41.080 --> 06:45.920
Tell us a little bit about the talk as an overview, and we can kind of dive into the different

06:45.920 --> 06:46.920
elements of it.

06:46.920 --> 06:47.920
Sure.

06:47.920 --> 06:49.880
Well, the way that paper that came about was actually pretty fun.

06:49.880 --> 06:55.480
So I just did a postdoc at Harvard with this guy, Matt Johnson, who is now at Google Brain.

06:55.480 --> 07:00.520
And when I showed up, I had just sort of de-converted from Bayesian non-parametrics where I was

07:00.520 --> 07:04.400
trying to fit giant, infinite dimensional graphical models to everything.

07:04.400 --> 07:08.440
And sort of I had sort of realized the limitations of these approaches.

07:08.440 --> 07:13.800
And I was really excited about variational autoencoders where we learn a generative model

07:13.800 --> 07:14.920
using a neural network.

07:14.920 --> 07:18.880
And we also use a neural network to learn to do inference in that model.

07:18.880 --> 07:22.720
And sort of as I said in the talk, people have been able to write down really rich generative

07:22.720 --> 07:24.360
models for a long time.

07:24.360 --> 07:27.720
But they haven't been able to do inference in them, and that's sort of the limiting factor.

07:27.720 --> 07:28.720
Okay.

07:28.720 --> 07:32.640
With regards to this paper, Matt was coming from a sort of also old school background where

07:32.640 --> 07:37.800
he was saying, oh, I want to use graphical models, linear dynamic systems, like nice models

07:37.800 --> 07:39.880
that we can analyze and understand.

07:39.880 --> 07:42.080
And I was telling him, no, no, no, you've got to forget all of that.

07:42.080 --> 07:44.400
You have to just use neural networks to do inference.

07:44.400 --> 07:46.400
We can fit everything with a variational autoencoder.

07:46.400 --> 07:49.280
And we had this long back and forth over the course of about a year.

07:49.280 --> 07:52.360
In the end, it was kind of like my chocolate and your peanut butter.

07:52.360 --> 07:54.480
And we had this nice synthesis of ideas.

07:54.480 --> 07:57.720
And we sort of said, oh, maybe we can combine these and get sort of the best of both worlds.

07:57.720 --> 07:58.720
Okay.

07:58.720 --> 08:01.720
So when you say graphical model, what do we mean with that?

08:01.720 --> 08:02.720
Yeah.

08:02.720 --> 08:03.720
Let me mean by that.

08:03.720 --> 08:04.720
So that's a pretty broad phrase.

08:04.720 --> 08:09.280
It basically just means it doesn't mean graphics as we traditionally talk about graphics.

08:09.280 --> 08:10.280
That's a starting place.

08:10.280 --> 08:13.000
So the graphical model means like graphs.

08:13.000 --> 08:15.800
And the idea is that initially when people started writing down models, they just wrote

08:15.800 --> 08:16.800
down equations.

08:16.800 --> 08:19.080
I would say what the probability of different things were.

08:19.080 --> 08:21.920
And it was pretty hard to analyze these models.

08:21.920 --> 08:28.680
And so, you know, I think Kevin Murphy, Daphne Collar, near Friedman, these guys said,

08:28.680 --> 08:33.880
what if we actually represent the dependence of different variables as a graph?

08:33.880 --> 08:37.960
And you know, the idea is that if I say that, you know, like smoking causes cancer, then

08:37.960 --> 08:40.960
I would have an arrow going from smoking to cancer.

08:40.960 --> 08:44.840
And this says something about how the probability of these things change together.

08:44.840 --> 08:48.920
So if I get cancer, that doesn't make me smoke, but if I get smoke, I make me get cancer.

08:48.920 --> 08:52.680
And once we start to have, you know, ten or a hundred different variables, keeping

08:52.680 --> 08:57.280
track of all these arrows or these relationships becomes pretty tricky.

08:57.280 --> 09:02.120
But when we express these models as a graph, we can automatically analyze them and ask, you

09:02.120 --> 09:08.760
know, whether we think that like, you know, drinking wine causes, you know, better like test

09:08.760 --> 09:10.960
scores through some complicated mechanism.

09:10.960 --> 09:14.480
Or we can also ask, you know, what information would we need to learn in order to tell us

09:14.480 --> 09:15.480
what the answer is?

09:15.480 --> 09:16.480
Yeah.

09:16.480 --> 09:20.080
And so this is sort of this, it's not quite old fashioned AI, but it's sort of this, like,

09:20.080 --> 09:25.960
we're going to have these rational, understandable models where every sort of human concept has

09:25.960 --> 09:29.200
a little box that we put it in and we try to understand everything that's happening in

09:29.200 --> 09:30.200
our models.

09:30.200 --> 09:31.200
Okay.

09:31.200 --> 09:32.200
Okay.

09:32.200 --> 09:37.160
So when we talk about graphs and applying graphs to these types of models, so one of the

09:37.160 --> 09:41.600
things that I think of at least is the, some of the deep learning frameworks like TensorFlow

09:41.600 --> 09:46.600
allow you to create neural network architectures using graphs, but that's like at a higher level

09:46.600 --> 09:48.000
than what you're talking about, right?

09:48.000 --> 09:49.000
Yeah.

09:49.000 --> 09:53.680
So that's kind of a confusingly similar idea because, you know, we write down these

09:53.680 --> 09:57.760
computation graphs where we have these arrows that mean A is a function of B. And that's

09:57.760 --> 10:01.640
also the same sort of relationship we're talking about when we write down graphical models.

10:01.640 --> 10:05.480
But in graphical models, the relationships are all probabilistic, like we're not saying

10:05.480 --> 10:09.480
that A determines B, we're saying the probability of B depends on A.

10:09.480 --> 10:10.480
Okay.

10:10.480 --> 10:14.560
And then the thing we can do with graphical models that we can't do with neural networks

10:14.560 --> 10:18.960
so easily is to go backwards and say, you know, given that someone has cancer, what is

10:18.960 --> 10:23.480
the chance that they smoked, which is something, you know, you can't, well, it's not as easy

10:23.480 --> 10:25.640
to sort of run a neural network backwards.

10:25.640 --> 10:30.080
But when we run a graphical model backwards, we're sort of asking what are the hidden causes

10:30.080 --> 10:32.440
of the things that we observed?

10:32.440 --> 10:36.120
And this is this, this is what we tend to refer to as like the inference problem.

10:36.120 --> 10:37.120
Okay.

10:37.120 --> 10:41.440
How do we figure out what, like, how does what we saw change what we believe about what

10:41.440 --> 10:43.320
we didn't see?

10:43.320 --> 10:48.240
And you know, this, as an example, you can even view, for instance, like learning grammar,

10:48.240 --> 10:51.760
like when babies learn language, they hear all these sentences and there's this hidden

10:51.760 --> 10:55.800
thing, which is, you know, grammar and vocabulary and all these rules about how language goes

10:55.800 --> 10:56.800
together.

10:56.800 --> 11:01.360
And so the problem of hearing a bunch of sentences and then trying to figure out which

11:01.360 --> 11:05.640
is the likely rules of that language is an inference problem.

11:05.640 --> 11:09.960
And so this is kind of why people have been really excited about these generative models

11:09.960 --> 11:12.960
are like also called latent variable models for a long time.

11:12.960 --> 11:13.960
Yeah.

11:13.960 --> 11:18.440
Like the idea is the grammar is this latent unseen variable that we have to infer.

11:18.440 --> 11:23.320
And for instance, and also this motivates why people have been so excited by inference.

11:23.320 --> 11:27.560
So if you go to nips, there's like the advances in approximate vision inference workshop

11:27.560 --> 11:29.280
and, you know, variance of it.

11:29.280 --> 11:34.320
And it's one of my favorite parts of nips because it sounds really boring and dry.

11:34.320 --> 11:40.280
But, you know, this, the inference problem is kind of the bottleneck for doing all the

11:40.280 --> 11:44.960
cool things that babies can do that we can't do with, at least, that's at least one bottleneck.

11:44.960 --> 11:48.280
You know, even if we solve inference, there's probably more problems that we need to solve.

11:48.280 --> 11:53.160
But for instance, people like Josh Tenenbaum and MIT for, you know, now like 20 years or

11:53.160 --> 11:57.640
so heavy to get him on the show, his name has come up probably like three times just

11:57.640 --> 11:58.640
today.

11:58.640 --> 11:59.640
Yeah.

11:59.640 --> 12:03.120
And he's just a really inspiring person to talk to you because he really saw this vision,

12:03.120 --> 12:07.760
you know, a long time ago that, guys, guys, guys, if we could just, you know, figure out

12:07.760 --> 12:12.600
how to do inference, we would be able to not only explain like how humans do all these

12:12.600 --> 12:15.440
things, but also get machines to do them ourselves.

12:15.440 --> 12:17.840
So he's been, you know, looking into these for a long time.

12:17.840 --> 12:22.400
And I think actually all that I would be sure to keep an eye on the stuff that's coming

12:22.400 --> 12:27.360
out of his, his lab because inference methods have just been making, like these major leaps

12:27.360 --> 12:30.440
and bounds in the last three or four years.

12:30.440 --> 12:34.920
You know, I think inference methods is maybe another, like name collision because we refer

12:34.920 --> 12:41.920
to inference as kind of using models generally, but again, we're talking about something different

12:41.920 --> 12:42.920
here.

12:42.920 --> 12:45.280
We're talking about, yeah, maybe I would say probabilistic inference.

12:45.280 --> 12:46.280
Yeah.

12:46.280 --> 12:47.880
I agree that this word is completely overloaded.

12:47.880 --> 12:52.640
And also in frequent statistics, it has another meaning in frequentistic, what is frequentist

12:52.640 --> 12:53.640
statistics?

12:53.640 --> 12:57.840
Oh, well, I can even say that a lot of long back, so I would call myself a vision.

12:57.840 --> 13:03.040
So it's kind of like asking, you know, like a liberal to describe a Republican or like

13:03.040 --> 13:05.320
a Catholic described a Protestant or something.

13:05.320 --> 13:11.520
But you know, they're interested in like worst case guarantees, computing p-values, doing

13:11.520 --> 13:16.360
hypothesis testing, and basically making procedures that can tell us, you know, what does the

13:16.360 --> 13:21.760
data say about this particular question, regardless of what we happen to think before.

13:21.760 --> 13:29.680
And then the division side says, well, let's actually ask how to combine what we saw today

13:29.680 --> 13:30.880
with what we knew before.

13:30.880 --> 13:34.000
I mean, actually, that's the standard answer.

13:34.000 --> 13:39.120
To me, the real answer is, visions consider all possibilities, and they just keep all them

13:39.120 --> 13:41.960
around and weight them all equally, or not equally, weight them according to how well

13:41.960 --> 13:42.960
they fit the data.

13:42.960 --> 13:47.760
And frequentists try to identify like the best possible hypothesis.

13:47.760 --> 13:53.200
And these are like good reasons to do both, these are very different schools of thought.

13:53.200 --> 13:58.400
And you know, there's been this unfortunate bit of tribalism, I think, where people naturally

13:58.400 --> 14:02.280
tend to like to form groups, and then, yeah, so that has definitely happened, and that's

14:02.280 --> 14:05.120
been a thing for like 70 years, since it's six now.

14:05.120 --> 14:12.280
So yeah, maybe a digression, are there a set of things that you think of about statistics

14:12.280 --> 14:17.960
that you wish more people doing, machine learning, or deep learning, knew or understood better?

14:17.960 --> 14:21.120
Okay, well, I'm going to take that as a yes.

14:21.120 --> 14:25.960
There's like a little rant that I've been wanting to go on for a long time, which even

14:25.960 --> 14:29.080
like Princess at Harvard, when they were teaching machine learning, someone said, what's

14:29.080 --> 14:30.800
the difference between a frequentist and a vision?

14:30.800 --> 14:36.320
And then they said, oh, the frequentist treats the data as a random variable, and the

14:36.320 --> 14:39.320
vision treats the truth as a random variable.

14:39.320 --> 14:44.680
That is sort of technically what is happening, but it's just like a bizarre mis-framing

14:44.680 --> 14:47.160
of the entire discussion.

14:47.160 --> 14:51.600
I mean, like a random variable is sort of not necessarily random, and it's not necessarily

14:51.600 --> 14:56.880
a variable, it's like a very bad name, but the idea is that, you know, the frequentist

14:56.880 --> 15:01.040
is going to say, you know, if this thing was the case, how likely would I have been to

15:01.040 --> 15:02.720
see the data that I saw?

15:02.720 --> 15:06.280
That's a sort of analysis that is often done, I mean, there's all sorts of methods, and

15:06.280 --> 15:10.960
the vision will say, given that I did see this data, what do I believe about the truth?

15:10.960 --> 15:14.760
Even though I think the truth is fixed, and it's not random, because I don't know it,

15:14.760 --> 15:19.800
I can use probability to describe my state of uncertainty, and that doesn't mean that

15:19.800 --> 15:20.800
I think it's random.

15:20.800 --> 15:24.800
That just means I am using probability to describe my uncertainty.

15:24.800 --> 15:25.800
Right.

15:25.800 --> 15:26.800
Okay.

15:26.800 --> 15:27.800
Yeah, so that's my public service announcement.

15:27.800 --> 15:28.800
Okay.

15:28.800 --> 15:34.040
And so is there, you know, so that's kind of describing these two tribes.

15:34.040 --> 15:39.520
Are there ways that you see those two kind of schools of thought influencing the way

15:39.520 --> 15:44.080
folks approach, you know, machine learning and AI that, you know, particularly that you

15:44.080 --> 15:50.680
think, you know, a little bit more kind of commonality or something would kind of advance

15:50.680 --> 15:51.680
us as a community?

15:51.680 --> 15:57.840
Well, it's kind of funny, because deep learning has represented a sort of de facto sort

15:57.840 --> 16:02.720
of third direction, or maybe synthesis, and these debates about vision and frequentist

16:02.720 --> 16:09.520
methods, I think, really took a backseat to saying, let's just define a probabilistic model

16:09.520 --> 16:13.080
that will give us a continuous loss function that we can optimize and use gradient-based

16:13.080 --> 16:17.320
optimization to do maximum likelihood estimation.

16:17.320 --> 16:22.680
And deep learning in a nutshell, and this is sort of like takes elements from both.

16:22.680 --> 16:26.680
Now, of course, that people are saying, how do we, you know, train deep learning models

16:26.680 --> 16:28.000
with less data?

16:28.000 --> 16:30.920
People are looking more into vision, deep learning, which actually can be made to look

16:30.920 --> 16:33.640
a lot like standard deep learning, where you just add a little bit of noise to everything,

16:33.640 --> 16:35.040
which is really nice.

16:35.040 --> 16:39.720
Yeah, I guess the thing is that most of the classic, like, frequentist methods are based

16:39.720 --> 16:44.080
on taking really simple methods that are easy to analyze and prove things about, just

16:44.080 --> 16:50.080
sort of developing those, whereas for neural networks, you can't really say much about them

16:50.080 --> 16:55.960
in these sort of like hard-bound proving asymptotic sense that you could with like maximum likelihood

16:55.960 --> 16:58.160
estimation or, well, okay, sounds.

16:58.160 --> 17:02.080
Let's say with, like, the sort of simple estimators that people like to use in science.

17:02.080 --> 17:08.280
Yeah, so people used to love these frequentist estimators because they were simple and fast.

17:08.280 --> 17:12.720
And now we sort of accepted that if you pay this price of having a little bit more complex

17:12.720 --> 17:17.400
models and like maybe a GPU or something, you're going to get better enough performance

17:17.400 --> 17:21.480
that you'll just forget about any asymptotic guarantees that the other models might have

17:21.480 --> 17:22.480
had.

17:22.480 --> 17:24.760
What are some examples of frequentist models?

17:24.760 --> 17:27.080
Oh, frequentist methods, let's see.

17:27.080 --> 17:34.040
So there was this whole cottage industry of frequentist kernel methods, let's define

17:34.040 --> 17:40.720
a non-parametric estimator by considering all possible functions in some infinite dimensional

17:40.720 --> 17:48.360
Hilbert space that could possibly separate our data or explain it or model its density.

17:48.360 --> 17:52.840
And you know, these methods still have a place and I think one of the, like, oral presentations

17:52.840 --> 17:57.200
that nips the series is on these methods, so they're not like gone, but they're sort of

17:57.200 --> 18:01.760
definitely in a little bit of, there's definitely like a kernel winter happening right now.

18:01.760 --> 18:02.760
Okay.

18:02.760 --> 18:10.640
So then the, in fact, your presentation was talking about in some sense how to combine elements

18:10.640 --> 18:13.400
of both of these schools of thought?

18:13.400 --> 18:14.760
Yeah, exactly.

18:14.760 --> 18:18.680
So I think when I got to Harvard, I was sort of like one of these people who recently

18:18.680 --> 18:23.000
de-converts from a religion and they just had nothing about the fantasy about it and Matt

18:23.000 --> 18:26.520
was sort of, you know, saying, well, are you sure you want to, you know, really throw

18:26.520 --> 18:27.520
this all out?

18:27.520 --> 18:31.320
And yeah, and we really did have this problem of analyzing this, this most data and they

18:31.320 --> 18:36.200
had fit just like the pure graphical model sort of standard approach to the data and it

18:36.200 --> 18:40.120
had a, had a bunch of problems with, you know, well, let's hit pause there and talk about

18:40.120 --> 18:41.120
the data.

18:41.120 --> 18:42.120
Sure.

18:42.120 --> 18:43.520
So that we're all on the same page.

18:43.520 --> 18:44.520
Sure.

18:44.520 --> 18:49.480
So the data is a bunch of connect video of mice running around in the dark and the idea

18:49.480 --> 18:54.120
is that when biologists want to measure what happens when we, you know, change the genes

18:54.120 --> 18:59.680
of a mouse or give it a drug or, you know, expose it to like the odor of a fox or whatever,

18:59.680 --> 19:02.360
they need to quantify how it's behavior has changed.

19:02.360 --> 19:06.080
So they can write about how, you know, our model of autistic mice do this more often, but

19:06.080 --> 19:09.600
then when we give them the drug, they act more normally or something like that.

19:09.600 --> 19:15.080
And I couldn't tell from the video whether that was kind of top down or like through a

19:15.080 --> 19:18.600
glass floor looking up or something like a top down, okay.

19:18.600 --> 19:24.040
So the idea is that right now they have a army of grad students who spend thousands of

19:24.040 --> 19:28.600
hours watching this video and then saying, okay, and now the mouse, you know, ate something

19:28.600 --> 19:32.200
and now he ran over there and now he stood up and now he went over to his buddy.

19:32.200 --> 19:38.000
And you know, this is cruel both to these students and it's also introduces sort of variability

19:38.000 --> 19:43.680
between different people who might, you know, labeling error or just differences, right?

19:43.680 --> 19:47.320
It's hard to like canonically say like, okay, this is a mouse that is, you know, grooming

19:47.320 --> 19:48.320
or not, right?

19:48.320 --> 19:53.920
And also even the labeling noise more so than error, maybe, it's different interpretations

19:53.920 --> 19:56.160
of what the mouse is doing at a given time.

19:56.160 --> 19:57.160
Yeah, exactly.

19:57.160 --> 20:01.200
And really, you know, we can imagine this changing systematically across labs, maybe in

20:01.200 --> 20:05.160
different countries, maybe it's hard, there's like a language barrier, maybe eat something

20:05.160 --> 20:06.160
so they're different, right?

20:06.160 --> 20:11.240
So we'd really like to automate this part of the scientific pipeline, both just because

20:11.240 --> 20:16.080
it will save the grad students time, but also because it'll, you know, help us do better

20:16.080 --> 20:20.640
science by removing one or by standardizing one part of the entire pipeline.

20:20.640 --> 20:21.640
Mm-hmm.

20:21.640 --> 20:27.640
So you have this data, was there, were there a fixed number of classes or activities that

20:27.640 --> 20:33.840
you were trying to capture or was part of the challenge trying to identify, you know,

20:33.840 --> 20:35.960
how many fixed sets of activities there were?

20:35.960 --> 20:36.960
Yeah.

20:36.960 --> 20:37.960
Great question.

20:37.960 --> 20:42.280
So we really wanted to make sure that we didn't have to tell it exactly how many different

20:42.280 --> 20:46.320
classes of activity that were, because that we kind of defeat the purpose and it would

20:46.320 --> 20:50.640
also sort of make you wonder, well, what if I had given it one more class or one less,

20:50.640 --> 20:52.800
what would it, the results have been totally different?

20:52.800 --> 20:58.720
So this is kind of one of the benefits of being patient is that you can compare the model

20:58.720 --> 21:01.680
fit in a systematic way between different models.

21:01.680 --> 21:08.520
So we, what we did was we said, okay, there are up to, I think we chose like 40 different

21:08.520 --> 21:09.720
clusters.

21:09.720 --> 21:17.120
And the idea was that we, 40 clusters are 40 classes within those, well, classes and

21:17.120 --> 21:19.880
clusters are the same thing in the way that I'm talking about this.

21:19.880 --> 21:20.880
Okay.

21:20.880 --> 21:21.880
Sorry.

21:21.880 --> 21:25.800
And then the idea is that we could let the model choose how often different activities

21:25.800 --> 21:29.080
appeared and some of them it would just never use.

21:29.080 --> 21:33.280
And the idea was that we tell there are at least 40 clusters and it will say, I can explain

21:33.280 --> 21:35.880
this, this data with only 20.

21:35.880 --> 21:38.160
And so, you know, the other 20 just stay off forever.

21:38.160 --> 21:40.800
So we're automatically learning the number of clusters.

21:40.800 --> 21:44.920
We just have to make sure we have a good upper bound on how many there could possibly be.

21:44.920 --> 21:45.920
And I'm sorry.

21:45.920 --> 21:50.560
So you make clusters in a sense of cluster data points is that that will define a class.

21:50.560 --> 21:53.400
Is that the right way to think about this or that's a really good point.

21:53.400 --> 21:58.120
So I guess I mean clusters in a more abstract sense, where we're clustering the dynamics

21:58.120 --> 22:00.440
of the mouse movement.

22:00.440 --> 22:03.760
The idea of being that a behavior is not, you know, a particular pose and the most

22:03.760 --> 22:04.760
be it.

22:04.760 --> 22:07.840
It's not a particular pose that the mouse might be in.

22:07.840 --> 22:12.760
But it's the way that he moves from one pose to another or, you know, when he's grooming

22:12.760 --> 22:16.040
he's like, you know, moving in this sort of circular motion or something like that.

22:16.040 --> 22:22.400
So I mean a cluster in the dynamics is one behavior that he could be doing.

22:22.400 --> 22:23.400
Okay.

22:23.400 --> 22:28.400
So how many, how many of these clusters ended up being identified?

22:28.400 --> 22:34.480
To be honest, I forget and I think it was around 20, but so I just have to say Matt was

22:34.480 --> 22:37.760
the first author and he's the one who really spent a lot of time with the data.

22:37.760 --> 22:38.760
Okay.

22:38.760 --> 22:39.760
Okay.

22:39.760 --> 22:45.240
And so how did, how did the graph, the graphical analysis or the graphical element of this

22:45.240 --> 22:46.240
play in?

22:46.240 --> 22:47.240
Yeah.

22:47.240 --> 22:51.400
So the alternative, the baseline that we could have done would just be to say that there's

22:51.400 --> 22:56.280
some recurrent neural network that defines how the mouse sort of changes through time.

22:56.280 --> 23:01.600
And then we'll have some continuous factor that is changing and we don't necessarily really

23:01.600 --> 23:04.640
know what that can you expect our means.

23:04.640 --> 23:08.240
And so that would have probably fit the data pretty well.

23:08.240 --> 23:12.560
We would have been able to like predict the mouse's future movements.

23:12.560 --> 23:14.400
I think about as well.

23:14.400 --> 23:18.840
But we wouldn't have been able to look in and say, oh, there's these distinct clusters.

23:18.840 --> 23:22.680
So that was sort of the home motivation was the interpretability of this model.

23:22.680 --> 23:27.520
So what is the, what's the process for building a graphical model?

23:27.520 --> 23:33.360
Like are you literally identifying states and transition vectors and things, what's things

23:33.360 --> 23:34.360
like that?

23:34.360 --> 23:36.200
Or is it more abstract and mathematical?

23:36.200 --> 23:41.480
Well, we try to make it as abstract as possible because we want to let the data speak for

23:41.480 --> 23:43.640
itself as much as possible.

23:43.640 --> 23:47.760
So all we did was we said, you know, there's 40 different states, the musket being, and

23:47.760 --> 23:51.080
there's some probability of transitioning from each one to each other one.

23:51.080 --> 23:52.160
And we don't know what that is.

23:52.160 --> 23:54.200
So the model has to learn that.

23:54.200 --> 23:59.520
It also has to learn how those states influence the dynamics and it also has to learn how those,

23:59.520 --> 24:04.000
like the mouse's body state corresponds to actual video frames.

24:04.000 --> 24:08.000
So the idea is that all we basically said is there's some discrete stuff that controls

24:08.000 --> 24:12.200
some continuous stuff, that controls some video stuff.

24:12.200 --> 24:15.960
And all the connections between those things and the connections through time had to be

24:15.960 --> 24:17.840
learned automatically.

24:17.840 --> 24:23.640
And did you end up finding that I'm thinking about like the density or sparsity of the

24:23.640 --> 24:28.920
connection graph, like does that play a significant role in this or what did it end up kind

24:28.920 --> 24:29.920
of looking like?

24:29.920 --> 24:30.920
Great question.

24:30.920 --> 24:34.720
So I do think that mouse's behavior transition is probably our sparse.

24:34.720 --> 24:40.320
Like maybe he never goes from eating to like standing up right away or something like that.

24:40.320 --> 24:46.240
We didn't actually put any capacity for the model to, or we didn't sort of put any prime

24:46.240 --> 24:49.640
information about whether the transition matrix was sparse or not.

24:49.640 --> 24:55.080
To be honest, we didn't look at how sparse the learned matrix was, but I bet it was sparse.

24:55.080 --> 24:56.080
Yeah.

24:56.080 --> 24:59.440
So these are the sorts of like refinements that we would generally like to make.

24:59.440 --> 25:03.520
And actually I want to say these are the sorts of refinements that we would like the,

25:03.520 --> 25:06.440
our learning algorithm to be able to propose on its own.

25:06.440 --> 25:10.280
So at the end of the talk, I sort of said, so right now we built.

25:10.280 --> 25:12.440
The model, but you know, what if we got it wrong?

25:12.440 --> 25:16.360
What if mouse behavior isn't discrete or what if, you know, when there's two mice involved,

25:16.360 --> 25:20.280
there's some more complicated structure that, you know, we just don't understand most

25:20.280 --> 25:22.640
sociology so we don't even know how to write it down.

25:22.640 --> 25:28.120
So what we'd really like to do is try learning both all the parameters of these models and

25:28.120 --> 25:30.480
which types of structure they should have as well.

25:30.480 --> 25:33.840
And there's no sort of technical reason why we can't do this.

25:33.840 --> 25:38.040
It's just that it requires searching over this discrete space, which is sort of always

25:38.040 --> 25:39.040
a big pain.

25:39.040 --> 25:43.840
And are there things that, are there things kind of happening in the field that you think

25:43.840 --> 25:46.400
will enable you to do that?

25:46.400 --> 25:50.240
Like is it just going to be brute force, you know, a better compute or are there, you

25:50.240 --> 25:56.400
know, folks doing research that you think will lend themselves to, or what are the research

25:56.400 --> 25:58.840
areas that will lend themselves to figuring this out?

25:58.840 --> 25:59.840
Right.

25:59.840 --> 26:00.840
So I'm glad you asked.

26:00.840 --> 26:05.600
So I am perfectly think that in the next few years we're going to see a lot of progress

26:05.600 --> 26:11.840
in models of how to fit discrete models or methods of discrete, sorry, I think we're going

26:11.840 --> 26:16.000
to see a lot of progress on methods to fit discrete models.

26:16.000 --> 26:20.320
And sort of the deep learning revolution has basically been all about continuous everything.

26:20.320 --> 26:24.560
So we have continuous parameters, you know, continuous predictions, we have a latent

26:24.560 --> 26:25.560
variable model.

26:25.560 --> 26:27.480
The latent variables are mostly continuous.

26:27.480 --> 26:31.320
The stuff that I talked about today was just like a tiny step we added like one sort

26:31.320 --> 26:33.960
of easy to handle discrete latent variable.

26:33.960 --> 26:38.200
But really the stuff that I think is more interesting is going back to the grammar example, like

26:38.200 --> 26:43.120
learning an entire grammar on a language or even learning a parse tree for a given sentence

26:43.120 --> 26:47.520
is this complicated discrete object that, you know, you can't even say it's like one

26:47.520 --> 26:51.680
out of a hundred possibilities is actually like these entire trees of rules.

26:51.680 --> 26:56.720
And so there still isn't a much better way to handle these sorts of models than we had,

26:56.720 --> 26:58.480
you know, 10 years ago.

26:58.480 --> 27:02.600
So the thing, so one thing that I've been really excited about and getting my students to

27:02.600 --> 27:09.920
work on is how do we find sort of continuous relaxations or gradient estimators for models

27:09.920 --> 27:12.120
with discrete latent structure?

27:12.120 --> 27:16.720
And this is going to let us, I mean, if it works, do all sorts of things like learn hard

27:16.720 --> 27:21.560
attention models, train gans to generate text, you know, let's see.

27:21.560 --> 27:26.480
Yeah, as I said, learn grammars, learn models with these interpretable latent structures,

27:26.480 --> 27:28.080
learn to do things like produce programs.

27:28.080 --> 27:32.120
I mean, obviously none of this stuff is going to work out of the box.

27:32.120 --> 27:38.200
But again, gradient based estimation is sort of a really, really great method because it

27:38.200 --> 27:42.360
scales to, you know, millions of parameters in a way that something like evolutionary

27:42.360 --> 27:44.200
algorithms, I think, never will.

27:44.200 --> 27:47.680
So there might be another way forward, but the way forward I'm excited about is trying

27:47.680 --> 27:51.600
to get good gradient estimators for models with the discrete structure.

27:51.600 --> 27:52.600
Okay.

27:52.600 --> 28:00.280
And just so I can make sure I understand the single latent variable in the example that

28:00.280 --> 28:09.360
you presented is the cluster that the mouse is in at a given time, and it's discreet

28:09.360 --> 28:14.360
because, you know, it's a cluster you kind of quantized it inherently, exactly.

28:14.360 --> 28:18.000
Are there other formulations of that same, I mean, I, there are lots of formulations

28:18.000 --> 28:21.360
of that problem that aren't necessarily discreet.

28:21.360 --> 28:27.280
And so you went down this particular path, why again, just for interpretability.

28:27.280 --> 28:30.440
So the idea is, yeah, we could have said that there's, you know, 10 actions that he can

28:30.440 --> 28:34.560
be doing to some degree, you know, maybe he's eating a little bit, maybe he's running

28:34.560 --> 28:35.560
a little bit.

28:35.560 --> 28:40.280
And then the point is that we think it would have been really a lot harder to interpret

28:40.280 --> 28:42.960
what these, these variables meant.

28:42.960 --> 28:46.280
And we would, if we wanted to say what he was doing at a given time, we'd have to say

28:46.280 --> 28:49.080
these 10 numbers instead of just like this nice one.

28:49.080 --> 28:50.080
Right.

28:50.080 --> 28:53.600
And that's the kind of thing we typically see, like in image interpretation, like, you

28:53.600 --> 28:59.320
know, in this image, there's a umbrella with whatever probability and a girl with whatever

28:59.320 --> 29:00.320
probability.

29:00.320 --> 29:05.360
And so you've got this kind of continuous probability distribution of the things that

29:05.360 --> 29:10.560
are in the image and you could do similar with, with a video clip, you know, the mouse

29:10.560 --> 29:15.600
is doing x, y, z with some probability and, you know, eating with some probability and

29:15.600 --> 29:17.240
grooming with another probability.

29:17.240 --> 29:18.240
Right.

29:18.240 --> 29:21.840
But the idea that the, so the probabilities are continuous, but there's still a big difference

29:21.840 --> 29:26.000
between having a model where the variables are continuous and we're certain about them

29:26.000 --> 29:29.440
or where the variables are discrete and we're uncertain about them.

29:29.440 --> 29:33.400
This, this is an interesting point, which is that when you look at reality, sort of reality

29:33.400 --> 29:34.680
is always continuous.

29:34.680 --> 29:37.200
So why do we even have this discrete structure?

29:37.200 --> 29:41.680
And sort of one reason that it arises naturally is because when we have to describe stuff to

29:41.680 --> 29:45.480
each other in language, we have to choose, you know, which word to use, right?

29:45.480 --> 29:47.000
Like, am I hungry or am I sleepy?

29:47.000 --> 29:49.880
We don't have a whole, yeah.

29:49.880 --> 29:53.520
And we can modify this with verbs, but again, we don't have like some continuous signal

29:53.520 --> 29:56.680
that we can send to each other to just, you know, we can't just give each other high

29:56.680 --> 29:57.680
dimensional vectors.

29:57.680 --> 29:58.680
That would be amazing.

29:58.680 --> 30:01.440
Then, you know, language learning would be a lot easier.

30:01.440 --> 30:04.240
And in fact, there's been some work recently by like OpenAI and some other groups on

30:04.240 --> 30:09.000
like how to teach agents to communicate and they come across this exact same problem,

30:09.000 --> 30:14.360
which is, you know, the agents have to choose a discrete word to say to the other agent,

30:14.360 --> 30:16.160
but there's no way to backprop through that.

30:16.160 --> 30:19.760
There's no gradient signal that says, oh, you should have said this word a little bit less,

30:19.760 --> 30:20.760
right?

30:20.760 --> 30:21.760
It's not clear what that would mean.

30:21.760 --> 30:22.760
Have you seen the movie, her?

30:22.760 --> 30:23.760
Right.

30:23.760 --> 30:24.760
Yeah.

30:24.760 --> 30:25.760
Love that movie.

30:25.760 --> 30:26.760
People have been telling me forever to watch it.

30:26.760 --> 30:30.680
And I just started watching it on this trip and I'm not all the way through with it.

30:30.680 --> 30:36.440
But there was this one part where the, for those who haven't seen it, maybe I shouldn't

30:36.440 --> 30:38.800
give any spoilers on the podcast, that wouldn't be right.

30:38.800 --> 30:45.040
But there's an interesting point where, where this one AI talks about is talking to a human

30:45.040 --> 30:50.480
and says, hey, can I go offline with this other AI and communicate post verbally, which

30:50.480 --> 30:52.880
is exactly what you're talking about, exactly.

30:52.880 --> 30:56.200
So, I mean, it does raise the question, why do we want these artificial agents to even

30:56.200 --> 31:00.160
use discrete words to talk to each other when they can just communicate post verbally, as

31:00.160 --> 31:01.160
you say?

31:01.160 --> 31:02.160
Right.

31:02.160 --> 31:05.120
And I think maybe the answer is, well, we still want them to talk to us and they're going

31:05.120 --> 31:07.760
to have to probably use words to do that.

31:07.760 --> 31:09.920
That's one way, place where this comes up.

31:09.920 --> 31:10.920
Okay.

31:10.920 --> 31:12.560
So what other kinds of things are you working on?

31:12.560 --> 31:18.920
So I'm working a little bit on meta learning and just recently, I've sort of decided that

31:18.920 --> 31:22.560
the way that I was approaching this and that, you know, I think to sort of the mainstream

31:22.560 --> 31:25.000
way now, I think there might be another way forward.

31:25.000 --> 31:29.840
So there's been a lot of work recently where people have said, okay, I want to have like

31:29.840 --> 31:32.520
a robot or a little agent that's going to be able to learn really quickly.

31:32.520 --> 31:37.040
So I want to put it in a new environment and it's going to, in a few seconds, you know,

31:37.040 --> 31:40.720
figure out what's going on and then have a good policy of how to act or something like

31:40.720 --> 31:41.720
that.

31:41.720 --> 31:46.680
The sort of brute force way to do this, no one really did very much until a couple years

31:46.680 --> 31:53.240
ago was to back propagate through the entire learning procedure of, you know, the robot.

31:53.240 --> 31:56.960
So take those, you know, three seconds of him learning his way around the world.

31:56.960 --> 32:01.280
And if everything he did is continuous, we can actually just ask, you know, if I had changed

32:01.280 --> 32:06.200
his learning rules a little bit, how much better would he have done on the task?

32:06.200 --> 32:07.200
And this is fine.

32:07.200 --> 32:13.960
Compute this automatically with automatic differentiation and this also shows up when we have to tune

32:13.960 --> 32:15.560
the hyper parameters of our model, right?

32:15.560 --> 32:20.200
We want to fit an entire neural network, but we have like a learning rate or like a regularization

32:20.200 --> 32:23.920
parameter that we said at once at the beginning, and then it totally changes the outcome at the

32:23.920 --> 32:25.160
end.

32:25.160 --> 32:30.600
So a couple of years ago, me and my colleague, Dougal McClaren wrote a paper where we actually

32:30.600 --> 32:34.360
did back propagate through the entire training procedure of training a neural network for

32:34.360 --> 32:35.680
hundreds of iterations.

32:35.680 --> 32:40.040
And we got, you know, exact gradients and we could use gradient based optimization to tune

32:40.040 --> 32:41.040
our hyper parameters.

32:41.040 --> 32:45.240
And this is like, you know, really exciting because before that, everyone had to use these

32:45.240 --> 32:50.520
black box methods like random search or vision optimization that is kind of like Harry.

32:50.520 --> 32:53.520
And then after that, there was like this paper learning to learn by gradient descent,

32:53.520 --> 32:54.520
by gradient descent.

32:54.520 --> 32:55.520
It's like an amazing title.

32:55.520 --> 32:58.760
I wish that I had thought of that title for our paper, which is like much more boring

32:58.760 --> 32:59.760
title.

32:59.760 --> 33:00.760
Yeah.

33:00.760 --> 33:01.760
Doing the same sort of ideas.

33:01.760 --> 33:04.600
And you know, reinforcement learning people are really excited about this right now.

33:04.600 --> 33:10.640
So, but there's another way forward, right, which is to train a neural network to look

33:10.640 --> 33:16.520
at a problem, maybe, you know, take in the hyper parameters and then just directly output

33:16.520 --> 33:19.560
the optimal weights of a neural network.

33:19.560 --> 33:23.640
So skip the entire training procedure and just have the hyper parameters, the weights.

33:23.640 --> 33:24.640
Yeah, exactly.

33:24.640 --> 33:25.640
Yeah.

33:25.640 --> 33:28.480
We're still going to have to tune some hyper parameters, but we can train a neural network

33:28.480 --> 33:32.840
to just directly output the optimal weights.

33:32.840 --> 33:37.800
And that sounds maybe done because if you think, well, if you think what I'm going to do

33:37.800 --> 33:43.040
is now train a whole bunch of neural networks like I did before and then, you know, learn

33:43.040 --> 33:47.520
to predict the final outputs, then that would be slow and that would be a waste of time.

33:47.520 --> 33:51.600
But it turns out we can train a neural network to produce, like, I'll call this like a

33:51.600 --> 33:55.520
hyper network because it produces the weights of another neural network.

33:55.520 --> 33:59.480
I can train a hyper neural network to produce an optimal neural network without ever having

33:59.480 --> 34:01.520
seen an optimal neural network.

34:01.520 --> 34:05.840
I can just have it, you know, start off produce a bad neural network and then ask, you

34:05.840 --> 34:11.360
know, use back prop to determine how should I have adjusted the parameters of my hyper network

34:11.360 --> 34:15.040
so that it would have given me a better actual network.

34:15.040 --> 34:19.440
And then this gets very hard to talk about and everything is very good and confusing.

34:19.440 --> 34:23.720
I thought you were going to go in a direction of something like a GAN or something like

34:23.720 --> 34:24.720
that.

34:24.720 --> 34:25.720
Maybe.

34:25.720 --> 34:30.360
I mean, so the GAN does have this generator and I mean, I guess, yeah, I didn't really

34:30.360 --> 34:31.360
thought about that.

34:31.360 --> 34:36.280
We could train again to produce a network, but I guess the discriminator would have to

34:36.280 --> 34:41.960
see sort of the, our train networks and sort of real optimal networks and distinguish between

34:41.960 --> 34:42.960
them.

34:42.960 --> 34:46.160
But the whole point is I want to avoid ever having to start with a bunch of like optimal

34:46.160 --> 34:47.160
networks.

34:47.160 --> 34:48.160
Right.

34:48.160 --> 34:52.640
So I guess we can call this amortized optimization where amortized optimization is just

34:52.640 --> 34:57.440
like, okay, we learn to do optimization by sort of practicing, producing the optimal

34:57.440 --> 34:58.440
thing.

34:58.440 --> 35:02.400
You know, this idea has been sort of staring us in the face because this is what variational

35:02.400 --> 35:03.760
autoencoders do.

35:03.760 --> 35:09.520
They do amortized inference and that's sort of like a name of this little subfield where

35:09.520 --> 35:13.360
they say we're going to learn to look at the data and train a neural network to produce

35:13.360 --> 35:18.520
the optimal posterior, like the optimal probability of the latent variables.

35:18.520 --> 35:22.720
And again, these are trained in the same way where we never see the optimal posterior directly.

35:22.720 --> 35:26.800
We just can use gradient signals to tell us how to get better and better.

35:26.800 --> 35:30.080
And then after we train for a while, we sort of hope that we're almost optimal.

35:30.080 --> 35:31.080
Cool.

35:31.080 --> 35:36.120
Well, one thing I want to say is that the second idea of the using the hyper network to

35:36.120 --> 35:39.760
avoid training is due to my student John Lorraine.

35:39.760 --> 35:43.760
This is the beauty of this job is now I get to look good because all these brilliant

35:43.760 --> 35:46.160
students are coming up with ideas and then I get asked about them.

35:46.160 --> 35:48.120
So, but I have to give credit, we're credit to Steve.

35:48.120 --> 35:49.120
Awesome.

35:49.120 --> 35:50.120
Awesome.

35:50.120 --> 35:51.120
Well, thank you very much.

35:51.120 --> 35:54.560
This was a really interesting conversation and certainly has a lot of my neurons firing

35:54.560 --> 35:57.000
trying to figure out all the stuff that we talked about.

35:57.000 --> 36:00.920
And there's a lot of interesting stuff to dig into here.

36:00.920 --> 36:02.520
I appreciate you taking the time.

36:02.520 --> 36:03.520
Oh, my foot.

36:03.520 --> 36:04.520
Awesome.

36:04.520 --> 36:05.520
Thank you.

36:05.520 --> 36:11.520
All right, everyone, that's our show for today.

36:11.520 --> 36:17.120
Thanks so much for listening and for your continued feedback and support.

36:17.120 --> 36:23.120
Thanks to your support, this podcast finished the year as a top 20 technology podcast on

36:23.120 --> 36:25.160
Apple Podcasts.

36:25.160 --> 36:30.080
My producer says that one of his goals this year is to crack the top 10, but we definitely

36:30.080 --> 36:33.400
cannot do that without your support.

36:33.400 --> 36:38.760
What we need you to do is to head on over to your podcast app, rate the show, hopefully

36:38.760 --> 36:44.960
we've earned five stars, leave us a glowing review and share it with your friends, family,

36:44.960 --> 36:50.000
co-workers, Starbucks baristas, Uber drivers, everyone.

36:50.000 --> 36:55.760
Every review and rating goes a long way, so thank you so much in advance.

36:55.760 --> 37:00.840
For more information on David or any of the topics covered in this episode, head on over

37:00.840 --> 37:05.600
to twimmolai.com slash talk slash 96.

37:05.600 --> 37:10.440
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

37:10.440 --> 37:13.600
or via Twitter at twimmolai.

37:13.600 --> 37:20.600
Thanks once again for listening and catch you next time.

