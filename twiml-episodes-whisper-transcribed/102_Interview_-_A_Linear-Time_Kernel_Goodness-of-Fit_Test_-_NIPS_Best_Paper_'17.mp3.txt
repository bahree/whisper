Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A quick thanks to everyone who participated in last week's Twimble Online Meetup.
It was another great one.
If you missed it, the recording will be posted to the Meetup page at twimbleai.com slash
meetup.
Definitely check it out.
I never cease to be amazed by the generosity and creativity of the Twimble community.
And I'd like to send a special shout out to listener Sharon Glander for her exceptional
sketch notes.
Sharon has been creating beautiful hand sketch notes of her favorite Twimble episodes
and sharing them with the community.
Sharon, we truly love and appreciate what you're doing with those.
So please keep up the great work.
We'll link to her sketch notes in the show notes for this episode.
And you should definitely follow her on Twitter at sharingglander for more.
This is your last chance to register for the rework, deep learning and AI assistant
summits in San Francisco, which are this Thursday and Friday, January 25th and 26th.
These events feature leading researchers and technologists like the ones you heard in
our Deep Learning Summit series last week.
The San Francisco event is headlined by Ian Goodfellow of Google Brain, Daphne Kohler
of Calico Labs and more.
Definitely check it out and use the code Twimbleai for 20% off of registration.
In this episode of the show, I host the largest group of guests I've ever had on a single
podcast.
I speak with Arthur Gretton, Wittowat Jekritam, Zoltan Zabo, and Kenji Fukumizu, who, alongside
with Wenkai Shu, authored the 2017 Nipps Best Paper Award winner, a linear time kernel
goodness of fit test.
In our discussion, we cover what exactly a goodness of fit test is and how it can be used
to determine how well a statistical model applies to a given real world scenario.
The group and I then discuss this particular test, the applications of this work, as well
as how this work fits in with other research the group has recently published.
Enjoy.
Alright everyone, I am in Long Beach, California at Nipps and I've got the stink pleasure
of being seated with a group who is authored one of the Best Paper Award winners.
Here at Nipps, it is by far the largest group that I've ever interviewed at one time on
a podcast.
So I'm going to allow them to introduce themselves and please name, role, title, affiliation,
and maybe while you're at it, a little bit about your research and areas of interest.
Thank you.
So I'm Arthur Gretton.
I'm a professor at the Gatsby Computational Neuroscience Unit at University College London.
So my research interests, I generally know how to represent and compare probabilities.
So on one hand, if you've got two groups of objects, you're trying to tell whether they're
similar or different, that's one of the problems that we might address.
Another is in reasoning.
So if you're observing something that might have an effect on something else, you want
to know how strong that effect is and how confident you are in whether that effect exists.
Great.
Thanks, Arthur.
Hello.
My name is Witowat Gretton.
I'm from the Gatsby Unit University College London.
I'm a PhD student studying computer science.
So my interests are statistical tests, hypothesis testing, and a bit on Bayesian inference.
Okay, great.
Kenji.
Hello.
I'm Kenji Fukumizu.
I'm a professor at the Institute of Statistical Mathematics in Tokyo.
And I'm basically interested in statistics and the machine learning.
And my interest is also ranging from the mathematical aspect of machine learning to the application
of machine learning.
Okay.
Great.
And Zaltan?
Hello.
I'm Zaltan Sable from C-MAP, Center of Applied Mathics, Equality Technique France.
I'm a research associate professor, genuinely interested in information theory, connoe algorithms,
how to define similarities between many different objects, graphs, time series, dynamical systems,
vectors, and how to apply it in different information, theoretical context, including hypothesis
testing.
Okay.
I want to tell us about the paper that you guys submitted.
So I'm happy to do that.
So the paper is a goodness of fit test.
What this means is that we have some model of the world, and we want to tell whether that
model reflects the world well or not, and if not, why not.
So an example that we used in the talk was that if you have a model of where crime occurs
in Chicago, you might want to know whether that model accurately predicts where the crime
will occur, and if there is some shortcoming to the model, you might want to know where
that is.
As an example, we used a rather crude crime model, which was mistakenly predicting crimes
in the lake next to the city where clearly there weren't any crimes.
So this would automatically flag those, so there's no piracy at the moment in Chicago.
So this is a specific instance.
The difficulty we encounter with this is that if the model that you have of the world
is very complicated, then it can be very difficult to compare this with data.
So in our crime example, imagine trying to figure out whether it's likely that somebody's
going to commit a crime or this is going to depend on their social network, on the state
of the economy, on the political atmosphere, and so on.
All of these things are very complicated and interact, so you can state what the interactions
are, but then to figure out how these all combined to create a probability is impossible.
It's just not mathematically feasible.
Nonetheless, we might want to know when our test has difficulties and the way that we've
formulated it, our test is able to decide on the, or to locate the mismatch between data
and model without having to figure out these probabilities and do this impossible computation.
And what was the title of the paper?
So the title is a linear term, kernel goodness of fit test, linear time kernel goodness
of fit test.
How is this other standard set of goodness of fit test that folks would use otherwise?
So in certain simple cases, there are goodness of fit test.
So if you have a very simple model that you're comparing with like a simple Gaussian, then
there are other goodness of fit tests.
In the case of the Gaussian, we don't have this complexity that I mentioned where you
have many interacting effects that make computing probabilities in close form impossible.
So what we have tried to address is the case when these models are very complex.
And what, out of curiosity, what brought you all together from disparate parts of the
world?
Yes, we are like frequent collaborators.
So Zoltan used to be at Caspi unit.
We worked together before.
I and other also frequently visit Kenji at Institute of Statistical Mathematics, so kind
of frequent collaborators.
And so with the, you know, when I think about a best paper, you know, there are lots of
reasons why a paper might win best paper, you know, brought applicability, elegance of
the solution and others, you know, why do you think this paper was selected as one of
the best papers for Nips?
I think it's a combination of broad applicability and the analysis that comes with it.
So one, I guess, key thread in modern machine learning is like the formulating generative
models for complicated data.
So one of the key research directions of DeepMind, for example, is reinforcement learning.
They want an agent that is in a world and that is able to learn from that world.
What this requires in practice to train it is a simulation of the world, which is very realistic.
So one important, I guess, component of that is to know when your simulation of the world
is not accurate.
What is it missing?
I think for this reason, a paper which is able to, like, troubleshoot these models of
the world that people might use in practice is a very much interest across the community.
I think sort of coupled with this sort of very applied, I guess, benefit is that we have
some nice theory results.
So one of the phrases in the title was linear time.
So the time that it takes is no more than the time that it takes to sort of look at each
item.
Quadratic time means you sort of have to look at all pairs of items.
So that becomes very expensive.
So what another result that we showed in the paper is of the various ways that one could
derive a linear time goodness of fit test with the properties that we've talked about,
the way that we've proposed is provably better.
So it's going to give you a test which has the number of true positives, I should say,
is going to improve faster for this test and for the other alternatives.
Yeah.
When you say linear time, is it linear in the number of examples or the number of distributions
that you're trying to fit to or linear in the number of examples?
Okay.
We have a single model, you know, our grand map of Chicago, and then we're looking, it's
linear in the number of crimes.
Okay.
Okay.
So what's the process for applying it?
So this goodness, then this new goodness of fit test takes in two inputs.
Okay.
The first one is a model.
Okay.
For example, in the crime, Chicago crime example, the model, let's say we want to model
spatial density of robbery events in Chicago, say, just going to be the model, then just
some density function.
The second input would be a collection of points.
In this case, it's going to be observed robbery events where each point is just like
the larger your language coordinate of one event, for example.
And the way that this works is that the test constructs some function, it's essentially
a scoring function that tells where the mismatch is between the model and the data.
And once we have this scoring function, we can then use it to find sort of a region where
the mismatch is the largest.
And then we can pinpoint, okay, that is where there's a mismatch.
And then this is how we criticize the model.
So in the end, we would get a region indicating, okay, this is where the model doesn't fit quite
well to the data.
And then as a modeler, they can just improve the model based on the hints, on the evidence
given by the test.
So when you say region, region of what?
We're not talking about a region of the map of Chicago, we're talking about a region
of the map.
Yeah, a physical region, actually.
And does that correspond to, you know, if we're not talking about a map example, but it
sounds like this applies more broadly, what's the generalization of a region?
Right.
So the technical term would be, it's going to be the domain where your data live in.
So whatever that domain is, so I don't know.
So typically, I guess if you have data, I'd represent it at a table, then you have many
columns.
I guess columns correspond to the number of features or the number of attributes.
Then let's say your data live in Rd, where D is the number of columns, then that is going
to be like the equivalent of the region that I mentioned.
So it's going to identify regions in your feature space.
In the feature space, yeah.
Okay.
Right.
Okay.
Let me give you maybe two other examples, because this paper is, in fact, part of a larger,
let's say, a package of hypothesis tests in your time hypothesis tests.
And we use this technology in, for example, computer vision or natural language processing.
In the first case, you have, let's say, images of, let's say, happy or sad people.
Okay.
And then the question is whether you can detect the difference between the two emotions,
or if there's difference, which, let's say, masses are responsible for this difference.
Okay.
So that's the domain, in this case, let's say, muscle activities or parts of the face.
In the other application, natural language processing, we had, let's say, documents from
two different topics, categories, like neuroscience, Bayesian inference.
And then the question was, again, whether these two possibly different topics can be discriminated.
And if this is the case, what are the keywords you should look at?
In this case, that's the domain, what are the most distinguishing keywords?
Mm-hmm.
So in the case of the first example, you, in applying this result, you would identify, for
example, regions of the face, that, what?
That distinguish between the emotions or...
Yeah, so that's right.
So for instance, if you had sort of emotions that required you to frown or to, yeah, to
put the sides of your mouth downwards, like you're unhappy, which causes the sort of lines
on each side to stand out, these would be the regions, the spatial regions in the face,
which distinguish the sort of contented emotions from the angry or upset emotions.
So yeah, so the domain is sort of all of the possible regions in the face, just the spatial
domain.
And then the salient regions that matter in distinguishing them are the locations around
the eyebrows and to each side of the nose and mouth.
And so this, one of the areas that it sounds like this plays into is the domain of explainability
of AI models.
Is that one of the primary motivators as well for this research?
I think so, yes, because, yeah, it's basically a way of showing the shortcoming of your model,
like what it is that your model fails to explain about your data.
So I think, yeah, explainability is really one of the, regions, I think that the paper
was given the award, yeah.
And we are using the more and the more complex model, the recent three, and so it's very
important to, well, say that we are, that model correctly reflects our data, that's our
motivation.
And so what are the requirements of the model that would allow us to apply this?
Is it applicable to like box set of models or do we need to know something about, you
know, either the models or the distributions of our data or other things in order to apply
this technique?
So it does need to be a probabilistic model.
So by contrast, one might think of these adversarial networks where there you might not have actually
a model of the probabilities of the images that it's generating, if it's generating images.
So what we do require is that we're able to, you know, write the model as something that,
you know, if you were able to, like, to normalize it to take the sum over all possible states
would be a valid probability.
Some ways of generating data from randomness, just take the data and apply a bunch of transforms
to it, but it's not clear how or if you could turn that into a probability of the outputs
given the random noise that you fed in.
We are only able to deal at this stage with the case where you have this, you know, thing
that you could write as a probabilistic model, were you able to normalize it?
Okay.
And maybe taking a step back, what was the fundamental realization or innovation that
kind of led you down the path that led to this paper?
It sounds like it's all time you mentioned this paper is one in a series of works, but
what inspired this particular result?
Zoltan mentioned it was one of a sequence of works.
So the first work was just in comparing sets of objects.
So in Zoltan's example, sets of faces with positive and negative emotions.
So notice that model never appeared in that description.
It's just, I have set A set B and I want to know why and where these two sets are different.
The second test in the series was a test of whether two things are dependent.
So as an example, if you show to a human some movie, then their brain will be active in
a way that depends on the movie that they're watching.
And so this dependence, you see, can we take a step back and as you talk through each of
these papers in this sequence, kind of what the key results were as well?
Sure.
So for the first paper, we were able to show again, a linear time test of where these two
sets of objects were different.
So again, it only costs me a time to compute.
So that means the linear and the number of objects that we're comparing.
And we were also given a diagnosis of where it was that the two sets were different.
And so my first kind of flash intuition on this would be to try to do some kind of difference
or something like that and maybe convolve the images together or something is, are you
thinking about it in the same domain or more like from a pure probabilistic perspective
or?
So we need to think in terms of sets of objects rather than objects.
So for example, if one looks at a pair of faces, you can take their difference and you
can see where that difference occurs.
If you have two sets, it can be that the mean is the same, but one set has a higher variance
than the other.
And so we care about any difference that might occur between two sets of objects.
So the second paper was about dependence testing.
And in this case, again, we have a linear time test.
We also care about dependence that might be quite complicated.
So the dependence that you might learn in statistics 101, you notice that when one thing increases,
like, you know, I press down my accelerator, my cargo's fastest, linear dependence.
What we might care about is dependence that is much more complicated than that.
For example, like if you are adjusting a parameter on a robot arm, it might, on average, hit the
same target.
But it might be that if you under-damped the robot arm, it's half the time above the target,
half the time below.
So the variance is going up depending on this parameter, even though if you just looked
at the means, you would get the same mean.
So this, you know, this is a very trivial example, but illustrating this sort of complexity
of dependence.
So that's, you know, first of all, testing pairs of samples, second testing dependence.
And then this led us to say, well, what if we don't have, you know, two sets of samples,
but a sample in a model, what's the best way to approach that?
And that led to our third paper.
Okay.
Great.
Great.
And does the sequence continue?
What's next for the group of you?
So one possible future work is that continuing from the third work, which is, so now that we
have a model and we have one data set, right?
But in practice, in reality, most of the time your model is probably wrong.
And you know, a priori that it is, it is probably wrong.
But now a more relevant question is given two models that are wrong, two competing models
that are wrong, which one fits better?
That is, this is now a model comparison instead of, so in the current version is model
acquitticism.
There's only one model when we try to criticize where it goes wrong.
But now we can also extend to two models that we know are wrong, but we want to ask,
okay, which one fits better?
I think this is one possible direction.
And then maybe which fits better where, as opposed to hiring, exactly, exactly, exactly.
Exactly.
That's what we are going to continue.
Exactly.
Nice.
For the rest of you, any kind of parting words as we wrap up, there's another possibility
of extension.
So here, all these linear time tests, the underlying assumption is that the probabilistic model lies
on essentially on vectors, like data machine, Euclidean vectors, which is possibly the simplest
concept to understand.
But one might argue maybe there are some dependencies between the coordinates somehow, so like if
you, for example, think of graphs or like images, not all the pixels can vary completely
in the pendants.
So often there's some underlying load-dimensional structure in the background, hidden under
the hood.
So that's another possible direction to extend this framework.
Awesome.
Any other thoughts?
How's Neb's been for everyone?
Great.
I'm very impressed by Neb's giving a word to this work because this is the age of artificial
intelligence, and many people are looking at their application with deep learning, but
our research is very basic ones, but we are using any method, we are using many complex
models, and the model criticism is very important and basic research, and I'm very happy that
Neb's gives respect to such type of basic work.
And there was a call here at Neb's somewhat controversial call for kind of more basic
work and more rigor in the way we look at machine learning.
Any thoughts or comments on that?
It sounds like you would all agree with that general idea.
So there was an inspiring call to arms, which has caused a lot of discussion, and in a way,
I think it's also a call to arms that is already being acted on in the sense that this
year there has been a lot more thought about fundamentals of algorithms that are being
used very successfully in the past two years.
One example that was raised in this call to arms is understanding when you're optimizing
a model with a huge number of parameters, as deep learning methods are, what the pathologies
of these optimization methods are, when correlations cause you to converge very slowly or converge
poorly, many of the, I guess, presentations by the optimization community, the SNPs were
actually addressing that.
So I think there was a standing ovation, as you know, when this call to arms was made.
And I think that's because, like, the spirit of this Vali Rahimi's words was very much
in the air that people were saying, OK, let's, we've got this amazing success on applications.
Now let's understand what's still holding us back, and then that will help us to progress
even further.
Hmm.
Great.
Well, Arthur with Wat, Kenji, Zoltan, thanks so much for taking a few minutes to chat
with us about your paper, and congratulations on the award.
Thank you very much.
Thanks, Zoltan.
Alright, everyone, that's our show for today.
Thanks so much for listening, and for your continued feedback and support.
For more information on Arthur with Wat, Zoltan, Kenji, or any of the topics covered in this
episode, head on over to twomlai.com slash talk slash 100.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page,
or via Twitter, directly to me at At Sam Charrington, or to the show at At Twomlai.
Thanks once again for listening, and catch you next time.
