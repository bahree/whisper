1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,440
I'm your host Sam Charrington.

4
00:00:23,440 --> 00:00:28,120
Before I introduce this week's guest, here's a quick reminder about the upcoming Twimble

5
00:00:28,120 --> 00:00:35,520
Online Meetup, which will be held tomorrow, September 12th, at 3 o'clock, specific time.

6
00:00:35,520 --> 00:00:40,200
The discussion will be led by Nikola Kuchereva, who will be presenting learning long-term

7
00:00:40,200 --> 00:00:45,640
dependencies with gradient descent is difficult, by Yasuo Benjio and company.

8
00:00:45,640 --> 00:00:50,320
Of course, it's best if you've taken a look at the paper in advance, but even if not,

9
00:00:50,320 --> 00:00:53,560
you're sure to learn something from the discussion.

10
00:00:53,560 --> 00:00:59,440
For more information or to register, visit twimlai.com slash meetup.

11
00:00:59,440 --> 00:01:01,680
See you online tomorrow.

12
00:01:01,680 --> 00:01:05,680
If there's any one topic that I've received a bunch of requests about covering on the

13
00:01:05,680 --> 00:01:12,760
show, it's the subject of this week's discussion, evolutionary algorithms and machine learning.

14
00:01:12,760 --> 00:01:18,800
My guest this week is Risto Mikulainen, professor of computer science at UT Austin, and vice

15
00:01:18,800 --> 00:01:22,680
president of research at sentient technologies.

16
00:01:22,680 --> 00:01:27,080
During our talk, Risto and I discussed some of the things sentient is working on in the

17
00:01:27,080 --> 00:01:33,040
financial services and retail fields, and we dig into the technology behind them, evolutionary

18
00:01:33,040 --> 00:01:38,760
algorithms, which is also the focus of his research at University of Texas.

19
00:01:38,760 --> 00:01:44,640
I really enjoyed this week's interview and learned a ton, and I'm sure you will too.

20
00:01:44,640 --> 00:01:50,040
Before we get to the show, a word about this week's sponsor, Clydeira.

21
00:01:50,040 --> 00:01:54,280
You probably think of Clydeira primarily as the Hadoop company, and you're not wrong

22
00:01:54,280 --> 00:01:55,280
for that.

23
00:01:55,280 --> 00:02:00,160
But did you know that they also offer software for data science and deep learning?

24
00:02:00,160 --> 00:02:02,240
The idea here is pretty simple.

25
00:02:02,240 --> 00:02:06,680
If you work for a large enterprise, you probably already have Hadoop in place, and your Hadoop

26
00:02:06,680 --> 00:02:12,480
cluster is filled with lots of data that you want to use in building your models.

27
00:02:12,480 --> 00:02:18,000
But you still need to easily access that data, process it using the latest open source

28
00:02:18,000 --> 00:02:22,880
tools and harness bursts of compute power to train your models.

29
00:02:22,880 --> 00:02:26,800
This is where Clydeira's data science workbench comes in.

30
00:02:26,800 --> 00:02:30,640
With the data science workbench, Clydeira can help you get up and running with deep learning

31
00:02:30,640 --> 00:02:36,920
without massive new investments by implementing an on-demand self-service deep learning platform

32
00:02:36,920 --> 00:02:40,680
on your existing CDH cluster.

33
00:02:40,680 --> 00:02:47,160
To learn more about the data science workbench, visit twimmaleye.com slash Clydeira.

34
00:02:47,160 --> 00:02:53,400
And for a limited time, Clydeira is offering a free drone to qualify participants who register

35
00:02:53,400 --> 00:02:54,920
for a demo.

36
00:02:54,920 --> 00:03:00,520
Again, the URL for that is twimmaleye.com slash Clydeira.

37
00:03:00,520 --> 00:03:05,360
And now on to the show.

38
00:03:05,360 --> 00:03:11,200
All right, everyone.

39
00:03:11,200 --> 00:03:17,440
I am on the line with Risto McGulainen, who is vice president of research at sentient

40
00:03:17,440 --> 00:03:24,560
technologies and a professor of computer science at University of Texas at Austin.

41
00:03:24,560 --> 00:03:31,680
Risto and I recently tried to connect at the O'Reilly AI conference in New York.

42
00:03:31,680 --> 00:03:36,680
Unfortunately, we weren't able to do an in-person interview, but we were able to get on the

43
00:03:36,680 --> 00:03:42,240
line together and I'm really looking forward to this conversation and I think you'll really

44
00:03:42,240 --> 00:03:47,000
enjoy it because we'll be talking about something we haven't talked about yet on the podcast

45
00:03:47,000 --> 00:03:50,080
and that is evolutionary algorithms.

46
00:03:50,080 --> 00:03:52,760
So Risto, welcome to the podcast.

47
00:03:52,760 --> 00:03:53,760
Thank you.

48
00:03:53,760 --> 00:03:54,760
Pleas to be here.

49
00:03:54,760 --> 00:03:55,760
It's great to have you on.

50
00:03:55,760 --> 00:03:59,960
Why don't we get started by having you tell us a little bit about your background?

51
00:03:59,960 --> 00:04:07,800
Okay, yes, so I got my PhD at UCLA 1990 and was working at the time of neural networks,

52
00:04:07,800 --> 00:04:12,640
which I've continued doing all my career, but got also interested in evolutionary algorithms

53
00:04:12,640 --> 00:04:18,160
about the same time and have gradually drifted more in that direction and especially in the

54
00:04:18,160 --> 00:04:20,000
intersection of those two things.

55
00:04:20,000 --> 00:04:25,960
So we've been evolving neural networks since the early 90s and you can view that as one

56
00:04:25,960 --> 00:04:31,160
way of training neural networks in domains where you cannot really do other types of learning

57
00:04:31,160 --> 00:04:33,800
like deep learning, supervised learning.

58
00:04:33,800 --> 00:04:35,360
So that's been my research focus.

59
00:04:35,360 --> 00:04:39,520
I've also originally I did a lot of work in natural language processing with neural networks

60
00:04:39,520 --> 00:04:42,960
and then understanding the visual cortex with neural networks.

61
00:04:42,960 --> 00:04:48,520
Most recently, this evolution-based work has focused on building robotic control game

62
00:04:48,520 --> 00:04:55,160
characters and more recently solving problems in the real world using using this technology.

63
00:04:55,160 --> 00:04:56,960
And that's what I'm doing at Sentiant.

64
00:04:56,960 --> 00:05:01,600
I'm on Lee from UT Austin, I've been for two years trying to take the technology to

65
00:05:01,600 --> 00:05:04,280
the real world and it's been a lot of fun.

66
00:05:04,280 --> 00:05:07,840
So tell us a little bit about what Sentiant does and what the focus is there.

67
00:05:07,840 --> 00:05:12,480
Sure, Sentiant's been around actually for 10 years, although they came out of stealth

68
00:05:12,480 --> 00:05:14,360
only three years ago or two years ago.

69
00:05:14,360 --> 00:05:16,000
That's a long time in stealth.

70
00:05:16,000 --> 00:05:19,320
Well, it took a while to develop all the infrastructure.

71
00:05:19,320 --> 00:05:24,600
So initially, Sentiant was doing stock trading and still is doing stock trading as one of

72
00:05:24,600 --> 00:05:25,600
the applications.

73
00:05:25,600 --> 00:05:30,080
It's a great first application because you don't need to have much customer support and

74
00:05:30,080 --> 00:05:31,080
other things like that.

75
00:05:31,080 --> 00:05:35,760
You just run your algorithms and they trade directly and make money, but it took a long

76
00:05:35,760 --> 00:05:41,240
time to build those algorithms and also build the infrastructure, the computing infrastructure.

77
00:05:41,240 --> 00:05:48,080
So evolution is one of those technologies that requires a lot of computing power to excel.

78
00:05:48,080 --> 00:05:53,920
And here at Sentiant, we built that kind of a computing power distributed system across

79
00:05:53,920 --> 00:06:00,720
the internet using, at that time, two million CPUs to evolve stock traders.

80
00:06:00,720 --> 00:06:04,360
And that was very successful and it was a lot of fun to build that.

81
00:06:04,360 --> 00:06:09,840
That was the first product and then we changed the course to added another direction rather

82
00:06:09,840 --> 00:06:13,960
going to e-commerce building intelligence interfaces to e-commerce.

83
00:06:13,960 --> 00:06:15,880
So there are two products there.

84
00:06:15,880 --> 00:06:21,480
One of them is Sentiant Aware, which is a visual interface to catalog of products that

85
00:06:21,480 --> 00:06:23,400
understands what the user wants.

86
00:06:23,400 --> 00:06:28,160
It's more like a personalized assistant that you might find in Nordstrom or other fancy

87
00:06:28,160 --> 00:06:29,480
department store.

88
00:06:29,480 --> 00:06:34,160
You click on a choices and the system will understand what it is that you're looking

89
00:06:34,160 --> 00:06:37,120
for and a couple of clicks will get you what you want.

90
00:06:37,120 --> 00:06:42,280
The other one is Sentiant Sentiant, which is a way of using evolutionary computation to

91
00:06:42,280 --> 00:06:46,920
design web interfaces so that they are most effective.

92
00:06:46,920 --> 00:06:51,560
For instance, optimizing conversion rate on an e-commerce website is one of the applications

93
00:06:51,560 --> 00:06:52,560
of that technology.

94
00:06:52,560 --> 00:06:54,720
So those are the three products.

95
00:06:54,720 --> 00:06:59,760
But behind all that, we develop, we are technology companies, so we develop evolution

96
00:06:59,760 --> 00:07:04,440
computation methods, deep learning methods, combinations of those as well as surrogate

97
00:07:04,440 --> 00:07:08,400
optimization and back by optimization in various domains.

98
00:07:08,400 --> 00:07:10,600
So that's our technology pace.

99
00:07:10,600 --> 00:07:11,600
Okay.

100
00:07:11,600 --> 00:07:14,840
Is the, actually, I'll get back to that.

101
00:07:14,840 --> 00:07:18,120
You mentioned two million CPU cores.

102
00:07:18,120 --> 00:07:24,560
Is that the cloud, is that distributed or those in your own data centers?

103
00:07:24,560 --> 00:07:25,560
Yeah.

104
00:07:25,560 --> 00:07:26,560
That's interesting.

105
00:07:26,560 --> 00:07:31,160
This is like a grid machine, sort of like folded or sitting at home.

106
00:07:31,160 --> 00:07:32,160
Yeah.

107
00:07:32,160 --> 00:07:33,160
Exactly.

108
00:07:33,160 --> 00:07:34,160
Same idea.

109
00:07:34,160 --> 00:07:36,560
Utilizing freely available compute time.

110
00:07:36,560 --> 00:07:41,840
I mean, not freely available, we prefer it, but available idle compute time in around

111
00:07:41,840 --> 00:07:42,840
the world.

112
00:07:42,840 --> 00:07:47,640
And this is, I think, a really interesting proposition and there's so much compute

113
00:07:47,640 --> 00:07:52,560
distributed now in people's laptops, PCs, cell phones.

114
00:07:52,560 --> 00:07:55,080
And if he could harness that, we could do a lot of computing.

115
00:07:55,080 --> 00:07:56,080
But of course, things change.

116
00:07:56,080 --> 00:08:00,560
That was the CPUs were perfectly good in the first several years when we were evolving

117
00:08:00,560 --> 00:08:02,000
stock traders.

118
00:08:02,000 --> 00:08:05,280
More recently, we evolved deep learning neural networks.

119
00:08:05,280 --> 00:08:09,200
And the need is a little different because they are trained on GPUs.

120
00:08:09,200 --> 00:08:13,440
So now we have to have access to GPUs and the demographics has changed a bit.

121
00:08:13,440 --> 00:08:16,320
And this is a very rapidly changing space.

122
00:08:16,320 --> 00:08:21,200
It's also become very economical to buy a bunch of GPUs, thousands of them perhaps, and

123
00:08:21,200 --> 00:08:22,640
build your own center.

124
00:08:22,640 --> 00:08:28,120
So we are continuous looking for opportunities to harness computation, whatever form it is.

125
00:08:28,120 --> 00:08:31,040
But we have expertise in both kinds of setups.

126
00:08:31,040 --> 00:08:32,040
Okay.

127
00:08:32,040 --> 00:08:37,800
And so is the company still using this highly distributed grid like infrastructure, or have

128
00:08:37,800 --> 00:08:41,600
you shifted to an owned infrastructure?

129
00:08:41,600 --> 00:08:42,600
Yeah.

130
00:08:42,600 --> 00:08:47,200
We're still using it and we also exploring other ways of getting the computation.

131
00:08:47,200 --> 00:08:50,120
They are, they serve a little bit different purposes.

132
00:08:50,120 --> 00:08:54,200
One of them is very massively parallel and not necessarily that reliable.

133
00:08:54,200 --> 00:08:56,520
And that's good for some applications.

134
00:08:56,520 --> 00:08:59,240
And in others, you need more reliability.

135
00:08:59,240 --> 00:09:03,160
The compute has to be accessible all the time and it has to finish.

136
00:09:03,160 --> 00:09:07,480
And therefore, you need different kinds of sources for compute and we are pursuing several

137
00:09:07,480 --> 00:09:09,600
sources as a result.

138
00:09:09,600 --> 00:09:14,680
And you give us examples of the kinds of applications that work well and for each?

139
00:09:14,680 --> 00:09:15,680
Yeah.

140
00:09:15,680 --> 00:09:22,200
So for instance, a stock trading is very robust because we are evolving traders and evolution

141
00:09:22,200 --> 00:09:23,200
in general.

142
00:09:23,200 --> 00:09:29,800
Evolution in general is robust because you have a population of solutions and you are testing

143
00:09:29,800 --> 00:09:35,960
that population in a distributed fashion across the different compute sites.

144
00:09:35,960 --> 00:09:40,440
And if something goes wrong and you don't get back an evaluation of one of your candidates,

145
00:09:40,440 --> 00:09:41,440
that's okay.

146
00:09:41,440 --> 00:09:43,200
You have hundreds of other candidates.

147
00:09:43,200 --> 00:09:48,720
It's okay to lose one candidate or have it only partially evaluated because there's

148
00:09:48,720 --> 00:09:53,800
a lot of noise in this evolution research process and it actually thrives on diversity

149
00:09:53,800 --> 00:09:55,960
and multiple alternatives.

150
00:09:55,960 --> 00:10:01,200
So it's not depend on any single alternative or an accurate evaluation.

151
00:10:01,200 --> 00:10:06,440
It's based on evaluating a lot of candidates and many times.

152
00:10:06,440 --> 00:10:11,040
And that you can do on a compute source that's very unreliable.

153
00:10:11,040 --> 00:10:16,600
Now if you are building a particular application based on say deep learning, you want that

154
00:10:16,600 --> 00:10:21,600
deep learning network to be very well built and very well trained and reliable and sometimes

155
00:10:21,600 --> 00:10:24,480
that training will take several days.

156
00:10:24,480 --> 00:10:29,240
So it has to be a reliable source so that you can get back that result of the training.

157
00:10:29,240 --> 00:10:31,840
Otherwise, you waste a lot of time on it.

158
00:10:31,840 --> 00:10:34,960
So that's where we need more reliable sources.

159
00:10:34,960 --> 00:10:35,960
Okay.

160
00:10:35,960 --> 00:10:42,640
And so is the company still working on financial services or did you migrate or pivot from

161
00:10:42,640 --> 00:10:45,960
financial services to the e-commerce solution?

162
00:10:45,960 --> 00:10:49,200
You know, we definitely working on trading as well.

163
00:10:49,200 --> 00:10:54,760
It's a more like an AI based hedge fund than financial services.

164
00:10:54,760 --> 00:10:57,560
And there we have expanded to different markets.

165
00:10:57,560 --> 00:11:02,960
So that part is growing and the algorithms have also evolved to some degree, but the

166
00:11:02,960 --> 00:11:05,800
base is still to run a hedge fund using AI.

167
00:11:05,800 --> 00:11:08,760
So the traders are automatic.

168
00:11:08,760 --> 00:11:13,240
They actually are looking at the stock market and deciding what the trends are and what

169
00:11:13,240 --> 00:11:15,800
should be done and then to make those trades.

170
00:11:15,800 --> 00:11:20,360
Of course, in the end, there's a person also watching this with a hand on a red button

171
00:11:20,360 --> 00:11:24,080
in case something goes wrong, but that almost never happens.

172
00:11:24,080 --> 00:11:27,360
I don't think it actually has happened in that urgency.

173
00:11:27,360 --> 00:11:30,440
But sometimes also the stock market is interesting that there might be something happening in

174
00:11:30,440 --> 00:11:31,440
a stock market.

175
00:11:31,440 --> 00:11:35,040
You can tell that now this is not a usual situation.

176
00:11:35,040 --> 00:11:40,320
It's going haywire and we'll stop the trading and let it come back and become more normal.

177
00:11:40,320 --> 00:11:45,560
So there's a human in the loop, but it is interesting that AI can do much of this on its own.

178
00:11:45,560 --> 00:11:51,880
And are you trading your own book or are you working with other hedge funds and helping

179
00:11:51,880 --> 00:11:52,880
them trade?

180
00:11:52,880 --> 00:11:53,880
Yeah.

181
00:11:53,880 --> 00:11:56,160
Currently trading our own funds.

182
00:11:56,160 --> 00:12:00,960
And there's a plan to open it up in the future for investors.

183
00:12:00,960 --> 00:12:01,960
Interesting.

184
00:12:01,960 --> 00:12:07,960
I wonder why, if you've got that working, it's kind of this perpetual money machine, right?

185
00:12:07,960 --> 00:12:10,800
Why even bother with retail?

186
00:12:10,800 --> 00:12:11,800
That's a good question.

187
00:12:11,800 --> 00:12:18,040
Well, in the AI world and in the startup world and in the Silicon Valley, it's always

188
00:12:18,040 --> 00:12:20,840
grow scale, get more.

189
00:12:20,840 --> 00:12:26,880
And this was a successful, is a successful technology, so now the question is, what else can

190
00:12:26,880 --> 00:12:27,880
you do with it?

191
00:12:27,880 --> 00:12:33,360
And indeed, it's nice to have several different technologies and bases, and especially then

192
00:12:33,360 --> 00:12:36,680
because you start getting a cross pollination of them.

193
00:12:36,680 --> 00:12:41,560
So the second one we looked at was very different from Evo's algorithms, was deep learning.

194
00:12:41,560 --> 00:12:47,760
And we've found that we could use it to encode human preferences, what humans perceive

195
00:12:47,760 --> 00:12:53,240
as similar and visually, and identify that this would be something that the retail industry

196
00:12:53,240 --> 00:12:59,240
could really use because retail is changing rapidly and fundamentally.

197
00:12:59,240 --> 00:13:04,920
From the brick and mortar stores, we are changing into internet-based commerce.

198
00:13:04,920 --> 00:13:10,040
And this is a really big change, but internet commerce is quite clunky still today.

199
00:13:10,040 --> 00:13:14,840
It's hard to find what you want in those web interfaces.

200
00:13:14,840 --> 00:13:19,640
And the reason that was pretty obvious was that you have to know what you're looking

201
00:13:19,640 --> 00:13:20,640
for.

202
00:13:20,640 --> 00:13:23,800
And you have to know the keywords for those items that you're looking for.

203
00:13:23,800 --> 00:13:28,360
And if you do, you may be able to find and do your satisfactory, some product that you

204
00:13:28,360 --> 00:13:31,640
interested in, but if you don't, it can be very frustrating.

205
00:13:31,640 --> 00:13:35,680
And as a matter of fact, we've done some tests on that, typically in an e-commerce site,

206
00:13:35,680 --> 00:13:42,000
in a week, 15% of the catalog is actually seen by the users because they have to go

207
00:13:42,000 --> 00:13:45,480
through these rigid categories and keywords.

208
00:13:45,480 --> 00:13:50,320
And it's very hard to find different variability or diversity in a catalog.

209
00:13:50,320 --> 00:13:54,000
We identified very early on that this is actually something that could be done differently.

210
00:13:54,000 --> 00:13:58,480
We could use human-like perception of similarity, visual similarities.

211
00:13:58,480 --> 00:14:05,560
So if you're thinking of, say, a catalog of shoes or jackets or sunglasses, then a human

212
00:14:05,560 --> 00:14:07,760
can identify something that he likes.

213
00:14:07,760 --> 00:14:12,080
So he likes very quickly by being presented a bunch of alternatives.

214
00:14:12,080 --> 00:14:13,680
This is the one I like the most.

215
00:14:13,680 --> 00:14:16,480
And then another set of candidates comes up.

216
00:14:16,480 --> 00:14:18,560
And these are now based on the first click.

217
00:14:18,560 --> 00:14:21,920
They are similar to the first click, but variation around that area.

218
00:14:21,920 --> 00:14:23,400
And now you can click again.

219
00:14:23,400 --> 00:14:28,480
And it turns out, in about seven clicks, you can find anything in the catalog, according

220
00:14:28,480 --> 00:14:31,160
to our evaluation.

221
00:14:31,160 --> 00:14:37,120
And in that process, the users actually see about 70% of the catalog each week.

222
00:14:37,120 --> 00:14:39,760
And this is a situation where everybody wins.

223
00:14:39,760 --> 00:14:43,840
As a user, you have access to more variety.

224
00:14:43,840 --> 00:14:49,880
And the e-commerce vendor will sell a motor catalog, and the manufacturers get their

225
00:14:49,880 --> 00:14:52,480
products out there for people to see.

226
00:14:52,480 --> 00:14:57,480
So it is just fundamentally a better way to access e-commerce catalogs.

227
00:14:57,480 --> 00:15:02,440
And it's based on understanding human perception and training machines, turning neural networks

228
00:15:02,440 --> 00:15:04,120
to represent those similarities.

229
00:15:04,120 --> 00:15:08,320
That sounds like a really interesting application.

230
00:15:08,320 --> 00:15:09,320
So both...

231
00:15:09,320 --> 00:15:15,000
Well, actually, you said the financial services you started with applying evolutionary algorithms

232
00:15:15,000 --> 00:15:21,360
and with this e-commerce application, the focus is more on deep learning.

233
00:15:21,360 --> 00:15:22,360
Exactly.

234
00:15:22,360 --> 00:15:23,360
Is that the right way to think about it?

235
00:15:23,360 --> 00:15:24,360
Yes.

236
00:15:24,360 --> 00:15:25,360
That's exactly right.

237
00:15:25,360 --> 00:15:29,560
Maybe let's dive into evolutionary algorithms now.

238
00:15:29,560 --> 00:15:34,400
Tell us, you know, you gave us a little bit of a taste of it earlier, but what does that

239
00:15:34,400 --> 00:15:35,400
mean?

240
00:15:35,400 --> 00:15:36,400
All right.

241
00:15:36,400 --> 00:15:41,600
So evolution, and you can see there's this form of similar to reinforcement learning,

242
00:15:41,600 --> 00:15:46,400
where you do not get the correct answer to learn from.

243
00:15:46,400 --> 00:15:50,800
I mean, in deep learning and most of the machine learning applications that are out there

244
00:15:50,800 --> 00:15:55,360
are based on these large data sets, where somebody has collected the data.

245
00:15:55,360 --> 00:16:01,160
This is a situation, and this is the right categorization or action at that situation.

246
00:16:01,160 --> 00:16:07,480
For instance, you know, images and their classification, categorization, or speech and a transcription

247
00:16:07,480 --> 00:16:08,800
of the speech.

248
00:16:08,800 --> 00:16:14,560
And this is very doable, and we have systems that have come up, mechanical turk and

249
00:16:14,560 --> 00:16:18,240
mighty AI and others who are now collecting and forming such data sets.

250
00:16:18,240 --> 00:16:24,240
And then systems like deep learning can be involved to build models of those kinds of

251
00:16:24,240 --> 00:16:25,240
tasks.

252
00:16:25,240 --> 00:16:31,440
And it's very powerful, but there's a lot of tasks in the world where those actual correct

253
00:16:31,440 --> 00:16:33,160
answers are not known.

254
00:16:33,160 --> 00:16:38,520
So for instance, driving a car, a robot navigating, playing any kind of game.

255
00:16:38,520 --> 00:16:42,480
You don't know what the actual optimal answers are.

256
00:16:42,480 --> 00:16:46,080
And in such domains, the learning is based on exploration.

257
00:16:46,080 --> 00:16:49,000
You try out things and see how well they work.

258
00:16:49,000 --> 00:16:51,600
In a big picture, this might be called reinforcement learning.

259
00:16:51,600 --> 00:16:53,480
You get reinforcement to your actions.

260
00:16:53,480 --> 00:16:58,480
Now reinforcement learning, actually, as a term refers to a category or class of algorithms

261
00:16:58,480 --> 00:17:03,120
that is a little different from evolution into the social or traditional sense.

262
00:17:03,120 --> 00:17:07,480
They are mostly those reinforcement learning items are mostly based on value function approaches

263
00:17:07,480 --> 00:17:12,480
where you list all your actions and learn how good each action is in each state.

264
00:17:12,480 --> 00:17:16,600
And that's a different approach to solving a problem when you don't have gradients, when

265
00:17:16,600 --> 00:17:19,800
you don't have supervised targets.

266
00:17:19,800 --> 00:17:21,200
Evolution is a different approach.

267
00:17:21,200 --> 00:17:27,120
The idea there is that you have a whole population of solutions and you evaluate each solution.

268
00:17:27,120 --> 00:17:33,520
And as in biology, those who evaluate well get to reproduce more and generate offspring.

269
00:17:33,520 --> 00:17:37,680
And the offspring is somehow generated in various ways from the parents.

270
00:17:37,680 --> 00:17:41,640
You take an encoding of the parent and encoding of another parent.

271
00:17:41,640 --> 00:17:44,400
And then you cross them over just like in biology.

272
00:17:44,400 --> 00:17:47,880
So there's some kind of a linear encoding us like DNA.

273
00:17:47,880 --> 00:17:51,800
You can take part of that DNA from one parent and another part from another parent to form

274
00:17:51,800 --> 00:17:52,800
an offspring.

275
00:17:52,800 --> 00:17:57,120
And that means that you are combining properties of both parents into the offspring.

276
00:17:57,120 --> 00:18:00,800
And this is the fundamental idea of the evolutionary search.

277
00:18:00,800 --> 00:18:06,760
You are trying to recombine components or schemas or building blocks that are sometimes

278
00:18:06,760 --> 00:18:09,880
called to find better combinations of them.

279
00:18:09,880 --> 00:18:12,520
And there's another component which is important as well.

280
00:18:12,520 --> 00:18:13,520
That's my question.

281
00:18:13,520 --> 00:18:14,520
Yeah.

282
00:18:14,520 --> 00:18:17,560
Before you dive into that, I have a question about something you said about

283
00:18:17,560 --> 00:18:19,520
reinforcement learning.

284
00:18:19,520 --> 00:18:26,080
And that is you said that the reinforcement learning is characterized by these value functions.

285
00:18:26,080 --> 00:18:31,880
And in order to evaluate these value functions, we apply gradient-based approaches.

286
00:18:31,880 --> 00:18:38,120
And you said that in order to do that, we can only do that under supervision of some sort.

287
00:18:38,120 --> 00:18:44,200
And elaborate on that because we're typically applying, as you said, reinforcement learning

288
00:18:44,200 --> 00:18:47,840
in what we think of unsupervised problems.

289
00:18:47,840 --> 00:18:48,840
Right.

290
00:18:48,840 --> 00:18:51,520
So, supervised learning is different from reinforcement learning.

291
00:18:51,520 --> 00:18:55,320
So that supervised learning means that we do have correct answers and we can calculate

292
00:18:55,320 --> 00:18:56,320
gradients.

293
00:18:56,320 --> 00:18:59,640
In reinforcement learning, we don't have those gradients.

294
00:18:59,640 --> 00:19:01,560
We don't have correct targets.

295
00:19:01,560 --> 00:19:03,560
So it's based on exploration.

296
00:19:03,560 --> 00:19:09,880
So in reinforcement learning, the agent will try out different actions and will eventually

297
00:19:09,880 --> 00:19:13,160
get an evaluation of how well those actions worked out.

298
00:19:13,160 --> 00:19:19,000
And then dynamic programming is, incremental dynamic programming is typically used then to

299
00:19:19,000 --> 00:19:26,840
propagate that reward back to the earlier actions and changing their value in this representation

300
00:19:26,840 --> 00:19:28,920
of how good each action is.

301
00:19:28,920 --> 00:19:31,920
So it is not supervised.

302
00:19:31,920 --> 00:19:36,200
It thinks it's a little confused because, confusing because this, in this purest form,

303
00:19:36,200 --> 00:19:41,720
this kind of value function learning applies to a table of actions in a table of states.

304
00:19:41,720 --> 00:19:43,120
But that is very limiting.

305
00:19:43,120 --> 00:19:47,880
So we need to be able to approximate a large number of actions and large number of states.

306
00:19:47,880 --> 00:19:52,600
And that means that a lot of times we use a function approximator and then use gradient

307
00:19:52,600 --> 00:20:00,000
descent to build that function approximator using the propagated values as targets.

308
00:20:00,000 --> 00:20:06,160
So we use gradient descent as a way of generalizing reinforcement learning.

309
00:20:06,160 --> 00:20:11,160
But the information for reinforcement learning comes from the outcomes of these exploration

310
00:20:11,160 --> 00:20:12,160
episodes.

311
00:20:12,160 --> 00:20:13,520
Right, right.

312
00:20:13,520 --> 00:20:20,720
And one of the challenges with reinforcement learning is that you are, sorry, way to articulate

313
00:20:20,720 --> 00:20:26,560
this year, kind of weighing exploration and exploitation and your exploration tends to

314
00:20:26,560 --> 00:20:29,800
be focused around fixed paths.

315
00:20:29,800 --> 00:20:36,880
And you don't really have a built-in mechanism to combine different exploratory directions.

316
00:20:36,880 --> 00:20:40,680
And that sounds like that's one of the advantages of evolutionary algorithms.

317
00:20:40,680 --> 00:20:41,680
Yes.

318
00:20:41,680 --> 00:20:43,120
This is absolutely right.

319
00:20:43,120 --> 00:20:47,640
So there are two main challenges for reinforcement learning.

320
00:20:47,640 --> 00:20:49,800
One of them is this large search space.

321
00:20:49,800 --> 00:20:54,520
Like I said, it works really well if you have a smaller space where you can enumerate

322
00:20:54,520 --> 00:20:56,520
all your actions and states.

323
00:20:56,520 --> 00:21:01,880
And it's difficult when those grow and become maybe continuous.

324
00:21:01,880 --> 00:21:06,840
And another problem is that it works well when your state contains all the information

325
00:21:06,840 --> 00:21:07,840
from the past.

326
00:21:07,840 --> 00:21:11,240
It's uniquely specified or exactly specified.

327
00:21:11,240 --> 00:21:16,480
If you have partially observable states, then it becomes very difficult to learn using

328
00:21:16,480 --> 00:21:18,320
this kind of value function approach.

329
00:21:18,320 --> 00:21:24,360
And both of those are actually addressed if you evolve, use evolution to construct neural

330
00:21:24,360 --> 00:21:26,760
networks in such tasks.

331
00:21:26,760 --> 00:21:32,240
So neural networks work very well when they are in continuous domains.

332
00:21:32,240 --> 00:21:34,040
You don't have to enumerate all the possibilities.

333
00:21:34,040 --> 00:21:37,320
They are already doing the function approximation.

334
00:21:37,320 --> 00:21:41,320
And regarding the second problem, you can evolve recurrent neural networks.

335
00:21:41,320 --> 00:21:46,680
And when you have recurrency, your entire sequence of actions that you've taken up to this

336
00:21:46,680 --> 00:21:51,320
state is taken into account in making the next decision.

337
00:21:51,320 --> 00:21:56,800
And therefore, it can disambiguate much of the state ambiguity and therefore work better

338
00:21:56,800 --> 00:22:00,320
and partially observable problems as well.

339
00:22:00,320 --> 00:22:01,320
So that is true.

340
00:22:01,320 --> 00:22:05,760
You have to have a decision-making system such as a neural network as a base.

341
00:22:05,760 --> 00:22:10,520
But then you can evolve that neural network in order to get over these two challenges

342
00:22:10,520 --> 00:22:12,160
to reinforcement learning.

343
00:22:12,160 --> 00:22:18,760
One challenge to reinforcement learning is the issue of reward attribution.

344
00:22:18,760 --> 00:22:26,240
Is that covered under the partially observable state issue or is that a separate category

345
00:22:26,240 --> 00:22:27,840
of challenge?

346
00:22:27,840 --> 00:22:30,440
And how was that addressed by evolutionary algorithms?

347
00:22:30,440 --> 00:22:33,200
Well, they are related.

348
00:22:33,200 --> 00:22:40,360
So reward attribution, if you have a partially observable problem and you make a decision,

349
00:22:40,360 --> 00:22:44,000
part of the credit for that success for that decision depends on how you got to that

350
00:22:44,000 --> 00:22:47,000
state, those unobserved variables.

351
00:22:47,000 --> 00:22:51,840
So if you have a system that takes into account all the actions that got you to that state,

352
00:22:51,840 --> 00:22:55,200
you are doing the credit assignment better and more accurately.

353
00:22:55,200 --> 00:22:57,720
So yeah, in that sense, they are related.

354
00:22:57,720 --> 00:23:06,320
So maybe is there a way to, you know, granted that a podcast is limited and it's been with,

355
00:23:06,320 --> 00:23:12,320
you know, not having a visual component here, but is there a way to kind of walk us through

356
00:23:12,320 --> 00:23:20,120
the, you know, the setup for an evolutionary algorithm and how it's applied to training

357
00:23:20,120 --> 00:23:21,120
neural networks?

358
00:23:21,120 --> 00:23:22,120
Sure.

359
00:23:22,120 --> 00:23:25,720
And I've been vigorously waving my hands here, like you haven't been able to see all

360
00:23:25,720 --> 00:23:26,720
along.

361
00:23:26,720 --> 00:23:30,320
So yeah, the big, big starting point that's different from reinforcement learning is,

362
00:23:30,320 --> 00:23:31,920
is the population.

363
00:23:31,920 --> 00:23:37,520
So you have a collection of individuals and typically it's 100 to 100 maybe.

364
00:23:37,520 --> 00:23:40,680
In some cases, different, different variances might be smaller.

365
00:23:40,680 --> 00:23:42,520
And so what is an individual?

366
00:23:42,520 --> 00:23:43,520
Yes.

367
00:23:43,520 --> 00:23:48,280
It's indeed individuals represents a solution, a potential solution to the problem.

368
00:23:48,280 --> 00:23:51,640
Like in this case, in the simplest case, it could be a neural network.

369
00:23:51,640 --> 00:23:55,600
And it means all the weights and all the nodes, weight values and all the nodes and the

370
00:23:55,600 --> 00:23:57,200
structure of the neural network.

371
00:23:57,200 --> 00:23:58,200
Okay.

372
00:23:58,200 --> 00:24:00,720
So it's pre constrained by architecture.

373
00:24:00,720 --> 00:24:06,840
You've defined an architecture for this target neural network and you're using the evolutionary

374
00:24:06,840 --> 00:24:13,680
algorithm to figure out its various parameters as opposed to evolve the network architecture

375
00:24:13,680 --> 00:24:14,680
itself.

376
00:24:14,680 --> 00:24:16,720
Well, that's the simplest way of doing it.

377
00:24:16,720 --> 00:24:21,520
But the most powerful evolutionary neural network systems actually evolve the architecture

378
00:24:21,520 --> 00:24:22,520
as well.

379
00:24:22,520 --> 00:24:23,520
Oh, right.

380
00:24:23,520 --> 00:24:29,400
You have to have a representation that can be encoded into a, into a string typically

381
00:24:29,400 --> 00:24:32,960
or a tree, but most often a string like DNA.

382
00:24:32,960 --> 00:24:38,160
So you have a DNA representation of that solution and it may include the weights on the connections,

383
00:24:38,160 --> 00:24:43,200
making, include the hyper parameters like the slope of the sigmoid or something like that.

384
00:24:43,200 --> 00:24:48,360
And it may include the topology, like how the graph is actually connected between nodes.

385
00:24:48,360 --> 00:24:50,520
It's just an issue of how you encode it.

386
00:24:50,520 --> 00:24:54,520
But the, but the end result is that you will have some kind of a string representation

387
00:24:54,520 --> 00:24:59,720
or a tree in a more general sense for each of the individuals, for each of the neural network,

388
00:24:59,720 --> 00:25:01,120
the solutions.

389
00:25:01,120 --> 00:25:03,760
And now you, and now you have a population of them.

390
00:25:03,760 --> 00:25:08,600
And then on that population, you run an evolutionary algorithm and there are many flavors of those

391
00:25:08,600 --> 00:25:13,520
what I've been talking about is, is the genetic algorithm flavor, John Holland's tradition.

392
00:25:13,520 --> 00:25:15,040
There are many others too.

393
00:25:15,040 --> 00:25:20,120
But that is the most closely associated with biology in that you take each individual,

394
00:25:20,120 --> 00:25:23,120
you test them in a task, you evaluate them in a task.

395
00:25:23,120 --> 00:25:26,200
And that's where the parallel computing comes in because you have a population, you can

396
00:25:26,200 --> 00:25:31,600
send each individual to a different machine across the internet to be evaluated.

397
00:25:31,600 --> 00:25:37,320
And sometimes most of the time the evaluation is the most computationally expensive part

398
00:25:37,320 --> 00:25:41,800
of the algorithm because you may be driving a robot or you may be doing stock trading

399
00:25:41,800 --> 00:25:44,960
or whatever it is that you're doing, it takes time to evaluate.

400
00:25:44,960 --> 00:25:49,120
And that's really nice because evolution algorithms parallelize very well in that sense.

401
00:25:49,120 --> 00:25:51,440
Each individual can be evaluated separately.

402
00:25:51,440 --> 00:25:55,200
And then you get back to your values, the fitness values, how well they perform in the

403
00:25:55,200 --> 00:25:56,200
task.

404
00:25:56,200 --> 00:26:01,280
And now the entire population is associated, each one individual is associated with a fitness

405
00:26:01,280 --> 00:26:05,240
value and you can find the best ones and you can find the worst ones.

406
00:26:05,240 --> 00:26:11,360
The worst ones you throw away and the best ones you pair up so that you can do crossover,

407
00:26:11,360 --> 00:26:17,040
as I mentioned earlier, and also the second component of evolution, the mutation.

408
00:26:17,040 --> 00:26:24,120
Also, it gives you new combinations of building blocks or schemas, partial solutions.

409
00:26:24,120 --> 00:26:28,320
Mutation creates new ones because it's not necessarily, you usually initialize the population

410
00:26:28,320 --> 00:26:32,920
randomly, like random weight neural networks, but it doesn't guarantee that you have the

411
00:26:32,920 --> 00:26:35,200
weight in the right place when you need it.

412
00:26:35,200 --> 00:26:38,880
So mutation is a mechanism for creating that.

413
00:26:38,880 --> 00:26:43,520
It's a random change in the weight value or random change in the topology that happens

414
00:26:43,520 --> 00:26:49,200
on a low probability, about 2% or 4% or so, as part of that evolutionary step.

415
00:26:49,200 --> 00:26:54,840
So you take in your parents and you create their offspring and that offspring then is

416
00:26:54,840 --> 00:26:59,120
used to replace those poor individuals that were thrown away.

417
00:26:59,120 --> 00:27:04,000
And that creates your new population, a new generation, and then this process repeats.

418
00:27:04,000 --> 00:27:10,600
In this process, in essence, you are doing a parallel search in the solution space, starting

419
00:27:10,600 --> 00:27:16,640
from your 100 initial random individuals, you gradually reward those that perform the

420
00:27:16,640 --> 00:27:22,960
best and you perform more search in the areas where there's better solutions.

421
00:27:22,960 --> 00:27:24,200
But it's in parallel.

422
00:27:24,200 --> 00:27:29,800
So you're still not following just one potential kind of solution, but you are in parallel

423
00:27:29,800 --> 00:27:33,520
focusing on multiple potentially good solutions.

424
00:27:33,520 --> 00:27:38,400
And eventually, then you find some really good ones and the algorithm focuses on refining

425
00:27:38,400 --> 00:27:43,080
that area and finding the absolute best in a smaller area around the best solution.

426
00:27:43,080 --> 00:27:44,920
That's what ends up happening.

427
00:27:44,920 --> 00:27:47,480
And that's a evolution algorithm in a nutshell.

428
00:27:47,480 --> 00:27:52,600
As I said, there are many variations and they're also interesting to talk about, but that

429
00:27:52,600 --> 00:27:58,240
principle of parallel search in a solution space, which is directed by these periodic

430
00:27:58,240 --> 00:28:02,120
evaluations in the real world, is the essence of the method.

431
00:28:02,120 --> 00:28:03,120
Okay.

432
00:28:03,120 --> 00:28:09,760
So it sounds like, maybe perhaps not if applied to a deep neural network, but if applied

433
00:28:09,760 --> 00:28:17,080
to a more simple type of machine learning model, essentially what we're doing is a search

434
00:28:17,080 --> 00:28:23,240
of the solution space that could be analogous to a grid search of hyper parameters.

435
00:28:23,240 --> 00:28:33,080
But in the evolutionary world, we're able to, we're able to constrain our search and

436
00:28:33,080 --> 00:28:34,560
as well as parallelize it.

437
00:28:34,560 --> 00:28:40,040
And so I'm imagining that, you know, from like a big O perspective, this is, you know,

438
00:28:40,040 --> 00:28:44,840
log, log and time as opposed to N or something like that with a grid search.

439
00:28:44,840 --> 00:28:46,360
Am I thinking about that the right way?

440
00:28:46,360 --> 00:28:47,360
Yeah.

441
00:28:47,360 --> 00:28:48,360
That's intuitively.

442
00:28:48,360 --> 00:28:49,360
I think that's correct.

443
00:28:49,360 --> 00:28:53,680
It's, of course, a different question of can we prove that's results, but that is

444
00:28:53,680 --> 00:28:55,040
in essence what's happening.

445
00:28:55,040 --> 00:28:59,040
So it is, it is not laying all your eggs in one basket.

446
00:28:59,040 --> 00:29:05,400
It is a pursuing multiple alternatives at once and it's then gradually focusing the search

447
00:29:05,400 --> 00:29:10,480
where the most promises, whereas creatures is just brute force approach, you spread your

448
00:29:10,480 --> 00:29:15,600
all your individuals as wide as possible and find something in there and end up wasting

449
00:29:15,600 --> 00:29:19,720
a lot of time and also not necessarily finding the very best one because you are limited

450
00:29:19,720 --> 00:29:21,360
by the, by the grid.

451
00:29:21,360 --> 00:29:25,000
So in that sense, that you could think of it as more intelligent version of that, much

452
00:29:25,000 --> 00:29:26,000
more intelligent.

453
00:29:26,000 --> 00:29:29,320
It's actually sometimes amazing how efficient it can be.

454
00:29:29,320 --> 00:29:35,440
So we've done a sentient, a couple of demonstrations in a multiplexer domain, which is a very easy

455
00:29:35,440 --> 00:29:41,640
domain to solve symbolically, but you can create it into a, turn it into a search problem.

456
00:29:41,640 --> 00:29:46,600
You have to find the right encoding of bits so that you get the right answers to the,

457
00:29:46,600 --> 00:29:49,800
to any given address from your bit string.

458
00:29:49,800 --> 00:29:53,240
Now the search space is huge in this case.

459
00:29:53,240 --> 00:29:59,200
It can be, and we've solved 70 bit multiplexer, which the search space is two to the two to

460
00:29:59,200 --> 00:30:00,600
the seven days.

461
00:30:00,600 --> 00:30:05,520
It's a very large search space and it's still evolution can find solutions quite quickly

462
00:30:05,520 --> 00:30:09,080
in hundreds of thousands or millions of trials.

463
00:30:09,080 --> 00:30:14,040
You'll find solution in that, that, that, that magnificently large space.

464
00:30:14,040 --> 00:30:19,600
Can you take a step back and elaborate on that problem and how it's applied or how it's

465
00:30:19,600 --> 00:30:20,600
used in practice?

466
00:30:20,600 --> 00:30:25,000
Well, multiplexer is, it's a benchmark problem.

467
00:30:25,000 --> 00:30:27,880
It's not something that you would solve using evolution algorithm.

468
00:30:27,880 --> 00:30:33,240
It's only used to illustrate how, or evaluate the different algorithms and how it works,

469
00:30:33,240 --> 00:30:35,360
how, how well they work.

470
00:30:35,360 --> 00:30:39,200
You have a number of address bits and then you have a number of data bits.

471
00:30:39,200 --> 00:30:41,720
So that is your individual.

472
00:30:41,720 --> 00:30:47,800
And then what you're learning is rules, how to map those address bits to a data bit.

473
00:30:47,800 --> 00:30:49,440
And that's what the multiplexer does.

474
00:30:49,440 --> 00:30:52,000
Given an address, it gives you one of the data bits.

475
00:30:52,000 --> 00:30:57,840
So you can evolve it to solve that in various ways, you evolve rules and the rules express

476
00:30:57,840 --> 00:31:02,480
that, okay, if we have this, this and this bit in the addresses, then this is the output.

477
00:31:02,480 --> 00:31:08,880
So now the space that you're searching is the set of all, all rule sets.

478
00:31:08,880 --> 00:31:12,480
And that space is humongous and we can calculate how large it is.

479
00:31:12,480 --> 00:31:17,680
This is worked done by John Kosa a long time ago just to demonstrate how effective learning

480
00:31:17,680 --> 00:31:20,440
algorithms can be, evolution, learning algorithms can be.

481
00:31:20,440 --> 00:31:25,440
At that time, I think we were considering 11 multiplexer, which has a search base of

482
00:31:25,440 --> 00:31:29,280
10 to the, I think it's 616.

483
00:31:29,280 --> 00:31:32,440
And you can solve it by searching this set of rules.

484
00:31:32,440 --> 00:31:38,800
And in the rules, you have a certain way of identifying the input bits and then ending

485
00:31:38,800 --> 00:31:44,560
and auring them and so on, performing logical operations so that in the end, you get the

486
00:31:44,560 --> 00:31:49,440
data bit and you can calculate how large that search base is, how many ways there are

487
00:31:49,440 --> 00:31:54,800
to combine these input bits, address bits and the logical operations on them.

488
00:31:54,800 --> 00:31:58,880
And therefore you can estimate how large the search base is and how hard the problem is.

489
00:31:58,880 --> 00:32:08,640
So in 11 case, it's 10 to the 616th and we expanded it to the 2 to the 7th, and it still

490
00:32:08,640 --> 00:32:09,640
works.

491
00:32:09,640 --> 00:32:14,200
And that is, I think to me, it's very amazing that evolution can very quickly identify

492
00:32:14,200 --> 00:32:19,800
what actually works, those component solutions, those building blocks and then put them together

493
00:32:19,800 --> 00:32:23,520
into a solution when search base is huge.

494
00:32:23,520 --> 00:32:24,520
So that is power.

495
00:32:24,520 --> 00:32:29,480
But not everything is, of course, amenable to evolutionary computation.

496
00:32:29,480 --> 00:32:35,160
Before we jump into that, you mentioned provability.

497
00:32:35,160 --> 00:32:38,560
Can you elaborate on the work that's been done there?

498
00:32:38,560 --> 00:32:43,160
For example, you mentioned in describing evolutionary algorithm that you kind of throw

499
00:32:43,160 --> 00:32:52,000
away the worst performing models or parameter sets, and then you mate, if that's the right

500
00:32:52,000 --> 00:32:54,680
word, the best performing ones.

501
00:32:54,680 --> 00:33:01,320
And you know, is there a formal proof that we're not susceptible to some local minimum

502
00:33:01,320 --> 00:33:06,960
maxima, and you might get some better result by mating non-performers and performers?

503
00:33:06,960 --> 00:33:10,440
Yeah, there's quite a bit of theory on evolutionary algorithms.

504
00:33:10,440 --> 00:33:15,400
It started already in the 70s with the Schema theorem, and that says the Schema theorem

505
00:33:15,400 --> 00:33:22,280
states that in this process, the shorter the Schema's are, the combinations of elements

506
00:33:22,280 --> 00:33:26,160
in the solution, in that bit string, in that genetic string.

507
00:33:26,160 --> 00:33:32,320
The shorter they are, and the more powerful they are, the more strongly they actually affect

508
00:33:32,320 --> 00:33:34,440
the fitness of the organism.

509
00:33:34,440 --> 00:33:39,400
Then more prominent, they will be in future generations, they will become more prevalent

510
00:33:39,400 --> 00:33:42,880
in the population, and that is a theoretical result.

511
00:33:42,880 --> 00:33:49,600
The Schema theorem shows that this happens in this mechanism, and how does a short Schema

512
00:33:49,600 --> 00:33:52,600
apply to a practical problem?

513
00:33:52,600 --> 00:33:53,600
What does that mean?

514
00:33:53,600 --> 00:34:00,240
Well, so Schema lengths means that you have, well, so let's think of a genetic encoding,

515
00:34:00,240 --> 00:34:04,400
for instance, a string of weights on a neural network.

516
00:34:04,400 --> 00:34:09,560
So the Schema means that a good neural network has this weight on this connection, this

517
00:34:09,560 --> 00:34:12,040
weight on this connection, and this weight on this connection.

518
00:34:12,040 --> 00:34:14,160
That's a Schema of length 3.

519
00:34:14,160 --> 00:34:20,000
And it may be that indeed you have these kinds of interactions between weights in a neural

520
00:34:20,000 --> 00:34:24,760
network, and you have to set them right in order for the network to perform well.

521
00:34:24,760 --> 00:34:30,640
And the Schema theorem just says that if they are short, small segments of weights, or segments

522
00:34:30,640 --> 00:34:34,760
of the neural network that are powerful, they are easier to find.

523
00:34:34,760 --> 00:34:41,560
Now if the Schema covers a large number of weights, that's more difficult to find.

524
00:34:41,560 --> 00:34:43,040
Obviously, it makes sense, right?

525
00:34:43,040 --> 00:34:47,200
So you have to set more numbers correctly in order to see the benefit.

526
00:34:47,200 --> 00:34:51,280
And the Schema theorem just says that if you have short Schema's easy ways of gaining

527
00:34:51,280 --> 00:34:55,680
the benefit, finding these three weights of the right values, that's going to be very

528
00:34:55,680 --> 00:34:59,960
easy to find, and it will very quickly become prominent in the population.

529
00:34:59,960 --> 00:35:01,880
And from the point of view of biology, that makes sense.

530
00:35:01,880 --> 00:35:06,960
I mean, if there are some genes that are very short, I mean, just single genes instead

531
00:35:06,960 --> 00:35:11,200
of interaction with multiple genes, that's going to be very prominent and very quickly

532
00:35:11,200 --> 00:35:13,120
propagate to the population.

533
00:35:13,120 --> 00:35:17,720
And that's the mathematics of Schema theorem just expresses that idea.

534
00:35:17,720 --> 00:35:25,440
And when we're training with evolutionary algorithms, is the Schema length a constant,

535
00:35:25,440 --> 00:35:31,480
or does this method kind of zoom in and zoom out to different levels of resolution?

536
00:35:31,480 --> 00:35:32,480
Yes, exactly.

537
00:35:32,480 --> 00:35:33,480
It will do that.

538
00:35:33,480 --> 00:35:36,560
We don't know ahead of time what the Schemas are like.

539
00:35:36,560 --> 00:35:40,080
So we define the encoding.

540
00:35:40,080 --> 00:35:44,880
And you try to put in as much insight into that encoding as possible.

541
00:35:44,880 --> 00:35:49,560
You try to define a search space where you believe, first of all, that the solutions lie

542
00:35:49,560 --> 00:35:54,760
in that search space, and also that it's easy to move around in that search space.

543
00:35:54,760 --> 00:35:59,680
And that is that requires human thinking and creativity, but evolution will then find

544
00:35:59,680 --> 00:36:04,320
you the best combinations of those elements in that search space.

545
00:36:04,320 --> 00:36:05,680
But encoding matters.

546
00:36:05,680 --> 00:36:09,760
In some cases, it is obvious and it's given by the domain very much.

547
00:36:09,760 --> 00:36:14,760
Like in neural networks, it's an obvious encoding to have ways put together into a string.

548
00:36:14,760 --> 00:36:18,920
But then when you think about it, some more you realize that he actually is a topology

549
00:36:18,920 --> 00:36:20,920
of the neural network matters as well.

550
00:36:20,920 --> 00:36:25,760
And then you want to have some way of encoding topology and letting evolution discover different

551
00:36:25,760 --> 00:36:26,760
topologies.

552
00:36:26,760 --> 00:36:31,480
And then you discover things like, well, if in order to make it easier to search the space,

553
00:36:31,480 --> 00:36:35,560
which now became much bigger because it's all the topologies in addition to just all

554
00:36:35,560 --> 00:36:41,920
the weight values, a clever idea is that let's start with a very small neural network, a

555
00:36:41,920 --> 00:36:43,240
small search space.

556
00:36:43,240 --> 00:36:48,200
Initially, we just start with neural networks that connect inputs directly to the outputs.

557
00:36:48,200 --> 00:36:53,400
No hidden nodes at all and let evolution run in that space for a while, find good, such

558
00:36:53,400 --> 00:36:55,040
simple networks.

559
00:36:55,040 --> 00:36:57,520
And then we'll add complexity.

560
00:36:57,520 --> 00:37:00,240
We add a hidden node and another hidden node.

561
00:37:00,240 --> 00:37:04,880
And then we add recurrent connections and gradually discover complexity as we go.

562
00:37:04,880 --> 00:37:08,880
And this principle, instead of starting with all kinds of topologies of neural networks

563
00:37:08,880 --> 00:37:14,520
as the initial population, if we start small, if we start simple and gradually complexify

564
00:37:14,520 --> 00:37:18,920
evolution is much more powerful in finding these complex solutions.

565
00:37:18,920 --> 00:37:23,600
So that's what I meant by being smart about how you encode it and how you let evolution

566
00:37:23,600 --> 00:37:25,280
to search the space.

567
00:37:25,280 --> 00:37:26,280
It makes a big difference.

568
00:37:26,280 --> 00:37:31,680
And this is one principle has turned out over and over again to be very useful.

569
00:37:31,680 --> 00:37:36,480
How complex can you get with this technique?

570
00:37:36,480 --> 00:37:45,920
If you come up with an encoding that can represent convolution layers and rectifying layers

571
00:37:45,920 --> 00:37:53,080
and the kinds of things that we do and CNNs for computer vision, can this thing come

572
00:37:53,080 --> 00:37:59,800
up with a very deep model, the types of models that we're using for object recognition nowadays

573
00:37:59,800 --> 00:38:01,600
or is it more limited?

574
00:38:01,600 --> 00:38:02,600
Yes.

575
00:38:02,600 --> 00:38:03,600
It's a good question.

576
00:38:03,600 --> 00:38:07,120
So there's really been two approaches to doing this.

577
00:38:07,120 --> 00:38:11,960
And the first one I just described was that you have an encoding that in principle could

578
00:38:11,960 --> 00:38:15,760
encode anything, any kind of connectivity and you build up from that.

579
00:38:15,760 --> 00:38:20,160
And that is actually very good on tasks like the reinforcement learning problem where you

580
00:38:20,160 --> 00:38:26,240
have to define a custom kind of recurrency that retains just the right information over

581
00:38:26,240 --> 00:38:30,680
time for you to disambiguate the partially observable problem.

582
00:38:30,680 --> 00:38:36,280
Very recently in the last two years or so, maybe two years or so, and other approaches

583
00:38:36,280 --> 00:38:41,120
emerged and that's specifically to evolve deep learning architectures.

584
00:38:41,120 --> 00:38:46,400
And there's an interesting difference in that those architectures are composed of specific

585
00:38:46,400 --> 00:38:47,400
components.

586
00:38:47,400 --> 00:38:54,920
You mentioned different convolutional neural networks, LSTM networks, drop out as a parameter

587
00:38:54,920 --> 00:39:01,360
max pool layers, all kinds of structures now exist that people compose these deep learning

588
00:39:01,360 --> 00:39:02,680
architectures from.

589
00:39:02,680 --> 00:39:06,720
And it now makes sense to do this a little differently because we know that there are some

590
00:39:06,720 --> 00:39:08,920
components that are useful.

591
00:39:08,920 --> 00:39:15,440
Well, let's give those components as a source material or raw material for evolution.

592
00:39:15,440 --> 00:39:20,120
And now you can come up with a mechanism that maybe operates in a couple of different

593
00:39:20,120 --> 00:39:25,240
levels, you could evolve the weights, you could evolve the components, and then you could

594
00:39:25,240 --> 00:39:29,560
evolve the overall topology that's based on those components.

595
00:39:29,560 --> 00:39:34,440
And this is currently where the deep learning neural evolution is, evolving deep learning

596
00:39:34,440 --> 00:39:35,440
networks.

597
00:39:35,440 --> 00:39:38,880
There are multiple approaches, but by and large, they do this.

598
00:39:38,880 --> 00:39:45,880
They evolve the hyper parameters of those networks and maybe the topology of the networks.

599
00:39:45,880 --> 00:39:50,720
But then the weights are trained using a supervised training set like image recognition,

600
00:39:50,720 --> 00:39:51,720
you mentioned.

601
00:39:51,720 --> 00:39:57,520
So there's still a big benefit from evolving those deep learning networks, even if you're

602
00:39:57,520 --> 00:40:02,600
applying it eventually to a supervised problem, because it's very hard to construct the right

603
00:40:02,600 --> 00:40:04,800
topology for your problem.

604
00:40:04,800 --> 00:40:07,560
Let's evolve and do that.

605
00:40:07,560 --> 00:40:10,720
And then use the training to set the weights.

606
00:40:10,720 --> 00:40:14,880
Weights there are millions, billions of weights, but potentially it's very hard for evolution

607
00:40:14,880 --> 00:40:20,000
to get every single one right if it needs to do a mutation to get the weight value right.

608
00:40:20,000 --> 00:40:21,920
Deep learning is much better to doing that.

609
00:40:21,920 --> 00:40:26,960
But the topology matters as well, and the architects of the components matters as well,

610
00:40:26,960 --> 00:40:31,600
and hyper parameters matter, and that we can optimize using evolution.

611
00:40:31,600 --> 00:40:43,280
So help us understand, so what you just said was the evolution is better for the architectural

612
00:40:43,280 --> 00:40:47,840
identifying the architectural solution, the connectivity, and the types of layers, and

613
00:40:47,840 --> 00:40:48,840
things like that.

614
00:40:48,840 --> 00:40:54,480
And traditional deep learning training techniques are better for the weights.

615
00:40:54,480 --> 00:40:55,480
Why exactly is that?

616
00:40:55,480 --> 00:40:59,280
You mentioned because there are a lot of weights, but I thought that was an advantage of the

617
00:40:59,280 --> 00:41:00,640
evolutionary approach.

618
00:41:00,640 --> 00:41:07,920
Well, again, so it's actually a very simple point that if you are evolving the entire network

619
00:41:07,920 --> 00:41:12,400
including the weights, then evolutionary operators need to set the weight values.

620
00:41:12,400 --> 00:41:15,040
And that means crossover or mutation.

621
00:41:15,040 --> 00:41:18,240
And mutation means that you are changing each weight randomly.

622
00:41:18,240 --> 00:41:22,760
And if you have a million weights, that's a very slow process.

623
00:41:22,760 --> 00:41:26,960
In contrast, something like deep learning of backpropagation, stochastic gradient descent,

624
00:41:26,960 --> 00:41:31,040
every time you show an example, you can change every single weight.

625
00:41:31,040 --> 00:41:36,960
So there's a lot more parallelism, it's a lot more efficient way of changing the weight.

626
00:41:36,960 --> 00:41:42,160
So what we're saying here is, if you've got training data, use traditional training

627
00:41:42,160 --> 00:41:47,720
techniques gradient descent, which uses that training data to accelerate training.

628
00:41:47,720 --> 00:41:48,720
Yeah, exactly.

629
00:41:48,720 --> 00:41:53,240
And where you don't have training data is kind of at this higher level.

630
00:41:53,240 --> 00:41:58,400
Well, it seems like you kind of, if you have the training data, you should also be able

631
00:41:58,400 --> 00:42:02,640
to use that for the architectural stuff.

632
00:42:02,640 --> 00:42:07,640
But I guess we don't have techniques for doing that gradient descent.

633
00:42:07,640 --> 00:42:09,640
That's where evolutionary comes in.

634
00:42:09,640 --> 00:42:10,640
Exactly.

635
00:42:10,640 --> 00:42:11,640
That's exactly the point.

636
00:42:11,640 --> 00:42:12,640
Okay.

637
00:42:12,640 --> 00:42:17,080
Now, of course, people do research and try to break free of these restrictions.

638
00:42:17,080 --> 00:42:22,280
And you could, for instance, use evolution for the weights as well, even in deep learning

639
00:42:22,280 --> 00:42:23,280
networks.

640
00:42:23,280 --> 00:42:26,360
But there's an interesting approach that allows you to do it.

641
00:42:26,360 --> 00:42:30,480
And that is that you don't evolve every single weight separately.

642
00:42:30,480 --> 00:42:33,040
But you evolve, say, patterns of weights.

643
00:42:33,040 --> 00:42:34,040
Color evolution.

644
00:42:34,040 --> 00:42:36,680
Well, it's a different level of evolution.

645
00:42:36,680 --> 00:42:42,040
It's called co-evolution, co-evolution of different levels to co-evolution of topology,

646
00:42:42,040 --> 00:42:46,920
co-evolution with components and then weights.

647
00:42:46,920 --> 00:42:52,120
But the trick here is that instead of having to set each value independently using mutation

648
00:42:52,120 --> 00:42:55,880
crossover, you have some kind of a pattern that you are evolving.

649
00:42:55,880 --> 00:42:59,760
And then you, that pattern, you use that pattern to derive the weight values.

650
00:42:59,760 --> 00:43:05,120
And in extreme, that pattern could be given by a different neural network, a separate

651
00:43:05,120 --> 00:43:06,360
neural network.

652
00:43:06,360 --> 00:43:11,680
So you're evolving a neural network whose outputs then give you the weight values.

653
00:43:11,680 --> 00:43:15,360
That technique is called compositional pattern producing neural net.

654
00:43:15,360 --> 00:43:19,400
And it's been used to evolve these deep learning architectures.

655
00:43:19,400 --> 00:43:24,280
And this is an example of how you can still use evolution, even if you don't have supervised

656
00:43:24,280 --> 00:43:25,280
training data necessarily.

657
00:43:25,280 --> 00:43:28,400
You could still use a deep learning network with millions of weights.

658
00:43:28,400 --> 00:43:31,920
But you have to use this kind of an indirect encoding of its weights, perhaps through another

659
00:43:31,920 --> 00:43:32,920
neural network.

660
00:43:32,920 --> 00:43:41,240
And so when you're using evolutionary algorithms to evolve the architecture and the connectivity

661
00:43:41,240 --> 00:43:51,840
of deep neural network or anything for that matter, are you applying evolutionary algorithms

662
00:43:51,840 --> 00:43:56,880
hierarchically or that as an approach hierarchically like, you know, first evolve the connectivity

663
00:43:56,880 --> 00:44:03,800
and separately evolve the layers or is it all done at once?

664
00:44:03,800 --> 00:44:04,800
All of that is possible.

665
00:44:04,800 --> 00:44:09,800
And all of that is under, under research right now, you know, I guess I should have anticipated

666
00:44:09,800 --> 00:44:10,800
that.

667
00:44:10,800 --> 00:44:11,800
Yes.

668
00:44:11,800 --> 00:44:12,800
Yes.

669
00:44:12,800 --> 00:44:13,800
Exactly.

670
00:44:13,800 --> 00:44:14,800
And you mentioned co evolution.

671
00:44:14,800 --> 00:44:16,840
That's a powerful approach in evolution.

672
00:44:16,840 --> 00:44:21,920
And that means that you have two evolutionary processes, two or more going on at once and

673
00:44:21,920 --> 00:44:23,000
they interact.

674
00:44:23,000 --> 00:44:28,040
So in this case, it would be that you evolve the module that might be a convolutional layer

675
00:44:28,040 --> 00:44:31,880
or some other, or LSTM type of a module.

676
00:44:31,880 --> 00:44:34,840
And then you evolve a topology how you connect them together.

677
00:44:34,840 --> 00:44:36,600
And that means that you have two populations.

678
00:44:36,600 --> 00:44:42,520
One of them encodes a different LSTM node and the other one encodes how they are connected.

679
00:44:42,520 --> 00:44:47,440
And then you can evolve them at the same time and evaluate them together as a single architecture

680
00:44:47,440 --> 00:44:52,200
and then the individuals inherit the fitness of the entire architecture.

681
00:44:52,200 --> 00:44:53,920
And that's a very interesting approach.

682
00:44:53,920 --> 00:44:58,200
You could also do it differently if you have a, if you have a way of assigning a fitness

683
00:44:58,200 --> 00:45:03,280
to say a single in LSTM node somehow, like maybe it's memory capacity or something, then

684
00:45:03,280 --> 00:45:07,720
you could evolve that first and evolve a bunch of different maybe LSTM nodes for different

685
00:45:07,720 --> 00:45:09,520
kinds of fitness functions.

686
00:45:09,520 --> 00:45:13,880
Use those as raw material at a higher level evolution and do it sequentially.

687
00:45:13,880 --> 00:45:15,360
That's also possible.

688
00:45:15,360 --> 00:45:17,480
But yeah, go ahead.

689
00:45:17,480 --> 00:45:24,680
It almost suggests that there's probably some analog for, for Gens, a generative adversarial

690
00:45:24,680 --> 00:45:30,040
networks like generative adversarial evolutionary algorithms or something like that is.

691
00:45:30,040 --> 00:45:32,320
Does that mean anything in this world?

692
00:45:32,320 --> 00:45:33,320
Yes, exactly.

693
00:45:33,320 --> 00:45:36,920
And that's actually how Gens kind of got started and motivated that.

694
00:45:36,920 --> 00:45:37,920
Oh, really?

695
00:45:37,920 --> 00:45:38,920
Yes.

696
00:45:38,920 --> 00:45:43,640
So it was possible to evolve input examples that broke the deep learning network that had

697
00:45:43,640 --> 00:45:49,680
been trained very well in a training set and, and, and then Jeff Kloon evolved these patterns

698
00:45:49,680 --> 00:45:54,720
that were pretty much just noise, but the network's still confident to claim that, oh, that's

699
00:45:54,720 --> 00:45:55,720
a dog.

700
00:45:55,720 --> 00:46:00,440
And, and this is like the school bus giraffe kind of example, that kind of thing.

701
00:46:00,440 --> 00:46:03,240
I'm not familiar with that example school bus giraffe.

702
00:46:03,240 --> 00:46:04,240
What is that?

703
00:46:04,240 --> 00:46:11,560
I'm, I may be mixing up my objects and animals here, but there is some set of famous examples

704
00:46:11,560 --> 00:46:17,320
where, for whatever reason, if you give, you know, one of the same famous object detection

705
00:46:17,320 --> 00:46:22,960
CNN's picture of a zebra or a giraffe or something like that, it confidently proclaims

706
00:46:22,960 --> 00:46:24,360
it to be a school bus.

707
00:46:24,360 --> 00:46:25,360
Oh, I see.

708
00:46:25,360 --> 00:46:26,360
I see.

709
00:46:26,360 --> 00:46:27,360
Yeah.

710
00:46:27,360 --> 00:46:29,920
There are many demonstrations of this same, same problem that that sounds like it's one

711
00:46:29,920 --> 00:46:30,920
of them.

712
00:46:30,920 --> 00:46:31,920
Yes.

713
00:46:31,920 --> 00:46:35,840
So you can mix the categories, but the most impressive demonstration to me is that there's

714
00:46:35,840 --> 00:46:38,160
an image that looks pretty much just noise.

715
00:46:38,160 --> 00:46:42,800
You cannot see anything in it, but still the deep learning system somehow declares that

716
00:46:42,800 --> 00:46:45,920
to be, I don't know, a school bus or something else.

717
00:46:45,920 --> 00:46:46,920
Right.

718
00:46:46,920 --> 00:46:51,120
So, but this, the origin of that was that it was possible to evolve those images.

719
00:46:51,120 --> 00:46:55,840
It wasn't just that we look at, look at bad mistakes in a training set, but evolve these

720
00:46:55,840 --> 00:47:00,120
images that we had no constraints on what they had to be and they turned out to be pretty

721
00:47:00,120 --> 00:47:03,160
much as noise looking images, noise images.

722
00:47:03,160 --> 00:47:07,720
And evolution discovered that this is a way of getting the deep learning network to perform

723
00:47:07,720 --> 00:47:08,720
very poorly.

724
00:47:08,720 --> 00:47:09,720
Wow.

725
00:47:09,720 --> 00:47:11,080
You've confident into something else.

726
00:47:11,080 --> 00:47:12,080
Yeah.

727
00:47:12,080 --> 00:47:13,080
That's really interesting background.

728
00:47:13,080 --> 00:47:14,080
I didn't realize that.

729
00:47:14,080 --> 00:47:18,920
And from there on, the whole field of generally a traversor and network study, like how could

730
00:47:18,920 --> 00:47:19,920
we actually do this?

731
00:47:19,920 --> 00:47:25,480
How could we have an adversarial training, the trainer or supervisor so that the network

732
00:47:25,480 --> 00:47:26,800
could actually perform better?

733
00:47:26,800 --> 00:47:31,800
Because you could use those images to train it as well as just break it.

734
00:47:31,800 --> 00:47:36,920
And this is, so this is a very close relation and I think it's still being explored.

735
00:47:36,920 --> 00:47:41,280
It's possible to evolve training sets and it's possible to evolve also these systems that

736
00:47:41,280 --> 00:47:45,480
are learning from them so that they become more robust and also they develop internal

737
00:47:45,480 --> 00:47:48,160
representations that are more representative.

738
00:47:48,160 --> 00:47:53,560
So we can use evolution to bias these learning systems in a way we want and make it more

739
00:47:53,560 --> 00:47:57,400
general, make them interpretable and more robust.

740
00:47:57,400 --> 00:47:58,400
Interesting.

741
00:47:58,400 --> 00:47:59,400
Interesting.

742
00:47:59,400 --> 00:48:04,440
I interrupted you earlier when you were about to talk about classes of problems for which

743
00:48:04,440 --> 00:48:08,200
evolutionary algorithms are particularly suited and not suited.

744
00:48:08,200 --> 00:48:09,200
Right.

745
00:48:09,200 --> 00:48:14,880
So this, of course, a lot of work in trying to expand the space of possible problems.

746
00:48:14,880 --> 00:48:18,360
Now, we were comparing reinforcement learning with evolution.

747
00:48:18,360 --> 00:48:21,800
I would like to point out that reinforcement learning has a little bit different perspective

748
00:48:21,800 --> 00:48:23,040
and goal.

749
00:48:23,040 --> 00:48:28,240
The idea there is to model a learning of an individual more or less during its lifetime.

750
00:48:28,240 --> 00:48:34,920
So as it's living its life and it's performing every step counts and in a sense, it's like

751
00:48:34,920 --> 00:48:36,440
an online method.

752
00:48:36,440 --> 00:48:41,480
The typical application of evolution is offline engineering type of an application that you

753
00:48:41,480 --> 00:48:46,560
do have, you do have a simulator for instance of the system you're trying to, or the environment

754
00:48:46,560 --> 00:48:48,240
for the system trying to evolve.

755
00:48:48,240 --> 00:48:51,800
And you can fail miserably in some of these candidates.

756
00:48:51,800 --> 00:48:57,320
The only thing that counts is that in the end you have a very well engineered system.

757
00:48:57,320 --> 00:48:59,760
And that is a different kind of a perspective.

758
00:48:59,760 --> 00:49:03,920
You don't get penalized for your exploration in evolutionary algorithms.

759
00:49:03,920 --> 00:49:07,600
You only get evaluated in the final result versus in reinforcement learning.

760
00:49:07,600 --> 00:49:11,640
It's a continual lifelong learning system perhaps.

761
00:49:11,640 --> 00:49:13,600
Now, and that makes sense.

762
00:49:13,600 --> 00:49:18,760
I mean, evolution is an engineering approach can can be we can evolve.

763
00:49:18,760 --> 00:49:22,600
For instance, controllers for finalist rockets is one thing that we did.

764
00:49:22,600 --> 00:49:27,960
You couldn't possibly use it in a physical system because you would have to explode hundreds

765
00:49:27,960 --> 00:49:29,440
of thousands of rockets.

766
00:49:29,440 --> 00:49:32,960
But if you have a simulator for that system, you can do anything you want with it.

767
00:49:32,960 --> 00:49:35,120
You can explore very wild solutions.

768
00:49:35,120 --> 00:49:39,280
And as a matter of fact, some of those wide solutions developed into really good solutions

769
00:49:39,280 --> 00:49:40,280
in the end.

770
00:49:40,280 --> 00:49:44,640
And in the end, we have a controller for a finless rockets that keeps its table in a reliable

771
00:49:44,640 --> 00:49:45,640
way.

772
00:49:45,640 --> 00:49:50,640
And that's the kind of typical application for an evolutionary algorithm system.

773
00:49:50,640 --> 00:49:54,880
Now, in on the other hand, if you have an online system that needs to learn online while

774
00:49:54,880 --> 00:49:58,640
it's performing, that's not as easy to use evolution for that.

775
00:49:58,640 --> 00:50:01,040
You have to build extra machinery for it.

776
00:50:01,040 --> 00:50:02,040
But you can.

777
00:50:02,040 --> 00:50:04,880
That's the kind of a reinforcement learning system that reinforcement learning initially

778
00:50:04,880 --> 00:50:06,160
came from.

779
00:50:06,160 --> 00:50:09,080
But it has also been to confuse everybody.

780
00:50:09,080 --> 00:50:13,880
It has been used to do engineering design as well, just like evolution is used to do online

781
00:50:13,880 --> 00:50:14,880
learning.

782
00:50:14,880 --> 00:50:19,000
But that's not the opposite to the origin and that's that sort of the first application

783
00:50:19,000 --> 00:50:22,800
would be engineering versus online learning.

784
00:50:22,800 --> 00:50:26,560
You mentioned simulation and that reminded me of a question that I had earlier.

785
00:50:26,560 --> 00:50:32,840
Is there a relationship in, you know, particularly in the context of the trading work that you've

786
00:50:32,840 --> 00:50:38,040
done between Monte Carlo types of approaches, the evolutionary approaches?

787
00:50:38,040 --> 00:50:39,040
Yeah.

788
00:50:39,040 --> 00:50:46,080
So Monte Carlo's simulation means just that you randomize your domain and let and generate

789
00:50:46,080 --> 00:50:47,800
new situations that way.

790
00:50:47,800 --> 00:50:54,000
Now, when you use it as a solution mechanism, you're banking on the idea that even randomized

791
00:50:54,000 --> 00:50:57,040
solutions are likely to be successful sometimes.

792
00:50:57,040 --> 00:51:03,720
So you could think of evolution as a 2.0 of that that you actually trying to learn from

793
00:51:03,720 --> 00:51:05,200
your mistakes.

794
00:51:05,200 --> 00:51:09,320
You're trying to learn from your, those trials and that is using crossover on a good

795
00:51:09,320 --> 00:51:13,840
at candidates and mutation on a good candidates and focus to search more.

796
00:51:13,840 --> 00:51:17,760
And that is, I think, a good way to formulate the relationship.

797
00:51:17,760 --> 00:51:22,320
You could even look at some of the evolutionary algorithm methods.

798
00:51:22,320 --> 00:51:26,960
We've been talking about generic algorithms, which is crossover mutation based.

799
00:51:26,960 --> 00:51:31,520
There are other evolution methods that are closer to something I want to call in that.

800
00:51:31,520 --> 00:51:38,640
They will build a statistical model of the domain, which individual components, schemas,

801
00:51:38,640 --> 00:51:41,720
are how reliable they are in predicting the fitness.

802
00:51:41,720 --> 00:51:45,320
You form that kind of probabilistic model of your search base.

803
00:51:45,320 --> 00:51:48,880
And then when you construct new individuals, when you construct offspring, you don't

804
00:51:48,880 --> 00:51:52,160
do it based on crossover mutation, you do it statistically.

805
00:51:52,160 --> 00:51:54,160
You sample from that model.

806
00:51:54,160 --> 00:51:59,600
And it's still a population based approach in that sense falls under the evolution algorithms,

807
00:51:59,600 --> 00:52:03,600
but it's closer to probabilistic reasoning and Monte Carlo methods and other statistical

808
00:52:03,600 --> 00:52:04,600
methods.

809
00:52:04,600 --> 00:52:05,600
And they perform quite well.

810
00:52:05,600 --> 00:52:07,080
Those methods perform quite well too.

811
00:52:07,080 --> 00:52:09,560
No, interesting, interesting.

812
00:52:09,560 --> 00:52:21,800
Is there a simple way to kind of categorize or characterize, I should say, the various types

813
00:52:21,800 --> 00:52:27,360
of evolutionary algorithms or approaches or the, you know, the various tweaks that have

814
00:52:27,360 --> 00:52:31,120
evolved to the basic generic approach?

815
00:52:31,120 --> 00:52:35,280
Well, we can attempt to do it.

816
00:52:35,280 --> 00:52:36,280
Researches are very creative.

817
00:52:36,280 --> 00:52:41,520
I mean, whenever you come up in this category, then they will cross the categories.

818
00:52:41,520 --> 00:52:46,560
And that's part of how research works, too, that you will combine ideas across different

819
00:52:46,560 --> 00:52:47,960
approaches and categories.

820
00:52:47,960 --> 00:52:53,040
But generic algorithms is one where the close connection to biology is obvious, crossover

821
00:52:53,040 --> 00:52:54,600
mutation.

822
00:52:54,600 --> 00:52:59,480
And then there are these statistics based approaches, like I mentioned, sometimes called

823
00:52:59,480 --> 00:53:04,720
estimation of distribution algorithms where you estimate the probabilistic model of what

824
00:53:04,720 --> 00:53:05,720
works.

825
00:53:05,720 --> 00:53:11,400
For example, from that, there are methods based on evolution strategy where you don't have

826
00:53:11,400 --> 00:53:18,120
crossover, but you have a small population and you mostly using mutation to do the search.

827
00:53:18,120 --> 00:53:20,320
But you make the mutations intelligent.

828
00:53:20,320 --> 00:53:26,000
If you have a small population, you form, for instance, a covariance matrix of how your

829
00:53:26,000 --> 00:53:27,760
mutations call vary.

830
00:53:27,760 --> 00:53:33,680
Try to find these interactions specifically and then use that model of the interactions

831
00:53:33,680 --> 00:53:36,240
to construct new individuals.

832
00:53:36,240 --> 00:53:42,240
There's differential evolution, there's different buzzwords, I can give you a lot of these,

833
00:53:42,240 --> 00:53:44,080
but there's a large number of them.

834
00:53:44,080 --> 00:53:50,720
And they are based on sometimes letting go of the biological analogy and instead focusing

835
00:53:50,720 --> 00:53:55,600
on the idea that what if you do have a population, how could you utilize the information in that

836
00:53:55,600 --> 00:53:59,600
population to construct new individuals better?

837
00:53:59,600 --> 00:54:04,240
And that is kind of the general umbrella of evolution algorithms.

838
00:54:04,240 --> 00:54:09,320
And you could even think of an interesting direction trying to go towards the biology,

839
00:54:09,320 --> 00:54:14,920
on the opposite direction, talking about abstracting it to probabilistic reasoning, probabilistic

840
00:54:14,920 --> 00:54:17,120
and stochastic search.

841
00:54:17,120 --> 00:54:22,880
But you could also go towards biology and there's an interesting idea, try to take some ideas

842
00:54:22,880 --> 00:54:26,080
that are more biological, for instance, in directing coding.

843
00:54:26,080 --> 00:54:33,680
The fact that in biology, DNA, the actual ribbon, like that, asic does not really specify

844
00:54:33,680 --> 00:54:40,120
the full individual, there's a large network of interactions that come after generating

845
00:54:40,120 --> 00:54:42,640
the proteins from the DNA and RNA.

846
00:54:42,640 --> 00:54:48,240
So genetic regulatory networks are a huge part of biological construction from the DNA

847
00:54:48,240 --> 00:54:49,680
to an individual.

848
00:54:49,680 --> 00:54:53,960
And currently we are pretty much missing that in these algorithms.

849
00:54:53,960 --> 00:54:58,360
So there's a lot of complexity in this kind of indirect encoding.

850
00:54:58,360 --> 00:55:02,480
There are approaches that try to invoke genetic regulatory networks.

851
00:55:02,480 --> 00:55:08,240
There are approaches that are trying to include a developmental phase, which is also big in biology.

852
00:55:08,240 --> 00:55:14,240
So after the individual is constructed, after it's born, there's usually a period of learning

853
00:55:14,240 --> 00:55:15,240
that happens.

854
00:55:15,240 --> 00:55:19,120
Interaction with the environment and that then constructs the final individual.

855
00:55:19,120 --> 00:55:24,080
For instance, human brain, we have 30,000 genes maybe in the genome.

856
00:55:24,080 --> 00:55:29,320
There's no way the brain can be specified except at a course kind of instructive level,

857
00:55:29,320 --> 00:55:31,040
pattern level.

858
00:55:31,040 --> 00:55:35,960
Most of the brain structure is actually learned in an interaction with the environment.

859
00:55:35,960 --> 00:55:39,720
Evolution just produces a starting point for that learning.

860
00:55:39,720 --> 00:55:44,840
So combining learning and evolution that way is fundamentally biological and it's also

861
00:55:44,840 --> 00:55:49,440
something that we should be looking into and we are looking into, people are looking into it.

862
00:55:49,440 --> 00:55:55,840
This is really a fascinating area for folks that want to dig in a little bit deeper or

863
00:55:55,840 --> 00:55:59,360
learn more, where is the best place to start?

864
00:55:59,360 --> 00:56:02,360
Are there canonical papers they should start at?

865
00:56:02,360 --> 00:56:06,760
Is there a resource that you'd like to recommend people take a look at?

866
00:56:06,760 --> 00:56:13,240
Sure, there are some classic books that are about evolution algorithms and can find

867
00:56:13,240 --> 00:56:18,840
genetic algorithm evolution computation books that are textbooks, Holland, Mitchell, Goldberg

868
00:56:18,840 --> 00:56:19,840
for instance.

869
00:56:19,840 --> 00:56:24,960
They tend to be a little bit old right now because the field is developing very rapidly.

870
00:56:24,960 --> 00:56:31,120
It has been explosive growth, but the typical sources on Scholarpedia for instance and

871
00:56:31,120 --> 00:56:37,160
then the tutorials that appear in the main conferences I think would be a great source.

872
00:56:37,160 --> 00:56:42,400
There will be actually a genetic and evolution computation conference Gecko starts tomorrow

873
00:56:42,400 --> 00:56:44,720
as a matter of fact in Berlin.

874
00:56:44,720 --> 00:56:50,160
And the first two days are tutorials and lots of those tutorials are online, even some

875
00:56:50,160 --> 00:56:54,480
of the videos about the tutorials online and I think that those are really a great way

876
00:56:54,480 --> 00:56:55,480
to get started.

877
00:56:55,480 --> 00:57:01,120
It is a big field and in a very diverse field, it's kind of interesting because evolution

878
00:57:01,120 --> 00:57:05,560
drives on diversity, the algorithms drive on diversity, but the field is also tremendously

879
00:57:05,560 --> 00:57:07,160
diverse.

880
00:57:07,160 --> 00:57:12,240
So that is a bit of a challenge that you may get lost in all these terminology and all

881
00:57:12,240 --> 00:57:17,000
the different approaches, and that's why I'm recommended that maybe starting from one

882
00:57:17,000 --> 00:57:21,960
of the textbooks even if it's a little bit older is a good idea that gives you the perspective

883
00:57:21,960 --> 00:57:27,120
and then looking at the tutorials and maybe there on it, you should have access to a

884
00:57:27,120 --> 00:57:29,800
lot of literature on the internet.

885
00:57:29,800 --> 00:57:36,440
Are there tools and frameworks or do the deep learning frameworks, the TensorFlow's of

886
00:57:36,440 --> 00:57:37,440
the world, and the like?

887
00:57:37,440 --> 00:57:44,800
Do they support, have any kind of support for evolutionary algorithms or are folks rolling

888
00:57:44,800 --> 00:57:45,800
their own?

889
00:57:45,800 --> 00:57:52,040
Yeah, so TensorFlow, I don't think currently has evolution component, but it is likely

890
00:57:52,040 --> 00:57:53,040
that it will in the future.

891
00:57:53,040 --> 00:57:57,320
It's an open source project and people are contributing to it, so it's very likely to

892
00:57:57,320 --> 00:57:58,320
happen.

893
00:57:58,320 --> 00:58:04,480
There's for instance, ECJ, evolution competition in Java, that's a big effort, George Mason

894
00:58:04,480 --> 00:58:09,240
University, and that software includes many different evolutionary algorithms, and that

895
00:58:09,240 --> 00:58:13,800
I think is currently the best source to get started with, software system to get started

896
00:58:13,800 --> 00:58:14,800
with.

897
00:58:14,800 --> 00:58:15,800
Great, great.

898
00:58:15,800 --> 00:58:21,680
Well, to wrap things up, can you let folks know what's the best way to check in on you

899
00:58:21,680 --> 00:58:23,080
or to get in touch with you?

900
00:58:23,080 --> 00:58:29,000
Well, I have a name that's very easy to find, if you can figure out the spelling.

901
00:58:29,000 --> 00:58:35,600
Yeah, so I have a website at UT Austin, where much of my research from my whole career is

902
00:58:35,600 --> 00:58:36,600
always there.

903
00:58:36,600 --> 00:58:43,560
I try to keep that up to speed, and of course, Cynthia itself has a set of blog posts and

904
00:58:43,560 --> 00:58:47,920
web pages describing the technology we are developing here, which is evolution, competition

905
00:58:47,920 --> 00:58:48,920
and deep learning.

906
00:58:48,920 --> 00:58:54,040
So those sites are usually quite well up to date, and then we have pointers to more material

907
00:58:54,040 --> 00:58:56,320
that you can use those as a starting point.

908
00:58:56,320 --> 00:58:59,600
Okay, great, and we'll link to both of those in the show notes.

909
00:58:59,600 --> 00:59:00,600
Very good.

910
00:59:00,600 --> 00:59:01,600
Thank you.

911
00:59:01,600 --> 00:59:02,720
All right, well, Risto, thank you very much.

912
00:59:02,720 --> 00:59:03,720
This was amazing.

913
00:59:03,720 --> 00:59:07,720
It was a lot of fun.

914
00:59:07,720 --> 00:59:14,120
All right, everyone, that's our show for today.

915
00:59:14,120 --> 00:59:20,440
Thanks so much for listening, and of course, for your ongoing feedback and support.

916
00:59:20,440 --> 00:59:27,560
For the notes for this episode, head on over to twimlai.com slash talk slash 47.

917
00:59:27,560 --> 00:59:29,360
I've got a quick favor to ask.

918
00:59:29,360 --> 00:59:35,120
If you enjoy the podcast, and especially if you like this episode, please take a moment

919
00:59:35,120 --> 00:59:40,040
to jump on over to iTunes and leave us a five-star review.

920
00:59:40,040 --> 00:59:46,080
We'd love to read these, and it lets others know that the podcast is worth tuning into.

921
00:59:46,080 --> 00:59:50,720
Another thanks to this week's sponsor, Claudeira, for more information on their data science

922
00:59:50,720 --> 00:59:57,680
workbench or to schedule your demo and get a drone, visit twimlai.com slash Claudeira.

923
00:59:57,680 --> 01:00:02,560
I'd also like to send a huge shout out to friend of the show Hilary Mason, whose company

924
01:00:02,560 --> 01:00:07,240
Fast Forward Labs was acquired by Claudeira just last week.

925
01:00:07,240 --> 01:00:13,560
For more on Hilary and Fast Forward Labs, check out my interview with her at twimlai.com

926
01:00:13,560 --> 01:00:18,840
slash talk slash 11, and my interview with the former president of that company, Catherine

927
01:00:18,840 --> 01:00:23,560
Hume, at twimlai.com slash talk slash 20.

928
01:00:23,560 --> 01:00:52,080
Thanks again for listening, and catch you next time.

