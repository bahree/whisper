WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:21.760
I'm your host Sam Charrington.

00:21.760 --> 00:24.680
Hey what's up everyone, this is Sam.

00:24.680 --> 00:29.080
It's been nearly a month since we closed the books on our very first conference, Twimal

00:29.080 --> 00:31.160
Con AI Platforms.

00:31.160 --> 00:35.000
I really hope you've enjoyed the content we've shared from the event.

00:35.000 --> 00:39.920
We went into the conference with three main goals, first to provide attendees with great

00:39.920 --> 00:45.440
content focused on the many ways, organizations of all types are automating, accelerating

00:45.440 --> 00:48.400
and scaling machine learning and AI.

00:48.400 --> 00:53.560
Thanks to an incredible roster of speakers, we were able to deliver just that.

00:53.560 --> 00:58.520
Next, we wanted to bring the extended Twimal community together and the results here

00:58.520 --> 01:00.480
were truly amazing.

01:00.480 --> 01:04.680
I had the great fortune of meeting and conversing with a ton of wonderful people, including

01:04.680 --> 01:11.440
longtime Twimal listeners, former podcast guests and attendees new to the Twimalverse.

01:11.440 --> 01:16.000
One thing that I heard universally was how open and engaged everyone was and how much

01:16.000 --> 01:20.160
folks were enjoying the substantive meaningful conversations they were having with other

01:20.160 --> 01:21.480
attendees.

01:21.480 --> 01:24.840
To me, that sounds like a mission accomplished.

01:24.840 --> 01:30.280
Finally, TwimalCon was also an opportunity to celebrate all that we've been able to

01:30.280 --> 01:32.760
accomplish together as a community.

01:32.760 --> 01:36.680
This includes the many innovations that our speakers are presenting on in conference

01:36.680 --> 01:43.400
breakouts, as well as Twimal community achievements, like reaching our third year, 300th episode

01:43.400 --> 01:47.160
and 5 million download in the last few months.

01:47.160 --> 01:52.000
To be able to share these milestones with so many awesome friends, new and old was an

01:52.000 --> 01:54.680
amazing experience for all of us.

01:54.680 --> 01:58.880
I wanted to take this opportunity to thank each of you in our community for all of the

01:58.880 --> 02:01.960
many ways that you support and engage with us.

02:01.960 --> 02:03.600
Thank you so much.

02:03.600 --> 02:06.400
And now on to the show.

02:06.400 --> 02:10.240
All right, everyone.

02:10.240 --> 02:13.800
I am on the line with Archena Venkatraman.

02:13.800 --> 02:19.280
Archena is the John C Malone Assistant Professor of Electrical and Computer Engineering at

02:19.280 --> 02:21.440
John Hopkins University.

02:21.440 --> 02:24.360
Archena, welcome to the Twimal AI podcast.

02:24.360 --> 02:27.880
Thank you, Sam, and thank you for inviting me to be here today.

02:27.880 --> 02:34.640
I'm really excited about our conversation, and we'll jump right in by having you share

02:34.640 --> 02:36.360
a little bit of your background.

02:36.360 --> 02:41.400
It sounds like you had maybe more clarity than most starting from as early as the fourth

02:41.400 --> 02:42.400
grade.

02:42.400 --> 02:48.000
Yes, so both my parents are engineers.

02:48.000 --> 02:53.440
My mom is a professor of electrical engineering, and my dad is a professor of mechanical engineering.

02:53.440 --> 02:59.600
So as you can imagine, we had a very strong educational focus growing up.

02:59.600 --> 03:03.160
And I decided fairly early on, engineering was very cool.

03:03.160 --> 03:07.560
It was a way of solving problems, and I was very good in math and science, and I enjoyed

03:07.560 --> 03:08.960
them.

03:08.960 --> 03:16.640
And so around fourth grade is when I asked my mom about colleges, and where to go to

03:16.640 --> 03:22.880
college, and what were good schools, and she said that the best school was MIT.

03:22.880 --> 03:26.720
And I checked, and it was very close to home, which in the fourth grade was important to

03:26.720 --> 03:27.720
me.

03:27.720 --> 03:35.760
So I decided I would go to MIT, and so I worked through middle school, high school, learning

03:35.760 --> 03:42.640
lots of science, and doing many extracurricular activities, and finally did end up going to

03:42.640 --> 03:44.680
MIT for undergrad.

03:44.680 --> 03:45.680
And I liked it so much.

03:45.680 --> 03:49.080
I stayed there for masters and PhD.

03:49.080 --> 03:55.360
And through that educational pathway at MIT, I started in electrical engineering.

03:55.360 --> 04:02.960
I explored different areas, so I did research experiences in devices, in nanofabrication.

04:02.960 --> 04:09.520
And through those research experiences and coursework, I realized I loved signal processing.

04:09.520 --> 04:13.960
So that was the area that really gravitated towards me.

04:13.960 --> 04:18.680
I loved the idea of transforming signals into various domains to understand different

04:18.680 --> 04:24.400
properties of being able to estimate properties and characterize uncertainty.

04:24.400 --> 04:31.080
And so that was my concentration in undergraduate, and I did a master's with Alan Oppenheim,

04:31.080 --> 04:35.560
who is one of the fathers of modern day signal processing.

04:35.560 --> 04:38.280
And we looked at different signal representations.

04:38.280 --> 04:44.080
And I found that a very worthwhile experience and definitely helped me build fundamentals.

04:44.080 --> 04:49.320
And at the same time, I always was missing an application to it.

04:49.320 --> 04:55.680
And that's how I stumbled upon my PhD direction, which was loosely medical imaging.

04:55.680 --> 05:02.600
So sort of translating these properties of informatics and data science into understanding

05:02.600 --> 05:07.880
brain functionality through functional neuroimaging data, FMRI.

05:07.880 --> 05:16.240
And at the same time, still exploring and learning more and applying ideas from machine learning

05:16.240 --> 05:20.880
to these data streams and two different clinical populations.

05:20.880 --> 05:24.320
And that's kind of how my career has evolved.

05:24.320 --> 05:27.680
And I'm really enjoying the ride so far.

05:27.680 --> 05:35.120
And do you have a more pointed focus currently at Johns Hopkins, or are you looking out broadly

05:35.120 --> 05:38.080
around the intersection of these areas?

05:38.080 --> 05:41.680
So I think my research right now is very broad.

05:41.680 --> 05:48.560
So the most general characterization is that we are developing new machine learning tools

05:48.560 --> 05:57.600
and frameworks and algorithms to better understand and potentially treat neurological and psychiatric

05:57.600 --> 05:58.600
disorders.

05:58.600 --> 06:02.920
But in the space, in the neuro space, this is a very broad spectrum.

06:02.920 --> 06:11.160
So it ranges from basic science type questions where the goal is biomarker discovery and exploration

06:11.160 --> 06:14.520
through the ideas of clinical translation.

06:14.520 --> 06:21.800
So how do we provide information that might be helpful or actionable to even very far

06:21.800 --> 06:29.480
out their explorations of trying to understand perception and being able to alter perception,

06:29.480 --> 06:32.760
again, using different data streams and machine learning tools.

06:32.760 --> 06:36.600
And I would say throughout the spectrum, everything is very different.

06:36.600 --> 06:38.600
The disorders that we work on are different.

06:38.600 --> 06:43.280
The data sets that we use are different and then our modeling approaches are different.

06:43.280 --> 06:47.080
But they still have that flavor of kind of machine learning.

06:47.080 --> 06:48.880
So how much can you mind from the data?

06:48.880 --> 06:53.680
What can you understand and what can you formulate and pass on as information?

06:53.680 --> 07:00.360
Let's maybe dig into one or two of those areas to get a sense for the way you're able

07:00.360 --> 07:02.720
to apply machine learning.

07:02.720 --> 07:08.200
You mentioned basic science is one of the foundational areas and you're looking at things like

07:08.200 --> 07:13.920
biomarker discovery, how do MLM AI play into that?

07:13.920 --> 07:23.680
So one of the kind of the fields or disciplines that I've been heavily involved with is the

07:23.680 --> 07:29.600
branch of, I guess, computational neuroscience known as connectivity.

07:29.600 --> 07:36.800
So at a high level, it's treating the brain as a network of interconnected parts and

07:36.800 --> 07:42.960
not only looking at specific functionality of a region, but also considering that regions

07:42.960 --> 07:47.240
communicate with each other in different ways and trying to build models based on those

07:47.240 --> 07:49.200
communication patterns.

07:49.200 --> 07:54.840
So the data that we use is called resting state fMRI data.

07:54.840 --> 08:02.480
So unlike a conventional MRI acquisition where you would have the subject perform a certain

08:02.480 --> 08:08.280
task in the scanner, resting state differs in that it's a passive acquisition.

08:08.280 --> 08:14.920
So the subject is just instructed to lie there quietly, passively, and oftentimes just

08:14.920 --> 08:17.120
fixate on a crosshair.

08:17.120 --> 08:24.120
And what people have shown over the last decade or so is that the correlation patterns

08:24.120 --> 08:29.400
in this kind of steady state signal, they actually reflect different functional systems

08:29.400 --> 08:30.400
in the brain.

08:30.400 --> 08:36.640
So they tell us about communication patterns that are relevant in terms of cognitive functionality

08:36.640 --> 08:38.720
and biological functionality.

08:38.720 --> 08:47.040
And so based on this data, there have been a lot of sort of work of trying to formulate

08:47.040 --> 08:51.000
machine learning questions and they typically involve predictions.

08:51.000 --> 08:59.920
So can we predict based on this functional connectivity data, which subjects have a neurological

08:59.920 --> 09:02.120
disorder in which don't?

09:02.120 --> 09:08.840
And as sort of in conjunction with that prediction, can we identify different co-activation patterns

09:08.840 --> 09:16.120
or connectivity patterns in the brain that are sort of predictive of eventual diagnosis?

09:16.120 --> 09:19.560
And so that is kind of the general area.

09:19.560 --> 09:25.480
Now the work that we're doing right now is going one step beyond just a simple binary

09:25.480 --> 09:29.560
classification, so prediction of case versus controls.

09:29.560 --> 09:36.800
And we're trying to say, can we from this FMI data predict clinical severity of a particular

09:36.800 --> 09:37.800
patient?

09:37.800 --> 09:43.600
And we've the data that we've been using and our focus thus far has been on autism spectrum

09:43.600 --> 09:45.600
disorder.

09:45.600 --> 09:51.280
And the reason that sort of this continuous prediction task is relevant is because in

09:51.280 --> 09:57.400
any neuropsychiatric disorder, you just have a range of different behavioral characteristics,

09:57.400 --> 10:04.240
different symptoms, severity, different ability to respond to various interventions.

10:04.240 --> 10:09.760
And so building these predictive models, first of all, it allows you just in a black

10:09.760 --> 10:15.880
box sense to figure out what are strengths and weaknesses of different patients in your cohort.

10:15.880 --> 10:22.080
And also incorporating these ideas of biomarker discovery, if you can emphasize or if you

10:22.080 --> 10:27.720
can pick out or extract certain patterns in the brain that are relevant for understanding

10:27.720 --> 10:33.480
clinical severity or different manifestations, it might give other researchers a clue as

10:33.480 --> 10:39.920
to how do we develop better behavioral therapies, how do we evaluate behavioral therapies in

10:39.920 --> 10:44.840
terms of observing these patterns and watching as they grow in fade.

10:44.840 --> 10:52.240
And potentially, how do we develop new therapies based on other imaging techniques or other

10:52.240 --> 10:59.120
modalities such as drug discovery or electrostimulation, et cetera?

10:59.120 --> 11:03.080
How far along are you in this line of research?

11:03.080 --> 11:08.000
So this, again, it's very much at the level of basic exploration.

11:08.000 --> 11:11.000
So right now, we have developed frameworks.

11:11.000 --> 11:16.840
So they're, again, new machine learning frameworks that couple this discovery component, which

11:16.840 --> 11:21.920
is interpretable, which we're using a dictionary learning type framework with the predictive

11:21.920 --> 11:22.920
modeling.

11:22.920 --> 11:26.280
So just at a high level, a regression type framework.

11:26.280 --> 11:31.320
And so we've been able to, unlike other methods that we've seen and other methods that

11:31.320 --> 11:36.880
we've tried, we're able to predict severity to some extent, and we're even able to predict

11:36.880 --> 11:38.120
multi-score severity.

11:38.120 --> 11:46.560
So if you are looking at different quantifications of the patient, so being able to simultaneously

11:46.560 --> 11:53.040
understand those, with that said, I mean, there's certainly a prediction error, which is

11:53.040 --> 11:56.240
one direction we're moving in.

11:56.240 --> 12:01.160
And I think the way to tackle this is another aspect that machine learning is very good at,

12:01.160 --> 12:03.920
which is integrating data across different data sets.

12:03.920 --> 12:09.000
And so now we're trying to bring in other imaging types to get a more comprehensive picture

12:09.000 --> 12:14.880
of brain functionality or brain communication or connectivity, hopefully to improve that

12:14.880 --> 12:16.320
moving forward.

12:16.320 --> 12:21.160
But at the same time, our predictive models right now, they're performing at what is currently

12:21.160 --> 12:23.160
state of the art in the field.

12:23.160 --> 12:26.000
And at the same time, we're able to preserve that interpretability.

12:26.000 --> 12:31.720
And so we're finding patterns and interactions that are sort of interpretable from the autism

12:31.720 --> 12:38.840
standpoint and could eventually provide some sort of biomarker that might be meaningful.

12:38.840 --> 12:45.400
You mentioned that the models that you're using are state of the art, are there well-established

12:45.400 --> 12:51.200
benchmarks for these types of problems?

12:51.200 --> 12:56.360
In this space, so in the neuro space and in the functional connectomics space, I think

12:56.360 --> 13:03.440
one of the challenges is that we don't have very good benchmarks, both in terms of methodology

13:03.440 --> 13:06.960
and in terms of data.

13:06.960 --> 13:13.040
And so this is one of the reasons why the kind of neuro space and especially functional

13:13.040 --> 13:19.840
neuro imaging is unlike other areas where AI or machine learning has made very rapid

13:19.840 --> 13:20.840
advancement.

13:20.840 --> 13:27.120
So if you think about computer vision and image recognition and activity recognition, there

13:27.120 --> 13:33.960
are vast open source data sets, some millions and millions of examples, and here we are developing

13:33.960 --> 13:38.880
machine learning frameworks that essentially have to learn from tens of examples.

13:38.880 --> 13:45.120
And tens of examples to learn very complex functions means that sort of out of the box

13:45.120 --> 13:46.640
algorithms tend to fail.

13:46.640 --> 13:51.840
And so it's a lot more about building structured assumptions into the model in part to reduce

13:51.840 --> 13:57.080
the parameter space and in part to try and guide what you think is reasonable around all

13:57.080 --> 13:59.560
of the noise in the data itself.

13:59.560 --> 14:05.200
I'd love to dig a little bit deeper into the models and this framework.

14:05.200 --> 14:08.320
And when you say models in framework, are you using those interchangeably more or less

14:08.320 --> 14:11.560
or did they have very distinct meanings for you?

14:11.560 --> 14:13.320
No, I use them interchangeably.

14:13.320 --> 14:20.800
Okay, and so you mentioned that the framework is, you said dictionary layered, and I

14:20.800 --> 14:26.320
missed, I think I missed a word there, dictionary learning, dictionary learning.

14:26.320 --> 14:33.880
And so can you elaborate on how you're using the elements of the framework to incorporate

14:33.880 --> 14:40.560
in the operating knowledge about the relationships between the system level knowledge that you

14:40.560 --> 14:41.560
mentioned?

14:41.560 --> 14:50.360
So the way we are incorporating or structuring the model, it's in essence, we, so the dictionary

14:50.360 --> 14:55.440
learning component, or you can think of it as a basis expansion component, essentially

14:55.440 --> 15:00.760
what it assumes is that the kind of global connectivity pattern that we observe, which

15:00.760 --> 15:08.000
in our case is an input positive semi-definite correlation matrix, it is explained or it can

15:08.000 --> 15:14.080
be represented by a sparse collection of what we call elementary subnetworks.

15:14.080 --> 15:18.680
So these are canonical patterns of co-activation across the brain.

15:18.680 --> 15:22.160
You mentioned positive semi-definite connection matrix.

15:22.160 --> 15:27.760
This is your sparse matrix of connections between these different, and what level are

15:27.760 --> 15:28.760
you looking at?

15:28.760 --> 15:33.440
Are these connections between neurons or larger structures in the brain?

15:33.440 --> 15:38.800
So the correlation matrix, it's a Pearson correlation matrix.

15:38.800 --> 15:44.560
The dimensionality is region, so region in the brain.

15:44.560 --> 15:51.480
So if you think of the brain, you can parsulate it into different regions, and there are standard

15:51.480 --> 16:00.720
anatomical atlases that capture specific structures, and so they try to parsulate in a biologically

16:00.720 --> 16:02.400
meaningful fashion.

16:02.400 --> 16:08.480
So the benefit of region parsulation is that you reduce the amount of noise that's there

16:08.480 --> 16:14.120
if you take individual voxels, which are kind of the smallest unit of volume, very analogous

16:14.120 --> 16:16.280
to pixels and an image.

16:16.280 --> 16:21.280
So at the voxel level, this data is very, very noisy and very variable.

16:21.280 --> 16:27.240
So by kind of averaging across a slightly larger region, you tend to reduce some of that

16:27.240 --> 16:29.080
random noise.

16:29.080 --> 16:36.760
So once you've done that parsulation, you essentially have in this FMRI data, a time course or a

16:36.760 --> 16:39.080
signal from every region.

16:39.080 --> 16:43.280
And so you can compute a correlation matrix where every element of the correlation matrix

16:43.280 --> 16:47.120
is just the correlation between the time course at one region and the time course at another

16:47.120 --> 16:48.120
region.

16:48.120 --> 16:53.240
And so in this sort of functional connectivity, brain connectivity space, that's a standard

16:53.240 --> 16:56.520
input that people use into modeling frameworks.

16:56.520 --> 17:02.120
And it's kind of the the elementary unit of information that we tend to extract from

17:02.120 --> 17:05.240
these resting state FMRI data.

17:05.240 --> 17:08.440
Are these connectivity matrices?

17:08.440 --> 17:15.080
Do you get them as a essentially a time series of these connectivity matrices?

17:15.080 --> 17:20.520
So there has been a little work in looking at dynamic evolution.

17:20.520 --> 17:26.440
The most common approach is to kind of compute a single correlation matrix across the entire

17:26.440 --> 17:27.440
acquisition.

17:27.440 --> 17:31.560
So the acquisition is about six minutes long.

17:31.560 --> 17:38.000
So if you think of the sort of N by N region by region as the dimensionality of this correlation

17:38.000 --> 17:41.840
matrix, that's the input to the model.

17:41.840 --> 17:47.840
And so we assume that input can be represented by sort of a low-rankty composition of elementary

17:47.840 --> 17:48.840
subnetwork.

17:48.840 --> 17:53.720
So a subnetwork you can think of as a pattern of co-activation across the brain.

17:53.720 --> 17:58.200
So almost like a heat map of which regions are co-activating or highly correlated with

17:58.200 --> 18:01.800
each other in which regions are anticorrelated with each other.

18:01.800 --> 18:04.680
And so that is kind of the structured assumption.

18:04.680 --> 18:07.600
One of the structured assumptions on the model.

18:07.600 --> 18:16.200
The second assumption is that what is differing on an individual level is the contribution

18:16.200 --> 18:24.600
of those subnetworks to kind of create an entire brain connectivity or entire brain functionality.

18:24.600 --> 18:32.840
And so because those contributions, so not only are they salient for the data representation,

18:32.840 --> 18:36.480
so those are the features that we are using in the predictive modeling.

18:36.480 --> 18:40.480
And so there's a coupling between the data representation and the predictive modeling.

18:40.480 --> 18:45.480
And so in the predictive modeling, what we're trying to do is actually predict some measure

18:45.480 --> 18:47.200
of clinical severity.

18:47.200 --> 18:53.400
So in autism, there are kind of different batteries that you can use to quantify clinical

18:53.400 --> 18:54.400
severity.

18:54.400 --> 18:58.200
The most common is called the Autism Diagnostic Observation Schedule.

18:58.200 --> 19:01.480
So it's typically administered on children.

19:01.480 --> 19:09.160
And it's like a clinician evaluation where they kind of create settings for the child

19:09.160 --> 19:16.160
to play and the child to describe sort of different stories and to observe different

19:16.160 --> 19:19.480
sort of movie clips and explain what they think is happening.

19:19.480 --> 19:25.040
And so there's kind of a standard battery that is used to come up with a level of severity

19:25.040 --> 19:27.640
or level of deficit for autism.

19:27.640 --> 19:35.920
And so your model is making predictions into the space of this diagnostic system.

19:35.920 --> 19:41.200
And what does that look like, is that a kind of a one-to-ten numeric scale, or is it multi-dimensional

19:41.200 --> 19:46.200
across different types of behaviors or expressions of the disorder?

19:46.200 --> 19:54.400
The ADOS exam, so typically you would use the kind of the total measure that the clinician

19:54.400 --> 19:59.160
quantifies across this behavioral paradigm and get a single number.

19:59.160 --> 20:04.120
And it ranges from approximately zero to 30, just based on how it's designed.

20:04.120 --> 20:09.760
There are other diagnostic criteria and they were also using and also, and they provide

20:09.760 --> 20:12.400
a little bit different perspective of the patient.

20:12.400 --> 20:16.680
So the second one is called the Social Responsiveness Scale.

20:16.680 --> 20:19.280
So this is actually a parent report.

20:19.280 --> 20:23.560
So instead of observing the child, you give a questionnaire to a parent or a caregiver

20:23.560 --> 20:26.680
or a teacher about the behavior of the child.

20:26.680 --> 20:31.720
And that's a different way of scoring in terms of clinical manifestation.

20:31.720 --> 20:38.480
And then we're also using another type of behavioral paradigm that was developed by our, is commonly

20:38.480 --> 20:43.600
used by our collaborators at the Kennedy Krieger Institute.

20:43.600 --> 20:50.440
And so this is a, it's a behavioral paradigm that essentially measures the ability of autistic

20:50.440 --> 20:56.000
children to perform gestures on command and also to imitate gestures.

20:56.000 --> 21:03.520
And so one of the interesting observations of children with autism is that in addition

21:03.520 --> 21:10.080
to the well-known social issues, there are also issues with visual and motor systems.

21:10.080 --> 21:12.040
So visual and motor integration.

21:12.040 --> 21:17.320
So loosely hand-eye coordination is one of them ability to imitate gestures is another.

21:17.320 --> 21:21.800
And they, that type of deficit almost parallels the social dysfunction.

21:21.800 --> 21:26.120
So that's a different scale that we're also trying to predict as well.

21:26.120 --> 21:33.240
You mentioned earlier that the frameworks that you've developed achieve state-of-the-art

21:33.240 --> 21:39.440
performance, is that relative to other machine learning approaches?

21:39.440 --> 21:47.840
Or is this a scenario where you're comparing the result of your system to a clinician's

21:47.840 --> 21:49.800
ability to make predictions?

21:49.800 --> 21:54.600
I don't imagine clinicians are looking at FMRI data and trying to predict autism or

21:54.600 --> 21:57.120
am I wrong there?

21:57.120 --> 21:58.120
You are correct.

21:58.120 --> 22:01.360
Clinicians do not use FMRI data to predict autism.

22:01.360 --> 22:08.720
So in most neuropsychiatric disorders, the prediction is, well, the diagnosis is based

22:08.720 --> 22:12.960
on behavioral information and behavioral testing.

22:12.960 --> 22:21.160
So when I say state-of-the-art, what I'm trying to convey is just in relation to other machine

22:21.160 --> 22:22.160
learning algorithms.

22:22.160 --> 22:28.640
So on this data set, we have implemented a variety of algorithms from kind of basic regression

22:28.640 --> 22:34.600
models, which are very, very early iterations of machine learning through other kind of

22:34.600 --> 22:39.520
regression, well, sort of, so-port vector regression, random forests, which are a little

22:39.520 --> 22:45.200
bit more current, and we've even tried end-to-end deep learning approaches.

22:45.200 --> 22:50.920
And so in comparison to that spectrum, at least on our data set, we have found this kind

22:50.920 --> 22:55.760
of hybrid approach where we're looking, we're sort of coupling a data representation,

22:55.760 --> 23:00.080
as well as the predictive modeling to essentially perform the best.

23:00.080 --> 23:01.080
Okay.

23:01.080 --> 23:05.120
And with the advantage that you retain some degree of interpretability, which you may

23:05.120 --> 23:08.320
sacrifice in the end-to-end deep learning space.

23:08.320 --> 23:09.320
Yes.

23:09.320 --> 23:17.280
And so the work that you're doing, looking at predicting autism here, this is just one

23:17.280 --> 23:24.640
of the many disorders that you have looked at in your research, and another one is epilepsy.

23:24.640 --> 23:27.280
Can you talk a little bit about that work?

23:27.280 --> 23:28.280
Sure.

23:28.280 --> 23:34.880
So the epilepsy project, it's a little distinct from what we're trying to do with autism,

23:34.880 --> 23:40.640
and that the work with epilepsy is closer to clinical translation.

23:40.640 --> 23:46.920
So here, our goal is to actually provide information that would be relevant to clinicians

23:46.920 --> 23:49.520
as they're treating these patients.

23:49.520 --> 23:55.480
And so just to kind of give you a background on the application itself, so epilepsy is

23:55.480 --> 23:58.760
one of the most common neurological disorders.

23:58.760 --> 24:03.720
And so in general, the first line of defense is medication, and there are a variety of

24:03.720 --> 24:06.320
anti-aplectic drugs that have been developed.

24:06.320 --> 24:11.400
However, it's estimated that 20 to 40% of patients don't respond to medication in the

24:11.400 --> 24:13.800
sense that they'll continue to have seizures.

24:13.800 --> 24:17.840
And this is really our target cohort.

24:17.840 --> 24:23.360
And so for these patients that are called medically refractory, so they don't respond to these

24:23.360 --> 24:28.240
epileptic drugs, there's very limited alternatives.

24:28.240 --> 24:34.840
And it turns out the best alternative that we have right now is if we can identify, and

24:34.840 --> 24:41.040
if we can trace the seizures to a specific region in the brain that's triggering them,

24:41.040 --> 24:45.640
then clinicians can go in and surgically remove that part of the brain.

24:45.640 --> 24:51.520
And currently, that's kind of state of the art in terms of care, and that will probably

24:51.520 --> 24:58.220
have the best likelihood of the patient recovering in terms of their seizures being alleviated.

24:58.220 --> 25:04.360
And so our focus is on this realm of seizure detection and localization.

25:04.360 --> 25:08.960
So detecting when a seizure occurs through sort of time series measurements and also being

25:08.960 --> 25:13.880
able to localize what is the general area of the brain and what is a specific area of

25:13.880 --> 25:20.080
the brain that the seizures might be coming from, so that again, once we have that target,

25:20.080 --> 25:21.880
it can be acted upon.

25:21.880 --> 25:29.600
And so we're using a variety of machine learning tools and developing new frameworks to be

25:29.600 --> 25:36.480
able to look at non-invasive data, so data collected from EEG and from MRI in order

25:36.480 --> 25:40.800
to provide that localization information to clinicians.

25:40.800 --> 25:44.640
Can you talk a little bit about some of the models that you've developed in some more

25:44.640 --> 25:45.640
detail?

25:45.640 --> 25:46.640
Sure.

25:46.640 --> 25:53.120
So much of the work, especially in the last few years, has focused on EEG data.

25:53.120 --> 26:02.000
So EEG and Zophography is probably the first type of data that's collected for patients

26:02.000 --> 26:04.000
when they go into a hospital.

26:04.000 --> 26:10.400
And here they will actually be admitted into an epilepsy monitoring unit and these EEGs,

26:10.400 --> 26:14.920
so they're kind of sensors that are adhered to the scalp externally.

26:14.920 --> 26:19.400
They're placed on the patient and then the patient is monitored over several days.

26:19.400 --> 26:25.120
So that when they have seizures, clinicians can record that activity happening.

26:25.120 --> 26:32.800
And so currently the state of the sort of standard of care is actually to identify seizures

26:32.800 --> 26:35.600
both detect and localize by eye.

26:35.600 --> 26:42.880
So essentially a clinician will view all of these signals kind of on a computer screen

26:42.880 --> 26:48.720
and scroll through them and then try and visually identify markers of a seizure.

26:48.720 --> 26:53.920
So when it starts and also kind of based on when it starts trying to trace where they

26:53.920 --> 26:57.040
think it's happening, it's originating in the brain.

26:57.040 --> 27:03.480
From the EEG meaning they're looking at which of the leads, is it as simple as which

27:03.480 --> 27:11.000
of the leads is closest to a region that is likely to be functioning in this way?

27:11.000 --> 27:15.880
It's very similar, it's kind of which of the leads are manifesting these particular

27:15.880 --> 27:21.120
signatures that they've been trained to recognize.

27:21.120 --> 27:26.720
And so as you can imagine this is very time consuming, it's prone to human error.

27:26.720 --> 27:33.320
And so just one very kind of on the surface simple task is just developing machine learning

27:33.320 --> 27:36.920
algorithms that can do this automated detection.

27:36.920 --> 27:43.000
So essentially augmenting or replacing kind of what they're doing currently.

27:43.000 --> 27:46.720
And it turns out this is a very challenging problem.

27:46.720 --> 27:50.600
And it's challenging because EEG data is extremely noisy.

27:50.600 --> 27:56.160
And in fact the noise in EEG data tends to overwhelm the signal in terms of the particular

27:56.160 --> 28:02.480
seizure signatures and the particular seizure signatures look a lot like baseline EEG.

28:02.480 --> 28:08.240
So if you or I were to look at these EEG recordings, we would not be able to tell at all what

28:08.240 --> 28:10.080
the origin of the seizure is.

28:10.080 --> 28:14.680
It's kind of years and years of training and sort of pattern matching and building up

28:14.680 --> 28:21.600
models in their heads over kind of what particular features are we looking for and these features

28:21.600 --> 28:25.080
might change from patient to patient, etc.

28:25.080 --> 28:31.040
And so the way we're approaching it is to recognize that just at instantaneous points

28:31.040 --> 28:35.920
in time this data is extremely noisy and we might not be able to get a good detection.

28:35.920 --> 28:42.360
So kind of treat it as a temporal process and essentially to recognize that perhaps

28:42.360 --> 28:48.200
the spreading of these seizure activity or these abnormal activities are just as meaningful

28:48.200 --> 28:51.360
for identifying where and when the seizure is starting.

28:51.360 --> 28:55.920
So if a seizure is starting in a particular area of the brain, so that'll correspond to

28:55.920 --> 29:00.480
a certain area of EEG electrodes.

29:00.480 --> 29:05.400
We expect that that activity will spread locally before it just jumps randomly to another

29:05.400 --> 29:06.400
area.

29:06.400 --> 29:12.920
And so by modeling this kind of local spreading pattern and then inferring kind of where

29:12.920 --> 29:17.960
and when it starts from the data, we can potentially do a better job of both detection

29:17.960 --> 29:19.480
and localization.

29:19.480 --> 29:24.080
So the frameworks we're using here are based on probabilistic graphical models.

29:24.080 --> 29:30.240
And so roughly these probabilistic graphical models allow you to specify sort of latent

29:30.240 --> 29:35.400
or hidden random variables and they capture unoperable phenomenon in our case, the spreading

29:35.400 --> 29:37.000
of the seizure activity.

29:37.000 --> 29:42.080
And then there's observed variables which are related to the statistics of your data

29:42.080 --> 29:45.240
or the features of your data that you're interested in.

29:45.240 --> 29:50.600
And so sort of at a high level, these are, this is the framework that we're looking at.

29:50.600 --> 29:55.600
And we're actually embedding some deep learning into this because the latent variables, they

29:55.600 --> 30:00.000
give us interpretability because what we really care about is the progression of a seizure

30:00.000 --> 30:04.080
and backtracking the onset of seizure activity.

30:04.080 --> 30:09.320
And at the same time, we have a lot of EEG data and so we're training deep neural networks

30:09.320 --> 30:14.480
or artificial neural networks as a complex likelihood function.

30:14.480 --> 30:18.880
So being able to mine different patterns from the data and kind of feed that into the more

30:18.880 --> 30:21.280
interpretable element of the model.

30:21.280 --> 30:22.280
Interesting.

30:22.280 --> 30:24.840
It sounds like a super challenging problem.

30:24.840 --> 30:31.400
And one of the things that jumps out at me is we're often benchmarking or training on

30:31.400 --> 30:38.600
ground truth data and I wonder, how do we know how accurate the physicians are, right?

30:38.600 --> 30:45.840
And they're in the sense of, you know, they are making decisions based on these very

30:45.840 --> 30:49.320
noisy signals, the same noisy signals that you have to deal with.

30:49.320 --> 30:58.120
And they make a decision to take some action, maybe remove part of someone's brain.

30:58.120 --> 31:09.520
But how well do we know if at all how right they were in that it strikes me as just very

31:09.520 --> 31:16.840
difficult to kind of localize from EEG data to a specific part of a brain that's not

31:16.840 --> 31:18.920
acting correctly?

31:18.920 --> 31:21.400
So that's a fantastic question.

31:21.400 --> 31:24.240
So I think there are many layers to that question.

31:24.240 --> 31:30.120
So in terms of clinician accuracy, it's actually unclear how accurate they are.

31:30.120 --> 31:33.480
So they do have a lot more information than just the EEG.

31:33.480 --> 31:39.920
They have the patient history, patient behavior during a seizure is actually supposed to

31:39.920 --> 31:45.760
be fairly relevant in terms of likely areas that the seizure might start.

31:45.760 --> 31:52.080
Oftentimes they have MRI data, so a neuro radiologist can go through it and look for just

31:52.080 --> 31:54.280
structural abnormalities.

31:54.280 --> 32:02.160
I don't know and I haven't come across a systematic study that kind of quantifies asking a variety

32:02.160 --> 32:06.760
of experts across a variety of institutions and then try and understand kind of coordinates

32:06.760 --> 32:08.880
between them.

32:08.880 --> 32:16.640
One interesting statistic is that if you look at meta reviews, long-term seizure freedom,

32:16.640 --> 32:21.680
so postoperative seizure freedom, so if they actually go in and remove kind of a part

32:21.680 --> 32:29.400
of the brain, it's not dramatically high, so after, so if you're looking at five years

32:29.400 --> 32:32.600
seizure freedom rates, it's only about 50%.

32:32.600 --> 32:33.600
Okay.

32:33.600 --> 32:34.600
Right.

32:34.600 --> 32:41.400
So it's, again, there's many factors that could influence the 50%, so it could be that

32:41.400 --> 32:46.880
that particular patient develops some other type of lesion or problem and that took over,

32:46.880 --> 32:50.320
or it could be that there was an error in the care pathway.

32:50.320 --> 32:54.000
So maybe that patient should never have gone for surgery because it turns out there are

32:54.000 --> 32:59.760
multiple areas of the brain that were triggering seizures and that was missed during the review,

32:59.760 --> 33:05.520
or maybe the initial localization was incorrect and so the incorrect portion of the brain was

33:05.520 --> 33:06.520
removed.

33:06.520 --> 33:11.520
And so I think it's unclear which of those factors are involved and hopefully by inserting

33:11.520 --> 33:17.320
some machine learning into that process, it's not that we would replace clinicians because

33:17.320 --> 33:22.160
I don't think our algorithms are nearly at that point yet, but we might be able to provide

33:22.160 --> 33:28.240
information and if we, if the algorithm identifies kind of contradictory information to what

33:28.240 --> 33:33.200
the initial evaluation was, it'll allow the clinician to go back and really focus on

33:33.200 --> 33:37.280
that other area and say, do I really think there's a problem there or is there something

33:37.280 --> 33:39.200
I might have missed?

33:39.200 --> 33:43.680
Maybe we should check it out with another type of data modality.

33:43.680 --> 33:51.480
And are there, are you or are there folks doing multimodal models here?

33:51.480 --> 33:59.040
It strikes me that, you know, in your description of the resources that the physicians have

33:59.040 --> 34:04.880
access to to try to make a prediction, it's much more than just, you know, the time series

34:04.880 --> 34:08.760
date off of an EEG leaf, there's a lot more that they're looking at.

34:08.760 --> 34:15.280
And so it would ultimately we'd want models to be able to incorporate more of that data

34:15.280 --> 34:18.240
as well as that happening in the research.

34:18.240 --> 34:24.640
So that is exactly where my research is headed and so the work that we're doing in the lab.

34:24.640 --> 34:30.120
So we've been focusing on EEG, but just because it's more readily available.

34:30.120 --> 34:37.000
And we recently received some internal funding, a competitive internal funding award to acquire

34:37.000 --> 34:40.240
a multimodal MRI data for some of these patients.

34:40.240 --> 34:48.080
And so some of the work that I had done in my postdoc a while back was, well, showed that

34:48.080 --> 34:53.080
the resting state of MRI that I had mentioned earlier for the autism project that might actually

34:53.080 --> 34:59.720
be useful as another biomarker of seizure origin, especially in cases where there is no

34:59.720 --> 35:03.720
obvious physical lesion that you can see in the brain.

35:03.720 --> 35:09.160
And so by incorporating this resting state of MRI information, structural MR, we're hoping

35:09.160 --> 35:12.120
we can get a finer grain picture.

35:12.120 --> 35:16.640
And then again, once you have models in different modalities, you can look at concordance between

35:16.640 --> 35:23.080
them to first identify is this a reasonable surgical candidate and also what is the potential

35:23.080 --> 35:29.360
area where is most likely the seizure local localization area.

35:29.360 --> 35:35.040
So we've talked about some of the basic science work you're doing, the epilepsy work as

35:35.040 --> 35:39.720
an example of how you're trying to translate to treatment.

35:39.720 --> 35:44.520
You also mentioned work around perception, what do you mean there?

35:44.520 --> 35:47.440
So this is an interesting project.

35:47.440 --> 35:54.600
It started as kind of a pet project idea of mine, but I guess we've tried to pursue it.

35:54.600 --> 36:01.520
So the sort of at a high level, what the project is trying to do is manipulate emotional

36:01.520 --> 36:03.960
cues in human speech.

36:03.960 --> 36:10.560
The way this project idea came about is actually a lot of the work that I had done on autism.

36:10.560 --> 36:15.480
So my postdoc at Yale focused almost exclusively on autism.

36:15.480 --> 36:21.920
Again, it was more through imaging, but at the same time, I was thinking that one of

36:21.920 --> 36:28.520
the one of the hallmark features of autism is that the patients have difficulty perceiving

36:28.520 --> 36:33.360
social and emotional cues and that's particularly true in verbal.

36:33.360 --> 36:40.320
So in language language domain, and so what if we could take speech and amplify emotional

36:40.320 --> 36:45.720
cues to the point where an individual with autism could understand them readily, right?

36:45.720 --> 36:50.040
So if it's very exaggerated, it's at least high functioning individuals don't tend to have

36:50.040 --> 36:51.200
a problem.

36:51.200 --> 36:55.760
It's when it's more subtle that they tend to differ in terms of their perception relative

36:55.760 --> 36:58.240
to their peers who don't have autism.

36:58.240 --> 37:02.760
So if we could do that amplification, and if we could do it computationally, maybe we

37:02.760 --> 37:09.520
can use it as either way to study autism or as an assistive technology, right?

37:09.520 --> 37:14.360
And by doing it computationally, you also have the benefit that you can start slowly undoing

37:14.360 --> 37:15.360
it, right?

37:15.360 --> 37:21.680
So you can think about doing a really well, so a really exaggerated emotion amplification

37:21.680 --> 37:26.880
and then over time reducing it or over the course of an experiment reducing it and trying

37:26.880 --> 37:31.880
sort of quantify at the level an individual can perceive it.

37:31.880 --> 37:39.120
It turns out that no one knows how to amplify emotional cues and speech, it's an unsolved

37:39.120 --> 37:40.120
problem.

37:40.120 --> 37:47.800
I was going to ask that in order to be able to do this amplification based on, you know,

37:47.800 --> 37:53.760
with a dial that adjusts to the level of requirement, we need to be able to do it at all

37:53.760 --> 37:56.240
and I have not seen that.

37:56.240 --> 38:02.280
I think we're making a lot of progress on the, you know, detecting emotions via, you

38:02.280 --> 38:09.160
know, facial image data and speech data, but I've not seen much in terms of kind of

38:09.160 --> 38:14.640
modulating steady state speech data to kind of add that kind of inflection.

38:14.640 --> 38:21.840
Yes, it's an unsolved problem and it's actually not a commonly studied problem.

38:21.840 --> 38:27.400
I would say emotion from speech is hard in general, so even emotion recognition from speech

38:27.400 --> 38:31.120
is quite difficult without, without the video data.

38:31.120 --> 38:37.280
I mean, accuracy is tend to be fairly low and then synthesis or this amplification process

38:37.280 --> 38:39.640
is even more challenging.

38:39.640 --> 38:46.160
So essentially, well, in order to actually implement the project idea, we have to figure

38:46.160 --> 38:51.520
out how to manipulate emotions in speech and so that's one area that we're working

38:51.520 --> 38:57.600
on and in order to manipulate emotions in speech, we need a data and it turns out there

38:57.600 --> 39:02.800
are not a lot of parallel data sets, especially for English, for other languages, I think

39:02.800 --> 39:07.360
there exists a few, but for English and by parallel, I mean to say that if you want to

39:07.360 --> 39:14.080
learn a modulation function, you do want examples of consistent sentences and consistent

39:14.080 --> 39:15.080
actors, right?

39:15.080 --> 39:18.200
So you want the same person saying the same thing in different emotions to be able to

39:18.200 --> 39:20.480
learn a modulation function.

39:20.480 --> 39:27.400
And so with a very, very talented undergraduate student and a very talented graduate student,

39:27.400 --> 39:33.000
we collected this data, so we hired actors from the Baltimore area to come in and read different

39:33.000 --> 39:35.640
things with different emotions.

39:35.640 --> 39:42.920
And we have been developing AI frameworks to try and do this emotion morphing process,

39:42.920 --> 39:47.360
which is what we call it, this modulation process.

39:47.360 --> 39:52.920
And I think we have some preliminary success and we're hoping to build off of it, again,

39:52.920 --> 39:58.440
with integrating these model based frameworks and deep learning strategies and it's been

39:58.440 --> 40:03.360
very interesting learning about this other field and this other type of data which I've

40:03.360 --> 40:08.360
never worked with before and what the sort of tricks and subtleties are.

40:08.360 --> 40:11.000
Have you published the data set?

40:11.000 --> 40:20.280
So yes, so we wrote an initial paper on the data set itself and that appeared in your

40:20.280 --> 40:26.080
speech a few months or last month, I'm sorry, and the data is available.

40:26.080 --> 40:32.040
So it's on my lab website, there's a link where you essentially have to fill out a Google

40:32.040 --> 40:37.960
form or will attach to Google form and then once that's done, we'll send you a download

40:37.960 --> 40:41.760
link to actually download the data and play around with it.

40:41.760 --> 40:42.760
Got it.

40:42.760 --> 40:44.760
And what's the scope of the data set?

40:44.760 --> 40:51.720
How many samples across, how many kind of neutral utterances?

40:51.720 --> 40:56.560
So this initial data collection, it was a little small scale.

40:56.560 --> 40:59.640
So what we focused on were very short utterances.

40:59.640 --> 41:05.440
So single words, multi words phrases and then just a simple noun for predicate sentence

41:05.440 --> 41:11.200
structure because of course linguistics play a role in emotion perception and I think

41:11.200 --> 41:16.200
once you get very complicated, the number of linguistic configurations gets to be very

41:16.200 --> 41:17.200
high.

41:17.200 --> 41:19.040
So it's short utterances.

41:19.040 --> 41:27.960
So there are 10 actors I believe and sort of 250 utterances in five different emotions.

41:27.960 --> 41:33.960
So it's a total about six hours if you just sum up the audio clips.

41:33.960 --> 41:37.240
And what's the approach to modulation?

41:37.240 --> 41:44.040
Are you using something that you might use to synthesize speech like a wave net or something

41:44.040 --> 41:50.520
along those lines or are you doing more traditional signal processing?

41:50.520 --> 41:52.720
So it's a blend between the two.

41:52.720 --> 42:00.200
So wave net is its outstanding tool, but the domain it's looking at is text to speech synthesis.

42:00.200 --> 42:03.920
So essentially given text, it'll output speech.

42:03.920 --> 42:08.280
And it'll output speech in this wave net voice essentially.

42:08.280 --> 42:13.400
The problem that we're focusing on is kind of inputting speech and outputting speech.

42:13.400 --> 42:17.120
And so we've tried a couple of different techniques.

42:17.120 --> 42:23.400
So essentially the input speech you can decompose into kind of a pitch contour, a spectrogram

42:23.400 --> 42:25.480
and a periodicity signal.

42:25.480 --> 42:28.840
So it's a standard analysis pipeline.

42:28.840 --> 42:32.920
And so what controls emotion tends to be intonation.

42:32.920 --> 42:40.520
So in terms of perception and the signal that has a greatest role in intonation is the

42:40.520 --> 42:41.520
pitch contour.

42:41.520 --> 42:46.480
So how pitch varies as someone is speaking a word or a phrase.

42:46.480 --> 42:52.080
And so we've tried, so right now we're targeting this pitch contour as kind of this low-dimensional

42:52.080 --> 42:58.360
feature representation that can help us do this emotion manipulation.

42:58.360 --> 43:00.160
And so we've tried a couple of different things.

43:00.160 --> 43:02.640
We've just tried end-to-end pitch prediction.

43:02.640 --> 43:08.000
So inputting a pitch and then for a given emotion, just outputting a pitch value for that

43:08.000 --> 43:10.000
particular frame.

43:10.000 --> 43:15.360
And then we're right now trying kind of a combination of a model-based approach that's

43:15.360 --> 43:22.040
based on sort of registration or aligning signals using differential geometry or exponential

43:22.040 --> 43:23.120
mapping.

43:23.120 --> 43:27.800
And then that framework itself, we need to be able to predict the parameters of that

43:27.800 --> 43:32.960
warping or that registration so that we're using some deep learning approaches to do the

43:32.960 --> 43:34.480
prediction.

43:34.480 --> 43:39.800
And so the combination of the two is quite interesting and we're sort of, again, trying

43:39.800 --> 43:46.200
to go further in terms of the analysis to get a more consistent emotion warping process.

43:46.200 --> 43:53.200
This is a great introduction to some of the work you're doing there across what sounds

43:53.200 --> 43:59.480
like a very broad array of projects, so hats off to you.

43:59.480 --> 44:00.480
Thank you.

44:00.480 --> 44:07.320
I guess I'm curious, your thoughts on kind of where you see this all going.

44:07.320 --> 44:09.840
So I think it depends on the project.

44:09.840 --> 44:15.960
I mean, at a high level, we want to make everything work better than it currently does.

44:15.960 --> 44:21.880
So again, in terms of the basic science, to be able to do better, more targeted predictions

44:21.880 --> 44:25.600
with multimodal data.

44:25.600 --> 44:31.680
So in that same vein, I have some projects, or I have another project on sort of understanding

44:31.680 --> 44:34.040
imaging and genetic interactions.

44:34.040 --> 44:39.360
So and this is for schizophrenia, so being able to identify genetic markers that relate

44:39.360 --> 44:42.320
to imaging and also are predictive of diagnosis.

44:42.320 --> 44:47.760
And so kind of in that basic science realm to again, make the models more robust, more

44:47.760 --> 44:55.840
generalizable across data sets to extract biomarkers and potentially with these collaborations.

44:55.840 --> 45:00.520
As additional data comes in validated on sort of validate the models and algorithms on

45:00.520 --> 45:04.600
those so that they can be broadly disseminated as tools.

45:04.600 --> 45:09.760
I think on the translational side, we certainly want to do a better job of localization.

45:09.760 --> 45:17.680
So we just published probably the first method that kind of can take scalp EEG and then in

45:17.680 --> 45:21.840
many cases just simultaneously detect and localize the seizures.

45:21.840 --> 45:27.600
But again, our accuracy is not as high as it needs to be for clinical translation.

45:27.600 --> 45:32.960
And so to understand what sorts of other information can we put in there, can we use more data

45:32.960 --> 45:35.600
to get a better predictive model.

45:35.600 --> 45:40.520
And also to again see how this performs in a prospective fashion.

45:40.520 --> 45:46.880
And at the same time, look at multimodal MR as another non-invasive conglomeration of

45:46.880 --> 45:52.280
modalities and improve the prediction and be able to track that and validate that the

45:52.280 --> 45:55.040
outcome actually is reasonable.

45:55.040 --> 46:00.280
And then on the kind of the speech front, I think we have some preliminary success in

46:00.280 --> 46:02.120
terms of being able to do this motion.

46:02.120 --> 46:09.120
So emotion warping, so both quantitatively sort of how well do we predict what that emotional

46:09.120 --> 46:11.480
pitch contour looks like.

46:11.480 --> 46:16.360
We've done some qualitative experiments where we've reconstructed the speech and used

46:16.360 --> 46:21.160
Amazon Mechanical Turk to see to have people rate the speech utterances.

46:21.160 --> 46:23.080
But we'd like to go farther than that.

46:23.080 --> 46:28.960
So definitely improve our ability to create emotions and to manipulate emotions also to

46:28.960 --> 46:34.320
be able to generalize to longer phrases and do this in a more real time fashion.

46:34.320 --> 46:39.800
So I think that's the short term and then the long term is in epilepsy, being able

46:39.800 --> 46:47.680
to create an automated pipeline for patients to come in and be diagnosed, sort of plan out

46:47.680 --> 46:52.320
best therapeutic strategy and essentially improve outcomes.

46:52.320 --> 46:57.240
And then for the speech to actually use it to study different neuropsychiatric conditions

46:57.240 --> 47:02.480
and potentially combine them with other assistive technologies.

47:02.480 --> 47:03.480
Great.

47:03.480 --> 47:07.200
Well, thanks so much for taking the time to share all of that with us.

47:07.200 --> 47:12.080
No, thank you again for inviting me and having me speak here.

47:12.080 --> 47:16.280
All right, everyone.

47:16.280 --> 47:18.200
That's our show for today.

47:18.200 --> 47:23.880
To learn more about today's show, including our guests, visit twomelai.com.

47:23.880 --> 47:28.440
If you missed twomelcon or want to share what you learned with your team, be sure to visit

47:28.440 --> 47:34.480
twomelcon.com slash videos for more information about twomelcon video packages.

47:34.480 --> 48:03.000
Thanks so much for listening and catch you next time.

