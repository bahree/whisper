1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,480
I'm your host Sam Charrington.

4
00:00:31,480 --> 00:00:36,320
Today's episode is part of a series of shows on the topic of AI for the benefit of society

5
00:00:36,320 --> 00:00:39,760
that were excited to have partnered with Microsoft to produce.

6
00:00:39,760 --> 00:00:43,480
In this show, we're joined by Lucas Joplin and Zach Parisa.

7
00:00:43,480 --> 00:00:48,040
Lucas is the chief environmental officer and Microsoft, spearheading the company's

8
00:00:48,040 --> 00:00:52,880
five-year $50 million AI for Earth commitment, which seeks to apply machine learning and

9
00:00:52,880 --> 00:01:00,160
artificial intelligence across four key environmental areas, agriculture, water, biodiversity, and climate

10
00:01:00,160 --> 00:01:01,400
change.

11
00:01:01,400 --> 00:01:06,600
Zach is co-founder and president of Sylvia Terra, a Microsoft AI for Earth grantee whose mission

12
00:01:06,600 --> 00:01:12,720
is to help people use modern data sources to better manage forest habitats and ecosystems.

13
00:01:12,720 --> 00:01:16,680
In our conversation, we discussed the ways that machine learning and AI can be used to

14
00:01:16,680 --> 00:01:23,200
advance our understanding of forests and other ecosystems and support conservation efforts.

15
00:01:23,200 --> 00:01:27,360
We discussed how Sylvia Terra uses computer vision and data from a wide array of sensors

16
00:01:27,360 --> 00:01:32,960
like LIDAR, combined with AI, to yield more detailed small area estimates of the various

17
00:01:32,960 --> 00:01:35,080
species in our forests.

18
00:01:35,080 --> 00:01:39,800
And we also discussed another AI for Earth project, Wild Me, a computer vision based

19
00:01:39,800 --> 00:01:46,840
wild life conservation project that we discussed with Jason Holmberg back in episode 166.

20
00:01:46,840 --> 00:01:51,520
Before diving in, I'd like to thank Microsoft for their support of the show and their sponsorship

21
00:01:51,520 --> 00:01:54,120
of this series.

22
00:01:54,120 --> 00:01:58,160
Microsoft is committed to ensuring the responsible development and use of AI and is empowering

23
00:01:58,160 --> 00:02:03,400
people around the world with this intelligent technology to help solve previously intractable

24
00:02:03,400 --> 00:02:09,960
societal challenges, spanning sustainability, accessibility, and humanitarian action.

25
00:02:09,960 --> 00:02:14,040
Learn more about their plan at Microsoft.ai.

26
00:02:14,040 --> 00:02:15,880
Enjoy the show.

27
00:02:15,880 --> 00:02:19,400
Alright, everyone.

28
00:02:19,400 --> 00:02:25,600
I am here with Lucas Joppa and Zach Parissa.

29
00:02:25,600 --> 00:02:27,560
Lucas is the CEO of Microsoft.

30
00:02:27,560 --> 00:02:33,000
No, not that CEO, but the Chief Environmental Officer.

31
00:02:33,000 --> 00:02:36,760
And Zach is the co-founder and president of Sylvia Terra.

32
00:02:36,760 --> 00:02:40,040
Lucas and Zach, welcome to this Week in Machine Learning and AI.

33
00:02:40,040 --> 00:02:41,040
Thanks for having us here.

34
00:02:41,040 --> 00:02:42,040
It's a huge pleasure.

35
00:02:42,040 --> 00:02:43,040
Great to be here.

36
00:02:43,040 --> 00:02:44,040
Awesome.

37
00:02:44,040 --> 00:02:45,800
So let's dive right in.

38
00:02:45,800 --> 00:02:51,480
We'll be talking about Microsoft's AI for Earth initiative.

39
00:02:51,480 --> 00:02:58,280
But before we jump into that, Lucas, as the CEO of Microsoft, I think I'm going to run

40
00:02:58,280 --> 00:03:03,280
this one all day.

41
00:03:03,280 --> 00:03:06,560
Tell me a little bit about your background on how you came to be the CEO of Microsoft.

42
00:03:06,560 --> 00:03:07,560
Yeah, sure.

43
00:03:07,560 --> 00:03:10,560
So I would say I never dreamed of being the CEO of anything.

44
00:03:10,560 --> 00:03:11,560
That's for sure.

45
00:03:11,560 --> 00:03:17,240
Particularly in the standard context of it, much less what it means in my specific title

46
00:03:17,240 --> 00:03:18,560
is the Chief Environmental Officer.

47
00:03:18,560 --> 00:03:22,760
I mean, I grew up kind of in far northern rural Wisconsin.

48
00:03:22,760 --> 00:03:24,600
I was obsessed with being outside.

49
00:03:24,600 --> 00:03:29,960
And my approach to kind of school and life in general was, how can I get done with anything

50
00:03:29,960 --> 00:03:32,840
that I need to get done with so I can go play out in the woods?

51
00:03:32,840 --> 00:03:41,000
I think I thought I was going to grow to be a game warden or something similar to that.

52
00:03:41,000 --> 00:03:44,800
And technology was not a big factor in my life as well.

53
00:03:44,800 --> 00:03:49,720
I mean, I've never had a computer growing up or a TV or anything else.

54
00:03:49,720 --> 00:03:56,960
And I eventually found my way into university, started discovering that I was really interested

55
00:03:56,960 --> 00:04:03,000
in thinking about a career in environmental science, studied wildlife ecology, again,

56
00:04:03,000 --> 00:04:06,760
not the traditional career path for somebody at Microsoft.

57
00:04:06,760 --> 00:04:11,220
One often spent a little time in the United States Peace Corps in Malawi working for the

58
00:04:11,220 --> 00:04:13,240
Department of National Parks and Wildlife.

59
00:04:13,240 --> 00:04:15,400
Then came back and did my PhD in ecology.

60
00:04:15,400 --> 00:04:21,080
And it was really then that I kind of started to put together this, the two kind of incredible

61
00:04:21,080 --> 00:04:27,000
ages that I think we're alive in today and the way kind of ICR world, which is that we're

62
00:04:27,000 --> 00:04:29,600
doing business here at the intersection of the information age.

63
00:04:29,600 --> 00:04:36,120
And then this also incredible age of negative human impacts on Earth's natural systems.

64
00:04:36,120 --> 00:04:42,080
And it was during my PhD, I just was really struggling with what's the right way to do science

65
00:04:42,080 --> 00:04:45,440
at a way that scales with the scale of the problem.

66
00:04:45,440 --> 00:04:49,680
And that's when computing, programming, machine learning, all kind of came flooding into my

67
00:04:49,680 --> 00:04:55,240
life at the same time and it up at Microsoft and Microsoft Research leading programs in

68
00:04:55,240 --> 00:04:59,720
environmental and computer science and then things just progressed from there.

69
00:04:59,720 --> 00:05:05,880
You actively involved in academic research and a number of organizations, can you share

70
00:05:05,880 --> 00:05:06,880
a little bit about that?

71
00:05:06,880 --> 00:05:08,920
We talked about it a bit earlier.

72
00:05:08,920 --> 00:05:15,400
Yeah, sure, once you live long enough in the academic world, the kind of the Pavlovian

73
00:05:15,400 --> 00:05:21,280
response towards some of the rewards that that environment installs in you, I mean, I'm

74
00:05:21,280 --> 00:05:25,480
not proud to say it, but since I'm not proud, I should just say it, I'm still that academic

75
00:05:25,480 --> 00:05:31,760
that checks their citations every day when I wake up over breakfast.

76
00:05:31,760 --> 00:05:37,160
So while I definitely have a much larger and more expanded purview of roles and responsibilities

77
00:05:37,160 --> 00:05:43,640
here at Microsoft, I still think, you know, science is important, science is what drives

78
00:05:43,640 --> 00:05:48,800
all of the kind of environmental sustainability decisions that we make here at this company.

79
00:05:48,800 --> 00:05:55,240
It's what ultimately led to why we invested in this program AI for Earth.

80
00:05:55,240 --> 00:05:59,560
And I firmly believe that you have to understand the details.

81
00:05:59,560 --> 00:06:03,400
If you're going to, if you're going to try to lead an organization somewhere with a big

82
00:06:03,400 --> 00:06:08,080
picture of vision, if you don't understand the details, if you don't understand the science,

83
00:06:08,080 --> 00:06:09,400
then it's difficult to do that.

84
00:06:09,400 --> 00:06:13,440
And just the way my brain works, the easiest way to understand the details is to kind of

85
00:06:13,440 --> 00:06:17,400
get your hands dirty and be in there with the rest of the world kind of trying to build

86
00:06:17,400 --> 00:06:18,800
the solutions of the future.

87
00:06:18,800 --> 00:06:21,080
And so that's where academic research for me comes in.

88
00:06:21,080 --> 00:06:26,440
It's just that opportunity to actually like go really deep and work on kind of both sides

89
00:06:26,440 --> 00:06:27,440
of the equation.

90
00:06:27,440 --> 00:06:29,160
I still publish in the environmental science literature.

91
00:06:29,160 --> 00:06:35,520
I still publish in the computer science literature and, you know, the most depressing thing about

92
00:06:35,520 --> 00:06:40,400
that is how few of us there are that do both of those things.

93
00:06:40,400 --> 00:06:46,160
And you know, it's one of the things that I spend a lot of my time every day doing is

94
00:06:46,160 --> 00:06:49,120
just trying to bring those two worlds together.

95
00:06:49,120 --> 00:06:52,960
And so yeah, and publishing is a fantastic way to do that.

96
00:06:52,960 --> 00:06:54,960
And Zach, you're a forester.

97
00:06:54,960 --> 00:06:55,960
Yeah.

98
00:06:55,960 --> 00:06:56,960
Yeah.

99
00:06:56,960 --> 00:06:58,960
I didn't know that was a thing beyond the Subaru.

100
00:06:58,960 --> 00:06:59,960
Right.

101
00:06:59,960 --> 00:07:00,960
Sure enough.

102
00:07:00,960 --> 00:07:01,960
Yeah.

103
00:07:01,960 --> 00:07:04,000
It is absolutely a thing.

104
00:07:04,000 --> 00:07:08,280
And kind of an exciting, I think, there's kind of a rebirth in forestry now.

105
00:07:08,280 --> 00:07:13,760
And so I'm hoping that it'll become a more broadly known thing here before too long.

106
00:07:13,760 --> 00:07:16,840
But tell us about your background and about Soviet terror.

107
00:07:16,840 --> 00:07:17,840
Yeah.

108
00:07:17,840 --> 00:07:18,840
Sure.

109
00:07:18,840 --> 00:07:24,640
So my, the start of my story actually isn't terribly dissimilar than Lucas's.

110
00:07:24,640 --> 00:07:29,760
So I grew up in North Alabama, though, not Wisconsin, but in a, in kind of this funny place

111
00:07:29,760 --> 00:07:36,800
that was like North Alabama's, you know, covered in woods, but it also has a NASA installation

112
00:07:36,800 --> 00:07:38,120
in Huntsville, Alabama.

113
00:07:38,120 --> 00:07:41,360
And my youth was basically just spent in the woods.

114
00:07:41,360 --> 00:07:45,680
When I, when I was in first grade, I wanted to be an entomologist.

115
00:07:45,680 --> 00:07:48,960
When I was in third grade, I wanted to be a zoologist.

116
00:07:48,960 --> 00:07:55,080
I went through, you know, geology and so on and so forth until I finally met somebody

117
00:07:55,080 --> 00:07:56,160
who was a forester.

118
00:07:56,160 --> 00:07:59,720
And I, you know, until you meet somebody and you have somebody sort of walk you through

119
00:07:59,720 --> 00:08:02,400
what that is, it's, it's kind of an obscure field.

120
00:08:02,400 --> 00:08:09,480
And what that is to me is sort of the confluence of economics and ecology for me.

121
00:08:09,480 --> 00:08:13,640
And so it was this brilliant opportunity at the time where it lets the way that I saw

122
00:08:13,640 --> 00:08:19,280
it because it brought together everything that I cared about, you know, from the ecology

123
00:08:19,280 --> 00:08:26,400
side, you know, insects and soils, geology, the interconnected nature of all of those,

124
00:08:26,400 --> 00:08:28,080
all of those systems.

125
00:08:28,080 --> 00:08:34,200
And, but also the economic side, sort of, you know, not only what, what the forest is,

126
00:08:34,200 --> 00:08:40,000
but also what we want it to be and how we value that as a society and how we mean to take

127
00:08:40,000 --> 00:08:46,040
it from one place now, which is where we find it today, to where we want it to be and

128
00:08:46,040 --> 00:08:49,880
sort of what we, you know, what we believe we need.

129
00:08:49,880 --> 00:08:52,880
And so that was, that was my entrance into it.

130
00:08:52,880 --> 00:08:55,800
And I believed I would carry that out.

131
00:08:55,800 --> 00:09:03,200
I would, I would live and work as a forester by managing some tract of land for some owner

132
00:09:03,200 --> 00:09:08,880
more than that's public or private, but that I would be focused on that landscape.

133
00:09:08,880 --> 00:09:16,240
And going through undergrad, what I became really interested in were oddly and a surprise

134
00:09:16,240 --> 00:09:24,280
to, you know, to me was the quantitative aspects of certain problems like insects in a forest.

135
00:09:24,280 --> 00:09:30,040
When I first got into forestry, you know, my freshman year, there was a massive outbreak

136
00:09:30,040 --> 00:09:34,120
of southern pine beetle in the US south and it was killing lots of pine trees.

137
00:09:34,120 --> 00:09:38,560
And so that was a really compelling problem to me because it relates so much not only

138
00:09:38,560 --> 00:09:43,600
to the, you know, that trees themselves in the beetle, but also how we've managed them

139
00:09:43,600 --> 00:09:48,320
historically and sort of what, how that impacts local economies, that type of thing.

140
00:09:48,320 --> 00:09:55,640
And so, so I really, I started into pheromone plume modeling of all things in a forested

141
00:09:55,640 --> 00:10:00,360
system and trying to take measurements of concentrations of pheromones in locations

142
00:10:00,360 --> 00:10:05,600
and backtrack to where that originated from in the winter to try and deal with these

143
00:10:05,600 --> 00:10:10,760
beetles more, more effectively. And what that, what I learned from that or what I sort

144
00:10:10,760 --> 00:10:18,280
of, what I gathered was, was that there was this incredible ability to scale up my interests

145
00:10:18,280 --> 00:10:23,160
to, to, you know, to still focus on the things that I loved most, but to look at them with

146
00:10:23,160 --> 00:10:28,920
a different lens and to potentially affect change in a different way than I conceived

147
00:10:28,920 --> 00:10:35,160
of before. And so, you know, I wound up doing work in Brazil. You know, I was really interested

148
00:10:35,160 --> 00:10:41,440
in tropical forestry. I took some time off from undergrad to do that and, you know, worked

149
00:10:41,440 --> 00:10:48,280
in, like other areas Bolivia in, in South America. And there I got to see situations where

150
00:10:48,280 --> 00:10:53,680
people were dependent on different aspects of, of land in different ways and more direct

151
00:10:53,680 --> 00:10:59,200
ways than I think I was familiar with from my youth in, in the US South, where, you know,

152
00:10:59,200 --> 00:11:04,160
they were hurting animals. They were collecting nuts, fruits, things like that. They're collecting

153
00:11:04,160 --> 00:11:10,960
fuel wood to, to stay warm to cook. And they were also wanting to, to sell wood into a market

154
00:11:10,960 --> 00:11:17,280
and to, to develop as, as communities. And so forestry is about trade-offs. You, you

155
00:11:17,280 --> 00:11:22,680
know, there are a lot of things that you, we can do. And there are a lot of potential

156
00:11:22,680 --> 00:11:30,120
futures that we have before us. But we have to address the complexity of those systems

157
00:11:30,120 --> 00:11:35,680
in more comprehensive ways than we have in the past. There's far more than just a timber

158
00:11:35,680 --> 00:11:41,840
market now. There's far more than just to concern for delivery of wood to build houses.

159
00:11:41,840 --> 00:11:45,920
And that's, you know, when we spoke just a little bit before, but that was experienced

160
00:11:45,920 --> 00:11:51,360
very acutely here in the Pacific Northwest when people were confronting the issue of whether

161
00:11:51,360 --> 00:11:57,040
we had enough spotted owl habitat or spotted owls themselves or, or not. And whether

162
00:11:57,040 --> 00:12:02,200
we had managed appropriately in the past to accommodate those and, you know, and what,

163
00:12:02,200 --> 00:12:06,560
and everything that's related to that, to that species, all the habitats and other species

164
00:12:06,560 --> 00:12:12,280
that are related or whether we haven't, whether we've failed. And if we needed to go back

165
00:12:12,280 --> 00:12:18,280
and reconsider the ways that we make decisions. And that was, that was a really freighted conversation.

166
00:12:18,280 --> 00:12:25,920
That was, it, it brought, I mean, people to the, to kind of boiling points. And that

167
00:12:25,920 --> 00:12:29,840
was before my time, really, before I really entered into the profession in any meaningful

168
00:12:29,840 --> 00:12:36,040
way. But, but that type of conversation goes on now. And it's even more complicated. And

169
00:12:36,040 --> 00:12:41,480
there are more issues than, and more dimensions that we have to consider than there were then.

170
00:12:41,480 --> 00:12:47,800
And to have constructive conversations, we have to have information to, to inform those,

171
00:12:47,800 --> 00:12:54,520
those discussions, to facilitate the kind of communication that yields solutions that

172
00:12:54,520 --> 00:12:59,920
people can live with. And I'm presuming that that need is what led you to found Sylvia

173
00:12:59,920 --> 00:13:06,160
Tara. It is. Yeah. Absolutely. And so what is Sylvia Tara? What is the, what do we do? Yeah.

174
00:13:06,160 --> 00:13:12,280
Failing to answer the questions here. So Sylvia Tara is, we, we provide information,

175
00:13:12,280 --> 00:13:16,200
just like what I, you know, what I was speaking about there. Our objective is to help people

176
00:13:16,200 --> 00:13:24,200
use modern data sources, like remotely sensed information from satellites, from, you know,

177
00:13:24,200 --> 00:13:32,880
from aerial bases, from UAVs, and modern modeling techniques to, to help get more resolution

178
00:13:32,880 --> 00:13:38,000
on information and get more accuracy and precision on information, not only just about trees,

179
00:13:38,000 --> 00:13:43,160
but, but about habitats and, and beyond. And so that's, that's the focus of our company.

180
00:13:43,160 --> 00:13:47,280
You know, we've been at this for about nine years. A lot of the folks that we work with

181
00:13:47,280 --> 00:13:52,960
are timber companies. We also work with non like environmental NGOs. We work with government

182
00:13:52,960 --> 00:13:58,680
agencies. And all of them, you know, they have effectively the same questions. They have

183
00:13:58,680 --> 00:14:06,400
very similar, similar needs. And so, you know, initially, up until now, we've been providing

184
00:14:06,400 --> 00:14:10,840
data project to project to help them answer those critical questions that they, that they

185
00:14:10,840 --> 00:14:16,120
confront on a regular basis. And the, you know, I guess the, the reason I'm, I'm in this

186
00:14:16,120 --> 00:14:21,240
room with you all here today is that we, we were able to start working with Microsoft AI

187
00:14:21,240 --> 00:14:29,040
for Earth, and to, to begin to scale and expand that work, to build a, a foundational data

188
00:14:29,040 --> 00:14:34,080
set that we can, that we can start to use to answer these, these questions and to build

189
00:14:34,080 --> 00:14:37,480
on to, to improve our ability to manage for the future.

190
00:14:37,480 --> 00:14:44,640
That's maybe a good segue to taking a step back and look as what is AI for Earth?

191
00:14:44,640 --> 00:14:49,360
Sure. Well, I mean, thinking the context of this conversation, you can think about it.

192
00:14:49,360 --> 00:14:53,960
What is AI for Earth? It's why a reformed forester who's now the co-founder of a startup

193
00:14:53,960 --> 00:14:57,520
and a reformed wildlife ecologist who's now the chief environmental officer at Microsoft

194
00:14:57,520 --> 00:15:01,520
are at a table talking with you on twimble. I feel like we're in this recursive

195
00:15:01,520 --> 00:15:08,520
process, right? Yeah. No, exactly that. I can't even see you guys anymore. I'm just staring

196
00:15:08,520 --> 00:15:12,560
at myself in an infinity mirror here. So what AI for Earth is is as of Tuesday of this

197
00:15:12,560 --> 00:15:19,920
week, a one-year-old program. Okay. Happy birthday. Thank you. Thank you. It was fantastic.

198
00:15:19,920 --> 00:15:24,160
We spent it celebrating with our colleagues at National Geographic in Washington, D.C.

199
00:15:24,160 --> 00:15:30,200
In the woods? Yeah. Unfortunately, no, but at the founder's table of one of the most,

200
00:15:30,200 --> 00:15:37,880
you know, iconic and exploration-driven organizations in the world, right? And so it was an incredible

201
00:15:37,880 --> 00:15:44,520
time. But what AI for Earth is is a five-year $50 million commitment on behalf of Microsoft

202
00:15:44,520 --> 00:15:49,840
to deploy our 35 years. It will actually a little bit more than 35 years of fundamental

203
00:15:49,840 --> 00:15:58,640
research in the core fields of AI and machine learning to deploy those to affect change

204
00:15:58,640 --> 00:16:03,520
in these forkey areas of environment that we care deeply about, which is agriculture,

205
00:16:03,520 --> 00:16:08,440
water, biodiversity, and climate change. And the reason that we're doing that is because,

206
00:16:08,440 --> 00:16:13,400
you know, we recognize that at Microsoft, you know, I already spoke about this kind of

207
00:16:13,400 --> 00:16:19,320
tale of two ages, really. This time of this information age and this time of incredible

208
00:16:19,320 --> 00:16:23,560
and negative impacts of human activities on Earth's natural systems. And you look and

209
00:16:23,560 --> 00:16:28,040
you realize that as a society, we're facing almost an unprecedented challenge. We somehow

210
00:16:28,040 --> 00:16:31,600
have to figure out how to mitigate and adapt to changing climates, insure resilient water

211
00:16:31,600 --> 00:16:37,120
supplies, sustainably feed a human population rapidly growing to 10 billion people all while

212
00:16:37,120 --> 00:16:41,880
stemming this ongoing and catastrophic loss of biodiversity that we see around the world.

213
00:16:41,880 --> 00:16:46,200
And we've got to do that while ensuring that the human experience continues to improve

214
00:16:46,200 --> 00:16:51,200
all around the world for everybody that, you know, economic growth and prosperity continue

215
00:16:51,200 --> 00:16:55,280
to grow. And so, you know, that's why I say it's an unprecedented challenge. I mean,

216
00:16:55,280 --> 00:17:01,040
the scope and the scale are just incredible. And if you look at, um, at the scope and

217
00:17:01,040 --> 00:17:05,080
scale, the problem, and you step back and you ask yourselves, uh, the same question as

218
00:17:05,080 --> 00:17:11,400
a company that I asked myself during my PhD, which is, well, what are the things that are

219
00:17:11,400 --> 00:17:17,320
growing in the same exponential fashion as the scale and complexity of that challenge

220
00:17:17,320 --> 00:17:22,680
of our environmental challenge? Well, pretty much the only trends that are that are happening

221
00:17:22,680 --> 00:17:27,920
in an analogous fashion are in the tech sector. And particularly in the broader field of

222
00:17:27,920 --> 00:17:32,840
AI and the more narrow kind of machine learning learning approaches that are getting a lot

223
00:17:32,840 --> 00:17:39,320
of attention today. And so, um, and so that's when we decided to put together this program

224
00:17:39,320 --> 00:17:43,480
to actually say, hey, you know what, we've been investing as a company for over a decade

225
00:17:43,480 --> 00:17:47,720
at the intersection of environmental science and computer science. I led research programs

226
00:17:47,720 --> 00:17:52,840
in our blue sky research, um, division called Microsoft research for, for a fair number of

227
00:17:52,840 --> 00:17:59,560
years on that. But then the technology reached a point, the criticality of the, of the societal

228
00:17:59,560 --> 00:18:04,360
challenge, I think reached a point that it was time for a company like Microsoft to step

229
00:18:04,360 --> 00:18:10,400
in and actually start to deploy some of those resources and deploy them in ways that ensure

230
00:18:10,400 --> 00:18:15,800
that we ultimately change the way that we monitor model and then ultimately manage

231
00:18:15,800 --> 00:18:21,080
Earth's natural systems in a, in a way that we've never been able to before. And we started

232
00:18:21,080 --> 00:18:27,920
out, as I said a year ago with basically nothing but aspiration. Uh, we looked back this

233
00:18:27,920 --> 00:18:35,480
past Tuesday, uh, on, uh, this event that we had national geographic where we inducted

234
00:18:35,480 --> 00:18:42,960
a new set of grantees into our, into our portfolio and realized that in that short year,

235
00:18:42,960 --> 00:18:48,240
we'd set up relationships with organizations all over the world, over 200 organizations

236
00:18:48,240 --> 00:18:53,360
all over the world, each that are dedicated to taking a machine learning first approach

237
00:18:53,360 --> 00:18:58,320
to solving challenges in these, in these four domain areas that we focus on. They're

238
00:18:58,320 --> 00:19:03,120
on all set, they're working on all seven continents now over 50 countries in the world, 34

239
00:19:03,120 --> 00:19:07,760
countries here in the United States. And then today get the opportunity to sit down with

240
00:19:07,760 --> 00:19:12,960
one of, one of the grantees, right, to hear a little bit more about, you know, just their

241
00:19:12,960 --> 00:19:19,880
particular experience. Um, and, uh, and talk about the ways that, that machine learning

242
00:19:19,880 --> 00:19:27,320
in particular can fundamentally change our ability to, um, to understand what's going

243
00:19:27,320 --> 00:19:34,280
on on planet Earth. Because I think that most people don't take the time to step back

244
00:19:34,280 --> 00:19:44,160
and realize when they hear terms like information age, just how narcissistic that really is,

245
00:19:44,160 --> 00:19:50,120
that almost every bit of information that we've been collecting is about ourselves, right?

246
00:19:50,120 --> 00:19:54,360
It's about where the nearest Starbucks is. It's about what people who searched for also

247
00:19:54,360 --> 00:20:01,120
searched for, right? And it's at the peril of ignoring the rest of, rest of life on Earth

248
00:20:01,120 --> 00:20:05,560
and the ways that it supports us and our economies. It's what Sylvia Terra, I think, is, is

249
00:20:05,560 --> 00:20:10,440
so focused on is using vast amounts of data, new approaches and machine learning to actually

250
00:20:10,440 --> 00:20:17,160
just ask some simple questions. Like where are all the trees in the United States? We don't

251
00:20:17,160 --> 00:20:23,560
know answers to things like that. I mean, that just blows my mind, you know? Um, and so,

252
00:20:23,560 --> 00:20:28,720
um, that's where a lot of this came from. It's just a fundamental desire to change, change

253
00:20:28,720 --> 00:20:34,920
our ability to monitor and model life on Earth. I guess that isn't all that simple, but,

254
00:20:34,920 --> 00:20:39,920
but I also think it's completely and totally doable, right? I mean, look, look at where

255
00:20:39,920 --> 00:20:44,320
we've come from, from an information process and capacity over the past 25 years to where

256
00:20:44,320 --> 00:20:48,400
we are today. I mean, if you would have tried to predict every little bit of it, it would

257
00:20:48,400 --> 00:20:52,560
have been impossible, but, um, but it kind of seems pre-ardane now that you look back

258
00:20:52,560 --> 00:20:53,560
at it.

259
00:20:53,560 --> 00:21:00,680
When I think about the types of systems that we've been talking about thus far, both the

260
00:21:00,680 --> 00:21:07,280
economic systems, political systems, as well as the biological systems, it jumps out

261
00:21:07,280 --> 00:21:15,440
of me that there's a tremendous amount of complexity in those systems and machine learning,

262
00:21:15,440 --> 00:21:22,360
deep learning in particular has this great ability to, like, pick out patterns and abstract

263
00:21:22,360 --> 00:21:28,200
away from complexity, which kind of says to me, oh, it's a no-brainer to apply machine

264
00:21:28,200 --> 00:21:35,840
learning to this. Um, but then, you know, we're still very early on in our ability to put

265
00:21:35,840 --> 00:21:43,880
these, you know, these machine learning to work. And I guess, um, curious, yeah, maybe,

266
00:21:43,880 --> 00:21:49,320
maybe for you, Zach, like, where do you think the opportunity is with applying machine

267
00:21:49,320 --> 00:21:57,640
learning and AI for, uh, the types of problems that, uh, concern you in particular with regard

268
00:21:57,640 --> 00:21:58,640
to forest?

269
00:21:58,640 --> 00:22:04,840
Yeah, yeah, absolutely. So one, I guess, kind of listening, listening to Lucas there,

270
00:22:04,840 --> 00:22:09,280
you know, one thing that kind of jumps out at me from, from when you first spoke in

271
00:22:09,280 --> 00:22:13,200
that, uh, your response to the second question there is, there are lots of people that are

272
00:22:13,200 --> 00:22:17,160
very interested in natural resources and there are lots of people that are very interested

273
00:22:17,160 --> 00:22:21,000
in machine learning and AI, but it is a very small community of people. It's, I think it's

274
00:22:21,000 --> 00:22:25,440
rare that you, you know, it's uncommon to start out believing you're going to spend all

275
00:22:25,440 --> 00:22:31,520
your time outside and then find yourself curled up in front of some code. Uh, so, you

276
00:22:31,520 --> 00:22:37,200
know, the first thing, you know, I think there's a lot of opportunity for, um, you know,

277
00:22:37,200 --> 00:22:41,520
for people to make that leap and to see, just to begin to see that as a more natural,

278
00:22:41,520 --> 00:22:48,120
uh, a more natural thing, uh, because the, the questions are very, they're very complex.

279
00:22:48,120 --> 00:22:54,680
And so, um, again, just like Lucas said, most of our, most of our focus has been on how

280
00:22:54,680 --> 00:23:01,720
to market to somebody to buy a cup of coffee here versus there, um, and how to think about

281
00:23:01,720 --> 00:23:07,680
social networks and how to think about marketing networks and transportation networks.

282
00:23:07,680 --> 00:23:12,680
And I think, you know, it's, it's exciting to see that begin to percolate down and transition

283
00:23:12,680 --> 00:23:21,120
to, uh, the story behind, um, how all of those materials come into our, into our world

284
00:23:21,120 --> 00:23:25,400
and life. The fact is that everything around us, or I think the surprising fact is that

285
00:23:25,400 --> 00:23:29,040
everything around us, every, every little bit of technology and everything that built

286
00:23:29,040 --> 00:23:33,200
this, this room that, you know, that we're in or that your listeners are in, those things

287
00:23:33,200 --> 00:23:39,760
were either grown or mined. Every piece of that, uh, every little bit has some geographic

288
00:23:39,760 --> 00:23:46,840
story, some geographic story, some physical story, some environmental story. And if we

289
00:23:46,840 --> 00:23:52,360
were to be confronted with all of those stories, you know, just from one day of our consumption,

290
00:23:52,360 --> 00:23:57,120
one day of us interacting as we normally do, uh, it would take us years to even sift

291
00:23:57,120 --> 00:24:02,000
through all of those stories. There's, there's no way, there's no way, but those stories

292
00:24:02,000 --> 00:24:09,400
all amassed to have a, a very large impact in how we all live. And so to me, that is the

293
00:24:09,400 --> 00:24:14,640
huge opportunity here. You know, we, you know, we with, with Microsoft AI for Earth have

294
00:24:14,640 --> 00:24:19,480
worked on this, this, uh, data set for the continental US at high resolution to inform

295
00:24:19,480 --> 00:24:26,320
about, you know, down to species and diameters, where, where trees are and what those structures

296
00:24:26,320 --> 00:24:33,640
and compositions are and moving forward, what they could be. Um, but that's not going to

297
00:24:33,640 --> 00:24:40,200
stop, you know, the fact that we are all consumers and that while we have a conservation, uh,

298
00:24:40,200 --> 00:24:46,640
need, we also have a consumptive need. And there's, I think there's so much opportunity

299
00:24:46,640 --> 00:24:52,400
to begin to, to investigate how we balance that and how we feel about that and to engage

300
00:24:52,400 --> 00:24:58,840
a meaningful conversation as, you know, at, at multiple levels in society about how that

301
00:24:58,840 --> 00:25:05,080
can best be done. So like, you know, ask about opportunities. I mean, I, I was never excited

302
00:25:05,080 --> 00:25:11,160
about AI or stats or machine learning for the sake of, you know, I mean, it is awesome.

303
00:25:11,160 --> 00:25:16,320
I now understand that. But it, uh, you know, it's, and I, I do get jammed up about, you

304
00:25:16,320 --> 00:25:21,480
know, exciting advances there. But it's about what it can answer. I mean, that's, that's

305
00:25:21,480 --> 00:25:25,840
what drew me out of the woods and put me in front of a computer. It was the ability to

306
00:25:25,840 --> 00:25:32,520
start to, to, to even think about those, those big questions, uh, and, and put it all like,

307
00:25:32,520 --> 00:25:39,160
like distill it to, to something simple and right in front of us. And, and so that's, that's

308
00:25:39,160 --> 00:25:44,280
the opportunity. It, it allows us to know more about our world and ourselves and to create

309
00:25:44,280 --> 00:25:50,240
a better, uh, a better world and a, and sort of a better image of our, of ourselves.

310
00:25:50,240 --> 00:25:57,080
Can we maybe dig into a little bit more detail of either the data set that you just mentioned

311
00:25:57,080 --> 00:26:05,000
or another project and talk through, uh, the process through which Soviet era uses machine

312
00:26:05,000 --> 00:26:10,720
learning, the challenges that you run into, um, maybe walk us through a scenario.

313
00:26:10,720 --> 00:26:14,320
Sure. Absolutely. And, and I'll just briefly kind of tell you where we're coming from.

314
00:26:14,320 --> 00:26:18,320
People have been managing forests for, you know, hundreds, you know, a couple hundred

315
00:26:18,320 --> 00:26:23,040
years. And in the US, about a hundred, a hundred plus, uh, and they needed information

316
00:26:23,040 --> 00:26:28,400
then as they do now, they, uh, but to get that, they would do a statistical survey. They

317
00:26:28,400 --> 00:26:32,400
would go, you know, the go and put measurements in and you work up an average and you make

318
00:26:32,400 --> 00:26:39,320
a plan based on that average. Um, that has been effective and it's, you know, it's what

319
00:26:39,320 --> 00:26:44,720
people use a lot still today. But, uh, what, what we were, what we're focused on doing

320
00:26:44,720 --> 00:26:50,720
is bringing imagery into bear in model assisted and model based methods to yield small area

321
00:26:50,720 --> 00:26:57,720
estimates. Uh, and, you know, for us, it's at a 15 meter resolution. Uh, and for a 15 meter

322
00:26:57,720 --> 00:27:03,920
pixel, what we're predicting is the number of stems, their sizes and species. And when

323
00:27:03,920 --> 00:27:08,800
I say size, I mean, the diameter of the, the trunk of the tree at four and a half feet

324
00:27:08,800 --> 00:27:13,960
off the ground. And from there to, you know, in a hierarchical context to predict them,

325
00:27:13,960 --> 00:27:21,560
maybe the height of the tree or the ratio of crown to, uh, to just clear bowl at the bottom.

326
00:27:21,560 --> 00:27:26,800
And from there, maybe they're herbaceous, you know, since we can infer or predict maybe

327
00:27:26,800 --> 00:27:31,440
the light conditions, uh, under that forest, how much herbaceous, uh, plant matter there

328
00:27:31,440 --> 00:27:36,720
may be there, uh, carrying that forward. How many, um, how many herbivores that could

329
00:27:36,720 --> 00:27:42,760
support scaling that up, how many large carnivores that could support. But for now, the, the,

330
00:27:42,760 --> 00:27:47,040
the primary piece, this, uh, foundational data set that we've worked out with Microsoft

331
00:27:47,040 --> 00:27:54,000
on is the tree list information for each one of those pixels, which hasn't existed before.

332
00:27:54,000 --> 00:28:00,320
But that opens up so many doors, uh, for what, what we can begin to build on to and model

333
00:28:00,320 --> 00:28:07,160
further, further down the line. And so at a resolution of 15 meters, a single pixel

334
00:28:07,160 --> 00:28:14,520
might contain how many trees contain, it could contain an awful lot. Um, you know, easily,

335
00:28:14,520 --> 00:28:17,960
and this is the tricky thing is a tree could be as small as a seedling. It could be as large

336
00:28:17,960 --> 00:28:24,120
as a Sequoia. So it could, it could have less than one, right? Uh, it could have 300, you

337
00:28:24,120 --> 00:28:28,760
know, packed, but, you know, small tiny little trees packed in tightly. And this is the fundamental

338
00:28:28,760 --> 00:28:34,640
difference about what we're working on here, you know, to me, then, you know, then, then

339
00:28:34,640 --> 00:28:40,320
where we're coming from, which is we need to transition away from the binary, or like,

340
00:28:40,320 --> 00:28:46,000
basically qualitative classifications, forest non-forest. That's not actually that informative

341
00:28:46,000 --> 00:28:51,800
about what that forest can, uh, you know, what habitat it can provide, what, you know,

342
00:28:51,800 --> 00:28:57,440
what maybe we need to do or not do to ensure that it's the type of forest that's going to

343
00:28:57,440 --> 00:29:03,160
continue providing the things we care about, clean water, you know, carbon out of the atmosphere,

344
00:29:03,160 --> 00:29:07,400
um, would to build this table, you know, those, those are the types of things. And so beginning

345
00:29:07,400 --> 00:29:13,560
to quantify those, those aspects is very important. And when I began working with this, um,

346
00:29:13,560 --> 00:29:18,040
you know, there, everything was on the table. I mean, we were, there, there was the potential

347
00:29:18,040 --> 00:29:25,480
to use LIDAR and neural nets, uh, to try and, uh, clarify discrete trees. Um, we do not

348
00:29:25,480 --> 00:29:35,960
do that at, you know, I, uh, for various reasons, largely bias in results. Um, but, uh, for us,

349
00:29:35,960 --> 00:29:41,640
you know, parting out species became a massive problem. So if you have, let's say, 40 trees

350
00:29:41,640 --> 00:29:47,400
of multiple species in one pixel, how do you begin to differentiate those when you're looking

351
00:29:47,400 --> 00:29:54,760
at one pixel of data from lots of imagery sources? And, you know, that, that's, uh, that was

352
00:29:54,760 --> 00:29:58,680
a technical challenge. So one of, one of the things that I think is interesting about this

353
00:29:58,680 --> 00:30:05,720
is like, you're talking about forestry, right? And whether or not people know it's a profession,

354
00:30:05,720 --> 00:30:09,400
it's an extremely old one, you know, some of the people that go on, you don't think that,

355
00:30:09,400 --> 00:30:12,600
you don't think that you're going to be talking about machine learning. You also don't think that

356
00:30:12,600 --> 00:30:17,160
you're necessarily going to be talking about philosophy or existential questions, but you asked

357
00:30:17,160 --> 00:30:23,320
to, you asked a question about 15 meter resolution, right? Which, when you work with organizations

358
00:30:23,320 --> 00:30:28,520
like Sylvia Tara that are looking down at the world and asking what is there, you end up having

359
00:30:28,520 --> 00:30:34,520
these existential conversations about what is a thing, right? At what level should we be taking

360
00:30:34,520 --> 00:30:38,680
data points to be able to feed into these machine learning algorithms? Because when you incorporate

361
00:30:38,680 --> 00:30:42,920
the Z dimension or the Z dimension or whatever you want to call it, whatever part of this planet

362
00:30:42,920 --> 00:30:52,920
Earth is from, you can be, you can be looking down at, in, multitude of different objects, right? And

363
00:30:52,920 --> 00:30:58,520
depending on what sensor you're using, you may only see one of them or you may see many of them

364
00:30:58,520 --> 00:31:04,360
if you're using something like LIDAR and you're able to kind of get your, sent your, your laser

365
00:31:04,360 --> 00:31:10,040
sensors enough to see enough of those things. And so you, you start struggling with all of these

366
00:31:10,040 --> 00:31:17,560
questions that are actually fairly unarticulated in the modern machine learning literature quite frankly,

367
00:31:17,560 --> 00:31:23,560
where, you know, all the standard libraries take in a 300 by 300 pixel, you know, this image and

368
00:31:23,560 --> 00:31:30,440
they all have these harsh expectations and, you know, and sure, maybe we think we all left the

369
00:31:30,440 --> 00:31:34,760
world of frequencies, statistics behind, but we still carry over kind of the ghosts of a lot of

370
00:31:34,760 --> 00:31:40,920
those, you know, harsh binary classification results. And so it's just fascinating, I think,

371
00:31:40,920 --> 00:31:47,160
to think about not just like what's hard in the, in the forestry space and how modern machine

372
00:31:47,160 --> 00:31:51,880
learning techniques can help transform that, but also what the problems in the applications

373
00:31:51,880 --> 00:31:56,920
of an organization like Sylvia Terra and then the rest of our AI first grantees, what that brings

374
00:31:56,920 --> 00:32:02,760
back to the machine learning community, which is what's hard here, right? Why, why can't we just

375
00:32:02,760 --> 00:32:08,360
take all the deep neural network advances that we've made and just voila, we've solved all of the

376
00:32:08,360 --> 00:32:15,320
world's problems, right? It's because, as you said, we're still at the infancy of a lot of what

377
00:32:15,320 --> 00:32:23,960
we hope to achieve in machine learning. We just also recognize the severely short amount of time

378
00:32:23,960 --> 00:32:29,000
that we have to answer some of these bigger and, you know, kind of environmental questions. And

379
00:32:29,000 --> 00:32:33,880
so we have got to take everything that we have at our disposal and start to deploy it.

380
00:32:33,880 --> 00:32:40,760
You mentioned sensors and light hours, kind of a very specific curiosity question. I've always

381
00:32:40,760 --> 00:32:47,240
associated light hours like a local, you know, a very short-range local sensing mechanism.

382
00:32:48,040 --> 00:32:50,200
Is that not the case? Can you do light hour from satellites?

383
00:32:51,080 --> 00:32:57,080
Yes. Yes. Plane. What are all the sensors? A new sensor was just launched, you know, a couple

384
00:32:57,080 --> 00:33:01,720
what weeks ago. Something like that. So there's a new, there's a Jedi sensor that's going,

385
00:33:01,720 --> 00:33:08,280
it's called Jedi. I'm used to it now, but I was going to say, use the light or Luke. Jedi,

386
00:33:08,280 --> 00:33:14,840
Jedi is his, his, no, no. Well, NASA is a designation, but they're strapping this thing onto,

387
00:33:14,840 --> 00:33:23,000
onto the space station. It's going to be pulsing down, you know, not the poles, but basically everything

388
00:33:23,000 --> 00:33:31,400
between. And I think it's full waveform, lidar. And yeah, so absolutely, and even historically,

389
00:33:31,400 --> 00:33:38,440
there was ISAT, which was a satellite-based lidar sensor. But moreover and more commonly in forestry,

390
00:33:40,120 --> 00:33:45,640
and a lot of the, you know, even in urban areas, they're collecting lidar information from airplanes

391
00:33:45,640 --> 00:33:50,680
at, you know, at different altitudes and different point densities, something, you know, a common one

392
00:33:50,680 --> 00:33:57,400
might be like 12 or 24 points per square meter. And those, you know, when you see that over forest

393
00:33:57,400 --> 00:34:02,680
at Canopy, some of those pulses reach the ground. And so the best elevation models that you see in

394
00:34:02,680 --> 00:34:08,760
the US right now are lidar-derived elevation models. And that's the source of a lot of the

395
00:34:08,760 --> 00:34:13,000
information that we're getting. And you see it in a lot of floodplain areas, the Mississippi Delta

396
00:34:13,000 --> 00:34:17,880
area so that we can better understand how flooding may occur or may not occur in certain areas.

397
00:34:17,880 --> 00:34:22,920
One more thing that I'm always struck by when you when you start thinking about remote sensing and

398
00:34:22,920 --> 00:34:27,320
just sensing in general is applied to environmental systems is that as we start to take a more

399
00:34:27,320 --> 00:34:33,640
digital or computational approach to sensing, we almost by definition have got to start taking a more

400
00:34:33,640 --> 00:34:39,480
machine learning approach to driving insights. Because what computers are able to do, and I don't

401
00:34:39,480 --> 00:34:43,960
know, maybe I'm just missing the conversation or maybe the conversation isn't as fully kind of

402
00:34:43,960 --> 00:34:48,600
articulated as it could be, but computers are able to sense the world and so many more

403
00:34:48,600 --> 00:34:54,680
dimensions than people are, right? And why do we model? Well, we model because we need a

404
00:34:54,680 --> 00:35:01,400
simplifying function to help us understand an already complex world. And so what was already

405
00:35:01,400 --> 00:35:07,720
complex according to our five senses has now become exponentially more complicated with things like

406
00:35:07,720 --> 00:35:14,120
hyperspectral resolution monitoring where you're getting thousands of bands back of imagery plus

407
00:35:14,120 --> 00:35:20,520
things like LIDAR that are getting 24 points per square meter. You can't humans don't even know,

408
00:35:20,520 --> 00:35:25,000
it's interesting. People always complain that they don't understand what the net what the layers

409
00:35:25,000 --> 00:35:31,240
and a deep neural network do. We also have no idea how to even interpret most of the signals that

410
00:35:31,240 --> 00:35:35,080
are coming back from the most advanced sensors in the world because they don't correspond to

411
00:35:35,080 --> 00:35:41,640
dimensionality that we that we live in. I was just going to ask that when I've talked to folks

412
00:35:41,640 --> 00:35:48,840
that are using LIDAR in the context of self-driving vehicles, and this whole idea of sensor fusion

413
00:35:49,640 --> 00:35:54,760
comes into play and making sense of all these disparate data sources, you know, that in that

414
00:35:54,760 --> 00:36:01,160
example are very local. And now we're talking about, you know, global data sources or at least,

415
00:36:01,160 --> 00:36:07,720
you know, much larger scale and, you know, with overlapping tiles and capabilities. There's

416
00:36:08,360 --> 00:36:16,840
a ton of complexity there is are those? Is that type of complexity, some of the complexity that

417
00:36:17,800 --> 00:36:23,880
your company is working on managing or do you count on upstream providers to kind of sort a lot

418
00:36:23,880 --> 00:36:28,360
of that out for you? So that's exactly the type of complexity that we deal with. I mean, there

419
00:36:28,360 --> 00:36:35,560
are an enormous pool of potential data sources that exist and they all have, you know, potentially

420
00:36:35,560 --> 00:36:40,920
very useful attributes and some of them less so. They have different time stamps associated with

421
00:36:40,920 --> 00:36:45,400
them and there's one very nice thing about measuring forests as long as you don't mess with them.

422
00:36:45,400 --> 00:36:50,200
They tend not to move too much. So trees are, you know, they're pretty willing subjects to be,

423
00:36:50,200 --> 00:36:54,200
you know, just to be measured, but they are always changing. There's growth associated,

424
00:36:54,200 --> 00:36:59,240
there's natural, there's naturally occurring disturbance, there's, you know, human cause disturbance

425
00:36:59,240 --> 00:37:04,440
and both of those we want to keep track of. But yeah, what, you know, what I see our role

426
00:37:05,480 --> 00:37:12,600
right now as being is taking that, you know, that massive pool of potential sources of remotely

427
00:37:12,600 --> 00:37:20,760
sensitive data and the very small and often underappreciated pool of field measurements,

428
00:37:20,760 --> 00:37:27,160
the things that we actually might care about and translating between those things and creating

429
00:37:27,160 --> 00:37:33,240
something that is more highly resolved, more accurate, more precise and more useful

430
00:37:33,240 --> 00:37:37,800
than what could otherwise be achieved. So yeah, draw the signal out of the noise, the classic,

431
00:37:37,800 --> 00:37:44,520
you know, classic tale. I think if I look at kind of the full portfolio of AI for Earth grantees,

432
00:37:44,520 --> 00:37:53,160
well over 200, you see that at least in my mind, Sylvia Terra is, as an organization,

433
00:37:53,160 --> 00:37:58,120
one of the most mature, right? They're actually, they're, they're out of the lab,

434
00:37:58,120 --> 00:38:03,400
they're a startup business model, et cetera, et cetera. And when I think about why that is in

435
00:38:03,400 --> 00:38:07,640
the context of machine learning, why they're able to take advantage of that, it's because of one

436
00:38:07,640 --> 00:38:15,240
thing that we just heard, which is they're taking advantage of these ground-based data points

437
00:38:15,240 --> 00:38:21,000
that they can use to train their models, right? And that's because forestry is something that is

438
00:38:21,000 --> 00:38:25,400
so inherently tied to our broader economy that we have here in the United States and all around

439
00:38:25,400 --> 00:38:31,240
the world, a history of going out boots on the ground and putting a tape measure around a tree.

440
00:38:31,240 --> 00:38:40,520
And a GPS signal next to it and saying, this tree is here. It's of this height and it's of this

441
00:38:40,520 --> 00:38:47,960
species. And that's so rare in the broader environmental space, right? And that's, it's one of the

442
00:38:47,960 --> 00:38:53,560
reasons that I think organizations like Sylvia Terra are unfortunately kind of standing alone in

443
00:38:53,560 --> 00:38:58,440
many respects is because there's so few data sets. It's called machine learning because we're

444
00:38:58,440 --> 00:39:03,720
teaching computers, right? And to teach you have to be taught or to be taught, you need to be shown

445
00:39:03,720 --> 00:39:10,040
examples. And it's why we've seen so significant of advances in other fields of machine learning,

446
00:39:10,040 --> 00:39:17,720
but not in others. And there's just so few annotations in our space that when you come into a forestry

447
00:39:17,720 --> 00:39:22,600
space where the US government has paid money for the past hundred years to go out and figure all

448
00:39:22,600 --> 00:39:27,720
this out, companies like Sylvia Terra can stand on top of that and really just kind of zoom off

449
00:39:27,720 --> 00:39:33,880
ahead. But they are in many ways the exception to the rule, which is unfortunate, I think.

450
00:39:35,080 --> 00:39:40,200
Do you find that the kind of work that you're doing, you know, we talked about the

451
00:39:41,560 --> 00:39:48,600
sensing and pulling all that information together. Does this put you at the kind of the research

452
00:39:48,600 --> 00:39:54,680
frontier of using machine learning techniques? Or are you able to use off the shelf types of

453
00:39:54,680 --> 00:39:59,880
models? Where does your work fall in the spectrum of complexity?

454
00:40:02,840 --> 00:40:09,320
Maybe complexity is not the right word, just in terms of the innovation cycle. Are you

455
00:40:09,320 --> 00:40:15,000
able to apply things that people are doing in other fields pretty readily? Or are you having to

456
00:40:15,000 --> 00:40:19,880
push the limits and pull right out of academic research or things like that?

457
00:40:19,880 --> 00:40:25,320
You know, it's a little bit of both. I mean, our core algorithm has been, you know,

458
00:40:25,320 --> 00:40:32,200
it's matured over the last nine years of doing the work that we have. And we're a small team.

459
00:40:32,200 --> 00:40:41,000
I mean, we're 10 people effectively. And I guess when I got into this, I originally, like when I

460
00:40:41,000 --> 00:40:46,920
thought this quant path was something that was really resonated with me that I connected with

461
00:40:46,920 --> 00:40:51,960
and that I saw a value in, I originally then thought I was going to be a professor.

462
00:40:51,960 --> 00:40:56,920
I would be a researcher somewhere. I would be putting papers out because that must be how change

463
00:40:56,920 --> 00:41:02,040
happens. And my path changed when I went around to people that I'd worked with in industry and

464
00:41:02,040 --> 00:41:08,440
asked them what papers they were reading to affect to change the way that they worked. What was the

465
00:41:08,440 --> 00:41:14,040
most influential journals that they were reading? And the answer was that they weren't reading the

466
00:41:14,040 --> 00:41:19,640
journals. They were busy managing land and that they wanted a tool, not a publication.

467
00:41:20,520 --> 00:41:26,120
And that, I mean, that was a little eye-opening. And so that's what I, you know,

468
00:41:27,160 --> 00:41:32,840
max my other, Max Nova, my co-founder and I sort of set about to do is build tools.

469
00:41:33,400 --> 00:41:39,480
But I don't, I don't really accept like a full dichotomy between, you know, you know,

470
00:41:39,480 --> 00:41:45,880
is it researcher? Is it kind of off the shelf type stuff? I mean, we pride ourselves in our,

471
00:41:45,880 --> 00:41:50,120
you know, in our ability not only to understand the systems that we're working in, but also

472
00:41:51,000 --> 00:41:58,920
to be abreast of what's happening in modern computational techniques and modeling effort,

473
00:41:58,920 --> 00:42:04,920
you know, modeling tools. So, which I imagine everybody would probably say, right? Like everybody

474
00:42:04,920 --> 00:42:10,440
would like to tell you, no, we're right on the edge. But we're also, what I, the funny thing that

475
00:42:10,440 --> 00:42:15,480
I learned when I got into this on the app, I'm on the applied side. I mean, I, I talk with people

476
00:42:15,480 --> 00:42:20,040
that are trying to figure out wildfire modeling and how to, how to pick which communities to,

477
00:42:20,840 --> 00:42:28,120
to, you know, to allocate funds and effort to help manage a forest to prevent catastrophic fires.

478
00:42:28,120 --> 00:42:31,640
I work with people that are trying to figure out how to manage for forest carbon. I work with

479
00:42:31,640 --> 00:42:36,680
people that try and figure out how to manage forests to deliver wood to a mill to make paper.

480
00:42:38,280 --> 00:42:44,520
But what's, I guess, striking to me from the, from where I started to now is I thought that what

481
00:42:44,520 --> 00:42:50,120
people needed to see was the math. I thought I would show up at their offices and be like, good

482
00:42:50,120 --> 00:42:55,400
news. We figured it out. Check this new method out. Check, you know, we pipe in this data. We put

483
00:42:55,400 --> 00:43:00,680
in these measurements from the ground. We're able to model this more effectively now. And what I,

484
00:43:00,680 --> 00:43:06,760
what I learned is that if, if I can't communicate effectively about, about what we've done,

485
00:43:06,760 --> 00:43:11,960
if it really truly seems like magic, then it is, by definite, it's incredible in like the,

486
00:43:11,960 --> 00:43:17,480
in the truest sense of the word, it is not credible, you know, and, and credibility counts.

487
00:43:18,200 --> 00:43:27,960
And so I, in some cases where, when we're working with people, we may not use the most fantastic

488
00:43:27,960 --> 00:43:33,560
new thing. We may use something that is slightly more costly in terms of input data that it

489
00:43:33,560 --> 00:43:41,080
requires or costly in terms of model fit. But that is more easily understood and explained and

490
00:43:41,080 --> 00:43:50,280
more robust to, like the boot test, you know, you go out and it, it, it just makes sense. So I,

491
00:43:50,280 --> 00:43:58,120
that's, you know, and Lucas, does that experience ring true for the other, the other grantees that

492
00:43:58,120 --> 00:44:02,360
you work with or is there, are there a spectrum of experiences there in terms of where they are

493
00:44:02,360 --> 00:44:10,360
and applying some, some of our grantees are using almost commodity services at this moment,

494
00:44:10,360 --> 00:44:16,200
you know, I mean, Microsoft, for instance, has a, has a service called custom vision AI,

495
00:44:16,200 --> 00:44:21,960
sorry, custom vision API, where you just, they want to do some of our grantees want to do

496
00:44:22,920 --> 00:44:27,800
simple image recognition tasks and the service works for them. And all they, they literally just

497
00:44:27,800 --> 00:44:32,840
drag in a whole bunch of photos of one type and a whole bunch of photos of another type and the

498
00:44:32,840 --> 00:44:38,280
system learns it and produces a result for them. And that's fine, right? So that's pretty far

499
00:44:38,280 --> 00:44:44,600
on the one side of just like commoditized services. Then there's other grantees that are out there

500
00:44:44,600 --> 00:44:51,960
creating exceptionally custom algorithms for their work. I think we've got a grantee

501
00:44:53,160 --> 00:44:59,720
called wildme that does basically facial recognition for species so that they can provide better

502
00:44:59,720 --> 00:45:05,960
wildlife population estimates of species like giraffe and zebra, things that they can, you

503
00:45:05,960 --> 00:45:10,840
know, everybody knows a giraffe or everybody's heard that every giraffe's pattern is unique,

504
00:45:10,840 --> 00:45:14,920
but look at a couple photos of giraffes and you'll realize just how hard it is for the human eye

505
00:45:14,920 --> 00:45:22,040
to spot those differences, right? So they're building algorithms to differentiate any particular,

506
00:45:22,040 --> 00:45:28,760
you know, zebra giraffe and then plug those into statistical models for estimating populations.

507
00:45:29,560 --> 00:45:33,800
There's nothing off the shelf that does that. In fact, most of the main libraries they have to go

508
00:45:33,800 --> 00:45:41,560
back and modify the core code of. So it's a full, full spectrum and we're willing to support

509
00:45:41,560 --> 00:45:47,640
all of it, right? Because what we're trying to get people to understand is, well, first and foremost,

510
00:45:47,640 --> 00:45:52,680
we're just trying to break down the access barrier, right? We want to ensure that budget isn't a

511
00:45:52,680 --> 00:45:57,640
barrier to getting this stuff done because I can assure you and many of your listeners are aware,

512
00:45:57,640 --> 00:46:03,160
sometimes the latest machine learning kind of approaches can be fairly expensive. If not,

513
00:46:03,160 --> 00:46:07,320
you know, it might be an open source library, but somebody needs a thousand GPUs to run this

514
00:46:07,320 --> 00:46:14,360
sitting on, right? So we make sure that the infrastructure gets in the hands of folks, et cetera,

515
00:46:14,360 --> 00:46:19,480
but it's also just kind of awareness that you could be thinking about this. You don't have to be,

516
00:46:20,200 --> 00:46:25,880
we want the world's leading machine learning scientists to be thinking about what they could be doing,

517
00:46:25,880 --> 00:46:29,000
but we don't want the rest of the world to think that they have to be one of the world's

518
00:46:29,000 --> 00:46:34,280
machine learning experts to have a crack at this, right? That there is software and services

519
00:46:34,280 --> 00:46:39,720
that can help them as well. So we see the full spectrum and I think it's super healthy.

520
00:46:41,000 --> 00:46:45,240
We also see the full spectrum of kind of, if I would encapsulate what Zach was saying there

521
00:46:45,240 --> 00:46:51,240
in just kind of two words of interest in what we would call explainable AI, right?

522
00:46:51,240 --> 00:47:00,760
Do people really care why an algorithm said that this was a draft and that was a zebra?

523
00:47:01,320 --> 00:47:07,800
Not really. You don't have to explain that to them, right? Do they want to understand why

524
00:47:08,680 --> 00:47:15,720
some decision support algorithm like a land, like a spatial optimization algorithm that assigns

525
00:47:15,720 --> 00:47:21,160
this part of the country or this part of the county into protected land and this part into

526
00:47:21,160 --> 00:47:28,360
industrial use and this part into urban growth and expansion? How that works? And why people thought

527
00:47:28,360 --> 00:47:33,560
that this was the better policy than that? Probably so. Yes, they do, right? So I think, you know, people,

528
00:47:33,560 --> 00:47:37,240
there's, I think there's a lot of hand-ranging and angst right now around conversations,

529
00:47:37,240 --> 00:47:43,000
like explainable AI, I don't remember. And I think it's just like, it's no different than the

530
00:47:43,000 --> 00:47:50,760
conversation we've always had about modeling, which is why it's a model of a complex system.

531
00:47:51,400 --> 00:47:57,640
Why are you building it? If it's being built to just do a simple classification task and it's

532
00:47:57,640 --> 00:48:04,280
easy for a human to go and check the accuracy left or right, then great, you know, you can use

533
00:48:04,280 --> 00:48:09,800
some really advanced statistical techniques. If it's something that if that model instead is a

534
00:48:09,800 --> 00:48:16,520
model of, for instance, a human decision process, then I think the onus on kind of explainability is

535
00:48:16,520 --> 00:48:26,040
much higher. So along those lines, we've used computation to understand the environment, climate

536
00:48:26,040 --> 00:48:32,200
for a very long time, you know, whether, for example, has, you know, been a great focus of

537
00:48:32,200 --> 00:48:37,640
high-performance computing, you know, taking a step back from, you know, the fact that we're all

538
00:48:37,640 --> 00:48:44,200
really excited about AI, do you, where do you think AI offers unique opportunities relative to

539
00:48:44,200 --> 00:48:49,640
the things that we've done for a long time? Sure. Well, I mean, the answer to that will be

540
00:48:49,640 --> 00:48:53,720
super complex. I'll try to make it simple. And, you know, you mentioned whether, I think, sure,

541
00:48:53,720 --> 00:49:01,320
there's no question that statistics and math and then kind of the computational platforms that

542
00:49:01,320 --> 00:49:05,960
started to support them over or the recent decades have been used for environmental monitoring.

543
00:49:05,960 --> 00:49:10,920
I mean, Fisher was, you know, it goes all the way back to some of these guys. We're biologists,

544
00:49:10,920 --> 00:49:18,840
right? The bigger question is, why are we kind of excited about this today? And for me,

545
00:49:18,840 --> 00:49:26,760
it really is the full kind of broad definition of what we mean by AI. It's the recognition that

546
00:49:26,760 --> 00:49:32,200
we're finally deploying computers, computing systems that can collect unprecedented amounts of data.

547
00:49:32,200 --> 00:49:36,520
And that's just amounts. But, you know, we were talking about the full kind of crazy dimensionality

548
00:49:36,520 --> 00:49:44,040
of the data that we're starting to take on. So we've got this breakthrough in data. We've got

549
00:49:44,040 --> 00:49:51,080
this breakthrough in infrastructure where you can, you know, I made a joke about needing 1,000

550
00:49:51,080 --> 00:49:59,240
GPUs. Well, if you need one 1,000, 10,000, you just got to turn a knob these days and get access

551
00:49:59,240 --> 00:50:04,200
to it. And wherever you are on that knob is still a lot cheaper than a supercomputer. Extremely.

552
00:50:05,240 --> 00:50:12,840
So, and then we have made crazy advances in just a whole plethora of algorithms. But for a lot

553
00:50:12,840 --> 00:50:19,480
of the most important ones, we've directly accelerated the compute through the perspective of those

554
00:50:19,480 --> 00:50:25,000
algorithms, right? So for the first time. And then, of course, we've made it so easy to deploy

555
00:50:25,000 --> 00:50:31,080
these algorithms as web-based services as APIs, right? And then, of course, the software infrastructure

556
00:50:31,080 --> 00:50:37,560
stack and all of that is incredible. So, and we've made it commodity level infrastructure.

557
00:50:37,560 --> 00:50:42,200
Anybody can get access to this stuff. You know, you hear this term democratizing AI. What we

558
00:50:42,200 --> 00:50:47,800
mean by that is bringing it all into a stack that anybody can use. You don't need access to a

559
00:50:47,800 --> 00:50:53,080
government run supercomputer anymore. So that's all one side of it. The other thing is from

560
00:50:53,080 --> 00:50:58,840
Weather is a great example here where traditional weather force casting was strong numerical

561
00:50:58,840 --> 00:51:05,320
simulation, right? And that's one type of math, right? But there wasn't a lot of learning

562
00:51:05,320 --> 00:51:11,800
in real time about what was going on. We took a physical process. We built a model that we

563
00:51:11,800 --> 00:51:17,320
thought strongly corresponded with it. And then we ran numerical simulations of it fast forward.

564
00:51:17,320 --> 00:51:21,400
And yeah, just for the simulation perspective, you need a lot of compute. But then the question

565
00:51:21,400 --> 00:51:26,440
is, but all sorts of crazy things happen when we do that that we don't quite understand, right?

566
00:51:26,440 --> 00:51:30,840
Little eddy fluxes happen in some atmospheric layer or whatever. And we don't really know why.

567
00:51:31,880 --> 00:51:38,440
And then the weather community started using machine learning to not necessarily learn why,

568
00:51:38,440 --> 00:51:43,800
but to be able to predict for one reason or another when those things were going to come. And

569
00:51:43,800 --> 00:51:49,240
weather forecasting got a lot better. Same thing is happening now in climate modeling as well.

570
00:51:49,240 --> 00:51:55,320
We know there's things that we just can't do from our traditional approach to climate modeling.

571
00:51:55,320 --> 00:52:00,920
There's a whole new group that's just kind of spun out that's taking a purely machine learning

572
00:52:00,920 --> 00:52:06,840
first approach to building a new climate model for the world. And not positioning themselves

573
00:52:06,840 --> 00:52:13,960
as better, but positioning themselves as complementary. And so I think that there's a lot of,

574
00:52:13,960 --> 00:52:22,120
um, there's a lot of, uh, work that's just happened in commoditizing all of this stuff as well as,

575
00:52:22,120 --> 00:52:27,560
you know, recognizing that while we've taken a hugely mathematical statistical and computational

576
00:52:27,560 --> 00:52:32,920
approach to doing some of the stuff in the in the past, machine learning is a different approach,

577
00:52:32,920 --> 00:52:39,880
right? It's a data driven approach. Um, and that can be very complementary. And we've seen it accelerate

578
00:52:39,880 --> 00:52:45,160
extremely economically important things like weather cap forecasting, forestry, agriculture,

579
00:52:45,160 --> 00:52:52,120
and on and on. As we wind up, Zach, can you share something that you're particularly excited about

580
00:52:52,120 --> 00:52:59,560
kind of looking forward in terms of the application of AI to forestry? Yeah. So it's absolutely,

581
00:52:59,560 --> 00:53:05,240
I mean, obviously we're excited to be releasing this, this data set, but it's, it's really about what

582
00:53:05,240 --> 00:53:13,480
it enables. Um, we're, we're excited to see more, uh, nuanced and, uh, reactive markets

583
00:53:13,480 --> 00:53:21,640
around environmental services like species habitat, carbon, water, uh, be informed by, by these

584
00:53:21,640 --> 00:53:29,400
type of data. And, uh, to play a part in, in that process to integrate these, uh, these concerns

585
00:53:29,400 --> 00:53:36,280
into, into ongoing management decisions. So that's, I mean, that's the, that's the, the biggest

586
00:53:36,280 --> 00:53:41,000
piece. It's what you can, what you can do with, with this information is you move it from, well,

587
00:53:41,000 --> 00:53:47,080
from data to information to, to decisions. And Lucas, how about from, from your practice, you

588
00:53:47,960 --> 00:53:55,480
look at this from both a, uh, you know, very technical and resource perspective, but also as

589
00:53:55,480 --> 00:54:02,280
managing and interacting with this portfolio of innovators that are working in this space. What,

590
00:54:02,840 --> 00:54:09,080
what are you excited about? Well, ultimately what I'm the, the future I kind of see and the way

591
00:54:09,080 --> 00:54:14,600
that we've structured the whole program is we think the world fundamentally needs is the ability,

592
00:54:15,000 --> 00:54:20,440
or what, what society needs is the ability to query the planet by x, y and t.

593
00:54:20,440 --> 00:54:27,080
Hmm. We need to be able to understask questions, just like we ask some potentially Z. What's that?

594
00:54:27,080 --> 00:54:32,520
No, Z. No, Z. Well, so I was, I was actually speaking with my team the other day and I sent a

595
00:54:32,520 --> 00:54:38,360
slide that said x, y, t of posture, you know, uh, a positive z. And I was like, and I said

596
00:54:38,360 --> 00:54:44,520
stretch goal, right? So, um, so, uh, yeah, if we get the z dimension, then, then I'll be, then I

597
00:54:44,520 --> 00:54:49,000
can retire. Uh, but no, I think, you know, ultimately that's where we need to go. We need to be

598
00:54:49,000 --> 00:54:53,640
able to allow people to ask for any particular piece of land or water. What was there? What's there?

599
00:54:53,640 --> 00:54:59,640
Now, what could be there? And a, and empower policymakers to figure out what should be there,

600
00:54:59,640 --> 00:55:04,840
right? We're far from that. Now, Microsoft's always had kind of a empowering and ecosystem

601
00:55:04,840 --> 00:55:09,640
of customers and partners approach. We don't look, you know, we don't look at the world and say,

602
00:55:09,640 --> 00:55:17,080
oh, say we buy into my x, y, t vision. We don't see that as some fantastical crystal ball

603
00:55:17,080 --> 00:55:23,080
that the world spins around and taps on, right? We see it as a constellation of services

604
00:55:23,720 --> 00:55:31,160
and products and solutions brought by all sectors. And so what we're looking to do is engage with

605
00:55:31,160 --> 00:55:36,920
the Sylvia terrors of the world. Unfortunately, there are far too few at the moment. So engage with

606
00:55:36,920 --> 00:55:43,560
those that are there. Bring up the next generation and the next and the next until eventually,

607
00:55:43,560 --> 00:55:49,320
there's kind of a self-supporting community of machine learning kind of born, you know, we talk

608
00:55:49,320 --> 00:55:55,160
about born digital. I kind of think about born machine learning, you know, these organizations

609
00:55:55,160 --> 00:56:01,640
that it's just baked into their DNA. But the organization is not, it doesn't exist because of

610
00:56:01,640 --> 00:56:09,160
machine learning. It exists because of the challenges that we face in the environmental space.

611
00:56:09,160 --> 00:56:17,160
They just are capable of ingesting machine learning approaches natively and efficiently.

612
00:56:17,160 --> 00:56:22,680
And treat space and time as first-class data citizens in this world of machine learning.

613
00:56:23,720 --> 00:56:28,760
Fantastic. Well, Lucas and Zach, thanks so much for taking the time to chat with me.

614
00:56:29,320 --> 00:56:31,240
Thank you. It was a pleasure. Yeah. Thanks, Sam. Appreciate it.

615
00:56:31,240 --> 00:56:41,000
All right, everyone. That's our show for today. For more information on Lucas, Zach, or any of

616
00:56:41,000 --> 00:56:48,680
the topics covered in the show, visit twimmelai.com slash talk slash 228. To follow along with the AI

617
00:56:48,680 --> 00:56:56,760
for the benefit of society series, visit twimmelai.com slash AI for society. As always, thanks so much

618
00:56:56,760 --> 00:57:06,760
for listening and catch you next time.

