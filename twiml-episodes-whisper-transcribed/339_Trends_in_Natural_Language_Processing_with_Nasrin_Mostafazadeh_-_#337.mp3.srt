1
00:00:00,000 --> 00:00:25,760
Hey everyone, hope you all had a wonderful holiday.

2
00:00:25,760 --> 00:00:30,520
For the next few weeks we'll be running back the clock with our second annual AI Rewind

3
00:00:30,520 --> 00:00:32,120
series.

4
00:00:32,120 --> 00:00:36,960
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,

5
00:00:36,960 --> 00:00:43,120
and other developments that made us splash in 2019 in key fields like machine learning,

6
00:00:43,120 --> 00:00:49,480
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.

7
00:00:49,480 --> 00:00:55,680
Be sure to follow along with the series at twomolai.com slash rewind 19.

8
00:00:55,680 --> 00:01:00,080
As always, we'd love to hear your thoughts on this series, including anything we might

9
00:01:00,080 --> 00:01:01,160
have missed.

10
00:01:01,160 --> 00:01:06,600
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

11
00:01:06,600 --> 00:01:11,560
a comment on the show notes page you can find at twomolai.com.

12
00:01:11,560 --> 00:01:13,920
Happy New Year, let's get into the show.

13
00:01:13,920 --> 00:01:19,040
Alright everyone, welcome back to our AI Rewind 2019 series.

14
00:01:19,040 --> 00:01:23,800
This episode will be covering NLP, and I've got the pleasure of being on the line with

15
00:01:23,800 --> 00:01:26,520
Nasreen Mostafazadeh.

16
00:01:26,520 --> 00:01:34,320
She is a senior AI research scientist at Elemental Cognition, Nasreen, welcome back to the twomolai

17
00:01:34,320 --> 00:01:35,320
podcast.

18
00:01:35,320 --> 00:01:39,600
Hi Sam, glad to be back, thanks for having me.

19
00:01:39,600 --> 00:01:42,120
Definitely glad to be speaking with you again.

20
00:01:42,120 --> 00:01:49,480
We last spoke back in August of 2018, when we spoke about contextual modeling for language

21
00:01:49,480 --> 00:01:53,360
and vision, some of your research.

22
00:01:53,360 --> 00:01:59,400
This time we'll be reviewing some of your thoughts on the most important papers and developments

23
00:01:59,400 --> 00:02:05,880
more broadly in the field that you work in, natural language processing in 2019.

24
00:02:05,880 --> 00:02:11,480
I'll have folks refer back to that previous episode for a little bit more about you and

25
00:02:11,480 --> 00:02:17,440
your background and what you're working on, but to get this conversation started, why

26
00:02:17,440 --> 00:02:22,520
don't we just start with your kind of broad take on 2019 in NLP?

27
00:02:22,520 --> 00:02:26,600
What was the, you know, was it a big year for NLP?

28
00:02:26,600 --> 00:02:33,920
Sure, so actually I think, yeah, I think 2019 was actually an exciting year for NLP,

29
00:02:33,920 --> 00:02:40,000
where, you know, these sort of large pre-trained neural models have been stretched widely to

30
00:02:40,000 --> 00:02:42,320
various different directions.

31
00:02:42,320 --> 00:02:47,080
And, you know, slowly, but surely as a community, we've started to think about, like, the

32
00:02:47,080 --> 00:02:51,280
problems they have, the weaknesses, the blindness bias they have.

33
00:02:51,280 --> 00:02:56,200
So I can say this is sort of paradigm shift that we are seeing in NLP, you know, sort

34
00:02:56,200 --> 00:03:02,960
of be art in 2020 now, kind of started, I can, you know, reflect back on the decade.

35
00:03:02,960 --> 00:03:08,400
This paradigm should have started, you know, back in 2015 to 2016 or so.

36
00:03:08,400 --> 00:03:13,960
And various NLP tasks could, you know, start to get tackled by a relatively, you know,

37
00:03:13,960 --> 00:03:18,880
straightforward approach that you would just encode the input text.

38
00:03:18,880 --> 00:03:24,040
It could be, you know, looked at as a sequence of words, sequence of characters, et cetera.

39
00:03:24,040 --> 00:03:32,640
Then you use, like, attention to actually basically look back into the encoded representation

40
00:03:32,640 --> 00:03:36,720
when you're trying to predict something for the task, which could be a sequence of

41
00:03:36,720 --> 00:03:37,720
output tokens.

42
00:03:37,720 --> 00:03:44,560
So back in the late 2017 or so, you know, Chris Manning, which is one of the paniers of

43
00:03:44,560 --> 00:03:50,400
our field had this, basically, there was this code and belief from him that he's, he

44
00:03:50,400 --> 00:03:56,440
believed in biosteums, he had a hegemony, which he believed that basically, no matter what

45
00:03:56,440 --> 00:04:02,340
the task is out there, an LP task, if you try a biosteum at it and use attention to

46
00:04:02,340 --> 00:04:10,080
attend back to the, basically input, input encoding of the input, you basically can achieve

47
00:04:10,080 --> 00:04:11,080
state of the art.

48
00:04:11,080 --> 00:04:14,800
Now, this referring back to the attention is all you need paper.

49
00:04:14,800 --> 00:04:18,760
So attention is that only you need paper is more recent, so that was then the transform

50
00:04:18,760 --> 00:04:19,760
is came to picture.

51
00:04:19,760 --> 00:04:22,320
This has been LSTNs where it's still a thing, right?

52
00:04:22,320 --> 00:04:23,320
That's amazing.

53
00:04:23,320 --> 00:04:29,760
That how fast the field is moving in 2017 is still the, as I said, like the consensus

54
00:04:29,760 --> 00:04:34,800
in an LP was that you can achieve achieved state of the art if you just throw a biosteum

55
00:04:34,800 --> 00:04:35,800
at it with attention.

56
00:04:35,800 --> 00:04:37,400
That was the recipe.

57
00:04:37,400 --> 00:04:41,600
And back in that time, I remember like when I was like giving talks, I would conclude

58
00:04:41,600 --> 00:04:46,440
that look, although that has been true for a bunch of, a host of different benchmarks,

59
00:04:46,440 --> 00:04:51,840
it happens that for the test, it requires vast amounts of background knowledge, reasoning

60
00:04:51,840 --> 00:04:58,120
and basically require establishing a long context, we can, we can not yet achieve a state

61
00:04:58,120 --> 00:05:02,800
of the art or near human performance using these biosteum models.

62
00:05:02,800 --> 00:05:10,040
So fast forward, just one year, in 2018, we had like LMO, this deep contextualized word

63
00:05:10,040 --> 00:05:17,360
representation that basically started sort of this one more step forward of building these

64
00:05:17,360 --> 00:05:22,480
large language models, which happened to be contextualized, so preaching on a very

65
00:05:22,480 --> 00:05:28,360
large corpus and then fine tune on downstream test, which itself started meeting lots and

66
00:05:28,360 --> 00:05:34,480
lots of different state of the arts and establishing, you know, brand new state of the arts.

67
00:05:34,480 --> 00:05:39,760
And so the test that I had in mind when I was personally criticizing the fact that, oh,

68
00:05:39,760 --> 00:05:46,240
look, by throwing biosteums with attention on a particular benchmark, you don't necessarily

69
00:05:46,240 --> 00:05:50,480
issue a state of the art, for a conist's reasoning task, which is something that I personally

70
00:05:50,480 --> 00:05:55,120
very passionate about and happens to be a minor line of research.

71
00:05:55,120 --> 00:06:00,240
So the particular task was a story closed test, which I talked with you in the last

72
00:06:00,240 --> 00:06:05,320
time I talked with you, specifically a story closed test, which is this task that given

73
00:06:05,320 --> 00:06:11,960
a sequence of four sentences, which form a coherent story, very short story, the task is

74
00:06:11,960 --> 00:06:16,800
to choose between two alternative endings to that story, which, you know, is designed

75
00:06:16,800 --> 00:06:22,200
basically to evaluate a system's commonsense reasoning capabilities.

76
00:06:22,200 --> 00:06:28,520
So what happened in 2017 is that in mid 2017 or so, the attention is all unique paper came

77
00:06:28,520 --> 00:06:33,840
out, the transformer paper that you just mentioned a minute or two ago.

78
00:06:33,840 --> 00:06:41,520
So that paper basically enabled a cascading effect of other very large pre-trained transformer

79
00:06:41,520 --> 00:06:46,880
models that could actually establish the state of the art in various commonsense reasoning

80
00:06:46,880 --> 00:06:47,880
tasks.

81
00:06:47,880 --> 00:06:55,160
And being the GPT-1 paper, so the GPT-1 paper came out around, in 2018, which was, you

82
00:06:55,160 --> 00:06:58,160
know, this, they called it like, generative pre-training model.

83
00:06:58,160 --> 00:07:03,920
This was a very large language model that open AI folks have basically trained on a very

84
00:07:03,920 --> 00:07:08,400
large diverse corpus and then fine tune on a small data sets.

85
00:07:08,400 --> 00:07:14,520
And actually, the data set that they highlighted as to the place where they've made the most,

86
00:07:14,520 --> 00:07:20,000
you know, amazingly, basically, progress happened to be a story close to the, you know, benchmark

87
00:07:20,000 --> 00:07:22,760
that I really cared about.

88
00:07:22,760 --> 00:07:29,040
So they had gotten, you know, notably they had gotten like around 86 or so percent accuracy

89
00:07:29,040 --> 00:07:34,760
in which was exceedingly better than the previous numbers that people had reported on

90
00:07:34,760 --> 00:07:36,000
the test set.

91
00:07:36,000 --> 00:07:41,640
And so that really sort of changed my personal mind about verbure going with this.

92
00:07:41,640 --> 00:07:47,400
I started believing in the fact that, oh, look, although these models may seem to be sort

93
00:07:47,400 --> 00:07:53,360
of doing pattern recognition at the scale, which may not go hand in hand with doing reasoning

94
00:07:53,360 --> 00:07:57,520
and connecting to that and all these sorts of things that we care about and label as commonsense

95
00:07:57,520 --> 00:08:02,760
reasoning, if we, you know, do them in the right way or give these models enough chance

96
00:08:02,760 --> 00:08:07,560
of being trained for, you know, on the right data sets, fine tune on right data sets, etc.

97
00:08:07,560 --> 00:08:10,400
They are actually capable of doing knowledge transfer.

98
00:08:10,400 --> 00:08:17,880
So I think that sort of set the ground up for us to move into 2019, verbure had more

99
00:08:17,880 --> 00:08:25,040
and more of these very large pre-trained models that then you could basically fine tune

100
00:08:25,040 --> 00:08:30,560
on various downstream tasks and establish a state of the art, no matter whether or not

101
00:08:30,560 --> 00:08:35,680
they are from our very core and healthy tasks, like a task such as product speech tagging

102
00:08:35,680 --> 00:08:40,440
or very, like, semantically oriented tasks such as the story costus itself, commonsense,

103
00:08:40,440 --> 00:08:41,760
reasoning, etc.

104
00:08:41,760 --> 00:08:47,920
So I think this has been the main exciting thing about 2019, where we could see that this

105
00:08:47,920 --> 00:08:53,800
wasn't just the, you know, glimpse of how this wasn't just the one time thing that these

106
00:08:53,800 --> 00:08:58,160
models could perform well, it continued into 2019.

107
00:08:58,160 --> 00:09:03,760
And I think I'm actually excited about seeing verbure go with improving these, and, you

108
00:09:03,760 --> 00:09:08,520
know, we will talk more about the downsides of these models, but yeah, I'm very excited

109
00:09:08,520 --> 00:09:12,840
to see verbure are going with this paradigm shift into 2020.

110
00:09:12,840 --> 00:09:20,240
Yeah, I chatted a little bit in one of my previous conversations in this series.

111
00:09:20,240 --> 00:09:26,480
It was a conversation with Zach Lipton, in particular, about the role that these transformer

112
00:09:26,480 --> 00:09:31,320
models have played in NLP, and his take was pretty interesting.

113
00:09:31,320 --> 00:09:37,360
It was focused on the notion that the amount of compute that went into creating these

114
00:09:37,360 --> 00:09:44,000
models, creates a huge barrier for, or sets a new kind of a new standard that creates

115
00:09:44,000 --> 00:09:50,120
a huge barrier for folks that want to do future research on the model side, the amount

116
00:09:50,120 --> 00:09:55,840
of compute required to develop models that can achieve state-of-the-art performance

117
00:09:55,840 --> 00:09:59,800
is, you know, such a high bar, are you seeing that as well?

118
00:09:59,800 --> 00:10:04,560
Absolutely, actually, one of the papers that I wanted to highlight, which I think I can

119
00:10:04,560 --> 00:10:10,400
just highlight it now, is this very amazing work, I would say, that came out of UMass, this

120
00:10:10,400 --> 00:10:17,040
is from Struvo et al, a CL 2019 paper called Energy and Policy Consideration for Deep Learning

121
00:10:17,040 --> 00:10:18,040
in NLP.

122
00:10:18,040 --> 00:10:23,920
So I would say that, yes, as I was saying, we've come a very long way in making advancements

123
00:10:23,920 --> 00:10:29,280
in NLP, and through these very large pre-trained models that we are building, but they have

124
00:10:29,280 --> 00:10:34,720
two major sort of policy, you know, like external implications, right?

125
00:10:34,720 --> 00:10:41,480
One of them is the fact that these are, these require really expensive and extensive resources,

126
00:10:41,480 --> 00:10:47,200
you know, millions of dollars are basically used, you know, in terms of like cloud, etc.,

127
00:10:47,200 --> 00:10:53,120
for basically building these models, which is very much sort of unique and makes it

128
00:10:53,120 --> 00:10:58,520
entitled to the top players in the field, such as like the, you know, large tech companies.

129
00:10:58,520 --> 00:11:05,280
And I think that this sort of implies, as if AI research would tend to get privatized

130
00:11:05,280 --> 00:11:11,800
and only accessible to the players in industry, with access to resources, which is of course

131
00:11:11,800 --> 00:11:16,960
not fair, is not fair and it will have lots of other implications for the society as

132
00:11:16,960 --> 00:11:22,360
whole and who will have access to these kinds of amazing outcomes of AI.

133
00:11:22,360 --> 00:11:27,560
So I think that's definitely a major problem that we have.

134
00:11:27,560 --> 00:11:31,280
And along with that, the reason I wanted to highlight this paper is that something else

135
00:11:31,280 --> 00:11:38,280
that we have in Manpada, as much about art, environmental implications, basically, of

136
00:11:38,280 --> 00:11:41,640
these large models that we are building, right?

137
00:11:41,640 --> 00:11:47,360
So this paper that I referenced, Energy and Policy Concereship for Deep Learning, says

138
00:11:47,360 --> 00:11:51,720
that although people keep talking about the fact that we are throwing, we need to throw

139
00:11:51,720 --> 00:11:58,640
so much money at these models, which only a handful of players are capable of doing, we

140
00:11:58,640 --> 00:12:04,680
also are basically increasing our carbon footprint.

141
00:12:04,680 --> 00:12:10,040
So the tagline was actually, which got a lot of news coverage, was that training a single

142
00:12:10,040 --> 00:12:15,640
AI model can emit as much carbon as five cars in their lifetime.

143
00:12:15,640 --> 00:12:18,960
And I think that's pretty, you know, crazy, right?

144
00:12:18,960 --> 00:12:23,840
I think that the number of reciting was something around like, you know, more than like half

145
00:12:23,840 --> 00:12:29,520
a million pounds of carbon dioxide is emitted after just basically training one of these

146
00:12:29,520 --> 00:12:32,160
large models that we were just talking about.

147
00:12:32,160 --> 00:12:37,440
So I think that that's just a major consideration that we should take into account moving forward

148
00:12:37,440 --> 00:12:39,240
as a field.

149
00:12:39,240 --> 00:12:44,640
It's definitely huge and I would refer folks interested in learning more about that to

150
00:12:44,640 --> 00:12:52,760
check out my interview from back in July of 2019 with MS Trubel, the author of the paper

151
00:12:52,760 --> 00:12:55,120
that you're referring to.

152
00:12:55,120 --> 00:13:01,320
So with that in mind, when you stepped back and thought about some of the more important

153
00:13:01,320 --> 00:13:07,520
things or more interesting things to you in 2019, you divided that into a couple of key

154
00:13:07,520 --> 00:13:08,520
trends.

155
00:13:08,520 --> 00:13:11,720
Do you want to talk about the first of those?

156
00:13:11,720 --> 00:13:20,080
Sure, absolutely. So the first theme that I wanted to highlight was interpretability,

157
00:13:20,080 --> 00:13:23,120
ethics, fairness and bias in NLP.

158
00:13:23,120 --> 00:13:29,000
So this also happened to be one of the traumatic paper tracks for our, you know, one of our

159
00:13:29,000 --> 00:13:32,280
major conferences in 2019.

160
00:13:32,280 --> 00:13:39,560
And I think the time is actually ripe for us as a community to start thinking about the,

161
00:13:39,560 --> 00:13:44,600
you know, ethical implications of our work and basically, I think in beyond just making

162
00:13:44,600 --> 00:13:49,520
scientific improvements, but also about what are we actually enabling.

163
00:13:49,520 --> 00:13:54,880
So I think it's been really great in the shape learning in AI community as a whole that

164
00:13:54,880 --> 00:13:58,080
in the past like three, four years or so.

165
00:13:58,080 --> 00:14:03,240
A lot of players in the field are talking about ethics in AI, but the truth is that I think

166
00:14:03,240 --> 00:14:10,240
it has been long overdue and we need to educate practitioners and scientists so much more

167
00:14:10,240 --> 00:14:11,240
on the topic.

168
00:14:11,240 --> 00:14:17,320
And I think it's been really a positive change that in our conferences at least, we've started

169
00:14:17,320 --> 00:14:24,240
making particular tracks, particular themes, et cetera, for highlighting these particular

170
00:14:24,240 --> 00:14:27,760
considerations and giving them the credit that they deserve.

171
00:14:27,760 --> 00:14:39,760
Yeah, it seems like not long ago, the conversation in this area in NLP was rather more simplistic

172
00:14:39,760 --> 00:14:41,440
than it is today.

173
00:14:41,440 --> 00:14:47,680
You know, we would talk a lot about the kind of the word to VEC example, you know, and

174
00:14:47,680 --> 00:14:50,880
several of those were popular.

175
00:14:50,880 --> 00:14:55,880
But now it seems like the conversation is quite a bit more nuanced.

176
00:14:55,880 --> 00:14:57,880
Is that something you would agree with?

177
00:14:57,880 --> 00:14:58,880
Yes, absolutely.

178
00:14:58,880 --> 00:15:04,680
The field is definitely maturing and as I said, the fact that we are establishing particular

179
00:15:04,680 --> 00:15:12,640
tracks for just soliciting papers and submissions for these particular considerations is definitely

180
00:15:12,640 --> 00:15:14,760
helping that movement.

181
00:15:14,760 --> 00:15:19,280
And yeah, I think I can go ahead and tell you a little bit more about the particular

182
00:15:19,280 --> 00:15:24,240
papers that I had in mind that I wanted to highlight.

183
00:15:24,240 --> 00:15:28,000
So I think I will go ahead and talk about, so there are different angles right to this

184
00:15:28,000 --> 00:15:29,000
problem.

185
00:15:29,000 --> 00:15:35,600
Sort of ethics and fairness in AI and like de-biasing, basically, AI and hands-on-up

186
00:15:35,600 --> 00:15:44,320
models is one end of things and then basically building explainable AI in NLP systems is the

187
00:15:44,320 --> 00:15:49,640
other end of a spectrum because we want to have these systems be accountable towards

188
00:15:49,640 --> 00:15:54,000
the predictions that they are making which goes hand in hand with sort of de-biasing them

189
00:15:54,000 --> 00:15:59,120
or, you know, basically better societal use cases that they could have.

190
00:15:59,120 --> 00:16:03,240
So I will start with the explanation one.

191
00:16:03,240 --> 00:16:08,680
So explanation is this really overloaded term and, you know, today probably won't be

192
00:16:08,680 --> 00:16:13,400
the day that you are going to cover what explanation means.

193
00:16:13,400 --> 00:16:19,360
But I'm going to highlight one paper through which I'm going to mention a few other papers

194
00:16:19,360 --> 00:16:26,440
that sort of started a debate in the field in NLP field this year about explanation.

195
00:16:26,440 --> 00:16:32,160
So this paper is titled Attention is Not Explanation, it's a work that came out of North

196
00:16:32,160 --> 00:16:38,560
Eastern University, published in NACCLE 2019, the authors were Valous and Jean.

197
00:16:38,560 --> 00:16:44,400
So this paper, as the title suggests, is talking about attention and not being explanation.

198
00:16:44,400 --> 00:16:49,960
And so what they're actually trying to highlight is the fact that you remember just a few

199
00:16:49,960 --> 00:16:57,880
minutes ago, I was talking about this paradigm of encoding and then attending and then decoding

200
00:16:57,880 --> 00:17:00,760
for doing multiple NLP tasks.

201
00:17:00,760 --> 00:17:08,440
So attention has been used and often presented at least implicitly as this relative importance

202
00:17:08,440 --> 00:17:12,040
of input kind of a measurement that we've had in the field.

203
00:17:12,040 --> 00:17:19,320
Basically pretty much like a common citation for summarizing this commonly helped you by

204
00:17:19,320 --> 00:17:26,080
Leah Alt has a 16 paper, it was that attention sort of provides an important way of explaining

205
00:17:26,080 --> 00:17:28,000
the inner workings of neural models.

206
00:17:28,000 --> 00:17:33,640
So it's like pretty much, it was pretty much established until this conversation was

207
00:17:33,640 --> 00:17:37,760
started by this paper, that attention is something that you can count as explanation.

208
00:17:37,760 --> 00:17:43,680
Again, I'm not going to argue or basically define what explanation means, but even loosely

209
00:17:43,680 --> 00:17:48,560
there were enough people in the community to count it's attention as explanation.

210
00:17:48,560 --> 00:17:52,960
And you know, for me, like as someone working in Communism's reasoning, caring about deep

211
00:17:52,960 --> 00:17:58,160
natural language understanding, like basically going beyond what's explicit out there, et

212
00:17:58,160 --> 00:18:03,880
cetera, I personally took so many issues with that leave because as you can imagine, there

213
00:18:03,880 --> 00:18:09,320
are so many tasks where your answer or whatever the inner workings of your reasoning engine

214
00:18:09,320 --> 00:18:14,000
is, of your reason, reason paradigm is it's not going to be anything explicit in the input

215
00:18:14,000 --> 00:18:17,840
that you can even highlight as the attention waits.

216
00:18:17,840 --> 00:18:22,920
So that's a major obviously shortcoming, but sitting data side even for tasks like say

217
00:18:22,920 --> 00:18:29,000
a squad, et cetera, where you are actually going to attend literally to parts of the input

218
00:18:29,000 --> 00:18:34,880
of text to provide the prediction is still like people were using that as their explanation.

219
00:18:34,880 --> 00:18:41,520
So this work actually was critical of that premise, basically, it was claiming that it

220
00:18:41,520 --> 00:18:48,600
has been unclear what is the relationship between attention waste and the model outputs.

221
00:18:48,600 --> 00:18:54,640
And so they argue that if attention wants to be a faithful explanation for any models

222
00:18:54,640 --> 00:18:59,320
prediction, it should have two particular characteristics.

223
00:18:59,320 --> 00:19:06,080
One is that there should be a correlation between the inputs and outputs, which means that

224
00:19:06,080 --> 00:19:12,880
the way that they quantify this is that attention waste should be correlated with measures

225
00:19:12,880 --> 00:19:15,320
of feature importance that we have.

226
00:19:15,320 --> 00:19:21,320
And then the second point that they make that they think for a faithful explanation should

227
00:19:21,320 --> 00:19:27,200
be held to is the fact that the models explanation should be exclusive, meaning that if we change

228
00:19:27,200 --> 00:19:32,880
the attention distribution dramatically, of course the prediction should also change.

229
00:19:32,880 --> 00:19:38,560
So these are the two main basically points that they made and they went ahead and presented

230
00:19:38,560 --> 00:19:45,360
actually various experiments for showing that for the first point that actually attention

231
00:19:45,360 --> 00:19:51,640
waste are not correlated with measures of feature importance like the grading based ones.

232
00:19:51,640 --> 00:19:56,120
And for the second one, they actually showed that even if you shuffle like randomly shuffle

233
00:19:56,120 --> 00:20:01,320
the distribution of the attention rates, there are many cases where the predictions are

234
00:20:01,320 --> 00:20:03,440
actually going to stay constant.

235
00:20:03,440 --> 00:20:09,640
So they conclude that the standard attention modules do not provide any meaningful and systematic

236
00:20:09,640 --> 00:20:15,760
explanations of basically the community should stop treating them as such.

237
00:20:15,760 --> 00:20:20,360
The interesting thing that happened after this paper came out, and as I said, it was

238
00:20:20,360 --> 00:20:25,720
accepted and published at NACCLE of an overmajor conference, was that there was a follow-up

239
00:20:25,720 --> 00:20:31,000
paper to it which was titled Attention is Not to Not Explanation.

240
00:20:31,000 --> 00:20:41,880
Yeah, so this was a work that came out of Georgia Tech, again like publishing ACL in

241
00:20:41,880 --> 00:20:49,760
EMNLP 2019, sorry, that was arguing that this approach that the authors of the attention

242
00:20:49,760 --> 00:20:53,880
is not explanation took had some problems, right, and then there was some back and forth

243
00:20:53,880 --> 00:20:57,920
they actually encouraged the audience that if they're interested they can go and read

244
00:20:57,920 --> 00:21:05,040
the blog post or respective blog post that they had basically arguing the different points

245
00:21:05,040 --> 00:21:06,040
that they had.

246
00:21:06,040 --> 00:21:11,760
But I think that the conclusion is that I think this whole thread was very healthy for the

247
00:21:11,760 --> 00:21:18,880
community to start thinking about such presumptions that we make before digging deeper and basically

248
00:21:18,880 --> 00:21:22,320
proving what we are counting as XYZ.

249
00:21:22,320 --> 00:21:30,000
So I think that was a very interesting example of a good scientific contribution to the

250
00:21:30,000 --> 00:21:38,320
community where we go back in time and look at what we assume to be true and just dig deeper.

251
00:21:38,320 --> 00:21:43,720
And so in line with that I actually want to mention, so there has been lots of other

252
00:21:43,720 --> 00:21:52,400
actually follow-up papers, I want to highlight one actually toolkit that came out of AI2

253
00:21:52,400 --> 00:21:54,760
which is called Alan LK Interprete.

254
00:21:54,760 --> 00:21:59,720
So this actually happened to get the best demo paper awarded our major, one of our major

255
00:21:59,720 --> 00:22:06,040
conferences as well, which is a toolkit that makes it easy for different people to apply

256
00:22:06,040 --> 00:22:11,360
and visualize actually such saliency maps for whatever model they're deploying.

257
00:22:11,360 --> 00:22:19,000
I think this whole thread, as I was saying, was very good for reminding people that you

258
00:22:19,000 --> 00:22:24,680
have to think about interpretability, you have to think about what you count as interpretable

259
00:22:24,680 --> 00:22:31,960
and for the very least you should be able to visualize what is salient and how you can

260
00:22:31,960 --> 00:22:34,720
do adversarial attacks towards different models.

261
00:22:34,720 --> 00:22:41,120
And I think the kind of open source tooling that AI2 does is really helpful for enabling

262
00:22:41,120 --> 00:22:48,440
individuals across like academia and industry to basically dig deeper and deliver on the

263
00:22:48,440 --> 00:22:51,120
premise of interpretability.

264
00:22:51,120 --> 00:22:57,320
Is there a quick way for you to summarize where the community ended up through this back

265
00:22:57,320 --> 00:23:01,960
and forth and the subsequent papers on this issue of the relationship between attention

266
00:23:01,960 --> 00:23:03,760
and explanation?

267
00:23:03,760 --> 00:23:10,480
Yes, so I would say that this is just my personal view.

268
00:23:10,480 --> 00:23:16,520
I told you that the fateful explanation that the authors of the original paper we're talking

269
00:23:16,520 --> 00:23:18,960
had this was this twofold thing.

270
00:23:18,960 --> 00:23:22,560
They were saying that there should be a correlation between inputs and outputs.

271
00:23:22,560 --> 00:23:27,800
I think that the way that they had done it wasn't rigorous enough in the eyes of the, you

272
00:23:27,800 --> 00:23:33,720
know, other paper, but the way that they had rebuttal actually to me again as someone

273
00:23:33,720 --> 00:23:35,400
reading their argument made sense.

274
00:23:35,400 --> 00:23:42,920
I think they did a good enough of a job with justifying why the correlation was in place.

275
00:23:42,920 --> 00:23:47,720
But I do agree with the authors, the Georgia Tech authors that yes, explanation is this

276
00:23:47,720 --> 00:23:49,760
very loosely defined term.

277
00:23:49,760 --> 00:23:56,360
It's not clear what the original authors meant by explanation and maybe that's title of

278
00:23:56,360 --> 00:24:00,000
attention is not explanation was too overloaded, right?

279
00:24:00,000 --> 00:24:03,400
They could have specified what they mean by explanation.

280
00:24:03,400 --> 00:24:10,200
I think, yes, so I would say that the community should stick to not calling attention,

281
00:24:10,200 --> 00:24:11,200
explanation.

282
00:24:11,200 --> 00:24:17,880
So it's one thing though that I actually agreed with the original authors paper.

283
00:24:17,880 --> 00:24:22,560
Georgia authors point was that they had said that although that they would seem that that

284
00:24:22,560 --> 00:24:28,400
title is overloaded, it's as saying that, you know, correlation is not causation.

285
00:24:28,400 --> 00:24:31,400
It doesn't mean that correlation can never be causation, right?

286
00:24:31,400 --> 00:24:36,040
There are, if you do your studies rigorously, et cetera, there are types of correlation

287
00:24:36,040 --> 00:24:38,960
which are in the causation.

288
00:24:38,960 --> 00:24:43,120
But you can still say that in a sense that, oh, look, be careful, don't count correlation

289
00:24:43,120 --> 00:24:44,120
as causation.

290
00:24:44,120 --> 00:24:51,440
So I think, yeah, so that's pretty much my overview of observing the back and forth.

291
00:24:51,440 --> 00:24:55,640
And then the next paper that you identified is more on the kind of fairness and bias

292
00:24:55,640 --> 00:24:58,640
and of the spectrum, which one was that?

293
00:24:58,640 --> 00:25:05,720
So that paper was titled, what's in a name, reducing bias and values without access to

294
00:25:05,720 --> 00:25:08,240
protected attributes.

295
00:25:08,240 --> 00:25:15,600
So that was a paper that came out in, for example, our knackle 2019 and actually won the

296
00:25:15,600 --> 00:25:25,080
best thematic paper award a knackle by Romanov at all, Umanas Loel, MSR, CMU, collaboration.

297
00:25:25,080 --> 00:25:31,520
So the reason I wanted to highlight this paper is, well, first of all, it happened to have

298
00:25:31,520 --> 00:25:35,720
been highlighted by the community before by getting the best paper award.

299
00:25:35,720 --> 00:25:41,480
But second of all, they had a pretty amazingly simple approach and yet strong results, which

300
00:25:41,480 --> 00:25:47,160
I think should be something that we do more and more so in our community.

301
00:25:47,160 --> 00:25:54,080
So basically, this paper highlights the fact that, look, we are at this day and age deploying

302
00:25:54,080 --> 00:25:59,200
lots and lots of AI systems that are automating decision-making in our daily lives.

303
00:25:59,200 --> 00:26:03,320
And some of these decision-making scenarios are high stakes.

304
00:26:03,320 --> 00:26:08,720
So for example, like we have applications of AI in criminal justice, we have it in overcruiting,

305
00:26:08,720 --> 00:26:09,720
et cetera.

306
00:26:09,720 --> 00:26:16,680
And having deploying bias models can basically yield very negative outcomes in people's daily

307
00:26:16,680 --> 00:26:17,680
lives.

308
00:26:17,680 --> 00:26:22,120
And we should be, like, as a community mindful, as like practitioners, again, as scientists,

309
00:26:22,120 --> 00:26:26,360
the term should be really mindful about the such implications of the models that you're

310
00:26:26,360 --> 00:26:27,360
building.

311
00:26:27,360 --> 00:26:34,120
So, you know, as you were saying earlier, there are, there have been, like, these representational

312
00:26:34,120 --> 00:26:40,440
biases, like, about award embedding and how, like, I don't know, if you do, like, the

313
00:26:40,440 --> 00:26:48,280
classical analogy for Wurtubek, like, X's to Y, like, as, like, Z's to what, if you do

314
00:26:48,280 --> 00:26:52,040
the analogy for, like, say, we are talking about recruiting, right?

315
00:26:52,040 --> 00:26:56,080
You want to know what kind of jobs go with what kind of people.

316
00:26:56,080 --> 00:27:01,680
So if you're an adjust man to computer programmer, it's, like, been cited a lot around that

317
00:27:01,680 --> 00:27:03,560
it would save women as to homemaker, right?

318
00:27:03,560 --> 00:27:08,760
And these are obviously very problematic when these kind of, these kinds of, like, representational

319
00:27:08,760 --> 00:27:15,680
biases can turn into, like, a really harmful, allocative biases in downstream tasks.

320
00:27:15,680 --> 00:27:21,800
So this paper is particularly trying to address how to mitigate these allocative harms that

321
00:27:21,800 --> 00:27:25,920
come out of these, you know, bias representations.

322
00:27:25,920 --> 00:27:31,120
Their tagline is pretty cool, actually, their tagline is five bias with bias, which is really

323
00:27:31,120 --> 00:27:32,120
awesome.

324
00:27:32,120 --> 00:27:38,920
Agline, they say that they basically want to leverage bias representations, like, word embeddings

325
00:27:38,920 --> 00:27:44,440
basically, to the bias of classifier, so very simple idea, strong results.

326
00:27:44,440 --> 00:27:45,440
So what did they do?

327
00:27:45,440 --> 00:27:51,800
They actually based their study on a prior data set on occupation classification.

328
00:27:51,800 --> 00:27:59,360
This is a data set of 400,000 or so public bias of biographies, short biographies of

329
00:27:59,360 --> 00:28:05,080
different individuals that are aligned with the 28 different possible occupations that

330
00:28:05,080 --> 00:28:06,080
they could have.

331
00:28:06,080 --> 00:28:10,400
So you read, like, little paragraph of, like, you know, expires, you did this and this

332
00:28:10,400 --> 00:28:15,880
and then there's a title of the occupation matched with.

333
00:28:15,880 --> 00:28:24,160
So prior work had shown that bias exists in this task, which is, so in the way that when

334
00:28:24,160 --> 00:28:31,000
you're trying to predict what is the occupation, it's supervised in terms of gender and race.

335
00:28:31,000 --> 00:28:38,160
So the way that they're sort of measuring this bias is by the classification accuracy

336
00:28:38,160 --> 00:28:39,560
gap that they are seeing.

337
00:28:39,560 --> 00:28:43,840
So this is also, like, this was a prior work that came before this work by, you know,

338
00:28:43,840 --> 00:28:49,720
the same sort of team of authories, very de-quantified this problem as the true positive

339
00:28:49,720 --> 00:28:54,640
rate that existed, the true positive rate difference that exists between genders for this

340
00:28:54,640 --> 00:28:56,760
particular downstream tasks.

341
00:28:56,760 --> 00:29:00,640
So they have this, I really enjoyed reading this paper, they have this very nice graph

342
00:29:00,640 --> 00:29:07,520
that they show that, for example, it's more accurate to predict the job of, I don't

343
00:29:07,520 --> 00:29:13,040
know, like being a model for a female, that it is to predict the job of being a doctor,

344
00:29:13,040 --> 00:29:15,040
physician for a female.

345
00:29:15,040 --> 00:29:21,680
And the fact is that because the bias in the actual true positive, the population already

346
00:29:21,680 --> 00:29:27,880
exists, it's sort of this compounding bias that happens at prediction time, which was also

347
00:29:27,880 --> 00:29:30,680
supported by earlier work.

348
00:29:30,680 --> 00:29:35,360
One interesting thing I want to mention is that you would think that if that maybe these

349
00:29:35,360 --> 00:29:40,320
models, so imagine you're just building your most vanilla classifier, right, imagine

350
00:29:40,320 --> 00:29:46,960
the biased model that I was saying, you just feed in the bios, you attend, et cetera,

351
00:29:46,960 --> 00:29:51,880
you make a prediction, 28 categories, like labels that you're generating.

352
00:29:51,880 --> 00:29:56,680
You would think that if you scrub the gender indicators from the bios, let's say like the,

353
00:29:56,680 --> 00:30:03,120
you know, the proper nouns, the, you know, gender products, et cetera, maybe you will be

354
00:30:03,120 --> 00:30:08,720
able to de-biased these models, meaning that you can shrink that true positive rate gap

355
00:30:08,720 --> 00:30:10,200
that I was mentioning.

356
00:30:10,200 --> 00:30:14,960
But they, this study in the prior work actually showed that scrubbing such explicit gender

357
00:30:14,960 --> 00:30:18,360
indicators does no good, so no difference at all.

358
00:30:18,360 --> 00:30:25,080
So the same accuracy, same TPR, like true positive rate gap, as with a model that uses

359
00:30:25,080 --> 00:30:31,600
the explicit gender indicators, which goes to showing that the bias is source from elsewhere,

360
00:30:31,600 --> 00:30:40,160
which is a very interesting kind of a realization that this work and the prior work have had.

361
00:30:40,160 --> 00:30:45,320
So in order to overcome this problem, they, they have this very simple yet super effective

362
00:30:45,320 --> 00:30:52,200
idea that they are saying that we want to use the embedding of names as the universal

363
00:30:52,200 --> 00:30:56,960
proxies for race, gender, and presumably age.

364
00:30:56,960 --> 00:31:03,440
So what they say is that look turns out in names of individuals, but just, you know, different

365
00:31:03,440 --> 00:31:07,960
kind of proper nouns, the names and family names, we are already encoding lots and lots

366
00:31:07,960 --> 00:31:08,960
of biases.

367
00:31:08,960 --> 00:31:15,040
So they even like show like they prove in the paper show sort of how the gender and race

368
00:31:15,040 --> 00:31:20,400
could be core highly correlated with these names that you cluster them.

369
00:31:20,400 --> 00:31:26,920
So they go ahead and define this very simple way of sort of debiasing the classifier that

370
00:31:26,920 --> 00:31:33,080
you build by discouraging the model to learn a correlation between the name embedding and

371
00:31:33,080 --> 00:31:35,040
the predict, predicted label.

372
00:31:35,040 --> 00:31:41,720
So get your any, you know, vanilla classifier, all you do is that you swap in your existing

373
00:31:41,720 --> 00:31:46,920
objective function with this new objective function that now penalizes, basically penalizes

374
00:31:46,920 --> 00:31:55,040
the model if there's a correlation between the name embedding and the predicted label.

375
00:31:55,040 --> 00:32:01,000
So they show very like really strong results that by doing so, they can really minimize

376
00:32:01,000 --> 00:32:06,880
the gap, the TPR gap that I was mentioning.

377
00:32:06,880 --> 00:32:11,400
So that was their conclusion that this is achievable, basically moving forward.

378
00:32:11,400 --> 00:32:18,000
If you are deploying map data models and industry in really high stake situation, this name

379
00:32:18,000 --> 00:32:22,760
embedding has happened to be a good proxy for debiasing the model.

380
00:32:22,760 --> 00:32:28,320
But they emphasize that the bias is not zero yet, so there is definitely further room

381
00:32:28,320 --> 00:32:35,640
for improvement of such high stake, this predictive models in future.

382
00:32:35,640 --> 00:32:43,640
So kind of when you think about the relationship between the explainability aspect of the first

383
00:32:43,640 --> 00:32:50,320
paper that you mentioned and the bias fairness, you know, do you, you know, there are other

384
00:32:50,320 --> 00:32:56,600
papers that are in different kind of points on this axis that are worth mentioning for

385
00:32:56,600 --> 00:32:59,240
folks that want to dig in deeper.

386
00:32:59,240 --> 00:33:06,240
No, nothing very particular in mind, I do think that again, these are kind of new developments

387
00:33:06,240 --> 00:33:08,400
in the NLP community.

388
00:33:08,400 --> 00:33:15,080
And I think, you know, there's this definitely strong like connection between building models

389
00:33:15,080 --> 00:33:21,000
that can explain themselves and hence us being able to diagnose their bias towards their

390
00:33:21,000 --> 00:33:22,000
predictions.

391
00:33:22,000 --> 00:33:25,720
Nothing else that I can think of right now, honestly.

392
00:33:25,720 --> 00:33:30,440
But yeah, there are actually conferences, outside of the NLP community, like the fat

393
00:33:30,440 --> 00:33:35,160
conference, et cetera, that have lots of amazing work coming out of them.

394
00:33:35,160 --> 00:33:40,400
In the, you know, it's the same area, maybe they take language as one of their tasks that

395
00:33:40,400 --> 00:33:44,120
every now and then they report results on.

396
00:33:44,120 --> 00:33:48,240
And I think that people should definitely check those conferences out.

397
00:33:48,240 --> 00:33:52,080
But it sounds like an area that you expect to see more of in the future, but I guess we'll

398
00:33:52,080 --> 00:33:55,920
get to predictions, let's not get ahead of ourselves.

399
00:33:55,920 --> 00:34:03,720
And before we do that, kind of the next batch of papers that you identified go back to kind

400
00:34:03,720 --> 00:34:12,160
of your initial take on 2019 and the role of these large pre-trained models, walk us

401
00:34:12,160 --> 00:34:15,000
through the papers that you had in mind there.

402
00:34:15,000 --> 00:34:16,000
Sure.

403
00:34:16,000 --> 00:34:24,320
So I think it's a needless to say that this year has been the year of transfer learning

404
00:34:24,320 --> 00:34:31,960
for NLP sort of continue, as I was saying, continuing on 2018, but more so maybe 2019, because

405
00:34:31,960 --> 00:34:36,000
we saw a real world impact through this work, basically.

406
00:34:36,000 --> 00:34:44,640
So I want to mainly highlight two main such models, one birth and one GPT-2, which I think

407
00:34:44,640 --> 00:34:51,480
people have heard enough of, but I don't think that we can really end the year without

408
00:34:51,480 --> 00:34:53,720
sort of mentioning them at least.

409
00:34:53,720 --> 00:35:00,240
So as like I'm sure probably your audiences have heard a lot, branches despite directional

410
00:35:00,240 --> 00:35:05,480
and encoder from transfer model by Google AI folks.

411
00:35:05,480 --> 00:35:11,680
It came out actually in 2018, so kind of sort of maybe not 2019 paper, but it actually

412
00:35:11,680 --> 00:35:17,000
got officially published in knackle 2019 and got the base paper worth there.

413
00:35:17,000 --> 00:35:21,160
So I think, you know, whatever we can count it 2019 paper.

414
00:35:21,160 --> 00:35:27,680
And this paper basically is just yet another large pre-trained language model, maybe the

415
00:35:27,680 --> 00:35:32,520
only main difference that is notable is the fact that their training objective was different.

416
00:35:32,520 --> 00:35:38,040
They had this training objective called MAST language model, but the main reason that

417
00:35:38,040 --> 00:35:42,960
this paper got as much attention as it did was the fact that right after it came out

418
00:35:42,960 --> 00:35:50,280
and throughout the ML community, it was achieving different states of the art for wide variety

419
00:35:50,280 --> 00:35:57,520
of an LP task, ranging from question answering to national language inference, etc.

420
00:35:57,520 --> 00:36:03,520
So I was just checking the end of day and to this day this paper has collected like 2,300

421
00:36:03,520 --> 00:36:10,040
citations and counting and it's definitely, I think, by any stretch of imagination the

422
00:36:10,040 --> 00:36:13,160
paper of the year in terms of the impact it has had.

423
00:36:13,160 --> 00:36:17,560
And I think I don't want to spend like much time talking about like all the other models

424
00:36:17,560 --> 00:36:22,200
that came out, like there's so many models that have, you know, been built sort of on top

425
00:36:22,200 --> 00:36:26,080
of birds and being inspired by birds, or birds, etc.

426
00:36:26,080 --> 00:36:30,840
But I think the main thing that I want to highlight is the fact that Bert got used a lot

427
00:36:30,840 --> 00:36:37,720
in industry, basically at Disney and I just, there's so many, like, I was personally surprised

428
00:36:37,720 --> 00:36:42,560
and you would see like these different startups that even their job posts, one of the requirements

429
00:36:42,560 --> 00:36:46,840
that they've list is like, oh, like you have to have work with Bert and I'm like, what?

430
00:36:46,840 --> 00:36:47,840
Like, what?

431
00:36:47,840 --> 00:36:53,680
I mean, living and it makes like Bert becomes like a requirement for like people to recruit.

432
00:36:53,680 --> 00:37:01,160
So this just goes to saying that there was this fear of missing out in the, you know, industry

433
00:37:01,160 --> 00:37:05,760
for people who were not actually improving whatever underlying and healthy pipelines they

434
00:37:05,760 --> 00:37:07,120
had through bird.

435
00:37:07,120 --> 00:37:13,000
And as I said, it was due to the fact that there were so many positive signals from the,

436
00:37:13,000 --> 00:37:19,440
you know, corresponding positive and progress in the other tests that everyone thought

437
00:37:19,440 --> 00:37:23,760
that, oh, whatever XYZ test they're working on should also be one of them.

438
00:37:23,760 --> 00:37:28,920
So anyways, that, that's one of the interesting, I would say, observations about the affected

439
00:37:28,920 --> 00:37:30,760
Bert had in the community.

440
00:37:30,760 --> 00:37:38,680
And the second is I think the main notable use case of even maybe NLP, but at least Bert

441
00:37:38,680 --> 00:37:44,840
in the industry was the fact that Google, itself, Google search itself, I reported that

442
00:37:44,840 --> 00:37:49,000
they have not incorporated Bert into their search engines.

443
00:37:49,000 --> 00:37:50,280
This is pretty grand, right?

444
00:37:50,280 --> 00:37:57,520
Like Google being one of the major tech companies search being the major of that company.

445
00:37:57,520 --> 00:38:04,360
I think this is just really, congratulations to the authors of the Bert paper who, you

446
00:38:04,360 --> 00:38:06,120
know, this is making real world impact.

447
00:38:06,120 --> 00:38:11,880
And I think that as research scientists, a lot of us basically dream of being able to

448
00:38:11,880 --> 00:38:16,440
make something in real world that actually serves a real problem.

449
00:38:16,440 --> 00:38:22,360
So Google actually was, you know, cited the exciting in their blog post that they, like,

450
00:38:22,360 --> 00:38:29,520
I don't know, like one out of 10 search queries are now improved by using Bert for both

451
00:38:29,520 --> 00:38:33,840
re-ranking of the hits that you retrieve and you make a query.

452
00:38:33,840 --> 00:38:38,800
And also generating those snippets that are, like, these little summaries of the pages

453
00:38:38,800 --> 00:38:43,720
basically that are retrieved, which is, you know, pretty amazing to hear, honestly, as

454
00:38:43,720 --> 00:38:50,480
just an LLP researcher. And I think that the main difference that they were citing was

455
00:38:50,480 --> 00:38:56,880
the fact that now, because this is going from keyword search, which is, like, you know,

456
00:38:56,880 --> 00:39:01,920
all the school, like, just search, like, information retrieval, et cetera, they are moving away

457
00:39:01,920 --> 00:39:05,240
from that on towards natural language understanding for search.

458
00:39:05,240 --> 00:39:11,040
They are capable of doing much more sophisticated query understanding, natural language understanding.

459
00:39:11,040 --> 00:39:16,160
So, like, for example, they highlighted the fact that they are now understanding prepositions

460
00:39:16,160 --> 00:39:21,600
much better than they used to. So, for example, queries such as, I don't know, like,

461
00:39:21,600 --> 00:39:29,600
2019 Brazil traveler to USA needs a visa. They were saying that before Bert, they didn't

462
00:39:29,600 --> 00:39:37,080
know that this means that, like, it should be a Brazilian traveling to US. But now with

463
00:39:37,080 --> 00:39:41,760
Bert, they know that, like, what that to preposition actually means, and hence retrieve

464
00:39:41,760 --> 00:39:48,360
better results, which is, you know, pretty, I would say, amazing outcome for the community

465
00:39:48,360 --> 00:39:49,840
to see such an impact.

466
00:39:49,840 --> 00:39:54,600
That's awesome. Yeah. I have definitely, well, I guess it goes without saying that I've

467
00:39:54,600 --> 00:40:03,760
seen it all over the place as well. But you also mentioned GPT-2. Did you want to chat

468
00:40:03,760 --> 00:40:08,480
about that as well? Absolutely. So, another thing that we cannot go

469
00:40:08,480 --> 00:40:15,520
without talking about is GPT-2. GPT-2 definitely in the way that the whole, you know, release

470
00:40:15,520 --> 00:40:21,240
of it was handled was one of the highlights of the year in AI, for sure, let alone NLP.

471
00:40:21,240 --> 00:40:28,840
So, just, you know, for whoever that might not be familiar with GPT-2 was this another

472
00:40:28,840 --> 00:40:35,680
larger scale pre-trained language model that basically came out of opening AI. The main

473
00:40:35,680 --> 00:40:40,920
feature of it being the fact that it was large enough so their largest model was 1.5,

474
00:40:40,920 --> 00:40:46,160
like, billion-providers. So, it was large enough and trained on good enough of it, you

475
00:40:46,160 --> 00:40:54,560
know, sort of high-quality curated web scale data set that they were able to showcase

476
00:40:54,560 --> 00:41:01,400
that they are generating really coherent outputs, paragraphs, and stories, you call it.

477
00:41:01,400 --> 00:41:06,360
They also showed that through this particular very large language model that they've built,

478
00:41:06,360 --> 00:41:12,400
they are able to do zero-shot generalization to other downstream tasks. So, not fine-tuning,

479
00:41:12,400 --> 00:41:18,480
meaning you don't have it even a small scale particular training corpus to fine-tune the

480
00:41:18,480 --> 00:41:24,240
model, just literally zero-shot right out of the box. You show that you can do, you know,

481
00:41:24,240 --> 00:41:29,960
some level of, you know, you can compete with a state-of-the-art in some, you know, much less

482
00:41:29,960 --> 00:41:33,920
than a state-of-the-art, but still makes some performance in machine translation, question

483
00:41:33,920 --> 00:41:39,160
answer, and reading comprehension, summarization, et cetera. So, this was the work, but I would

484
00:41:39,160 --> 00:41:46,840
say that the attention to this work, God, was not due to the performance necessarily,

485
00:41:46,840 --> 00:41:52,680
but due to the way that it was released. So, what happened, and I'm sure, you know,

486
00:41:52,680 --> 00:41:56,760
I know you guys have already covered this, so I'll just mention this quickly, that this

487
00:41:56,760 --> 00:42:02,480
state-release process that they had in mind, where they basically cited that given how

488
00:42:02,480 --> 00:42:08,760
amazing this work, this model is working, it's too basically dangerous for it to be released

489
00:42:08,760 --> 00:42:14,000
to the public community for the potential of misuse. So, they held back, and they did

490
00:42:14,000 --> 00:42:20,000
this stage-release process, they released the smallest model in February, 2019, then

491
00:42:20,000 --> 00:42:28,120
in November, they finally reached a full 1.5 million-parameter model, but that whole process

492
00:42:28,120 --> 00:42:33,440
sort of created this GPT-2 saga, of course, there were so many people that were kind of

493
00:42:33,440 --> 00:42:39,800
outraged by the fact that, oh my goodness, this is open, and open AI, how could you not

494
00:42:39,800 --> 00:42:45,520
release something that you have created, and then there were some proponents saying that

495
00:42:45,520 --> 00:42:50,120
I don't know, and this is a good example of the community sort of thinking about the implications

496
00:42:50,120 --> 00:42:55,480
of their work, et cetera. So, sitting data side, and I don't think it's the, as I said,

497
00:42:55,480 --> 00:43:00,800
I know you guys have already debated this issue, but I think it was an interesting moment

498
00:43:00,800 --> 00:43:07,280
for the NLP and AI research in general to have gone through this. But I wanted to mainly

499
00:43:07,280 --> 00:43:12,320
talk about the text generation aspects. So, sitting beside the PR, and like the rights

500
00:43:12,320 --> 00:43:18,360
or wrongs that the opening AI folks did, forward a way that they released this model. For

501
00:43:18,360 --> 00:43:22,520
me, as a researcher myself, like having worked in text generation and still working on

502
00:43:22,520 --> 00:43:29,640
it, I was really excited to get my hands on the largest spas model that they had given

503
00:43:29,640 --> 00:43:37,000
them examples that they had in their paper, because the problem of doing cohere in national

504
00:43:37,000 --> 00:43:42,800
language generation has been one of the longest-running problems in NLP community, for sure.

505
00:43:42,800 --> 00:43:49,360
And I think anyone would be excited to know how far we've gone and tackling that problem.

506
00:43:49,360 --> 00:43:56,440
So just looking back in time, again, like 2017 or so before any of these models were out,

507
00:43:56,440 --> 00:44:03,200
I myself, like sort of characterized very viarbit language generation, saying that,

508
00:44:03,200 --> 00:44:11,160
look, we have these at the time, this Verde RNN, LM, so Recurin Neural Network-based language

509
00:44:11,160 --> 00:44:15,880
model is not transformed based language models. I would characterize them as being locally

510
00:44:15,880 --> 00:44:21,200
coherent, meaning that they are very much capable of generating grammatical sentences,

511
00:44:21,200 --> 00:44:27,480
but then generating logically sound paragraphs are still so longer than a sentence, and like

512
00:44:27,480 --> 00:44:32,480
something like a story or narratives, which happens to be my area of research. They were

513
00:44:32,480 --> 00:44:36,920
still super lacking. So just looking at the examples that they had in their paper made

514
00:44:36,920 --> 00:44:42,440
me really excited to just try it out, right? So I was talking about story closed test,

515
00:44:42,440 --> 00:44:49,160
very given like four sentences, a particular, very simplistic story generated basically

516
00:44:49,160 --> 00:44:55,160
the ending, right? So I basically, after even the initial releases, the smaller models

517
00:44:55,160 --> 00:45:00,720
put in them, but the latest and largest model, I personally tried out this, did you

518
00:45:00,720 --> 00:45:07,720
see the GPT2 model for various such story closed test instances? So one, for example, that

519
00:45:07,720 --> 00:45:12,920
I would want to highlight was just this very simple story that I'm going to build up on

520
00:45:12,920 --> 00:45:18,560
top of. So the story is Lili was writing her scooter, a bike turn in front of her, she

521
00:45:18,560 --> 00:45:28,160
tried to break abruptly, and she fell on the ground. And so that way that GPT2 continues,

522
00:45:28,160 --> 00:45:35,400
as the story is, Lili fell into the lake. I dragged her out. She said that she could not

523
00:45:35,400 --> 00:45:40,840
go down. I was desperately searching for another slant, blah, blah, blah, and you know, it

524
00:45:40,840 --> 00:45:48,360
goes forever. So it goes to saying that the one of the major problems that I would characterize

525
00:45:48,360 --> 00:45:54,560
about, like, neural language models for a national engagement generation back in 2017 was

526
00:45:54,560 --> 00:46:02,360
that, here, as him is very good in hypothesizing why a model's generation is actually logically

527
00:46:02,360 --> 00:46:07,400
sound, because if you have this way of projecting our own sense of meaning, right, from even

528
00:46:07,400 --> 00:46:12,320
the most mindless generation. And I think what's happening here is just very much still in

529
00:46:12,320 --> 00:46:17,280
line with that as well, that sure, you can hypothesize that probably Lili then fell into the

530
00:46:17,280 --> 00:46:22,240
lake and then I dragged her out and she says she could not go down, et cetera. But it's

531
00:46:22,240 --> 00:46:26,360
clear, as you know, the farther you go, and this was just one example, as I said, like,

532
00:46:26,360 --> 00:46:31,200
I tried to sum so many more, the more you play with the model, the more you see that that

533
00:46:31,200 --> 00:46:37,160
logically sound generation globally at, like, paragraph, little story, little, et cetera,

534
00:46:37,160 --> 00:46:42,000
is still something that, as a community, we are lacking. And I think that there's a wide

535
00:46:42,000 --> 00:46:46,400
range of, you know, generation tasks that you can work on and care about. And I guess

536
00:46:46,400 --> 00:46:52,520
I would characterize this still to this day, I would say that generating logically sound

537
00:46:52,520 --> 00:46:58,560
stories, narratives that make sense, a show common sense is, you know, one of the major

538
00:46:58,560 --> 00:47:06,000
bottlenecks, I would say, of building an LP model that can work well, basically, effectively.

539
00:47:06,000 --> 00:47:11,400
How would you characterize the differences between the smaller model and the full model that

540
00:47:11,400 --> 00:47:17,400
was released later in the year in terms of storage generation? That's a very good question.

541
00:47:17,400 --> 00:47:23,240
I, I never did a quantitative analysis, right? And that goes to one of the main other problems

542
00:47:23,240 --> 00:47:27,760
we have in the area that I actually wanted to highlight, which is evaluation. These still

543
00:47:27,760 --> 00:47:33,960
don't, like, as a community, we don't have a, a good way of evaluating generation. So

544
00:47:33,960 --> 00:47:39,720
it's like we don't have a, they have automatically evaluating whether or not a system is generating

545
00:47:39,720 --> 00:47:45,080
something sound, sort of meaning AI, judging AI, that's a major problem. So because of

546
00:47:45,080 --> 00:47:52,520
that, it's been really hard to know, like, to sort of quantitatively measure progress.

547
00:47:52,520 --> 00:47:57,680
So that's just a separate problem we have, which I'm hoping some, it should be something

548
00:47:57,680 --> 00:48:02,280
actually that, as a community, we work harder on. And as you can imagine, right, the reason

549
00:48:02,280 --> 00:48:10,600
we do industry courses as a multi-choice test set was so that it's evaluable, quickly,

550
00:48:10,600 --> 00:48:16,320
systematically, easily, right? But it comes with a caveat of being basically gameable and

551
00:48:16,320 --> 00:48:21,880
all the kinds of, you know, biases that we find about the data sets. So generation is

552
00:48:21,880 --> 00:48:25,920
ideal, but then the flip side is we don't know how to evaluate generation. So setting

553
00:48:25,920 --> 00:48:31,000
that aside qualitatively, and, you know, their proxies, we can use blue, et cetera, but

554
00:48:31,000 --> 00:48:36,640
the fact that they don't correlate with human judgment is an issue that we just yet have

555
00:48:36,640 --> 00:48:41,480
an address. But yeah, quantitatively looking at the results, there wasn't honestly that

556
00:48:41,480 --> 00:48:48,400
much difference between the largest GPT-2 models that I've played with and the smallest,

557
00:48:48,400 --> 00:48:54,560
but definitely a summary that people have to do more systematic evaluation.

558
00:48:54,560 --> 00:48:59,800
Yeah, so one more thing I wanted to mention here, hopefully real quick, is about the fact

559
00:48:59,800 --> 00:49:04,640
that so in the community after GPT-2 came out, there have been lots of back and forth in

560
00:49:04,640 --> 00:49:08,680
different use cases that people have found. And, you know, there was even this, the interview

561
00:49:08,680 --> 00:49:13,800
that was done with GPT-2, so many different angles that this whole line of research basically

562
00:49:13,800 --> 00:49:19,960
has taken in the public eye, media coverage, et cetera. One thing that people, if you

563
00:49:19,960 --> 00:49:25,200
people at least have rightfully pointed out is the fact that are we actually making real

564
00:49:25,200 --> 00:49:29,640
progress towards national language understanding and national language generation through such

565
00:49:29,640 --> 00:49:34,960
pieces of work? Are these models capable of building so-called mental models of the

566
00:49:34,960 --> 00:49:39,960
world, right? So this is something that I'm personally extremely passionate about. And

567
00:49:39,960 --> 00:49:44,320
like I'm hope, you know, at Elemental Commission, one of the pieces of work that we are hoping

568
00:49:44,320 --> 00:49:50,240
to come out next year is exactly on this. So for the lily story that I was mentioning,

569
00:49:50,240 --> 00:49:56,400
for example, like for any even like a child, human child reading that story, they would

570
00:49:56,400 --> 00:50:02,640
know the causal chain of events that happened. They would know the emotional, you know, turbulence

571
00:50:02,640 --> 00:50:07,080
that the character went through like, oh, she was like writing a scooter and then the

572
00:50:07,080 --> 00:50:11,720
bike turn, oh, how was she feeling then? When she fell on the ground, oh, did she skin

573
00:50:11,720 --> 00:50:17,720
her knee? How did she feel after being injured, et cetera? And we can basically build this

574
00:50:17,720 --> 00:50:24,320
pretty consistent models of the world, mental models of the world, even as children read

575
00:50:24,320 --> 00:50:30,560
a very short story. So I think that we are very far away from building an AI system that

576
00:50:30,560 --> 00:50:38,320
can showcase such common implicit common sense, understanding of the world, even as a five-year-old

577
00:50:38,320 --> 00:50:43,440
child would do. And I think there's enough evidence that the likes of GPT-2 are not

578
00:50:43,440 --> 00:50:47,400
doing that, given the mistakes that they're making. And I think as a community, we should

579
00:50:47,400 --> 00:50:50,720
focus on tackling such problems moving forward.

580
00:50:50,720 --> 00:50:57,480
Yeah, I mean, this is probably a good time to note that the interview that I did with

581
00:50:57,480 --> 00:51:03,200
David Faruji from Elemental Cognition, the title of that one was, are we being honest

582
00:51:03,200 --> 00:51:09,180
about how difficult AI really is? Actually, turn out to be our number one, you know, most

583
00:51:09,180 --> 00:51:19,120
popular show of 2019. Oh, wow. That's awesome. Yeah, yeah. But that was one of several that,

584
00:51:19,120 --> 00:51:24,600
you know, spoke to kind of, you know, maybe a sobering perspective on the way we think

585
00:51:24,600 --> 00:51:29,280
about AI and building models and what they're really capable of, what they, you know,

586
00:51:29,280 --> 00:51:33,840
what we should be expecting out of them. And you're kind of echoing that same sentiment.

587
00:51:33,840 --> 00:51:38,920
Exactly. Yes. Yes. So what's the next paper on your list?

588
00:51:38,920 --> 00:51:45,480
So now that we kind of covered our bases with the main two pioneers or whatever we can

589
00:51:45,480 --> 00:51:54,400
call them BERT and GPT-2s of the pre-train paradigm that we were living in 2019, I think

590
00:51:54,400 --> 00:52:03,040
it's good to highlight one of the major advances that we could make through the likes of BERT,

591
00:52:03,040 --> 00:52:09,840
et cetera, on a downstream task that was held as one of the feats of the year. So this

592
00:52:09,840 --> 00:52:17,800
is a work that came out of AI-2 as well. It's called from F to A on the, on the New York

593
00:52:17,800 --> 00:52:24,840
region science exam and overview of the RISTO project. So this is basically an accomplishment

594
00:52:24,840 --> 00:52:30,560
that AI-2 had, which fills on top of the work that they've been doing for the past like

595
00:52:30,560 --> 00:52:37,240
four or five years, at least, on tackling science exams. So, you know, what Lake Paul

596
00:52:37,240 --> 00:52:44,240
Allen had this dream of doing, like building a digital RISTODL, and actually four years

597
00:52:44,240 --> 00:52:50,920
ago or so, they made a challenge for the research community to come up with an AI system that

598
00:52:50,920 --> 00:52:58,120
can be 10-8 greater in this standardized science test. So back then, to belief was that,

599
00:52:58,120 --> 00:53:04,440
you know, we've, okay, we've built like IBM Watson, which is good at jeopardy. Can

600
00:53:04,440 --> 00:53:08,600
we build a system that doesn't do jeopardy, but it's somewhat simpler, it just beats

601
00:53:08,600 --> 00:53:14,800
an 8th grader. So, as I said, that was like one of the, like, one of Paul Allen's dreams,

602
00:53:14,800 --> 00:53:20,520
but back in time, when they did this as a Kaggle contest on the best system that was submitted

603
00:53:20,520 --> 00:53:26,800
got around like 60-something percent, which was far, far away from the human performance,

604
00:53:26,800 --> 00:53:32,760
of course, or like a 8th grader performance to pass the test. So, fast forward, one of

605
00:53:32,760 --> 00:53:39,880
the main advances, I would say, in 2019, that was made was that they, through using births,

606
00:53:39,880 --> 00:53:46,760
so, both births and perverda, language models, they could boost their performance from

607
00:53:46,760 --> 00:53:55,960
63 or something, I think, percent that they had achieved in 2016 to now 90.7% in 2019,

608
00:53:55,960 --> 00:54:04,720
which was a passing score. So, this was pretty much of a feat in the field. For, you know,

609
00:54:04,720 --> 00:54:10,080
various reasons, this, you know, like the choice of science exams is something that we

610
00:54:10,080 --> 00:54:15,760
can debate, whether or not it's a good benchmark, but at least from the surface level, it seems

611
00:54:15,760 --> 00:54:22,160
like, you know, science, such science questions require national linkage understanding, having

612
00:54:22,160 --> 00:54:26,560
common sense knowledge, pretty broad common sense knowledge, knowing how the world works,

613
00:54:26,560 --> 00:54:31,520
et cetera, and then reasoning capabilities, right? And also from like a more practical

614
00:54:31,520 --> 00:54:36,960
standpoint, exams are accessible, measurable, right, the multi-choice exam, of course,

615
00:54:36,960 --> 00:54:44,160
is something that you can quickly evaluate. So, it seems like a pretty compelling, these,

616
00:54:44,160 --> 00:54:49,200
you know, the following seem like a pretty compelling reason to, to one account that is a good metric.

617
00:54:49,200 --> 00:54:56,640
But, of course, as many even teachers would argue, standards, tastes like multiple choice tests

618
00:54:56,640 --> 00:55:01,920
are not the best measure of intelligence. They are gameable, really like even children who

619
00:55:01,920 --> 00:55:06,800
get good tests scores are not necessarily the most intelligent and learn the best in their classes.

620
00:55:07,520 --> 00:55:15,440
So, there are those aspects, and I actually go at that AI2 folks have been pretty good with not

621
00:55:15,440 --> 00:55:22,800
letting this get hyped up, right, out of their scope of what they would characterize, beyond their

622
00:55:22,800 --> 00:55:28,560
scope of what they would characterize as their real outcome of this work. So, they even themselves

623
00:55:28,560 --> 00:55:35,040
did some adversarial testing of the model. They showcase that if you add various other

624
00:55:35,920 --> 00:55:41,120
multiple, like, what are some of their choices to this multiple choice instances that are like

625
00:55:41,120 --> 00:55:47,680
likely to sort of be the answer, it's just this challenging answer. The model's performance

626
00:55:47,680 --> 00:55:55,120
drops from 90%, 90 plus percent that it had thought into 60%. So, they have really even themselves

627
00:55:55,120 --> 00:56:00,320
highlighted the fact that, look, this is, this is it. It's a narrow particular test set,

628
00:56:00,320 --> 00:56:05,120
standard test set that this model is working well on. It doesn't mean true intelligence. Please

629
00:56:05,120 --> 00:56:12,880
don't title this as, now we have an AI system that can be ties, coolers, etc. So, with that caveat,

630
00:56:12,880 --> 00:56:18,000
aside from one more thing, by the way, they also mentioned that the real eighth graders

631
00:56:18,720 --> 00:56:25,360
also answered the questions that include diagrams and they don't. So, that's another, you know,

632
00:56:25,360 --> 00:56:31,040
point to take into account. But yeah, even all this, I think still, this is pretty amazing that

633
00:56:31,040 --> 00:56:36,560
why basically introducing these large language models that do implicitly include lots of

634
00:56:36,560 --> 00:56:42,880
word knowledge, lots of contextualized knowledge, lots of grammatical even knowledge,

635
00:56:42,880 --> 00:56:49,120
you are able to boost your performance on such a test. Nice, nice. What's the next paper on your list?

636
00:56:49,760 --> 00:56:57,280
So, the next paper is called right forward and wrong reasons. So, first of all, I love the

637
00:56:57,280 --> 00:57:04,720
title. I think it surpasses so many things that it has gone wrong and could go wrong

638
00:57:05,280 --> 00:57:12,400
in our, you know, without benchmarking and and open community. So, this this paper is sort of

639
00:57:12,400 --> 00:57:18,880
for me an example from a host of different papers that have come out and are trying to show us

640
00:57:18,880 --> 00:57:26,080
the blind spots of these models or all various ways that they are making the supposedly rights

641
00:57:26,080 --> 00:57:33,360
predictions, but all for the wrong reasons. This actually, this kind of paradigm, not paradigm,

642
00:57:33,360 --> 00:57:38,960
maybe a realization, I'd call it. And then on the community started a few years back and actually

643
00:57:38,960 --> 00:57:44,000
to my knowledge, at least one of the first ones was on the very story close test test that we

644
00:57:44,000 --> 00:57:50,880
ourselves did, very then we made it into a challenge. The top performing model actually had this

645
00:57:50,880 --> 00:57:58,560
observation that turns out there in these biases in the way that the endings in the story

646
00:57:58,560 --> 00:58:04,320
cost us are authored by our crowdsource workers. So, things like the fact that, oh, it turns out

647
00:58:04,320 --> 00:58:09,840
that the wrong ending is, you know, often has like negative adjectives or it's like a longer

648
00:58:09,840 --> 00:58:18,080
shorter, etc. So, these like synthetic sort of biases that are in the in our test sets which

649
00:58:18,080 --> 00:58:23,840
we don't even realize. Now, remember us talking about how difficult it was to construct these

650
00:58:23,840 --> 00:58:29,520
test examples without kind of various types of tells in them that, you know, would tip off

651
00:58:29,520 --> 00:58:37,200
the model. Exactly. Exactly. It's very hard and I think we talked back then that you're even lucky

652
00:58:37,200 --> 00:58:42,240
as, you know, just in the research community, we're lucky even to be catch these, right? God knows

653
00:58:42,240 --> 00:58:49,440
which other benchmarks that we are using on a day to day basis have other implicit on like hidden

654
00:58:49,440 --> 00:58:55,520
biases that we are not even aware of or is so hard to uncover. So, I think this is just, you know,

655
00:58:55,520 --> 00:59:02,960
you thought other papers is 2019 paper, those outcomes like the story close test that was like 2017

656
00:59:02,960 --> 00:59:07,280
actually. So, this is like two, three years after and still we are dealing with this problem.

657
00:59:07,280 --> 00:59:14,640
So, this particular paper that was co-authored by folks from Johns Hopkins and Brown appeared in

658
00:59:14,640 --> 00:59:22,880
ACL 2019, which to me is like just highlighting the growing movement in an LP community to move

659
00:59:22,880 --> 00:59:30,400
beyond interpreting the test sets, you know, leaderboards as just pure achievements. But,

660
00:59:30,400 --> 00:59:36,960
care more about analyzing what's actually the thing that these models are learning and hard to

661
00:59:36,960 --> 00:59:44,400
perform in book. So, they actually, what this particular paper observes is that for the particular

662
00:59:44,400 --> 00:59:51,760
task of MNLI, which is this multi-genre and natural language inference data set, they show that

663
00:59:51,760 --> 00:59:59,760
there are superficial syntactic properties such as, like, whether or not the words in the sentence

664
00:59:59,760 --> 01:00:05,120
that is going to be the, you know, on the prediction set overlaps with the one in the input.

665
01:00:05,120 --> 01:00:10,480
So, like, pretty superficial, you kind of like go and scratch your head like, oh my god,

666
01:00:10,480 --> 01:00:14,960
how come we are still doing this and we're having these problems after like three years of people

667
01:00:14,960 --> 01:00:21,280
talking about this. But this is, you know, their reality. We've been, you know, sort of evaluating

668
01:00:21,280 --> 01:00:26,080
our models on the benchmarks, which still have these hidden problems. And as I said, because

669
01:00:26,080 --> 01:00:30,560
it's really hard, as you were discussing, it's really hard to uncover social biases.

670
01:00:30,560 --> 01:00:36,640
So, what they did is that, and I think I can actually mention just once more, natural language

671
01:00:36,640 --> 01:00:44,320
inference is the task where given a particular input, sentence, the system is, and another sentence,

672
01:00:44,320 --> 01:00:49,600
you are supposed to classify whether or not the second sentence is an entailment or a contradiction,

673
01:00:49,600 --> 01:00:55,680
or in some of these benchmarks neutral, meaning that it doesn't necessarily contradict or entail.

674
01:00:55,680 --> 01:01:01,120
So, anyways, they did this analysis. They made this data set, sort of an adversarial data set,

675
01:01:01,120 --> 01:01:08,480
Kant's called Kant's data set, where they actually curate these particular test instances,

676
01:01:09,200 --> 01:01:15,360
which sort of uncover whether or not a particular model is using substantive heuristics.

677
01:01:15,360 --> 01:01:20,880
For example, they have this heuristic, called lexical overlap, just pure lexical overlap.

678
01:01:20,880 --> 01:01:27,760
The definition is that assume that a premise, which is the input sentence on the left hand side,

679
01:01:27,760 --> 01:01:32,800
entails all the hypotheses constructed from the words in the premise.

680
01:01:32,800 --> 01:01:38,400
So, if you, it can also, for example, the use is that, for example, if the premise is the doctor

681
01:01:38,400 --> 01:01:45,120
was paid by the actor, you can hypothesize that the doctor paid the actor just because it has the

682
01:01:45,120 --> 01:01:51,920
fully word lexical word overlap is going to be entailed by that sentence, but it is wrong, right?

683
01:01:53,040 --> 01:01:58,960
So, the model that basically shouldn't say that that is an entailment, but if a model is biased

684
01:01:58,960 --> 01:02:05,440
towards using such lexical overlap heuristics, it will wrongly classify that as correct and entailment.

685
01:02:06,240 --> 01:02:11,680
So, what they do is that they show, show that actually, on this Kant's data set that they create,

686
01:02:11,680 --> 01:02:18,240
that is actually true, that a lot of like a majority actually have the state-of-the-art models

687
01:02:18,240 --> 01:02:25,440
on M and L.I. Data set, they're doing this very thing, that they were actually very inaccurate in

688
01:02:25,440 --> 01:02:34,320
classifying such instances where the heuristic flip basically. So, they actually show that,

689
01:02:34,320 --> 01:02:40,240
although that's the case, they show that if they augment these models and retrain them using

690
01:02:40,240 --> 01:02:47,280
the Kant's data set, they can improve their performances. But the reason, as I said, the main reason

691
01:02:47,280 --> 01:02:52,560
I wanted to highlight this paper is that still, 2019, we are dealing with the same problem

692
01:02:52,560 --> 01:02:59,440
that we were dealing in 2017 of having models that are biased towards the intricacies of the

693
01:02:59,440 --> 01:03:03,680
test sets and train sets that they're getting trained on, and that's something that we have to

694
01:03:03,680 --> 01:03:10,400
keep working on moving forward. Yeah, I suspect that, you know, different versions of these problems

695
01:03:10,400 --> 01:03:16,320
will keep us busy for quite some time, which actually leads us quite nicely into your predictions

696
01:03:16,320 --> 01:03:25,120
for the field. Yes, absolutely. So, I think that, as I was just, you know, the way that we started

697
01:03:25,120 --> 01:03:30,480
this whole conversation, I think they've come really along, Bay, in the past couple of years,

698
01:03:30,480 --> 01:03:36,720
if not like the past decade, and tackling lots of low-hanging fruits in NLP using these

699
01:03:36,720 --> 01:03:43,600
really amazing tools that we've built. But, you know, I think the papers that I had selected

700
01:03:43,600 --> 01:03:49,680
kind of nicely highlight the problems we have as well, like the limitations and the kind of

701
01:03:50,960 --> 01:03:57,520
weaknesses that these models tend to keep showing. And I think 2020 should be the year that we

702
01:03:57,520 --> 01:04:04,080
start to get ambitious again, and think about how much, you know, harder kinds of problems

703
01:04:04,960 --> 01:04:11,520
we can tackle moving forward, now that we have covered the basis sort of. So, actually this year,

704
01:04:12,160 --> 01:04:18,240
2020, for the first time in the history of ACL conference, so ACL being a, you know, major

705
01:04:19,680 --> 01:04:25,200
computational linguistic community conference, we have a special theme that asks the community

706
01:04:25,200 --> 01:04:32,320
to write papers to reflect back on the progress of the field and what V as a community should be

707
01:04:32,320 --> 01:04:38,160
focusing on moving forward. And I think that's pretty refreshing because it indicates that there

708
01:04:38,160 --> 01:04:44,400
is this consensus that, look, from the outside, it feels like there's so many benchmarks that keep

709
01:04:44,400 --> 01:04:51,040
getting beaten every month or so through these new other tools that come out bigger, better.

710
01:04:51,040 --> 01:04:57,200
But where are we going with this? Are we actually defining truly what natural language

711
01:04:57,760 --> 01:05:05,120
understanding means? Are we truly working on systems that show common sense, you know,

712
01:05:05,120 --> 01:05:14,480
inferences of even a child? Are we actually building systems that can transfer the knowledge that

713
01:05:14,480 --> 01:05:19,520
they have, what they learn from a test to another without really needing to get, you know,

714
01:05:19,520 --> 01:05:26,480
retrained, et cetera? So, I think I would love for that to be how the, you know,

715
01:05:26,480 --> 01:05:33,920
shift their focus in 2020. I think that we should focus on the things that we cannot do yet,

716
01:05:33,920 --> 01:05:40,160
or have not basically have had the chance of doing because of having focused on the simpler

717
01:05:40,160 --> 01:05:46,240
problems. I think one major issue we have with all these new things that we've built is that

718
01:05:46,240 --> 01:05:52,560
still to this day in the industry, there are a lot of systems that use like old-school,

719
01:05:53,280 --> 01:05:57,840
you know, rule-based models, pattern recognition, and the sense of just doing

720
01:05:57,840 --> 01:06:04,640
reggae smashing, et cetera, because, you know, they know how they work, they know how to turn it off

721
01:06:04,640 --> 01:06:11,120
and think the system, whatever makes a stupid mistake. But these, you know, very accurate actually

722
01:06:11,120 --> 01:06:16,400
neural models, they often make stupid, stupid mistakes that we don't even know why, right? And

723
01:06:16,400 --> 01:06:22,160
that's, I think, something that needs to be looked into, how can we build, sort of,

724
01:06:22,160 --> 01:06:27,600
better controls over these highly accurate models to know where they could go wrong,

725
01:06:27,600 --> 01:06:35,120
can we get guarantees, et cetera? And I think the more we move into areas at high stakes,

726
01:06:35,120 --> 01:06:41,760
the more the need to do so. Do you think those controls look more like changes to the way

727
01:06:41,760 --> 01:06:47,920
these models are trained or evaluated or lost functions or things like that, or more like hybrid

728
01:06:47,920 --> 01:06:55,440
types of systems that incorporate elements of rules and elements of more modern NLP?

729
01:06:56,160 --> 01:07:04,160
I think it could be either, right? I think what was very refreshing about the way that deep learning,

730
01:07:04,160 --> 01:07:10,000
a revolutionized NLP in the past, a couple of years, is the fact that despite the mainstream,

731
01:07:10,000 --> 01:07:14,560
there were many, you know, like, even if a smaller community, but there were folks who were still

732
01:07:14,560 --> 01:07:19,920
doing research in the area and thinking beyond what the mainstream is dictating. And I think,

733
01:07:19,920 --> 01:07:25,600
in order to make tremendous progress moving forward, we do need people who think differently.

734
01:07:25,600 --> 01:07:30,480
We do need people who think they know, like, there's no way that deep learning is going to be the

735
01:07:30,480 --> 01:07:35,280
silver bullet we have to think about a hybrid system. Or people who believe that, no, there's no way

736
01:07:35,280 --> 01:07:41,120
that we can have, like, symbolic models incorporated into these neural models, and we have to just

737
01:07:41,120 --> 01:07:47,120
fix the way that we do training in order to exhibit better generalization, better transfer of knowledge,

738
01:07:47,120 --> 01:07:52,160
et cetera. So I think there's no way for me or anyone, honestly, to say which one is necessarily

739
01:07:52,160 --> 01:07:59,120
going to thrive. I think the more people we have in the community caring about the right problems,

740
01:07:59,120 --> 01:08:04,400
as opposed to the right approaches, that the higher chances of tackling these major remaining

741
01:08:04,400 --> 01:08:10,640
problems in the area. What else do you foresee? So there are a couple of other things. I think that

742
01:08:11,440 --> 01:08:19,120
the will start having more rigorous evaluations in place. I think that we would better know the

743
01:08:19,120 --> 01:08:25,040
implications of establishing state-of-the-art on various benchmarks. As I was saying, there are

744
01:08:25,040 --> 01:08:31,680
even environmental implications of all the sort of fact-planting that we do at this day and age.

745
01:08:31,680 --> 01:08:37,600
And I think more people should think about those aspects of their work. Actually, there was

746
01:08:37,600 --> 01:08:45,520
a work, another work called gray eye by UW people that they were encouraging the community to

747
01:08:46,160 --> 01:08:52,800
also report the efficiency of their resource usage, along with the other classical metrics,

748
01:08:52,800 --> 01:08:57,280
such as accuracy, et cetera. And they are reporting numbers. And I think those are really

749
01:08:57,280 --> 01:09:03,760
interesting directions that the community could take. And potentially, we may no longer count

750
01:09:03,760 --> 01:09:08,480
the best work of the year, the largest work of the year. Maybe we know that, oh, look,

751
01:09:08,480 --> 01:09:15,040
this just had this really negative implication environmentally and whatever it wasn't fair. So we

752
01:09:15,040 --> 01:09:23,920
can think beyond that, basically. And another thing is such a no-brainer. I think we are going

753
01:09:23,920 --> 01:09:29,440
to start to work more and more on explainable models and interpretable models. So it's very

754
01:09:29,440 --> 01:09:36,880
commonly the reason people care about explanation and interpretability is for the accountability issue,

755
01:09:36,880 --> 01:09:42,720
for fairness issue, et cetera, which is really major. But the reason I personally am a big advocate

756
01:09:42,720 --> 01:09:49,280
and I've been interested in working on this sort of past couple of years on V2. So even more broadly

757
01:09:49,280 --> 01:09:54,720
at elemental cognition is the fact that explanation is this inherent capability of human beings,

758
01:09:54,720 --> 01:10:01,280
right? Even a little child can explain the kinds of reasoning that they do. Of course,

759
01:10:01,280 --> 01:10:08,240
we can argue, again, what is explanation. But I think building models that can be held accountable

760
01:10:08,240 --> 01:10:13,600
towards the predictions they make and have ways of explaining it to an average human, which I would

761
01:10:13,600 --> 01:10:17,680
argue should be through national language, is going to be something that we will see more and more

762
01:10:17,680 --> 01:10:24,560
in 2020. And the probably, hopefully, last thing that I would mention is, of course, we have

763
01:10:24,560 --> 01:10:31,200
to build causal models of the world that I was mentioning. We need to build systems that show

764
01:10:31,200 --> 01:10:37,360
common sense, build systems that are able to basically build this causal map of the world, how

765
01:10:37,360 --> 01:10:44,240
the events basically follow each other, how do we know this happens versus the other thing doesn't

766
01:10:44,240 --> 01:10:50,320
happen. And what are the implications in terms of the emotions of characters who vary what,

767
01:10:50,320 --> 01:10:55,600
et cetera. So I think these are really kinds of directions that the field should be taking moving

768
01:10:55,600 --> 01:11:01,040
forward in the decade, not necessarily 2020. And I'm hoping, really, in the next eight,

769
01:11:01,040 --> 01:11:08,000
nine years or so, we are going to say that finally, we have a system that can start to at least

770
01:11:08,000 --> 01:11:13,840
show the basic common sense understanding of a five-year-old child. That's awesome. Awesome.

771
01:11:14,640 --> 01:11:20,160
Nestering, thanks so much for taking the time to review your favorite papers of 2019 with us and

772
01:11:20,160 --> 01:11:28,960
to talk through your predictions. At no doubt, it will be an exciting year in 2020 and NLP and

773
01:11:28,960 --> 01:11:34,240
looking forward to keeping in touch on it. Yes, same here. Thank you so much, Sam,

774
01:11:34,240 --> 01:11:36,800
looking forward to 2020. Thanks so much.

775
01:11:40,560 --> 01:11:46,000
All right, everyone, that's our show for today. For more information on today's guest or for

776
01:11:46,000 --> 01:11:52,160
links to any of the materials mentioned, check out twimmelai.com slash rewind19.

777
01:11:53,120 --> 01:11:57,440
Be sure to leave us a five-star rating and a glowing review after you hit that subscribe

778
01:11:57,440 --> 01:12:09,120
button on your favorite podcast catcher. Thanks so much for listening and catch you next time.

