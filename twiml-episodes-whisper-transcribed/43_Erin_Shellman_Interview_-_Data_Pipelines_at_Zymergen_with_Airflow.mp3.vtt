WEBVTT

00:00.000 --> 00:15.840
Hello and welcome to another episode of Twimmel Talk, the podcast where I interview

00:15.840 --> 00:21.200
interesting people, doing interesting things in machine learning and artificial intelligence.

00:21.200 --> 00:23.760
I'm your host Sam Charrington.

00:23.760 --> 00:29.600
This is the third and final show in our series of podcasts from the recent Wrangle Conference.

00:29.600 --> 00:34.080
As you might know, a few weeks ago I was in San Francisco for Wrangle, which is a great

00:34.080 --> 00:38.480
little conference brought to you by our friends over at Cloud Era.

00:38.480 --> 00:43.280
This is the second time I've attended a Wrangle, and each year it brings an interesting and

00:43.280 --> 00:48.640
diverse community of data scientists to an intimate and informal setting for great talks

00:48.640 --> 00:54.520
on real data science issues and projects, not to mention cowboy hats and barbecue.

00:54.520 --> 00:59.960
If you haven't yet caught the first two episodes in our Wrangle series, Twimmel Talk No. 39

00:59.960 --> 01:05.600
with Drew Conway and Twimmel Talk No. 40 with Sheriff Rao, you'll want to be sure to check

01:05.600 --> 01:06.600
those out.

01:06.600 --> 01:10.640
They're both great interviews and the intro to the first show in the series includes

01:10.640 --> 01:15.920
important announcements about the series as well as our latest ticket giveaway, our online

01:15.920 --> 01:19.960
research paper discussion group, and my email newsletter.

01:19.960 --> 01:24.600
To show you're listening to now features my interview with Aaron Schelman. Aaron is a

01:24.600 --> 01:30.480
statistician and data science manager with Zymergen, a company using robots and machine

01:30.480 --> 01:33.400
learning to engineer better microbes.

01:33.400 --> 01:38.200
If you're wondering what exactly that means and involves, I was too, and we talk about

01:38.200 --> 01:40.000
it in the interview.

01:40.000 --> 01:45.920
Our conversation focuses on Zymergen's use of a patchy airflow, an open source data management

01:45.920 --> 01:49.440
platform originating at Airbnb.

01:49.440 --> 01:54.920
Aaron and her team uses airflow to create reliable, repeatable data pipelines for their machine

01:54.920 --> 01:59.200
learning applications, and we explore all that in the interview.

01:59.200 --> 02:04.080
A quick note before we dive in, as is the case with my other field recordings, there's

02:04.080 --> 02:08.560
a bit of unavoidable background noise in this interview, sorry about that.

02:08.560 --> 02:37.600
And now on to the show.

02:37.600 --> 02:40.760
Now we start by having you tell us a little bit about your background.

02:40.760 --> 02:41.760
Sure.

02:41.760 --> 02:47.520
So my background sort of at the intersection of computer science, statistics, and biology.

02:47.520 --> 02:49.800
So I went to graduate school at the University of Michigan.

02:49.800 --> 02:54.480
I did my masters in biostatistics, and then my PhD in bioinformatics.

02:54.480 --> 03:01.080
So always kind of working at the intersection of data and biology, and when I graduated

03:01.080 --> 03:05.560
sort of maybe counterintuitively, I came out to the West Coast, I live in Seattle, and

03:05.560 --> 03:08.280
I worked at Nordstrom in Nordstrom technology.

03:08.280 --> 03:09.280
Interesting.

03:09.280 --> 03:10.280
Yeah.

03:10.280 --> 03:13.400
And I was on a really cool team called the Data Lab there and built product recommendations

03:13.400 --> 03:14.400
for Nordstrom.com.

03:14.400 --> 03:15.400
Okay.

03:15.400 --> 03:18.360
And then did a stint at AWS, and now I'm at Symargin.

03:18.360 --> 03:19.360
Okay.

03:19.360 --> 03:20.360
Nice.

03:20.360 --> 03:22.000
And what specifically do you do at Symargin?

03:22.000 --> 03:23.000
Yeah.

03:23.000 --> 03:27.840
So I was initially, I was the first data scientist at the company, and I was a data scientist

03:27.840 --> 03:32.680
there for the better part of nearly two years, and then recently I've transitioned into

03:32.680 --> 03:34.560
managing the data science group there.

03:34.560 --> 03:38.400
And we're now eight people including myself, so a lot of growth since I started.

03:38.400 --> 03:39.400
Okay.

03:39.400 --> 03:40.400
So yeah.

03:40.400 --> 03:45.880
So a friend of mine and a former, a previous podcast guest, Josh Blum, has said that the worst

03:45.880 --> 03:49.280
job in the world is to be a company's first data scientist.

03:49.280 --> 03:50.800
Do you agree with me?

03:50.800 --> 03:53.840
That is a, there's a lot of truth to that.

03:53.840 --> 03:56.720
It really depends, yeah, it depends on what you like.

03:56.720 --> 04:02.400
So what I like about, so actually maybe surprisingly all of the jobs that I've had, I've been

04:02.400 --> 04:08.120
the first onto that team, and that wasn't really on purpose, but what I like about it is

04:08.120 --> 04:13.640
that you really have the ability to kind of structure what the goals and what the mission

04:13.640 --> 04:16.640
is, who you hire and how you build out that team.

04:16.640 --> 04:22.160
And I really enjoy that part of the job, sort of the higher level, maybe not so data-centric

04:22.160 --> 04:23.160
parts of the job.

04:23.160 --> 04:24.160
Okay.

04:24.160 --> 04:26.160
And you definitely have that kind of stuff more available to you when you're the only

04:26.160 --> 04:28.160
person on the team.

04:28.160 --> 04:32.640
It hasn't been like building out a team from having that experience as being the very

04:32.640 --> 04:33.640
first.

04:33.640 --> 04:37.880
Does it change your perspective on team composition and how you build it out?

04:37.880 --> 04:41.960
Yeah, it definitely changes your perspective because when you're the only person you're

04:41.960 --> 04:48.240
quite resource-constrained, and so the hiring matters in some sense more than when you

04:48.240 --> 04:54.280
have quite a bit more staff because every, you know, you double to two people that's

04:54.280 --> 04:58.120
still quite, not very many people, and so it's really important that you get somebody

04:58.120 --> 05:02.080
who has skills that complement your own or somebody who can teach you a lot of things

05:02.080 --> 05:04.520
that you don't know to make everybody more productive.

05:04.520 --> 05:09.480
So it's definitely a different experience than being in a big group with lots of people,

05:09.480 --> 05:10.680
but I actually, I think I like it more.

05:10.680 --> 05:11.680
I prefer it.

05:11.680 --> 05:12.680
Okay.

05:12.680 --> 05:13.680
Awesome.

05:13.680 --> 05:14.680
Awesome.

05:14.680 --> 05:17.920
Now, Zymergen, I'm betting is a company that not a lot of people in our audience know

05:17.920 --> 05:19.600
about what does the company do?

05:19.600 --> 05:20.600
Yeah.

05:20.600 --> 05:24.840
We're a little bit different than, you know, the Airbnb's and the Facebook's and all

05:24.840 --> 05:29.200
of those type of those companies, though we use a lot of Airbnb's technology, so kind

05:29.200 --> 05:33.360
of like Airbnb for microbes, not really.

05:33.360 --> 05:39.080
But yeah, so at Zymergen, what we're doing is we partner with companies who use industrial

05:39.080 --> 05:45.920
fermentation to make materials and molecules, and so what we do is we operate, we optimize

05:45.920 --> 05:52.800
strains, microbial strains, to be more efficient or more effective at producing molecules

05:52.800 --> 05:55.680
of interest to our customers through fermentation.

05:55.680 --> 06:00.680
So often these are companies who are already using fermentation at scale to produce molecules.

06:00.680 --> 06:01.680
So it turns out-

06:01.680 --> 06:02.680
I'm thinking beer.

06:02.680 --> 06:03.680
Yeah, exactly.

06:03.680 --> 06:04.680
Is it beer?

06:04.680 --> 06:05.680
Yeah.

06:05.680 --> 06:06.680
Another use case is zero.

06:06.680 --> 06:07.680
Yeah.

06:07.680 --> 06:11.680
So, you know, we, the common application, right, of fermentation is to make alcohols,

06:11.680 --> 06:16.440
alcoholic beverages, but it turns out that you can use that process to create lots of

06:16.440 --> 06:20.800
different types of molecules, and you can use that to make molecules that can be precursors

06:20.800 --> 06:24.400
for pretty complicated materials as well, and so that's what we do.

06:24.400 --> 06:28.200
We kind of do the same process, but we're making all kinds of different types of molecules

06:28.200 --> 06:29.520
for different applications.

06:29.520 --> 06:31.720
What are some examples of those applications?

06:31.720 --> 06:36.200
So examples of applications in general, not specific to Zymergen, are, well, for example,

06:36.200 --> 06:43.160
insulin is sort of a classical example of using fermentation in health sciences to produce

06:43.160 --> 06:44.160
insulin.

06:44.160 --> 06:49.320
So that was a huge revolution in terms of being able to create it because it was a very

06:49.320 --> 06:53.640
expensive and kind of grew some way that we used to do it in the past, which is largely

06:53.640 --> 06:56.760
through extracting it from pigs, which is not pretty.

06:56.760 --> 07:01.300
Obviously, if you feel it's a lot better if you can, you know, use microbes and do it in

07:01.300 --> 07:05.280
giant fermenters, and you can produce a lot more at lower costs, so it's kind of the

07:05.280 --> 07:06.480
same thing, yeah.

07:06.480 --> 07:12.000
And so is it a direct byproduct of fermentation, or is it, is there a byproduct that's used

07:12.000 --> 07:13.000
in its creation?

07:13.000 --> 07:18.440
Yeah, it kind of depends on the microbe and the molecule that you're producing, but

07:18.440 --> 07:24.040
often what we're doing is sort of augmenting or kind of ramping up normal metabolic processes

07:24.040 --> 07:29.880
in the cell, so these microbes will ingest, you know, sugars, metabolize things like that

07:29.880 --> 07:35.600
to create these molecules, sort of as sometimes their waste products, it really depends, and

07:35.600 --> 07:39.960
they excrete those into the surrounding fluid inside the tank, and then we harvest that

07:39.960 --> 07:40.960
or we extract that.

07:40.960 --> 07:41.960
Oh, wow.

07:41.960 --> 07:43.720
Yeah, to get those molecules.

07:43.720 --> 07:44.720
Wow.

07:44.720 --> 07:46.200
So what was your talk about?

07:46.200 --> 07:50.840
Yeah, so I was talking about some of the sort of the challenges that we face.

07:50.840 --> 07:55.160
So I was talking a little bit about our mission, actually, of the Data Science Teams mission,

07:55.160 --> 07:59.440
and our goal is to use our testing platform.

07:59.440 --> 08:04.440
So the way that we do what we do as I imagine stepping back for a second is that we rely

08:04.440 --> 08:10.880
on robotic automation, sort of combined with machine learning to build this test platform

08:10.880 --> 08:15.720
that allows us to simultaneously measure the performance of lots of different strains

08:15.720 --> 08:16.720
in parallel.

08:16.720 --> 08:17.720
Okay.

08:17.720 --> 08:21.320
And we use our goal on the Data Science team is to use all of that data that we're generating

08:21.320 --> 08:26.840
through this high throughput screening process, use all of that to then make machine learning

08:26.840 --> 08:31.960
models or make predictive models to help us make better decisions about the experiments,

08:31.960 --> 08:33.600
the strains that we design in the first place.

08:33.600 --> 08:37.840
So basically to help the scientists design better strains so that we don't have to spend

08:37.840 --> 08:42.440
as much time experimenting if we could get to the solution or get to the answer faster,

08:42.440 --> 08:43.440
that's really our goal.

08:43.440 --> 08:48.840
That's an ambitious goal, it's not easy to do, and so part of what I was talking about

08:48.840 --> 08:52.280
was sort of the things that make it hard to accomplish that.

08:52.280 --> 08:57.120
So what are sort of the practical data issues that we encounter and how we're solving those

08:57.120 --> 09:03.440
so that we get really clean in analysis ready or modeling ready data for those complicated

09:03.440 --> 09:04.440
models.

09:04.440 --> 09:05.440
Okay.

09:05.440 --> 09:10.400
And so what are some specific examples of the data sources and data types that feed your

09:10.400 --> 09:11.400
models?

09:11.400 --> 09:18.000
Yeah, so by and large, a lot of the data that we're consuming is really kind of measurements

09:18.000 --> 09:19.960
that represent concentrations.

09:19.960 --> 09:27.840
So we've got these microbes, they're metabolizing things and they're excreting these compounds

09:27.840 --> 09:32.040
or these molecules into the solution around them and then we measure the concentration

09:32.040 --> 09:37.080
of that so that we can tell whether the microbe has improved over its predecessor and then

09:37.080 --> 09:40.720
move that into make a decision based on that essentially.

09:40.720 --> 09:44.320
And so for the most part, the data that we're working with is some measurement, is some

09:44.320 --> 09:45.320
measure of concentration.

09:45.320 --> 09:46.320
Got it.

09:46.320 --> 09:49.720
And what's the scale that this is happening at?

09:49.720 --> 09:56.320
Like is this concentrations in vats of things or like microarrays or somewhere in between?

09:56.320 --> 09:57.960
Yeah, that's a really great question.

09:57.960 --> 10:00.720
So it's kind of all of those things.

10:00.720 --> 10:07.120
So what we do practically in our testing platform is we kind of do it more, not quite as dense

10:07.120 --> 10:11.160
as a microarray, but typically it's something like 96-well plates.

10:11.160 --> 10:17.400
So we have these kind of plates that have 96-wells, each of those wells contains fluid, contains

10:17.400 --> 10:20.880
the microbe and sort of the experimental input.

10:20.880 --> 10:25.720
And that's the level that initially when we're doing our initial screens that we're experimenting

10:25.720 --> 10:26.720
at.

10:26.720 --> 10:32.840
Once a strain, for example, demonstrates improvement compared to its predecessor, it'll go into

10:32.840 --> 10:37.080
another round of that testing, we basically want to validate or replicate that performance

10:37.080 --> 10:38.080
again.

10:38.080 --> 10:42.280
Once it's done that, we actually validate those strains in fermenters.

10:42.280 --> 10:47.120
And so it's kind of a challenge to, you know, the size of a fermentation tank is much

10:47.120 --> 10:50.120
larger than that of a tiny well on a plate.

10:50.120 --> 10:54.560
And so we always want to validate those strains before we deliver them to a customer, for

10:54.560 --> 10:59.800
example, to make sure that the performance and the plate actually represents the performance

10:59.800 --> 11:03.120
that we expect in the fermentation tanks.

11:03.120 --> 11:08.160
And so we do both at Xymrgin, and then we also like to partner with people and run them

11:08.160 --> 11:10.520
at scale in their tanks, too, when possible.

11:10.520 --> 11:11.520
Okay.

11:11.520 --> 11:12.520
Interesting.

11:12.520 --> 11:16.880
And so you started your talk, talking a little bit about kind of the context and your

11:16.880 --> 11:20.360
mission, and then what?

11:20.360 --> 11:25.680
Oh, and then, yeah, and so I was talking a little about some of the challenges we face

11:25.680 --> 11:31.880
with our data, some kind of practical challenges, and how we're using Airflow to build a pipeline

11:31.880 --> 11:33.560
that addresses some of those challenges.

11:33.560 --> 11:34.560
Okay.

11:34.560 --> 11:35.560
What's Airflow?

11:35.560 --> 11:39.160
So Airflow is, it's a Apache incubating project.

11:39.160 --> 11:45.400
It's a Python module, basically, that allows you to construct data processing workflows,

11:45.400 --> 11:50.800
and you're basically constructed DAG of that workflow, and allows you to do things like

11:50.800 --> 11:55.920
scheduling, monitor the progress of those jobs, and even a little bit of reporting.

11:55.920 --> 11:56.920
Yeah.

11:56.920 --> 12:00.720
So it basically helps us orchestrate all of our sort of complicated ETL steps.

12:00.720 --> 12:01.720
Okay.

12:01.720 --> 12:03.520
Where are you eating the data from?

12:03.520 --> 12:05.280
Where does it tend to live?

12:05.280 --> 12:13.240
Yeah, so, and mostly, so we have what's called a limbs, it's sort of a biology, that's

12:13.240 --> 12:14.240
right.

12:14.240 --> 12:15.240
Okay.

12:15.240 --> 12:19.040
I've never met anyone who was, like, not a biologist who knew what that was.

12:19.040 --> 12:20.560
Yes, that's exactly what it is.

12:20.560 --> 12:24.720
So we have a, we have a limbs, and we have a corresponding front end that the scientists

12:24.720 --> 12:29.360
can upload and sort of download data from, and then that gets persisted to a sort of a single

12:29.360 --> 12:33.160
source of truth, like a data warehouse, and that's, for the most part, with the data scientists

12:33.160 --> 12:34.160
access the data from.

12:34.160 --> 12:35.160
From a data warehouse?

12:35.160 --> 12:36.160
Okay.

12:36.160 --> 12:39.520
So you use Airflow, how long have you been using that?

12:39.520 --> 12:42.400
We've been using it, I guess, almost about a year.

12:42.400 --> 12:43.400
Okay.

12:43.400 --> 12:46.000
And that's, you mentioned earlier, you use AirBnB stuff.

12:46.000 --> 12:47.600
Airflow is an AirBnB product?

12:47.600 --> 12:48.600
Yeah, it was a project.

12:48.600 --> 12:52.500
Yeah, exactly, it was an AirBnB product that I think they opened source, and then it was

12:52.500 --> 12:57.360
picked up by Apache, and now it's kind of an incubating project, which, yeah.

12:57.360 --> 13:04.120
And how does it compare to, I'm trying to remember the name of the product that complements,

13:04.120 --> 13:09.280
like Google Cloud, has their data flow, and there's an open source, it's also Apache,

13:09.280 --> 13:10.280
it's not.

13:10.280 --> 13:13.000
It's like oozee or, yeah, no.

13:13.000 --> 13:14.320
It's comparable to that.

13:14.320 --> 13:15.320
It's comparable to oozee?

13:15.320 --> 13:16.320
Yeah.

13:16.320 --> 13:17.320
Okay.

13:17.320 --> 13:22.560
And it's agnostic to sort of, I think oozee is sort of a part of the Hadoop ecosystem.

13:22.560 --> 13:23.560
Airflow is not.

13:23.560 --> 13:27.600
So it's pretty generic in that sense, so you can really use it, it's pretty powerful

13:27.600 --> 13:28.600
in that sense.

13:28.600 --> 13:32.960
So it doesn't have any opinions, really, about the platform when where your data come

13:32.960 --> 13:33.960
from.

13:33.960 --> 13:34.960
Okay.

13:34.960 --> 13:43.160
And so what are the implications on the way you kind of craft and deploy the analytics that

13:43.160 --> 13:51.320
sit on top of the underlying, you know, the data, kind of the data engineering pieces?

13:51.320 --> 13:55.800
Like does airflow, does the weight, any of the semantics of airflow have direct impact

13:55.800 --> 14:00.800
on the way you view the analytics, or is it, you know, kind of separate concerns?

14:00.800 --> 14:01.800
I don't know.

14:01.800 --> 14:05.880
I think maybe they're largely separate concerns, although I guess one of some one of the sort

14:05.880 --> 14:11.960
of use cases that I described was, you know, we do a lot of experimentation as I'm

14:11.960 --> 14:14.600
origin, lots of different types of experiments.

14:14.600 --> 14:19.040
And our scientists use lots of different types of tools to work with data.

14:19.040 --> 14:23.080
And the result of that is sometimes the data doesn't make it into our limbs, so it doesn't

14:23.080 --> 14:24.840
make it into the warehouse.

14:24.840 --> 14:28.640
And one way that we've addressed that is that airflow has all these nice sort of hooks

14:28.640 --> 14:32.640
or operators into third party things like Dropbox.

14:32.640 --> 14:37.800
And so one thing that we've had success with is to be able to work with the scientists

14:37.800 --> 14:43.400
and get them to make some standards around where they put their data in Dropbox, and then

14:43.400 --> 14:48.240
we make really lightweight ingestion pipelines to grab that data and ingest it into our limbs

14:48.240 --> 14:49.240
for them.

14:49.240 --> 14:53.840
And then we're using also a NARB&B product called SuperSet, which is sort of a dashboarding

14:53.840 --> 14:54.840
tool.

14:54.840 --> 14:59.040
So we've now hooked that up so that we can ingest data from Dropbox, and then we can

14:59.040 --> 15:03.160
produce dashboards for the scientists to actually consume their own data that way.

15:03.160 --> 15:07.920
And that's been kind of a success story, making really lightweight stuff, doesn't take

15:07.920 --> 15:12.760
very long to make it all, and can surface the results right there pretty quickly.

15:12.760 --> 15:13.760
Okay.

15:13.760 --> 15:18.000
And so what were some of the insights that you were sharing about using airflow?

15:18.000 --> 15:23.200
Yeah, so I was sharing, I was kind of stepping through a couple of use cases that we, well,

15:23.200 --> 15:28.880
I was describing some of the our limitate or the challenges that we face with our data.

15:28.880 --> 15:33.520
And then sort of the challenges we had when we were working on our own sort of homegrown

15:33.520 --> 15:40.200
ETL solution or platform, and why we eventually sort of abandoned that and adopted airflow.

15:40.200 --> 15:43.840
And so I imagine that a lot of people start there.

15:43.840 --> 15:44.840
Yeah.

15:44.840 --> 15:45.840
Like what are some of those challenges?

15:45.840 --> 15:46.840
Yeah.

15:46.840 --> 15:53.080
So one of the, I think more difficult challenges was that, I mentioned that largely the,

15:53.080 --> 15:56.560
a lot of the data that we're working with is concentrations of things, so we're measuring

15:56.560 --> 16:01.000
how much of something there is in a certain volume of solution.

16:01.000 --> 16:05.280
It turns out measuring concentration of something is not that straightforward, so there are

16:05.280 --> 16:09.680
a lot of different ways that you can measure the concentration of a solution, of something

16:09.680 --> 16:11.480
in a solution.

16:11.480 --> 16:16.480
And depending on the group and whether, like who they're working with and sort of the

16:16.480 --> 16:20.960
way that they choose to measure that, that has implications for the data that we can

16:20.960 --> 16:21.960
expect.

16:21.960 --> 16:25.520
But we need to basically process everything the same way, regardless of the sort of the

16:25.520 --> 16:28.640
format or the type of experiment that they used.

16:28.640 --> 16:33.040
And so that was challenging to kind of articulate ourselves, there's just a lot of overhead

16:33.040 --> 16:36.560
and there's sort of a lot of logic that we would have to encode to do that.

16:36.560 --> 16:42.080
Another challenge was describing this sort of complex dependencies in between our processing

16:42.080 --> 16:43.080
steps.

16:43.080 --> 16:46.080
So, you know, I need this to happen and then that's going to kick off another job that

16:46.080 --> 16:50.360
does this and that's going to kick off something else, but orchestrating sort of that communication

16:50.360 --> 16:54.800
and writing all the logic to, for what do I do if the first thing fails or what if I,

16:54.800 --> 16:59.240
what do I do if the second thing fails and doing all of that ourselves is challenging.

16:59.240 --> 17:02.760
And we have all the, we have data coming in at different velocities and that's also hard

17:02.760 --> 17:03.760
to orchestrate.

17:03.760 --> 17:09.040
So some of our products, they start processing data as soon as a scientist uploads data

17:09.040 --> 17:11.480
into the limbs or into the warehouse.

17:11.480 --> 17:16.440
Others can be scheduled and so it runs nightly or weekly and, or doing all that orchestration

17:16.440 --> 17:19.280
ourselves was, was very challenging.

17:19.280 --> 17:25.160
And, and so I imagine the upside is that airflow kind of handles, handles all of this for you.

17:25.160 --> 17:30.280
It, you know, does the impedance matching from, you know, the different, different velocities

17:30.280 --> 17:32.400
of information coming in and things like that.

17:32.400 --> 17:33.400
Yeah.

17:33.400 --> 17:39.240
So, airflow does a lot of things for us in terms of sort of handling different data inputs

17:39.240 --> 17:42.200
and being sort of agnostic to that.

17:42.200 --> 17:44.040
It's been huge for us for that.

17:44.040 --> 17:49.160
So, we've basically in our processing steps, we have created sort of a generic interface.

17:49.160 --> 17:52.600
So we have sort of three big processing nodes that need to happen.

17:52.600 --> 17:54.280
And they all have very generic interfaces.

17:54.280 --> 17:57.160
So they don't know anything about the data that they're going to receive.

17:57.160 --> 18:01.160
And then basically we, we contextualize it at the time that we've received data.

18:01.160 --> 18:06.560
And so, and that, that has made it very flexible and modular for us so that we can, you know,

18:06.560 --> 18:08.760
a new experimental platform comes online.

18:08.760 --> 18:12.160
It will be very easy for us to apply the same, well, I wouldn't say very easy.

18:12.160 --> 18:16.280
It will be much easier for us to apply the same set of processing steps with a new data

18:16.280 --> 18:20.880
set just because we've, we've gone through the effort of making the interface generic.

18:20.880 --> 18:23.520
So that, and that was something that's harder to do.

18:23.520 --> 18:24.520
Yeah.

18:24.520 --> 18:25.520
Sorry.

18:25.520 --> 18:26.520
Are these three, you said nodes?

18:26.520 --> 18:31.760
Are the three nodes, are they, is this an artifact of kind of the way you would ideal

18:31.760 --> 18:36.400
or design your own processes or is this something that's kind of imposed on you by the way air

18:36.400 --> 18:37.400
flow does things?

18:37.400 --> 18:38.400
Oh, no.

18:38.400 --> 18:42.600
It's, it's more just like sort of the flow of the pipeline, the processing pipeline.

18:42.600 --> 18:45.480
And it, which so happens to have three steps.

18:45.480 --> 18:46.480
Yeah.

18:46.480 --> 18:50.240
So like the, the initial step, one of the, one of the things that we see, you know, I

18:50.240 --> 18:53.720
mentioned, I think that we use, part of the reason we're able to do what we do is that

18:53.720 --> 18:58.600
we rely heavily on robotic automation to do a lot of the heavy lifting in the lab, but

18:58.600 --> 19:04.680
robots fail sometimes or weird things happen in the lab, you know, it's a, it's, the experimentation

19:04.680 --> 19:06.160
is, is challenging.

19:06.160 --> 19:10.640
And so the result of that is we'll see sort of extreme values or like outlying values

19:10.640 --> 19:13.520
that, you know, typically indicate a process failure.

19:13.520 --> 19:16.600
And so that first step is really just an outlier detection step.

19:16.600 --> 19:21.840
So let's, let's identify those process failures and filter those out of any downstream processes.

19:21.840 --> 19:26.440
The second step of that pipeline is something called normalization.

19:26.440 --> 19:30.520
And that's really meant to address sort of another challenge that we face, which are batch

19:30.520 --> 19:35.240
effects, which is a very common phenomenon and high throughput screening environment.

19:35.240 --> 19:41.320
So, you know, you've got a bunch of, you've got a chip with a lot of samples or a lot

19:41.320 --> 19:44.320
of, you know, probes or on it.

19:44.320 --> 19:47.000
And you're asking a lot of questions in a very tight space.

19:47.000 --> 19:51.280
And so these types of environments tend to have strong temporal effects.

19:51.280 --> 19:55.680
So even if you imagine you do the exact same experiment this week and next week, they

19:55.680 --> 19:58.160
might look like they're coming from different distributions.

19:58.160 --> 20:02.600
And that doesn't actually reflect meaningful biological variability.

20:02.600 --> 20:07.360
It's actually just kind of a reflection of the process or of the temperature in the room

20:07.360 --> 20:08.360
at the time.

20:08.360 --> 20:12.000
So yeah, the person who ran it or all these other things that we don't actually care

20:12.000 --> 20:13.000
about.

20:13.000 --> 20:14.000
They're kind of nuisance things.

20:14.000 --> 20:15.000
Right.

20:15.000 --> 20:19.240
And so the second step of that processing pipeline is normalizing the data to try to

20:19.240 --> 20:22.280
eliminate those process, process-related biases.

20:22.280 --> 20:23.280
Okay.

20:23.280 --> 20:26.880
And then sort of the third step and sort of the third challenge that we face with our

20:26.880 --> 20:32.640
data is we have a motto at Zymrogen that is any micro, any molecule.

20:32.640 --> 20:38.240
And what that means is that we, we've built a testing platform that is agnostic to

20:38.240 --> 20:41.560
our customers, microbe, and to the molecule that they're making.

20:41.560 --> 20:48.080
We think that we have a process that will allow us to optimize those strains regardless

20:48.080 --> 20:51.080
of the actual application.

20:51.080 --> 20:55.680
That's amazing from a sort of a business strategy point of view because we can work in lots

20:55.680 --> 20:56.680
of different industries.

20:56.680 --> 21:00.920
We can work with lots of different bugs and make lots of different types of molecules.

21:00.920 --> 21:06.320
But it can be challenging from a data perspective because it can result in a proliferation

21:06.320 --> 21:07.320
of solutions.

21:07.320 --> 21:14.160
So we don't always have agreement on what the right way or whether something that constitutes

21:14.160 --> 21:18.640
an improvement for one group might not be considered an improvement in another group.

21:18.640 --> 21:22.960
And from a modeler's perspective, it's not always clear what the result of the experiment

21:22.960 --> 21:26.520
it is to us as consumers of that test data in a way.

21:26.520 --> 21:31.200
And so one way that we're addressing that is this sort of third piece of that processing

21:31.200 --> 21:36.200
pipeline where we actually do the matching up of the candidate strain with its reference

21:36.200 --> 21:40.560
strain, and we do that testing, and we, like statistical hypothesis testing, and we write

21:40.560 --> 21:46.360
that result so that regardless, sort of independent of the decisions that our scientists make,

21:46.360 --> 21:50.960
we have some indication of what we think happened in the experiment in a sort of a consistent

21:50.960 --> 21:54.840
view of what improvement looks like for our models.

21:54.840 --> 22:01.960
Interesting is it from a modeling and statistical perspective, is it challenging to imagine

22:01.960 --> 22:07.680
it to be challenging to kind of normalize results from comparing one molecule to another

22:07.680 --> 22:11.000
or one biological process to another?

22:11.000 --> 22:12.480
Is that challenging?

22:12.480 --> 22:19.600
So in general, we're not doing that, so we don't share data at all between sort of projects

22:19.600 --> 22:20.600
or...

22:20.600 --> 22:27.480
I guess what I understood you to say that, you know, you've built this general platform for

22:27.480 --> 22:35.320
testing the results of these molecule production, microptomolecule production processes,

22:35.320 --> 22:40.720
and then as a way to make sure that you understand their efficacy, you know, you're kind of

22:40.720 --> 22:46.000
taking that analysis all the way to, you know, what would tell me, what is that end result

22:46.000 --> 22:48.320
that you're driving for?

22:48.320 --> 22:54.520
Yeah, so that note or that last step in the pipeline is often called hit detection in

22:54.520 --> 22:59.120
sort of the biology, the high throughput screening literature, and that's really just

22:59.120 --> 23:05.560
the process of identifying in your screen, which of those candidates that you were exploring

23:05.560 --> 23:08.160
seem to have the characteristics that you're looking for.

23:08.160 --> 23:14.160
And so it's sort of a, it's not as, because it's a screening scenario where our statistical

23:14.160 --> 23:17.360
criteria isn't quite as high as it would be if you were doing a single test, like I

23:17.360 --> 23:21.960
want to know, you know, we're screening, and so actually we care more about keeping false

23:21.960 --> 23:27.320
negatives low than we do about having high false positive rate and you know, retesting

23:27.320 --> 23:29.720
something that actually wasn't a very good strength.

23:29.720 --> 23:34.400
We would rather waste some resources testing something that wasn't good than lose the

23:34.400 --> 23:38.320
opportunity to test again something that actually was good.

23:38.320 --> 23:42.240
Meaning you're screening for possibilities, and the more possibilities you have, the

23:42.240 --> 23:45.520
more opportunity you have to find the thing that actually works.

23:45.520 --> 23:50.560
Yeah, exactly, yeah, yeah, yeah, so it's kind of a different, like way of thinking about

23:50.560 --> 23:55.320
it then, you know, often like in an A, B test, for example, you want to know the right

23:55.320 --> 23:56.320
answer.

23:56.320 --> 23:57.320
Right, right.

23:57.320 --> 23:58.400
Okay, got it.

23:58.400 --> 24:04.560
And so you talked about the challenges that led to deploying airflow.

24:04.560 --> 24:09.200
What about the challenges of deploying airflow and kind of building out this system?

24:09.200 --> 24:11.400
Did you encounter anything in particular there?

24:11.400 --> 24:16.640
Yeah, so, you know, it's sort of probably challenges that are typical of adopting anything

24:16.640 --> 24:19.200
that's kind of an early project.

24:19.200 --> 24:20.200
Right.

24:20.200 --> 24:26.440
One of our bigger challenges was really just finding non-trivial examples of how it's

24:26.440 --> 24:27.920
used in production.

24:27.920 --> 24:33.840
So what I think was some of, at least I experienced this, I feel like this was sort of the general

24:33.840 --> 24:38.160
experience of everybody on the team is that, you know, there's a certain way you're supposed

24:38.160 --> 24:43.880
to write these DAGs or these workflows in airflow every time I would write when I felt

24:43.880 --> 24:45.040
like it was the wrong way.

24:45.040 --> 24:50.560
So I would either try to, it felt like I was putting too little processing sort of in one

24:50.560 --> 24:56.560
node and kind of making too many nodes or it felt like I had one big node that did everything.

24:56.560 --> 25:00.840
And so it was really hard to like get a sense for what the right way to construct, what

25:00.840 --> 25:06.760
unit of work was appropriate and was sort of intended by the design of airflow.

25:06.760 --> 25:08.840
That was challenging, took some getting used to it.

25:08.840 --> 25:13.360
The team is using it a lot now, so I feel like we've got that a pretty good handle on

25:13.360 --> 25:14.360
that now.

25:14.360 --> 25:20.440
That was certainly challenging, sort of getting familiar with it and finding good examples

25:20.440 --> 25:25.440
of non-trivial examples of its use, which I don't know.

25:25.440 --> 25:30.880
It's interesting, it's something that comes up a lot in my conversations with folks in

25:30.880 --> 25:31.880
different domains.

25:31.880 --> 25:42.080
You know, both in take as an example, architecting neural nets or this, there's the documentation

25:42.080 --> 25:48.840
and there's the research and the literature, so much of adopting these new technologies,

25:48.840 --> 25:54.120
like tribal knowledge or black art, or it just practice that, you know, you often don't

25:54.120 --> 25:56.920
find good sources for how to do that stuff.

25:56.920 --> 26:03.040
Yeah, and that makes it, I don't know, that's kind of a friction on adopting this stuff.

26:03.040 --> 26:08.400
Like you want to see some success stories of somebody having used it so that you can

26:08.400 --> 26:14.920
be sure that before abandoning, like our home grown ETL system, we want to be reasonably

26:14.920 --> 26:21.200
sure that the thing that we move to works isn't going to be suffer from the same problems.

26:21.200 --> 26:25.120
And of course, at the understanding that it is a new project and that there will be

26:25.120 --> 26:27.720
sort of foibles as a result of that.

26:27.720 --> 26:31.520
But in general, we want to make sure it solves the bulk of the problems that we have with

26:31.520 --> 26:33.000
our own solution.

26:33.000 --> 26:35.200
And that can be hard to demonstrate sometimes.

26:35.200 --> 26:39.600
But in our case, it worked out Airflow is a really powerful tool for our group.

26:39.600 --> 26:44.360
Has that changed at all since you started to use it, are you seeing more of these examples

26:44.360 --> 26:50.720
or more, you know, better documentation of these more subtle, either use case examples

26:50.720 --> 26:55.720
like you described, or kind of some of these more subtle design philosophies and decisions?

26:55.720 --> 27:01.280
Yeah, I mean, even today, like I was the speaker right before me to talk a lot about Airflow

27:01.280 --> 27:04.520
and then right after me, it was a panel where they talked a bit about Airflow.

27:04.520 --> 27:10.160
So it seems like it is now becoming quite a popular tool and being used pretty pervasively.

27:10.160 --> 27:14.720
So I expect that we'll see more and more, more and more of these kind of stories of how

27:14.720 --> 27:16.320
it's being used in production.

27:16.320 --> 27:19.840
And it's a very flexible tool and it has a lot of functionality.

27:19.840 --> 27:23.640
And so we're using it in ways that we didn't actually expect initially.

27:23.640 --> 27:25.920
And so I'm excited to see how other people are using it.

27:25.920 --> 27:30.600
I'm sure there are a lot of really creative things that are being developed on it.

27:30.600 --> 27:31.600
Nice.

27:31.600 --> 27:39.040
And for the, or is it intended, do you think that the main consumer of the tool is a data

27:39.040 --> 27:45.440
science team, as opposed to a data engineering team or some other variation on the term?

27:45.440 --> 27:48.320
Yeah, I think that's a good question.

27:48.320 --> 27:53.400
I'm not totally sure what the intentions were, but it's definitely a tool that's very

27:53.400 --> 27:59.000
powerful for data scientists in terms of just like the way that we think and being able

27:59.000 --> 28:02.800
to construct workflows that way, it just feels very natural.

28:02.800 --> 28:07.800
I guess it's even a hard question to answer because data science and data scientist means

28:07.800 --> 28:10.200
so many different things.

28:10.200 --> 28:12.200
This actually came up in my last interview as well.

28:12.200 --> 28:17.840
We talked about how this has evolved since five years ago when everything was considered

28:17.840 --> 28:24.000
data science, or data science was considered needing to know all of the different bits

28:24.000 --> 28:30.280
and pieces of moving the data around and doing the analytics and getting it to production.

28:30.280 --> 28:38.520
When I hear you describe the tool, I think plumbing, I think kind of no-level stuff.

28:38.520 --> 28:44.520
And that was really the source of the question, like, is it is, do you think that that is typical

28:44.520 --> 28:51.200
for data science to kind of dive into that level, or is our data scientist typically

28:51.200 --> 28:56.560
that's supported by other groups that are kind of putting the plumbing in place?

28:56.560 --> 29:03.160
Yeah, so for us, the way that we ended up kind of productionizing our airflow environment

29:03.160 --> 29:06.360
was with the collaboration with data engineering.

29:06.360 --> 29:11.560
We have, our group is pretty engineering-heavy anyway, all the data scientists are fairly

29:11.560 --> 29:12.920
do a lot of engineering.

29:12.920 --> 29:18.280
And so of course we had help from actual data engineers, and then members of the team

29:18.280 --> 29:24.000
who have that skill set as well, are responsible for sort of doing all the configuration and

29:24.000 --> 29:25.960
the start-up scripts for people.

29:25.960 --> 29:32.400
Because our group is pretty mixed in terms of the background and interests and the skills.

29:32.400 --> 29:38.000
And so that's been great, like, they spent a lot of time really developing structure

29:38.000 --> 29:42.320
around how to get it set up, and so that all of the data scientists, when they come

29:42.320 --> 29:46.920
online, can easily set it up and then start doing what they already know how to do,

29:46.920 --> 29:49.880
which is construct the workflows and Python.

29:49.880 --> 29:51.240
So it's kind of two levels.

29:51.240 --> 29:55.800
I think we did probably need help from data engineers to actually get it up and get it

29:55.800 --> 30:00.160
running consistently, and sort of in a production level of an environment.

30:00.160 --> 30:05.520
But then once it's there, it's actually, I think, a very simple tool for the average

30:05.520 --> 30:11.200
data scientist to quickly start making processing workflows.

30:11.200 --> 30:15.600
And part of that is that it has a really cool out-of-the-box UI, so you can write your

30:15.600 --> 30:19.760
workflow in Python, and then you can go to the UI, you can run it there, you can view

30:19.760 --> 30:24.440
all kinds of metrics about the pipeline, there's even stuff about sort of which tasks in

30:24.440 --> 30:28.080
that pipeline are sort of the bottlenecks, and what are the performance metrics of each

30:28.080 --> 30:29.360
of the individual tasks.

30:29.360 --> 30:35.240
So it allows you to kind of see, get visibility into those workflows that, in the past, I

30:35.240 --> 30:38.880
haven't had actually really anywhere, so it's great for that.

30:38.880 --> 30:39.880
Interesting.

30:39.880 --> 30:46.520
Is that user experience, is it kind of analogous to a notebook, or is it more like a job processing

30:46.520 --> 30:47.520
type of tool?

30:47.520 --> 30:52.000
No, it's more like a bonafide application.

30:52.000 --> 30:59.160
So it's got sort of a table of all of the jobs that are around, and you can kind of change

30:59.160 --> 31:04.600
the scheduling right there in the UI, turn them on and off, and then tab over to metrics

31:04.600 --> 31:05.800
and all kinds of other things.

31:05.800 --> 31:11.560
You can view the logs from there, so if something failed, obviously you can log into the machine

31:11.560 --> 31:16.320
and view the logs there, or you can just use the UI and view the logs directly, see what

31:16.320 --> 31:20.800
happened with your job, you'll get a bunch of rich diagnostics about which part of the

31:20.800 --> 31:23.560
workflow failed, and all kinds of stuff right there in the UI.

31:23.560 --> 31:25.560
So it's actually, it's a pretty developed tool.

31:25.560 --> 31:26.560
Oh wow.

31:26.560 --> 31:27.560
It's really helpful.

31:27.560 --> 31:28.560
Oh, very nice.

31:28.560 --> 31:29.560
Very nice.

31:29.560 --> 31:32.520
Anything else you'd like to share with the audience, or leave with the audience?

31:32.520 --> 31:38.960
I guess just check it out, it's been a really great tool for us, so the reason that we

31:38.960 --> 31:44.600
invested in it really is to support our work in predictive strain design, that is sort

31:44.600 --> 31:50.800
of our mission is to use machine learning so that we can construct better strains and help

31:50.800 --> 31:54.120
our scientists get to the solution faster.

31:54.120 --> 31:58.480
And airflow has been incredible in helping us solidify that processing pipeline to support

31:58.480 --> 31:59.480
that work.

31:59.480 --> 32:04.280
It's a clean analysis ready or modeling ready data that's consumable directly from this

32:04.280 --> 32:10.320
pipeline, and a lot of it, a lot of the headache of what's sort of traditionally, or maybe

32:10.320 --> 32:13.920
not associated with being a data scientist, but with data scientists know it's actually

32:13.920 --> 32:18.600
about, can be sort of addressed with airflow, or at least can be ameliorated some so that

32:18.600 --> 32:23.800
it's not, so that 90% of your time isn't actually spent cleaning data and munging it and moving

32:23.800 --> 32:24.800
it around.

32:24.800 --> 32:29.720
We can do, it's very flexible, can do all kinds of different types of tasks, and that's

32:29.720 --> 32:35.040
been really helpful for getting, sort of, trying to eliminate sort of the boring stuff so

32:35.040 --> 32:37.000
that we can do the cool stuff.

32:37.000 --> 32:38.760
Awesome, awesome.

32:38.760 --> 32:43.400
If I can draw you back in, that was kind of a great summary, but we haven't really

32:43.400 --> 32:48.600
talked a lot about the specific models that you use, like we've talked about this tool

32:48.600 --> 32:50.600
that helps you get the data to the models.

32:50.600 --> 32:51.600
Yeah.

32:51.600 --> 32:55.600
A bit about the types of models that you're building, the modeling techniques you're

32:55.600 --> 32:57.160
using, things like that.

32:57.160 --> 33:04.480
Yeah, I can't talk a ton about them, but the conceptually what we're doing is we're

33:04.480 --> 33:09.000
taking information about what we know the scientists are engineering into the strains,

33:09.000 --> 33:15.200
so the type of change that they're making, where it is, so what gene is it that's being

33:15.200 --> 33:21.400
perturbed, or being engineered, and we're basically making models to predict combinations

33:21.400 --> 33:28.800
of those changes, so the typical microbe has something between three and 5,000 genes in

33:28.800 --> 33:35.000
it, and so if we were to perturb each of those individually and then start perturbing

33:35.000 --> 33:40.480
each pairwise combination, that's an infinite, combinatorical space to explore, and so what

33:40.480 --> 33:44.000
we're trying to do instead is take all of the information about the things that we know

33:44.000 --> 33:48.200
we've already done, and then make predictions about how we think those are going to perform

33:48.200 --> 33:49.720
when they're combined.

33:49.720 --> 33:54.760
So a little bit of evolutionary genetic algorithm types of things, or...

33:54.760 --> 34:02.400
A lot of things, actually, so we have a lot of different types of models and some work

34:02.400 --> 34:07.560
in better context than others, so we have models that kind of address different things.

34:07.560 --> 34:11.040
Some work better when you have a bunch more information, so after a project is fairly

34:11.040 --> 34:15.600
mature, and we have a whole lot of test data that we can use to train on, our models are

34:15.600 --> 34:20.280
different than when we're at the cold start problem, where really all we know, maybe

34:20.280 --> 34:24.440
is the metabolic structure of the microbe, we haven't started doing anything yet, so

34:24.440 --> 34:30.920
how do we guide the experimentation in that case when we don't know at all about really

34:30.920 --> 34:36.240
where to start, and so those suite of models are a little bit different and they rely

34:36.240 --> 34:42.000
more on structural information about metabolism than experimental information, which can

34:42.000 --> 34:45.840
happen later, after we've collected a bunch of information from experiments.

34:45.840 --> 34:48.880
Okay, all right, very cool, very cool.

34:48.880 --> 34:52.040
Thank you so much, everyone, for taking the time to jump on the podcast with me.

34:52.040 --> 34:53.680
Yeah, thanks so much for having me.

34:53.680 --> 34:55.680
There's a lot of fun, thank you.

34:55.680 --> 34:56.680
Yeah.

34:56.680 --> 35:02.680
All right, everyone, that's our show for today.

35:02.680 --> 35:07.880
Thanks so much for listening and for your continued support of this podcast.

35:07.880 --> 35:12.320
For the notes for this episode, or for any feedback or questions, please leave a comment

35:12.320 --> 35:18.000
on the show notes page at twimmaleye.com slash talk slash 41.

35:18.000 --> 35:23.520
Thanks again to Cloud Era, our sponsor for the Rangle Conference series of podcasts.

35:23.520 --> 35:28.400
To learn more about Cloud Era and the company's data science workbench products, visit them

35:28.400 --> 35:34.760
at cloudera.com and be sure to tweet at them using at cloud era to thank them for their

35:34.760 --> 35:37.120
support of this podcast.

35:37.120 --> 35:41.800
If you're interested in joining the twimmale online meetup, we'll discuss research papers

35:41.800 --> 35:46.320
like Apple's recent paper on generative adversarial networks.

35:46.320 --> 35:50.400
You can register for that at twimmaleye.com slash meetup.

35:50.400 --> 35:56.280
And don't forget to sign up for the newsletter at twimmaleye.com slash newsletter.

35:56.280 --> 35:58.520
Thanks again for listening and catch you next time.

