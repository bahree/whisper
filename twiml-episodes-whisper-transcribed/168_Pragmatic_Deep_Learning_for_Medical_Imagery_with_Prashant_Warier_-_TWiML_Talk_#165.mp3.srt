1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,440
I'm your host Sam Charrington.

4
00:00:31,440 --> 00:00:35,880
I'd like to start today's show with a huge shout out for everyone who's taken the time

5
00:00:35,880 --> 00:00:39,280
to vote for us in the People's Choice Podcast Awards.

6
00:00:39,280 --> 00:00:43,000
It's been awesome to hear from those of you who have loved words of encouragement and

7
00:00:43,000 --> 00:00:46,000
we certainly appreciate all the love.

8
00:00:46,000 --> 00:00:50,800
If you're a fan of the pod and you've already voted, we'd also like to encourage you to

9
00:00:50,800 --> 00:00:56,080
head over to your Apple or Google podcast app or wherever you listen to podcasts and

10
00:00:56,080 --> 00:00:58,920
leave us a five star rating or review.

11
00:00:58,920 --> 00:01:05,200
They are super helpful as we push to grow this show and community.

12
00:01:05,200 --> 00:01:11,880
In this episode, I'm joined by Prashant Warrior, CEO and co-founder of cure.ai, a company

13
00:01:11,880 --> 00:01:15,640
building AI-powered software for radiology.

14
00:01:15,640 --> 00:01:19,920
In our conversation, Prashant and I discussed the company's work building products for

15
00:01:19,920 --> 00:01:23,560
interpreting head CT scans and chest X-rays.

16
00:01:23,560 --> 00:01:27,560
Prashant shares with us some great insights into some of the things he and his team have

17
00:01:27,560 --> 00:01:33,560
learned in bringing a commercial product to market in this space, including the gap between

18
00:01:33,560 --> 00:01:38,760
academic research papers and commercially viable software, the challenge of data acquisition

19
00:01:38,760 --> 00:01:44,320
and how to best capitalize on what data you do have access to and much more.

20
00:01:44,320 --> 00:01:48,520
We also touch on the application of transfer learning in this space and the algorithms

21
00:01:48,520 --> 00:01:53,840
and annotation pipelines, Cure has developed to support 3D scans.

22
00:01:53,840 --> 00:01:55,520
Let's get to it.

23
00:01:55,520 --> 00:01:58,760
All right, everyone.

24
00:01:58,760 --> 00:02:01,040
I am on the line with Prashant Warrior.

25
00:02:01,040 --> 00:02:04,000
Prashant is the CEO of cure.ai.

26
00:02:04,000 --> 00:02:06,880
Prashant, welcome to this week in machine learning and AI.

27
00:02:06,880 --> 00:02:08,480
Great to be here, Sam.

28
00:02:08,480 --> 00:02:10,160
Thank you so much for inviting me.

29
00:02:10,160 --> 00:02:11,160
Absolutely.

30
00:02:11,160 --> 00:02:14,400
Let's get started by having you tell the audience a little bit about your background

31
00:02:14,400 --> 00:02:19,000
and how you got involved in working in ML and AI.

32
00:02:19,000 --> 00:02:20,000
Great.

33
00:02:20,000 --> 00:02:25,600
I have been working in the applied math space for a long time.

34
00:02:25,600 --> 00:02:29,240
My undergraduate degree was in operations research.

35
00:02:29,240 --> 00:02:33,640
I got a PhD in operations research and then started working on it.

36
00:02:33,640 --> 00:02:35,640
At that time, this was not part of data science.

37
00:02:35,640 --> 00:02:38,480
There was no field like data science when I did my PhD.

38
00:02:38,480 --> 00:02:44,360
But after my PhD, which was in our research, where we were basically optimizing

39
00:02:44,360 --> 00:02:49,720
truck routes, so figuring out how shipments should be routed from origin to destination,

40
00:02:49,720 --> 00:02:53,560
how we should assign drivers to trucks, and then again, there are lots of rules around

41
00:02:53,560 --> 00:02:57,280
how many hours a driver can drive and the duty time that it has.

42
00:02:57,280 --> 00:03:02,640
So I mean incorporating all those rules to figure out how drivers should be allocated

43
00:03:02,640 --> 00:03:03,640
to trucks.

44
00:03:03,640 --> 00:03:04,640
I mean, assigned to trucks.

45
00:03:04,640 --> 00:03:08,440
So those are the kinds of problems that I was solving during my PhD, which were primarily

46
00:03:08,440 --> 00:03:10,680
integer programming problems.

47
00:03:10,680 --> 00:03:14,920
So I did a lot of work around optimization then.

48
00:03:14,920 --> 00:03:20,920
And then immediately after that, I started working on price optimization and McDonald optimization

49
00:03:20,920 --> 00:03:25,240
demand forecasting for, I started working for SAP.

50
00:03:25,240 --> 00:03:28,480
And so there, I mean, we were working on these problems in the retail space, retail and

51
00:03:28,480 --> 00:03:29,720
fashion retail space.

52
00:03:29,720 --> 00:03:33,120
So this, I mean demand forecasting, some of these problems are very much, I mean, they

53
00:03:33,120 --> 00:03:35,360
are basically your dependent variable as a sales.

54
00:03:35,360 --> 00:03:39,920
And then you have a bunch of independent variables, which are, I mean, which again depend

55
00:03:39,920 --> 00:03:41,640
upon the kind of product that you're selling.

56
00:03:41,640 --> 00:03:45,240
I mean, so a fashion product will have very different independent variables than if you

57
00:03:45,240 --> 00:03:49,600
look at, for example, products which are retail, I mean, typical retail products like a can

58
00:03:49,600 --> 00:03:53,720
of milk, for example, or fruits or vegetables, right?

59
00:03:53,720 --> 00:03:56,600
So we were sort of worked on that for a while.

60
00:03:56,600 --> 00:04:02,520
And then I mean, I was in the US at that time, came back to India and then I was, I set

61
00:04:02,520 --> 00:04:09,160
up a company, which is called Imagna, which was basically using machine learning to understand

62
00:04:09,160 --> 00:04:10,720
customer behavior from cookie data.

63
00:04:10,720 --> 00:04:16,440
So we were basically dropping cookies at, on multiple e-commerce websites and getting

64
00:04:16,440 --> 00:04:20,440
a lot of detailed information about how customers are behaving on those websites.

65
00:04:20,440 --> 00:04:24,440
So you can actually see how they buy clothing, for example, what products they looked at,

66
00:04:24,440 --> 00:04:27,840
what kind of sizes they looked at, what colors they looked at, what styles they looked

67
00:04:27,840 --> 00:04:31,720
at, and all of that, or for example, I mean, integrating, for example, what, what travel

68
00:04:31,720 --> 00:04:35,640
sites they went into, I mean, how they, where they're traveling to, what travel locations

69
00:04:35,640 --> 00:04:38,160
they've looked at, or what kind of movie shows they're watching.

70
00:04:38,160 --> 00:04:45,320
So we had a very consolidated set of data from cookies, which we were then utilized to do

71
00:04:45,320 --> 00:04:48,960
real-time bidding on ads, on basically online ads.

72
00:04:48,960 --> 00:04:51,320
And we had a built a real-time bidding platform.

73
00:04:51,320 --> 00:04:55,560
And that was then, I mean, so that was again, I mean, yeah, even I was, I was heading

74
00:04:55,560 --> 00:04:56,560
that company.

75
00:04:56,560 --> 00:04:59,840
So I was also, I mean, playing more of a commercial role in that.

76
00:04:59,840 --> 00:05:06,600
And then that got acquired by the organization that I currently work for called Tractil Analytics.

77
00:05:06,600 --> 00:05:08,840
I acquired my previous company.

78
00:05:08,840 --> 00:05:11,880
And then I joined Fractil Active Data Scientist.

79
00:05:11,880 --> 00:05:16,720
And in that role, again, I mean, Active Data Scientist, I was working on some problems.

80
00:05:16,720 --> 00:05:21,520
And then we sort of identified there is a large gap in the radiology space where we felt

81
00:05:21,520 --> 00:05:25,440
that deep learning could make a substantial impact by automating interpretation of images.

82
00:05:25,440 --> 00:05:31,160
So that's how I sort of transitioned from the Active Data Scientist role at Fractil

83
00:05:31,160 --> 00:05:33,920
into becoming the CEO of QR.AI.

84
00:05:33,920 --> 00:05:37,400
And I've been working on building this over the last two and a half years.

85
00:05:37,400 --> 00:05:39,640
And so that's, that's, that's my story.

86
00:05:39,640 --> 00:05:46,200
And Cure is currently in the process of rolling out a kind of a packet solution focused on

87
00:05:46,200 --> 00:05:48,960
chest radiology, is that right?

88
00:05:48,960 --> 00:05:53,480
So we have exactly, I mean, so one of our solutions is in chest radiology, which basically

89
00:05:53,480 --> 00:05:57,200
can automatically interpret chest X-rays.

90
00:05:57,200 --> 00:06:01,480
And a second solution is on head CD scans.

91
00:06:01,480 --> 00:06:06,040
So which is basically automatically interpreting head CD scans, those are the two main solutions

92
00:06:06,040 --> 00:06:07,040
we have built.

93
00:06:07,040 --> 00:06:10,240
I mean, there are some more solutions that we've built over the last several years.

94
00:06:10,240 --> 00:06:13,760
But we, I mean, from a commercial perspective, we focus on these two.

95
00:06:13,760 --> 00:06:19,920
And so there's been over the past few years, there've been a ton of work.

96
00:06:19,920 --> 00:06:24,880
You know, in a lot of ways, it feels like a kind of gold rush in the medical field.

97
00:06:24,880 --> 00:06:30,200
When you look at the research journals, it's, hey, we've got this new tool, CNN, let's

98
00:06:30,200 --> 00:06:36,120
apply it to radiological image type A, type B, type C. There's a, you know, a series of

99
00:06:36,120 --> 00:06:37,400
these types of papers.

100
00:06:37,400 --> 00:06:41,760
I'm curious what you've learned about the gap between what you might read in a paper

101
00:06:41,760 --> 00:06:46,120
and actually bringing a product to market.

102
00:06:46,120 --> 00:06:47,840
So that's a, that's a very good question, Sam.

103
00:06:47,840 --> 00:06:51,440
I think I think I have a very strong point of view on that.

104
00:06:51,440 --> 00:06:55,000
I figured you might.

105
00:06:55,000 --> 00:06:57,640
So I mean, there are lots of papers.

106
00:06:57,640 --> 00:07:01,040
I mean, because CNN's, I mean, can read images really well, right?

107
00:07:01,040 --> 00:07:04,560
And so people tell that, okay, I'll just take a CNN and apply it to this medical data

108
00:07:04,560 --> 00:07:06,280
that I have and I'll get good results.

109
00:07:06,280 --> 00:07:08,400
And they do get good results because they're overfitting on the data.

110
00:07:08,400 --> 00:07:09,720
So you have 2000 images.

111
00:07:09,720 --> 00:07:14,760
Let's say you have got 2000 X-rays to train your algorithm on and you sort of label them.

112
00:07:14,760 --> 00:07:16,400
You label each of those images as normal.

113
00:07:16,400 --> 00:07:17,800
Let's say these are X-rays, right?

114
00:07:17,800 --> 00:07:19,680
You label them as normal or abnormal.

115
00:07:19,680 --> 00:07:24,720
And you order on that and you will get some, I mean, you basically put some as validation

116
00:07:24,720 --> 00:07:25,800
testing and so on.

117
00:07:25,800 --> 00:07:29,880
And you get good results, but what happens is that especially with X-rays, right?

118
00:07:29,880 --> 00:07:31,560
There is a huge variety of data.

119
00:07:31,560 --> 00:07:36,320
I mean, if you go from a Philips machine to a GE machine or you go from one set into

120
00:07:36,320 --> 00:07:40,280
another, one center to another, I mean, there are different settings, different in machines.

121
00:07:40,280 --> 00:07:44,480
There is a wide variation and what, what happens typically is you take a data set, you train

122
00:07:44,480 --> 00:07:48,360
on the data set, you validate, you test, I mean, you do all the right things.

123
00:07:48,360 --> 00:07:51,640
But when you take that algorithm and you try to generalize to a new data set, it does

124
00:07:51,640 --> 00:07:53,040
not work well at all.

125
00:07:53,040 --> 00:07:54,040
And we saw this initially.

126
00:07:54,040 --> 00:07:57,600
And when we started working out, working on this problem, on the chest X-ray problem,

127
00:07:57,600 --> 00:08:03,480
we had around 25,000 scans, 25,000 X-rays along with their reports.

128
00:08:03,480 --> 00:08:09,200
So we did what we did is we took the reports, we ran NLP on those reports, some rule-based

129
00:08:09,200 --> 00:08:13,480
NLP, not machine learning base, but rule-based natural language processing, which basically

130
00:08:13,480 --> 00:08:16,600
extracted out the abnormality from the report.

131
00:08:16,600 --> 00:08:20,280
So if, for example, one of the abnormalities is called floral effusion, where it's fluid

132
00:08:20,280 --> 00:08:21,280
in the lungs.

133
00:08:21,280 --> 00:08:25,560
We said we will extract out a bunch of abnormalities, so we trained the algorithms to do that,

134
00:08:25,560 --> 00:08:26,960
to train these NLP algorithms.

135
00:08:26,960 --> 00:08:31,320
So now they've got an X-ray and the corresponding abnormality, the chest X-ray and the corresponding

136
00:08:31,320 --> 00:08:32,320
abnormality.

137
00:08:32,320 --> 00:08:34,640
And we trained our models on that.

138
00:08:34,640 --> 00:08:39,520
And then what we saw is that when we had a new data set, that was trained on 25,000

139
00:08:39,520 --> 00:08:40,520
images.

140
00:08:40,520 --> 00:08:44,160
And when we got a new data set, it did not translate at all, we were getting around

141
00:08:44,160 --> 00:08:49,600
90% accuracy on the first data set, on the second data set that we got from another hospital,

142
00:08:49,600 --> 00:08:52,360
we were somewhere in the 60s, I think.

143
00:08:52,360 --> 00:08:55,040
And so it was, I mean, substantially lower.

144
00:08:55,040 --> 00:09:00,200
And then we figured that I think, I mean, there is a lot of variation in the data, and then

145
00:09:00,200 --> 00:09:01,600
that's also, that's one challenge.

146
00:09:01,600 --> 00:09:05,920
And of course, I mean, over the last 20,500 years, we have increased our X-ray database

147
00:09:05,920 --> 00:09:10,120
from that to 25,000 number to around 1.5 plus million.

148
00:09:10,120 --> 00:09:15,640
So today, we have a huge amount of data, so it sort of has learned a wide variety of data

149
00:09:15,640 --> 00:09:19,920
patterns that it sees, I mean, acquisition data, acquisition patterns, because X-ray acquisition

150
00:09:19,920 --> 00:09:21,720
can occur, I mean, in many different ways.

151
00:09:21,720 --> 00:09:26,040
And so it has learned a lot of these acquisition patterns, learned about a lot of different machines

152
00:09:26,040 --> 00:09:27,400
that are generating those X-rays.

153
00:09:27,400 --> 00:09:29,000
So that makes it a little bit more generalizable.

154
00:09:29,000 --> 00:09:32,080
I mean, I think the more amount of data that you have, it becomes more generalized.

155
00:09:32,080 --> 00:09:38,120
You mentioned using NLP to pull information out of reports or records.

156
00:09:38,120 --> 00:09:43,960
Is this kind of data mining, the electronic medical records to produce labels for your

157
00:09:43,960 --> 00:09:44,960
data set?

158
00:09:44,960 --> 00:09:46,600
Or was there something else happening?

159
00:09:46,600 --> 00:09:48,560
It's not electronic medical records.

160
00:09:48,560 --> 00:09:53,680
So it's basically, I mean, for, I mean, you could think of it as a medical record, but

161
00:09:53,680 --> 00:09:56,640
basically for every radiology image, you will have a radiology report.

162
00:09:56,640 --> 00:09:58,880
It's part of radiology information systems.

163
00:09:58,880 --> 00:10:02,200
So you can, so you are basically just putting that radiology report.

164
00:10:02,200 --> 00:10:03,960
You're not pulling any more medical records.

165
00:10:03,960 --> 00:10:05,800
So ideally, I mean, a patient will have a lot more.

166
00:10:05,800 --> 00:10:09,320
I mean, they will have the history of why they came to the hospital, why they took strain

167
00:10:09,320 --> 00:10:10,320
the first place.

168
00:10:10,320 --> 00:10:14,000
And then maybe there is some microbiological test, which gets done afterwards, where they

169
00:10:14,000 --> 00:10:15,760
are diagnosed with some disease, right?

170
00:10:15,760 --> 00:10:17,840
A radiology report may not contain all of that.

171
00:10:17,840 --> 00:10:21,760
It will basically tell you that I found these patterns on the X-ray.

172
00:10:21,760 --> 00:10:25,040
For example, it might say that I found a fluid effusion, which is basically some fluid

173
00:10:25,040 --> 00:10:28,200
in the lungs, or that could be due to tuberculosis or other disease.

174
00:10:28,200 --> 00:10:30,040
But a radiologist will not report on that.

175
00:10:30,040 --> 00:10:32,520
They will report on what they see in the X-ray.

176
00:10:32,520 --> 00:10:34,840
So it's more visible, visible features.

177
00:10:34,840 --> 00:10:35,840
Okay.

178
00:10:35,840 --> 00:10:41,080
These radiology reports, are they coded or are they, is it just kind of natural text notes

179
00:10:41,080 --> 00:10:43,000
from the radiologists?

180
00:10:43,000 --> 00:10:44,560
They are natural text.

181
00:10:44,560 --> 00:10:48,960
I mean, sometimes, I mean, again, every hospital will have their own format template for reporting.

182
00:10:48,960 --> 00:10:53,720
There are some standard plates recommended by the radiology society of North America.

183
00:10:53,720 --> 00:11:00,920
But again, I mean, I think a lot of people don't use those templates, I mean, but so to

184
00:11:00,920 --> 00:11:05,720
answer your question, I mean, they are very varied, but they have some structure to it.

185
00:11:05,720 --> 00:11:13,800
These questions may be inspired by an image that you have on the Cure website.

186
00:11:13,800 --> 00:11:18,760
You're basically looking at a brain scan on the monitor.

187
00:11:18,760 --> 00:11:25,000
I'm wondering if setting aside machine learning and AI, are we getting to a point where radiologists

188
00:11:25,000 --> 00:11:31,920
are looking at these images digitally and creating bounding boxes and electronically

189
00:11:31,920 --> 00:11:38,360
notating these images in a way that will make it easier for future ML and AI applications

190
00:11:38,360 --> 00:11:42,560
or is it still all manual today?

191
00:11:42,560 --> 00:11:43,560
I mean, absolutely.

192
00:11:43,560 --> 00:11:49,360
I think what we are seeing is that definitely there is, I mean, a lot of work going into

193
00:11:49,360 --> 00:11:51,840
words, creating annotated data sets.

194
00:11:51,840 --> 00:11:53,560
And so a lot of hospitals are working on that.

195
00:11:53,560 --> 00:11:57,160
I mean, I think a lot of the hospitals in the US, I mean, for example, a mass general

196
00:11:57,160 --> 00:12:01,000
hospital, they have their own data science teams.

197
00:12:01,000 --> 00:12:03,120
And those teams are also working on similar problems.

198
00:12:03,120 --> 00:12:07,880
So there is definitely a lot of effort going towards creating these annotated data sets.

199
00:12:07,880 --> 00:12:10,320
So I would not say that, I mean, a lot of radiologists are working on it.

200
00:12:10,320 --> 00:12:15,520
I think focused groups in different parts of the world are working on these kinds of annotations.

201
00:12:15,520 --> 00:12:17,960
And it's not something which is completely standardized yet.

202
00:12:17,960 --> 00:12:18,960
Right.

203
00:12:18,960 --> 00:12:22,920
But it sounds like you're also saying that it's not, you know, we're not necessarily seeing

204
00:12:22,920 --> 00:12:29,840
the radiologists tool change to be more like a data annotation exercise.

205
00:12:29,840 --> 00:12:34,000
It's happening separately and it's some data science team that may be, you know, doing

206
00:12:34,000 --> 00:12:36,200
some of our things to what you were doing.

207
00:12:36,200 --> 00:12:37,200
Exactly.

208
00:12:37,200 --> 00:12:38,680
Because I mean, I'll tell you why.

209
00:12:38,680 --> 00:12:39,680
Right.

210
00:12:39,680 --> 00:12:44,640
So for example, one of the things we had to do was to when we were trying to detect bleeds

211
00:12:44,640 --> 00:12:49,920
from, I mean, bleeds from head CT scans, they had to actually go in or not as, but I

212
00:12:49,920 --> 00:12:55,160
mean, trained specialists had to go in and mark out the bleed on those scans.

213
00:12:55,160 --> 00:12:58,880
And that's a time-taking process because the CT scan of the head will somewhere between

214
00:12:58,880 --> 00:13:00,800
50 to 100 slices.

215
00:13:00,800 --> 00:13:04,240
And for each slice, there'll be either there's some, some of those slices, obviously, will

216
00:13:04,240 --> 00:13:05,240
not have a bleed.

217
00:13:05,240 --> 00:13:09,440
Or maybe all of them don't have a bleed, but some, you have a bleed, you have to actually

218
00:13:09,440 --> 00:13:12,480
mark the boundary of the bleed, which is not a continuous boundary.

219
00:13:12,480 --> 00:13:16,520
I mean, it's sort of a ragged boundary and then you have to mark through, mark all of

220
00:13:16,520 --> 00:13:17,520
that.

221
00:13:17,520 --> 00:13:19,880
That's time-consuming for radiologists.

222
00:13:19,880 --> 00:13:24,240
And so it will not be easy for somebody to sort of incorporate that into the radiology

223
00:13:24,240 --> 00:13:28,120
workflow because they want to see a scan, they want to report on it in a few minutes and

224
00:13:28,120 --> 00:13:29,120
then be done with that.

225
00:13:29,120 --> 00:13:32,160
And marking out the boundaries will take them maybe 20, 30 minutes.

226
00:13:32,160 --> 00:13:36,800
So I don't think some of these are easily incorporated into the workflow, which is why we sort

227
00:13:36,800 --> 00:13:41,040
of started doing this national language processing because we said there is a lot of data already

228
00:13:41,040 --> 00:13:45,480
available in the report because I am saying that, I mean, the radiologist is already writing

229
00:13:45,480 --> 00:13:49,040
that there is this plural effusion in this part of the lung.

230
00:13:49,040 --> 00:13:53,640
So we can extract out all that information from those reports and use that for training.

231
00:13:53,640 --> 00:13:57,560
So rather than getting them to sort of manually mark out these abnormalities.

232
00:13:57,560 --> 00:14:01,160
So you also mentioned these different acquisition patterns.

233
00:14:01,160 --> 00:14:06,920
These are related to the different types of radiology machines.

234
00:14:06,920 --> 00:14:09,000
These are radiology machines.

235
00:14:09,000 --> 00:14:14,600
The amount of exposure, I mean, amount of radiation that you produce.

236
00:14:14,600 --> 00:14:16,800
I mean, so there are, I mean, everybody will have a different setting.

237
00:14:16,800 --> 00:14:18,960
I mean, so these machines also have some settings.

238
00:14:18,960 --> 00:14:23,560
And so there are many different settings and so the, especially for X-rays, I mean, especially

239
00:14:23,560 --> 00:14:27,560
for X-rays, the data from each center is sort of slightly different.

240
00:14:27,560 --> 00:14:30,560
I mean, there is not exactly the same.

241
00:14:30,560 --> 00:14:31,560
Okay.

242
00:14:31,560 --> 00:14:32,560
It's interesting.

243
00:14:32,560 --> 00:14:38,800
I think, you know, there's a tendency to think about the, you know, CNNs or ML and AI

244
00:14:38,800 --> 00:14:44,880
in general as these, you know, maybe give them, give them more credit for being naturally

245
00:14:44,880 --> 00:14:47,680
generalized than they are.

246
00:14:47,680 --> 00:14:52,960
My daughter is doing an internship with the podcast currently where she's taking podcasts

247
00:14:52,960 --> 00:14:58,000
that we've already recorded and running them through automated transcription and looking

248
00:14:58,000 --> 00:15:03,120
at a bunch of different services and kind of rating their performance and, you know,

249
00:15:03,120 --> 00:15:09,200
over time, we've learned that some of the services do better with, you know, phone calls,

250
00:15:09,200 --> 00:15:14,840
some of them do better with podcasts, some of them do better with audio that's recorded

251
00:15:14,840 --> 00:15:20,760
in a room like the, the characteristics, the, you know, very subtle characteristics of

252
00:15:20,760 --> 00:15:27,840
the input data have a, a huge impact on the algorithm's ability to perform all and extract

253
00:15:27,840 --> 00:15:32,520
texts from them, much more so than you might think.

254
00:15:32,520 --> 00:15:37,680
And it sounds like you've, you've had very similar experiences on the, on the input side

255
00:15:37,680 --> 00:15:39,040
with these radiological images.

256
00:15:39,040 --> 00:15:44,160
So I have a very interesting story in this, I mean, so that's, when we started working

257
00:15:44,160 --> 00:15:47,840
on X-rays, this probably around two years back, so we, we were sort of still tying up

258
00:15:47,840 --> 00:15:52,280
with hospitals, we did not have any, any hospital data, any, any real data coming from our

259
00:15:52,280 --> 00:15:53,280
collaborators.

260
00:15:53,280 --> 00:15:55,960
So we were, we said, we will mine the internet for data.

261
00:15:55,960 --> 00:16:00,600
So we looked at some internet sources, we collected some 10,000 images and we said, let's,

262
00:16:00,600 --> 00:16:05,280
let's try to train a model on this and train the model on that and that was very accurate.

263
00:16:05,280 --> 00:16:09,080
I mean, it was super accurate in determining normal from abnormal.

264
00:16:09,080 --> 00:16:11,600
So it was classifying normal versus abnormal.

265
00:16:11,600 --> 00:16:15,080
And then we said, I mean, I mean, this is a very small amount of data, right?

266
00:16:15,080 --> 00:16:18,800
I mean, typically if you're looking at images, you'll look at millions, millions of images

267
00:16:18,800 --> 00:16:19,800
to train algorithms.

268
00:16:19,800 --> 00:16:20,800
Let me say it.

269
00:16:20,800 --> 00:16:21,800
Let's, let's see.

270
00:16:21,800 --> 00:16:23,280
I mean, so we look, we create an attribution algorithm.

271
00:16:23,280 --> 00:16:28,080
So we were attributing the figuring out which, I mean, if you attribute what exactly the

272
00:16:28,080 --> 00:16:30,040
algorithm is learning, you can find out boxes.

273
00:16:30,040 --> 00:16:34,000
So we can basically, for example, there is a something called occlusion, occlusion based,

274
00:16:34,000 --> 00:16:38,640
occlusion based attribution where you can black out one box, one small box within the

275
00:16:38,640 --> 00:16:43,320
whole image and then see what is the attribution of that to the prediction.

276
00:16:43,320 --> 00:16:47,480
So if you, if you black out a box because of which the algorithm is saying that that particular

277
00:16:47,480 --> 00:16:52,960
x-rays abnormal, then suddenly it becomes normal, the prediction changes from normal to abnormal.

278
00:16:52,960 --> 00:16:57,560
And you look at how each box is impacting the probability of being normal or abnormal.

279
00:16:57,560 --> 00:17:01,360
So the boxes which have the highest impact will have the highest difference between the

280
00:17:01,360 --> 00:17:04,000
normal versus abnormal probability when they are blacked out.

281
00:17:04,000 --> 00:17:09,040
So we created these attribution methods and we were trying to attribute the algorithm,

282
00:17:09,040 --> 00:17:10,320
what the algorithm was learning.

283
00:17:10,320 --> 00:17:15,280
And we found that the algorithm actually was learning to distinguish something very simple

284
00:17:15,280 --> 00:17:17,720
because what was there, these were internet images.

285
00:17:17,720 --> 00:17:21,680
So typically, all the abnormal images had a lot of text on them.

286
00:17:21,680 --> 00:17:26,760
So some typing or some written text and the normal images did not have any annotation,

287
00:17:26,760 --> 00:17:29,120
any kind of text or anything like that.

288
00:17:29,120 --> 00:17:32,120
So it was basically any to recognize typewritten letters.

289
00:17:32,120 --> 00:17:33,640
I mean, that's what it's pointing to.

290
00:17:33,640 --> 00:17:36,920
So if it sees typewritten letters, it will assume that it's abnormal.

291
00:17:36,920 --> 00:17:39,040
If it doesn't see typewritten letters, it's normal.

292
00:17:39,040 --> 00:17:40,800
So that is what it was learning.

293
00:17:40,800 --> 00:17:43,000
And so these algorithms are very, very good.

294
00:17:43,000 --> 00:17:46,400
I mean, they're very smart, but also they can learn very different things.

295
00:17:46,400 --> 00:17:49,000
I mean, then what do you expect them to learn?

296
00:17:49,000 --> 00:17:54,600
So you've learned the importance of real data and these different input patterns.

297
00:17:54,600 --> 00:17:59,720
And you've kind of learned some things about the model's ability to generalize across

298
00:17:59,720 --> 00:18:00,720
this.

299
00:18:00,720 --> 00:18:01,720
What else have you learned?

300
00:18:01,720 --> 00:18:06,840
See, other things that we have learned are also that, I mean, if you look at, when we

301
00:18:06,840 --> 00:18:14,440
look at all the research in image and all of them are 224 by 224 images, because that's

302
00:18:14,440 --> 00:18:15,440
the image net size.

303
00:18:15,440 --> 00:18:19,640
And we look at most research is about that size of image.

304
00:18:19,640 --> 00:18:24,240
And if you look at an x ray, I mean, typical chest x ray will be around 4,000 by 4,000 pixels.

305
00:18:24,240 --> 00:18:28,760
So if you are downsizing, so initially, when we started out doing research, we started

306
00:18:28,760 --> 00:18:34,520
with taking that 4,000 by 4,000 or I mean, that size image and downsizing it to, to

307
00:18:34,520 --> 00:18:39,600
2, 2, 2, 2, 2, 3, 4, 24, 24, which basically takes away a lot of the detail that you're looking

308
00:18:39,600 --> 00:18:40,600
at.

309
00:18:40,600 --> 00:18:43,880
I mean, so you cannot, I mean, obviously, if you have a lot of data, it'll, it'll

310
00:18:43,880 --> 00:18:44,880
learn.

311
00:18:44,880 --> 00:18:48,680
Well, I mean, it will be able to detect really large abnormalities, it'll not be able to detect

312
00:18:48,680 --> 00:18:49,920
subtle abnormalities.

313
00:18:49,920 --> 00:18:55,440
And so then we had to devise our own methods to sort of work with higher resolution images.

314
00:18:55,440 --> 00:18:58,000
So today, we are able to work with full resolution images.

315
00:18:58,000 --> 00:19:01,520
So we had to sort of look at patches at a time and do a lot of different techniques.

316
00:19:01,520 --> 00:19:05,760
I mean, some of these are our own IP, so we have not published it yet.

317
00:19:05,760 --> 00:19:09,600
So I cannot reveal it, but there is, there's a lot of work which we have done which sort

318
00:19:09,600 --> 00:19:13,800
of enables us to look at the full size image rather than looking at these down sampled

319
00:19:13,800 --> 00:19:14,800
images.

320
00:19:14,800 --> 00:19:17,640
And that's all the backs of what I see, I mean, out there in terms of literature.

321
00:19:17,640 --> 00:19:24,080
I mean, everybody is taking these standard sort of dense nets or resonates and then applying

322
00:19:24,080 --> 00:19:26,040
them to the down sampled images.

323
00:19:26,040 --> 00:19:30,560
And that also loses a lot of information from that original image.

324
00:19:30,560 --> 00:19:35,120
So that's, if you look at most of the literature, that's what is happening right now.

325
00:19:35,120 --> 00:19:40,960
Is there anything that you can say generally about the approach that you've taken to look

326
00:19:40,960 --> 00:19:42,760
at the large images?

327
00:19:42,760 --> 00:19:49,160
Are you like windowing across the images or any hints you can give us?

328
00:19:49,160 --> 00:19:50,920
So it's a patch based approach.

329
00:19:50,920 --> 00:19:58,360
I mean, we look at patches, we determine some patches and so that's, that's all I can

330
00:19:58,360 --> 00:19:59,360
see.

331
00:19:59,360 --> 00:20:05,080
There is definitely some very, very strong IP in there which sort of, because I don't

332
00:20:05,080 --> 00:20:07,880
think anybody right now is working on large images.

333
00:20:07,880 --> 00:20:11,360
I think everybody else is working on smaller sized images.

334
00:20:11,360 --> 00:20:17,920
And we also tried, I mean, so we also tried training these, I mean, larger sized using the

335
00:20:17,920 --> 00:20:22,360
typical dense nets, and it sort of doesn't perform as well.

336
00:20:22,360 --> 00:20:24,200
I mean, or sometimes it doesn't converge at all.

337
00:20:24,200 --> 00:20:27,720
So we saw lots of convergence problems with larger sized images.

338
00:20:27,720 --> 00:20:31,720
And do you have any intuition for why that is?

339
00:20:31,720 --> 00:20:38,120
I don't actually, I don't, I mean, we have tried that and I really don't have clue why

340
00:20:38,120 --> 00:20:39,400
why that doesn't work typically.

341
00:20:39,400 --> 00:20:42,040
I mean, so not sure.

342
00:20:42,040 --> 00:20:43,040
But there is not much testing.

343
00:20:43,040 --> 00:20:46,400
I mean, honestly, when I look at research, there is not much testing that has been done

344
00:20:46,400 --> 00:20:52,080
on larger images because most of the work, I mean, 224 by 224 is a decent size for your

345
00:20:52,080 --> 00:20:53,080
regular images.

346
00:20:53,080 --> 00:20:56,920
So that's, that's sort of, I don't, I don't see that there is a lot of work done in

347
00:20:56,920 --> 00:20:58,600
that, in that area.

348
00:20:58,600 --> 00:21:05,560
In your approach and or the literature that you've seen, does transfer learning play a lot

349
00:21:05,560 --> 00:21:10,800
or are you looking at pre-trained models or models, you know, that are pre-trained with

350
00:21:10,800 --> 00:21:14,560
image net or some other data set, or are you just training from scratch?

351
00:21:14,560 --> 00:21:21,360
So we have, we found that a pre-training with ImageNet and then using that on our data

352
00:21:21,360 --> 00:21:24,840
set running, I mean, using pre-training on image net did not improve performance on

353
00:21:24,840 --> 00:21:26,760
the on our data sets.

354
00:21:26,760 --> 00:21:30,840
But what we found is that there is opportunity to do transfer learning within the domain.

355
00:21:30,840 --> 00:21:35,800
So I could basically learn from other types of x-rays and use that learning to test x-rays.

356
00:21:35,800 --> 00:21:39,040
So there is some, some of that learning that we have been able to use.

357
00:21:39,040 --> 00:21:43,280
But again, that the transfer learning there, the impact of that is very small.

358
00:21:43,280 --> 00:21:46,880
I mean, I think it also has to do with the amount of data that we have because we have so

359
00:21:46,880 --> 00:21:47,880
much data.

360
00:21:47,880 --> 00:21:50,520
Probably, I mean, the transfer learning is not that impactful.

361
00:21:50,520 --> 00:21:54,720
But if you have a amount of data, then maybe if you transfer knowledge from other other

362
00:21:54,720 --> 00:21:56,720
domains, it might be more useful.

363
00:21:56,720 --> 00:21:57,720
What else have you learned?

364
00:21:57,720 --> 00:22:03,000
I think, I mean, some of the learnings are about the annotations also.

365
00:22:03,000 --> 00:22:09,720
I mean, so what we have learned is that we can do annotations at multiple levels and

366
00:22:09,720 --> 00:22:14,360
bring, I mean, train models which can do multi-level, so basically there are multiple types

367
00:22:14,360 --> 00:22:15,360
of classification.

368
00:22:15,360 --> 00:22:19,160
So one is that, one is to segment out the abnormality.

369
00:22:19,160 --> 00:22:24,080
So you're actually, the annotation will involve actually going into that image or that slice

370
00:22:24,080 --> 00:22:28,040
of a CD scan and then saying that this is the boundary of the abnormality.

371
00:22:28,040 --> 00:22:33,040
So clearly marking that out, a second type of annotation that we have done is looking

372
00:22:33,040 --> 00:22:35,240
at, for example, on a CD scan, slice by slice.

373
00:22:35,240 --> 00:22:39,240
So you can basically look at one slice and say whether that slice is abnormal or not,

374
00:22:39,240 --> 00:22:43,960
which is a much easier task than actually annotating and going out and marking out the

375
00:22:43,960 --> 00:22:45,240
abnormality on that.

376
00:22:45,240 --> 00:22:50,840
So that's a second type of annotation and then the third type of annotation is at a scan

377
00:22:50,840 --> 00:22:55,000
level, either at the whole CD scan level or at the extra level, using the report, we can

378
00:22:55,000 --> 00:22:58,720
extract out the abnormality on that particular scan.

379
00:22:58,720 --> 00:23:01,920
So I mean, we have models which combine all three of these.

380
00:23:01,920 --> 00:23:05,520
So we can, we have losses, I mean, losses which will take the segmentation losses and

381
00:23:05,520 --> 00:23:10,200
add the classification losses at a slice level and the classification losses at the scan

382
00:23:10,200 --> 00:23:11,680
level and combine all of that.

383
00:23:11,680 --> 00:23:16,560
So those are the models where we can actually take all the data that we have because we

384
00:23:16,560 --> 00:23:21,000
have 350,000 HD scans and we cannot expect to label all of them.

385
00:23:21,000 --> 00:23:26,320
I mean, we cannot expect to sort of, 350,000 HD scans each with, let's say, 100 slices.

386
00:23:26,320 --> 00:23:30,880
So you're talking about 35 million slices.

387
00:23:30,880 --> 00:23:34,360
So we cannot expect to sort of mark out all of them.

388
00:23:34,360 --> 00:23:38,520
So you have to mark out a subset of that and so we have marked out a subset of that.

389
00:23:38,520 --> 00:23:43,160
We have labeled at a slice level, a subset of that and then of course, for each of those

390
00:23:43,160 --> 00:23:47,360
350,000 scans, we have got a report so we have, we can extract out the abnormalities

391
00:23:47,360 --> 00:23:49,520
from those as well.

392
00:23:49,520 --> 00:23:53,280
So we have combined all of this and then the models that perform the best on our data

393
00:23:53,280 --> 00:23:57,080
are ones which combine all this knowledge, all the knowledge from all of these types

394
00:23:57,080 --> 00:23:58,080
of annotation.

395
00:23:58,080 --> 00:24:02,800
So just to clarify then, if you're, you know, when I think about a traditional, like

396
00:24:02,800 --> 00:24:09,120
a resonant type of a model, that's a model that is basically looking at a single image.

397
00:24:09,120 --> 00:24:14,640
Is the implication that you've built out an architecture that, you know, is almost,

398
00:24:14,640 --> 00:24:20,080
I'm almost thinking of it like a 3D type of architecture that is, that takes in multiple

399
00:24:20,080 --> 00:24:25,920
slices and kind of understands the relationship between these slices relative to the whole

400
00:24:25,920 --> 00:24:26,920
scan.

401
00:24:26,920 --> 00:24:27,920
Right.

402
00:24:27,920 --> 00:24:28,920
Right.

403
00:24:28,920 --> 00:24:29,920
Exactly.

404
00:24:29,920 --> 00:24:30,920
Exactly.

405
00:24:30,920 --> 00:24:34,720
Tell me a little bit about the process of defining or coming up with this model architecture.

406
00:24:34,720 --> 00:24:39,520
How did you arrive at the final architecture for doing that?

407
00:24:39,520 --> 00:24:42,320
So I think, I mean, it's, I mean, primarily trial and error.

408
00:24:42,320 --> 00:24:48,400
I mean, reading the latest papers and, I mean, we have, I mean, around 10 plus deep learning

409
00:24:48,400 --> 00:24:49,400
scientists now.

410
00:24:49,400 --> 00:24:53,280
I mean, so, I mean, each person focused on their own problem.

411
00:24:53,280 --> 00:24:58,600
And I mean, I think what, what we have basically been doing is, I mean, looking at what is

412
00:24:58,600 --> 00:25:03,000
the latest research in a physics space and trying to implement that.

413
00:25:03,000 --> 00:25:06,400
I mean, so we've implemented many different techniques from many different papers.

414
00:25:06,400 --> 00:25:07,400
Some of them work.

415
00:25:07,400 --> 00:25:08,400
Some of them don't work.

416
00:25:08,400 --> 00:25:09,400
Or a lot of them don't work.

417
00:25:09,400 --> 00:25:10,400
I mean, very few of them work.

418
00:25:10,400 --> 00:25:13,880
But so a lot of them don't translate directly.

419
00:25:13,880 --> 00:25:18,120
I mean, I think what we have seen is that there are lots of new techniques out there, lots

420
00:25:18,120 --> 00:25:22,800
of new, I mean, which show very promising results on the paper itself shows very promising

421
00:25:22,800 --> 00:25:23,800
results.

422
00:25:23,800 --> 00:25:27,600
But then when we apply it to the domain that we are working in, it doesn't really translate

423
00:25:27,600 --> 00:25:28,600
well.

424
00:25:28,600 --> 00:25:29,600
So we have seen that.

425
00:25:29,600 --> 00:25:33,160
But I mean, typically our architectures are very simple. I mean, in fact, what we have

426
00:25:33,160 --> 00:25:37,080
found is that the simpler you make it, the better it is.

427
00:25:37,080 --> 00:25:42,120
Can you give me an example of something that you tried that was more on the complex side

428
00:25:42,120 --> 00:25:47,000
that you thought would add a lot of value, but it ended up doing something simpler, worked

429
00:25:47,000 --> 00:25:48,000
out better?

430
00:25:48,000 --> 00:25:52,000
You were trying GANs at some point of time to see if we could generate abnormal and normal

431
00:25:52,000 --> 00:25:57,520
X-rays using GANs and then use that for training or use that to do a data augmentation.

432
00:25:57,520 --> 00:26:04,680
And we have tried other techniques, I mean, even some of the techniques from video action

433
00:26:04,680 --> 00:26:08,600
recognition and videos we've tried for our head CD scan technology.

434
00:26:08,600 --> 00:26:12,240
And some of these, I mean, again, some of these have not really worked out.

435
00:26:12,240 --> 00:26:17,000
I mean, I think more complex technology, we have not seen that it sort of translates

436
00:26:17,000 --> 00:26:18,000
to real.

437
00:26:18,000 --> 00:26:22,080
I mean, again, I mean, I would not say that some of the technologies that we developed have

438
00:26:22,080 --> 00:26:23,080
not been complex.

439
00:26:23,080 --> 00:26:27,120
But again, I think in general, I mean, I would not, I would say that a lot of things that

440
00:26:27,120 --> 00:26:28,800
we've tried have not worked out.

441
00:26:28,800 --> 00:26:34,600
Yeah, it sounds like you're, you know, you're describing a world in which we might think

442
00:26:34,600 --> 00:26:43,040
of as applied ML and AI, there's still a lot of research involved in your process for

443
00:26:43,040 --> 00:26:46,000
developing this product suite and bringing it to market.

444
00:26:46,000 --> 00:26:47,000
Absolutely.

445
00:26:47,000 --> 00:26:50,920
I mean, I think there is a huge amount of research, there is a huge amount of domain

446
00:26:50,920 --> 00:26:57,160
knowledge, which we have brought in, I mean, which just makes the algorithm much better.

447
00:26:57,160 --> 00:27:00,000
So I mean, simple things actually make it easier.

448
00:27:00,000 --> 00:27:03,600
I mean, so for example, if you're detecting fractures, if you see, I mean, if you just

449
00:27:03,600 --> 00:27:06,440
train a model on fractures, you'll probably see a lower accuracy.

450
00:27:06,440 --> 00:27:10,160
But if you look at what is around the fracture, I mean, if you also detect bleeds and use

451
00:27:10,160 --> 00:27:14,400
that to, I mean, prove your knowledge about the situation, you'll probably get, we actually

452
00:27:14,400 --> 00:27:16,600
get a better accuracy, better probability.

453
00:27:16,600 --> 00:27:20,400
So I mean, if you have a fracture, you most likely have a bleed in the brain also.

454
00:27:20,400 --> 00:27:24,960
So I think bringing in a lot of domain knowledge is critical in some of the areas that we're

455
00:27:24,960 --> 00:27:25,960
working on.

456
00:27:25,960 --> 00:27:29,680
I mean, for example, detecting tuberculosis, you have to look at certain types of features.

457
00:27:29,680 --> 00:27:32,960
So you have looking at certain things, I mean, on the upper lungs, I mean, so it fits

458
00:27:32,960 --> 00:27:35,960
in the lower part of the lungs, that's not going to be very relevant.

459
00:27:35,960 --> 00:27:40,880
So we have, I mean, brought in a huge amount of radiology knowledge into what the algorithms

460
00:27:40,880 --> 00:27:44,080
have developed and those actually add a lot of value.

461
00:27:44,080 --> 00:27:47,520
And of course, I mean, the research also is the, I mean, we also have to do a lot of research

462
00:27:47,520 --> 00:27:52,320
because as I mentioned earlier, most of the material out there is focused on the types of

463
00:27:52,320 --> 00:27:56,120
images that are much smaller and also two-dimensional, right?

464
00:27:56,120 --> 00:27:59,960
I mean, so you're looking at mostly two-dimensional videos, two-dimensional images.

465
00:27:59,960 --> 00:28:02,800
Only in case of videos, you're looking at something which is not too dimensional, but

466
00:28:02,800 --> 00:28:05,200
at least your sequence of images.

467
00:28:05,200 --> 00:28:10,560
And we look at a CD scan or a MRI, I mean, all of these are three-dimensional images.

468
00:28:10,560 --> 00:28:14,040
You have to sort of get knowledge from all of the slides and bring that all together

469
00:28:14,040 --> 00:28:16,440
to infer a classification there.

470
00:28:16,440 --> 00:28:21,080
So we have to, I mean, congest, I mean, sort of ingest knowledge from these sources, which

471
00:28:21,080 --> 00:28:25,040
again, there is not much literature out there on some of these topics.

472
00:28:25,040 --> 00:28:30,120
So though, of course, I mean, there are lots of people now working on this, on technologies

473
00:28:30,120 --> 00:28:31,520
in the medical space.

474
00:28:31,520 --> 00:28:36,200
But when we started, I mean, which we started around two and a half years back, there were

475
00:28:36,200 --> 00:28:42,000
very few people who were working on deep learning in the healthcare space, which is exponentially

476
00:28:42,000 --> 00:28:44,200
increased in the last couple of years.

477
00:28:44,200 --> 00:28:51,800
And out of curiosity, have you directly applied any of the work or models that originated

478
00:28:51,800 --> 00:28:56,480
in looking at videos to this CT scan problem?

479
00:28:56,480 --> 00:28:57,480
We have.

480
00:28:57,480 --> 00:28:58,480
We have.

481
00:28:58,480 --> 00:29:00,440
Did that work out or was that another dead end or?

482
00:29:00,440 --> 00:29:02,920
Well, I think that that's something that has worked out for us.

483
00:29:02,920 --> 00:29:04,760
I mean, some of these techniques have worked out for us.

484
00:29:04,760 --> 00:29:08,120
But I would not be able to go into much detail on that right now because it's still something

485
00:29:08,120 --> 00:29:13,280
that we are working on and we don't have, I mean, I, yeah, so we have not published anything

486
00:29:13,280 --> 00:29:14,280
on that yet.

487
00:29:14,280 --> 00:29:19,560
The company has been active in publishing some of his research findings.

488
00:29:19,560 --> 00:29:25,200
What are some of the recent work that you've published?

489
00:29:25,200 --> 00:29:27,520
So we have published recently.

490
00:29:27,520 --> 00:29:34,200
We published our work on head CT algorithm, which basically detects in multiple kinds

491
00:29:34,200 --> 00:29:39,520
of bleeds, fractures, midland shift, mass effect from head CT scans.

492
00:29:39,520 --> 00:29:45,240
And this, we sort of published some details of the algorithm or, I mean, this is our,

493
00:29:45,240 --> 00:29:48,680
the earlier version of our algorithm, which is probably around nine, I mean, around a year

494
00:29:48,680 --> 00:29:49,680
old.

495
00:29:49,680 --> 00:29:55,440
And we sort of did a validation study of that on on a data set where, so we, we, we had

496
00:29:55,440 --> 00:30:01,840
a data set of 350,000 of which we had basically a training set, a validation set and a testing

497
00:30:01,840 --> 00:30:02,840
set.

498
00:30:02,840 --> 00:30:07,720
But then what we did is we went out and collected an additional 500 scans, which was from

499
00:30:07,720 --> 00:30:09,880
a very different source, completely new source.

500
00:30:09,880 --> 00:30:14,400
And we had those three sort of reported or basically a radiologist had to go in and mild on

501
00:30:14,400 --> 00:30:17,920
a user interface, whether they found an interapparent primal bleed, whether they found a fracture

502
00:30:17,920 --> 00:30:18,920
and so on.

503
00:30:18,920 --> 00:30:20,280
So just a tick box.

504
00:30:20,280 --> 00:30:24,480
And we had sort of three radiologists do that on those 500 scans.

505
00:30:24,480 --> 00:30:27,320
And we also compared our accuracy against that.

506
00:30:27,320 --> 00:30:35,040
And we found that we were 95% at 0.95 plus you see on all of these, all of these detecting

507
00:30:35,040 --> 00:30:36,640
all of these abnormalities.

508
00:30:36,640 --> 00:30:38,200
Now this is, this is slightly older.

509
00:30:38,200 --> 00:30:41,520
I mean, so the technology that we have published there is maybe around a year old.

510
00:30:41,520 --> 00:30:48,600
Now it is currently, I mean, what we are looking at is around 0.97 plus 0.97 plus on abnormalities.

511
00:30:48,600 --> 00:30:53,280
So it's, it's improved a little bit since we published that work.

512
00:30:53,280 --> 00:30:56,120
Has, have you published any data sets in this space?

513
00:30:56,120 --> 00:30:57,120
Yes.

514
00:30:57,120 --> 00:30:59,800
So we also open sourced this data set of 500 scans.

515
00:30:59,800 --> 00:31:05,640
So it's actually for exactly 491, we have published that, we open sourced that data set along

516
00:31:05,640 --> 00:31:09,440
with the ground truth for that, which is basically the reading of the radiologists.

517
00:31:09,440 --> 00:31:13,520
So I mean, one of the challenges that we found is that everybody claims their own accuracy

518
00:31:13,520 --> 00:31:17,240
and there is no common data set on which people can sort of compare their accuracies.

519
00:31:17,240 --> 00:31:20,480
Like, for example, imagine it is there, you can compare accuracy and imagine it.

520
00:31:20,480 --> 00:31:23,360
There is nothing in the, in the, in most of these space.

521
00:31:23,360 --> 00:31:26,320
I mean, so head CT, we said, let's put together this data set.

522
00:31:26,320 --> 00:31:29,120
And then let's, I mean, we, we published our results for that.

523
00:31:29,120 --> 00:31:33,320
I mean, of course, we anticipate that other people can also use that to publish their results

524
00:31:33,320 --> 00:31:34,680
on that particular data set.

525
00:31:34,680 --> 00:31:37,080
And you said, how many scans in the data set?

526
00:31:37,080 --> 00:31:38,080
491.

527
00:31:38,080 --> 00:31:44,800
And is, is that that number of scans enough to really build interesting models against?

528
00:31:44,800 --> 00:31:48,080
It will be hard to get a very accurate model.

529
00:31:48,080 --> 00:31:50,120
You could start and you could build some models on that.

530
00:31:50,120 --> 00:31:52,240
It, it definitely, you could build models.

531
00:31:52,240 --> 00:31:56,480
I mean, maybe the accuracy will suffer a little bit because it's not very large.

532
00:31:56,480 --> 00:32:02,880
I mean, so basically if you, we have 491 of which, I would say around 300 or so, I mean,

533
00:32:02,880 --> 00:32:05,480
I don't remember the exact numbers, but around 300 of them are abnormal.

534
00:32:05,480 --> 00:32:10,160
And of those abnormalities, if you look at a specific kind of bleed, let's say an extra

535
00:32:10,160 --> 00:32:13,880
dual bleed, there are probably around 40, 50 scans which have got extra dual bleeds.

536
00:32:13,880 --> 00:32:18,760
So that itself may not be enough for you to determine and be able to train a model to determine,

537
00:32:18,760 --> 00:32:20,520
to identify an extra dual bleed.

538
00:32:20,520 --> 00:32:25,640
But this data set, the intention was not for people to train more, more for people to validate.

539
00:32:25,640 --> 00:32:29,840
So you can get and measure your algorithm, which you've trained on another data set.

540
00:32:29,840 --> 00:32:33,800
And you can measure your performance on this data set and you can definitely measure that

541
00:32:33,800 --> 00:32:35,200
to a very high level of accuracy.

542
00:32:35,200 --> 00:32:36,200
Okay.

543
00:32:36,200 --> 00:32:37,400
So it's four of more of a testing data set.

544
00:32:37,400 --> 00:32:38,400
Got it.

545
00:32:38,400 --> 00:32:43,880
And are the scans, these full three-dimensional scans with multiple slices each as well?

546
00:32:43,880 --> 00:32:46,360
Or are they static slices?

547
00:32:46,360 --> 00:32:47,360
They are there three-dimensional.

548
00:32:47,360 --> 00:32:51,440
So they have, I would say, anywhere from 5200 slices per scan.

549
00:32:51,440 --> 00:32:55,640
What kind of uptake have you seen on that, has anyone, have you seen other groups doing

550
00:32:55,640 --> 00:32:58,240
validation against it and publishing their results?

551
00:32:58,240 --> 00:33:03,920
I mean, I have not seen, at least, I have not looked at it in the last month or so.

552
00:33:03,920 --> 00:33:07,800
But I mean, before that, I have not seen, we released this data set in March.

553
00:33:07,800 --> 00:33:10,920
So it's not been a long time.

554
00:33:10,920 --> 00:33:14,440
But we have seen, so I don't know if other people have published, I mean, I have not seen

555
00:33:14,440 --> 00:33:17,280
anybody publish against it as far as I know.

556
00:33:17,280 --> 00:33:22,000
But I have seen, we have seen, I mean, hundreds of people download that data.

557
00:33:22,000 --> 00:33:25,640
And so hopefully we'll see some publications out of that soon enough.

558
00:33:25,640 --> 00:33:31,840
When you're building out these models, so you've looked at head CT scans, you've looked

559
00:33:31,840 --> 00:33:41,040
at chest X-rays, you've looked at skull fractures, is your objective to create a single model

560
00:33:41,040 --> 00:33:49,080
that is able to detect abnormalities in each of these different, very different scenarios

561
00:33:49,080 --> 00:33:56,040
or are you building a suite of very specialized tools that is specially trained and focused

562
00:33:56,040 --> 00:33:59,840
on one particular problem space?

563
00:33:59,840 --> 00:34:04,080
I mean, it's much more, I mean, actually, we are building very, very specialized tools.

564
00:34:04,080 --> 00:34:08,280
In fact, if you look at chest X-ray, we detect 15 different types of abnormalities.

565
00:34:08,280 --> 00:34:11,640
And for each abnormality, we have multiple models.

566
00:34:11,640 --> 00:34:13,240
And so it's an ensemble of many models.

567
00:34:13,240 --> 00:34:17,520
So the number of models that go into even detecting these abnormalities from a single

568
00:34:17,520 --> 00:34:22,520
chest X-ray, on a single chest X-ray, we run around 112 models right now to determine

569
00:34:22,520 --> 00:34:25,040
all the different things we can report on that X-ray.

570
00:34:25,040 --> 00:34:27,800
So we are building very, very specialized models.

571
00:34:27,800 --> 00:34:32,680
And I don't think, I mean, there is no, I mean, we will not, definitely, I do not anticipate

572
00:34:32,680 --> 00:34:37,640
that we will be building models, which can detect multiple kinds of abnormalities from

573
00:34:37,640 --> 00:34:39,640
different kinds of images.

574
00:34:39,640 --> 00:34:41,880
So we'll have very specialized algorithms for each.

575
00:34:41,880 --> 00:34:47,480
When you take a step back and you think about bringing these types of tools to market,

576
00:34:47,480 --> 00:34:53,840
what are some of the things that you've done with this and mine to build?

577
00:34:53,840 --> 00:34:56,640
I want to say build scale, but that's not really the right word.

578
00:34:56,640 --> 00:35:05,400
It's almost like manage technical debt to manage all of these models and to allow you to efficiently

579
00:35:05,400 --> 00:35:08,640
manage all of these models, I guess, and bring them to market.

580
00:35:08,640 --> 00:35:14,640
Is there tooling that you build or an approach that you've taken that helps you manage all

581
00:35:14,640 --> 00:35:18,880
of the complexity created by having so many very specific models?

582
00:35:18,880 --> 00:35:23,320
I mean, I would also add, there is one more complexity is that these models are improving

583
00:35:23,320 --> 00:35:25,040
at a very, very fast rate.

584
00:35:25,040 --> 00:35:29,840
I mean, so every month are models improved by a few percentage points.

585
00:35:29,840 --> 00:35:31,040
So that is also there.

586
00:35:31,040 --> 00:35:34,880
I mean, so in terms of a release cycle for our algorithms, we also have to figure that

587
00:35:34,880 --> 00:35:35,880
out.

588
00:35:35,880 --> 00:35:39,800
So there are multiple challenges, but what we are, one of the things that we are doing

589
00:35:39,800 --> 00:35:44,200
is to deploy these models as a cloud-based service.

590
00:35:44,200 --> 00:35:46,360
So that is the preferred model of deployment for us.

591
00:35:46,360 --> 00:35:50,400
So in that cloud-based service, then you can sort of, you are basically, you can host

592
00:35:50,400 --> 00:35:53,320
those models and you have a full ownership of the models.

593
00:35:53,320 --> 00:35:57,760
And if you want to sort of upgrade them, it's not too much of an effort because you are

594
00:35:57,760 --> 00:35:58,760
owning that.

595
00:35:58,760 --> 00:36:03,080
So that's sort of the model that we prefer in terms of deployment.

596
00:36:03,080 --> 00:36:07,480
Of course, I mean, some places we are working towards an on-premise deployment and then

597
00:36:07,480 --> 00:36:10,920
in those scenarios, upgrading the models, I mean, they get a static version, I mean, they

598
00:36:10,920 --> 00:36:13,720
get a current version and then upgrading them will be challenging.

599
00:36:13,720 --> 00:36:18,760
So cloud-based service allows us to, allows us to mitigate some of the problems that you

600
00:36:18,760 --> 00:36:19,760
talk about.

601
00:36:19,760 --> 00:36:22,320
Prashant, thanks so much for taking the time to chat with me.

602
00:36:22,320 --> 00:36:28,720
This has been super interesting and I've enjoyed learning from what you've learned.

603
00:36:28,720 --> 00:36:29,720
Thank you so much, Sam.

604
00:36:29,720 --> 00:36:33,200
Great talking to you as well and thank you for inviting me on this podcast.

605
00:36:33,200 --> 00:36:34,200
Great talking to you.

606
00:36:34,200 --> 00:36:35,200
Fantastic.

607
00:36:35,200 --> 00:36:36,200
Thank you.

608
00:36:36,200 --> 00:36:41,440
All right, everyone, that's our show for today.

609
00:36:41,440 --> 00:36:46,600
For more information on Prashant or any of the topics covered in this episode, head over

610
00:36:46,600 --> 00:36:51,520
to twimmalai.com slash talk slash 165.

611
00:36:51,520 --> 00:36:57,440
Don't forget to visit twimmalai.com slash nominate and cast your vote for us in the People's

612
00:36:57,440 --> 00:36:59,800
Choice Podcast Awards.

613
00:36:59,800 --> 00:37:08,680
As always, thanks so much for listening and catch you next time.

