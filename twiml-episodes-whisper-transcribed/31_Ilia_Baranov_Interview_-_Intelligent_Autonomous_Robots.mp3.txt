Hello and welcome to another episode of Twimo Talk, the podcast why interview interesting
people, doing interesting things and machine learning and artificial intelligence.
I'm your host Sam Charrington.
I've mentioned here on the podcast a couple of times that I'm keenly interested in industrial
applications of machine learning and AI.
I've come a long way in my research in this area and I'm very close to publishing a special
report on the topic.
If you're interested in learning more about this work when it's completed, I've put up
a form at twimolayi.com slash industrial AI.
Fill out that form and I'll let you know when the report is available.
In conjunction with this work, we've got something really special to debut here on the podcast.
For the next several weeks, I'll be interviewing some very interesting guests working on topics
connected to industrial AI.
We'll be discussing different application areas like manufacturing, warehouse automation
and logistics and practice areas like robotics, reinforcement learning and simulation.
We've been planning this for quite some time and we're very excited for you to hear what
we've cooked up.
Of course, as always, we love your comments, feedback and suggestions, which you can leave
on the series page at twimolayi.com slash industrial AI.
Our first guest in the industrial AI series is Ilya Baranov, engineering manager at Clear
Path Robotics.
Ilya is responsible for setting the engineering direction for all of Clear Path's research
platforms.
He likes to describe his role at the company as both enabling and preventing the robot
revolution.
He's a longtime contributor to the open source robotics community and RAS and open source
robotic operating system.
In our conversation, we cover a lot of ground, including what it really means to field autonomous
robots, the use of autonomous robots and research and industrial environments, the different
approaches and challenges to achieving autonomy and much more.
This was a really fun interview and I'm excited to share it with you.
Before we get started, I'd like to give a huge thank you to the team over at Banzai.
Banzai, who is supporting this podcast series, as well as my forthcoming report, has been
a big supporter of my research in this area.
Banzai offers an AI platform that empowers enterprises to build and deploy intelligent
systems.
I've been following them since their initial launch just over a year ago when I'm at
the founders at a conference and I've been very impressed with both the team and technology.
If you're trying to build AI-powered applications, focus on optimizing and controlling the systems
in your enterprise, you should take a look at what they're up to.
They've got a really unique approach to building AI models that lets you use high-level code
to model the real world concepts in your application, automatically generate train and evaluate
low-level models for your projects, using technologies like deep learning and reinforcement
learning, and easily integrate the models into your applications and systems using APIs.
You can check them out at bions.ai and definitely tell them that you appreciate their support
of the podcast.
And now on to the show.
All right, everyone, I am on the line with Ilya Baranov, who is an engineering manager
with Clear Path Robotics, and I'm excited to have Ilya on to talk about the intersection
between robotics and machine learning and AI.
How are you doing?
Oh, I'm doing great today.
Thank you.
Awesome, awesome.
Why don't we start with a little bit of your background?
Can you tell us a little bit about how you got to Clear Path?
For sure.
So I've always really liked robotics ever since a young age.
I probably started out with Lego, as I'm sure most of us started it this way.
And I really wanted to go to school at University of Waterloo due to their co-op program.
So when I joined them in the first year, I joined their robotics team.
And at the time, we had a pretty broad robotics team where the first years would do sort
of small competitions with sumo robots and line following.
And then the upper years would usually work towards their capstone project or final
year design project.
Okay.
And so I really liked this one group that was doing a capstone project that was doing
autonomous mind-sweeping.
And so I asked to help them out instead of doing the first year stuff.
Because by that point, I had gotten beyond the line following stuff.
And so I ended up helping them out with their GPS solution to position their robot.
And that team turned into Clear Path, essentially.
Oh, wow.
Okay.
Yeah.
So it's quite interesting.
There's a lot of learning to do, even especially in the first year.
One of the funny things that ended up happening is a little bit of the code and firmware
that I wrote and Ryan Garry wrote made our robot go in circles whenever it'd get to its
actual weight point that it was designated to go to.
We didn't really have any good tolerance on our GPS goal.
So it'd get there.
And then GPS would drift a little bit and so chase it and GPS would drift a little more
and it would chase it back.
And so.
Oh, wow.
So yeah, that's a kind of funny thing that happens when you're trying to do this stuff
in university.
Uh-huh.
Well, I think it would help folks understand a little bit more of the context of what we're
talking about.
If we go into a little bit of Clear Path, what is the company focused on?
Yeah, for sure.
So out of those kind of routes, the four founders decided that they would try to have
a go at it and create robotics for research.
But they actually started on the idea of can we take this mind-sweeping robot idea and
actually apply it to the real world?
And they were fairly surprised to find out that large defense industries didn't want
to buy from four guys in a garage.
So they had to kind of switch their idea and so they went with what they knew.
They knew that they understood the kind of university level research application of
robotics.
They had talked to a lot of professors and so they decided to create platforms.
So Clear Path really got started creating robotics platforms for research.
Okay.
So the idea being that if you're a researcher and you're trying to do some work outdoors
or indoors, any kind of development for robotics, for positioning, or movement, or interaction,
instead of spending the term, you and your grad student creating this robot, you would
purchase one from us already integrated with the sensors that you wanted to use.
Okay.
Got it.
Yeah.
So this kind of platformed approach to research robots and especially our kind of niche
that we found was the outdoor rugged market because there had been a lot of indoor ones
like the pioneer from, sorry, I'm forgetting their name right now, but yeah, there
been a few kind of indoor robots before but nobody had really done an outdoor robot platform
at the time.
Okay.
When I look on the Clear Path site, there are, it's not just one or a couple of different
types of robots.
There are a bunch of different platforms, both kind of these rugged, ruggedized outdoor
ones as well as some indoor ones.
There's UAVs, there are ones for the, you know, sea, it's called USV, so sea vehicle.
And there's even a video of one towing a plane, is that a real application of one of these
robots?
Not quite, actually, it's funny you should mention that.
That was actually a fun shoot we did for the Discovery Channel, where we tried to find
the limits of the towing capability of our grizzly platform.
Okay.
And by the time we had towed a fully fueled jet, pretty much up a little incline, you can't
really see it in the video, but the actual tarmac there sloped.
We couldn't really figure out what was the next big thing we could tell, so we got to
stop there.
Nice.
Nice.
I mean, it looks awesome.
It's very impressive.
Yeah.
Yeah.
It's a lot of fun.
Definitely.
Yeah.
So the grizzly and the more modern version, the Wardhog is meant for outdoor heavy work.
So agriculture and mining is really two of the places where we've seen a lot of interest
there.
Okay.
So one, you know, it's funny, you could find these trivial sounding applications for
these things.
I have a real need.
For example, we worked with a vineyard where they had a problem where they'd have birds
come and eat the grapes constantly.
And so there's two real solutions to this.
One is you put a bird netting, which is crazy expensive because you have to cover vast
fields with this fairly expensive mesh.
You're not about to tell us about autonomous scarecrow, are you?
Pretty much.
Pretty much.
Yeah.
Yeah.
So the other solution they went with is they made birdbangers.
So they're basically this big propane air cannon that goes off every once in a while, makes
a loud sound and scares them off.
Oh, wow.
Yeah.
But the crow's in particular got clever and they figured out that the birdbanger was always
in one place and didn't really scare them anymore.
And whenever people would move around the birdbanger, the crow's would notice that people
were carrying it around and they just move away from that area, but still eat the grapes
elsewhere.
Oh, wow.
And so what we ended up doing is mounting one of these birdbangers on the grizzly and
have it autonomously drive up and down the rows very quietly and then let it off and
then go to the next spot quietly and let it off.
And because it's electric and it's so slow, you know, or it could be made to go slowly,
it worked.
It scared them off.
Awesome.
So, you know, it sounds like a completely ridiculous use case, but it actually, you know,
it was worth the time, it was worth the money.
The complexity was low enough that we could actually assemble this application pretty much
from building blocks.
And now was this a research application or is this one of, does the company also do commercial,
slash industrial solutions?
Yeah, absolutely.
So that one was research, but in the last few years, what we've done is we've created
this new division inside of clear path called automotors and so what automotors does is
we create these indoor platforms, the auto 1500 and the auto 100 to move around payload
indoors.
So in a factory or warehouse setting is kind of our target market right now.
Okay.
So we took up a big chunk of our team.
We grew from roughly 20 people when I joined and now we're about 187 and most of that
growth has come from the automotor side.
Okay.
I'll mention this because it was initially confusing to me, the clear path automotors
has nothing to do with the auto autonomous truck company that Uber bought.
No, but they were spelled the same.
Yeah.
Yeah.
It kind of sort of happened that we filed that auto motor or auto trademark and branding
a little bit before that.
And yes, the two companies are totally unrelated, just a total coincidence.
And the auto motor's platform is somewhat reminiscent of the Keeva style of autonomous
warehouse materials handling robots.
Do you, you know, the Keeva is kind of deployed as this entire system.
Are you guys focusing just on the robots or are you also developing this entire warehouse
automation system to go with them?
Absolutely.
We're doing the entire system all at once.
And one thing I'd like to kind of point out is that Keeva in my mind is a little bit more
of the old school style of robots wherein every single robot individually is actually fairly
dumb.
No, they'll follow QR codes on the floor and they get commands from the control system
to go from point A to point B in a straight line and that's it, right?
And when they do their pickup operation, it's essentially a blind pickup operation.
And so the entire system is only applicable when you have a kind of lights out where
there's no humans beyond a certain point, whereas the systems that we make are actually
intended to work with humans.
And so they're intended to take the same walkways that you'd see forklifts drive down,
the same walkways that you'd see people walk around.
So we think our system is a lot more flexible because you don't have to structure the entire
warehouse around the robots.
In fact, ideally, really nothing changes because it's infrastructure free.
You could have any factory could drop in a few autos and they could get to work on day
one pretty much.
Oh, that's awesome.
That's a great distinction to make.
And it's also a good segue into one of the big questions that I've had or really a distinction
that I think is worth exploring at least is, you know, I think in the robotics field we
throw around the term autonomous quite a bit or I've seen it thrown around quite a bit.
And I'm wondering, you know, I'd like to explore what exactly that means, right?
I think, you know, just given the example you just provided in terms of the way the
design principles of auto relative to Kiva, you know, Kiva will call their robots autonomous
as well, but they're, you know, being controlled and directed by some central machine.
So autonomous doesn't necessarily imply intelligent, let's say, you know, how do you guys think
of, you know, what autonomous means?
And is there any standardization around that term in the robotics industry like there
are levels of standardization of, you know, what it means to be self-driving or autonomous
in the automobile industry?
They're starting to be and they're mostly focused around the concept of safety.
Okay.
So what level of safety is it to work with these robots in human areas?
Okay.
And so, and this goes back really to the older, well, still still current welding style robots
for car plants, for example, where the arm will come down and weld the specific point
completely blindly, right?
It has no concept of what it's doing realistically and has no concept of what's in the way.
And so it will gladly, yeah, it will gladly smash right through somebody, right?
And so that I would call, now I'm unsure if there is a kind of a growing term, but around
here what we say is that systems like that are automated, but they're not necessarily
autonomous.
Okay.
So that the system is doing a thing automatically, it is repeating the same motion, but it
is not actually planning its own path, it's not making its own decisions.
And so that's the case here as well.
And we've had, you know, there have been mobile robots inside of factories for quite a
while.
I mean, Toyota has been making them probably since the 80s.
And yeah, and these systems will follow magnetic tape on the floor or buried markers.
And they'll go from station to station, they'll pick up a load, they'll go somewhere else
and drop it off.
And again, I consider that system automated, but it's not really autonomous, it's only
follows a very specific set of instructions and can't deviate, it can't replan.
And if anybody trips its safety laser, it will just stop, it has no other option.
So then going back to our mobile scarecrow, if you will, when you say that it's autonomously
navigating the vineyard, to what degree is it actually doing that autonomously?
Yeah, absolutely.
Well, I mean, that specific one was kind of more of a test case here in Niagara region.
So that one was actually just using GPS waypoints going from point to point.
And if it saw something in front of it using a stereo camera, it would just stop.
Okay.
So that was one of those cases where it's kind of automated, but not autonomous.
Okay.
However, in the auto cases, what they're doing is they actually use laser scanners to build
up a map of the environment and find their own path through the environment.
Okay.
And so they're actively each unit is making its own decisions, even though we have this
overarching dispatching system that will command each unit to do certain tasks.
The actual execution of that task is usually up to the robot itself.
Okay.
Interesting.
And to what degree at auto are you getting into notions of the robots collaborating
with one another or with humans?
Yeah.
Absolutely.
So with one another, the one kind of interesting story I can tell is there's a client
we have where we're working towards replacing the standard assembly line, essentially.
So you can imagine vehicles or machines going by on an assembly line.
And if you have a different range of products, you could have three different assembly lines
for a simple, a medium and a very complex product, for example.
So instead of that, what we're doing is the robot actually carries the product and then
there's only one assembly line.
And so as the large auto 1500 is carrying the product, smaller robots are driving up and
delivering, here's a simple part for the cheap model, here's a much more complex part
for the expensive model.
And oh, by the way, the more complex model needs more time for assembly.
And so the robot will drive slower and it'll have bigger buffers between it and the next
auto 1500.
So if you can imagine it, it's this totally mobile assembly line using robots as the actual
carriers.
Right.
Oh, that's interesting.
Yeah.
Absolutely.
And so that's a case where not only are robots collaborating with each other because you
have smaller deliveries to larger deliveries and this kind of dynamic spacing.
But you also have humans collaborating directly with the robots where they know that their
parts are arriving just in time for them to use.
And yeah, and you can imagine there's hundreds of different ways that this is more optimal
than a assembly line.
If something is too slow or something gets broken or something's not quite right, that can
be pulled out of the assembly line without the rest of the assembly line even noticing.
And since the dispatcher system is keeping track, it will automatically adjust the distance
between robots.
In general, when you're deploying a system like this, what's the kind of level of abstraction
if you will that the customer needs to deal with?
Yeah, that's a great question.
So we have this entire mapping system.
And if you could kind of imagine, let's take a case of a small manufacturer.
So they want to use the auto 100 to move parts around the facility.
So on day one, an auto 100 arrives at their facility and they drive it around.
So this first step, they're physically driving it with a little joystick.
The idea being that humans are especially people who work at the facility, much better
know what is a dangerous area and what's a safe area than a machine could.
So for the initial step, we really don't want them driving autonomously.
We want somebody to kind of shepherd them around.
As it does that first pass, it builds up this map of the environment that is then uploaded
to the control system or the dispatcher.
And after that, all the robots now know this map and the user from an abstraction perspective,
they get this two dimensional floor plan that they can then draw on with just a mouse
click.
This is cell one.
This is cell two.
Don't go here.
This is a one way zone.
This is a slow speed zone, so on and so on.
And so they're essentially painting in the map in what style that they want the robots
to work in.
Okay.
And lastly, the robots are also able to detect what is a charging station, what is a docking
station if they need higher precision.
And those are automatically, those are features automatically added to the map.
So at the end of this, you know, first day, the user has a map of their plant.
They know where all the cells are, where the charging stations are, and where all the robots
are positioned.
After that, they can either do individual commands and tasks or fleet white commands.
So they can tell a robot, I need you to go from here to here right now, and that's kind
of a one time command.
Or you could set up a chain saying, I need robots from this dock to this dock every 30
seconds.
And then it takes off.
Okay.
And then a next level would be integrating that in with some type of plant management system
that is, you know, based on kind of flow of, you know, machines and materials or integrating
into a CNC or something like that to, I, what is that, you know, integrating into the
broader production pipeline, what is that fit in?
Yeah.
So that's not something we've started out quite yet.
That's definitely on our roadmap.
Okay.
However, we have a lot of integration partners that have started doing that.
So we have seen quite a few different full robotic arms and dual manipulator systems mounted
on top of auto units.
Oh, wow.
Okay.
And actually, at this year's ICRA 2017, we're going to have a UR5 arm on top of a rich
back, which is a research robot.
But the research robots are also now deploying this autonomy research kit for use by researchers.
And one of the demonstrations we'll have there is trivially scripting the arm to pick up
an object, drive somewhere and put it down in about, you know, five, ten lines of code.
Okay.
Oh, wow.
One of the things that you mentioned that has come up in, and my research is being really
important.
And a lot of ways distinct from the treatment of AI and robotics in kind of the academic
literature is this idea of learning from the subject matter expertise that is inherent
in the customer environment or the ultimate deployment environment of the robot.
So for example, you talked about, hey, let's let humans drive the robot around.
So we don't have to let the robot bang around the warehouse for a week to try to figure
out what's what.
And then letting humans then come in and put markings on the map to identify areas that
robots shouldn't go or should go or directions, things like that.
I'm curious your perspective on that notion generally of capturing human expertise in
the programming of the robot with AI and any challenges that it represents, other areas
that you might see it come up, things like that.
Yeah, for sure.
So one of the early problems we had was when we were looking into getting these robots
into factories, we had two different responses.
One of them was, this is fantastic.
The robot can go anywhere.
I don't have to do any planning.
It'll just figure everything out, which isn't quite right because really to get to that
kind of level of information, you're talking about a general purpose AI, which we're not
close to.
Right.
And then the other end of the spectrum, which we also got was, well, how can I depend
on this thing if it's not exactly following this path that I laid on the floor for it?
And so some of our early efforts involved putting virtual tape into the map.
So behaviors to tell the robot that I need you to follow this exact path and don't
deviate from it.
If anything gets in your way, stop, otherwise keep going.
And that kind of behavior really comes from the learning that a lot of the times in the
academic community, getting the thing to work and work well and not crash into things is
quite an achievement.
It's quite difficult.
That's what we spend a lot of time on.
But especially when you're talking about industrial use cases, the customer is looking at it as
equipment.
They're not really looking at it as this self-driving vehicle.
And so they don't care that it's doing these magic, creating maps and navigating.
They care about what is the tag time, how efficient is it, and what is its downtime.
And a lot of the time, you will find that through these people working there for years and
years, they know what the most efficient route is.
They know that even though this route is longer in terms of footpath or absolute distance,
it will take a shorter time because it tends to be less congested, for example.
So adding in that human layer knowledge of a here's an area that is more clear usually
is kind of an interesting trick that we've had to fuse the best of human knowledge of
the environment with this kind of superhuman level planning and optimization that computers
can do.
Great.
Great.
Can you talk a little bit about all of the various ways, maybe catalog the various ways that
you guys use machine learning in AI with the auto systems?
Yeah.
So some of the things we're really looking into, one of them, it's sort of on the border
machine learning, but one of the main kind of problems of creating these systems is getting
an accurate map.
And so there's a standard way to use laser data and turn it into a map just by basically
it's called graph slam.
And that method works well on its own, but it does have inherent problems of kind of distortions
and things like that.
So you do have to apply a little bit of intelligence to the way that you're creating these maps and
that you know that on a factory floor, for example, it would be very unlikely for the outer
wall to curve.
Right.
Right.
So that just doesn't look right to a human and so it shouldn't look right to our algorithms
either.
And I mean, too, you're doing things like maybe classifying points and curve fitting and
that kind of thing to try to generate the ultimate map.
Generate much better maps.
Yeah.
And the kind of the step further than that is after we have a decent map, we can also
think about can we classify obstacles in that space?
So just using laser data or perhaps camera data or ultrasonic data, can we classify that
I'm fairly confident this is a forklift.
If this is a forklift, that means that it has these large times.
If I can't see the times, then either they're out of my laser scanning range or something
else is going wrong.
So I'm going to give this an extra wide birth because I can't tell where the hazard is.
Or otherwise, oh, I do see the times.
I know exactly where it is.
So I'm going to shrink my approach distance and I can be more efficient in my pathening.
Okay.
So obstacle recognition and being able to predict what an obstacle is going to do in terms
of movement or speed or direction is definitely something that makes planning more efficient.
And how about in terms of the robot, the navigation?
I mean, we've talked about a little bit of the the pathing is all of the the pathing more
or less deterministic, you know, outside of the fact that it has to detect objects and,
you know, react to them or the is the general path that takes more or less set or will one
of these robots ever deviate if it, you know, knows of an alternate path and find some obstacle
that, you know, isn't moving or not moving quickly enough.
Yeah.
Absolutely.
It's, it's making a constant projection of how long it's going to take to arrive to its
end goal.
Okay.
And it's updating the system.
So the system knows this robot needs a 30 second time from point A to point B and it's
only going to make it in in 32 seconds.
So something's not quite right.
So this path is either not possible to do in this time or there's more detourists or obstacles
in the way than there should be at this time.
And so the robot's constantly making this projection of what is the fastest way to get
there?
What is the most efficient way to get there?
And if it's on a path and it sees obstacles or even if other robots in the space have detected
those obstacles and uploaded them to the supervising system, then that robot will know,
okay, this is likely a less ideal path, so I'm going to take this alternative path.
Can you talk a little bit about the, maybe the difference between machine learning and
AI that is running on the robots versus running on the supervising system?
What functions maybe sit where?
Yeah, I can talk a little bit about that.
Ideally speaking, we don't want to have our individual robots constantly trying to adjust
all the time or constantly trying to learn all the time.
Because this runs into one of the fundamental problems you have, especially applying AI
and industry, is that you have to be probably safe, right, especially with these machines
being very large and fast.
You have to be sure that even though there's a hardware safety system on board, triggering
a hardware emergency stop is not a desirable effect because you just, at the very best
case, you've messed up your, your tack time, right, you've messed up your delivery window.
But in the worst case, you can't stop fast enough and you actually hit something.
So, we need to ensure that the robots on the robot level have as predictable behavior as
possible.
And this even goes up to the level of, if you'll notice, the auto 1500s and the 100s
have a LED strip around the edge.
And so what we do is because, especially the auto 1500, it's fundamentally, doesn't really
have a front in a rear.
They're almost a mirror image of each other.
And so the robot doesn't care if it's going forwards or backwards.
And so that leads to a lot of confusion for people because they couldn't tell what the
robot was planning to do, right.
And then people will naturally, you know, if you can imagine you see this machine behaving
in a predictable way, you'll naturally start to stop and be wary and kind of clog up
the root, right.
So, having people comfortably know where the robot's planning to go means though naturally
clear out of that space.
And then it's kind of a reinforcement effect in that the robot will move better.
And so that LED strip, what it actually does is it actually puts headlights at the front
and tail lights at the back like a car.
And it will actually blink yellow blinking turning signals when it's turning in one direction
of the other.
And people really respond naturally to that because we have this kind of built in language
for roads.
Right.
What does it look like when cars coming to stop?
Well, the back red tail lights will flare brighter.
Right.
What does it look like when it's trying to pass?
Well, it might blink the white front lights telling you, go ahead, you know, you go first.
And so all these kind of behaviors really make it much more predictable in its movement.
And as a side effect, they make it more safe.
What else goes into, I mean, the so you've talked about safety systems, but do customers,
you mentioned provably safe.
What goes into the proving of the safety and to what degree do you have to do that are
they're a specific regulatory requirements that you run into that have standards for proving
safety.
And, you know, what are those and how do you address them?
Yeah.
Absolutely.
So there's one of the the fields where we're seeing here is that a lot of the standards
in industry for so called collaborative robots are really designed around collaborative
arms.
So universal robot or the backster from rethink, right?
These are robots that are intended to work with humans.
And so they're very, very clearly defined on maximum inertia's pinch points and things
like that.
But actually proving that a system that is mobile on the ground, that's a much more tricky
story.
Okay.
So there's a large set.
I think we currently follow 12 or 13 different standards for safe systems working with people.
Okay.
And so from the very basic level, things like the emergency stop system has to be dual
redundant.
So at any given time, if any one system fails, the system can still stop safely.
Mm-hmm.
Other things going all the way up to our planner on the robot itself should never plan
into a space that it can become unsafe.
So even in the worst case scenario, some horrible bug freezes the entire computer, the robot
should still be able to stop itself safely.
Mm-hmm.
Right?
Well, you have the highest level, which is the supervisory system, commanding robots and
kind of hinting them a most reasonably safe path.
Mm-hmm.
Then you have the actual autonomy on board the robot, ensuring that it's going at a safe
speed with a safe clearing distance in front of it and planning into a safe space.
Then a lower level down, you have the microcontroller that's doing the low level command and real
time control of brakes, motor and coder values and those kind of thing.
And then even outside of all that, you have this completely separate hardware safety system
that's tied into the safety lasers.
So that when the lasers are set to mode, that it's expecting the robot to travel at one
meter per second, we know that the robot will stop at this speed in one meter seconds.
So the laser, you know, safety trip range has to be at least this far.
Okay.
And so a lot of the challenge to build this safe system was, can we prove and can we test
out that the robot will actually stop at this speed under all of these conditions?
Right.
Right.
And so how we did that is we actually have a motion tracking system similar to what's
used in the film industry.
And we put tracker dots all over our robots and we would run them at carburet obstacles
over and over again for hours from different directions, different speeds, different payloads,
everything and figure out exactly how long it takes to stop in all of these conditions.
Okay.
So the interesting things we found out is there's actually a slight omission in some of
the safety standards in that the safety standard assumes if you're going around an arc or curve,
if you stop the motors, so you remove power from them, the robot will naturally go in a
straight line.
Mm-hmm.
So you can imagine, you know, you're swinging away it on a string.
If you let go, it should travel in a straight line.
Right.
But that's not the case because you actually do have rotational momentum on your robot.
So you will actually go in a, you will continue to go in a curve, not as much of a curve
as you are going, but more than you would expect.
Right.
And so these kind of things are really hard to see unless you do these thousands of hours
of testing and you track it down to the millimeter.
Mm-hmm.
But that really shapes the way that our laser safety field sets work.
Mm-hmm.
And it seems like that doesn't even take into account the, you know, the physics of whatever
your payload is.
You know, if you've got a bunch of palatized things stacked up, you know, 10 feet tall,
you're going to have kind of a dispersion radius of things all over the, the warehouse
floor that you need to kind of keep track of if you're trying to figure out, you know,
what the dangerous radius around this, this robot is.
Absolutely.
Yeah.
And so the kind of next steps that we're really looking into is, can we use our load sensors
to ensure that we never tip anything over even if somebody puts something improperly on
top of the robot?
Can we, can we use camera systems to ensure that we have looking around corners that the
robot can't see?
And so to start breaking ahead of time, can we project light ahead of the robot so that
people know that the robot's coming around the corner?
Those kind of systems.
Yeah.
There's definitely, there's, there's lots and lots of different directions to go, but
at the most fundamental level, it's ensuring that your systems are dual redundant and you
have a completely hardware-based safety stop system.
It sounds like in a lot of ways, there's a convergence between industrial robotics and
consumer autonomous vehicles and the technologies that are perhaps accelerating due to the interest
in self-driving cars are going to kind of find their way into the industrial realm.
Are you guys seeing that?
Yeah.
Yeah.
And undoubtedly, you know, the consumer autonomous vehicle market is a very large market, but
in my opinion, it's a much more difficult market to capture.
Right.
The nice thing about the inside of a factory is it never rains.
So you're never going to have the fire sprinklers off, right?
Yeah, exactly.
So you're never going to have rain, you're never going to fog, you're never going to ice.
And so those, these are all things that we don't have to worry about.
And so we have much more predictable systems.
And so a lot of the times, it's actually kind of nice that the autonomous vehicle market
is thinking about all these problems to make their lasers and cameras more robust against
these effects that we don't even have to worry about.
So we get this additional robustness, essentially, for free.
Do you guys, to what degree do you guys use reinforcement learning?
That's technology that has come up quite a bit in my research into these systems.
Is that something that you guys are looking at or using?
We're definitely looking at it, especially as I mentioned, with the idea of, can we classify
obstacles that we see in our space?
But that isn't yet released into our actual industrial offerings at the moment.
Okay.
And so over time, you're adding more capability in terms of like computer vision and things
like that, generally, to be able to detect and differentiate between various obstacles.
So that is that primarily for the safety use case or there are other things that you
envision the robots doing with the ability to make those kinds of distinctions?
Yeah.
So the real three things are safety, efficiency, and accuracy.
Okay.
So with safety, having a stereoscopic camera system, such as on the auto 100, it allows
you to see obstacles outside of the field of range of the lighter.
Okay.
So especially the auto 100, we expect it to be used more in colored environments and smaller
environments.
So it really needs to see that there's a desk that's up above where the lighter can see.
To ensure that doesn't crash into it.
Right.
Right.
The second idea in terms of efficiency is if we can classify obstacles, we can figure out
what their behavior movement is.
And so naturally as a person, if you're walking down the street and you see somebody walking
towards you, I mean, and sometimes humans will make this mistake, but you'll go to the
left, they'll go to the left, you'll go to the right, they'll go to the right.
You know, you could do that little dance.
Right.
Right.
So the idea here is can we kind of heuristically figure out a way to ensure that robots
don't do that kind of behavior, that they actually have a almost a personality, they'll tend
to stick to the wall when they see a person very visibly get out of your way or vice versa,
you know, they're carrying something very heavy and very quickly, they'll, they'll, you
know, sound a little alarm, even though you're not in the range, you're not in the way yet,
they'll warn you ahead of time because they know you might get in the way.
Right.
Right.
And so those kind of things.
So that would be kind of the efficiency state.
Well, I mean, and safety again as well, but also ensuring that the robot doesn't have
to be out of the way or can plan ahead of time.
Yeah.
So basically in terms of actual localization, camera gives you several orders of magnitude
more rich data than just a laser.
And so you can start positioning yourself based on paint or markings or you name it light
fixtures.
And so that gives you this added level robustness and accuracy in terms of positioning yourself
in three dimensional space.
Okay.
And all those things actually require machine learning quite to a large extent.
You have to understand that even if this obstacle doesn't look like it's in the way, if
you recognize it as a table, you can make certain assumptions like, okay, well, tables
usually touch the floor.
And so even if I can't see the legs on the other side, I know there's something on the
other side.
And if I know roughly the dimensions, I know roughly how big I have to avoid this obstacle
by even if I can't see the other end of it.
Right.
Right.
And the challenge is that I think you're trying to overcome with the computer vision applications
is the, the lidar is generally only giving you a two-dimensional kind of lay of the land.
Right.
And it's typically they're mounted pretty low.
So like anything that's around knee level, it sees and anything below or above that,
it's kind of oblivious to is that the way yours work as well.
Yeah.
That's close.
I mean, we try to mount ours right about ankle level.
Okay.
Or as low as we possibly can.
Okay.
Because you really want to see people's ankles or feet, ideally, when you know exactly what
their contact point to the floor is.
Okay.
But yeah, as I mentioned earlier, kind of one of our early issues we had a lot was forklifts.
So a forklift driver who didn't put his tines down.
Yeah.
So the two kind of big forks at the front, they would be sticking up in the air right about
knee level.
Okay.
And it's kind of a deadly area, if you think about it, because it's just out of range
of the LiDAR.
Yeah.
Yeah.
And so that was quite tricky.
And you're really playing this bouncing game, you theoretically you want to be right at
ground level.
But the problem with that is a, you can't really mount a laser that low because you'll
just start to scrape it on the floor.
And the other problem is is that floors aren't actually that flat.
Right.
So you do actually, there's another aspect there of kind of human guided machine learning
where you want the robots to understand that this is not an obstacle, nor is it a hill.
It's just the floors slightly uneven.
I would have thought that, you know, the poor concrete floors would be a lot more flat
and reliable than anything in my old wood floor house.
You would be amazed.
It's really a factor of distance, right?
Your factory floor is so massive that's a 0.1 degree difference turns into, you know,
a few feet.
Yeah.
So it's quite surprising.
So tell me this, are there any particular, you know, beyond the things we've talked about
in the computer vision domain, are there any trends or research or technologies that
you're tracking in the machine learning, deep learning AI domain that kind of have
you excited for the implications to robotics?
Yeah.
So what we're taking a close look at is understanding maps and understanding the behavior of mapping
and positioning.
So can we, for example, understand that here's a factory floor and just off the factory
floor, there's a lunchroom, for example.
Okay.
And as a human, you would assume that somewhere between 11 o'clock in the morning and
1 p.m.
in the afternoon, the lunchroom is going to be very busy.
So don't bother to walk through there because you know it's going to be busy.
And so can we apply that kind of concept to our understanding of maps to the robots
understanding maps?
So as they're driving around, they collect this data, they send it up, and then our processes
to system can understand that this area is very busy at this time.
Even though I have no vision of it, I know from last week and the week before that this
area is probably too busy for me to plan through.
So I'm just going to avoid it without even trying.
So that's definitely an area that we're looking into is understanding maps and kind of gaining
a more fundamental level understanding of how space is changing throughout the day.
Okay.
And that example that I give is kind of most easy to understand.
But there's also, especially small factories and small kind of cell-based manufacturing.
Things change every day.
This day you're making cases for a cell phone.
The next day you're making, I don't know, car dashboard accessories, that kind of stuff.
So people, but also their equipment is moving around on the factory floor and having the
robot understand that this drill press that was here yesterday is now over here, but it's
the same object.
So when I ask you to go to the drill press, you don't go to the old location, you go to
the new location.
So again, those are things that mapping and understanding of the space is something that
we're really looking into.
So you're about to head off to ICRA, which is in Singapore this year.
That's the International Conference on Robotics and Automation.
Is that right?
That's correct.
What are you excited to see there?
Oh.
Well, all of it pretty much.
We've seen quite a large growth in these two kind of fields.
One is mobile manipulation.
So not only having these human safe arms working in their cell, but actually moving from
cell to cell.
And so one of our industrial partners, what they did is they're working on very high precision,
very trackable manufacturing.
So they're making jet engine parts or nuclear reactor parts.
And so every single part needs to have a history and needs to be checked every single step.
So one of our autos is carrying around a manipulator.
The manipulator will come up to a station, take a block of aluminum, put it in a CNC and
see, start it, wait for it to finish, take it out, bring it to the measuring station, measure
it, label it, track it in their database.
And so we've been working on that for a little while, but now we're starting to see a lot
of different companies take a crack at that same problem.
Is can we make multi kind of flexible manipulation?
And the other big trend we're seeing is survey robotics.
So remote inspection of pipelines and power lines and power stations and places where it's
just very far to get to and you don't really want a human there all the time.
But you do want to have high quality data come back from it.
And so those are two fields that we see, even large industrial partners, or just large
companies in general.
For example, DJI got started in the kind of consumer space, but they're really making
inroads in the, can they do remote survey of data with their drones?
So that should be pretty interesting as well.
And I'm starting to see, there are a number of companies that are trying to tackle the
indoor industrial drone problem or space.
For example, flying around warehouses or supermarkets to, you know, hopefully overnight
where no one's in there to take inventory, which is a super expensive process for companies.
Yeah.
And you guys have, are you, are you less far, I presume, I should say, that given that
you have, you know, auto motors, which is kind of the commercialization and kind of industrial
facing company focused on the, you know, indoor materials handling platforms, are you kind
of moving in a commercial direction with the UAV and the USB, the aerial and the seaborne
vehicles as well, or are those more research oriented for, you know, the foreseeable future?
So definitely the unmanned aerial vehicles tend to be a little bit more research focused
for now, at least on our side, simply the reason being that the main preventor from them
being used for most tasks is just battery life.
Yeah.
So if you're doing aerial surveys and you're doing aerial surveys of data, that's great.
15, 20 minutes, that's all pretty much you need to collect the data you need.
Yeah.
But if you're doing something like store inventory, you need to do an hour or two hours.
Right.
And it's just not really feasible.
On the other hand, though, the unmanned surface vehicle are heron.
That's an interesting case because we had actually done some work there, becoming a data
provider to provide data for municipalities and companies about tailings ponds and overflow
ponds in those kind of places where you need to know the depth and water quality of a small
body of water.
Okay.
And so the idea would be that different companies would take this heron out to a site, do
an automated survey, a GPS guided automated survey of the depth and quality of a pond.
And then that data is uploaded to us and we provide this data as a service.
Oh, super interesting.
So that's something that we're looking into as well.
So that's actually quite a lot closer to a real direct commercial application where the
user is using it just as another piece of equipment and not really worrying about it
being a robot.
Oh nice.
And maybe switching gears a little bit, you are kind of active in the open source robotics
community.
There's a robot, ROS is robotics operating system or robot operating system that you've done
quite a bit of work with.
And is that what's the intersection with with ClearPath is through the ClearPath platforms
run some version of ROS?
Yeah.
Yeah.
So ROS is this idea of can we apply the Linux ideal where Linux is free and open source
and you can run it on anything, can we apply that to robots as well?
So the idea behind ROS is it runs predominantly on top of Ubuntu, but it can run on a few
other platforms.
And it will talk the same language from microcontrollers all the way up to servers.
Okay.
And so that makes assembling a robot very, very easy because you can add on an arbitrary
microcontroller, an almost an arbitrary sensor because most of them now have supported
ROS drivers and assemble your robot almost as Lego bricks, which is what we do.
And so we became a very early supporter of ROS and we continue to be one of the kind of
the largest companies that provide all of our platforms are ROS compatible, all of our
research platforms.
Oh nice.
And is there a standard interface between ROS and kind of higher level machine learning
AI stuff or are those kind of two separate things at this point?
At this point they're a little bit separate, but they're starting to get there.
So one example is everybody should be familiar with the OpenCV framework for image processing.
And OpenCV actually came out of Willa Garage, which also started ROS.
Okay.
So at the same time, Willa Garage created the kind of open source robotics movement and
a lot of these early fundamental libraries for image processing, a little bit of the machine
learning work and those kind of tasks.
Hmm.
Interesting.
Interesting.
Yeah.
And I guess this is also an aside, but I did a, I got in on a Kickstarter for this little
miniature LiDAR system called Scance.
Have you ever come across that?
I believe so.
Yeah.
I haven't done anything with it, which is the case with most of my electronic Kickstarter
projects, but it looks like it's pretty cool that you can make all of the components in
this ecosystem from the platforms to the UAVs to the sensors, including LiDAR, they've
just become so affordable and miniature as it's become very accessible, which I think contributes
to people being able to do lots of things and play with different ideas, including kind
of the ML and AI angle.
Yeah.
Absolutely.
The cost of LiDAR is coming down quite significantly.
But more than that, really, the real enabler in my mind is the rapid growth of processing
power for images.
So not only the libraries are designed to run on CPUs, but also GPU-based processing
of images, because as nice as LiDARs are, and we use them almost everywhere, fundamentally,
we build the world around humans and humans mostly use vision for their navigation.
And so almost everything we've structured, we've structured around this idea of you
have a roughly human sized object that can see roughly human distances, and that's how
we get around it.
So fundamentally, I believe that in the long run, in the next 50 to 100 years, most of
our systems will start to use cameras as their predominant source.
And that's beneficial even today, because a relatively decent camera is maybe $20.
And it's more than good enough to do cutting edge research.
And you know, you can start with two cameras that are the same model.
You put them on a ruler, so they're a measured distance apart.
You calibrate out their small imperfections.
And then you attach it to a consumer grade laptop, and you have a cutting edge stereoscopic
vision platform that you can start doing research on.
That's awesome.
Any particular, you know, for folks that are interested in the hobbyist angle here, any
particular links or pointers or places that you find are helpful for folks getting started?
Yeah.
Well, I'll kind of suggest that people visit the Clear Path Robotics Ross 101 series of
tutorials, where we can help people get started on Ross.
We have a little virtual machine, so you don't have to install Ubuntu on your home computer.
You can kind of just run it virtually.
And all of our robots are simulated there, along with sensors.
So you could actually simulate, for example, Husky with a stereo camera, with a LiDAR,
with an IMU, and move around a virtual map and start doing mapping, navigation, path
planning, all of that for free on any laptop.
Oh, that's amazing.
I'm definitely going to have to do that.
Well, you brought up simulation.
That's a topic that I want to dig into also, but I fear we are bumping up against
a time constraint here, so I'm going to save that for another conversation.
But thank you so much for taking the time to speak with me about your work.
It is really fascinating stuff that kind of speaks to the, you know, the kid geek playing
with Legos and trying to make stuff automated.
I really appreciate you taking the time out.
Absolutely.
Thank you very much.
All right.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued support, comments, and feedback.
We're excited to hear what you think about this show and the industrial AI series we've
just kicked off.
I'd also like to thank our sponsor, Banzai, once again, be sure to check out what they're
up to at vons.ai.
Speaking of Banzai, they'll also be at the O'Reilly AI conference in New York City later
this month.
If you'd like to attend, you can save 20% on registration by using our special code PC
Twimble, PC-TW-I-M-L.
We'll include the code and a link to the registration page in the show notes.
I'd love to meet up with listeners at the conference.
In fact, I'm planning a community meetup during the event and I'll share details as soon
as they've been ironed out.
As usual, the notes for this episode can be found at twimbleai.com slash talk slash
27.
For information on industrial AI, my report on the topic or the industrial AI podcast
series, visit twimbleai.com slash industrial AI.
As always, remember to post your favorite quote or takeaway from this episode and we'll
send you a laptop sticker.
You can post them as comments to the show notes page via Twitter, you are following us
at at Twimbleai, aren't you, or via our Facebook page.
Thanks again for listening and catch you next time.
