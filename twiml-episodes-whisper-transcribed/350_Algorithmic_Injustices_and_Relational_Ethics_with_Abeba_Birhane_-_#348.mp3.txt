Welcome to the Tumel AI Podcast.
I'm your host, Sam Charrington.
Hey, what's up everyone?
Last time I mentioned the study group, I'm hosting around IBM's AI Enterprise Workflow
courses and the webinar I'll be doing this Saturday with Ray Lopez, the courses instructor.
It's been awesome to see the strong community response to this new program.
These courses complement the modeling focus courses you may have taken and drill in on the
end-to-end process of building, deploying, and managing real-world enterprise AI and machine
learning projects.
Unique to this program is the full immersion experience it offers.
Consider this.
You've just been hired as a data scientist working for a streaming media company.
Your mission, should you choose to accept it, is to apply the tools of data science and
machine learning to drive engagement and growth at the company.
The examples, business problems, data sets, and tools you'll work with throughout the
course are all woven around this scenario.
The webinar will be this Saturday, February 15th, at 9.30 a.m. Pacific time.
To register or learn more, visit twimlai.com slash AI Workflow.
And now on to the show.
All right, everyone.
I am on the line with Ababa Barhani.
Ababa is a PhD student at University College Dublin.
Ababa, welcome to the Twimlai podcast.
Thank you so much for having me, Sam.
I'm really excited about this conversation.
We had an opportunity to meet in person after a long while interacting on Twitter at the
most recent Nureps conference, in particular, the Black and AI workshop, where you not
only presented your paper algorithmic injustices toward a relational ethics, but you won
best paper there.
And so I'm looking forward to digging into that and some other topics.
But before we do that, I would love to hear you kind of share a little bit about your
background.
And I will mention for folks that are hearing the sirens in the background, while I mentioned
that you are from University College Dublin, you happen to be in New York now at the AIES
conference in association with Triple AI, and as folks might know, it's hard to avoid
sirens and construction in New York City.
So just consider that background or mood ambiance background sounds.
So you're background.
Yes.
Yes.
How did you get started working in AI ethics?
So my background is in cognitive science and particularly a part of cognitive science
called embodied cognitive science, which has roots in cybernetics and systems thinking.
The idea is to focus on the social, on the cultural, on the historical, and kind of to
view cognition in continuity with the world, with historical backgrounds and all that's
as opposed to your traditional approach to cognition, which just treats cognition as
something located in the brain or something formalizable, something that can be computed.
So yeah.
So that's my background.
And during my masters, I lean towards the AI side of cognitive science the more I delve
into it, the more I, much more attracted to the ethics side, to injustices, to the social
issues, and so the more the PhD goes on, the more I find myself in the ethics side.
Was there a particular point that you realized that you were really excited about the ethics
part in particular, or did it just evolve for you?
I think it just evolved.
So when I started out, I attained of my masters and at the start of the PhD, my idea is
that, you know, we have this new, relatively new school thing, way of thinking, which is
embodied cogsai, which I quite like very much, because it emphasizes, you know, ambiguities
and messiness and contingencies as opposed to, you know, drawing green boundaries.
And so the idea is yes, I like the idea of redefining cognition as something relational,
something inherently social, and something that is continually impacted and influenced
by other people and the technologies we use.
So the technology aspect, the technology end was my interest.
So initially, the idea is yes, technology is constitutes aspect of our cognition.
You have the famous 1998 thesis by Andy Clarke and David Chalmers, the extended mind,
where they claimed, you know, the iPhone is an extension of your mind.
So you can think of it that way, and I was kind of advancing the same line of thought.
But the more I dealt into it, the more ISO, yes, digital technology, whether it's, you
know, ubiquitous computing, such as face recognition systems on the street, or your phone,
whatever, yes, it does impact, and it does continually shape and reshape our cognition
and what it means to exist in the world.
But what became more and more clear to me is that not everybody is impacted equally.
The more privileged you are, the more in control of you are asked to, you know, what can
influence you and what you can avoid.
So that's where I become more and more involved with the ethics of computation and its impact
on cognition.
The notion of privilege is something that flows throughout the work that you presented
at Black and AI, the algorithmic injustices paper, and this idea, this construct of relational
ethics.
What is relational ethics, and what are you getting at with it?
Yeah.
So relational ethics is actually not a new thing, a lot of people have theorized about it
and have written about it.
But the way I'm approaching it, the way I'm using it is, it's, I guess it kind of springs
from this frustration that for many folk who talk about AI ethics or fairness or justice,
most of it comes down to, you know, constructing this neat formulation of fairness or mathematical
calculation of who you should be included and who should be excluded.
What kind of data do we need, that sort of stuff.
So for me, relational ethics is kind of, let's leave that for a little bit and let's zoom
out and see the bigger picture.
And instead of using technology to solve the problems that emerge from technology itself,
so which means centering technology, let's instead center the people that are, especially
people that are disproportionately impacted by, you know, the limitations or the problems
that arise with the development and implementation of technology.
So there is a robust research in, you can call it AI fairness or algorithmic injustice.
And the pattern is that the more you are at the, at the bottom of the intersectional level,
that means the further away from you are from, you know, your stereotypical white cisgender
domain, the more, the bigger the negative impacts are on you, whether it's classification
or categorization or whether it's being scaled and scored by hiring algorithms or looking
for housing or anything like that, the more you move away from that stereotypical category,
you know, the status quo, the more the heavy the impact is on you.
So the idea of relational ethics is kind of to think from that perspective to take that
as a starting point.
So these are the groups or these are the individuals that are much more likely to be impacted.
So in order to put them at advantage or in order to protect their welfare, what do we need
to do?
So the idea is to start from there and then ask for their question.
Instead of, you know, saying here, we have this technology or we have this set of algorithms
or calculations, how do we apply them or how do we then use them to, you know, for a
better or a fairer outcome?
And sometimes the answer you arrive at is that a particular technology shouldn't exist
in a given form.
Yeah, exactly, exactly.
So I think one of the downsides of obsessively working on some matrices or some equations
on fairness is that you forgot to ask in the first place, do we, should we even do
this in the first place?
And I think some people have articulated this really well.
You can think of this in terms of the, you know, face recognition systems that are becoming
very normalized and common, especially in the states.
Do you feed your face recognition algorithm with diverse data in order so that it recognizes
everybody equally or do you stop and think, do we actually need face recognition systems
in the first place, you know what I mean?
Yeah.
And that's a question that, you know, honestly, I have trouble with in a lot of ways because
I think there are certainly problematic uses of facial recognition, but often the question
is posed or the assertion is made that, you know, we shouldn't use the technology or we
should, you know, I guess, you know, it's not uncommon to hear people kind of take this
position of, hey, we can't put the genie back in the bottle.
And, you know, I think on some levels, I get that that, you know, maybe that's a copout,
but in other ways, it's like pragmatic.
How do you balance the idealism that I think is probably core to the approach you're trying
to take with pragmatism that recognizes what is already happening and the way technology
tends to develop and evolve?
I guess my approach, at least in the paper I presented at Neurips is that, and if you're
starting point really is the welfare of, you know, the most disadvantaged, then I don't
know how that cash is out with pragmatism or even with the whole idea of fairness because
for most approaches to fairness, whether it's explicitly laid out or whether it's implicitly
implied, the idea is, it's very utilitarian, in a sense, you have, you aspire to arrive
at, you know, the greatest happiness for the greatest number of people.
So, which really doesn't work for, if you are from a disadvantaged group or if you
are a minority because you will never, you are not a minority, you are not a majority
in the first place.
So, any solutions that aspire to please the majority will always have negative consequences
and it just doesn't work.
So, that's the struggle with when you want to prioritize the needs and the welfare of
the least privileged and on the other hand, some form of pragmatism or what's based for
the majority or, you know, the greater the whole society, that's the tension that's
probably, that will always exist probably.
And so, that's at the heart of this idea of relational in a sense, it's, you know, pragmatic
relative to who.
Exactly, but also, you have other, other face recognition might still be, still arrived,
still open to so much controversy.
But you have other examples such as, you know, Facebook recently got a patent for socio-economic
group classification of their users and they haven't said much about how, where they
are going to apply it or how they are going to use it.
But, you know, tools like that, you can see it's insidious and anything, it's very, very
unlikely, anything positive or anything good will come out of it, especially for users,
for people who's socio-economic status is, you know, from a really poor background.
So, the idea of, I guess, relational ethics as well as questioning, do we need these tools
in the first place?
It's also thinking about, you know, the bigger picture of what automation, whether it's
job applications or whether it's housing or whether it's insurance, it's what it's doing
to society.
And what kind of values are we prioritizing and embracing in the process?
So, it's kind of thinking of ethics more of, more as a habit, as kind of constantly thinking
of what kind of society we want to live in as opposed to thinking of, I have this piece
of tool or this piece of equipment and how do I make it fair or how do I twitch with
it or work it to find that balance?
You mentioned earlier the, you know, that a lot of fairness is thinking about data bias
and accommodating for data bias, you know, setting aside the issue of whether the thing that
you're trying to address, you know, something that should be done at all, that, you know,
the issue of data bias is kind of just one small piece of the overall fairness puzzle.
How do you think broadly about kind of the different aspects of AI fairness and AI ethics?
Do you have a categorization or framework or, you know, way of thinking about it that you
found helpful?
Yeah.
So, this is actually at the heart of the whole relational ethics trying to reframe the
whole idea of what ethics is.
So, because as you said, a lot of people working on AI ethics really are about, you know, whether
it's explainability or calculating fairness or justice, it really is usually lost in
the fine-grained details.
So, it's not something implementable that I provide, but it's about kind of really zooming
out and thinking, you know, what are we doing, what are we prioritizing, how are we defining
bias, how are we defining ethics, how are we approaching these concepts in general.
And one of the aspects that I've read that as part of the paper that I emphasized is,
I guess it's the nature, the inherent nature of machine learning, which is that we are
continually predicting whether it's health issues, whether it's socio-economic issues, whether
it's who is going to be, you know, the best employee, it's all about making predictions
based on whatever data we get our hands on.
So, the idea of relational thinking is kind of rethinking the whole idea of predicting
as something we need to stop and pause and think.
Instead of continually predicting how about we kind of take it easy and first analyze
and think about the patterns that we are getting, trying to understand the reality and things
as they are, as opposed to using whatever data we have as evidence for our prediction
or as input into our predicting tool.
So the one of the examples I give is Cati O'Neill in Weapons of Destruction also mentions
this is if you take algorithms used in policing in the legal system instead of, say, recidivism
algorithms instead of striving to predict who is likely to reoffend or who is likely
to commit crime or what area should be policed more, we use our tools or we develop our tools
in a way that lend themselves for us to understand why are this group of, this demographic
or this group of people coming up as higher risks?
What can we do?
How do we rehabilitate, say, prisoners instead of how do we catch them when they reoffend?
So it's really switching mentality.
It's thinking about how do we make the society or the better, the world a better place?
How do we help people get back on their foot rather than how do we, you know, rather
than playing a gacha rather than how do we catch them again?
But so thinking of kind of prioritizing understanding, you know, questioning why do we find
these patterns that we are finding and how do we improve that really kind of aligns with
this relational thinking I've been, I've been talking about as opposed to, you know, creating
and building these predictive tools.
Yeah, and how it was interesting that there's multiple levels to this idea of prioritizing
understanding.
There's, you know, as, you know, individuals working in these areas, we should prioritize
our understanding of the people involved in the scenarios that were involved in and how
the people are interacting and affected in these various scenarios.
But also, you're also suggesting that we should prioritize, you know, the tools that we
build to enhance our understanding as opposed to, you know, just spitting out more and more
predictions.
Yeah, yeah.
Exactly.
I haven't yet seen many tools that aim to understand so much of what I come across is
always predictive tools.
And I think prioritizing understanding really will contribute to, you know, the larger,
greater, greaterness of society.
Again, this is not something you can formulate or you can come up with a set of steps that
you can implement.
It's, it's more of kind of changing your habit, developing a different set of habits.
It's something you continually keep in the back of your mind, whether you are an ecosystem
or an engineer or a data scientist.
So it's really zooming out and looking at the larger picture.
But also, it's not to oppose that we should throw out all implementable tools we have on
whether it's fairness or accountability or explaining, explainability, and it's just
that we have to also look at the larger picture.
And I guess another aspect of relational ethics is you might have these implementable tools,
you might have the set of tools to make your system better, but the idea is if you think
of these concepts such as fairness or ethics or even your own set of solutions as something
that are continually changing.
So this is at the, I guess this goes back to at the start I was talking about how the
idea of embodied cognitive science at its core comes from systems thinking and cybernetics
and the social sciences, and at the heart of it is that not only can you define cognition
in isolation from others or in isolation from the tools you use or in isolation from
the environment, it's also that whatever your definition of cognition or whatever your
understanding of the person has to account for the nature of reality, which is that its
never stable, it's never fixable, it's constantly changing, so and it's very contextual.
So you are some a certain type of personality at the moment with certain expected norms
talking to me on the on here, but some other time in some different contexts, in some
different environments, you are also slightly different person.
So the underlying idea is whatever concepts we are dealing with, whatever solutions we
have, they cannot claim to to finalize things, they cannot stabilize this continually moving
nature of being and whatever is ethical in this context might not be ethical in other contexts.
So I think relational ethics helps you leave whatever solution you have somewhat partially
open, so that you can reiterate, so you can revise and change with whatever new evidence
or new data comes up your way the next day or the next year.
So this treating of things as moving and changing really is fundamental, it helps us realize
our solution now is only for now with an unlimited context, with an unlimited environment.
And I think that's a really important thing we can all pay attention to.
The example you gave of who is Sam in these different contexts makes me think a little
bit about and linguistics, the idea of code switching and I may speak in a particular
way when I'm on the podcast and then when I'm at home, I may speak in a slightly different
way and when I'm out in the neighborhood, I might speak in a slightly different way.
And I haven't seen much in machine learning or NLP that tries to capture that or take
a account of that. Do you have some examples of examples of how you might envision machine
learning systems, you know, if they were to follow this aspect of relational.
Yeah, so this is really difficult and I guess at the heart of a lot of issues and so
when you can assume things are stable and somewhat, you know, you can grasp them with whatever
tools or language you have, it's much easier to construct theories or to construct some
sort of tool which is why I mean that this stability, you know, translates to the,
you know, everything's coming from a identical distribution, which is at the foundation
of most of what we do in machine learning.
Yeah, again, I'm not really a computer scientist, as I say at the beginning, I'm a cognitive
scientist and I think about cognition in persons and I don't know any NLP tools or machine
learning approaches that account for this continual change and contexts.
But also even within the cognitive science movement, especially embodied cognitive science
which is trying to push the importance of these change, you know, language and context
is one of the things it struggles with is because it's difficult to formalize and make
up, provide something conclusive, but when you are underlying change, it ends up dealing
with a lot of theorizing as opposed to producing something, something you can model or something
you can formalize. So I guess it's an existing tension.
Again, it's much better to think of it as a habit and to acknowledge this continual
changing nature of things in a sense that acknowledgement makes you aware that your
tool or your solution or your theory is only as good as the, it's a specification and
the context and that acknowledgement further encourages you to live not to conclude your
solution or your tool as something finalizable, something that will be good all the times
for all contexts, but something that you have to leave a little open partially open, something
that needs revision continually. So that's again, for me, for me at the moment, the
best one can do is acknowledge this change and context and leave this partial openness
and embrace reiteration and revision. One of the other ideas in the paper is that, or at
least I, you know, I interpret it as that along the lines of the idea of prioritizing understanding
over prediction, one of the ideas in the paper is that, you know, when we predict, it's
often based on these very reductive labels that we're applying to things. The examples
you gave are successful versus not criminal versus not, and you kind of point out that that
is inherently problematic in many cases. Exactly. You can also look at a lot of algorithms
with them trying to categorize or identify gender identities and I think that's one of the
most obvious cases where the harm of doing so, the harm of categorization becomes very starkly clear
because usually stereotypically and in most societies, you would categorize
gender as, you know, a male or female. Sometimes you might have bisexuals, but as we know, gender
identities are much more, more than just those categories and not only they are larger in number
those categories, but we all know that they are fluid. Someone that was bisexual or take a
trans person, for example, people change their sexual and gender identities. How do you then?
Then it becomes easy to see how difficult it is for whatever algorithmic tools we are developing
to account for that change. But also, as we developed that tool in category, kind of come up with
these categories, in a sense where this advantage and excluding anybody that doesn't
belong in those categories that we have created, this is where, again, the most vulnerable are,
you know, impacted the most. Yeah, so it's problematic in that regard.
I'm curious what kind of reaction you've seen to the paper.
So, at Nureps, it was overwhelmingly positive. It was my first time in Nureps and I went in thinking,
oh, this is, you know, a machine learning AI conference. I'm just, you know, the
outlier cognitive scientist, slash atheists. So I went in feeling I'm not going to fit very well.
But it was really, really positive. And I was, even when I was, when the announcement that
my paper had won the base paper came, I just could not believe it. You know, as a grad student,
you go to conferences, you present a poster or whatever. And some parts of you sometimes,
you know, deep down, you think, oh, I might win, you know, I might have a chance for a
base poster or something like that. But I went into Nureps, like, there is no chance, I'm just
going to relax. Enjoy the dinner. It was dinner party. And I was really shocked. And so it's,
it's been really positive. I have presented similar ideas previously to very exclusively,
kind of very software engineer, machine learning, deep learning, researchers. And
people really are not interested in my ideas because people want something implementable,
something they can code into, you know, something formal, something they can use. So what I'm asking
is a reframing, a rethinking and in a sense, a changing of habits. And it's almost like an
activism. It's like asking, what kind of society do you want to live in? So for some people,
that's really difficult and something they would rather not get involved in. But the more I
interact with people, but and also on Twitter, it's, it's really, really encouraging. People seem
to like what I have to say. So I'm happy. Really quickly before we wind down, you are in New York
to present a more recent paper. I believe it's more recent paper that you have worked on robot
rights. Can you talk a little bit about that paper? Yes, it's unfortunate that the title has
robot rights because it really is not about robot rights. It's robot rights question mark.
Let's talk about human welfare instead. So this paper, I worked on it with my colleague,
Yeli Van Dyke from University of Twenth. He also, he also comes from a distributed
cognition in body cognitive science background. And we talk about this a lot on Twitter.
And we are constantly getting caught up in this Twitter debates whether, you know,
machines can be sentient or whether robots should be given rights, blah, blah, it goes on.
And it's the same kind of pattern of interaction. So very and over and over again. And I think about
five, four, four, five months ago, I asked him on Twitter, how about we write a paper on this?
And writing the paper came really, really easy because we have the same background. We think I like
the idea of the paper is it has in a sense, it's twofold. The first one is kind of philosophical.
So we lay out how robots are not the type of beings that can either be granted or denied rights.
We lean on a lot of, you know, embodied coxay as I was saying earlier, this notion of cognition
as inherently social, inherently relational, people inherently, you know, value-laden,
constantly striving to make meaning of the world. So we use that post-cartesian approach to
to get at the heart of how philosophically speaking robots or animation learning tools
or any missions at all are not the same beings as humans or even animals. And then the second part
we get at the urgent questions that AI ethics really need to focus. Because sometimes, not sometimes,
most times, it's really frustrating to hear robot ethics classified as part of AI ethics. And
for me, personally, it comes across as worrying about future, may happen, may not happen, may become
sentient. A lot of it is really contemplation and thinking ahead about the future. And it's
all that contemplation and philosophical musing, taking so much of the AI ethics space is just unfair.
So a lot of the second part of our paper deals with how the very idea of AI itself, whether it's,
you know, computer vision, whether it's autonomous systems, it's never autonomous. There is
always humans in the loop. And not only that, it's not possible without the exploitive human
labor, whether it's tagging your old data for that's going to be part of an autonomous system,
or some sort of image recognition. Even when you do recaptures, you are in a sense
kind of being, you are putting in your own unpaid labor into making mission space.
So the argument we made, we make there is that AI systems are never autonomous and they never
will be. But in the argument of whether they are autonomous or not, we lose sight of the people
who are underpaid, such as the mechanical Turk or micro workers, they never enter into the
debates. We also teach upon, you know, how the robot systems, such as, you know, Roomba or whatever
are invading private spaces as they roam around our houses and how that should be more
urgent and crucial as opposed to some stereotypical humanoid, such as Rob, what's your name, Sophia.
And yeah, and we get a little bit on algorithmic injustice as well, how the least privileged,
the most disenfranchised are the most impacted and how that should be the focus of AI ethics
as opposed to, you know, hypothetical sindian things. So that's the core of the paper.
And there's been quite a bit of discussion about this one on Twitter. In fact, we're not going to
be able to get into it very deeply, but I would encourage folks to take their reactions to Twitter.
Is it fair to say that this one has been more controversial than the previous one?
It appears to be so. And to be honest, when we wrote it, we wrote it as, when we have those
never ending conversations about rights again on Twitter, instead of repeating the conversation,
we'll just have a paper to point to. But it has provoked a lot of very strong reaction from people
both defending rights for robots and both thinking it's really idiotic to even discuss rights for
robots. So apparently it's controversial. Interesting. People love their robots, I guess.
Yeah, I guess. Yeah. Yeah. Well, Ababa, it has been so great to have a chance to chat with you
in more detail about what you're up to. Thanks so much for taking the time to share with us.
Thank you so much. It's been great. Thank you.
All right, everyone. That's our show for today. For more information about today's guest,
visit twommalai.com slash shows. To learn more about the AI enterprise workflow study group
I'll be leading, visit twommalai.com slash AI workflow. Of course, if you like what you hear on
this podcast, please take a moment to subscribe, rate, and review the show on your favorite
pod catcher. Thanks so much for listening and catch you next time.
