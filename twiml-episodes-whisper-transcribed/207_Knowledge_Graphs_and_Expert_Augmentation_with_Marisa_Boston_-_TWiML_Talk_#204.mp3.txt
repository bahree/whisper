Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Marissa Boston, Director of Cognitive Technology in KPMG's Cognitive
Automation Lab.
Marissa and I caught up to discuss some of the ways that KPMG's using AI to build tools
that help augment the knowledge of their various teams of professionals.
We start out with a discussion of knowledge graphs and how they can be used to map out
and relate various concepts.
We then explore how they use these in conjunction with NLP to create insight engines, tools
that curate and contextualize news and other text-based data sources to produce a series
of content recommendations that help their users work more effectively.
Finally, Marissa shares some great general principles for using AI to augment human experts.
Before we dive in, over the next few weeks, I'll be bringing you some great interviews
from the road, including AWS re-invent, NURRIPS and CUBECON, and I would love to connect
with any listeners and attendants.
Feel free to shoot me a message via at Sam Charrington on Twitter, email, the Twimble website,
LinkedIn, or if you just see me walking by, don't be afraid to say hi.
If you're heading to NURRIPS, look for the listener meetup and the AI platforms meetup
that I've posted in the Hoover app.
See you around.
And now on to the show.
All right, everyone.
I am on the line with Marissa Boston.
Marissa is director of Cognitive Technology for KPMG's Cognitive Automation Lab.
Marissa, welcome to this Weekend Machine Learning and AI.
Thank you.
It is great to chat with you.
Why don't we get started with a little bit of your background?
I understand you spent some time studying at Cornell in upstate New York.
That's right.
I did my PhD at Cornell.
My area of focus was computational linguistics, which is a mouthful, but it's essentially
building out computer, basically computational models of how humans understand sentences
was the work that I was doing.
So it was a really interesting way to take technology and try to see how we can investigate
scientific matters, how the human is understanding language and things like that.
And in your work there, were you focused primarily on traditional linguistic models or were
you working with statistical models?
Yeah, I actually built out statistical parsers.
So it was kind of like this, we did both essentially.
So I worked on statistical sentence parsers, but what I would do is I would use theories
from cycle linguistics and from linguistics to help determine where difficulty might be
in a sentence.
And then I would actually test if that difficulty is coming through in the same way for the human
as it would be for the computer based on how I encoded those both the linguistic structures
and the psychological theories as well.
Oh, interesting, interesting.
And after your PhD?
After my PhD, I decided to go into industry research.
I went to the nuanced communications where I worked in their AI and NLP labs, and that
was really fun.
I got to do all sorts of things I worked on, the original Watson system, helping to tune
it to healthcare.
And I worked on other types of question answering and textual inference systems and also virtual
assistance for customer service centers.
And then you moved over to KPMG, right?
I did.
I ended up moving out of tech and I wanted to go into more of a consulting firm and a professional
services firm because I wanted to get more of a sense of how we can build out the
appropriate environment for these technologies and what are the business models that can
support them in order to ensure that they are actually being used appropriately.
So I was kind of getting sick of, you know, I realized fairly quickly that no matter
how good my designs are, my architecture is my implementations.
If the business model doesn't support it, then you're not really going to be able to
get much use out of it.
So I really wanted to get a better sense of how we can build these tools more appropriately.
When you say the business model needs to support it and creating the appropriate environment,
what all is involved in that?
I think it really comes down to, first of all, having the appropriate boundaries.
So one of the things that interested me about coming to KPMG is that they're an audit
firm, audit and tax accounting.
And so they have a lot of regulatory pressures on them.
And I find this interesting because a lot of times in tech, we think that we have to invent
everything.
And I think that usually this leads to some kind of a wild west mentality in terms of what
technologies are out there and whether they're appropriate or not.
I was really interested in seeing what kind of human boundaries might be there in terms
of regulations and in terms of the types of technologies we can actually provide.
So that was one thing.
The other thing is that in a lot of ways, it teaches you to be an ambassador, not just
for what the technologies can do, but also what the technologies can't do.
And you start to be able to ask the question of whether these technologies should be employed
in this way.
Or is this really going to, I find in tech often, everybody's going after accuracy or specific
requirements along those lines.
But instead, when you're at a professional services firm, you start to think about things
in terms of more human metrics like, you know, are we actually leading to higher quality?
Things like that that I think is really key in helping to understand things.
And finally, I do think that it's about helping the business transform digitally so that
they can help support these systems.
So just like anything else, I mean, they need the appropriate data.
They need the appropriate improvement methodologies so that they can go forward.
And actually, you know, you have all of this beautiful technology and all these beautiful
methods.
Can you actually implement them, but then also improve them so that they can realize their
real potential?
Yeah.
I like the way you put that.
One of the things that I am finding, you know, interesting and inspiring or I've been
inspiring is maybe not the word but heartening is that we're starting to hear more of a
shift from the kind of asking the Ken Wee question to the should we question, and maybe
it's not even a shift, but you know, we're at least starting to hear the should we question
in the context of AI and thinking about ethics and appropriateness and, you know, the implications
of some of this, some of these technologies are those questions that you're focused on
answering as well?
Yeah.
And in fact, KPMG even before I started was already going down this line.
So they're backing and data analytics has always been around the trust aspect, right?
And really about the fact that their clients employ them not for, you know, efficiency or
something along those lines, but really for trust.
And also KPMG de-auditors, especially, you know, we are responsible for trust in the capital
markets because we audit financial statements, you know, we have that responsibility.
And so this has been a part of the conversation at KPMG for a long time and it really intrigued
me because when we, when they were embarking on building out these types of technologies,
they already had that mindset.
And it was now really a question of how do we bring in the appropriate people to help
us build these out in this way that we want to do.
And I find that really, you know, at first it might have felt a little bit like a step
backward because it means that, you know, you don't have the gobs of data.
You can't use the way to state-of-the-art everything, right?
It means that a lot of times you have to specifically pick your partners for, according
to a different criteria than you normally would.
But in the end, I think that what we're seeing, especially now, and this is now two years
after I started at the firm and probably, you know, several years before that KPMG had
already started on this journey, now you're actually seeing that that others are catching
up to that way of thinking as well.
So one of the main topics that we wanted to jump into in this conversation is some work
that you've been doing recently around the idea of knowledge graphs.
Can you start us off by explaining what a knowledge graph is?
Sure.
So a knowledge graph, I think a very simple way of thinking of a knowledge graph is essentially
just a network of concepts.
And knowledge graphs are a simplification of more formal ways of defining relationships
between concepts and entities.
But that simplification allows us to use a variety of techniques that might not always
be possible if we go for the stronger versions.
And so for at KPMG, for example, we have a lot of processes and a lot of expertise and
knowledge that we want to have encoded in a way that we can make use of.
And a lot of times, it's already been put into something like a taxonomy that someone
somewhere in the organization has to keep up to date in some way.
And what we try to do is we've also hired an oncologist to come in and not only help
formalize that knowledge in a way that's actually a little bit better.
But we also use take that kind of stronger version and we simplify it into knowledge graphs
to allow us to, for example, take our knowledge about one area and apply it to our knowledge
about another area or take internal knowledge and apply it to external news, for example.
So we use knowledge graphs to be able to take our internal taxonomies, ontologies about
client issues and be able to map them to news that is out there.
And the knowledge graph is actually a much easier way to do this than a more formal method.
And it also allows us to work with a variety of news vendors that are already using these
types of technologies.
So when I think of things like taxonomies and ontologies, one of the first things that
I think of is this idea of knowledge management.
It's an area that I did some work in a long time ago.
Knowledge management and document management, there's, you know, those are, that's a kind
of mature space unto itself.
Where does cognitive come into all this?
So it is a mature place, you know, a mature space.
Those technologies are mature, but that doesn't necessarily mean that everybody has implemented
them appropriately and that that implementation extends across the whole enterprise.
And so one of the things that we found and I admit, this is not my specific area of expertise.
This is, we have others within my lab, even, who have years of expertise in this area,
but I had the opportunity to work with them.
But the main challenge with knowledge management is that it's not necessarily done in a way
or if it has been done, it hasn't necessarily been done in a way that allows us to take
advantage of the data or the information that's available for artificial intelligence systems.
A lot of times, that's one of the problems.
Now at KPMG, and I think that you see this set of variety of other firms and organizations
as well, that digital transformation hasn't necessarily happened evenly.
So while there might be certain places where we have, say, data lakes, in the end, we don't
have them everywhere.
On top of that, we have another issue, which is that we have a lot of restrictions when
it comes to client information.
We are very restrictive and risk averse when it comes to client information.
And so a lot of the processes end up being manual because of that.
So there has to be a culture shift there as well.
So you're absolutely right, some of these methods are fairly mature.
And we know what the methods are, we just have to implement them.
And I'd say, for example, in the way that we're using knowledge graphs, this is not something
that we're making up.
In fact, if you look at a lot of CRM solutions, this is how they operate.
They have some kind of an internal taxonomy or internal idea of what are important concepts.
You can go off and look at external news or else you can look at information between services
and engagements and you try to map them out appropriately.
We're doing exactly the same thing.
But I think the main distinction is that we're putting our own spin on it.
We're taking information that has long been internal to KPMG.
And now we have the technology to be able to use that information and map it out to external
news in a way that actually benefits our workers.
So taking advantage of these techniques that have matured under the banner of knowledge
management, are you also then incorporating in artificial intelligence and cognitive
technologies?
Well, presumably you are.
Is that correct?
Yes.
Yeah, we do.
So this is one particular project where we're using knowledge graphs.
Now, we don't just serve up the information, right?
So it's not just that we have external news and we say, hey, here's a bunch of news articles.
We actually do this in a way that allows us to contextualize it according to specific
recommendations that will help our account leads.
So we have information about the services KPMG offers.
We have information about previous engagements.
We have information about the clients.
We have now external news and we're able to use the knowledge graph to try to map all
of these appropriately and then also try to see what are the top recommendations we have
for services.
And then we use our news articles or previous engagements as support for that, right?
So it's kind of like evidence for why we're giving this recommendation.
And all of this, so it not just the ability to use knowledge graphs in this way, to be
able to map across all of these different areas, but also the recommendation engines use
a variety of techniques that, you know, whether they're, it's kind of difficult to say where
they fit in, right, to AI, but they're within the context of general machine learning and
general practice.
It's more a question of how do we apply it to our specific business?
Maybe let's take a step back and kind of put a fine point on this project.
You are essentially building up a, you know, would you call it a dashboard for these account
managers that helps them kind of stay on top of news that affects their clients?
Yeah, exactly.
So this is kind of an additional insight engine that we provide to our account managers.
And one of the things to take into account is that especially when these systems go live,
you know, we would put this up and, you know, our top account managers say for our, you
know, most, you know, our top 100 or 200 accounts, right?
These are people who really know their clients, these are people who really know what's going
on.
They are reading the news and they have their own intuitions.
These are our top experts in terms of what's important.
Now this system is likely not going to provide that much insight for these top accounts where
we already have a lot of people looking into this and a lot of this information isn't
missed.
So where you really start to see the advantage of this is when you extend past that and
what we're really trying to think of is how do we take the quality that we're able to
provide for our top, from our top experts to our top clients and how do we expand that
beyond?
And when you think further back, so when you think about the fact that, you know, other
account managers might have, you know, like 20 accounts that they have to, you know,
20 clients that they that they have to somehow manage and juggle and they might not have
to be able to, for example, look into the news every single day for those.
At that point, you start to see why this kind of a recommendation engine might help us find
things that we might have normally missed and help us expand our quality beyond the what
we're currently able to provide to only a few clients.
Right.
Right.
Well, I can relate very personally to the challenge that you're trying to address here, just
in my own role as an industry analyst and, you know, someone that needs to stay on top
of, you know, not just kind of what the large vendors are doing in AI, but what the smaller
vendors are doing, what the, you know, the major technology platforms and frameworks and
there's just a ton of information out there.
And, you know, like you said, I've got some kind of go to sources and some go to feeds
that and tools, even that I can rely on to help me stay informed, but I am often, you
know, almost daily, you know, wishing that I had something that was smarter, that was
more intelligent, more connected, more tied into the things that I need to stay on top
of that could be almost like a, you know, a personal agent, a personal assistant, a
dashboard that surfaced all of the things that I need to, needed to know, but also, you
know, pushed down the things that aren't as important for me and was able to learn
the difference between the two over time.
Exactly.
And I think there's two important things about you're saying there.
One is, I think just the process of understanding how to encode the expertise that you have,
right?
So the process that we have, our experts go through, like especially with our chief
ontologist in terms of being able to better hone, what are the important concepts?
How are they doing their jobs right now?
How do we take that important information and how do we build out exactly what those intuitions
might be?
I think that's one really good thing that comes out of transforming, say your, you know,
what you yourself do as an analyst into something that could be more like a digital
process, right?
Then the other thing as you're saying is absolutely, once you have that, you can really start
to play with how do we start thinking about how we can expand the quality, you know, you
want to be able to focus your attention and your top quality to the best things.
And this really gets into the expert augmentation space, which is a key tenet of how we approach
building these systems at KPMG for, say, our auditors, right?
You want to be able to take your expertise, try to be able to go out and discover as much
information as you can, but then also allow the human to come back and make conclusions
on that.
And hopefully through the process of, there would be an iterative process where you would
be training, say, this assistant, you know, we try to make it out so that it would be
more like an intern at first, right?
So what could you expect of your intern?
We try to get our systems to a level where on day one, they'd be kind of like an intern,
right?
But hopefully through the process, we would improve it to the point where they could become
relied on more and more.
And that's really what you want to get to.
I love that idea.
So what are some of the, some of the technology pieces upon which you built this.
You mentioned Watson earlier and working with some of the first Watson technologies for
healthcare are using Watson in here as well.
So we had originally done, so I had previously done work with Watson.
And now when it was a lot of the work that we're doing at KPMG, some of it goes with IBM
products and various things in Watson.
Now this particular one, we originally, the original prototype was with Watson company
Analyzer, which was, they had bought Alchemy News, which was a news service and they were
able, they were essentially doing this, they had a knowledge graph, Alchemy News had a
knowledge graph of news articles for the news, and they were able to deliver content
in a specific way.
And what we wanted to do is we, you know, their knowledge graph wasn't quite as extensive
for financial services or for the information we had, right?
So it was definitely good because it allowed you to have a general view of news, which is
something KPMG can't have.
We don't have that kind of taxonomy, but we definitely have much more specificity when
it comes to client issues with financial services.
And it turned out that being able to map those two across was not working out very well.
So what we ended up doing was we ended up bringing it in-house and we built, we've played
with Watson Discovery services and using that here.
We use Watson Knowledge Studio for Entity Extraction and the actual knowledge graph and the
reasoning components actually aren't Watson components right now, but that was mostly
because of just the difficulty and, you know, making sure that IP, RIP state, R's and
their IP state there's, so it was more of that kind of a concern.
Oh, it makes sense. And so what is the Watson Discovery service and what's the role
that that piece plays for you?
So Watson Discovery service is where IBM has put a lot of their search works recently.
So they used to have something called Retrieven Rank and they've built out Watson Discovery
service to be kind of a much stronger search engine.
And the advantage is that you can use Watson Knowledge Studio.
So Entity Recognition, you kind of can train your own Entity Recognizers.
You can do a search over, you know, it allows you to index.
It allows you to create a ranking engine over what's searched and it allows you to build
out queries.
It's kind of like a big search interface, but they also call it the Discovery service
because they've also expanded that.
So that now you can kind of, there's also an interactive quality.
So it's not just the delivery of a search engine, but there's also an interactive quality
in terms of you're being able to look through your data and find insights from it.
So they've combined a few services through it.
And so how does the Knowledge Studio interact with the Discovery service?
What you can do with Knowledge Studio is, Knowledge Studio allows you to build out Entity
Recognition, essentially, or Entity Extraction models.
One of the first use cases or examples I think of when I hear that is like a chat bot
where you're trying to identify like intense or, you know, entities that you're, that
someone is asking about via a chat conversation.
Is it also, in this case, is it used in that context or like in a search context where
you're refining search results based on entities?
So what you can do is you can actually use those entities to index what you're searching
over so that you could, just like in a, in a chat bot, you would sit there and you'd
say, you know, you want to mark out the, the fact that, you know, somebody wants to fly
to Calgary, say Calgary would be where they're flying to, right?
So what's a Knowledge Studio would be able to tell you that that would be like the location.
To similarly, you could mark up, say, all of the search pages that you're searching
over, right?
So all of the content you're trying to search through, and what's a Knowledge Studio
could go through and say, oh, well, you know, every time you seek Calgary, that's actually
a location that someone could fly to or something like that.
So it's, it's basically the same thing and it allows you to augment your search appropriately
with, with concepts that, you know, you can train on yourself.
So for example, for us, you know, a specific individual could be a corporation and you
might be able to find that from just any named entity recognizer, but for our purposes,
we really could think of that person as a borrower, as a borrower.
So that's like a borrower in a loan agreement, say, and we would want to capture that information.
That's what Watson Knowledge Studio would allow you to do.
It would take that information, say, you know what?
This is the borrower and this is the guarantor name in this, in this loan agreement.
Watson Knowledge Studio allows you to, to find that information and to train models that
can, that can help you find the information for the concepts that you're interested in.
And then from that, you can build out our Knowledge Graph, of course.
Okay, interesting.
And so presumably with this example you gave of the, the borrower and the creditor, it's
not like a place, you know, versus a company name.
They're both company names and so presumably it's able to figure out the specific entity
types based on context.
Yeah, exactly.
So essentially in the back end, it has, it uses features of, you know, different types
of, of sentence based features.
So I assume they have ngrams of some kind and other things that allows them to differentiate
that.
And then in terms of how you train it, right, it's all about you, you annotate yourself
and you, you train it based on lots of information.
And so if you provide it with the appropriate types of information that can differentiate
those two, you should be able to arrive at a model for borrower and a model for guarantee.
Do you have a sense for the number of training examples that are required kind of per entity
in order to get a reasonable level of accuracy?
So yeah, generally what they recommend IBM.
So to be clear, I've never worked with IBM for IBM, but I have worked with their products.
Generally, it's going to be about 50 positive examples that you want to provide.
And for each entity and a positive example, of course, means that it's actually there.
So you can't just like provide 50 documents and sometimes the guarantee name is there and
sometimes it isn't, right?
So you want to have at least 50 examples.
In practice, we actually, it really does depend on a lot of variables in terms of where
that entity shows up and what you're trying to differentiate it from in order to determine
what the appropriate number are, but I would be hesitant to even rely on results if
you don't have at least 50 positive examples.
We usually go for about 200 to start.
I imagine it's per entity.
Yeah, per entity, exactly.
So you're right.
You could have a document where you're trying to get 10 things out.
You can't just go for 50 of those.
You might have to end up.
That's why I say we usually end up with about 200 documents, and hopefully through there
we can find at least 50 positive examples for each of the things we're trying to pull.
You have the entity recognition system.
You've got the discovery service, which is kind of a search engine that can take advantage
of these entities.
We've experimented with what's in discovery service, but I don't think actually right
now we have that in play, but that's one possible way that you could actually build out this
kind of thing.
Essentially, what we've built out is it's a typical recommendation engine, but we also
have a graph-based database for the, that represents the knowledge graph.
We essentially pull in, we pull in specific articles, we rank them, and we rank them according
to what the recommendation is for the service.
So for example, it's not, we want to recommend a specific service that KPMG offers for a client.
And then once we've made that recommendation, we show as evidence, news articles, or say
history of engagements, other engagements that are similar, a variety of other factors
that may come into play to determine that recommendation.
So as you can see here, the knowledge graph is not in and of itself what we're delivering
is more of a way for us to rank appropriately the evidence for us to recommend a service
for a client.
Where do the, where do the news feeds come from?
So we have, we use the news feeds that are most important for KPMG, but you can imagine
they're, they're very similar to what you would normally expect, but it's not going to
be like tech news feeds, it's going to be more business oriented news feeds.
Right.
So maybe like a Thompson Reuters or something like that, that is like a third party feed
that you're pulling in.
Exactly.
Something like that.
I've talked a little bit about in the ideal world you've got.
This is able to learn over time from the interactions with the actual users.
Have you implemented that part of it yet?
So we actually just went live across 700 accounts just about a month ago.
So we, we're still very much new in terms of the improvement strategy, but we have piloted
that.
And now it's a question of how much, you know, what, what it really comes down to is you,
we're not just going to accept, for example, any feedback as some, as a corrective measure
for our recommendation engine.
So it's really going to be a process where the technologists and the data scientists
work hand in hand with the, with our best experts to determine, you know, based on the
use and the patterns that we're seeing, what are the things that will allow us to best
optimize and improve the system without, for example, going too far down edge cases or,
you know, being, you know, really looking at the data, but having an expert side over
it.
That's really going to be a key part of the improvement strategy.
You mentioned the entity recognition piece, and I recently did some work with actually
a group of accounting firms talking to them about AI and its implications in that space.
And one of the use cases that came up quite a bit, and KPMG has been very active in this
area as well, has been using natural language processing for document and contract review.
And in particular, lease review and lease abstraction, there's been some recent regulatory
changes that go into effect the beginning of next year, I believe, that change the way
leases are accounting for and many of the firms have jumped on AI as a way to automate
the review of, you know, in some cases, tens of thousands or hundreds of thousands of leases
to figure out what those, the terms of those leases are, and this concept of entity recognition
plays pretty heavily in that whole use case.
Have you been involved in any of the work on that type of application?
Absolutely.
Yeah, you're absolutely right.
So the changes in IFRS 16 are what are causing all of everybody to kind of go around
in a panic here about this.
Yeah, so this, you're right.
So I think at the end, what everybody would love to see is an automatic way of finding
this information and a contract pulling it out appropriately and then determining based
on it what the appropriate treatment is.
And KPMG has a few different options.
So one thing is we have the KPMG leasing tool, which is really once, you know, you have
the, the terms and all of the, and all of the options and all of this information out
of the lease, it's able to tell you what the appropriate treatment is.
But one of the difficulties is that it's actually a manual process to go from the contract
itself, the language in the contract, into an understanding of that contract to the
point where you could actually say extract entities or facts from it in a way that would
make the meaningful input into the KPMG leasing tool or any other, let's call it calculator
of what the appropriate treatment would be for the leases.
So this lease abstraction or we call it extraction work is something that my group has been
involved in.
There's a variety of groups who are involved in it across the firm with a variety of different
ways of doing it.
So we have people who are going after this from a very specific, you know, delivery model
where, you know, you have 10,000 leases and you want this particular, you know, just
a few entities abstracted from them, then we'll be able to deliver something that gives
you like a spreadsheet of all of that.
Now the work that I've been working on has been more, I'd say, more at the managed service
or more at the expert augmentation level where I haven't just been focused on lease contracts.
I've been working for the past few years on trying to build out a contract reading tool
more generally for auditors.
Where the idea is that I'm coming from the perspective that the facts that have to be extracted
actually require a significant amount of human interpretation.
Like so, usually with extraction, you have, let's say an entity and it's fairly easy to
take, pull that string out of the text and then it's even better if that's the exact
string that you would want to use further down the process, like whether, you know, input
it into an Excel or put it into a calculation, but generally there's some level of human
interpretation in terms of taking the that, say, entity and applying a calculation or
resolving it in some way so that it's meaningful later on.
My feeling especially when you look across the space of contracts is that, you know, maybe
about 20 to 30 of these entities and leases are fairly easy to extract, but just based
on the, you know, the differences in language around real estate leases versus equipment
leases, for example, it's very difficult to build models once you get beyond those
20 and 30.
And a lot of times you do have to go beyond that to build up the calculations appropriately.
So we build out expert augmentation systems for auditors where it helps them find the information
in the original document.
When we're able to find it, we take them there when we're able to, when we can't find
the exact information, we try to take them to where in the text we think it is.
And even better when it's something that requires several pieces of text that need to be resolved,
we try to bring them to all of that text so that they can resolve it more appropriately.
So we don't kind of, we really keep our knowledge workers in the system.
This is very much expert augmentation for a managed service, which means a human's always
going to be the person who's the ultimate arbiter.
We're not really looking for automatic extraction, but there are other parts of the organization
that are using very different techniques in order to be able to do that as well.
Can you give us a specific example of where you need this expert's perspective to fully
resolve an entity?
Yeah, I think a lot of it comes down to.
So sometimes you have to determine, for example, what is the, I'm thinking of service contracts,
but there are other contracts where you have specific terms and you have to think about
the frequency of something.
A lot of times the way to determine the frequency usually involves a few different areas where
they'll, they'll name a frequency, a specific frequency, but then later on they'll say, except
in the case where you have this in which case it goes to this other type of frequency.
Well, this is exactly the kind of thing where when you look at what the input these calculators
are, it's going to be something like monthly.
When you look at the text, it requires a significant amount of human interpretation to get
from say three paragraphs to be able to determine that it's actually monthly for this
specific period.
This is the kind of thing that I'm thinking of as being somewhat difficult.
Now, you could argue that these might be edge cases and it could be when it comes to
the IFRS 16 changes, right?
So I'm thinking about this from the perspective of auditors who have to go through a lease
and find all of the information very, you know, very specifically they can't miss information.
Right?
It's a different perspective if you're looking at this from the advisory case where it's
kind of like, I got 10,000 things to go through like just find me, you know, the top 10 and
then I'll figure out what to do with the rest.
Just I need to, you know, I need to do this quickly.
So it's a different use case, but I come at it from the perspective of the auditor and
helping them not miss information rather than just trying to get the information as quickly
as possible.
It's an interesting idea, a lot of the lease extraction or contract extraction products,
projects that I've seen are kind of pulling out this simple, you know, some simple entity
or trying to do simple calculations to resolve, call it first degree abstract entities
like, you know, the contract duration is a year, you know, and you've got the date that
the contract is signed, you can figure out the end date of the contract, for example.
But it does strike me that I definitely can see how in some scenarios what you really need
is to pull out the rules themselves because that's what the person who's going to end up
reviewing this, you know, needs to be able to think about are the, you know, the different,
you know, scenarios.
And even the representation of that, like, how do you stick a rule into an excels, you
know.
Exactly.
Exactly.
And when you look at some of the excels sell spreadsheets that are being input into these
tools, I mean, it's really ridiculous how they try to break this apart because you
can see the kind of contortions they had to go through in order to build this out appropriately
to get all of the information they need to apply the appropriate treatment, the calculation
that determines the appropriate treatment and the fact that there's a lot of complexity
there.
I think that it's important.
And I should say, for the easier entities, we're doing the same thing as everyone else.
We're using entity extraction and we're doing fairly well with it.
But the difference is that we're, we want to be able to provide a tool or tooling that
allows our auditors to read any type of contract and find the appropriate information.
And I think that's a slightly different take on it.
But, but yeah, believe me, everyone in KPMG, well, a lot of people in KPMG are very much
interested in what the appropriate way to do this is.
And I think it really does make sense for KPMG to have a variety of business models depending
on what the client needs.
Absolutely.
And as you will know, not just KPMG.
Right.
Very, very fast.
Interesting.
So I'm curious as we maybe start to wrap up, like, you know, a lot of what you are focused
on here is this idea of expert augmentation or augmented intelligence, you know, have
you want to describe it, are there some general principles that you've taken away from the
various places that you've worked to use AI to, to augment experts?
Yeah.
I think so. I mean, I know that there are some tenets that I really feel strongly about
in the systems that I design and that I try to push for.
One of them, I mean, it comes back to what we had discussed early in the conversation.
It really is about the appropriate way to codify knowledge and expertise and finding the
best way you can do that.
And I think at KPMG, what we've learned is that we try to employ various techniques, but
we really want to make sure that we use the best possible technique for posterity.
So we do use ontologies and we try to encode our information in ontologies as much as we
can.
But at the same time, we try to extract away from those or simplify those when it's needed
for a very specific technical implementation.
So we understand the limitations of both and we try to be able to use them appropriately
in the right context.
The other thing that I think is really important is I'm very interested in the type of human
that we are augmenting and the role that this augmentation has.
So I think that it's very different building out systems for efficiencies versus building
out systems for quality.
And I personally have always kind of been intrigued by expert augmentation for quality.
So like I said, this might come back to the fact that I was working on Watson for health
care originally.
And that's one of the driving questions, right?
You don't necessarily, you want to make a doctor's life easier, but so that you can
increase the quality of what they're able to provide.
And so it's very similar for me, I really think about the fact that our end result, for
example, when we're looking at audit, is not to miss information.
We, you know, that is the biggest risk for audit is we have to be able to say we've looked
at all the information we're able to deliver the appropriate opinion based on it.
And I find that is a very intriguing way to build out these systems.
And I really want to make sure that the knowledge workers are in that system and are being
there to train the system appropriately.
So I've been, I talk a lot about machine teaching versus machine learning and this whole idea
of having the system as kind of an intern at first for our engagement teams.
This is the kind of thing that I think will help us build out the next generation of
systems.
Because in the end, there are, there are a few of these experts.
You know, I'm not, I'm not working with like say in the consumer space where we have
millions of users, you know, in the end, there might be only a handful of experts
in the world with this particular who understand this.
And so how do we encode that information?
How do we capture the decisions they're making and how do we, and how do we make their
lives easier so that they can, in the end, deliver higher quality decisions?
Yeah, really interesting.
I can, you know, I can continue to talk about this forever.
Like I said, a lot of it, I can relate to very personally and, you know, one of the things
I'm most, yeah, I tell people all the time, like I think the exciting thing about AI is
its ability to give people superpowers.
And it's particularly the case for, you know, or at least I think about it most frequently,
you know, for folks like me that are trying to manage these torrents of information.
And AI is a really powerful, you know, set of tools to help us do that.
In fact, there's so many different ways that AI can be used to, you know, augment, you
know, expert workers or knowledge workers.
And so it's really interesting to learn a bit about the different ways that you're enabling
that.
Well, thank you.
It's a pleasure talking to you as well, and it's exciting to hear that there's interest
in this little corner of the world that I'm excited about it.
Absolutely.
Thanks so much, Marisa.
Thank you, Sam.
All right, everyone, that's our show for today.
For more information on Marisa or any of the topics covered in this episode, head on over
to twimolei.com slash talk slash 204.
If you're a fan of the show and you haven't already done so or you're a new listener
and you like what you hear, please head over to your Apple or Google podcast app and leave
us a five-star rating and review.
Your reviews help inspire us to create more and better content and they help new listeners
find the show.
As always, thanks so much for listening and catch you next time.
