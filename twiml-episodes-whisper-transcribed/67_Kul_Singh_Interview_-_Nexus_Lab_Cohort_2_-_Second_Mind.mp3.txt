Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I'd like to start off this show by sending out a huge thank you to everyone listening.
We've dropped a ton of great interviews over the past few weeks and through your dedication
we continue to see a growing outpouring of feedback comments and shares with each release.
If you're a regular listener but don't normally send in feedback we'd really love to hear
from you.
So please head on over to Apple Podcasts or wherever you listen and leave us a review.
A 5 star review is of course appreciated but what's most important is that your voice
is heard.
It lets us know what you like or what you feel we can improve on and it also lets those
looking for a new machine learning and AI podcast know that they should join the Twimble
community.
Speaking of community, the details of our next Twimble online meetup have been posted.
On Tuesday November 14th at 3pm Pacific time, we'll be joined by Kevin T who will be presenting
his paper, active preference learning for personalized portfolio construction.
If you've already registered for the meetup, you should have received an invitation
with all the details.
If you still need to register, head on over to twimbleai.com slash meetup to do so.
We hope to see you there.
Now as some of you may know, we spent a few days last week in New York City hosted by
our great friends at NYU Future Labs.
About six months ago, we covered their inaugural AI Summit, an event they hosted to showcase
the startups in the first batch of their AI Nexus Lab program, as well as the impressive
AI talent in the New York City ecosystem.
While we were more than excited when we found out they would be having a second summit
so soon, this time we had the pleasure of interviewing the four startups of the second
AI Nexus Lab batch, Mount Clevverist, Bite.ai, Second Mind, and Bowtie Labs.
We also interviewed a bunch of the speakers from the event and will be sharing those discussions
over the upcoming weeks.
In this show, I speak with Cool Singh, CEO and founder of Second Mind.
Second Mind is building an integration platform for businesses that allows them to bring augmented
intelligence to voice conversations.
We talked to Cool about the concept behind Second Mind and how the company combines ambient
listening with a low latency matching system to help users eliminate an estimated two
and a half hours of manual searches per day, and now on to the show.
All right, everyone, I am here at NYU Future Labs, speaking with some of the AI Nexus startups,
and I am with Cool Singh right now, founder and CEO of Second Mind.
Cool, welcome to this week in Machine Learning and AI.
Thank you.
It's a great time.
How do we get started by having you tell me a little bit about your background?
Sure.
I came to New York as a derivatives trader, and so I was doing what was considered now
with considered AI back then.
We were doing AI's effectively expected value, and we were doing very intensive Monte Carlos
simulations and doing from mortgage derivatives and other structured type products.
So that was where I started, and then I took doing that and building the systems that
I was on the trading side, but building the systems on that, I got an interest in technology,
and I started two companies that were in the low latency side of it.
So two technology companies, and basically one company is actually still operational with
the product still being adopted across many enterprises, Fortune 100 companies, and I have
two patents in terms of low latency systems.
And then I actually started a lab with a team in Ukraine and Eastern Europe that was focused
on NLP and AI.
That sort of built back, and it's early in terms of NLP, but they've been doing it for
a while, but built early shop bots around travel and some other things.
So I've been doing this for a while, and then I started having an interest in an idea
around this particular company's second mind.
And so what does second mind do?
So basically we're looking to solve the time that you're spending on search.
And basically we sort of accept the fact that we have to find information, and we do have
search engines now, and we accept that's great.
But the fact is we're now spending about two and a half hours out of our day on average
searching for information.
That's 30%.
Okay.
If I told you you're stuck in traffic for two and a half hours a day, you're like, okay,
I have to fix my life.
But this is what we're doing.
This is in 2016, the type of data, so it's not like this is getting worse, the problem.
I mean, just as you say that, I reject that notion, but it sounds about right.
Yeah.
I mean, you have to think of it as simple as saying, are you available for lunch next
week?
You're searching for your calendar.
Where should we go for lunch?
You're searching for restaurants or whatever it may be.
It could be for information like from your CRM or for if you're in the trading world for
market data.
So you're constantly looking for information.
And the fact is we also don't know what we don't know.
So that problem takes time.
So that whole situation creates this situation where we're constantly looking and searching
for data, people are overwhelmed in that thing.
And we've heard this information overload, but this is actually a slightly different
thing where yes, we have so much data in so many different places and to manage it and
so forth.
We have search engines, but that's still not working.
So what we're looking to do is say, hey, can we solve that problem specific to conversations?
Okay.
The fact is the impetus for many of our searches are driven by a conversation, just as I gave
examples for.
So clients are saying, can you have that information?
And the fact is when you're on a call or an in-person conversation like we are now and
someone asked for that, you're either having the multi-task, which is obviously stressful,
or you're saying I have to get back to you later on that.
And what's worse when you have a client here to say I have to get back to you, someone
else is going to close that business.
So from a business standpoint, this is a very costly problem because they're spending,
not only the money to employees, it works out from an $80,000 every side, about $14,000
annually that's just gone and wasted.
And on top of that, you're using those two hours a day of searching?
Exactly, that type.
And then you're also losing the opportunity cost in terms of the revenue of not closing
the clients when you have that information, right?
So that's the problem we're looking to solve and so what we basically have developed is
you can think of this matching engine.
I'm sort of using a Wall Street prerogative here, but basically, you can think of like
it for it because an exchange would do for stocks, matching buyers and sellers, what we're
doing is identifying the events in a conversation and mapping it to any data, whether it's internal
or external, and pushing it to people in real time.
And so that can be information from your CRM, from your market data, from say a Bloomberg
or Reuters, it can be from your email or files.
But someone says, you know, as you get that file, I said, it shows up right away.
You know, you don't have to sit there and fidget.
This is a typing part takes time.
Right.
So that's sort of the thing.
And the way you can think of the product, it's sort of like, you can think of like a
slack for conversation or the glue to provide all those applications that people can
developers and companies can tie in and it's as a feed as you're speaking.
And you're using conversation and speaking as the way you describe these interactions,
but then you reference slack, is it primarily for textual communications or are we literally
talking about speaking?
Yeah.
So our focus is around voice competition.
That's sort of where RIP is.
Sorry.
And we can do regular conversation, text and chat conversations as well.
Will we see the real opportunity and the unique differentiations around voice conversations?
Yeah.
I mean, when I think of the, you know, the way we've envisioned some of these interactions
like from a sci-fi perspective and kind of this notion of the augmented human, right?
Like it's like you want something that's kind of passively listening to all these interactions
that you are having and then just making you smarter by popping that email up or, oh,
you just mentioned lunch, you know, here with three places that, you know, fit the profile
of the kind of place you like to go to without having to pull out the phone and the keyboard
and all that kind of stuff.
Exactly.
It's a big vision we have, it's around the AR side and you sort of where people can be
effectively Jarvis and Iron Man where you can show that.
And so, I mean, that's not that far afresh from our standpoint because it's just a screen
from our standpoint.
We can display that information on any screen and basically it's just what we're providing
is a low latency matching system to provide that data and any screen so that it can be
that phone, your mobile phone, it can be on the desktop.
If we can connect with the enterprise voice, it can be a video calls or it can be in an
AR.
And so, do you end up taking a position on kind of the pervasive listening aspect of
that or?
That's a great question.
So, one of the unique IP aspects of our product is that we don't need to record conversations
optionally we can.
So, a lot of the companies that sort of are in the voice type space are recording data.
What we've really, again, goes back to sort of my background with sort of like in line
streaming of this data, throw it a process and throw it away, in fact, we are core data
that we're capturing as a metadata of the mapping engine of the maps.
That's where we're really focused on.
So, we basically are looking to process in line, everything's in memory, we're not, you
know, that's sort of, so, you know, where a lot of the companies are saying, hey, we're
building on this unique part of the components and so forth, where our unique things that we're
doing is focused on the real time low latency on the front end, not on the training side
so much, but on the front end where very few companies are basically, we have, like, you
can think of it like, again, the best way to think of is like a high-frequency training
system or like what you see on Wall Street where you're processing trades very, very fast
and we're looking at cashing that data very fast and processing and mapping it very fast
to that conversation.
So, that's really where the unique sauce comes in and so we don't need to record the
conversation and start analyzing it, capturing the events and we will store the events so
that could be like data that could be saved in the CRM or so forth or summering, but
it's not necessarily the actual conversation.
Okay.
So, I get that you're not required to record the data in order to do the things that
you're trying to do, but a lot of companies, you know, aren't recording data because they
need to to deliver their service to recording the data because they use it to train and
make their algorithm smarter.
So, you know, kind of flipping it on the other side, you know, if you're taking a hard
position, it says we're not going to record any data.
Do you miss an opportunity to produce better product?
That way.
It's not, so to make it clear, there's the recording of the audio, which we're not focused
on.
Where we are focused on is we are storing the metadata, which is the mapping piece that
we are maintaining and that we do need to know.
So, basically, for that standpoint, we can then see where our engine is getting false
negatives, false positives, and so forth.
So, we're seeing and we can see where we are mapping the right data or the wrong data
so forth.
So, that's for us, I mean, that's why, you know, using this analogy of where real-time
mapping engine or matching engine.
Let's use the analogy.
If you go to whatever you might use for an online trading system and you go, I want
to buy Apple and it gets you Google, you're not going to be happy with it, right?
So, that's what we are looking to ensure we don't do that, but it's different than recording
someone's conversations, storing those conversations, where a lot of the companies are focusing
on analytics and storing that, which then becomes, it's actually just a legal aspect to
it.
You have to tell people this conversation is being recorded and monitored, that's a law
across the most states.
Because we are not recording or monitoring those conversations, it's just being analyzed
and, you know, inline and done away effectively.
We don't need to even say that in that standpoint.
You take in the voice and then you are you converting that to text and then your metadata
is based on a text transcription of what the person said.
So, they were identifying keyword to key phrases and then when we were building, so basically
our chief scientist had written some papers on this.
He was most recently a professor at Institute of Budapest, where he wrote some papers around
the concept of attribute variable matrices and doing it in real time.
So basically the idea would be in a conversation, it's not always a linear process of finding
and achieving it when an event is achieved, so when I say an event means, when an event
is triggered.
Okay?
So, if you think about it, if I say, if I go to buy a train ticket and I say, okay, I
want to buy a train ticket, okay, I can't buy the ticket yet, the person asked me, where
are you going and what date, what time, now I can start maybe by the train ticket, that
the trigger is now finally, it gets me how many people and so forth, right?
So in the same way, we are doing the same thing where we basically set up this matrices
which says, okay, let's identify from the text, we can start filling out that matrices
and is there an event trigger?
If one set event gets triggered, it could be one keyword, it could be a bunch of keywords
that we need to hear.
So for example, it could be on a trading floor, it could be how many shares traded, that
you can't maybe trigger there, but if someone said, what do you think of IBM, and then
said, how many shares traded, or now we know for IBM, right?
Or someone says, how many shares of IBM traded, we can then find that.
So in that vein, we don't need to record the audio or save the text, we just need to
capture the fact that someone wants to know how many shares and what stuff, and we can
capture in any order, and then when it's triggered, we map it to the data that we know
where to find it.
So I guess the personal assistant type of use case that we discussed is kind of a clear
one.
Are there other use cases, or what's the initial target customer and use case that you're
going after for this?
So probably as I said before, there is that knowledge worker in general that we use that.
We're seeing interests from the financial services vertical, financial advisors, traders,
where there's a need for having that information, salespeople having to get information, close
that sale, and having that data for managing that information, and so forth, obviously.
But any market, maybe think about a real estate agent, someone's calling up, but they can't
manage all the properties in the MLS listings, right?
So someone says, what properties available in the West Village here in New York, they're
not going to know.
So we could push intelligently what those properties are based on some criteria that
person says.
You can now deliver that information intelligently.
One area we're seeing, we're sort of working with a data provider and exploring these
around attorneys.
They're on the phone all the time, they're in conversation all the time, and they're constantly
trying to get information on precedents and other legal facts that require so much search
and data.
So the more we can push that to the person as they're having the conversation, when now
you don't have to have that asynchronous process of, let me get back to you and let's set
up another call.
Let me think about the time it takes to set up another call, get on the call, the process
of a few minutes of talking, okay, chat, chat, chat, okay, now let's get to it.
Then, oh, I don't know that answer, let me write that down, get back to you.
And that constant process, we are trying to streamline that where you can now take maybe
two, three, four calls and you can make it into one, maybe two.
But you're basically now significantly reducing the amount of time that people need to be
on those calls and searching, and you're basically making people, you know, much more efficient
productive, and as you sort of said, hopefully a little, a dream of being a super intelligent
or smarter than otherwise.
So we've talked a little bit about the front end of this process.
How are you handling the kind of knowledge, retrieval aspect of it on the back end?
It strikes me that there are a number of interesting challenges there that you'd have to overcome.
So the first thing we're doing is in terms of our core product, we are offering it to,
you know, we're initially focused on enterprises, and we're offering that as a virtual machine.
And we offer, again, this goes back to some of the IP and technology I had built in a previous
companies, which is sort of like a very much of a published subscribe real-time architecture.
So that's just, I'm sort of, you know, one of the things I've always loved about building
companies is you sort of, you find the, the sort of the gap between two areas, and you
sort of focus on there, because if you focus on, on just a machine learning and so forth,
you obviously have very big players that have a lot of expertise, and you focus on, say,
publish, subscribe in real-time, where you have TIPCO and IBM that are very big players
in there.
But you get in the gaps, you can sort of build a very exciting company that's sort of in
the wedge.
So we're building sort of like a wedge company that's in between those markets, which is
saying, hey, we're building a published, scribe, event-based technology that's using machine
learning, and a lot of the capabilities around there.
And what we're doing there is to, we're leveraging what we call, like a high-performance caching
engine, where companies can open data sockets from any database or technology system to
our product, and we're basically have both an in-memory cache as well as a cache that
could sit in storage basically that based companies would be able to access data in real
time, and we'd be able to show it in the feed.
So it's not a major integration process, and basically, it's just opening up a data
socket to our caching engine, and then we would pull that, and a lot of that, a lot
of this between this feature recognition NLP and the cache are all running in memory.
So on this back inside of things, what are some of the data sources that you envision
your early customers connecting to?
Is there like, you know, a top three lists of, you know, their email servers or something
else?
Or do you expect it to be more specialized, maybe proprietary systems or databases?
Yeah, great question.
I mean, what we've done for building the initial prototypes, we did, you know, the Dropbox,
Google Drive, Gmail, Yahoo Finance, to be able to show that.
So now, right now, you can have a conversation on our product, and you know, you ask for
the price of Amazon, it shows up.
You ask for an email, and on X, it shows up.
We're also working on exchange integration, and what we're doing is there's certain integrations
where they're more broad across many enterprises, and doing a very tight, custom integration
where we can find information very fast.
So exchange integration gives us both enterprise emails as well as a calendar information.
I mean, that's awesome.
So it's just, hey, when I gave you example, when you available, boom, who was on that
call?
You know, who's going to be on that call?
You can bring in all that information and show that person.
You don't have to sit there and fidget your email, your calendars, whatever.
So a lot of enterprises now are using things like box and Dropbox, so that would be a more
general type of integration that we would offer.
Where we see, for example, for financial services, we talk, you know, a conversation would
be with routers and market data, my Bloomberg sources there, same thing with legal, like,
routers, Alexa Snack is our major players, Salesforce to be an integration on the sales side.
So, you know, MLS would be an integration to real estate.
That's sort of the, the integrations are really not, the focus really, it gets to being,
make sure to get the language, the purpose for each one, so you're not, you're getting
good matches, that's the idea.
And how have you tackled that challenge?
So initially, we're focused on initially a couple of articles.
We're not, we're not going to target the entire marketplace and we have customer interest
in both side.
We're actually, in the very close to releasing a enterprise version of the product here
within a couple of weeks, so excited about that.
And then we have, you know, these are Fortune 50 type companies that are, that are going
to be demoing the product too and go from there effectively.
And then in terms of, you know, the goal is to expand the capabilities to a little, you
know, a little more of the cognitive side of things where we can, you know, this is
sort of why we joined the, and why we use the program.
And so we can work closely with some of the professors and doing some of the things
around building out our APIs so we can do a little more, and not have to be so limited
to building out each vertical, but you can do probably to more of a broader solution
for multiple type of verticals.
Okay.
Cool.
So what's next for the company?
So, you know, as I said, you know, we're excited about releasing this product here and
getting these customers teed up.
It's one of these things where it's fun to show the product because it's, it's sort
of magical.
It's also, you want to move faster, but you have to obviously go sort of a certain course
to sort of, you know, achieve those results.
And basically, we're just, it's first completing this next, what we call the beta of the enterprise
version, get these customers using the product and using it, and then scale from it.
Great.
I really appreciate you spending some time with me.
It sounds like you guys are doing really interesting things, and I'm looking forward to
keeping up with you as the company evolves.
Thank you.
I appreciate the time as well.
Great.
Thank you.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on cool, second mind, or any of the topics covered in this episode,
head on over to twimlai.com slash talk slash 65.
To follow along with the NYU Future Labs AI Summit series, which will be piping to
your favorite pod catcher all week, visit twimlai.com slash AI Nexus Lab 2.
Of course, you can send along your feedback or questions via Twitter to at twimlai or
at Sam Charrington or leave a comment or write on the show notes page.
Thanks again to NYU Future Lab for their sponsorship of the show.
For more information on the AI Nexus Lab program, visit futurelabs.nyc.
And of course, thanks again for listening and catch you next time.
