1
00:00:00,000 --> 00:00:16,720
All right, everyone. I am on the line with Errol Coolmaster. Errol is head of AI foundation

2
00:00:16,720 --> 00:00:21,520
at H&M Group. Errol, welcome to the Twomel AI podcast.

3
00:00:21,520 --> 00:00:23,840
Thank you so much. It's a pleasure to be here.

4
00:00:23,840 --> 00:00:29,320
Hey, it's great to have you on the show and I'm looking forward to digging into our conversation.

5
00:00:29,320 --> 00:00:33,080
I'd love to have you start us off by sharing a bit about your journey.

6
00:00:33,080 --> 00:00:35,480
How did you come to work in AI?

7
00:00:35,480 --> 00:00:40,360
Yeah, thank you so much. My journey really started about 15 years ago.

8
00:00:40,360 --> 00:00:43,480
I was still in school studying finance.

9
00:00:43,480 --> 00:00:49,160
I got the opportunity to join one of the largest bank here in the Nordic region of Europe.

10
00:00:49,160 --> 00:00:53,000
So I joined Nordia Bank, basically as a fraud analyst in the beginning.

11
00:00:53,000 --> 00:00:55,400
Keep in mind, this was 15 years ago.

12
00:00:55,400 --> 00:01:01,720
We weren't into deep tech back then. Basically, I was set at the position where they said,

13
00:01:01,720 --> 00:01:07,640
can you help us fix the fraud? So immediately, I was exposed to real-time problems,

14
00:01:08,520 --> 00:01:14,120
trying to in real-time work with the credit card companies to stop it.

15
00:01:14,840 --> 00:01:19,000
In the past, it was just rule-based, given that I had a finance background studying math

16
00:01:19,000 --> 00:01:25,000
at the time. I started doing statistical analysis, improving the results significantly,

17
00:01:25,000 --> 00:01:29,800
back then, my love for data, machine learning, and algorithms were born.

18
00:01:30,680 --> 00:01:36,520
So basically, I just took it from there. I then, of course, graduated.

19
00:01:36,520 --> 00:01:39,720
Went on to be a part of building up the fraud unit in the bank.

20
00:01:40,440 --> 00:01:45,480
Did that for a few years when I went on to actually anti-monolondering?

21
00:01:45,480 --> 00:01:49,640
So I was a part of building up the first detection systems in the bank as well,

22
00:01:50,520 --> 00:01:53,480
which was really interesting. I did that for about three years.

23
00:01:53,480 --> 00:01:57,480
And algorithms, screening, I couldn't get enough of it.

24
00:01:58,360 --> 00:02:01,880
I had my first child, and my second child during that period,

25
00:02:02,600 --> 00:02:06,360
when I realized I need some more exposure to data problems.

26
00:02:06,840 --> 00:02:12,600
I felt like this field is moving so fast. So I got to opportunity within the bank to have

27
00:02:13,320 --> 00:02:18,200
open roles, so to say. So I moved around a little bit in the bank every day.

28
00:02:18,200 --> 00:02:23,400
Was it the children that gave you the insight that you needed more exposure to data?

29
00:02:23,400 --> 00:02:24,600
The data problems?

30
00:02:24,600 --> 00:02:29,160
Definitely. Here is Sweden. We are very, we are very luxurious.

31
00:02:29,160 --> 00:02:33,080
We get a lot of free time. We get children. We get about a year off.

32
00:02:34,280 --> 00:02:38,360
I didn't take all the time, but it makes you think a little bit about what should I focus on.

33
00:02:38,840 --> 00:02:39,640
Okay.

34
00:02:39,640 --> 00:02:43,160
I realized I want to go broader, for all of these folks in and of me.

35
00:02:44,680 --> 00:02:49,160
So at that time, I had the opportunity to start playing around with Hadoop installations, Vanilla.

36
00:02:49,160 --> 00:02:55,800
Of course, this was our early days still. I had the opportunity to integrate the SaaS platform

37
00:02:55,800 --> 00:03:01,160
with Hadoop, distributed computing, early days of Spark, and I just was hooked.

38
00:03:02,040 --> 00:03:05,560
So I deep-dived into the technical aspect of data science.

39
00:03:06,920 --> 00:03:11,960
And I just couldn't get enough. I really love this field, and I have a strong passion for it.

40
00:03:12,840 --> 00:03:16,760
After a year or so, in that relatively free role, I got a phone call.

41
00:03:16,760 --> 00:03:23,800
And it was from a headhunter working at London, and they were recruiting for lead data scientists

42
00:03:23,800 --> 00:03:30,120
role for Vodafone Group. So basically, the second largest cell phone provider in the world.

43
00:03:31,560 --> 00:03:38,280
And I said, this is a great opportunity. So I decided to take my kids and wife and move to the UK,

44
00:03:39,240 --> 00:03:44,680
where I worked out to their global office, setting up their AI department as one of their

45
00:03:44,680 --> 00:03:50,040
lead data scientists. So, still, early days for me and early days for Vodafone. So I had the

46
00:03:50,040 --> 00:03:56,040
opportunity to work with some very talented individuals, and being a part of building out that team

47
00:03:56,040 --> 00:04:05,400
that I think is now over 500 people. We still, I would say, had set up infrastructure that I had

48
00:04:05,400 --> 00:04:11,400
some problems with today's perspective on the premium, large clusters, distributed computing,

49
00:04:11,400 --> 00:04:16,600
and very long lead times to actually rolling out models. So I got a little bit of frustrated.

50
00:04:17,160 --> 00:04:21,880
I like living in the fast lane, especially when it comes to extracting value.

51
00:04:22,840 --> 00:04:27,800
So I spent about a year and a half in London when I had the opportunity to actually start

52
00:04:27,800 --> 00:04:35,080
my own business. So I spent a bit of a lot of time in airports moving back and forth between

53
00:04:35,080 --> 00:04:42,280
Sweden and London, servicing clients, primarily banks and startups, with AI implementations.

54
00:04:43,400 --> 00:04:49,560
But after less than a year, my partner decided to pull out, and my good friends that think

55
00:04:49,560 --> 00:04:54,600
big analytics that was purchased by Territate, gave me a call and asked why not to join us,

56
00:04:54,600 --> 00:05:00,680
not to partners pull them. So I decided to do so. I came in as a director of data science for

57
00:05:00,680 --> 00:05:06,280
the Nordics Eastern Europe, Russia, managing a team of 25 people approximately.

58
00:05:07,080 --> 00:05:13,800
And what I did was traveling around in this region, advising the large corporations on how to

59
00:05:13,800 --> 00:05:22,840
implement AI and data. I did so for about a year. Then I get a really interesting phone call

60
00:05:23,560 --> 00:05:28,040
from one of the largest companies, not just here in the Nordics, but also in the world.

61
00:05:28,040 --> 00:05:36,600
H&M Group said, Hey, we are investing heavily in AI. We want to build out this internal capability.

62
00:05:36,600 --> 00:05:45,640
Can you help us do it? I at that point I said, okay, so do you have buying from senior stakeholders?

63
00:05:45,640 --> 00:05:51,080
Do you have budget? And do you have data? And I got a positive answer to all three of those.

64
00:05:51,800 --> 00:05:57,080
And I said, okay, let's do it. And that was two and a half years ago. And I haven't stopped

65
00:05:57,080 --> 00:06:03,720
running since. That's fantastic. So for those who may not be familiar with H&M Group,

66
00:06:04,680 --> 00:06:09,160
tell us a little bit about the company. So the H&M Group, of course,

67
00:06:09,160 --> 00:06:16,200
come from the original H&M brand that would started in 1947 here in Sweden in a small town called

68
00:06:16,200 --> 00:06:22,760
Vesteros. It was a very successful business model in the beginning. The founder,

69
00:06:22,760 --> 00:06:29,080
Aling Passion said he wanted to democratize fashion. Fashion in the pre-war era was relatively

70
00:06:29,080 --> 00:06:36,680
expensive. So he basically cut prices and made it available for everyone. In the off-the-war

71
00:06:36,680 --> 00:06:41,800
Europe, everybody wanted to kind of get the economy started. So this was a spot on.

72
00:06:42,440 --> 00:06:49,000
The model was so successful. So in around year 2000, they had 2000 stores when we opened in

73
00:06:49,000 --> 00:06:57,800
Manhattan. And now today, 20 years later, we have 5,000 stores globally. H&M, of course,

74
00:06:58,840 --> 00:07:03,080
the group consists of more than the H&M brand, which is very well known for most people.

75
00:07:03,080 --> 00:07:11,240
We have other brands like weekday, monkey, another story, H&M home, etc. So we as a group,

76
00:07:11,240 --> 00:07:17,560
we serve all of them. And most additionally, these initiatives are still being centrally managed

77
00:07:17,560 --> 00:07:25,000
out of Sweden. Okay, nice. And what you mentioned, you got that call that said that they wanted to

78
00:07:26,040 --> 00:07:33,800
invest heavily in AI. What stage was H&M at when they gave that call?

79
00:07:34,680 --> 00:07:41,560
So I mean, so the listeners listening to this, you probably met the CEOs or the senior managers

80
00:07:41,560 --> 00:07:48,120
who says, hey, AI, I've heard about it at a conference somewhere. What is it? Shouldn't we be

81
00:07:48,120 --> 00:07:56,280
investing in it? H&M came to that conclusion in around 2016. That meant that the CEO at the

82
00:07:56,280 --> 00:08:02,760
time called you on passion, the grandson of the founder, bent to business development, and he gave

83
00:08:02,760 --> 00:08:07,160
that line pretty much. What are we doing within AI? So business development went out into the

84
00:08:07,160 --> 00:08:13,640
business and said, we got some bad news. We're not doing much. We do have a CRM department.

85
00:08:13,640 --> 00:08:18,200
We do have some central initiatives here and there, but hey, we're H&M. We're not doing it a

86
00:08:18,200 --> 00:08:23,640
scale. We're not taking advantage of our economy of scale economy we could be doing here.

87
00:08:24,440 --> 00:08:31,160
So business development, they engage in external consultancy firm for the first proof of

88
00:08:31,160 --> 00:08:37,720
concepts in around 2017, with the aim to go to production if they were good. They were good,

89
00:08:37,720 --> 00:08:43,000
their first indicative results. So they went to production. They started to see payback

90
00:08:43,000 --> 00:08:48,360
immediately. The return on investment was less than a year in most of the business cases they

91
00:08:48,360 --> 00:08:55,720
started. It was so successful. So in 2018, when I got that call, they decided to establish AI

92
00:08:55,720 --> 00:09:02,920
as its own function. And a function in H&M is kind of a big deal. One private act is sustainability

93
00:09:02,920 --> 00:09:07,640
department. If you know H&M a little bit sustainability is something very important for us,

94
00:09:07,640 --> 00:09:16,200
then you can imagine how important AI is now. So maybe tell us a little bit about various

95
00:09:16,200 --> 00:09:26,040
use cases for AI at H&M. How do you organize them? Do you think about them by business unit or

96
00:09:26,040 --> 00:09:31,320
by functional area? I'm imagining that or based on our previous conversation that there's

97
00:09:31,320 --> 00:09:39,720
quite broad use at this point. Yeah. So H&M covers the entire value chain of a retail company,

98
00:09:39,720 --> 00:09:47,000
except actually manufacturing the clothes. So for us, AI needs to cover the entire value chain.

99
00:09:47,000 --> 00:09:52,760
We have an end-to-end responsibility here. So when we started assessment of what use cases to

100
00:09:52,760 --> 00:10:00,680
actually do, we looked on value and feasibility. So we wanted to pick the low hanging fruits first,

101
00:10:00,680 --> 00:10:06,840
which was a very good strategy. So we looked on, okay, in the beginning of the cycle,

102
00:10:06,840 --> 00:10:11,960
when we're starting to, before we start manufacturing the clothes, when we actually do the design,

103
00:10:12,760 --> 00:10:19,640
can we detect the trends? They're going to be interesting relevant in eight months when

104
00:10:19,640 --> 00:10:24,680
we're going to get the clothes back. So we started one of the use cases was fashion forecasting,

105
00:10:25,400 --> 00:10:29,720
very interesting looking on social media, whether the trends. So our designers could go in

106
00:10:30,280 --> 00:10:35,960
with their hypotheses and actually validate them in this tool. How the parts of this was,

107
00:10:35,960 --> 00:10:42,200
of course, demand forecasting. So when we do have some of these things, how much should we buy?

108
00:10:42,920 --> 00:10:48,040
That's also very important for us. In the past, when we had hyper growth in the physical retail,

109
00:10:48,040 --> 00:10:51,800
we could just say we're going to grow 10% next year. If we didn't reach that target,

110
00:10:51,800 --> 00:10:57,240
we could open a new store and put all the garments there and just sell them out anyway.

111
00:10:57,240 --> 00:11:01,800
Now we needed to be more granular in our predictions. So demand forecasting for the company was,

112
00:11:01,800 --> 00:11:08,040
of course, something we focused quite a lot on. Then on the other side, we needed to a way to

113
00:11:08,040 --> 00:11:15,640
negotiate with different production plants. So we applied AI into different pricing algorithms,

114
00:11:15,640 --> 00:11:22,920
in negotiations, and also vendor selection algorithms for the production part. On top of that,

115
00:11:22,920 --> 00:11:28,920
when everything is produced, we need to allocate them to warehouses. So allocation became a

116
00:11:28,920 --> 00:11:33,880
relatively good use case for us as well. What garments should go ware and to what stores should

117
00:11:33,880 --> 00:11:39,400
again. It's not a one-size-fits-all. It needs to be individual depending on the market as well.

118
00:11:40,200 --> 00:11:46,120
And then, of course, in the end of the sales cycle, we need to handle pricing. So markdowns for

119
00:11:46,120 --> 00:11:51,720
the online store was one of our more successful use cases. And then, of course, covering everything

120
00:11:51,720 --> 00:11:56,760
is the personalization, recommendation engines, and tailored offering for our customers.

121
00:11:56,760 --> 00:12:02,360
So those were our main relatively high-level use cases that we focused on most recently.

122
00:12:03,880 --> 00:12:09,720
So you joined H&M at a time where the company had completed several

123
00:12:11,000 --> 00:12:19,960
proof-of-concept efforts, had demonstrated some value. I believe didn't have much of a team

124
00:12:19,960 --> 00:12:26,280
relied primarily on external resources at this point. What were your first steps in

125
00:12:28,440 --> 00:12:33,400
taking on the challenge to scale this more broadly within the organization?

126
00:12:35,480 --> 00:12:40,520
It's a very good question. And I think my first reaction when joining H&M is what have I given

127
00:12:40,520 --> 00:12:48,760
myself into? It was relatively what it's nested with some really talented consultants,

128
00:12:48,760 --> 00:12:53,080
but they were focused so much on value. And I came in with the perspective of course with value,

129
00:12:53,080 --> 00:12:59,960
but how do I scale these type of efforts? I started thinking how do we actually recruit the

130
00:12:59,960 --> 00:13:04,840
amount of people that we need to recruit to replace these consultants? How do we streamline the

131
00:13:04,840 --> 00:13:11,240
technology and architecture? So one of the first things we started doing was communicating a lot

132
00:13:11,240 --> 00:13:16,840
more around the topics we were working on to make the market aware of that we were doing this

133
00:13:16,840 --> 00:13:21,320
type of investments to be able to attract talent into some of the very challenging

134
00:13:21,320 --> 00:13:27,720
problems we were working on. So we started crafting a story line that we wanted to introduce

135
00:13:28,360 --> 00:13:34,120
to the general market as well. You should never underestimate communication to create a bit of bus

136
00:13:34,120 --> 00:13:39,400
in the Nordics because that's primarily where we started out, but also then more globally as well.

137
00:13:39,400 --> 00:13:47,400
Then of course popping the hood of all the use cases, looking at the technical architecture,

138
00:13:48,280 --> 00:13:53,640
trying to understand what the external consultant's firm had actually built. So some of the first

139
00:13:53,640 --> 00:14:00,840
thing I did was going out and hiring more technical people that could understand the engineering aspect

140
00:14:01,640 --> 00:14:06,120
is one thing to build a model, but it's another thing to push it to production and keep it there,

141
00:14:06,120 --> 00:14:12,040
which requires a lot of engineering effort. So my first hire was some really senior machine learning

142
00:14:12,040 --> 00:14:19,240
engineers and we created something we call a reference architecture because we wanted to maintain

143
00:14:19,240 --> 00:14:24,600
autonomy of the actual data science teams that were building the algorithms and the products

144
00:14:25,240 --> 00:14:29,720
to be able to run as fast as possible but to be able to get as much central support as possible.

145
00:14:30,520 --> 00:14:35,240
It didn't make sense for us to make 10 different technical choices when they were trying to solve

146
00:14:35,240 --> 00:14:40,840
the same problem. So we started streamlining some of these choices to be able to speed up deliveries,

147
00:14:41,640 --> 00:14:48,680
started to harmonize the technical choices, and then of course started taking the use case over

148
00:14:48,680 --> 00:14:58,920
and rolling them out at a faster pace. At that point we're there existing data scientists within

149
00:14:58,920 --> 00:15:05,000
different business units and you were supporting them centrally or were you also staffing up

150
00:15:05,000 --> 00:15:10,120
across the business in the various business units or departments?

151
00:15:11,640 --> 00:15:15,240
It's a very relevant question. I've seen different companies do it differently. The

152
00:15:15,240 --> 00:15:22,920
DGNM approach was to do it centrally from the start, basically to incubate the capability

153
00:15:22,920 --> 00:15:28,440
rather to spread it out, what we realized and what I realized from my experience as well is

154
00:15:28,440 --> 00:15:32,680
if you put a data science in a team as the only technical and algorithmic person,

155
00:15:32,680 --> 00:15:36,840
they're going to have a problem delivering not because they are a pad at delivering

156
00:15:37,560 --> 00:15:43,160
but they're not in the right context. So you tend to see a relatively high turnover of data

157
00:15:43,160 --> 00:15:51,240
scientists being set as an isolated island in a business unit. So we decided to initially

158
00:15:51,240 --> 00:16:02,040
start with a central incubation. When you kind of pull back the covers on these proof of

159
00:16:02,040 --> 00:16:09,720
concepts, you were looking for the, you're trying to understand them technically,

160
00:16:10,680 --> 00:16:17,960
but what did you also learn from a value perspective? Like often there's misalignment between

161
00:16:18,760 --> 00:16:24,840
the needs of a proof of concept meaning to show something flashy or fancy and the ultimate

162
00:16:24,840 --> 00:16:33,080
needs of the business and how you align in a sustainable long-term way. Did that kind of issue

163
00:16:33,080 --> 00:16:39,240
come up for you? I think that some of the really good things with the external firm that we

164
00:16:39,240 --> 00:16:44,840
worked, but they were really good at understanding the business requirements. They worked very closely

165
00:16:44,840 --> 00:16:49,480
and I think a strategy from H&M side given that they have had over the technical development

166
00:16:49,480 --> 00:16:54,520
to an external firm was to actually have an integrated person from the business in each use case.

167
00:16:55,080 --> 00:17:02,680
So the alignment around the value and the delivery of these things, I would say we're very well

168
00:17:02,680 --> 00:17:07,720
aligned. The problems were more on the technical side because showing something shiny

169
00:17:08,840 --> 00:17:16,120
doesn't always scale. My learnings and experiences start with the simplest thing that's better than

170
00:17:16,120 --> 00:17:21,800
random that gives some sort of initial uplift, but you can scale it better. You don't go with the

171
00:17:21,800 --> 00:17:27,640
most complex technology or algorithm from start because you don't know how that will scale.

172
00:17:28,520 --> 00:17:33,800
So it's better to create a stable and scalable infrastructure rather than focusing on the modeling

173
00:17:34,440 --> 00:17:40,680
from day one if you're serious about these things. And so what were some of the things that you

174
00:17:40,680 --> 00:17:46,440
built into that infrastructure as you started to pull these projects together? So some of the things

175
00:17:46,440 --> 00:17:51,640
that we did is we looked at the process and what I realized is if you understand what you're building,

176
00:17:52,440 --> 00:17:56,520
then you can map the processes and then you can actually automate most of the things.

177
00:17:57,720 --> 00:18:04,760
vis-à-vis the MLOPS type of setup. So what we did was basically to map out the development

178
00:18:04,760 --> 00:18:11,960
process and look on where are some of the major pain points that we have today. What I started

179
00:18:11,960 --> 00:18:19,560
realizing is that we had a lot of stale clusters on the cloud that weren't used 70% of the time.

180
00:18:19,560 --> 00:18:26,520
So we started moving over to a more scalable infrastructure. So we moved most of our development

181
00:18:26,520 --> 00:18:32,680
away from Jupyter notebooks into Databricks, given that then we could have on-demand compute,

182
00:18:32,680 --> 00:18:38,840
and we could have more virtual control over the infrastructure. So we standardized on how we

183
00:18:38,840 --> 00:18:45,400
actually did model development. With those things we also started looking on the the orchestration.

184
00:18:45,400 --> 00:18:51,000
So we standardized it a lot around the work for orchestration, went to going more into to airflow,

185
00:18:51,880 --> 00:18:55,720
building new pipelines into that things, and also looking on how do we production lies and how do

186
00:18:55,720 --> 00:19:02,440
we follow up on these things. So for each of these process step we took a few design decisions

187
00:19:03,000 --> 00:19:09,560
and created supporting infrastructure for that. And then did you go to the actual

188
00:19:12,520 --> 00:19:19,480
maybe taking a step back, were you building infrastructure and kind of promoting these proof of

189
00:19:19,480 --> 00:19:26,600
concepts into a more production capability at the same time, or did one come before the other?

190
00:19:26,600 --> 00:19:32,840
Did you pause on the PSE, build infrastructure, and then return to the PSEs to make them scale? How did

191
00:19:32,840 --> 00:19:41,080
you approach all of that? Yeah, I mean my general comment is super hard. But it's only concept of

192
00:19:41,080 --> 00:19:45,880
technical depth. So these PSEs, some of them were in production already delivering value and

193
00:19:45,880 --> 00:19:50,280
if you are using production delivering value, it's hard to pull back because the business is

194
00:19:50,280 --> 00:20:00,200
going to go crazy. But what we started doing was we slowly worked with the different products,

195
00:20:00,200 --> 00:20:08,280
identifying their major pain points, and gave them boilerplate code to start migrating towards.

196
00:20:09,000 --> 00:20:13,080
So some of the engineers they went in, they did a small proof of concept for instance moving

197
00:20:13,080 --> 00:20:18,520
into air flowing Kubernetes in some of the use cases and showed on the benefits and then we had

198
00:20:18,520 --> 00:20:26,280
them paid down the technical depth. In our OCRs, I even formulated goals for the product area to

199
00:20:26,280 --> 00:20:33,400
actually have no technical depth in their use cases or product in this case. So we mandated them

200
00:20:33,400 --> 00:20:37,560
that not all your efforts would go into new feature development and you roll out to new markets,

201
00:20:37,560 --> 00:20:43,720
it should be paying down the depth that we still owe on the proof of concepts.

202
00:20:46,600 --> 00:20:56,840
Can you walk us through a particular use case kind of before you had this infrastructure and

203
00:20:56,840 --> 00:21:03,800
you know once you establish the infrastructure and how that use case evolved along with

204
00:21:03,800 --> 00:21:07,480
the platform technologies that you put in the place?

205
00:21:08,440 --> 00:21:15,560
Yeah. I can tell you about the first use case we started as the H&M team,

206
00:21:15,560 --> 00:21:19,080
solely based on the reference architecture and how we actually developed that.

207
00:21:19,960 --> 00:21:26,120
So we started a sort of quantification for online, which is demand for causing for the online

208
00:21:26,120 --> 00:21:32,360
store, from scratch. So we wanted a green field with all the experience of all the proof of

209
00:21:32,360 --> 00:21:37,960
concepts you can consider in mind based on the reference architecture. Basically what we started

210
00:21:37,960 --> 00:21:43,560
of course with was creating this blueprint, the reference architecture, and then started building

211
00:21:43,560 --> 00:21:49,720
the different building blocks out of that. So from scratch, we of course made a modeling on

212
00:21:49,720 --> 00:21:54,840
Databricks. We created the the Airflow orchestration, the integration, the scoring, the

213
00:21:54,840 --> 00:22:00,600
hyper parameter tuning, the interference part and all of that. It took M2M to build out this

214
00:22:00,600 --> 00:22:05,960
infrastructure approximately 12 months. And I'm not going to say this was super false but given

215
00:22:05,960 --> 00:22:11,960
the complexity and dependencies that we had and the team was relatively new to building this,

216
00:22:11,960 --> 00:22:17,320
I think it was relatively good. And were you building the infrastructure for this particular

217
00:22:17,960 --> 00:22:23,880
product or were you building the infrastructure with many products in mind and this was kind of

218
00:22:23,880 --> 00:22:30,360
a tracer bullet? That's exactly the question I wanted to get to because we had all the

219
00:22:30,360 --> 00:22:34,840
use cases in mind. So this time we didn't just build it for one, we built it for many.

220
00:22:36,120 --> 00:22:41,160
And the learning that we made from that is the second use case that we started which was

221
00:22:41,160 --> 00:22:47,320
balancing a warehouse that took six months. Basically the same prerequisites, of course,

222
00:22:47,320 --> 00:22:52,760
not the same data and the same business output but the process and everything associated with it

223
00:22:52,760 --> 00:22:57,880
is very similar. So we could reuse much of the infrastructure because we built it in such a way.

224
00:22:57,880 --> 00:23:02,760
So all of a sudden we had saved 50% of development time in the new use case.

225
00:23:03,800 --> 00:23:10,360
And our new strategic approach is to even reduce that further with 50% and hopefully getting

226
00:23:10,360 --> 00:23:16,120
it down to three months in the ones that we're starting right now. So we see enormous productivity

227
00:23:16,120 --> 00:23:21,720
gain with designing an infrastructure that's reusable, raw data and building independent use

228
00:23:21,720 --> 00:23:29,400
cases at this scale because it doesn't make sense. Yeah. You've talked about some of the trade-offs

229
00:23:29,400 --> 00:23:36,840
between simplicity and complexity and modeling. I'm wondering if you can talk a little bit about

230
00:23:36,840 --> 00:23:41,960
what the portfolio looks like from that perspective, what kinds of models are you using

231
00:23:44,200 --> 00:23:47,640
and the way you're approaching it today now that you've built some of the infrastructure?

232
00:23:47,640 --> 00:23:55,160
Yeah. So most of the things that we have taken to production the last year or so are all

233
00:23:55,160 --> 00:24:02,440
relatively simple. Most of our models are like GBM. Like GBM is sort of a house model these days.

234
00:24:02,440 --> 00:24:08,600
I mean it produces good results and its computational power isn't that expensive.

235
00:24:09,320 --> 00:24:15,240
So we get the results that we need and it's also relatively easy to integrate into the use

236
00:24:15,240 --> 00:24:20,760
cases and the infrastructure that we have. When we start it out and somebody consults the early

237
00:24:20,760 --> 00:24:26,360
use cases and some of our early use cases, many of the people in the teams throw themselves

238
00:24:26,360 --> 00:24:34,280
directly into the latest research, wanted to do neural networks, wanted to get just a CR.1 uplift.

239
00:24:35,080 --> 00:24:41,240
But what we realize as well is this isn't a cagle competition. It is not about optimizing

240
00:24:41,240 --> 00:24:45,880
that metrics and that then you're done. It's about carrying it over into production

241
00:24:45,880 --> 00:24:52,280
into infrastructure as well. So these days we tend to start with something relatively simple

242
00:24:52,280 --> 00:25:02,120
as a benchmark. It can be anything from from like GBM, XGBoost and some other regression models

243
00:25:02,120 --> 00:25:08,760
for instance. And now we're slowly moving towards more advanced tools. So looking at

244
00:25:08,760 --> 00:25:15,240
the graph neural networks, et cetera, in our more latest modeling because we then see an extra

245
00:25:15,240 --> 00:25:22,120
need for it. So it is a relatively wide range. We have experts in all of these areas,

246
00:25:22,120 --> 00:25:27,960
we have optimization experts as well in some of our use cases. So we try to keep it as simple as

247
00:25:27,960 --> 00:25:37,880
possible but we're not 100% there yet. You talked about one of your biggest challenges being

248
00:25:37,880 --> 00:25:47,400
scaling your organization from zero to many employees, data scientists. Do you run into the issue

249
00:25:47,400 --> 00:25:53,720
of the folks that you're trying to attract wanting to play with cool, fancy, shiny toys,

250
00:25:53,720 --> 00:25:59,960
and here you're working with XGBoost and light GBM. How do you address that?

251
00:25:59,960 --> 00:26:07,960
Yeah, it's a it's a super hard. I think it also comes with seniority and I think it also comes

252
00:26:07,960 --> 00:26:14,920
with establishing different types of teams. What we're saying is the most valuable thing for us

253
00:26:14,920 --> 00:26:20,200
right now when we look on scale are the simpler models that they produce a good value and the

254
00:26:20,200 --> 00:26:25,640
reason to maintain and and put into production. But we also have established research teams

255
00:26:25,640 --> 00:26:32,440
which have as a purpose to look on how can we prove what we have today. How can we introduce more

256
00:26:32,440 --> 00:26:38,760
more research into these product teams but they have another time perspective. So what we tend

257
00:26:38,760 --> 00:26:43,800
to talk to our data scientists and machine learning engineers is that you have to consider

258
00:26:43,800 --> 00:26:49,400
timing into this. We have to have a positive ROI in our investment today to actually get the

259
00:26:49,400 --> 00:26:54,920
funding to be able to scale this but at the same time we are investing in the research

260
00:26:54,920 --> 00:26:58,360
for the research needs to be heavily associated with what you're doing with as well.

261
00:26:59,480 --> 00:27:06,040
So we are trying to have a long and a short term perspective. Can we can we attract everyone

262
00:27:06,040 --> 00:27:11,240
that wants to work with the latest and the shiniest thing? No, of course not. Hopefully we can get

263
00:27:11,240 --> 00:27:17,160
some of them and that will be enough. And hopefully one day we are advanced enough so that most

264
00:27:17,160 --> 00:27:23,080
of the things we're working on are the latest. But we are a company with a lot of legacy and it's

265
00:27:23,080 --> 00:27:32,280
going to take us a bit of time before we are there. You just touched on a topic that I hear fairly

266
00:27:32,280 --> 00:27:38,120
frequently when talking to folks that are in enterprise leadership positions and that is the

267
00:27:38,840 --> 00:27:47,560
importance of managing the project portfolio. You know needing to have some kind of swing for

268
00:27:47,560 --> 00:27:57,640
the fences, big goals in mind to keep executives excited and keep the funding coming in but also

269
00:27:57,640 --> 00:28:05,160
needing to have small wins and pick off low hanging fruit, that kind of thing. I'm wondering if

270
00:28:05,160 --> 00:28:12,200
you see that as well and how you approach that portfolio management aspect of the role.

271
00:28:12,200 --> 00:28:19,960
Yeah, I would like to put it a little bit more context as well. So we had a major reorganization

272
00:28:19,960 --> 00:28:25,880
recently integration AI with IT department and business development into one large unit called

273
00:28:25,880 --> 00:28:31,720
business tech. So these days we are enabling domain as well which means we we serve all of the

274
00:28:31,720 --> 00:28:38,280
the business with enabling capabilities. So the majority of the use cases actually is prioritized

275
00:28:38,280 --> 00:28:44,200
by other units than ourself these days and we support them with building them out. This put us

276
00:28:44,200 --> 00:28:50,200
in a tricky situation trying to identify how should we accelerate AI now given that we are

277
00:28:50,200 --> 00:28:56,600
enabling domain. So we sat down and we formulated a strategy which we called the Fountainhead which

278
00:28:56,600 --> 00:29:03,080
is the encapsulation of all AI capabilities. So one of the cornerstones of that is going from vertical

279
00:29:03,080 --> 00:29:08,440
capabilities which is basically use case by use case because we see that's not a scalable business

280
00:29:08,440 --> 00:29:15,880
wallet into a horizontal scaling. Basically what that does it gives us another factor of

281
00:29:15,880 --> 00:29:22,920
prioritizing. If we in the past prioritized our use cases in our portfolio by value and feasibility

282
00:29:22,920 --> 00:29:30,760
we have now added the notion of reusability. And the notion of reusability basically is because

283
00:29:30,760 --> 00:29:36,760
take the move box balancing of warehouses use case that we had that's in season demand for

284
00:29:36,760 --> 00:29:41,880
costing. If we build it just for move box this is not an infrastructure question but if we build

285
00:29:41,880 --> 00:29:47,320
that specifically demand for costing just for move box that means that we created once and we use

286
00:29:47,320 --> 00:29:54,040
it once. But if we prioritize our use cases for all the use cases now that can do in season demand

287
00:29:54,040 --> 00:30:00,040
for costing because we just created that capability and we prioritize them that means we reach value

288
00:30:00,040 --> 00:30:06,600
much faster. So we scale smarter not harder by just doing more use cases. So we have flipped

289
00:30:06,600 --> 00:30:14,200
the roadmap over in our portfolio of AI use cases work together with the domain and created that

290
00:30:14,200 --> 00:30:22,200
type of roadmap to prioritize times the value which gives us a relatively good ROI quickly

291
00:30:22,200 --> 00:30:28,040
within the first year is our estimate rather than a sequence with small value lumps along the

292
00:30:28,040 --> 00:30:33,320
way that the next few years. So we have we have spent quite a lot of time recently

293
00:30:34,440 --> 00:30:37,480
and are seeing that the first results already know.

294
00:30:40,440 --> 00:30:45,480
Maybe to replay that a little bit to make sure that I'm catching what you're saying.

295
00:30:46,440 --> 00:30:52,600
It sounds like you started out you had a bunch of proof of concepts and you built some

296
00:30:52,600 --> 00:31:00,760
fairly low level infrastructure things like Databricks and Airflow and other things to enable you

297
00:31:00,760 --> 00:31:09,080
to scale those proof of concepts and more recently what you're seeing is you're kind of pushing up

298
00:31:09,080 --> 00:31:14,280
the level of abstraction and so you're saying okay instead of you know building out this

299
00:31:15,240 --> 00:31:21,880
vertically integrated demand forecasting app for a particular use case. Hey well we need to do

300
00:31:21,880 --> 00:31:28,360
forecasting across lots of different use cases potentially. Let's build another level of platform

301
00:31:28,360 --> 00:31:34,600
kind of at the forecasting level or the application level. You know at the risk of throwing around

302
00:31:34,600 --> 00:31:41,400
too many buzzwords here. You're spot on and that's exactly what we're doing. We increased the

303
00:31:41,400 --> 00:31:45,000
abstraction level so I think there's a very good summary of what we're actually doing.

304
00:31:45,000 --> 00:31:51,400
Focusing more on the reusability effect notches on technology but on the output as well.

305
00:31:52,600 --> 00:31:59,240
Because we are right now obsessed with scaling the value aspect. I did the math that is not

306
00:31:59,240 --> 00:32:06,120
complicated. If we are 100 people today or 120 issue working on the AI use cases we have around

307
00:32:06,840 --> 00:32:13,560
10 use cases in production right now. If we're going to have all our core operational decisions

308
00:32:13,560 --> 00:32:18,680
amplified by AI by 2025 which is our tech clip that we're aiming towards. We're going to need

309
00:32:18,680 --> 00:32:24,280
thousands of people if we're scaling it vertically. So we need not the way of assessing the talent

310
00:32:24,280 --> 00:32:35,080
gap and the time to value assessment as well. You mentioned that part of this recent reorg

311
00:32:35,080 --> 00:32:45,240
put you in the same organization as IT. I'm curious about the implications of that or the impacts

312
00:32:45,240 --> 00:32:57,400
of that and how you work together with IT. What ways has it impacted your workflow in the way you

313
00:32:57,400 --> 00:33:05,960
approach your test? I think one of the good decisions that was taken a few years back was that we

314
00:33:05,960 --> 00:33:14,280
were going to start the AI efforts all on cloud with basically no limitations around the technology.

315
00:33:14,280 --> 00:33:20,600
We could pick pretty much whatever we wanted. It was a curse and a blessing. But if you look

316
00:33:20,600 --> 00:33:26,280
on a traditional IT on the prem traditional waterfall approaches relatively long cycles,

317
00:33:26,280 --> 00:33:36,520
deployment, secretations, not so agile. Going into the new business tech organization where we

318
00:33:36,520 --> 00:33:43,480
established around 250 product teams also supposed to work agile. The IT department coming down from

319
00:33:43,480 --> 00:33:49,080
a monolithic type of situation trying to break that down into product teams relatively small.

320
00:33:49,800 --> 00:33:54,920
Of course they have a lot of legacy. So we are running really fast. We are able to ship

321
00:33:54,920 --> 00:34:01,320
with every delivery. The integration with IT is still relatively few. But what we are

322
00:34:01,320 --> 00:34:06,920
experienced now is that we are encountering some of the traditional processes from that type of

323
00:34:06,920 --> 00:34:13,240
organization. So I would say it's good for the company because we are getting everybody up to

324
00:34:13,240 --> 00:34:18,360
the same stand that on how you deliver in a modern software organization or more than tech

325
00:34:18,360 --> 00:34:24,040
organization. But on our side of course the lead times become a little bit longer when we're

326
00:34:24,040 --> 00:34:43,720
trying to integrate. But it sounds like you are still operating. Well is AI operating primarily

327
00:34:43,720 --> 00:34:48,120
cloud first but the rest of IT is not primarily cloud first. Is that part of what I am hearing?

328
00:34:48,120 --> 00:34:55,160
The strategy is cloud first for the company. Then of course the migration takes a bit of time

329
00:34:55,160 --> 00:35:02,680
for the rest of the organization. So we are 100% cloud first which makes life a lot of easier.

330
00:35:03,960 --> 00:35:16,200
And so when you get a proof of concept or a project to production does that stay within your org

331
00:35:16,200 --> 00:35:23,400
for operational responsibility or is there a handoff to IT? How have you structured around

332
00:35:24,760 --> 00:35:31,080
ops long time? So IT doesn't exist in a longer as a notion. It's just business techno.

333
00:35:33,000 --> 00:35:37,320
But what it is on the upside is that we have the notion of you run it, you build it.

334
00:35:38,760 --> 00:35:43,880
What we said try to do is work together with the domains. So our domains like sourcing and

335
00:35:43,880 --> 00:35:48,920
production, customer fulfillment, etc. They are responsible for the actual juice case. We are

336
00:35:48,920 --> 00:35:56,200
responsible for the capability, the best practice, and the algorithms. So what we do is we work

337
00:35:56,200 --> 00:36:01,320
together with the domain. They are going to run the product long term. But we are going to make

338
00:36:01,320 --> 00:36:07,000
sure that they have the specialist to solve the problem. So take in season demand for a casting

339
00:36:07,000 --> 00:36:12,600
for instance. We are running the algorithmic, the part that actually produces. They will handle

340
00:36:12,600 --> 00:36:18,360
the integration and the actual usage of the product. So we can specialize in being specialist

341
00:36:18,360 --> 00:36:22,520
in demand for casting. And they can integrate with the business and the change management

342
00:36:22,520 --> 00:36:25,960
and run it. So it's going to be two product teams working together.

343
00:36:27,000 --> 00:36:36,200
Okay. And so does the business units that sounds like have their own technical capability

344
00:36:36,200 --> 00:36:45,000
to operate these applications long term. But they rely on business tech as and enable it to help

345
00:36:45,000 --> 00:36:53,000
them build in emerging technologies. Is that the idea? Yeah. So the business units, they are

346
00:36:53,000 --> 00:36:58,280
actually once running the business. And then business tech is a traditional old IT organization,

347
00:36:58,280 --> 00:37:04,280
but being a more agile organization these days. So we serve the needs of the business units

348
00:37:04,280 --> 00:37:10,440
in the group. They don't run, but they use the products that we develop. So for instance,

349
00:37:10,440 --> 00:37:15,880
so for instance on the customer side, what we're building now is a platform to integrate with

350
00:37:15,880 --> 00:37:21,160
hnm.com to make it fully personalized. We can deploy microservices to get interact and

351
00:37:21,160 --> 00:37:26,840
create the personalized experience. Of course, those that actually are working on the use cases,

352
00:37:26,840 --> 00:37:31,720
working with the sales and everything that's our online sales unit. And that sits in the business

353
00:37:31,720 --> 00:37:37,400
units of the hnm brand and business tech is creating the technology to enable them to do their job

354
00:37:37,400 --> 00:37:47,080
the best possible way. Got it. So you know, in all of this in your journey there at hnm group

355
00:37:47,080 --> 00:37:54,040
growing the team from zero to I think you said 120 employees you've gone through, you know,

356
00:37:54,040 --> 00:38:01,240
reorgs and transformations like what what are some of your your biggest learnings and all that

357
00:38:01,240 --> 00:38:13,800
around how to scale NAI team from, you know, from the start. I think one of my biggest learnings

358
00:38:13,800 --> 00:38:21,880
is perfection is the enemy of done. I think if you wait around and want to create the best product

359
00:38:21,880 --> 00:38:27,080
or want to create the best technology, you're never going to to reach that end state. And if

360
00:38:27,080 --> 00:38:32,840
you're serious about running fast and scaling, then you have to start now. It takes time. Even

361
00:38:32,840 --> 00:38:38,360
though we have run extremely fast and lost 12 half years, we have just started. We are extracting

362
00:38:38,360 --> 00:38:43,800
value. Many of the companies I worked with and seen doing these things, they wait too long. They

363
00:38:43,800 --> 00:38:48,040
want to build the perfect infrastructure, they want to build the perfect model, they want to evaluate,

364
00:38:48,040 --> 00:38:53,320
they want to get everybody on board. Of course, they call the management, management is extremely

365
00:38:53,320 --> 00:38:58,280
important. Communication is extremely important, but at some point you just have to say, let's just

366
00:38:58,280 --> 00:39:05,640
do it. The start up mentality, if there's value on the table, let's go out and get it. Scaling is

367
00:39:06,680 --> 00:39:13,880
being an entrepreneur, where you start something from from zero, you have to have a bit of guts as

368
00:39:13,880 --> 00:39:20,280
well and just go with the notion. And just to get tactical on that particular point,

369
00:39:20,280 --> 00:39:31,000
often with these machine learning projects in particular, the looming perfection or the

370
00:39:32,040 --> 00:39:38,440
small percent of increase in whatever your target metric is, always an temptation out there.

371
00:39:38,440 --> 00:39:46,680
How do you and your teams stay focused and know when done is, where in the process do you know

372
00:39:46,680 --> 00:39:54,840
that and how do you define it? It's super hard in case by case, but at some point you have to have

373
00:39:54,840 --> 00:40:00,440
a baseline, you have to have a target, but it's like you say in software, they shift with every

374
00:40:00,440 --> 00:40:06,840
delivery. This is not the research lab. We do have one of those as well, and they have different

375
00:40:07,720 --> 00:40:12,520
prerequisites and different targets. What we want to get to here is we want to be able to

376
00:40:12,520 --> 00:40:18,360
ship and show something within every sprint. Every PEI needs to be able to ship something,

377
00:40:19,400 --> 00:40:24,760
so don't expect to be able to sit and fiddle around and try to optimize for too long.

378
00:40:25,880 --> 00:40:29,880
And you have to work with your stakeholders as well and agree on a level and have an open

379
00:40:29,880 --> 00:40:35,400
dialogue. What we don't want are unicorn data scientists that solves everything themselves.

380
00:40:36,280 --> 00:40:40,920
What we want is talented individuals to work together with the team and have a transparent

381
00:40:40,920 --> 00:40:45,720
dialogue around their process and what's hard, what's not working, so that everybody can take

382
00:40:45,720 --> 00:40:49,960
an informed decision if we should go to production with what they have or if we should wait in other

383
00:40:49,960 --> 00:40:59,400
sprits. You've talked a bit about thinking like a startup entrepreneur. You've mentioned

384
00:41:00,680 --> 00:41:06,840
terms that we know from Silicon Valley companies, OKRs and technical debt and the like.

385
00:41:06,840 --> 00:41:14,840
You know, clearly you're trying to operate like a startup within H&M Group. Does that create

386
00:41:14,840 --> 00:41:21,320
friction for you in a large established enterprise and how have you addressed those kinds of issues

387
00:41:21,320 --> 00:41:30,520
if you've encountered them? Definitely. I've seen people looking at us and saying why can they

388
00:41:30,520 --> 00:41:36,360
do these type of things without actually looking at themselves and try it. I think what it comes

389
00:41:36,360 --> 00:41:45,480
to is having a good story and communication around these topics. What I try to do in my communication

390
00:41:45,480 --> 00:41:50,920
is that I'm doing this for the entire company. This is for the better good of this company. It's

391
00:41:50,920 --> 00:41:57,560
not for just the success of the AI department. So the story is really around how we as a company

392
00:41:57,560 --> 00:42:02,360
come around and it's not about threatening other people. It's about amplifying other people

393
00:42:02,360 --> 00:42:09,480
to make sure that the people in this organization get to stay away from the boring long tail type

394
00:42:09,480 --> 00:42:15,240
of problems and get to focus on the really interesting stuff. AI is really good at the long tail

395
00:42:15,240 --> 00:42:22,040
type of problems and let's be humans that can focus on the creative things, the development of

396
00:42:22,040 --> 00:42:26,680
the organization and the business. So we try to position ourselves not as a threat, but as an

397
00:42:26,680 --> 00:42:34,040
enabler for people in this organization to be their best themself. Awesome. Is there

398
00:42:35,800 --> 00:42:43,080
is part of the way you talk about what your group is doing? Do you have some moonshot

399
00:42:43,960 --> 00:42:51,640
project or vision that kind of captures what you're looking to create over the long term?

400
00:42:51,640 --> 00:42:57,800
Yeah, I think it goes back to the tech clip we want to do. We want to have all core operational

401
00:42:57,800 --> 00:43:05,640
decisions amplified by AI by 2025. That's where we want to be and I think it's possible with the

402
00:43:05,640 --> 00:43:13,320
strategy that we have created and I truly believe that if we're able to onboard more resources

403
00:43:13,320 --> 00:43:21,000
of talent and grow the president we will be able to transform this company to a truly an AI

404
00:43:21,000 --> 00:43:27,080
first type of company. Awesome, awesome. Well, Ariel, thanks so much for taking the time to share a bit

405
00:43:27,080 --> 00:43:57,000
about what you're up to. Very cool stuff. Thank you so much. That's your beer. My pleasure, thanks.

