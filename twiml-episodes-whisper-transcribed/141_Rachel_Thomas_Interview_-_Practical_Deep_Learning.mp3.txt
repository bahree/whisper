Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Tomorrow, May 15, the Twimble online meetup is back.
Our main event will be a presentation by Santosh GSK on the paper Yolo 9000, Better Faster
Stronger, which was written by Joseph Redman and Ali Farhadi.
Yolo, or you only look once, is a very popular image object detection system and will thoroughly
review it, along with the broader object detection landscape, the current state of detection
algorithms, and the various challenges ahead.
If you aren't already signed up, head over to twimbleai.com slash meetup to register.
See you there.
In this episode, I'm joined by Rachel Thomas, founder and researcher at Fast AI.
If you're not familiar with Fast AI, the company offers a series of courses including practical
deep learning for coders, cutting edge deep learning for coders, and Rachel's computational
linear algebra course, and they're all free.
The courses are designed to make deep learning more accessible to those without the extensive
math backgrounds that some other courses assume.
Rachel and I cover a lot of ground in this conversation, starting with the philosophy and
goals behind the Fast AI courses.
We also cover Fast AI's recent decision to switch their courses from TensorFlow to PyTorch,
the reasons for this, and the lessons they've learned in the process.
We chat about the role of the Fast AI deep learning in library as well, and how it was
recently used to help their team achieve top results on a popular industry benchmark of
training time and training cost by a factor of more than 10.
I also announce on this show that I'll be working through the Fast AI practical deep learning
for coders course starting in June, and I'm organizing a study and support group via
the Twimble Online Meetup.
So if you're like me, and taking this course has been on your list for a while, but you've
put it off, let's knock it out together.
You can join in by registering for the meetup at twimlai.com slash meetup, and letting
us know that you're in on the Fast AI channel that I've created there.
And now on to the interview.
All right, everyone.
I am on the line with Rachel Thomas.
Rachel is founder and researcher at Fast AI and an assistant professor with a data institute
at the University of San Francisco.
Rachel, welcome to this weekend machine learning in AI.
Oh, thank you.
Thanks for having me.
Why don't we get started by having you tell us about your background and how you made
your way into machine learning in AI?
Sure.
Yeah.
So I studied math and computer science in college, but I was really focused on kind of stuff
that was as theoretical as possible.
I did a PhD in math planning, planning to become a professor, and then kind of changing
my mind towards the end, and stumbled into working into finance as a quant.
And that's where I first kind of started working with data a lot, and really enjoyed
it.
And so I read about data science being kind of this new field and tech.
And yeah, I moved to San Francisco and started working for tech startups.
I was an early engineer and data scientist at Uber, and then I returned to teaching.
So I love teaching always, I think, come back to it in some form and taught full-stacks
offer development to women at Hackbright.
And then two years ago, Jeremy Howard and I started, fast, AI with the goal of making
deep learning more accessible and easier to use.
This kind of brings together my enjoyment of machine learning as well as, yeah, trying
to create a more inclusive field, yeah, and really make the field more accessible.
And at Fast Day, I would do a mix of research and education.
So our goal is we teach a course, practical deep learning for coders that's available
for free online, and we want every time we teach it to be able to teach it to an even
broader audience and have even better, faster results.
And so part of that is building the libraries and tools we need to make that possible.
Okay.
Awesome.
Well, we'll dig into all of that.
Before we do, I will fess up to not having gone through the course yet.
It has been on my list.
And I do plan to do it.
And in fact, I'll throw this out there.
If anyone in the listening audience wants to do a kind of a support group or a group for
going through the course, I will commit to doing it in June.
So we can start in June 1st.
On June 1st, I should say.
We've got a in our meetup, we've got a slight channel, and I think I've already created
a channel for doing the course because I'd hope to do it sooner.
But if anyone wants to join me in getting started with it in June, I think it'd be great
to do it as a group.
Yeah, that's great.
I was going to say we have, when we have forums, forums.fast.ai, so definitely check those
out for asking questions and getting help.
But yeah, we really encourage people to find groups to do it with because I know a lot
of people, it really helps to have that accountability and people to talk with as they're working
through the course.
And I know a few different companies where groups have worked through the course together
and you know, like have lunch once a week to discuss how they're doing.
Oh, yeah, that sounds great.
Yeah, there's also this program, AI Saturdays, that we only found out about this after a
while.
But I think they're in like 60 countries and it's like people get together on Saturdays
and in the morning, they work through the course together.
Oh, wow.
Oh, very cool.
Yeah, yeah.
So that's AI Saturdays.
Very cool.
Well, why don't we start by having you talk a little bit about the course and go a little
bit deeper into the motivation behind the course, what makes it unique, relative to all
of the other courses that are out there and kind of how you see the education landscape
around deep learning.
Sure.
Yeah.
This is the course that I wish had existed five years ago when I was first getting interested
in deep learning.
Mm-hmm.
And it kind of came with, I think, a lot of me and Jeremy's frustrations at the time with
a kind of existing materials.
But a lot of resources for deep learning are either they're very theoretical.
And so, you know, they're not too accessible to people that don't have a graduate math background.
And even as someone that did have a graduate math background, it was not that helpful for
coding.
It was like, I want to build kind of like practical applications using this.
And yeah, reading the theory wasn't that helpful to me.
We're starting to see, I think, a lot more practical courses and tutorials out there.
But many of them kind of settle for these, you know, they work on toy problems and have
like, okay, results.
But we really wanted something that would get you to the state of the art and that you
could use in the workplace and have state of the art results, but have it be super practical.
So our course is distinctive and that there are no math prerequisites.
The only prerequisite is one year of coding experience.
And it gets you to the state of the art.
Something else that's pretty unusual about it is we use a stop down teaching approach.
So most technical education is, we call it bottom up, but it's where you have to learn
each individual like underlying component that you'll be using.
And then, you know, eventually you can put them together to do something interesting.
And this is how math is taught as well.
And so it's like, for years, students are kind of like, what's the big picture?
Why am I learning, you know, like all these little components?
And you can do really awesome stuff later, but so many people lose the motivation when
they don't have that big picture.
And so our goal with the course is to get you training and training a model right away,
like in three lines of code.
And then as time goes on, we get into these underlying details.
And so this is a lot more similar to how they sports or music are taught, where, you
know, kids can be playing baseball, even if they don't know the formal rules, they might
not have, you know, a full team or a full nine innings.
And as they get older, they learn more rules.
And so that's kind of what we're doing with people learning is like, show you how to
use it.
And then we explain how it works later.
And that's kind of the opposite approach for many, many courses.
And there are two courses or at least two parts to the second version of the course.
How are the two parts differentiated?
Sure.
Yeah.
So part one, we call practical deep learning for coders, and that kind of goes over a lot
of, I guess, like core areas of, you know, like using convolutional neural networks for
image classification.
We do a little bit of language with RNNs.
We also do, we cover how to work on tabular data, collaborative filtering, so for, you
know, making predictions of, you know, like movie recommendations, so that's all in part
one.
Just kind of really to get you using the tools proficiently.
And then part two is cutting edge, cutting edge deep learning.
And so that's a lot more, which you had to start reading and implementing papers.
Oh, nice.
And it's, yeah, it's exciting to see because we've had a lot of students are like, oh,
my gosh, this is so intimidating.
You know, I did not think I would be able to like read one of these papers and implement
it to have them have them doing that now.
Oh, fantastic.
Well, I'll plan on hitting part two as well this summer.
Yeah.
Yeah.
We'll be releasing the updated version of that in the next month or two.
So we just wrapped up the in person version.
So we kind of teach it in person and then we really sit online.
But that includes stuff like bounding boxes and gans.
We did some really neat stuff with language as we're using transfer learning to kind of
use a language model for then text classification problems, style, transfer.
Oh, very cool.
And the course is offered both online and in person.
What's the motivation behind doing the in person course given that you can reach so many
people doing it online?
So I think with the in person course, it's really helpful to have that kind of energy
and feedback.
I think it's hard to record a course just in an empty room, but to be getting student
questions.
And we have to really get to know a lot of the students taking the in person course.
There's been a pretty good community around it.
The in person course, I should say it meets one evening a week in San Francisco.
So most people taking the course are working full-time, most of them work in tech.
As part of the in person course, we've also had two programs.
We have diversity fellows, and so this is to encourage more women, people of color and
LGBTQ people to take the course, and that's really, I think, helped us get a more diverse
audience, which is great.
And then we also have an international fellows program, and that's people that are remote
from all over the world, but they are participating in the course in real time.
And so I think that those have been really important components of the in person course.
One of the things that I noticed not too long ago was there were some announcements
about the course shifting from, actually, I forget the framework that it was using before.
So version one part one was in caros, and then version one part two was a blend of caros
and tensorflow.
I should say, yeah, we were using caros on top of the ono for the original version, and
then we introduced some tensorflow, but then kind of last summer when we started working
on version two, we switched to pie torch.
And I guess, actually now that I think about it, we had used some pie torch in version
one part two, because we found there were some stuff that was just almost impossible for
us to do in tensorflow, and so we had started using pie torches, it was pretty soon after
it released, and it was just such a fun language to use, and made a lot of things feel so much
more intuitive and easier, so this year, yeah, the course was entirely in pie torch, as
well as a high level API that we've written in our own library, fast AI that sits on top
of pie torch.
What were some of the things that were impossible to do in tensorflow that you were able to do
in pie torch?
So I remember we were having a lot of trouble with teacher forcing, so this is in natural
language processing, where for text model, we're trying to predict what the next word
in a sentence will be, you initially, as you're training it, you want to give it the right
answer initially, and then kind of with a probability, you want to be reducing the chance,
so since this is an RNN, you're predicting one word and then the next, and then the next,
if you always give it what the actual next word is, the network is going to make kind of
be more willing to make wild predictions, because it's not going to like hurt it long-term,
but if you never give it, you can kind of get too off, and so that was something to be
able to kind of have this probabilistic change happening while you're training that we found
pretty difficult.
Is that something that teacher forcing, is that an issue around initialization, or is
it an issue around labels, or neither?
It's more, it's more an issue on kind of how you're training, although actually I don't
want to focus too much on that, I should say kind of like high level, like the big reasons
we changed were one, I think a key thing for me is pie torch is easier to debug, and
I think in any sort of coding, just being able to debug easily is really important.
So with TensorFlow, and so this TensorFlow has now released a dynamic version, but for the
first few years of its life, TensorFlow was just a new construct, it's called a static
computation graph, and so you're kind of constructing the graph, and then you execute
stuff on it, and so by the time you get an error, it can feel very removed from the line
of code that actually caused that error, whereas pie torch is a dynamic graph, and so when
you get an error, it's right at the line of code that caused it, and so I think that feels
much easier to debug.
I should note that TensorFlow has released an early execution mode that does this and
is more like pie torch, and this is something I think that TensorFlow is having to play
ketchup to pie torch, I think they kind of saw how successful pie torch was with the
dynamic graph, and so I think TensorFlow is still behind in this area, and it's tough
because TensorFlow has such a huge code base that I think it's harder for them to be nimble
when they make changes.
Right.
So that's one area, something else that's great about pie torch is it's written in kind
of this very standard Python object-oriented paradigm, and so I think for people that
have done other Python programming or either other object-oriented programming in different
languages, I think it feels a lot more natural and intuitive, just kind of how it's how
it's structured, whereas TensorFlow has, it has a lot of TensorFlow specific conventions
that you have to learn around sessions and scope, but don't, yeah, just kind of aren't
as commonly used in other programming languages.
And so do you see pie torch?
And I also, I'll say one more thing about the makes pie torch great is, so TensorFlow
is a much larger library, and that can be difficult because it's like, I don't know, they're
both four ways to do anything you want to do, whereas pie torch is kind of a much smaller
set of features, but they were designed to be super flexible.
And so it's very easy to kind of build, build what you want because you have these very
flexible pieces that you can combine well.
Okay.
And so do you think all of those have made pie torch a better choice for education, but
maybe not a strong that choice for production use cases, or do you think it's a solid
choice for production as well?
I think it's a solid choice for production as well.
I think something to remember is in a lot of pieces, you don't need to be training in
production, and so you can train a model and then kind of have your, really, you can
just take your predictions and put them on like an end point for, you know, if you have
like a flask app or, you know, any other sort of app, like you don't necessarily kind of
need this machine learning component in production when you're making your inferences,
like you can just kind of like have that function, yeah, attach to something else.
I think that yeah, TensorFlow, like if you, I mean, if you need to be doing deep learning
on an edge device, yeah, TensorFlow is definitely way more developed for that.
And if you're doing something at Google scale, where you do want to be using, yeah, which
I think very few people other than Google are.
So I think I think that PyTorch can, yeah, can be good for production for most people.
And to what degree has the relative lack of, you know, library, community contributed
components and that kind of thing, you know, to what degree do you think that holds back
PyTorch?
I mean, I think, I think some of their members, PyTorch, it's just, I mean, it's crazy
to think it's really, I was like January or February 2017 that it came out.
So it's really just a little bit more than a year.
And so I think that they've made great progress in that time.
It is still, yeah, this kind of very young project.
And I remember when, you know, there was this point in time, I forget how long ago it was,
but I remember this point in time where, you know, prior to, prior to, you know, the general
sense was that TensorFlow had pretty much locked things up in terms of deep learning frameworks
and all of a sudden, PyTorch came out of nowhere.
Yeah.
And then I think you, yeah, you all piled on soon after that.
I really, like, Google has just spent so much on marketing TensorFlow that it's hard,
like, and they just have really, really marketed TensorFlow because I feel like I talked to
so many people that, like, don't do deep learning, have never used TensorFlow, but they've heard
about it.
And I think it's great because they, but it, like, what they've heard sounds like mostly
kind of like marketing from Google.
So I think that that has, that has really impacted or kind of skewed, skewed things towards
making TensorFlow seem more popular for, I mean, this is also hard to remember, you know,
across, you know, it's been incorporated into core TensorFlow now, but that was not originally
a part of TensorFlow.
And so, yeah, like part very first version of our course, like we were using caros on top
with the ono, although yeah, the ono has now been, been deprecated.
The space is changing so quickly.
It is.
Yeah.
Do you feel like it's, it's stabilized or do you think it could change any day?
Yeah, I think it'll continue changing, tear me actually wrote in our blog post last year
when we announced we were switching to PyTorch that basically everyone working in the
field should assume we'll have to switch languages and frameworks a few more times because
it's just, it's just going to keep changing possibly.
That sounds really scary for, you know, putting on my enterprise hat, someone that wants
to start, you know, really building, you know, real stuff and, and, you know, business
critical functionality of the fluidity.
What do you think about efforts like Onyx and some of these other things that are trying
to create, you know, either portability between frameworks or cross compilation to, you
know, different frameworks.
Yeah.
I'm not as familiar with those, but from what I know, I think that's a great idea.
Yeah, and I meant to bring up Onyx earlier as another potential solution to someone that's
working in PyTorch and wants to deploy to production.
Yeah, I know, I think those are good efforts.
And yeah, I know, I said that it's changing a lot.
I mean, I think that they're, you know, if you want to be doing this stuff in production,
like you absolutely can and should be right now, I don't feel like it's, everything's
going to change tomorrow.
But I think, I think there's something exciting about being in a relatively young field that
is, is so dynamic and where, you know, a lot of the changes we're seeing are, you know,
are these huge improvements?
Mm-hmm.
So you mentioned that in addition to the course, there's also a fast AI library that you
distribute and use in the course.
And I've recently seen you publish some benchmarks that I think have something to do with that
library.
Can you talk about the library and its purpose and the benchmarks?
Yeah.
And so the library, the goal of it, and I should also put the disclaimer, we need to release
more documentation about around it and that's the thing that we're working on right now,
and we'll be, we'll be coming out this summer.
But the goal of the library was to kind of give a high level API and code, and also to
encode kind of a lot of best practices and smart defaults.
Like I think something that can be overwhelming, like when you're first learning, deep learning
and the kind of the impression that you get from it, some places are, you know, there
are just so many hyper parameters, and it's like, oh my gosh, I've got all these hyper
parameters.
What do I choose?
And how do I tune those?
And so we really just kind of wanted to make it easier and give you good defaults.
And so if you want, you can have to really just think about like one hyper parameter.
And then, you know, later on, if there's more stuff you want to change, that's easy to
do as well.
And to kind of have nice high level abstractions.
So that was the goal of the library.
And so it's a great teaching tool, but I think it's also a good thing to use in practice.
And you mentioned our benchmarks.
And so this was part of Stanford, the Stanford Dawn lab hosted a competition called Stanford
Dawn bench.
And so this was for, there are two categories, ImageNet and CIFAR 10, which are these
classic, image classification problems.
However, typically, you know, like the ImageNet competition was about being most accurate.
This was about being fastest and cheapest.
And, you know, there was a baseline.
You had to be at least 93% accurate, but past that point, yeah, what was fastest and
cheapest?
Sounds a lot like a TPC benchmarks, if you're familiar with those.
I am not.
I think it's not TPS.
That's the office-based thing.
It's a transaction processing council.
It's like they do a transaction per second benchmarks across, like, you know, big iron.
And there was a point in time where commodity computers, clusters of commodity compute
started to supplant the, you know, the big expensive monolithic hardware boxes.
And it sounds like from just kind of casually seeing some of the tweets and stuff about
your benchmarks, that that's kind of what you, you know, what some of your results were
about.
Well, so what we found, so for the CIFAR 10, which is a smaller data set, although I think
it's a size that's more, it's like 160 megabytes, that is representative of what a lot of
businesses and companies have.
We won both sections, fastest and cheapest.
For the image net, these are larger data set, it's like 160 gigabytes.
We were the fastest on publicly available infrastructure, fastest on GPUs, fastest on
a single machine, and lowest actual cost.
And I should say this was, this was Jeremy working with a group of our fast-day-eye students
who kind of have this in-person study group that's been meeting every day.
But I think it was really exciting to prove that the fast-day-eye library, you know, was
super helpful to this.
And so I mean, this was, you know, it's like fast-day-eye library, which is built on top
of a pie torch.
We're using Nvidia GPUs on AWS and AWS spot instances.
But a lot of, so like Google and Intel, like their general strategy in this competition
was kind of just having way more hardware.
And we really tried to approach it as kind of using more algorithmic creativity because
kind of a core, I think, part of our mission and like the thesis we're trying to prove
with fast-day-eye is that deep learning can be accessible to people from all backgrounds
and that you don't have to, you know, be able to afford like these very, very expensive
clusters of machines and that you don't, you know, you don't need to have a PhD from
Stanford.
And so here, you know, this was a group of like part-time students and we're trying to
try to do things cheaply.
Her David and Goliath story.
Yes.
Yes, exactly.
Yeah, our, we had an entry on a single machine that was faster than Intel's entry on a cluster
of 128 machines.
Oh, wow.
And so, and so what we were doing here was just, yeah, kind of being creative with our algorithms
and also using, I don't know, in the deep learning community, there's some results that
have really kind of been neglected because they're not from, there's a lot of attention
in the community to, you know, like what is Stanford doing?
What is OpenAI doing?
And, you know, these people that kind of have this name, Cache, or recognition.
And so part of what we were doing was kind of implementing results from other people
that are at these, you know, less famous or less prestigious institutions and showing
like, hey, you know, we can, we can use this to, to get, to get faster results.
How would you attribute the benefits, the performance, the ability to, to run at low
cost between the fast AI library and pie torch?
In other words, were you competing against other pie torch based entrants?
Oh, now we have a shoe who gets, yeah.
So I know the Google team was definitely using TensorFlow.
And I am pretty sure Intel was using TensorFlow as well.
Or even what about the structure?
Tell us about the structure of the task and the, some of the, the key things, and it's
interesting, the stuff we were implementing, they're actually not that complicated as ideas.
So one of them is this idea of super convergence, and this is something that Leslie Smith,
who works at the Naval, Naval Research Laboratory found, but it's the idea that you can
use way higher learning rates if you lower your momentum.
And momentum is kind of a factor used in, in optimization of, you know, they're all
these variants on stochastic gradient is that, dissented.
So momentum is kind of a commonly used part of that.
And I don't know if anybody else is lowering though the, the momentum part, the, dynamically
as training happens.
So backing off momentum as you're converging?
Well, as, well, actually, it's, it's backing off momentum as you're increasing your
learning rates, these high rates, and then you decrease your learning rate again and increase
your momentum.
So kind of keeping, you kind of have this triangle shape with both, and they're like inversely
related.
Interesting.
Yeah.
So it's like, and you can check that we have a blog post where it kind of writes more details
about what we were doing.
But yeah, it was like learning rates, increasing, all the momentum's decreasing, and then
learning rate decreases, well, momentum increases.
But that, yeah, allows for much faster training.
And so here, you know, this is something that, these are things that, the combination
of fast AI and high torch made very easy to implement.
And having, you know, having that kind of dynamic change as you're learning is something
that is typically harder to do in TensorFlow.
So this was, yeah, I mean, I think it's kind of hard to talk about fast AI.
And, you know, it's the flexibility of high torch that may make fast AI possible.
And, you know, fast AI is fairly flexible as well, since it's written on high torch.
So that was one component.
And then there was this other idea of progressive resizing where you start out training your
network on small versions of your images, which makes sense.
It's kind of just trying to learn, you know, like, very, very rough things, and then as
training happens, you're using kind of larger and larger versions of the images.
And so again, yeah, high torch is great to kind of have that dynamic nature.
Oh, that sounds really interesting as well.
And this is also something I think it's exciting, because it's like the ideas on their own
like are not, you know, it's not like this, I don't know, like complicated thing of math
equations, you know, it's, you know, it's like, hey, this makes sense, you know, intuitively
of like, start training on small images and then train on larger ones.
And, you know, it's also like that way your network kind of has information about images
of different sizes.
And so those are, you know, a lot of, a lot of the breakthroughs and Jeremy points
this out in the blog post, a lot of these breakthroughs that have really helped us in
deep learning, you know, things like using rectified linear units for activations and drop
out where you kind of randomly drop a lot of, a lot of your weights to avoid overfitting
each time or batch normalization, they're actually fairly simple ideas.
And they need things in some ways like easier to understand while at the same time really
improving the performance of neural networks.
So that we really think a lot of the breakthroughs are going to come by kind of, you know, these
new insight where you can like do something differently and not just from getting bigger
and bigger clusters of computers.
Interesting.
And to what extent do you think the techniques that were used in the benchmark are, you
know, practical techniques?
In other words, are they kind of not gaming the benchmark with a negative connotation,
but were they kind of designed to, you know, compete well in the benchmark, but not something
that you would necessarily do in practice or, you know, they're very, very practical.
Yeah.
So there are all things you could do in practice.
And some of them do still kind of raise issues, it's a one issue that we ran into and
I think it's a kind of understudied issue in the field is that how to best use multiple
GPUs, I think often people, they kind of release with new GPUs, you know, like how many calculations
can be done on them, how quickly, but that doesn't take into account that like training
your network really changes when you go from one GPU to many GPUs.
And so you don't necessarily, well, you don't get the speed up of, you know, going from
one GPU to a GPU is does not mean you're going to do stuff eight times faster.
And so this is an area where I think that definitely needs to be more researched on on kind
of how do you get the, get the most out of going from, yeah, one GPU to multi GPUs.
Did you do anything with, with low precision in this, in this particular benchmark?
We did.
Yes.
And so, and this is key to Nvidia's new bolt to architecture with key to this because
it gives you half precision floating point data.
And this was something that, I guess Nvidia had provided an open source demonstration
and then we had a student that worked on this to incorporate the ideas into fast AI.
And this is Andrew Shaw.
But the kind of issue is that so half precision, you know, is kind of letting you do stuff
using less space, but you do need to convert to full precision for some of the steps.
So like when you multiply by your learning rate, but so that's something we've got, we've
got implemented now.
Yeah, that's something that I have the sense gets glossed over a bit and Nvidia's presentations
about this.
It's not like flipping a half, you know, a low precision switch and everything gets
faster.
Yeah.
You have to really dig into it.
Yeah, because you have to think about where, you know, where is it useful and not going
to hurt you?
Yeah, versus, okay, like where the places where you need full precision.
And I think, I mean, I think that happens in a lot of kind of talking about hardware
specs, of course, you know, because you're just like, you know, focused on this piece.
And, you know, deep learning is this, you know, system that's got so many components
and thinking about how is it work as a whole.
Mm-hmm. And so where do you see the library going?
Is it still evolving quickly?
Is it?
Yeah.
Yes.
Still evolving quickly.
And I think we definitely still, like even coming from the competition, have a few
more things we need to get incorporated.
And then we also, we really want to work on documentation and usability in terms of
for new users.
I think if you're, you know, working through the course, that really gives you a lot
of a lot of information and context of kind of seeing how things are built that we
want to, yeah, kind of make it even more, more user friendly beyond that.
And we'll continue, you know, as we, as we teach the course and as new papers come out,
I think, and as we're doing more research, you know, that gives us kind of more ideas
of things to implement.
And do you see it primarily as an educational tool or something that someone would use?
Yes.
We see it as something that would be useful in the workplace.
Okay.
And is the, to what extent, I mean, you kind of addressed this in introducing it, but,
you know, when I think of higher level abstractions, I certainly get the notion of, you know,
saving folks that are new to the field from a lot of the details, but that saving, you
know, usually comes from hiding, which can be a bit of an impediment to education and
understanding.
How do you balance those two?
Yeah, that's a good question.
So I mean, some of it is this, you know, this top down teaching approach I described earlier
of like, yeah, like initially we are hiding a lot of things, but I think having the code
written in such a way that it, like, we definitely then get into the details later on, and it
is written in a way that, like, makes it easy for people to customize and if they need
to modify things on a, on a lower level, they can do so.
And so, like, our goal with the library is to kind of be providing both of those.
And this is also, I mean, I think this is something that PyTorch is good at as well, but
that if, yeah, if you want to, you know, add something to the library or change how it's
doing something like there are places to add custom hooks and to use, I think the library
is constructed to make that easy and reasonable.
And is the idea that you would, you know, maybe use the library as a wrapper to some things
that you don't want to, you know, mess with the details about, but then, you know, when
you want to dig into those details, you would not use the library and go straight to PyTorch
or is there a-
I mean, I think it's, I think you would continue to use the library, it would more just
be if you needed to change some, I don't know, if you're running some experiments, you
know, your researcher kind of doing your own thing, you can, or you're, you know, you're
trying out a new architecture that maybe then you would need to dip into changing some
of the details, but I think that our goal is to have it, you know, I think part of what
this competition showed is that the library itself is achieving state-of-the-art results.
And so it's not that it's been a hurt your performance or something to be using this high
level library.
Right.
So the performance and cost objectives of this benchmark are clearly important, particularly
as folks are running these types of workloads, not just on machines that are sitting on
their desk, but in the cloud where they're paying for them by the minute or hour.
But another key often maybe overlooked benchmark or metric is around productivity and the ability
to iterate, innovate quickly.
Is anyone, are there any benchmarks for that?
It's harder to do.
Yeah, I don't know how you would benchmark that.
I mean, I would say like, it's almost a bit of a meme on Twitter of a lot of people saying
that like they have to, they use TensorFlow for their jobs, but they use PyTorch for their
experimentation.
I feel like that's what they're doing in the evenings.
And I mean, there are plenty of companies using PyTorch as well, but just that I think
that's part of the reason people love PyTorch is it's so easy to iterate and kind of
experiment the way you're talking about, because I think, you know, there are people that
will say like, oh, you know, competition graphs, like theoretically, that's letting the
compiler optimize more, so you should get better performance with a, or sorry, with a static
competition graph and with dynamic, but it's just that's not what people have found in
practice.
And I think part of that is with, with the dynamic framework, you're able to iterate
so much quicker.
So you do end up kind of getting better performance, but yeah, I don't, I don't know if any benchmarks
that are formally, formally studying the distinction.
Okay.
So one of the often cited barriers to folks getting into this field is the level of math
that's required and one of the things that you've done personally to try to address
this is develop a class on linear algebra, computational linear algebra in particular.
Can you talk a little bit about that?
Sure.
Yeah.
So computational linear algebra.
So this is a course I teach in the master's program at USF, but I've released all the
videos and notebooks online.
So it's a study of getting matrix, or so you're getting computers to do matrix calculations
with acceptable speed and acceptable accuracy.
And so I would say whether or not you have taken or liked kind of traditional linear algebra,
this is a very different field because a traditional linear algebra course is typically,
you know, having students kind of do these matrix computations by hand, which is just
like a whole different set of considerations from when you're getting a computer to do
them.
But one of the kind of key themes of the course is the idea of decomposing matrices.
As you can think of this, it's kind of analogous to, you know, like you can factor a number
into primes and that's useful, you know, because primes have the special property, the
same idea with matrices, you can kind of factor them into component matrices that have special
properties.
And so the course is, it's similar teaching philosophy to the deep learning course and
that it's all, it's very code based.
It's all in Jupyter notebooks and it's all centered around applications and so applications
we look at are removing the background from a surveillance video of, you know, identifying
what's foreground and background, topic modeling for a corpus of documents, kind of digging
into, you know, Google's PageRank algorithm, because that's actually a matrix decomposition.
And so kind of going, singular value decomposition, which is used for compressing data.
It's also used for, you can remove errors.
There's a really, really neat example of kind of a highly corrupted data set where there's
just all these errors in the picture and you can actually remove it and kind of find what
the underlying picture probably was of, this is of people's faces.
So yeah, the course is very kind of application focused, but yeah, computational linear algebra
includes a lot of things like approximate algorithms or randomized algorithms, you know,
like often you, you know, like what are the cases where you don't need to be super precise
or even, you know, often your data only has so much precision, so it doesn't make sense
to, you know, try to get an algorithm that's going to give you something accurate to the
10th decimal place.
It also looks at, you know, issues around a machine epsilon and the kind of errors that
can get introduced through through computer calculations.
And what's machine epsilon?
And so that's just, so the way, you know, it's like numbers are infinite and continuous
and it's like computers are, you know, finite and limited.
So floating point representation noise.
Yeah, floating point representation, exactly.
Okay.
Well, that sounds like a really interesting class, too.
This is how my classes to take lists gets really long.
Right.
Right.
And so this one is in a structured class, but you've published all the materials online.
Yeah, all the materials online.
So yeah, on GitHub or all the Jupyter notebooks and then all the videos are on YouTube.
Oh, that sounds very cool.
Yeah, I feel like I've been talking a lot about like why I like pi torch more than TensorFlow.
But I did want to bring up that I think TensorFlow has some really neat developments happening.
And so I'm definitely still keeping my eye on TensorFlow.
I think they, so they Chris Lattner announced at the TensorFlow Dev Summit two months ago
that they're going to be releasing its Swift for TensorFlow.
And so, you know, Swift is the language that Chris Lattner invented for iOS development.
And I see I spent several months, I guess, in 2015 kind of like learning Swift and learning
how to build mobile apps.
And it is a really neat language that they're going to be kind of having a version of that.
So it's like a whole nother language that would be, it would not be just for iOS.
And it's, you know, specifically for neural networks, this Swift for TensorFlow version.
That's something that really interests me.
And then it was kind of exciting to hear some of the new releases.
Like they now have a TensorFlow JavaScript version of TensorFlow, which is I think really great.
They have released these Google Colab notebooks where you can kind of like basically launch these interactive examples.
So I felt bad that I was perhaps criticizing TensorFlow earlier when I do think that there are some interesting developments happening there.
Yeah, I think there's no question that Google's making some tremendous contributions with TensorFlow
and that broader ecosystem.
Yeah, and like, I mean, right now we're using PyTorch for the course.
And you know, there are things I love about PyTorch, but I'm definitely not.
And in general, I'm not until like, you know, saying like, hey, this is the language I'm always going to use.
I hate other languages or anything.
You know, it's you want to recognize kind of what each language is contributing and be flexible.
So should we move on to them versus Emax now?
I am Twitter and Twitter recently.
I just did this poll about what would you call this type of data?
It got more comments and engagement than like I think anything else I've ever tweeted.
I was like asking asking programmers to name something as a.
Tabs and spaces, anyone?
Yeah.
It's underrated a lot of debate.
Nice, nice.
Awesome.
Is there anything else that you'd like to touch on or any final parting words for the audience?
Definitely. I just want to encourage people to check out the deep learning course.
That's a course dot fast dot AI.
Absolutely. And if if anyone who's listening wants to join me in going through the course,
starting the beginning of June, either hit me up on Twitter or via the twomla.com website
or just go ahead and join our meetup and you'll get an invitation to our Slack channel.
And just chime in there and we will we will get it going.
Great.
No, I think it's awesome.
You're doing that.
Awesome. Well, thanks so much, Rachel.
Thank you, Sam.
All right, everyone. That's our show for today.
For more information on Rachel or any of the topics covered in this episode,
head on over to twomla.com slash talk slash 138.
Remember, the twomla online meetup is tomorrow and starting in June,
we'll be organizing a group to take the fast AI practical deep learning course.
Don't miss either and sign up for both at twomla.com slash meetup.
Okay, thanks so much for listening and catch you next time.
