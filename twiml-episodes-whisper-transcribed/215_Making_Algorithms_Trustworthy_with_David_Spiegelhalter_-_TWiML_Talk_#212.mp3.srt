1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,640
I'm your host Sam Charrington.

4
00:00:31,640 --> 00:00:37,120
In this, the second episode of our NURRIP series were joined by David Spiegelhalter, chair

5
00:00:37,120 --> 00:00:42,280
of the Winton Center for Risk and Evidence Communication at Cambridge University and president

6
00:00:42,280 --> 00:00:45,080
of the Royal Statistical Society.

7
00:00:45,080 --> 00:00:50,280
David, who was an invited speaker at NURRIP's, presented on making algorithms trustworthy,

8
00:00:50,280 --> 00:00:55,760
what can statistical science contribute to transparency, explanation and validation.

9
00:00:55,760 --> 00:00:59,600
In our conversation, David and I explore the nuance difference between being trusted

10
00:00:59,600 --> 00:01:04,600
and being trustworthy and its implications for those building AI systems.

11
00:01:04,600 --> 00:01:10,160
We also dig into how we can evaluate trustworthiness, which David breaks into four phases, the inspiration

12
00:01:10,160 --> 00:01:16,760
for which he drew from British philosopher Onora O'Neill's ideas around intelligent transparency.

13
00:01:16,760 --> 00:01:17,760
Enjoy.

14
00:01:17,760 --> 00:01:23,480
Alright everyone, I am here in Montreal for the NURRIP's conference and I've got the pleasure

15
00:01:23,480 --> 00:01:26,280
of being seated with David Spiegelhalter.

16
00:01:26,280 --> 00:01:32,600
David is chair of the Winton Center for Risk and Evidence Communication at Cambridge, as

17
00:01:32,600 --> 00:01:38,040
well as president of the Royal Statistical Society and he was one of the invited speakers

18
00:01:38,040 --> 00:01:42,440
here at NURRIP's, talking on making algorithms trustworthy.

19
00:01:42,440 --> 00:01:45,240
David, welcome to this week in machine learning and AI.

20
00:01:45,240 --> 00:01:49,880
Thank you for having me, it's our pleasure.

21
00:01:49,880 --> 00:01:54,200
Before we jump into the topic of your talk, please share a little bit of your background

22
00:01:54,200 --> 00:02:00,040
and how you get involved in statistics, machine learning and kind of the confluence of

23
00:02:00,040 --> 00:02:01,040
the two.

24
00:02:01,040 --> 00:02:06,320
Exactly, well I'm a statistician as you can tell and I was around in one of the last summers

25
00:02:06,320 --> 00:02:12,120
of AI in the 1980s and I was very interested in computer-aided diagnosis such as it was

26
00:02:12,120 --> 00:02:16,960
then an interest in statistical approaches to that, using simple Bayesian methods or

27
00:02:16,960 --> 00:02:23,120
logistic regressions, the standard stuff and that was an exciting time and I got very

28
00:02:23,120 --> 00:02:29,280
interested in this new idea of Bayesian networks and graphical models and so in the 1980s

29
00:02:29,280 --> 00:02:33,360
I really worked and developed this thing called the Louds and Spiegelhalter algorithm that

30
00:02:33,360 --> 00:02:38,080
was for exact propagation in Bayesian networks and we did a lot of work in that and then

31
00:02:38,080 --> 00:02:44,200
I went into Bayesian graphical modeling, developing a bug software for Bayesian Markov-J

32
00:02:44,200 --> 00:02:54,040
Monte Carlo analysis and so on and worked all the time in this intersection of machine learning

33
00:02:54,040 --> 00:02:56,640
and AI and statistics.

34
00:02:56,640 --> 00:03:00,560
For the last 10 years I've been much more to do with communication and I've got a job

35
00:03:00,560 --> 00:03:05,600
that involves communicating statistics and risk and evidence and now we've got a center,

36
00:03:05,600 --> 00:03:10,280
a strange center in the maths department at Cambridge with a great gang of psychologists

37
00:03:10,280 --> 00:03:15,640
and communication specialist XBBC people, web designers, I'm very interested in producing

38
00:03:15,640 --> 00:03:21,120
trustworthy material that communicates numbers and statistics and risks and predictions

39
00:03:21,120 --> 00:03:22,120
and so on.

40
00:03:22,120 --> 00:03:23,280
Okay, that's really interesting.

41
00:03:23,280 --> 00:03:29,720
I was wondering what the meaning of risk and evidence communication was, almost anything

42
00:03:29,720 --> 00:03:31,320
to do with numbers.

43
00:03:31,320 --> 00:03:34,680
It's better than public communication and statistics.

44
00:03:34,680 --> 00:03:35,680
Right.

45
00:03:35,680 --> 00:03:36,680
Okay.

46
00:03:36,680 --> 00:03:37,680
Fantastic.

47
00:03:37,680 --> 00:03:42,440
And so you're here at NURBS talking about making algorithms trustworthy.

48
00:03:42,440 --> 00:03:43,440
What does that mean?

49
00:03:43,440 --> 00:03:46,000
Yeah, the issue of trust is very important.

50
00:03:46,000 --> 00:03:50,640
I've been very influenced by this wonderful philosopher in the UK and Nora O'Neill who studied

51
00:03:50,640 --> 00:03:56,560
Kant and has come up with this very important idea which has been very influential that

52
00:03:56,560 --> 00:04:00,680
organizations and developers of systems shouldn't be trying to be trusted.

53
00:04:00,680 --> 00:04:03,440
There's the wrong objective to try to be trusted.

54
00:04:03,440 --> 00:04:08,280
Well, they should be doing what we all should be doing is trying to be trustworthy in other

55
00:04:08,280 --> 00:04:13,520
ways to earn that trust because that is within our control to be trustworthy.

56
00:04:13,520 --> 00:04:18,800
And this idea of being trustworthy has a big impact in the UK, the National Statistics

57
00:04:18,800 --> 00:04:22,800
Code now puts trustworthiness as its number one objective.

58
00:04:22,800 --> 00:04:28,240
Why is that nuance important between being trusted and being trustworthy because being trusted

59
00:04:28,240 --> 00:04:32,040
is something you want, but other people can only offer it up to you.

60
00:04:32,040 --> 00:04:36,720
You trust worthy of something within your control, and that means really analyzing what

61
00:04:36,720 --> 00:04:38,240
it means to be trustworthy.

62
00:04:38,240 --> 00:04:39,240
Okay.

63
00:04:39,240 --> 00:04:44,840
And so what does that mean from a statistical perspective or how can statistics inform trustworthiness?

64
00:04:44,840 --> 00:04:50,800
Well, I think that in the talk I break trustworthiness of an algorithm or any sort of system into

65
00:04:50,800 --> 00:04:55,600
two components that the system itself should be trustworthy, the claims it makes should

66
00:04:55,600 --> 00:04:56,600
be trustworthy.

67
00:04:56,600 --> 00:05:00,840
You should be able to rely on them or if you can't rely on them, it can tell you how

68
00:05:00,840 --> 00:05:02,520
confident it is.

69
00:05:02,520 --> 00:05:06,960
The other thing is that what is very important is that the claims made about the system are

70
00:05:06,960 --> 00:05:10,760
trustworthy by the developers, by the commercial entity or whatever.

71
00:05:10,760 --> 00:05:14,480
So you've got not only believed the system, but you've got to believe what's said about

72
00:05:14,480 --> 00:05:15,480
the system.

73
00:05:15,480 --> 00:05:20,320
Now, what that leads you into very quickly is the importance of evaluation.

74
00:05:20,320 --> 00:05:25,520
And in my talk I draw an analogy with the highly developed evaluation phases that are

75
00:05:25,520 --> 00:05:29,280
used in drug development and pharmaceuticals, which statisticians I've worked in that area

76
00:05:29,280 --> 00:05:30,560
for decades.

77
00:05:30,560 --> 00:05:33,320
And they're just very briefly four phases.

78
00:05:33,320 --> 00:05:35,800
Phase one is safety on a few healthy people.

79
00:05:35,800 --> 00:05:41,000
Phase two is proof of concept done on some selected people to try to optimize the dosage.

80
00:05:41,000 --> 00:05:45,560
Phase three are the big control trials in which you actually compare it with a comparator

81
00:05:45,560 --> 00:05:50,000
and that allows you to market the drug and phase four is post-marketing surveillance.

82
00:05:50,000 --> 00:05:54,880
And what I did was draw an analogy with developing algorithms that are going to go into practice

83
00:05:54,880 --> 00:06:02,480
that phase one is just the digital testing that people do on a set of test cases.

84
00:06:02,480 --> 00:06:07,040
Phase two is laboratory tests where you actually compare it, say with doctors if you've got

85
00:06:07,040 --> 00:06:10,800
a medical system and do the user center design for the interface.

86
00:06:10,800 --> 00:06:14,560
And phase three is where the field tests where it actually goes out there and you actually

87
00:06:14,560 --> 00:06:18,360
evaluate what is impact is, which might be beneficial, but it could be harmful.

88
00:06:18,360 --> 00:06:20,520
You never know what side effects you might have.

89
00:06:20,520 --> 00:06:24,040
And phase four then is the post, once the thing is out there, monitoring to make sure it's

90
00:06:24,040 --> 00:06:27,520
not degrading and that it's not making mistakes.

91
00:06:27,520 --> 00:06:31,440
And so I suppose what I'm saying is that on the whole when I read about evaluations,

92
00:06:31,440 --> 00:06:35,280
they rarely get past phase one, they just sort of accuracy on test cases.

93
00:06:35,280 --> 00:06:40,480
Some of them moving into phase two comparison of diagnostic systems with medical, with experts

94
00:06:40,480 --> 00:06:41,480
and things like that.

95
00:06:41,480 --> 00:06:47,160
Almost nothing about phase three, what actually is the benefit impact when these things are

96
00:06:47,160 --> 00:06:50,280
put into practice in society and properly evaluated.

97
00:06:50,280 --> 00:06:55,280
And I think that the, in order for claims about a system to be trustworthy, then you need

98
00:06:55,280 --> 00:06:57,800
a much more rigorous evaluation.

99
00:06:57,800 --> 00:07:03,840
In order for claims about a system to be trustworthy, you need to have a much more rigorous evaluation.

100
00:07:03,840 --> 00:07:05,840
My sense is that we're very far from that today.

101
00:07:05,840 --> 00:07:06,840
Exactly.

102
00:07:06,840 --> 00:07:10,960
And I suppose what I'm saying, this field is developed so wonderfully.

103
00:07:10,960 --> 00:07:16,120
It's so, the stuff at the conference is so amazing, but it's still, for all that fantastic

104
00:07:16,120 --> 00:07:22,000
technical capacity at a very early stage, because when these things start moving into society,

105
00:07:22,000 --> 00:07:24,880
you find, you know, people are saying, hey, come on, you know, I want to divide it.

106
00:07:24,880 --> 00:07:28,160
It's not, you know, it's not immediately obvious that this is going to be a good thing

107
00:07:28,160 --> 00:07:29,640
in all areas.

108
00:07:29,640 --> 00:07:34,680
And so I think, you know, this area is due to mature into something which does rigorous

109
00:07:34,680 --> 00:07:35,680
evaluations.

110
00:07:35,680 --> 00:07:36,680
It's interesting.

111
00:07:36,680 --> 00:07:42,520
So one of the controversies that last year's Neurup's, then Nips was kind of a call for

112
00:07:42,520 --> 00:07:48,760
increased theoretical rigor around deep learning in particular, but, you know, our current

113
00:07:48,760 --> 00:07:52,360
approaches to AI in general.

114
00:07:52,360 --> 00:07:57,880
This is a call for rigor also, but a very different one, one from more of a statistical

115
00:07:57,880 --> 00:07:58,880
perspective.

116
00:07:58,880 --> 00:08:03,720
And it's about a rigorous test of what does it mean to actually implement this?

117
00:08:03,720 --> 00:08:05,080
Do you need both?

118
00:08:05,080 --> 00:08:10,200
Because you need the rigorous sort of internal analysis in order to demonstrate what it says

119
00:08:10,200 --> 00:08:11,760
is trustworthy.

120
00:08:11,760 --> 00:08:17,160
So because the part of the trustworthiness, of course, this is where we get to explanation

121
00:08:17,160 --> 00:08:21,160
is to be able to say why it's come up with its conclusion, to be able to justify that

122
00:08:21,160 --> 00:08:22,160
conclusion.

123
00:08:22,160 --> 00:08:25,760
And from the other statistical perspective, I take very strongly because statisticians

124
00:08:25,760 --> 00:08:29,840
are obsessed with uncertainty, getting the error bars right, you know, with as much

125
00:08:29,840 --> 00:08:33,160
concerned with the uncertainty as we are about the point estimate.

126
00:08:33,160 --> 00:08:34,920
And so that's what we bring.

127
00:08:34,920 --> 00:08:40,320
And I think again, if a claim is going to be made, and especially when it's made with

128
00:08:40,320 --> 00:08:43,760
some uncertainty or lack of confidence, you've got to understand what that means.

129
00:08:43,760 --> 00:08:51,120
You've got to rely not on the claimed confidence of what is, what is say, what an algorithm

130
00:08:51,120 --> 00:08:52,120
comes up with.

131
00:08:52,120 --> 00:08:59,240
And you've talked to you provide examples of this, the kinds of claims that you envision

132
00:08:59,240 --> 00:09:04,320
this kind of model being applied to and, you know, what you'd expect to see or what you've

133
00:09:04,320 --> 00:09:07,600
seen in kind of passing a claim through these filters.

134
00:09:07,600 --> 00:09:12,080
Well, in the talk, I just give various examples of different phases of how some statistical

135
00:09:12,080 --> 00:09:16,680
ideas can come in, just at the early phase when, you know, you're comparing algorithms

136
00:09:16,680 --> 00:09:19,720
on your database to see decide which is the best one.

137
00:09:19,720 --> 00:09:23,800
You know, I talk about ranking algorithms and how you use some bootstrap methods on the

138
00:09:23,800 --> 00:09:24,800
test set.

139
00:09:24,800 --> 00:09:28,120
You can get a probability that any algorithm is actually the best rather than just producing

140
00:09:28,120 --> 00:09:29,440
a simple league table.

141
00:09:29,440 --> 00:09:32,960
Again, there's been a lot of statistical work on league tables and essentially taking them

142
00:09:32,960 --> 00:09:37,520
apart because just because something happens to rank best on one particular set of data,

143
00:09:37,520 --> 00:09:39,880
it does not mean it's the best algorithm.

144
00:09:39,880 --> 00:09:42,880
Incidentally, for the football team, just because the football team is top of the league,

145
00:09:42,880 --> 00:09:46,120
it doesn't mean it's the best team because there's always luck involved and we're rather

146
00:09:46,120 --> 00:09:48,760
good at trying to put numbers on luck.

147
00:09:48,760 --> 00:09:50,840
So there's that aspect of the phase two.

148
00:09:50,840 --> 00:09:55,760
And again, the, you know, recent critique of systems that have made comparisons with doctors

149
00:09:55,760 --> 00:10:00,360
saying diagnostic systems, which are actually being slightly, you know, quite pulled apart

150
00:10:00,360 --> 00:10:02,640
because of their lack of statistical rigor.

151
00:10:02,640 --> 00:10:06,240
And, you know, it's very good they got to that stage, but actually they're not doing

152
00:10:06,240 --> 00:10:07,240
them very well.

153
00:10:07,240 --> 00:10:11,360
You're not doing to the standard of rigor that one would expect.

154
00:10:11,360 --> 00:10:15,120
And for phase three, I talk about an ultra-alars involved in the diagnostic system.

155
00:10:15,120 --> 00:10:20,080
It's a terrible system, but it actually helped when it was put into practice.

156
00:10:20,080 --> 00:10:23,360
And it's because it wasn't because what the system, the computer was saying, it's because

157
00:10:23,360 --> 00:10:28,600
it just, it changed the culture of data collection and encouraging people to make early diagnoses

158
00:10:28,600 --> 00:10:30,560
and being more confident about their work.

159
00:10:30,560 --> 00:10:34,960
So there's all sorts of unintended ways that systems might benefit, but also unintended

160
00:10:34,960 --> 00:10:38,800
ways in which they might harm.

161
00:10:38,800 --> 00:10:45,960
And so, you know, I went through those applications, but then I went on to this idea of transparency,

162
00:10:45,960 --> 00:10:49,200
which is, you know, is an element of trustworthiness.

163
00:10:49,200 --> 00:10:53,200
And this philosopher, Anora Neal, has got some great things to say about transparency.

164
00:10:53,200 --> 00:10:56,560
She thinks transparency can be really dangerous.

165
00:10:56,560 --> 00:11:02,280
It's not an end in itself, especially in the sense of disclosure in that, you know, you

166
00:11:02,280 --> 00:11:05,760
can be very transparent, and yet nobody can understand what's going on, if you, you

167
00:11:05,760 --> 00:11:08,920
could release the code or something like that, that's very transparent, but people

168
00:11:08,920 --> 00:11:09,920
is hopeless.

169
00:11:09,920 --> 00:11:10,920
Right, hopeless.

170
00:11:10,920 --> 00:11:15,560
So she's really pulled apart transparency and so she's making this appeal for intelligent

171
00:11:15,560 --> 00:11:19,600
openness, which means that any information you give, and this is a really good checklist,

172
00:11:19,600 --> 00:11:23,440
any information you give should be accessible, so people are going to be able to get to it.

173
00:11:23,440 --> 00:11:24,440
It's going to be intelligible.

174
00:11:24,440 --> 00:11:25,960
They're going to understand it.

175
00:11:25,960 --> 00:11:26,960
It's going to be usable.

176
00:11:26,960 --> 00:11:27,960
It's going to meet their needs.

177
00:11:27,960 --> 00:11:31,680
And it's going to be accessible, which means somebody needs to be able to check the working.

178
00:11:31,680 --> 00:11:35,760
Not everybody, but somebody out there, you know, needs to be able to check the working

179
00:11:35,760 --> 00:11:36,760
if necessary.

180
00:11:36,760 --> 00:11:41,360
Now, when you're using deep learning methods, you know, that's really quite a hard challenge

181
00:11:41,360 --> 00:11:42,360
to counter.

182
00:11:42,360 --> 00:11:46,000
I did mention what I thought was some very nice work being done by Google DeepMind,

183
00:11:46,000 --> 00:11:50,400
with in London, with more fields hospital and analysing eye scans, in which they've,

184
00:11:50,400 --> 00:11:55,560
you know, delivered trained this network, so to provide intermediate steps, segmentation

185
00:11:55,560 --> 00:12:00,400
map, so that it doesn't just go straight to a diagnosis, got a probabilistic diagnosis,

186
00:12:00,400 --> 00:12:04,000
but it's actually putting in the intermediate steps, which seems a really cool thing.

187
00:12:04,000 --> 00:12:07,600
Obviously, and that's because that project seems to be very strongly influenced by the

188
00:12:07,600 --> 00:12:08,600
clinicians themselves.

189
00:12:08,600 --> 00:12:09,600
You want that.

190
00:12:09,600 --> 00:12:13,800
That's the way in which they used to thinking about it, and they wanted to map their way

191
00:12:13,800 --> 00:12:14,800
of thinking.

192
00:12:14,800 --> 00:12:19,320
And because many people are claiming now that you don't necessarily have to make a trade

193
00:12:19,320 --> 00:12:25,840
off in performance in order to get a much more interpretable model, that it actually,

194
00:12:25,840 --> 00:12:29,320
you know, there's vast numbers of models, vast numbers of options, it gives very similar

195
00:12:29,320 --> 00:12:33,000
performance, especially, and that, especially, there's actually a lot of the differences

196
00:12:33,000 --> 00:12:36,520
in performance and largely illusory, that was what I talked about earlier.

197
00:12:36,520 --> 00:12:37,520
Okay.

198
00:12:37,520 --> 00:12:43,720
And so, you know, actually, you know, the struggle to, you know, among the great space of

199
00:12:43,720 --> 00:12:50,440
models you can use to choose one that actually enables a much more transparent, much better

200
00:12:50,440 --> 00:12:55,320
explanation, makes it more trustworthy, because people can see the reasoning.

201
00:12:55,320 --> 00:13:02,760
You ran off several of the qualities that's noranil outlined, but they're, they're all

202
00:13:02,760 --> 00:13:08,000
very subjective and, and seem to be in some ways that are with this statistical rigor

203
00:13:08,000 --> 00:13:09,000
that we're talking about.

204
00:13:09,000 --> 00:13:12,880
No, yeah, but no, no, exactly, but that's, I think I, I spent all my time with this

205
00:13:12,880 --> 00:13:13,880
psychologist.

206
00:13:13,880 --> 00:13:14,880
Yes.

207
00:13:14,880 --> 00:13:18,600
So, I've been very influenced by this, and I'm not even going to try to define exactly

208
00:13:18,600 --> 00:13:23,000
what explainable or interpretable or transparent means.

209
00:13:23,000 --> 00:13:29,760
But you can be quite rigorous about your evaluation of some of these aspects.

210
00:13:29,760 --> 00:13:34,440
For example, in the interfaces for the systems we build, we evaluate three things in which

211
00:13:34,440 --> 00:13:38,840
they have an impact on people, they're cognitive, do they understand it, the behavioural, what

212
00:13:38,840 --> 00:13:42,920
does it do to their, their behaviour and their intentions, and the affective, how does

213
00:13:42,920 --> 00:13:46,280
it affect their emotions, and we want to measure all of those, and they can be completely

214
00:13:46,280 --> 00:13:47,280
different.

215
00:13:47,280 --> 00:13:51,120
So, it's very important to get that feeling of what, you know, what do people get from

216
00:13:51,120 --> 00:13:52,120
it?

217
00:13:52,120 --> 00:13:57,680
And for example, through surveys, yeah, yeah, so we, the psychologists would do, you

218
00:13:57,680 --> 00:14:02,080
know, actual direct-to-face-to-face interviews on people, this is the phase two evaluations

219
00:14:02,080 --> 00:14:06,360
within a laboratory, we get patients in, get them to talk through a system, actually

220
00:14:06,360 --> 00:14:10,880
do some eye tracking as well, see how they're using it, using it, and then evaluating

221
00:14:10,880 --> 00:14:12,200
on these things.

222
00:14:12,200 --> 00:14:16,840
The metrics are quite difficult to do, you know, the satisfaction with which a decision

223
00:14:16,840 --> 00:14:20,840
has been made is quite a tricky thing to evaluate, but you need to try to be able

224
00:14:20,840 --> 00:14:21,840
to do it.

225
00:14:21,840 --> 00:14:26,240
You know, these rather loose things, it is worth the effort of trying to measure them

226
00:14:26,240 --> 00:14:27,760
as accurately as possible.

227
00:14:27,760 --> 00:14:32,560
I used as an advocate, an idea of a system we put a front end on could predict, which

228
00:14:32,560 --> 00:14:36,600
is for women merely diagnosed with breast cancer, who are trying to decide what other therapies

229
00:14:36,600 --> 00:14:42,040
to have apart from surgery, which is based on a fairly, you know, basic statistical analysis

230
00:14:42,040 --> 00:14:47,600
of the database of 4,000 cases, and it produces survival curves up to 15 years for women,

231
00:14:47,600 --> 00:14:49,480
and then looks at what would be the effect.

232
00:14:49,480 --> 00:14:54,920
These are personalized to various attributes of the tumor and the woman, and then, you

233
00:14:54,920 --> 00:15:00,840
know, it says how that survival would change if you take particular therapies, and those

234
00:15:00,840 --> 00:15:05,720
the effect of the therapies, all that data is based on clinical trial, causal data from

235
00:15:05,720 --> 00:15:08,120
randomized clinical trials.

236
00:15:08,120 --> 00:15:09,640
And that's fine.

237
00:15:09,640 --> 00:15:14,240
Our idea is that the system, which is currently used by doctors, will be used by doctors

238
00:15:14,240 --> 00:15:18,200
talking to patients, it already, and even by patients themselves and support groups,

239
00:15:18,200 --> 00:15:22,080
but we're using exactly the same system for all these different groups, and that means

240
00:15:22,080 --> 00:15:27,160
having very good explanation facilities, both for the terms, but also ways of portraying

241
00:15:27,160 --> 00:15:29,480
the risks to patients, and this is serious stuff.

242
00:15:29,480 --> 00:15:33,720
This is the chance people are going to be alive in 10 years' time, but with very careful

243
00:15:33,720 --> 00:15:40,640
use of wording and imagery and even the colour and accepting that we can do it.

244
00:15:40,640 --> 00:15:45,040
And the point about this is that for explanation like that, is that two things, one size does

245
00:15:45,040 --> 00:15:46,800
not fit all.

246
00:15:46,800 --> 00:15:50,560
There are people who've got different needs, they've got different levels of understanding

247
00:15:50,560 --> 00:15:52,680
about numbers and graphics.

248
00:15:52,680 --> 00:15:57,120
And so what you need is both multi-layered explanation, with a very simple level at the

249
00:15:57,120 --> 00:16:01,200
top, through to much deeper level, we put the maths in, if you want to, you can see a PDF

250
00:16:01,200 --> 00:16:04,280
with all the maths in, you can get the code if you really want it.

251
00:16:04,280 --> 00:16:08,520
So you've got all those layers of explanation vertically, but also horizontally.

252
00:16:08,520 --> 00:16:13,040
So when we're explaining 15-year survival, we can provide bar charts and survival curves

253
00:16:13,040 --> 00:16:18,480
and icon arrays and tables and text, etc., all of those options, depending on what people

254
00:16:18,480 --> 00:16:20,000
prefer to see.

255
00:16:20,000 --> 00:16:23,640
So you've got both vertical and horizontal explanation choices.

256
00:16:23,640 --> 00:16:28,040
There's no correct way to do it, but you can try to evaluate all of it.

257
00:16:28,040 --> 00:16:31,040
And there are only some people who want to see the stuff at the bottom, but they should

258
00:16:31,040 --> 00:16:35,080
be able to see it, because that's part of the accessible openness.

259
00:16:35,080 --> 00:16:40,360
That example is a compelling one, I find that, you know, often time we're dealing with

260
00:16:40,360 --> 00:16:48,320
physicians, there's a presumption of trust or trustworthiness that, you know, may work

261
00:16:48,320 --> 00:16:52,440
for a lot of people, but sometimes you want a little bit more data, and they're not

262
00:16:52,440 --> 00:16:53,440
always prepared to...

263
00:16:53,440 --> 00:16:54,440
Things are changing.

264
00:16:54,440 --> 00:16:56,000
People are making much, not everybody.

265
00:16:56,000 --> 00:16:57,000
You know, I don't know.

266
00:16:57,000 --> 00:16:58,880
I just completely have them off top of my head.

267
00:16:58,880 --> 00:17:03,280
I'd say half people who are quite prepared still go along with a very paternalist point

268
00:17:03,280 --> 00:17:04,280
of view.

269
00:17:04,280 --> 00:17:05,280
You know, thank you very much.

270
00:17:05,280 --> 00:17:06,280
Tell me what to do.

271
00:17:06,280 --> 00:17:07,280
Right.

272
00:17:07,280 --> 00:17:08,280
Just tell me what to do.

273
00:17:08,280 --> 00:17:09,280
I don't want to know anything else.

274
00:17:09,280 --> 00:17:14,240
You know, are asking questions and actually wanting to exercise some of their own rights.

275
00:17:14,240 --> 00:17:15,240
I don't know.

276
00:17:15,240 --> 00:17:19,320
I've got friends who have used the system that we've been working on in order to challenge

277
00:17:19,320 --> 00:17:21,320
their doctors and saying, okay, I'm not...

278
00:17:21,320 --> 00:17:22,320
It's only a tiny benefit.

279
00:17:22,320 --> 00:17:24,320
I know I'm going to get terrible side effects.

280
00:17:24,320 --> 00:17:25,320
I'm not going to have it.

281
00:17:25,320 --> 00:17:26,320
Yeah.

282
00:17:26,320 --> 00:17:29,080
And they've using that to challenge us and power them and power them.

283
00:17:29,080 --> 00:17:30,080
I think this is very valuable.

284
00:17:30,080 --> 00:17:36,640
Not only that, but in the UK now, there's a much stronger legal structure on what must

285
00:17:36,640 --> 00:17:41,440
be explained to people in order to get informed consent for treatment, and in others, all

286
00:17:41,440 --> 00:17:43,960
this should be explained to people.

287
00:17:43,960 --> 00:17:48,040
So we're providing it as actually some of the tools to allow doctors to carry out their

288
00:17:48,040 --> 00:17:49,040
work better.

289
00:17:49,040 --> 00:17:50,040
Okay.

290
00:17:50,040 --> 00:18:00,040
There's a thread in the AI community around taking ideas from adjacent fields like electrical

291
00:18:00,040 --> 00:18:05,920
engineering, the idea of data sheets or model cards, some folks have called them and

292
00:18:05,920 --> 00:18:11,640
basically different ways of documenting the characteristics or biases of different

293
00:18:11,640 --> 00:18:14,080
AI data sets or systems.

294
00:18:14,080 --> 00:18:20,720
And it sounds like a part of what you're doing is a similar idea, but applying ideas from

295
00:18:20,720 --> 00:18:26,720
clinical trials and the statistical methods associated with clinical trials and medical

296
00:18:26,720 --> 00:18:33,440
and pharmaceutical field to the way we talk about and communicate around AI systems and

297
00:18:33,440 --> 00:18:34,440
machine learning models.

298
00:18:34,440 --> 00:18:39,200
And yes, and not just the pharmaceutical area, but there's been involved for years in

299
00:18:39,200 --> 00:18:45,520
building prognostic systems for people and then both evaluating them and putting them

300
00:18:45,520 --> 00:18:46,520
into practice.

301
00:18:46,520 --> 00:18:50,320
And one of the crucial things about the evaluation that people would get really obsessed about

302
00:18:50,320 --> 00:18:55,480
in a sort of pedantic way that statisticians tend to operate in is that the probabilities

303
00:18:55,480 --> 00:18:56,720
must be meaningful.

304
00:18:56,720 --> 00:19:02,600
If you say 70% probability for something or 70 out of 100 chance, then it's got a meaning

305
00:19:02,600 --> 00:19:07,680
that 70 out of the number of times you say that, it should happen in 70% at the time.

306
00:19:07,680 --> 00:19:12,480
The undercalibrated probabilities, in other words, the uncertainty, the accuracy of the

307
00:19:12,480 --> 00:19:19,520
uncertainty, it is important as the accuracy of the main number, it is a very statistical

308
00:19:19,520 --> 00:19:25,000
idea and yet it's very important because otherwise you get these grossly over-confident things

309
00:19:25,000 --> 00:19:30,600
like, oh, I'm 99% sure that this is the diagnosis, that is grossly misleading, that really

310
00:19:30,600 --> 00:19:31,840
is terrible.

311
00:19:31,840 --> 00:19:37,680
So that's, I think, another very important way, thing that can be brought from statistics,

312
00:19:37,680 --> 00:19:42,400
which has worked a lot on how to evaluate the calibration of probabilities or test statistics

313
00:19:42,400 --> 00:19:48,240
to use and so on in order to check that element of trustworthiness of the claim.

314
00:19:48,240 --> 00:19:49,240
Right, right.

315
00:19:49,240 --> 00:19:53,800
This all calls to mind, at least in the US, I don't know if it's somewhere in the UK,

316
00:19:53,800 --> 00:19:58,360
when you're advertising pharmaceuticals, there's like you have your 30 minute ad and then

317
00:19:58,360 --> 00:20:02,480
you're long, that's red.

318
00:20:02,480 --> 00:20:03,480
Exactly.

319
00:20:03,480 --> 00:20:06,280
Well, you shouldn't have to do that.

320
00:20:06,280 --> 00:20:08,080
That's just some, you know, regulation.

321
00:20:08,080 --> 00:20:09,320
What you want should, we know.

322
00:20:09,320 --> 00:20:13,160
But at the same time, you know, that's kind of a summary of a data sheet.

323
00:20:13,160 --> 00:20:16,200
Yeah, that's not trustworthy communication.

324
00:20:16,200 --> 00:20:20,920
That's like having to sign, you know, we're getting some software and those terms and conditions,

325
00:20:20,920 --> 00:20:23,080
16 pages of terms and conditions.

326
00:20:23,080 --> 00:20:24,080
Right.

327
00:20:24,080 --> 00:20:26,160
Yes, that is not intelligent openness.

328
00:20:26,160 --> 00:20:33,880
No way is that accessible, usable, incomprehensible, accessible, is break breaks every rule.

329
00:20:33,880 --> 00:20:36,160
And it's terrible at sort of communication.

330
00:20:36,160 --> 00:20:37,160
Right.

331
00:20:37,160 --> 00:20:43,200
It's there to obey a law, but it is a complete sham in terms of good communication.

332
00:20:43,200 --> 00:20:44,200
I agree.

333
00:20:44,200 --> 00:20:45,200
I agree.

334
00:20:45,200 --> 00:20:51,160
At the same time, it is one step better than, you know, yes, or no, the computers, yes,

335
00:20:51,160 --> 00:20:52,480
yes, the computers exactly.

336
00:20:52,480 --> 00:20:53,480
Exactly.

337
00:20:53,480 --> 00:21:00,640
And of course, the worst systems of all are proprietary systems that are used in courts

338
00:21:00,640 --> 00:21:04,960
to decide about recidivism, risk, or bail, right, like that, are shocking because they're

339
00:21:04,960 --> 00:21:05,960
proprietary.

340
00:21:05,960 --> 00:21:06,960
They're totally done transparent.

341
00:21:06,960 --> 00:21:09,760
You've got no idea what information is being used in them.

342
00:21:09,760 --> 00:21:15,200
I mean, that's absolutely shocking again, that breaks every rule, you know, everything

343
00:21:15,200 --> 00:21:19,240
I'm trying to, I'm talking about is broken by that kind of system.

344
00:21:19,240 --> 00:21:21,560
So key takeaways from your talk.

345
00:21:21,560 --> 00:21:22,560
Oh.

346
00:21:22,560 --> 00:21:23,560
Yeah.

347
00:21:23,560 --> 00:21:27,280
Well, that's basically basic statistical ideas, you know, and their experience and other

348
00:21:27,280 --> 00:21:28,280
words.

349
00:21:28,280 --> 00:21:29,280
I've got a lot to offer.

350
00:21:29,280 --> 00:21:31,480
A lot to offer.

351
00:21:31,480 --> 00:21:35,280
But also, I can't, you know, I'm not just taking ideas from, you know, from statistics.

352
00:21:35,280 --> 00:21:40,680
I'm taking ideas from philosophy and psychology and empirical testing and things that really

353
00:21:40,680 --> 00:21:45,160
in this maturing disciplines, unbelievably important discipline, I think could take

354
00:21:45,160 --> 00:21:47,040
a lot more account off.

355
00:21:47,040 --> 00:21:48,040
Great.

356
00:21:48,040 --> 00:21:53,600
Very much in line with some of the key themes that I'm hearing at this year's Neurobs,

357
00:21:53,600 --> 00:21:55,160
you know, it's, in fact, two of them.

358
00:21:55,160 --> 00:22:01,000
One is the importance of, you know, fairness, transparency, et cetera.

359
00:22:01,000 --> 00:22:05,280
And the other is kind of the importance of interdisciplinary approaches and you're kind

360
00:22:05,280 --> 00:22:07,400
of bringing those right to you with that.

361
00:22:07,400 --> 00:22:10,880
And there's some wonderful work going on, you know, this morning I really featured the

362
00:22:10,880 --> 00:22:16,080
FATML, you know, social impact statements, you know, lists that they've got partly because

363
00:22:16,080 --> 00:22:19,200
they do not identify transparency as an objective.

364
00:22:19,200 --> 00:22:24,360
They've, they've learned themselves that transparency just means to an end, you know, it's,

365
00:22:24,360 --> 00:22:29,440
it's no good just being transparent, unless you obey a Neel's ideas of what transparency

366
00:22:29,440 --> 00:22:30,440
means.

367
00:22:30,440 --> 00:22:37,320
Well, we'll definitely provide a pointer to Nora Neel and your work as well, of course.

368
00:22:37,320 --> 00:22:38,320
Yeah.

369
00:22:38,320 --> 00:22:40,120
The talk's up on Facebook as well, it just looks to.

370
00:22:40,120 --> 00:22:41,120
Oh, fantastic.

371
00:22:41,120 --> 00:22:42,120
Fantastic.

372
00:22:42,120 --> 00:22:44,520
Well, David, thank you so much for taking the time.

373
00:22:44,520 --> 00:22:48,160
No, the real pleasure, real pleasure, thank you very much for asking me.

374
00:22:48,160 --> 00:22:53,760
All right, everyone, that's our show for today.

375
00:22:53,760 --> 00:22:59,840
For more information on David or any of the topics covered in this episode, visit twimmelai.com

376
00:22:59,840 --> 00:23:02,440
slash talk slash 212.

377
00:23:02,440 --> 00:23:09,320
You can also follow along with our NURP series at twimmelai.com slash NURP's 2018.

378
00:23:09,320 --> 00:23:16,320
As always, thanks so much for listening and catch you next time.

