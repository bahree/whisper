1
00:00:00,000 --> 00:00:21,800
So, other one, it's another full stack, the learning static group, and we have more

2
00:00:21,800 --> 00:00:28,040
presentations for the day, so we have, honestly, we're ready.

3
00:00:28,040 --> 00:00:37,560
We have Avvynier's presentation on the model inter-operability and overview, and also

4
00:00:37,560 --> 00:00:50,360
Sanyam's and Akash's presentation on some OPPOSOR still, JVN, IO, so it's cool.

5
00:00:50,360 --> 00:00:54,080
Let me open my chat so I can also see what's in it.

6
00:00:54,080 --> 00:00:59,320
I'll keep an eye on you in the chat, okay, that's great, thank you.

7
00:00:59,320 --> 00:01:03,040
So let's maybe start with the, so we have about half an hour for each presentation, I think

8
00:01:03,040 --> 00:01:04,840
that should be fine.

9
00:01:04,840 --> 00:01:14,800
So let's maybe start with the full stack deep learning thing, so for today we had quite

10
00:01:14,800 --> 00:01:22,000
challenging agenda, a couple of lessons, 7, 8, 9, 10, 11, but actually 7, let me show

11
00:01:22,000 --> 00:01:33,640
a look, so 7, yeah, so 7 is about the machine learning teams, and that was about half an

12
00:01:33,640 --> 00:01:38,880
hour of video, less than 8, it's not available, so that's easy.

13
00:01:38,880 --> 00:01:44,080
Less than 9 was optional, because that's a guest speaker and that's about 45 minutes

14
00:01:44,080 --> 00:01:45,080
of video.

15
00:01:45,080 --> 00:01:52,200
And then less than 10, it's about troubleshooting, that's about an hour and a half.

16
00:01:52,200 --> 00:01:58,520
And then we said we're going to try to attempt the lesson 11, which is labs, but we said

17
00:01:58,520 --> 00:02:04,200
not all of them, but 6 and, 6 and 7.

18
00:02:04,200 --> 00:02:14,040
So as you remember, labs were in the Geek app somewhere, so the labs were kind of

19
00:02:14,040 --> 00:02:19,920
splitting the four sessions, session 1, 2, so the third session is actually the lesson

20
00:02:19,920 --> 00:02:34,760
7 and 8, that was the plan anyway, so we can start, actually, so I can open slides,

21
00:02:34,760 --> 00:02:57,320
I'm not going to go through all the slides as last time, because there are too many of

22
00:02:57,320 --> 00:03:04,000
them, and I'm just going maybe to scroll, just ask questions or make some comments,

23
00:03:04,000 --> 00:03:11,200
or anything related to the course, what was interesting, what was fun, what was not clear,

24
00:03:11,200 --> 00:03:16,080
would try to talk about this.

25
00:03:16,080 --> 00:03:25,520
Yeah, so he was talking about how many companies are struggling to hire AI experts.

26
00:03:25,520 --> 00:03:33,920
I believe that was the, yeah, that was what he was talking about, so it's a good thing

27
00:03:33,920 --> 00:03:38,440
for people looking for jobs, but at the same time, I think it's not easy to find a job

28
00:03:38,440 --> 00:03:46,160
without experience, that's another kind of comment that is there around machine learning,

29
00:03:46,160 --> 00:03:47,480
finding jobs.

30
00:03:47,480 --> 00:03:56,720
You guys have some experience about finding jobs, so looking for people, for experts in AI.

31
00:03:56,720 --> 00:03:59,160
I have experience of getting rejected from Google.

32
00:03:59,160 --> 00:04:06,160
Oh, yeah, I kind of converse up from one point, people say that's a lot of, is it a big

33
00:04:06,160 --> 00:04:11,480
demand for AI experts, on the other hand, yeah, I also heard that, and I kind of seen

34
00:04:11,480 --> 00:04:19,240
my couple of cities themselves, it's, yeah, it's interesting.

35
00:04:19,240 --> 00:04:30,720
Yeah, so there are different roles, so in machine learning, data science, AI, that's

36
00:04:30,720 --> 00:04:38,000
I think there are a couple of roles we could split that into, so like DevOps, could be

37
00:04:38,000 --> 00:04:47,600
one role, so people who make the deployment of the machine learning models, the data engineers

38
00:04:47,600 --> 00:04:55,360
kind of might be a bit similar, but maybe more on a data, so they keep the look after

39
00:04:55,360 --> 00:05:01,520
the databases and stuff like that, then the machine learning engineer and other similar

40
00:05:01,520 --> 00:05:06,760
kind of like we have machine learning researcher and machine learning engineer, so that's kind

41
00:05:06,760 --> 00:05:13,880
of very similar, but different again, machine learning engineer is more about software, software

42
00:05:13,880 --> 00:05:19,840
engineering site and then machine learning researcher is more about the deep learning or machine

43
00:05:19,840 --> 00:05:26,480
learning algorithms models, making them better, and then the data scientist likes is like

44
00:05:26,480 --> 00:05:39,720
everything else, it's like very broad topic, yeah, I think that's, that's, yeah, so again,

45
00:05:39,720 --> 00:05:48,640
the DevOps, DevOps, data engineering, yeah, building data pipelines, machine learning engineer

46
00:05:48,640 --> 00:05:54,160
or trying to deploy, research to try and better models, yeah, data science is everything

47
00:05:54,160 --> 00:05:58,720
else, so data scientists could be someone who works with Excel, it could be someone who

48
00:05:58,720 --> 00:06:03,680
works with TensorFlow, yeah, it's just to check the description of the job, I think that's

49
00:06:03,680 --> 00:06:08,760
what it's trying to sell as well, I think it's also worth mentioning that many startups

50
00:06:08,760 --> 00:06:14,080
especially use these terms very interchangeably, so someone might be hiring an ML engineer

51
00:06:14,080 --> 00:06:18,440
but they might pitch it as a data scientist position, I've seen that for quite a bit of

52
00:06:18,440 --> 00:06:40,160
course, yeah, yeah, and I've seen a lot of, like, if you just type, when you try to open

53
00:06:40,160 --> 00:06:52,360
the machine learning, and just, yeah, those type of kind of, you see those allow this kind

54
00:06:52,360 --> 00:06:58,080
of like, what's data science, what's computer science, what's math, and every time, every

55
00:06:58,080 --> 00:07:02,520
time you look at it, they are different, so there is no like common understanding, some

56
00:07:02,520 --> 00:07:09,400
of them like very interested in complex, some of them are more like basic, like, like

57
00:07:09,400 --> 00:07:14,680
the data scientist is someone who knows how to work with software, who know math, and

58
00:07:14,680 --> 00:07:19,520
who is good at data communication, and that's the true data scientist, but then, again,

59
00:07:19,520 --> 00:07:28,280
I've sold so many of those, and there is just not common understanding of what that is,

60
00:07:28,280 --> 00:07:34,640
it's, yeah, so this one's different, this one's got data science on the right side,

61
00:07:34,640 --> 00:07:43,320
and IT skills, business skills, there's just no, yeah, this one, yeah, this one's like

62
00:07:43,320 --> 00:07:50,040
again, data science is like, I'm growth term, and then you have the AI, very growth field,

63
00:07:50,040 --> 00:07:55,560
and then machine learning, and then deep learning as a subset of machine learning, so, yeah,

64
00:07:55,560 --> 00:08:01,720
I think this is fun, but we just need to check the job description, and some companies

65
00:08:01,720 --> 00:08:08,360
maybe has got more specific descriptions, some companies who have like more growth,

66
00:08:08,360 --> 00:08:12,320
maybe they don't know themselves, or they don't know what they want to do, so just someone

67
00:08:12,320 --> 00:08:17,120
is mentioning the chat, that they found the working of data kind was a great deal to

68
00:08:17,120 --> 00:08:22,480
recently, but like popular projects, then what to talk about, and also very little, it's

69
00:08:22,480 --> 00:08:31,480
difficult to join the team, even as a volunteer, yeah, yeah, data kind, I'm afraid about

70
00:08:31,480 --> 00:08:41,360
this, so, it's not, I'm not sure, I think it's an NGO, only a few are supposed to, yeah,

71
00:08:41,360 --> 00:08:48,240
it's a non-profit, and they basically data scientists and developers work for free,

72
00:08:48,240 --> 00:09:00,000
and they have a small, small team of project managers, and that basically organize work,

73
00:09:00,000 --> 00:09:10,600
I think on this slack, I saw there was a call for volunteers to help judgegoogle.org challenge,

74
00:09:10,600 --> 00:09:15,760
I think that was a great experience, I think a few people from here were working there,

75
00:09:15,760 --> 00:09:24,040
but after that I started working with a local data kind in Washington, DC, and then once

76
00:09:24,040 --> 00:09:29,520
I started working on the project, because they're kind of open, it's great that you can

77
00:09:29,520 --> 00:09:35,000
talk about the project, and the data kind has a good reputation, so it was, it was a good

78
00:09:35,000 --> 00:09:43,440
way of, you know, presenting skills. So you can apply for like, yeah, they, what I found

79
00:09:43,440 --> 00:09:50,600
is, I registered online with them, but I never got a call for any project through registration

80
00:09:50,600 --> 00:10:02,280
online, but because I was on two slacks, one for Google ML challenge, and then it was

81
00:10:02,280 --> 00:10:09,040
for Washington, DC chapter, periodically they post different projects, and the skill

82
00:10:09,040 --> 00:10:21,200
match, you know, I think it was cool to accept it. Oh yeah, nice one, okay, I'm going

83
00:10:21,200 --> 00:10:23,120
to do it now, but, and the other one you mentioned, there are a couple of different places,

84
00:10:23,120 --> 00:10:27,360
there was a slack, you saw it? Oh yeah, so basically they have, they have a bunch of local

85
00:10:27,360 --> 00:10:37,280
chapters, data kind of local chapters, okay. Yeah. Okay, yeah, the other one that was

86
00:10:37,280 --> 00:10:44,880
hard about this was the, I think the Havan in India, I said, I'm not sure, maybe in Bangalore.

87
00:10:44,880 --> 00:10:49,600
Yeah, I think the one in Bangalore, yeah. Yeah, I think they had something, let's, let's

88
00:10:49,600 --> 00:10:58,200
have a look. Yeah, UK, Bangalore, India, Washington, DC, Singapore, San Francisco, yeah.

89
00:10:58,200 --> 00:11:05,480
So, yeah, so my, my advice would be, I mean, just register, like, didn't have, but like

90
00:11:05,480 --> 00:11:10,920
because I started, I was able to start on one project, then getting on slack, like,

91
00:11:10,920 --> 00:11:17,320
then people start sharing opportunities. Yeah, that was nice, that was useful. Can you

92
00:11:17,320 --> 00:11:25,880
do that remotely? Yeah, so local chapters, they usually, they want local people, but a lot

93
00:11:25,880 --> 00:11:33,640
of times they post remote projects. Okay, nice. If I, if I hear, if anybody hears about

94
00:11:33,640 --> 00:11:39,800
another call for volunteers, like outpost, yeah. That would be great. Another one I've heard

95
00:11:39,800 --> 00:11:52,920
was the fellowship. Yeah. I don't see this website working. I've also heard about inside

96
00:11:52,920 --> 00:12:02,360
fellows program. I think it's it's along similar lines. So I have some experience with inside

97
00:12:02,360 --> 00:12:10,520
fellow fellowship. I actually got accepted there to start last January, but I decided against

98
00:12:10,520 --> 00:12:17,640
joining just at work. They had, like, they had a lot of interesting projects starting and I was

99
00:12:17,640 --> 00:12:23,880
basically could do the same at work and, you know, get paid. But inside, it's an interesting,

100
00:12:23,880 --> 00:12:29,960
it's basically, they don't have any regular classes, but you do research like with them,

101
00:12:29,960 --> 00:12:37,080
and they don't, they sort of like lightly kind of supervise you, but they help you, they help

102
00:12:37,080 --> 00:12:44,200
you sell yourself. And then they, and then they work as, as you, as a job placement, basically,

103
00:12:44,200 --> 00:12:49,960
they get like some, some money from the company if you get once you get hired. So it's completely

104
00:12:49,960 --> 00:12:56,760
free. Interesting. They have two programs like that. So one is fast AI, not fast, one is inside

105
00:12:56,760 --> 00:13:08,920
fellowship, and another is data, the data incubator. But inside, I actually see a lot of people,

106
00:13:09,960 --> 00:13:15,000
a lot of people went through, especially coming from PhD, a lot of people went through that program,

107
00:13:15,000 --> 00:13:21,160
like I see people who are working. Actually, a lot of people who some interviewed went through

108
00:13:21,160 --> 00:13:32,040
inside. Okay. Yeah, interesting. So they also fellowship, guys, I guess something similar.

109
00:13:32,600 --> 00:13:38,760
I think you have to apply for them and complete some sort of a challenge project to get accepted.

110
00:13:39,800 --> 00:13:45,960
And then you kind of work with them for free. Again, it's not paid, but I guess you can learn

111
00:13:45,960 --> 00:13:53,160
for like this September, October, like four months. And after that, and they have a lot of

112
00:13:53,960 --> 00:13:59,880
different locations around the world. So because they don't accept remotes, so we have to be

113
00:13:59,880 --> 00:14:05,880
in one of those positions. And then I think they help you to find a job or something like that. So

114
00:14:05,880 --> 00:14:17,000
that it's again, for four months, full time, do something for them and learn a lot, but yeah. Okay, nice.

115
00:14:22,520 --> 00:14:27,480
The other side of balance, through some scenarios, through Slack, was kind of Delta helpers.

116
00:14:27,480 --> 00:14:34,600
So it's not to find a job or a staff, but there's a lot of people who sit on Twitter. I'm okay to help

117
00:14:34,600 --> 00:14:44,760
with questions and stuff. And at least it's quite long. So I guess that's also a second way to kind

118
00:14:44,760 --> 00:14:51,800
of ask people questions, how to get into data science or obviously a data scientist's work

119
00:14:51,800 --> 00:15:06,520
at a day-to-day on its show. I don't know. Okay. Interesting. We've not seen you screened for sharing it.

120
00:15:13,000 --> 00:15:19,560
So I was showing the data helpers side. Data helpers.org. So there's like someone

121
00:15:19,560 --> 00:15:24,840
asking right now. Could you see? Yeah, good. Someone said on Twitter, hey, can you help

122
00:15:24,840 --> 00:15:28,920
put some questions in their science? And there's a lot of people said, yeah, I'm happy to help,

123
00:15:28,920 --> 00:15:35,640
since the list is quite long. People that said yes. And yeah, some of the research.

124
00:15:37,800 --> 00:15:40,920
We're asking some people about data science.

125
00:15:40,920 --> 00:15:53,320
Yeah. So, yeah. So they're not talking about skills. So like machine learning, skills wise,

126
00:15:53,320 --> 00:15:58,200
and software engineering. What do you need to know to kind of work in such a domain?

127
00:16:01,960 --> 00:16:09,400
So like they develop more mostly software engineering, data engineering is software engineering

128
00:16:09,400 --> 00:16:13,800
with some ML experience. And of course, the machine learning engineering.

129
00:16:14,600 --> 00:16:17,480
So again, machine learning class and software engineering skills.

130
00:16:18,520 --> 00:16:23,960
Machine learning research is something that Jeremy Howard teaches us what to do. So that's mostly

131
00:16:23,960 --> 00:16:29,240
the fast AI courses, I guess. And the data scientists have to be everything.

132
00:16:32,280 --> 00:16:38,920
And then they were talking about teams. And because they did some interviews about

133
00:16:38,920 --> 00:16:43,080
some companies about how to structure a team, but they didn't find any consensus anywhere to

134
00:16:43,080 --> 00:16:48,920
do it. So they're like everything is different. And they do it in different ways.

135
00:16:58,200 --> 00:17:02,040
Yeah, I think that was quite interesting. And on the guest lecture, the

136
00:17:02,040 --> 00:17:08,440
I forgot the person who was talking about the founder of the Sweden biases.

137
00:17:09,480 --> 00:17:14,680
They found actually that the improvements of the accuracy of the model

138
00:17:15,480 --> 00:17:20,120
is mostly coming from the first like two weeks of working in the model. After that,

139
00:17:20,120 --> 00:17:24,920
you're not going to get a lot better. So I thought that was quite interesting. And that's

140
00:17:24,920 --> 00:17:38,440
similar from the other alukas. Alukas was talking about this as well. And there was no slides for that.

141
00:17:40,040 --> 00:17:46,040
But he showed the same if I can find it. Yeah, I think that's exactly the same graphs.

142
00:17:46,040 --> 00:17:55,160
And that's maybe the bar 20. Okay, so I'm going to go through that. So what I found is that

143
00:17:56,440 --> 00:18:00,280
I think that's the one in the middle. The accuracy of the best model

144
00:18:01,720 --> 00:18:07,880
goes up a lot in the first like two weeks or even that's even like two days. And after that,

145
00:18:08,520 --> 00:18:13,960
it doesn't help. Even like I think the one on the right is like number of participating teams

146
00:18:13,960 --> 00:18:18,840
like on the Kaggle. It increases a lot, but the improvement is not getting better.

147
00:18:19,720 --> 00:18:26,120
So this is kind of like the sweet spot between accuracy and how much time you want to spend.

148
00:18:27,080 --> 00:18:29,640
I think they wanted to highlight that. I found it quite interesting.

149
00:18:31,240 --> 00:18:38,040
And of course in Kaggle, you're going to win. Even if it was just like 0.001%,

150
00:18:38,040 --> 00:18:43,960
you're in normal projects. It's not that important to get such an accuracy.

151
00:18:47,240 --> 00:18:52,200
As far as the managing ML teams, like any team, just kind of challenging the ML teams.

152
00:18:52,920 --> 00:18:53,960
Different I guess.

153
00:18:59,240 --> 00:19:04,680
Yeah, they're hiring. We're talking about more hiring like LinkedIn. And we talked about some other

154
00:19:04,680 --> 00:19:18,600
also nice recruiting that conferences this course. Like they talked about interviews a little bit.

155
00:19:19,400 --> 00:19:26,120
Some people still do the whiteboard. Some people ask it to do like part programming.

156
00:19:26,120 --> 00:19:35,480
They would give you some quizzes. I think what's quite interesting, I think, makes most sense these

157
00:19:35,480 --> 00:19:45,320
days. This for me is like to take home ML projects. Although this can be a lot of work from you.

158
00:19:46,120 --> 00:19:53,080
But at least it's kind of more like a more real life project. Because if you take that project home,

159
00:19:53,080 --> 00:19:59,080
you can Google, you can even find help from other people. This is how this works. Even if you work

160
00:19:59,080 --> 00:20:05,880
at the company, you're not going to do stuff on the whiteboard. You're going to use all the resources

161
00:20:05,880 --> 00:20:12,200
you can to solve that project. For me, that kind of like makes sense. But it's going to take

162
00:20:12,200 --> 00:20:17,320
a lot of time from you. It's like a week or maybe even more sometimes. So that depends.

163
00:20:17,320 --> 00:20:23,560
They were talking about how to prepare for interview.

164
00:20:30,440 --> 00:20:34,920
I think they were joking there's eggs on maybe the walls. I don't know. How many of the walls?

165
00:20:42,120 --> 00:20:46,200
Oh, there are some questions. I was watching those ideas some time ago for a

166
00:20:46,200 --> 00:20:53,160
god about that. Do you want to answer those questions? Why does there is no outblock in the

167
00:20:53,160 --> 00:21:00,520
ResNet architecture help with the vanishing gradient problem? Well, I think because the

168
00:21:00,520 --> 00:21:16,360
image of that. Oh, yeah. We can find some some ResNet to the ResNet.

169
00:21:18,760 --> 00:21:25,240
Yeah, I think that's the ResNet stuff. So I think in ResNet, we get this kind of blocks.

170
00:21:25,240 --> 00:21:32,360
Yeah, so compared to the kind of like a normal 34 layer plan layer network, we have those.

171
00:21:33,240 --> 00:21:39,160
They call the ResNet blocks when they have the skip connections. So they go through the

172
00:21:39,160 --> 00:21:48,760
convolution layers and then they get added the same signals signal as it goes into the layers.

173
00:21:48,760 --> 00:21:52,200
But without going through the layers, the kind of called the skip connection.

174
00:21:52,200 --> 00:21:58,600
So I believe those skip connections, they help you to avoid the vanishing gradient problem.

175
00:22:00,440 --> 00:22:02,280
That would be my answer to that question at least.

176
00:22:07,080 --> 00:22:10,200
I also have the following learning curve from the training on single batch,

177
00:22:11,000 --> 00:22:13,640
which in which of the following could be the course.

178
00:22:13,640 --> 00:22:24,440
And this is, oh, this is the error. Okay. So the error is going up and down, but it's still roughly

179
00:22:24,440 --> 00:22:29,080
about 50%. It's not improving. Sometimes it's improving. Sometimes you're getting worse.

180
00:22:30,120 --> 00:22:31,400
So what could be the problem? But,

181
00:22:32,920 --> 00:22:36,520
cheerful flybals, learning rate too low, learning rate too high,

182
00:22:36,520 --> 00:22:45,320
the American stability too big, too big a model. That's interesting. We have any idea?

183
00:22:47,960 --> 00:22:48,760
I'm not so sure.

184
00:22:55,720 --> 00:22:59,480
The learning rate is too low. Yeah.

185
00:22:59,480 --> 00:23:09,480
Maybe just if the learning rate is too low, you would be going like slowly to slow to your

186
00:23:11,000 --> 00:23:15,880
minimum. But in this case, with iterations, we're not really going

187
00:23:17,240 --> 00:23:21,160
for lower error rate. With the two high learning rate, they'll be going

188
00:23:21,160 --> 00:23:28,440
maybe quicker to the minimum, but then you will go, I think, to some higher values.

189
00:23:30,680 --> 00:23:36,360
The shuffle flybals. What do I mean by shuffle flybals? I'm not so sure. I'm with like a mixed

190
00:23:36,360 --> 00:23:46,440
flybals. I don't know. It'll be more than I don't think so. Maybe it's a learning rate, I think.

191
00:23:46,440 --> 00:23:55,640
Question number three. For each of the following prediction tasks,

192
00:23:55,640 --> 00:23:59,960
select the loss function that is best suited for. So the predict sale price of a house listed

193
00:23:59,960 --> 00:24:07,560
for a sale. And then we'll have four, five answers. Okay. That's also a nice one.

194
00:24:07,560 --> 00:24:16,680
So we have the mean squared error, we have categorical cross entropy, binary cross entropy,

195
00:24:16,680 --> 00:24:21,480
CTC loss and gannels. Okay, maybe I need to go through that slacker again.

196
00:24:26,760 --> 00:24:31,960
That was basically the lecture seven,

197
00:24:31,960 --> 00:24:41,400
lecture eight, and then lecture nine. It was a thing was interesting, but I'm going to treat

198
00:24:41,400 --> 00:24:50,760
this as the optional lecture. And then lecture nine, lecture eight was about troubleshooting.

199
00:24:50,760 --> 00:25:04,920
I don't have a lot of time, but I like this picture. So you throw like a lot of data in the model,

200
00:25:04,920 --> 00:25:10,600
I just trust, yeah. And you just trust that you're going to get right output. And if it's not

201
00:25:10,600 --> 00:25:18,440
right, just draw more data or just mix whatever the data will be. So it's interesting.

202
00:25:18,440 --> 00:25:20,440
Yeah.

203
00:25:28,680 --> 00:25:32,840
Yes, I again, a lot of times going to take for you to kind of debug your model.

204
00:25:38,680 --> 00:25:39,560
So there are a couple of,

205
00:25:39,560 --> 00:25:49,400
I'm just going quickly because just ask questions or comments from that lecture because we don't

206
00:25:49,400 --> 00:25:57,880
have that much time to go in detail. Actually, I think that was the, this is the answers for those

207
00:25:57,880 --> 00:26:03,000
questions, because this is the same graph, right? And they're talking here about the labels side

208
00:26:03,000 --> 00:26:09,560
of us. I guess the result is like this, that's going to be because of the shuffle labels.

209
00:26:10,520 --> 00:26:12,680
That must be the case. Okay, interesting.

210
00:26:16,120 --> 00:26:23,080
Yeah, they were talking how to find out if your model is performing poorly. And I like the idea,

211
00:26:25,160 --> 00:26:29,640
yeah, I think this is quite interesting graph. So if you have your learning rates set right,

212
00:26:29,640 --> 00:26:35,480
do your minimum. But if you learn your rate is too high, you're going to get quicker to your

213
00:26:35,480 --> 00:26:41,800
minimum, but it will not get as low as it should. But it's too high learning rate. It will go

214
00:26:43,800 --> 00:26:49,000
up, but if the learning rate is too low, it will get to low point, but it's going to be slow.

215
00:26:51,480 --> 00:26:54,120
But I think it was interesting for me was,

216
00:26:54,120 --> 00:27:07,080
was that like they said, we should start. Yeah, and this slide was quite interesting. So the

217
00:27:07,080 --> 00:27:14,600
common data set construction issues, so we have not enough data. So deep learning models, they need

218
00:27:14,600 --> 00:27:19,800
quite some data. So maybe if you can use transfer learning, maybe not that many data points,

219
00:27:19,800 --> 00:27:25,800
but if you start training from scratch, then you need quite some data. The class imbalances,

220
00:27:25,800 --> 00:27:31,160
so say if one class got like 500 images, another class got just one couple of images,

221
00:27:31,160 --> 00:27:37,880
that's going to be an issue. The noisy label. So I guess this is when your labels not necessarily

222
00:27:37,880 --> 00:27:43,640
are true for the picture. So say if a picture of a car and then on a picture, there's no car.

223
00:27:43,640 --> 00:27:49,960
And then when you're training or your test set is coming from different distributions.

224
00:27:49,960 --> 00:27:56,040
So I think he was given an example when you're training on a car like in an autonomous vehicle

225
00:27:56,040 --> 00:28:00,840
problem, when you're training pictures from like day pictures and then a test are like

226
00:28:00,840 --> 00:28:09,480
two degrees. So that could be a complete different start. Yeah, and it's difficult to

227
00:28:09,480 --> 00:28:13,800
troubleshoot because you might have issues with your data, you might have issues with your code,

228
00:28:13,800 --> 00:28:19,880
you might have issues with the math behind it. So it's not so easy. So what they

229
00:28:21,880 --> 00:28:27,240
what they recommend, and that's actually quite true, just start simple and then gradually

230
00:28:30,440 --> 00:28:35,480
and that's I was thinking about because what we what we've learned on the first day,

231
00:28:35,480 --> 00:28:43,400
we've learned is already quite quite complex models because by default Jeremy uses this like one

232
00:28:45,560 --> 00:28:51,400
one cycle policy, there's a lot of like the dropouts and weight decays and all that stuff.

233
00:28:52,440 --> 00:29:00,040
So I was thinking about that and maybe maybe the next step I'd like to to find out the simplest

234
00:29:00,040 --> 00:29:06,920
model and just to try and a very simple model. But the fast AI works and maybe fast AI is also

235
00:29:06,920 --> 00:29:16,840
a good start. Yeah, so that's that's kind of nice flow chart. Start simple, implement and debug,

236
00:29:16,840 --> 00:29:23,080
evaluate the model, choose hyper parameters, improve your data in the model, and if it is quite

237
00:29:23,080 --> 00:29:35,400
done, start simple, so choose like simple architecture. I'm just going to go very quickly with that

238
00:29:35,400 --> 00:29:39,480
just ask questions or comments from this lecture because I think there's a lot of slides.

239
00:29:40,680 --> 00:29:43,400
It's like 100 40 slides and we don't have that much time.

240
00:29:43,400 --> 00:29:55,960
Yeah, so that's again kind of like because Jeremy is using resident as a first model and

241
00:29:55,960 --> 00:30:01,400
that naturally they recommend something more simple like Linux. So there's different approaches

242
00:30:01,400 --> 00:30:20,600
to this and everything but something to think about. Yeah, but they give you like some default

243
00:30:20,600 --> 00:30:26,120
options for optimizers. For example, possibly other optimizers are the same as in fast AI. It's

244
00:30:26,120 --> 00:30:31,560
also animal optimizer. The learning rate three and like four, I think Jeremy used like three

245
00:30:31,560 --> 00:30:37,240
and like three or something like that. So it's close. Activation is radio, so that's the same with

246
00:30:37,240 --> 00:30:49,800
Jeremy's using. He or had initiation was also the same. Regularization, they suggest to start with

247
00:30:49,800 --> 00:30:59,720
none. I think when the fast AI we get some, I'm not so sure. That's more about

248
00:31:05,400 --> 00:31:10,120
also they recommend to start with a small trading set about 10,000 examples.

249
00:31:11,960 --> 00:31:17,640
And there's a fixed number of objects classes and create as a simpler synthetic trading set.

250
00:31:17,640 --> 00:31:22,840
I think it's a good device. I was I was working recently in a Kaggle dataset and that's like

251
00:31:22,840 --> 00:31:33,720
268 training, 126,000 training images. And and for some reason the model is working very, very slow.

252
00:31:33,720 --> 00:31:40,200
So if you start with the huge number of images, you might have issues somewhere and that's going to

253
00:31:40,200 --> 00:31:47,480
take you time to debug it and so on. So if you start with some make sure you're

254
00:31:47,480 --> 00:31:51,480
model works and then try to increase that maybe more sense.

255
00:32:05,640 --> 00:32:13,080
And also this kind of the same start with like small number of lines. I'm going to mute everyone.

256
00:32:13,080 --> 00:32:17,640
I want to. So just unmute yourself.

257
00:32:24,360 --> 00:32:27,880
Yeah, just first of all, good thing with the ground. That's the first step.

258
00:32:31,240 --> 00:32:36,840
I apologize I'm going so fast with that, but it's just too much of a step to cover in one,

259
00:32:36,840 --> 00:32:45,800
have an hour. Yeah, so and the next step they say it's overfit a single batch.

260
00:32:45,800 --> 00:32:50,280
So if you kind of overfit a single batch, then then there's some issues with your model.

261
00:32:50,280 --> 00:32:56,360
So I think that's interesting, interesting things so they say just overfit your model first.

262
00:32:56,920 --> 00:33:02,040
And once you can do that, then you can try to optimize performance of your model. I think that's

263
00:33:02,040 --> 00:33:10,280
quite interesting. I'm going to skip some slides now to step three. And it's also good if you have,

264
00:33:10,280 --> 00:33:16,760
if we have like known results or like image net, of course, we can compare ourselves to a lot of

265
00:33:16,760 --> 00:33:21,880
other guys who try to do the same thing, but in some cases it's not possible. I guess like Kaggle

266
00:33:21,880 --> 00:33:27,400
gives you that possibility. You can compare your solution to other guys on a leaderboard,

267
00:33:27,400 --> 00:33:31,880
but in some cases it's not possible, but at least if you can.

268
00:33:41,160 --> 00:33:46,040
Yeah, and then once you just keep iterating until your model performs up to expectations,

269
00:33:46,760 --> 00:33:52,120
again, we don't have to make very precise up to 0.001 percent model,

270
00:33:52,120 --> 00:34:00,040
just whatever makes sense for our project.

271
00:34:06,040 --> 00:34:11,880
And then they talk about what pull is a test error. It's a combination of more errors. It's just

272
00:34:11,880 --> 00:34:22,040
to be aware of that. And again, we was talking about the training data could be from

273
00:34:22,040 --> 00:34:27,320
day scenes, and then test data could be from the night. And that's going to cause an issue for

274
00:34:27,320 --> 00:34:29,320
for your model.

275
00:34:38,280 --> 00:34:38,760
What else?

276
00:34:44,520 --> 00:34:48,200
Right, so in the deep plan, you can have underfeiting.

277
00:34:48,200 --> 00:34:57,000
So try to fix that, try different network, different model, more detailed model, like more parameters.

278
00:34:59,400 --> 00:35:04,680
Once you can overfeed, then you need to deal with overfeiting. So that's next step.

279
00:35:05,320 --> 00:35:10,280
So once you start overfeiting, then you need to address that by some sort of dropouts and

280
00:35:10,280 --> 00:35:14,360
other regularization, other techniques that helps you with overfeiting.

281
00:35:14,360 --> 00:35:19,160
I'm going to scroll quickly.

282
00:35:26,520 --> 00:35:32,200
The point see of me. So the address the distribution shift. So again, if you have different test

283
00:35:32,200 --> 00:35:39,080
set, different training set, different data sets, something to think about. And the rebalance

284
00:35:39,080 --> 00:35:45,560
data set if applicable. So yeah, if you have, again, night and day pictures, try to make them balanced.

285
00:35:46,680 --> 00:35:51,400
And that's another problem I had with that data set from Kaggle. I had to like find a thousand

286
00:35:51,400 --> 00:35:59,160
classes. And some classes had like 500 pictures in. And some classes only had like a couple of them.

287
00:36:00,040 --> 00:36:04,600
So I believe that also can cause an issue. So one technique for that would be to over sample.

288
00:36:04,600 --> 00:36:12,600
So it just is basically take the same image a couple of times to make up for the missing numbers in

289
00:36:12,600 --> 00:36:24,440
that class. And then hyper parameter tuning. So like the hyper parameters, like how many layers

290
00:36:24,440 --> 00:36:29,880
in a network. So you can look and choose between the rest of the 34 or 50, 150, I believe.

291
00:36:29,880 --> 00:36:36,840
The white installation, the kernel size could be different stuff like that. You can choose different

292
00:36:36,840 --> 00:36:45,160
optimizers, different batch sizes, different learning rates. Yes, there's a lot of stuff to

293
00:36:45,160 --> 00:37:00,920
play with. And we can do this manually. So just try an error. Another one is like a grid search.

294
00:37:02,280 --> 00:37:07,640
So I believe there will be like certain tooling that would help you to search different

295
00:37:07,640 --> 00:37:16,440
hyperparameters. I don't know if they show any tools for that. Might be in a different lecture or

296
00:37:16,440 --> 00:37:35,080
something. Another one would be just random. I'm not sure about this method by

297
00:37:35,080 --> 00:37:48,040
as an hyperparameter optimization. So then at the end he says that the debugging is hard.

298
00:37:51,400 --> 00:37:57,000
So we start iterative process, start simple and then increase complexity.

299
00:37:57,000 --> 00:38:05,880
They're simple, implement in debug, evaluate, tune hyperparameters and improve model on data.

300
00:38:05,880 --> 00:38:15,080
So that kind of like a workflow they propose can make sense. And they give some of resources

301
00:38:15,080 --> 00:38:21,000
where we can learn more, so the machine learning, yearning. I think this is some sort of a book

302
00:38:21,000 --> 00:38:28,680
or something. I don't know what that is actually. Maybe you can check. So you can sign up for a draft

303
00:38:28,680 --> 00:38:35,160
free copy. Okay. I think it's a book, right? It looks like a book. It's a book I'm doing.

304
00:38:35,880 --> 00:38:47,800
Okay. Yeah. Never heard about that one. And the car part is three. It's a long three.

305
00:38:47,800 --> 00:38:59,400
Maybe I need to read that one. Okay. And there's a blog post.

306
00:39:03,720 --> 00:39:11,000
Okay. It's just something for me to still read about. Some homework.

307
00:39:11,000 --> 00:39:20,680
Yeah. So that was lesson 10, which brings us to a lab session. But because I didn't do lab,

308
00:39:20,680 --> 00:39:25,800
I kind of talked much about the lab. So I still have lab to do as a homework. But if there's

309
00:39:25,800 --> 00:39:42,600
anyone else on the call that did the lab work. And maybe once we talk about this, we can listen to that.

310
00:39:42,600 --> 00:40:00,440
You can chat as well, but I don't see anyone volunteering to. So maybe we can shift labs to another session

311
00:40:00,440 --> 00:40:07,240
on the week. And on the next week, that's what we've done. So do we have any more questions in the

312
00:40:07,240 --> 00:40:24,280
chat? Let's see below. Charters, do you know? No questions. Okay. That's great. So next week,

313
00:40:24,280 --> 00:40:29,960
we're going to have our last session of that. No, no, is it? That's right. That's going to be the last

314
00:40:29,960 --> 00:40:40,520
session. So we're going to finish the labs. So lecture 11, the labs 9, 6 to 9. And that's the

315
00:40:42,040 --> 00:40:51,800
habit. So that's end of the lab sessions. And then the lectures 12, 13, 12 is a testing

316
00:40:51,800 --> 00:40:59,480
on deployment. And 13 is the research directions. Lecture 14 is a guest lecture from Jeremy. And

317
00:40:59,480 --> 00:41:07,880
lecture 15 is a guest lecture from Richard. So those two again, optional. So if we don't count

318
00:41:07,880 --> 00:41:15,400
the optional, we have two video lectures and the labs 8 and 9 for next week. Okay, excellent.

319
00:41:15,400 --> 00:41:21,160
So we also in the agenda, the Avingash's presentation, we have Avingash in the call.

320
00:41:23,000 --> 00:41:29,800
So there's a question from Sagar. He mentions, he says if anyone, he wants to ask if anyone has

321
00:41:29,800 --> 00:41:35,960
had the chance to try the tools mentioned in the data management or infrastructure and doing lecture.

322
00:41:35,960 --> 00:41:53,880
Yeah, so I didn't. And they, they showed a lot of tools in the presentation. They showed a lot of tools.

323
00:41:53,880 --> 00:42:09,720
I'm just trying to show that page again. This kind of is from Google, from some paper from Google

324
00:42:09,720 --> 00:42:16,280
they said, this is your RML called. And it's so small compared to the whole ecosystem of deep learning.

325
00:42:16,280 --> 00:42:23,160
So that just shows the how big that is. And then they show this picture, which shows like all

326
00:42:23,160 --> 00:42:31,480
sorts of different tools for different problems. I've logged into the white and biases, but because

327
00:42:31,480 --> 00:42:38,680
I could not run that lab because yeah, I couldn't install the torch libraries for some reason.

328
00:42:39,560 --> 00:42:45,400
I couldn't run it, but there's a lot of tools to try, but also in a lot of time.

329
00:42:45,400 --> 00:42:52,360
Actually, we'll also be doing the presentation of tools. Yeah,

330
00:42:52,360 --> 00:42:55,640
into that. Excellent, excellent.

331
00:43:01,800 --> 00:43:05,400
Okay, so we can move on to next presentation.

332
00:43:05,400 --> 00:43:20,440
Well, there are more questions. Yeah, the question, how to practice labs. I don't know actually to

333
00:43:20,440 --> 00:43:28,280
answer the best answer to that. So all the code is here in a GitHub. And in a lecture, they use

334
00:43:28,280 --> 00:43:35,160
the suites and biases, and they have the code to run the Jupyter notebooks from there. But because

335
00:43:35,160 --> 00:43:40,040
we don't have that code anymore, it doesn't work anymore, we have to find a way to do it. And so far,

336
00:43:40,040 --> 00:43:48,200
I did not find a way to do it. I know some people did, it's like Michael, he managed to run the code,

337
00:43:50,040 --> 00:43:56,840
and he used call lines instead of the pip end of as they recommend here. Because it's TensorFlow,

338
00:43:56,840 --> 00:44:04,520
I had some issues installing some TensorFlow libraries. I think Pythos requires Google 10 and

339
00:44:04,520 --> 00:44:11,480
TensorFlow requires Google 9. I don't know if that seems or maybe that position. Yeah, because they give

340
00:44:11,480 --> 00:44:19,480
you like installation setup, you have to set up thing, but so I follow that, but it didn't work,

341
00:44:19,480 --> 00:44:28,440
so I don't have time to debug it. But if someone know how to do it and wants to shout to us,

342
00:44:28,440 --> 00:44:37,880
other people have to do it, they'll be great. I haven't tried to do it. Yeah.

343
00:44:39,720 --> 00:44:45,720
Also, hands up to everyone if you try installing TensorFlow, I need a break in everything.

344
00:44:45,720 --> 00:44:54,600
To an extent, but even Zoom is crashing, so a world of caution. Exactly.

345
00:44:57,320 --> 00:45:03,000
So that's kind of, because this was created as a bootcamp thing, it wasn't created as a MOOC,

346
00:45:03,560 --> 00:45:13,720
so they never really, you know, if we go back. Sagar has mentioned it's a problem with TensorFlow's

347
00:45:13,720 --> 00:45:19,720
newest version. Yeah, I think it's common as related to the lab. Yeah, because that was created as a

348
00:45:19,720 --> 00:45:25,720
bootcamp one of them, it's not a MOOC officially, so I guess no one is really maintaining that repo

349
00:45:25,720 --> 00:45:31,000
or making sure it works still. So it's nice we can use it and nice we can see all the videos,

350
00:45:31,720 --> 00:45:41,000
but yeah, maybe the code needs some updating or, you know, my telling. So that's the thing.

351
00:45:41,000 --> 00:45:49,400
But we'll try it for next week, if we cannot make it work. It's a shame, but yeah.

352
00:45:51,000 --> 00:45:54,040
So Sunyan, do you want to take covert presentation and present the

353
00:45:55,400 --> 00:45:57,480
Sure. JVM too.

354
00:45:58,520 --> 00:46:01,960
Oh, definitely. I just requested Avina, she's agreed that I can go first.

355
00:46:02,760 --> 00:46:09,400
My Zoom is crashing constantly because of the Swift thing, so Akash will do the presentation and I'll

356
00:46:09,400 --> 00:46:20,280
monitor the chart. Hi, everyone. This is Akash here. I hope you can see me here. I can see my video.

357
00:46:20,280 --> 00:46:26,520
Great. Yeah, great. It's great to be. I was actually listening in for the past half an hour.

358
00:46:26,520 --> 00:46:32,600
Great discussion. He wonder why I haven't been participating. I've been part of this earlier, but yeah.

359
00:46:32,600 --> 00:46:39,880
So maybe I can I share my screen? Yeah, sure. I'm sharing.

360
00:46:40,440 --> 00:46:46,120
Meanwhile to those unaware, there's been this huge meetup in India that was being run by Akash.

361
00:46:47,240 --> 00:46:52,920
So all of the Bangalore meetups around pasta and even data science would have been mostly

362
00:46:52,920 --> 00:47:01,800
promo cards. Those who might have a take. Excellent. I think this is like the pie torch from zero to

363
00:47:02,600 --> 00:47:05,800
whatever. Yeah, it is. Excellent.

364
00:47:07,640 --> 00:47:17,480
All right. Great. Thanks. Thanks. I am. So, okay, I'm going to share my screen. I hope this was great.

365
00:47:17,480 --> 00:47:23,880
Cool. Can you see my screen? Yep. Yes. Okay. Awesome. So, yeah, quick introduction guys.

366
00:47:23,880 --> 00:47:30,600
My name is Akash. I'm joining from Bangalore. The first time I'm joining the Twimmil AI Study

367
00:47:30,600 --> 00:47:36,920
Group call. So just quick background about myself. I was a software engineer for a few years

368
00:47:36,920 --> 00:47:42,680
and then switched over to machine learning after doing the first AI course online.

369
00:47:42,680 --> 00:47:49,560
And that was just that just completely changed my perspective on how we were looking at data

370
00:47:49,560 --> 00:47:53,800
science and machine learning. Like in college, it was so mathematical and so dry, but faster,

371
00:47:53,800 --> 00:47:58,120
I was so hands on. So that's how I got involved and started actually switched over my career

372
00:47:58,120 --> 00:48:05,640
completely. Started doing ML consulting work and freelancing while also in the mean and as part

373
00:48:05,640 --> 00:48:10,520
of this, I also like along with a friend of mine, we realized that although there are a lot of

374
00:48:10,520 --> 00:48:17,320
people who are interested in data science in India and Bangalore, but there's no group

375
00:48:18,680 --> 00:48:24,520
community, especially having a lot of technical talks. There's too much networking and that kind

376
00:48:24,520 --> 00:48:30,040
of stuff. So we started the data science network about eight months ago and it has been a wonderful

377
00:48:30,040 --> 00:48:35,000
journey that so far we've had about 5,000 members in our meetup groups and we have meetups every

378
00:48:35,000 --> 00:48:39,880
other week. So, yeah, just really excited to be part of the data science community and learning

379
00:48:39,880 --> 00:48:49,560
new things every day. So what I'm here to present today is there's a tool that I've been working

380
00:48:49,560 --> 00:48:55,800
on with a few friends and like Simon has also been involved recently. This is a tool called

381
00:48:56,360 --> 00:49:01,960
Jovian. This is something that was born out of our own problem that we really love Jupiter

382
00:49:01,960 --> 00:49:08,920
notebooks, the interactive nature, the way we can plot graphs and go back and forth and fix things.

383
00:49:10,200 --> 00:49:19,000
I felt like that really opened up things for me in terms of trying a lot of experiments out,

384
00:49:19,000 --> 00:49:23,720
but it also led to a lot of problems, especially coming from a software engineering background.

385
00:49:23,720 --> 00:49:28,280
I was used to having like get kind of a workflow where you sort of branch out, make some change

386
00:49:28,280 --> 00:49:34,520
and come back. But what really happens, what we've seen happens in a lot of machine learning

387
00:49:34,520 --> 00:49:39,800
projects is something like this. If you're using, so the question we asked us is how we'd be tracking

388
00:49:39,800 --> 00:49:45,320
our machine learning experiments, all the different ideas that we're trying. Now notebooks are great

389
00:49:45,320 --> 00:49:49,480
to try things out, but then because they're so rich, they have so many graphs and things like

390
00:49:49,480 --> 00:49:54,200
that, they can be large in size. What we tend to do is we tend to keep creating copies and copies

391
00:49:54,200 --> 00:49:59,880
of data. So this is just like one particular project that I was working on, where we had

392
00:49:59,880 --> 00:50:03,960
over a course of three months, we had like 60 different notebooks and then we would try to put

393
00:50:03,960 --> 00:50:08,600
everything into the just the file name so that and this would be sitting there lying on a cloud

394
00:50:08,600 --> 00:50:13,400
machine and there's no sense getting putting this into gate because this is like 200 MB of Jupiter

395
00:50:13,400 --> 00:50:19,480
notebooks. So that became, you know, that we found that very tricky. We also tried using log files,

396
00:50:19,480 --> 00:50:23,480
but then log files are sort of even trickier where you have like a code in a notebook which you

397
00:50:23,480 --> 00:50:28,120
have updated, but then your log file looks something like this where you have to sort of go into it

398
00:50:28,120 --> 00:50:34,440
and try and parse things out. It's a big mess, right? We tried spreadsheets as well where we would

399
00:50:34,440 --> 00:50:41,000
sort of keep track of things in a spreadsheet like on what date, which experiments, which network

400
00:50:41,000 --> 00:50:46,440
we tried, what hyper parameters we used, and what kind of results we got. But that also sort of

401
00:50:46,440 --> 00:50:50,840
made things really messy because now you have your notebooks somewhere and then you have your data

402
00:50:50,840 --> 00:50:55,160
somewhere and then you have maybe your checkpoints and models somewhere and nobody is really kept

403
00:50:55,160 --> 00:51:00,920
track of the library. So TensorFlow has updated, PyTorch is updated and it gets all very messy.

404
00:51:00,920 --> 00:51:06,200
So we started creating a tool for ourselves and that's what I want to show you. The goal was to

405
00:51:06,200 --> 00:51:11,400
start with Jupiter notebooks and make them more shareable, make them like collaborative so that

406
00:51:12,440 --> 00:51:17,880
people can work or work together on a single notebook, try different ideas and manage the entire

407
00:51:17,880 --> 00:51:24,760
workflow around them. So I will quickly jump into an example. I guess that's the best way to do it.

408
00:51:24,760 --> 00:51:31,560
So this is Jupiter notebook running on my local host. It's an example of classifying handwritten

409
00:51:31,560 --> 00:51:38,040
digits from the MNIST dataset by just training a CNN using Keras. It's pretty straightforward.

410
00:51:38,040 --> 00:51:43,560
You prepare some data, download the data, take a look at the data and then do some pre-processing,

411
00:51:43,560 --> 00:51:49,240
one-hot encoding, have some like a two-categorical which does like one-hot encoding here. Then,

412
00:51:49,240 --> 00:51:55,080
as you start out, as we just saw in the lecture that you start with a very simple model. So I have

413
00:51:55,080 --> 00:52:00,920
a very simple model of single conglare 32 filters and then I just flatten that and pass it through

414
00:52:00,920 --> 00:52:08,840
a fully connected layer. Then I compile it, I train it, I look at the accuracy, okay that looks

415
00:52:08,840 --> 00:52:13,640
pretty good. Maybe it also helps to look at how the model was training so I can plot

416
00:52:13,640 --> 00:52:19,160
how the accuracy is changing. So I guess the dots are the training laws and the validation

417
00:52:19,160 --> 00:52:26,120
laws is the line. So the pretty standard model, right? And then finally, you evaluate it on the

418
00:52:26,120 --> 00:52:31,320
test set and then get the accuracy and laws. Now, suppose you want to share this with somebody

419
00:52:31,320 --> 00:52:35,000
so that they can try it out as well, right? Maybe you're doing this on your local machine or your

420
00:52:35,000 --> 00:52:40,760
Google machine, on your on your DCP cloud machine. You want to send it across to somebody,

421
00:52:40,760 --> 00:52:46,040
just get quick thoughts, get maybe some inputs, maybe they're helping you learn. This is where our

422
00:52:46,040 --> 00:52:51,080
tool comes in, Jovian. So you can just can simply install, install Jovian.

423
00:52:57,000 --> 00:53:03,800
And yeah, I think I already have it. Then you import the library and then you just say

424
00:53:03,800 --> 00:53:11,240
jovian.com it. Okay. And when you say that, it asks you to, it asks you to provide an API key.

425
00:53:11,880 --> 00:53:17,160
An API key is something that you can find by simply logging into the website. It's a, okay,

426
00:53:17,160 --> 00:53:22,440
I'm already logged in, I guess. So I'm just going to copy, it's a one click login with GitHub.

427
00:53:22,440 --> 00:53:27,400
So I just get it, it just gives me an API key, which I can enter here. Now, once I do this,

428
00:53:27,400 --> 00:53:32,920
what we do is we take a snapshot of the Jupyter notebook. We take a snapshot of the entire

429
00:53:32,920 --> 00:53:39,640
backing environment. So here I'm using Anaconda. So we capture the Anaconda environment behind the

430
00:53:39,640 --> 00:53:43,880
scenes. And I'd like to ask you, do you know how to export the Anaconda environment? Because there's

431
00:53:43,880 --> 00:53:48,760
a really complex command that you have to try, especially to make sure that the environment is

432
00:53:48,760 --> 00:53:56,920
reproducible across between like Mac OS and Linux and Windows. But anyway, we do a bunch of these

433
00:53:56,920 --> 00:54:03,320
things behind the scenes. And then we give you, we upload it to the cloud and you get a quick,

434
00:54:03,320 --> 00:54:07,400
like a URL where you can see the notebook. So you don't need to have Jupyter running.

435
00:54:08,440 --> 00:54:12,600
You can, you know, the notebook is there. Apart from that, we also have,

436
00:54:13,640 --> 00:54:18,200
all the file, we also have the environment that has been captured. So these are all the libraries

437
00:54:18,200 --> 00:54:25,240
that I'm using here, right? And then you can simply take this link as I'm taking right now. And

438
00:54:25,240 --> 00:54:35,080
you can simply take this link and send it to someone. And there, that's, that's, you know,

439
00:54:35,080 --> 00:54:37,960
then now you can click the link and you can try it out. Now, if you want to quickly, if you want

440
00:54:37,960 --> 00:54:42,200
to try this out on your own machine, all you need to do is you can click the clone button.

441
00:54:43,080 --> 00:54:47,960
Now go back to your terminal. Let me open the new one here.

442
00:54:47,960 --> 00:54:57,320
What, let me just rename this to something nice. Yeah, there you go. Okay, so I'm just going to,

443
00:55:02,360 --> 00:55:06,600
I'm just going to run the clone command. Again, to run this clone command, I need to have

444
00:55:06,600 --> 00:55:16,440
the Jovian library installed. So I, I'll do like pip install Jovian, which in my case, I already do

445
00:55:17,480 --> 00:55:23,640
better just to show you. And this installs a CLI command called Jovian. So that's where you can

446
00:55:23,640 --> 00:55:28,600
paste the command that gets copied when you click the clone button. And that downloads all the files.

447
00:55:28,600 --> 00:55:33,240
And it gives you a bunch of instructions for what you need to do next, right? So I'm just going to

448
00:55:33,240 --> 00:55:39,320
enter this directory, run this Jovian install command. So what this does, the Jovian install,

449
00:55:39,320 --> 00:55:44,600
depending on like, as you can see here, I have an environment or YAML file, environment or Mac

450
00:55:44,600 --> 00:55:49,880
West file. So it looks into if there's a environment specific, if there was specific file, it uses

451
00:55:49,880 --> 00:55:54,440
that. Otherwise, it uses environment or YAML. The reason we need to do this is because Konda

452
00:55:54,440 --> 00:56:00,120
environments and Konda packages have different versions across different platforms. So sometimes

453
00:56:00,120 --> 00:56:05,240
things don't work. Sometimes things fail and then it just errors out. So what we do is we try to

454
00:56:05,240 --> 00:56:12,840
install. If there's an error, we, like, comment out those lines from your environment, try to

455
00:56:12,840 --> 00:56:17,240
make a best set and try this three four times till the entire environment is installed, right?

456
00:56:17,240 --> 00:56:21,880
And this has been an open issue on Konda for over a year where people are saying, I just can't

457
00:56:21,880 --> 00:56:28,040
reuse my environment fine. But we try to solve that with just doing a few hacks and trying the

458
00:56:28,040 --> 00:56:34,840
next best thing. But yeah. So what this does, you know, you could do some work on your local

459
00:56:34,840 --> 00:56:39,880
machine and then maybe push it to Jovian and then open it up on your cloud machine, pull from

460
00:56:39,880 --> 00:56:43,960
there, clone from there and start running, right? So now at this point, if I do, if I run the

461
00:56:43,960 --> 00:56:49,560
Konda activate command, that activates my new environment, which is just created and I can go to

462
00:56:49,560 --> 00:56:57,800
put a notebook and start playing around with that notebook, right? So simple push and simple

463
00:56:57,800 --> 00:57:05,400
commit and clone flow that that we do. Now, the, now the next thing that we've also done on top of

464
00:57:05,400 --> 00:57:10,680
this is maybe you're just looking for some feedback or some inputs. So what we've done is we've

465
00:57:10,680 --> 00:57:15,720
built out a commenting interface on top of Jupiter. So for instance, if you have a question,

466
00:57:15,720 --> 00:57:20,840
you know, very specifically about, let's say, about the model. Somebody has a question saying,

467
00:57:20,840 --> 00:57:30,360
why are you using kernel size 32? So they can just comment it on that particular cell and then

468
00:57:30,360 --> 00:57:34,840
I will get a email notification where I can click through and reply to them and then they will

469
00:57:34,840 --> 00:57:39,560
get an email notification, right? So this allows for very quick feedback and discussion on Jupiter

470
00:57:39,560 --> 00:57:44,200
notebooks. When you're, you know, one person can actually do the coding and the others can all

471
00:57:44,200 --> 00:57:49,240
come in and give their thoughts on it. This was something that we found missing. So we just

472
00:57:49,240 --> 00:57:56,280
decided to add that as well. So next, the way we use it is we run some workshops here. So often,

473
00:57:56,280 --> 00:58:01,400
we put all the, we put all the Jupiter notebooks, environments, everything on Jogin and then let

474
00:58:01,400 --> 00:58:05,960
people clone it, try out something on their own and then push the new versions and then get feedback

475
00:58:05,960 --> 00:58:12,360
from us, right? So that, it, it helps to do that very easily. Okay. So coming back to this,

476
00:58:12,360 --> 00:58:17,720
now one thing is that, okay, the, it's great that I have the Jupiter notebook. It's great that I

477
00:58:17,720 --> 00:58:23,400
have the environment I can reproduce it. But what ends up happening is, you know, even if you just,

478
00:58:23,400 --> 00:58:28,200
you make some changes, you try, you create a new version, you, you ultimately you end, start

479
00:58:28,200 --> 00:58:33,960
ending up with like a half a dozen dozen or so Jupiter notebooks, right? And what you really want

480
00:58:33,960 --> 00:58:41,000
is, and what you really want is you want to be able to track specific things about notebooks.

481
00:58:41,000 --> 00:58:45,480
Like for instance, in this notebook or in this experiment, what are the hyper parameters that

482
00:58:45,480 --> 00:58:51,000
tried and what were the success metrics I got, right? So that's where we have created a very simple

483
00:58:51,000 --> 00:59:00,520
API as well. Is there a question? Okay. Yeah. That's where we have created a very simple API as well.

484
00:59:00,520 --> 00:59:09,080
You know, all you, all you need to do is just import Jogin and then we have a couple of APIs,

485
00:59:09,080 --> 00:59:15,880
we have log hyperparamts. Okay. So this is two log hyper parameters. So for instance, here I'm

486
00:59:15,880 --> 00:59:25,720
just going to add a small note about the architecture that this is the simpler con 32 plus dense,

487
00:59:25,720 --> 00:59:35,880
right? Maybe I'll also add a note about the batch size. You know, the batch size I'm using is 128.

488
00:59:35,880 --> 00:59:46,600
I'm using three epochs and what else? Yeah. What else is interesting here? I think I'm using the RMS

489
00:59:46,600 --> 00:59:54,040
prop optimizer, right? So this is currently a bit manual, but what we're trying to do is,

490
00:59:54,040 --> 00:59:59,160
now when you do a model.fit, we're trying to create a callback so that all of this information

491
00:59:59,160 --> 01:00:03,640
can be picked up automatically, but I just wanted to show you the basic API as I do that.

492
01:00:03,640 --> 01:00:10,840
So that logs are hyperparameters. Then once the model is trained, I might also want to actually

493
01:00:10,840 --> 01:00:19,160
keep track of the metrics. So that's where I can do jogin.log metrics. And here I'm just going

494
01:00:19,160 --> 01:00:28,680
to put in these things. So I want to mention that even for category competition, like this is how

495
01:00:28,680 --> 01:00:33,080
most people usually track, like they have multiple notebooks and then they create spread sheets.

496
01:00:33,080 --> 01:00:38,280
So this is essentially looking at replacing it with just one single interface.

497
01:00:39,160 --> 01:00:45,080
Yeah. And right now, I'm actually having to type a lot, but the idea is this will be just like a

498
01:00:45,080 --> 01:00:51,480
single callback that you introduced into your history, you know, at least for the popular frameworks.

499
01:00:51,480 --> 01:00:55,320
So now I've captured some metrics. I've captured them hyperparameters and we'll take a look at

500
01:00:55,960 --> 01:01:00,680
what they look like. But apart from that, the other thing is also that, you know, maybe I might

501
01:01:00,680 --> 01:01:07,560
this code, this might be some common code that I use across all my notebooks. So I might

502
01:01:07,560 --> 01:01:14,440
actually put this in like a utils file, right? So maybe I'll do like create a utils for utils file.

503
01:01:14,440 --> 01:01:34,760
And we put this here, call this plot history. Okay, very simple. And I'm just going to import

504
01:01:34,760 --> 01:01:45,080
the from utils import plot history and then this plot history does the same thing, right?

505
01:01:45,080 --> 01:01:49,880
So now I also have a dependency on a certain file. And another thing is like I've trained this

506
01:01:49,880 --> 01:01:55,400
model. Maybe this is a simple example, but then in certain cases, your model can actually have

507
01:01:56,680 --> 01:02:00,920
been trained for a long time and you might want to just keep the save the weights. So that's where

508
01:02:00,920 --> 01:02:05,560
you probably want to do something like, you want to save the model as well. So I'm just going to

509
01:02:05,560 --> 01:02:21,080
save the model, model.savemscnn.h5. Okay. And yeah, so now I have like a dependency on a file,

510
01:02:21,080 --> 01:02:25,800
I've lost some hyperparameter, some metrics. This time when I commit, what I'm going to do is I'm

511
01:02:25,800 --> 01:02:31,320
also going to include the files that are depend on. So this is utils.py and the artifacts that I've

512
01:02:31,320 --> 01:02:43,880
generated. So this is like amnestcnn.h5. Right. So now I commit once again, what it does is it create

513
01:02:43,880 --> 01:02:48,760
updates that notebook. So it creates a new version essentially. Once again, it captures the environment

514
01:02:48,760 --> 01:02:54,840
to make sure if you've created, I did any new libraries that get captured. Any additional files and

515
01:02:54,840 --> 01:02:59,240
any artifacts are also captured. And also the metrics and hyperparameters, everything is then

516
01:02:59,240 --> 01:03:05,640
taken. And then now you have a more start to have a more complete picture of your experiment.

517
01:03:05,640 --> 01:03:11,400
Right. So you have the notebook, you have environment, you have the utils files right here, you have

518
01:03:11,400 --> 01:03:16,200
the artifact. Now we are trying to differentiate between sources and artifacts because, you know,

519
01:03:16,200 --> 01:03:22,920
as you might have faced, if you train a resident 152 and then create, save a checkpoint,

520
01:03:22,920 --> 01:03:27,080
that's going to be like a 120 MB file. If you check that into Git and then you train two or three

521
01:03:27,080 --> 01:03:31,640
more of these, suddenly you have a Git repository, that's one GB. So every time you push and somebody

522
01:03:31,640 --> 01:03:36,280
tries to pull it, they're downloading, you know, one GB worth of stuff because the weight change

523
01:03:36,280 --> 01:03:40,920
every single time, right. So that's why we differentiate between artifacts and sources and,

524
01:03:40,920 --> 01:03:46,520
you know, artifact and we provide an option to actually clone without actually downloading the

525
01:03:46,520 --> 01:03:49,800
artifact because if you just want to reproduce it, you probably don't need it. On the other hand,

526
01:03:49,800 --> 01:03:55,400
if you just need the artifact, you can download it right here as well. So that's the files, the

527
01:03:55,400 --> 01:03:59,960
dependency and the artifact. But also, this is where it starts to get interesting, where,

528
01:04:00,760 --> 01:04:04,920
you know, now you start to get, as you log some hyper parameters and some metrics, you start to get

529
01:04:04,920 --> 01:04:09,400
a timeline of what was actually happening in this project. So, you know, I have here, it's a

530
01:04:10,360 --> 01:04:17,000
corn 32 with a dense, three pox, RMS prop, and I go to an accuracy of 97%. That's pretty good,

531
01:04:17,000 --> 01:04:24,440
but maybe I can do better. And as you start to sort of keep doing this for multiple times,

532
01:04:24,440 --> 01:04:30,440
start to record multiple versions, what you can do is, so here I have another, you know, the same

533
01:04:30,440 --> 01:04:34,840
example that I've tried a few versions. So you can check the versions, okay, not here.

534
01:04:39,160 --> 01:04:43,160
Yeah, so same example I've tried, I've recorded a few versions and all the versions are visible

535
01:04:43,160 --> 01:04:48,680
here and the version drop down. And I can actually go click on compare versions, right? So now,

536
01:04:48,680 --> 01:04:55,480
whichever version has a record, all those records show up here. So I can see when it was created,

537
01:04:55,480 --> 01:05:01,720
who created it? So come back to this, you know, the notes that I've added here and the hyper parameters

538
01:05:01,720 --> 01:05:06,680
and the metrics. So it starts to give you a good overview. One thing that we've added and we're

539
01:05:06,680 --> 01:05:10,040
adding more things is, you know, you can select which columns you're interested in. Maybe

540
01:05:10,040 --> 01:05:15,880
epochs is not that important, maybe you don't need hyper parameters at all. So you can just look at

541
01:05:15,880 --> 01:05:20,920
the metrics. Another thing that we're doing is, you know, if you have some runs which you're not

542
01:05:20,920 --> 01:05:25,640
interested in, you can actually just select them and remove them, you can reorder the columns,

543
01:05:25,640 --> 01:05:32,280
you can actually, you can add in notes here, come and change, change in entry because maybe it was

544
01:05:32,280 --> 01:05:38,360
recorded incorrectly, right? So I'm going to try to make this very powerful, pretty much almost

545
01:05:38,360 --> 01:05:44,680
as powerful as a spreadsheet, but then more targeted for the data science use case, okay?

546
01:05:44,680 --> 01:05:51,560
And then finally, the last thing that I want to show you is collaboration. So now this is just me

547
01:05:51,560 --> 01:05:55,320
working and maybe me sharing with somebody, but you can see here that there are other authors here as

548
01:05:55,320 --> 01:06:01,240
well. That's where we have an option. So a bunch of things, one is a visibility that if you want to,

549
01:06:01,960 --> 01:06:06,920
so currently this is a public notebook, anybody can go on my profile and find it. I can make it secret

550
01:06:06,920 --> 01:06:12,520
so that only people with the link can access it. We're also working on private notebooks so that

551
01:06:12,520 --> 01:06:18,360
you have to be logged in and added as a collaborator. I can archive this notebook once it starts to

552
01:06:18,360 --> 01:06:23,080
get filled up on my profile and then finally, I can add collaborator so I can just enter that username

553
01:06:24,360 --> 01:06:33,160
and yeah, and that will, that will add them as a collaborator, right? So now what happens is

554
01:06:33,160 --> 01:06:39,320
every time they clone and they try to commit, this is going to add to this particular project itself,

555
01:06:39,320 --> 01:06:44,440
right? So slowly starting from a notebook, we're just trying to build a complete story of a machine

556
01:06:44,440 --> 01:06:50,280
learning, of a machine learning project and trying to make it as seamless as possible so that,

557
01:06:50,280 --> 01:06:55,800
you know, as a ML student or engineer or data scientist, you have to do minimum work.

558
01:06:56,680 --> 01:07:01,560
And we have been trying this for about four or five months now using it at workshop.

559
01:07:01,560 --> 01:07:07,960
It has been pretty helpful for us. A few community related, a few inputs for the community were,

560
01:07:07,960 --> 01:07:12,440
hey, this is great. But what if I could, I want to be able to run this immediately as well.

561
01:07:12,440 --> 01:07:17,560
So that's where we have created integrations with binder and Kaggle. So if I click run on binder,

562
01:07:17,560 --> 01:07:23,080
what this does is this immediately goes to mybinder.org. I don't know if you're familiar. Mybinder is

563
01:07:23,080 --> 01:07:27,560
like a free hosted Jupyter notebook which can create a rather Jupyter notebook

564
01:07:27,560 --> 01:07:34,520
instantly from any repository, get repository. So that's what we've added. It's going to take a time

565
01:07:34,520 --> 01:07:39,160
over. Okay, so it looks like, and what it does is it actually caches. So first time you click,

566
01:07:39,160 --> 01:07:43,480
it's going to install all these things, but next time it's just going to keep a Docker image around

567
01:07:43,480 --> 01:07:49,000
so that this can start immediately. Another thing is if you want to use a GPU, you can click run on

568
01:07:49,000 --> 01:07:56,520
Kaggle and then this would run immediately on your, on your Kaggle account, right? And you just

569
01:07:56,520 --> 01:08:10,200
need to connect your Kaggle account. There you go. So yeah, so that's that's pretty much it from my,

570
01:08:10,200 --> 01:08:18,440
I don't know why this is not working. Okay, I'll look into this. We're also adding colab and a bunch

571
01:08:18,440 --> 01:08:25,080
of other platforms soon, but yeah, that's it. So that's pretty much a lot and just wanted to show

572
01:08:25,080 --> 01:08:29,480
you what we are building and if it is useful. So we are still, I would say we are still in beta.

573
01:08:29,480 --> 01:08:34,120
We're still building. We have a lot of things working, but with this, still a long way we want to

574
01:08:34,120 --> 01:08:39,160
go. So would love, you know, if you could try it out, give us feedback, tell us what you would want

575
01:08:39,160 --> 01:08:44,920
in this, because we are very open to just trying to just building what, what will serve the,

576
01:08:46,040 --> 01:08:52,840
serve the user's best, right? Yeah, and we have been personally using it more as like in Kaggle

577
01:08:52,840 --> 01:08:59,160
competitions as like a internal leaderboard to keep track of who's doing what and you know,

578
01:08:59,160 --> 01:09:03,800
we can quickly click through see their approach, see their loss, see their notes and things like that,

579
01:09:03,800 --> 01:09:10,200
right? So building it as we use it. Yeah, so that's that's pretty much it. If you have any questions,

580
01:09:10,200 --> 01:09:17,000
I'm happy to answer. I think you know, it's very good, very nice, very like complete picture.

581
01:09:17,000 --> 01:09:23,320
So I've not, I didn't know about this too, so I know a lot. I've been trying to use also Kaggle,

582
01:09:23,320 --> 01:09:29,400
it's got something kind of not similar, but at least they have this idea of committing your

583
01:09:29,400 --> 01:09:37,160
kernels. Yes. So Kaggle, you can commit and then it saves your data, saves or not book and then

584
01:09:37,160 --> 01:09:42,200
it saves your output files. Yeah. But it doesn't, it doesn't save all the nice information as you

585
01:09:42,200 --> 01:09:49,400
have like architecture. It doesn't have that much. It's not that rich. Yeah. Yeah. One thing also is

586
01:09:49,400 --> 01:09:55,800
that Kaggle, it has to be, you sort of have to commit it on Kaggle, so you can't really run it

587
01:09:55,800 --> 01:10:00,440
on your local machine. It's right. We wanted to have it flexible and have it open.

588
01:10:02,120 --> 01:10:09,960
Yeah. Another quick note was is that this is most of this is open source. So the Jovian Python

589
01:10:09,960 --> 01:10:14,520
library is completely open source. We're trying to open source a backend as well, but it's just,

590
01:10:14,520 --> 01:10:19,800
you know, we have to pull out all the specific backend specific things before we can, you know,

591
01:10:19,800 --> 01:10:25,800
make it generalized enough. So yeah, I'll post this link here as well in case you want to post,

592
01:10:25,800 --> 01:10:29,560
create, you know, try it out. If you're facing issues, you can always switch out to a

593
01:10:29,560 --> 01:10:40,680
share of cellar poll requests. The other thing, whatever isn't open source is free and will stay that. Yeah. Right. Thank you.

594
01:10:41,480 --> 01:10:47,640
How I like envision it is like, especially for Kaggle, like if you run something locally,

595
01:10:47,640 --> 01:10:52,520
and then you also want to run it on a kernel, and you like want to get away from the flakiness

596
01:10:52,520 --> 01:10:56,840
of the kernel, if you run it locally, and say if you're participating in a kernel's only

597
01:10:56,840 --> 01:11:02,520
competition, you get to run it on a kernel. Or if you want to check it out on an AWS instance,

598
01:11:02,520 --> 01:11:07,320
like if you want to run your BFI, it's your most model, you could run the 32 core machine,

599
01:11:07,320 --> 01:11:13,400
just run it there, get the trained weights that have the inference run on Kaggle kernel,

600
01:11:13,400 --> 01:11:19,800
inference will be a competition or even locally. That's once more something you can do with this.

601
01:11:19,800 --> 01:11:29,800
Very easy. Yeah, folks, right. Okay. All right. And you can reach out to me in case you have any

602
01:11:29,800 --> 01:11:36,440
questions, I think. Sayam will share the contact details, possibly. Yeah. Right. Okay. So I'll

603
01:11:36,440 --> 01:11:42,920
start. Yeah. You could join our slack as well. So people can ask them. Yeah. Yeah. Yeah. That's great.

604
01:11:42,920 --> 01:11:48,200
I would love to. Yeah. I'll be there. I'll slack them. Yeah. Then we can ask questions. They're

605
01:11:48,200 --> 01:11:54,200
all right. I'll share the link in the slack as they're like, if anyone has any feedback,

606
01:11:54,760 --> 01:12:00,440
I think it's very open to community feedback also to please ping me or him once he joins for any feedback

607
01:12:00,440 --> 01:12:03,640
or suggestions. Next on. Thank you.

608
01:12:09,080 --> 01:12:13,800
So thank you for the presentation. We have we have one more presentation for today.

609
01:12:13,800 --> 01:12:22,120
It's Avinish. It's going to present. Yeah. Hi, Michael. Hi. I'm a properly audible. Yeah. Hi.

610
01:12:22,120 --> 01:12:26,840
Yeah. So do you want me to take the presentation today or?

611
01:12:29,480 --> 01:12:37,480
Yes, it's 720. Maybe you can at least tell us what you want to talk about. And if that's not

612
01:12:37,480 --> 01:12:45,080
enough time for you, then maybe you can reschedule. But what do you think?

613
01:12:54,600 --> 01:12:55,640
Come on here, Avinish.

614
01:12:55,640 --> 01:13:11,960
Hello, Avinish. Still there? Can you hear me now? Yeah. Yeah. So the presentation that I wanted to do

615
01:13:11,960 --> 01:13:17,160
was on model interpretability and visualization. In that, I will be walking through a Kaggle kernel,

616
01:13:18,040 --> 01:13:24,440
which so it's on food classification like hot dog version 2 where it doesn't it doesn't just say

617
01:13:24,440 --> 01:13:30,840
whether it's hot dog or not, it can also detect the other classes of food. So using that,

618
01:13:30,840 --> 01:13:36,280
I'll be explaining how the Inelaya Activations look and also a bit about activation maps.

619
01:13:37,000 --> 01:13:43,480
So this is what the talk is going to be. But I feel it is going to take at least 22 to 25 minutes.

620
01:13:43,480 --> 01:13:49,560
Okay. And you'll try to talk next week. Yeah, I'm fine with that.

621
01:13:49,560 --> 01:13:56,120
Okay. If you could just share the link with us, we'll check it out for the week. Sure. Yeah.

622
01:13:57,000 --> 01:14:03,080
I will actually really sorry if I took up too much time. Sorry. I'm saying I'm really sorry if I

623
01:14:03,080 --> 01:14:09,720
took up some of your time. Not at all, Akash. It was very good presentation. Okay. Thanks.

624
01:14:10,680 --> 01:14:18,040
Yeah. So I'll share the link, the Kaggle kernel link. And we can discuss that in the next

625
01:14:18,040 --> 01:14:34,200
in the next week's session. Okay. Great. So I'll move that for next week. Do you have anything else

626
01:14:34,200 --> 01:14:41,480
to share for today? And then use? Yeah. I have some open slots for the Chai time data science

627
01:14:41,480 --> 01:14:46,760
conversation that I've shared for this upcoming week. So if anyone wants to talk about anything,

628
01:14:46,760 --> 01:14:57,000
please feel free to use any of the topic. What is the most, what's the most things people

629
01:14:57,000 --> 01:15:01,800
like to talk about in this discussion? Can you? I don't want to hear like names and things.

630
01:15:01,800 --> 01:15:09,160
There might be some statistics. Was the most interesting box most asked things about?

631
01:15:09,160 --> 01:15:19,320
But it's also the thing, but I guess Kaggle and Fastie, then machine learning jobs.

632
01:15:20,120 --> 01:15:24,920
Okay. Nice. Nice. That was very nice of you, Sanyam.

633
01:15:26,520 --> 01:15:30,040
Thank you. Just thank you for helping me. Yeah. That's nice. That's nice.

634
01:15:32,360 --> 01:15:36,920
So when I found this page on data helpers, maybe something you would be

635
01:15:36,920 --> 01:15:43,240
interested to have your name to it, I was thinking. I just learned about it today. I have the tab open

636
01:15:43,240 --> 01:15:51,000
and check it out after this. Yeah. That's excellent. That's excellent. Okay.

637
01:15:53,400 --> 01:15:57,160
So if that's everything for today, I guess we'll just close for today and

638
01:15:58,360 --> 01:16:02,920
we'll meet next week, the same time, the same day, for our last session of

639
01:16:02,920 --> 01:16:17,240
I'm going to show it again. If I can still open for suggestions. Hopefully, a Fastie is released

640
01:16:17,240 --> 01:16:22,680
by then to the public. We could start our Fastie session as soon. I'll be happy to

641
01:16:22,680 --> 01:16:27,640
volunteer to take those notes. Yeah, indeed. Good. You mentioned that. I forgot about this.

642
01:16:27,640 --> 01:16:33,960
So, indeed, it's so next week, we're going to have our, I hope we can see my screen now.

643
01:16:33,960 --> 01:16:38,920
Yeah. Yeah. So next week, we're going to have the last Fast full stack deep learning static

644
01:16:38,920 --> 01:16:47,320
group thing. And then, and then say it's a blank page. So I've collected from all the,

645
01:16:47,320 --> 01:16:51,160
from Slack, from what people were saying that what potentially you could do next,

646
01:16:51,160 --> 01:16:54,280
like secure and private AI, there were some people interested doing

647
01:16:54,280 --> 01:17:00,680
ML course from open data science, which starts September 2nd, I think, for the last session

648
01:17:01,320 --> 01:17:07,960
of that course in the current format. And there was like advanced NLP, which spacey,

649
01:17:07,960 --> 01:17:13,560
all of those are free, by the way, so it's a good look. We can do all of them. Fastie, I released,

650
01:17:13,560 --> 01:17:19,160
kind of silently released, quietly released, NLP because the videos are not released yet,

651
01:17:19,160 --> 01:17:25,640
so it's only notebooks. The statistical learning is more like machine learning from Stanford,

652
01:17:25,640 --> 01:17:31,560
free introduction with PyTorch, free, there are some Stanford courses on MLP,

653
01:17:32,920 --> 01:17:38,920
recommend that the deep unsupervised learning, that we cannot do all of them at once.

654
01:17:39,800 --> 01:17:45,720
Well, we could, if we create like separate groups, led by different people and meeting at

655
01:17:45,720 --> 01:17:52,440
some times, maybe even overlapping with some others. So that's another option. We could do more

656
01:17:52,440 --> 01:17:57,240
courses, but then there would be like smaller groups doing the courses. Or we could try to keep

657
01:17:57,240 --> 01:18:03,000
it as one group and then just select a course. Of course, something that a lot of us,

658
01:18:03,880 --> 01:18:09,880
this is how the group started, was the Fastie I deep learning course, part one, was by June last

659
01:18:09,880 --> 01:18:17,800
year. So the part two, it's not released to public it, and I didn't hear when they're going to do it.

660
01:18:19,400 --> 01:18:25,880
Didn't you do, but I'm not sure if that still works. Yeah, exactly. I think they now busy with,

661
01:18:28,760 --> 01:18:36,680
they now busy with updating the Fastie I library to V version two. So I'm not sure what's the

662
01:18:36,680 --> 01:18:44,520
priority for them now, because they also promised a couple of lectures, livestream lectures to the

663
01:18:44,520 --> 01:18:48,360
part two course, which also it's not sure when they're going to be available.

664
01:18:51,560 --> 01:19:00,680
Yeah, so we can continue that discussion on a slack, but it's just up to us to decide what to do next.

665
01:19:00,680 --> 01:19:06,840
I want to suggest that I gave these courses a look, and in my opinion like most of these scenes are

666
01:19:06,840 --> 01:19:13,800
covered in Fastie, except for a security AI. So and personally, I'd also like to like this

667
01:19:13,800 --> 01:19:19,800
through the, anyways, I will be going through the materials again from part two. So it'll be really

668
01:19:19,800 --> 01:19:25,400
cool to have other people, other people also joining the study group, but again, like I'll wait for

669
01:19:25,400 --> 01:19:32,520
the common opinion. Yeah, it's a fair point, because the Fastie I part to was closed, was only

670
01:19:32,520 --> 01:19:39,080
open to some people. So when we open this to the public, to general public, I think I think it

671
01:19:39,080 --> 01:19:46,840
makes sense to kind of do it again. I tentatively put like one lecture per two weeks, because those

672
01:19:46,840 --> 01:19:53,160
lectures are quite heavy, I guess. Very dense. Yeah. So maybe that's an idea to do like one lecture.

673
01:19:53,160 --> 01:19:59,240
So we can meet every week, but then we can split the lecturing two halves and then talk about one

674
01:19:59,240 --> 01:20:12,200
lecture for two weeks, maybe. Okay. Excellent. So let's continue the discussion on the slack.

675
01:20:15,400 --> 01:20:21,960
And next week we still talk, Fastie, sorry, full stack deep learning, and I'm going to move

676
01:20:21,960 --> 01:20:31,000
the ethnic presentation for next week. So at least we have also agenda. Also the Daniel and

677
01:20:31,000 --> 01:20:35,400
Abin, they also wanted to talk about deployment of deep learning models, but then they put

678
01:20:35,400 --> 01:20:46,040
it on hold. So maybe they also talk about this in future soon. Okay. Excellent. So thank you for joining

679
01:20:46,040 --> 01:20:53,640
today. Talk to you next week. Thank you for hosting, Michael. Thank you.

