Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we continue our train AI series with a conversation with Sarah Ayrney, director
of data science at Salesforce Einstein.
Sarah and I sat down at the train AI conference to discuss her talk notes from the field,
the platform people and processes of Agile Data Science.
Sarah and I dig into the concept of Agile Data Science, exploring what it means to her
and how she's seen it done at Salesforce and other places she's worked.
We also dig into the notion of machine learning platforms, which is a keen area of interest
for me.
We discuss some of the common elements we've seen in M.O. platforms and when it makes
sense for an organization to start building one.
As you may know by now, we're celebrating the second anniversary of the podcast this
week.
Have you found this podcast useful? If so, we want to hear how.
Submit your written comments via the page at twimble.ai slash 2av or leave us a voicemail
at 636-735-3658.
We'll be sharing a few of your stories with your permission, of course, and a special
podcast episode celebrating the Twimble community.
Again, the second anniversary page is at twimble.ai slash 2av.
Finally, a shout out to our friends over at Figure 8 for their sponsorship of this week's
series.
Figure 8 provides the essential human and aloop AI platform for data science and machine
learning teams.
The Figure 8 software platform trains, tests, and tunes machine learning models to make
AI work in the real world.
Learn more at www.figure-8.com.
This episode was recorded live on site at the train AI conference, so there is some unavoidable
background noise, and now on to the show.
All right, everyone.
I am at the train AI conference, and I've got the pleasure to be seated across from Sarah
A. Ernie.
Sarah is Director of Data Science with Salesforce Einstein.
Sarah, welcome to this week in machine learning and AI.
Thank you so much for having me.
I'm really excited to jump into the topic of Agile Data Science, which is what you were
speaking about here at the conference today, but before we do that, tell us a little bit
about your background and how you got into ML and AI and data science.
Absolutely.
Actually, I started off way back in undergrad when I had the opportunity to work on my
two passions, biology and computer science together.
They opened up a major bioinformatics, and back then it was an ascent field, but I was
able to do some research in cancer biology, kind of on leukemia and detecting patterns,
and then spent some time doing remote research in Caltech on DNA binding and trying to understand
if there were patterns we could recognize there, and while I didn't have a specific path
and decided to go to grad school for biomedical informatics, kind of building on that.
The lab that I ended up joining was a AI lab, although it was completely focused on
bioinformatics, it still was generally part of that department, and so I got a little bit
more exposure, and started doing things again related to DNA and genomics, but as any
grad student at Stanford started company on the side.
That's not cliche at all, but it was actually with a good purpose, at the time, actually
Obama had announced the stimulus grants, and it was over one of these late-night conversations
with professors that we realized there was this opportunity to support other researchers
that didn't necessarily have all of the things required in order to complete and apply for
these grants, where they might have really strong background in some biological portion,
but didn't have the informatics side of it.
So originally we thought, well, we would offer consulting services to them and kind of
stick to our field, and turns out that space of consulting is a lot bigger, and our group
ended up doing things well outside of that.
So I did some work predicting what websites people would be interested in browsing, and
kind of completely went off the rails from the biomedical background, but all related
to the stimulus grants are totally a field.
No, it was, it's almost as if you put out a press release on the topic, and it just
snowball from there, and people kind of hear that you're available and around.
So no, it ended up coming through other avenues, it was very interesting.
There were a lot of things that originally started off really around grant writing,
and helping startups that wanted to apply for these small business innovation research
grants.
SRS, yeah.
So, yeah.
So people...
Been there.
So it's interesting, because I think people apply to them when they hear about them,
and if they're not from an academic background, don't necessarily know how to write these
grants that will be received well by an academic audience, and so our experience on kind
of academic grant writing was already very helpful.
The informatics part was maybe only a portion of it, and then eventually it moved outside
of the healthcare realm, and I realized I had just passion and data in general.
And although when I was graduating, I still knew that I cared about healthcare and doing
something in that space, so I was completely ready to let go of it.
I wanted to leave it a little bit open, and so I joined a software company where I was
actually more or less a data scientist for hire.
I worked on the particular distributed database that the company Greenblum actually was selling,
and working with my customers with the expertise in biomedical and chromatics, I'd be able
to speak their language, but still really be doing raw data science.
And a lot of the time, it wasn't necessarily in my area of expertise, but it was somebody
who spoke their language.
And there again, over time, I drifted from that background, started doing work on jet
plane engines and banking fraud, and it was just all over the place.
So that was great, and it was exciting.
But what I really wanted to do was deploy models at crazy scale to my customers, and so
Salesforce really offers that opportunity to build models in this really amazing platform
that allows me to just reach a lot of people, so that they are empowered to use AI.
And I can do the work around in innovative data science on the other end.
Before we jump into the agile data science element, Einstein, and a lot of ways Einstein
is starting to feel like Watson, for me, is like this umbrella brand that covers so many
things, like in your words, what is Einstein?
So the purpose of Einstein really is just to democratize AI.
So Salesforce has many customers, and any customer, regardless of size and industry, should
be able to harness that power.
And so the goal is to allow our customers to use AI to have better interactions with
our customers to make them more predictive.
And so we have products, some packaged apps that I've worked on like predictive leads
scoring that allow sales rep to understand which of the sales leads are likely to convert
to an opportunity based on the data that they have that have historically converted.
So while it might sound like a big umbrella, there are some very specific use cases for
which we have packaged apps.
And then there's a product that I'm currently working on the Einstein prediction builder,
which is really intended to, again, democratize AI by allowing our admins to set up things
that they want to predict, explicitly some prediction that we don't know they'll want,
but they would like to have to augment what they're currently able to have their users
do.
So we have, you know, Salesforce admin, I do some kind of forecast, and I want to predict
the close date on a deal or something like that.
And I can train using my own historical data and maybe some of yours and so now it is
absolutely.
We are, we are very strict on kind of focusing on single tenant, being able to leverage
its own data.
So only my data, but the example is the kind of thing that I might be able to predict
using this platform.
Yeah.
Well, we're moving forward on right now in prediction builder are binary classifications
and regressions and the goal is to allow you to build these predictions and we're working
on making the UI really intuitive and allow the admins to do this in a way that's making
it easy to set up and then following on, allowing them to inject those results directly
back because sort of going back to my talk a bit today, there's this part around AI
and certainly when I was working at my previous job, where we spend a lot of time building
models without thinking about how to get them in front of our customers in the end.
And so I spent all this time building a model thinking it's really great and then I'm
ready to push live to production and that doesn't go anywhere.
It just kind of sits with me and that's what's amazing with Einstein is really that it's
right there.
You're going to be able to have it immediately available when it's done building.
So agile data science, what does that mean for you?
Yeah, well, my talk today focused around that concept and insisting that it's of course
around people in process to make it possible, but there really is a platform underlying
that enables it.
So pivotal, my previous company, there is a practice pivotal labs that is all about agile
and extreme programming.
And when we were part of that team, it was really important for us to try and understand
how to integrate into that process.
And agile has been around, it's meant to be adaptive and to work, but there certainly
is a mindset that comes with how to approach it, how to have sprints, how to have stories,
how to be able to go live and test things.
And it doesn't necessarily translate perfectly in data science.
So what's interesting is when we think about for us trying to push our changes live, it's
important to have a way to not feel once you've built a model that is just going nowhere.
It needs to be connected, so really thinking MVP with what it means to actually add any
model to a product.
And that's exactly where I was focusing was a model that is part of a product.
And then secondarily, making it possible to run experiments so that you're not moving
off into a corner and doing it elsewhere and then figuring out how to translate it over.
So really that you have frameworks in place and for Salesforce, we have this platform that
makes it possible for data scientists to use services to get access to what they need
and to share algorithms, to share future engineering steps, and to allow us to run experiments
without having access to data, but with understanding how performance the results might be at
the end.
So setting aside what the customer might use, how do you kind of express agile internally
and the way you do data science within Salesforce?
Yeah, so the team that I run, the prediction builder team, we have multiple models in production,
but we use one underlying code base that automates the entire future engineering process.
So it's AutoML that we use.
And the platform team as a whole is responsible for having services around, you know, safe
access to data around making compute scalable and possible.
And these elements around automated machine learning are really where we get to implement
that agile methodology really cleanly.
So when you have these shared repositories, what that means is if you have models in production
and you can continuously monitor their performance, which is really, really critical, you're able
to understand where there may be a problem, where models are not performant and be able
to go ahead and iterate and come up with solutions to it.
So some of the examples that I gave was, you know, you see a drop in AUROC and you decide
that you actually want to do something about it and, you know, assuming you have access
to the data, you can do a deep dive and understand what has happened.
Understand if there are some underlying elements, maybe there are fields that are not being
processed correctly.
Salesforce has some very unique problems around leakage detection.
Turns out that when you have a pull of data, that is from a moment where they're setting
up the predictions.
We don't know what came before a certain, yes, no event.
So an easy example is if you were trying to predict churn and the customer has a field
called Reason for Churn and they don't explicitly exclude it, we actually need to detect that
problem because if we're now using Reason for Churn in our model, we'll have a problem.
So those types of things would appear up front in a holdout set that we're doing extremely
well, but over time you would see there's a degradation in performance because of course
you're not doing a good job.
So many of the new data sets that were in that scoring open set and so it's that kind
of monitoring and identifying of problems to which we then have to create resolutions.
So if it's introducing a new test to find leakers, if it's coming up with a new way of processing
text, if it's, you know, at some point you can imagine wanting to use sentiment as just
a feature that you engineer out of any text field that comes in.
It's really having that shared repository being able to add that new feature.
So taking the time, building a new way of engineering it, adding it on to that library or
that service, and then iterating, that's sort of one portion.
There's like the user story around implementation.
There's also really that platform though around how do you now run an experiment?
So if you have a new feature that you're adding, and in our case you can have so many
customers that are building models using your underlying AutoML, you would need to understand
what the impact is of adding that feature to any one of your customers.
And so being able to do that in a way that doesn't require us to look at data, but allows
us to run an experiment at scale to understand what the impact would be.
Are there any regressions that happen for lack of a better term in our model performance
across the board?
So to make sure I understand that you've got these customers, customer data, it's all
single tenant.
It sounds like you're not testing against it, but you are.
What exactly are, how exactly are you doing that or what are you doing?
Our team is not able to do is so in general, we can't look at the data, so trust is our
number one value across the board it sells for us, and so we cannot request access that
is not our data.
But can you deploy models to it that don't return the data and test statistically?
Yeah, so you can understand it on aggregate level.
What is your performance?
Is there a change in performance?
And we do have customers that we might pilot a product with where we can work and understand
what's going on.
They can request investigations, but it's not a system where we need to be able to go
in and look at every single model.
Similarly, although Salesforce is at such a massive scale, any company will have multiple
models in production at some point.
I don't believe any company sets out to have exactly one model.
And it's unlikely that at some point, every model is going to have such a heavy data science
component that rather than monitoring and alerting to a degradation performance, you would
want to be able to just have multiple models in production and probably a small number
of data scientists supporting those efforts and iterating.
So having a platform where you have a shared set of services for engineering and for understanding
that if you add a feature or if you change an algorithm or you decide to tune parameters
in a certain way, understanding the impact across the board becomes really critical.
And so having those monitoring services there, although it's really important for Salesforce
is extremely important for any company that's out there.
I'm envisioning, when you say platform, I'm envisioning something analogous to like Uber's
published quite a bit on Michelangelo there, internal platform, and some other companies
have.
Netflix talks about their platform a lot too, it's really great.
LinkedIn, I mean, a lot of companies have spent time, I think what's so unique about
what Salesforce does is our platform is around running multiple models for our customers.
And it's really around taking what all of these companies have done and the number of
data scientists and engineers that go into building those platforms and making it accessible
to our customers so that they can benefit from what it is that we have done and be able
to access a few clicks instead and sort of know that you have that data science prowess
behind it and people that are paying attention to constantly improving and making things
better on your behalf.
So the first of these elements is a repository and you've mentioned a couple of things that
might be in that repository, models and features.
What else is in that repository?
So in general, and again, this will vary by companies that are building for their purposes.
If it's, for example, a single application that you have, there's always going to be an
element around data.
So how is it there?
You're storing data.
How are you indexing it?
How do you access it?
Data scientists don't want to be paying attention to like what folder is this living in
or what table or whatever the history is.
So what we do is we build APIs to let us request data in a certain way so that it can be
again automated, there exists a customer that wants to use the services and therefore being
able to reference by APIs and running code against that is then possible.
And although again, not everyone is operating at Salesforce scale, no data scientist necessarily
wants to spend time thinking about how data moves in and out.
But those services need to be there to make it possible and to make a possible index
what's happening, look for drifts, understand how to monitor that scores are being pushed
back.
The scores have changed over time.
All of these elements around data services, monitoring services and then actual modeling
and feature engineering as well.
So interestingly enough, this topic is timely for me.
I just organized an event last week where we spent quite a bit of time talking about
this notion of, you know, machine learning platforms and agile data science and things
of that like and one of the questions that came up is, you know, for a company that is
not necessarily on a platform like Salesforce, but is doing data science, you know, when
do they start to collect some of this stuff into a platform?
When does it make sense to build a platform?
If you're a, you know, an internet company that's probably earlier than if you're a traditional
enterprise, how do you know when it's time to start consolidating some of this functionality,
building repositories, building, monitoring, framework, stuff like that?
So monitoring is from the start.
Like anytime you have an application, you're, you're going to monitor everything that's
happening on those apps.
So as soon as you're wanting to add a model to it, you have to have monitoring around
your models.
It's to treat a model any different from another feature of your app.
And if does your, your, your data scientists is service because they want to know what's
happening, not by always requesting access and moving over, they need to be alerted to
an issue the same way anyone else would be.
Also for allowing them to triage the problem more quickly, I understand that in general,
there might be a question about waiting until you have multiple models maybe or some opportunities
for sharing, but I argue and that was sort of one of my take homes on the talk today.
Sort of my main point is to plan for multiple models always.
I cannot imagine an organization that doesn't commit to having more than one model.
And by doing so, really doing some upfront planning, certainly at my last job, there
were many companies that were kind of in this position of having a lot of smaller organizations
within that could have benefited from sharing, from understanding.
And of course, there's always a question of investment.
And I think it's a struggle when you don't invest time in the platform, I was just talking
with several people today about the fact that there's an expectation that you should
have good return on investment for your modeling and for the data scientists to hire and everything
that's there.
And of course with that, they want to have the normal agile processes in place of having
waste assess the risk and how much longer and like, when am I going to see something
from it? And as a data scientist, and certainly coming from graduate school, we think of it
as like, oh, we're entering a research project.
We have absolutely no idea if this will work and, you know, let us go work on it for three
months, six months a year and we'll get back to you.
And that may be one way to approach it, but inevitably that leads to all around frustration,
both on kind of the business that is trying to get something.
And also the data scientists, who then once they're ready, don't necessarily know how
to go to production.
And so if we really want a more agile approach as a whole, you must build platforms to support
your data scientists' ability to iterate, ability to understand how well something is
performing and ability to fail early.
Because if we don't commit to having data pipelines, monitoring services, waste to deploy
models from the start, we will inevitably find ourselves in the position of every single
time starting over, rather than building for multiple apps from the beginning.
Do you feel like data science is standardized enough across organizations and use cases
that we can do that, right?
Like a classic problem is, you know, building a platform before you're, you know, the problem
that the platform is trying to solve, right?
You build all kinds of features, oh, I need a, you know, model repository with versioning
that does XYZ.
This is what the API looks like, and then you go to actually build something and it doesn't,
you know, it's like you're building the bridge from both sides and they don't meet in the
middle.
Oh, I love that.
There's like a famous bridge between Germany and Switzerland that every time I drive by
with my dad who lives there, he always talks about how the German side and the Swiss side
agreed to build it.
And when they came to me in the middle, they were off by a huge number because Switzerland
measures from elevation from the Mediterranean and Germany from the North Sea.
And that was the reason why that happened.
It's like very funny.
It's like starting a race at zero and one.
No, there's like a, there are, you can find blogs about this on that.
It's very funny.
But I totally understand what you're saying.
And absolutely at the outset, I, and I should have done the same here.
I level set my talk with saying, look, there are different ways of doing this.
Different types of models.
And my examples are, there are models that are informing strategic decisions and my classic
examples are from, you know, back in my biomedical informatics days, they're, you know, trying
to identify the next drug target to go after.
And so we're doing a bunch of data science to understand where to head next with our research.
And they're kind of these others that are like doing automated decisions and they're
ones that are like augmenting an app and maybe informing something.
So like predictive leads going, we'll dislate, convert.
And you're absolutely right, depending on what it is that you're building, you would want
to focus on the different elements first.
But across the board, there will always be things that make sense like how to bring your
data in a way that's access controlled in a way that is also accessed via APIs.
I cannot think of a reason to not want to do that.
There's no reason to have to like, imagine what folder or table or scheme, it's a, it's
painful to think of it that way.
There's a corollary there that the data lake is not enough.
Look, it's, it's all iterative.
You know, we all started on a path together on a journey.
And what I really hope and the hope of my talk was to share how we're, we're moving forward
on this.
And, and to learn from each other with it.
So I, I know that again, during my consulting days where I was asked to do these projects
and then kind of convince the organizations that what we had done was good enough.
And now let's, let's figure out how to build an app.
That was interesting because it was almost like the companies needed convincing that data
science was going to do something good.
And if we can all agree that there will be value in models, if only we accepted it, can't
we start by saying that we're looking to build a use case into something.
And let's come up with some, let's create a roadmap for it and then decide what to execute
against first in an MVP mindset.
And the other element is that that agile request that we have of data scientists does need
to come with that commitment to support them and to support the differences between data
scientists and software developers.
They have a lot of things in common around wanting to monitor their models or their, their
services around wanting to, you know, understand how to improve around trying out new technologies
or algorithms, but they just have slightly different ways that those need to be solved.
And platform commitment needs to be there to make that possible.
Let's take it from a software development perspective.
I'm a software engineer.
I'm building some kind of component or service, microservice, whatever.
It's part of agile, part of DevOps.
I don't know, part of whatever you want to call it, like the current best practices.
I'm also building the tests and responsible for the tests that, you know, would ultimately
monitor the performance of my service, right?
A big part of DevOps is I also own that service and production and I'm responsible for
its upkeep, so it's in my, it's to my advantage to build the tests that will allow me to
be able to do that.
Data sciences is a bit different at most places, I think, because there's this bit of a,
I don't know if you want to call it a wall, that may be too strong, but there's, you know,
the person who's building the models isn't the person that's putting that in the model
into production and is responsible for, they're responsible for different elements of it,
right?
Data sciences is responsible for the performance of that model over time.
Are they also building those monitoring tests or is the engineering team that's implementing
the model, building those overall monitoring tests?
Like, where does that sit?
Those, I think those are hard questions to answer as a whole, I think, in the abstract
meaning.
Yeah.
Yeah.
So that's elsewhere for what is that?
Well, so I think one thing is to consider that I guess this, this concept of like completely
isolated teams is, is difficult.
I think it's important for data science and engineering to sit extremely closely in,
in prediction builder, for example, we have a very hybrid team, we have, we have a designer,
we have front end, we have data scientists, and our data scientists are very good engineers
as well, in the sense that they've learned how to write code or at least work on these
libraries that are able to run in production.
And part of that is because of the way we have built that platform, we can run that code
and run experiments using that code and make changes.
And if you just think about it as adding a component in and not necessarily what you're
describing, which is kind of completely offline building and then, okay, it looks good on
the sample data and then what you're saying throwing it over the wall, but instead having
the ability to run it in those same environments and make sure that it's performant.
And of course, having a team that's dedicated to understanding, you know, scalability concerns
of those exist, to make sure that those things are all very intertwined and that monitoring
happens in the same way that we have the same alerts for, you know, SLAs are not met,
well, your SLAs are not met for models aren't running fast enough, they're not reaching
completion fast enough, you're not, you know, pushing back scores, you've seen a drop
in the number of scores, you've seen your valuations have changed.
I mean, you can kind of hijack some of the same tools, or you can build your own, but
I would suggest looking at how suitable existing infrastructure is and existing tools that
exist for the rest of your app and seeing how many of those could also now support data
scientists as well.
I think there is a desire across the board as well to integrate these teams into stop
treating them as completely isolated and I really think part of it is by introducing them
to that same culture, the same agile processes, then maybe they wouldn't feel the need to
feel so separate as well, but you really do need to support them in that change.
And I think that's maybe an interesting segue to people in process, like we've talked
a lot about technology, that's only one leg of the stool, so to speak, you've certainly
touched on people in process throughout, but did you have a specific message in your talk
around the people aspect of all this?
Yeah, so some of it was really around my own journey from starting an extremely research
oriented world, and to be honest, back in grad school, I wish that somebody had introduced
agile to me, I would have benefited from really not spending time on obsessively thinking
about one particular problem and instead creating an MVP and iterating, and probably would
have also helped in terms of when do I publish a paper if you think of that as your quote
unquote release.
So it was around that journey on acknowledging the similarities that I have with developers
around the needs that I have, how they are served by either a process or by a monitoring
tool or tests that they write, and how those aren't identical for me, but there are things
that I want that, again, bring the platform in, if I could only modify those elements
I might be able to get, but also a little bit around what it means to have a user story
for a data scientist, so sort of acknowledging my asks and my desire to want to push live,
thinking then next on the process side, given that there's a platform in place, how can
I now go through identifying opportunities where the team focuses on things that are
near-term and things that are long-term.
So we have a backlog of things that we want to achieve, we have investigations that are
ongoing, and those investigations follow a certain pattern, which is identify a source
cause, and from that, create user stories around solutions that you want to implement,
and those are sometimes very, very tangible, very easy, tuning a hyper parameter, trying
to understand if you could add a new feature, maybe there's a bug, something I give an example
of like a text field that's blank, should be treated as a null, something very explicit
like that, very tangible, and you can have this running backlog of those items, but we
also acknowledge that at any stage there could be a lot of open opportunities.
Maybe you want to try new word embeddings, or you want to try a segmented model, and
those are larger problems that require longer-term efforts, and to kind of balance out those
near-term data science focused and require data science chops to identify the issues against
the longer-term or innovative opportunities, we have architects on the team that are
able to take on those bigger meteor problems to understand how do we tackle using something
completely different, and choose when to promote that into something that will push into
production.
So you do need to have a large enough team to support both sides of that.
And so is this, are you identifying a role that's specifically a data science architect,
as opposed to like a traditional engineering architect, or developer architect?
So yeah, I guess the role of architect is very specific, which is somebody who's at the
level of solving these meteor problems, not necessarily the architect that you're mentioning,
but they do have a solid understanding and ability to work and communicate with a engineering
architect.
So yeah, is this architect on the data science side, is that even the right way to say it,
is someone that's come up through data science, what I've not heard much of the notion
of a data science architect, if that's what we're even calling this?
Yes, I mean, that's sort of the term we use, and maybe that's too specific to us, but
it is the folks that serve that role are individuals that have strong machine learning chops and
strong engineering chops to the sense that they can understand, you know, where something
would fail, where there might be a problem with scalability, where there might be a whole
new paradigm that needs to be introduced.
And at times, folks need to come together and learn, but there are people that are essentially
given the space and time to be able to go after these other problems, as opposed to the ones
that are more focused on the MVP near term.
That we have kind of our productivity zone and our incubation zone, and yeah, a team
needs to be large enough to support it, but it's important.
Makes perfect.
Yeah, and how about on the process side, are there observations that you've made from
a process perspective that makes this all more efficient, more smoothly flowing?
Yeah, aside from actually using, you know, your traditional backlog and stories, what
we do that, I think, again, borrowing these techniques from elsewhere, really around
doing prioritizations around, you know, what is the value of going after a certain element
versus an ease of implementation to allow us to triage where to spend our time?
And value is something really hard to assess.
How do I know what the value is going to be of introducing a new algorithm?
How do I know what the value is going to be of, you know, maybe going after a new way
to process texts or engineering features, what I can do, or, you know, something like
a segmented model, what I can do is look at my models and how they're currently failing,
what, you know, are there a lot of, a lot more text fields in one of my models, and therefore
I believe that if I make a change, it would have a lot of impact.
And sort of measuring where I believe there would be a big shift as one way to prioritize
my backlog, and again, around the ease of implementation really focusing around what
will be a longer term and more risky element that I need to work on, but I won't have
the same kind of immediate need to shift.
Have you made any attempts to, you know, taking all these, you know, the technology approach,
platform approach, the people approach, the process approach?
Have you made any attempts to try to characterize the net advantage over not doing these things?
I mean, I'm imagining they've all evolved organically, and so the answer is probably
no, but you know, certainly there's some degree of overhead associated with these different
steps.
How do you continue to justify it?
Yeah, so the most obvious, I guess, metric that you could use is how quickly your models
are improving over time, and then how quickly you're able to ship.
So I know at previous companies, there might be like the actual how often we have new releases
that could be used.
What's interesting is in data science, of course, it's a little bit challenging because if
I release a new model, that could impact my end customers.
So, but what we can do is understand how much improvement we're getting on in performance,
because we do have models and we do have summary stats on how they're performing and we can
see how they've changed over time.
And of course, it's a bit tricky when you're in an early phase or in a company that has
a smaller number of models in production, but if you can see how quickly you're making
progress or improving outcomes or, you know, in the instance of something more explicit
like getting customer feedback, you know, we might have like a time to resolution that
their improvements there, that those are metrics that you can capture at the end of your
sort of unwilling to say, like, you know, how quickly am I releasing as you're a measure?
Any other topics that you touched on in your talk that we haven't covered yet?
Well, I think overall, at Salesforce, we have this really cool AutoML pipeline that we've
been developing that really focuses around future engineering and, you know, automating
the process of selecting models.
And I know that I think that term AutoML is something that isn't always so well received
by data scientists as a whole, I think I've noticed that at times there's this question
around, oh, is like this automation way of data scientists, essentially.
And it actually isn't at all.
And the reason why I think it's such an important concept for everyone to introduce is that what
AutoML really represents is harnessing your data scientists, repeated future engineering
techniques, repeated analyses, finding opportunities to do these things.
And instead of having everybody come up with a new, you know, like, stop word removal,
or language class virus sentiment, now share those.
Work together on saying, I would like to use, you know, this shared library and let's
instead focus together on how to automate that process and identifying ways to, you know,
produce more features and instead focus on the fun, hairy problems, like, what's the
next set of features that I want to engineer?
So it's not really such a scary space.
It's actually very empowering, I feel, for data scientists to consider what they can contribute
there.
It's been a great chat.
Thank you so much.
Anything else you'd like to mention before we close out?
No, I'm so appreciative of this opportunity.
Thank you so much.
All right.
Thank you.
All right, everyone, that's our show for today.
For more information on Sarah or any of the topics covered in this episode, head over
to twimmel.ai slash talk slash 143.
Thanks again to figure eight for their sponsorship of this episode.
To follow along with the train AI series, visit twimmel.ai slash train AI 2018.
And finally, show us some love for the podcast's second anniversary and share how it's been
helpful to you over at twimmel.ai slash 2 AV.
Thank you so much for listening and catch you next time.
