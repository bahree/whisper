WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twomo AI Podcast.

00:13.400 --> 00:18.240
I'm your host Sam Charrington.

00:18.240 --> 00:25.560
Hey, what's up everyone, a few quick updates from Twomo HQ.

00:25.560 --> 00:29.880
As some of you may have noticed, we recently relaunched the Twomo newsletter.

00:29.880 --> 00:32.520
If you've already signed up, awesome.

00:32.520 --> 00:37.440
If not, well, you're missing out on weekly podcast updates, recommendations from the team,

00:37.440 --> 00:38.720
and much more.

00:38.720 --> 00:45.040
Head over to twomoAI.com slash newsletter to get signed up.

00:45.040 --> 00:50.200
After you check out today's podcast, I encourage you to check out our latest demo casts.

00:50.200 --> 00:54.440
This time around, I was joined by Vila Toulos, who you might remember from our conversation

00:54.440 --> 01:00.440
late last year about Netflix's recently open source library, Metaflow, for building

01:00.440 --> 01:03.880
and managing real-life data science projects.

01:03.880 --> 01:08.040
Based on the data we've seen with our previous couple of demo casts, we've decided to no

01:08.040 --> 01:13.240
longer publish the audio-only versions of those here in the main podcast feed.

01:13.240 --> 01:18.960
So you'll need to jump over to twomoAI.com slash Metaflow to check it out.

01:18.960 --> 01:25.120
And now on to the show.

01:25.120 --> 01:26.120
All right, everyone.

01:26.120 --> 01:28.200
I am on the line with David Adybo.

01:28.200 --> 01:35.640
David is co-founder and CTO of Analytical AI, as well as a Kaggle Grandmaster.

01:35.640 --> 01:38.000
David, welcome to the TwomoAI podcast.

01:38.000 --> 01:39.480
Yeah, thank you, Sam.

01:39.480 --> 01:41.040
Happy to be here.

01:41.040 --> 01:42.040
You know what?

01:42.040 --> 01:43.280
I am really looking forward to this chat.

01:43.280 --> 01:48.080
I think this is the first time I've had a sibling on the show.

01:48.080 --> 01:54.680
I interviewed your brother, Steven Adybo, back in July of last year, where we were talking

01:54.680 --> 01:56.640
about retinal image generation.

01:56.640 --> 02:02.040
And he was up on Twitter recently, talking you up and your conquests on Kaggle.

02:02.040 --> 02:04.840
And I thought, man, I've got to get this conversation as well.

02:04.840 --> 02:09.920
So congratulations for being the first sibling, at least to my knowledge on the podcast.

02:09.920 --> 02:13.160
Yeah, thank you.

02:13.160 --> 02:17.320
Why don't we start out by having you share a little bit about your background.

02:17.320 --> 02:20.800
How did you come to work on machine learning?

02:20.800 --> 02:24.680
When I started my PhD, I was interested in machine learning.

02:24.680 --> 02:29.600
I wasn't quite sure where to go back then, so there was a lot of literature review, a

02:29.600 --> 02:32.160
lot of just digging around.

02:32.160 --> 02:37.480
And back then, you know, I read a lot of papers, I kind of tried to get a sense of where

02:37.480 --> 02:40.680
to go to get useful information.

02:40.680 --> 02:45.800
And Kaggle kept creeping up and coming up in various things I read.

02:45.800 --> 02:51.680
And so I finally joined Kaggle, and it kind of put, it gave me an opportunity to put

02:51.680 --> 02:56.440
some of that theoretical background into practice.

02:56.440 --> 02:59.960
And a lot of things made a lot more sense when you actually practice.

02:59.960 --> 03:02.560
So what were you studying for your PhD?

03:02.560 --> 03:09.520
I was looking to investigate deep learning and medical imaging and how the advances in

03:09.520 --> 03:13.000
deep learning at the time could be applied to various medical imaging problems.

03:13.000 --> 03:16.280
And that sounds like an interest that runs in the family.

03:16.280 --> 03:21.000
Yeah, it does.

03:21.000 --> 03:25.080
You know, besides from any kind of genetic contribution, what sparked your interest in

03:25.080 --> 03:26.600
that?

03:26.600 --> 03:31.120
I wanted, actually, the first neural network I ever, I wanted to see if I could build

03:31.120 --> 03:34.280
something to recognize a face in an image.

03:34.280 --> 03:40.520
And this was actually before I started my, I selected my thesis topic.

03:40.520 --> 03:45.400
And so that was really how I got it really interested in machine learning because it

03:45.400 --> 03:48.440
was something I was kind of blindly doing.

03:48.440 --> 03:53.440
And you know, as you're digging around, I wrote my first neural network in C sharp, which

03:53.440 --> 03:55.760
was it worked horribly.

03:55.760 --> 03:58.760
And this was in 2012.

03:58.760 --> 04:04.400
And you know, that kind of made me realize that you read about how neural networks should

04:04.400 --> 04:06.000
work and all these things.

04:06.000 --> 04:11.400
But you never really, I never really knew at the time what, what the appropriate frameworks

04:11.400 --> 04:16.960
to use and, and, and how you go about it, basically.

04:16.960 --> 04:17.960
So.

04:17.960 --> 04:21.320
And so what, why C sharp, were you a Microsoft developer?

04:21.320 --> 04:26.960
Yeah, I was a, I was a Microsoft enterprise developer, I had done all the Microsoft

04:26.960 --> 04:27.960
specifications and stuff.

04:27.960 --> 04:29.720
So I was pretty good at C sharp at the time.

04:29.720 --> 04:32.920
So I said, let me, let me try to do this as C sharp.

04:32.920 --> 04:37.160
And it was just the absolute wrong idea and wrong approach to use at the time.

04:37.160 --> 04:44.400
But, you know, if you, if you have a hammer, you know, you tried to, so if I'm putting

04:44.400 --> 04:50.960
the pieces together correctly, you were the Microsoft enterprise developer before you went

04:50.960 --> 04:54.760
back for your PhD, what prompted you to go back for your PhD?

04:54.760 --> 04:58.840
Well, it's something I always wanted to do, you know, after I got my master's, I worked

04:58.840 --> 05:05.120
in industry a little bit and I always knew I, I needed to kind of pivot back and, and

05:05.120 --> 05:06.320
complete the PhD.

05:06.320 --> 05:11.600
So that was kind of, it was something that was in the plans before I, but I wanted to,

05:11.600 --> 05:14.960
after being in school for so long, you want to work and gain some experience.

05:14.960 --> 05:15.960
So.

05:15.960 --> 05:16.960
Yeah.

05:16.960 --> 05:17.960
Nice.

05:17.960 --> 05:24.240
And so you started working on neural networks in C sharp and kind of, you know, banged

05:24.240 --> 05:27.480
away with that hammer that you, you had.

05:27.480 --> 05:30.840
And it was, it was the absolute wrong, wrong thing to do.

05:30.840 --> 05:34.280
I just need nowhere to go or to start back then.

05:34.280 --> 05:40.120
You kind of were pursuing, you know, this formal, you know, research and education with

05:40.120 --> 05:48.920
the PhD, but then you realized that there was kind of a practical compliment to that in

05:48.920 --> 05:49.920
Kaggle.

05:49.920 --> 05:50.920
Yeah.

05:50.920 --> 05:56.900
Um, how did you start with, you know, once you, you know, created an account and joined

05:56.900 --> 06:02.880
Kaggle, quote unquote, like how did you actually start did you just jump in and do a competition?

06:02.880 --> 06:09.520
So I actually created my account for the first time in, I think it was 2014 or 2015.

06:09.520 --> 06:14.640
And I didn't actually, I looked around, I read the, I didn't actually start competing

06:14.640 --> 06:15.920
until a year later.

06:15.920 --> 06:16.920
Okay.

06:16.920 --> 06:25.120
So I first joined and then I, I just spoke around and then later on, I kept working

06:25.120 --> 06:27.840
on my stuff and I got better at it.

06:27.840 --> 06:31.200
I kind of identified the right frameworks to use.

06:31.200 --> 06:33.880
I got a little bit more comfortable.

06:33.880 --> 06:40.440
And then I think I did my first competition in, in 2016.

06:40.440 --> 06:46.440
And, and that was, I think the data science bowl at the time that one was for trying to

06:46.440 --> 06:57.640
detect the volume of, of, of ejection in, in heart, um, cardio grams or, so that was

06:57.640 --> 06:59.240
my first competition.

06:59.240 --> 07:00.240
Okay.

07:00.240 --> 07:04.600
And that competition, I did okay, you know, but I didn't do really well, but it was good

07:04.600 --> 07:06.640
experience to kind of get my feet wet.

07:06.640 --> 07:07.640
Mm-hmm.

07:07.640 --> 07:08.640
Yeah.

07:08.640 --> 07:12.240
And were you, we had you partnered up with folks for that one or were you working independently?

07:12.240 --> 07:13.240
Yeah.

07:13.240 --> 07:21.280
We have a guy that we're, Jason Zeng, we're both in the same program and we're both doing

07:21.280 --> 07:23.040
kind of deep learning and medically mentioned.

07:23.040 --> 07:25.520
So we both went into that competition together.

07:25.520 --> 07:26.520
Okay.

07:26.520 --> 07:27.520
Yeah.

07:27.520 --> 07:28.520
Cool.

07:28.520 --> 07:37.720
And so that was 2016 fast forward to 2020 and you've seen quite a bit of success and

07:37.720 --> 07:44.440
Kaggle competitions, can you talk about some of the, uh, your more recent results or at

07:44.440 --> 07:47.080
least the results that you're most proud of?

07:47.080 --> 07:48.080
Yeah.

07:48.080 --> 07:53.440
So, um, shortly after that competition, there was another one that was, uh, for also medical

07:53.440 --> 07:54.920
image and related competition.

07:54.920 --> 08:00.000
It was for segmenting ultrasound images in the neck.

08:00.000 --> 08:04.800
And there was, uh, fortunately for me, um, that competition was actually the competition

08:04.800 --> 08:05.800
I did best in.

08:05.800 --> 08:11.960
So there was a, there was a new architecture, a new paper that was written on this, uh,

08:11.960 --> 08:16.840
thing called, it's an encoder, the quarter network is called a unit.

08:16.840 --> 08:21.200
And it hadn't really been used on Kaggle before that architecture.

08:21.200 --> 08:25.240
And I'd read the paper about the architecture.

08:25.240 --> 08:29.640
And when I read the paper, it, because I thought about this too, you know, like how do you segment

08:29.640 --> 08:33.160
things to something I've been thinking about, but when I read that paper, the, the ideas

08:33.160 --> 08:36.200
in the, in the paper made a lot of sense, right?

08:36.200 --> 08:39.840
It was like, and it almost seemed so obvious that what, that was what you were supposed

08:39.840 --> 08:45.960
to do to segment images with convolutional neural networks where you try to preserve, um,

08:45.960 --> 08:51.120
you try to preserve the localization information from the original images and things.

08:51.120 --> 08:53.480
So that paper really sparked my interest.

08:53.480 --> 08:54.480
I read it.

08:54.480 --> 08:56.680
I was, I was a fan of it.

08:56.680 --> 09:00.760
And this competition came up and somebody mentioned it.

09:00.760 --> 09:07.640
And back then, there was a framework called LaZange and, uh, the, and those frameworks

09:07.640 --> 09:13.080
are deprecated now, but back then, that was actually what I did, what I used.

09:13.080 --> 09:16.040
And I, I, I, I said, okay, I'm going to give it a shot.

09:16.040 --> 09:19.840
I'm trying, I'm going to try to implement this unit architecture.

09:19.840 --> 09:20.840
And I did.

09:20.840 --> 09:25.280
And I had no hopes of actually doing well in that competition.

09:25.280 --> 09:26.640
But I said, let me give it a shot.

09:26.640 --> 09:33.600
I implemented it, I trained the network and to my surprise, it worked.

09:33.600 --> 09:37.160
And I was so, I was just so happy that it worked.

09:37.160 --> 09:40.480
I had no idea I was going to finish a second place in the competition.

09:40.480 --> 09:46.400
So, um, so that's probably the one I was most proud of because, um, I had to hack a lot

09:46.400 --> 09:51.880
of things back then, you know, there was, even in 2016, there was, there was not, there

09:51.880 --> 09:57.760
are not a lot of best practices of how you do segmentation, even back then.

09:57.760 --> 10:02.560
So there was this, a lot of the frameworks out there had not specified how you do data

10:02.560 --> 10:09.880
augmentation, you know, for image and masks because when you train, when you train segmentation

10:09.880 --> 10:15.400
networks to avoid overfitting, you have to augment the image and the mask together.

10:15.400 --> 10:19.480
And you have to find the right kind of augmentation strategies and things like that.

10:19.480 --> 10:26.640
So I hacked, there was, I created a good augmentation strategy, a good augmentation framework.

10:26.640 --> 10:30.360
And, you know, I saw a lot of people in the challenge were struggling with this idea,

10:30.360 --> 10:34.880
you know, and they were not probably implementing the right augmentation strategies because,

10:34.880 --> 10:38.920
you know, like I said, this was the first time they used this unit in this competition.

10:38.920 --> 10:42.760
But I was kind of a little bit ahead of the game back then.

10:42.760 --> 10:48.200
And so, yeah, so I finished in second place and it was, it was, it was probably my, the

10:48.200 --> 10:51.080
proudest challenge, I'm, that's awesome.

10:51.080 --> 10:53.400
And that was out of like 950 teams, right?

10:53.400 --> 10:55.720
Yeah, that was out of 950 teams.

10:55.720 --> 11:02.720
Were there other teams that used the unit architecture in that competition?

11:02.720 --> 11:09.440
I think a lot of the teams used the unit, but they were implemented in, they used, they

11:09.440 --> 11:12.160
probably all used the same unit.

11:12.160 --> 11:15.600
And probably that was something that gave me an advantage because I don't think people

11:15.600 --> 11:20.080
had taken the time to learn how to build that unit architecture back then.

11:20.080 --> 11:24.880
So somebody posts that, hey, this is a great implementation of the unit and everybody

11:24.880 --> 11:27.240
just follows the crowd and uses it.

11:27.240 --> 11:31.520
And I think that kind of, if you did something different or you implemented your own and

11:31.520 --> 11:35.480
you kind of improved it a little bit, maybe more than what everybody else was using, you

11:35.480 --> 11:41.480
had a chance of doing a little better than they did if you kind of implemented it.

11:41.480 --> 11:48.320
That's, that's one interesting thing that I find about Kaggle is that it is both at the

11:48.320 --> 11:52.560
same time, you know, kind of inherently this competition, right?

11:52.560 --> 11:58.120
That's what it is, but there's also a lot of collaboration and people sharing kind

11:58.120 --> 12:00.440
of results and kernels and things like that.

12:00.440 --> 12:01.440
Yeah, yeah, yeah.

12:01.440 --> 12:05.760
Yeah, talk a little bit about that dynamic from the perspective of someone who participates

12:05.760 --> 12:06.760
in it.

12:06.760 --> 12:13.200
Yeah, so the kernels are great because it gives, it gives a good starting point like you

12:13.200 --> 12:20.480
can take a kernel and utilize it and kind of quickly get up to speed in what the challenge

12:20.480 --> 12:22.240
is about.

12:22.240 --> 12:27.800
And it gives you, it kind of does a lot of the preliminary hard work you might have

12:27.800 --> 12:32.400
had to do to kind of kind of figure out how to get started.

12:32.400 --> 12:34.560
The next level, yeah, great.

12:34.560 --> 12:37.720
So the kernels are great in that it gets you started.

12:37.720 --> 12:42.960
But being a competition, it's, it's, my approach is generally that if you do what everybody

12:42.960 --> 12:45.280
else does, you're not going to win.

12:45.280 --> 12:50.400
So usually you usually have to take those kernels and try to try to improve them and add

12:50.400 --> 12:55.240
your own unique improvement, optimization just to them because it's a competition after

12:55.240 --> 12:56.240
all.

12:56.240 --> 12:57.240
Yeah.

12:57.240 --> 13:01.600
So the kernels are great and the sharing is great and it gets people up and running

13:01.600 --> 13:02.600
fast.

13:02.600 --> 13:03.600
Yeah.

13:03.600 --> 13:10.240
Like you're, you had the success with the ultrasound nerve segmentation competition

13:10.240 --> 13:12.880
relatively early in your Kaggle journey.

13:12.880 --> 13:16.640
Yeah, it was actually my, my, my second or third competition.

13:16.640 --> 13:19.440
So I was pretty, and that's why I'm also proud of it.

13:19.440 --> 13:20.440
That's pretty encouragement.

13:20.440 --> 13:21.440
Yeah.

13:21.440 --> 13:23.760
It was, I got up there pretty quick.

13:23.760 --> 13:26.360
I think I was, like I said, I was really surprised.

13:26.360 --> 13:27.960
I said, let me implement this.

13:27.960 --> 13:33.120
And it kind of gave me the confidence that, you know, if you just apply your ideas, you

13:33.120 --> 13:36.120
know, you might, you should give it a shot.

13:36.120 --> 13:40.720
Just just give it a shot and, and you never know how it turns out.

13:40.720 --> 13:41.720
Mm-hmm.

13:41.720 --> 13:46.240
You know, it's, it's amazing to think that, you know, just three years ago was kind of

13:46.240 --> 13:50.640
the wild, wild west when it, you know, comes to these kinds of problems.

13:50.640 --> 13:57.400
How was the game evolved over the past three years and not to say that that what your,

13:57.400 --> 14:01.240
your accomplishment was easy, but is it, you know, is it as easy?

14:01.240 --> 14:07.440
Is it still possible to, you know, do the kinds of things that you did in that competition?

14:07.440 --> 14:08.440
Yeah.

14:08.440 --> 14:10.720
You know, talk a little bit about how it has evolved.

14:10.720 --> 14:11.720
Yeah.

14:11.720 --> 14:15.960
So, it's evolved because now, you know, like then it was the wild west.

14:15.960 --> 14:21.440
Now things, a lot of things you could have done in the past again and edge are now standardized

14:21.440 --> 14:23.040
well understood.

14:23.040 --> 14:25.920
So that's good because it moves the field forward.

14:25.920 --> 14:31.120
You know, people have settled on best practices and best strategies.

14:31.120 --> 14:34.160
So it definitely has evolved things that have improved.

14:34.160 --> 14:40.080
It gets harder now to win competitions without really innovating or really looking into

14:40.080 --> 14:45.520
percular things about the data sets or the problem that you can exploit.

14:45.520 --> 14:51.240
So there's definitely, it's not as, it's, I think it's easier to start now than it was

14:51.240 --> 14:57.000
back then because you have a lot of kind of examples and things to go by.

14:57.000 --> 15:02.320
But still being a competition platform, it still requires pushing the limits a little

15:02.320 --> 15:07.920
bit in terms of innovating and finding kind of things that could give you an edge.

15:07.920 --> 15:15.040
So can you give us some examples of, you know, how you've innovated in competitions and,

15:15.040 --> 15:19.280
you know, where you found patterns in the data that you were able to exploit?

15:19.280 --> 15:20.800
How has this played out for you?

15:20.800 --> 15:21.800
Okay.

15:21.800 --> 15:26.960
So the next really good competition I did in was one sponsored by the Department of Homeland

15:26.960 --> 15:34.720
and the TSA for detecting, for detecting threats in the provisioned body scanners.

15:34.720 --> 15:35.720
You seeing the airports?

15:35.720 --> 15:37.120
AKA the newtoscopes.

15:37.120 --> 15:38.120
Yeah.

15:38.120 --> 15:39.120
Yeah.

15:39.120 --> 15:43.920
So, and that was the largest competition on Kegel in terms of price money.

15:43.920 --> 15:47.000
So it was kind of like the highest profile competition.

15:47.000 --> 15:49.720
And we came in, I came in third place in that one.

15:49.720 --> 15:53.720
And that was another one that requires requires money.

15:53.720 --> 15:57.000
The overall purchase was about $1.5 million.

15:57.000 --> 15:58.000
Wow.

15:58.000 --> 16:04.480
So, um, so in that one, we had to innovate because the data set was peculiar.

16:04.480 --> 16:06.000
It was very large.

16:06.000 --> 16:09.400
It was the largest data set ever on Kegel.

16:09.400 --> 16:15.920
And so that required the barriers on that one were required, kind of solving the problem

16:15.920 --> 16:21.960
of dealing with really, really large data and terabytes and, and, you know, you had

16:21.960 --> 16:23.200
to get an edge in that.

16:23.200 --> 16:27.080
And then you had to, the data was also three-dimensional.

16:27.080 --> 16:31.400
And you had to come up with architectures that could deal with three-dimensional data.

16:31.400 --> 16:35.720
So the, so in that, that was another competition where there were no best practices in terms

16:35.720 --> 16:37.920
of how you go about solving this problem.

16:37.920 --> 16:42.200
So if you could, it was one of those, another one of those low-hanging fruits where if you

16:42.200 --> 16:47.560
could quickly come up with an optimal approach, you were more likely to do really well in

16:47.560 --> 16:48.560
it.

16:48.560 --> 16:53.400
So we came up with a very creative architecture that allowed us to perform well.

16:53.400 --> 17:02.120
And so did you join that competition because it looked interesting or did you join that

17:02.120 --> 17:08.240
competition because you saw that there were these issues that would provide kind of a

17:08.240 --> 17:10.000
direction for you to innovate in?

17:10.000 --> 17:11.000
Yeah.

17:11.000 --> 17:15.080
So that competition, like I said, I'll still in my PhD program back then.

17:15.080 --> 17:19.560
My part of my research was dealing with medical imaging data and a lot of the medical imaging

17:19.560 --> 17:24.680
data are three-dimensional CT or MRI.

17:24.680 --> 17:29.120
And I already started thinking about three-dimensional imaging.

17:29.120 --> 17:33.760
So I already started, the architecture that you used was one I already started doing research

17:33.760 --> 17:34.760
in.

17:34.760 --> 17:41.640
I wrote about my thesis for handling the difficulties with the three-dimensional imaging are the

17:41.640 --> 17:47.640
large volumes, and even with the state of the RGPUs we have today, if you want to process

17:47.640 --> 17:54.960
at the original resolution of all these medical images, it's often impossible, right, without

17:54.960 --> 18:01.600
sound sampling or reducing the size of the data to a point where you actually lose useful

18:01.600 --> 18:03.120
information.

18:03.120 --> 18:07.800
So I had started working on some of these problems.

18:07.800 --> 18:13.920
And I'd actually already used the architecture I used on Parkinson's disease data so that

18:13.920 --> 18:22.560
just to see if we could detect or classify Parkinson's disease in brain scans.

18:22.560 --> 18:24.840
So it was something I was working on.

18:24.840 --> 18:29.680
And when that challenge came around, I quickly identified that the data looked very similar

18:29.680 --> 18:34.640
to the medical imaging data I was working with and I already had an approach I'd been investigating

18:34.640 --> 18:40.320
so I said we're just going to crack this method loose on the data and it worked out well.

18:40.320 --> 18:41.320
Nice.

18:41.320 --> 18:44.120
Can you walk us through the method?

18:44.120 --> 18:50.960
Yeah, so it's usually most of the CNNs out there are work with two-dimensional data and

18:50.960 --> 18:57.240
a lot of the methods in deep learning work with two-dimensional imaging and that's well

18:57.240 --> 18:58.240
understood.

18:58.240 --> 19:03.680
But when you come to 3D, the memory requirements, they grow a lot.

19:03.680 --> 19:08.840
So this method I used kind of combined, so you want to learn features in the two-dimensional

19:08.840 --> 19:09.840
space.

19:09.840 --> 19:14.880
But there's also a third dimension which is the third axis where you also can correlate

19:14.880 --> 19:21.120
features across multiple two-dimensional images because the volume is essentially a

19:21.120 --> 19:23.400
step of two-dimensional images.

19:23.400 --> 19:28.400
So what we did was we combined the convolutional neural network, the 2D convolutional neural

19:28.400 --> 19:30.520
network with an LSTM.

19:30.520 --> 19:38.080
And LSTM is a different kind of architecture that kind of models sequences of data.

19:38.080 --> 19:44.560
So and it requires less the input vectors of much smaller than images.

19:44.560 --> 19:48.560
So if you could use the convolutional neural network to kind of learn the two-dimensional

19:48.560 --> 19:54.760
vectors that you need to feed into the LSTM, you could reduce the memory requirements required

19:54.760 --> 19:57.040
to train a three-dimensional volume.

19:57.040 --> 20:03.280
And that was kind of the innovation in the architecture where we were able to.

20:03.280 --> 20:14.680
So it sounds like you're using the, if I get this right, you're using the CNN to do something

20:14.680 --> 20:19.080
along the lines of dimensionality reduction of your two-dimensional images and you're

20:19.080 --> 20:21.080
eating that into an LSTM.

20:21.080 --> 20:22.080
Yes, exactly.

20:22.080 --> 20:23.080
That's what it is.

20:23.080 --> 20:24.080
Interesting.

20:24.080 --> 20:29.000
So by doing that, we didn't have to reduce the resolution of the input images.

20:29.000 --> 20:32.920
So we're able to learn rich two-dimensional futures.

20:32.920 --> 20:38.320
And also, you know, the body scanners, they give you this kind of three-dimensional view

20:38.320 --> 20:40.120
of the person.

20:40.120 --> 20:43.760
And there's kind of a correlation from frame to frame.

20:43.760 --> 20:49.560
If you see something in one frame and you don't see it in another frame, in the next frame,

20:49.560 --> 20:51.040
is it a false positive?

20:51.040 --> 20:51.880
Is it a false negative?

20:51.880 --> 20:58.720
So that kind of temporal effect of kind of going around something, kind of also benefited

20:58.720 --> 21:00.120
that architecture.

21:00.120 --> 21:03.520
So that was one where we could innovate and do well in a challenge.

21:03.520 --> 21:04.520
Nice.

21:04.520 --> 21:12.600
And was your, was the challenge to classify the images or to do segmentation or...

21:12.600 --> 21:14.600
No, it was to classify the image.

21:14.600 --> 21:19.120
So if you had something on your ankles hitting on your clothing, they wanted to know that

21:19.120 --> 21:21.520
there's a threat underneath the left ankle.

21:21.520 --> 21:27.000
There is 17 body zones around the person, the lower legs, backs, and you just have to

21:27.000 --> 21:28.000
tell...

21:28.000 --> 21:32.480
Then you're on that, just have to predict a probability of a threat in a specific body

21:32.480 --> 21:33.480
part.

21:33.480 --> 21:34.480
Okay.

21:34.480 --> 21:35.480
Yeah.

21:35.480 --> 21:36.840
Any other tricks that came to bear there?

21:36.840 --> 21:40.720
You mentioned that part of the challenge had to do with just dealing with the volume of

21:40.720 --> 21:41.720
data.

21:41.720 --> 21:42.720
What did you end up doing there?

21:42.720 --> 21:43.720
Yeah.

21:43.720 --> 21:49.920
So that dealt with just writing a lot of, you know, code to deal with processed data

21:49.920 --> 21:55.520
fast, multi-threaded, find ways to optimize your training so you could load as much data

21:55.520 --> 21:57.920
as possible, as fast as possible.

21:57.920 --> 22:01.160
So that was more like an engineering challenge, you know.

22:01.160 --> 22:03.200
How do you train faster?

22:03.200 --> 22:04.680
How do you load data faster?

22:04.680 --> 22:08.520
How do you get through your training epochs faster?

22:08.520 --> 22:10.280
How do you do your inferencing faster?

22:10.280 --> 22:11.280
So I...

22:11.280 --> 22:12.480
And where did you do all this?

22:12.480 --> 22:18.120
Did you have, did you use university equipment or did you have your own equipment or do

22:18.120 --> 22:19.440
you use cloud?

22:19.440 --> 22:25.760
Because I use AWS Amazon, I write GPU instances on Amazon.

22:25.760 --> 22:26.760
Okay.

22:26.760 --> 22:30.920
And what did you end up spending on the competition?

22:30.920 --> 22:38.760
So this competition we spent probably maybe $2,000 in GPU compute costs over three months.

22:38.760 --> 22:40.440
Would you end up winning?

22:40.440 --> 22:42.000
What is third place?

22:42.000 --> 22:44.000
Third place was worth $200,000.

22:44.000 --> 22:45.000
Nice.

22:45.000 --> 22:46.000
So good profit margin.

22:46.000 --> 22:47.000
Yeah.

22:47.000 --> 22:48.000
That's good.

22:48.000 --> 22:49.560
That was a good profit margin.

22:49.560 --> 22:57.560
I guess I'm curious like the way you thought about the spend on AWS, you know, you know,

22:57.560 --> 23:01.040
obviously before the competition ended, you didn't know if you were winning, but you

23:01.040 --> 23:06.040
had signal that you were doing well, like, did you kind of managing it or did you?

23:06.040 --> 23:07.040
Yeah, yeah.

23:07.040 --> 23:08.320
You definitely manage it.

23:08.320 --> 23:11.320
You definitely manage it.

23:11.320 --> 23:16.760
You have to weigh the cost and the benefit and fortunately, usually what I do is, as

23:16.760 --> 23:22.880
soon as I enter, I want to see myself in the top 30 to give, to know if it's worth,

23:22.880 --> 23:26.600
you know, pushing harder, maybe spending more.

23:26.600 --> 23:30.560
So that's usually how it works, you know, I usually come up with an approach.

23:30.560 --> 23:33.320
I hardly ever pivot in this competition.

23:33.320 --> 23:39.760
I kind of try to come up with a plan that before you even start, yeah, before I start,

23:39.760 --> 23:47.760
but I wanted that plan to show signs of promise right away because oftentimes my strategy

23:47.760 --> 23:49.360
is that I hardly ever pivot.

23:49.360 --> 23:54.720
I hate pivoting because when in this challenge is usually you find out you waste a lot of

23:54.720 --> 24:00.200
time experimenting with approaches that just lead to dead ends.

24:00.200 --> 24:07.120
So I try to think out the problem well ahead of time and then just give it a shot at that

24:07.120 --> 24:11.480
first approach and hope it shows good promise.

24:11.480 --> 24:19.600
It's almost like because of the factors that you describe, like a lot of winning the competition

24:19.600 --> 24:25.920
is being on the winning side of an information asymmetry.

24:25.920 --> 24:35.160
And so the approach is one of, you know, figuring out a strategy.

24:35.160 --> 24:41.080
And if you end up in the top 30, it means you're probably not on the losing side, dramatic,

24:41.080 --> 24:45.960
you know, dramatically on the losing side of some information asymmetry, whereas that's

24:45.960 --> 24:48.360
an interesting way to think about it.

24:48.360 --> 24:53.080
Has that way of thinking about it evolved over the four years you've been doing this,

24:53.080 --> 24:56.640
or did you, did you start pivoting like everybody else does?

24:56.640 --> 25:02.160
Yeah, sometimes, you know, if a lot of time, it depends on the interest in the competition.

25:02.160 --> 25:05.680
And there's a lot of interest, just my personal interest is a lot.

25:05.680 --> 25:10.680
I will pivot if something is not working and I'll, because I don't mind, but if it's

25:10.680 --> 25:17.400
not there, you know, yeah, so it's not like I don't pivot, but it's just, if the cost

25:17.400 --> 25:22.800
of time and commitment you need, you can kind of gauge that from the first time you see

25:22.800 --> 25:26.600
the competition is a lot, you don't want to spend too much time.

25:26.600 --> 25:31.840
And also there's also this diminishing return effect where if you pivot, at least this

25:31.840 --> 25:38.240
is just my approach, is that if you try too many ideas that fail, it's kind of defeating.

25:38.240 --> 25:44.000
So you want to kind of throw your hard punch the first time and try to kind of get a good

25:44.000 --> 25:45.400
result that way.

25:45.400 --> 25:47.760
That's kind of how I approach it.

25:47.760 --> 25:55.000
Is, are there some competitions that you join to play and others that you join to win?

25:55.000 --> 25:59.520
Yeah, most of the competitions I do well in our computer vision.

25:59.520 --> 26:06.840
So I usually try to do computer vision to do well in and win the other machine learning

26:06.840 --> 26:07.840
competitions.

26:07.840 --> 26:11.120
I do them to learn and to figure out things.

26:11.120 --> 26:18.000
What are some other examples of this idea of kind of exploiting unique aspects of the

26:18.000 --> 26:19.000
problem?

26:19.000 --> 26:20.000
Yeah.

26:20.000 --> 26:22.640
So, and these are some of my secrets.

26:22.640 --> 26:29.120
I don't know if I want to give away too much, but I find the discussion forums on

26:29.120 --> 26:35.320
Kaggle very, very useful, like what participants are talking about.

26:35.320 --> 26:42.120
A lot of the times, you know, you reach through those forums and you can kind of, you can kind

26:42.120 --> 26:47.720
of get a sense of an approach or some, you can find kind of connect threads.

26:47.720 --> 26:48.720
So that's another part.

26:48.720 --> 26:52.760
You try to find information you need that maybe people haven't seen.

26:52.760 --> 26:57.600
But there's something they're all trying to say, but I don't know how to explain it.

26:57.600 --> 27:02.120
But there is information in just reading the mindsets or trying to understand what people's

27:02.120 --> 27:03.120
experiences are.

27:03.120 --> 27:08.600
Because oftentimes that aspect of it too, you know, people who are doing well, sometimes

27:08.600 --> 27:10.160
they're usually quiet.

27:10.160 --> 27:15.200
That was one that I noticed in Kaggle too was when I started, the people at the top of

27:15.200 --> 27:20.400
the leaderboard, I always try to look to see what they're talking about.

27:20.400 --> 27:23.320
And they're, you never find anything because they're just quiet.

27:23.320 --> 27:26.800
It's like they know something and they don't want to share.

27:26.800 --> 27:31.800
You know, so there's that aspect of trying to dig in the forums to try to, you know,

27:31.800 --> 27:36.040
try to learn approaches that could help in a challenge.

27:36.040 --> 27:43.120
That makes me think a little bit about research in general and how, you know, often you'll

27:43.120 --> 27:50.120
find that there are a bunch of, you know, more or less kind of parallel introductions

27:50.120 --> 27:56.680
of innovative ideas because, you know, some period of time before, like there was something

27:56.680 --> 27:59.200
in the air and nobody quite knew how to say it.

27:59.200 --> 28:00.200
Exactly.

28:00.200 --> 28:04.880
But there was something in the air and if you can kind of figure that out, that leads you

28:04.880 --> 28:07.320
down the road to some innovation.

28:07.320 --> 28:08.320
Exactly.

28:08.320 --> 28:10.440
That's kind of what I'm trying to say.

28:10.440 --> 28:15.920
So you also participated in a distracted driver competition.

28:15.920 --> 28:18.240
That was what State Farm sponsored?

28:18.240 --> 28:20.240
What was that one all about?

28:20.240 --> 28:27.360
Yeah, that challenge was a challenge to identify distracted drivers.

28:27.360 --> 28:28.360
I think there are different things.

28:28.360 --> 28:35.560
It was the driver playing with the radio, was a driver looking, I forgot in all the categories

28:35.560 --> 28:41.280
in that challenge, but it was kind of to identify distracted drivers.

28:41.280 --> 28:49.280
And that was just they had a dash cam in the car and the task was to kind of indicate

28:49.280 --> 28:54.040
what if the driver was paying attention to driving or was the driver eating or drinking

28:54.040 --> 29:00.640
or using makeup or playing with the radio or doing something else.

29:00.640 --> 29:05.360
So that was another image in competition I did well in.

29:05.360 --> 29:13.080
The trick, one of the things I did in that one that really helped was we used a clever

29:13.080 --> 29:19.920
data augmentation technique that enabled us to convince the data a lot.

29:19.920 --> 29:28.160
We kind of, I don't know if it's relevant, I can explain the technique is we, so if

29:28.160 --> 29:35.000
you, if we had two images of a driver that was playing with the radio, right, two different

29:35.000 --> 29:41.440
drivers, we could take 75% of one image and combine it with the remaining 25% of the

29:41.440 --> 29:44.200
other image and form a new image.

29:44.200 --> 29:47.120
So that creates a kind of an additional image.

29:47.120 --> 29:53.400
So it's, it's more likely some part of the driver, it just that augmentation approach

29:53.400 --> 29:55.520
on how does create a lot more.

29:55.520 --> 30:03.080
You had a picture of one driver playing with the radio and then a picture of the same

30:03.080 --> 30:07.600
driver not playing with the radio, no, no, no, the picture of a different driver that

30:07.600 --> 30:09.360
said was playing with the video.

30:09.360 --> 30:10.360
So with the radio.

30:10.360 --> 30:16.960
Two drivers that are both playing with the radio, you created other images and was this

30:16.960 --> 30:19.000
like a linear interpolation of the two images.

30:19.000 --> 30:23.640
Yeah, yeah, this is just a flat combination, you just combine it and you take some percentage

30:23.640 --> 30:28.440
of this one and some, I mean, you, you join them horizontally or vertically.

30:28.440 --> 30:32.320
And because you're just trying to learn, the neural network has to be able to learn,

30:32.320 --> 30:37.480
you know, if it's a partial image of somebody doing something, there's still enough information

30:37.480 --> 30:43.000
there to, to kind of try to estimate the best prediction.

30:43.000 --> 30:49.320
So you're literally just like 75% of the image horizontally is one picture and 25% is

30:49.320 --> 30:50.320
another picture.

30:50.320 --> 30:51.320
Yeah.

30:51.320 --> 30:52.320
And that worked.

30:52.320 --> 30:53.320
Yeah.

30:53.320 --> 30:54.320
Yeah.

30:54.320 --> 30:55.320
Wow.

30:55.320 --> 30:56.320
Yeah.

30:56.320 --> 31:00.600
And we did that, we were able to, I think I don't know, we had maybe something like that.

31:00.600 --> 31:02.600
What made you try that?

31:02.600 --> 31:05.040
We're trying to deal with overfitting, right?

31:05.040 --> 31:10.640
So the problem with the way the data was collected was that there were a few subjects

31:10.640 --> 31:15.800
in the, in the training set and neural networks tend to overfit, they just memorize, right?

31:15.800 --> 31:20.280
They just memorize the images, right?

31:20.280 --> 31:26.040
Because once they see somewhere, the visor in a certain position or some unique future

31:26.040 --> 31:29.480
in the image, they just latch on to that and they make the correct prediction every time

31:29.480 --> 31:30.720
in training.

31:30.720 --> 31:36.480
But that doesn't generalize to, it doesn't generalize to an unseen validation set, because

31:36.480 --> 31:40.600
they're exploiting, you know, peculiarities of each.

31:40.600 --> 31:45.440
So by doing this administration strategy, you kind of break all those false assumptions

31:45.440 --> 31:49.160
that the neural network might have made to improve.

31:49.160 --> 31:55.000
So it really has to learn the task of maybe identifying when to try and explain what

31:55.000 --> 31:56.000
a radio.

31:56.000 --> 31:57.000
So.

31:57.000 --> 31:58.000
Wow.

31:58.000 --> 32:03.520
Did you also innovate on the model architecture or something off the shelf?

32:03.520 --> 32:06.200
No, we just used off the shelf in that one.

32:06.200 --> 32:10.080
We didn't do anything special with the model architecture.

32:10.080 --> 32:17.080
And a lot of Kaggle competitions are one by like these really kind of monstrosity ensembles

32:17.080 --> 32:18.080
of things.

32:18.080 --> 32:21.880
Is that true generally in vision or less so?

32:21.880 --> 32:22.880
Yeah.

32:22.880 --> 32:28.640
It's true in vision, but it's true in vision when you get to inserting competitions.

32:28.640 --> 32:33.800
In other competitions like the Homeland Security Challenge, we could have kept our position

32:33.800 --> 32:35.400
with one model.

32:35.400 --> 32:37.720
So it's true.

32:37.720 --> 32:44.720
But if one model is, if you really spend time can do, if you focus on one model, you can

32:44.720 --> 32:47.720
almost do it as well as a massive ensemble.

32:47.720 --> 32:51.320
But oftentimes the ensemble is the easy way out.

32:51.320 --> 32:55.160
But the ensembles, there's a cost associated with that, at least for computer vision in

32:55.160 --> 32:56.880
terms of GPU time.

32:56.880 --> 33:02.600
If you have infinite compute resources, you might be able to get away with ensembling.

33:02.600 --> 33:08.560
But oftentimes you have to wait the cost of training many models with focusing on one

33:08.560 --> 33:11.440
and trying to get it as good as possible.

33:11.440 --> 33:19.760
So if you were starting Kaggle today, we actually, we've got a study group, these are folks

33:19.760 --> 33:25.480
that are kind of like an online meetup of folks that are doing Kaggle together.

33:25.480 --> 33:31.600
They've been, I think it's been going for maybe eight weeks now, and they may be about

33:31.600 --> 33:35.480
to start a new session of it.

33:35.480 --> 33:42.400
But the sense is that one of the key things to do is to find other folks to work with.

33:42.400 --> 33:44.840
Is that true in your experience?

33:44.840 --> 33:53.120
Yeah, working with other people usually helps sharing ideas and kind of creating diversity

33:53.120 --> 33:54.760
of approaches.

33:54.760 --> 33:58.360
Most of my competitions have teamed up with somebody.

33:58.360 --> 34:03.920
And everybody brings unique perspectives that help in challenges.

34:03.920 --> 34:11.480
If you were talking to folks that are interested in starting, how would you advise them to kind

34:11.480 --> 34:13.840
of go at it?

34:13.840 --> 34:22.440
Okay, so one part of, at least I see, is that in AI, a lot of people make things a little

34:22.440 --> 34:24.760
more complicated than they need to be.

34:24.760 --> 34:25.760
Really?

34:25.760 --> 34:37.200
I don't know if that's new information, but so the key is actually, the solutions for

34:37.200 --> 34:41.080
problems, at least my approach, is that these solutions are usually simple.

34:41.080 --> 34:44.640
They're not, they're not complicated.

34:44.640 --> 34:51.040
And a barrier to starting Kagel is that there's this idea that it's hard, it's difficult.

34:51.040 --> 34:56.320
If you look at AI, for example, and the progression of deep learning, at least from my experience

34:56.320 --> 35:01.200
and my research, the key contributions have been really simple ideas.

35:01.200 --> 35:12.720
So for example, before 2012, when Alex net won that computer vision challenge, there

35:12.720 --> 35:16.920
was this idea that neural networks were hard to train, there was this vanishing gradient

35:16.920 --> 35:22.440
problem, there was all this stuff going on, you just couldn't train neural networks.

35:22.440 --> 35:27.520
And you should avenge your paper on the difficulties of training neural networks.

35:27.520 --> 35:33.360
And what he found was that the activation function they were using, which was the sigmoid,

35:33.360 --> 35:36.760
was causing, was a problem.

35:36.760 --> 35:42.600
So all, in my opinion, all the breakthroughs we've seen has been a result of the regular

35:42.600 --> 35:49.120
activation function that sort of eliminated that vanishing gradient problem.

35:49.120 --> 35:51.840
So that idea is pretty simple.

35:51.840 --> 35:54.640
It was a simple change in the activation function.

35:54.640 --> 35:59.800
Between then and the other next key contribution, there are a lot of complex things that people

35:59.800 --> 36:06.080
explain and try to cloud the actual real important things that push the ball forward.

36:06.080 --> 36:12.280
And Kegel said of the same way, I kind of feel like a lot of challenges you just have

36:12.280 --> 36:18.120
to kind of look at it with a creative approach and just open your mind that the solution

36:18.120 --> 36:19.120
is simple.

36:19.120 --> 36:27.480
It's just you're just a few ideas away from having kind of like a really good solution.

36:27.480 --> 36:34.840
So I think this is a sort of joining teams is teaming up with people is a good idea.

36:34.840 --> 36:40.200
But generally, you just have to believe that you can do well in a challenge and really

36:40.200 --> 36:42.800
give it a shot.

36:42.800 --> 36:46.440
Because Kegel can be discouraging, you know, it's, you can be discouraging if you go

36:46.440 --> 36:48.080
in and you don't do well.

36:48.080 --> 36:51.720
Fortunately for me, I didn't experience that well, maybe my first challenge, because you

36:51.720 --> 36:55.040
know, my first challenge, I thought, yes, I'm going to blow this thing away.

36:55.040 --> 37:01.200
And you know, you can be, you'll be really surprised, you know, how much you still have

37:01.200 --> 37:02.200
to learn.

37:02.200 --> 37:03.200
Yeah.

37:03.200 --> 37:04.760
So teaming up is a good way.

37:04.760 --> 37:09.960
But I still feel you can do it on your own, just have the motivation to to enter a challenge

37:09.960 --> 37:16.080
and stick at it, don't get discouraged if you don't do well initially.

37:16.080 --> 37:21.080
So teaming up persistence, keeping a simple, what else?

37:21.080 --> 37:23.360
Yeah, that's, that's about it.

37:23.360 --> 37:29.680
And, and reading top solutions, following up at the end of the challenge.

37:29.680 --> 37:35.660
And oftentimes, Kegel, the winners usually share their approaches and follow up and kind

37:35.660 --> 37:40.920
of look and correlate what the winners did with what you did and what you could have done

37:40.920 --> 37:42.640
that or that you missed.

37:42.640 --> 37:46.400
But I think that's it, it's really following up and just being persistent.

37:46.400 --> 37:51.040
So those are the kind of the general tips for folks that want to get started.

37:51.040 --> 37:55.640
What about the expert tips for folks that have been banging their head against it for

37:55.640 --> 37:59.560
a while and haven't gotten to, you know, where they'd like to get?

37:59.560 --> 38:03.200
Do you have any, you know, I think, I think, or ninja habits or anything like that.

38:03.200 --> 38:05.000
I think those tips are universal.

38:05.000 --> 38:06.760
Those tips works for the expert to the experts.

38:06.760 --> 38:12.280
If they stay persistent, they look at the solutions of the top teams, what they did.

38:12.280 --> 38:13.280
That's it.

38:13.280 --> 38:14.440
Okay.

38:14.440 --> 38:16.760
You just kept it simple.

38:16.760 --> 38:17.760
Yeah.

38:17.760 --> 38:18.760
Awesome.

38:18.760 --> 38:25.520
You know, what we didn't talk about yet is analytical AI, which is your, your day job.

38:25.520 --> 38:26.520
What, what is that?

38:26.520 --> 38:27.520
What do you do there?

38:27.520 --> 38:28.520
Yes.

38:28.520 --> 38:33.920
So at Annalclei, we, due to the Homeland Security Challenge, we got a couple of projects.

38:33.920 --> 38:39.760
We're developing various, we're working with various equipment manufacturers to that are

38:39.760 --> 38:48.560
usually in the security space to help identify threats, either on person screening or in

38:48.560 --> 38:51.880
baggage, CTE or X-ray.

38:51.880 --> 38:59.560
And we're also working on various fintech products where we're trying to use AI for things

38:59.560 --> 39:02.800
like technical analysis and things like that.

39:02.800 --> 39:03.800
Interesting.

39:03.800 --> 39:12.480
I did, I, this wasn't a full interview, but I've talked to a group down in Austin, I think.

39:12.480 --> 39:15.360
And I've seen a couple of press releases about this.

39:15.360 --> 39:22.480
I get a couple of year of groups that are using a bunch of sensors, maybe mounted on a police

39:22.480 --> 39:31.640
vehicle or on a drone that are like trying to identify the presence of weapons, you know,

39:31.640 --> 39:38.560
on a person, and the drone mounted ones are the ones that are kind of the most crazy

39:38.560 --> 39:39.560
sounding.

39:39.560 --> 39:40.560
Yeah.

39:40.560 --> 39:41.560
Does that stuff work?

39:41.560 --> 39:43.640
Is that kind of in the domain of stuff that you've been looking at?

39:43.640 --> 39:47.640
That's not necessarily the domain of what we're looking at, like we're, we're looking

39:47.640 --> 39:54.920
at working with teams that make things like passive screening, like tear hurts and, you

39:54.920 --> 40:01.840
know, like, or X-ray for baggage at the airport.

40:01.840 --> 40:08.560
It's not like drone mounted or these are like people screening devices, like the station

40:08.560 --> 40:15.040
at somewhere and they try to make you walk through maybe at events or stadiums and identify

40:15.040 --> 40:17.400
threats under clothing and things like that.

40:17.400 --> 40:18.400
Okay.

40:18.400 --> 40:19.400
Okay.

40:19.400 --> 40:25.680
So opinion on the drone mounted viability.

40:25.680 --> 40:26.680
Awesome.

40:26.680 --> 40:28.720
Well, David, it has been great chatting with you.

40:28.720 --> 40:34.440
Thanks so much for sharing a bit about what you've been up to and all of the great Kaggle

40:34.440 --> 40:35.440
tips.

40:35.440 --> 40:36.440
Yeah.

40:36.440 --> 40:37.440
Thanks.

40:37.440 --> 40:38.440
It was great talking to you too.

40:38.440 --> 40:39.440
Thanks.

40:39.440 --> 40:40.440
Thanks, David.

40:40.440 --> 40:47.400
All right, everyone, that's our show for today to learn more about today's guests or

40:47.400 --> 40:53.080
the topics mentioned in the interview, visit TwomoAI.com slash shows.

40:53.080 --> 40:58.520
Don't forget to check out our demo cast with Vila at TwomoAI.com slash Metaflow.

40:58.520 --> 41:03.080
And of course, be sure to subscribe to our YouTube channel while you're there.

41:03.080 --> 41:07.480
If you like what you hear on the podcast, please subscribe, rate, and review the show on

41:07.480 --> 41:09.520
your favorite pod catcher.

41:09.520 --> 41:20.160
Thanks so much for listening and catch you next time.

