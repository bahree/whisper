All right, everyone. I am here with Roberto Bandizan. Roberto is an AI researcher at Qualcomm.
Roberto, welcome to the Twomo AI podcast.
Hi, Sam. Thank you. I'm really looking forward to our chat. We'll be talking about your paper
at ICLR, probabilistic numerical, numeric CNNs. But before we dig into that, I'd love to hear
you share a little bit about your background and how you came to work in AI. Sure. So my background
is in physics, and that's how I got into AI by basically applying deep learning to some physics
problems. So before joining Qualcomm, I was working on characterizing new phases of matter and
their potential for quantum computation. So phase of matter is basically one of the states
you can find matter or materials around you. And one classical example is provided by the
Easy Model. So in the Easy Model, you have some binary units, some bits which can be either
up or down. And you can find this system into phases. One order phase, which is at a low
temperature, where all these units, we call them spins, point in the same direction, and a high
temperature disorder phase, where these units point in random directions. It turns out that
characterizing phases of matter is a very challenging problem. And so researchers in the field
they started to use AI for that. So that's how I got into AI. And as I was, you know, just learning
about the techniques that were being developed, if this was around 2017-2018, for these problems,
I got very interested in AI per se, and then I decided a career shift and to move
in there to do research in AI. At this point, it was also when the paper by Taco Cohen and Max
Welding were my colleagues at Qualcomm. Their paper on spherical CNNs came out. And so at that
point, I understood that, you know, studying neural networks using insights from physics was a very
useful and interesting thing. And that's what I wanted to work on. So did you get, were you familiar
with the spherical CNN paper before you came to work with Taco and Max? Yes, yes, definitely.
That's indeed one of the links that brought me to Qualcomm, yeah. That's fantastic. So tell us
a little bit about your research interests. Yeah, so after I joined Qualcomm, I got involved
into quite a few exciting projects. So one of them has to deal with the neural data compression.
So here, the problem is that you want to compress data to, you know, send them over to some receiver.
And you can actually use neural networks in particular, some forms of auto encoder to do that.
So that's a quite, you know, interesting problem from the theoretical point of view. It involves
generative modeling and things like that. But it is also super impactful because, you know,
it can immediately translate into different ways to compress data. So that's a very interesting
direction. Otherwise, I'm also very excited about Quantum AI. So after joining Qualcomm, I also
had the chance to develop some ideas around quantum deep learning. So this is the, you know,
intersection between quantum computation and the NDI. And so here is where also my background
in quantum thesis was quite useful to, you know, get started quickly on this field, which is
a rapidly developing field, which I think is one of the most interesting, you know, directions,
which can disrupt AI in the future. And finally, more recently, I got interested in also applying
machine learning to combinatorial optimization problems. So, you know, in industry, we have a lot
of combinatorial optimization problems to solve. And so the potential of using machine learning
to, you know, improve on the classical techniques is very interesting. And again,
this is a very interesting, you know, area between, you know, very theoretical mathematical work,
which, you know, is interesting to me and also very impactful work. So, yeah, in summary,
I had the chance, you know, of both doing impactful work for society, you say at large,
but also, you know, being able to do some long-term research like quantum AI. So I'm very excited
about, you know, 15s that are going on at Qualcomm. I know. Nice. And when you say combinatorial
optimization, these are problems like traveling salesmen and map coloring and that kind of thing,
like classical combinatorial problems. Yeah, precisely. So, in fact, you know, in the last few
years, maybe three, four years, there's been quite, you know, good progress on using deep learning
for these problems. So, you know, traveling salesmen indeed is the standard paradigm of combinatorial
optimization problem, right? When you have a salesman, we need to find the optimal tool to visit
the series of cities. And so it has, of course, also direct applications in industry, for example,
vehicle routing, right? Where a company has to deliver stuff and so we need to find the optimal
route. And so, yeah, traveling salesmen is certainly a great example. And it also historic,
I just driven most of the research combinatorial optimization and it is also in the intersection
with machine learning. But also, I'm thinking about other problems like cheap design or, you know,
some problems, combinatorial problems in wireless. So, these are also problems that are, you know,
hard. So, in fact, there are some empty complete, you know, problems there. And, you know,
these are also very interesting application areas for us.
So, and cheap design, that would be things like routing traces on a circuit board or on a
chip. Yeah, so there are indeed a few stages of cheap designs and all of them, in fact,
involve different combinatorial optimization problems. And, yeah, so as you say, routing,
so finding the optimal connections between the different logical gates for memories on a
cheap canvas. And also, you can think about indeed also placing these components on a cheap
in the optimal way to minimize area and stuff like that. Indeed, this is all very interesting
combinatorial problems where, you know, I could disrupt.
And on the wireless side, I'm imagining that's like frequency allocations and things like that,
or what are some of the applications there? So, I have in mind, you know, things like
some coding problems where, you know, you basically have sent some signal over and then you want to,
you know, retrieve what was the bit string that was sent. But, you know, your signal has been
corrupted by a noisy channel and you want to retrieve that, that these strings, that's one
example of things that one could do or some other error correcting problems and things like that,
yeah. So, there's a bit of a relationship between the combinatorics and kind of information
theoretical types of problems and oppression, which is where you spend a bulk of your time.
Yeah, absolutely. It is also fair to say indeed that all of these problems, you know,
compression, also other problems that we work on, like quantization. These are all combinatorial
nature. So, these are certainly possible use cases for this nesting field of ML for combinatorial
optimization. Yeah. Nice, nice. It's an important area. Yeah. So, you're probabilistic numeric CNN's
paper. What's the problem area that you're addressing there? Sure. So, there, the problem area
is the application of deep learning to, you know, signals that are not necessarily
sampled on a grid. Think about, you know, in many applications of deep learning, you, for
example, want to model some time series and these time series, you know, have not been,
you know, sampled uniformly. So, that's one possible, you know, use case for the models we
want to develop in probabilistic numeric CNN's. But more generally, the motivation there
is really, you know, trying to think about the signals that we want to model, you know,
in machine learning, in their continuous formulation, not in their discrete formulation,
which is, you know, the natural formulation you observe, you know, when you measure something.
And so, basically, just to unpack a little bit of the title, right? So, the title starts
with probabilistic numeric. So, the inspiration for this work is really this field of probabilistic
numeric. So, you certainly have a media with a convolution on your networks, so I do not need to
introduce that. But so, let me just spend a couple of words on probabilistic numerics, which
might not be familiar to everyone. So, probabilistic numeric is a recent field in statistics,
which tries to, I mean, not necessarily recently, but recently, I think there's been a
quite big developments. And so, it tries to quantify the uncertainty that a numerical program
has, due to the discreteness of the sampling procedure of its input. To make this concrete,
let's think about the problem of computing a numerical integral. You know, the function that you
want to integrate analytically, you can have evaluated everywhere on a continuous range in its
domain, but necessarily, you need to sample that function on a discrete sample of points,
because your memory and time to compute that integral is finite. And so, probabilistic numeric
tells you a way to derive uncertainty from this sampling procedure. And the way it is done
is via Bayesian inference. So, in Bayesian inference, one has an agent, a machine learning model,
that has a prior. Here, the prior will be over the set of possible functions that you want to
integrate. And in technical terms, it is a Gaussian process. So, a Gaussian prior on this set of
functions. And then, upon measuring that function, you want to integrate on some points in your
domain, you update your prior to a posterior. And this allows you to immediately translate to some
uncertainty on the result of your integral. So, now, your numerical program will not just return
a number, which will return a probability distribution. And this probability distribution will be
picked around some value, and there will be an uncertainty. So, that's your discretization error.
So, we thought that this is quite interesting also for machine learning. And so, we,
indeed, start from the same philosophical standpoint, you know, the images that we want to model
in machine learning, the time series that we want to model, are in fact, continuous signal.
And so, we necessarily need to discretize them because we want to put them in a computer.
And, but, you know, this procedure will come with some uncertainty with some errors. And so,
it is important to quantify those. And so, that's basically the motivation behind this work.
If I could replay that to make sure I'm understanding with classical numerical programs, like I'm thinking
back to Fortran, numerical computing and undergrad, like you've got some function, and you want to
compute an integral for it. And you just, you do that. There are established outcomes for doing that.
What probabilistic layers on top of that is allowing you to look at your, the function that you're
integrating, not as a single function, but as a distribution of functions. And what you have then,
you know, then your kind of classic quantization error now becomes a probabilistic quantization error.
Yeah, that's a good summary. Thanks. And maybe just to clarify, so here we are really looking at the
quantization, right, in the domain of the function. So, the value that the function takes
are still continuous. And so, that's the kind of quantization we are looking at. And so, indeed,
in our probabilistic numeric CNN, so we start from this idea. And then we develop on top in your
network. I also want to interrupt to say that when you say the quantization is in the domain of
the function, meaning as opposed to the range, which is your think about your vertical amplitude,
here we're talking about you're taking different points in time that may or not be, may or may
not be, well, are not uniform. And so, that's where your quantization is coming in. So, you've got a
time series, but you're not getting data in every second or millisecond or whatever it comes
irregularly. And you're trying to figure out quantization error based on that irregularity.
That's precisely it. Yeah. Okay. Yeah. All right. Cool. So, and indeed, maybe just to give you a little
more about the paper, so we develop the idea of using probabilistic numeric for deep learning.
So, the first step in this procedure is that we start indeed from a regular sample time series,
for example, or even from an image, which has been sub-sampled in a regular way. And what we do
is that we interpolate that. So, we interpolate that in a probabilistic way. So, like it, is that
like a probabilistic numerical programs do. And that gives us my posterior distribution over
our input. And what we do then on this posterior distribution is that we apply a neural net. So,
now this posterior distribution is a distribution of a continuous functions. And so, the technical
contribution that we make in this paper is to devise a neural network on continuous functions.
So, typically, your CNN, we lack on vectors, right? There's some array of numbers.
Here, our probabilistic numeric CNN is defined directly in the continuum. And that turns out
to be quite powerful. And also unlocks, you know, new models and new mechanisms for learning.
Which have, what does that exactly mean? I think, yes, I'm so used to thinking about the input
to a CNN being a vector. I'm not even sure how to unpack it being continuous.
Right. So, indeed, you're not going to store that function in your computer, because by definition,
indeed, you're going to need to, you know, have, you know, an infinite number of points if you
want to store all the values. What you're going to store is just some function of form,
some code that allows you to evaluate that function, right? And so, that's somehow the input to,
to your, you know, neural network. And so, to be precise, indeed, about what happens,
we still have a neural network which works by interliving linear, nonlinear layers. But now,
and so the nonlinear layer, you can actually morally understand that it's going to be very
similar to what you're used to do at each point of your function applied nonlinearity. But now,
the real, you know, new part of the work is about the linear layer. So, we devise actually a
new convolutional layer, which is defined in terms of a linear PD, the partial differential
equation. So, this partial differential equation is a linear operation on an input function,
which is the input sum out to the PD, namely the value that you have the initial condition
to your differential equation. So, what happens is that, you know, if you want to impose actually
translation equivalence that you have, you know, in convolutional layer, this restricts the forms
of differential equations that you can input in your neural network. And interestingly,
you know, one of the simplest things you can do is to use the kind of generalized diffusion
equation. So, diffusion, you know, is a process from physics, which you can understand, you know,
for example, when you have a glass of water, you put some dye into it and this dye diffuses
over time. And so, similarly here, you know, we have our image, which is encoded, you know,
in some function. And that function gets blurred, similar to the diffusion process over time.
So, that's really what we mean, you know, by the layer on continuous functions. So,
it is defined formally. And it turns out that for certain choices, indeed, of layers,
we can do computations analytically. So, we can actually propagate these, you know,
functional forms in our code analytically. And so, that's a pretty cool.
And so, we can, you know, ultimately, we can devise a practical procedure to, you know,
start from our input signal, which was, you know, this sub-sampled signal, then interpolate it,
then we apply this convolutional layer as PDEs. We interleave with some non-linearities.
And what we get out, after some of these layers, and perhaps the pooling and so on,
we get out, you know, a prediction. Like, you know, we want to classify this time serious,
for example, this input image. And so, we want to get out a class label, right? As we do usually,
but on top of that, we also get out an uncertainty. And actually, this uncertainty is also there
at T-EV intermediate layer. And it's really an uncertainty that is related to the, you know,
entity, the input signal didn't have maybe information in certain regions of space or time.
So, in this way, we know, you know, we can characterize indeed what is the error that we make.
And more interesting, we can also choose where to sample the signal in order to reduce uncertainty.
So, these are all the interesting applications that we can think about with this model.
Is that, is that ladder point choosing where to sample? Is that a, like a byproduct of
going through the process in the same direction, or is it more like going through the process
backwards? I don't know if that question makes it. It's going to the process backwards, you're right,
it's a small, you basically, you know, find a certain uncertainty and this uncertainty will be a
function of where your value to your input. So, you can compute some kind of derivative of that
to the spread to the inputs to minimize the uncertainty. And that's, you know, that can be
useful, you know, when it is, for example, costly to get data points. You can optimize the points
that are most informative. Or, you know, when you have maybe some data on meshes and things like that,
you know, where discretization errors are important. So, there are a lot of interesting, you know,
use cases. So, in this paper, actually, we focus mostly on a benchmarking this model on a couple
of data sets. So, one is the super pixel classification of images. So, super pixel, you know, is just
an image, which is sub-sampled, but again, the points are not on a grid. Before we get to the
benchmark, another question about the architecture here. So, you, one of the key innovations or
contributions, it sounds like it's this PDE layer. Yes. And PDE's arise in physics all the time,
like you can, I'm imagining the inspiration of that was thinking about the problem, like the
closed form problem and how you might solve it. And then, you know, that involves PDE's.
Yes. So, PDE, yeah, good. Sorry. No, no, I was, I was going to, you know, but then you get to that
so that your PDE, you have this PDE layer that you think needs to be involved in here, but it,
you have the constraints of translation, invariance from CNNs, and then suddenly you're like,
okay, diffusion is the answer. And like, where did that come from? Was that? Did you,
did you recognize diffusion as like a translational independence by thinking about a glass of water,
or is that like a known physics thing? Yeah. So, yeah. So, the, the way we got there, and actually,
I would like at this point to amend the one of the big omissions that I've done in the beginning,
which is not to acknowledge that the first author of this paper is Mark Fincy, who was doing
an internship with us last summer. So, he's really the main driver driving force in this project.
And so, you know, Mark came up with this proposal, and I guess it was a bit of a mixture of two things.
One thing was intuition, and the other thing may become from physical reasoning, and the other
thing was just a mathematical formalism. So, we wrote down, you know, the most general,
basically, local, you know, linear layer in the former PDE, and then, you know, basically,
this turned out to be diffusion when you imposed translation in variance.
And actually, we also, you know, did something a bit more general. So, we also consider the,
you know, symmetries like rotation and things like that. So, we thought a little bit about
spherical CNN at the beginning. So, there is, you know, interest in the community in characterizing
equipment against under more general symmetries. And so, it turns out that, you know,
beautifully, also, in this context, we can get, you know, PDE's, which are, you know,
equivalent under more general transformations, like rotations at things like that.
And that's actually quite interesting, I believe, because, you know, one of the problems with,
you know, getting to work, also, the equivalent under rotations, say, is that you necessarily need
to discretize things on a lattice. And so, at that point, you know, the rotation for a certain angle
becomes, you know, pretty tricky to get it to work well and necessarily, you know, you will have
some error, which is due to the, basically, mesh of your lattice and so on. In this context,
we avoided this problem. So, our model is defining the continuum, and you know, it is basically
equivariant under arbitrary rotations. So, that's a pretty cool, I think, feature,
and also equivalent on the arbitrary translation. So, that's, I think, a pretty cool feature too.
Right. Yeah. I'll just interject really quickly that the, this whole idea of
equivalence and spherical CNNs and gauge equivalence is a big focus of the AI research team
there at Qualcomm. And for folks that want to dig in more, that's probably the best place to
start as the first interview I did with Max Welling on gauge equivalence CNNs, or we talk about
a lot of this, what equivalence is and why it's important. And we'll drop a link to that in the show
notes. Yeah, thanks. I also listened to that. It was a great episode. Yeah. Awesome. Awesome.
So, you were talking about benchmarking? Yeah, indeed. So, I was talking about the fact
that we benchmarked on a couple of data sets. So, the first one was this super pixel images. So,
you start from an image. And, you know, suppose it is defined on a grid and then you sub-sample it.
So, you take away some of these points. It's such a way that then it becomes, you know,
the grid structure is lost. And, you know, at this point, your regular CNN will not work well
for this data type. There are a few other competitors out there, but it turns out that our model
basically established a new state of the art for this task. So, three times reduction in the
test error. And so, this was quite encouraging. And we also applied, you know, the model to
medical time series. So, in this case, you know, you can think about, you know, patients
goes to the hospital. And then, you know, for example, the doctor measures, you know, blood pressure,
things like that. And this is done at the regular times, right? So, this is also a good, you know,
case of a regular time series. And then, based on these measurements, you want to predict,
you know, if the patient will recover things like that. And in fact, so, this is our,
these are pretty important data sets to look at. And so, we also applied our model to these
data sets and show competitive results there too. Yeah, so, we basically, you know, think that
this point of view is very powerful. And, yeah, in fact, we have a few, you know, future directions
in mind that came after this, this work. And what are those? Right. So, one of the interesting
directions for this work, in my opinion, is to connect it to quantum computation. In fact,
one of the promising platforms for quantum computation is a so-called quantum optical computer.
So, here, optical means that you use light as the, you know, basically unit of information that
you want to manipulate. And it turns out that, you know, there are states of light that, you know,
people know how to build in a lab, which are closely related to Gaussian processes.
So, there is a beautiful connection here between states of light and Gaussian processes.
And they immediately disperse a connection also between, you know, our model and a possible
generalization to a quantum model, so a quantum neural network. And so, I mean, started about this
because, you know, it seems to me a very natural, basically, way to encode the data via this
relationship between Gaussian processes and certain states, Gaussian states of light. So,
that's, I think, a very natural way to encode data in a quantum computer, which operates on
with optical elements. And therefore, you know, there is also a nice way to interpret, basically,
the probabilistic numeric, new and network from this point of view of quantum information,
quantum optics, quantum mechanics. So, there is a big, basically, direction here that I'm
excited about, which spurred out of this, of this paper, actually, and this new way to think
about inputs to neural networks. Think about neural networks.
Is the primary connection there thinking about continuous functions, or is there also this
aspect of missing or irregularly sample data? Yeah, I would say both. So, the fact that indeed,
you have continuous function relates to what, you know, physicists call, basically, kind of model
that physicists use, which is called quantum field theory. So, the quantum field theory is also
a continuous field. And so, your continuous field classically, right, which is your function,
which is continuous, becomes now a quantum function, so it becomes a quantum field.
And this is quite exciting, I think, because this, you know, these quantum fields are relevant
for describing, you know, elementary particles. So, this kind of experiments that you see,
you know, in this, they collide as a lecture and so on. So, this, you know, continuous formulation
allows you to make a very interesting connection between very different fields, which are described
in a very similar formalism. And so, this is a very interesting thing. And also the, you know,
sampling, the irregularly sample nature of the inputs is also, I think, naturally captured
in terms of these Gaussian states of light that I was talking about. So, yes, I would say both,
I, in my opinion, are very natural candidates, you know, that allow you to, I think, propose
the interesting models for quantum neural networks. And what are some of the, you talked about kind
of the, the, going back to the benchmarks, the sub sampled images and the, the healthcare time
series, is there also a compression application for this paper as well? No, I would say that
a compression was not our main focus, but I can see maybe where you're going. So, if you can
condense, you know, your input in some mean and covariance of the Gaussian, that's maybe also
way to think about if compressed it to a few numbers. So, that's an interesting spin that I didn't
think about. So, it was not really our, our focus here, but yeah, it might be. Got it.
Cool. So, again, this is a paper that you're presenting at ICLR. What else's Qualcomm
have going on at the conference? Sure. So, another paper is from my colleagues,
Tis, Farazindal, and Iris Halben and Taco Cohen. And so, this paper is about adaptive
neural compression. So, here, the, so we thought a little bit before, you know, about what is the
idea of neural compression? So, neural codex. And so, typically, there is a problem, which is the
problem that, you know, you want to have a small neural network, because you want, you know, to,
to have a low computational burden to do, to do this codec process. But at the same time, a low
neural network, low complexity neural network, we will typically, you know, not generalize well.
And so, the idea here of the authors in this paper that we present at ICLR was to do adaptive
or fine-tuned compression. So, the idea is that, okay, you have trained your models on training
data, but now you deploy it. But it turns out that, you know, the test data can be different
from the training data. As I said, you can suffer from generalization problems. But you can,
you know, imagine now that in the scenario where, for example, your sender, you know, is some,
you know, at your sender side of the data, you can, you're willing to spend compute time.
And, you know, you're sending this data to this compressed data to some low power devices,
like mobile phones. And in this scenario, it makes sense, you know, that at your sender time,
you can fine-tune your, your, basically, codec on the test data. And then, on top of sending,
you know, the transmitted image or video to your mobile phone, you also send some bits that are
related to the difference in the weights of your, you know, adaptive neural network codec.
And so, the authors in this show that, you know, you can indeed reserve some bandwidth
for this delta in the weights, on top of the bandwidth for the stream of the video that you want
to send. And it turns out that if you do that, if you jointly optimize the model to do the best
possible thing, so to minimize the rate, the number of bits if you transmit and optimize the
reconstruction accuracy, you actually can do better if you do this procedure, you know,
of sending over some of the bits for your daily weights, then if you're just in occupied
the whole bandwidth for your, for your stream. So that's a pretty up-promising, I think,
direction, which can have a few interesting, you know, applications, a direct application,
in fact, also for PACOM. Now, I think this one was counterintuitive for me. I maybe misremembering
the information theory, but I thought like Nyquist or Hammond or Heming or something like that
said that it doesn't matter how you chop up your channel, you know, if you have a fixed bandwidth,
there's some certain maximum throughput that you'll be able to get, but here you're kind of
splitting your channel into kind of in-band and out-of-band or something like that and getting
better results. Yeah, so here the idea is really that, you know, you want to basically send
the certain number of bits, right? That's your somehow, the rate that you're willing to
send. So that's somehow the amount of information that you would like to send. And at that
point, you would like to do, you know, the best possible job given that constraint. So what is the,
you know, choice of my codec, you know, encoder, right, to give me the best description of my input
in such a way that when I decode it, I get the highest reconstruction quality. And so that's the
setup. And so in this setup, you know, what we showed is that you can actually reserve some of
these bits that you transmit for the, you know, weights. And so that was a new idea that, you know,
people have not thought about. And yeah, in this setting, this is beneficial. But you're right,
there are certain some terrific bounds to the, you know, rate distortion performance that you
can achieve, but you know, we are certainly within these bounds. And yeah.
Mm hmm. Okay. So yeah, we're talking about the different, the, the thing that I was thinking
about it applies to the theoretical bounds, but I would not say that. So we have some ability to
operate within that envelope. I would say closer to the boundary with this procedure than without.
Got it. Got it. Cool. Any other papers? Qualcomm AR research papers at ICLR?
Yeah. Yeah, certainly. There are a few other papers. I can highlight a paper by my colleague,
Pim, the one who is a PhD student of Max. And he has a paper on mesh CNN. So here the idea
is that, you know, you have tasks or meshes like, you know, shape segmentation or 3D shape
reconstruction and things like that. And so you would like to use a graph neural network for
these tasks, but graph neural networks suffer from the problem that they are, they are oblivious
to geometry. So by definition of the graph structure, they do not have information about the
geometry. So in particular, if you have, you know, a vertex with two edges connected to it,
you know, it doesn't matter if you move these edges around, basically because the graph neural
networks do not, convolution neural networks do not see the angle between these edges. And so the
idea of this mesh CNN is to use, you know, gauge equivariance tools to build this geometry
into graph neural networks. It turns out that if you do that, you get, you know, much better results
for these tasks or meshes. So we have also other works on, you know, temporal localization
of actions. And also we have a, we are organizing together with UC Irvine and Disney research.
We are organizing a workshop at ICLR, a workshop on neural compression. So that's a certain
and excited opportunity to, you know, get together with the experts in information theory, communication,
and deep learning to indeed afford the advance of this effort of getting better codex using neural
networks. Nice, nice. So you've talked, we spoke earlier on kind of where you saw the
probabilistic numeric research going kind of more broadly when you think about your research area
and the area of your team. What, what are you most excited about? Where do you see that going?
Yeah. So indeed, we spoke earlier and I was hinting at quantum neural networks. So this is
certainly something that I believe would be a drive for innovation in AI in the future. You know,
recent years, last couple of years, I've seen a tremendous, you know, excitement in the quantum
computing community, you know, with the supremacy experiments. So we are really at an era where
we are starting, you know, to seriously think about, you know, these things. And so it's really
timely, I think, to get serious about, you know, thinking about how can you use quantum
information of quantum computation to enhance machine learning. So that's I think a very exciting place
to be. It is true that, you know, it is still a recent deal and, you know, there is certainly a lot
to do and it is still an open question to figure out what's the best way to use quantum computers
for machine learning. So that's why I think it's a very exciting area. So I've been thinking
a little bit about this over the last year. And so one of the things also, I've been thinking
about was the problem, you know, of benchmarking these models. So we talked a little bit about
this direction where you can use the quantum optical computers and the relation to, you know,
probabilistic numerics, CNN and so on. But, you know, in general, the problem here is that we do
not have these devices to run the quantum neural networks as scales that we would like right now,
right? So what do we do? Certainly, we can indeed base on intuition and small experiments,
figure out what are the most promising models. And I think that I worked on was to also try to
find an interpolation between, you know, your classical neural net and a quantum neural net.
So basically, we came up with this quantum deformed neural networks, which is, you know,
is some work I did also with Maxwell. And so the idea here is that, you know, you can think
about your classical neural network as embedded in a quantum computer. And in fact, the architecture
we are thinking about now is the qubit architecture, which is an interesting relation, you know,
to binary neural nets, because you know, a qubit is basically the, you know, quantum equivalent
of a bit. And so there is a relation with quantization of neural net, so that you can also explore
using quantum mechanics and so on. But the most interesting thing is that indeed we managed to
map this binary neural net or probabilistic binary neural net. In fact, on a quantum computer,
using qubits. And then we started to deform this model, so to introduce gates, which, you know,
use pure quantum effects like entanglement and superposition. And so we do that in a way that,
you know, we slow interpolate between a regime, which we can simulate classically, which is basically
the classical neural net regime. And the regime, you know, which is basically intractable classically,
which will require a quantum computer. And along the way, there is some, some, you know,
intermediate regime where you can still do some classical simulations using some tools from
quantum physics, which are called tensor networks. And basically, this allows you to start from a good,
you know, prior somehow for your model, which is this classical network, the format by doing
this, indeed, these modifications. And then you can, you know, use the classical simulations of
the quantum model to learn that. So that you can implement as a differential program. And so we
show actually some modest gains with respect to, you know, the classical neural net by introducing
this quantum gates. And we can actually provide, you know, the first example where you can simulate
quantum model at the scale of, you know, real world data. So that was interesting for us.
But yeah, more generally, you know, there are many, indeed, the different directions at the moment
are also related to optimization problems. So we talked about computer optimization, right?
And machine learning and also, you know, quantum computing is also an exciting area for exploring
new algorithm for combinator optimization. So yeah, to summarize, indeed, the quantum AI, I think,
is a very interesting direction. And also, I also think that the machine learning for combinator optimization
is a very interesting direction. So this is also pretty recent. And I believe that here we will see,
you know, large scale adoption of this technique, because it has been shown recently that, you know,
you can actually enhance your classical solvers for, you know, integer linear programs or stuff
like that, which, you know, people use routinely for solving their problems in the industry. You can
actually, you know, augment with neural networks, these solvers in such a way that the decisions
that these solvers make are better informed and basically are faster. And the idea here is that,
you know, you can adapt, basically, your, your solver to the data distribution that you really
care to solve, you know, a delivery company which will routinely solve, you know, the driving
assessment problem in the same city, you know, if you not want to deal with the most arbitrary
hard instances of a traveling assessment problem. And this is where machine learning, I think,
we really put an edge. So it will allow you, you know, to tailor your optimization algorithm
to the instances that you care about. And ultimately, in ultimately this, you know, leads to,
you know, better performance for combinator optimization solvers. And, you know, and also
potentially, you know, it allows you to discover new strategies, like using reinforcement learning
as we have seen, you know, you know, alpha-go alpha-fold. So super excited, I think. Awesome. Awesome.
Overbirdo, thanks so much for sharing a bit about what you're working on. And what some of the
folks in your team are working on at ICLR. It's been really great chatting with you.
Thank you, Sam. Pleasure for me, too. Thank you.
