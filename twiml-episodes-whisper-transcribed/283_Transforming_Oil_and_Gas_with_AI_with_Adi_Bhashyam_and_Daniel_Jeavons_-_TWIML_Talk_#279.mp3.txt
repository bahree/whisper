Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
You are invited to join us for the very first Twimblecon conference which will focus on
the tools, technologies and practices necessary to scale the delivery of machine learning
and AI in the enterprise.
The event will be held October 1st and 2nd in San Francisco and early bird registration
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.
Before we continue, a big thanks to the folks at C3 for their sponsorship of today's show
which was recorded at their C3 Transform Conference earlier this year.
C3's AI suite helps enterprises rapidly develop, deploy and operate AI at scale and offers
adaptable AI powered applications for predictive maintenance, fraud detection, customer engagement
and many more use cases.
Take a look at what they're up to at c3.ai and if you enjoyed this conversation be sure
to let them know on Twitter at c3 underscore ai and thank them for their support of the
show.
Alright let's do it.
Alright everyone I am here at the C3 Transform Conference in San Francisco and I've
got the amazing pleasure to be seated with Dan Gevins, a veteran of the podcast and
Adi Bashaam, Dan is the general manager of data science at Shell and Adi is the vice
president of Ford Deployed Solutions, alliances and strategy at C3.
So Dan was on the show twimblecon number 202 back in November which was rolled out as
part of our AI platform series.
So Dan I'm going to refer folks back to that show to get your full background but before
we dive into the heart of the topic Adi tell us a little bit about your background.
Sure happy to.
So I have had a few roles at C3 but I've been at C3 now for about four years.
I currently lead forward deployed solutions that think of that as sales consulting, sales
engineering.
I also lead other alliances and strategy arc.
I joined C3 in the product organization and had to get my hands dirty both verifying
machine learning algorithms deployed correctly and also writing product spec, actually build
applications that use those algorithms to actually solve business problems.
Prior to C3 I was at McKinsey and company.
I was there for a total of about 12 years which in consulting years feels like 50.
And in that time and most recently before I left I was largely leading our predictive
analytics and big data solutions focused on B2C subscription businesses, things like
telecom and other SaaS businesses.
I have an MBA from the Kellogg school at Northwestern University just not the Chicago and I'm
a computer science and I have a computer science degree from IIT Bombay.
So I have my graduate work was an electrical engineering at Northwestern.
I lived in the basement of tech for I live mostly in Jacob's center, but yes.
Nice.
Dan I just had an opportunity to catch part of your presentation here or actually all
of your presentation here at the conference and one thing that really struck me was at
least from my perspective based on our last conversation how much has matured around
AI platforms at Shell in just four months.
So you had a slide that says Shell.ai and like a platform or three different we didn't
talk about any of that stuff.
Talk about the kind of evolution over.
So it's kind of funny because obviously you get to talk about more as it matures right
so that's that's part of it.
Obviously you didn't come up with it and do it in form.
Exactly.
Exactly.
No I think we were looking for a banner to bring this all together.
So I talked a lot about in the presentation the real importance that this is not just
a platform play, it's also a play around how do you create a narrative and a culture
and a way of working that starts to drive transformation right across the business.
And that's the vision really.
So we're doing that under this banner of Shell.ai.
If you want to check it out we have a website now, Shell.ai is real, you can just google
it and you'll see a lot of the things we're doing.
We've got some little videos out there talking about some of the work that we're doing
and we're going to put more and more out there over the coming months.
And it's becoming a bit of a narrative within Shell and also something we're using quite
prolifically now with the external world as well.
And just out of curiosity why externalize that?
I'm assuming you're not going to have external users of Shell.ai, I'm not going to go create
an account and put in my credit card and run some TensorFlow.
Yeah, that's right.
Exactly right.
It's not going to be that sort of thing.
I think there will be, so there's two reasons.
And increasingly we recognize within Shell that this whole journey is about partnership.
It's about partnership with companies like C3.
It's about partnership with the innovative tech startups right here in San Francisco
of which I'll be meeting a few this week.
It's about also trying to attract the top talent that want to work on really tough problems
in the energy industry.
And the final thing is it's also about partnership with some of the people we want to work with
and because a lot of our assets are ventures there.
It's not just Shell, it's other companies involved.
So we need to be able to say to them, this is what we're doing.
Now let's work together.
So our spirit is not one of creating a product that we want to sell, but it's much more
something which we say let's work together on this because that's the whole spirit of
digital.
So let's maybe dig in a little bit into this platform that you've created.
You described kind of three specific elements.
Do you walk us through the elements and the roles that they play?
Yeah, absolutely.
So if you think at the raw end, we've got a whole raft of advanced citizen data scientists.
It's a horrible term.
I don't like it, but it kind of characterizes what they are.
These are people who are engineers there, subsurface specialists in Shell.
They've typically got some sort of technical background and they're quite comfortable with
algorithms.
So the question is how can I make it really easy for those folks to use some standard
tools that are available on the market to come in and rapidly prototype something.
So we work with companies like Ultrix and Databricks to make it really easy for those people
to come in and also other companies like MathWorks, for example, and the MATLAB product, which
actually has come on quite a long way in the last few years.
So we create that environment largely focused on R and Python to allow them to prototype
these things quickly.
We've then got kind of a group of people who also need a platform where they want to
train deep learning.
So we're applying deep learning in spaces like Autonomous Well Drilling.
We talked about that the last time, deep learning on seismic fault detection.
We're also looking at deep learning applications in a number of other areas, so things like machine
vision or natural language processing.
And those guys tend to want to work in the latest open source technologies.
So they might be comfortable using a Databricks and a Spark, but they also need things like
Cubeflow and Kubernetes.
They want to be able to manage their data versionings.
So we work with companies like Packidum for that.
So we're looking at those sort of technologies and bringing those together into an integrated
hole to allow our real specialist data scientists to train things at scale and to make sure that
they're able to deal with the big data volumes in an predominantly cloud-based environment.
But then you've got to take that to the masses somehow, right?
So you've got to be able to deploy that easily and effectively.
And so we're working with the cloud vendors, but we're also working with companies like
C3 to help really take those to enterprise grade software solutions that you can then
deploy out to your business users and that they can get the benefits from the applications
that we're building in the context of their day-to-day work.
Clearly, as an organization, you're not afraid of rolling up your sleeves and integrating
pieces together.
A lot of the other companies in fact that we featured in our platform series, the likes
of Facebook and Airbnb, they're building platforms.
They kind of rolled up their sleeves, glued a bunch of stuff together or welded, in
the case maybe, you're not afraid to do that.
Why rely on a partner like C3 to provide more of an off-the-shelf kind of solution as
opposed to just going that extra step and doing the welding or gluing?
It's a great question.
I think we thought about it, so just being completely honest and Adi and I had some
tough conversations around this, but I think the real thing for me was, if you look at
my data science heritage as an organization, we trace our origins, my team right back
to the 1970s, where Shell was the industry leader in things like scenario planning and
we found it from the first statistics groups in the industry, the issue is that we don't
have that same heritage in software engineering.
We were really looking for a partner that's going to help us move forwards with not just
the stitching together of being able to make it work, but actually being able to make
it work at scale in a production-ready product that's going to allow us to deploy this right
across our business very, very fast.
That's something that we weren't, we didn't feel we were fully equipped to do and that's
also where C3 have been helping us in that whole journey.
And I think the other thing that's worth saying is we liked the reusability of that type
system.
We know that a lot of the setups that you have in the sort of, I guess the cloud space,
they don't really think about a common data model to knit it all together.
Having that ability to create reusable data assets that can then be built on for other
software development is actually pretty powerful, particularly when you start thinking about
a world in which you've got a lot of common data in common areas that is then reusable
for multiple use cases.
Now, I think we've talked about in my previous interviews here a bit about the data model
and the notion of virtual data lakes and the like, but from your perspective as helping
to support Shell, how have you been able to kind of bring these ideas to bear to support
their use cases and what are some of the things that you've learned in that process?
It's a great question.
I think, let me answer that question to very distinct ways.
In the work we are doing with the Shell COE team, the center of excellence team, the core
value of the C3 AI suite or the platform is really around very high or rapid extensibility
and reusability.
And hopefully, and I think we're doing this, though there's no easy peer to peer or
like to like comparison, significantly reducing the amount of lines of code you actually do
write when you build an app.
So to give you an illustration of that with Dan's team, what we were presented with was
a fully working solution that they had prototyped that had scaled to about an order of 28 to 30
units, in this case valves, right?
All drawn on a historical data set with some data feeds from one location.
What we did with it as part of a very intense month in the month of May last year was replicate
that historic data set 2,000 times, thus creating something like 400,000 unique valves that
we had to deal with, not 20, but 400,000.
And then train automatically in the platform a unique machine learning model for each valve,
discovering along the way the features that affected the performance of that valve under
the normal operating conditions, as well as under anomalous operating conditions.
The work I did to do that was entirely baked into the platform, really what I wrote was
a parallelization script that went and trained 400,000 models, right?
Turns out in doing so I spun up something like 150 workers, elastically in the cloud, used
it to train these models, persisted those model weights back into the platform as objects
with metadata, callable as an API, and callable on demand.
And having seen that, I think the question naturally became why not use this now to scale,
right?
And that's kind of where we ended the proof of technology and said, okay, now let's put
this into a production type application.
Where we are in that journey is like Dan described a little earlier in our, in his keynote
at C3 transform, we have fully scaled out from 20 valves to about 400 valves from one location.
We have also built out the infrastructure to talk to three distinct data sources from
Shell, along with what we call canonicals that talk to those systems seamlessly irrespective
of what data is being sent to us.
But similarly, I have trained not just the model itself, but I've written algorithms
or programming logic that trains the model automatically for any one of those assets
and persists both the model and all associated metadata with it and makes it available
to the end user, right?
When you start solving those problems, what happened naturally in the course of this
is one project, the valves project got started earlier.
They had more teething problems and they just kind of had to solve issues along the way.
The second project, the compressor project started about order of two months after valves.
And when they started, they realized from their teammate sitting kind of two desks down
that that teammate had already solved the problem of ingesting all sensitive data.
That teammate had already solved the problem of what a model looks like in Shell with associated
metadata fields.
Had already solved the problem of transforming data from the raw data feed into the variants
that I need to use to support the asset hierarchy within Shell.
Done problem.
The other piece that got better in the second time we did it was the understanding of what
the app needs to do, right?
And this is to dance point around actually building cloud-based software applications, right?
What's the specification of what problem we're solving?
What should each screen look like?
Having done that, invariably because everything in C3 is an object that we call a type, had
digitized some parts of the UI and other elements in the middleware that actually transformed
data and make it presentable, were instantly available downstream for other applications
as well.
So there is immense reusability.
We solved data integration problems effectively once and then reuse it for all cases.
And by the way, having done that, you then point your data scientists at actually solving
the complex problems, which is how do I now detect a compressor anomaly every 10 minutes
with very high degree of certainty?
That's the hard problem.
Not the problem of stitching data together and building data transforms and doing the plumbing.
And to that point, as it relates to scaling things up, that's where I believe C3 really
makes a difference.
The other piece that I'm talking to Darbert is to also use C3 as a means of experimentation
as well, right?
So the idea is if a model is a type with associated metadata, a Jupyter notebook is also a type
with associated metadata.
And so we are about to launch essentially what we call the notebook service.
So you spin up a notebook on demand, whether it's an R notebook or a Python notebook.
It turns out once you do that, the platform, which doesn't care what it's asked to store
as long as it can represent it somehow, can store notebooks, can store versions of notebooks,
can store variants of notebooks, including ownership data, can allow identity-based access
control into those notebooks so that you're seamlessly connected and can get going.
With no requirement whatsoever to store data in C3 or use the C3 platform for anything
more than that, turns out that when you're actually solving a problem that has some gold
as you mind, it is then a trivial exercise to push it into C3 and then scale with it,
right?
And that's where we're going on that side of it.
So the idea is, start with scaling, we got that.
But then let's also extend to experimentation, not just for core machine learning, but also
then deep learning frameworks, NLP, etc.
And just building on that, it's quite appealing because just to quantify some of the scale
we're talking about, if you look at just one of our assets, it's spitting off around
100,000 measurements per minute.
So if you think of that over five years, you're talking between 70 and 80 billion rows
of data.
And if you're a data scientist in that sort of domain, actually just getting hold of that
data is problematic and being able to put it in a form where you can process it.
If C3's already done that data munging for us and put that in a form with some of these
production apps that we're already developing, and they can manage versioning on the experimentation,
it's quite an appealing prospect.
So I think there's a lot of merit to the platform.
I think the challenge is obviously getting it to the point where all of this works together,
but we like the vision.
So I think one of the things that you mentioned that I'm curious about in the process of
this Valve project, replicating the data, I forget how many times, 440,000 or 2,000 times.
In some of the previous conversations I've had today, one of the themes was not needing
to replicate the data.
Can you just reconcile those ideas for me?
Yeah, it's absolutely.
I think the idea of not replicating the data, which you might have heard from some of
the other conversations you had with C3 folks, was really around this idea of referring
to the capability of the C3 platform to virtualize any data source.
And so the idea is, if you've already stored some data in the cloud, or you've already
built something where it sits in a data warehouse that you can access, you do not need to then
copy that over and make a copy of that entire data set inside the C3 cluster.
Because the C3 object model allows external virtual references to any system that it can
access, right? So in that sense, there is no need to copy data again if you've already
made it available.
What we're talking about when I said we replicated data 2,000 times was more to just purely demonstrate
scalability, right?
So it's, I have one unit of data.
We have not yet had done the work to extract all the data from all parts of Shell, but
I know it's there and I know I need to work on all of that data set at some point.
But because Dan wanted to see it work at that scale, we said, well, you don't have it.
What's the best way I can mill it?
And so I literally ran a job on inside of C3 to replicate that 2,000 times and port it.
Got it.
So that's all we did.
One of the things I find so interesting about this conversation that ties to some work
I'm doing around this platform Zbook is this idea of scale.
And like we throw this word around, and it means more than one thing at least.
One of these things is we've got 100,000 measurements for this one system per year, and
if we have 10 of these systems, we multiply, right?
But there's also a sense in which it means the ability to scale our ability to get models
in a production, right?
The ability to operationalize more quickly.
Dan, I'm wondering, when you think about, do you have different words for that, or how
do you think about that whole space?
Well, I think my problem is a number of, it's scale at every level, right?
So let me just talk about that.
So if you talk about my business, so we have 43,000 retail sites.
That's just one part of Shell.
We're bigger than Starbucks, bigger than McDonald's, and the retail business, in terms of our
global network.
So I have a problem that I have lots of things across my business, and I do all of those
things create data.
So I have a data problem, if you will, that I've got vast scale in terms of the data that
I need to deal with.
That's one.
But then if I move that up a level, I've then got a problem which is I need to aggregate
that data at scale.
So I've got to be able to bring that into an aggregate environment in the cloud, because
a lot of these things aren't cloud native.
They have legacy systems.
We need to be able to bring all that together.
Then I have a problem at scale in terms of machine learning, because if you think about
the vows problem, it's not good enough to do 16 vows and solve the problem for one business
unit in a very small area of the business.
I need to be able to do this for half a million vows worldwide, and so I've got a machine
learning at scale problem, which means I've got a model management problem, which is something
we talked about a little bit the last time.
And then I've got a problem with the fact that I've then got users at scale.
So the people that want to need to consume that, I've got potentially tens, hundreds, thousands
of users across the whole of Shell.
We're 80 plus thousand person organization.
We've also got huge numbers of contractors, partners, suppliers, all of them potentially
need to get insight from some of the applications that I'm developing.
And so at the end of the day, it's that scale at all of those levels that's so problematic.
And you actually have to have something that can solve all of those things if you want
to get anything into production.
And so we talk a lot about the focus on scaling and replicating, so getting it to that scale
and then replicating it to allow it to generate benefits because the problem with digital technology
is it's very easy to solve the problem once, but it also tends to be very expensive.
And the benefits in any business model in the digital space comes from that replication
thrust.
And so the key thing is we've got to be able to create a platform strategy that allows
us to do that because otherwise we become a very expensive cost center.
Your last comment reminds me a bit of the Nvidia founder and CEO, whenever he gets up on
the stage and holds up a new GPU, he's like, this GPU costs a billion dollars.
Exactly.
Right?
Because that first up front effort is so significant, but then as you're able to replicate
it, the incremental cost goes down.
Exactly.
Right.
You mentioned in your talk, I think a new use case that we hadn't talked about previously
the reward engine.
Yeah.
Is that one new?
Yeah, it is new.
It's one that we've just gone live with pretty recently.
It's now live in the UK market.
What it's all about is we talk a lot in our retail business about treating customers as
a guest.
So when you come to a shell station, we want you to feel like you're the most special
person in the world to us and we want you to have a great experience.
And that means treating you really well with our service champions as we call them.
And you actually arrive on the forecourt or in the store, but it also means digitally you
need to have a fantastic customer experience.
And so we did a lot of thinking about that.
And we basically took the same principles that Amazon and Facebook apply and we said,
how can we make the experience that you get whenever you turn up at a shell station
extremely personal?
So we know what you want from us and we want you to have that experience.
We want you to have the sensitivity around how much data you share with us.
But once you do share your data, we want to give you a great experience with great
offers and great benefits of being part of shell.
And so that's what the reward engine is all about.
It's a capability that sits under the hood of our loyalty system and it effectively dictates
what you would experience you get at the site when you turn up to shell.
And we're really proud of it.
It's just gone live in the UK, like as I said, we're planning to roll it out globally.
Can you talk a little bit about it technically in terms of data sources, types of models,
what the models are doing?
To a lot of the data comes from a combination of things, it comes from transaction history,
it comes from information we have about the store, where it is, what the traffic around
that is, et cetera, et cetera.
We also have a whole bunch of information around, for example, the loyalty scheme you've
done with us previously, how many visits you've had, what sort of things you've bought
on the loyalty scheme, how many points you've got, how many visits you've got, I should
say.
And so we take all of that information and we effectively, we take a Bayesian inference
approach effectively and I'm horribly simplifying it.
But what we do is we say a shell customer typically buys fuel and then we overlay a clustering
approach and a series of rules as well to create a combination of things we want you to
have.
So we want you to get rewarded for frequency.
So we apply visit logic over the top.
But we also infer from history of customers who behave in a similar way to the way that
you behave.
So it's kind of like a camins, if you think about it that way.
And so we're trying to cluster the behavior, infer your behavior, but ultimately try to
then create offers that are relevant, but then continually iterate on that.
So that's the inference aspect.
So we constantly overlay behavioral observations in real time, back onto that customer and then
use that to make the offers.
One of the things I've learned over the past, over the course of the day in some of the
sessions here at the conferences, kind of the evolution of C3, right, started it with
the focus of this data layer that we've talked a bunch about.
And actually you correct me if I'm wrong.
I mean, the impression I have is that the company's kind of been pulled into machine learning
in AI and working with companies like Shell and NL and others and is kind of building on
that underlying data platform.
Yeah, I think the question that I'm getting is like, there's so much, you know, a data
platform is a huge challenge.
And then we're talking about Bayesian inference and TensorFlow and machine learning models
like the, I think one of the speakers earlier talked about the number, it was maybe Tom talked
about the number of connections between all these things.
How do you manage that?
And I'm presumably part of the goal is to keep that simple for the people that are using
the platform.
I imagine it then wants to keep things, you know, the platform view simple to his consumers.
So let me answer that question two ways, Sam.
The first is it would be helpful to your listeners to understand how C3 got to where we are,
right?
And a little bit of just our evolution and and why it is the way we do certain things
in certain way.
The second is to give you a sense of how we think about platform users and frankly the answer
is there are many platform users, they're all very different and you've got to expose
different parts of the platform to them, but not everything to everyone.
And in doing so, the right way is how we manage down the complexity.
So let me answer the first thing first, right?
So so C3 got started as a carbon trading company, okay?
And the original C was represented carbon, right?
And it came out around, I mean, the conversation at the time, and this is going back to 2009,
2010, was around this thesis that because of the carbon trading act, everyone will need
to measure their carbon footprint.
And then because some have better and some have worse carbon footprint relative to each
other, that is the opportunity to both measure it, monitor it, and then mitigate it by trading.
Right?
So there's therefore a natural need for companies like Cisco and Dell and GE to actually
have software that measured their carbon footprint.
And you would need to therefore measure facility use, person use, travel use, all of these
different things.
And so the company that is now C3, was also called C3 at the time, but the C stood for
carbon.
And we got going in the business of really being able to ingest a huge amount of data
all related to measuring a company's carbon footprint, where did this data come from?
It came from facility plans.
It came from energy utility bills.
It came from energy utility meters.
It came from self-reported lead certification of buildings, things like that.
Turns out we built that company or that piece of software in a way that said, and this
was I think ahead of its time, that recognized that if we went to 10 enterprises, each of
those 10 enterprises would have a different data model, different sources of data, but
they were all trying to solve the same problem.
And so we built that first piece of software to essentially allow a modular separation between
the data we were getting and the use of that data going up.
And because a lot of our hunters came from civil systems, that was the core of how it
was built.
And so a lot of that DNA kind of got transferred over.
Turns out that market went to bust, carbon trading sort of evaporated.
And we said, okay, we built this technology, what do we do with it?
What we did with it was point it at a problem where there was a huge amount of data available
that was the same carbon trading problem time, 10 or times 100, where we believed we could
engineer the scalability required to solve the problem.
And that was really building software solutions for the electric grid or the utilities, right?
Which was at the time, and this is 2011, 2012, the most censored industry of them all, right?
Sensored in the sense of having a lot of censors.
Having a lot of censors as opposed to anything else, right?
Turns out that these guys have been actually pretty good at installing censors, deploying
censors, not just for smart meters, but also transformers, vibration sensors on grid
equipment, re-closers, the works.
And now one of our largest customers whom you may have heard on stage today was our customer
when we were that company called C3 Energy.
And the first problem was, can you deal with all this data, turns out we could.
Because again, because we had modularized and abstracted the way we built the original
piece of software, adding cloud services, elastic scalability, et cetera, came naturally,
while retaining that abstraction.
This is kind of the genesis of what we call the type system.
And so the first problem to us was, can you handle 800 terabytes of data that I sent
to you and then process it at a million transactions per second?
No AI, no machine learning, that was it, right?
Turns out we could.
And turns out we proved it to them in order of a few cycles, just like dance, one million
model problem.
Having sought that, they said, okay, what do we do with this technology?
And it turns out the most valuable problems to them, again, going back to like, where's
the money, were in the application of these very large data sets, but also machine learning
algorithms on that.
And so we built in the same modular approach, the ability to build both temporal and spatial
analytics, and then machine learning models using those temporal and spatial analytics,
again, in this abstracted way.
So that everyone's writing as little code as possible, but you're expressing logic very
quickly.
That's been now hardened to the point where it works seamlessly on all flavors and
all types of data over the next, over the subsequent five issues, right?
And in doing so, we've gone from offering SaaS solutions to the utilities to a platform
that has these capabilities to all industries.
And in doing so, we've now focused on building out these capabilities that look like better
cloud services, better performance, better ML and AI and application development tooling,
and more applications, all using the same logic.
So you could call it chance, you could call it a natural necessity of the types of problems
we solved, and therefore we're exposed to next.
So as I like to joke, the price for the pie eating contest is more pie, and we just kept
winning them every few weeks.
And that's been our evaluation, right?
A lot of the initial impetus came from customer conversations, right?
It's like, hey, what problems do you need that are real actually need solving?
A lot of it came from our roadmap, and we see this constant conversation going on, and
it happened today, and it will happen tomorrow in the rest of our conference and so on.
And that's how we think about this, right?
So there's, and if you listen to Tom talking about the types of people we hire, we hire
people that are restless, that are not willing to say the jobs done were done, right?
There's always something new to solve, and it's constantly evolving.
I suspect, if you look at our road, if you fast forward a year and look back at what
we built, about half of the things we built would have come from our own product managers
thinking through like, what do we need?
And the remaining half would come from customer demands, and it's like, hey, I need this to solve
this problem.
You haven't solved it yet.
Well, fine.
We'll go solve.
Right?
And that's really how this will evolve.
As we go.
It's managing the complexity of all that.
So now let's go to that.
Right?
So as we grow our capabilities in the platform, I think about how we organize ourselves, right?
Which gives you a window as to how we think about the world.
We have a core platform engineering team and the core platform engineering team deals with
everything related to infrastructure services, whether they are cloud or on premise.
They do everything related to security and authentication.
Is it there's a pod that deals with data management and data source externalization?
So how do I write connectors to MongoDB or Snowflake or whatever else?
And there is a processing analytics engine team within the platform team that deals with
what's the most efficient way to store a time series across archival storage and SSD
storage so that I can actually access data in the fastest way possible.
And how do I parametrize that so that it's easy to solve?
All of these pods in core platform engineering are building with the same mindset, which
is how do I abstract the services I provide to the layer above me from the raw code I
write to actually manage the layer below me?
Right?
Everyone's doing the same thing.
So at that level, you've got those pods.
If you solve those correctly, which is kind of what they do, we then offer up to the next
team within C3, which looks like application development, tool development and machine
learning engineering, a series of native services that they can then use to build their own
services.
What are they doing?
Machine learning engineering is literally building what we call an ML pipeline, right?
An ML pipeline is essentially a sequence of steps that's executable in a runtime that
is declared on compile or previously that can use any framework that you want to pull
in, whether it's TensorFlow or Keras or anything else, along with any data processing and outputs
that you need to do with it, right?
And what they're doing is really writing bindings that can use a series of these frameworks
and a series of external tools that can work with the platform.
Application engineering is building applications, inventory optimization, predictive maintenance,
et cetera, with features, functionality directed by the product managers, invariably
in conversation with our customers.
And the tools team is building via metadata APIs that the platform team exposes, the tooling
to then change platform settings through very minimal, either no code or no code tools
that they make available through browsers, right?
And then we have customer service and sales engineering teams that use all of these capabilities
to then go solve customer problems with a rich toolkit that sort of hangs on their
ways like Batman's utility belt and they're pulling out what they need, right?
But very rarely are they solving problems that go down to say, I need a new database
connector.
If that's the case, the platform team goes solves and it makes available a service above
that they use and so on.
Everything in C3 is a type and therefore the database connector to Impala is a type,
right?
The machine learning pipeline that uses Tesseract is a type.
The application widget on the inventory optimization screen that shows the latest deviation from
safety stock is a type, right?
And the data integration tool logic that finds the best match for any ingested data terms
of its target source in terms of its target field is a type or a functionality, right?
And so everything's abstracted always and you're really dealing with internal APIs that
work seamlessly across the platform.
That's how we manage down the complexity.
Now if you're a customer, right, then we are, then the question to ask is, what are you
in that customer?
If you're an end user of a customer, you don't care about the C3 platform at all.
All you care about is what's the URL you forget to and what's the big red shiny button
I need to press, right?
That's all you care about.
If you're an application developer in a customer, at a customer site, like a lot of Dan's team
that are building applications with us.
You care about the data ingestion APIs, the machine learning engineering APIs, maybe
the application logic APIs.
You don't care about the platform fundamentals at all because that's given to you.
If you're a data scientist on the safety platform, you care about what pipelines the machine
learning engine team has made available and how I can call the time series engine or the
data wrangling engine in Jupyter Notebook to do what I need to do.
That's really all you care about.
So depending on who you are, it actually changes what we expose and that then also manages
down the complexity.
Frankly, there are about three people in the company that know it all and let's just
say that they are incredibly valuable.
Before we started rolling, I was mentioning that one of the first articles I wrote seven
years ago, seven plus years ago when I started my company was about machine learning platforms.
And at the time, I heard a lot of feedback that says that we're never going to be able
to generalize machine learning.
Like we're going to have to, you know, it's always going to be a problem that's solved
on a snowflake by snowflake basis on a problem by problem basis.
Dan, you're building platforms, what's your take on that?
What's your experience on that and how close are we to, you know, can we call it an
Irvana of having a generalized platform that we can apply to a wide variety of business
problems?
I think we're getting closer and closer to that.
I think I would, so I'd answer that in a couple of ways.
I think the first is that we still have a view that you still solve machine learning problems
case by case.
So we start with the hypothesis.
What is the core question you're trying to answer and how do you prove that with data?
It's still fundamental to, you know, the scientific method, if you will, that we employ in the
way we go about solving any problem we get from our business users.
What I think is increasing the interest thing is that we are starting in the same way as
the software industry figured out that even when you're solving a problem, you can create
your reusability.
We're seeing that same trend emerging in machine learning.
So you can have generic pipelines which are mostly the same user-consistent set of frameworks
and then ultimately can be deployed to solve different problems within a given space.
So the example I always use is natural language processing.
So I talked about Tesseract, right, Space C is becoming commonplace in that domain.
At the end of the day, what you can do in most cases is stitch together a series of building
blocks with Python, NLTK and those sorts of things and ultimately get to an end product
where you can use that same pipeline to answer multiple questions.
Now, you still need a data scientist that knows how to tweak the knobs if you want to
put it that way.
But at the same time, you can reduce the time to value so you're not rebuilding everything
every time.
And that's really what the theme across my team, we're really looking to say, when I go
to solve that problem, can I take a reusable asset that someone else has built somewhere
else and it reduced the time to value, whether that be at the data level or at the framework
level to allow you to solve the business problem as fast as possible.
Now I think the next generation of that is making that accessible so the end users can
tweak those parameters.
And that's the AutoML development, it's the work in the self-service and actually some
of the things that C3 are trying to do to expose some of their core platform functionality.
And you see that right across the industry right now.
I think the other thing that's going to come and hit us quite hard, though, which is
emerging is how do you infuse data security into all of that?
Because I think there's been an explosion of ideas in this space and somehow we need to
make sure that we do that responsibly and there's a lot of thinking going into that, particularly
within Shell right now, to make sure that we've got the platform right underpin all of
that and that those reusable assets have also security baked in.
So I see the whole industry growing up, I see it becoming much more like the software
industry.
In general.
It's a great analogy.
We started this interview talking about the strategy you've made in four months.
You want to predict us four months out?
That's a great question.
So there's a few things on our roadmap.
I'll talk about some of those.
I think for us, optimization is the next big horizon.
So how do we start to use a combination of traditional optimization techniques, stuff
like Cplex, for example, and traditional solvers, in conjunction with new optimization techniques
like deep reinforcement learning, to start to solve some of the toughest optimization problems
across the energy industry?
That's a really exciting space for us.
I think the other thing about that space is that's when you start ironically getting back
to C3's original mission, which is trying to reduce carbon, because in that whole space,
if you can really optimize, you can start to look at trade-offs between your greenhouse
gas emissions and your production, as well as trying to look at places where you need
to tighten up.
So I think that's going to be a huge thing for us.
I think the other thing that's super interesting is how do you start to look at an emerging
class of energy customer who are typically very green in their outlook, tech savvy, and
wants everything digitally?
So how do we meet that need as an organization?
And how do we make that, make it really easy to interact with Shell, and how do we give
you a range of offerings that allow you to solve your energy needs, and do that taking full
advantage of AI?
And that's the vision that I'm super excited about.
That's where we're really trying to push the envelope next.
Well, Dan, Eddie, thanks so much for taking the time to chat with me, great conversation.
Thank you.
Thank you.
All right, everyone, that's our show for today.
For more information about today's show, visit twimmelai.com.
Be sure to visit twimmelcon.com for information or to register for Twimmelcon AI platforms.
Thanks again to C3 for their sponsorship of today's episode.
To check out what they're up to, visit c3.ai.
As always, thanks so much for listening and catch you next time.
