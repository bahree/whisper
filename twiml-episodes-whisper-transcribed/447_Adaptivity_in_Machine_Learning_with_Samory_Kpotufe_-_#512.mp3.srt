1
00:00:00,000 --> 00:00:19,160
All right, everyone. I am here with Samary Potoufet. Samary is an Associate Professor at Columbia University.

2
00:00:19,160 --> 00:00:26,080
Samary, welcome to the Twomol AI podcast. Thank you. Thank you, Sam. Let's get started by

3
00:00:26,080 --> 00:00:33,960
having you share a little bit about your background. You work in statistical machine learning. How

4
00:00:33,960 --> 00:00:45,600
did you get there? How did I get into statistical machine learning? Somehow, somehow, I

5
00:00:45,600 --> 00:00:56,400
started in math in undergrad in mathematics. At some point, I was doing mostly pure math in

6
00:00:56,400 --> 00:01:02,800
undergrad. At some point, I wanted to do something where I could see the applications of math,

7
00:01:02,800 --> 00:01:09,520
where I could see the impact of mathematics. I wasn't so sure, so I went off and worked for

8
00:01:09,520 --> 00:01:16,400
some time. Then, at one point, I was watching TV and so robots on TV and got interested in how

9
00:01:16,400 --> 00:01:22,880
that was done. So, I started applying for grad school and eventually I got into machine learning.

10
00:01:22,880 --> 00:01:27,280
I didn't know at the time that it was going to become a big field and now it's a big field. So,

11
00:01:27,280 --> 00:01:33,760
great. That's awesome. Tell us a little bit about your research interest. What do you focus on?

12
00:01:33,760 --> 00:01:42,960
Very generally, I mean, I'm interested mostly in machine learning itself. However, in statistical

13
00:01:42,960 --> 00:01:50,080
aspect of machine learning, and by statistical aspect, I mean questions such as how much resources

14
00:01:50,720 --> 00:01:59,120
are needed to achieve low error in classification, for instance. By resources, resources could be

15
00:01:59,120 --> 00:02:09,760
number of samples. It could also involve constraints on the problem. So, maybe we cannot always have

16
00:02:09,760 --> 00:02:16,080
labeled samples. Maybe we have to return, maybe the classifier has to return a classification

17
00:02:17,280 --> 00:02:23,280
fast. So, there are all these potential constraints in real world applications that I'm interested in.

18
00:02:23,280 --> 00:02:29,520
And given those constraints, what are the best, what is the best possible performance of classification

19
00:02:29,520 --> 00:02:37,920
procedures? So, at a high level, that's that's my general interest. But, but I'm interested in

20
00:02:37,920 --> 00:02:42,160
being able to say mathematically, being able to give guarantees, being able to say mathematically,

21
00:02:42,880 --> 00:02:47,120
this is the nature of the problems we are looking at and this is the nature of the best possible

22
00:02:47,120 --> 00:02:52,880
algorithms for these problems. Okay. Yeah, I was going to mention that, you know, we all care

23
00:02:52,880 --> 00:02:57,680
about the amount of data that's required. Yeah. But, there's a little bit of a different,

24
00:02:57,680 --> 00:03:04,400
you approach that question from the perspective of a theoretician as opposed to a practitioner.

25
00:03:04,400 --> 00:03:09,360
I'm wondering if you can comment on kind of the relationship between theory and practice. And

26
00:03:09,360 --> 00:03:17,600
you're the relationship for your work, you know, to practitioners. So, I'll say this, I feel like

27
00:03:17,600 --> 00:03:27,040
the theoreticians within machine learning should mostly be inspired by practice. And so,

28
00:03:27,040 --> 00:03:32,400
I try my best to be inspired by practice in the sense that I will look at practical problems.

29
00:03:32,880 --> 00:03:39,200
And as myself, what would be the key questions that a practitioner might need answered,

30
00:03:39,200 --> 00:03:44,080
and that the data might not reveal at once? Right. And so, I'll give you some examples of such

31
00:03:44,080 --> 00:03:56,000
questions. And can we use math to sort of understand these questions? And so, that's at a very high

32
00:03:56,000 --> 00:04:02,480
level, given to give an exact example, if someone thinks for instance of clustering, right?

33
00:04:03,040 --> 00:04:07,600
There are tons of different clustering procedures out there, tons of clustering algorithms out there.

34
00:04:07,600 --> 00:04:16,000
And the practitioner has data in front of them. And they might ask, which one of these clustering

35
00:04:16,000 --> 00:04:20,640
algorithms should I use for my particular application for my particular data? That's a question

36
00:04:20,640 --> 00:04:27,040
that is very hard to answer directly just from practice. Why? Because I'm taking clustering here,

37
00:04:27,040 --> 00:04:30,880
because it's hard to even test anything. You don't have label data. It's hard to test the

38
00:04:30,880 --> 00:04:38,000
quality of your clustering if you don't have ground truth. So, here you don't have ground truth. So,

39
00:04:38,000 --> 00:04:45,280
so the main thing you can rely on is modeling your problem and asking, on the these assumptions

40
00:04:45,280 --> 00:04:51,200
I can make on my data, what is the best possible procedure? And so, we try to answer those questions

41
00:04:51,200 --> 00:05:01,120
mathematically. I hope that goes at it and says it a bit for you. And maybe talk more concretely

42
00:05:01,120 --> 00:05:08,400
about the types of problems that you are trying to model. Do you think about them? Kind of very

43
00:05:08,400 --> 00:05:15,760
broadly at the level of clustering, classification, that kind of thing, or do you make more granular

44
00:05:15,760 --> 00:05:22,080
assumptions about, I guess, both the problems and the data and the constraints and other things?

45
00:05:22,080 --> 00:05:30,800
It depends on how far the particular question I'm interested in is. There are some questions where

46
00:05:30,800 --> 00:05:39,760
we are very much at a high level, where we are thinking about the basic task, such as classification,

47
00:05:39,760 --> 00:05:47,200
clustering, and such. And then there are problems that we understand quite well already. And in

48
00:05:47,200 --> 00:05:52,160
those cases, I might start thinking about specific algorithms and how to distinguish between

49
00:05:52,160 --> 00:06:01,200
specific algorithms. I hope I'm answering the question right. Maybe you want to make it a bit more

50
00:06:01,200 --> 00:06:09,200
precise. Should I make it? I think the way to make it a bit more concrete is for us to talk about

51
00:06:09,200 --> 00:06:17,760
specific problems that you are interested in and research and kind of drill into detail there.

52
00:06:17,760 --> 00:06:22,720
Let's start with a recent paper or something that you're excited about.

53
00:06:22,720 --> 00:06:28,720
Since we were talking about clustering, let me just give some concrete problems in clustering.

54
00:06:28,720 --> 00:06:37,680
Right. First, we all know at a high level how one might be fine clustering. The problem of

55
00:06:37,680 --> 00:06:43,840
clustering is, I believe that the data comes from different groups, different subgroups.

56
00:06:43,840 --> 00:06:52,880
I don't have labels. However, I'm hoping that at least geometrically in some space,

57
00:06:52,880 --> 00:06:59,040
I should be able to discover that the data groups well into so many groups. Right.

58
00:06:59,040 --> 00:07:06,160
Right. So the issue that comes up right away is what do we mean by the data groups well?

59
00:07:06,160 --> 00:07:12,960
We can say, for instance, that the data groups well in the sense that the data points belong

60
00:07:12,960 --> 00:07:19,440
into the same group are closer to each other than to other data points. And that gives a sense of

61
00:07:19,440 --> 00:07:24,480
they close their well. But the moment I say they are closer to each other, that means I'm

62
00:07:24,480 --> 00:07:29,760
making an assumption on a notion of distance. How far they should be from each other.

63
00:07:29,760 --> 00:07:33,840
And then the question comes up, what notion of distance is the right notion of distance

64
00:07:34,480 --> 00:07:41,120
here that I should be using. Right. So that all depends sort of on the downstream task.

65
00:07:41,920 --> 00:07:46,960
And so the theoretician might then start asking that question, what is the downstream task?

66
00:07:46,960 --> 00:07:53,520
And then what is the right notion of distance for this downstream task? Or we can go higher and say,

67
00:07:53,520 --> 00:07:58,240
how do we define clustering at all properly? Maybe this is not the right way to define clustering.

68
00:07:58,240 --> 00:08:01,200
Another way to define clustering is probably through densities.

69
00:08:02,960 --> 00:08:07,760
What I mean by that is, if I throw a sand on the floor,

70
00:08:07,760 --> 00:08:16,480
right, I might say that or it closes into the sand clustered into a few groups. And what do I mean

71
00:08:16,480 --> 00:08:23,200
by that? There is a different density of sand in different parts of the space, the floor here.

72
00:08:23,200 --> 00:08:29,520
Right. And so that gives me a different notion, a different notion of clustering, maybe what I'm

73
00:08:29,520 --> 00:08:38,160
trying to find in my data are regions of high density of points. Now that's a whole other, sorry.

74
00:08:38,160 --> 00:08:45,760
Yeah, you would think that the two ways of looking at this distance and density there,

75
00:08:46,720 --> 00:08:49,600
you know, they're measuring the same underlying phenomenon. There's a lot of

76
00:08:49,600 --> 00:08:57,520
there, but maybe one mathematical formulation or expression lends itself to the way you're

77
00:08:57,520 --> 00:09:04,000
trying to approach the problem. Yeah. Exactly. Exactly. Because that falls back also again into

78
00:09:04,000 --> 00:09:09,600
what do I even mean by the distance between points in the first formulation? In the first

79
00:09:09,600 --> 00:09:13,360
formulation, there are many different notions of distance that I might be looking at.

80
00:09:13,360 --> 00:09:21,840
Yeah. And in this formulation here, the notion of a density is a bit more of a robust notion

81
00:09:21,840 --> 00:09:30,000
in this other formulation. And then let's say that there is such a, that I agree with that notion

82
00:09:30,000 --> 00:09:35,120
that this is my notion, it's the regions of highest density. Right. Then there are tons of

83
00:09:35,120 --> 00:09:40,960
questions that come up. What level of density do I look at? If I look at a particular resolution,

84
00:09:40,960 --> 00:09:47,440
I might see particular clusters. If I zoom down, I might see even more clusters. So there are

85
00:09:47,440 --> 00:09:54,560
questions that come up right away as to what threshold and what threshold am I calling my notion

86
00:09:54,560 --> 00:10:04,400
of density? And can we answer that automatically with an algorithm? Can an algorithm discover that

87
00:10:04,400 --> 00:10:09,840
automatically? And in what situations, depending on the downstream task, what notion of

88
00:10:09,840 --> 00:10:15,680
closer ratio we'll be using? So the type of questions that come up there are, so first, you

89
00:10:15,680 --> 00:10:19,920
define a notion of clustering. Next, you ask the question, on the notion of clustering,

90
00:10:20,560 --> 00:10:30,000
is it possible at all to discover the clusters? To what level can I discover the clusters,

91
00:10:30,000 --> 00:10:37,040
meaning what error do I expect if I have so many data points? So here I'm assuming there is a

92
00:10:37,040 --> 00:10:40,800
ground truth, of course. And what error do I expect with respect to the ground truth,

93
00:10:41,600 --> 00:10:45,280
depending on the number of data points that I have? And then the next question

94
00:10:45,280 --> 00:10:54,000
becomes that of what I call adaptivity. Adaptivity of people might call it self-tuning and such.

95
00:10:54,000 --> 00:11:00,480
Every cluster in procedure comes up with all sort of tuning parameters. Here, an implicit

96
00:11:00,480 --> 00:11:06,240
tuning parameter that I threw in a second ago is if I talk about densities while density level,

97
00:11:06,240 --> 00:11:12,720
while level, what resolution? And that's sort of a tuning parameter. And so are there algorithms

98
00:11:12,720 --> 00:11:18,160
that can look at the data by themselves? And discover also the right tuning parameter.

99
00:11:18,160 --> 00:11:23,440
And I called, we call that, at least in statistics, it's called adaptivity. And machine learning

100
00:11:23,440 --> 00:11:31,680
people tend to call that auto-ML or self-tuning and such. And so, but those are the type of questions

101
00:11:31,680 --> 00:11:40,080
that me and other theoreticians ask, are these possible at all? Is this possible? And if it is

102
00:11:40,080 --> 00:11:45,920
possible, what are the family of procedures that achieve these goals?

103
00:11:47,200 --> 00:11:54,400
And so tying back to kind of this, you know, the problem of the practitioner as the touchstone

104
00:11:54,400 --> 00:12:02,240
or inspiration is the ultimate goal to be able to say, if your problem looks like this or if your

105
00:12:02,240 --> 00:12:08,800
data looks like this, then you want to use L1 norm, L2 norm, whatever, or, you know, this set of

106
00:12:08,800 --> 00:12:15,920
density metrics versus that set of density metrics. Exactly. So that would be one goal. That would be

107
00:12:15,920 --> 00:12:21,120
one way the theoretician can be useful to the practitioner, right, is to be able to say that

108
00:12:21,120 --> 00:12:27,920
for these types of problems, this is the algorithms that seem to be, or these are the decisions,

109
00:12:27,920 --> 00:12:36,960
the practical decisions that seem to be most appropriate. Yeah. Right. The eventual, the big goal,

110
00:12:36,960 --> 00:12:43,280
I think, for any machine learner, whether theoretician or non-territician, is to come up with data algorithm

111
00:12:43,280 --> 00:12:49,600
that is sort of almost a black box and can decide on its own which situation it is in by just looking

112
00:12:49,600 --> 00:12:54,320
at the data. And that falls back into what I was calling adaptivity. It self-tunes. At the end,

113
00:12:54,320 --> 00:12:59,840
it has nearly no tuning parameter because all its internal tuning, it does on its own.

114
00:12:59,840 --> 00:13:08,640
And identifies the setting or the situation in which it is and adjust itself. So it's a

115
00:13:08,640 --> 00:13:16,640
grand goal. We do know that very generally, it's not possible. But we also do know that we didn't

116
00:13:16,640 --> 00:13:24,880
turn on the certain restrictions on the universal problems to say it is possible. And in fact,

117
00:13:24,880 --> 00:13:30,320
to some extent, humans do it. But it's because sort of our universal problem is restricted.

118
00:13:33,120 --> 00:13:42,480
And these kind of adaptive algorithms is that something that you that, well, clearly,

119
00:13:42,480 --> 00:13:47,200
this is something that you're interested in and research are there. You know, specific examples

120
00:13:47,200 --> 00:13:58,240
of research that you've done that in this area? Yeah, yeah. So, for instance, I'll give a

121
00:13:58,240 --> 00:14:04,560
high-level example. Here, during my PhD and maybe a little bit after my PhD,

122
00:14:05,600 --> 00:14:10,800
here are some of the problems I was looking at. I was looking at simple questions

123
00:14:10,800 --> 00:14:14,720
having to do with nearest neighbor methods. These are the most basic procedures out there,

124
00:14:14,720 --> 00:14:21,600
right, nearest neighbor methods. We do know that nearest neighbor methods when the dimension

125
00:14:21,600 --> 00:14:29,200
of the data is very high, do quite poorly in general. And you can show it mathematically that

126
00:14:29,200 --> 00:14:33,280
their convergence rates, whether you're in classification or regression, that their convergence

127
00:14:33,280 --> 00:14:44,080
rate gets worse with dimension. And dramatically worse with dimension. In other words,

128
00:14:44,080 --> 00:14:53,360
how dramatic you need a sample size exponential in dimension in order to achieve decent convergence,

129
00:14:53,360 --> 00:15:00,080
right, at least in the worst case. So that's how we quantify that. So that's always been known,

130
00:15:00,080 --> 00:15:03,840
that dimension, and it's called the curse of dimension, that dimension is an issue.

131
00:15:03,840 --> 00:15:16,800
However, people then felt like maybe we can, if it so happens that the data lies, the data is

132
00:15:16,800 --> 00:15:25,680
very high-dimensional, but intrinsically is low-dimensional. So examples of that, one example

133
00:15:25,680 --> 00:15:34,320
that I like to give is consider a robotic arm with sensors on the robotic arm. It has a lot of

134
00:15:34,320 --> 00:15:40,160
sensors, so it's generating a very high-dimensional data set. However, the arm has few degrees of

135
00:15:40,160 --> 00:15:47,600
freedom. And so we expect that that data lies on the lower-dimensional surface in a very high-dimensional

136
00:15:47,600 --> 00:15:56,960
space. And so what people don't try to do was come up with procedures called manifold learning,

137
00:15:58,240 --> 00:16:05,280
procedures, which will try to discover that manifold, and then re-represented that in that

138
00:16:05,280 --> 00:16:11,680
sort of manifold space so that it's now lower-dimensional, and then try to run these algorithms

139
00:16:11,680 --> 00:16:17,360
such as nearest neighbors in this lower-dimensional space, where the hope is that it will perform much

140
00:16:17,360 --> 00:16:22,320
better here. So again, stepping back, we're starting with a very high-dimensional problem,

141
00:16:22,880 --> 00:16:29,040
but we know that in a lot of machine learning applications, even though the data appears

142
00:16:29,040 --> 00:16:35,200
high-dimensional, it's in fact often low-dimensional, we just need to discover that lower-dimensional,

143
00:16:35,200 --> 00:16:41,520
medium, and then run the data, run the algorithm in that new space, in that lower-dimensional

144
00:16:41,520 --> 00:16:50,480
space, right? So is manifold learning an example of dimensionality reduction?

145
00:16:50,480 --> 00:16:56,400
Yeah, exactly. It's an example of non-linear dimensionality reduction.

146
00:16:57,200 --> 00:17:00,480
So now, what you've done is you've increased the pipeline.

147
00:17:00,480 --> 00:17:07,440
So now you have your manifold learning procedure, which also has to make a decision, multiple

148
00:17:07,440 --> 00:17:15,440
internal decisions. What dimension is the manifold? Is it really a manifold? Is it really a smooth

149
00:17:15,440 --> 00:17:23,600
manifold, or is it just a bunch of subspaces of different dimension? It has a lot of internal

150
00:17:23,600 --> 00:17:29,600
decisions to make. So in the end, your entire pipeline becomes this manifold learning plus the

151
00:17:29,600 --> 00:17:34,720
eventual classification, right? And you increase the pipeline, you've added in a lot of

152
00:17:38,000 --> 00:17:44,000
tuning parameters. And so the question of adaptivity comes in. Is there just one algorithm

153
00:17:44,000 --> 00:17:50,240
that I can look at that automatically does as well as if I found the right manifold, the right

154
00:17:50,240 --> 00:17:58,160
dimension, everything? Okay. So that's where adaptivity comes in. What am I being adaptive to? I'm

155
00:17:58,160 --> 00:18:03,440
being adaptive to the structure of the data without knowing a priori the structure of the data.

156
00:18:03,440 --> 00:18:12,000
So one of my earlier work was trying to understand to what extent it is that existing procedures,

157
00:18:12,000 --> 00:18:16,960
such as nearest neighbor, already adapt to the structure of the data, to the intrinsic structure

158
00:18:16,960 --> 00:18:23,120
of the data without needing manifold learning, without needing the other ways of

159
00:18:23,120 --> 00:18:29,040
of re-representing data, reducing dimension in some ways, dictionary learning is another one.

160
00:18:29,040 --> 00:18:33,360
So without needing dictionary, dictionary learning applies in the cases of sparsity where we

161
00:18:33,360 --> 00:18:41,040
believe that the data is high dimensional, but it's very sparse. And so it so turns out that a

162
00:18:41,040 --> 00:18:48,720
lot of existing procedures are automatically adaptive to the structure of the data. And that came

163
00:18:48,720 --> 00:18:54,000
out mathematically by looking at the problem mathematically and then asking, oh, if we say now that

164
00:18:54,000 --> 00:18:58,320
my data that is very high dimensional lies very close to a low dimensional subspace,

165
00:19:01,040 --> 00:19:06,800
what convergence can we prove? And it so turns out that the convergence we could prove is

166
00:19:06,800 --> 00:19:11,440
then in terms of the lower dimension rather than in terms of the higher dimension.

167
00:19:11,440 --> 00:19:21,680
So just so I can replay that the you know these algorithms like nearest neighbor that we could

168
00:19:21,680 --> 00:19:26,320
throw you know tons of different types of problems at some high dimensional

169
00:19:29,600 --> 00:19:35,280
will have problems in the general case with high dimensional data, but if the data happens to be

170
00:19:35,280 --> 00:19:45,200
the data that has this low dimensional structure. Yeah, exactly. Then the performance of those

171
00:19:45,200 --> 00:19:53,760
out it sounds like the performance of that algorithm is going to be more akin to what we'd

172
00:19:53,760 --> 00:20:00,240
expected the dimension of the data was low dimensionality. That's exactly what I'm saying. Yeah.

173
00:20:00,240 --> 00:20:05,280
Yeah. Here all of a sudden it turns out that these algorithms and it's not just nearest neighbors.

174
00:20:05,280 --> 00:20:11,360
We are starting to understand that it's many other algorithms. It's it's particular classification

175
00:20:11,360 --> 00:20:19,760
trees. It recently people understand that even things such as neural net net adaptive to

176
00:20:19,760 --> 00:20:27,280
intrinsic dimension in in these ways things such as support vector machines being understood as

177
00:20:27,280 --> 00:20:32,000
adaptive in these ways, but at that time when I started working on these problems it was a bit

178
00:20:32,000 --> 00:20:37,440
unclear what was adaptive in these ways. And so happened that it was yeah. Go ahead. I think I

179
00:20:37,440 --> 00:20:45,120
got my I have my question from before. It's like okay. Is it two different things to say that

180
00:20:45,680 --> 00:20:54,320
the algorithms will perform better because the data has this inherent low dimensionality,

181
00:20:54,320 --> 00:21:08,480
this inherent structure versus the algorithms are adaptive. Is it is it is it is it some combination

182
00:21:08,480 --> 00:21:16,320
of the hyper parameters or or something. Oh, okay. Okay. I see. Yeah. That makes them adaptive

183
00:21:16,320 --> 00:21:24,960
under a certain set of constraints of the data or is okay. I see. I see that. No, no. This is a very

184
00:21:24,960 --> 00:21:33,280
good question. Let me put it this way. So I had two things in mind that I would explain the

185
00:21:33,280 --> 00:21:38,320
question. So first I was talking about the entire pipeline. If you were to do dimension reduction

186
00:21:38,320 --> 00:21:43,600
and all that you could you see right away that one question that comes up right away there is

187
00:21:43,600 --> 00:21:49,840
what dimension should I reduce to. Okay. Which is the parameter of the problem. Yep. Right.

188
00:21:50,720 --> 00:21:56,400
What sort of low dimensional structure do I have? Right. It's another hyperparameter of the problem.

189
00:21:57,600 --> 00:22:06,240
And I'm using the term adaptivity here loosely to say that I'm adapting to these various hyperparameters

190
00:22:06,240 --> 00:22:13,680
of the problem without knowing them a priori. Yep. Exactly in the sense that you just explained

191
00:22:13,680 --> 00:22:20,480
which is that if the data happens to be so structured then we do automatically better without knowing

192
00:22:20,480 --> 00:22:26,320
that a priori. Now there is a nuance. What does it mean to do automatically better? Nier's

193
00:22:26,320 --> 00:22:34,400
never methods. I know just in fact I know just one algorithm. It's a family of algorithms. And it's

194
00:22:34,400 --> 00:22:42,480
a family even in the simplest possible ways. So I can decide to run a nearest neighbor by saying

195
00:22:42,480 --> 00:22:48,240
I'm only going to use the nearest data points or I'll use the k nearest data points. The two

196
00:22:48,240 --> 00:22:54,320
nearest or three or four those become parameters of the problem. Right. So now there is another

197
00:22:54,320 --> 00:22:59,280
level of adaptivity which is how do I choose those parameters automatically of the problem.

198
00:23:00,080 --> 00:23:05,680
So it so turns out that if I'm say in regression depending on the loss that I'm using if I'm say

199
00:23:05,680 --> 00:23:15,840
in regression then regression is a hard enough problem that finding, finding tuning to the best

200
00:23:15,840 --> 00:23:22,400
possible parameter is not as hard. Does that sort of make sense? Find tuning to the best possible

201
00:23:22,400 --> 00:23:26,160
parameter is not as hard since the problem itself is hard and we cannot do so well anyway

202
00:23:27,600 --> 00:23:32,800
in a non-parametric regression. The complexity of the problem is self-masked. Exactly, exactly.

203
00:23:33,600 --> 00:23:40,400
And so here you can show that in fact you can just do cross validation on the number of neighbors

204
00:23:41,360 --> 00:23:47,600
and you'll get the best possible rate as if you knew what structure the data lie down.

205
00:23:47,600 --> 00:23:54,320
Right. Okay. In classification it's a bit more complicated. Classification is a much easier

206
00:23:54,320 --> 00:24:01,200
problem than regression and so it's a bit more complicated. Cross validation gets you half the way

207
00:24:02,480 --> 00:24:06,800
for hard classification problems but for super easy classification problems to get the best

208
00:24:06,800 --> 00:24:14,640
possible result you might need more refined, quote-unquote adaptive procedures to choose now the

209
00:24:14,640 --> 00:24:22,480
parameters of your algorithm and there are various ideas out there and those are the various things

210
00:24:22,480 --> 00:24:35,120
I'm interested in. So I think the initial point that you made that you're talking about the

211
00:24:35,120 --> 00:24:44,480
pipeline being adaptive as opposed to the algorithm and even talking about the algorithm

212
00:24:44,480 --> 00:24:48,320
in this way. Yeah. The reality of these things is that they are all the same.

213
00:24:48,320 --> 00:24:53,600
Right. Another of them are the pipeline. It's all the same because I can just make my algorithm

214
00:24:53,600 --> 00:24:59,520
more complicated and it becomes a whole internal pipeline. And in fact that's what I'll say this

215
00:24:59,520 --> 00:25:06,000
way that for me that's what neural networks are. It's just a whole pipeline of sub procedures.

216
00:25:06,000 --> 00:25:19,200
Yeah. Yeah. Interesting. Interesting. And you're saying also the kind of the process that we

217
00:25:19,200 --> 00:25:27,680
typically use in practice to optimize these pipeline slash algorithms is the adaptive part.

218
00:25:27,680 --> 00:25:33,280
And we know that that's why. Yeah. Yeah. It's the adaptive part. So they are really

219
00:25:33,280 --> 00:25:39,680
multiple. I use the term adaptivity here at least here fairly loosely. In the research paper

220
00:25:39,680 --> 00:25:44,640
I have to be very much more precise as to what I mean by adaptivity because yeah. But you're

221
00:25:44,640 --> 00:25:49,040
bringing exactly the type of questions why we need to make it more precise when we say adaptivity.

222
00:25:50,320 --> 00:25:55,920
There is the notion of adaptivity to various sub problems, families or problems.

223
00:25:55,920 --> 00:26:02,400
Right. And then there is also the side problem of our algorithms themselves.

224
00:26:03,840 --> 00:26:11,040
You can view every parameter of your algorithm as trying to address one particular sub problem.

225
00:26:12,480 --> 00:26:18,640
And that's how they are tied. Does that sort of make sense? The parameters of our procedures.

226
00:26:18,640 --> 00:26:24,480
An example would be helpful I think. Here all I'm saying is that if I fix the parameter of my

227
00:26:24,480 --> 00:26:30,240
procedures, if I just fix the parameters, there is always one problem on which it's going to do

228
00:26:30,240 --> 00:26:37,200
well. Right. Right. Okay. So in some sense it's easy to think about it this that way that the

229
00:26:37,200 --> 00:26:42,800
parameters or the various configurations of my algorithm are addressing as subfamily or problems.

230
00:26:44,560 --> 00:26:49,680
And as I'm trying to be adaptive to a whole hierarchy of families or problems, I need to also

231
00:26:49,680 --> 00:26:56,160
be tuning my algorithm. Right. Funding the right parameter for the right problem. And right. And

232
00:26:56,160 --> 00:27:00,880
so those are those interactions that we are carrying about that we try to understand. Got it. So

233
00:27:00,880 --> 00:27:09,280
you're saying a broken clock is right twice a day. Exactly. Yeah. And that's the perfect algorithm

234
00:27:09,280 --> 00:27:14,400
at that time. But what we're really carrying about is an algorithm that works well across

235
00:27:14,400 --> 00:27:20,640
problems. Right. Right. Right. And we do know that in very in general, it's not possible. In

236
00:27:20,640 --> 00:27:27,040
general, in the following sense, you probably've heard such things as people talk about the no-free

237
00:27:27,040 --> 00:27:32,320
large theorem. Yeah. And so we know that in general sense, it's not possible. But we do know that

238
00:27:32,320 --> 00:27:38,080
if we restrict the family of problems, it is possible to some extent. There are situations where

239
00:27:38,080 --> 00:27:42,080
adaptivity is not possible at all. And we might get into that when we talk about transfer

240
00:27:42,080 --> 00:27:48,960
learning, multitask learning and all that. So and where I hope at least what I'm trying to

241
00:27:50,400 --> 00:27:57,360
to bring out is the fact that adaptivity in some ways is what we are seeking in machine learning.

242
00:27:57,360 --> 00:28:02,080
We are seeking that black box procedure that looks at the data and just knows this is the type of

243
00:28:02,080 --> 00:28:07,040
that I'm dealing with. This is how I should self configure to do as well as possible on this data.

244
00:28:07,040 --> 00:28:15,520
Yeah. So yeah, let's move on to transfer learning. What are you looking at in that area?

245
00:28:17,040 --> 00:28:24,880
Yeah. So in that area, I feel like most theoretical questions are quite open.

246
00:28:25,440 --> 00:28:33,040
Right. So I don't know if I need to define transfer learning quickly here for, I mean, in fact,

247
00:28:33,040 --> 00:28:39,360
I should because there is one thing. People use the term transfer learning and domain adaptation,

248
00:28:39,360 --> 00:28:47,680
all the all these terms fairly loosely. And so when I'm using it, I need to be clear what I'm

249
00:28:47,680 --> 00:28:53,840
trying to refer to. Okay. I was going to say in general, I would expect our listeners to be

250
00:28:53,840 --> 00:28:58,560
familiar with the term. Of course, of course. Yeah. I'm sure that the listeners are familiar with

251
00:28:58,560 --> 00:29:04,240
the term, but the key point for me here is that when I use the term, I need to make clear this is

252
00:29:04,240 --> 00:29:10,640
what I'm trying to say, not some other thing that I might have in mind. Right. And so transfer

253
00:29:10,640 --> 00:29:17,120
learning, what is transfer learning for me? Generally, we have, I have data from a set of tasks.

254
00:29:17,760 --> 00:29:26,800
Sorry, I have data from either a set of tasks or from a single task. And that data gave me

255
00:29:26,800 --> 00:29:31,120
information about a particular problem. Maybe it's a classification problem of some sort. However,

256
00:29:33,120 --> 00:29:39,120
the problem on which I like to do well is a different problem. So mathematically, we tried to

257
00:29:39,120 --> 00:29:44,640
look at it as two different distributions that drew data from a particular distribution,

258
00:29:44,640 --> 00:29:50,320
but that distribution is not representative of the eventual distribution that's that I would like.

259
00:29:50,320 --> 00:29:57,200
Right. So that a different problem, but that could also be the same problem with different data.

260
00:29:57,200 --> 00:30:02,400
Exactly. That could be the same problem with different data. And that maybe the data has shifted

261
00:30:02,400 --> 00:30:08,240
somehow. And here is a simple case of the of the same problem. And in fact, that's where we

262
00:30:08,240 --> 00:30:12,560
starting. Let's look at first the case where it's the same problem essentially. Right.

263
00:30:12,560 --> 00:30:19,920
So I've given various talks on this on this so far. And I always start with the same

264
00:30:22,000 --> 00:30:27,920
motivating example. And the motivating example is Apple's Siri voice assistant. Right.

265
00:30:29,440 --> 00:30:36,560
And if Apple people are listening to me, I hope they understand why I'm saying this.

266
00:30:36,560 --> 00:30:41,360
Apple Siri still doesn't understand me that well at all because of an accent. And why?

267
00:30:41,360 --> 00:30:48,080
Because it was mostly trained on American English speakers at first. Right. And

268
00:30:52,080 --> 00:30:58,320
and it's doing much better now, but one has to think about what they had to do. They had to acquire

269
00:30:58,320 --> 00:31:04,560
a lot more representative data. So you could view the original task as that where they got

270
00:31:04,560 --> 00:31:11,520
data from a particular distribution, but it's a distribution of American accent. Right. And then if

271
00:31:11,520 --> 00:31:18,480
you try to deploy it, let's say in Scotland, it's a different distributions. It's the same problem.

272
00:31:19,040 --> 00:31:22,160
It's a different distribution of accent. You still have Americans there. You have some Scottish

273
00:31:22,160 --> 00:31:28,400
there. Here in the US, you have a few Scottish there in Scotland. You have a lot more. So it has

274
00:31:28,400 --> 00:31:37,280
shifted, but it's the same problem. So at this point, there are various proposals out there to

275
00:31:37,280 --> 00:31:43,040
try and solve this problem faster. But what are the key issues? The key issues will be, for the

276
00:31:43,040 --> 00:31:50,080
practitioner, how much more that I should acquire from let's me call the target desk, how much more

277
00:31:50,080 --> 00:31:56,400
represented that I should acquire. So that's a question of resources. Right. And then once,

278
00:31:56,400 --> 00:32:01,600
and here there is a cost, right, a financial cost, and we want to acquire as little as possible.

279
00:32:03,600 --> 00:32:10,160
So how little can I get by with? And that's really where the idea of transfer is coming in.

280
00:32:11,200 --> 00:32:16,160
What information can I transfer? And then there is the question of the how, how do I transfer

281
00:32:16,160 --> 00:32:21,440
such information? So what is the proper procedure? Once I have that additional data to transfer

282
00:32:21,440 --> 00:32:26,160
the information, or do I need that additional data at all? Right. So those are very, those are

283
00:32:26,160 --> 00:32:34,400
basic questions. Right. And those questions, we want to try and understand, we want to try and

284
00:32:34,400 --> 00:32:41,600
understand the principles behind them. The principles behind transfer. Yeah. Just to jump in, I

285
00:32:41,600 --> 00:32:48,560
love where this is going because I think, and maybe this is more practitioner than a theorist

286
00:32:48,560 --> 00:32:54,080
perspective. I think, you know, it's easy to kind of get, take a snapshot of, you know, where

287
00:32:54,080 --> 00:32:59,360
transfer learning is today, like you've got a neural network, you train it, you take, you know,

288
00:32:59,360 --> 00:33:05,120
the lower layers, and then you kind of retrain on your, your, your top layers for your specifically.

289
00:33:05,120 --> 00:33:09,120
For your new data, exactly. That's transfer learning. But I think you're suggesting that,

290
00:33:09,760 --> 00:33:14,720
actually, there's probably a lot of different things that one could think about doing to affect

291
00:33:14,720 --> 00:33:20,160
transfer and maybe some are better than others. Exactly. There are many more potential procedures

292
00:33:20,160 --> 00:33:25,520
there. Right. Here, for instance, you just describe one. And another could be, and actually,

293
00:33:25,520 --> 00:33:31,360
people do this, maybe I'm going to take the new data and keep, take my system data, put it all

294
00:33:31,360 --> 00:33:37,280
together, pretend it's from the same distribution and retrain everything. Right. That's another

295
00:33:37,280 --> 00:33:43,920
potential procedure. And so there are many procedures out there. And, but before, let's put it this

296
00:33:43,920 --> 00:33:48,800
way. Let's even say that your favorite procedure is the one that you just described. I take the,

297
00:33:50,320 --> 00:33:59,200
a layer of a neural network. This is still presuming that I knew how to pick the target data.

298
00:33:59,920 --> 00:34:07,600
I knew which, which amount of target data I should be using. Right. So there was a decision there,

299
00:34:07,600 --> 00:34:18,480
also. So, so another basic question is, is there even a way for me to know a priori or to discover

300
00:34:18,480 --> 00:34:28,560
over time what amount of target data is required is needed? In full of a sudden, I acquire 1000 data,

301
00:34:28,560 --> 00:34:35,680
1000 data points. And I'm not doing well. What does that tell me? Does that tell me that the,

302
00:34:35,680 --> 00:34:43,760
uh, I didn't acquire enough? Or not the right or I'm not doing the right, uh, transfer.

303
00:34:45,440 --> 00:34:50,480
Or not the right data or not the right data. Yeah. Right. Yeah. It's kind of getting to like active

304
00:34:50,480 --> 00:34:55,440
learning and exactly. So they get into active learning. It gets into into a different form of

305
00:34:55,440 --> 00:35:00,720
active learning, active learning on the transfer. What are the limitations of that? Uh, it gets into,

306
00:35:00,720 --> 00:35:11,120
uh, um, uh, it gets into understanding what's the right notion of distance or what's the,

307
00:35:11,120 --> 00:35:16,400
by distance here, I really just mean information. What's the notion, the right notion of information

308
00:35:17,520 --> 00:35:24,400
to sort of quantify the information my original data already had about the target application.

309
00:35:24,400 --> 00:35:30,080
To be able to say that it doesn't have enough information, so I need that much more.

310
00:35:31,280 --> 00:35:38,480
Mm-hmm. Representative data. I think what you're saying is if we had a way to know

311
00:35:39,760 --> 00:35:47,280
what it was about our initial training data that made it unique and special and most informative

312
00:35:47,280 --> 00:35:53,760
to the model that we've trained. Yeah. And what it was about the data that we need in order to

313
00:35:53,760 --> 00:35:58,480
have our model perform better on the transfer task, you know, there may be a shortcut

314
00:35:58,480 --> 00:36:03,360
than what we're exactly. Then we might start identifying the right data to pick. Yeah. Yeah.

315
00:36:03,360 --> 00:36:09,200
And the right amount to pick. Yeah. Right. And so some of my early theoretical work on this problem,

316
00:36:09,200 --> 00:36:14,720
I've only started on this problem a few years back. And some of the, the first questions there

317
00:36:14,720 --> 00:36:20,800
are that question. What is the right notion of information? The source has about the target.

318
00:36:20,800 --> 00:36:31,280
Mm-hmm. Right. And how do you begin to characterize that? So, so you cheat, right? That's the first

319
00:36:31,280 --> 00:36:36,880
thing you do. You go and you read a lot of papers because a lot of small people have thought about

320
00:36:36,880 --> 00:36:42,880
the problem. And you try to see, okay, what are the notions they've come up with and what are the

321
00:36:42,880 --> 00:36:52,080
limitations of such notions? Right. And then what's there still to be answered? And, and here,

322
00:36:52,080 --> 00:36:58,640
one of the things that I claim is that they are told, okay, this is the one thing that I'm still

323
00:36:58,640 --> 00:37:04,480
doing that everybody, every theoretician has done on this problem so far, which is model the problem

324
00:37:04,480 --> 00:37:08,880
as just probability distributions. I have a, I have data that I drew from a probability

325
00:37:08,880 --> 00:37:14,400
distribution. That's my source. And then there is another probability distribution, which is my target.

326
00:37:15,920 --> 00:37:22,480
And then there are tons of notion of information that relate to probability distributions. And also

327
00:37:22,480 --> 00:37:26,560
notions of distance between probability distributions. And then we can start there and start asking,

328
00:37:26,560 --> 00:37:32,160
do these notions of information or notion of distance, do they sort of capture the hardness of

329
00:37:32,160 --> 00:37:37,680
transfer? And I can say how hard transfer is, if I had a million data points from my source,

330
00:37:37,680 --> 00:37:45,520
and just only 1,000 from my target, how hard would transfer be? And those are the places people

331
00:37:45,520 --> 00:37:51,680
have started. And so we started looking at those notions and then saying, okay, a lot of the

332
00:37:51,680 --> 00:37:56,320
various notions that people have looked at are notions that were developing other areas for

333
00:37:56,320 --> 00:38:01,600
in probability theory or in information theory and such. But we're developed for other problems.

334
00:38:01,600 --> 00:38:10,240
And not necessarily for the exact problems we are looking at, classification on the transfer

335
00:38:10,240 --> 00:38:16,160
setting. And in which case we have to start looking at classification and then start asking,

336
00:38:16,160 --> 00:38:23,200
what makes it easy to transfer a classifier from one distribution to another distribution?

337
00:38:23,680 --> 00:38:29,440
And what's entering that? So then we can ask, as we bring in the question, as we refining the

338
00:38:29,440 --> 00:38:36,320
question, we can start seeing what's essential to the problem, right? So for instance,

339
00:38:36,320 --> 00:38:41,120
if I'm using neural networks, I can ask the question and I'll step away from neural networks,

340
00:38:41,120 --> 00:38:46,720
even I'll just say, I'm using a family of classifiers, right? I'm using a family of classifiers,

341
00:38:46,720 --> 00:38:50,960
then I can step back and ask, okay, when is transfer easy? I can say transfer is easy,

342
00:38:51,440 --> 00:38:57,200
if on my original data, my original data, my source has information about the target,

343
00:38:57,200 --> 00:39:06,640
if on a for the particular family of classifiers I'm using, if whenever a classifier in my family

344
00:39:06,640 --> 00:39:14,560
does very well in on the source, I expected to not be too far from the best classifier on the target.

345
00:39:15,360 --> 00:39:19,200
I expected not to do too bad on the target. That could be one simple notion.

346
00:39:19,200 --> 00:39:27,920
And that in itself start driving down a notion of distance between them, given my preferred

347
00:39:28,560 --> 00:39:33,840
set of classifiers. And now we're talking about distance between the classifiers or distance

348
00:39:33,840 --> 00:39:39,280
between the data sets. Distance between the data sets or in fact between the problems,

349
00:39:40,160 --> 00:39:45,440
because I'm viewing the data sets as being drawn from a distribution, which is now the problem

350
00:39:45,440 --> 00:39:53,280
that I have. And part of the problem is transferring using just my given family of algorithms.

351
00:39:53,280 --> 00:39:56,960
Maybe I'm using neural networks, maybe I'm using random forests, and that's my given family

352
00:39:56,960 --> 00:40:01,120
of algorithms. And that's the family of algorithms within which I would like to stick as the

353
00:40:01,120 --> 00:40:06,480
practitioner. From my family of algorithms, the problem of transfer might be different,

354
00:40:07,120 --> 00:40:12,000
or the hardness of transfer might be different if I were to switch to another set of algorithms.

355
00:40:12,000 --> 00:40:17,920
And I hope that that one should be intuitive to people. If I'm using neural net, transfer

356
00:40:17,920 --> 00:40:21,920
on the neural net might not be the same as transfer on the classification trace.

357
00:40:23,920 --> 00:40:33,280
And even though the data is the same. So it's questions of that type that we try to answer.

358
00:40:33,280 --> 00:40:39,120
And then from there, try to answer more refined questions. Once we start getting a sense of good

359
00:40:39,120 --> 00:40:46,240
notions, like meaning notions that are predictive of hard, hard transfers, then we can start asking

360
00:40:46,240 --> 00:40:51,680
other questions. Okay, now under these notions, we know how what's the best we could possibly do.

361
00:40:51,680 --> 00:40:55,040
And then we can start asking are there algorithms that can attain

362
00:40:57,360 --> 00:41:02,480
these rates that can attain these performance? And then are there adaptive algorithms?

363
00:41:02,480 --> 00:41:06,080
Are there algorithms that can attain these performance without any knowledge

364
00:41:06,080 --> 00:41:14,160
about the underlying problem parameters? How much information about the problem parameters do I

365
00:41:14,160 --> 00:41:19,520
need to give an algorithm before it can do as best as possible for the problem?

366
00:41:21,120 --> 00:41:23,520
And that's again where I fall back into adaptivity.

367
00:41:23,520 --> 00:41:30,800
Yeah, maybe just summarizes part. I think, you know, a lot of what we're seeing in this conversation

368
00:41:30,800 --> 00:41:38,240
is you hear often, you know, machine learning works. We just don't know how or why it works.

369
00:41:38,240 --> 00:41:42,000
And I think you're illustrating in the context of transfer learning,

370
00:41:42,720 --> 00:41:49,200
how a theoretician goes about trying to advance our understanding of how a particular thing.

371
00:41:49,200 --> 00:41:55,200
Exactly, exactly. And so, and I can tell you some of the simplest questions we,

372
00:41:55,200 --> 00:42:02,000
we are starting to address, take the simplest or most naive heuristic in transfer.

373
00:42:02,000 --> 00:42:06,080
I take both data sets, throw them together, pretend it's the same,

374
00:42:06,080 --> 00:42:10,720
and rerun my training out and retrain algorithms on this new data.

375
00:42:10,720 --> 00:42:11,840
How well does this do?

376
00:42:13,360 --> 00:42:16,720
And so, we can start answering those questions. Okay, it's nearly

377
00:42:18,960 --> 00:42:23,360
the best you can do in particular situations. And we can say what exact situations it often

378
00:42:23,360 --> 00:42:29,760
has to do with how noisy your data is. And if it's not so noisy, and whether your,

379
00:42:29,760 --> 00:42:36,480
whether the pattern, your best classifier is shared between source and target is the problem,

380
00:42:36,480 --> 00:42:41,360
the same how close are the problems. And so, on the various situations, we can say,

381
00:42:41,360 --> 00:42:44,800
okay, this particular set of algorithms will do as well as possible.

382
00:42:44,800 --> 00:42:52,240
In other cases, the algorithms that do well will be or adaptive will be closer to meta-learning

383
00:42:52,240 --> 00:42:58,000
type procedures. And so, so we can start addressing those questions. But again, the whole thing

384
00:42:58,000 --> 00:43:03,840
for me is step back completely and start asking the most basic questions about the problem.

385
00:43:03,840 --> 00:43:09,360
Yeah, yeah, yeah. One more area I'd like to explore. You've also been doing some work in

386
00:43:09,360 --> 00:43:12,320
unsupervised learning. Can you talk a little bit about that?

387
00:43:12,320 --> 00:43:18,560
Yeah, so, so it's mostly the, I mean, I've talked a bit about that so far with the

388
00:43:18,560 --> 00:43:23,680
clustering, although I didn't quite as specifically talk about my work in clustering.

389
00:43:24,880 --> 00:43:30,080
But, and then lately, we've been looking at some applications in IoT,

390
00:43:31,200 --> 00:43:38,960
in allied detection and such in IoT. So, so in clustering, mostly the type of questions I've

391
00:43:38,960 --> 00:43:45,200
looked at and I'm still looking at, there are two lines of questions. The first is what I

392
00:43:45,200 --> 00:43:51,280
alluded to earlier, which is, let's imagine that I define clustering as just notions of

393
00:43:52,800 --> 00:43:59,040
in terms of density levels, in terms of finding regions of high density in the data.

394
00:43:59,040 --> 00:44:07,280
To what extent can I do this? And can I do this also adaptively? Can I? And there there was a key

395
00:44:07,280 --> 00:44:13,680
question that we had, which was, there are tons of in density based clustering itself,

396
00:44:13,680 --> 00:44:16,720
there are tons of heuristics, I will call them heuristics and it's not a bad term,

397
00:44:17,920 --> 00:44:23,760
there are tons of heuristics that do extremely well in practice. And yet, when you try to prove any

398
00:44:23,760 --> 00:44:29,200
guarantee about these heuristics, it's really hard. You cannot show that day. And however,

399
00:44:29,200 --> 00:44:34,160
they do better than anything we can prove beautiful guarantees about. And so one of the questions

400
00:44:34,160 --> 00:44:41,520
there was, can we come up with a heuristic that does, that compete with existing heuristics?

401
00:44:41,520 --> 00:44:49,120
And yet can guarantee the recovery of clusters if we define clusters as regions of high density of

402
00:44:49,120 --> 00:45:01,040
the data. So it's that upper question there. Then, then lately, I was, I've been looking at

403
00:45:01,040 --> 00:45:12,560
more, I mean, it's coming up in more in the IoT domain, internal of things. And here,

404
00:45:13,280 --> 00:45:18,160
the type of questions that are coming up, I have to do with constraints on the clustering and

405
00:45:18,160 --> 00:45:26,480
alloy detection and all that. In IoT, we want to just monitor traffic, network traffic. And be

406
00:45:26,480 --> 00:45:35,200
able to quickly say, when there is an anomaly in the network traffic. And the anomaly could just be

407
00:45:35,200 --> 00:45:42,880
a new device was introduced into the network, into the house. Or we are observing a new modality

408
00:45:42,880 --> 00:45:52,160
of the device. And these devices could be anything. It could be a smart coffee maker,

409
00:45:52,160 --> 00:45:58,320
smart fridge, it could be a network, it could be a sensor in a city, et cetera.

410
00:45:58,320 --> 00:46:03,520
And applications might be, you know, anything from cybersecurity to performance,

411
00:46:03,520 --> 00:46:11,040
management. Exactly. Yeah, et cetera. And, and even detecting, yeah,

412
00:46:11,040 --> 00:46:15,120
cybersecurity goes into it, right? Detecting simply that a new device was introduced,

413
00:46:15,120 --> 00:46:21,120
and this device looks a lot like a camera. So somebody threw a camera onto my network.

414
00:46:21,120 --> 00:46:27,360
Right. So something like that. And, and from your, I guess you're getting to this, but your,

415
00:46:29,120 --> 00:46:37,360
your data sources like net flow traffic traffic. And we are assuming that I can only observe the

416
00:46:37,360 --> 00:46:45,120
pattern of a packet. I cannot read a packet in network packet. All I know is that so many

417
00:46:45,120 --> 00:46:50,640
packets are being sent per second to these particular addresses and all that. And so it's very unsupervised.

418
00:46:50,640 --> 00:46:55,520
I don't know. It's an unsupervised problem. I don't have any, any labels. And however,

419
00:46:55,520 --> 00:47:00,080
this is where constraints come in. Huge constraints come into the into this right away.

420
00:47:00,080 --> 00:47:06,640
And the constraints of the following form, we have to be able to run the detector, train and

421
00:47:06,640 --> 00:47:15,040
run the detector. Let's say on a router at home, on a very small device. We don't have a huge

422
00:47:15,040 --> 00:47:22,480
server to, to do this. So all of a sudden, a lot of our library detectors out there,

423
00:47:22,480 --> 00:47:30,720
let's, for instance, let's think about one-class SVM. It just don't run well in these settings.

424
00:47:31,360 --> 00:47:35,760
Why I'm taking one-class SVM? One-class SVM like support vector machines in general

425
00:47:37,120 --> 00:47:44,960
require a lot of computation. So require a lot of computation because they deal with

426
00:47:44,960 --> 00:47:55,360
this, this, this very large matrices that they have to, to, to play with. And so right away,

427
00:47:55,360 --> 00:48:01,680
the question that comes up, questions of how do we reduce the data in some ways, right?

428
00:48:01,680 --> 00:48:05,680
Or how do we reduce this? If you're familiar with support vector machines, they work on

429
00:48:05,680 --> 00:48:10,960
a so-called gram matrix, which is essentially a representation of interactions between

430
00:48:10,960 --> 00:48:15,840
data points, okay? But because the moment I talk about interactions between data points,

431
00:48:16,720 --> 00:48:22,880
if I have n data points, I'm talking about n squared operations already, right? And so

432
00:48:24,480 --> 00:48:30,320
we want to reduce this and reduce, reduce this quickly and to be able to run on a nano computer

433
00:48:30,320 --> 00:48:36,160
on Raspberry Pi or something like that, right? And so it's not only space savings, but it's also

434
00:48:36,160 --> 00:48:41,520
time savings. And so for me as a theoretician, the first question that comes up is,

435
00:48:44,080 --> 00:48:49,360
first of all, can I maintain performance of a clustering

436
00:48:51,040 --> 00:48:57,520
after reducing data? And what type of data reduction is useful here? There are various things

437
00:48:57,520 --> 00:49:04,640
that people have tried. Nitron is a method called Nitron sketching method, all sort of

438
00:49:04,640 --> 00:49:12,320
a sub-sampling method that reduce these data representations. And then the questions for me was,

439
00:49:12,320 --> 00:49:20,000
okay, can I modify this method and guarantee that let's say in the context of clustering or in

440
00:49:20,000 --> 00:49:26,640
the context of allied detection, the performance is essentially maintained and yet I've achieved

441
00:49:26,640 --> 00:49:33,600
my constraint on size. So it's questions of that type. And here it's very practical because I

442
00:49:33,600 --> 00:49:39,360
cannot just answer the question. I have to, in fact, implement it and get it working and eventually

443
00:49:39,360 --> 00:49:50,560
deploy it. And presumably is this work in progress or have you come up with another question?

444
00:49:50,560 --> 00:49:56,640
Oh, so we have a couple, yeah, we have a couple papers on it. And you probably heard of Johnson

445
00:49:56,640 --> 00:50:04,320
leader in Strauss dimension reduction and random projection, things like that. So part of that

446
00:50:04,320 --> 00:50:08,560
the work, the initial paper on this work was all theory was trying to understand, okay,

447
00:50:08,560 --> 00:50:16,480
all these problems out there, all these methods out there sketching, nice from, can we recast

448
00:50:16,480 --> 00:50:25,440
this method in different terms that we can understand better, right? So sketching doesn't look at

449
00:50:25,440 --> 00:50:32,720
right away like a random projection. And we're asking sketching, which is really just

450
00:50:32,720 --> 00:50:44,800
sub sampling of a matrix. Can we view it as a random projection in a very high dimensional

451
00:50:44,800 --> 00:50:48,400
space, in an infinite dimensional space? Why infinite dimensional space? Because that's exactly

452
00:50:48,400 --> 00:50:52,960
where OCSVM's and all that work. They work in an infinite dimensional space. And I'm doing this

453
00:50:52,960 --> 00:50:58,320
sketching on a finite matrix. Can I view that somehow as a projection in an infinite dimensional

454
00:50:58,320 --> 00:51:03,280
space? Why do I want to view it that way? Because I do understand projections. We understand what

455
00:51:03,280 --> 00:51:08,240
projections do and what they maintain and all that. So that was the first step in the work.

456
00:51:08,240 --> 00:51:12,960
And then the next step has been, okay, now that we know how it works, we know for what type of

457
00:51:12,960 --> 00:51:20,560
clusters we preserve the clustering. And now do we have that type of clusters in this IoT

458
00:51:20,560 --> 00:51:25,520
applications? It so turns out that we did a lot of data analysis and, okay, we do have that

459
00:51:25,520 --> 00:51:30,960
type of clusters often. And now we are implementing it. And we have a paper on archive trying to show

460
00:51:30,960 --> 00:51:37,200
that, okay, look at this running on a nano computer on the Raspberry Pi. It runs almost, it runs

461
00:51:38,000 --> 00:51:46,640
20, 30 times faster than the original OCSVM, but we have the same performance. And so, but it's

462
00:51:46,640 --> 00:51:53,520
understanding this type of things. Very cool. Very cool. There's maybe a bit of a

463
00:51:53,520 --> 00:52:00,960
digression before we close out, but I was thinking earlier when we were talking about clustering

464
00:52:00,960 --> 00:52:09,600
and density and like zooming in and out. And kind of seeing the data at different granularities,

465
00:52:09,600 --> 00:52:14,480
it reminded me a little bit of like fractals and like fractal theory. And I'm just have,

466
00:52:14,480 --> 00:52:21,440
does that come up in theory? Yeah, it does come up a lot. So, so for instance, you might,

467
00:52:22,240 --> 00:52:26,240
so when I was talking about dimension earlier and I was even when I was saying, okay,

468
00:52:27,280 --> 00:52:32,240
let's say data is very high dimensional, but it's very structured, right.

469
00:52:33,440 --> 00:52:38,000
There are tons of structures, tons of intrinsic structures out there. Manifold is just one,

470
00:52:38,000 --> 00:52:43,840
right? And Manifold is just one such structure. And so, a first thing in that line of research was to

471
00:52:43,840 --> 00:52:54,000
try and understand what notion of intrinsic dimension can we use to capture all these

472
00:52:54,800 --> 00:53:01,840
low dimensional structured at once rather than working on understanding each one of them separately.

473
00:53:01,840 --> 00:53:05,120
Is there a notion that captures them at once? And there are fractal notions of dimension that

474
00:53:05,120 --> 00:53:11,360
do capture these notions of intrinsic dimension at once. And then once you understand that you can

475
00:53:11,360 --> 00:53:16,800
then ask the question, okay, let me now say that my data is low dimensional in the sense that

476
00:53:16,800 --> 00:53:23,840
it's fractal dimension is small. How do nearest neighbors work? Okay. Right. So, so that's how we

477
00:53:23,840 --> 00:53:29,920
approach those those problems. We first have to sort of step back again and say, okay, what is the

478
00:53:29,920 --> 00:53:35,440
essential quantity we need to work with here? And and and fractals somehow have to do with

479
00:53:35,440 --> 00:53:42,400
is representability of data or the information in something in in in the data space. And so,

480
00:53:43,760 --> 00:53:50,240
so so yeah, so a lot of notions we work with have this sort of recursive structure to them.

481
00:53:50,240 --> 00:53:58,080
Awesome. Awesome. And one last thing, you're chairing a conference that conference is happening now,

482
00:53:58,080 --> 00:54:05,440
so I assume. Yeah. Yeah. Tell us about the conference. So, this is called conference on learning

483
00:54:05,440 --> 00:54:11,360
theory. And so, called has been sort of the flagship conference for machine learning theory.

484
00:54:12,800 --> 00:54:18,480
I don't know how many years now, 20, 30 years, something like that. And so yeah, so it's

485
00:54:18,480 --> 00:54:26,560
happening this year in what we all know, sort of unfortunate circumstances, the times we live in.

486
00:54:26,560 --> 00:54:32,080
And yeah, and this year, one of the things I'll say is that this year we have a really, really,

487
00:54:32,080 --> 00:54:40,400
really nice program. So, so let me put it this way. I feel that somehow at the same time we are

488
00:54:40,400 --> 00:54:50,640
in strange times and hard times for a lot of people, but people also tend to focus and solve

489
00:54:50,640 --> 00:54:57,520
hard problems around this sort of tough times. And I feel that reflecting in somehow in the program

490
00:54:57,520 --> 00:55:04,720
we have this year, a lot of beautiful problems were solved. And we were just odd at the type of results

491
00:55:04,720 --> 00:55:11,840
that to be presented next week at the conference. Yeah. Awesome. Awesome. Yeah. Well, we will include a

492
00:55:11,840 --> 00:55:19,040
link to the conference in the show notes as well as the archive link to the network traffic

493
00:55:19,040 --> 00:55:24,720
representation paper. Samry, so great to catch up with you. Thanks so much for taking the time.

494
00:55:25,280 --> 00:55:30,560
Yeah, thank you so much Sam for having me and thanks for all the very interesting questions

495
00:55:30,560 --> 00:55:58,720
and the very pointed questions. Awesome. Thank you.

