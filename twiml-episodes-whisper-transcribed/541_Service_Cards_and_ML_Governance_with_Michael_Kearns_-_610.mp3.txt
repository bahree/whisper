Hey, what's up everyone? I'm super excited to kick off another episode of the Twoma AI podcast
I am of course your host Sam Charrington and today I am coming to you live from the future frequency
podcast studio at the AWS re-invent conference in Las Vegas, Nevada. I am joined by Michael Kerns
Michael is a professor in the Department of Computer and Information Science at UPenn as well as an
Amazon scholar with a focus on fairness and privacy in machine learning and related topics at AWS.
A quick note if I sound a little funny do not try to adjust your audio settings it is me
after a few days here at re-invent and in this zero humidity desert my usual podcast voice is
given away to a little bit of a very white slow jams voice but before we get going be sure to
take a moment to hit that subscribe button wherever you're listening to today's show and if you
want to check out the studio you can bounce over to youtube to check us out. Michael welcome to the
podcast thank you great to be here it's great to have you on the show we had the the pleasure of
sitting next to one another at a dinner a couple of months ago I guess in New York and had a great
conversation I'm gonna struggle to try and recreate a lot of that conversation because we touched
we touched on you know everything from philosophy to your work of course but you know thanks once
again for for joining me here yeah I'm excited to be here I'd love to get started by having you
share a little bit about your background and how you came to work in machine learning yeah so first
of all I'm an old-timer I've been around a long time and so I went to graduate school in the late
1980s just to set the context for your listeners who might be considerably younger these days um the
field of machine learning barely existed in the late 1980s uh-huh the conferences that we now know
as noreps and icml were really in their first couple of years at that point and machine learning
at that time was considered sort of a a boutique obscure subfield of the then discredited larger
field of AI which was going through its famous AI winter so suffice to say I've seen a lot of
change in my career I initially came to machine learning from really an algorithmic and theoretical
angle so in those early days when you know machine learning barely existed in particular
there weren't sort of formal foundations or models for thinking about machine learning the way
we're used to now and at that time there was also of course the ongoing kind of debate between
people that came to AI from a more logical formalism and those that were starting to adopt more
probabilistic formalism which of course is by far and away dominated today and so at that time
there were very few ways of thinking rigorously about machine learning and making comparisons between
different algorithms or even saying what would constitute good performance for an algorithm
and as somebody who came to computer science in general really through the theoretical computer
science approach you know I liked theoretical computer science a lot and still do but I was always
interested in its application to sort of unusual areas so a lot of theoretical computer science is
about very practical algorithmic problems like you know traveling salesmen for instance
and I knew that wasn't kind of my bag and so I you know went to graduate school particularly to
work with somebody who was starting to think my advisor a less valiant who was starting to think
about kind of mathematical ways and algorithmic ways of thinking about machine learning and so I
did that and then you know spent the first decade of my career at the late great think tank bell
laboratories in Marie-Hell New Jersey where by the way a lot of you know major the luminaries of
the field like Yannick-Lacoon were colleagues and Rob Shapiri and Vladimir Vapnik it was just a
gold and a year for the early days of machine learning had great colleagues and all of our time to
do research and eventually we all migrated you know to universities and then in some cases back
to industry as well but that's already how I came to the topic I don't recall if I mentioned to you
that I did a summer at Bell Labs I was based in Whipney okay New Jersey during grad school
okay I focused on statistical modeling of computer networks okay yeah yeah it was just it was
a great place it was I mean it still exists but it's not it's not the same yeah yeah it's a
wonderful and I didn't think of it this way at the time but it was a great alternative to being
a junior faculty member somewhere because you know you had all of your time for research you didn't
need to think about teaching or sitting on committees or writing grants or the like and so in
some ways if you were really dialed in on your research you could be much more productive from a
research perspective than you could have been in an analogous position as a junior faculty member
where you would have had all these other you know concerns as well as the pressure of getting tenure
and as you mentioned the brain trust there was just it was great yeah yeah all of us whenever we
get together and you know we we try to do that at the big conferences when we're all there we're
always you know immediately devolved into reminiscing and telling stories from that decade it was great
and so tell us a little bit about the focus of your research nowadays yeah so you know I've been
working in machine learning for a long time and as time has gone on I've kind of evolved a bit
too and so even though I still kind of come at many problems primarily from a mathematical algorithmic
perspective first I do get involved in quite a bit of experimental work and you know I think like
everyone else in machine learning I've watched roughly the last decade with you know some amount
of surprise in alarm I mean this field that was nothing when I started in it is now this successful
standalone industry and you know just to give some subjective history of let's say the time
since early you know early 2010s when deep learning first started to become a powerful technology
you know I think I along with men you know my colleagues kind of shared in the excitement
of that initial period and you know big problems being solved that had you know before before
that been very intractable and then you know around 2015 or 2016 all of us scientists at the same
time that society learned it realized that there are harmful side effects to trained models
in ML if one is not careful and so there was a bit of a buzzkill I think around you know 2015
2016 and you know like many of my colleagues I think my first thought about this is like okay these
are serious problems we do not want to be training models that are you know making consequential
decisions about ordinary people that exhibit you know significant demographic bias for instance
and being a scientist first and foremost my first thought on this topic was to think about
technical solutions to those problems you know in other words if we don't like something about
the behavior of our trained models I mean after all we trained them so we could think about changing
the way we train them in the first place rather than waiting for harms to occur and then looking
for non-technical solutions yeah I think I've come to appreciate especially in the time I've
spent an Amazon that you need non-technical solutions as well and that includes you know diversity
of inputs to the design process of products and services diversity of technical teams that are
training models you need people with legal regulatory public policy backgrounds as well but
but the way I got into you know what what is now called responsible AI was primarily first thinking
about how could we change the way we do ML in a way that would mitigate things like demographic
bias privacy leaks and the like and we'll return and jump deeper into your research on those areas
but before we do share a little bit about your role at Amazon yeah so I'm first of all part of this
very clever mechanism that Amazon has called the Amazon Scholar Program which makes it very very
flexible and easy for people like me to spend significant time at Amazon while firmly maintaining
our roles in academia and also to be able to ramp you know sort of our commitment level to Amazon
up and down so for example you know utility academics yes exactly and so right right and so I've
spent the last three summers full time at Amazon and I'm quite involved during the year and so I'm
basically part of a constellation of teams within AWS that think seriously about many different
aspects of responsible AI one of those teams is a centralized team that I participated in the
formation of when I joined back in the summer of 2020 which is designed to be a centralized team
that works with the product and service teams on careful quantitative assessments of different
responsible AI principles in our trained models and services so this would include things like
demographic fairness it would include things like thinking about you know whether what are the
risks of that a trained model might exfiltrate inadvertently properties of the training data for
example we think about robustness explainability you know all the things you hear about when the
topic of responsible AI is mentioned you know I should note that even prior to my arrival when I
got there was very clear to me that many of the product and service teams were already doing
this kind of work even though it wasn't called responsible AI at the time very seriously on their
own so for example when I started talking to people who work on Amazon transcribe which is our
speech transcription service you know I the first thing I learned that surprised me is that just
in North American English alone there are dozens and dozens of regional accents and dialects
and each one of these has different properties and presents different challenges for speech recognition
in transcription and so long before I showed up that team diligently goes out and collects and
annotates spoken you know spoken data that's then transcripted by transcribed by humans in order
to do both bias assessments and to improve training of our models what was different about this
centralized team was that there were a couple of reasons for it one was it felt that this topic
was becoming sufficiently important and serious that it merited having a centralized team that first
of all had a certain arms length objectivity and distance from the product teams themselves
but of course we need to work with the product and service teams on these audits and then the other
part of this centralized team is meant to sort of codify best practices collect data sets that
might be able to be used for assessments on multiple different products and services and eventually
I think develop platforms and tooling around responsible AI that can be turned back to the product
teams to make their work more efficient and higher quality and so that's that's sort of the science
end of the work I do at Amazon but then I often get pulled into discussions about you know how
we talk about responsible AI in public via PR and you know analyst relations had get occasionally
involved in public policy discussions and legal and regulatory discussions and that's an entirely
you know that's a very interesting evolving landscape itself that I think we'll see a lot of
important developments in the next five years or so and this is kind of an approximate description
of my portfolio within AWS with that in mind maybe one of the things we can jump into is the
announcement year this week of the service cards it was part of a broader umbrella of ML governance
capabilities that was announced as part of the the SageMaker product family of course you know
I'm presuming by name alone that you know credit you know it goes to folks behind the original
model cards paper absolutely like Debraji and like Mitchell to make a brew and others right you talk
a little bit about that and yeah so first of all there were two related but distinct announcements
today one was on sort of model cards within SageMaker and that's of course really customer facing
that's for our customers who have their own data have the expertise to do their own machine learning
but want to do machine learning in a responsible way so SageMaker model cards is meant to help our
customers in that regard not a little bit more distant from that effort okay on the other hand
the service cards that we announced for three of our major kind of vertical AI services around
face recognition speech transcription and identity validation from like government identities
government ID cards these are closely related to model cards and obviously that particular paper
was an inspiration not just to us but I think to many people in the responsible AI community
you'll notice that we don't call those model cards that we call them service cards and there's
a very deliberate reason for that one of them is our typical AI service will have many many models
behind it to give a very concrete example if you think about the problem of face recognition
you know a good face recognition engine will deploy multiple models there will be one model
that just identifies the bounding boxes around the faces in an image there will be perhaps a
separate model which makes adjustments to those faces for instance if your head was tilted in
your selfie it'll write it so that it's oriented properly and then of course there's finally the
model which is actually doing the matching and discard deciding whether this face matches you
know the one that's on file for you for example and so since our customers and the end users of our
customers experience like the holistic system and to end not the individual models we call them
service cards because they're really like model cards but for the entire service right the thing
that you would kind of experience at the API level and yeah for that distinction I had not heard
the service cards announcement okay yeah so when you mentioned service cards earlier I thought
we were using two different names yeah yeah yeah so they're they're related but distinct yeah
and so to say a little bit more about them you know so first of all a lot of work went into these
cards first of all there's the underlying technical quantitative assessments that sort of form the
quantitative backbone of the information on the service cards but of course you know these cards
got reviewed by many non technical people as well and many stakeholders you know including legal
need to weigh in on as well so it's been a very interesting process because it's been of you know
internally a very diverse multidisciplinary process to kind of converge on on what we wanted
to put on these cards just hearing that it sounds like the cards kind of worked as designed it was
not taking information that you already had produced and you know formatting it according to
some card the card the the process of creating the service card inspired some set of work beyond
what was already in place for these services yeah although I would say that the technical work you
know the the the actual assessments of things like bias or robustness or privacy that works been
ongoing since long before I even showed up at Amazon okay and and so you know what I think the
most important thing about this announcement is that we're committing to doing this on an ongoing
basis right we're like setting a standard for ourselves and when you look at the cards which
have actually been released right there's a couple of purposes they serve first and foremost it's
to communicate information to our customers about some details about how the models were trained
how we perceive the intended use what we perceive the intended use cases to be a little bit about
you know our quantitative assessments of demographic bias and we can talk a little bit more about what
we say there and what we don't say and why and I think they're sort of good reasons for why we say
the amount that we say now but all that being said I think this is a you know this is a baby step
for us but it's a big baby step for us and I think that the most important part is the commitment to
do this for sort of all of our AI services going forward and not just to do it once and say like
okay now we've done it first transcribe we're no check that off because use cases change the
data being fed to these services change and so these cards we need to revisit them at some cadence
and redo the quantitative assessments redo the language on the guidance that we provide to
customers on the cards and so that's I think what I'm most excited about which is you know there's
the thing that the literal cards that were released today but then there's the commitments to a
process and I think that that isn't many ways the thing that will have the greatest internal
traction within AWS because we you know we kind of as the saying goes and Amazon we've walked through
a one way door and and because we're walking through a one day way door I think we are you know in
my view as a scientist we're you know naturally a bit conservative about how much detail we reveal
at the beginning but I personally I strongly expect it over time it's not just that we'll do more
of these cards and revisit the ones we've already done I think the information on them will evolve
and it'll evolve in a way that starts providing more quantitative detail as we go forward so this is
like our our first dipping of the toe in the water on that last point you're referring to the detail
that you're providing about the services that are documented in the card as opposed to the details
of the process of creating the cards yeah yeah and so but I think both will evolve right I mean I think
the language would do we would we use the cards the the way you know and also the amount that we the
amount of detail we provide on the underlying sort of quantitative assessments that are in some
ways the technical backbone of the cards you mentioned some nuances in the the way that you present
certain measurements can you talk a little bit more about that yeah so a couple things so first
of all there was a lot of healthy internal discussion and debate about how much and what to say
about our demographic bias assessments which are quite extensive you know if I were to show you
the full details of them in the end we decided to you know give information about what the demographic
groups we investigated were and of course these vary radically sometimes by service right because
in something like a face recognition service you have you know you you you have visible features
like skin tone hair length the jewelry things like this that are you know correlated with different
demographic groups in spoken language you don't have that right but you do have regional accents
in dialect so like what groups you're going to kind of audit for for lack of a better term and
what kinds of data sets you're going to cure age to do that assessment going very radically by
different services there was debate you know about whether we should release information like okay
you know on this service the worst performing group among these demographic groups was such and such
group and I think there's two very good reasons that in the end we decided not to do that at this point
one is is that the honest scientific truth is that the identity of the group with the worst
performance and what that worst performance looks like can vary radically from data set to data set
so it really can be the case that just on the problem with speech recognition you know different
benchmark data sets the the group that is the best performing and the word group that has the
worst performance can completely change from one data set to another the other common I think
that's worth making is that and this is again some sort of a lesson I've learned at Amazon
the vast majority of kind of fairness notions in the scientific literature on on the topic
essentially adopt some kind of equalization of harm notion it's like okay we're building a model
for consumer lending we think the biggest harm is like a false is a false negative I I predict
that you will not repay alone and so I don't give it going to you whereas in fact you were a
credit worthy and would have repaid it and so then I settled something like okay across these
different combinations of racial and gender groups I want to equalize the false rejection rate
across different groups okay and I think we hopefully don't think that way within AWS and the
reason for that is a couplefold first of all it can just be the case right that some groups present
a greater challenge on a particular problem to another group and so if you insist on equalizing
rates of harm across different groups it could be that the only way you can achieve that is to
deliberately do worse on groups that you're doing better on in order to raise their rate of harm
up to match that of the worst performing group what's an example of that I think the simplest
example would be it may not always be this way but in general things like you know facial hair
and sunglasses present a challenge to face recognition right because there's some kind of
occlusion of your underlying facial structure okay that may not always be the case by the way
maybe at some point we'll figure out ways for instance of you know detecting bone structure
better in a way that would let us kind of see through facial hair but to the extent that it's
makes sense to people that right now facial hair makes face recognition more difficult
if there's a culture or a demographic group in which that is common it's going to be you know
that's it's going to be a it's a harder challenge from a scientific standpoint so the view we adopt
instead rather than saying like well you know successes when we equalize the error rates across
groups our goal is to make every error group error rate as small as we possibly can even if that
doesn't mean that we can equalize all of them and we don't want to do the sort of nonsensical
thing from a product and performance standpoint of you know in the interest of some academic
notion of fairness deliberately doing worse on some group and so the technical work that goes
under that of course involves you know you find out what your worst performing group isn't usually
not always but usually the best solution to get an improvement on that group is to go out and get
better and more data for that particular group but it's because of these two reasons one is that we
don't think in this equalization term and also what the worst and best performing groups are can
change radically from data set to data set we give kind of high level guidance on like what the
worst performing group number was but without sort of saying this was the specific group
that witnessed that number now you could have provided additional information and
specified the data set why did you choose not to do that yeah there was also a healthy internal
debate about how much say about data sets and I think in the initial cards that we're releasing
today we say relatively little about that part of that is because you know first of all
many many data sets go into the training of our models as well as the assessment and all you know
quite often there's many more assessment data sets in the workshop or at least they're very
designed to be different right because you're essentially trying to do stress tests of models so
you know you normally would expect to get very good performance on the type of data that you trained
on but when you start stress testing different use cases things will deliberately will look worse
I think there was also the fear that since so much goes into the training of an ML model and
your technical viewers will know this you know the the cartoon view of machine learning is that
it's a very streamlined almost button pushing process right I get a data set you know and I
I push some button and pie torch now it comes in my model and great but you know the like just
I don't I don't think I'm giving away any big secret at least among the scientists of your viewers
that the amount of artisanal tinkering that goes into modern machine learning is just mind-boggling
and in many ways is actually kind of increased with the rise of deep learning because you know there's
what's the architecture how deep is the network how wide is the network what exactly are this
reactivation units what is the architecture between layers do you have convolutional units etc etc
and the honest truth is that you know even though we have rigorous and effective train test
methodology the way the soup is made is there's a lot of trial and error and you know very things and
so sort of releasing just information about the data sets without sort of the context of the
rest of the training I think we thought it would mislead customers in particular into kind of
equating the training process with just the properties of the data set you're kind of saying that
that reproducibility as a goal is kind of intractable for what you were trying to accomplish
and so you didn't look exactly right so much detail that someone might want to reproduce exactly
exactly like we didn't want to give the illusion that we think these cards have enough
information for you to go try to replicate what they clearly don't and and and we also you know
the other thing is the goal of these cards I think even in their original conception as model
cards in the paper that you you mentioned you know these are meant to be short and readable to
a very wide audience and so the more kind of technical minutia you get into the longer these
cards will become the less they'll be they won't be like cards anymore they'll be like manuals
and the and the audience for them will become more and more limited I mean many people before
offered the analogy of these are you know sort of like the analog of nutrition labels on food
right it's like ordinary shoppers should be able you know who care about it should be able to
pick it up and you know say like oh I don't like some of these ingredients or I'm allergic to them
um and so that that's kind of the goal here that being said I do expect that as time goes on we
will you know our our cards will evolve to say more about our data sets and other topics as well
including possibly more quantitative information about demographic performance okay one one of the
things that we've talked talked thus far about naturally is um data sets comes up a lot in this topic
there was a period of time where there was a pretty vocal contentious argument about
our algorithms bias versus our data sets bias I think that was maybe a couple of years ago
that that really flared up are you still seeing that argument play out or what's your take there
yeah I mean I think both of the things you said are true right so I definitely think it's true
that if you have heavily biased data coming into your training process and you train models
in the ordinary way that doesn't attempt to you know look for or correct that bias then you
should expect to get it in your model as well models are doing yeah it's not the only way though
right and I think maybe the more subtle point that is less widely realized or discussed is that
I mean first of all every data set has some kind of bias right it might not be demographic bias
but it's gathered under certain conditions and if you train a model on data under those conditions
and then try running it on data from radically different conditions you know bad things will
happen it might not be demographic bias but it'll certainly at least be poor performance
right but the other thing can happen is even if you have a data set that you've scrupulously
you know verified is free of at least the demographic biases that you've checked for and care
about you could still end up with a trained model that was heavily biased against one or another
demographic group and the reason for this is that again at a high level it's pretty simple right
machine learning doesn't give you for free anything that you didn't explicitly ask for and it
also doesn't avoid things that you wanted to avoid that you didn't explicitly tell it to avoid
so for instance I'm training a large neural network on some data set that is free of demographic
bias whatever that means but you know the training process is a journey through this very high
dimensional parameter space looking for sort of the minimum error point on the data right and if
it happens to be the case that you know there's a point in the point that minimizes the error
in model space happens to do very poorly on some particular demographic group even though there
might have been a different point even a nearby point in model space that had only slightly higher
overall error but did much better on that demographic group well since you just said no find
the minimum error and you didn't mention anything about like by the way if there happens to be a
point you know in model space that has only infinitesimally larger error but does much better from
a demographic fairness perspective than pick that one then you're not going to get that right and
so at a conceptual level the solution to this is pretty straightforward right instead of solving
what we would you know in technical jargon call a straight up optimization problem minimize the
error in model space on the data you solve a constrained optimization problem right where the
constrained optimization problem is minimize the error on the data subject to these fairness
conditions and this is where kind of the research gets interesting because you know you have to
figure out you know this these are computers and algorithms after all I can't just sort of wave
my hands and say I have to like be able to mathematically specify the same way that the error objective
is mathematically specified I need to be able to mathematically specify what the fairness
constraints are and there's more than one choice for what those constraints look like and those
correspond to sort of different mathematical definitions of fairness and there are different
algorithmic ways of trying to sort of bind a solution to this constrained optimization problem
and this has been sort of a subject of very very active research over the past seven or so years
how does that dichotomy if that's where I turn play out in practice at a place like amazon and I
guess I'm asking are are you know biases in the data sets you know top of mind and easier to
root out than biases in the algorithmic process and you know or or not so much you just have to
be aware that they're there and yeah I don't think I have a a binary answer to that question I think
my intuition is that you know especially in the era of deep learning the training of models has
become very very computationally intensive and very expensive and we are now training very very
large models and so there is a complexity and opaqueness to that process that's you know in my
mind perhaps greater than the mysteries of the data set that goes into that process right I mean
usually if you have certain demographic groups in mind and and by the way you have data annotated
by those demographic groups because if you don't know the demo if you're you know if you say okay I
want to you know make sure that I'm fair by race for example but the data I have is not annotated
by race I can't even audit right so we we need that kind of data or some way or some proxy for
that kind of data but I feel like the problem of assessing whether a data set has bias is is at
this point maybe a more straightforward problem than you know thinking about whether your training
process might inadvertently lead to biases and you know just to touch on a topic that we can
discuss more if you want a good example I would give is that you know you think about the rise of
these very powerful generative models in the past few years you know large language models by GPT
three and things like Dolly you know you enter in some prefix text and you know it auto completes
with you know sentient grammatically correct you know sometimes quite compelling text I mean I
found in experimenting with these things sentient your opinion you put in you put in you know if you
type in prefix text it's kind of like you might have from a novel it has emotional human content
in it you know you get out of completions that are like a short story and you know not always but
sometimes I've been sitting there really thinking you know this is pretty good I think I want to
keep reading this yeah but of course there's some limit on the on the length of the completion if
you're using the other source versions of the short story sort of stops in mid sentence but
you know if you think about what for instance fairness would even mean in these kind of large
language models I mean we have a very good handle I think in relative terms scientifically
on notions of fairness for simple prediction problems like classification or regression
but like what would it mean for a large language model to be fair right I think this is an important
scientific question that we are only beginning to kind of grapple with at all and to say a
little bit more about what I mean you know I could give you very very narrow senses in which I
might ask for a language model to be fair so for example in my own kind of anecdotal experimentation
with some of these models I find that if you type in some prefix of text that mentions an
ungendered name like Chris for instance but you don't use any pronouns so you haven't committed
to the gender of Chris it almost always autocompletes with male pronouns okay so I could say okay you
know a fair language model should you know for this list of ungendered names in North America have a
roughly equal mixture of pronouns in the autocompletion but when you think about distribution of
Chris's is that the gender distribution of Chris I don't know I don't know you know you so you
could ask whether it should match or you know or what what it is right just to but my point here is
that you know anybody who's played around with these things would almost certainly criticize what
I just proposed is like oh my god that is just such a narrow definition of fairness given the power
and the complexity of the output of these models and I think that critique would be right but that's
kind of you know that's where our thinking is right now and I think you know that this is an
area that is going to present a great challenge to the research community in coming years.
Have you seen any early research attempting to address the question?
Well I mean you do have the famous you know word embedding paper from I think roughly 2015 or
2016 but I still think of that which is a very nice paper and a very influential paper but it's
still kind of talking about one particular kind of bias right which is kind of the association
with between gender and the occupations and and other you know sort of other parts of speech
and so you know I think something that we think about a lot is like toxicity in language models
and is this model you know generating slightly more negative sentiment completions when prompted
with you know one type of prompt maybe related to some demographic group or identity versus another
another really interesting thing is you know and this is why I think this area is so fertile
for both research and just you know you know as a society thinking about these problems
there have been a couple of recent papers in which the use case of a large language model
was really deliberately to replicate the biases and correlations that are present in society.
So in one that just came out like a month or so ago the author is basically there's like I
can't remember the name of it I don't think it's the American census survey it's been some other
there's some you know long-standing survey that some organization does I'm blanking on the exact name
of it but the details are less important than what they did this organization goes out and
interviews Americans on their views on various topics so they will go out and sit down with
real citizens take demographic information like this is a housewife you know age 45 who lives
in the Midwest and then they'll you know so they'll collect a bunch of demographic information
and they'll ask the subject their views on things like gun control and abortion and other
controversial topics okay and they publish these things at some cadence and so in this paper about
a month ago they basically used they tried to use a large language model to replicate the numerical
findings of such a survey by designing prompts and those prompts would say something like well
you know Christina is a housewife who lives in the Midwest her attitude on gun control is and then
they you know push the large language model button and then they elicit from that some response
and then they tabulated them yeah and the upshot was that you know they found that you could pretty
up could approximately fairly well replicate the numerical findings of the actual survey by the LLM
and the reason I'm going down this rabbit hole is that this is a use case we can just you know
people can decide whether they think this is a productive use case or not but this is a use case
in which you wouldn't want the the LLM to have been eradicated for any kinds of correlations for
instance between where people live and their views on certain topics or their gender in certain
topics and so it's easy to sort of say like oh well fairness should sort of eradicate these types
of correlations that exist in society but there might it could be that the most valuable use cases
actually are to capture those biases in these very powerful systems. Just thinking about that
from a practical perspective you're you're painting a picture in which perhaps
part of the way these language models are you know rolled out and used more broadly is
you know controlling for the bias of the language model based on the needs of a particular use case
and you know from that perspective to your point there's no single the the goal isn't even a single
definition of fairness it's fairness with regards to a use case and you design for that. Yeah
how you see things. Yeah I mean but I mean I mean so I guess I could imagine a future in which
you know the user of the LLM goes to some dashboard and sort of you know sets sets a bunch of
dials in a way that like okay you know I don't I want you to preserve the bias between gender
and you know or a correlation between gender and attitudes on gun control.
Were you going to want the output of this to be rated R versus rated PG 13 versus rated
PG? Yeah yeah yeah and sort of toxicity of course is a whole other you know separate can of
worms that fairness and you know these are very very thorny issues and so you know it's an
interesting time because as as the power of these things is grown you know to the point that
you know even people in the field like me can be amazed by what they can do you know there's
this sense that okay there's some very interesting conceptual and science questions here but there's
also a pretty serious responsibility on the AI community to you know control this stuff and to
you know and to set guidelines for its use and to decide what use cases are appropriate and
decide what kinds of generative models we should even be building at all even though it might be
possible and so I think you know in many ways these generative models have kind of pushed to the
four some very very difficult questions that weren't quite present when we were just building
powerful models for making point predictions yeah yeah awesome well Michael it's been wonderful
to have an opportunity to chat with you yeah yeah I think we did a pretty good
approximation to our New York dinner conversation so it's been a pleasure and I hope to
hope to see you again soon maybe at a dinner in New York my pleasure thank you
