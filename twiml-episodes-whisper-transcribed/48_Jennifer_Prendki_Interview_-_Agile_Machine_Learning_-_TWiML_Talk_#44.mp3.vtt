WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.600
I'm your host Sam Charrington.

00:23.600 --> 00:27.560
The details are set for the next Twimble Online Meetup.

00:27.560 --> 00:28.840
Mark your calendars.

00:28.840 --> 00:35.200
The September Meetup will be held on Tuesday the 12th from 3 to 4 p.m. Pacific time.

00:35.200 --> 00:40.120
The discussion will be led by Nikola Kuchereva, who will be presenting learning long-term

00:40.120 --> 00:45.800
dependencies with gradient descent is difficult by Joshua Benjiro in company.

00:45.800 --> 00:50.320
This is one of the classic papers on recurrent neural networks so you won't want to miss

00:50.320 --> 00:51.560
it.

00:51.560 --> 00:58.120
For additional details or to join the meetup, head over to twimbleai.com slash meetup.

00:58.120 --> 01:02.160
If you missed the first meetup, the recording is available on that page as well.

01:02.160 --> 01:05.040
My guess this week is Jennifer Prinky.

01:05.040 --> 01:09.320
That name might sound familiar as she was one of the great speakers for my future of

01:09.320 --> 01:11.960
data summit back in May.

01:11.960 --> 01:17.560
At the time, Jennifer was a senior data science manager and principal data scientist at Walmart

01:17.560 --> 01:22.880
Labs, but she's since moved on to become head of data science at Atlassian.

01:22.880 --> 01:27.240
Back at the summit, Jennifer gave an awesome talk on what she calls data mixology.

01:27.240 --> 01:32.240
The slides for which can be found on the show notes page at twimbleai.com slash talk slash

01:32.240 --> 01:33.560
46.

01:33.560 --> 01:37.760
Our conversation this time begins with a recap of that talk, after which we shift our

01:37.760 --> 01:42.040
focus to some of the practices she helped develop and implement at Walmart around the

01:42.040 --> 01:45.800
measurement and management of machine learning models and production.

01:45.800 --> 01:50.760
And more generally, building agile processes and teams for machine learning.

01:50.760 --> 01:55.400
Before we jump in, I want to give a big thank you to our friends at cloud error for sponsoring

01:55.400 --> 01:56.680
this show.

01:56.680 --> 02:00.640
You probably think of cloud error primarily as the Hadoop company, and you're not wrong

02:00.640 --> 02:01.640
for that.

02:01.640 --> 02:05.400
But did you know they also offer software for data science and deep learning?

02:05.400 --> 02:07.000
Yep, they do.

02:07.000 --> 02:08.800
The idea is pretty simple.

02:08.800 --> 02:13.440
If you work for a large enterprise, you probably already have Hadoop in place, and your Hadoop

02:13.440 --> 02:18.400
cluster is filled with lots of data that you want to use in building your models.

02:18.400 --> 02:24.200
But you still need to easily access that data, process it using the latest open source tools

02:24.200 --> 02:28.280
and harness bursts of compute power to train your models.

02:28.280 --> 02:32.480
This is where cloud error's data science workbench comes in.

02:32.480 --> 02:36.440
With the data science workbench, cloud error can help you get up and running with deep

02:36.440 --> 02:41.960
learning without massive new investments by implementing an on demand self service deep

02:41.960 --> 02:45.720
learning platform on existing CDH clusters.

02:45.720 --> 02:49.520
From a tech perspective, data science workbench is pretty neat.

02:49.520 --> 02:54.480
It uses Kubernetes to transparently schedule workloads across the cluster, supporting our

02:54.480 --> 03:00.480
Python and Scala and deep learning frameworks like TensorFlow, Keras, Cafe, and Theano.

03:00.480 --> 03:06.920
And as of last month's 1.1 release, GPUs on the Hadoop cluster are fully supported.

03:06.920 --> 03:11.200
The folks at cloud error are so confident that you're going to like what you see that for

03:11.200 --> 03:16.040
a limited time, they're offering a drone to qualified participants who register for

03:16.040 --> 03:19.240
a demo of the data science workbench.

03:19.240 --> 03:24.520
For your demo and drone, visit twimmaleye.com slash cloud error.

03:24.520 --> 03:29.040
And now on to the show.

03:29.040 --> 03:37.400
All right, everyone, I am on the line with Jennifer Prinky.

03:37.400 --> 03:43.880
Jennifer is a senior data science manager at Wal-Mart Labs, specializing in machine

03:43.880 --> 03:44.880
learning.

03:44.880 --> 03:49.360
And I am super excited to have her on the line with me, Jennifer, welcome to the show.

03:49.360 --> 03:50.360
Hi, Sam.

03:50.360 --> 03:51.360
Nice to be here.

03:51.360 --> 03:52.840
Yeah, nice to have you here.

03:52.840 --> 03:55.200
And it is so nice to speak with you again.

03:55.200 --> 03:57.200
Folks recognize Jennifer's name.

03:57.200 --> 04:02.000
It's because Jennifer was one of the speakers at the future of data summit.

04:02.000 --> 04:07.320
And she so graciously offered to spend some time with us to talk a little bit about what

04:07.320 --> 04:10.200
she's doing at Wal-Mart Labs.

04:10.200 --> 04:14.600
Before we jump into that, Jennifer, why don't we have you spend a little bit of time talking

04:14.600 --> 04:20.040
about your background and how you ended up working in machine learning at Wal-Mart?

04:20.040 --> 04:21.040
Sure.

04:21.040 --> 04:25.640
So actually, when I tell people what my background is, they're a little bit surprise because

04:25.640 --> 04:28.640
I'm actually a particle physicist originally.

04:28.640 --> 04:33.920
And so the reason why it's not as crazy as you might think at first is that I was doing

04:33.920 --> 04:38.000
the type of particle physics where you have lots of data to treat.

04:38.000 --> 04:43.120
And so I was actually working with a huge amounts of data even before the word data science

04:43.120 --> 04:45.840
become as trendy as it is today.

04:45.840 --> 04:51.600
So I mean, the reason why I eventually switched to pure data science and specifically retail

04:51.600 --> 04:57.240
data science is that I was looking for like lots of data, interesting data to work with.

04:57.240 --> 05:02.080
And so it actually turns out that retail has lots of very interesting challenges for somewhat

05:02.080 --> 05:03.920
passionate with data to work with.

05:03.920 --> 05:06.320
So here I am, right?

05:06.320 --> 05:07.320
Fantastic.

05:07.320 --> 05:08.320
Fantastic.

05:08.320 --> 05:13.120
Can you tell us a little bit about the talk that you gave at the summit?

05:13.120 --> 05:15.920
What were your goals for that presentation?

05:15.920 --> 05:16.920
Yes.

05:16.920 --> 05:20.520
So my topic for the summit was something I call data mixology, right?

05:20.520 --> 05:26.800
I mean, so my goal was to try to set society people to the fact that the real challenge with

05:26.800 --> 05:31.360
big data today is not necessarily velocity or volume as people think.

05:31.360 --> 05:33.640
It's really about viability, right?

05:33.640 --> 05:38.040
Because when you start plugging in several data sources, sometimes you have to rethink

05:38.040 --> 05:44.960
your model entirely and you have to deal with all challenges related to data silos and

05:44.960 --> 05:48.600
understanding the quality of the data coming from different sources.

05:48.600 --> 05:53.680
And so, yeah, I mean, I really thought that this was a topic that was not necessarily covered

05:53.680 --> 05:57.640
enough in the different conferences that I had been around recently.

05:57.640 --> 06:00.880
So I thought it was an interesting topic to cover.

06:00.880 --> 06:06.080
It was definitely an interesting topic and it was clear as you were delivering it that

06:06.080 --> 06:08.480
it came from your experience.

06:08.480 --> 06:13.680
How did these issues of silos manifest themselves in your world?

06:13.680 --> 06:21.920
Now, so the way it comes around on my experience is that I recently started a new team where

06:21.920 --> 06:27.960
essentially the goal is to try to use both stores data from Walmart and online data from

06:27.960 --> 06:29.440
Walmart and bring them together.

06:29.440 --> 06:30.440
Okay.

06:30.440 --> 06:35.680
So in the Walmart world, truth is, I mean, the Walmart e-commerce business and the Walmart

06:35.680 --> 06:38.800
stores business are essentially separated.

06:38.800 --> 06:42.320
It's not the same people and even the data lives in separate places.

06:42.320 --> 06:48.280
It's not necessarily trivial for an e-commerce data scientist at Walmart to access the store

06:48.280 --> 06:49.960
sales data, for example.

06:49.960 --> 06:55.240
And so as we were trying to bring these two worlds together, I actually came to discover

06:55.240 --> 07:00.120
first-hands all the different challenges you have from bringing different data sources

07:00.120 --> 07:02.640
together, even when it comes from the same company.

07:02.640 --> 07:05.160
So this is exactly how I came up.

07:05.160 --> 07:08.920
You like to come to speak about this topic.

07:08.920 --> 07:17.720
And a lot of companies are pursuing ideas like data lakes or that idea by various different

07:17.720 --> 07:18.720
names.

07:18.720 --> 07:22.960
Is that something that you guys ended up doing or did you take a different approach to

07:22.960 --> 07:24.600
integrating all this data?

07:24.600 --> 07:27.640
Now, we're absolutely taking that direction, right?

07:27.640 --> 07:33.440
But as you can imagine, right, I mean, the challenge for Walmart is really that you have a Walmart

07:33.440 --> 07:39.240
e-commerce, which is a tech company that there's more recent and really like a typical Silicon

07:39.240 --> 07:40.560
Valley company.

07:40.560 --> 07:46.440
And on the other hand, this huge Walmart company, legacy company that has lots of data.

07:46.440 --> 07:49.240
They actually been gathering data for a long time now.

07:49.240 --> 07:53.320
I think they were one of the first companies actually realized that data was so important.

07:53.320 --> 07:57.880
And so you really have to deal with different types of systems altogether.

07:57.880 --> 08:00.200
We're not necessarily using the same technology.

08:00.200 --> 08:05.640
So we're definitely after the creation of a data lake where all data scientists across

08:05.640 --> 08:10.080
the company would be able to come and look at their, the same data.

08:10.080 --> 08:11.880
But it's a long road, right?

08:11.880 --> 08:16.400
I think every company that is trying to tackle this challenge knows that it is a long road.

08:16.400 --> 08:20.800
And it requires a lot of different skillset and lots of different people and expertise

08:20.800 --> 08:22.960
to actually achieve the goal.

08:22.960 --> 08:27.160
It's funny, I think the way that some of the vendors in the space talk about it is that

08:27.160 --> 08:32.840
you just set up, you know, set up a Hadoop cluster and run some ETL jobs and you'll have

08:32.840 --> 08:33.840
a data lake.

08:33.840 --> 08:38.240
What are some of the challenges that you ran into and what makes it, what makes the road

08:38.240 --> 08:39.240
long?

08:39.240 --> 08:42.840
Now, I mean, so I mean, I'll give you a specific example.

08:42.840 --> 08:48.760
So one of the very interesting data sets that everybody across the company wants to work

08:48.760 --> 08:51.600
with is the online engagement data, right?

08:51.600 --> 08:57.000
I mean, essentially, which items does the customer actually click on and what do they eventually

08:57.000 --> 08:58.000
buy, right?

08:58.000 --> 09:01.800
And so this is a data set that, for example, stores doesn't have access to because they

09:01.800 --> 09:04.880
don't have engagement, they just have their final purchases.

09:04.880 --> 09:09.560
So they don't have any way to measure properly the interest of the customer as long as they

09:09.560 --> 09:11.280
don't purchase something.

09:11.280 --> 09:16.040
And so people have to keep like actually getting this data from us and they actually get

09:16.040 --> 09:21.840
data dump, right, and they don't necessarily create like a exhaustive signal pipelines

09:21.840 --> 09:23.160
to get this real time.

09:23.160 --> 09:27.520
And so there are lots of different versions of these data that live across the company.

09:27.520 --> 09:32.960
And so whenever we Walmart e-commerce make a change to this data, it's not easy to

09:32.960 --> 09:34.960
communicate these changes to other teams.

09:34.960 --> 09:40.880
And so one of the challenges is you don't necessarily know any more which is the original

09:40.880 --> 09:42.520
source of truth.

09:42.520 --> 09:46.840
So in that specific case, it might be easier because you know who the owner is, but in some

09:46.840 --> 09:50.400
other cases, we don't necessarily even know where the data is coming from.

09:50.400 --> 09:55.800
And so everybody's interested in the same data, but this data exists in multiple versions.

09:55.800 --> 10:00.600
And it's actually very hard to come up with a procedure to actually figure out which one

10:00.600 --> 10:05.120
is the best one and which one is the accurate source of truth.

10:05.120 --> 10:09.440
That actually gives us a really interesting segue to one of the main topics that I wanted

10:09.440 --> 10:12.640
to dig in with you here on the podcast.

10:12.640 --> 10:18.880
And that is one of the interesting aspects of your role is leading a team that's focused

10:18.880 --> 10:26.440
on measuring and auditing for the various machine learning models at Walmart.

10:26.440 --> 10:33.200
And you mentioned this source of truth and data providence is kind of one small aspect

10:33.200 --> 10:34.200
of that.

10:34.200 --> 10:39.840
And tell us a little bit about your role and some of the type of work that you're focused

10:39.840 --> 10:40.840
on in that role.

10:40.840 --> 10:41.840
Right.

10:41.840 --> 10:42.840
Definitely.

10:42.840 --> 10:48.320
So I'm actually a part of a group called the search algorithms team.

10:48.320 --> 10:52.600
So we're essentially the group of data scientists and machine learning experts that take care

10:52.600 --> 10:58.800
of all machine learning algorithms that you would see at work on the walmart.com page,

10:58.800 --> 10:59.800
right?

10:59.800 --> 11:05.360
That includes learning to rank algorithms and involves everything related to the understanding

11:05.360 --> 11:06.360
of the customer.

11:06.360 --> 11:11.240
So we actually split down the responsibilities on my team into three different portions.

11:11.240 --> 11:15.760
So there is something called the perceived team, which is essentially in charge of trying

11:15.760 --> 11:17.560
to understand what the customer wants.

11:17.560 --> 11:18.560
Right.

11:18.560 --> 11:24.000
I mean, so query understanding it involves a lot of natural language processing algorithms,

11:24.000 --> 11:28.600
auto completion algorithms, spell checking algorithms would be their responsibilities.

11:28.600 --> 11:29.600
Okay.

11:29.600 --> 11:30.600
Here's the guide team.

11:30.600 --> 11:35.520
So the guide team is about learning to rank and showing the right items once you think

11:35.520 --> 11:37.720
you understand what the customer is looking for.

11:37.720 --> 11:43.360
And then we have this measure team, which is my team that essentially takes care of helping

11:43.360 --> 11:49.040
the others understand their weaknesses, suggest new data sets that they can use, suggest

11:49.040 --> 11:55.080
best practices, make sure that these other algorithms are retrained properly at the proper

11:55.080 --> 12:00.640
frequency, catch problems early on, so we're essentially creating models to take care

12:00.640 --> 12:01.640
of other models, right?

12:01.640 --> 12:06.800
I mean, so we create specific measurement scoring systems that range from data quality

12:06.800 --> 12:08.920
to customer satisfaction.

12:08.920 --> 12:13.680
So we're trying to bring like a essentially what the team that gets a real profound understanding

12:13.680 --> 12:18.200
of the other algorithms in order to help the others understand what they need to do to

12:18.200 --> 12:20.080
make it even better.

12:20.080 --> 12:26.680
And are you primarily focused on helping the search teams or are you, do you also work

12:26.680 --> 12:32.040
with teams outside of search that are doing data science and machine learning?

12:32.040 --> 12:36.080
So that's an interesting question, because my original mission was definitely to help

12:36.080 --> 12:37.400
the search team.

12:37.400 --> 12:41.320
But we actually, it turns out that we are the only measured team within the company.

12:41.320 --> 12:45.440
And so once people started understanding what we're doing, we actually get lots of

12:45.440 --> 12:50.560
requests from other teams to actually help them as well, right?

12:50.560 --> 12:54.680
Search is obviously an area where you have lots of different teams that are involved

12:54.680 --> 12:55.680
with us, right?

12:55.680 --> 13:00.280
And so we're really focusing on search, but you can imagine that the team in charge of

13:00.280 --> 13:05.440
the inventory and the catalog is also a team, teams that we are very close, closely working

13:05.440 --> 13:06.440
with.

13:06.440 --> 13:09.520
So it's pretty natural that we also bring measurements for them.

13:09.520 --> 13:15.200
And so another area where we're also partnering with other teams is that we actually created

13:15.200 --> 13:21.400
an entire process called machine learning lifecycle management, which is essentially a checklist

13:21.400 --> 13:27.400
of things that we believe all machine learning models should, I mean, people who work on machine

13:27.400 --> 13:30.400
learning models should do before pushing something to production.

13:30.400 --> 13:33.720
And so it actually turns out that we have a pretty efficient system now.

13:33.720 --> 13:39.000
So I mean, we are essentially requiring data scientists to provide, you know, like a very

13:39.000 --> 13:43.840
clear view of what the accuracy is, but also what the performance of the algorithm is

13:43.840 --> 13:48.560
in terms of the amount of CPU that their model consume when they're retraining and so

13:48.560 --> 13:50.120
for sets, so on and so on.

13:50.120 --> 13:56.640
We are not trying to expand this process to the entire e-commerce section of Walmart.

13:56.640 --> 14:00.560
And it actually turns out that lots of people are interested by that because the challenging

14:00.560 --> 14:04.240
data science is oftentimes in a company like ours.

14:04.240 --> 14:08.520
You have machine learning engineers who are really like engineering people who don't necessarily

14:08.520 --> 14:13.080
understand the limitation of data science properly speaking, right?

14:13.080 --> 14:17.440
And so they are not necessarily trained to think in terms of evaluating the accuracy

14:17.440 --> 14:21.080
and making the proper checks before sending something to production.

14:21.080 --> 14:25.160
They are the type of people who are really looking forward to see their model in action

14:25.160 --> 14:29.840
and they don't necessarily take the time to evaluate the statistical performance of the

14:29.840 --> 14:30.840
models.

14:30.840 --> 14:34.840
And so creating this, you know, like this process is really making sure that everybody's

14:34.840 --> 14:37.760
on the same page that things are running properly in production.

14:37.760 --> 14:44.360
It's interesting, it makes me think of a few years ago when the software development community

14:44.360 --> 14:50.400
went through this process of like industrializing the delivery of software and that resulted

14:50.400 --> 14:57.840
in ideas like lean and agile methodologies and DevOps and things like that.

14:57.840 --> 15:05.160
And it sounds like you guys are kind of on the, you know, the cutting edge of an industrialization

15:05.160 --> 15:14.200
wave of machine learning, not to be confused at all with the industrial AI line of inquiry

15:14.200 --> 15:17.120
that we've talked about here in the podcast recently.

15:17.120 --> 15:22.880
But I love this idea of a machine learning lifecycle model.

15:22.880 --> 15:27.840
What can you tell us about that model and the, you know, the various steps and stages

15:27.840 --> 15:31.440
and requirements that you've put in place for the teams there?

15:31.440 --> 15:32.440
Right.

15:32.440 --> 15:37.240
So I mean, you definitely write about, you know, like that being like a new wave of agile,

15:37.240 --> 15:38.240
right?

15:38.240 --> 15:42.160
I mean, agile for data science or machine learning, this is exactly what we're after.

15:42.160 --> 15:47.720
So I mean, as we were putting the like the first steps together, I actually came to realize

15:47.720 --> 15:49.920
that it is really a cultural problem, right?

15:49.920 --> 15:53.960
I mean, because if you want to reach a stage where things are done properly, you're really

15:53.960 --> 15:58.960
about trying to fix tech debt, but people usually think of tech debt as code debt, right?

15:58.960 --> 16:04.240
I mean, this is the way that people came to know code debt and truth is tech debt is much

16:04.240 --> 16:05.240
more than this, right?

16:05.240 --> 16:08.640
I mean, there's a, there are actually more pieces to tech debt than just code debt.

16:08.640 --> 16:13.440
There is a definitely data that related to like the quality of your data, but also the

16:13.440 --> 16:17.280
data sets that you may not be using, but your competitors are using, right?

16:17.280 --> 16:21.760
I mean, so if you're actually in a situation where, for example, we know, for example,

16:21.760 --> 16:26.280
that Amazon is using a specific data set that we have, but we are not using, you're actually

16:26.280 --> 16:29.360
aware in the data debt situation, right?

16:29.360 --> 16:31.320
Then there is the notion of system debt.

16:31.320 --> 16:38.120
So the case where you're using legacy systems and you're not improving and getting to use

16:38.120 --> 16:45.600
the latest versions of a specific software or not like a newest cutting-edge software that

16:45.600 --> 16:47.560
is intended into three.

16:47.560 --> 16:51.360
And then you have machine learning that so machine learning that is really when you're using

16:51.360 --> 16:54.600
a machine learning model, not to the best of its ability, right?

16:54.600 --> 16:58.600
I mean, so, for example, if you don't understand at which frequency you should be retraining

16:58.600 --> 17:02.600
a model, you don't understand, you don't monitor the inputs and outputs.

17:02.600 --> 17:06.240
It's definitely also a situation that you have to take care of.

17:06.240 --> 17:10.360
So I mean, the steps that, you know, when somebody asked me, how, what should I do to

17:10.360 --> 17:15.920
actually get started with, you know, like a automation and I'll try to, like, basically

17:15.920 --> 17:17.560
audit my models, what should I do?

17:17.560 --> 17:21.840
So my answer to that is, it's not necessary, something that's very complicated.

17:21.840 --> 17:25.920
It's really about a process and also creating a culture in your company where everybody

17:25.920 --> 17:29.800
understands that making things right is important.

17:29.800 --> 17:34.120
And so it really depends on the kind of model you're dealing with, but like usually one thing

17:34.120 --> 17:38.880
I suggest everybody should do is make sure that you document everything that you're doing,

17:38.880 --> 17:39.880
right?

17:39.880 --> 17:44.280
I mean, so it may sound like a cheesy answer, you know, but it's definitely super important.

17:44.280 --> 17:48.960
We actually turned out that most of the times when we didn't have a model performing well

17:48.960 --> 17:51.920
enough, it wasn't necessary because of the model itself.

17:51.920 --> 17:56.400
It was because we didn't have a clear understanding of what the model was doing, right?

17:56.400 --> 18:00.880
And so we were not able to reproduce the same model.

18:00.880 --> 18:02.560
There was a lack of transparency.

18:02.560 --> 18:07.040
And so for example, you would have a new engineer coming over and trying to take over the

18:07.040 --> 18:11.840
project and they wouldn't even know how the model was built.

18:11.840 --> 18:16.360
So the other thing is you're, it's extremely important that you have a clear understanding

18:16.360 --> 18:21.240
of what your failures and weaknesses were, so that I mean, people tend to forget that

18:21.240 --> 18:25.960
you're like in the concept of machine learning, life cycle management, there is the worst

18:25.960 --> 18:26.960
cycle, right?

18:26.960 --> 18:30.680
I mean, so there is an opportunity for everybody to learn about their weaknesses in order

18:30.680 --> 18:35.120
to make sure that the next iteration of your model is better.

18:35.120 --> 18:36.120
Right.

18:36.120 --> 18:40.520
And you're like, so definitely think about the culture that you have to bring in your

18:40.520 --> 18:44.920
company and make sure that you keep in track of everything you're doing, that it is

18:44.920 --> 18:49.680
very clear the data you're using, it is very clear that you understand the quality of

18:49.680 --> 18:52.640
your data and you understand your challenges.

18:52.640 --> 18:57.640
On the various teams there, can you tell me a little bit about the relationship between

18:57.640 --> 19:04.880
data scientists and people with a statistical orientation and developers and engineers?

19:04.880 --> 19:08.640
Yeah, I can absolutely tell you about that.

19:08.640 --> 19:14.200
So actually my team has a statistical analyst, data scientist and machine learning engineers.

19:14.200 --> 19:17.800
So people sometimes struggle to understand what the difference is.

19:17.800 --> 19:23.520
So really our, in our view, a statistical analyst are people who know how to play with

19:23.520 --> 19:25.600
the data really, really well.

19:25.600 --> 19:31.200
So they essentially like can get you, you know, like a very clear understanding of whether

19:31.200 --> 19:36.200
your data is sufficient entropy and sufficient variance for you to build a model and then

19:36.200 --> 19:40.120
can give you answers very quickly to get started.

19:40.120 --> 19:44.400
The data scientist is actually the person that would, I would say like prototype a model,

19:44.400 --> 19:45.400
right?

19:45.400 --> 19:48.640
And so once you have an understanding that your data is good enough for you to solve a

19:48.640 --> 19:53.680
specific problem, the data scientist will come up with a solution and essentially try

19:53.680 --> 19:58.960
to assess which is the best type of machine learning model for you to solve that problem.

19:58.960 --> 20:02.720
So we don't necessarily expect like the statistical analyst to be someone who's an expert

20:02.720 --> 20:03.720
in machine learning.

20:03.720 --> 20:07.160
I mean, of course, they have some understanding that they are not the persons that be in

20:07.160 --> 20:09.560
charge of creating a model.

20:09.560 --> 20:14.400
And then the machine learning engineer is someone that knows how to optimize this machine

20:14.400 --> 20:17.400
learning model and make it work at scale.

20:17.400 --> 20:22.440
So they're really like focusing on making everything efficient and I mean, they really have

20:22.440 --> 20:24.200
the ability to push that to production.

20:24.200 --> 20:29.080
So having all this skill set together in one team has been really helpful for us because

20:29.080 --> 20:32.360
it really helps us move things to production really quickly.

20:32.360 --> 20:37.240
One of the things I've seen in the past with organizations that have a model similar to

20:37.240 --> 20:42.400
yours, although I think less sophisticated in the way you are managing it and the machine

20:42.400 --> 20:49.360
learning life cycle processes that you've introduced is a little bit of friction in kind

20:49.360 --> 20:56.520
of the interface between the data scientists and the machine learning engineers where you

20:56.520 --> 21:02.760
would have a data scientist, you know, create a model kind of coded up using, you know, maybe

21:02.760 --> 21:07.280
even a set of tools that the are not the set of tools that the ML engineers are working

21:07.280 --> 21:14.200
with kind of throw it over the wall and then have this machine learning engineer who,

21:14.200 --> 21:20.240
you know, maybe less sophisticated in understanding the model, you know, try to implement it often

21:20.240 --> 21:26.200
in, you know, going from, you know, Python, for example, to Java or something like that.

21:26.200 --> 21:32.120
And that both resulting in, you know, creating an opportunity for the introduction of errors

21:32.120 --> 21:39.400
as well as slowing cycle time and iteration time, just because of the back and forth over

21:39.400 --> 21:45.960
this barrier, how have you guys seen that at all and how have you addressed it?

21:45.960 --> 21:50.920
Now, so I definitely saw see how that problem can arise, right?

21:50.920 --> 21:55.240
I mean, so I think like at the very beginning, when this team was still very decent, I mean,

21:55.240 --> 21:57.200
we definitely had that problem.

21:57.200 --> 22:02.880
The way we kind of sold it is that there is actually a very decent overlap between the

22:02.880 --> 22:05.280
data scientists and the machine learning engineer.

22:05.280 --> 22:10.240
So usually the data scientists would actually code something, which is pretty close to what

22:10.240 --> 22:14.720
would end up being in production except that it is not necessarily functioning as scale,

22:14.720 --> 22:15.720
right?

22:15.720 --> 22:17.360
I mean, so usually they use the same language.

22:17.360 --> 22:19.000
So that's for sure.

22:19.000 --> 22:23.560
The other thing is we make sure that I actually like I have my machine learning engineers

22:23.560 --> 22:25.720
and my data scientists working pairs.

22:25.720 --> 22:26.720
Okay.

22:26.720 --> 22:30.720
So the machine learning engineer is actually involved in the early stages as well, but he's

22:30.720 --> 22:32.480
not the tech lead for that portion, right?

22:32.480 --> 22:38.080
I mean, so he actually gets to be involved and immersed with the model like very early

22:38.080 --> 22:42.880
on, which gives him some more sophisticated understanding of the model that makes it easier

22:42.880 --> 22:47.440
for him to him or her to actually push it to production later.

22:47.440 --> 22:51.600
So we don't really have like this French association faces really like the entire pair

22:51.600 --> 22:56.160
is working throughout the process except that the first phase is the phase where the data

22:56.160 --> 23:00.120
scientists is in charge and the last phase is the phase where the machine learning person

23:00.120 --> 23:01.120
is in charge.

23:01.120 --> 23:02.120
Okay.

23:02.120 --> 23:07.040
That's another really adaptation of the agile idea or at least the pair programming notion

23:07.040 --> 23:11.960
of agile to this machine learning lifecycle.

23:11.960 --> 23:12.960
Interesting.

23:12.960 --> 23:13.960
Interesting.

23:13.960 --> 23:21.520
So you develop these models, you get them in production and then you are tasked with

23:21.520 --> 23:26.960
tracking and measuring and auditing their performance, not just when you're putting them

23:26.960 --> 23:30.360
in the production, but over time, tell us a little bit about that cycle.

23:30.360 --> 23:31.800
Yeah, sure, sure.

23:31.800 --> 23:37.080
So I mean, the interesting thing was, you know, like at first when we came up with this

23:37.080 --> 23:42.080
new model of having like an external team kind of measuring things was pretty interesting,

23:42.080 --> 23:43.080
right?

23:43.080 --> 23:47.320
Because our other teams up to that point in time they were actually used to essentially

23:47.320 --> 23:52.560
come up with a success metric that they would use for essentially build a model and they

23:52.560 --> 23:57.360
would actually use the same success metric for measuring and auditing this model themselves,

23:57.360 --> 23:58.360
right?

23:58.360 --> 24:02.360
And so the value proposition was that you're kind of in the situation where you have a

24:02.360 --> 24:03.360
conflict of interest, right?

24:03.360 --> 24:04.360
Right.

24:04.360 --> 24:05.360
Right.

24:05.360 --> 24:10.360
And even if you want to be like a really truthful, I mean, if the same person is actually coming

24:10.360 --> 24:14.680
up with measurements and actually assessing their own models, they don't necessarily see

24:14.680 --> 24:16.320
things in a different light, right?

24:16.320 --> 24:21.520
I mean, so the value proposition here is that you have a different person that doesn't

24:21.520 --> 24:25.800
know or knows very little about the model auditing things and actually come up with their

24:25.800 --> 24:29.560
own definition of what success means for that model, right?

24:29.560 --> 24:32.880
So we had a little bit of tension at the beginning, as you can imagine, right?

24:32.880 --> 24:35.960
Because it's almost like you use the word auditing, right?

24:35.960 --> 24:37.480
And that's definitely what we do, right?

24:37.480 --> 24:42.360
I mean, so you're in a situation where everybody's wondering like, well, what is the status

24:42.360 --> 24:43.360
of my model?

24:43.360 --> 24:46.680
Well, these guys are going to find anything wrong with my model.

24:46.680 --> 24:51.800
So it took some time for us to actually make it very clear that we are not here to actually

24:51.800 --> 24:52.800
judge your work.

24:52.800 --> 24:55.560
We're actually here to help you improve it, right?

24:55.560 --> 24:56.560
Right.

24:56.560 --> 24:57.560
Right.

24:57.560 --> 24:58.560
Right.

24:58.560 --> 25:00.880
But I mean, I think everybody's very comfortable right now that we're actually in charge

25:00.880 --> 25:03.920
of making sure of the quality of the model.

25:03.920 --> 25:06.960
So we have a very good dynamic with the other teams right now.

25:06.960 --> 25:13.480
And it comes to measuring the performance of the models that you guys are using.

25:13.480 --> 25:20.560
Are you focusing on business metrics or technical model performance metrics or a combination

25:20.560 --> 25:21.560
of both?

25:21.560 --> 25:24.120
It's definitely a combination of both.

25:24.120 --> 25:28.120
I mean, so the reason why we believe that there should be an entire team focused on this

25:28.120 --> 25:32.520
is as you can imagine, there is not one single metric per model, right?

25:32.520 --> 25:33.520
Sure.

25:33.520 --> 25:37.560
We actually have like some models actually use like several metrics or several tens of

25:37.560 --> 25:42.040
metrics to actually make sure that we have a comprehensive view of how the model is

25:42.040 --> 25:43.040
performing.

25:43.040 --> 25:48.880
And so it ranges from like a how accurate is the model to how efficient is the model in

25:48.880 --> 25:52.920
terms of your like is it using too much CPU as I mentioned earlier?

25:52.920 --> 25:56.480
And is it is it impacting the customer in a proper way, right?

25:56.480 --> 26:02.640
So our belief is that you should have a specific metric for every single model separately.

26:02.640 --> 26:08.240
So in retail, it is pretty traditional to use typically like the number of add to cards

26:08.240 --> 26:13.840
or the number of clicks or even the revenue as a measurement of your like success when

26:13.840 --> 26:16.800
you, for example, run a bit tests, right?

26:16.800 --> 26:21.080
So our belief is that because you have these two steps, right, understanding the customer

26:21.080 --> 26:26.040
through perceive and guiding the customer through guide, we believe that you should have

26:26.040 --> 26:30.800
metrics specific to each one of these portions, specifically because otherwise you're looking

26:30.800 --> 26:34.760
at all models in terms of add to cards, it doesn't really make sense, right?

26:34.760 --> 26:39.600
Because the perception phase is really about understanding the customer not necessarily.

26:39.600 --> 26:45.960
So for example, if I ever drop in add to cards, it is possible that my new perceived algorithm

26:45.960 --> 26:51.080
is really working well, but because there is a bottleneck with the guide phase, I won't

26:51.080 --> 26:53.160
see that this model is performing well, right?

26:53.160 --> 26:58.280
I mean, so really making sure that you have very narrow and very specific metrics, even

26:58.280 --> 27:02.840
if it means having many of them is definitely working very well for us.

27:02.840 --> 27:05.160
I guess I have mixed feelings about that hearing it.

27:05.160 --> 27:11.840
I wonder about local minima, local maxima, or I guess probably a better way to put it

27:11.840 --> 27:19.120
is unit test versus integration test or system tests, like what if you're creating, you

27:19.120 --> 27:25.720
have a measure that the perceived team is able to maximize, but it doesn't maximize

27:25.720 --> 27:31.640
the overall metric of something like revenue or an add to card.

27:31.640 --> 27:33.640
How do you manage that?

27:33.640 --> 27:35.120
That's a very good question, actually.

27:35.120 --> 27:40.080
We see that problem very often, so basically, you would have a new perceived algorithm

27:40.080 --> 27:46.160
that performs really well, but you'd actually see that it actually causes the guide performance

27:46.160 --> 27:47.160
to drop, right?

27:47.160 --> 27:50.800
I mean, so you definitely have this kind of cannibalization problems.

27:50.800 --> 27:52.400
Let me give you an example, right?

27:52.400 --> 27:57.400
I mean, so we actually figured at some point that when you're actually improving the accuracy

27:57.400 --> 28:05.920
or the efficiency of your autocompletion algorithms, it essentially drops the performance of the

28:05.920 --> 28:07.520
spell check algorithm.

28:07.520 --> 28:08.520
Why?

28:08.520 --> 28:12.400
Because if people can use the autocompletion algorithm, they're not going to finish entering

28:12.400 --> 28:17.480
the queries by end, which means that the spell checking algorithm is not called that

28:17.480 --> 28:18.480
offer, right?

28:18.480 --> 28:21.400
I mean, it's pretty logical, if you think about it.

28:21.400 --> 28:25.000
So I mean, this is exactly the kind of thing you want to observe, because in that specific

28:25.000 --> 28:29.480
scenario that essentially allows us to say, you know what, it's worth investing more

28:29.480 --> 28:36.720
time, making a perfect autocompletion algorithm rather than making a perfect spell check algorithm.

28:36.720 --> 28:41.680
So you actually use these inefficiencies to determine which algorithms you should focus

28:41.680 --> 28:42.680
on.

28:42.680 --> 28:43.680
Interesting.

28:43.680 --> 28:51.360
So a related question that I've had for folks in the retail space is around short

28:51.360 --> 28:58.280
sighted versus long sighted models, and this, an example here might be, you know, as

28:58.280 --> 29:05.680
we talked about, it's pretty common to optimize your models around add to carts or even, you

29:05.680 --> 29:12.520
know, short term, you know, even immediate revenue creation or even something like profitability

29:12.520 --> 29:19.080
to be kind of one level higher in business impact.

29:19.080 --> 29:27.720
But I wonder if, when you're doing that, if it's possible that you are sub optimizing the

29:27.720 --> 29:33.360
broader metric like customer lifetime value or something along those lines, is that something

29:33.360 --> 29:35.520
that you think about there at all?

29:35.520 --> 29:38.320
Now, we definitely have that as a metric.

29:38.320 --> 29:40.800
So you suggested like a customer lifetime value.

29:40.800 --> 29:45.360
This is one of the metric you would monitor against the entire process, which is why I

29:45.360 --> 29:49.080
say that you need to have several metrics for every model, right?

29:49.080 --> 29:54.080
I mean, so we make sure that we keep track of all different aspects and dimensions of

29:54.080 --> 29:55.080
the problem.

29:55.080 --> 30:00.040
But as in always in business, at the end of the day, you have to follow a business decision

30:00.040 --> 30:01.040
as well, right?

30:01.040 --> 30:05.840
And so if the goal of the company is to increase revenue drastically over the next quarter,

30:05.840 --> 30:10.080
I mean, you, at the end of the day, you, you align your decision based on this as well,

30:10.080 --> 30:11.080
right?

30:11.080 --> 30:14.320
I mean, at the end of the day, the final choice of what you're like, which algorithm

30:14.320 --> 30:17.680
you should improve comes down to a business decisions.

30:17.680 --> 30:22.560
Our goal is really to make sure that they have all information in hand and handy to actually

30:22.560 --> 30:24.320
make a decision based on that, right?

30:24.320 --> 30:29.000
I mean, so whatever they decide to do, we make sure that they are aware that if they choose

30:29.000 --> 30:34.960
to do a specific or take a specific decision, it may impact customer lifetime value out

30:34.960 --> 30:36.600
these kind of things.

30:36.600 --> 30:45.280
Are there other instances where you are, where you're working to balance short-term versus

30:45.280 --> 30:47.920
long-term optimization targets?

30:47.920 --> 30:53.800
Well, I mean, obviously for as far as I've seen things that were more so far as really

30:53.800 --> 30:56.800
like this kind of optimization would come down to a business decision, right?

30:56.800 --> 31:02.000
I mean, so I don't think we're already reached a level where we can forecast our predict

31:02.000 --> 31:09.480
the future well enough to actually get to this side to comprehensive knowledge that brings

31:09.480 --> 31:13.080
everybody on the same page, for sure.

31:13.080 --> 31:14.080
Right.

31:14.080 --> 31:19.240
Are you in the process of auditing these various teams?

31:19.240 --> 31:28.800
Do you have a list, either formal or in your head of these are the top end things that

31:28.800 --> 31:35.720
people tend to do wrong or put another way, what's your advice for folks that want to

31:35.720 --> 31:42.600
learn from what you've learned from your teams on how they should approach modeling?

31:42.600 --> 31:43.600
Right.

31:43.600 --> 31:44.600
Definitely.

31:44.600 --> 31:50.040
So I would see three things that I believe are good take-up is for everybody who's trying

31:50.040 --> 31:51.320
to tackle this problem.

31:51.320 --> 31:56.840
So the first one is definitely what I would say before making sure you document everything,

31:56.840 --> 32:01.480
especially in large organizations where the turnover of your employees is really high,

32:01.480 --> 32:02.480
right?

32:02.480 --> 32:06.160
I mean, you want to make sure that if something went wrong with the past model, at least

32:06.160 --> 32:10.880
you know what went wrong and you're at the ability of fixing this in the next situation.

32:10.880 --> 32:16.920
And so make sure that anyone can actually grab that model and reproduce the same results.

32:16.920 --> 32:17.920
That's one thing.

32:17.920 --> 32:23.720
The other thing is I actually noticed that many times when our models are unsuccessful,

32:23.720 --> 32:28.360
it is essentially not due to a performance issue from the model side, it's actually a problem

32:28.360 --> 32:30.040
with the inputs.

32:30.040 --> 32:35.280
So a failure in one of the systems or like typically it retains something you could see happening

32:35.280 --> 32:37.560
is a seasonality pattern, right?

32:37.560 --> 32:43.920
I mean, so basically your model was meant to function well for your inputs to be in a

32:43.920 --> 32:48.640
specific range and you have to make sure that it is still the same range, right?

32:48.640 --> 32:53.160
I mean, so actually monitoring the inputs and the outputs goes a very long way.

32:53.160 --> 32:57.960
It doesn't necessarily mean that you have to monitor things very closely, but you're

32:57.960 --> 33:03.240
essentially get a sense that the number of average number of add to cards you see on

33:03.240 --> 33:08.360
a specific day is still pretty close to what you would expect them and what it was when

33:08.360 --> 33:10.840
you actually trained your model.

33:10.840 --> 33:11.840
Right, right.

33:11.840 --> 33:16.200
The last thing is, I would say that one issue I've seen as well is not necessarily an

33:16.200 --> 33:22.560
issue, but data scientists tend to, I would say that not necessarily overfitting the

33:22.560 --> 33:26.520
way you would think about it, but like use too much data for the models.

33:26.520 --> 33:32.320
So something that we're actually requiring from all our data scientists now is that when

33:32.320 --> 33:40.440
they suggest a specific amount of data for retrain the models, we actually ask them to train

33:40.440 --> 33:45.160
the same model, exact same model with the lesser amount of data and they actually do that

33:45.160 --> 33:47.280
for several data points.

33:47.280 --> 33:54.240
And we actually build this curve of essentially CPU consumption versus accuracy of the model.

33:54.240 --> 33:58.960
And I actually turned out that in our case, many people were using, I would say four times

33:58.960 --> 34:02.120
too much data compared to what was actually needed.

34:02.120 --> 34:03.120
Oh wow.

34:03.120 --> 34:06.400
So essentially that means that you're using four times too much CPU, right?

34:06.400 --> 34:11.280
I mean, so you're sticking you to four times longer to train these models.

34:11.280 --> 34:15.840
So essentially, of course, it's better to use more data, but if you're going to increase

34:15.840 --> 34:20.640
your accuracy by just one percent by throwing four times as much data, it doesn't really

34:20.640 --> 34:21.640
make sense, right?

34:21.640 --> 34:22.640
That means four.

34:22.640 --> 34:27.600
So I'm definitely, I think that data scientists are not trained to think in terms of money

34:27.600 --> 34:28.600
optimization, right?

34:28.600 --> 34:29.600
That means four.

34:29.600 --> 34:35.480
So this is something that we've made sure now that everybody is actually aware of conscious

34:35.480 --> 34:39.600
of the amount of CPU they're using when they're training the models.

34:39.600 --> 34:43.880
And have you developed a set of rules of thumb?

34:43.880 --> 34:50.800
Is there a way to generalize that or is the right way for them to, do they always need

34:50.800 --> 34:57.160
to run the models with four different data points and understand where the kind of that

34:57.160 --> 35:01.760
utility curve and pick the right point on it?

35:01.760 --> 35:03.760
This is the way we're functioning right now, right?

35:03.760 --> 35:07.960
I mean, of course, for the future, I have some hope of coming up with a, I mean, it's

35:07.960 --> 35:09.960
a very iterative process, right?

35:09.960 --> 35:13.840
I mean, so the way we've been thinking about this data, I mean, as we ask people,

35:13.840 --> 35:18.760
to do things, people actually start creating their own scripts and their own tools to actually

35:18.760 --> 35:20.120
perform these tasks.

35:20.120 --> 35:25.560
So when something comes across as being easy to generalize, we try to make sure that this

35:25.560 --> 35:28.160
is also accessible to other team members.

35:28.160 --> 35:33.040
And so over time, we're actually building this data base of tools that everybody can use

35:33.040 --> 35:34.440
for their specific problems.

35:34.440 --> 35:38.800
And so we're moving towards automations, just like it's a very slow process because as

35:38.800 --> 35:43.000
you may guess, like we have very different types of models and not everything can be

35:43.000 --> 35:46.160
used for the models as well.

35:46.160 --> 35:54.320
Do you have some kind of tool or platform in place for deploying and managing the various

35:54.320 --> 36:00.880
models or the individual teams do that themselves for their own services?

36:00.880 --> 36:04.560
I guess, you know, part of the question is thinking about it, like what's happening

36:04.560 --> 36:10.920
on the dev side of things, folks are forming, you know, dev ops teams around microservices

36:10.920 --> 36:16.360
that, you know, have full lifecycle responsibilities for those services, are you doing similar things

36:16.360 --> 36:17.360
around models?

36:17.360 --> 36:20.000
Yes, so we're moving in that direction.

36:20.000 --> 36:25.400
So we're actually developing our own compute platform where essentially all models will

36:25.400 --> 36:26.400
be trained.

36:26.400 --> 36:30.440
And so that platform would actually be talking to the data like DRK, right?

36:30.440 --> 36:35.480
I mean, but again, it's a very slow process because you have to train people to use that

36:35.480 --> 36:41.200
new platform, there are some paradigms that are not necessarily very obvious to everybody.

36:41.200 --> 36:44.840
We try to make sure that, you know, like everybody gets to use their favorite language

36:44.840 --> 36:49.640
in that platform, but essentially we're also loading that compute platform with the tools

36:49.640 --> 36:50.880
that was mentioned before.

36:50.880 --> 36:53.560
So that everything is in one place.

36:53.560 --> 36:57.960
Everybody's aware of, you know, like what tools exist to make your life easier as a data

36:57.960 --> 37:00.840
scientist or a machine learning engineer?

37:00.840 --> 37:03.640
And is this a home-run platform or something that you're...

37:03.640 --> 37:04.640
Yes.

37:04.640 --> 37:12.160
I'm imagine when you talked about the monitoring, the inputs and outputs of the models that

37:12.160 --> 37:16.440
struck me as really interesting, and I imagine some platform that, you know, you would tie

37:16.440 --> 37:21.480
into a monitoring system that when you're, you know, as part of your documentation phase,

37:21.480 --> 37:27.840
you're able to describe the expected bounds of a given model.

37:27.840 --> 37:30.000
And then this thing is monitoring the inputs.

37:30.000 --> 37:34.600
And if it starts, if you start seeing inputs outside of the bound, this thing would shoot

37:34.600 --> 37:40.000
off, you know, red flags and start paging people, have you gotten there yet or is that part

37:40.000 --> 37:41.000
of what you're working for?

37:41.000 --> 37:42.000
Yes.

37:42.000 --> 37:43.000
Yes, absolutely.

37:43.000 --> 37:44.000
Absolutely.

37:44.000 --> 37:46.000
This is exactly what we're trying to do right now, right?

37:46.000 --> 37:47.000
I mean, so it's...

37:47.000 --> 37:52.080
For, I mean, the challenge with this specific is that when you have, like, you're like

37:52.080 --> 37:56.800
supervised models and like a numerical data, it's fairly easy to monitor the inputs, right?

37:56.800 --> 37:57.800
I mean, so...

37:57.800 --> 37:58.800
Right.

37:58.800 --> 38:02.800
I mean, so for some of the models, it's actually already in place, where essentially,

38:02.800 --> 38:03.800
we...

38:03.800 --> 38:25.800
Whenever an input goes outside of, like, minus two sigma plus two sigma boundaries, it's actually

38:25.800 --> 38:30.800
shooting an email to the person in charge of monitoring the bottle and they would actually

38:30.800 --> 38:34.040
know that, you know, something is potentially about to happen, right?

38:34.040 --> 38:38.320
I mean, so we're definitely geared towards this, like, I mean, one thing we definitely

38:38.320 --> 38:42.400
want to achieve in the near future is a model that allows you to understand that your model

38:42.400 --> 38:43.560
is expiring before...

38:43.560 --> 38:44.560
Before it's time, right?

38:44.560 --> 38:49.600
I mean, so right now, I think most companies are thinking of retraining models in terms

38:49.600 --> 38:51.400
of a regular sequence, right?

38:51.400 --> 38:54.600
I mean, basically, I retrain my model every other week.

38:54.600 --> 38:55.600
Right.

38:55.600 --> 39:00.200
So when you're in retail, there may be lots of happenings, there may be, like, holidays

39:00.200 --> 39:04.560
and sometimes you have to retrain things faster, unless you have something in place to let

39:04.560 --> 39:09.120
you know that the bottle is about to change or needs to be updated, you would actually

39:09.120 --> 39:12.960
learn that by your customer complaining about getting the wrong results or something that's

39:12.960 --> 39:15.080
not accurate or relevant to our searches, right?

39:15.080 --> 39:19.600
And so you don't want that to happen because it essentially involves that the customer needs

39:19.600 --> 39:24.040
to have a bad experience for you to be aware that something's wrong with your model.

39:24.040 --> 39:25.040
Right.

39:25.040 --> 39:29.760
So make sure that we can catch these problems early in the process before it actually impacts

39:29.760 --> 39:31.000
the customer.

39:31.000 --> 39:32.000
Mm-hmm.

39:32.000 --> 39:38.240
And so what are some of the methodologies that you use to identify these expiring models?

39:38.240 --> 39:44.520
Well, again, it's about like finding the right metric to actually assess the satisfaction

39:44.520 --> 39:45.520
of the customer, right?

39:45.520 --> 39:50.240
I mean, but I don't think there's like a one-true, only metric that works for all cases,

39:50.240 --> 39:53.680
but I mean, again, it's the mission of the measure team, right?

39:53.680 --> 39:54.680
Right.

39:54.680 --> 39:55.680
And measuring things as well.

39:55.680 --> 39:56.680
So another word with a model.

39:56.680 --> 39:57.680
Capturing the dissatisfaction.

39:57.680 --> 40:00.200
Sorry, sorry for cutting you off with just a paraphrase.

40:00.200 --> 40:04.240
In other words, the model is expiring when it stops performing if there's not some other

40:04.240 --> 40:05.480
dimensions to it.

40:05.480 --> 40:06.480
Yeah.

40:06.480 --> 40:07.480
Right.

40:07.480 --> 40:08.480
Right.

40:08.480 --> 40:09.480
Okay.

40:09.480 --> 40:10.480
Interesting.

40:10.480 --> 40:16.120
As part of this measure team, you also are chartered with specifically looking for weaknesses

40:16.120 --> 40:18.560
in other people's models.

40:18.560 --> 40:21.200
What does that look like and how do you approach that?

40:21.200 --> 40:29.960
And I guess I'm thinking of looking for corner cases or cases in the data that these teams

40:29.960 --> 40:35.760
might not have thought about that based on your experience, you could foresee causing

40:35.760 --> 40:37.840
poor model performance.

40:37.840 --> 40:40.640
How do you approach that part of the role?

40:40.640 --> 40:43.760
Now, there are definitely two components to it, right?

40:43.760 --> 40:47.040
I mean, so there's definitely weaknesses that you would see.

40:47.040 --> 40:53.800
And like a specific model that requires like a free-contrading or is extremely sensitive

40:53.800 --> 40:57.760
to seasonality would be something that we would like to look at and try to figure out

40:57.760 --> 40:59.520
like what is causing this, right?

40:59.520 --> 41:05.600
I mean, so the way we do that is essentially, we essentially keep track of, for example,

41:05.600 --> 41:09.480
the assume that your model is something like a logistic regression model because it's

41:09.480 --> 41:11.760
a easy to explain.

41:11.760 --> 41:17.720
So you would be able to see like what parameters are extremely stable over time and essentially

41:17.720 --> 41:19.840
don't change even when you will train the model.

41:19.840 --> 41:24.680
And which one of these parameters are actually extremely volatile and have a very big error

41:24.680 --> 41:25.680
to it, right?

41:25.680 --> 41:30.040
And so we would actually understand very with precision, but parameters are causing the

41:30.040 --> 41:31.640
model to underperform.

41:31.640 --> 41:36.720
So that's kind of like a reverse engineer, other people's model in order to understand

41:36.720 --> 41:37.880
what the weaknesses are.

41:37.880 --> 41:42.400
So that's one thing we do when we're trying to kind of automate.

41:42.400 --> 41:47.920
The other piece is something which is like something you have an inkling that requires

41:47.920 --> 41:48.920
to be updated, right?

41:48.920 --> 41:53.440
I mean, so an example of something we have tried to do recently is that we were trying

41:53.440 --> 41:57.680
to add the notion of geolocation to which he personalized the results depending on

41:57.680 --> 42:00.080
your location in the country, right?

42:00.080 --> 42:06.800
And so I mean, you know that this needs to be taken into account and you know that you're

42:06.800 --> 42:11.000
going to add that feature in the model, but the question is like, what is your base data

42:11.000 --> 42:13.760
set and your best bet to actually add that to the model, right?

42:13.760 --> 42:19.960
I mean, so this is why we have statistical analyst trying to assess the quality of the

42:19.960 --> 42:22.880
different data sets that we have available.

42:22.880 --> 42:27.320
So this is where our job actually gets interesting because we get to touch to lots of different

42:27.320 --> 42:29.080
data sets across the company, right?

42:29.080 --> 42:34.080
And try to understand what is the data source that we could use to actually improve these

42:34.080 --> 42:37.560
signals and make our search engine better.

42:37.560 --> 42:45.520
So do you have, is this maybe goes back to our platform discussion a moment ago, but

42:45.520 --> 42:54.360
is there a place that has a dashboard of all the models that are running in Walmart?

42:54.360 --> 42:58.920
I guess I'm wondering at the granularity at which you track this like, do you have a

42:58.920 --> 43:04.480
master view of all deployed models and their performance and you can do trend analysis

43:04.480 --> 43:11.000
across this and see, you know, where logistic regression, you know, types of models work

43:11.000 --> 43:17.080
versus other things or are these things managed more on a product by product basis?

43:17.080 --> 43:22.320
No, no, we're definitely geared towards like at least for search, I mean, we're definitely

43:22.320 --> 43:28.880
moving forward to a phase where we get to see a holistic view of all models in product

43:28.880 --> 43:29.880
model.

43:29.880 --> 43:30.880
At one time, right?

43:30.880 --> 43:35.760
I mean, so basically if lots of your models are using the same base model, like it's

43:35.760 --> 43:39.840
fairly easy to do, it gets more complicated if you have many different types of machine

43:39.840 --> 43:44.200
learning models in production, but we definitely believe that you should have a comprehensive

43:44.200 --> 43:45.840
view of everything.

43:45.840 --> 43:50.360
For the reason we mentioned earlier that you have some crosstalk happening across models,

43:50.360 --> 43:51.360
right?

43:51.360 --> 43:54.520
I mean, it's possible that the fact that one model is underperforming is caused by another

43:54.520 --> 43:59.880
one overperforming and so we believe that you cannot keep things segmented and just keep

43:59.880 --> 44:01.680
track of one product at a time.

44:01.680 --> 44:06.320
I mean, I really strongly believe that having a comprehensive view as much as possible

44:06.320 --> 44:08.360
is really important.

44:08.360 --> 44:12.960
Getting to the level where we have a comprehensive view of all the models across the company is

44:12.960 --> 44:21.080
going to be very challenging as you can imagine, but to what extent do you use machine learning

44:21.080 --> 44:23.680
models to manage these models?

44:23.680 --> 44:25.880
And then how do you do that?

44:25.880 --> 44:27.800
That's definitely what we want to do, right?

44:27.800 --> 44:33.600
I mean, I sometimes call my team like the team that creates machine learning models of

44:33.600 --> 44:34.760
machine learning models, right?

44:34.760 --> 44:40.800
And so essentially the way you would do that is essentially using the parameters of the

44:40.800 --> 44:43.960
other models as a feature for another model, right?

44:43.960 --> 44:47.320
I mean, so basically you're kind of, as you mentioned earlier, right?

44:47.320 --> 44:49.080
I mean, you want to manage your things over time.

44:49.080 --> 44:53.240
So essentially like a trend analysis would be something that could, you know, you could

44:53.240 --> 44:57.600
definitely use machine learning for this type of management.

44:57.600 --> 45:01.600
And are you doing this at all today or is it more directional?

45:01.600 --> 45:02.800
Getting started.

45:02.800 --> 45:03.800
Okay.

45:03.800 --> 45:04.800
Interesting.

45:04.800 --> 45:05.800
Interesting.

45:05.800 --> 45:10.480
Yeah, I can imagine if you have all of your model data, all of your parameter data,

45:10.480 --> 45:15.520
all of your performance data, you know, then part of what your measure team is able to

45:15.520 --> 45:20.920
do is someone brings you a model and some data and you can just run your meta model against

45:20.920 --> 45:23.640
it and predict whether their model is going to work or not.

45:23.640 --> 45:25.240
It sounds like a great application.

45:25.240 --> 45:26.240
Yeah.

45:26.240 --> 45:27.240
Awesome.

45:27.240 --> 45:30.640
Is there anything else that your team is focused on that we haven't talked about so far?

45:30.640 --> 45:33.200
Well, I mean, I've covered most of it.

45:33.200 --> 45:37.840
I mean, the one thing is like I mentioned this effort to actually bring the stores data

45:37.840 --> 45:39.720
together with the online data.

45:39.720 --> 45:42.400
So this is an effort we started pretty recently.

45:42.400 --> 45:45.000
One of the challenges we're trying to tackle is the following.

45:45.000 --> 45:49.640
So, uh, the search is actually an interesting problem because where, as you can imagine, we're

45:49.640 --> 45:53.960
using like lots of different data sources to rank the items we're showing to the customer,

45:53.960 --> 45:54.960
right?

45:54.960 --> 45:59.560
And so essentially we're using data related to the content of the items.

45:59.560 --> 46:04.040
So if somebody searches for TV Samsung, you want to show that you're like the right

46:04.040 --> 46:06.160
brand and the right product for sure.

46:06.160 --> 46:10.560
But then the question is among Samsung TVs, which one do you want to show first, right?

46:10.560 --> 46:14.960
And so the answer to that is you're showing the one that is the most popular.

46:14.960 --> 46:20.920
So we sometimes run into a problem because I think, for example, of like a smaller type

46:20.920 --> 46:23.080
of item that you would usually buy in the store, right?

46:23.080 --> 46:26.720
I mean, so sometimes people connect to walmart.com website.

46:26.720 --> 46:31.440
They enter a search and they actually decide to go buy that item in store.

46:31.440 --> 46:36.680
So the reason why they actually search for that item was to check the inventory in their

46:36.680 --> 46:38.800
local, local Walmart store, right?

46:38.800 --> 46:44.280
I mean, so for us as the search team, this is really a problem because if you see someone

46:44.280 --> 46:47.360
click on the item, but they eventually they don't purchase it.

46:47.360 --> 46:51.200
We take that as a bad sign that we didn't show the right item, right?

46:51.200 --> 46:55.680
And so essentially that would cause us to demote that item over time.

46:55.680 --> 47:00.560
And it's very possible that the item that we showed was actually the one that the customer

47:00.560 --> 47:01.560
meant to see, right?

47:01.560 --> 47:05.360
I mean, and so it is very possible that eventually they bought that.

47:05.360 --> 47:10.880
So closing the loop with that and actually like attributing a specific store purchase

47:10.880 --> 47:15.720
to a specific online search is something that we're trying to do now, right?

47:15.720 --> 47:18.960
I mean, so I think people have heard of the new Google attribution, right?

47:18.960 --> 47:23.400
And they actually get to track you when you shop in store as well as online.

47:23.400 --> 47:29.200
I mean, we're essentially trying to do that for essentially mapping the, like essentially

47:29.200 --> 47:33.440
mapping the gap between the stories and the online experience.

47:33.440 --> 47:34.440
Mm-hmm.

47:34.440 --> 47:39.320
And that that's what the data lake enables you to do by pulling all that information into

47:39.320 --> 47:42.120
the one place and allowing folks to build models across it.

47:42.120 --> 47:44.120
Yeah, definitely.

47:44.120 --> 47:45.120
Interesting.

47:45.120 --> 47:50.960
Anna, are you, to what extent are you using external data sources and building your search

47:50.960 --> 47:52.960
models?

47:52.960 --> 47:57.360
So we're, I mean, we do, you know, like I don't know that we're using like a lot of

47:57.360 --> 47:58.360
data sources.

47:58.360 --> 48:03.840
I mean, the external data sources we essentially use is to, I would say for monitoring purposes,

48:03.840 --> 48:04.840
right?

48:04.840 --> 48:08.680
I mean, so for example, we're trying to catch instances where we have a cold stock

48:08.680 --> 48:09.680
problem, right?

48:09.680 --> 48:13.440
I mean, so if something doesn't sell really well at Walmart, when you actually know this

48:13.440 --> 48:17.400
is a very popular item on the marketplace, you would try to do something about it.

48:17.400 --> 48:22.520
But we don't necessarily use that to create and build new models or essentially focusing

48:22.520 --> 48:24.480
on our own data at this point.

48:24.480 --> 48:25.480
Got it.

48:25.480 --> 48:26.480
Got it.

48:26.480 --> 48:27.480
All right.

48:27.480 --> 48:30.760
Well, this has been a really, really interesting conversation and I appreciate you taking

48:30.760 --> 48:33.720
the time out to chat with us about what you're up to.

48:33.720 --> 48:39.680
But I think folks can learn a ton about the machine learning lifecycle management challenge

48:39.680 --> 48:44.920
and, and, and learn a ton from the way you guys have taken it on at, at Walmart.

48:44.920 --> 48:47.560
I really appreciate you taking the time to join us.

48:47.560 --> 48:48.560
No worries.

48:48.560 --> 48:50.360
I mean, I always love talking about this topic.

48:50.360 --> 48:51.360
So my pleasure.

48:51.360 --> 48:52.360
Awesome.

48:52.360 --> 48:55.360
Thanks so much, Jennifer.

48:55.360 --> 48:59.320
All right, everyone.

48:59.320 --> 49:05.680
What's our show for today for the notes for this episode head on over to twimmaleye.com

49:05.680 --> 49:13.160
slash talk slash 46, whether this is your first or 50th show, I want to thank you so much

49:13.160 --> 49:14.560
for listening.

49:14.560 --> 49:16.120
I really want to hear from you.

49:16.120 --> 49:21.040
So please take a moment to comment on the show notes page or on Twitter with your feedback

49:21.040 --> 49:26.440
or questions or just what you found most interesting and useful about this episode.

49:26.440 --> 49:31.320
Also, if you share your favorite quote via a comment or social media, we'll send you

49:31.320 --> 49:34.360
one of our fab laptop stickers.

49:34.360 --> 49:39.200
Another thanks to this week's sponsor, Claudeira, for more information on their data science

49:39.200 --> 49:47.000
workbench or to schedule your demo and get a free drone, visit twimmaleye.com slash Claudeira.

49:47.000 --> 49:51.160
If you subscribe to my newsletter, you already know that I've got a busy month ahead as

49:51.160 --> 49:53.280
far as events go.

49:53.280 --> 49:58.840
The week of September 18th, I'll be in San Francisco for the O'Reilly Artificial Intelligence

49:58.840 --> 49:59.840
Conference.

49:59.840 --> 50:04.360
There's also a chance that on Saturday the 16th, I'll make it to the Scaling Deep Learning

50:04.360 --> 50:08.720
Conference NSF, which looks to be an interesting one.

50:08.720 --> 50:13.120
The following week, I'll be at Strange Loop, a great technical conference held each year

50:13.120 --> 50:15.440
right here in St. Louis.

50:15.440 --> 50:21.200
Now I love meeting up with listeners, so if you're planning to be at any of these events,

50:21.200 --> 50:26.240
please drop me a note via a comment, the contact form, or Twitter.

50:26.240 --> 50:29.920
For more info on any of these events, check out the show notes.

50:29.920 --> 50:59.880
Thanks again for listening and catch you next time.

