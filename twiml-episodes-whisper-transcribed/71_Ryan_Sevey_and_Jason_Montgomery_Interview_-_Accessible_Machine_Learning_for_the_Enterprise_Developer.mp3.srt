1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,120
I'm your host Sam Charrington.

4
00:00:23,120 --> 00:00:28,600
Thanks so much to those of you who participated in the Twimble Online Meetup last week, and

5
00:00:28,600 --> 00:00:31,680
it's Kevin T from Sigopt for presenting.

6
00:00:31,680 --> 00:00:36,040
You can find the slides for Kevin's presentation in the Meetup Slack channel as well as in

7
00:00:36,040 --> 00:00:37,840
this week's show notes.

8
00:00:37,840 --> 00:00:42,400
Our final Meetup of the Year will be held on Wednesday, December 13th.

9
00:00:42,400 --> 00:00:48,160
Bring your thoughts on the top machine learning and AI stories of 2017 for our discussion segment.

10
00:00:48,160 --> 00:00:53,560
For our main presentation, prior Twimble Talk guest Bruno GonzÃ¡lez will be discussing

11
00:00:53,560 --> 00:00:59,160
the paper, Understanding Deep Learning requires rethinking generalization.

12
00:00:59,160 --> 00:01:03,720
By Xiu Juan Zhang from MIT and Google Brain and others.

13
00:01:03,720 --> 00:01:09,440
You can find more details and register at twimlai.com slash Meetup.

14
00:01:09,440 --> 00:01:13,600
If you receive my newsletter, you already know this, but Twimble is growing and we're

15
00:01:13,600 --> 00:01:19,440
looking for an energetic and passionate community manager to help expand our programs.

16
00:01:19,440 --> 00:01:23,760
This full-time position can be remote, but if you happen to live in St. Louis, all the

17
00:01:23,760 --> 00:01:24,760
better.

18
00:01:24,760 --> 00:01:28,120
If you're interested, please reach out to me for additional details.

19
00:01:28,120 --> 00:01:32,040
I should mention that if you don't already get my newsletter, you are really missing

20
00:01:32,040 --> 00:01:37,120
out and should visit twimlai.com slash newsletter to sign up.

21
00:01:37,120 --> 00:01:42,000
This week we'll be featuring a series of shows recorded from Strange Loop, a great developer

22
00:01:42,000 --> 00:01:46,680
focus conference that takes place every year right in my backyard.

23
00:01:46,680 --> 00:01:49,880
Not literally in my backyard, but here in St. Louis.

24
00:01:49,880 --> 00:01:54,520
The conference is a multidisciplinary melting pot of developers and thinkers across a variety

25
00:01:54,520 --> 00:01:58,960
of fields and we're happy to be able to bring a bit of it to those of you who couldn't

26
00:01:58,960 --> 00:02:00,680
make it in person.

27
00:02:00,680 --> 00:02:06,000
Later this week, you'll hear from Sumit Shintala, a research engineer at Facebook AI Research

28
00:02:06,000 --> 00:02:12,680
Lab, Matt Taylor, open source community manager at Nementa, Alison Parrish, professor in the

29
00:02:12,680 --> 00:02:19,480
interactive telecommunications program at NYU and Sam Richie, software engineer at Stripe.

30
00:02:19,480 --> 00:02:25,360
We'd like to send a huge shout out to Nexosis, who helped make this series possible.

31
00:02:25,360 --> 00:02:30,000
Nexosis is a company focused on making machine learning more easily accessible to enterprise

32
00:02:30,000 --> 00:02:31,000
developers.

33
00:02:31,000 --> 00:02:34,800
In fact, you'll learn a bunch more about the company and what they're up to in this

34
00:02:34,800 --> 00:02:40,960
show, which features my interview with Nexosis Founders, Ryan C.V. and Jason Montgomery.

35
00:02:40,960 --> 00:02:45,920
Ryan and Jason and I discuss how they got their start by applying ML to identifying cheaters

36
00:02:45,920 --> 00:02:51,360
in video games, the application of machine learning for time series data analysis, and

37
00:02:51,360 --> 00:02:54,520
of course, the Nexosis machine learning API.

38
00:02:54,520 --> 00:02:59,880
If you like what you hear, then invite you to get your free Nexosis API key and discover

39
00:02:59,880 --> 00:03:02,640
what they can bring to your next project.

40
00:03:02,640 --> 00:03:11,480
You can do that at nexosis.com slash twimmel, that's n-e-x-o-s-i-s dot com slash twimmel.

41
00:03:11,480 --> 00:03:14,480
And now on to the show.

42
00:03:14,480 --> 00:03:25,600
Hey everyone, I am on the line with Ryan C.V. and Jason Montgomery.

43
00:03:25,600 --> 00:03:33,720
Ryan is the CEO and co-founder of Nexosis and Jason is the C.T.O. and also a co-founder.

44
00:03:33,720 --> 00:03:36,120
Welcome to this week in machine learning and AI, guys.

45
00:03:36,120 --> 00:03:37,120
Thanks for having us.

46
00:03:37,120 --> 00:03:38,120
Yeah, awesome to be here.

47
00:03:38,120 --> 00:03:40,160
Thanks so much for hosting us.

48
00:03:40,160 --> 00:03:41,160
Fantastic.

49
00:03:41,160 --> 00:03:42,160
Fantastic.

50
00:03:42,160 --> 00:03:46,760
So, as is the tradition here on the show, why don't we get started by having each of

51
00:03:46,760 --> 00:03:51,960
you introduce yourselves and tell us a little bit about your journey to machine learning

52
00:03:51,960 --> 00:03:52,960
and AI?

53
00:03:52,960 --> 00:03:59,560
Yeah, sure, so happy to, as you mentioned, I'm the CEO and co-founder of Nexosis, Jason

54
00:03:59,560 --> 00:04:05,040
here is our C.T.O. and kind of just going back to maybe the beginning of how all this

55
00:04:05,040 --> 00:04:06,040
came to be.

56
00:04:06,040 --> 00:04:11,040
Jason and I actually met each other at a company called American Electric Power, which is headquartered

57
00:04:11,040 --> 00:04:12,240
here in Kamba, Ohio.

58
00:04:12,240 --> 00:04:17,160
Him and I were both on the cybersecurity engineering team.

59
00:04:17,160 --> 00:04:24,640
And AP, I believe is the largest generator of electricity in the United States.

60
00:04:24,640 --> 00:04:27,880
So when you think about that, you can think about all the different assets that they have

61
00:04:27,880 --> 00:04:34,240
to put in the field, which kind of directly correlates to how much data is being collected.

62
00:04:34,240 --> 00:04:39,560
And basically our task bear was to ensure that all their assets were secure from both

63
00:04:39,560 --> 00:04:42,400
internal and external attackers.

64
00:04:42,400 --> 00:04:47,640
We quickly realized that the amount of data that was being generated would be pretty much

65
00:04:47,640 --> 00:04:53,000
impossible for the analyst team to go through and identify any kind of anomalies things of

66
00:04:53,000 --> 00:04:54,320
that nature.

67
00:04:54,320 --> 00:04:58,200
So Jason and I then started thinking and hearing more about machine learning and there were

68
00:04:58,200 --> 00:05:02,400
classes offered online, I believe through Stanford University.

69
00:05:02,400 --> 00:05:05,640
And him and I both signed up to do these online courses.

70
00:05:05,640 --> 00:05:10,800
And we went and did all the homework, did all the, you know, kind of bonus material, if

71
00:05:10,800 --> 00:05:14,280
you will, and we got to a point where the instructor said, okay, you now know more than

72
00:05:14,280 --> 00:05:15,520
everyone in the valley.

73
00:05:15,520 --> 00:05:19,680
And we're like, all right, that's a good stopping point.

74
00:05:19,680 --> 00:05:26,520
So it was becoming very, very, just becoming very mathematical and that's not necessarily

75
00:05:26,520 --> 00:05:27,520
that thing.

76
00:05:27,520 --> 00:05:33,400
But, you know, we are both very technical individuals, lots of development experience, things of that

77
00:05:33,400 --> 00:05:37,320
in our background and just quite frankly, like learning six players of mathematics isn't

78
00:05:37,320 --> 00:05:38,320
that appealing.

79
00:05:38,320 --> 00:05:43,720
But it's not, you know, like cool, I guess it's neat to understand the math behind this

80
00:05:43,720 --> 00:05:47,160
one algorithm, but quite frankly, that wasn't where we're taking it, right?

81
00:05:47,160 --> 00:05:49,840
We were, we were taking a class early, I understand how we could use machine learning

82
00:05:49,840 --> 00:05:52,240
to solve a very particular problem.

83
00:05:52,240 --> 00:05:58,600
So out of that, we just kind of long story short, Jason ended up leaving American Electric

84
00:05:58,600 --> 00:05:59,600
Power.

85
00:05:59,600 --> 00:06:00,600
He went to varicode.

86
00:06:00,600 --> 00:06:05,000
He was a dot net, principal dot net researcher for them, varicode is an information security

87
00:06:05,000 --> 00:06:08,680
company that does software.

88
00:06:08,680 --> 00:06:12,960
Yeah, we do a binary static analysis, you submit your binaries and we find flaws in the

89
00:06:12,960 --> 00:06:17,720
code and give you sort of a report that says, you need to fix code in these places or

90
00:06:17,720 --> 00:06:18,720
what not.

91
00:06:18,720 --> 00:06:22,480
So I did, I was a researcher there for about three years.

92
00:06:22,480 --> 00:06:27,920
Reason we bring that up is when he left AP to go to varicode, I shortly thereafter left

93
00:06:27,920 --> 00:06:34,000
AP to go to healer Packard, but Jason, I kept doing joint research projects together

94
00:06:34,000 --> 00:06:38,040
just mostly out of fun, right, in our spare free time.

95
00:06:38,040 --> 00:06:42,200
So that means that we lost access to the AP data, which was, that's fine.

96
00:06:42,200 --> 00:06:45,880
And we were kind of still enthralled with this notion of what machine learning could

97
00:06:45,880 --> 00:06:46,880
do.

98
00:06:46,880 --> 00:06:49,080
And we started thinking about other use cases.

99
00:06:49,080 --> 00:06:53,760
And I forget exactly how it happened, but I asked Jason one day why he didn't enjoy

100
00:06:53,760 --> 00:07:00,040
online gaming and his response was basically, there's a lot of cheaters in it and I want

101
00:07:00,040 --> 00:07:04,240
to know if I'm getting, you know, under nominated by someone that video gave that it's because

102
00:07:04,240 --> 00:07:07,240
they're actually better than me and not because they're using cheats.

103
00:07:07,240 --> 00:07:11,080
So I thought, well, that's kind of an interesting feedback.

104
00:07:11,080 --> 00:07:16,080
I said, I wonder if we could use machine learning to identify patterns and, you know, public

105
00:07:16,080 --> 00:07:20,880
data sets available via the steam API that would identify a cheater.

106
00:07:20,880 --> 00:07:25,120
And I think really the whole notion back then was if you're cheating, you're trying

107
00:07:25,120 --> 00:07:31,560
to do something that a normal human wouldn't do, so it should show up in the stats, right?

108
00:07:31,560 --> 00:07:37,400
Like if you see this ridiculously high, like headshot percentage, you're probably cheating

109
00:07:37,400 --> 00:07:39,240
unless you're a professional, right?

110
00:07:39,240 --> 00:07:43,800
Or into the stats show, like for the last two years, you have like an accuracy rating

111
00:07:43,800 --> 00:07:47,680
of like 15%, and then suddenly overnight your accuracy jumps to 50%.

112
00:07:47,680 --> 00:07:50,280
Well, that's suspicious, right?

113
00:07:50,280 --> 00:07:55,480
So steam makes it pretty easy to get all that data that we needed via their API.

114
00:07:55,480 --> 00:08:01,160
So we spent about six months and we created a proof of concept that basically had a pretty

115
00:08:01,160 --> 00:08:02,160
good accuracy rating.

116
00:08:02,160 --> 00:08:08,200
It was like 88% accurate on identifying whether or not a player should be banned by

117
00:08:08,200 --> 00:08:09,400
valving a Thai cheat.

118
00:08:09,400 --> 00:08:15,000
Well, let me, if I can interrupt and ask a question or a couple of questions, I'm assuming

119
00:08:15,000 --> 00:08:21,120
then that you were using supervised learning for this task, and if that's the case, what

120
00:08:21,120 --> 00:08:22,600
did you use for labels?

121
00:08:22,600 --> 00:08:27,600
Did you manually label some number of users that you thought were cheaters?

122
00:08:27,600 --> 00:08:33,600
And then how did you validate this to determine your 88% accuracy rate did?

123
00:08:33,600 --> 00:08:38,800
Did steam also publish, you know, whether they thought folks were cheating or did you just

124
00:08:38,800 --> 00:08:41,280
look and see if they were eventually banned or something like that?

125
00:08:41,280 --> 00:08:42,480
Yeah, that's a good question.

126
00:08:42,480 --> 00:08:47,720
So what we did was we took the professional gaming league websites.

127
00:08:47,720 --> 00:08:53,080
We took data from there and our theory behind using them to train not cheaters was that

128
00:08:53,080 --> 00:08:57,480
people tend to be watching the professional gamers more, there's more eyes on them.

129
00:08:57,480 --> 00:08:59,520
So cheating would be more obvious.

130
00:08:59,520 --> 00:09:02,320
And then we went to the, there was a website called Backband.

131
00:09:02,320 --> 00:09:06,600
Its whole purpose was to take steam IDs and categorize those that got banned based on

132
00:09:06,600 --> 00:09:07,920
certain dates.

133
00:09:07,920 --> 00:09:12,600
We were able to correlate those with the steam API and pull down their game stats.

134
00:09:12,600 --> 00:09:14,000
We did some cleaning of the data.

135
00:09:14,000 --> 00:09:18,720
We looked at, you know, when they purchased CSGO if they had the game because we, the data

136
00:09:18,720 --> 00:09:22,080
set wasn't perfect like all data has lots of crap in it.

137
00:09:22,080 --> 00:09:28,040
But we spent a lot of time sort of analyzing it and making sure we had a pretty, pretty

138
00:09:28,040 --> 00:09:32,880
confident set of cheaters and a pretty confident set of those we felt weren't cheating.

139
00:09:32,880 --> 00:09:34,720
And that was sort of the process we went.

140
00:09:34,720 --> 00:09:38,400
Awesome. And so what do you, what do you do with this proof of concept?

141
00:09:38,400 --> 00:09:39,400
Yeah.

142
00:09:39,400 --> 00:09:44,480
So at this point in time, we released the research to the public at a security conference

143
00:09:44,480 --> 00:09:48,240
called Derby Con and I think 2014.

144
00:09:48,240 --> 00:09:52,240
And there were some Reddit posts and it got a lot of traction actually valve reach out

145
00:09:52,240 --> 00:09:57,400
to me about the research and kind of was asking how we did it and whatnot.

146
00:09:57,400 --> 00:10:01,520
And I told them because at the time, that was like, well, fix the issue that be more

147
00:10:01,520 --> 00:10:05,120
than enough for us.

148
00:10:05,120 --> 00:10:12,160
So that kind of naturally led to the formation of Nexosis because as we were going through

149
00:10:12,160 --> 00:10:19,840
the model building, we tried using different services like Microsoft's ML Studio, we tried

150
00:10:19,840 --> 00:10:24,000
using Amazon's API for machine learning.

151
00:10:24,000 --> 00:10:27,000
We tried using Google's product API.

152
00:10:27,000 --> 00:10:32,960
And at the time, that toe was still a company which became Tory, which then sold the Apple.

153
00:10:32,960 --> 00:10:37,160
So we tried all these different things on the market and the conclusion that we reached

154
00:10:37,160 --> 00:10:40,720
from all this was nothing's really out there for the developer.

155
00:10:40,720 --> 00:10:45,240
Everything is really catered and aimed at the data scientists.

156
00:10:45,240 --> 00:10:50,920
You so often understand how to train and build a model and then you just get into this

157
00:10:50,920 --> 00:10:53,600
question of how do I make the model better.

158
00:10:53,600 --> 00:10:57,160
And like fundamentally, you have to know the algorithm you want to use and you have to

159
00:10:57,160 --> 00:11:01,800
know why you would want to use algorithm A over algorithm B. So there's just lots of issues

160
00:11:01,800 --> 00:11:06,800
that from a developer point of view, i.e., we want to just get something out there that's

161
00:11:06,800 --> 00:11:11,320
doing a good job, a very, very painful experience.

162
00:11:11,320 --> 00:11:15,240
So we then set out with that knowledge and tact and we said, look, why don't we look at

163
00:11:15,240 --> 00:11:21,080
machine learning under the lens of a developer and how would a developer want to consume machine

164
00:11:21,080 --> 00:11:22,800
learning.

165
00:11:22,800 --> 00:11:27,400
And this has been done in other industries for a while, right?

166
00:11:27,400 --> 00:11:32,800
So the one that I can point to just off off my head is Twilio.

167
00:11:32,800 --> 00:11:38,200
And you think about what Twilio did, kind of very similar when you look at the data science

168
00:11:38,200 --> 00:11:40,000
machine learning landscape.

169
00:11:40,000 --> 00:11:44,560
Prior to Twilio, if you wanted to create a communications application, you would have

170
00:11:44,560 --> 00:11:51,200
to really have someone on staff that understood the telcal infrastructure, understood voice

171
00:11:51,200 --> 00:11:55,200
over IP protocols and how to configure it.

172
00:11:55,200 --> 00:11:58,640
We actually have a slide here internally that kind of compares, here's what they'll look

173
00:11:58,640 --> 00:12:01,160
like before Twilio and here's what they'll look like after.

174
00:12:01,160 --> 00:12:04,920
I think the world today looks very similar on the data science point of view, right?

175
00:12:04,920 --> 00:12:11,000
You have to know all the different just how do you do VTL, how do you deal with missing

176
00:12:11,000 --> 00:12:12,000
variables?

177
00:12:12,000 --> 00:12:13,240
There's just so much to it.

178
00:12:13,240 --> 00:12:18,000
And then again, how do you know what algorithm you want to use and when blah, blah, blah,

179
00:12:18,000 --> 00:12:19,000
right?

180
00:12:19,000 --> 00:12:20,600
It becomes very complicated.

181
00:12:20,600 --> 00:12:25,440
So the whole notion of exosys is saying, look, developers, you use the same language that

182
00:12:25,440 --> 00:12:26,520
you're used to using.

183
00:12:26,520 --> 00:12:32,360
If you're a.NET developer, come to us, you know,.NET, C-Sharve, if you're a Java developer,

184
00:12:32,360 --> 00:12:33,360
then go use Java.

185
00:12:33,360 --> 00:12:38,280
You don't have to learn a new language to incorporate machine learning to your project.

186
00:12:38,280 --> 00:12:41,440
And that's really the whole basis of exosys.

187
00:12:41,440 --> 00:12:42,440
Okay.

188
00:12:42,440 --> 00:12:49,040
When I think about what, let's take Azure ML for instance, when I think about what they're

189
00:12:49,040 --> 00:12:50,040
trying to do.

190
00:12:50,040 --> 00:12:53,000
It sounds exactly like what you're describing, right?

191
00:12:53,000 --> 00:12:59,200
They're trying to create an API that lets a developer deliver, you know, machine learning

192
00:12:59,200 --> 00:13:00,600
types of applications.

193
00:13:00,600 --> 00:13:05,920
You know, they've got the studio, you can even do it drag and drop if you're so interested.

194
00:13:05,920 --> 00:13:10,680
If you're interested in doing so, maybe we can dig a little deeper and you can, when

195
00:13:10,680 --> 00:13:16,000
you're faced with skeptics that say, you know, how can you enable a developer to do machine

196
00:13:16,000 --> 00:13:20,360
learning without knowing anything about data science, like how do you address that?

197
00:13:20,360 --> 00:13:21,960
It's a good question.

198
00:13:21,960 --> 00:13:27,680
When we looked at ML Studio, for instance, a lot of the work, as you know, is data preparation,

199
00:13:27,680 --> 00:13:30,760
data cleaning, ETL, it's like 80% of the work.

200
00:13:30,760 --> 00:13:35,560
And so, and then they're scaling, there's how do you do imputation strategies, how do

201
00:13:35,560 --> 00:13:39,240
you aggregate data, all these sort of questions.

202
00:13:39,240 --> 00:13:42,880
And developers are very used to using working with data and data types.

203
00:13:42,880 --> 00:13:47,200
And you can typically take a data type, identify, and you kind of know what step you need to

204
00:13:47,200 --> 00:13:50,040
do to impute or aggregate at that point.

205
00:13:50,040 --> 00:13:55,480
And so, giving the developer the tools in their hands to sort of define maybe the data types

206
00:13:55,480 --> 00:13:59,600
with metadata, but not really having to worry about what needs to happen before it

207
00:13:59,600 --> 00:14:03,200
can go into the algorithm, before we convert it all into numbers, whether we need to one

208
00:14:03,200 --> 00:14:08,120
hot and code, categorical data, things like that, you know, how do we pick features.

209
00:14:08,120 --> 00:14:11,880
All of that stuff is, there's a lot of automation that can be done to simplify that.

210
00:14:11,880 --> 00:14:15,680
Now, you still have to know your data, you have to know a good question you want to ask,

211
00:14:15,680 --> 00:14:19,640
you have to validate that, you know, you are submitting features that make sense.

212
00:14:19,640 --> 00:14:23,280
We're not going to know your data for you, but in a lot of ways, we can automate some

213
00:14:23,280 --> 00:14:28,760
of that heavy lifting at scale, and then we can build lots and lots of models, looking

214
00:14:28,760 --> 00:14:32,320
at different combinations of those things, and then sort of finding what falls out of

215
00:14:32,320 --> 00:14:33,320
that.

216
00:14:33,320 --> 00:14:36,840
And just to add to it, the other thing with all the different platforms is that you

217
00:14:36,840 --> 00:14:41,840
also have to know how to go in and manually tune the models to make them better, right?

218
00:14:41,840 --> 00:14:46,840
So if you're in Azure Studio, and I don't know, you pick an algorithm, which I think

219
00:14:46,840 --> 00:14:48,600
is the first hurdle, right?

220
00:14:48,600 --> 00:14:54,000
So your developer, you're in Azure, and now you have, you know, 15 different algorithms

221
00:14:54,000 --> 00:14:55,400
underneath regression.

222
00:14:55,400 --> 00:14:57,760
So which one are you going to pick, right?

223
00:14:57,760 --> 00:15:00,320
Like your developer, you're sitting there, you're seeing 15 algorithms, how do you know

224
00:15:00,320 --> 00:15:01,880
which one to use?

225
00:15:01,880 --> 00:15:07,360
And how do you then go about making that model better, right, be it tuning, or maybe

226
00:15:07,360 --> 00:15:10,520
you have new features, with Nexus, we do all that for you.

227
00:15:10,520 --> 00:15:18,840
So we have probably close to like 150 different algorithms at this point that are in the platform.

228
00:15:18,840 --> 00:15:24,720
And basically just kind of high level how it works is we hold these tournaments and the

229
00:15:24,720 --> 00:15:29,760
algorithm that's performing the best based on scoring metrics is the one that's used.

230
00:15:29,760 --> 00:15:34,080
So you as a developer don't even have to come to us and say, hey, I'm not, like I'm going

231
00:15:34,080 --> 00:15:38,760
to use, who's the decision trees, you don't even have to know that you just have to say,

232
00:15:38,760 --> 00:15:43,320
hey, here's the problem that I have, here's what I want to solve for you guys build all

233
00:15:43,320 --> 00:15:47,080
the models and then you guys tell me they want us doing the best, right?

234
00:15:47,080 --> 00:15:50,400
They're sort of like a universal pipeline we use, depending on the type of that problem

235
00:15:50,400 --> 00:15:51,400
you want to solve.

236
00:15:51,400 --> 00:15:54,840
And it sort of makes a lot of those decisions based on metadata that they provide as

237
00:15:54,840 --> 00:15:55,840
well.

238
00:15:55,840 --> 00:16:00,120
Are there specific types of problems that that this works for?

239
00:16:00,120 --> 00:16:05,200
It sounds a little bit too good to be true to be kind of applicable to everything or anything.

240
00:16:05,200 --> 00:16:09,680
Like I can give it any kind of data and it could do any kind of algorithm.

241
00:16:09,680 --> 00:16:15,200
Yeah, so right now we're not going super deep into what is traditionally viewed as the

242
00:16:15,200 --> 00:16:20,360
deep learning space, which that's pretty well solved, right?

243
00:16:20,360 --> 00:16:26,200
Like if you want to go predict, is this a hot dog or not, like, there's something up there

244
00:16:26,200 --> 00:16:28,080
or do that for you already.

245
00:16:28,080 --> 00:16:30,720
So looking at Valley reference, right?

246
00:16:30,720 --> 00:16:35,400
And like so we're not really super focused on deep learning, we are starting to incorporate

247
00:16:35,400 --> 00:16:40,400
more things around voice and speech, so you know, P and not exactly sure how deep

248
00:16:40,400 --> 00:16:46,480
I'm going in that vein, but we're really more focused on the true machine learning and

249
00:16:46,480 --> 00:16:49,240
kind of the layer that's above deep learning.

250
00:16:49,240 --> 00:16:54,920
So you know, we're not going to probably release any time soon, kind of like an image recognition

251
00:16:54,920 --> 00:16:56,720
element into our API.

252
00:16:56,720 --> 00:17:01,120
Again, I think that's been solved and it's been solved pretty well, but where you don't

253
00:17:01,120 --> 00:17:07,560
see a lot of stuff is underneath like regression classification, clustering, and again, kind

254
00:17:07,560 --> 00:17:11,440
of all the real machine learning types of elements.

255
00:17:11,440 --> 00:17:17,360
So today we launched the API doing time series, which I think is also a little bit unique.

256
00:17:17,360 --> 00:17:23,560
We don't see many platforms out there that have true time series capabilities in it.

257
00:17:23,560 --> 00:17:29,120
And time series is just naturally kind of very hard thing to create models in and really

258
00:17:29,120 --> 00:17:31,000
apply machine learning to them.

259
00:17:31,000 --> 00:17:35,040
So we launched with any kind of time series problems that today, if you have a time series

260
00:17:35,040 --> 00:17:38,920
like question like demand forecasting, API, we do that.

261
00:17:38,920 --> 00:17:44,560
We just release regression, so any type of regression problem can be solved with the API.

262
00:17:44,560 --> 00:17:48,280
And later this quarter will be releasing the classification endpoint, and then we'll

263
00:17:48,280 --> 00:17:53,480
just continue going down that vein of machine learning if that makes sense.

264
00:17:53,480 --> 00:17:54,480
Okay.

265
00:17:54,480 --> 00:18:01,000
And now it sounds like, you know, when I think about the the tournaments that you described

266
00:18:01,000 --> 00:18:05,880
as kind of conceptually happening on the back end, you know, it strikes me that for a

267
00:18:05,880 --> 00:18:12,480
given problem, you know, let's say I've got a bunch of time series data and I am trying

268
00:18:12,480 --> 00:18:17,240
to do a predictive maintenance type of application.

269
00:18:17,240 --> 00:18:19,240
Is that the kind of thing that you might do?

270
00:18:19,240 --> 00:18:20,240
Yeah.

271
00:18:20,240 --> 00:18:26,920
That's one of the most just because of where we were and we did the XR's retail accelerator,

272
00:18:26,920 --> 00:18:32,240
a lot of the early use case with the API are more about, hey, I have this store is in

273
00:18:32,240 --> 00:18:38,480
Columbus, Ohio, and I need to know of the 100,000 products that I have, how many I need

274
00:18:38,480 --> 00:18:43,120
to have on the shelf for next week, right, so when I place my reorder, how many new products

275
00:18:43,120 --> 00:18:45,320
I need to have in my warehouse.

276
00:18:45,320 --> 00:18:49,160
So more than the main forecast and type of element, but yeah, predictive maintenance would

277
00:18:49,160 --> 00:18:50,160
work as well.

278
00:18:50,160 --> 00:18:51,160
Okay.

279
00:18:51,160 --> 00:18:58,560
And so the time series in that that lot of cases in the retail case is transactions, transaction

280
00:18:58,560 --> 00:18:59,560
history.

281
00:18:59,560 --> 00:19:00,560
Right.

282
00:19:00,560 --> 00:19:01,560
Yep.

283
00:19:01,560 --> 00:19:02,560
Okay.

284
00:19:02,560 --> 00:19:08,040
And either of these cases, like I'm thinking about the, you know, you've got some set

285
00:19:08,040 --> 00:19:16,720
of models then that you are training the, that you're training against this data.

286
00:19:16,720 --> 00:19:21,000
So there's kind of a fan out there, but and then for each of these models, you've also

287
00:19:21,000 --> 00:19:25,680
got to, you know, do the hyper parameter tuning and all that.

288
00:19:25,680 --> 00:19:27,240
So there, there's a fan out there.

289
00:19:27,240 --> 00:19:33,320
It strikes me that I guess I'm trying to put my hands on like the, the scale aspects of

290
00:19:33,320 --> 00:19:34,720
the problem.

291
00:19:34,720 --> 00:19:39,720
It seems like for any given individual problem, you end up doing a ton of different training

292
00:19:39,720 --> 00:19:40,720
runs.

293
00:19:40,720 --> 00:19:44,640
And I'm wondering, you know, if there's some way you can kind of characterize or help me

294
00:19:44,640 --> 00:19:50,880
understand the, you know, the, the way that that looks from an underlying resource perspective.

295
00:19:50,880 --> 00:19:51,880
Yeah.

296
00:19:51,880 --> 00:19:55,760
We have a very dynamic workflow engine that starts, it's cute driven.

297
00:19:55,760 --> 00:20:00,400
So we can scale in and out of number of CPUs we want to use.

298
00:20:00,400 --> 00:20:03,840
So it, it scales up and down automatically based on demand.

299
00:20:03,840 --> 00:20:08,280
So it's, we built, we spent a lot of time building automation to sort of handle that.

300
00:20:08,280 --> 00:20:11,920
So we're not, you know, we have the cloud to work with so we can do scale sets.

301
00:20:11,920 --> 00:20:15,560
We can do a lot of different things in parallel.

302
00:20:15,560 --> 00:20:20,760
So you know, we could build a hundred or a thousand models all simultaneously and compare

303
00:20:20,760 --> 00:20:25,120
different results, try different hyper parameters, try different feature combinations and things

304
00:20:25,120 --> 00:20:26,120
like that.

305
00:20:26,120 --> 00:20:29,920
And then once we get sort of down to the, to a solution that works well that they're

306
00:20:29,920 --> 00:20:35,280
happy with, they can sort of tune more parameters around that or go with that model and use it

307
00:20:35,280 --> 00:20:36,280
to predict.

308
00:20:36,280 --> 00:20:37,280
Okay.

309
00:20:37,280 --> 00:20:42,840
Is there any particular method that you're using to do the hyper parameter optimization?

310
00:20:42,840 --> 00:20:47,080
I would have to ask our data science team to get into details of that.

311
00:20:47,080 --> 00:20:52,720
We do not have PhDs and, you know, 10 years of experience in the industry.

312
00:20:52,720 --> 00:20:55,920
While we do have some understanding of machine learning, we thought it important to hire

313
00:20:55,920 --> 00:21:00,000
a research team to solve some of the more complicated issues that we're not qualified to

314
00:21:00,000 --> 00:21:01,000
solve.

315
00:21:01,000 --> 00:21:06,120
And so in just as we, he means him and I, yes, we do have PhDs at Nexosis.

316
00:21:06,120 --> 00:21:07,120
Yes, we do.

317
00:21:07,120 --> 00:21:08,120
Right.

318
00:21:08,120 --> 00:21:09,120
Yeah.

319
00:21:09,120 --> 00:21:10,120
Got it.

320
00:21:10,120 --> 00:21:11,120
Got it.

321
00:21:11,120 --> 00:21:15,880
And so when we talk about the kind of the time series, I guess it kind of makes sense

322
00:21:15,880 --> 00:21:22,240
that you, that that was a, uh, uh, initial place to start in that that was a little bit

323
00:21:22,240 --> 00:21:27,040
of the kind of data problem that you ran into it at the power company, is that right?

324
00:21:27,040 --> 00:21:28,040
Yep.

325
00:21:28,040 --> 00:21:33,200
Can you talk it all to kind of a unique, you know, anything unique challenges or things

326
00:21:33,200 --> 00:21:36,080
that you do with regards to time series?

327
00:21:36,080 --> 00:21:42,040
One thing that I don't think people think about is using the product skew example that

328
00:21:42,040 --> 00:21:48,200
we were discussing earlier, if you have a hundred thousand skews, that means ideally you

329
00:21:48,200 --> 00:21:52,480
have a hundred thousand models, right, so every skew has its own model.

330
00:21:52,480 --> 00:21:58,200
And the real power of what we're doing is that each individual skew could have a completely

331
00:21:58,200 --> 00:22:00,880
different type of algorithm winning, right?

332
00:22:00,880 --> 00:22:06,560
So if you're selling snow shovels has one skew item, a different type of fundamental

333
00:22:06,560 --> 00:22:09,960
algorithm might be winning based on location too, right?

334
00:22:09,960 --> 00:22:15,040
So if you have a store in Columbus, Ohio, then you have a store in like Atlanta, Georgia,

335
00:22:15,040 --> 00:22:20,160
very, very different results, even though that's theoretically the exact same item, right?

336
00:22:20,160 --> 00:22:25,200
So that's really the other powers that we're able to take into account, okay, you're selling

337
00:22:25,200 --> 00:22:30,000
a snow shovel in Columbus, Ohio, and it's probably going to be snowier here, so you're going

338
00:22:30,000 --> 00:22:31,000
to sell a lot more.

339
00:22:31,000 --> 00:22:32,000
Right.

340
00:22:32,000 --> 00:22:35,000
And again, just a different algorithm might be winning here on Columbus, even though it's

341
00:22:35,000 --> 00:22:39,640
the same exact item, and then just thinking that even farther out when you think about

342
00:22:39,640 --> 00:22:44,560
things like bottled water as an example, you know, an algorithm that's going to predict

343
00:22:44,560 --> 00:22:48,120
how much water you're going to sell might look a lot different than an algorithm that's

344
00:22:48,120 --> 00:22:52,440
going to predict, you know, how many white t-shirts you're going to sell, right?

345
00:22:52,440 --> 00:22:57,840
The feature importance in both of those might be dramatically different and they are different,

346
00:22:57,840 --> 00:22:58,840
right?

347
00:22:58,840 --> 00:23:03,760
So that's the other thing when you start thinking about scale, one model or one algorithm

348
00:23:03,760 --> 00:23:07,800
doesn't fit all, right?

349
00:23:07,800 --> 00:23:14,360
And so you're able to like, so if as a developer, in this maybe dig into this retail

350
00:23:14,360 --> 00:23:19,040
case, as a developer, how am I giving you my data?

351
00:23:19,040 --> 00:23:21,120
I'm assuming that's the starting place.

352
00:23:21,120 --> 00:23:22,120
Yeah.

353
00:23:22,120 --> 00:23:26,160
So what we do is we have a couple different ways you can import data through the API right

354
00:23:26,160 --> 00:23:30,840
now, and it's through JSON or CSV, and then we have some S3 endpoints where you can put

355
00:23:30,840 --> 00:23:34,440
larger data sets, and then we'll ingest them from there.

356
00:23:34,440 --> 00:23:36,120
Okay.

357
00:23:36,120 --> 00:23:41,120
And so I'm giving you this data via the API.

358
00:23:41,120 --> 00:23:49,120
And how am I doing anything then to kind of describe this data or are you like figuring

359
00:23:49,120 --> 00:23:50,440
it all out somehow?

360
00:23:50,440 --> 00:23:52,760
So we try to have what we call sane defaults.

361
00:23:52,760 --> 00:23:57,160
So if you didn't give us any metadata and you uploaded a data set, we would do some basic

362
00:23:57,160 --> 00:24:01,760
analysis of that data set and try to create appropriate data types for each one.

363
00:24:01,760 --> 00:24:04,320
Now is that going to be, is that going to work great?

364
00:24:04,320 --> 00:24:08,520
Probably not, but will it work well enough to sort of get some results?

365
00:24:08,520 --> 00:24:09,520
That's our hope.

366
00:24:09,520 --> 00:24:14,480
As people sort of learn more about what they need to do with the data, we have metadata

367
00:24:14,480 --> 00:24:20,840
that you can use to sort of describe this is a string, this is categorical data, or

368
00:24:20,840 --> 00:24:26,920
I need an imputation strategy on this numeric field that does mean mode or, you know, that's

369
00:24:26,920 --> 00:24:27,920
sort of thing.

370
00:24:27,920 --> 00:24:32,520
So you can start to describe and we'll try to make some of those decisions for you,

371
00:24:32,520 --> 00:24:36,480
but it's better certainly once the developer gets in and sort of gets their hands around

372
00:24:36,480 --> 00:24:39,480
their own data and understand what they need to do with it.

373
00:24:39,480 --> 00:24:40,480
Okay.

374
00:24:40,480 --> 00:24:42,600
And you've mentioned imputation strategy a couple of times.

375
00:24:42,600 --> 00:24:43,600
Tell us what that means.

376
00:24:43,600 --> 00:24:49,080
The idea there is if you want to look at aggregating data over time, the question is around

377
00:24:49,080 --> 00:24:50,080
do you add it up?

378
00:24:50,080 --> 00:24:52,280
Do you summon it or do you take an average, right?

379
00:24:52,280 --> 00:24:57,040
If you're looking at the weather, adding them, temperatures doesn't make any sense, right?

380
00:24:57,040 --> 00:24:58,560
You want to average it over time.

381
00:24:58,560 --> 00:25:03,640
So it's that general idea there that you can sort of indicate what type of data it is

382
00:25:03,640 --> 00:25:08,840
and then we'll take that step of how you want to handle that aggregation or imputation

383
00:25:08,840 --> 00:25:09,840
around that.

384
00:25:09,840 --> 00:25:10,840
So fields are missing, right?

385
00:25:10,840 --> 00:25:15,680
We might put in something else, or if you want to roll up your daily or hourly forecasts

386
00:25:15,680 --> 00:25:19,680
up to a monthly forecast, we're going to use different strategies to sort of roll that

387
00:25:19,680 --> 00:25:24,120
data up as well as fill in empty values.

388
00:25:24,120 --> 00:25:33,440
So I give you this time series of transactional data for this retail use case that has date

389
00:25:33,440 --> 00:25:39,160
time, skew purchase price, and maybe some other stuff.

390
00:25:39,160 --> 00:25:43,480
Like how do I then, how do I tell you what problem I'm trying to solve?

391
00:25:43,480 --> 00:25:48,760
You're basically going to tell us the column that you're wanting to predict off of, right?

392
00:25:48,760 --> 00:25:49,760
Yep.

393
00:25:49,760 --> 00:25:53,320
So in that same metadata, you just say, these are features and this is a target.

394
00:25:53,320 --> 00:25:55,880
And you can turn those on and off for each run as well.

395
00:25:55,880 --> 00:26:00,000
So if you want to turn off a feature, you could then predict on McCollum.

396
00:26:00,000 --> 00:26:01,000
You could turn it back on.

397
00:26:01,000 --> 00:26:02,640
You could do what if scenarios that way, too?

398
00:26:02,640 --> 00:26:07,560
Just sort of say what if we did this or that with those features, too.

399
00:26:07,560 --> 00:26:13,240
And then are you able to do anything with around like artificial features, like so features

400
00:26:13,240 --> 00:26:18,600
that aren't in the data and the, you know, predicting home price example, you might want

401
00:26:18,600 --> 00:26:23,520
to look at the number of rooms times the number of bathrooms and that, you know, particular

402
00:26:23,520 --> 00:26:28,560
and artificial feature might have some predictive value, you know, that either of those features

403
00:26:28,560 --> 00:26:30,680
by themselves doesn't have.

404
00:26:30,680 --> 00:26:34,520
I mean, it depends, I would say in those cases, no, we're not going to just sort of brute

405
00:26:34,520 --> 00:26:38,040
force through all your columns and try multiplying them and see if something happens or dividing

406
00:26:38,040 --> 00:26:39,040
by something, you know what I mean?

407
00:26:39,040 --> 00:26:40,480
Like, that's a hard problem.

408
00:26:40,480 --> 00:26:44,640
So I think in that sense, you need to know a little bit of your data and what the indicators

409
00:26:44,640 --> 00:26:45,640
are.

410
00:26:45,640 --> 00:26:49,880
But in other cases, we can sort of do some interesting things with maybe holiday calendars

411
00:26:49,880 --> 00:26:53,160
and we can automatically overlay with your time series data.

412
00:26:53,160 --> 00:26:57,280
We've done some work sort of with, you know, Latin launch locations, we can incorporate

413
00:26:57,280 --> 00:27:00,840
whether automatically certain weather features that might help.

414
00:27:00,840 --> 00:27:05,440
So it depends on what it is and we have some interesting ideas and plans in the future

415
00:27:05,440 --> 00:27:06,520
for that as well.

416
00:27:06,520 --> 00:27:13,000
But no, I mean, at some point you have to sort of understand what the indicators are that

417
00:27:13,000 --> 00:27:14,480
may help you predict, right?

418
00:27:14,480 --> 00:27:18,160
Like, we're not, we're not a crystal ball.

419
00:27:18,160 --> 00:27:23,840
So I tell you what, what columns I want to predict on and how does the, does, you mentioned

420
00:27:23,840 --> 00:27:32,520
earlier that the, the platform will, you know, build individual models for each of the,

421
00:27:32,520 --> 00:27:33,520
the excuses.

422
00:27:33,520 --> 00:27:38,680
Is that something that I need to tell it to do or is that, is it always doing that?

423
00:27:38,680 --> 00:27:43,280
It always does that with a caveat, you know, once you have a model, though, you can reuse

424
00:27:43,280 --> 00:27:44,280
it.

425
00:27:44,280 --> 00:27:47,320
The problem with time series data is, of course, as often yesterday as a better predictor

426
00:27:47,320 --> 00:27:50,240
of today or tomorrow than a week ago.

427
00:27:50,240 --> 00:27:55,200
So there's sort of the notion of how, how long my algorithm is good for, right?

428
00:27:55,200 --> 00:27:58,840
And so on the time series, you end up rebuilding models a lot more frequently than you might

429
00:27:58,840 --> 00:28:03,480
with like a regression model that's just, you know, not as concerned about those sort

430
00:28:03,480 --> 00:28:08,280
of yesterday as the future or two days ago, three days ago, last week.

431
00:28:08,280 --> 00:28:09,280
Okay.

432
00:28:09,280 --> 00:28:13,720
Maybe tell me a little bit about some of the kind of technical challenges that you had

433
00:28:13,720 --> 00:28:16,920
to overcome to put all this together.

434
00:28:16,920 --> 00:28:20,760
Boy, yeah, that's, that's a big question.

435
00:28:20,760 --> 00:28:21,760
That's the word start.

436
00:28:21,760 --> 00:28:23,760
That could be a second podcast.

437
00:28:23,760 --> 00:28:29,160
Yeah, I think really, you know, when you try to build a generalized platform, it's, there's

438
00:28:29,160 --> 00:28:34,120
so many challenges, you know, around handling all sorts of data, not, you know, and we can't

439
00:28:34,120 --> 00:28:35,120
boil the ocean.

440
00:28:35,120 --> 00:28:39,520
So we have to make a lot of trade-offs and choices around what is the MVP going to look

441
00:28:39,520 --> 00:28:40,520
like?

442
00:28:40,520 --> 00:28:42,160
What is the next version going to look like?

443
00:28:42,160 --> 00:28:43,160
What can wait?

444
00:28:43,160 --> 00:28:44,160
What do we have to have now?

445
00:28:44,160 --> 00:28:45,160
How do we get something to market?

446
00:28:45,160 --> 00:28:50,360
And so from the product side, it's been a big challenge to sort of make those trade-offs

447
00:28:50,360 --> 00:28:54,760
and then get the work done in a way that we feel good about the results.

448
00:28:54,760 --> 00:29:02,120
So yeah, I think building sort of that ETL pipeline in a general sense, sort of that pipeline

449
00:29:02,120 --> 00:29:05,760
around, you know, do you scale the data before you run the algorithms?

450
00:29:05,760 --> 00:29:06,760
When do you have to do that?

451
00:29:06,760 --> 00:29:12,440
And sort of building in sort of all that core capability around, how do we get any sort

452
00:29:12,440 --> 00:29:16,640
of data into a common sort of matrix to do ML on?

453
00:29:16,640 --> 00:29:18,960
And there's lots of different paths to get there.

454
00:29:18,960 --> 00:29:23,640
And I think one of the big challenges was trying to define the best use cases for us

455
00:29:23,640 --> 00:29:26,080
to sort of get the broadest hit.

456
00:29:26,080 --> 00:29:29,520
Now, you know, we have to trade-off some accuracy for that and that's okay.

457
00:29:29,520 --> 00:29:34,640
As we go on, I think we feel like we can get better and go deeper in a lot of areas.

458
00:29:34,640 --> 00:29:35,640
Mm-hmm.

459
00:29:35,640 --> 00:29:39,560
Yeah, that was a question that I had earlier and I didn't ask it.

460
00:29:39,560 --> 00:29:47,680
You know, often when, you know, I've talked to folks working on generalized machine learning

461
00:29:47,680 --> 00:29:55,360
platforms, the idea tends to be, you know, you're trying to get the developer, the organization

462
00:29:55,360 --> 00:29:59,760
from, you know, zero to 80 percent.

463
00:29:59,760 --> 00:30:04,560
And then maybe once they're, you know, at 80 percent have some maturity, they may find

464
00:30:04,560 --> 00:30:09,920
it they, you know, that they need to invest further to get to, you know, 90 percent.

465
00:30:09,920 --> 00:30:13,160
Is that the general way you think about the problem as well?

466
00:30:13,160 --> 00:30:19,840
Yeah, to a certain degree, it's always, I just think where we are today, getting people

467
00:30:19,840 --> 00:30:23,000
familiar with what machine learning can do is kind of the first hurdle.

468
00:30:23,000 --> 00:30:26,600
And then as you were saying, how do we get better performance after we actually have

469
00:30:26,600 --> 00:30:31,760
something in the environment or some kind of application deploy that's using machine

470
00:30:31,760 --> 00:30:32,760
learning?

471
00:30:32,760 --> 00:30:36,400
So I think there's this natural evolution and that's one of the things that we've kind

472
00:30:36,400 --> 00:30:41,280
of built into our own platform is let's talk about like a regression problem.

473
00:30:41,280 --> 00:30:46,080
You were talking about house prices, I think earlier, well, let's say your initial data

474
00:30:46,080 --> 00:30:51,560
set has like 15 features and it's like room size and how many bedrooms and whatever, right?

475
00:30:51,560 --> 00:30:54,720
And you build your model and it's pretty good.

476
00:30:54,720 --> 00:31:00,000
But then as time goes on, you might think, well, hey, you know what, maybe this new thing

477
00:31:00,000 --> 00:31:03,200
that I just thought about is going to have a big impact and maybe that new thing in this

478
00:31:03,200 --> 00:31:05,960
scenario is the school rating in your area.

479
00:31:05,960 --> 00:31:10,040
So now you're going to bring in the whole new feature of school ratings for the house

480
00:31:10,040 --> 00:31:11,680
that you're looking at.

481
00:31:11,680 --> 00:31:14,360
And then with us, it's just naturally going to build a new model.

482
00:31:14,360 --> 00:31:15,960
So you don't even have to think about it.

483
00:31:15,960 --> 00:31:19,280
All right, well, now I have the same feature to idea into algorithm or not.

484
00:31:19,280 --> 00:31:22,960
If you do need a new algorithm, our platform is just going to figure that out and it's going

485
00:31:22,960 --> 00:31:27,040
to say, all right, well, you know, maybe you're using like a classio regression or something

486
00:31:27,040 --> 00:31:32,520
before and now you're going to use whatever else because you added all these new features.

487
00:31:32,520 --> 00:31:35,520
I think that's really the other big power of this, right?

488
00:31:35,520 --> 00:31:40,520
And as people start looking at the accuracy and the results, I think the natural question

489
00:31:40,520 --> 00:31:43,240
is always going to be, well, how can we do better, right?

490
00:31:43,240 --> 00:31:50,160
And we are trying to spend a lot of time and energy on that educational component, which

491
00:31:50,160 --> 00:31:53,680
is kind of answering that question, okay, we have this today.

492
00:31:53,680 --> 00:31:54,680
How do we get a better, right?

493
00:31:54,680 --> 00:31:57,600
Is it going to get better if we add in school ratings?

494
00:31:57,600 --> 00:32:02,920
Maybe probably, but let's figure out what happens when we actually do it and then they'll

495
00:32:02,920 --> 00:32:08,280
figure out, okay, well, this had no impact or had a big impact and then that should lead

496
00:32:08,280 --> 00:32:11,920
to the next question of, all right, well, what else could we add, maybe to make it even

497
00:32:11,920 --> 00:32:12,920
better, right?

498
00:32:12,920 --> 00:32:18,240
And you could just keep going down that old trend and naturally just our platform is going

499
00:32:18,240 --> 00:32:23,040
to figure out, okay, I should mature and add new features, we're going to pick maybe

500
00:32:23,040 --> 00:32:24,040
a new algorithm.

501
00:32:24,040 --> 00:32:26,040
It's going to perform even better.

502
00:32:26,040 --> 00:32:27,360
Yeah, it's funny.

503
00:32:27,360 --> 00:32:35,880
A lot of those things, those activities that you're describing on the spectrum of improving

504
00:32:35,880 --> 00:32:39,400
your algorithms are things that I think of as data science.

505
00:32:39,400 --> 00:32:48,720
Like if you separate out the knowledge of the underlying math from the process and the

506
00:32:48,720 --> 00:32:56,400
way of thinking about data and features that have some predictive value and using those

507
00:32:56,400 --> 00:33:04,040
to create predictions, like that to me, a lot of that is what data science is really

508
00:33:04,040 --> 00:33:05,040
about.

509
00:33:05,040 --> 00:33:10,640
And I'm wondering if you, you know, is it that you end up teaching developers, those parts

510
00:33:10,640 --> 00:33:15,440
of data science in order to get them productive on this platform or are you finding that there

511
00:33:15,440 --> 00:33:20,840
are, you know, maybe folks with different roles that understand that stuff but don't understand

512
00:33:20,840 --> 00:33:22,160
the math.

513
00:33:22,160 --> 00:33:28,360
How do you kind of see the audience and for what you're doing and is it evolving at all?

514
00:33:28,360 --> 00:33:31,000
I think the audience is absolutely evolving.

515
00:33:31,000 --> 00:33:36,520
I think as we look at the future, we're releasing some additional features that should really

516
00:33:36,520 --> 00:33:41,120
marry this notion of developers teaming up with data scientists.

517
00:33:41,120 --> 00:33:44,360
We want to enable more collaboration in that space.

518
00:33:44,360 --> 00:33:50,640
We think one of the main things as we look at more mature organizations is assorting

519
00:33:50,640 --> 00:33:53,040
the time from R&D to production.

520
00:33:53,040 --> 00:33:58,000
So if you're a data scientist and now you're collaborating real time over, you know, this

521
00:33:58,000 --> 00:34:02,840
maybe beta application that the developer has made that kind of really speeds things up

522
00:34:02,840 --> 00:34:07,560
and then as the data scientist is looking at what the developer may be made as an initial

523
00:34:07,560 --> 00:34:12,280
group of concept, you know, yes, the data scientist might really be honed in on how to make

524
00:34:12,280 --> 00:34:13,680
this better.

525
00:34:13,680 --> 00:34:18,720
So that's one approach and then I think again, you just have so many, from an innovation

526
00:34:18,720 --> 00:34:25,040
perspective, enabling developers to quickly prototype things as a tremendous value, right?

527
00:34:25,040 --> 00:34:28,880
And then just being able to show if you're a developer, you know, maybe you have a data

528
00:34:28,880 --> 00:34:32,840
science friend or someone at your company is a data scientist, you know, being able to

529
00:34:32,840 --> 00:34:39,120
say, hey, look, I made this prototype application and it's doing X, you know, what do you think

530
00:34:39,120 --> 00:34:40,720
what else maybe could I do to it?

531
00:34:40,720 --> 00:34:44,320
So just fostering that collaboration is obviously really important to us.

532
00:34:44,320 --> 00:34:49,240
As we look at especially 2018, you should see more and more things start to be released.

533
00:34:49,240 --> 00:34:54,880
Just to add on to that, there's really not enough of data scientists to go around unfortunately.

534
00:34:54,880 --> 00:35:00,680
So we also enabling, I think, people to get some capability up is better than, you know,

535
00:35:00,680 --> 00:35:02,800
them just having to go, I guess, go hungry.

536
00:35:02,800 --> 00:35:03,800
So to speak.

537
00:35:03,800 --> 00:35:04,800
Yeah.

538
00:35:04,800 --> 00:35:05,800
Yeah.

539
00:35:05,800 --> 00:35:12,760
And where it's industry or vertical focused, where you're able to, you know, you've got

540
00:35:12,760 --> 00:35:16,560
data scientists that are thinking about the problems in those verticals and what the

541
00:35:16,560 --> 00:35:21,280
data needs to look like and what the features are so that you can, you know, guide or offer

542
00:35:21,280 --> 00:35:25,920
a special, you know, features for specific verticals or is that not a focus for you right

543
00:35:25,920 --> 00:35:26,920
now?

544
00:35:26,920 --> 00:35:27,920
Yeah.

545
00:35:27,920 --> 00:35:32,320
So shortly, we should be releasing this notion of kind of data templates.

546
00:35:32,320 --> 00:35:37,280
The idea behind that is that the bare minimum here are kind of the features that you would

547
00:35:37,280 --> 00:35:41,760
need in order to be successful with this type of problem, right?

548
00:35:41,760 --> 00:35:48,800
So skew level forecasting as an example, you know, the bare minimum features would be

549
00:35:48,800 --> 00:35:49,800
like the time, right?

550
00:35:49,800 --> 00:35:56,160
So ideally, you want a year or data, it can work with less than that, but best performances

551
00:35:56,160 --> 00:35:57,160
a year.

552
00:35:57,160 --> 00:36:01,840
And then you're going to want, you know, probably daily sales activity.

553
00:36:01,840 --> 00:36:05,680
And then beyond that, like those are just the bare minimum, you can create a forecast

554
00:36:05,680 --> 00:36:11,280
off just those two things because the platform automatically extracts the database on the

555
00:36:11,280 --> 00:36:15,080
timestamp, you know, if it's Monday to say Wednesday and I'll find the weekly and daily

556
00:36:15,080 --> 00:36:16,800
trends things of that nature.

557
00:36:16,800 --> 00:36:22,440
But then you could start adding into that template additional things that maybe aren't necessarily

558
00:36:22,440 --> 00:36:24,120
required with a nice to have, right?

559
00:36:24,120 --> 00:36:28,720
So a nice to have would be, do you have a promotion going on for that particular item,

560
00:36:28,720 --> 00:36:29,720
right?

561
00:36:29,720 --> 00:36:31,800
And that could be a binary one or zero.

562
00:36:31,800 --> 00:36:35,680
You could extractulate out like what kind of motion it was or it isn't like a buy one

563
00:36:35,680 --> 00:36:36,680
get one.

564
00:36:36,680 --> 00:36:37,680
Is it just a percent off?

565
00:36:37,680 --> 00:36:39,080
Things of that nature.

566
00:36:39,080 --> 00:36:41,240
But yeah, we have plans to kind of put out there.

567
00:36:41,240 --> 00:36:45,320
Here's a bare minimum that you need and here's kind of like the nice to have and kind

568
00:36:45,320 --> 00:36:46,320
of go from there.

569
00:36:46,320 --> 00:36:47,560
Oh, interesting.

570
00:36:47,560 --> 00:36:52,520
So in that example, where you're, you've got a developer, they've gone out and collected

571
00:36:52,520 --> 00:36:58,240
some sales and marketing, you know, historical data and they're doing a forecast.

572
00:36:58,240 --> 00:37:05,520
How do you articulate to this developer that may not be, you know, statistically sophisticated,

573
00:37:05,520 --> 00:37:09,440
the extent to which they should rely on this result that your platforms put out for

574
00:37:09,440 --> 00:37:10,440
them?

575
00:37:10,440 --> 00:37:13,400
Yeah, currently we put out some metrics on that.

576
00:37:13,400 --> 00:37:18,120
And what we've determined is we need to get a little more friendlier on those metrics.

577
00:37:18,120 --> 00:37:23,040
So we have some sort of education around what the metrics mean and sort of our, some

578
00:37:23,040 --> 00:37:25,400
of our learning on our documentation site.

579
00:37:25,400 --> 00:37:28,920
But we want to really go to the next level I feel like and really sort of hold their

580
00:37:28,920 --> 00:37:33,600
hand a little more on what the metric is saying about the model based on what the data

581
00:37:33,600 --> 00:37:34,600
they have.

582
00:37:34,600 --> 00:37:39,640
It's currently, I think we return mate scores if it's a time series site forecast.

583
00:37:39,640 --> 00:37:42,840
And yeah, what we plan to make it even more friendly than that, right?

584
00:37:42,840 --> 00:37:47,680
Because developer might not understand what mate scores may mean absolute percentage

585
00:37:47,680 --> 00:37:48,680
error.

586
00:37:48,680 --> 00:37:49,680
Uh-huh.

587
00:37:49,680 --> 00:37:50,680
Okay.

588
00:37:50,680 --> 00:37:56,600
So this is like these interfaces between kind of what you might expect the data scientists

589
00:37:56,600 --> 00:38:01,800
to know and what the developer might not know or some of the key challenges that you

590
00:38:01,800 --> 00:38:06,000
face as you evolve this and bring more people onto the platform.

591
00:38:06,000 --> 00:38:10,600
Yeah, honestly, I think that's a, you're just quite frankly, I think that's a minor challenge.

592
00:38:10,600 --> 00:38:15,360
I think the biggest challenge that we have and I think just the industry has in general

593
00:38:15,360 --> 00:38:17,040
is access to data.

594
00:38:17,040 --> 00:38:22,240
If I want to build, and obviously I was thinking about this over the weekend, I wanted to build

595
00:38:22,240 --> 00:38:29,280
a couple of different applications and the hurdle is getting the data sets, right?

596
00:38:29,280 --> 00:38:36,000
So if I wanted to build an application off of predicting cancer, there's certain cancer

597
00:38:36,000 --> 00:38:37,000
data sets already out there.

598
00:38:37,000 --> 00:38:41,600
It's a very famous, the breast cancer data set that kind of has the size of the tumor

599
00:38:41,600 --> 00:38:42,600
and things of that nature.

600
00:38:42,600 --> 00:38:47,040
But maybe a big important feature there is where the people live, right?

601
00:38:47,040 --> 00:38:52,400
If you live next to, I don't know, a waste site, you know, that's a little bit extreme,

602
00:38:52,400 --> 00:38:54,200
but let's say that you live there.

603
00:38:54,200 --> 00:38:55,680
Maybe that's why you have cancer, right?

604
00:38:55,680 --> 00:39:00,000
So maybe where people live might actually be a huge indicator whether or not you're going

605
00:39:00,000 --> 00:39:04,880
to develop cancer or that lump that you discovered is the minor eminigment, right?

606
00:39:04,880 --> 00:39:08,000
But it's very hard to get that data set, right?

607
00:39:08,000 --> 00:39:13,120
For one, you have the HIPAA issue, so there's that hurdle to get over.

608
00:39:13,120 --> 00:39:16,720
But then more than that, it's just, are people going to want to share the data?

609
00:39:16,720 --> 00:39:21,960
The other idea I had was predicting whether or not a startup would be successful.

610
00:39:21,960 --> 00:39:25,960
And one of the things that I think you would need to do that would be kind of the financial

611
00:39:25,960 --> 00:39:29,080
information on the startup, right, and how much money they're spending, where and the

612
00:39:29,080 --> 00:39:30,080
return.

613
00:39:30,080 --> 00:39:36,360
But again, trying to get reliable data that's going to help me build that model is kind

614
00:39:36,360 --> 00:39:37,360
of what I think right now.

615
00:39:37,360 --> 00:39:41,160
It's the biggest obstacle in the field.

616
00:39:41,160 --> 00:39:46,160
Any other thoughts on that particular point, any other obstacles that you see?

617
00:39:46,160 --> 00:39:48,680
Yeah, I mean, there's plenty of them.

618
00:39:48,680 --> 00:39:53,120
I think really walking them through what their data might be telling them is going to be

619
00:39:53,120 --> 00:39:54,120
helpful.

620
00:39:54,120 --> 00:39:59,040
So like one of the big challenges is you submit a data set and I think there needs to be

621
00:39:59,040 --> 00:40:03,920
more of an indicator of, you really can't do anything with this potentially in some

622
00:40:03,920 --> 00:40:04,920
situations.

623
00:40:04,920 --> 00:40:08,440
It's like, it's okay, and then like this is really good data.

624
00:40:08,440 --> 00:40:13,280
So I think sort of kind of defining and helping them along without just just raw metrics,

625
00:40:13,280 --> 00:40:18,640
you know, like give them a little more, I guess, safety feeling about their models is still

626
00:40:18,640 --> 00:40:19,640
a challenge.

627
00:40:19,640 --> 00:40:21,440
And then we have some other ideas around that.

628
00:40:21,440 --> 00:40:22,440
Yeah.

629
00:40:22,440 --> 00:40:26,800
I think just also the add to that one of the challenges is how machine learning AI, whatever

630
00:40:26,800 --> 00:40:31,920
you want to call it, today has been talked about in the media, it's put into the minds

631
00:40:31,920 --> 00:40:35,280
of a lot of people that machine learning is this magic bullet.

632
00:40:35,280 --> 00:40:39,520
And to give kind of a real example, we have some people today that have signed up for

633
00:40:39,520 --> 00:40:43,640
the API that want to use it to protect things like Bitcoin price.

634
00:40:43,640 --> 00:40:47,840
And the data set is just the price point on a day.

635
00:40:47,840 --> 00:40:51,040
And they think that machine learning is just going to like magically figure out what

636
00:40:51,040 --> 00:40:52,840
the price is going to be.

637
00:40:52,840 --> 00:40:56,800
And you know, sometimes we have to talk to the people who sign up on that use case or

638
00:40:56,800 --> 00:41:01,560
like, well, if that's all the features you have, do you really think that to say is the

639
00:41:01,560 --> 00:41:06,600
reason why Bitcoin jumped up or do you think there's this other feature out there that

640
00:41:06,600 --> 00:41:09,160
is really impacting and influencing it, right?

641
00:41:09,160 --> 00:41:14,840
So again, it's just people really have to, I think, get beyond that machine learning isn't

642
00:41:14,840 --> 00:41:16,520
a magical bullet.

643
00:41:16,520 --> 00:41:20,640
It's only going to find the patterns in the correlation in your data set.

644
00:41:20,640 --> 00:41:24,040
If it's not there in the data, you know, it's never going to find a pattern.

645
00:41:24,040 --> 00:41:25,040
Yeah.

646
00:41:25,040 --> 00:41:26,040
That's awesome.

647
00:41:26,040 --> 00:41:32,160
Anything else that you'd like to share with the audience as we come no close?

648
00:41:32,160 --> 00:41:35,560
You know, there's just the other only thing that I was thinking about is as we're talking

649
00:41:35,560 --> 00:41:38,280
about kind of our overall goal.

650
00:41:38,280 --> 00:41:43,160
I think really most of 2018 is going to be this theme of how do we enhance collaboration

651
00:41:43,160 --> 00:41:45,720
between developers and data scientists.

652
00:41:45,720 --> 00:41:49,680
One of the newer features that we'll be releasing in 2018 is something called bring your

653
00:41:49,680 --> 00:41:51,000
own algorithm.

654
00:41:51,000 --> 00:41:57,040
So what this is going to do is it will allow a data scientist to create maybe a very specific

655
00:41:57,040 --> 00:42:02,480
algorithm that is really good at solving for a particular thing.

656
00:42:02,480 --> 00:42:03,640
So let me give an example.

657
00:42:03,640 --> 00:42:10,800
Let's say that you are a data scientist at Best Buy or some big box retailer and let's

658
00:42:10,800 --> 00:42:16,840
say the retailer really cares about the price or the sales for like, I don't know, LED

659
00:42:16,840 --> 00:42:17,840
TVs.

660
00:42:17,840 --> 00:42:23,960
Well, the notion is that they might create a very specific algorithm that's really good

661
00:42:23,960 --> 00:42:30,160
at figuring out, hey, how many flat screen, LED 55 inch TVs that we're going to sell.

662
00:42:30,160 --> 00:42:33,600
And all that they would have to do is take that algorithm that they build in house.

663
00:42:33,600 --> 00:42:39,320
They could just plop it into our API and their algorithm will live just in their instance.

664
00:42:39,320 --> 00:42:40,840
It'll live by itself.

665
00:42:40,840 --> 00:42:45,520
Well, it'll live just in their instance by itself, but it will live side by side our 150

666
00:42:45,520 --> 00:42:46,920
plus algorithms.

667
00:42:46,920 --> 00:42:48,520
So they'll be able to see your real time.

668
00:42:48,520 --> 00:42:52,360
Hey, here's where our algorithm is winning and here's where it's not winning.

669
00:42:52,360 --> 00:42:58,960
I guess a related question that I had from earlier, as you're starting to get into collaboration

670
00:42:58,960 --> 00:43:04,480
and data scientists working with developers and developers exploring their data and creating

671
00:43:04,480 --> 00:43:12,120
new features and things like that, it opens up this whole set of issues around model management

672
00:43:12,120 --> 00:43:18,320
and model governance and model providence and stuff like that, are you the company thinking

673
00:43:18,320 --> 00:43:24,320
about any of those things or do you offer support for those kinds of challenges?

674
00:43:24,320 --> 00:43:27,280
Yeah, that's certainly come up in the past.

675
00:43:27,280 --> 00:43:33,440
We've had it come up primarily in the insurance industry, you know, the regulations around

676
00:43:33,440 --> 00:43:34,440
the model building.

677
00:43:34,440 --> 00:43:38,920
You really have to understand how the decision is coming about right because you can't

678
00:43:38,920 --> 00:43:44,200
discriminate in things of that nature, so yeah, for kind of the more enterprise customers

679
00:43:44,200 --> 00:43:49,680
will allow them to actually download that model and algorithm and kind of have that supporting

680
00:43:49,680 --> 00:43:55,280
documentation they need to have to prove that, you know, we're not using for lack of

681
00:43:55,280 --> 00:44:00,480
better word illegal types of data sets to create, you know, the prediction result.

682
00:44:00,480 --> 00:44:04,480
So I think as time goes on, we're going to see more and more of that.

683
00:44:04,480 --> 00:44:09,680
And again, right now, one way that we can solve it is just by giving a dock rise container

684
00:44:09,680 --> 00:44:13,600
that contains the algorithm and explain how it works and things of that nature.

685
00:44:13,600 --> 00:44:18,520
So yeah, I mean, as time goes on, I think we're going to see more and more of that.

686
00:44:18,520 --> 00:44:19,520
Okay.

687
00:44:19,520 --> 00:44:20,520
All right.

688
00:44:20,520 --> 00:44:21,520
Great.

689
00:44:21,520 --> 00:44:26,560
And then did you have an offer for like free access to the platform or free API key or

690
00:44:26,560 --> 00:44:28,920
something like that available to listeners?

691
00:44:28,920 --> 00:44:29,920
Yeah.

692
00:44:29,920 --> 00:44:35,360
One can sign up for the API for free, that's part of our philosophy as a company and being

693
00:44:35,360 --> 00:44:39,400
developer first is that we want people to use it.

694
00:44:39,400 --> 00:44:42,200
So we do have a community edition, it's 100% for free.

695
00:44:42,200 --> 00:44:45,760
It will always be free that community tier.

696
00:44:45,760 --> 00:44:52,200
So yeah, anyone can go to nexos.com, sign up for an API key and get started in under five

697
00:44:52,200 --> 00:44:53,200
minutes.

698
00:44:53,200 --> 00:44:54,200
Okay.

699
00:44:54,200 --> 00:44:55,200
Awesome.

700
00:44:55,200 --> 00:45:00,560
Well, guys, thank you so much for taking the time to chat with us.

701
00:45:00,560 --> 00:45:06,040
I think what you're doing is really interesting and I will certainly be looking forward to

702
00:45:06,040 --> 00:45:10,520
keeping up with you and I'd love for you to keep in touch and, you know, let me know about

703
00:45:10,520 --> 00:45:13,560
you know, as the platform evolves, your continued success, et cetera.

704
00:45:13,560 --> 00:45:18,280
Yeah, definitely was pleasure being on here and thanks so much for having us.

705
00:45:18,280 --> 00:45:19,280
Okay.

706
00:45:19,280 --> 00:45:23,760
So before we go, why don't you take a second and tell me a little bit about what's attraction

707
00:45:23,760 --> 00:45:24,760
been to date?

708
00:45:24,760 --> 00:45:29,080
How many users do you have or how many companies are using it?

709
00:45:29,080 --> 00:45:30,080
Yeah.

710
00:45:30,080 --> 00:45:35,800
So currently we released the act of the public on July, no, on 2017, today we have over

711
00:45:35,800 --> 00:45:42,520
4,000 developers that have signed up and we have close to 500 applications that have

712
00:45:42,520 --> 00:45:43,520
been deployed.

713
00:45:43,520 --> 00:45:44,520
Oh, wow.

714
00:45:44,520 --> 00:45:45,520
All right.

715
00:45:45,520 --> 00:45:50,160
Well, once again, thank you so much for taking the time to chat with me.

716
00:45:50,160 --> 00:45:51,160
I appreciate it.

717
00:45:51,160 --> 00:45:52,160
Yeah.

718
00:45:52,160 --> 00:45:53,160
Likewise, pleasure.

719
00:45:53,160 --> 00:45:54,160
It was fun.

720
00:45:54,160 --> 00:45:55,160
Thanks, guys.

721
00:45:55,160 --> 00:46:03,440
All right, everyone, that's our show for today.

722
00:46:03,440 --> 00:46:08,680
Thanks so much for listening and for your continued feedback and support.

723
00:46:08,680 --> 00:46:14,080
For more information on Ryan, Jason, or any of the topics covered in this episode, head

724
00:46:14,080 --> 00:46:18,880
on over to twimlai.com slash talk slash 69.

725
00:46:18,880 --> 00:46:22,680
This interview kicks off our Strange Loop 2017 series.

726
00:46:22,680 --> 00:46:28,400
To follow along with the series, visit twimlai.com slash ST Loop.

727
00:46:28,400 --> 00:46:34,320
Of course, you can send along your feedback and questions via Twitter to at twimlai or

728
00:46:34,320 --> 00:46:39,360
at Sam Charrington or leave a comment right on the show notes page.

729
00:46:39,360 --> 00:46:44,520
Thanks once again to Nexosis for their sponsorship of the show and this series.

730
00:46:44,520 --> 00:46:51,600
For more info on them and to get your free API key, visit nexosis.com slash twimlai.

731
00:46:51,600 --> 00:46:56,080
And of course, thanks once again to you for listening and catch you next time.

