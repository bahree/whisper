1
00:00:00,000 --> 00:00:16,880
All right, everyone. I am here with Suci Saria. Suci is the founder and CEO of Bayesian Health,

2
00:00:16,880 --> 00:00:22,560
the John C. Malone Associate Professor of Computer Science, Statistics and Health Policy,

3
00:00:22,560 --> 00:00:28,480
and the Director of the Machine Learning and Healthcare Lab at the Johns Hopkins University. Suci,

4
00:00:28,480 --> 00:00:34,560
welcome to the Tuomo AI Podcast. Thanks, Sam. The long affiliation that made me very nervous.

5
00:00:36,320 --> 00:00:42,320
It was quite a mouthful, but I am super excited to have you here on the show. This is an

6
00:00:42,320 --> 00:00:47,520
interview that I've been looking forward to for a very long time. I think I remember seeing one of

7
00:00:47,520 --> 00:00:54,080
your very early presentations on machine learning for sepsis, and this was how long ago was that work?

8
00:00:54,080 --> 00:01:00,640
I've been working on it for over six years now, so I don't know when which presentation you saw,

9
00:01:00,640 --> 00:01:05,920
but yeah, it's been a while. That's awesome. The podcast has been going strong for five,

10
00:01:05,920 --> 00:01:12,720
so it was probably early days for for both of us. So nonetheless, I'm excited to have you here

11
00:01:12,720 --> 00:01:18,320
on the show and would love to start out by having you introduce yourself to our audience,

12
00:01:18,320 --> 00:01:23,200
share a bit about your story and kind of give us a sense for how you came to work at this convergence

13
00:01:23,200 --> 00:01:27,520
of machine learning, AI, healthcare, medicine, all these great things.

14
00:01:28,640 --> 00:01:35,120
So I grew up in India, in like a tiny little town in India, and it just so happens, you know,

15
00:01:35,120 --> 00:01:39,280
India is a very nerdy place. It's people are totally encouraged to be engineers and computer science

16
00:01:39,280 --> 00:01:46,560
nerds at a young age, and I got into computer science very early and actually got fascinated by AI

17
00:01:46,560 --> 00:01:54,240
as a field, and just really got lucky and trained at a very young age with people who are luminaries

18
00:01:54,240 --> 00:01:59,520
in the field, which means got tons of opportunities that were uncharacteristic for someone my age and

19
00:01:59,520 --> 00:02:09,840
background. And in terms of for me, actually around 12 years ago, 2006, 2007, eight around then,

20
00:02:09,840 --> 00:02:14,080
I was kind of going through an early midlife crisis where I realized a lot of the kinds of ideas

21
00:02:14,080 --> 00:02:20,240
we were exploring in AI and machine learning, the applications at the time were advertising,

22
00:02:20,800 --> 00:02:27,920
or like, you know, personalization on a phone or personalization on a desktop, you know,

23
00:02:27,920 --> 00:02:36,160
email foldering. And what that made me think about was, you know, like, is that I wanted to sort of

24
00:02:36,160 --> 00:02:44,320
do something with more social immediate social impact. And that meant I considered everything,

25
00:02:44,320 --> 00:02:52,640
and around that time also got introduced to colleagues at Stanford who were physicians. So these were,

26
00:02:52,640 --> 00:02:57,680
like physicians who took care of premature babies, these babies at risk of major complications.

27
00:02:58,800 --> 00:03:03,520
And if you can turn out, because they're very, very tiny premature,

28
00:03:03,520 --> 00:03:08,000
treating them in a timely way is very important for being able to impact the health outcomes.

29
00:03:08,000 --> 00:03:12,160
Like, they're much more at risk for declining deteriorating, had poor having poor neurodevelopmental

30
00:03:12,160 --> 00:03:18,640
outcomes, and not surviving if you don't catch them in a timely way. So that was sort of my first

31
00:03:18,640 --> 00:03:26,240
introduction, actually, to moving from just sort of understanding and studying methodological problems

32
00:03:26,240 --> 00:03:33,360
in machine learning and AI, broadly applied to, like, hard messy time series data sets to thinking

33
00:03:33,360 --> 00:03:39,840
harder about real world applications, but he really could make an impact. And, you know, that

34
00:03:41,120 --> 00:03:45,600
and healthcare is just really hard. I didn't realize how hard it is and what I was getting into.

35
00:03:46,240 --> 00:03:50,160
But I really didn't. It's got me so many sleepless nights.

36
00:03:50,160 --> 00:03:55,040
But yeah, but, you know, it sort of also made me realize, like,

37
00:03:55,760 --> 00:04:02,400
wholly, majorly, like, there's so much opportunity. But, you know, it's going to require the

38
00:04:02,400 --> 00:04:10,320
right types of efforts to make progress. And that's how I got started and kept going down that path

39
00:04:10,320 --> 00:04:15,920
and actually have considered the whole nine gamut from, like, advising companies to

40
00:04:15,920 --> 00:04:20,560
the previous seeding particle research companies to now spinning out this company out of Hopkins.

41
00:04:21,280 --> 00:04:29,520
But to obviously being a professor, faculty, you know, innovating on research. And all throughout

42
00:04:29,520 --> 00:04:37,360
sort of with one singular focus, which is healthcare is moving from, you know, in 2009, there was the

43
00:04:38,560 --> 00:04:44,720
particularly big event that happened, which was the passage of the high tech act. That made it so

44
00:04:44,720 --> 00:04:50,560
that systems were going from no data to data, data were now going to be stored electronically at

45
00:04:50,560 --> 00:04:58,160
scale across hospitals and clinics around the country, which means it was entering this era,

46
00:04:58,160 --> 00:05:05,040
like almost like a 1999 where the web came. So in medicine, electronic data is coming,

47
00:05:05,040 --> 00:05:12,720
electronic infrastructure is coming. And since 2009, in the last 10 years, there's been widespread

48
00:05:12,720 --> 00:05:18,000
adoption because of policy changes off this electronic infrastructure. But the use of this data

49
00:05:18,000 --> 00:05:22,320
is still extremely limited in healthcare delivery. Like today, the way physicians practice is still the

50
00:05:22,320 --> 00:05:29,040
way, you know, practice occurred 50 years ago, 100 years ago, 200 years ago. And so the singular focus

51
00:05:29,040 --> 00:05:35,280
being there are so many opportunities for improving the quality of care if we could use data in a

52
00:05:35,280 --> 00:05:40,560
more intelligent way correctly using the right type of AI. And so how do we make that happen?

53
00:05:40,560 --> 00:05:49,760
Nice. And you kind of jumped directly to your recent history. But I noted that you earlier in

54
00:05:49,760 --> 00:05:55,680
your career, you interned with Eric Corvitz, who was a recent guest on the show and you did your

55
00:05:55,680 --> 00:06:01,040
PhD with Daphne Kohler, who's been on the show. You've had some amazing opportunities.

56
00:06:02,080 --> 00:06:06,640
Yeah, it's true. Actually, I have a funny story about that. So I didn't actually want to be

57
00:06:06,640 --> 00:06:13,520
faculty at all. My thought was, I'm going to go into industry and, you know, like I love the pace

58
00:06:13,520 --> 00:06:20,000
at which industry moves. And I was doing a lot of work. You know, the work we did in new units,

59
00:06:20,000 --> 00:06:25,040
we were able to show by using machine, like approaching this data from a new lens, you really

60
00:06:25,040 --> 00:06:29,920
could actually predict outcomes and these little babies, which babies at risk for complications,

61
00:06:29,920 --> 00:06:37,520
much earlier than physicians were recognizing them. So at the time, I remember there was opportunity

62
00:06:37,520 --> 00:06:43,520
to start a company to build improved new natal care. And Eric's been a mentor of mine for a while

63
00:06:43,520 --> 00:06:49,600
and Eric met me at Nurebs, where he's like, why aren't you becoming faculty sushi? And I sat

64
00:06:49,600 --> 00:06:55,520
then I was like, I don't know, things that academia move at just a pace that feels a tad bit too

65
00:06:55,520 --> 00:07:00,640
slow. And we had this sort of soul-searching conversation for like half an hour where

66
00:07:01,680 --> 00:07:06,000
that got me to like reconsider where I was like, you know, what we're doing is very foundational.

67
00:07:06,000 --> 00:07:11,440
I think this was back in 2011. Very, very foundational, the kind of work I was doing back then.

68
00:07:12,000 --> 00:07:18,160
And realizing like, we're very early in our use of, we need novel methodological developments

69
00:07:18,160 --> 00:07:23,040
that was really going to unleash this kind of data. Our technology wasn't ready yet at the time,

70
00:07:23,040 --> 00:07:28,880
which meant it really needed to be in a very deep research environment, being at a place like

71
00:07:28,880 --> 00:07:36,480
Hopkins, right, which is in a way the mech-off healthcare. Like, you get to sit next to people who are

72
00:07:36,480 --> 00:07:42,800
some of the leading policymakers who study, you know, guideline policy change and got treatments.

73
00:07:42,800 --> 00:07:48,400
And like, so in some sense, it felt like in order to be able to bring about any kind of change,

74
00:07:48,400 --> 00:07:53,600
coming to the center of the activity and trying to bring change from within could be really

75
00:07:53,600 --> 00:07:59,360
productive. And so yeah, so that was really exciting and really enjoyed, you know,

76
00:07:59,840 --> 00:08:04,240
Daffney and Triedar have had a number of really amazing people who influenced me from a very,

77
00:08:04,240 --> 00:08:11,120
very young age. That's awesome. You're, often when I'm talking to folks that are applying

78
00:08:11,120 --> 00:08:19,040
ML in the medicine and healthcare field, there's like, I guess the point I'm getting at is like,

79
00:08:19,040 --> 00:08:23,120
there's a distinction. You know, there's a set of folks that kind of think of it from a policy

80
00:08:23,120 --> 00:08:26,560
perspective and healthcare. And there's a set of folks that think about it from a medicine

81
00:08:26,560 --> 00:08:34,240
perspective. Your work seems to span the two. Is that true? That's right. So I think the way

82
00:08:34,240 --> 00:08:39,440
to think about this is almost everything starts with a discovery, right? You want to first figure

83
00:08:39,440 --> 00:08:45,680
out, you know, what is something? Where is there opportunity for change? And what would you do

84
00:08:45,680 --> 00:08:52,320
differently? So in other words, are you inventing a new software based? So often in machine learning,

85
00:08:52,320 --> 00:08:57,200
the kinds of interventions we'd be looking at is like new software based tools for being able to

86
00:08:57,200 --> 00:09:02,400
do diagnosis more correctly. New software based tools for the moving, doing early detection of

87
00:09:02,400 --> 00:09:09,680
adverse events, new software based tools for targeting drugs more precisely. Software based tools

88
00:09:09,680 --> 00:09:16,560
to avoid adverse drug effects. So these are all examples of totally new opportunities that machine

89
00:09:16,560 --> 00:09:22,960
learning and AI have opened up. And in order to scale any of them, they always start from a discovery

90
00:09:22,960 --> 00:09:28,320
phase. So you're learning about you're using data to identify what's possible. Then you construct,

91
00:09:28,320 --> 00:09:33,120
you know, just like you would construct a new drug, here you would construct a new piece of

92
00:09:33,120 --> 00:09:39,120
software just like a drug happens to be bits and bytes that's using the data to do something

93
00:09:39,120 --> 00:09:45,680
differently. You should ideally go through the same exact process of creation, validation,

94
00:09:45,680 --> 00:09:52,800
evaluation, showing it works in a prospective setting upon, you know, when used. And once you've

95
00:09:52,800 --> 00:09:58,000
done all that, then you move into policy. So when we think about policy, there are two levels of

96
00:09:58,000 --> 00:10:06,080
policy. They're sort of at the level of like clinical guidelines, which means societies have to go,

97
00:10:06,080 --> 00:10:14,480
there's a whole process in medicine dissemination of new ideas are much more rigorous, methodical.

98
00:10:14,480 --> 00:10:19,840
I would even say somewhat slow in the sense that you're trying to convince, you know, and you

99
00:10:19,840 --> 00:10:25,440
saw this with the vaccines, right? We had to think very hard about how is evidence communicated,

100
00:10:25,440 --> 00:10:31,120
because you can have all the right stats and data supporting something. It still matters how

101
00:10:31,120 --> 00:10:37,760
it's disseminated. And through what channels is it disseminated in order to build trust at scale.

102
00:10:37,760 --> 00:10:42,320
And so that's sort of where you move into the policy realm, where you are thinking hard about

103
00:10:43,200 --> 00:10:48,640
one, like clinical guidelines for a specific new invention. And then at the policy level,

104
00:10:48,640 --> 00:10:55,600
also what is the general mechanism of framework by which these kinds of ideas get absorbed over and

105
00:10:55,600 --> 00:11:02,080
over again at scale. And then of course, the implications of that on everyday practice, whether it's

106
00:11:02,080 --> 00:11:10,160
cutting costs, improving outcomes, and everything that's needed to accelerate disciplined adoption.

107
00:11:10,160 --> 00:11:22,320
And a big part of that is dealing with the whole pay system here, the payers and the insurance

108
00:11:22,320 --> 00:11:28,240
companies. And I was having a conversation with a friend who comes at things from that perspective.

109
00:11:28,240 --> 00:11:35,920
He's a pharmacist by training. And he noted that, you know, we're just now getting to the point

110
00:11:35,920 --> 00:11:42,800
where, you know, the first algorithms are getting coded by the insurance companies.

111
00:11:42,800 --> 00:11:48,400
I don't know that world very well. So I'm sure I'm butchering it. But it's taken a long time to get

112
00:11:48,400 --> 00:11:56,400
to a level of progress where we're, where we're seeing the kind of impact. Yeah. So actually,

113
00:11:56,400 --> 00:12:00,160
some, let me unpack that question, because I think there are like a couple of different things

114
00:12:00,160 --> 00:12:06,560
you touched on. So the first thing is, why has it taken this long? What is hard about it?

115
00:12:06,560 --> 00:12:10,560
What's taking long? And then the second question is, well, where do we stand now?

116
00:12:11,200 --> 00:12:16,400
Yeah. So let's start with like, is it taking long? What is taking so long? Why?

117
00:12:17,120 --> 00:12:22,560
So, you know, 10 years ago, electronic health records came to be, right? And the digital

118
00:12:22,560 --> 00:12:28,320
infrastructure started to be. Now, turns out, the data collected within these and through variables,

119
00:12:28,320 --> 00:12:32,720
which means there's been, and COVID's only accelerated this, right? There are, you know,

120
00:12:32,720 --> 00:12:37,760
now data being collected in social format through devices, through measurements in the clinic,

121
00:12:37,760 --> 00:12:43,600
in a hospital, also like any kind of billing data, reimbursement data, like lots of different

122
00:12:43,600 --> 00:12:49,600
sources for really building what is kind of like a digital longitudinal picture of a person.

123
00:12:50,240 --> 00:12:54,800
And how they've, you know, how they've evolved, but also how like different treatments have

124
00:12:54,800 --> 00:13:01,280
impacted them, which means now in the last five years, what's been possible is, you know,

125
00:13:01,280 --> 00:13:07,040
researchers like myself, you know, I was sort of relatively early in this movement, but like,

126
00:13:07,040 --> 00:13:12,560
really going neck deep to understand, head deep, to understand like, what's hard? Like this data,

127
00:13:12,560 --> 00:13:17,200
there are hundreds of different data streams. It's not like imaging data via one type of data,

128
00:13:17,200 --> 00:13:22,320
you have hundreds of different data coming in, different kinds of bias, different kinds of

129
00:13:22,320 --> 00:13:28,960
missingness, and you're integrating all these data to draw real-time clinical signals,

130
00:13:29,440 --> 00:13:36,000
but these signals have to be much more trustworthy, safe and precise compared to safe, you were just,

131
00:13:36,640 --> 00:13:43,040
you know, choosing whether to show somebody the ad for a shoe, right? It's like a whole different

132
00:13:43,040 --> 00:13:50,880
ballgame. And so a big part of this was being able to build the kinds of methodology and

133
00:13:50,880 --> 00:13:58,800
technology to be able to really draw safe reliable trustworthy inferences that could then power

134
00:13:58,800 --> 00:14:04,480
specific applications. That's the first part. Second is then tying it to real concrete use cases

135
00:14:04,480 --> 00:14:12,080
that are well supported by clinical users where there's naturally need for it, like hospitals

136
00:14:12,080 --> 00:14:17,600
are struggling or providers are struggling, they actually need solutions to help them. As opposed

137
00:14:17,600 --> 00:14:22,240
to, you know, often the way technologists start by solving problems is they start with the

138
00:14:22,880 --> 00:14:27,120
problems that are technically hard, that they find technically interesting and not all of them

139
00:14:27,120 --> 00:14:32,400
are technically useful. So in this case, we want to marry the heart, you know, we had to build

140
00:14:32,400 --> 00:14:37,360
the heart technology, but we also had to deeply understand medicine. The practice of medicine to

141
00:14:37,360 --> 00:14:43,920
understand where are their use cases where today people are struggling, where there is need,

142
00:14:43,920 --> 00:14:49,600
and we can marry the two. And then the third part, that is actually really hard. And so I thought

143
00:14:49,600 --> 00:14:54,640
we were like almost there back in 2016 and I was like, oh, this is so beautiful. We've written

144
00:14:54,640 --> 00:15:01,280
all these papers in a number of different use cases and we've started to show how it's feasible

145
00:15:01,280 --> 00:15:06,320
to take the technology and to even get it to a place where it's possible to show how it would be

146
00:15:06,320 --> 00:15:12,240
used by providers. But then I realized like, you know, there are all these other barriers like how

147
00:15:12,240 --> 00:15:19,040
the technology is delivered within a provider's workflow, like it's user experience. How do they,

148
00:15:19,040 --> 00:15:24,400
you know, how will they use it? Is it easy to use? And how does it communicate? Like going back to

149
00:15:24,400 --> 00:15:30,160
machine learning and AI, one of the super cool things in the field in the last couple of years

150
00:15:30,160 --> 00:15:35,680
that, you know, our lab, my team at Bayesian and then others in the field have been thinking about

151
00:15:35,680 --> 00:15:43,040
is how do we make machine learning amenable to collaboration with experts, right? So in my example,

152
00:15:43,040 --> 00:15:49,760
if physicians and nurses and care team members are going to use the software, these are high stakes

153
00:15:49,760 --> 00:15:54,720
decisions. We need them to be able to collaborate with these software outputs. It's not like a black

154
00:15:54,720 --> 00:16:02,880
box system where the system can just, you know, say something and overrule what the provider is going

155
00:16:02,880 --> 00:16:09,360
to do. It's actually, it requires teaming. It requires the ability for the software to identify

156
00:16:09,360 --> 00:16:16,720
patients at risk for the providers to come in and agree, disagree, reason with it. And that means

157
00:16:16,720 --> 00:16:20,800
it's a joint decision making process. And how do you facilitate that with machine learning in

158
00:16:20,800 --> 00:16:25,520
high stakes environment? So that's been, those are the kinds of areas where we need to keep pushing

159
00:16:25,520 --> 00:16:30,560
the field. And we've been able to make a lot of progress in terms of the quality of the underlying

160
00:16:30,560 --> 00:16:36,400
methodological stack to be able to get to really high quality inferences that are trustworthy.

161
00:16:36,400 --> 00:16:40,240
Of course, there's always room to improve. And then, you know, in terms of use, like we built

162
00:16:40,240 --> 00:16:45,520
and deployed, for example, in sepsis, which is one of the leading causes of inpatient death.

163
00:16:45,520 --> 00:16:50,000
I lost my nephew to sepsis. So it's sort of like a personal area that I've been working in for

164
00:16:50,000 --> 00:16:57,600
almost now seven years. And in sepsis, for instance, you know, timely treatment is one of the most

165
00:16:57,600 --> 00:17:03,600
effective ways to improve outcomes. Basically, the earlier you can catch a patient, evaluate and

166
00:17:03,600 --> 00:17:07,920
give them the right treatment, the more you're likely to completely alter the clinical trajectory.

167
00:17:10,000 --> 00:17:16,000
But and our very early work back in 2015 showed you could identify sepsis early using machine

168
00:17:16,000 --> 00:17:21,920
learning, but getting it to a place where you could identify it, surface it, get providers to use it,

169
00:17:21,920 --> 00:17:29,360
adopt it, act off of it to actually improve outcomes was like a whole five year, six year journey.

170
00:17:29,360 --> 00:17:36,880
And like, and basically now we're we're recently going to release a study that shows, you know,

171
00:17:36,880 --> 00:17:43,360
our experience deploying this with thousands of physicians and nurses using it over the course of a

172
00:17:44,240 --> 00:17:49,520
two and a half year period, where we've been able to see, you know, very meaningful adoption.

173
00:17:49,520 --> 00:17:55,440
So like 90% of cases that the software flags, the providers actually go in, look at what the

174
00:17:55,440 --> 00:18:01,040
tool has to say, and they provide an evaluation. And then they treat patients if they agree. And

175
00:18:01,040 --> 00:18:07,280
and that's lead to very meaningful shifts in in terms of in our cohort, what we found is

176
00:18:08,480 --> 00:18:14,960
in sepsis, every hour is daily is associated with increased risk of associated with significant

177
00:18:14,960 --> 00:18:19,920
increase in mortality and we've been able to meet very significantly impact how earlier these

178
00:18:19,920 --> 00:18:26,880
patients are getting treatment. And so that's an area that you've worked very closely and

179
00:18:26,880 --> 00:18:34,320
are you able to give us a sense for more broadly, you know, where are their pockets of success?

180
00:18:34,320 --> 00:18:41,520
I mean, you know, we over the past few years or or many years at this point, you know, we've

181
00:18:41,520 --> 00:18:48,880
gone through the waves of, oh, hey, we, you know, I can read, can read x-rays better than radiologists.

182
00:18:48,880 --> 00:18:55,520
So, you know, we're done there, you know, x-rays can identify, you know, cancer and biopsies,

183
00:18:55,520 --> 00:18:58,640
you know, we're done there. But, you know, when you talk to folks in the industry,

184
00:19:00,080 --> 00:19:05,760
you know, we're far far away from done. How is ours? Excellent, excellent, excellent point.

185
00:19:05,760 --> 00:19:10,480
So this is so hard as a researcher in the field, because I get to watch those headlines all the time.

186
00:19:10,480 --> 00:19:17,520
One of the big, big, so it's sort of the innovation has gone through phases. The first phase was

187
00:19:18,640 --> 00:19:24,000
hubris. We went in, there were people who went in and were like, we can do everything, because they

188
00:19:24,000 --> 00:19:28,720
like can do everything and anything, so it can do everything. They underestimated how hard medicine

189
00:19:28,720 --> 00:19:36,640
healthcare is. So that was hubris. Then phase two, a renewed pack of researchers who came in

190
00:19:36,640 --> 00:19:44,160
wiser and went in and really rolled up their sleeves dug in deep and came up with methods that

191
00:19:44,160 --> 00:19:49,600
actually work for this kind of data, integrating domain knowledge, causal reasoning,

192
00:19:50,240 --> 00:19:55,680
thinking about safety, reliability, actionability, that sort of thing. So then the next phase was

193
00:19:55,680 --> 00:20:01,520
a sequence of methods that were better, higher quality, which resulted in these kinds of headlines

194
00:20:01,520 --> 00:20:08,000
you've seen where they're doing evaluation studies in the lab where they say, okay, let's compare

195
00:20:08,000 --> 00:20:13,520
how the software does to how a human expert would do either by looking at historically

196
00:20:14,400 --> 00:20:20,000
on a population, what the human did in comparing the software in the background or putting them in

197
00:20:20,000 --> 00:20:25,200
front of the software and see if they would change their mind. What's new now is phase three,

198
00:20:25,200 --> 00:20:30,880
where basically we've gone from experiments in the lab to the kind of example I'm talking about in

199
00:20:30,880 --> 00:20:37,120
the last three years where we've now deployed in real life settings where providers are actually

200
00:20:37,120 --> 00:20:43,520
using these software and actually making decisions with it. That's been very hard to come to and

201
00:20:43,520 --> 00:20:47,520
it's been a long you know a long time coming and that's why this is so exciting to be able to get

202
00:20:47,520 --> 00:20:55,600
to a place where VC providers adopting VC providers in engaging, interacting and actually a changing

203
00:20:55,600 --> 00:20:59,760
practice in a meaningful way. So I think this phase three is going to be extremely exciting because

204
00:20:59,760 --> 00:21:07,600
we're going to see more and more. So through Bayesian for example we've applied sort of a platform

205
00:21:07,600 --> 00:21:13,200
that we've built which provides basically these kinds of real-time signals to empower providers

206
00:21:13,200 --> 00:21:18,160
to catch life threatening complications early to save lives and we've done this in a number of

207
00:21:18,160 --> 00:21:24,640
different clinical areas and my sense is like just like we've done it there are a couple of other

208
00:21:24,640 --> 00:21:29,600
you know there are other groups around the country now there are also sort of like in imaging some

209
00:21:29,600 --> 00:21:33,840
of these early results you saw where people are like I have software can do better in some of these

210
00:21:33,840 --> 00:21:38,640
areas people have already started operationalizing it. So there's this in diabetic orthinopathy

211
00:21:38,640 --> 00:21:46,000
which is an area where you know often diagnosis is missed because patients go to the prime and the

212
00:21:46,000 --> 00:21:52,640
question was can the automate the diagnosis or screening of diabetic orthinopathy with primary care

213
00:21:52,640 --> 00:21:58,480
providers and so there are groups now that have built software that gets deployed at primary

214
00:21:59,280 --> 00:22:03,280
physician's office that can be used for screening automated screening and then if they're at

215
00:22:03,280 --> 00:22:08,080
high risk they're sent to a specialist. So that's sort of an example there's already now in white

216
00:22:08,080 --> 00:22:14,080
spread use and and in terms of billing and coding and reimbursement which is sort of what you alluded

217
00:22:14,080 --> 00:22:19,200
to earlier that's now starting to happen for some of the treatments. So there are like a couple

218
00:22:19,200 --> 00:22:25,840
different treatments are already where these AI type screening diagnostic workflow is tools

219
00:22:26,480 --> 00:22:32,720
where you know they're getting reimbursed today by either from the health system paying for it

220
00:22:32,720 --> 00:22:38,800
itself or the insurance company is paying for it for the use of it. Yeah yeah and that ends up

221
00:22:38,800 --> 00:22:45,120
being a big accelerator for innovation in the space is the impression I'm under. You always have

222
00:22:45,120 --> 00:22:51,760
to understand at the end of the day if you you can do things to make you know this was for me one

223
00:22:51,760 --> 00:22:59,840
of the most rude of evenings and you know like back in 2011 I sort of thought well doesn't it make

224
00:22:59,840 --> 00:23:05,520
so much sense we could save lives would not be enough but the reality is that's not enough that's

225
00:23:06,400 --> 00:23:10,480
the way a provider would see it is or a physician would see it is there are so many opportunities

226
00:23:10,480 --> 00:23:15,600
for saving lives you need to solve more than one problem for me need to help me save lives but you

227
00:23:15,600 --> 00:23:20,320
also need me to help me do other things you need to save me time you need to make my job easier a

228
00:23:20,320 --> 00:23:26,960
health system administrator will say you need to help us cut costs you need to help us improve our

229
00:23:27,760 --> 00:23:33,760
you know reduce penalties that we get so this is where deep marriage of the domain and the

230
00:23:33,760 --> 00:23:39,280
financial system and how it works and then marrying that to where the use cases are where there's

231
00:23:39,280 --> 00:23:44,560
real opportunity for adoption near term and then obviously long term this is where policy plays

232
00:23:44,560 --> 00:23:50,240
a role again right as we see more examples like this it's not my policies frozen there's always

233
00:23:50,240 --> 00:23:56,320
opportunity for new policies to get adopted that incentivize the use of these kinds of technology

234
00:23:56,320 --> 00:24:02,320
so for instance healthcare historically has been pretty reactive which means you know when a

235
00:24:02,320 --> 00:24:06,960
problem happens you show up I look at what's happening and I fix it I think where AI can make a

236
00:24:06,960 --> 00:24:12,800
big difference done right is moving it from being reactive to proactive we can fork we can look at

237
00:24:12,800 --> 00:24:17,840
your data in a granular way we can forecast we can make it possible for you to anticipate these

238
00:24:17,840 --> 00:24:24,240
complications and act on a timely way that is pretty exciting but today in some scenarios

239
00:24:25,040 --> 00:24:31,440
moving to proactive care might actually reduce the amount systems are getting paid which means

240
00:24:31,440 --> 00:24:37,680
there's a natural financial barrier to the adoption of these kinds of technologies that is also changing

241
00:24:37,680 --> 00:24:42,080
this is you know awareness that's coming people are becoming aware you know there are new

242
00:24:43,440 --> 00:24:49,200
financial models that are coming you know new sets of financial models like

243
00:24:49,200 --> 00:24:55,440
band of payments and value-based care where there's incentive for systems to be more

244
00:24:55,440 --> 00:25:02,480
more focused on preventative care proactive care and that all of that will also mean more

245
00:25:02,480 --> 00:25:12,160
opportunities for AI to impact lives got it got it you in kind of describing this phase two you

246
00:25:12,160 --> 00:25:18,960
rattled off a handful of methodological changes improvements that have happened over the

247
00:25:18,960 --> 00:25:26,480
past few years it seems like an interesting area to maybe dig into a little bit deeper so that folks

248
00:25:26,480 --> 00:25:31,040
you know that are thinking about entering the space have some ideas for the the way that they

249
00:25:31,040 --> 00:25:38,320
need to approach problems in the space can you elaborate on what some of the big you know differences

250
00:25:38,320 --> 00:25:43,360
that you've seen and you know the way folks need to approach machine learning problems in

251
00:25:43,360 --> 00:25:52,480
healthcare nowadays yeah so one of the very very big differences is sort of in some of the other

252
00:25:52,480 --> 00:25:58,720
areas there's this notion of like you have really good gold standards and really clear evaluation

253
00:25:58,720 --> 00:26:04,720
metrics so for instance you could go into face recognition say and maybe there's a very nice

254
00:26:04,720 --> 00:26:10,800
data set where everybody sat down and everybody can agree this person is this person and that person

255
00:26:10,800 --> 00:26:16,560
that person doesn't so much debate about it and you can and if your goal is to do face recognition

256
00:26:16,560 --> 00:26:24,640
or face detection the error metric is pretty clear so you can go in get a data set it's well annotated

257
00:26:24,640 --> 00:26:30,480
there's in a whole lot of disagreement and you have a clear metric to optimize and then people can

258
00:26:30,480 --> 00:26:37,600
go to town with all the creative ideas for optimizing that metric there are so many ways in which

259
00:26:37,600 --> 00:26:45,520
health data sets are not bad so for example in most clinical areas the notion of like what is

260
00:26:45,520 --> 00:26:51,360
the goal standard and what is the metric you're optimizing for is very unclear so as so when we first

261
00:26:51,360 --> 00:26:57,600
frame the successio a success early detection problem the question was well how early do you want

262
00:26:57,600 --> 00:27:04,000
to detect it because if it's too early maybe providers won't recognize it but if it's too late but

263
00:27:04,000 --> 00:27:10,160
that's not very productive so that's one example the second example okay what is sepsis that's

264
00:27:10,160 --> 00:27:16,320
an existential question people will sit down and debate like this person was treated for sepsis

265
00:27:16,320 --> 00:27:22,640
because they likely were septic but somebody else might say yeah but this person was being a bit

266
00:27:22,640 --> 00:27:28,400
conservative and treating them so what do you do do you treat that person as septic or not septic

267
00:27:28,400 --> 00:27:35,680
or do you treat it so how do you think about that third you could take data set from one hospital

268
00:27:35,680 --> 00:27:41,920
or one health system and you could learn a model that is very good at predicting there but as soon

269
00:27:41,920 --> 00:27:47,440
but you know we've written numerous papers on the topic like when you move it to a different hospital

270
00:27:47,440 --> 00:27:52,800
if you have these big rich deep models that are very flexible can learn anything they can easily

271
00:27:52,800 --> 00:27:57,920
pick up patterns that are very specific to how people practice in that hospital when you go to a

272
00:27:57,920 --> 00:28:03,120
different hospital that method may not generalize at all in fact there are papers showing you know

273
00:28:03,120 --> 00:28:11,760
certain methods are very brittle easily break as you shift the underlying data and and you want

274
00:28:11,760 --> 00:28:19,440
methods that are robust and to these kinds of shifts so can we so we need almost sort of like new

275
00:28:19,440 --> 00:28:26,160
class of reliable learning methods or you know ship stable methods like they have a number of

276
00:28:26,160 --> 00:28:31,440
different names in the field but basically methods that like where if there are new sense things

277
00:28:31,440 --> 00:28:36,400
that change in the data they're not going to actually impact the quality of the learning system

278
00:28:37,040 --> 00:28:43,760
or differently put up front you can get guarantees that if certain types of new sense changes

279
00:28:43,760 --> 00:28:48,880
happen they're not actually going to hurt the software's performance in an unpredictable way

280
00:28:48,880 --> 00:28:54,160
which is something that's very important in applications like you know social impact applications

281
00:28:54,160 --> 00:29:01,760
right where it's often a question of life as opposed to say like advertising breaks by contrast

282
00:29:03,120 --> 00:29:07,680
I could talk about a number of other methodological issues but you know all of these like

283
00:29:08,640 --> 00:29:15,680
how do you like health data are like so messy so messy and there's a lot of messiness how do you take

284
00:29:15,680 --> 00:29:24,640
into account the like by tackling the messiness and the messiness and measurement models in an

285
00:29:24,640 --> 00:29:32,800
intelligent way you really can show 200 300 percent improvements in precision or sensitivity so

286
00:29:33,680 --> 00:29:39,760
so yeah I think yeah I was actually going to ask about that because in the in the setups all

287
00:29:39,760 --> 00:29:46,960
this you described high tech and the introduction of electronic medical records as kind of opening up

288
00:29:46,960 --> 00:29:55,360
this you know huge opportunity but I still hear from folks that you know as much as you know we

289
00:29:55,360 --> 00:30:04,320
digitized the data is still very very messy very dirty and that remains a huge constraint in this

290
00:30:04,320 --> 00:30:10,720
particular set of applications I'm just curious so there if you can elaborate on that and what

291
00:30:10,720 --> 00:30:18,160
kinds of examples you can give us to help us understand the state of healthcare data yeah so I

292
00:30:18,160 --> 00:30:24,160
think healthcare data is definitely far more messy than any other domain I've ever worked with

293
00:30:26,240 --> 00:30:30,640
the the way I think about it is there are certain things you can do with it and there are other

294
00:30:30,640 --> 00:30:35,440
things you can't do with it which is why it's even more important to have deep expertise in understanding

295
00:30:35,440 --> 00:30:39,920
the data the complexity of the data but also the problems you're looking to solve with it to

296
00:30:39,920 --> 00:30:45,120
understand the risk profile right just like in a drug there's a notion of risk benefit analysis

297
00:30:45,760 --> 00:30:51,280
right almost everything comes with like a some kind of side effect so how is it getting used on

298
00:30:51,280 --> 00:30:55,440
home what's the side effect what's the benefit and there's a risk benefit trade off so in the same

299
00:30:55,440 --> 00:31:02,320
vein in terms of how data sets there are some applications where today's technology is just

300
00:31:02,320 --> 00:31:10,960
not there there are other applications where today's technology isn't there but you could build

301
00:31:10,960 --> 00:31:16,640
the right technology to improve it and yet other applications where no amount of amazing

302
00:31:16,640 --> 00:31:22,160
technology can help you because the information doesn't exist so I think there's a there's

303
00:31:22,160 --> 00:31:26,880
um certainly a number of problems where the information doesn't exist and we just need new

304
00:31:26,880 --> 00:31:32,880
modalities but the vast majority of problems are of the kinds of problems where the data exists

305
00:31:33,520 --> 00:31:39,440
because today human experts like physicians nurses care team members are looking at this data

306
00:31:39,440 --> 00:31:45,040
and making decisions right so the data exists and there's an opportunity to leverage the data in a

307
00:31:45,040 --> 00:31:51,440
much more intelligent way to be able to so it's all to be able to improve the quality of decision

308
00:31:51,440 --> 00:31:56,720
making and outcomes right so it's like a human expert is looking at it yeah and they're making

309
00:31:56,720 --> 00:32:02,160
decisions so whether you like it or not it's happening and so now the question is how amenable

310
00:32:02,160 --> 00:32:09,280
are these and the way and I think that's where we need the right kind of um machine learning

311
00:32:09,280 --> 00:32:14,560
AI technologies that are you know humble where like the researchers building these tools are humble

312
00:32:14,560 --> 00:32:20,240
they understand the difficulty of it and they're intelligently approaching you know which problems

313
00:32:20,240 --> 00:32:26,960
too tackle and then doing very careful evaluation so I guess another team here is evaluation so

314
00:32:26,960 --> 00:32:33,760
because this area is hard and correctness is so crucial I almost feel like unlike other fields where

315
00:32:34,880 --> 00:32:42,320
people spend you know 10 units of time like 90% of the time developing a model and then 10 minutes

316
00:32:42,320 --> 00:32:48,160
writing it up here it's the flip it's like whatever time you might spend the model you need to

317
00:32:48,160 --> 00:32:53,280
spend 9x that much more time triangulating in many many ways to get to a place where you know it works

318
00:32:54,080 --> 00:33:02,160
yeah and and that makes this really hard so um yeah so I think I think it's very promising I think

319
00:33:02,160 --> 00:33:07,360
the data are exciting I think the loads of opportunities is just uh it requires more patience

320
00:33:08,000 --> 00:33:15,440
more thoughtfulness more carefulness uh maybe back to methodology your you've named your company

321
00:33:15,440 --> 00:33:23,840
Bayesian health uh you mentioned causality uh in that list of um tools yeah talk about the role of

322
00:33:23,840 --> 00:33:32,480
causality and and um maybe by extension the approach that Bayesian is taking yeah so um when they hear

323
00:33:32,480 --> 00:33:40,400
the name Bayesian they often think oh is it only using Bayesian methodologies um so um so let me just

324
00:33:40,400 --> 00:33:47,120
sort of first quickly explain that which is um just like any smart human when we have a lot of

325
00:33:47,120 --> 00:33:54,800
different data coming at us we integrate it over time to uh to update our view of what we think

326
00:33:54,800 --> 00:33:59,920
is happening that's how the best physicians practice right they're continuously doing new tests

327
00:33:59,920 --> 00:34:04,640
they're integrating the new piece of information coming in they're doing uncertainty quantification

328
00:34:04,640 --> 00:34:08,000
they're thinking about how certain and uncertain they are about the different pieces of information

329
00:34:08,000 --> 00:34:13,280
coming in and putting it all together to come up with the forecast which updates as new information

330
00:34:13,280 --> 00:34:18,640
arrives that's a very Bayesian way of thinking so it's that's basically so the Bayes the Bayesian

331
00:34:18,640 --> 00:34:23,760
health the name comes from the idea of building intelligence software that gives you the ability

332
00:34:23,760 --> 00:34:30,880
to do that with largely with large scale health data in terms of the kinds of techniques it's it's

333
00:34:30,880 --> 00:34:38,800
really um a comment you know so you ask me causal inference so when we have today for these models

334
00:34:38,800 --> 00:34:47,040
to be able to be um intelligible and actionable it's so important for it to not capture

335
00:34:47,040 --> 00:34:55,200
spurious correlations or spurious dependencies that are almost like hurt the user trust

336
00:34:55,200 --> 00:35:02,160
so in some sense that's where knowledge of the domain the data generating process and as a result

337
00:35:02,160 --> 00:35:08,640
uh techniques from causal inference are really helpful more more than in in being able to build

338
00:35:08,640 --> 00:35:14,800
models that are going to be more intelligible and actionable turns out these models are also more

339
00:35:14,800 --> 00:35:22,800
transportable or you know more likely to generalize as you go across sites or across uh you know even

340
00:35:22,800 --> 00:35:27,760
within the same site across time where you know data collection methodologies might change or practice

341
00:35:27,760 --> 00:35:33,600
patterns might change and so on and so forth by making models that aren't just learning memorizing

342
00:35:33,600 --> 00:35:40,640
what's in the data but it's reasoning about what if like if this were to happen then what if that

343
00:35:40,640 --> 00:35:48,640
were to happen then what i'll give you a simple example there was a really nice um paper a few years

344
00:35:48,640 --> 00:35:55,600
ago where it's a very well cited paper now where um this team of researchers were trying to learn

345
00:35:55,600 --> 00:36:01,040
a model for predicting patients who came in the pneumonia into a hospital emergency department

346
00:36:01,600 --> 00:36:06,640
their risk profile with the idea that if they were high risk they would place them in the intensive

347
00:36:06,640 --> 00:36:11,520
care unit which is the sicker unit or like the high acuity unit right like where the sickest patients

348
00:36:11,520 --> 00:36:17,200
go and then if they were lower risk maybe they would go to the floor and uh they took historical

349
00:36:17,200 --> 00:36:22,400
data sets and they learned using like you know classical supervised learning techniques a model

350
00:36:22,400 --> 00:36:27,520
and what that model did is looked at retrospective data and said okay great if the patient came in

351
00:36:27,520 --> 00:36:34,400
and they died then that's my training data for high risk if the patient survived that's my training

352
00:36:34,400 --> 00:36:39,440
data for not high risk and then based on that they learned a model and then what the model learned

353
00:36:39,440 --> 00:36:46,800
was turns out patients who had pneumonia with asthma were actually lower risk than patients with

354
00:36:46,800 --> 00:36:53,760
just pneumonia which is uh for totally counterintuitive because you know asthma complicates the case

355
00:36:53,760 --> 00:36:59,920
quite a bit and actually you tend to have poor outcomes now when they went and looked in the data

356
00:36:59,920 --> 00:37:04,320
they realize actually the reason that was happening is because the people with pneumonia and asthma

357
00:37:05,280 --> 00:37:10,080
were getting escalated to the intensive care unit versus the people with just pneumonia

358
00:37:10,080 --> 00:37:14,720
were on the floor but in the intensive care unit they were just getting constant monitoring,

359
00:37:14,720 --> 00:37:21,680
constant supervision, constant care which meant when you looked at the resulting models all it was

360
00:37:21,680 --> 00:37:28,000
really like it was ignoring the fact that this person had been so you know like what if what the

361
00:37:28,000 --> 00:37:32,560
model should have done is what would it would this person have been high risk if I didn't send

362
00:37:32,560 --> 00:37:37,680
them to the ICU would this person be high risk if I went exactly you want to do counterfactual

363
00:37:37,680 --> 00:37:43,600
reasoning so that's sort of an area where we you know very early on he's you know started realizing

364
00:37:43,600 --> 00:37:49,680
and with health data sets where you really are trying to reason about a patient's risk profile

365
00:37:49,680 --> 00:37:52,800
you really want to I mean this isn't just pertinent to health this is just pertinent across the

366
00:37:52,800 --> 00:37:58,560
board when you're looking at any kind of temporal or sequential decision making problem or decision

367
00:37:58,560 --> 00:38:02,720
making problem you want to ask you know under different interventions what would the trajectory

368
00:38:02,720 --> 00:38:08,320
have looked like so you can basically figure out what's the right thing to do and the question

369
00:38:08,320 --> 00:38:13,680
therefore one should be asking when developing these models is what would this person's risk profile

370
00:38:13,680 --> 00:38:19,680
be had they not been to the ICU as opposed to what's the risk profile ignoring what was done to

371
00:38:19,680 --> 00:38:26,800
them right right so that's sort of a very simple example of a place where you know how machine like

372
00:38:26,800 --> 00:38:33,440
the next trend these the types of machine learning approaches we're using embrace causality

373
00:38:33,440 --> 00:38:39,440
in order to be able to get more sensical models that are both more accurate more actionable but also

374
00:38:39,440 --> 00:38:49,680
more transportable and safe so that by the time this show is out in the wild and folks are listening

375
00:38:49,680 --> 00:38:57,360
to it Bayesian health will be announced and released and as part of that you're releasing a study

376
00:38:57,360 --> 00:39:01,200
can you tell us a little bit about the study that you're that will be published by the time folks

377
00:39:01,200 --> 00:39:11,440
yeah this yeah yeah so we're releasing a copy of a manuscript where basically starting in April

378
00:39:11,440 --> 00:39:18,960
2018 across five different hospital sites we built and deployed the software for early detection

379
00:39:18,960 --> 00:39:27,040
of sepsis and making it possible for care teams specifically providers and nursing staff to be able

380
00:39:27,040 --> 00:39:32,640
to get access to these real-time machine learning inferences entirely within the workflow

381
00:39:32,640 --> 00:39:38,080
to flag patients who are high risk and then make it possible and and so you know it we provided

382
00:39:38,080 --> 00:39:43,360
these dynamic workflows within the EMR so EMR is the electronic medical record it's the infrastructure

383
00:39:43,360 --> 00:39:49,840
the system they use for documenting for recording you know it's sort of the platform that they

384
00:39:49,840 --> 00:39:55,680
you know care team like providers spend the vast majority of the time in and so what we did is

385
00:39:55,680 --> 00:40:02,480
provided this real-time software that like pulls in in the background in real-time crunches it

386
00:40:03,360 --> 00:40:08,880
puts information back into the EMR into the provider's workflow flags a patient when they're at

387
00:40:08,880 --> 00:40:14,240
risk makes it very easy for the provider to come in and see why were they flagged more context

388
00:40:14,240 --> 00:40:20,240
around the patient in terms of you know what was the risk for certain comorbidities certain

389
00:40:20,240 --> 00:40:27,280
um certain adverse events and then also like what are the indicators that they could quickly look at

390
00:40:27,840 --> 00:40:33,520
and um now they look at it they evaluate and then if they agree it septic they treat it

391
00:40:33,520 --> 00:40:38,960
and what the study does is analyzes um and it's sort of a first study of a guy in the sense that

392
00:40:38,960 --> 00:40:44,800
it's large it's analyzing physician adoption factors impacting physician adoption which is

393
00:40:44,800 --> 00:40:51,120
really crucial for the purpose of building future CDS tools that have chance of getting increased

394
00:40:51,120 --> 00:40:58,400
adoption and also like yes sorry clinical decision support so any kind of like AI driven decision

395
00:40:58,400 --> 00:41:04,320
support decision augmentation tools and it's impact on outcomes and so what we find is one

396
00:41:05,360 --> 00:41:10,880
89% of the so it was a you know upwards of nearly 500,000 patients was screened through the

397
00:41:10,880 --> 00:41:18,960
software 10,000 cases of sepsis were analyzed and then providers when the software flagged a patient

398
00:41:20,480 --> 00:41:27,840
90% of the times or 89% to be precise providers came in you know it was a passive tool but they

399
00:41:27,840 --> 00:41:36,160
sorted out interacted with it put in an evaluation and then we found that basically when providers did

400
00:41:36,160 --> 00:41:43,200
that in patients on whom providers came in and entered in evaluation and treated them a 1.9

401
00:41:43,200 --> 00:41:49,200
hour median difference like so for the in the meeting case 9 1.9 hour difference in

402
00:41:50,080 --> 00:41:56,160
movement and treatment timing so and then this obviously has subsequent impact in terms of

403
00:41:57,040 --> 00:42:04,640
reductions in mortality, morbidity, length of stay, this manuscript only talks about

404
00:42:04,640 --> 00:42:08,480
impact on clinical treatment practice and the key clinical metrics people care about like time to

405
00:42:08,480 --> 00:42:14,160
antibiotics the next set of manuscripts will release will be more on clinical also discussing

406
00:42:14,160 --> 00:42:18,720
then verifying like the mortality impact and morbidity impact and then one of the interesting

407
00:42:18,720 --> 00:42:25,280
things was in sepsis being able to recognize it precisely is just really really hard and like

408
00:42:26,000 --> 00:42:31,040
what we were able to see in our study is like with very high sensitivity we were able to detect

409
00:42:31,040 --> 00:42:40,960
cases but also you know the like you know like the 10x higher precision than typically is seen

410
00:42:40,960 --> 00:42:45,920
with widely available software we were able to like help them you know like reduce false

411
00:42:45,920 --> 00:42:50,720
alerting quite a bit by basically improving the precision right so like in our study one in three

412
00:42:50,720 --> 00:42:56,000
cases were confirmed as septic and you know it's a needle in a haystack problem only a small

413
00:42:56,000 --> 00:43:00,560
number of cases in a given day gets sepsis so it's really a problem of like how do we go from

414
00:43:00,560 --> 00:43:08,000
a hundred cases to the three or four we need to worry about yeah yeah I'm curious about the

415
00:43:08,000 --> 00:43:13,840
adoption side of things what type or level of engagement did you have with the physicians who

416
00:43:13,840 --> 00:43:19,040
were you know ultimately gained access to did they gain access to the tool passively where they

417
00:43:19,040 --> 00:43:24,320
actively onboarded what goes into getting a physician on board and what have you learned about

418
00:43:24,320 --> 00:43:32,240
that process yeah so actually I think so we very actively onboarded them when we initially launched

419
00:43:32,240 --> 00:43:37,600
it in 2018 it was super funny we built it we were like all the papers here it shows it works

420
00:43:38,240 --> 00:43:45,360
the data's there we built it we integrated it we launched it and then literally two physicians

421
00:43:45,360 --> 00:43:51,520
used it at this particular site and it was so disheartening because I think that was like end of

422
00:43:51,520 --> 00:43:57,040
2017 early 2018 it was sometime around then I can't remember maybe sometime in 2017 it feels like

423
00:43:57,040 --> 00:44:01,680
forever ago but like I just remember distinctly like we launched it we thought it was going to be

424
00:44:01,680 --> 00:44:10,240
such a big deal and it was so sad when we were monitoring the data coming in to see literally like

425
00:44:10,960 --> 00:44:16,400
nobody was using it or like a small number of people who were deeply involved in the development

426
00:44:16,400 --> 00:44:22,320
were using it so we had to do a lot of work to get it from like machine learning researchers and

427
00:44:22,320 --> 00:44:29,040
engineers and launching a piece of software to people who understand human machine teaming and

428
00:44:29,040 --> 00:44:35,680
collaboration and workflow integration and design and you know like that gap needed to be closed

429
00:44:36,240 --> 00:44:40,640
and then part of that is also how you launched the software and in medicine this is really

430
00:44:40,640 --> 00:44:46,720
important because it's part of the trust building process so we basically partnered with champions

431
00:44:46,720 --> 00:44:53,920
in local sites to basically create materials that was really easy for them to read and you know

432
00:44:53,920 --> 00:45:00,000
gave very and the tool was designed to be very intuitive easy to use and then and it's very simple

433
00:45:00,000 --> 00:45:06,560
but basically making it so that we could basically have a little video where they could see

434
00:45:06,560 --> 00:45:12,000
understand how to use it and then we also had a whole infrastructure for monitoring engagement

435
00:45:12,000 --> 00:45:18,320
adoption which was super crucial because if you can monitor then we could understand like you know

436
00:45:18,320 --> 00:45:22,160
both in terms of the health of the models itself and how they were working as we scaled across

437
00:45:22,160 --> 00:45:27,600
sites but also like what was used looking like where there were barriers and how we could mitigate

438
00:45:27,600 --> 00:45:33,600
those barriers and so we partnered with our champions to be able to then close barriers for adoption

439
00:45:33,600 --> 00:45:37,600
right and in some and these were very barrier barriers some around software some around the

440
00:45:37,600 --> 00:45:43,440
perception some around the understanding and we took different approaches to closing these barriers

441
00:45:44,160 --> 00:45:50,720
based on you know what we learned awesome awesome um maybe to wrap things up you can share

442
00:45:50,720 --> 00:45:57,760
a little bit about where you think the field is headed yeah um I mean I'm super it's I'm super

443
00:45:57,760 --> 00:46:02,240
excited about where I think the next five years will be in this field I think we're really at a

444
00:46:02,240 --> 00:46:07,520
place now where you know the data exists the infrastructure exists the ability to deliver these

445
00:46:07,520 --> 00:46:12,560
inferences within workflow exists and the ability and our experiments and papers show the ability

446
00:46:12,560 --> 00:46:18,160
for you know and providers are willing to engage and they're engaging and we can have meaningful

447
00:46:18,160 --> 00:46:24,240
impact on practice so to me the next five years is about leveraging this whole stack to apply

448
00:46:24,240 --> 00:46:29,520
this thoughtfully in a number of other clinical areas and start doing really thoughtful evaluations

449
00:46:29,520 --> 00:46:35,520
like one of the very big things that's been missing is because there are no evaluations people don't

450
00:46:35,520 --> 00:46:40,160
know if something is working and if they don't know something is working they they are not going to

451
00:46:40,160 --> 00:46:46,800
adopt it and trust it and medicine this is such a key currency for anything so today a lot of

452
00:46:46,800 --> 00:46:52,720
software and medicine software based tools people are just people deployed but they haven't had the

453
00:46:52,720 --> 00:46:59,120
infrastructure to really evaluate measure efficacy um but in you know as we've sort of worked in this

454
00:46:59,120 --> 00:47:04,080
area and I think um you know we've we've developed in front of monitoring so to me the next five

455
00:47:04,080 --> 00:47:10,160
years is we others in the field release more and more studies showing efficacy that now accelerates

456
00:47:10,160 --> 00:47:15,040
adoption learning around what increases adoption and then obviously if you have a good quality

457
00:47:15,040 --> 00:47:18,000
intervention that is precise and gets adoption you're going to get outcomes.

458
00:47:19,520 --> 00:47:24,960
Awesome awesome. Well Suci thanks so much for joining us and sharing a bit about what you're

459
00:47:24,960 --> 00:47:33,680
up to and congrats on the launch of the company. It's been great to chat with you. Yeah like

460
00:47:33,680 --> 00:47:38,240
why Sam I'm so glad we finally got to catch up and hopefully the next time we speak a few years

461
00:47:38,240 --> 00:47:43,200
from now I'll have much more good news for you in terms of the number of different clinical areas

462
00:47:43,200 --> 00:47:57,360
that are already being impacted by AI in that real time. Fantastic thank you.

