WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:24.040
I'm your host Sam Charrington.

00:24.040 --> 00:29.760
Last week I spent some time at CES, the consumer electronics show in Las Vegas, exploring

00:29.760 --> 00:36.360
the vast sea of drones, cameras, paper thin TVs, robots, laundry folding closets and other

00:36.360 --> 00:37.760
smart devices.

00:37.760 --> 00:38.840
You name it?

00:38.840 --> 00:40.160
It was there.

00:40.160 --> 00:44.920
Of course, I was also able to sit down with some really interesting folks working on some

00:44.920 --> 00:48.360
pretty cool AI-enabled products.

00:48.360 --> 00:52.400
Head on over to our YouTube channel to check out some behind-the-scenes footage from my

00:52.400 --> 00:55.680
interviews and other quick takes from the show.

00:55.680 --> 01:01.960
And beyond the look up for our AI and consumer electronics series right here on the podcast,

01:01.960 --> 01:03.200
coming soon.

01:03.200 --> 01:08.280
The show you're about to hear is part of a series of shows recorded at the rework Deep Learning

01:08.280 --> 01:11.760
Summit in Montreal back in October.

01:11.760 --> 01:17.760
This was a great event, and in fact, their next event, the Deep Learning Summit San Francisco,

01:17.760 --> 01:23.200
is right around the corner on January 25th and 26th, and will feature more leading researchers

01:23.200 --> 01:28.320
and technologists like the ones you'll hear on the show this week, including Ian Goodfellow

01:28.320 --> 01:33.920
of Google Brain and Daphne Kohler of Calico Labs and more.

01:33.920 --> 01:40.160
Definitely check out the event and use the code TwimmelAI for 20% off of registration.

01:40.160 --> 01:46.480
In this show, I speak with Greg Dymus, Senior Computer Systems Researcher at Baidu.

01:46.480 --> 01:50.160
Greg joined me before his talk at the Deep Learning Summit where he spoke on the next

01:50.160 --> 01:52.680
generation of AI chips.

01:52.680 --> 01:57.480
Greg's talk focused on some of the work his team was involved in that accelerate Deep Learning

01:57.480 --> 02:03.600
training by using mixed 16-bit and 32-bit floating point arithmetic.

02:03.600 --> 02:07.920
We cover a ton of interesting ground in this conversation, and if you're interested

02:07.920 --> 02:12.440
in systems-level thinking around scaling and accelerating Deep Learning, you're really

02:12.440 --> 02:14.080
going to like this one.

02:14.080 --> 02:19.520
Of course, if you like this one, you're also going to like TwimmelTalk number 14 with

02:19.520 --> 02:25.040
Greg's former colleague, Shubo Sengupta, which covers a bunch of related topics.

02:25.040 --> 02:29.160
If you haven't already listened to that one, I encourage you to check it out.

02:29.160 --> 02:32.320
And now on to the show.

02:32.320 --> 02:43.400
Hey everyone, I am here at the Rework Deep Learning Conference in Montreal, and I've got

02:43.400 --> 02:49.480
the pleasure to be seated across from Greg Dymus from Baidu, and he's a Senior Researcher

02:49.480 --> 02:50.480
there.

02:50.480 --> 02:52.680
Greg, welcome to this week in Machine Learning and AI.

02:52.680 --> 02:53.680
Thanks for having me.

02:53.680 --> 02:54.680
Awesome.

02:54.680 --> 02:58.120
Why don't we get started by having you tell us a little bit about your background and

02:58.120 --> 03:00.680
how you got interested in ML and AI?

03:00.680 --> 03:02.280
Sure, absolutely.

03:02.280 --> 03:05.880
My background has traditionally been in high performance computing.

03:05.880 --> 03:10.640
I've been really excited about building really fast processors for important applications

03:10.640 --> 03:14.120
that enable new applications that people can use.

03:14.120 --> 03:17.800
You know, it's actually kind of a strange story how I got to AI.

03:17.800 --> 03:19.960
I used to be an AI skeptic.

03:19.960 --> 03:20.960
Okay.

03:20.960 --> 03:22.520
That's always a good place to start.

03:22.520 --> 03:23.520
Yeah.

03:23.520 --> 03:28.960
I always felt like AI would be valuable, but I just felt like there's no way that simple

03:28.960 --> 03:35.360
algorithms like Stochastic gradient descent could ever solve these complex, you know, highly

03:35.360 --> 03:38.280
multi-dimensional optimization problems.

03:38.280 --> 03:44.360
And then at one point, I remember sitting at Nvidia Research and hearing a talk from

03:44.360 --> 03:47.720
Jan LeCoon and just realizing, oh, I was wrong.

03:47.720 --> 03:50.480
I was totally wrong.

03:50.480 --> 03:55.880
And immediately after that, I joined by your research, Andrew was founding the Silicon

03:55.880 --> 04:01.000
Valley AI lab and it seemed like a great opportunity to learn more about AI.

04:01.000 --> 04:05.360
And man, it's been such a crazy ride to get to this point.

04:05.360 --> 04:06.360
I bet.

04:06.360 --> 04:10.160
So what was your path to get to that point that you even had an opinion that involves

04:10.160 --> 04:11.640
Stochastic gradient descent?

04:11.640 --> 04:12.640
Oh, sure.

04:12.640 --> 04:13.640
I mean, well, let's see.

04:13.640 --> 04:15.640
I like building things that are useful for people.

04:15.640 --> 04:20.760
I feel like computing in general has enabled many, you know, new capabilities like the

04:20.760 --> 04:26.680
internet and like videos and, you know, so many things that we take for granted every

04:26.680 --> 04:29.240
day, but it really make our lives better.

04:29.240 --> 04:32.240
And I was always really passionate about making computers even better.

04:32.240 --> 04:33.240
Okay.

04:33.240 --> 04:36.840
It's kind of this belief that although you might not know it going into it, if you make

04:36.840 --> 04:41.000
a faster computer or more efficient computer, someone will find a way of building something

04:41.000 --> 04:43.440
amazing on top of that.

04:43.440 --> 04:47.280
And so I spent a lot of time looking at applications.

04:47.280 --> 04:50.560
What are the things that you can use computers to do?

04:50.560 --> 04:52.440
And AI was always there.

04:52.440 --> 04:53.440
Okay.

04:53.440 --> 04:57.720
Just the feeling with AI has, you know, for me before deporning was that it would just

04:57.720 --> 05:04.440
be too hard that a lot of existing theory was kind of steering you down and had all of

05:04.440 --> 05:07.440
these really difficult challenges.

05:07.440 --> 05:11.960
And, you know, I'd seen a lot of people, very smart people spend a lot of time trying

05:11.960 --> 05:17.240
to tackle those problems and not quite getting all the way there.

05:17.240 --> 05:24.080
So do you recall what was it about Jan's talk that kind of made the light bulb go off

05:24.080 --> 05:30.000
and made you realize that it's to cast a green and to send was the answer?

05:30.000 --> 05:31.000
Sure.

05:31.000 --> 05:36.520
So you can look at these hierarchical feature representations and covenants.

05:36.520 --> 05:41.800
So when people, you know, look at images, you can tell, it's the world is hierarchical.

05:41.800 --> 05:43.400
You can break a chair down into pieces.

05:43.400 --> 05:44.400
It has arms and legs.

05:44.400 --> 05:48.440
You can break those pieces down, you know, recursively.

05:48.440 --> 05:54.480
And there's a lot of existing work that provides some evidence that vision algorithms will

05:54.480 --> 05:58.520
do similar things for recognition tasks.

05:58.520 --> 06:01.920
That wasn't ever really a question.

06:01.920 --> 06:07.400
And people were able to build by hand, you know, things like feature detectors and these

06:07.400 --> 06:09.920
hierarchical systems that worked reasonably well.

06:09.920 --> 06:12.600
They were just very difficult to build.

06:12.600 --> 06:18.520
And the interesting thing about Jan's talk was that, you know, systems could do it automatically.

06:18.520 --> 06:23.400
I thought before that that you would get stuck in these, you know, intractable optimization

06:23.400 --> 06:29.160
problems where even if a solution exists, you know, it's one out of some enormously large

06:29.160 --> 06:35.640
number, like, you know, two to the power of 10 to the power of 30, like something amazingly

06:35.640 --> 06:36.640
big.

06:36.640 --> 06:40.200
Like, you know, you think about different sizes of numbers.

06:40.200 --> 06:43.440
Sometimes I think of the number of atoms in the universe as being a big number.

06:43.440 --> 06:46.240
This is far, far bigger than that.

06:46.240 --> 06:50.680
And you were thinking that that number represents the represents what?

06:50.680 --> 06:55.120
How many things you'd have to search through to find a good search base for your, okay?

06:55.120 --> 06:59.880
Yeah, it's like the needle in an enormous haystack, more atoms than there are in the universe.

06:59.880 --> 07:02.880
How could you ever possibly hope to search through it efficiently?

07:02.880 --> 07:03.880
Wow.

07:03.880 --> 07:09.520
Actually, with these very simple algorithms, but, you know, reliably, I've seen since

07:09.520 --> 07:14.160
then for one application after another, for image recognition, for speech recognition,

07:14.160 --> 07:20.080
for synthesis, for language understanding, it works very reliably.

07:20.080 --> 07:24.400
Did you happen to catch any of Jeff Hinton's talk about this?

07:24.400 --> 07:28.680
What do you think about his kind of post SGD capsule?

07:28.680 --> 07:33.840
Well, it's not post SGD, actually, the starting point is that SGD is really the only

07:33.840 --> 07:36.680
thing that we know works, right?

07:36.680 --> 07:41.600
But it's more post kind of the traditional model of the neuron.

07:41.600 --> 07:42.600
Yeah.

07:42.600 --> 07:50.520
I almost think of it as like post-convenants, and, you know, one thing we've realized recently

07:50.520 --> 07:56.320
is just there is a lot of complexity in modeling, that while we like to think of deep warning

07:56.320 --> 08:01.520
as a general purpose learning algorithm, as you sort of playing it to different applications,

08:01.520 --> 08:06.080
like my experience has been spending a lot of time applying it to speech recognition.

08:06.080 --> 08:14.680
And you do get some benefits from more data and from some general purpose aspects of the

08:14.680 --> 08:19.080
learning algorithm to the extent that it's robust to different speakers or different

08:19.080 --> 08:24.720
variations in different environments, but you also get a lot of benefits from specialization.

08:24.720 --> 08:29.440
So finding the right neural network architecture seems like it matters a lot.

08:29.440 --> 08:34.560
And as we look into details for different applications, as we spend time tuning your

08:34.560 --> 08:40.080
own network architectures for different applications, you see very different structures emerge.

08:40.080 --> 08:46.560
So it wouldn't surprise me at all if there is a more efficient, more general purpose structure

08:46.560 --> 08:49.440
for vision than covenants.

08:49.440 --> 08:50.440
Yeah.

08:50.440 --> 08:56.520
One of the really helped me see that was a blog post by Stephen Merritti a while ago at

08:56.520 --> 09:01.560
May, be almost a year ago at this point, but he talked about network architecture being

09:01.560 --> 09:03.840
the new feature engineering.

09:03.840 --> 09:08.520
Hinton Salk was post-comments, but he did start it off by talking about, like calling

09:08.520 --> 09:14.360
into question the basic neuron structure, but he didn't necessarily, he did kind of pivot

09:14.360 --> 09:19.240
to talking about the network architecture at a higher level, right?

09:19.240 --> 09:26.960
Was there a piece in there where he suggested what might be the kind of successor to the

09:26.960 --> 09:30.000
traditional neuron architecture?

09:30.000 --> 09:36.160
I think it's about this concept of capsules, which might be groups of cooperating neurons.

09:36.160 --> 09:41.760
And then the way that they cooperate together might be more, more complex.

09:41.760 --> 09:46.080
Let's see, I feel like from a computational perspective, it's actually hard to get away

09:46.080 --> 09:51.600
from the formulation of neurons that we have as the basic building block where even if there

09:51.600 --> 09:57.720
is something that's algorithmically more efficient or more well matched to the problem, the computational

09:57.720 --> 10:02.640
building blocks that we have have been so highly tuned that if you made a very substantial

10:02.640 --> 10:08.320
change, it might be better kind of at the algorithm level, but it might be very inefficient

10:08.320 --> 10:11.080
on the type of computer that we know how to build today.

10:11.080 --> 10:17.200
It breaks this whole ecosystem that we've built up around this traditional way of building

10:17.200 --> 10:20.520
neurons and networks and solving them.

10:20.520 --> 10:25.800
Yeah, so much of the existing technologies are built on top of harder support and also

10:25.800 --> 10:30.280
algorithm support for linear algebra, like dense linear algebra.

10:30.280 --> 10:34.160
It's actually kind of surprising to me how effective that's been given that the primitives

10:34.160 --> 10:38.880
are so old and that they're so simple, it's very surprising to me that those building

10:38.880 --> 10:42.760
blocks have gotten us as far as they have.

10:42.760 --> 10:49.320
So we took a little digression, I guess, before we got to kind of what you're up to at

10:49.320 --> 10:50.320
Baidu.

10:50.320 --> 10:51.320
Sure.

10:51.320 --> 10:57.280
And Baidu, really the Silicon Valley AI lab is about building new breakthrough technologies

10:57.280 --> 11:02.760
in AI that especially have connections or enabled new products.

11:02.760 --> 11:09.760
We focus on things that we can't currently do today and there are really multiple ways

11:09.760 --> 11:11.520
we end up attacking this.

11:11.520 --> 11:16.840
The thing that I focus a lot on is just the idea of scale that as we have faster computers

11:16.840 --> 11:23.720
that can train larger or complex neural networks that it's not the only way, but that's a very

11:23.720 --> 11:28.520
reliable way of improving accuracy or enabling new capabilities.

11:28.520 --> 11:32.320
We've seen this in vision to a large extent.

11:32.320 --> 11:36.400
It's actually kind of interesting when we started working in Baidu, there is a question

11:36.400 --> 11:39.680
whether you could apply this outside of vision.

11:39.680 --> 11:42.800
We spent a lot of time looking into speech recognition.

11:42.800 --> 11:46.600
I think looking back on that, it works very reliably.

11:46.600 --> 11:51.800
You can definitely apply deporting outside of vision to many different applications.

11:51.800 --> 11:56.600
Sometimes now instead of thinking what is the new application that you can apply deporting

11:56.600 --> 12:02.000
to, I sometimes wonder, are there any applications that are not well matched that we won't be

12:02.000 --> 12:07.960
able to make significant progress on by just applying the simple recipe of deep architectures,

12:07.960 --> 12:11.040
large data sets, large scale computer?

12:11.040 --> 12:13.240
I haven't found one yet.

12:13.240 --> 12:16.280
We talked a little bit about before we got started.

12:16.280 --> 12:21.160
We talked a little bit about the fact that you worked with one of our previous guests,

12:21.160 --> 12:25.800
Shibos and Gupta, who was at Baidu and is now at Facebook, is that right?

12:25.800 --> 12:28.760
Yeah, he's playing Dota.

12:28.760 --> 12:39.400
Nice. He and I spoke pretty extensively about the speech translation, the specific name

12:39.400 --> 12:44.160
of the project with the Baidu speech, the deep speech, right?

12:44.160 --> 12:49.000
In the course of that conversation, we talked a little bit about some of the scalability

12:49.000 --> 12:54.160
challenges that your team ran into in tackling that problem.

12:54.160 --> 13:01.280
Here at this conference, you're talking about, you have to talk tomorrow, in fact, about

13:01.280 --> 13:03.760
some even further work that you've done.

13:03.760 --> 13:07.040
Can you tell us about what you're planning to talk about?

13:07.040 --> 13:08.640
Sure, definitely.

13:08.640 --> 13:12.760
This is definitely along the lines of scaling deep neural networks.

13:12.760 --> 13:18.560
We pretty consistently find that if you throw more data at the problem, it isn't the only

13:18.560 --> 13:24.840
way, but it is a very effective way of reducing error rates and improving accuracy.

13:24.840 --> 13:29.920
So oftentimes, when you keep throwing data at the problem, you eventually run into some

13:29.920 --> 13:31.320
limitation.

13:31.320 --> 13:38.000
Sometimes you run out of data, and sometimes you run out of patience to wait for your

13:38.000 --> 13:42.040
system to train.

13:42.040 --> 13:46.480
There's one example that I think drives this point at home that we once had a model that

13:46.480 --> 13:54.680
ran, I let it run on a large cluster, run about 64 GPUs for about six months.

13:54.680 --> 13:59.960
We were still getting improvements in accuracy at the time I decided to pull the plug.

13:59.960 --> 14:07.120
How are you, if you're still running your training model for, you're saying that kind of your

14:07.120 --> 14:11.360
incremental error is decreasing as you ran it?

14:11.360 --> 14:15.720
Yeah, this was a state of the art model, so it's actually improving the state of the art

14:15.720 --> 14:19.520
as it runs every minute, it's getting a little bit better than any model that we've had

14:19.520 --> 14:20.680
before.

14:20.680 --> 14:24.960
And then after six months, you really go back and look at that and say, well, I could

14:24.960 --> 14:31.360
let it run for another few years, but I'd like to use it now.

14:31.360 --> 14:34.480
So we're always looking for ways to improve speed.

14:34.480 --> 14:39.720
There's actually, oh man, there's something that's related to that that I can't talk

14:39.720 --> 14:40.720
about yet.

14:40.720 --> 14:46.480
It's kind of a weird situation to sometimes be in where like you know why something happens

14:46.480 --> 14:50.080
or you know that something will keep happening, but you can't talk about it.

14:50.080 --> 14:54.360
One of the things that I think I know is that for many applications, we will continue

14:54.360 --> 14:57.200
to see improvements in the state of the art from faster computers.

14:57.200 --> 15:01.280
I can't tell you why, but I'm pretty sure, I'll tell you why soon.

15:01.280 --> 15:02.280
Okay.

15:02.280 --> 15:09.160
Is this the, are we talking about a theoretical result or a, okay, yeah, I think so.

15:09.160 --> 15:11.560
We're nodding yes for those who can't see.

15:11.560 --> 15:13.480
I'm nodding yes.

15:13.480 --> 15:14.480
Yeah.

15:14.480 --> 15:17.800
This is one of those things that'll definitely be surprising to people when we can finally

15:17.800 --> 15:21.280
talk about it, but sorry, I can't talk about it today.

15:21.280 --> 15:24.920
But you, so you just have to take my word for it that we need faster computers.

15:24.920 --> 15:25.920
Okay.

15:25.920 --> 15:29.760
And the talk tomorrow is going to be about a way that we can make computers faster for

15:29.760 --> 15:30.760
deep warning.

15:30.760 --> 15:31.760
Okay.

15:31.760 --> 15:32.760
Yeah.

15:32.760 --> 15:36.520
I spend a lot of my time thinking about this, like what's the best that you could do?

15:36.520 --> 15:41.360
How fast could you possibly make a computer even our understanding of physics on our existing

15:41.360 --> 15:42.960
technology?

15:42.960 --> 15:48.960
I think one thing the industry is realizing is that we spend a lot of time focusing on general

15:48.960 --> 15:55.920
purpose computation, so building computers to run windows or your browser.

15:55.920 --> 16:00.280
But if you specialize, if you build a computer that's good, it only a few things and not

16:00.280 --> 16:03.080
everything, you can do a lot better.

16:03.080 --> 16:07.840
So we're exploring right now, how do you build computers that are good at AI for good

16:07.840 --> 16:08.840
and deep warning?

16:08.840 --> 16:14.920
And is this different than what others in the space are doing, like TPUs and things of

16:14.920 --> 16:16.320
that nature?

16:16.320 --> 16:17.680
Let's see.

16:17.680 --> 16:20.960
So this is about a very specific technique.

16:20.960 --> 16:25.280
It might be one technology that might go into a chip like a GPU or a TPU.

16:25.280 --> 16:26.280
Okay.

16:26.280 --> 16:29.840
Right now, many of these designs are using a lot of the same technologies.

16:29.840 --> 16:30.840
Okay.

16:30.840 --> 16:32.240
This is a new one.

16:32.240 --> 16:34.320
And this one has a pretty high upside.

16:34.320 --> 16:37.040
This one has a maybe order of magnitude upside.

16:37.040 --> 16:38.040
Okay.

16:38.040 --> 16:43.080
Well, before we dive into that, can we take a second to kind of characterize the thing

16:43.080 --> 16:48.320
that GPUs and TPUs are doing that's kind of gotten them the benefit and then we'll

16:48.320 --> 16:52.080
dive into, you know, this approach and what makes it different?

16:52.080 --> 16:53.080
Sure.

16:53.080 --> 16:54.080
Definitely.

16:54.080 --> 16:56.160
So, let's see.

16:56.160 --> 17:01.600
I feel like one of the, well, there are two, okay, there are a lot of differences.

17:01.600 --> 17:06.720
One thing that's worth keeping in mind is that modern processors incorporate probably

17:06.720 --> 17:12.040
thousands, probably even more optimizations.

17:12.040 --> 17:16.560
So these are technologies that will improve their performance in some way.

17:16.560 --> 17:20.240
They might be circuit level, they might be architecture level, they might be in the software

17:20.240 --> 17:21.240
stack.

17:21.240 --> 17:27.480
It's very hard because real designs are composed of, you know, you pick out your favorite

17:27.480 --> 17:31.800
out of this pool of thousands of technologies and that becomes the new processor that you

17:31.800 --> 17:33.040
build.

17:33.040 --> 17:38.320
These distinctions like TPU, GPU, CPU, they're very high level and they gloss over all of

17:38.320 --> 17:39.680
those details.

17:39.680 --> 17:40.680
Okay.

17:40.680 --> 17:47.600
So, I think what's more important than the name is what it does, how fast is it actually,

17:47.600 --> 17:50.320
like what is the result that you get from it?

17:50.320 --> 17:53.160
How fast does it run a model that you care about?

17:53.160 --> 17:57.800
We've seen things that were called CPUs being commonly used for training maybe 10 years

17:57.800 --> 17:58.800
ago.

17:58.800 --> 18:02.080
There was a transition where people started using GPUs.

18:02.080 --> 18:08.320
The important thing about GPUs was optimization for parallelism, that there is abundant

18:08.320 --> 18:11.680
parallelism in neural network computations.

18:11.680 --> 18:15.480
Some of the things that are, you know, a few, like a couple out of that list of thousand

18:15.480 --> 18:20.400
things that are being added into the next generation are optimizations around locality

18:20.400 --> 18:22.160
and low precision.

18:22.160 --> 18:27.880
So the technology I'm going to talk about tomorrow is focused on low precision, and there's

18:27.880 --> 18:34.960
a big difference when a lot of previous technologies have been discussed or proposed for low precision,

18:34.960 --> 18:38.840
it's mostly been focused on inference and not training.

18:38.840 --> 18:44.760
And this will be one of the first results, and especially the, as far as I know, largest

18:44.760 --> 18:50.240
skill result that focuses on using low precision for training, I think the high level conclusion

18:50.240 --> 18:52.960
is it finally works.

18:52.960 --> 18:54.920
It was enormously difficult.

18:54.920 --> 18:55.920
Wow.

18:55.920 --> 19:02.800
It was actually kind of a weird surprise that when you try doing low precision for training

19:02.800 --> 19:06.240
versus inference, we didn't really know that we would see this.

19:06.240 --> 19:09.840
We started looking into this, but it just turns out that for some reason, inference is

19:09.840 --> 19:15.480
so much easier than training, that even, you know, very drastic reductions in precision,

19:15.480 --> 19:20.400
like moving from double precision down to, you know, even 8-bit or possibly even lower

19:20.400 --> 19:22.920
fixed point representations.

19:22.920 --> 19:26.400
It works just fine across many different models.

19:26.400 --> 19:30.200
But if you try and do the same thing for training, things fail.

19:30.200 --> 19:35.360
It's actually kind of a funny point to me that we kind of made this implicit transition.

19:35.360 --> 19:39.200
CPUs commonly support a high performance double precision.

19:39.200 --> 19:46.200
CPUs don't, GPUs have historically optimized around single precision, so 32-bit floating

19:46.200 --> 19:49.200
point instead of 64-bit floating point.

19:49.200 --> 19:54.800
It turns out it's kind of expensive to do this in a GPU, to do 64-bit in a GPU, whereas

19:54.800 --> 20:00.720
it's pretty cheap in a CPU, because you don't have to replicate this thing, this unit

20:00.720 --> 20:01.720
very many times.

20:01.720 --> 20:02.720
Right.

20:02.720 --> 20:03.920
On a GPU, you have to replicate it a lot.

20:03.920 --> 20:08.160
So if you replicate something big a lot, it becomes expensive.

20:08.160 --> 20:13.960
It's kind of surprising that the whole industry, you know, when I started watching people train

20:13.960 --> 20:19.200
deep neural networks, they might write scripts in, you know, MATLAB or, you know, call CPU

20:19.200 --> 20:20.800
libraries directly.

20:20.800 --> 20:24.200
And those things, by default, use double precision.

20:24.200 --> 20:29.720
When the industry switched to GPUs, they switched from double precision to single precision.

20:29.720 --> 20:31.400
And we got so lucky.

20:31.400 --> 20:35.280
It turns out that it didn't really matter.

20:35.280 --> 20:37.320
But I think that was just by luck.

20:37.320 --> 20:39.000
And we tried doing the next step.

20:39.000 --> 20:44.920
We tried moving from single precision to half precision, so moving from 32-bit floating

20:44.920 --> 20:50.520
point to 16-bit floating point, things started failing all over the place.

20:50.520 --> 20:54.040
And what caused those failures?

20:54.040 --> 20:55.360
There were a lot of them.

20:55.360 --> 20:58.480
Let me try and draw a couple of big categories.

20:58.480 --> 21:02.000
One was just differences in range.

21:02.000 --> 21:06.320
So one of the points of having floating point, as opposed to fixed point, is that you have

21:06.320 --> 21:08.920
a very large dynamic range.

21:08.920 --> 21:14.680
You might, you know, be able to do an operation like an ad of a number that's, you know,

21:14.680 --> 21:16.480
where one number is a billion.

21:16.480 --> 21:20.360
And the other number is, you know, 10 to the minus 5.

21:20.360 --> 21:22.440
And that works.

21:22.440 --> 21:27.160
And so you need your range to extend from the smallest numbers that you want to deal

21:27.160 --> 21:30.440
with to the biggest numbers that you want to deal with.

21:30.440 --> 21:36.120
And it turns out that if you look at all of the operations that go on in four-propagation

21:36.120 --> 21:42.160
and backpropagation, the nonlinearities in the SGD algorithm, there's actually a pretty

21:42.160 --> 21:44.840
large dynamic range.

21:44.840 --> 21:48.800
Aren't we typically normalizing to try to get rid of some of that?

21:48.800 --> 21:50.520
Yeah, it's interesting.

21:50.520 --> 21:52.360
I'll come back to that point.

21:52.360 --> 21:55.360
It was an interesting thing related to that point.

21:55.360 --> 21:57.080
Yeah, let me come back to that.

21:57.080 --> 22:01.480
But I feel like the number one reason why when we just, so the first experiments we did

22:01.480 --> 22:08.520
were just convert all of the 32-bit numbers to 16-bit precision numbers and try using

22:08.520 --> 22:10.920
exactly the same algorithm.

22:10.920 --> 22:15.720
And also, and by due, you know, because we were working on speech recognition, we started

22:15.720 --> 22:18.480
doing this for recurrent neural networks.

22:18.480 --> 22:21.120
It turns out that was one of the harder examples.

22:21.120 --> 22:24.640
We started with one of the harder cases it turned out.

22:24.640 --> 22:26.720
And so we would see all sorts of failures.

22:26.720 --> 22:30.760
And what makes RNN particularly harder?

22:30.760 --> 22:33.920
I think it has to do with accumulated errors.

22:33.920 --> 22:39.680
So as you keep doing this repeated application of a matrix multiplication with the same weights,

22:39.680 --> 22:45.280
you're thinking about, or over time, this just encourages extreme values, either extremely

22:45.280 --> 22:47.720
small values or extremely large values.

22:47.720 --> 22:53.200
So sometimes people call this the vanishing gradients or exploding gradients problems.

22:53.200 --> 22:57.680
And for speech recognition, we see very long time series.

22:57.680 --> 23:02.560
We might see hundreds of iterations of an RNN or maybe thousands.

23:02.560 --> 23:07.360
And we saw, you know, large accumulated errors over time.

23:07.360 --> 23:16.000
One of the biggest sources we came across was when you're actually combining gradients with

23:16.000 --> 23:20.040
the gradient update with the master copy of the weights.

23:20.040 --> 23:25.320
So when you have this model, it turns out it seems like SGT just makes these repeated

23:25.320 --> 23:27.840
small updates to a model.

23:27.840 --> 23:31.880
And so if you look at it from a range perspective, there's a large difference in magnitude

23:31.880 --> 23:37.560
between the magnitude of the gradients and the magnitude of the weights.

23:37.560 --> 23:43.240
And so when you try and do operations on those, you know, numbers that have very different

23:43.240 --> 23:48.200
magnitudes, you get loss of information or you get errors.

23:48.200 --> 23:53.840
And that was one of the biggest problems we had with training in half precision.

23:53.840 --> 24:00.680
It seemed like, yeah, moving for some reason, the errors introduced from like the, in floating

24:00.680 --> 24:07.960
point, things work out well if the numbers are in different magnitudes, but not by too much.

24:07.960 --> 24:12.760
And so it turns out that the difference for single precision versus double precision

24:12.760 --> 24:17.960
was okay, but it ended up being borderline for multiple applications when we were looking

24:17.960 --> 24:21.720
at the difference between single precision and half precision.

24:21.720 --> 24:25.520
So we had to introduce some changes in order to deal with that.

24:25.520 --> 24:31.920
One of the questions that came up in this previous conversation with Shubo, in which we talked,

24:31.920 --> 24:37.240
we touched on some of this stuff, I think pretty tangentials, like the end of our conversation,

24:37.240 --> 24:43.960
I think if I remember correctly, but we're talking about a reduced precision and I think

24:43.960 --> 24:49.760
I asked the question like, you can reduce the precision in multiple places.

24:49.760 --> 24:57.280
You can reduce the precision in your weights, you can reduce the precision in your outputs.

24:57.280 --> 25:00.000
When you're talking about reduced precision, or you're talking, it sounds like you're

25:00.000 --> 25:04.720
talking about reduced precision everywhere, just running on reduced precision infrastructure

25:04.720 --> 25:10.080
on a reduced precision mode and not being particularly discriminating in terms of where

25:10.080 --> 25:11.280
you reduce the precision.

25:11.280 --> 25:13.040
Is that what you're referring to?

25:13.040 --> 25:17.160
Yes, we're trying to keep it simple.

25:17.160 --> 25:22.000
We feel like if it ends up getting very complex, then it's difficult for people to know how

25:22.000 --> 25:25.160
you would actually apply this to a real model.

25:25.160 --> 25:29.800
You get back into your architecture, feature engineering, complexity issues.

25:29.800 --> 25:35.280
We definitely didn't want to introduce this as another hyper parameter, or maybe this

25:35.280 --> 25:40.320
only works for a few layers, but it doesn't work in these places.

25:40.320 --> 25:45.360
You have to make this hard choice of deciding which ones to convert, which ones not.

25:45.360 --> 25:49.720
We wanted it just to be kind of like a switch, and you would turn on the switch, and you

25:49.720 --> 25:52.160
would get the performance improvement.

25:52.160 --> 25:58.560
I think we finally got to that point, but for this kind of reason, there were a lot of problems

25:58.560 --> 26:00.200
along the way.

26:00.200 --> 26:04.880
I mentioned the difference in magnitudes between the updates and the weights as a source

26:04.880 --> 26:06.720
of errors.

26:06.720 --> 26:12.160
The other big one was just accumulated errors in long dot products.

26:12.160 --> 26:19.520
It turns out that taking weights convert, quantizing them to 16-bit, and then doing multiplications

26:19.520 --> 26:24.280
of activations with those weights didn't introduce too many errors.

26:24.280 --> 26:28.520
But in neural networks, especially in recurrent neural networks, as layers get big, you end

26:28.520 --> 26:33.600
up with these long dot products, and so you're doing a running sum, kind of like over each

26:33.600 --> 26:40.200
row, or all of the inputs of a neuron.

26:40.200 --> 26:45.480
Each operation has an accumulated error, so everyone in the sequence is going to add some

26:45.480 --> 26:46.880
amount of error.

26:46.880 --> 26:52.360
And now we're not talking about error in the kind of machine learning modeling, since we're

26:52.360 --> 26:53.960
talking about floating point error.

26:53.960 --> 26:59.840
Yeah, we're talking about just, you know, you really wanted to do this multiply operation.

26:59.840 --> 27:01.200
You didn't get the exact result.

27:01.200 --> 27:04.800
We had to clamp it to a value that's representable by the computer.

27:04.800 --> 27:08.560
And so each time you do that, you introduce quantization error.

27:08.560 --> 27:14.920
And normally, as long as you have enough bits, the quantization error is small enough that

27:14.920 --> 27:18.200
it doesn't really affect the final result too much.

27:18.200 --> 27:22.800
Exactly what too much means is very application dependent, and into complex systems like neural

27:22.800 --> 27:27.240
networks, it's really hard to know how much error is too much error other than just trying

27:27.240 --> 27:29.480
it on a real application.

27:29.480 --> 27:34.600
But we found for real applications, like for speech recognition or for translation, the

27:34.600 --> 27:39.400
error introduced by doing ads in 16-bit was too much error.

27:39.400 --> 27:46.360
Models would diverge or models would achieve significantly worse accuracy than the 32-bit

27:46.360 --> 27:47.760
baselines.

27:47.760 --> 27:53.080
And so we went back to that and we tried a whole bunch of things, like we tried hierarchical

27:53.080 --> 27:58.000
reductions and a bunch of things that ended up just being complicated.

27:58.000 --> 28:01.760
And eventually we went back and looked at the circuits and came to the conclusion that

28:01.760 --> 28:05.800
it wasn't that expensive just to put in a 32-bit adder.

28:05.800 --> 28:11.520
So you have a bunch of 16-bit multipliers and then you have a few 32-bit adders.

28:11.520 --> 28:16.360
And if you look at the performance improvement that you get from that, it ends up being

28:16.360 --> 28:21.120
most of the performance improvement that you would have got if you would have built 16-bit

28:21.120 --> 28:24.440
multipliers and 16-bit adders.

28:24.440 --> 28:29.320
So yeah, we ended up with a mixed precision format.

28:29.320 --> 28:34.880
You end up doing multiplication in 16-bit but then the addition in 32-bit.

28:34.880 --> 28:36.520
And there are a lot of other things.

28:36.520 --> 28:39.720
We ended up looking at there's still some other failure cases but those are really the

28:39.720 --> 28:40.960
two big things.

28:40.960 --> 28:45.720
As long as you keep the master copy of weights in 32-bit and as long as you do all the

28:45.720 --> 28:52.120
additions in 32-bit, you can do all the multiplications and you can represent all the activations

28:52.120 --> 28:57.760
and intermediate copies of weights and weight gradients in 16-bit.

28:57.760 --> 29:02.400
To take a step back and make sure I understand why we're doing this, are we talking about

29:02.400 --> 29:07.640
performance and computational costs, are we talking about kind of unit compute costs

29:07.640 --> 29:12.840
for this chip by having narrower buses and things like that?

29:12.840 --> 29:17.000
Are we talking about training time performance?

29:17.000 --> 29:24.360
What are the factors that are driving us to say we want to do this and not just we can

29:24.360 --> 29:28.680
do it and reduce precision, we want to do this and reduce precision.

29:28.680 --> 29:29.680
Sure.

29:29.680 --> 29:31.760
Yeah, why do we want to do this and reduce precision?

29:31.760 --> 29:34.640
It's really so we can build more efficient hardware.

29:34.640 --> 29:36.840
Within without this technique you can just do a comparison.

29:36.840 --> 29:40.920
If you're building the same processor within without this technique, there's a fair amount

29:40.920 --> 29:42.240
of performance at play.

29:42.240 --> 29:48.360
It might be something like four to eight X difference in really both sides of it, total

29:48.360 --> 29:53.640
performance or energy per operation, which would translate into efficiency.

29:53.640 --> 30:02.240
By going to reduce performance or by going to reduce precision, we can increase some

30:02.240 --> 30:08.840
composite of performance and energy consumption by four to eight X, like nearly order of

30:08.840 --> 30:09.840
magnitude.

30:09.840 --> 30:15.280
Yeah, we could finish my six month model in maybe just a single month.

30:15.280 --> 30:19.040
And is it, I guess I'm trying to get at this question, I don't know if the question

30:19.040 --> 30:26.800
makes sense, but like, is it something inherent about the lower precision or is it the fact

30:26.800 --> 30:31.920
that the lower precision allows us to use new compute architectures that are faster in

30:31.920 --> 30:32.920
other ways?

30:32.920 --> 30:35.160
Oh, sure, definitely.

30:35.160 --> 30:39.600
So do you get this performance improvement on existing computers?

30:39.600 --> 30:43.440
You get some performance improvement because you're moving around less data, but it might

30:43.440 --> 30:47.800
be closer to two X, it really depends on whether you're compute bound or bandwidth bound,

30:47.800 --> 30:51.200
but the maximum might be more like two X.

30:51.200 --> 30:55.880
But if you build another computer, if you build a new processor that was optimized around

30:55.880 --> 30:59.520
this idea, you could do even better, you could realize the four to eight X.

30:59.520 --> 31:00.520
Okay.

31:00.520 --> 31:07.040
So low precision fundamentally allows you to do kind of train these neural nets by moving

31:07.040 --> 31:11.600
around less data, right, 16 bits instead of 64, for example.

31:11.600 --> 31:17.040
So you get some advantage in doing that, even if you're just in low precision mode on a

31:17.040 --> 31:24.120
general purpose computer, but it also allows you to build chips that are specific to running

31:24.120 --> 31:29.800
in low precision, and that gives you, that's where you get the big opportunity to bump up

31:29.800 --> 31:30.800
your speeds.

31:30.800 --> 31:31.800
Yes.

31:31.800 --> 31:32.800
Exactly.

31:32.800 --> 31:33.800
Okay.

31:33.800 --> 31:35.560
And so you were here talking about the actual chip.

31:35.560 --> 31:36.560
Is that correct?

31:36.560 --> 31:37.560
Oh, yeah.

31:37.560 --> 31:40.600
So we're going to talk about the Volta GPU from Nvidia.

31:40.600 --> 31:44.360
This is a, yeah, this is a collaboration within video.

31:44.360 --> 31:49.120
It's worth noting, you know, this hardware has been shipping for a while.

31:49.120 --> 31:53.160
But the side of it that we're talking about now is the validation that we've done on

31:53.160 --> 31:54.160
it.

31:54.160 --> 31:57.760
So we've actually shown that you can train models in low precision.

31:57.760 --> 32:04.640
We've looked at, you know, over 15 large scale, complete end to end deporting applications.

32:04.640 --> 32:07.920
So it's really easy to build hardware that gets great performance numbers, but isn't

32:07.920 --> 32:10.200
able to run any real algorithms.

32:10.200 --> 32:17.280
So from the point of view of low, of low precision, the Volta is like its general purpose, right?

32:17.280 --> 32:21.800
It's not a chip that specifically designed for low precision.

32:21.800 --> 32:25.840
Oh, it did actually have, they're called tensor cores.

32:25.840 --> 32:26.840
Right.

32:26.840 --> 32:31.600
It was the name for them is a tensor core that is this operation I'm talking about.

32:31.600 --> 32:32.600
Okay.

32:32.600 --> 32:38.880
It's a specialized unit that does 16 bit multiplication floating point with 32 bit floating point

32:38.880 --> 32:40.120
addition.

32:40.120 --> 32:43.000
That unit was designed as a result of the study.

32:43.000 --> 32:44.000
Got it.

32:44.000 --> 32:45.000
Got it.

32:45.000 --> 32:49.760
And now, if I remember correctly, when this was announced, they made a big deal about not

32:49.760 --> 32:53.920
the floating point side of things, but like N8 performance and things like that.

32:53.920 --> 32:55.320
How does that all fit in?

32:55.320 --> 32:56.320
Sure.

32:56.320 --> 32:57.320
Definitely.

32:57.320 --> 33:01.520
So I kind of alluded to this maybe in the beginning that inference just is easier for

33:01.520 --> 33:04.400
some reason than training.

33:04.400 --> 33:05.560
So that's all the inference.

33:05.560 --> 33:10.080
It's like we can do N8 on inference side and it is easy and it just works.

33:10.080 --> 33:11.080
And it's faster.

33:11.080 --> 33:12.080
Exactly.

33:12.080 --> 33:13.080
Got it.

33:13.080 --> 33:14.080
Okay.

33:14.080 --> 33:15.080
I don't know.

33:15.080 --> 33:17.880
I don't know that this whole topic has been really fully explored yet.

33:17.880 --> 33:22.840
Maybe someday in the future, we might see someone who gets in date training to work.

33:22.840 --> 33:26.560
But as far as I know, I've never seen it.

33:26.560 --> 33:30.920
I know there are a lot of, there's a lot of work on, you know, very reduced precision,

33:30.920 --> 33:33.120
like even down to binary.

33:33.120 --> 33:36.680
But one thing that's worth noting about these approaches is that they either have accuracy

33:36.680 --> 33:45.680
losses, so you trade precision for accuracy on the complete application or they only apply

33:45.680 --> 33:47.680
to inference and not to training.

33:47.680 --> 33:48.680
Mm-hmm.

33:48.680 --> 33:49.680
Okay.

33:49.680 --> 33:50.680
Got it.

33:50.680 --> 33:58.560
So, reduced precision, you did some validation that shows that essentially running in this

33:58.560 --> 34:01.360
mode is kind of a generalized approach you can take.

34:01.360 --> 34:06.160
Now, you know, things that you need to do or switches that you need to flip when you're

34:06.160 --> 34:11.760
training your model in order to get it to work accurately or to work correctly.

34:11.760 --> 34:12.760
Sure.

34:12.760 --> 34:16.560
So, one switch that you need to flip is you need to decide to do this.

34:16.560 --> 34:17.560
Okay.

34:17.560 --> 34:23.640
You need to decide to represent things in 16-bit and do your matrix multiplications or

34:23.640 --> 34:29.360
convolution operations in this mixed 16-bit, 32-bit format.

34:29.360 --> 34:33.160
That's somewhat of a global switch, you can just turn that on for the entire program.

34:33.160 --> 34:34.160
Okay.

34:34.160 --> 34:37.160
The other thing that you need to do that we found as essential is you need to master

34:37.160 --> 34:38.360
a copy of the weights.

34:38.360 --> 34:39.360
Mm-hmm.

34:39.360 --> 34:44.000
So, in your optimization algorithm, like your implementation of SGD, you need to have a

34:44.000 --> 34:48.840
separate copy in 32-bit of all the weights.

34:48.840 --> 34:53.360
And only when you're doing a Ford propagation or back propagation, do you convert from that

34:53.360 --> 34:54.600
into 16-bit?

34:54.600 --> 34:55.600
Okay.

34:55.600 --> 34:59.720
But, both of those changes we found are the first ones really easy.

34:59.720 --> 35:03.600
The second one can be encapsulated inside of the optimization algorithm.

35:03.600 --> 35:07.080
So at least when you're designing a network, you don't have to think about this.

35:07.080 --> 35:08.080
Okay.

35:08.080 --> 35:12.560
You know, I can envision how I might do this if I was writing the, you know, if I was

35:12.560 --> 35:18.000
implementing SGD myself to the higher level frameworks and toolkits all know how to

35:18.000 --> 35:21.600
do this or is that, you know, yet forthcoming.

35:21.600 --> 35:28.040
So it's straightforward to do this in most frameworks, but there needs to be developers who are

35:28.040 --> 35:30.920
working on those frameworks who will actually add support for this.

35:30.920 --> 35:31.920
Okay.

35:31.920 --> 35:36.120
You know, when we did this in the framework that we have in BIDU, it's something like a,

35:36.120 --> 35:37.920
you know, 15 lines of code change.

35:37.920 --> 35:38.920
Yeah.

35:38.920 --> 35:41.560
So it's really minor, but you still have to do that.

35:41.560 --> 35:44.920
Otherwise, you won't get access to the improved performance.

35:44.920 --> 35:45.920
Right.

35:45.920 --> 35:46.920
Okay.

35:46.920 --> 35:50.400
Anything else you talked about in your, or I keep saying it in past tense.

35:50.400 --> 35:54.760
Anything else you're going to talk about in your talk that you want to mention?

35:54.760 --> 35:59.480
I guess the last thing is that this is one piece.

35:59.480 --> 36:03.160
This is one technology that gives us a large improvement in performance.

36:03.160 --> 36:08.440
I think we're aware of a lot of them that haven't been realized yet.

36:08.440 --> 36:12.720
So you know, I mentioned before, the hardware industry for a long time has been kind of creeping

36:12.720 --> 36:13.720
along.

36:13.720 --> 36:18.960
It's actually very difficult to realize large improvements in sequential performance.

36:18.960 --> 36:25.600
But at least for parallel programs like graphics applications and things that would run on GPUs,

36:25.600 --> 36:29.040
performance has been increasing, you know, following something like the popular form of

36:29.040 --> 36:32.520
Moore's law, so exponential growth.

36:32.520 --> 36:37.680
For AI, if you're only thinking about running deep neural networks, you can probably do

36:37.680 --> 36:40.560
a lot better than that in the short term.

36:40.560 --> 36:43.960
So we might not have to wait 10 years to get 1,000 X faster.

36:43.960 --> 36:46.400
It might happen in just a few years.

36:46.400 --> 36:51.040
I'm going to mention some of the other ways that haven't been implemented yet, but that

36:51.040 --> 36:53.840
we know about it are likely to happen in the future.

36:53.840 --> 36:54.840
Can you rattle those off?

36:54.840 --> 36:58.160
This podcast will not be published before you're talked tomorrow.

36:58.160 --> 37:00.160
Sure.

37:00.160 --> 37:02.200
One of them is array parallelism.

37:02.200 --> 37:04.840
I think this is one of the other big ones is array parallelism.

37:04.840 --> 37:05.840
Okay.

37:05.840 --> 37:10.960
I had a Forbes article about this for us talking about locality, the importance of locality.

37:10.960 --> 37:17.880
If you build processors around the idea of locality and parallelism, not just parallelism, you

37:17.880 --> 37:22.960
end up with something that it looks, I call it like an array processor rather than a vector

37:22.960 --> 37:23.960
processor.

37:23.960 --> 37:24.960
Okay.

37:24.960 --> 37:28.640
You're thinking about the core instruction that you're doing instead of adding or multiplying

37:28.640 --> 37:33.120
two vectors together, you're adding or multiplying two arrays together.

37:33.120 --> 37:36.280
So you see things like this in designs like the TPU.

37:36.280 --> 37:40.160
I feel like the thing that is wrong with those designs is that they don't find the knee

37:40.160 --> 37:45.680
of the curve that this is beneficial, but you don't have to go all in on it to get most

37:45.680 --> 37:48.400
of the benefits and you actually are trading off.

37:48.400 --> 37:51.560
So you don't find the knee of the curve, what exactly does that mean?

37:51.560 --> 37:56.240
It means working on a raise is a good idea, but they don't have to be enormous arrays.

37:56.240 --> 37:57.240
Okay.

37:57.240 --> 38:02.280
And you actually are trading off flexibility for performance when you're making the arrays

38:02.280 --> 38:03.440
bigger.

38:03.440 --> 38:05.600
So you shouldn't make them enormous.

38:05.600 --> 38:09.840
You should make them big enough to get most of the savings and energy.

38:09.840 --> 38:14.520
In terms of order of magnitude, we're talking about like little teeny ones, like convolutional

38:14.520 --> 38:19.000
kernel sizes, are we talking about, you know, something bigger than that?

38:19.000 --> 38:20.000
Or...

38:20.000 --> 38:25.760
Yeah, it's like more like 16 by 16 than 256 by 256, that makes sense.

38:25.760 --> 38:26.760
Okay.

38:26.760 --> 38:27.760
Yeah.

38:27.760 --> 38:28.760
Interesting.

38:28.760 --> 38:31.120
Any others on that list that come to mind?

38:31.120 --> 38:36.160
One of the ones that doesn't work yet, but I think is very promising, is sparsity.

38:36.160 --> 38:37.160
Okay.

38:37.160 --> 38:40.400
We're talking with sparse representations rather than dense representations.

38:40.400 --> 38:46.600
And it might seem like that's incompatible with the one that I just said, the array parallelism.

38:46.600 --> 38:49.760
We haven't shown this yet, but I suspect that they're not incompatible.

38:49.760 --> 38:54.240
Well, what I'm hearing putting the two together is that, you know, we're living in a world

38:54.240 --> 38:58.120
that thinks about all this stuff as composite vector operations.

38:58.120 --> 39:02.640
And by thinking about this at the level of matrices, you know, there are opportunities

39:02.640 --> 39:03.640
there.

39:03.640 --> 39:04.640
Yes.

39:04.640 --> 39:05.640
Yeah.

39:05.640 --> 39:06.640
That's a good way of thinking about it.

39:06.640 --> 39:07.640
Interesting.

39:07.640 --> 39:09.360
Well, I really enjoyed this chat.

39:09.360 --> 39:11.240
Thank you so much for taking the time.

39:11.240 --> 39:12.240
Good to be here.

39:12.240 --> 39:13.240
Great.

39:13.240 --> 39:14.240
Thanks, Craig.

39:14.240 --> 39:15.240
Thank you.

39:15.240 --> 39:19.000
All right, everyone.

39:19.000 --> 39:21.120
That's our show for today.

39:21.120 --> 39:26.360
Thanks so much for listening and for your continued feedback and support.

39:26.360 --> 39:32.080
Thanks to you, this podcast finished the year as a top 40 technology podcast on Apple

39:32.080 --> 39:33.600
podcasts.

39:33.600 --> 39:38.240
My producer says that one of his goals this year is to crack the top 10.

39:38.240 --> 39:42.360
And to do that, we will need your help.

39:42.360 --> 39:45.960
Please head on over to the podcast app, rate the show.

39:45.960 --> 39:48.200
Hopefully we've earned your five stars.

39:48.200 --> 39:53.200
Leave us a glowing review and share it with your friends, family, co-workers, Starbucks

39:53.200 --> 39:56.640
baristas, Uber drivers, everyone.

39:56.640 --> 39:59.160
Every review and rating goes a long way.

39:59.160 --> 40:01.560
So thanks in advance.

40:01.560 --> 40:06.560
For more information on Greg or any of the topics covered in this episode, head on over

40:06.560 --> 40:10.760
to twomolei.com slash talk slash 97.

40:10.760 --> 40:16.240
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

40:16.240 --> 40:19.400
or via Twitter at at Twomolei.

40:19.400 --> 40:37.240
Thanks once again for listening and catch you next time.

