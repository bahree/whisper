1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,960
I'm your host Sam Charrington.

4
00:00:31,960 --> 00:00:37,160
Earlier this week I had a chance to speak with Chris Shaloo, senior software engineer at

5
00:00:37,160 --> 00:00:44,360
Google AI, about his project and paper on exploring exoplanets with deep learning.

6
00:00:44,360 --> 00:00:46,680
This is a great story.

7
00:00:46,680 --> 00:00:52,040
Chris, inspired by a book he was reading, reached out on a whim to a Harvard astrophysics

8
00:00:52,040 --> 00:00:57,400
researcher kicking off a collaboration and side project eventually leading to the discovery

9
00:00:57,400 --> 00:01:02,160
of two new planets outside of our solar system.

10
00:01:02,160 --> 00:01:07,720
In our conversation, Chris and I walked through the entire process he used to find these two

11
00:01:07,720 --> 00:01:13,360
exoplanets, including how he researched the domain as an outsider, how he sourced and

12
00:01:13,360 --> 00:01:18,080
processed his dataset and how he built and evolved his models.

13
00:01:18,080 --> 00:01:26,440
Finally, we discussed the results of his project and his plans for future work in this area.

14
00:01:26,440 --> 00:01:30,720
This podcast is being released in parallel with Google's release of the source code and

15
00:01:30,720 --> 00:01:35,840
data that Chris developed and used, which will link to in the show notes.

16
00:01:35,840 --> 00:01:40,480
If what you hear inspires you to dig into this area more deeply, you've got a nice head

17
00:01:40,480 --> 00:01:42,000
start.

18
00:01:42,000 --> 00:01:47,560
Chris was a really interesting conversation and I'm excited to share it with you.

19
00:01:47,560 --> 00:01:52,520
Before we jump into the interview, a reminder that the next Twimmel Online Meetup is quickly

20
00:01:52,520 --> 00:01:54,000
approaching.

21
00:01:54,000 --> 00:02:00,040
Be sure to join us next Tuesday, March 13th, for an in-depth review of reinforcement

22
00:02:00,040 --> 00:02:05,520
learning and the Google DeepMind paper, playing Atari with deep reinforcement learning, presented

23
00:02:05,520 --> 00:02:08,080
by Meetup member Sean Devlin.

24
00:02:08,080 --> 00:02:13,760
Get on over to twimmelai.com slash Meetup to learn more or register.

25
00:02:13,760 --> 00:02:18,080
Okay, let's get to it.

26
00:02:18,080 --> 00:02:25,800
Hey everyone, I am on the line with Chris Jaloo.

27
00:02:25,800 --> 00:02:28,600
Chris, welcome to this Weekend Machine Learning and AI.

28
00:02:28,600 --> 00:02:30,840
Thanks Sam, happy to be here.

29
00:02:30,840 --> 00:02:31,840
Awesome.

30
00:02:31,840 --> 00:02:35,640
Why don't we get started by having you tell the audience a little bit about your background

31
00:02:35,640 --> 00:02:38,960
and how you got interested in machine learning?

32
00:02:38,960 --> 00:02:39,960
Sure.

33
00:02:39,960 --> 00:02:44,880
So, I started out studying mathematics actually.

34
00:02:44,880 --> 00:02:51,920
In my undergraduate degree, I did a double major in pure and applied mathematics.

35
00:02:51,920 --> 00:02:57,400
And once I graduated, I then did a year of research in pure mathematics, studying certain

36
00:02:57,400 --> 00:02:59,920
classes of polynomials in fact.

37
00:02:59,920 --> 00:03:00,920
Oh man.

38
00:03:00,920 --> 00:03:07,640
I'm starting to break out and like sweat here, I think in grad school, the real analysis

39
00:03:07,640 --> 00:03:10,160
class I took was like the hardest class.

40
00:03:10,160 --> 00:03:11,160
Right.

41
00:03:11,160 --> 00:03:14,520
Well, I was actually studying these polynomials over finite fields.

42
00:03:14,520 --> 00:03:18,400
So it was more in the abstract algebra space.

43
00:03:18,400 --> 00:03:25,480
But to your point exactly, after I did this, I decided I wanted to do something with more

44
00:03:25,480 --> 00:03:29,280
of a concrete application in the real world.

45
00:03:29,280 --> 00:03:32,360
And so I knew I wanted to do a PhD.

46
00:03:32,360 --> 00:03:37,640
And so I looked at some other options and eventually I decided to pursue a PhD in biomechanical

47
00:03:37,640 --> 00:03:38,640
engineering.

48
00:03:38,640 --> 00:03:39,640
Oh wow.

49
00:03:39,640 --> 00:03:46,880
So I started that PhD, but after about a year I decided that perhaps a career in biomechanical

50
00:03:46,880 --> 00:03:49,760
engineering research wasn't for me.

51
00:03:49,760 --> 00:03:53,160
So I started to look for other opportunities at that point.

52
00:03:53,160 --> 00:03:57,960
So briefly, I went back to mathematics for a little while by teaching classes at a

53
00:03:57,960 --> 00:04:03,920
university, but I decided to apply for a job at Google mainly because I was excited

54
00:04:03,920 --> 00:04:08,960
by some of the really high impact world changing projects that I was hearing about coming

55
00:04:08,960 --> 00:04:11,360
out of Google in particular.

56
00:04:11,360 --> 00:04:14,200
I remember being excited by Project Loon.

57
00:04:14,200 --> 00:04:19,200
So for those who don't know that was a project, it's actually still going that tries to use

58
00:04:19,200 --> 00:04:25,960
networks of flying balloons to bring the internet to users in rural and remote areas.

59
00:04:25,960 --> 00:04:30,720
So I applied to Google and I was hired as a software engineer, but unfortunately I didn't

60
00:04:30,720 --> 00:04:33,640
get to work on Project Loon straight away.

61
00:04:33,640 --> 00:04:38,080
I was actually placed in the Google Display Ads team and I was working on ads for Google

62
00:04:38,080 --> 00:04:40,360
Maps and Gmail.

63
00:04:40,360 --> 00:04:42,960
And this is where I got my first taste of machine learning.

64
00:04:42,960 --> 00:04:47,840
I was actually working on models to try and show the most relevant ads to each user.

65
00:04:47,840 --> 00:04:52,000
And I learned a lot about machine learning in that role.

66
00:04:52,000 --> 00:04:57,840
And then early on in my career at Google, I saw a presentation here at Google by some

67
00:04:57,840 --> 00:05:01,720
researches in the Google Brain team where I currently work.

68
00:05:01,720 --> 00:05:07,760
And they had created a neural network that would automatically generate image captions.

69
00:05:07,760 --> 00:05:09,480
And I thought this was really exciting.

70
00:05:09,480 --> 00:05:15,440
This model that they created could accept raw pixels from photographs and output fully

71
00:05:15,440 --> 00:05:19,960
formed English sentences that would describe the photo.

72
00:05:19,960 --> 00:05:24,320
And so this model was actually trained using only images and captions as input.

73
00:05:24,320 --> 00:05:29,880
So quite amazingly, the model was not actually given any details about the English language

74
00:05:29,880 --> 00:05:32,200
except the captions.

75
00:05:32,200 --> 00:05:38,600
And it actually managed to learn not only how to caption images, but also how to write sentences

76
00:05:38,600 --> 00:05:42,680
with good English grammar, which I thought was pretty amazing as well, just from reading

77
00:05:42,680 --> 00:05:44,320
captions.

78
00:05:44,320 --> 00:05:49,080
And so at that point I decided I wanted to work on the Google Brain team.

79
00:05:49,080 --> 00:05:54,360
And luckily I got the chance to transfer to the brain team after two years in ads.

80
00:05:54,360 --> 00:06:00,160
And since then I've been working on machine learning research here in the brain team.

81
00:06:00,160 --> 00:06:03,940
And I've been lucky enough to work on many different areas of machine learning research

82
00:06:03,940 --> 00:06:05,440
since I joined the team.

83
00:06:05,440 --> 00:06:06,440
Awesome.

84
00:06:06,440 --> 00:06:11,560
And one of the areas that you did get to work on, you got a little bit of publicity on.

85
00:06:11,560 --> 00:06:17,120
This grew out of really a side project or a hobby project, is that right?

86
00:06:17,120 --> 00:06:24,680
Yeah, that's right, actually this project that I've been working on to discover planets

87
00:06:24,680 --> 00:06:31,640
with machine learning really grew out of a random sort of chance idea I had while I was

88
00:06:31,640 --> 00:06:35,480
reading an unrelated book.

89
00:06:35,480 --> 00:06:41,800
There's a book called Human Universe by Brian Cox, which is mainly about exploring our

90
00:06:41,800 --> 00:06:47,960
evolution as a species, but it also digs into the question of whether we are unique in

91
00:06:47,960 --> 00:06:49,840
the universe.

92
00:06:49,840 --> 00:06:59,240
And one of the fundamental pieces to that question is whether there are other planets like Earth

93
00:06:59,240 --> 00:07:02,440
out there and how many there are.

94
00:07:02,440 --> 00:07:08,440
And then one of the things he mentioned in this book was that it's actually quite difficult

95
00:07:08,440 --> 00:07:16,840
to detect these planets in sort of around far away stars.

96
00:07:16,840 --> 00:07:23,200
And one of the difficulties is actually digging through this huge amount of data that

97
00:07:23,200 --> 00:07:26,000
is collected by modern satellites.

98
00:07:26,000 --> 00:07:32,160
And so when I read that, I instantly connected that back to my work at Google where we train

99
00:07:32,160 --> 00:07:35,560
models to dig through large data sets all the time.

100
00:07:35,560 --> 00:07:40,240
And so I wondered if perhaps we could apply some of those same techniques to dig through

101
00:07:40,240 --> 00:07:43,520
these large astronomy data sets and look for exoplanets.

102
00:07:43,520 --> 00:07:44,520
Nice.

103
00:07:44,520 --> 00:07:47,160
And this is interesting to me.

104
00:07:47,160 --> 00:07:54,480
I've interviewed several people who have come from physics, including at least one astronomer

105
00:07:54,480 --> 00:08:02,640
Josh Bloom, who's at GE Digital now, who have started from this kind of deep domain

106
00:08:02,640 --> 00:08:09,240
knowledge and learned machine learning and applied it to what they're doing.

107
00:08:09,240 --> 00:08:13,480
But you're coming at it from almost the complete opposite approach, like you have some machine

108
00:08:13,480 --> 00:08:15,440
learning tools.

109
00:08:15,440 --> 00:08:22,120
And then you hear about this really interesting problem in astrophysics and you get involved

110
00:08:22,120 --> 00:08:25,120
and you actually do something really interesting with them.

111
00:08:25,120 --> 00:08:26,720
How did you get started?

112
00:08:26,720 --> 00:08:33,280
Well, you're right, that I actually don't have very much background or knowledge in astronomy

113
00:08:33,280 --> 00:08:34,280
at all.

114
00:08:34,280 --> 00:08:37,400
In fact, I still don't.

115
00:08:37,400 --> 00:08:45,960
And so the first thing I did was I jumped on Google and started looking for what these

116
00:08:45,960 --> 00:08:49,560
data sets actually looks like and who worked with them.

117
00:08:49,560 --> 00:08:55,760
And I was lucky enough to stumble across the name of an astrophysicist who was at the

118
00:08:55,760 --> 00:08:58,960
time working at Harvard, his name is Andrew van der Berg.

119
00:08:58,960 --> 00:09:07,360
And he seemed to be someone who had dealt with these data sets in which we looked for planets.

120
00:09:07,360 --> 00:09:15,080
And so I literally just sent him an email and said, hi, I work at Google in machine learning.

121
00:09:15,080 --> 00:09:19,480
Would you be interested in collaborating on a project together?

122
00:09:19,480 --> 00:09:21,720
And he actually was interested.

123
00:09:21,720 --> 00:09:26,520
And so this whole project has really been a partnership between Andrew and I where I

124
00:09:26,520 --> 00:09:32,760
have taken care of the machine learning side and the coding side.

125
00:09:32,760 --> 00:09:35,640
And he has taken care of the astronomy side.

126
00:09:35,640 --> 00:09:38,960
Tell me a little bit about the timeline of the project.

127
00:09:38,960 --> 00:09:42,200
How long did you spend, have you been working on this?

128
00:09:42,200 --> 00:09:48,480
So I believe I started working on this project in late 2016.

129
00:09:48,480 --> 00:09:54,040
So around September or October, I believe I first emailed Andrew.

130
00:09:54,040 --> 00:09:59,240
And initially I was working on this as you mentioned at the start as a side project.

131
00:09:59,240 --> 00:10:05,000
I was really just devoting 10 or 20% of my time to this for a few months.

132
00:10:05,000 --> 00:10:10,440
But it was around March or April in 2017.

133
00:10:10,440 --> 00:10:16,880
So perhaps six months later that we had managed to train a model that we thought we might

134
00:10:16,880 --> 00:10:21,200
be able to use to search for new exoplanets.

135
00:10:21,200 --> 00:10:29,800
And when we ran this model on a very small sample of stars, we actually found that we were

136
00:10:29,800 --> 00:10:34,120
able to discover some new signals that probably were exoplanets.

137
00:10:34,120 --> 00:10:39,360
And that's when we started taking this project a lot more seriously.

138
00:10:39,360 --> 00:10:45,840
And then I started working on it probably more towards 100% of my time.

139
00:10:45,840 --> 00:10:57,520
And so then for most of last year, for the rest of 2017, we really went back and carefully

140
00:10:57,520 --> 00:11:02,680
trained our model again and used it to actually carefully search these stars.

141
00:11:02,680 --> 00:11:08,360
Andrew did the validation of our new planets that we discovered and then we wrote the paper

142
00:11:08,360 --> 00:11:11,480
and that was published in January of this year.

143
00:11:11,480 --> 00:11:16,800
We'll dig into kind of how things shifted when you started working on this full time.

144
00:11:16,800 --> 00:11:24,040
But I suspect that there are a lot of folks out there who know some machine learning maybe

145
00:11:24,040 --> 00:11:30,680
have taken a deep learning course or working it and would love to apply it on a project

146
00:11:30,680 --> 00:11:32,240
like this.

147
00:11:32,240 --> 00:11:37,720
And so I'm really curious if you can dig into those first six months or even those first

148
00:11:37,720 --> 00:11:38,720
six weeks.

149
00:11:38,720 --> 00:11:44,320
When you're spending just 10, 20% of your time equivalent to what folks might have to

150
00:11:44,320 --> 00:11:48,440
work on a project like this in their nights and weekends.

151
00:11:48,440 --> 00:11:50,920
How did you approach it and what were some of the things you did?

152
00:11:50,920 --> 00:11:58,440
What did the data set look like and how did you set priorities for making progress with

153
00:11:58,440 --> 00:11:59,440
this?

154
00:11:59,440 --> 00:12:00,440
Sure.

155
00:12:00,440 --> 00:12:05,800
So actually when I started, I had this vague notion that we would use machine learning

156
00:12:05,800 --> 00:12:10,280
to search for planets but I didn't even know what the data looked like.

157
00:12:10,280 --> 00:12:17,960
I thought that what we might be doing is taking a model to input photographs from the

158
00:12:17,960 --> 00:12:26,520
sky and actually look for the actual planets in images on the sky.

159
00:12:26,520 --> 00:12:32,600
But it turns out that's not in fact the way that most exoplanets are discovered.

160
00:12:32,600 --> 00:12:38,680
So the format of the data in this case, it's actually not images of the sky.

161
00:12:38,680 --> 00:12:44,680
But rather we look at how the brightness of a star changes over time.

162
00:12:44,680 --> 00:12:50,280
And so if you're monitoring the brightness of a particular star in the sky and if a

163
00:12:50,280 --> 00:12:56,640
planet happens to pass in front of that star relative to the telescope that's recording

164
00:12:56,640 --> 00:13:02,440
the brightness, you'll actually see the brightness of that star dip down while the planet

165
00:13:02,440 --> 00:13:10,360
is blocking some of the star light and then the brightness will then increase again once

166
00:13:10,360 --> 00:13:15,120
the planet is no longer blocking any of the light from the star.

167
00:13:15,120 --> 00:13:20,960
So the data that we're looking at is actually a time series of brightness and what we're

168
00:13:20,960 --> 00:13:27,240
looking for is certain patterns in that brightness time series that would correspond to a planet

169
00:13:27,240 --> 00:13:29,440
passing in front of the star.

170
00:13:29,440 --> 00:13:36,400
Do you get it as a time series of brightness because it's been pre-processed from the

171
00:13:36,400 --> 00:13:43,280
state that you previously expected it to be, meaning there have some telescopes of captured

172
00:13:43,280 --> 00:13:49,200
images and within those images, individual stars are identified and some brightness value

173
00:13:49,200 --> 00:13:54,880
is calculated and kind of logged off into some time series data set or is there something

174
00:13:54,880 --> 00:13:56,440
else happening to produce that data?

175
00:13:56,440 --> 00:13:58,000
Do you have a sense for that?

176
00:13:58,000 --> 00:13:59,880
Yes, no, you're exactly right.

177
00:13:59,880 --> 00:14:08,560
So what happens is this data set has actually been pre-processed by a team at NASA.

178
00:14:08,560 --> 00:14:14,760
So the data originally comes from the Kepler Space Telescope which was in operation for

179
00:14:14,760 --> 00:14:22,640
the main part of its mission for eight years, beginning in 2009, I believe.

180
00:14:22,640 --> 00:14:29,280
Yeah, the main part of its mission was in operation for eight years, beginning in 2009, but there

181
00:14:29,280 --> 00:14:33,000
was looking at this specific section of the sky that we've been searching for the first

182
00:14:33,000 --> 00:14:34,720
four years.

183
00:14:34,720 --> 00:14:41,440
So there's this four-year data set from the Kepler Telescope and a team at NASA has already

184
00:14:41,440 --> 00:14:51,640
taken that data, has localized the stars in that data and converted that data into these

185
00:14:51,640 --> 00:14:56,720
time series and has actually posted that for free on the internet.

186
00:14:56,720 --> 00:14:59,360
You can actually download it.

187
00:14:59,360 --> 00:15:05,560
It takes up several terabytes and in fact, it takes several weeks to download because

188
00:15:05,560 --> 00:15:07,960
you don't get great download speeds.

189
00:15:07,960 --> 00:15:09,520
Even if you're on Google LAN?

190
00:15:09,520 --> 00:15:16,840
Yeah, even if you're on Google LAN, it's millions of files and we literally just had this

191
00:15:16,840 --> 00:15:21,720
big WGet script that would just download all these individual files and it took about

192
00:15:21,720 --> 00:15:24,080
two weeks to download.

193
00:15:24,080 --> 00:15:28,120
But that was going back to your original question here.

194
00:15:28,120 --> 00:15:34,440
That was one of the ways in which we were very lucky in this project.

195
00:15:34,440 --> 00:15:41,560
A lot of the data pre-processing was already taken care of by NASA.

196
00:15:41,560 --> 00:15:49,680
And so not only that, but we were also lucky in that we had a large training set of labelled

197
00:15:49,680 --> 00:15:56,960
examples from astronomers at NASA and at other universities.

198
00:15:56,960 --> 00:16:05,240
So I mentioned that this telescope was launched in 2009, so over eight years ago.

199
00:16:05,240 --> 00:16:13,160
And in that time, a lot of human astronomers have looked at certain patterns in this data

200
00:16:13,160 --> 00:16:20,040
set and have labelled them essentially as being, this is a signal that is a planet and

201
00:16:20,040 --> 00:16:22,680
this other signal is not a planet.

202
00:16:22,680 --> 00:16:25,800
So that was one of the other ways in which we were lucky.

203
00:16:25,800 --> 00:16:28,840
We didn't have to start from scratch with no training set.

204
00:16:28,840 --> 00:16:34,680
We actually had the data and we had a training set there that was already made for us.

205
00:16:34,680 --> 00:16:41,840
And those two reasons are probably key for us being able to make such rapid progress.

206
00:16:41,840 --> 00:16:46,120
And within about six months having all the pieces together and being able to find some

207
00:16:46,120 --> 00:16:48,400
new planet candidates.

208
00:16:48,400 --> 00:16:54,840
So I know that there are many other problems in, for example, astronomy that do have large

209
00:16:54,840 --> 00:17:03,120
amounts of data available, but perhaps that data is not in a form that is easily ingestable

210
00:17:03,120 --> 00:17:10,240
by a machine learning model and also perhaps there is not a training set of labelled examples

211
00:17:10,240 --> 00:17:13,360
that you can use to train a machine learning model.

212
00:17:13,360 --> 00:17:16,840
So I think those considerations are very important.

213
00:17:16,840 --> 00:17:24,800
If somebody is trying to, in their spare time, train a machine learning model to accomplish

214
00:17:24,800 --> 00:17:31,040
any task, the two things you want to ask yourself is, one, does this data exist in a format

215
00:17:31,040 --> 00:17:37,040
that I can, you know, obtain easily and process myself?

216
00:17:37,040 --> 00:17:41,960
And two, is there a training set that I can use to train my model?

217
00:17:41,960 --> 00:17:46,240
And can you tell us a little bit about the model development process?

218
00:17:46,240 --> 00:17:48,160
How did you approach that?

219
00:17:48,160 --> 00:17:49,480
Sure.

220
00:17:49,480 --> 00:17:57,880
So my process when I develop a model is to start simple and then try and build that up

221
00:17:57,880 --> 00:18:01,320
into something more complicated.

222
00:18:01,320 --> 00:18:09,440
And so the simplest thing that I thought I could try with this model was simply a linear

223
00:18:09,440 --> 00:18:17,160
logistic regression model, which is a very common and well understood type of machine learning

224
00:18:17,160 --> 00:18:22,360
model that has been used for decades successfully in a lot of tasks.

225
00:18:22,360 --> 00:18:33,840
And so I had this data from NASA, this time series of brightness values.

226
00:18:33,840 --> 00:18:43,200
And so the first consideration there was how am I going to feed this data into my model

227
00:18:43,200 --> 00:18:46,840
and we can perhaps get into that a little bit more.

228
00:18:46,840 --> 00:18:52,600
But once I had figured that out, I thought, well, let's just try the simplest model I

229
00:18:52,600 --> 00:18:56,240
can possibly think of.

230
00:18:56,240 --> 00:19:01,040
And you know, I think as a general rule of thumb, you know, your simple model should at

231
00:19:01,040 --> 00:19:05,920
least get you part of the way there, perhaps even most of the way there.

232
00:19:05,920 --> 00:19:11,680
And so we were actually able to train this simple model to actually have relatively good

233
00:19:11,680 --> 00:19:18,200
accuracy, the simple model that we ended up training first actually ended up having an

234
00:19:18,200 --> 00:19:21,240
accuracy of about 92%.

235
00:19:21,240 --> 00:19:28,920
And then several months later when we trained our big complicated neural network model,

236
00:19:28,920 --> 00:19:32,040
that model ended up having about 96% accuracy.

237
00:19:32,040 --> 00:19:39,360
So I think the lesson here is that you can certainly start simple and you'll probably

238
00:19:39,360 --> 00:19:44,520
get most of the way there before you start trying these, you know, really complicated

239
00:19:44,520 --> 00:19:45,520
models.

240
00:19:45,520 --> 00:19:51,160
You talked a bit about the data set, but what did the labels look like?

241
00:19:51,160 --> 00:19:57,960
I'm envisioning, I mean, so you've got these, you've got stars and you're trying to

242
00:19:57,960 --> 00:20:06,680
identify whether the stars have planets, are the labels, you know, yes planets, no planets

243
00:20:06,680 --> 00:20:12,200
or they, you know, numbers of planets, because you, part of what you were able to do with

244
00:20:12,200 --> 00:20:21,840
this research is identify new planets in known solar systems, meaning stars that already

245
00:20:21,840 --> 00:20:25,040
had existing planets, right?

246
00:20:25,040 --> 00:20:26,040
Right.

247
00:20:26,040 --> 00:20:27,040
Right.

248
00:20:27,040 --> 00:20:31,800
So I'm imagining the labels have to be more, somewhat more nuance than binary.

249
00:20:31,800 --> 00:20:36,800
Right.

250
00:20:36,800 --> 00:20:42,200
So, yeah, so if we imagine that the data for a single star is a very, very, very long time

251
00:20:42,200 --> 00:20:47,760
series of brightness measurements, actually the individual inputs to the model are not

252
00:20:47,760 --> 00:20:55,480
that entire time series, but their individual events on that time series that some other,

253
00:20:55,480 --> 00:20:59,800
you know, separate computer algorithm, a very basic algorithm that's not machine learning

254
00:20:59,800 --> 00:21:07,680
at all has gone through and has identified certain events on that time series where the brightness

255
00:21:07,680 --> 00:21:09,080
decreases.

256
00:21:09,080 --> 00:21:18,560
So as you said, certain stars can have many events where the brightness decreases, you know,

257
00:21:18,560 --> 00:21:24,680
it may have many planets, but it may also have something called star spots, which is like

258
00:21:24,680 --> 00:21:31,000
a dark spot on the star, and because the star itself is rotating, those dark spots when

259
00:21:31,000 --> 00:21:36,760
that, when the dark spot rotates into the, into the view of the telescope, that's also

260
00:21:36,760 --> 00:21:39,240
going to cause the brightness of the star to decrease.

261
00:21:39,240 --> 00:21:40,240
Right.

262
00:21:40,240 --> 00:21:45,040
So, so there are, there are many different types of events, some of which are astronomical,

263
00:21:45,040 --> 00:21:50,920
some of which are actually instrumental, there can cause the brightness of the star measured

264
00:21:50,920 --> 00:21:54,000
by the telescope to decrease.

265
00:21:54,000 --> 00:21:59,480
And so when we actually, when I'm actually talking about labels, I'm talking about a label

266
00:21:59,480 --> 00:22:05,120
for a particular event on the, on the star where we've actually observed the brightness

267
00:22:05,120 --> 00:22:06,200
decreasing.

268
00:22:06,200 --> 00:22:15,520
So that label is just brightness decreasing, or is it brightness decrease because of a

269
00:22:15,520 --> 00:22:16,520
planet?

270
00:22:16,520 --> 00:22:17,520
Yeah.

271
00:22:17,520 --> 00:22:21,800
So the label is is the, the second option you provided.

272
00:22:21,800 --> 00:22:26,560
So one of the possible labels is brightness decreased because of a planet.

273
00:22:26,560 --> 00:22:32,640
And the other possible labels are brightness decreased because of some astronomical event,

274
00:22:32,640 --> 00:22:34,680
for example, a star spot.

275
00:22:34,680 --> 00:22:42,240
And then the, the third possible label in, in the data set is that the brightness decreased

276
00:22:42,240 --> 00:22:50,040
for some sort of instrumental reason, perhaps there was some noise, instrumental noise,

277
00:22:50,040 --> 00:22:52,800
or, you know, something like that.

278
00:22:52,800 --> 00:22:56,800
And so when we trained the model, so those were really the only three labels in the data

279
00:22:56,800 --> 00:23:00,080
set that, that we were, that we were given that we started with.

280
00:23:00,080 --> 00:23:01,080
Okay.

281
00:23:01,080 --> 00:23:05,840
But when I train, when I train the model, I actually just, I, I binarized this.

282
00:23:05,840 --> 00:23:09,440
So one label was this event is a planet.

283
00:23:09,440 --> 00:23:12,440
And the other event is this event is not a planet.

284
00:23:12,440 --> 00:23:18,400
Can you give me a sense for, you know, how long did you spend kind of start, you know,

285
00:23:18,400 --> 00:23:24,240
familiarizing yourself with the data set and maybe learning a little bit about the background

286
00:23:24,240 --> 00:23:34,560
of the problem and the domain knowledge you might need to, to solve it versus modeling,

287
00:23:34,560 --> 00:23:39,960
versus applying your model to, to unlabeled data.

288
00:23:39,960 --> 00:23:40,960
That kind of thing.

289
00:23:40,960 --> 00:23:41,960
Sure.

290
00:23:41,960 --> 00:23:42,960
Yeah.

291
00:23:42,960 --> 00:23:49,560
And I think that it was about six months before we'd actually train the model and start

292
00:23:49,560 --> 00:23:51,080
a discovering planets.

293
00:23:51,080 --> 00:23:58,480
And certainly I would say that at least four to five of those months were me trying to

294
00:23:58,480 --> 00:24:05,320
understand this data to actually, you know, not only download it, but being able to open

295
00:24:05,320 --> 00:24:12,240
this file format, right, like, what, what, you know, what, what format does a, there's

296
00:24:12,240 --> 00:24:16,960
a, you know, brightness plot of a star come in from NASA.

297
00:24:16,960 --> 00:24:20,840
It ends up coming in a file with a dot FIT S extension.

298
00:24:20,840 --> 00:24:26,240
And so you need to be able to figure out how to open that extension and how to extract

299
00:24:26,240 --> 00:24:31,680
the data that you want and then actually how to pre-process that.

300
00:24:31,680 --> 00:24:40,240
Luckily, I had Andrew, you know, on the other end of the phone who would, who already

301
00:24:40,240 --> 00:24:45,080
had a lot of experience with this, which, which really helped, which really helped me.

302
00:24:45,080 --> 00:24:51,880
But nonetheless, I spent a lot of time just learning how to actually, you know, use this

303
00:24:51,880 --> 00:24:53,560
data and what it meant.

304
00:24:53,560 --> 00:24:56,480
I spent a lot of that time reading papers as well.

305
00:24:56,480 --> 00:25:01,960
There, there have been other people who have applied machine learning to similar problems.

306
00:25:01,960 --> 00:25:09,520
And so I need to understand what they did, you know, I don't want to reinvent the wheel.

307
00:25:09,520 --> 00:25:16,840
You know, I can try and learn from their mistakes and also the things they did that worked well.

308
00:25:16,840 --> 00:25:22,600
And so, you know, once it actually got to coding the model and training the model and testing

309
00:25:22,600 --> 00:25:27,880
the model, that was really only probably one of one out of the first six months.

310
00:25:27,880 --> 00:25:28,880
Wow.

311
00:25:28,880 --> 00:25:29,880
Wow.

312
00:25:29,880 --> 00:25:35,160
What were some of those things that you learn from other attempts that you applied to your

313
00:25:35,160 --> 00:25:37,280
own modeling process?

314
00:25:37,280 --> 00:25:45,160
Sure, yeah, so there are, there are several, there are several steps that are fairly

315
00:25:45,160 --> 00:25:52,080
standard in astronomy for processing these brightness time series.

316
00:25:52,080 --> 00:25:54,640
The brightness time series are typically called light curves.

317
00:25:54,640 --> 00:25:57,840
So I'll just start using the word light curves for them now.

318
00:25:57,840 --> 00:26:03,840
So the first thing you actually need to do with the light curve is actually, you need to flatten

319
00:26:03,840 --> 00:26:07,040
away what's called the stellar variability.

320
00:26:07,040 --> 00:26:12,920
So it turns out that the variability or the light from a star is not actually going to

321
00:26:12,920 --> 00:26:15,560
be constant over time.

322
00:26:15,560 --> 00:26:20,960
For various reasons, the brightness that you'll measure from a star is actually changing

323
00:26:20,960 --> 00:26:23,200
over time quite naturally.

324
00:26:23,200 --> 00:26:27,240
Meaning independent of an event, the star is getting further or whatever?

325
00:26:27,240 --> 00:26:33,280
Well, yeah, there's so there's several reasons, the star, the star is itself rotating.

326
00:26:33,280 --> 00:26:38,600
So different, perhaps different areas of the star's surface are brighter than other areas.

327
00:26:38,600 --> 00:26:47,000
And so when the star rotates, the brightness that you see actually actually changes.

328
00:26:47,000 --> 00:26:53,240
Also there's instrumental effects here that can also cause the sort of the baseline brightness

329
00:26:53,240 --> 00:26:55,200
to kind of drift over time.

330
00:26:55,200 --> 00:27:01,760
That's actually characteristic of the instrument on the Kepler telescope.

331
00:27:01,760 --> 00:27:08,200
So you need to fit away this kind of variability that you see in the brightness of the star.

332
00:27:08,200 --> 00:27:13,360
But you want to do this in such a way that you don't actually, you know, fit away the

333
00:27:13,360 --> 00:27:18,280
events that we're trying to classify, which is when the brightness decreases kind of more

334
00:27:18,280 --> 00:27:20,280
sharply.

335
00:27:20,280 --> 00:27:21,280
Okay.

336
00:27:21,280 --> 00:27:23,280
And suddenly.

337
00:27:23,280 --> 00:27:25,200
So there are various techniques to do that.

338
00:27:25,200 --> 00:27:33,520
And so we were able to use some of those techniques in order to sort of normalize the brightness

339
00:27:33,520 --> 00:27:40,400
of the star so that outside these events we're looking at, the brightness was totally flat.

340
00:27:40,400 --> 00:27:45,400
So that was certainly one of the key starting points for us.

341
00:27:45,400 --> 00:27:51,360
Were these other techniques machine learning types of techniques or more, you know, simpler

342
00:27:51,360 --> 00:27:55,160
data massaging types of approaches?

343
00:27:55,160 --> 00:27:56,160
Right.

344
00:27:56,160 --> 00:28:05,480
So the current, I guess state-of-the-art system for classifying these events as being

345
00:28:05,480 --> 00:28:13,160
planets or not planets is actually, it's called the RoboVetto, it was developed at NASA.

346
00:28:13,160 --> 00:28:18,960
And this is actually mainly not a machine learning technique.

347
00:28:18,960 --> 00:28:24,800
There is one component inside this RoboVetto that uses a machine learning technique.

348
00:28:24,800 --> 00:28:30,880
But for the most part, it's actually a decision tree that has been sort of crafted and honed

349
00:28:30,880 --> 00:28:32,440
by humans.

350
00:28:32,440 --> 00:28:41,200
So this piece of software, which was developed over a few years, actually runs hundreds

351
00:28:41,200 --> 00:28:44,280
of different heuristics on these events.

352
00:28:44,280 --> 00:28:50,360
And it has all these different thresholds and branches to determine whether this event

353
00:28:50,360 --> 00:28:53,360
should be classified as a planet or not.

354
00:28:53,360 --> 00:28:59,480
So that's the main technique that is actually used at the moment.

355
00:28:59,480 --> 00:29:06,840
There have been some other techniques that took on the problem that we were focused on.

356
00:29:06,840 --> 00:29:10,680
One of those used a random forest model.

357
00:29:10,680 --> 00:29:16,000
And another one used a sort of unsupervised clustering type model.

358
00:29:16,000 --> 00:29:24,120
And so we were sort of one of the first to decide to use neural networks for this problem.

359
00:29:24,120 --> 00:29:29,760
Coming back to the thinking about your labeled data, which you mentioned, you kind of

360
00:29:29,760 --> 00:29:39,480
binarized, you turned into, is this event on the light curve due to an orbiting planet

361
00:29:39,480 --> 00:29:41,880
or not?

362
00:29:41,880 --> 00:29:52,360
How do you get from that to being able to detect new orbiting planets on a known star that

363
00:29:52,360 --> 00:29:56,480
has, on a star that already has known orbiting planets?

364
00:29:56,480 --> 00:30:06,160
Or was that a consequence not necessarily related or produced by your algorithm?

365
00:30:06,160 --> 00:30:07,160
Sure.

366
00:30:07,160 --> 00:30:12,680
So I think I mentioned that these events that we detect on the light curve where the brightness

367
00:30:12,680 --> 00:30:19,560
dips are actually produced by a completely separate algorithm.

368
00:30:19,560 --> 00:30:27,920
And traditionally, what would happen was that, well, the astronomers would run some algorithm

369
00:30:27,920 --> 00:30:38,760
and would take a whole large set of these events that were detected and manually examine

370
00:30:38,760 --> 00:30:42,760
each of them by eye and decide which of those were caused by planets and which of those

371
00:30:42,760 --> 00:30:46,600
were caused by various other events.

372
00:30:46,600 --> 00:30:53,920
And so because this process was, you know, the time required was really dominated by the

373
00:30:53,920 --> 00:31:00,400
fact that the humans had to go in and examine these things, it wasn't possible to examine

374
00:31:00,400 --> 00:31:03,080
every possible signal.

375
00:31:03,080 --> 00:31:11,360
So what they did was they set some threshold and said, okay, we're only going to examine

376
00:31:11,360 --> 00:31:17,960
events detected above some certain signal to noise thresholds.

377
00:31:17,960 --> 00:31:23,760
And so now even with the signal to noise threshold, they still actually had to go through

378
00:31:23,760 --> 00:31:29,960
and examine over 30,000 events from the four-year capital emission.

379
00:31:29,960 --> 00:31:36,200
So it wasn't like they chose a really, really aggressive threshold.

380
00:31:36,200 --> 00:31:43,000
But the possibility still remained that if the threshold was lowered just a little bit

381
00:31:43,000 --> 00:31:50,600
and some of these, you know, lower signal to noise events, you know, could be considered

382
00:31:50,600 --> 00:31:55,800
that there may be some planets that were missed by the previous searches.

383
00:31:55,800 --> 00:31:58,840
And so that's what we looked at in this project.

384
00:31:58,840 --> 00:32:06,920
We took our train model and we used that model to go and look at these events that had not

385
00:32:06,920 --> 00:32:13,320
previously been classified because the signal to noise was below the traditional thresholds.

386
00:32:13,320 --> 00:32:18,920
And so that was how we were able to discover new planets around stars that had already

387
00:32:18,920 --> 00:32:21,920
been searched, you know, multiple times before.

388
00:32:21,920 --> 00:32:29,520
Did you run the algorithm against kind of all of the remaining data and thus you have,

389
00:32:29,520 --> 00:32:35,720
you know, we collectively now have a fairly high confidence that we found all of the possible

390
00:32:35,720 --> 00:32:44,160
exoplanets or was, you know, because of the data volume or whatever, you know, you also

391
00:32:44,160 --> 00:32:50,280
had some threshold or some slice of the data and there's still an opportunity to apply

392
00:32:50,280 --> 00:32:51,880
them or broadly.

393
00:32:51,880 --> 00:32:58,160
So out of the 200,000 stars in this data set, we've actually only run our model over 600

394
00:32:58,160 --> 00:32:59,160
meters.

395
00:32:59,160 --> 00:33:00,160
Wow.

396
00:33:00,160 --> 00:33:01,160
Okay.

397
00:33:01,160 --> 00:33:11,200
So we chose, we chose those 670 because those stars have already had already had multiple

398
00:33:11,200 --> 00:33:14,080
orbiting planets discovered around them.

399
00:33:14,080 --> 00:33:21,320
And it's a lot more likely that you're going to discover more planets around stars that

400
00:33:21,320 --> 00:33:27,080
we've already discovered planets around than if you just chose some random star.

401
00:33:27,080 --> 00:33:32,080
So we really chose these 670 because we just wanted to, you know, have the quickest test

402
00:33:32,080 --> 00:33:34,280
of our model possible.

403
00:33:34,280 --> 00:33:40,520
And so we restricted ourselves to this tiny sample and actually, you know, discovered

404
00:33:40,520 --> 00:33:45,600
two new planets and we were very excited about this and, you know, we decided to stop and

405
00:33:45,600 --> 00:33:46,600
write our paper.

406
00:33:46,600 --> 00:33:53,520
But certainly what we're currently doing right now is working on ramping up our current

407
00:33:53,520 --> 00:33:56,960
pipeline to actually search all 200,000 stars.

408
00:33:56,960 --> 00:34:00,960
And, you know, who knows what we're going to find when we do that.

409
00:34:00,960 --> 00:34:07,400
Talk a little bit about the process and approach you took once you switch gears from the more

410
00:34:07,400 --> 00:34:09,880
traditional approach to applying a neural net.

411
00:34:09,880 --> 00:34:13,520
I think you said you were using a logistic regression originally?

412
00:34:13,520 --> 00:34:14,520
Right.

413
00:34:14,520 --> 00:34:15,520
That's right.

414
00:34:15,520 --> 00:34:16,520
Yes.

415
00:34:16,520 --> 00:34:22,440
So the logistic regression model really just took in, say, one of these light curves, one

416
00:34:22,440 --> 00:34:27,960
of these time series and simply use those as features.

417
00:34:27,960 --> 00:34:34,880
Each independent point on the light curve was an independent feature to the model.

418
00:34:34,880 --> 00:34:42,440
And so the model could learn, for example, if this particular pixel, we fed our model,

419
00:34:42,440 --> 00:34:48,440
we fed the events into the model such that, you know, we'd normalized where we thought

420
00:34:48,440 --> 00:34:56,160
the light, the light was decreasing, we actually sort of like centered the event in the input.

421
00:34:56,160 --> 00:34:59,920
So the model knew where the event was supposed to be.

422
00:34:59,920 --> 00:35:05,760
And so the model could learn things like if this particular pixel has a brightness below

423
00:35:05,760 --> 00:35:12,720
average, you know, that might contribute to a classification of, you know, it being a

424
00:35:12,720 --> 00:35:18,280
planet, or it not being a planet, but essentially in the logistic regression model, each of those

425
00:35:18,280 --> 00:35:25,760
individual pixels was going to be considered independently of all the other pixels, essentially

426
00:35:25,760 --> 00:35:33,200
they were all going to be used as independent indicators of whether this time series,

427
00:35:33,200 --> 00:35:35,000
this light curve was a planet model.

428
00:35:35,000 --> 00:35:45,000
So your input vector to the logistic regression model was a vector of kind of the four years

429
00:35:45,000 --> 00:35:52,240
worth of these brightness values for a particular pixel at whatever time increment, they were

430
00:35:52,240 --> 00:35:53,880
captured at.

431
00:35:53,880 --> 00:36:00,160
So we didn't actually feed in all four years of data in one very, very long vector.

432
00:36:00,160 --> 00:36:06,080
What we did was we actually fed in just one of the events basically that we were looking

433
00:36:06,080 --> 00:36:07,080
at.

434
00:36:07,080 --> 00:36:14,040
So we, we really kind of like zoomed in on the light curve of the particular event that

435
00:36:14,040 --> 00:36:18,680
we wanted to know the classification for.

436
00:36:18,680 --> 00:36:27,320
So our, the dimensionality of our input vector was something like like 1000, 1000 time series

437
00:36:27,320 --> 00:36:30,920
points as opposed to all the time series points over four years.

438
00:36:30,920 --> 00:36:37,000
And did you presumably, like you, you knew where the event was so you bracket it around

439
00:36:37,000 --> 00:36:38,000
it?

440
00:36:38,000 --> 00:36:39,000
Exactly.

441
00:36:39,000 --> 00:36:44,400
We made sure that in our input, the event that we were considering was, was centered.

442
00:36:44,400 --> 00:36:45,400
Got it.

443
00:36:45,400 --> 00:36:46,400
Okay.

444
00:36:46,400 --> 00:36:53,160
So moving from sort of the, the simple model, the logistic regression model, you know,

445
00:36:53,160 --> 00:37:01,920
so, so one, one parallel we, we noticed with this problem was is, is to the problem of

446
00:37:01,920 --> 00:37:05,200
classifying objects and images.

447
00:37:05,200 --> 00:37:10,640
So with an image, your input is actually a two dimensional grid.

448
00:37:10,640 --> 00:37:14,920
But what you're doing is looking for, essentially looking for local patterns within that grid,

449
00:37:14,920 --> 00:37:24,520
you know, local groups of pixels will together form, you know, edges and shapes and images.

450
00:37:24,520 --> 00:37:29,440
And so what we had, we had it, a one dimensional grid or a vector.

451
00:37:29,440 --> 00:37:34,280
But we were also looking for local patterns, you know, in the grid.

452
00:37:34,280 --> 00:37:40,480
We were looking for shapes in which the light would gradually decrease as the planet sort

453
00:37:40,480 --> 00:37:46,680
of passes in front of the star and then gradually increase again, sort of like a U shape.

454
00:37:46,680 --> 00:37:54,080
And so with that parallel to image classification, we decided to use a convolutional neural network,

455
00:37:54,080 --> 00:37:59,480
which has been very successful for image classification in the past.

456
00:37:59,480 --> 00:38:03,840
And the, the only real difference here is that our inputs are one dimensional, rather

457
00:38:03,840 --> 00:38:11,480
than two or three dimensional. And so we, we used one-day convolutions and, and other

458
00:38:11,480 --> 00:38:17,520
than that, our model is very similar to sort of a classic convolutional neural network.

459
00:38:17,520 --> 00:38:23,560
And did you start with a, did you build it up from kind of scratch, or did you start

460
00:38:23,560 --> 00:38:29,320
with a, a deep kind of existing, deep network architecture?

461
00:38:29,320 --> 00:38:37,960
No, it's, it's actually a pretty simple classic architecture that I built up from scratch.

462
00:38:37,960 --> 00:38:46,040
So essentially, it's just convolutional layers, followed by max pooling layers for say 10

463
00:38:46,040 --> 00:38:51,360
or 12, 12 layers deep. I don't remember exactly how many layers we used, followed by some

464
00:38:51,360 --> 00:38:57,680
fully connected layers, followed by the output, which is the probability that this particular

465
00:38:57,680 --> 00:38:58,680
input is applied.

466
00:38:58,680 --> 00:39:04,640
And did the model evolve much at all as you began working with the data, or did that initial

467
00:39:04,640 --> 00:39:10,960
model that you, that you kind of settled on kind of hold throughout the project?

468
00:39:10,960 --> 00:39:16,560
Probably the, the biggest change that we made over the project was actually to feed in

469
00:39:16,560 --> 00:39:22,000
two separate representations of the light curve that we were looking at, of the event.

470
00:39:22,000 --> 00:39:25,560
I should say the event in the light curve that we were looking at.

471
00:39:25,560 --> 00:39:29,800
So this one of these was a, was a wide view of the event.

472
00:39:29,800 --> 00:39:35,000
And the other view was a, a very zoomed in narrow view of the event.

473
00:39:35,000 --> 00:39:43,440
And so what we, what we learned was that the model sometimes needed some information that

474
00:39:43,440 --> 00:39:51,640
was very, very far away from the, the event we were looking at in order to make its classification

475
00:39:51,640 --> 00:39:53,320
decision.

476
00:39:53,320 --> 00:40:00,080
And so that, that was sort of facilitated by feeding in a very, a wide view of the

477
00:40:00,080 --> 00:40:01,920
light curve.

478
00:40:01,920 --> 00:40:08,920
But also the model needs a fairly fine grained look at the, the shape of the light curve

479
00:40:08,920 --> 00:40:11,160
is, of the event as well.

480
00:40:11,160 --> 00:40:16,080
Sorry, as I mentioned, we were looking for U shaped patterns, but there are various,

481
00:40:16,080 --> 00:40:20,760
other astronomical events that can cause, for example, a V shaped pattern.

482
00:40:20,760 --> 00:40:26,560
And so what we did was we, we ultimately ended up feeding in two separate representations

483
00:40:26,560 --> 00:40:29,080
of the event into the model.

484
00:40:29,080 --> 00:40:35,720
And so each representation is treated by sort of individual, convolutional layers that

485
00:40:35,720 --> 00:40:36,720
are totally separate.

486
00:40:36,720 --> 00:40:42,360
And then they're, they're joined up towards the, the end and combined together to, to give

487
00:40:42,360 --> 00:40:44,360
the final output prediction.

488
00:40:44,360 --> 00:40:45,360
Okay.

489
00:40:45,360 --> 00:40:50,120
And maybe tell us a little bit about like how the time has been allocated like since

490
00:40:50,120 --> 00:40:55,120
that six month mark when you started applying the neural network.

491
00:40:55,120 --> 00:40:56,120
Right.

492
00:40:56,120 --> 00:41:02,920
So we spent a lot of time last year writing the paper.

493
00:41:02,920 --> 00:41:12,560
So some of the things that we, we did for that was come up with various ways to visualize

494
00:41:12,560 --> 00:41:15,760
the sort of things that the model had learned.

495
00:41:15,760 --> 00:41:22,560
You know, especially as this paper was going to be published in an astronomy journal rather

496
00:41:22,560 --> 00:41:28,800
than a machine learning journal, where people are perhaps not quite as familiar as, as,

497
00:41:28,800 --> 00:41:33,040
you know, the machine learning community in what exactly a convolutional neural network

498
00:41:33,040 --> 00:41:35,720
is and what it's doing.

499
00:41:35,720 --> 00:41:42,520
So we used some techniques, for example, to visualize the features of the input that

500
00:41:42,520 --> 00:41:45,720
the model had learned.

501
00:41:45,720 --> 00:41:50,360
And that was also a good sanity check for us to make sure the model was, was sort of

502
00:41:50,360 --> 00:41:55,920
looking at the right features in order to make its decision.

503
00:41:55,920 --> 00:42:03,120
So the, the technique we applied there, which is, which is quite a simple one is if, if

504
00:42:03,120 --> 00:42:08,720
you block out part of the input and then fear the input into the model and then you look

505
00:42:08,720 --> 00:42:13,520
at the model's prediction, if the, if the part of the input that you blocked out was really

506
00:42:13,520 --> 00:42:18,960
important for the model's decision, then, then the decision is going to change a lot.

507
00:42:18,960 --> 00:42:24,160
But if you block out sort of an unimportant region of the input, then the model's decision

508
00:42:24,160 --> 00:42:28,920
is, is not going to change, you know, at all perhaps.

509
00:42:28,920 --> 00:42:34,560
And what we found, and what we found was, was when we blocked out the regions that were,

510
00:42:34,560 --> 00:42:39,960
that, that, you know, certainly we thought, indicated that a planet had passed in front

511
00:42:39,960 --> 00:42:45,960
of the star, the model's prediction changed from predicting this was a planet to predicting

512
00:42:45,960 --> 00:42:48,920
it wasn't a planet, which was exactly what we had hoped.

513
00:42:48,920 --> 00:42:54,880
It means that we, we weren't sort of fitting to other aspects of the, of the like curve.

514
00:42:54,880 --> 00:43:00,760
We were really fitting our model was learning the features that we, we expected.

515
00:43:00,760 --> 00:43:06,160
So we spent, we spent quite a bit of time really digging into the model and, and producing

516
00:43:06,160 --> 00:43:13,160
sort of visualizations and explanations of its, of its predictions to, to put in the

517
00:43:13,160 --> 00:43:14,160
paper.

518
00:43:14,160 --> 00:43:15,160
Awesome.

519
00:43:15,160 --> 00:43:20,160
And you're going to be publishing some additional information about that this week is

520
00:43:20,160 --> 00:43:24,160
that to remember correctly, you're going to be publishing some of the code that you

521
00:43:24,160 --> 00:43:25,160
used as well.

522
00:43:25,160 --> 00:43:26,360
Yeah, that's right.

523
00:43:26,360 --> 00:43:32,480
So we published our paper this January, which outlines everything that we did.

524
00:43:32,480 --> 00:43:40,640
But this week, we're actually releasing all of the code that we used to train our model,

525
00:43:40,640 --> 00:43:44,720
which also includes actually the, you know, all the code you need to download the data

526
00:43:44,720 --> 00:43:51,360
from NASA's servers to process the data in the exact way that we did in the paper.

527
00:43:51,360 --> 00:43:56,720
I mentioned like fitting away the, the low frequency variability in the stars brightness,

528
00:43:56,720 --> 00:43:59,360
that's included as well.

529
00:43:59,360 --> 00:44:05,120
You know, all of this, the specifications for our model are included and the hyper parameters

530
00:44:05,120 --> 00:44:07,600
that we use to train the model.

531
00:44:07,600 --> 00:44:13,320
And so anyone will be able to download this data, we'll be able to, to train a model.

532
00:44:13,320 --> 00:44:18,600
And, you know, potentially they'll be able to use this to go searching.

533
00:44:18,600 --> 00:44:19,600
That's awesome.

534
00:44:19,600 --> 00:44:23,600
I hope being, if anything, to see out of this.

535
00:44:23,600 --> 00:44:25,440
Well, a few things.

536
00:44:25,440 --> 00:44:32,520
I'm hoping that some people are able to, perhaps, build on the work that we did.

537
00:44:32,520 --> 00:44:40,680
So as I mentioned, our project has been focused on data from NASA's Kepler telescope, which

538
00:44:40,680 --> 00:44:43,120
has been in operation for eight years now.

539
00:44:43,120 --> 00:44:49,600
But there's another telescope launching this year, or it's at least scheduled to launch

540
00:44:49,600 --> 00:44:55,920
this year by NASA, which is called the, the test, T-E-WS satellite.

541
00:44:55,920 --> 00:45:03,640
And that is going to use sort of a very similar approach to the Kepler satellite for recording

542
00:45:03,640 --> 00:45:09,440
this brightness data for many, many, many more stars even than Kepler did.

543
00:45:09,440 --> 00:45:15,960
And so I'm hoping that, you know, others in the astronomy community will perhaps build

544
00:45:15,960 --> 00:45:23,240
on this, this, this code that I'm releasing, and perhaps even apply it to, to other missions

545
00:45:23,240 --> 00:45:30,240
and, and use it to, to discover planets in, in data collected by the tests.

546
00:45:30,240 --> 00:45:31,240
That's awesome.

547
00:45:31,240 --> 00:45:32,240
Awesome.

548
00:45:32,240 --> 00:45:36,160
Well, that, that is some really amazing work.

549
00:45:36,160 --> 00:45:40,320
I appreciate you taking the time to share it with us.

550
00:45:40,320 --> 00:45:41,320
Great.

551
00:45:41,320 --> 00:45:42,320
Yeah.

552
00:45:42,320 --> 00:45:44,560
Thanks a lot for having me on.

553
00:45:44,560 --> 00:45:47,800
All right, everyone, that's our show for today.

554
00:45:47,800 --> 00:45:53,160
For more information on Chris, or any of the topics covered in this episode, you'll find

555
00:45:53,160 --> 00:45:59,280
the show notes at twimmaleye.com slash talk slash 117.

556
00:45:59,280 --> 00:46:04,000
If you have any questions for Chris, please post them there, and we'll make sure to bring

557
00:46:04,000 --> 00:46:06,440
them to his attention.

558
00:46:06,440 --> 00:46:11,000
If you're new to the pod and like what you hear, or you're a veteran listener and haven't

559
00:46:11,000 --> 00:46:16,560
already done so, head on over to your favorite podcast app and leave us your most gracious

560
00:46:16,560 --> 00:46:18,400
review and rating.

561
00:46:18,400 --> 00:46:22,160
It helps new listeners find us, which helps us grow.

562
00:46:22,160 --> 00:46:23,680
Thanks in advance.

563
00:46:23,680 --> 00:46:35,760
And of course, thanks so much for listening, and catch you next time.

